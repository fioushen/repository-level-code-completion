{"filename": "twitter_ae_training_for_generated_prompt_multitasks.py", "chunked_list": ["import argparse\nimport json\nimport os\nfrom datetime import datetime\nfrom torch import optim\nimport torch\nimport torch.multiprocessing as mp\nfrom torch.cuda.amp import GradScaler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, ConcatDataset", "from torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.tensorboard import SummaryWriter\nfrom transformers import AdamW\nimport random\nfrom src.data.collation_for_prompt_multitasks import Collator\nfrom src.data.dataset_for_prompt import MVSA_Dataset, Twitter_Dataset\nfrom src.data.tokenization_new_for_generated_prompt_multitasks import ConditionTokenizer\nfrom src.model.config import MultiModalBartConfig", "from src.data.tokenization_new_for_generated_prompt_multitasks import ConditionTokenizer\nfrom src.model.config import MultiModalBartConfig\nfrom src.model.MAESC_model_for_generated_aspect_prompt_multitasks import MultiModalBartModel_AESC\nfrom src.model.model import MultiModalBartModelForPretrain\nfrom src.training_multitasks import fine_tune\nfrom src.utils import Logger, save_training_data, load_training_data, setup_process, cleanup_process\nfrom src.model.metrics import AESCSpanMetric, OESpanMetric\nfrom src.model.generater_for_generated_prompt_multitasks import SequenceGeneratorModel\nimport src.eval_utils_multitasks as eval_utils\nimport numpy as np", "import src.eval_utils_multitasks as eval_utils\nimport numpy as np\nimport torch.backends.cudnn as cudnn\n\ndef get_parameter_number(model):\n    total_num = sum(p.numel() for p in model.parameters())\n    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return {'Total': total_num, 'Trainable': trainable_num}\n\ndef main(rank, args):\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    checkpoint_path = os.path.join(args.checkpoint_dir, timestamp)\n    tb_writer = None\n    add_name = 'epoch_num' + str(args.epochs)\n    add_name += 'last'\n\n    if args.is_sample:\n        add_name += 'sample_num' + str(args.sample_num)\n        add_name += 'start_idx' + str(args.start_idx)\n    if args.text_only:\n        add_name += ' only text'\n    else:\n        add_name += ' multi'\n    if args.bart_init == 0:\n        add_name += '_random_init_'\n    if args.checkpoint:\n        add_name = add_name + '-pretrain' + args.checkpoint.split('/')[-2]\n\n    add_name = add_name + str(args.lr)\n    log_dir = os.path.join(args.log_dir, timestamp + add_name)\n\n    # make log dir and tensorboard writer if log_dir is specified\n    if rank == 0 and args.log_dir is not None:\n        os.makedirs(log_dir)\n        tb_writer = SummaryWriter(log_dir=log_dir)\n\n    logger = Logger(log_dir=os.path.join(log_dir, 'log.txt'),\n                    enabled=(rank == 0))\n    # prf_logger = Logger(log_dir=os.path.join(log_dir, 'prf_log.txt'),\n    #                     enabled=(rank == 0))\n\n    # make checkpoint dir if not exist\n    if args.is_check == 1 and not os.path.isdir(checkpoint_path):\n        os.makedirs(checkpoint_path)\n        logger.info('Made checkpoint directory: \"{}\"'.format(checkpoint_path))\n\n    logger.info('Initialed with {} GPU(s)'.format(args.gpu_num), pad=True)\n    for k, v in vars(args).items():\n        logger.info('{}: {}'.format(k, v))\n\n    # =========================== model =============================\n\n    logger.info('Loading model...')\n\n    if args.cpu:\n        device = 'cpu'\n        map_location = device\n    else:\n        device = torch.device(\"cuda:{}\".format(rank))\n        map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n\n    tokenizer = ConditionTokenizer(args)\n    label_ids = list(tokenizer.mapping2id.values())\n    senti_ids = list(tokenizer.senti2id.values())\n    # print(label_ids)\n    # print(tokenizer.convert_ids_to_tokens(label_ids))\n    if args.model_config is not None:\n        bart_config = MultiModalBartConfig.from_dict(\n            json.load(open(args.model_config)))\n    else:\n        bart_config = MultiModalBartConfig.from_pretrained(args.checkpoint)\n\n    if args.dropout is not None:\n        bart_config.dropout = args.dropout\n    if args.attention_dropout is not None:\n        bart_config.attention_dropout = args.attention_dropout\n    if args.classif_dropout is not None:\n        bart_config.classif_dropout = args.classif_dropout\n    if args.activation_dropout is not None:\n        bart_config.activation_dropout = args.activation_dropout\n\n    bos_token_id = 0  # \u56e0\u4e3a\u662f\u7279\u6b8a\u7b26\u53f7\n    eos_token_id = 1\n\n    if args.checkpoint:\n        pretrain_model = MultiModalBartModelForPretrain.from_pretrained(\n            args.checkpoint,\n            config=bart_config,\n            bart_model=args.bart_model,\n            tokenizer=tokenizer,\n            label_ids=label_ids,\n            senti_ids=senti_ids,\n            args=args,\n            error_on_mismatch=False)\n        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n                                                 args.bart_model, tokenizer,\n                                                 label_ids)\n        seq2seq_model.encoder.load_state_dict(\n            pretrain_model.encoder.state_dict())\n        seq2seq_model.decoder.load_state_dict(\n            pretrain_model.span_decoder.state_dict())\n        model = SequenceGeneratorModel(seq2seq_model,\n                                       bos_token_id=bos_token_id,\n                                       eos_token_id=eos_token_id,\n                                       max_length=args.max_len,\n                                       max_len_a=args.max_len_a,\n                                       num_beams=args.num_beams,\n                                       do_sample=False,\n                                       repetition_penalty=1,\n                                       length_penalty=1.0,\n                                       pad_token_id=eos_token_id,\n                                       restricter=None)\n    else:\n        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n                                                 args.bart_model, tokenizer,\n                                                 label_ids)\n        model = SequenceGeneratorModel(seq2seq_model,\n                                       bos_token_id=bos_token_id,\n                                       eos_token_id=eos_token_id,\n                                       max_length=args.max_len,\n                                       max_len_a=args.max_len_a,\n                                       num_beams=args.num_beams,\n                                       do_sample=False,\n                                       repetition_penalty=1,\n                                       length_penalty=1.0,\n                                       pad_token_id=eos_token_id,\n                                       restricter=None)\n        # model = MultiModalBartModel_AESC(bart_config, args.bart_model,\n        #                                  tokenizer, label_ids)\n    model.to(device)\n    parameters = get_parameter_number(model) ##{'Total': 169351685, 'Trainable': 169351685}\n    print(parameters)\n\n    optimizer = AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n\n    scaler = GradScaler() if args.amp else None\n\n    epoch = 0\n    logger.info('Loading data...')\n    collate_twitter_ae = Collator(\n                                  task=args.task,\n                                  tokenizer=tokenizer,\n                                  mlm_enabled=False,\n                                  senti_enabled=False,\n                                  ae_enabled=False,\n                                  oe_enabled=False,\n                                  aesc_enabled=False,\n                                  anp_enabled=False,\n                                  twitter_ae_enabled=True,\n                                  has_prompt=args.has_prompt,\n                                  max_img_num=args.num_image_tokens,\n                                  text_only=args.text_only,\n                                  use_caption=args.use_caption)\n\n    train_dataset = Twitter_Dataset(args.dataset[0][1], split='train')\n\n    dev_dataset = Twitter_Dataset(args.dataset[0][1], split='dev')\n    test_dataset = Twitter_Dataset(args.dataset[0][1], split='test')\n\n    train_loader = DataLoader(dataset=train_dataset,\n                              batch_size=args.batch_size,\n                              shuffle=True,\n                              num_workers=args.num_workers,\n                              pin_memory=True,\n                              collate_fn=collate_twitter_ae)\n    dev_loader = DataLoader(dataset=dev_dataset,\n                            batch_size=args.batch_size,\n                            shuffle=False,\n                            num_workers=args.num_workers,\n                            pin_memory=True,\n                            collate_fn=collate_twitter_ae)\n    test_loader = DataLoader(dataset=test_dataset,\n                             batch_size=args.batch_size,\n                             shuffle=False,\n                             num_workers=args.num_workers,\n                             pin_memory=True,\n                             collate_fn=collate_twitter_ae)\n\n    callback = None\n    metric = OESpanMetric(eos_token_id, num_labels=len(label_ids))\n    model.train()\n    start = datetime.now()\n    best_dev_res = None\n    best_dev_test_res = None\n    best_test_res = None\n\n    while epoch < args.epochs:\n        logger.info('Epoch {}'.format(epoch + 1), pad=True)\n        fine_tune(epoch=epoch,\n                  model=model,\n                  train_loader=train_loader,\n                  test_loader=test_loader,\n                  metric=metric,\n                  optimizer=optimizer,\n                  args=args,\n                  device=device,\n                  logger=logger,\n                  callback=callback,\n                  log_interval=1,\n                  tb_writer=tb_writer,\n                  tb_interval=1,\n                  scaler=scaler)\n\n        if (epoch + 1) % args.eval_every == 0:\n            # train_dev = eval_utils.eval(model, train_loader, metric, device)\n\n            \n            res_dev, dev_aspects_num_acc = eval_utils.eval(args, model, dev_loader, metric, device)\n            res_test, test_aspects_num_acc = eval_utils.eval(args, model, test_loader, metric, device)\n\n        \n\n            logger.info('DEV  ae_p:{} ae_r:{} ae_f:{}, dev_aspects_num_acc: {:.4f}'.format(\n                res_dev['oe_pre'], res_dev['oe_rec'], res_dev['oe_f'], dev_aspects_num_acc))\n            logger.info('TEST  ae_p:{} ae_r:{} ae_f:{}, test_aspects_num_acc: {:.4f}'.format(\n                res_test['oe_pre'], res_test['oe_rec'], res_test['oe_f'], test_aspects_num_acc))\n            # logger.info('DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n            #     res_dev['ae_pre'], res_dev['ae_rec'], res_dev['ae_f']))\n            save_flag = False\n            if best_dev_res is None:\n                best_dev_res = res_dev\n                best_dev_test_res = res_test\n\n            else:\n                if best_dev_res['oe_f'] < res_dev['oe_f']:\n                    best_dev_res = res_dev\n                    best_dev_test_res = res_test\n\n            if best_test_res is None:\n                best_test_res = res_test\n                save_flag = True\n            else:\n                if best_test_res['oe_f'] < res_test['oe_f']:\n                    best_test_res = res_test\n                    save_flag = True\n\n            if args.is_check == 1 and save_flag:\n                current_checkpoint_path = os.path.join(checkpoint_path,\n                                                       args.check_info)\n                model.seq2seq_model.save_pretrained(current_checkpoint_path)\n                print('save model!!!!!!!!!!!')\n        epoch += 1\n    logger.info(\"Training complete in: \" + str(datetime.now() - start),\n                pad=True)\n    logger.info('---------------------------')\n    logger.info('BEST DEV:-----')\n    logger.info('BEST DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n        best_dev_res['oe_pre'], best_dev_res['oe_rec'], best_dev_res['oe_f']))\n\n    logger.info('BEST DEV TEST:-----')\n    logger.info('BEST DEV--TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n        best_dev_test_res['oe_pre'], best_dev_test_res['oe_rec'],\n        best_dev_test_res['oe_f']))\n\n    logger.info('BEST TEST:-----')\n    logger.info('BEST TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n        best_test_res['oe_pre'], best_test_res['oe_rec'],\n        best_test_res['oe_f']))", "\ndef main(rank, args):\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    checkpoint_path = os.path.join(args.checkpoint_dir, timestamp)\n    tb_writer = None\n    add_name = 'epoch_num' + str(args.epochs)\n    add_name += 'last'\n\n    if args.is_sample:\n        add_name += 'sample_num' + str(args.sample_num)\n        add_name += 'start_idx' + str(args.start_idx)\n    if args.text_only:\n        add_name += ' only text'\n    else:\n        add_name += ' multi'\n    if args.bart_init == 0:\n        add_name += '_random_init_'\n    if args.checkpoint:\n        add_name = add_name + '-pretrain' + args.checkpoint.split('/')[-2]\n\n    add_name = add_name + str(args.lr)\n    log_dir = os.path.join(args.log_dir, timestamp + add_name)\n\n    # make log dir and tensorboard writer if log_dir is specified\n    if rank == 0 and args.log_dir is not None:\n        os.makedirs(log_dir)\n        tb_writer = SummaryWriter(log_dir=log_dir)\n\n    logger = Logger(log_dir=os.path.join(log_dir, 'log.txt'),\n                    enabled=(rank == 0))\n    # prf_logger = Logger(log_dir=os.path.join(log_dir, 'prf_log.txt'),\n    #                     enabled=(rank == 0))\n\n    # make checkpoint dir if not exist\n    if args.is_check == 1 and not os.path.isdir(checkpoint_path):\n        os.makedirs(checkpoint_path)\n        logger.info('Made checkpoint directory: \"{}\"'.format(checkpoint_path))\n\n    logger.info('Initialed with {} GPU(s)'.format(args.gpu_num), pad=True)\n    for k, v in vars(args).items():\n        logger.info('{}: {}'.format(k, v))\n\n    # =========================== model =============================\n\n    logger.info('Loading model...')\n\n    if args.cpu:\n        device = 'cpu'\n        map_location = device\n    else:\n        device = torch.device(\"cuda:{}\".format(rank))\n        map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n\n    tokenizer = ConditionTokenizer(args)\n    label_ids = list(tokenizer.mapping2id.values())\n    senti_ids = list(tokenizer.senti2id.values())\n    # print(label_ids)\n    # print(tokenizer.convert_ids_to_tokens(label_ids))\n    if args.model_config is not None:\n        bart_config = MultiModalBartConfig.from_dict(\n            json.load(open(args.model_config)))\n    else:\n        bart_config = MultiModalBartConfig.from_pretrained(args.checkpoint)\n\n    if args.dropout is not None:\n        bart_config.dropout = args.dropout\n    if args.attention_dropout is not None:\n        bart_config.attention_dropout = args.attention_dropout\n    if args.classif_dropout is not None:\n        bart_config.classif_dropout = args.classif_dropout\n    if args.activation_dropout is not None:\n        bart_config.activation_dropout = args.activation_dropout\n\n    bos_token_id = 0  # \u56e0\u4e3a\u662f\u7279\u6b8a\u7b26\u53f7\n    eos_token_id = 1\n\n    if args.checkpoint:\n        pretrain_model = MultiModalBartModelForPretrain.from_pretrained(\n            args.checkpoint,\n            config=bart_config,\n            bart_model=args.bart_model,\n            tokenizer=tokenizer,\n            label_ids=label_ids,\n            senti_ids=senti_ids,\n            args=args,\n            error_on_mismatch=False)\n        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n                                                 args.bart_model, tokenizer,\n                                                 label_ids)\n        seq2seq_model.encoder.load_state_dict(\n            pretrain_model.encoder.state_dict())\n        seq2seq_model.decoder.load_state_dict(\n            pretrain_model.span_decoder.state_dict())\n        model = SequenceGeneratorModel(seq2seq_model,\n                                       bos_token_id=bos_token_id,\n                                       eos_token_id=eos_token_id,\n                                       max_length=args.max_len,\n                                       max_len_a=args.max_len_a,\n                                       num_beams=args.num_beams,\n                                       do_sample=False,\n                                       repetition_penalty=1,\n                                       length_penalty=1.0,\n                                       pad_token_id=eos_token_id,\n                                       restricter=None)\n    else:\n        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n                                                 args.bart_model, tokenizer,\n                                                 label_ids)\n        model = SequenceGeneratorModel(seq2seq_model,\n                                       bos_token_id=bos_token_id,\n                                       eos_token_id=eos_token_id,\n                                       max_length=args.max_len,\n                                       max_len_a=args.max_len_a,\n                                       num_beams=args.num_beams,\n                                       do_sample=False,\n                                       repetition_penalty=1,\n                                       length_penalty=1.0,\n                                       pad_token_id=eos_token_id,\n                                       restricter=None)\n        # model = MultiModalBartModel_AESC(bart_config, args.bart_model,\n        #                                  tokenizer, label_ids)\n    model.to(device)\n    parameters = get_parameter_number(model) ##{'Total': 169351685, 'Trainable': 169351685}\n    print(parameters)\n\n    optimizer = AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n\n    scaler = GradScaler() if args.amp else None\n\n    epoch = 0\n    logger.info('Loading data...')\n    collate_twitter_ae = Collator(\n                                  task=args.task,\n                                  tokenizer=tokenizer,\n                                  mlm_enabled=False,\n                                  senti_enabled=False,\n                                  ae_enabled=False,\n                                  oe_enabled=False,\n                                  aesc_enabled=False,\n                                  anp_enabled=False,\n                                  twitter_ae_enabled=True,\n                                  has_prompt=args.has_prompt,\n                                  max_img_num=args.num_image_tokens,\n                                  text_only=args.text_only,\n                                  use_caption=args.use_caption)\n\n    train_dataset = Twitter_Dataset(args.dataset[0][1], split='train')\n\n    dev_dataset = Twitter_Dataset(args.dataset[0][1], split='dev')\n    test_dataset = Twitter_Dataset(args.dataset[0][1], split='test')\n\n    train_loader = DataLoader(dataset=train_dataset,\n                              batch_size=args.batch_size,\n                              shuffle=True,\n                              num_workers=args.num_workers,\n                              pin_memory=True,\n                              collate_fn=collate_twitter_ae)\n    dev_loader = DataLoader(dataset=dev_dataset,\n                            batch_size=args.batch_size,\n                            shuffle=False,\n                            num_workers=args.num_workers,\n                            pin_memory=True,\n                            collate_fn=collate_twitter_ae)\n    test_loader = DataLoader(dataset=test_dataset,\n                             batch_size=args.batch_size,\n                             shuffle=False,\n                             num_workers=args.num_workers,\n                             pin_memory=True,\n                             collate_fn=collate_twitter_ae)\n\n    callback = None\n    metric = OESpanMetric(eos_token_id, num_labels=len(label_ids))\n    model.train()\n    start = datetime.now()\n    best_dev_res = None\n    best_dev_test_res = None\n    best_test_res = None\n\n    while epoch < args.epochs:\n        logger.info('Epoch {}'.format(epoch + 1), pad=True)\n        fine_tune(epoch=epoch,\n                  model=model,\n                  train_loader=train_loader,\n                  test_loader=test_loader,\n                  metric=metric,\n                  optimizer=optimizer,\n                  args=args,\n                  device=device,\n                  logger=logger,\n                  callback=callback,\n                  log_interval=1,\n                  tb_writer=tb_writer,\n                  tb_interval=1,\n                  scaler=scaler)\n\n        if (epoch + 1) % args.eval_every == 0:\n            # train_dev = eval_utils.eval(model, train_loader, metric, device)\n\n            \n            res_dev, dev_aspects_num_acc = eval_utils.eval(args, model, dev_loader, metric, device)\n            res_test, test_aspects_num_acc = eval_utils.eval(args, model, test_loader, metric, device)\n\n        \n\n            logger.info('DEV  ae_p:{} ae_r:{} ae_f:{}, dev_aspects_num_acc: {:.4f}'.format(\n                res_dev['oe_pre'], res_dev['oe_rec'], res_dev['oe_f'], dev_aspects_num_acc))\n            logger.info('TEST  ae_p:{} ae_r:{} ae_f:{}, test_aspects_num_acc: {:.4f}'.format(\n                res_test['oe_pre'], res_test['oe_rec'], res_test['oe_f'], test_aspects_num_acc))\n            # logger.info('DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n            #     res_dev['ae_pre'], res_dev['ae_rec'], res_dev['ae_f']))\n            save_flag = False\n            if best_dev_res is None:\n                best_dev_res = res_dev\n                best_dev_test_res = res_test\n\n            else:\n                if best_dev_res['oe_f'] < res_dev['oe_f']:\n                    best_dev_res = res_dev\n                    best_dev_test_res = res_test\n\n            if best_test_res is None:\n                best_test_res = res_test\n                save_flag = True\n            else:\n                if best_test_res['oe_f'] < res_test['oe_f']:\n                    best_test_res = res_test\n                    save_flag = True\n\n            if args.is_check == 1 and save_flag:\n                current_checkpoint_path = os.path.join(checkpoint_path,\n                                                       args.check_info)\n                model.seq2seq_model.save_pretrained(current_checkpoint_path)\n                print('save model!!!!!!!!!!!')\n        epoch += 1\n    logger.info(\"Training complete in: \" + str(datetime.now() - start),\n                pad=True)\n    logger.info('---------------------------')\n    logger.info('BEST DEV:-----')\n    logger.info('BEST DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n        best_dev_res['oe_pre'], best_dev_res['oe_rec'], best_dev_res['oe_f']))\n\n    logger.info('BEST DEV TEST:-----')\n    logger.info('BEST DEV--TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n        best_dev_test_res['oe_pre'], best_dev_test_res['oe_rec'],\n        best_dev_test_res['oe_f']))\n\n    logger.info('BEST TEST:-----')\n    logger.info('BEST TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n        best_test_res['oe_pre'], best_test_res['oe_rec'],\n        best_test_res['oe_f']))", "\n    # if not args.cpu:\n    #     cleanup_process()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset',\n                        action='append',\n                        nargs=2,\n                        metavar=('DATASET_NAME', 'DATASET_PATH'),\n                        required=True,\n                        help='')\n    # required\n\n    parser.add_argument('--checkpoint_dir',\n                        required=True,\n                        type=str,\n                        help='where to save the checkpoint')\n    parser.add_argument('--bart_model',\n                        default='/home/xiaocui/code/FW-MABSA/data/weights/bart-base',\n                        type=str,\n                        help='bart pretrain model')\n    # path\n    parser.add_argument(\n        '--log_dir',\n        default=None,\n        type=str,\n        help='path to output log files, not output to file if not specified')\n    parser.add_argument('--model_config',\n                        default=None,\n                        type=str,\n                        help='path to load model config')\n    parser.add_argument('--text_only',action='store_true', default=False, help='text_only')\n    parser.add_argument('--checkpoint',\n                        default=None,\n                        type=str,\n                        help='name or path to load weights')\n    parser.add_argument('--lr_decay_every',\n                        default=4,\n                        type=int,\n                        help='lr_decay_every')\n    parser.add_argument('--lr_decay_ratio',\n                        default=0.8,\n                        type=float,\n                        help='lr_decay_ratio')\n    # training and evaluation\n    parser.add_argument('--epochs',\n                        default=40,\n                        type=int,\n                        help='number of training epoch')\n    parser.add_argument('--eval_every', default=3, type=int, help='eval_every')\n    parser.add_argument('--lr', default=1e-2, type=float, help='learning rate')\n    parser.add_argument('--num_beams',\n                        default=4,\n                        type=int,\n                        help='level of beam search on validation')\n    parser.add_argument(\n        '--continue_training',\n        action='store_true',\n        help='continue training, load optimizer and epoch from checkpoint')\n    parser.add_argument('--warmup', default=0.1, type=float, help='warmup')\n    # dropout\n    parser.add_argument(\n        '--dropout',\n        default=None,\n        type=float,\n        help=\n        'dropout rate for the transformer. This overwrites the model config')\n    parser.add_argument(\n        '--classif_dropout',\n        default=None,\n        type=float,\n        help=\n        'dropout rate for the classification layers. This overwrites the model config'\n    )\n    parser.add_argument(\n        '--attention_dropout',\n        default=None,\n        type=float,\n        help=\n        'dropout rate for the attention layers. This overwrites the model config'\n    )\n    parser.add_argument(\n        '--activation_dropout',\n        default=None,\n        type=float,\n        help=\n        'dropout rate for the activation layers. This overwrites the model config'\n    )\n\n    # hardware and performance\n    parser.add_argument('--grad_clip', default=5, type=float, help='grad_clip')\n    parser.add_argument('--gpu_num',\n                        default=1,\n                        type=int,\n                        help='number of GPUs in total')\n    parser.add_argument('--cpu',\n                        action='store_true',\n                        help='if only use cpu to run the model')\n    parser.add_argument('--amp',\n                        action='store_true',\n                        help='whether or not to use amp')\n    parser.add_argument('--master_port',\n                        type=str,\n                        default='12355',\n                        help='master port for DDP')\n    parser.add_argument('--batch_size',\n                        type=int,\n                        default=64,\n                        help='training batch size')\n    parser.add_argument('--seed', type=int, default=42, help='seed')\n    parser.add_argument('--num_workers',\n                        type=int,\n                        default=0,\n                        help='#workers for data loader')\n    parser.add_argument('--max_len', type=int, default=10, help='max_len')\n    parser.add_argument('--max_len_a',\n                        type=float,\n                        default=0.6,\n                        help='max_len_a')\n    parser.add_argument('--ANP_loss_type',\n                        type=str,\n                        default='KL',\n                        help='ANP_loss_type')\n    parser.add_argument('--bart_init', type=int, default=1, help='bart_init')\n    parser.add_argument('--sample_num',\n                        type=int,\n                        default=500,\n                        help='sample_num')\n    parser.add_argument('--is_sample', type=int, default=1, help='is_sample')\n    parser.add_argument('--start_idx', type=int, default=0, help='start_idx')\n    parser.add_argument('--check_info', type=str, default='', help='start_idx')\n    parser.add_argument('--is_check', type=int, default=0, help='start_idx')\n    parser.add_argument('--task', type=str, default='twitter_ae', help='task')\n    parser.add_argument('--has_prompt',  action='store_true', default=False, help='whether has prompt')\n    parser.add_argument('--use_generated_prompt', action='store_true', default=False, help='whether use the generated prompt')\n    parser.add_argument('--use_different_senti_prompt', action='store_true', default=False, help='whether use different prompt for different sentiemnt in an instance')\n    parser.add_argument('--use_different_aspect_prompt', action='store_true', default=False, help='whether use different prompt for different aspects in an instance')\n    parser.add_argument('--num_image_tokens', type=int, default=2, help='the length of image_tokens')\n    parser.add_argument('--use_multitasks', action='store_true', default=False, help='whether use multitasks')\n    parser.add_argument('--loss_lambda', default=0.1, type=float, help='the weight of aspect_num classification loss')\n    parser.add_argument('--use_caption', action='store_true', default=False, help='whether use image caption')\n\n    args = parser.parse_args()\n\n    if args.gpu_num != 1 and args.cpu:\n        raise ValueError('--gpu_num are not allowed if --cpu is set to true')\n\n    if args.checkpoint is None and args.model_config is None:\n        raise ValueError(\n            '--model_config and --checkpoint cannot be empty at the same time')\n\n    return args", "\n\nif __name__ == '__main__':\n    args = parse_args()\n\n    # mp.spawn(main, args=(args, ), nprocs=args.gpu_num, join=True)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    cudnn.deterministic = True\n    main(0, args)"]}
{"filename": "twitter_sc_training_for_generated_prompt.py", "chunked_list": ["import argparse\nimport json\nimport os\nfrom datetime import datetime\nfrom torch import optim\nimport torch\nimport torch.multiprocessing as mp\nfrom torch.cuda.amp import GradScaler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, ConcatDataset", "from torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.tensorboard import SummaryWriter\nfrom transformers import AdamW\nimport random\nfrom src.data.collation_for_prompt import Collator\nfrom src.data.dataset_for_prompt import MVSA_Dataset, Twitter_Dataset\nfrom src.data.tokenization_new_for_generated_prompt import ConditionTokenizer\nfrom src.model.config import MultiModalBartConfig", "from src.data.tokenization_new_for_generated_prompt import ConditionTokenizer\nfrom src.model.config import MultiModalBartConfig\nfrom src.model.MAESC_model_for_generated_senti_prompt import MultiModalBartModel_AESC\nfrom src.model.model import MultiModalBartModelForPretrain\nfrom src.training import fine_tune\nfrom src.utils import Logger, save_training_data, load_training_data, setup_process, cleanup_process\nfrom src.model.metrics import AESCSpanMetric, OESpanMetric\nfrom src.model.generater_for_generated_prompt import SequenceGeneratorModel\nimport src.eval_utils as eval_utils\nimport numpy as np", "import src.eval_utils as eval_utils\nimport numpy as np\nimport torch.backends.cudnn as cudnn\n\n\ndef get_parameter_number(model):\n    total_num = sum(p.numel() for p in model.parameters())\n    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return {'Total': total_num, 'Trainable': trainable_num}\n\ndef main(rank, args):\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    checkpoint_path = os.path.join(args.checkpoint_dir, timestamp)\n    tb_writer = None\n    add_name = 'epoch_num' + str(args.epochs)\n    add_name += 'last'\n\n    if args.is_sample:\n        add_name += 'sample_num' + str(args.sample_num)\n        add_name += 'start_idx' + str(args.start_idx)\n    if args.text_only:\n        add_name += ' only text'\n    else:\n        add_name += ' multi'\n    if args.bart_init == 0:\n        add_name += '_random_init_'\n    if args.checkpoint:\n        add_name = add_name + '-pretrain' + args.checkpoint.split('/')[-2]\n\n    add_name = add_name + str(args.lr)\n    log_dir = os.path.join(args.log_dir, timestamp + add_name)\n\n    # make log dir and tensorboard writer if log_dir is specified\n    if rank == 0 and args.log_dir is not None:\n        os.makedirs(log_dir)\n        tb_writer = SummaryWriter(log_dir=log_dir)\n\n    logger = Logger(log_dir=os.path.join(log_dir, 'log.txt'),\n                    enabled=(rank == 0))\n\n    # make checkpoint dir if not exist\n    if args.is_check == 1 and not os.path.isdir(checkpoint_path):\n        os.makedirs(checkpoint_path)\n        logger.info('Made checkpoint directory: \"{}\"'.format(checkpoint_path))\n\n    logger.info('Initialed with {} GPU(s)'.format(args.gpu_num), pad=True)\n    for k, v in vars(args).items():\n        logger.info('{}: {}'.format(k, v))\n\n    # =========================== model =============================\n\n    logger.info('Loading model...')\n\n    if args.cpu:\n        device = 'cpu'\n        map_location = device\n    else:\n        device = torch.device(\"cuda:{}\".format(rank))\n        map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n\n    tokenizer = ConditionTokenizer(args=args)\n    label_ids = list(tokenizer.mapping2id.values())\n    senti_ids = list(tokenizer.senti2id.values())\n    # print(label_ids)\n    # print(tokenizer.convert_ids_to_tokens(label_ids))\n    if args.model_config is not None:\n        bart_config = MultiModalBartConfig.from_dict(\n            json.load(open(args.model_config)))\n    else:\n        bart_config = MultiModalBartConfig.from_pretrained(args.checkpoint)\n\n    if args.dropout is not None:\n        bart_config.dropout = args.dropout\n    if args.attention_dropout is not None:\n        bart_config.attention_dropout = args.attention_dropout\n    if args.classif_dropout is not None:\n        bart_config.classif_dropout = args.classif_dropout\n    if args.activation_dropout is not None:\n        bart_config.activation_dropout = args.activation_dropout\n\n    bos_token_id = 0  # \u56e0\u4e3a\u662f\u7279\u6b8a\u7b26\u53f7\n    eos_token_id = 1\n\n    if args.checkpoint:\n        pretrain_model = MultiModalBartModelForPretrain.from_pretrained(\n            args.checkpoint,\n            config=bart_config,\n            bart_model=args.bart_model,\n            tokenizer=tokenizer,\n            label_ids=label_ids,\n            senti_ids=senti_ids,\n            args=args,\n            error_on_mismatch=False)\n        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n                                                 args.bart_model, tokenizer,\n                                                 label_ids)\n        seq2seq_model.encoder.load_state_dict(\n            pretrain_model.encoder.state_dict())\n        seq2seq_model.decoder.load_state_dict(\n            pretrain_model.span_decoder.state_dict())\n        model = SequenceGeneratorModel(seq2seq_model,\n                                       bos_token_id=bos_token_id,\n                                       eos_token_id=eos_token_id,\n                                       max_length=args.max_len,\n                                       max_len_a=args.max_len_a,\n                                       num_beams=args.num_beams,\n                                       do_sample=False,\n                                       sc_only=True,\n                                       repetition_penalty=1,\n                                       length_penalty=1.0,\n                                       pad_token_id=eos_token_id,\n                                       restricter=None)\n    else:\n        print('++++++++++++++++++ No Pretrain ++++++++++++++++++++++++++++++++')\n        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n                                                 args.bart_model, tokenizer,\n                                                 label_ids)\n        model = SequenceGeneratorModel(seq2seq_model,\n                                       bos_token_id=bos_token_id,\n                                       eos_token_id=eos_token_id,\n                                       max_length=args.max_len,\n                                       max_len_a=args.max_len_a,\n                                       num_beams=args.num_beams,\n                                       do_sample=False,\n                                       sc_only=True,\n                                       repetition_penalty=1,\n                                       length_penalty=1.0,\n                                       pad_token_id=eos_token_id,\n                                       restricter=None)\n        # model = MultiModalBartModel_AESC(bart_config, args.bart_model,\n        #                                  tokenizer, label_ids)\n    model.to(device)\n\n\n    optimizer = AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n\n    scaler = GradScaler() if args.amp else None\n\n    epoch = 0\n    logger.info('Loading data...')\n    collate_twitter_sc = Collator(args.task,\n                                  tokenizer,\n                                  mlm_enabled=False,\n                                  senti_enabled=False,\n                                  ae_enabled=False,\n                                  oe_enabled=False,\n                                  aesc_enabled=False,\n                                  anp_enabled=False,\n                                  twitter_sc_enabled=True,\n                                  max_img_num=args.num_image_tokens,\n                                  has_prompt=args.has_prompt,\n                                  text_only=args.text_only,\n                                  use_caption=args.use_caption)\n\n    train_dataset = Twitter_Dataset(args.dataset[0][1], split='train')\n\n    dev_dataset = Twitter_Dataset(args.dataset[0][1], split='dev')\n    test_dataset = Twitter_Dataset(args.dataset[0][1], split='test')\n\n    train_loader = DataLoader(dataset=train_dataset,\n                              batch_size=args.batch_size,\n                              shuffle=True,\n                              num_workers=args.num_workers,\n                              pin_memory=True,\n                              collate_fn=collate_twitter_sc)\n    dev_loader = DataLoader(dataset=dev_dataset,\n                            batch_size=args.batch_size,\n                            shuffle=False,\n                            num_workers=args.num_workers,\n                            pin_memory=True,\n                            collate_fn=collate_twitter_sc)\n    test_loader = DataLoader(dataset=test_dataset,\n                             batch_size=args.batch_size,\n                             shuffle=False,\n                             num_workers=args.num_workers,\n                             pin_memory=True,\n                             collate_fn=collate_twitter_sc)\n\n    callback = None\n    metric = AESCSpanMetric(eos_token_id,\n                            num_labels=len(label_ids),\n                            conflict_id=-1)\n    model.train()\n    start = datetime.now()\n    best_dev_res = None\n    best_dev_test_res = None\n    best_test_res = None\n    # res_dev = eval_utils.eval(args, model, dev_loader, metric, device)\n    # for name, param in model.named_parameters():\n    #     print(name, param.shape)\n    while epoch < args.epochs:\n        logger.info('Epoch {}'.format(epoch + 1), pad=True)\n        fine_tune(epoch=epoch,\n                  model=model,\n                  train_loader=train_loader,\n                  test_loader=test_loader,\n                  metric=metric,\n                  optimizer=optimizer,\n                  args=args,\n                  device=device,\n                  logger=logger,\n                  callback=callback,\n                  log_interval=1,\n                  tb_writer=tb_writer,\n                  tb_interval=1,\n                  scaler=scaler)\n\n        print('test!!!!!!!!!!!!!!')\n        if (epoch + 1) % args.eval_every == 0:\n            # train_dev = eval_utils.eval(model, train_loader, metric, device)\n            res_dev = eval_utils.eval(args, model, dev_loader, metric, device)\n            res_test = eval_utils.eval(args, model, test_loader, metric,\n                                       device)\n            # print('sc_all_num', res_test['sc_all_num'])\n            logger.info('DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n                res_dev['ae_pre'], res_dev['ae_rec'], res_dev['ae_f']))\n            logger.info('DEV  sc_p:{} sc_r:{} sc_f:{}'.format(\n                res_dev['sc_pre'], res_dev['sc_rec'], res_dev['sc_f']))\n            logger.info('DEV  sc_acc:{}'.format(res_dev['sc_acc']))\n            logger.info('TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n                res_test['ae_pre'], res_test['ae_rec'], res_test['ae_f']))\n            logger.info('TEST  sc_p:{} sc_r:{} sc_f:{}'.format(\n                res_test['sc_pre'], res_test['sc_rec'], res_test['sc_f']))\n            logger.info('TEST  sc_acc:{}'.format(res_test['sc_acc']))\n\n            # logger.info('DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n            #     res_dev['ae_pre'], res_dev['ae_rec'], res_dev['ae_f']))\n            save_flag = False\n            if best_dev_res is None:\n                best_dev_res = res_dev\n                best_dev_test_res = res_test\n\n            else:\n                if best_dev_res['sc_acc'] < res_dev['sc_acc']:\n                    best_dev_res = res_dev\n                    best_dev_test_res = res_test\n\n            if best_test_res is None:\n                best_test_res = res_test\n                save_flag = True\n            else:\n                if best_test_res['sc_acc'] < res_test['sc_acc']:\n                    best_test_res = res_test\n                    save_flag = True\n\n            if args.is_check == 1 and save_flag:\n                current_checkpoint_path = os.path.join(checkpoint_path,\n                                                       args.check_info)\n                model.seq2seq_model.save_pretrained(current_checkpoint_path)\n                print('save model!!!!!!!!!!!')\n        epoch += 1\n    logger.info(\"Training complete in: \" + str(datetime.now() - start),\n                pad=True)\n    logger.info('---------------------------')\n    logger.info('BEST DEV:-----')\n    logger.info('BEST DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n        best_dev_res['ae_pre'], best_dev_res['ae_rec'], best_dev_res['ae_f']))\n    logger.info('BEST DEV  sc_p:{} sc_r:{} sc_f:{}'.format(\n        best_dev_res['sc_pre'], best_dev_res['sc_rec'], best_dev_res['sc_f']))\n    logger.info('BEST DEV  sc_acc:{}'.format(best_dev_res['sc_acc']))\n\n    logger.info('BEST DEV TEST:-----')\n    logger.info('BEST DEV--TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n        best_dev_test_res['ae_pre'], best_dev_test_res['ae_rec'],\n        best_dev_test_res['ae_f']))\n    logger.info('BEST DEV--TEST  sc_p:{} sc_r:{} sc_f:{}'.format(\n        best_dev_test_res['sc_pre'], best_dev_test_res['sc_rec'],\n        best_dev_test_res['sc_f']))\n    logger.info('BEST DEV--TEST  sc_acc:{}'.format(\n        best_dev_test_res['sc_acc']))\n\n    logger.info('BEST TEST:-----')\n    logger.info('BEST TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n        best_test_res['ae_pre'], best_test_res['ae_rec'],\n        best_test_res['ae_f']))\n    logger.info('BEST TEST  sc_p:{} sc_r:{} sc_f:{}'.format(\n        best_test_res['sc_pre'], best_test_res['sc_rec'],\n        best_test_res['sc_f']))\n    logger.info('BEST TEST  sc_acc:{}'.format(best_test_res['sc_acc']))\n\n    if not args.cpu:\n        cleanup_process()", "\ndef main(rank, args):\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    checkpoint_path = os.path.join(args.checkpoint_dir, timestamp)\n    tb_writer = None\n    add_name = 'epoch_num' + str(args.epochs)\n    add_name += 'last'\n\n    if args.is_sample:\n        add_name += 'sample_num' + str(args.sample_num)\n        add_name += 'start_idx' + str(args.start_idx)\n    if args.text_only:\n        add_name += ' only text'\n    else:\n        add_name += ' multi'\n    if args.bart_init == 0:\n        add_name += '_random_init_'\n    if args.checkpoint:\n        add_name = add_name + '-pretrain' + args.checkpoint.split('/')[-2]\n\n    add_name = add_name + str(args.lr)\n    log_dir = os.path.join(args.log_dir, timestamp + add_name)\n\n    # make log dir and tensorboard writer if log_dir is specified\n    if rank == 0 and args.log_dir is not None:\n        os.makedirs(log_dir)\n        tb_writer = SummaryWriter(log_dir=log_dir)\n\n    logger = Logger(log_dir=os.path.join(log_dir, 'log.txt'),\n                    enabled=(rank == 0))\n\n    # make checkpoint dir if not exist\n    if args.is_check == 1 and not os.path.isdir(checkpoint_path):\n        os.makedirs(checkpoint_path)\n        logger.info('Made checkpoint directory: \"{}\"'.format(checkpoint_path))\n\n    logger.info('Initialed with {} GPU(s)'.format(args.gpu_num), pad=True)\n    for k, v in vars(args).items():\n        logger.info('{}: {}'.format(k, v))\n\n    # =========================== model =============================\n\n    logger.info('Loading model...')\n\n    if args.cpu:\n        device = 'cpu'\n        map_location = device\n    else:\n        device = torch.device(\"cuda:{}\".format(rank))\n        map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n\n    tokenizer = ConditionTokenizer(args=args)\n    label_ids = list(tokenizer.mapping2id.values())\n    senti_ids = list(tokenizer.senti2id.values())\n    # print(label_ids)\n    # print(tokenizer.convert_ids_to_tokens(label_ids))\n    if args.model_config is not None:\n        bart_config = MultiModalBartConfig.from_dict(\n            json.load(open(args.model_config)))\n    else:\n        bart_config = MultiModalBartConfig.from_pretrained(args.checkpoint)\n\n    if args.dropout is not None:\n        bart_config.dropout = args.dropout\n    if args.attention_dropout is not None:\n        bart_config.attention_dropout = args.attention_dropout\n    if args.classif_dropout is not None:\n        bart_config.classif_dropout = args.classif_dropout\n    if args.activation_dropout is not None:\n        bart_config.activation_dropout = args.activation_dropout\n\n    bos_token_id = 0  # \u56e0\u4e3a\u662f\u7279\u6b8a\u7b26\u53f7\n    eos_token_id = 1\n\n    if args.checkpoint:\n        pretrain_model = MultiModalBartModelForPretrain.from_pretrained(\n            args.checkpoint,\n            config=bart_config,\n            bart_model=args.bart_model,\n            tokenizer=tokenizer,\n            label_ids=label_ids,\n            senti_ids=senti_ids,\n            args=args,\n            error_on_mismatch=False)\n        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n                                                 args.bart_model, tokenizer,\n                                                 label_ids)\n        seq2seq_model.encoder.load_state_dict(\n            pretrain_model.encoder.state_dict())\n        seq2seq_model.decoder.load_state_dict(\n            pretrain_model.span_decoder.state_dict())\n        model = SequenceGeneratorModel(seq2seq_model,\n                                       bos_token_id=bos_token_id,\n                                       eos_token_id=eos_token_id,\n                                       max_length=args.max_len,\n                                       max_len_a=args.max_len_a,\n                                       num_beams=args.num_beams,\n                                       do_sample=False,\n                                       sc_only=True,\n                                       repetition_penalty=1,\n                                       length_penalty=1.0,\n                                       pad_token_id=eos_token_id,\n                                       restricter=None)\n    else:\n        print('++++++++++++++++++ No Pretrain ++++++++++++++++++++++++++++++++')\n        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n                                                 args.bart_model, tokenizer,\n                                                 label_ids)\n        model = SequenceGeneratorModel(seq2seq_model,\n                                       bos_token_id=bos_token_id,\n                                       eos_token_id=eos_token_id,\n                                       max_length=args.max_len,\n                                       max_len_a=args.max_len_a,\n                                       num_beams=args.num_beams,\n                                       do_sample=False,\n                                       sc_only=True,\n                                       repetition_penalty=1,\n                                       length_penalty=1.0,\n                                       pad_token_id=eos_token_id,\n                                       restricter=None)\n        # model = MultiModalBartModel_AESC(bart_config, args.bart_model,\n        #                                  tokenizer, label_ids)\n    model.to(device)\n\n\n    optimizer = AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n\n    scaler = GradScaler() if args.amp else None\n\n    epoch = 0\n    logger.info('Loading data...')\n    collate_twitter_sc = Collator(args.task,\n                                  tokenizer,\n                                  mlm_enabled=False,\n                                  senti_enabled=False,\n                                  ae_enabled=False,\n                                  oe_enabled=False,\n                                  aesc_enabled=False,\n                                  anp_enabled=False,\n                                  twitter_sc_enabled=True,\n                                  max_img_num=args.num_image_tokens,\n                                  has_prompt=args.has_prompt,\n                                  text_only=args.text_only,\n                                  use_caption=args.use_caption)\n\n    train_dataset = Twitter_Dataset(args.dataset[0][1], split='train')\n\n    dev_dataset = Twitter_Dataset(args.dataset[0][1], split='dev')\n    test_dataset = Twitter_Dataset(args.dataset[0][1], split='test')\n\n    train_loader = DataLoader(dataset=train_dataset,\n                              batch_size=args.batch_size,\n                              shuffle=True,\n                              num_workers=args.num_workers,\n                              pin_memory=True,\n                              collate_fn=collate_twitter_sc)\n    dev_loader = DataLoader(dataset=dev_dataset,\n                            batch_size=args.batch_size,\n                            shuffle=False,\n                            num_workers=args.num_workers,\n                            pin_memory=True,\n                            collate_fn=collate_twitter_sc)\n    test_loader = DataLoader(dataset=test_dataset,\n                             batch_size=args.batch_size,\n                             shuffle=False,\n                             num_workers=args.num_workers,\n                             pin_memory=True,\n                             collate_fn=collate_twitter_sc)\n\n    callback = None\n    metric = AESCSpanMetric(eos_token_id,\n                            num_labels=len(label_ids),\n                            conflict_id=-1)\n    model.train()\n    start = datetime.now()\n    best_dev_res = None\n    best_dev_test_res = None\n    best_test_res = None\n    # res_dev = eval_utils.eval(args, model, dev_loader, metric, device)\n    # for name, param in model.named_parameters():\n    #     print(name, param.shape)\n    while epoch < args.epochs:\n        logger.info('Epoch {}'.format(epoch + 1), pad=True)\n        fine_tune(epoch=epoch,\n                  model=model,\n                  train_loader=train_loader,\n                  test_loader=test_loader,\n                  metric=metric,\n                  optimizer=optimizer,\n                  args=args,\n                  device=device,\n                  logger=logger,\n                  callback=callback,\n                  log_interval=1,\n                  tb_writer=tb_writer,\n                  tb_interval=1,\n                  scaler=scaler)\n\n        print('test!!!!!!!!!!!!!!')\n        if (epoch + 1) % args.eval_every == 0:\n            # train_dev = eval_utils.eval(model, train_loader, metric, device)\n            res_dev = eval_utils.eval(args, model, dev_loader, metric, device)\n            res_test = eval_utils.eval(args, model, test_loader, metric,\n                                       device)\n            # print('sc_all_num', res_test['sc_all_num'])\n            logger.info('DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n                res_dev['ae_pre'], res_dev['ae_rec'], res_dev['ae_f']))\n            logger.info('DEV  sc_p:{} sc_r:{} sc_f:{}'.format(\n                res_dev['sc_pre'], res_dev['sc_rec'], res_dev['sc_f']))\n            logger.info('DEV  sc_acc:{}'.format(res_dev['sc_acc']))\n            logger.info('TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n                res_test['ae_pre'], res_test['ae_rec'], res_test['ae_f']))\n            logger.info('TEST  sc_p:{} sc_r:{} sc_f:{}'.format(\n                res_test['sc_pre'], res_test['sc_rec'], res_test['sc_f']))\n            logger.info('TEST  sc_acc:{}'.format(res_test['sc_acc']))\n\n            # logger.info('DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n            #     res_dev['ae_pre'], res_dev['ae_rec'], res_dev['ae_f']))\n            save_flag = False\n            if best_dev_res is None:\n                best_dev_res = res_dev\n                best_dev_test_res = res_test\n\n            else:\n                if best_dev_res['sc_acc'] < res_dev['sc_acc']:\n                    best_dev_res = res_dev\n                    best_dev_test_res = res_test\n\n            if best_test_res is None:\n                best_test_res = res_test\n                save_flag = True\n            else:\n                if best_test_res['sc_acc'] < res_test['sc_acc']:\n                    best_test_res = res_test\n                    save_flag = True\n\n            if args.is_check == 1 and save_flag:\n                current_checkpoint_path = os.path.join(checkpoint_path,\n                                                       args.check_info)\n                model.seq2seq_model.save_pretrained(current_checkpoint_path)\n                print('save model!!!!!!!!!!!')\n        epoch += 1\n    logger.info(\"Training complete in: \" + str(datetime.now() - start),\n                pad=True)\n    logger.info('---------------------------')\n    logger.info('BEST DEV:-----')\n    logger.info('BEST DEV  ae_p:{} ae_r:{} ae_f:{}'.format(\n        best_dev_res['ae_pre'], best_dev_res['ae_rec'], best_dev_res['ae_f']))\n    logger.info('BEST DEV  sc_p:{} sc_r:{} sc_f:{}'.format(\n        best_dev_res['sc_pre'], best_dev_res['sc_rec'], best_dev_res['sc_f']))\n    logger.info('BEST DEV  sc_acc:{}'.format(best_dev_res['sc_acc']))\n\n    logger.info('BEST DEV TEST:-----')\n    logger.info('BEST DEV--TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n        best_dev_test_res['ae_pre'], best_dev_test_res['ae_rec'],\n        best_dev_test_res['ae_f']))\n    logger.info('BEST DEV--TEST  sc_p:{} sc_r:{} sc_f:{}'.format(\n        best_dev_test_res['sc_pre'], best_dev_test_res['sc_rec'],\n        best_dev_test_res['sc_f']))\n    logger.info('BEST DEV--TEST  sc_acc:{}'.format(\n        best_dev_test_res['sc_acc']))\n\n    logger.info('BEST TEST:-----')\n    logger.info('BEST TEST  ae_p:{} ae_r:{} ae_f:{}'.format(\n        best_test_res['ae_pre'], best_test_res['ae_rec'],\n        best_test_res['ae_f']))\n    logger.info('BEST TEST  sc_p:{} sc_r:{} sc_f:{}'.format(\n        best_test_res['sc_pre'], best_test_res['sc_rec'],\n        best_test_res['sc_f']))\n    logger.info('BEST TEST  sc_acc:{}'.format(best_test_res['sc_acc']))\n\n    if not args.cpu:\n        cleanup_process()", "\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset',\n                        action='append',\n                        nargs=2,\n                        metavar=('DATASET_NAME', 'DATASET_PATH'),\n                        required=True,\n                        help='')\n    # required\n\n    parser.add_argument('--checkpoint_dir',\n                        required=True,\n                        type=str,\n                        help='where to save the checkpoint')\n    parser.add_argument('--bart_model',\n                        default='/home/xiaocui/code/FW-MABSA/data/weights/bart-base',\n                        type=str,\n                        help='bart pretrain model')\n    # path\n    parser.add_argument(\n        '--log_dir',\n        default=None,\n        type=str,\n        help='path to output log files, not output to file if not specified')\n    parser.add_argument('--model_config',\n                        default=None,\n                        type=str,\n                        help='path to load model config')\n    # parser.add_argument('--text_only', default=0, type=int, help='text_only')\n    parser.add_argument('--checkpoint',\n                        default=None,\n                        type=str,\n                        help='name or path to load weights')\n    parser.add_argument('--lr_decay_every',\n                        default=4,\n                        type=int,\n                        help='lr_decay_every')\n    parser.add_argument('--lr_decay_ratio',\n                        default=0.8,\n                        type=float,\n                        help='lr_decay_ratio')\n    # training and evaluation\n    parser.add_argument('--epochs',\n                        default=40,\n                        type=int,\n                        help='number of training epoch')\n    parser.add_argument('--eval_every', default=3, type=int, help='eval_every')\n    parser.add_argument('--lr', default=1e-2, type=float, help='learning rate')\n    parser.add_argument('--num_beams',\n                        default=4,\n                        type=int,\n                        help='level of beam search on validation')\n    parser.add_argument(\n        '--continue_training',\n        action='store_true',\n        help='continue training, load optimizer and epoch from checkpoint')\n    parser.add_argument('--warmup', default=0.1, type=float, help='warmup')\n    # dropout\n    parser.add_argument(\n        '--dropout',\n        default=None,\n        type=float,\n        help=\n        'dropout rate for the transformer. This overwrites the model config')\n    parser.add_argument(\n        '--classif_dropout',\n        default=None,\n        type=float,\n        help=\n        'dropout rate for the classification layers. This overwrites the model config'\n    )\n    parser.add_argument(\n        '--attention_dropout',\n        default=None,\n        type=float,\n        help=\n        'dropout rate for the attention layers. This overwrites the model config'\n    )\n    parser.add_argument(\n        '--activation_dropout',\n        default=None,\n        type=float,\n        help=\n        'dropout rate for the activation layers. This overwrites the model config'\n    )\n\n    # hardware and performance\n    parser.add_argument('--grad_clip', default=5, type=float, help='grad_clip')\n    parser.add_argument('--gpu_num',\n                        default=1,\n                        type=int,\n                        help='number of GPUs in total')\n    parser.add_argument('--cpu',\n                        action='store_true',\n                        help='if only use cpu to run the model')\n    parser.add_argument('--amp',\n                        action='store_true',\n                        help='whether or not to use amp')\n    parser.add_argument('--master_port',\n                        type=str,\n                        default='12355',\n                        help='master port for DDP')\n    parser.add_argument('--batch_size',\n                        type=int,\n                        default=64,\n                        help='training batch size')\n    parser.add_argument('--seed', type=int, default=42, help='seed')\n    parser.add_argument('--num_workers',\n                        type=int,\n                        default=0,\n                        help='#workers for data loader')\n    parser.add_argument('--max_len', type=int, default=10, help='max_len')\n    parser.add_argument('--max_len_a',\n                        type=float,\n                        default=0.6,\n                        help='max_len_a')\n    parser.add_argument('--ANP_loss_type',\n                        type=str,\n                        default='KL',\n                        help='ANP_loss_type')\n    parser.add_argument('--bart_init', type=int, default=1, help='bart_init')\n    parser.add_argument('--sample_num',\n                        type=int,\n                        default=500,\n                        help='sample_num')\n    parser.add_argument('--is_sample', type=int, default=1, help='is_sample')\n    parser.add_argument('--start_idx', type=int, default=0, help='start_idx')\n    parser.add_argument('--check_info', type=str, default='', help='start_idx')\n    parser.add_argument('--is_check', type=int, default=0, help='start_idx')\n    parser.add_argument('--task', type=str, default='twitter_sc', help='task')\n\n    parser.add_argument('--num_image_tokens', type=int, default=2, help='the length of image_tokens')\n    parser.add_argument('--has_prompt',  action='store_true', default=False, help='whether has prompt')\n    parser.add_argument('--use_generated_prompt',  action='store_true', default=False, help='whether use the generated prompt')\n    parser.add_argument('--use_different_senti_prompt', action='store_true', default=False, help='whether use different prompt for different aspects in an instance')\n    parser.add_argument('--use_different_aspect_prompt', action='store_true', default=False, help='whether use different prompt for different aspects in an instance')\n\n    parser.add_argument('--text_only', action='store_true', default=False, help='whether only use text')\n    parser.add_argument('--use_caption', action='store_true', default=False, help='whether use image caption')\n\n\n    args = parser.parse_args()\n\n    if args.gpu_num != 1 and args.cpu:\n        raise ValueError('--gpu_num are not allowed if --cpu is set to true')\n\n    if args.checkpoint is None and args.model_config is None:\n        raise ValueError(\n            '--model_config and --checkpoint cannot be empty at the same time')\n\n    return args", "\n\nif __name__ == '__main__':\n    args = parse_args()\n\n    # mp.spawn(main, args=(args, ), nprocs=args.gpu_num, join=True)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    cudnn.deterministic = True\n    main(0, args)"]}
{"filename": "MAESC_training_for_generated_dual_prompts_multitasks_Aspect.py", "chunked_list": ["import argparse\nimport json\nimport os\nfrom datetime import datetime\nfrom torch import optim\nimport torch\nimport torch.multiprocessing as mp\nfrom torch.cuda.amp import GradScaler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, ConcatDataset", "from torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.tensorboard import SummaryWriter\nfrom transformers import AdamW\nimport random\nfrom src.data.collation_for_prompt_multitasks import Collator\nfrom src.data.dataset_for_prompt import MVSA_Dataset, Twitter_Dataset\nfrom src.data.tokenization_new_for_generated_prompt_multitasks import ConditionTokenizer\nfrom src.model.config import MultiModalBartConfig", "from src.data.tokenization_new_for_generated_prompt_multitasks import ConditionTokenizer\nfrom src.model.config import MultiModalBartConfig\nfrom src.model.MAESC_model_for_generated_dual_prompts_multitasks_Aspect import MultiModalBartModel_AESC\nfrom src.model.model_for_prompt import MultiModalBartModelForPretrain\nfrom src.training_multitasks import fine_tune\nfrom src.utils import Logger, save_training_data, load_training_data, setup_process, cleanup_process\nfrom src.model.metrics import AESCSpanMetric\nfrom src.model.generater_for_generated_prompt_multitasks import SequenceGeneratorModel\nimport src.eval_utils_multitasks as eval_utils\nimport numpy as np", "import src.eval_utils_multitasks as eval_utils\nimport numpy as np\nimport torch.backends.cudnn as cudnn\n# from thop import profile\n\ndef get_parameter_number(model):\n    total_num = sum(p.numel() for p in model.parameters())\n    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return {'Total': total_num, 'Trainable': trainable_num}\n", "\n\ndef main(rank, args):\n    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    checkpoint_path = os.path.join(args.checkpoint_dir, timestamp)\n    tb_writer = None\n    add_name = ''\n    log_dir = os.path.join(args.log_dir, timestamp + add_name)\n\n    # make log dir and tensorboard writer if log_dir is specified\n    if rank == 0 and args.log_dir is not None:\n        os.makedirs(log_dir)\n        tb_writer = SummaryWriter(log_dir=log_dir)\n\n    logger = Logger(log_dir=os.path.join(log_dir, 'log.txt'),\n                    enabled=(rank == 0))\n\n    # make checkpoint dir if not exist\n    if args.is_check == 1 and not os.path.isdir(checkpoint_path):\n        os.makedirs(checkpoint_path)\n        logger.info('Made checkpoint directory: \"{}\"'.format(checkpoint_path))\n\n    logger.info('Initialed with {} GPU(s)'.format(args.gpu_num), pad=True)\n    for k, v in vars(args).items():\n        logger.info('{}: {}'.format(k, v))\n\n    # =========================== model =============================\n\n    logger.info('Loading model...')\n\n    if args.cpu:\n        device = 'cpu'\n        map_location = device\n    else:\n        device = torch.device(\"cuda:{}\".format(rank))\n        map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n\n    tokenizer = ConditionTokenizer(args=args)\n    label_ids = list(tokenizer.mapping2id.values())\n    senti_ids = list(tokenizer.senti2id.values())\n\n    if args.model_config is not None:\n        bart_config = MultiModalBartConfig.from_dict(\n            json.load(open(args.model_config)))\n    else:\n        bart_config = MultiModalBartConfig.from_pretrained(args.checkpoint)\n\n    if args.dropout is not None:\n        bart_config.dropout = args.dropout\n    if args.attention_dropout is not None:\n        bart_config.attention_dropout = args.attention_dropout\n    if args.classif_dropout is not None:\n        bart_config.classif_dropout = args.classif_dropout\n    if args.activation_dropout is not None:\n        bart_config.activation_dropout = args.activation_dropout\n\n    bos_token_id = 0  # \u56e0\u4e3a\u662f\u7279\u6b8a\u7b26\u53f7\n    eos_token_id = 1\n\n    # import ipdb; ipdb.set_trace()\n\n    if args.checkpoint:\n        pretrain_model = MultiModalBartModelForPretrain.from_pretrained(\n            args.checkpoint,\n            config=bart_config,\n            bart_model=args.bart_model,\n            tokenizer=tokenizer,\n            label_ids=label_ids,\n            senti_ids=senti_ids,\n            args=args,\n            error_on_mismatch=False)\n        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n                                                 args.bart_model, tokenizer,\n                                                 label_ids)\n        seq2seq_model.encoder.load_state_dict(\n            pretrain_model.encoder.state_dict())\n        seq2seq_model.decoder.load_state_dict(\n            pretrain_model.span_decoder.state_dict())\n        model = SequenceGeneratorModel(seq2seq_model,\n                                       bos_token_id=bos_token_id,\n                                       eos_token_id=eos_token_id,\n                                       max_length=args.max_len,\n                                       max_len_a=args.max_len_a,\n                                       num_beams=args.num_beams,\n                                       do_sample=False,\n                                       repetition_penalty=1,\n                                       length_penalty=1.0,\n                                       pad_token_id=eos_token_id,\n                                       restricter=None)\n    else:\n        print('++++++++++++++++++ No Pretrain ++++++++++++++++++++++++++++++++')\n        seq2seq_model = MultiModalBartModel_AESC(bart_config, args,\n                                                 args.bart_model, tokenizer,\n                                                 label_ids)\n        model = SequenceGeneratorModel(seq2seq_model,\n                                       bos_token_id=bos_token_id,\n                                       eos_token_id=eos_token_id,\n                                       max_length=args.max_len,\n                                       max_len_a=args.max_len_a,\n                                       num_beams=args.num_beams,\n                                       do_sample=False,\n                                       repetition_penalty=1,\n                                       length_penalty=1.0,\n                                       pad_token_id=eos_token_id,\n                                       restricter=None)\n        # model = MultiModalBartModel_AESC(bart_config, args.bart_model,\n        #                                  tokenizer, label_ids)\n    model.to(device)\n\n    parameters = get_parameter_number(model) ##{'Total': 169351685, 'Trainable': 169351685}\n    print(parameters)\n    optimizer = AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n\n    scaler = GradScaler() if args.amp else None\n\n    epoch = 0\n    logger.info('Loading data...')\n    collate_aesc = Collator(\n                            args.task,\n                            tokenizer,\n                            mlm_enabled=False,\n                            senti_enabled=False,\n                            ae_enabled=False,\n                            oe_enabled=False,\n                            aesc_enabled=True,\n                            anp_enabled=False,\n                            max_img_num=args.num_image_tokens,\n                            has_prompt=args.has_prompt,\n                            text_only=args.text_only)\n\n    train_dataset = Twitter_Dataset(args.dataset[0][1], split='train')\n\n    dev_dataset = Twitter_Dataset(args.dataset[0][1], split='dev')\n    test_dataset = Twitter_Dataset(args.dataset[0][1], split='test')\n\n    train_loader = DataLoader(dataset=train_dataset,\n                              batch_size=args.batch_size,\n                              shuffle=True,\n                              num_workers=args.num_workers,\n                              pin_memory=True,\n                              collate_fn=collate_aesc)\n    dev_loader = DataLoader(dataset=dev_dataset,\n                            batch_size=args.batch_size,\n                            shuffle=False,\n                            num_workers=args.num_workers,\n                            pin_memory=True,\n                            collate_fn=collate_aesc)\n    test_loader = DataLoader(dataset=test_dataset,\n                             batch_size=args.batch_size,\n                             shuffle=False,\n                             num_workers=args.num_workers,\n                             pin_memory=True,\n                             collate_fn=collate_aesc)\n\n    callback = None\n    metric = AESCSpanMetric(eos_token_id,\n                            num_labels=len(label_ids),\n                            conflict_id=-1)\n    model.train()\n    start = datetime.now()\n    best_dev_res = None\n    best_dev_test_res = None\n    best_test_res = None\n    # res_dev = eval_utils.eval(model, dev_loader, metric, device)\n    while epoch < args.epochs:\n        logger.info('Epoch {}'.format(epoch + 1), pad=True)\n        fine_tune(epoch=epoch,\n                  model=model,\n                  train_loader=train_loader,\n                  test_loader=test_loader,\n                  metric=metric,\n                  optimizer=optimizer,\n                  args=args,\n                  device=device,\n                  logger=logger,\n                  callback=callback,\n                  log_interval=1,\n                  tb_writer=tb_writer,\n                  tb_interval=1,\n                  scaler=scaler)\n\n        print('test!!!!!!!!!!!!!!')\n        if (epoch + 1) % args.eval_every == 0:\n            # train_dev = eval_utils.eval(model, train_loader, metric, device)\n            res_dev, dev_aspects_num_acc = eval_utils.eval(args, model, dev_loader, metric, device)\n            res_test, test_aspects_num_acc = eval_utils.eval(args, model, test_loader, metric,\n                                       device)\n\n            logger.info('DEV  aesc_p:{} aesc_r:{} aesc_f:{}, dev_aspects_num_acc: {:.4f}'.format(\n                res_dev['aesc_pre'], res_dev['aesc_rec'], res_dev['aesc_f'], dev_aspects_num_acc))\n\n            logger.info('TEST  aesc_p:{} aesc_r:{} aesc_f:{}, test_aspects_num_acc: {:.4f}'.format(\n                res_test['aesc_pre'], res_test['aesc_rec'],\n                res_test['aesc_f'], test_aspects_num_acc))\n\n            save_flag = False\n            if best_dev_res is None:\n                best_dev_res = res_dev\n                best_dev_test_res = res_test\n\n            else:\n                if best_dev_res['aesc_f'] < res_dev['aesc_f']:\n                    best_dev_res = res_dev\n                    best_dev_test_res = res_test\n\n            if best_test_res is None:\n                best_test_res = res_test\n                save_flag = True\n            else:\n                if best_test_res['aesc_f'] < res_test['aesc_f']:\n                    best_test_res = res_test\n                    save_flag = True\n\n            if args.is_check == 1 and save_flag:\n                current_checkpoint_path = os.path.join(checkpoint_path,\n                                                       args.check_info)\n                model.seq2seq_model.save_pretrained(current_checkpoint_path)\n                print('save model!!!!!!!!!!!')\n        epoch += 1\n    logger.info(\"Training complete in: \" + str(datetime.now() - start),\n                pad=True)\n    logger.info('---------------------------')\n    logger.info('BEST DEV:-----')\n    logger.info('BEST DEV  aesc_p:{} aesc_r:{} aesc_f:{}'.format(\n        best_dev_res['aesc_pre'], best_dev_res['aesc_rec'],\n        best_dev_res['aesc_f']))\n\n    logger.info('BEST DEV TEST:-----')\n    logger.info('BEST DEV--TEST  aesc_p:{} aesc_r:{} aesc_f:{}'.format(\n        best_dev_test_res['aesc_pre'], best_dev_test_res['aesc_rec'],\n        best_dev_test_res['aesc_f']))\n\n    logger.info('BEST TEST:-----')\n    logger.info('BEST TEST  aesc_p:{} aesc_r:{} aesc_f:{}'.format(\n        best_test_res['aesc_pre'], best_test_res['aesc_rec'],\n        best_test_res['aesc_f']))", "\n    # if not args.cpu:\n    #     cleanup_process()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset',\n                        action='append',\n                        nargs=2,\n                        metavar=('DATASET_NAME', 'DATASET_PATH'),\n                        required=True,\n                        help='')\n    # required\n\n    parser.add_argument('--checkpoint_dir',\n                        required=True,\n                        type=str,\n                        help='where to save the checkpoint')\n    parser.add_argument('--bart_model',\n                        default='/home/xiaocui/code/FW-MABSA/data/weights/bart-base',\n                        type=str,\n                        help='bart pretrain model')\n    # path\n    parser.add_argument(\n        '--log_dir',\n        default=None,\n        type=str,\n        help='path to output log files, not output to file if not specified')\n    parser.add_argument('--model_config',\n                        default=None,\n                        type=str,\n                        help='path to load model config')\n    parser.add_argument('--text_only', action='store_true', default=False, help='whether only use text')\n    parser.add_argument('--checkpoint',\n                        default=None,\n                        type=str,\n                        help='name or path to load weights')\n    parser.add_argument('--lr_decay_every',\n                        default=4,\n                        type=int,\n                        help='lr_decay_every')\n    parser.add_argument('--lr_decay_ratio',\n                        default=0.8,\n                        type=float,\n                        help='lr_decay_ratio')\n    # training and evaluation\n    parser.add_argument('--epochs',\n                        default=35,\n                        type=int,\n                        help='number of training epoch')\n    parser.add_argument('--eval_every', default=1, type=int, help='eval_every')\n    parser.add_argument('--lr', default=1e-2, type=float, help='learning rate')\n    parser.add_argument('--num_beams',\n                        default=4,\n                        type=int,\n                        help='level of beam search on validation')\n    parser.add_argument(\n        '--continue_training',\n        action='store_true',\n        help='continue training, load optimizer and epoch from checkpoint')\n    parser.add_argument('--warmup', default=0.1, type=float, help='warmup')\n    # dropout\n    parser.add_argument(\n        '--dropout',\n        default=None,\n        type=float,\n        help=\n        'dropout rate for the transformer. This overwrites the model config')\n    parser.add_argument(\n        '--classif_dropout',\n        default=None,\n        type=float,\n        help=\n        'dropout rate for the classification layers. This overwrites the model config'\n    )\n    parser.add_argument(\n        '--attention_dropout',\n        default=None,\n        type=float,\n        help=\n        'dropout rate for the attention layers. This overwrites the model config'\n    )\n    parser.add_argument(\n        '--activation_dropout',\n        default=None,\n        type=float,\n        help=\n        'dropout rate for the activation layers. This overwrites the model config'\n    )\n\n    # hardware and performance\n    parser.add_argument('--grad_clip', default=5, type=float, help='grad_clip')\n    parser.add_argument('--gpu_num',\n                        default=1,\n                        type=int,\n                        help='number of GPUs in total')\n    parser.add_argument('--cpu',\n                        action='store_true',\n                        help='if only use cpu to run the model')\n    parser.add_argument('--amp',\n                        action='store_true',\n                        help='whether or not to use amp')\n    parser.add_argument('--master_port',\n                        type=str,\n                        default='12355',\n                        help='master port for DDP')\n    parser.add_argument('--batch_size',\n                        type=int,\n                        default=16,\n                        help='training batch size')\n    parser.add_argument('--seed', type=int, default=42, help='seed')\n    parser.add_argument('--num_workers',\n                        type=int,\n                        default=4,\n                        help='#workers for data loader')\n    parser.add_argument('--max_len', type=int, default=10, help='max_len')\n    parser.add_argument('--num_image_tokens', type=int, default=2, help='the length of image_tokens')\n    parser.add_argument('--max_len_a',\n                        type=float,\n                        default=0.6,\n                        help='max_len_a')\n\n    parser.add_argument('--bart_init',\n                        type=int,\n                        default=1,\n                        help='use bart_init or not')\n\n    parser.add_argument('--check_info',\n                        type=str,\n                        default='',\n                        help='check path to save')\n    parser.add_argument('--is_check',\n                        type=int,\n                        default=0,\n                        help='save the model or not')\n    parser.add_argument('--task', type=str, default='AESC', help='task type')\n    parser.add_argument('--has_prompt',  action='store_true', default=False, help='whether has prompt')\n    parser.add_argument('--use_generated_aspect_prompt',  action='store_true', default=False, help='whether use the generated aspect prompt')\n    parser.add_argument('--use_generated_senti_prompt',  action='store_true', default=False, help='whether use the generated sentiment prompt')\n    parser.add_argument('--use_different_senti_prompt', action='store_true', default=False, help='whether use different prompt for different aspects in an instance')\n    parser.add_argument('--use_different_aspect_prompt', action='store_true', default=False, help='whether use different prompt for different aspects in an instance')\n    parser.add_argument('--use_multitasks', action='store_true', default=False, help='whether use multitasks')\n    parser.add_argument('--loss_lambda', default=0.1, type=float, help='the weight of aspect_num classification loss')\n    \n\n    args = parser.parse_args()\n\n    if args.gpu_num != 1 and args.cpu:\n        raise ValueError('--gpu_num are not allowed if --cpu is set to true')\n\n    if args.checkpoint is None and args.model_config is None:\n        raise ValueError(\n            '--model_config and --checkpoint cannot be empty at the same time')\n\n    return args", "\n\nif __name__ == '__main__':\n    args = parse_args()\n\n    # mp.spawn(main, args=(args, ), nprocs=args.gpu_num, join=True)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    cudnn.deterministic = True\n    main(0, args)", ""]}
{"filename": "src/eval_utils_multitasks.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\n\ndef eval(args, model, loader, metric, device):\n    num_correct =0 \n    model.eval()\n    for i, batch in enumerate(loader):\n        # Forward pass\n        if args.task == 'twitter_ae':\n            aesc_infos = {\n                key: value\n                for key, value in batch['TWITTER_AE'].items()\n            }\n        elif args.task == 'twitter_sc':\n            aesc_infos = {\n                key: value\n                for key, value in batch['TWITTER_SC'].items()\n            }\n        else:\n            aesc_infos = {key: value for key, value in batch['AESC'].items()}\n        # import ipdb; ipdb.set_trace()\n        predict, predict_aspects_num = model.predict(\n            input_ids=batch['input_ids'].to(device),\n            image_features=list(\n                map(lambda x: x.to(device), batch['image_features'])),\n            attention_mask=batch['attention_mask'].to(device),\n            aesc_infos=aesc_infos, \n            aspects_num=batch['aspects_num'])\n        target_aspects_num = torch.tensor(batch['aspects_num']).to(predict_aspects_num.device)\n        num_correct += torch.eq(predict_aspects_num, target_aspects_num).sum().float().item()\n        \n        # print('predict is {}'.format(predict))\n\n        metric.evaluate(aesc_infos['spans'], predict,\n                        aesc_infos['labels'].to(device))\n        # break\n    aspects_num_eval_acc = num_correct/len(loader.dataset)\n    res = metric.get_metric()\n    model.train()\n    return res, aspects_num_eval_acc", ""]}
{"filename": "src/eval_utils.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\n\ndef eval(args, model, loader, metric, device):\n    model.eval()\n    for i, batch in enumerate(loader):\n        # Forward pass\n        if args.task == 'twitter_ae':\n            aesc_infos = {\n                key: value\n                for key, value in batch['TWITTER_AE'].items()\n            }\n        elif args.task == 'twitter_sc':\n            aesc_infos = {\n                key: value\n                for key, value in batch['TWITTER_SC'].items()\n            }\n        else:\n            aesc_infos = {key: value for key, value in batch['AESC'].items()}\n        # import ipdb; ipdb.set_trace()\n        predict = model.predict(\n            input_ids=batch['input_ids'].to(device),\n            image_features=list(\n                map(lambda x: x.to(device), batch['image_features'])),\n            attention_mask=batch['attention_mask'].to(device),\n            aesc_infos=aesc_infos, \n            aspects_num=batch['aspects_num'])\n        \n        # print('predict is {}'.format(predict))\n\n        metric.evaluate(aesc_infos['spans'], predict,\n                        aesc_infos['labels'].to(device))\n        # break\n\n    res = metric.get_metric()\n    model.train()\n    return res", ""]}
{"filename": "src/utils.py", "chunked_list": ["import logging\nimport sys\nimport os\n\nimport torch\nimport torch.distributed as dist\n\n\ndef setup_process(rank, world_size, master_port='12355'):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = master_port\n\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)", "def setup_process(rank, world_size, master_port='12355'):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = master_port\n\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n\ndef cleanup_process():\n    dist.destroy_process_group()\n", "\n\ndef save_training_data(path, optimizer=None, scaler=None, epoch=None):\n    checkpoint = {\n        'optimizer': None if optimizer is None else optimizer.state_dict(),\n        'scaler': None if scaler is None else scaler.state_dict(),\n        'epoch': epoch\n    }\n\n    torch.save(checkpoint, os.path.join(path, 'training_data.pt'))", "\n\ndef load_training_data(path, optimizer=None, scaler=None, map_location=None):\n    checkpoint = torch.load(os.path.join(path, 'training_data.pt'), map_location=map_location)\n\n    if optimizer is not None and 'optimizer' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n\n    if scaler is not None and 'scaler' in checkpoint:\n        scaler.load_state_dict(checkpoint['scaler'])\n\n    return checkpoint", "\n\nclass Logger:\n    def __init__(self, log_dir=None, enabled=True, pad_length=50):\n        self._logger = self._get_logger(log_dir) if enabled else None\n        self._pad_length = pad_length\n\n    def _pad_message(self, message):\n        return (\" \" + message + \" \").center(self._pad_length, '=')\n\n    def info(self, message, pad=False):\n        if self._logger is not None:\n            message = self._pad_message(message) if pad else message\n            self._logger.info(message)\n\n    def line(self):\n        if self._logger is not None:\n            self._logger.info('=' * self._pad_length)\n\n    @staticmethod\n    def _get_logger(log_dir=None):\n        \"\"\"\n        get a logger for displaying information to console or log to file (optional)\n        :param log_dir: str, logging path. None for not log to file\n        :return: logger\n        \"\"\"\n        logger = logging.getLogger()\n        logger.setLevel(logging.DEBUG)\n\n        stream_handler = logging.StreamHandler(sys.stdout)\n        stream_handler.flush = sys.stdout.flush\n        logger.addHandler(stream_handler)\n\n        if log_dir is not None:\n            file_handler = logging.FileHandler(log_dir)\n            formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n            file_handler.setFormatter(formatter)\n            logger.addHandler(file_handler)\n\n        return logger", "\n\nclass TaskType:\n    AFTER = 'after'\n    BEFORE = 'before'\n    INTENT = 'intent'\n    CAPTION = 'caption'\n    REGION_CAPTION = 'region_caption'\n\n    ALL_TYPES = {AFTER, BEFORE, INTENT, CAPTION, REGION_CAPTION}", ""]}
{"filename": "src/training_multitasks.py", "chunked_list": ["from datetime import datetime\n\nimport numpy as np\nfrom torch.cuda.amp import autocast\nimport src.model.utils as utils\nimport src.eval_utils as eval_utils\n# from src.utils import TaskType\nimport torch\n\ndef pretrain(task_list,\n             epoch,\n             model,\n             train_loaders,\n             optimizer_dict,\n             device,\n             args,\n             logger=None,\n             callback=None,\n             log_interval=1,\n             tb_writer=None,\n             tb_interval=1,\n             scaler=None):\n\n    # assert len(task_list) == len(train_loaders)\n\n    total_step = len(train_loaders[0])\n    model.train()\n    total_loss = 0\n\n    start_time = datetime.now()\n\n    for i, batchs in enumerate(zip(*train_loaders)):\n        # Forward pass\n        with autocast(enabled=args.amp):\n            loss_all = []\n            total_loss = 0\n            for cnt, task in enumerate(task_list):\n                batch = batchs[cnt]\n                # print(batch.keys())\n                if task == 'Sentiment':\n                    loss, prelogits = model.forward(\n                        task,\n                        input_ids=batch['input_ids'].to(device),\n                        image_features=list(\n                            map(lambda x: x.to(device),\n                                batch['image_features'])),\n                        attention_mask=batch['attention_mask'].to(device),\n                        senti_infos={\n                            key: value.to(device)\n                            for key, value in batch['Sentiment'].items()\n                        })\n                else:\n                    loss = model.forward(\n                        task,\n                        input_ids=batch['input_ids'].to(device),\n                        image_features=list(\n                            map(lambda x: x.to(device),\n                                batch['image_features'])),\n                        attention_mask=batch['attention_mask'].to(device),\n                        mlm_infos={\n                            key: value.to(device)\n                            for key, value in batch['MLM'].items()\n                        } if 'MLM' in batch else None,\n                        mrm_infos={\n                            key: value\n                            for key, value in batch['MRM'].items()\n                        } if 'MRM' in batch else None,\n                        senti_infos={\n                            key: value.to(device)\n                            for key, value in batch['Sentiment'].items()\n                        } if 'Sentiment' in batch else None,\n                        ANP_infos={\n                            key: value.to(device)\n                            for key, value in batch['ANP'].items()\n                        } if 'ANP' in batch else None,\n                        ANP_generate_infos={\n                            key: value.to(device)\n                            for key, value in batch['ANP_generate'].items()\n                        } if 'ANP_generate' in batch else None,\n                        ae_oe_infos={\n                            key: value\n                            for key, value in batch['AE_OE'].items()\n                        } if 'AE_OE' in batch else None)\n\n                # print(loss.dtype)\n                loss_all.append(loss)\n                optimizer_dict.zero_grad()\n\n                loss.backward()\n                optimizer_dict.step()\n\n            for k, v in zip(task_list, loss_all):\n                print(k + ':', v.item(), end=' ')\n            print()\n        # Backward and optimize\n\n        if logger is not None and i % log_interval == 0:\n            logger.info('Epoch [{}/{}], Step [{}/{}]'.format(\n                epoch + 1, args.epochs, i + 1, total_step))\n            loss_text = ' '.join(\n                [k + ':' + str(v.item()) for k, v in zip(task_list, loss_all)])\n            logger.info(loss_text + '\\n')", "\ndef pretrain(task_list,\n             epoch,\n             model,\n             train_loaders,\n             optimizer_dict,\n             device,\n             args,\n             logger=None,\n             callback=None,\n             log_interval=1,\n             tb_writer=None,\n             tb_interval=1,\n             scaler=None):\n\n    # assert len(task_list) == len(train_loaders)\n\n    total_step = len(train_loaders[0])\n    model.train()\n    total_loss = 0\n\n    start_time = datetime.now()\n\n    for i, batchs in enumerate(zip(*train_loaders)):\n        # Forward pass\n        with autocast(enabled=args.amp):\n            loss_all = []\n            total_loss = 0\n            for cnt, task in enumerate(task_list):\n                batch = batchs[cnt]\n                # print(batch.keys())\n                if task == 'Sentiment':\n                    loss, prelogits = model.forward(\n                        task,\n                        input_ids=batch['input_ids'].to(device),\n                        image_features=list(\n                            map(lambda x: x.to(device),\n                                batch['image_features'])),\n                        attention_mask=batch['attention_mask'].to(device),\n                        senti_infos={\n                            key: value.to(device)\n                            for key, value in batch['Sentiment'].items()\n                        })\n                else:\n                    loss = model.forward(\n                        task,\n                        input_ids=batch['input_ids'].to(device),\n                        image_features=list(\n                            map(lambda x: x.to(device),\n                                batch['image_features'])),\n                        attention_mask=batch['attention_mask'].to(device),\n                        mlm_infos={\n                            key: value.to(device)\n                            for key, value in batch['MLM'].items()\n                        } if 'MLM' in batch else None,\n                        mrm_infos={\n                            key: value\n                            for key, value in batch['MRM'].items()\n                        } if 'MRM' in batch else None,\n                        senti_infos={\n                            key: value.to(device)\n                            for key, value in batch['Sentiment'].items()\n                        } if 'Sentiment' in batch else None,\n                        ANP_infos={\n                            key: value.to(device)\n                            for key, value in batch['ANP'].items()\n                        } if 'ANP' in batch else None,\n                        ANP_generate_infos={\n                            key: value.to(device)\n                            for key, value in batch['ANP_generate'].items()\n                        } if 'ANP_generate' in batch else None,\n                        ae_oe_infos={\n                            key: value\n                            for key, value in batch['AE_OE'].items()\n                        } if 'AE_OE' in batch else None)\n\n                # print(loss.dtype)\n                loss_all.append(loss)\n                optimizer_dict.zero_grad()\n\n                loss.backward()\n                optimizer_dict.step()\n\n            for k, v in zip(task_list, loss_all):\n                print(k + ':', v.item(), end=' ')\n            print()\n        # Backward and optimize\n\n        if logger is not None and i % log_interval == 0:\n            logger.info('Epoch [{}/{}], Step [{}/{}]'.format(\n                epoch + 1, args.epochs, i + 1, total_step))\n            loss_text = ' '.join(\n                [k + ':' + str(v.item()) for k, v in zip(task_list, loss_all)])\n            logger.info(loss_text + '\\n')", "\n\ndef fine_tune(epoch,\n              model,\n              train_loader,\n              test_loader,\n              metric,\n              optimizer,\n              device,\n              args,\n              logger=None,\n              callback=None,\n              log_interval=1,\n              tb_writer=None,\n              tb_interval=1,\n              scaler=None):\n\n    total_step = len(train_loader)\n    model.train()\n    total_loss = 0\n\n    start_time = datetime.now()\n    num_correct =0\n\n    for i, batch in enumerate(train_loader):\n        # Forward pass\n        if args.task == 'twitter_ae':\n            aesc_infos = {\n                key: value\n                for key, value in batch['TWITTER_AE'].items()\n            }\n        elif args.task == 'twitter_sc':\n            aesc_infos = {\n                key: value\n                for key, value in batch['TWITTER_SC'].items()\n            }\n        else:\n            aesc_infos = {key: value for key, value in batch['AESC'].items()}\n            # import ipdb; ipdb.set_trace()\n            # print(\"+++++++++++++++++++++++++++++++++++++++++++\")\n            # print('aesc_infos is {}'.format(aesc_infos))\n        with autocast(enabled=args.amp):\n            loss, predict_aspects_num = model.forward(\n                input_ids=batch['input_ids'].to(device),\n                image_features=list(\n                    map(lambda x: x.to(device), batch['image_features'])),\n                attention_mask=batch['attention_mask'].to(device),\n                aesc_infos=aesc_infos, \n                aspects_num=batch['aspects_num'])\n            target_aspects_num = torch.tensor(batch['aspects_num']).to(predict_aspects_num.device)\n            num_correct += torch.eq(predict_aspects_num, target_aspects_num).sum().float().item()\n\n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n                epoch + 1, args.epochs, i + 1, total_step, loss.item()))\n        train_acc = num_correct/len(train_loader.dataset)\n        print('The accuracy of aspects_num is {:.4f} !!!!'.format(train_acc))\n        # Backward and optimize\n\n        cur_step = i + 1 + epoch * total_step\n        t_step = args.epochs * total_step\n        liner_warm_rate = utils.liner_warmup(cur_step, t_step, args.warmup)\n        utils.set_lr(optimizer, liner_warm_rate * args.lr)\n\n        optimizer.zero_grad()\n\n        loss.backward()\n        utils.clip_gradient(optimizer, args.grad_clip)\n\n        optimizer.step()"]}
{"filename": "src/data/tokenization_new_for_generated_prompt.py", "chunked_list": ["import torch\nimport numpy as np\nfrom transformers import BartTokenizer, AutoTokenizer\nfrom itertools import chain\nfrom functools import cmp_to_key\n# from src.utils import TaskType\n\n\ndef cmp(v1, v2):\n    if v1[0] == v2[0]:\n        return v1[1] - v2[1]\n    return v1[0] - v2[0]", "def cmp(v1, v2):\n    if v1[0] == v2[0]:\n        return v1[1] - v2[1]\n    return v1[0] - v2[0]\n\n\nclass ConditionTokenizer:\n    \"\"\"\n    tokenizer for image features, event and task type\n    this is NOT inherent from transformers Tokenizer\n    \"\"\"\n    def __init__(self,\n                 args,\n                 pretrained_model_name='/home/xiaocui/code/FW-MABSA/data/weights/bart-base',\n                 cls_token=\"<<cls>>\",\n                 mlm_token=\"<<mlm>>\",\n                 mrm_token=\"<<mrm>>\",\n                 begin_text=\"<<text>>\",\n                 end_text=\"<</text>>\",\n                 img_feat='<<img_feat>>',\n                 begin_img=\"<<img>>\",\n                 end_img=\"<</img>>\",\n                 img_caption='<<img_caption>>',\n                 begin_caption='<<cap>>',\n                 end_caption='<</cap>>',\n                 ae_token='<<AE>>',\n                 sc_token='<<SC>>',\n                 ae_oe_token=\"<<AOE>>\",\n                 sep_token=\"<<SEP>>\",\n                 aesc_token='<<AESC>>',\n                 pos_token='<<POS>>',\n                 neu_token='<<NEU>>',\n                 neg_token='<<NEG>>',\n                 aspect_prompt_token='<<AE_PROMPT>>',\n                 senti_prompt_token='<<SENTI_PROMPT>>',\n                 begin_prompt='<<prompt>>',\n                 end_prompt='<</prompt>>',\n                 senti_token='<<senti>>',\n                 ANP_token='<<ANP>>',\n                 ANP_generate_token='<<AOG>>'):\n        self._base_tokenizer = BartTokenizer.from_pretrained(\n            pretrained_model_name, )\n        # self._base_tokenizer = AutoTokenizer.from_pretrained(\n        #     pretrained_model_name)\n\n        self.additional_special_tokens = [\n            cls_token, mlm_token, mrm_token, begin_text, end_text, img_feat, begin_img, end_img, \n            img_caption, begin_caption, end_caption,\n            senti_token, ANP_token, ANP_generate_token,\n            pos_token, neu_token, neg_token, ae_oe_token, sep_token,\n            aesc_token, ae_token, sc_token, \n            aspect_prompt_token, senti_prompt_token, begin_prompt, end_prompt\n        ]\n        unique_no_split_tokens = self._base_tokenizer.unique_no_split_tokens\n        self._base_tokenizer.unique_no_split_tokens = unique_no_split_tokens + self.additional_special_tokens\n        self.unique_no_split_tokens = self._base_tokenizer.unique_no_split_tokens\n        print(self.unique_no_split_tokens)\n\n        self._base_tokenizer.add_tokens(self.additional_special_tokens)\n        self.cls_token = cls_token\n        self.mlm_token = mlm_token\n        self.mrm_token = mrm_token\n        self.begin_text = begin_text\n        self.end_text = end_text\n        self.img_feat = img_feat\n        self.begin_img = begin_img\n        self.end_img = end_img\n\n        self.img_caption = img_caption\n        self.begin_caption = begin_caption\n        self.end_caption = end_caption\n\n        self.ae_token = ae_token\n        self.sc_token = sc_token\n        self.ae_oe_token = ae_oe_token\n        self.sep_token = sep_token\n        self.senti_token = senti_token\n        self.ANP_token = ANP_token\n        self.ANP_generate_token = ANP_generate_token\n\n        self.aesc_token = aesc_token\n        self.pos_token = pos_token\n        self.neu_token = neu_token\n        self.neg_token = neg_token\n\n        self.aspect_prompt_token = aspect_prompt_token\n        self.senti_prompt_token = senti_prompt_token\n        self.begin_prompt = begin_prompt\n        self.end_prompt = end_prompt\n\n        self.cls_token_id = self.convert_tokens_to_ids(cls_token)\n        self.mlm_token_id = self.convert_tokens_to_ids(mlm_token)\n        self.mrm_token_id = self.convert_tokens_to_ids(mrm_token)\n        self.begin_text_id = self.convert_tokens_to_ids(begin_text)\n        self.end_text_id = self.convert_tokens_to_ids(end_text)\n        self.img_feat_id = self.convert_tokens_to_ids(img_feat)\n        self.begin_img_id = self.convert_tokens_to_ids(begin_img)\n        self.end_img_id = self.convert_tokens_to_ids(end_img)\n\n        self.img_caption_id = self.convert_tokens_to_ids(img_caption)\n        self.begin_caption_id = self.convert_tokens_to_ids(begin_caption)\n        self.end_caption_id = self.convert_tokens_to_ids(end_caption)\n\n        self.ae_token_id = self.convert_tokens_to_ids(ae_token)\n        self.sc_token_id = self.convert_tokens_to_ids(sc_token)\n        self.ae_oe_token_id = self.convert_tokens_to_ids(ae_oe_token)\n        self.sep_token_id = self.convert_tokens_to_ids(sep_token)\n        self.senti_token_id = self.convert_tokens_to_ids(senti_token)\n        self.ANP_token_id = self.convert_tokens_to_ids(ANP_token)\n        self.ANP_generate_token_id = self.convert_tokens_to_ids(\n            ANP_generate_token)\n        self.aesc_token_id = self.convert_tokens_to_ids(aesc_token)\n        self.pos_token_id = self.convert_tokens_to_ids(pos_token)\n        self.neu_token_id = self.convert_tokens_to_ids(neu_token)\n        self.neg_token_id = self.convert_tokens_to_ids(neg_token)\n\n        self.aspect_prompt_token_id = self.convert_tokens_to_ids(aspect_prompt_token)\n        self.senti_prompt_token_id = self.convert_tokens_to_ids(senti_prompt_token)\n        self.begin_prompt_id =  self.convert_tokens_to_ids(begin_prompt)\n        self.end_prompt_id =  self.convert_tokens_to_ids(end_prompt)\n\n        self.vocab_size = self._base_tokenizer.vocab_size\n        self.bos_token = self._base_tokenizer.bos_token\n        self.bos_token_id = self._base_tokenizer.bos_token_id\n\n        self.eos_token = self._base_tokenizer.eos_token\n        self.eos_token_id = self._base_tokenizer.eos_token_id\n        self.pad_token = self._base_tokenizer.pad_token\n        self.pad_token_id = self._base_tokenizer.pad_token_id\n        self.unk_token = self._base_tokenizer.unk_token\n        self.unk_token_id = self._base_tokenizer.unk_token_id\n\n\n        print('self.bos_token_id', self.bos_token_id)\n        print('self.eos_token_id', self.eos_token_id)\n        print('self.pad_token_id', self.pad_token_id)\n        print('self.begin_caption_token_id', self.begin_caption_id)\n        print('self.end_caption_token_id', self.end_caption_id)\n        print('self.aspect_prompt_token_id', self.aspect_prompt_token_id)\n        print('self.senti_prompt_token_id', self.senti_prompt_token_id)\n        print('self.begin_prompt_id', self.begin_prompt_id)\n        print('self.end_prompt_id', self.end_prompt_id)\n\n        if args.task == 'pretrain':\n            self.mapping = {'AE_OE': '<<AOE>>', 'SEP': '<<SEP>>'}\n        else:\n            if args.task == 'twitter_sc':\n                self.mapping = {\n                    'SC': '<<SC>>',\n                    'POS': '<<POS>>',\n                    'NEU': '<<NEU>>',\n                    'NEG': '<<NEG>>'\n                }\n            elif args.task == 'twitter_ae':\n                self.mapping = {\n                    'AE': '<<AE>>',\n                    'POS': '<<POS>>',\n                    'NEU': '<<NEU>>',\n                    'NEG': '<<NEG>>'\n                }\n            else:\n                self.mapping = {\n                    'AESC': '<<AESC>>',\n                    'POS': '<<POS>>',\n                    'NEU': '<<NEU>>',\n                    'NEG': '<<NEG>>'\n                }\n        self.senti = {'POS': '<<POS>>', 'NEU': '<<NEU>>', 'NEG': '<<NEG>>'}\n        self.senti2id = {}\n        for key, value in self.senti.items():\n            key_id = self._base_tokenizer.convert_tokens_to_ids(\n                self._base_tokenizer.tokenize(value))\n            assert len(key_id) == 1, value\n            # assert key_id[0] >= self.cur_num_tokens\n            self.senti2id[key] = key_id[0]\n        self.mapping2id = {}\n        self.mapping2targetid = {}\n        for key, value in self.mapping.items():\n            key_id = self._base_tokenizer.convert_tokens_to_ids(\n                self._base_tokenizer.tokenize(value))\n            assert len(key_id) == 1, value\n            # assert key_id[0] >= self.cur_num_tokens\n            self.mapping2id[key] = key_id[0]\n            self.mapping2targetid[key] = len(self.mapping2targetid) + 2\n        print(self.mapping2id)\n        '''\n        for AESC:\n        {'AESC': 50281, 'POS': 50276, 'NEU': 50277, 'NEG': 50278}\n        '''\n\n    def encode(self, *args, **kwargs):\n        return self._base_tokenizer(*args, **kwargs)\n\n    def pad_tokens(self, tokens):\n        max_len = max([len(x) for x in tokens])\n        pad_result = torch.full((len(tokens), max_len),\n                                self.pad_token_id,\n                                dtype=torch.long)\n        mask = torch.zeros(pad_result.size(), dtype=torch.bool)\n        for i, x in enumerate(tokens):\n            pad_result[i, :len(x)] = torch.tensor(tokens[i], dtype=torch.long)\n            mask[i, :len(x)] = True\n        return pad_result, mask\n    \n    def pad_tokens_with_maxlength(self, tokens, max_len):\n        pad_result = torch.full((len(tokens), max_len),\n                                self.pad_token_id,\n                                dtype=torch.long)\n        mask = torch.zeros(pad_result.size(), dtype=torch.bool)\n     \n        for i, x in enumerate(tokens):\n            # print(x)\n            pad_result[i, :len(x)] = torch.tensor(tokens[i], dtype=torch.long)\n            mask[i, :len(x)] = True\n        # print(\"=====================pad_result=========================\")\n        # print(pad_result)\n        # print(mask)\n        return pad_result, mask\n    \n\n    def encode_mlm_sentence(self, labels):\n        label_split = [x.split() for x in labels]\n        input_tokens = []\n        for split in label_split:\n            cur_num = 0\n            bpes = [self.bos_token_id]\n            for x in split:\n                tokens = self._base_tokenizer(x, add_prefix_space=True)\n                bpes = bpes + tokens\n            bpes.append(self.eos_token_id)\n            input_tokens.append(input_tokens)\n        return input_tokens\n\n    def encode_condition(self, task, img_num=None, caption=None, sentence=None, has_prompt=False, aspects_num=None,  text_only=False):\n        \"\"\"\n        tokenize text, image features and event\n        the output format (after decoded back):\n        task_type [<img> <img_feat> ... <img_feat> </img>] [<event> EVENT </event>] [<mlm> MLM </mlm>]\n\n        :param task_type: str or list[str]\n        :param img_num: int or list[int], the number of image features\n        :param event: str or list[str], event descriptions\n        :param mlm: str or list[str], sentence for masked language modeling\n        :return: dict {str: Tensor}, {\n                \"input_ids\": ...,\n                \"attention_mask\": ...,\n                \"event_mask\": ...,          only exist if event is given. 1 for the position with event tokens\n                \"mlm_mask\": ...,            only exist if mlm is given. 1 for the position with mlm tokens\n                \"img_mask\":...,             only exist if img_num is given. 1 for the position with img tokens\n            }\n        \"\"\"\n        \n        '''\n        [image_features] + is + [image_caption] + [text] + [aspect_prompt_token]*len_1(\u6700\u591a\u4e3a5) + has + [senti_prompt_token]*len_2 'sentiment'\n\n        prompt_1: [image_features] + is + [image_caption]  \n        + 'There is' <prompt> ([aspect_prompt_token]*len_1(\u6700\u591a\u4e3a5) + of [senti_prompt_token]*len_2 + 'sentiment' + <sep>)*n </prompt> + 'in'\n        + [text]\n\n        prompt_2: [image_features] + is + [image_caption]  \n        + <prompt> ([aspect_prompt_token]*len_1(\u6700\u591a\u4e3a5) + of [senti_prompt_token]*len_2)*n </prompt> \n        + [text]\n\n        prompt_3: [image_features] + is + [image_caption]  \n        + 'There is' <prompt> ([aspect_prompt_token]*len_1(\u6700\u591a\u4e3a5) + of <<NEU>> or <<POS>> or <<NEG>> + 'sentiment' + <sep>)*n </prompt> + 'in'\n        + [text]\n\n        '''\n\n        image_text = None\n        if img_num is not None:\n            if not isinstance(img_num, list):\n                img_num = [img_num]\n            image_text = []\n            for index, value in enumerate(img_num):\n                image_text.append(self.begin_img + self.img_feat * value +  ###\u5f15\u5165image_caption token\n                                  self.end_img)\n\n        # import ipdb; ipdb.set_trace()\n        if caption is not None:\n            if not isinstance(caption, list):\n                caption = [caption]\n            caption_split = [x.split() for x in caption]\n            image_caption_tokens = []\n            for split in caption_split:\n                '''\n                print(split)\n                \u4e3a\u65b9\u4fbf\u8d77\u89c1\uff0c\u56fa\u5b9acaption\u6587\u672c\u957f\u5ea6\n                '''\n                # print(\"+++++++++++++++++++++split before ++++++++++++++++++++++++\")\n                # print(len(split))\n                if len(split)>10:\n                    split = split[:10]\n                # print(\"+++++++++++++++++++++split after ++++++++++++++++++++++++\")\n                # print(len(split))\n                # print(split)\n                is_bpes = self._base_tokenizer.tokenize('is',\n                                                         add_prefix_space=True)\n                is_bpes = self._base_tokenizer.convert_tokens_to_ids(is_bpes) ##[16]\n                caption_word_bpes = [is_bpes]\n\n                caption_word_bpes.append([self.begin_caption_id])          \n\n                for caption_word in split:\n                    caption_bpes = self._base_tokenizer.tokenize(caption_word,\n                                                         add_prefix_space=True)\n                    caption_bpes = self._base_tokenizer.convert_tokens_to_ids(caption_bpes)\n                    caption_word_bpes.append(caption_bpes)\n                caption_word_bpes.append([self.end_caption_id])\n\n                _caption_word_bpes = list(chain(*caption_word_bpes))\n                image_caption_tokens.append(_caption_word_bpes.copy())\n\n        if sentence is not None:\n            if not isinstance(sentence, list):\n                sentence = [sentence]\n            sentence_split = [x.split() for x in sentence]\n            input_sentence_tokens = []\n            for split in sentence_split:\n                word_bpes = [[self.bos_token_id]]\n                for word in split:\n                    bpes = self._base_tokenizer.tokenize(word,\n                                                         add_prefix_space=True)\n                    bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                    word_bpes.append(bpes)\n                word_bpes.append([self.eos_token_id])\n\n                _word_bpes = list(chain(*word_bpes))\n                input_sentence_tokens.append(_word_bpes.copy())\n        \n        if has_prompt:\n            aspect_prompts_tokens = []     \n            for index, value in enumerate(aspects_num):\n                # aspect_prompts.append(self.begin_prompt + (self.aspect_prompt_token*2 + 'has' +  self.senti_prompt_token + 'sentiment' + self.sep_token) * value  +\n                #                   self.end_prompt)\n                aspect_prompt_bpes = [[self.begin_prompt_id]]\n                if value == 1:\n                    _be = 'is'\n                else:\n                    _be = 'are'\n                \n                there_bpes = self._base_tokenizer.tokenize('there',\n                                                        add_prefix_space=True)\n                there_bpes = self._base_tokenizer.convert_tokens_to_ids(there_bpes)\n                aspect_prompt_bpes.append(there_bpes)\n\n                _be_bpes = self._base_tokenizer.tokenize(_be,\n                                                        add_prefix_space=True)\n                _be_bpes = self._base_tokenizer.convert_tokens_to_ids(_be_bpes)\n                aspect_prompt_bpes.append(_be_bpes)\n\n                for i in range(value):\n                    if task == 'AESC':\n                        aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n\n                        of_bpes = self._base_tokenizer.tokenize('of',\n                                                                add_prefix_space=True)\n                        of_bpes = self._base_tokenizer.convert_tokens_to_ids(of_bpes)\n                        aspect_prompt_bpes.append(of_bpes)\n                        \n                        aspect_prompt_bpes.append([self.senti_prompt_token_id])\n\n                        senti_bpes = self._base_tokenizer.tokenize('sentiment',\n                                                                add_prefix_space=True)\n                        senti_bpes = self._base_tokenizer.convert_tokens_to_ids(senti_bpes)\n                        aspect_prompt_bpes.append(senti_bpes)\n\n                        if i <(value-1):\n                            aspect_prompt_bpes.append([self.sep_token_id])\n                    elif task == 'twitter_sc':\n                        aspect_prompt_bpes.append([self.senti_prompt_token_id])\n\n                        senti_bpes = self._base_tokenizer.tokenize('sentiment',\n                                                                add_prefix_space=True)\n                        senti_bpes = self._base_tokenizer.convert_tokens_to_ids(senti_bpes)\n                        aspect_prompt_bpes.append(senti_bpes)\n\n                        if i <(value-1):\n                            aspect_prompt_bpes.append([self.sep_token_id])\n                    elif task == 'twitter_ae':\n                        aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n\n                        if i <(value-1):\n                            aspect_prompt_bpes.append([self.sep_token_id])\n                    else:\n                        print('Not is right task, please check code!!!')\n\n\n                aspect_prompt_bpes.append([self.end_prompt_id])\n                in_bpes =  self._base_tokenizer.tokenize('in',\n                                                        add_prefix_space=True)\n                in_bpes = self._base_tokenizer.convert_tokens_to_ids(in_bpes)\n                aspect_prompt_bpes.append(in_bpes)\n                _aspect_prompt_bpes = list(chain(*aspect_prompt_bpes))\n                aspect_prompts_tokens.append(_aspect_prompt_bpes.copy())\n        if image_text is not None:\n            image_sentence = self.encode(image_text,\n                                         add_special_tokens=False,\n                                         return_tensors='pt',\n                                         padding=True)\n            image_ids = image_sentence['input_ids']\n            image_attention_mask = image_sentence['attention_mask']\n            \n            image_caption_tokens, image_caption_mask = self.pad_tokens_with_maxlength(\n                image_caption_tokens, max_len=20)\n\n\n            input_sentence_tokens, input_sentence_mask = self.pad_tokens(\n                input_sentence_tokens)\n            \n            aspect_prompts_tokens, aspect_prompts_mask = self.pad_tokens_with_maxlength(aspect_prompts_tokens, max_len=40)\n\n\n            if text_only:\n                image_attention_mask = torch.zeros(image_ids.size())\n                image_caption_mask = torch.zeros(image_caption_tokens.size())\n\n\n            input_ids = torch.cat((image_ids, image_caption_tokens, aspect_prompts_tokens, input_sentence_tokens), 1)\n            attention_mask = torch.cat(\n                (image_attention_mask, image_caption_mask, aspect_prompts_mask, input_sentence_mask), 1)\n        else:\n            input_sentence_tokens, input_sentence_mask = self.pad_tokens(\n                input_sentence_tokens)\n            input_ids = input_sentence_tokens\n            attention_mask = input_sentence_mask\n        encoded = {}\n        encoded['input_ids'] = input_ids\n        encoded['attention_mask'] = attention_mask\n        # build mlm mask\n        if sentence is not None:\n            sentence_mask = torch.zeros(input_ids.size(), dtype=torch.bool)\n            for index, value in enumerate(input_ids):\n                start = (value == self.bos_token_id).nonzero(as_tuple=True)[0]\n                end = (value == self.eos_token_id).nonzero(as_tuple=True)[0]\n                sentence_mask[index, start + 1:end] = True\n            encoded['sentence_mask'] = sentence_mask\n\n        # build img mask\n        if img_num is not None:\n            encoded['img_mask'] = encoded['input_ids'] == self.img_feat_id\n        \n        return encoded\n\n    def encode_label(self, label, img_num=None):  #generate labels for MLM task\n\n        # build text label\n        if not isinstance(label, list):\n            label = [label]\n\n        label_split = [x.split() for x in label]\n        label_tokens = []\n        for split in label_split:\n            word_bpes = [[self.bos_token_id], [self.mlm_token_id]]\n            for word in split:\n                bpes = self._base_tokenizer.tokenize(word,\n                                                     add_prefix_space=True)\n                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                word_bpes.append(bpes)\n            word_bpes.append([self.eos_token_id])\n            _word_bpes = list(chain(*word_bpes))\n            label_tokens.append(_word_bpes)\n        input_ids, attention_mask = self.pad_tokens(label_tokens)\n\n        output_shape = input_ids[:, 2:].shape\n        labels = torch.empty(output_shape, dtype=torch.long)\n        decoder_input_ids = torch.empty(input_ids[:, 1:].shape,\n                                        dtype=torch.long)\n        decoder_attention_mask = torch.empty(input_ids[:, 1:].shape,\n                                             dtype=torch.long)\n\n        for i in range(labels.size(0)):\n            labels[i] = input_ids[i][(input_ids[i] != self.bos_token_id)\n                                     & (input_ids[i] != self.mlm_token_id)]\n            decoder_input_ids[i] = input_ids[i][\n                input_ids[i] != self.eos_token_id]\n            decoder_attention_mask[i] = attention_mask[i][\n                input_ids[i] != self.eos_token_id]\n        labels[(labels == self.pad_token_id) | (labels == self.begin_img_id) |\n               (labels == self.end_img_id) | (labels == self.mlm_token_id) |\n               (labels == self.img_feat_id)] = -100\n        output = {\n            'mlm_labels': labels,\n            'mlm_decoder_input_ids': decoder_input_ids,\n            'mlm_decoder_attention_mask': decoder_attention_mask\n        }\n\n        return output\n    \n\n\n    def encode_senti(self, sentis):  #generate label for MSP task\n        senti_input_text = [\n            self.bos_token + self.senti_token for i in range(len(sentis))\n        ]\n        senti_input_text = self.encode(senti_input_text,\n                                       add_special_tokens=False,\n                                       return_tensors='pt',\n                                       padding=True)\n        senti_decoder_input_ids = senti_input_text['input_ids']\n        senti_decoder_attention_mask = senti_input_text['attention_mask']\n\n        sentiment = []\n        for senti in sentis:\n            sentiment.append(senti)\n            # else:\n            #     raise ValueError('sentiment label error!!')\n        output = {\n            'senti_labels': torch.from_numpy(np.array(sentiment)),\n            'senti_decoder_input_ids': senti_decoder_input_ids,\n            'senti_decoder_attention_mask': senti_decoder_attention_mask\n        }\n        return output\n\n    def encode_anp_dis(self, batch_size):\n        ANP_input_text = [\n            self.bos_token + self.ANP_token for i in range(batch_size)\n        ]\n        ANP_input_text = self.encode(ANP_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        output = {}\n        output['ANP_decoder_input_ids'] = ANP_input_text['input_ids']\n        output['ANP_decoder_attention_mask'] = ANP_input_text['attention_mask']\n\n        return output\n\n    def encode_anp_generate(self, ANP_words):  #generate label for AOG task\n        label_split = [x.split() for x in ANP_words]\n        label_tokens = []\n        for split in label_split:\n            word_bpes = [[self.bos_token_id], [self.ANP_generate_token_id]]\n            for word in split:\n                bpes = self._base_tokenizer.tokenize(word,\n                                                     add_prefix_space=True)\n                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                word_bpes.append(bpes)\n            word_bpes.append([self.eos_token_id])\n            _word_bpes = list(chain(*word_bpes))\n            label_tokens.append(_word_bpes)\n        input_ids, attention_mask = self.pad_tokens(label_tokens)\n\n        output_shape = input_ids[:, 2:].shape\n        labels = torch.empty(output_shape, dtype=torch.long)\n        decoder_input_ids = torch.empty(input_ids[:, 1:].shape,\n                                        dtype=torch.long)\n        decoder_attention_mask = torch.empty(input_ids[:, 1:].shape,\n                                             dtype=torch.long)\n\n        for i in range(labels.size(0)):\n            labels[i] = input_ids[i][\n                (input_ids[i] != self.bos_token_id)\n                & (input_ids[i] != self.ANP_generate_token_id)]\n            decoder_input_ids[i] = input_ids[i][\n                input_ids[i] != self.eos_token_id]\n            decoder_attention_mask[i] = attention_mask[i][\n                input_ids[i] != self.eos_token_id]\n\n        labels[(labels == self.pad_token_id) | (labels == self.begin_img_id) |\n               (labels == self.end_img_id) |\n               (labels == self.ANP_generate_token_id) |\n               (labels == self.img_feat_id)] = -100\n\n        output = {\n            'anp_generate_labels': labels,\n            'anp_generate_decoder_input_ids': decoder_input_ids,\n            'anp_generate_decoder_attention_mask': decoder_attention_mask\n        }\n        return output\n\n    def encode_aesc(self, label, aesc_spans, aesc_max_len):\n        # import ipdb; ipdb.set_trace()\n        target_shift = len(self.mapping2targetid) + 2\n        aesc_text = []\n        masks = []\n        gt_spans = []\n\n        flag = True\n        for text, span in zip(label, aesc_spans):\n            span = sorted(span, key=cmp_to_key(cmp))\n            word_bpes = [[self.begin_text_id]]\n            for word in text.split():\n                bpes = self._base_tokenizer.tokenize(word,\n                                                     add_prefix_space=True)\n                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                word_bpes.append(bpes)\n            word_bpes.append([self.end_text_id])\n\n            lens = list(map(len, word_bpes))\n            cum_lens = np.cumsum(list(lens)).tolist()\n            # print(\"self.mapping2targetid is {}\".format(self.mapping2targetid))\n            '''\n            self.mapping2targetid: {'AESC': 2, 'POS': 3, 'NEU': 4, 'NEG': 5} \n            '''\n            cur_text = [\n                0, self.mapping2targetid['AESC'], self.mapping2targetid['AESC']\n            ]\n            # print(\"====================cur_text is {}=============================\".format(cur_text))\n            mask = [0, 0, 0]\n            gt = []\n            for x in span:\n                s_bpe = cum_lens[x[0]] + target_shift\n                e_bpe = cum_lens[x[1] - 1] + target_shift\n                polarity = self.mapping2targetid[x[2]]\n                cur_text.append(s_bpe)\n                cur_text.append(e_bpe)\n                cur_text.append(polarity)\n                gt.append((s_bpe, e_bpe, polarity))\n                mask.append(1)\n                mask.append(1)\n                mask.append(1)\n            cur_text.append(1)\n            mask.append(1)\n\n            aesc_text.append(cur_text)\n            gt_spans.append(gt)\n            masks.append(mask)\n        span_max_len = max([len(x) for x in aesc_text])\n        for i in range(len(masks)):\n            add_len = span_max_len - len(masks[i])\n            masks[i] = masks[i] + [0 for ss in range(add_len)]\n            aesc_text[i] = aesc_text[i] + [1 for ss in range(add_len)]\n        \n       \n        output = {}\n        output['labels'] = torch.tensor(aesc_text)\n        output['masks'] = torch.tensor(masks)\n        output['spans'] = gt_spans\n\n        aspect_prompt_input_text = [\n            self.bos_token + self.aspect_prompt_token + self.aspect_prompt_token for i in range(len(label))\n        ]\n        aspect_prompt_input_text = self.encode(aspect_prompt_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        output['aspect_prompt_decoder_input_ids'] = aspect_prompt_input_text['input_ids']\n        output['aspect_prompt_decoder_attention_mask'] = aspect_prompt_input_text['attention_mask']\n        \n        senti_prompt_input_text = [\n            self.bos_token + self.senti_prompt_token for i in range(len(label))\n        ]\n        senti_prompt_input_text = self.encode(senti_prompt_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        output['senti_prompt_decoder_input_ids'] = senti_prompt_input_text['input_ids']\n        output['senti_prompt_decoder_attention_mask'] = senti_prompt_input_text['attention_mask']\n        \n\n        # print(\"---------------------output is {}------------------\".format(output))\n        # print('++++++++++++++++++++++output[labels] is {}+++++++++++++++++++++++++++++'.format(output['labels']))\n        return output\n\n    def encode_ae_oe(self, label, aspect_spans,\n                     opinion_spans):  #generate labels of AOE task\n        target_shift = len(self.mapping2targetid) + 2\n        ae_oe_text = []\n        masks = []\n        gt_spans = []\n\n        for text, ae_span, oe_span in zip(label, aspect_spans, opinion_spans):\n            word_bpes = [[self.begin_text_id]]\n            for word in text.split():\n                bpes = self._base_tokenizer.tokenize(word,\n                                                     add_prefix_space=True)\n                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                word_bpes.append(bpes)\n            word_bpes.append([self.end_text_id])\n\n            lens = list(map(len, word_bpes))\n            cum_lens = np.cumsum(list(lens)).tolist()\n            cur_text = [\n                0, self.mapping2targetid['AE_OE'],\n                self.mapping2targetid['AE_OE']\n            ]\n            mask = [0, 0, 0]\n\n            gt = []\n            for x in ae_span:\n                # print(x[0])\n                s_bpe = cum_lens[x[0]] + target_shift\n                e_bpe = cum_lens[x[1]] + target_shift\n                cur_text.append(s_bpe)\n                cur_text.append(e_bpe)\n                gt.append((s_bpe, e_bpe))\n                mask.append(1)\n                mask.append(1)\n            cur_text.append(self.mapping2targetid['SEP'])\n            mask.append(1)\n            for x in oe_span:\n                # print(x[0])\n                s_bpe = cum_lens[x[0]] + target_shift\n                e_bpe = cum_lens[x[1]] + target_shift\n                cur_text.append(s_bpe)\n                cur_text.append(e_bpe)\n                gt.append((s_bpe, e_bpe))\n                mask.append(1)\n                mask.append(1)\n            cur_text.append(1)\n            mask.append(1)\n\n            ae_oe_text.append(cur_text)\n            masks.append(mask)\n            gt_spans.append(gt)\n        span_max_len = max(len(x) for x in ae_oe_text)\n        for i in range(len(masks)):\n            add_len = span_max_len - len(masks[i])\n            masks[i] = masks[i] + [0 for ss in range(add_len)]\n            ae_oe_text[i] = ae_oe_text[i] + [1 for ss in range(add_len)]\n        output = {}\n        output['labels'] = torch.tensor(ae_oe_text)\n        output['masks'] = torch.tensor(masks)\n        output['spans'] = gt_spans\n        return output\n\n    def encode_mrm(self, box_cls):\n        mrm_input_text = [\n            self.bos_token + self.mrm_token + self.img_feat * 36\n            for i in range(len(box_cls))\n        ]\n        mrm_input_text = self.encode(mrm_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        mrm_decoder_input_ids = mrm_input_text['input_ids']\n        mrm_decoder_attention_mask = mrm_input_text['attention_mask']\n\n        output = {\n            'mrm_labels': torch.from_numpy(np.array(box_cls)),\n            'mrm_decoder_input_ids': mrm_decoder_input_ids,\n            'mrm_decoder_attention_mask': mrm_decoder_attention_mask\n        }\n        return output\n\n    def encode_twitter_ae(self, label, aspect_spans, ae_max_len):\n        target_shift = len(self.mapping2targetid) + 2\n        ae_text = []\n        masks = []\n        gt_spans = []\n        for text, span in zip(label, aspect_spans):\n            word_bpes = [[self.begin_text_id]]\n            for word in text.split():\n                bpes = self._base_tokenizer.tokenize(word,\n                                                     add_prefix_space=True)\n                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                word_bpes.append(bpes)\n            word_bpes.append([self.end_text_id])\n\n            lens = list(map(len, word_bpes))\n            cum_lens = np.cumsum(list(lens)).tolist()\n            # self.all_cum_lens.append(cum_lens)\n            # print(len(cum_lens), len(split))\n            cur_text = [\n                0, self.mapping2targetid['AE'], self.mapping2targetid['AE']\n            ]\n            mask = [0, 0, 0]\n            # print(text)\n            # print(len(cum_lens), len(text.split()))\n            gt = []\n            for x in span:\n\n                s_bpe = cum_lens[x[0]] + target_shift\n                e_bpe = cum_lens[x[1] - 1] + target_shift\n                cur_text.append(s_bpe)\n                cur_text.append(e_bpe)\n                gt.append((s_bpe, e_bpe))\n                mask.append(1)\n                mask.append(1)\n            cur_text.append(1)\n            mask.append(1)\n            # cur_text = cur_text + [\n            #     1 for i in range(ae_max_len - len(cur_text))\n            # ]\n            # mask = mask + [0 for i in range(ae_max_len - len(mask))]\n            # print(cur_text)\n            ae_text.append(cur_text)\n            masks.append(mask)\n            gt_spans.append(gt)\n        span_max_len = max(len(x) for x in ae_text)\n        for i in range(len(masks)):\n            add_len = span_max_len - len(masks[i])\n            masks[i] = masks[i] + [0 for ss in range(add_len)]\n            ae_text[i] = ae_text[i] + [1 for ss in range(add_len)]\n        output = {}\n        output['labels'] = torch.tensor(ae_text)\n        output['masks'] = torch.tensor(masks)\n        output['spans'] = gt_spans\n        # output['AE_masks'][:, 2] = 1\n\n        aspect_prompt_input_text = [\n            self.bos_token + self.aspect_prompt_token + self.aspect_prompt_token for i in range(len(label))\n        ]\n        aspect_prompt_input_text = self.encode(aspect_prompt_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        output['aspect_prompt_decoder_input_ids'] = aspect_prompt_input_text['input_ids']\n        output['aspect_prompt_decoder_attention_mask'] = aspect_prompt_input_text['attention_mask']\n        \n        return output\n\n    def encode_twitter_sc(self, label, aesc_spans, aesc_max_len):\n        target_shift = len(self.mapping2targetid) + 2\n        aesc_text = []\n        masks = []\n        gt_spans = []\n        # print(len(opinion_spans))\n        # print(len(self.all_cum_lens), len(opinion_spans))\n\n        flag = True\n        for text, span in zip(label, aesc_spans):\n            span = sorted(span, key=cmp_to_key(cmp))\n            word_bpes = [[self.begin_text_id]]\n            for word in text.split():\n                bpes = self._base_tokenizer.tokenize(word,\n                                                     add_prefix_space=True)\n                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                word_bpes.append(bpes)\n            word_bpes.append([self.end_text_id])\n\n            lens = list(map(len, word_bpes))\n            cum_lens = np.cumsum(list(lens)).tolist()\n\n            # if flag:\n            #     # print(word_bpes)\n            #     print(cum_lens)\n            #     flag = False\n            cur_text = [\n                0, self.mapping2targetid['SC'], self.mapping2targetid['SC']\n            ]\n            mask = [0, 0, 0]\n            # print(text)\n            # print(len(cum_lens), len(text.split()))\n            gt = []\n            for x in span:\n\n                s_bpe = cum_lens[x[0]] + target_shift\n                e_bpe = cum_lens[x[1] - 1] + target_shift\n                # if s_bpe >= cum_lens[-1] or e_bpe >= cum_lens[-1]:\n                #     break\n                polarity = self.mapping2targetid[x[2]]\n                cur_text.append(s_bpe)\n                cur_text.append(e_bpe)\n                cur_text.append(polarity)\n                gt.append((s_bpe, e_bpe, polarity))\n                mask.append(1)\n                mask.append(1)\n                mask.append(1)\n            cur_text.append(1)\n            mask.append(0)\n            # cur_text = cur_text + [\n            #     1 for i in range(aesc_max_len - len(cur_text))\n            # ]\n            # mask = mask + [0 for i in range(aesc_max_len - len(mask))]\n            # print(cur_text)\n            aesc_text.append(cur_text)\n            gt_spans.append(gt)\n            masks.append(mask)\n        span_max_len = max([len(x) for x in aesc_text])\n        for i in range(len(masks)):\n            add_len = span_max_len - len(masks[i])\n            masks[i] = masks[i] + [0 for ss in range(add_len)]\n            aesc_text[i] = aesc_text[i] + [1 for ss in range(add_len)]\n            # masks[i].extend([0 for ss in range(add_len)])\n            # aesc_text[i].extend([1 for ss in range(add_len)])\n\n        output = {}\n        # print(oe_text[0], len(oe_text))\n        # for xx in oe_text:\n        #     if xx == None:\n        #         print('opinion shit!!!!!!!!!!!!!!!')\n        # print(aesc_text[0])\n        # print(masks[0])\n        output['labels'] = torch.tensor(aesc_text)\n        output['masks'] = torch.tensor(masks)\n        output['spans'] = gt_spans\n\n        senti_prompt_input_text = [\n            self.bos_token + self.senti_prompt_token for i in range(len(label))\n        ]\n        senti_prompt_input_text = self.encode(senti_prompt_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        output['senti_prompt_decoder_input_ids'] = senti_prompt_input_text['input_ids']\n        output['senti_prompt_decoder_attention_mask'] = senti_prompt_input_text['attention_mask']\n\n        return output\n\n    def decode(self, token_ids, skip_special_tokens=False):\n        return self._base_tokenizer.decode(\n            token_ids, skip_special_tokens=skip_special_tokens)\n\n    def convert_tokens_to_ids(self, tokens):\n        return self._base_tokenizer.convert_tokens_to_ids(tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return self._base_tokenizer.convert_ids_to_tokens(ids)\n\n    def get_base_tokenizer(self):\n        return self._base_tokenizer\n\n    def __len__(self):\n        return len(self._base_tokenizer)"]}
{"filename": "src/data/collation_for_prompt.py", "chunked_list": ["import warnings\n\nimport numpy as np\nimport torch\nfrom itertools import chain\n# from src.utils import TaskType\n\n\nclass Collator:\n    \"\"\"\n    The collator for all types of dataset.\n    Remember to add the corresponding collation code after adding a new type of task.\n    \"\"\"\n    def __init__(self,\n                 task,\n                 tokenizer,\n                 is_mlm=False,\n                 has_label=True,\n                 mlm_enabled=False,\n                 mrm_enabled=False,\n                 senti_enabled=False,\n                 ae_enabled=False,\n                 oe_enabled=False,\n                 ae_oe_enabled=False,\n                 aesc_enabled=False,\n                 anp_enabled=False,\n                 anp_generate_enabled=False,\n                 twitter_ae_enabled=False,\n                 twitter_sc_enabled=False,\n                 has_prompt=False,\n                 text_only=False,\n                 use_caption=False,\n                 mlm_probability=0.0,\n                 mrm_probability=0.0,\n                 lm_max_len=30,\n                 max_img_num=2,\n                 max_span_len=20):\n        \"\"\"\n        :param tokenizer: ConditionTokenizer\n        :param mlm_enabled: bool, if use mlm for language modeling. False for autoregressive modeling\n        :param mrm_enabled: bool, if use mrm\n        :param rp_enabled: bool, if use relation prediction (VG)\n        :param ap_enabled: bool, if use attribute prediction (VG)\n        :param mlm_probability: float, probability to mask the tokens\n        :param mrm_probability: float, probability to mask the regions\n        \"\"\"\n        self.task = task\n        self._tokenizer = tokenizer\n        self._has_label = has_label\n        self._is_mlm = is_mlm\n        self._mrm_enabled = mrm_enabled\n        self._mlm_enabled = mlm_enabled\n        self._senti_enabled = senti_enabled\n        self._anp_enabled = anp_enabled\n        self._anp_generate_enabled = anp_generate_enabled\n        self._ae_enabled = ae_enabled\n        self._oe_enabled = oe_enabled\n        self._ae_oe_enabled = ae_oe_enabled\n        self._aesc_enabled = aesc_enabled\n        self._twitter_ae_enabled = twitter_ae_enabled\n        self._twitter_sc_enabled = twitter_sc_enabled\n        self._lm_max_len = lm_max_len\n        self._max_img_num = max_img_num\n        self._mlm_probability = mlm_probability\n        self._mrm_probability = mrm_probability\n        self._max_span_len = max_span_len\n        self.text_only = text_only\n        self.use_caption=use_caption\n        self.has_prompt=has_prompt\n        if mlm_enabled and not has_label:\n            raise ValueError(\n                'mlm_enabled can not be true while has_label is false. MLM need labels.'\n            )\n\n    def _clip_text(self, text, length):\n        tokenized = []\n        for i, word in enumerate(text.split()):\n            if i == 0:\n                bpes = self._tokenizer._base_tokenizer.tokenize(word)\n            else:\n                bpes = self._tokenizer._base_tokenizer.tokenize(\n                    word, add_prefix_space=True)\n            bpes = self._tokenizer._base_tokenizer.convert_tokens_to_ids(bpes)\n            tokenized.append(bpes)\n        _tokenized = list(chain(*tokenized))\n        return self._tokenizer.get_base_tokenizer().decode(_tokenized[:length])\n\n    def __call__(self, batch):\n        batch = [entry for entry in batch if entry is not None]\n\n        # image_features = [\n        #     torch.from_numpy(x['img_feat'][:self._max_img_num])\n        #     if 'img_feat' in x else torch.empty(0) for x in batch\n        # ]\n        # import ipdb; ipdb.set_trace()\n\n        image_features = [\n                        x['image_pixel_values']\n                        if 'image_pixel_values' in x else torch.empty(0) for x in batch\n        ]\n\n        image_caption = [x['caption'] for x in batch]\n\n        aspects_num = [x['aspects_num'] for x in batch]\n\n        img_num = [self._max_img_num for x in image_features]\n\n\n        target = [x['sentence'] for x in batch]\n    \n        sentence = list(target)\n\n        encoded_conditions = self._tokenizer.encode_condition(\n            task=self.task,\n            img_num=img_num, use_caption=self.use_caption,\n            caption=image_caption, sentence=sentence, has_prompt=self.has_prompt, aspects_num=aspects_num, text_only=self.text_only)\n\n        input_ids = encoded_conditions['input_ids']\n        output = {}\n        if self._is_mlm:\n            input_ids = self._mask_tokens(\n                inputs=input_ids,\n                input_mask=encoded_conditions['sentence_mask'])\n        condition_img_mask = encoded_conditions['img_mask']\n\n        if self._mrm_enabled:\n            encode_mrm = self._tokenizer.encode_mrm([x['cls'] for x in batch])\n            mrm_labels_all = encode_mrm['mrm_labels']\n            probability_matrix = torch.full(input_ids.shape,\n                                            self._mrm_probability,\n                                            dtype=torch.float)\n            masked_regions = torch.bernoulli(probability_matrix).bool()\n            input_ids[masked_regions\n                      & condition_img_mask] = self._tokenizer.cls_token_id\n            decoder_input_ids = encode_mrm['mrm_decoder_input_ids']\n            for i in range(input_ids.size(0)):\n                for j in range(36):\n                    if input_ids[i, j + 1] == self._tokenizer.cls_token_id:\n                        decoder_input_ids[i, j +\n                                          2] = self._tokenizer.cls_token_id\n            mrm_labels = []\n            for i in range(len(batch)):\n                # create mrm_labels\n                masked_indices = masked_regions[i][\n                    condition_img_mask[i]].nonzero(as_tuple=False)\n                mrm_label = mrm_labels_all[i]\n                mrm_labels.append(mrm_label[masked_indices].clone())\n\n                if len(image_features[i]) > 0:\n                    image_features[i][masked_indices] = torch.zeros(\n                        (len(masked_indices), 1, 2048),\n                        dtype=image_features[i].dtype)\n            MRM = {}\n            MRM['mrm_labels'] = mrm_labels\n            MRM['mrm_decoder_input_ids'] = decoder_input_ids\n            MRM['mrm_masks'] = decoder_input_ids == self._tokenizer.cls_token_id\n            MRM['mrm_decoder_attention_mask'] = encode_mrm[\n                'mrm_decoder_attention_mask']\n            output['MRM'] = MRM\n            output['task'] = 'MRM'\n        output['input_ids'] = input_ids\n        output['attention_mask'] = encoded_conditions['attention_mask']\n        output['image_features'] = image_features\n        output['input_ids'] = input_ids\n        output['aspects_num']=aspects_num\n        if self._has_label:\n            # encode mrm and mlm labels\n            if self._mlm_enabled:\n                mlm_output = self._tokenizer.encode_label(label=target,\n                                                          img_num=img_num)\n                output['MLM'] = mlm_output\n                output['task'] = 'MLM'\n\n            if self._senti_enabled:\n                output['Sentiment'] = self._tokenizer.encode_senti(\n                    [x['sentiment'] for x in batch])\n                output['task'] = 'Sentiment'\n\n            if self._anp_generate_enabled:\n                output['ANP_generate'] = self._tokenizer.encode_anp_generate(\n                    [x['ANP_words'] for x in batch])\n                output['task'] = 'ANP_generate'\n            if self._aesc_enabled:\n                output['AESC'] = self._tokenizer.encode_aesc(\n                    target, [x['aesc_spans'] for x in batch],\n                    self._max_span_len)\n                output['task'] = 'AESC'\n            if self._ae_oe_enabled:\n                output['AE_OE'] = self._tokenizer.encode_ae_oe(\n                    target, [x['aspect_spans'] for x in batch],\n                    [x['opinion_spans'] for x in batch])\n                output['task'] = 'AE_OE'\n            if self._twitter_ae_enabled:\n                output['TWITTER_AE'] = self._tokenizer.encode_twitter_ae(\n                    target, [x['aesc_spans'] for x in batch],\n                    self._max_span_len)\n            if self._twitter_sc_enabled:\n                output['TWITTER_SC'] = self._tokenizer.encode_twitter_sc(\n                    target, [x['aesc_spans'] for x in batch],\n                    self._max_span_len)\n\n        output['image_id'] = [x['image_id'] for x in batch]\n        output['gt'] = [x['gt'] for x in batch]\n        return output\n\n    def _mask_tokens(self, inputs, input_mask):\n        \"\"\"\n        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n\n        :param inputs: torch.LongTensor, batch data\n        :param input_mask: torch.Tensor, mask for the batch, False for the position with 0% probability to be masked\n        \"\"\"\n\n        labels = inputs.clone()\n        tokenizer = self._tokenizer.get_base_tokenizer()\n\n        # We sample a few tokens in each sequence for masked-LM training\n        probability_matrix = torch.full(labels.shape,\n                                        self._mlm_probability,\n                                        dtype=torch.float)\n        special_tokens_mask = [\n            tokenizer.get_special_tokens_mask(val,\n                                              already_has_special_tokens=True)\n            for val in labels.tolist()\n        ]\n        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask,\n                                                     dtype=torch.bool),\n                                        value=0.0)\n        if tokenizer.pad_token is not None:\n            padding_mask = labels.eq(tokenizer.pad_token_id)\n            probability_matrix.masked_fill_(padding_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(\n            labels.shape, 0.8)).bool() & masked_indices\n        inputs[indices_replaced & input_mask] = tokenizer.mask_token_id\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(\n            labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(tokenizer.vocab_size,\n                                     labels.shape,\n                                     dtype=torch.long)\n        inputs[indices_random & input_mask] = random_words[indices_random\n                                                           & input_mask]\n\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs", "class Collator:\n    \"\"\"\n    The collator for all types of dataset.\n    Remember to add the corresponding collation code after adding a new type of task.\n    \"\"\"\n    def __init__(self,\n                 task,\n                 tokenizer,\n                 is_mlm=False,\n                 has_label=True,\n                 mlm_enabled=False,\n                 mrm_enabled=False,\n                 senti_enabled=False,\n                 ae_enabled=False,\n                 oe_enabled=False,\n                 ae_oe_enabled=False,\n                 aesc_enabled=False,\n                 anp_enabled=False,\n                 anp_generate_enabled=False,\n                 twitter_ae_enabled=False,\n                 twitter_sc_enabled=False,\n                 has_prompt=False,\n                 text_only=False,\n                 use_caption=False,\n                 mlm_probability=0.0,\n                 mrm_probability=0.0,\n                 lm_max_len=30,\n                 max_img_num=2,\n                 max_span_len=20):\n        \"\"\"\n        :param tokenizer: ConditionTokenizer\n        :param mlm_enabled: bool, if use mlm for language modeling. False for autoregressive modeling\n        :param mrm_enabled: bool, if use mrm\n        :param rp_enabled: bool, if use relation prediction (VG)\n        :param ap_enabled: bool, if use attribute prediction (VG)\n        :param mlm_probability: float, probability to mask the tokens\n        :param mrm_probability: float, probability to mask the regions\n        \"\"\"\n        self.task = task\n        self._tokenizer = tokenizer\n        self._has_label = has_label\n        self._is_mlm = is_mlm\n        self._mrm_enabled = mrm_enabled\n        self._mlm_enabled = mlm_enabled\n        self._senti_enabled = senti_enabled\n        self._anp_enabled = anp_enabled\n        self._anp_generate_enabled = anp_generate_enabled\n        self._ae_enabled = ae_enabled\n        self._oe_enabled = oe_enabled\n        self._ae_oe_enabled = ae_oe_enabled\n        self._aesc_enabled = aesc_enabled\n        self._twitter_ae_enabled = twitter_ae_enabled\n        self._twitter_sc_enabled = twitter_sc_enabled\n        self._lm_max_len = lm_max_len\n        self._max_img_num = max_img_num\n        self._mlm_probability = mlm_probability\n        self._mrm_probability = mrm_probability\n        self._max_span_len = max_span_len\n        self.text_only = text_only\n        self.use_caption=use_caption\n        self.has_prompt=has_prompt\n        if mlm_enabled and not has_label:\n            raise ValueError(\n                'mlm_enabled can not be true while has_label is false. MLM need labels.'\n            )\n\n    def _clip_text(self, text, length):\n        tokenized = []\n        for i, word in enumerate(text.split()):\n            if i == 0:\n                bpes = self._tokenizer._base_tokenizer.tokenize(word)\n            else:\n                bpes = self._tokenizer._base_tokenizer.tokenize(\n                    word, add_prefix_space=True)\n            bpes = self._tokenizer._base_tokenizer.convert_tokens_to_ids(bpes)\n            tokenized.append(bpes)\n        _tokenized = list(chain(*tokenized))\n        return self._tokenizer.get_base_tokenizer().decode(_tokenized[:length])\n\n    def __call__(self, batch):\n        batch = [entry for entry in batch if entry is not None]\n\n        # image_features = [\n        #     torch.from_numpy(x['img_feat'][:self._max_img_num])\n        #     if 'img_feat' in x else torch.empty(0) for x in batch\n        # ]\n        # import ipdb; ipdb.set_trace()\n\n        image_features = [\n                        x['image_pixel_values']\n                        if 'image_pixel_values' in x else torch.empty(0) for x in batch\n        ]\n\n        image_caption = [x['caption'] for x in batch]\n\n        aspects_num = [x['aspects_num'] for x in batch]\n\n        img_num = [self._max_img_num for x in image_features]\n\n\n        target = [x['sentence'] for x in batch]\n    \n        sentence = list(target)\n\n        encoded_conditions = self._tokenizer.encode_condition(\n            task=self.task,\n            img_num=img_num, use_caption=self.use_caption,\n            caption=image_caption, sentence=sentence, has_prompt=self.has_prompt, aspects_num=aspects_num, text_only=self.text_only)\n\n        input_ids = encoded_conditions['input_ids']\n        output = {}\n        if self._is_mlm:\n            input_ids = self._mask_tokens(\n                inputs=input_ids,\n                input_mask=encoded_conditions['sentence_mask'])\n        condition_img_mask = encoded_conditions['img_mask']\n\n        if self._mrm_enabled:\n            encode_mrm = self._tokenizer.encode_mrm([x['cls'] for x in batch])\n            mrm_labels_all = encode_mrm['mrm_labels']\n            probability_matrix = torch.full(input_ids.shape,\n                                            self._mrm_probability,\n                                            dtype=torch.float)\n            masked_regions = torch.bernoulli(probability_matrix).bool()\n            input_ids[masked_regions\n                      & condition_img_mask] = self._tokenizer.cls_token_id\n            decoder_input_ids = encode_mrm['mrm_decoder_input_ids']\n            for i in range(input_ids.size(0)):\n                for j in range(36):\n                    if input_ids[i, j + 1] == self._tokenizer.cls_token_id:\n                        decoder_input_ids[i, j +\n                                          2] = self._tokenizer.cls_token_id\n            mrm_labels = []\n            for i in range(len(batch)):\n                # create mrm_labels\n                masked_indices = masked_regions[i][\n                    condition_img_mask[i]].nonzero(as_tuple=False)\n                mrm_label = mrm_labels_all[i]\n                mrm_labels.append(mrm_label[masked_indices].clone())\n\n                if len(image_features[i]) > 0:\n                    image_features[i][masked_indices] = torch.zeros(\n                        (len(masked_indices), 1, 2048),\n                        dtype=image_features[i].dtype)\n            MRM = {}\n            MRM['mrm_labels'] = mrm_labels\n            MRM['mrm_decoder_input_ids'] = decoder_input_ids\n            MRM['mrm_masks'] = decoder_input_ids == self._tokenizer.cls_token_id\n            MRM['mrm_decoder_attention_mask'] = encode_mrm[\n                'mrm_decoder_attention_mask']\n            output['MRM'] = MRM\n            output['task'] = 'MRM'\n        output['input_ids'] = input_ids\n        output['attention_mask'] = encoded_conditions['attention_mask']\n        output['image_features'] = image_features\n        output['input_ids'] = input_ids\n        output['aspects_num']=aspects_num\n        if self._has_label:\n            # encode mrm and mlm labels\n            if self._mlm_enabled:\n                mlm_output = self._tokenizer.encode_label(label=target,\n                                                          img_num=img_num)\n                output['MLM'] = mlm_output\n                output['task'] = 'MLM'\n\n            if self._senti_enabled:\n                output['Sentiment'] = self._tokenizer.encode_senti(\n                    [x['sentiment'] for x in batch])\n                output['task'] = 'Sentiment'\n\n            if self._anp_generate_enabled:\n                output['ANP_generate'] = self._tokenizer.encode_anp_generate(\n                    [x['ANP_words'] for x in batch])\n                output['task'] = 'ANP_generate'\n            if self._aesc_enabled:\n                output['AESC'] = self._tokenizer.encode_aesc(\n                    target, [x['aesc_spans'] for x in batch],\n                    self._max_span_len)\n                output['task'] = 'AESC'\n            if self._ae_oe_enabled:\n                output['AE_OE'] = self._tokenizer.encode_ae_oe(\n                    target, [x['aspect_spans'] for x in batch],\n                    [x['opinion_spans'] for x in batch])\n                output['task'] = 'AE_OE'\n            if self._twitter_ae_enabled:\n                output['TWITTER_AE'] = self._tokenizer.encode_twitter_ae(\n                    target, [x['aesc_spans'] for x in batch],\n                    self._max_span_len)\n            if self._twitter_sc_enabled:\n                output['TWITTER_SC'] = self._tokenizer.encode_twitter_sc(\n                    target, [x['aesc_spans'] for x in batch],\n                    self._max_span_len)\n\n        output['image_id'] = [x['image_id'] for x in batch]\n        output['gt'] = [x['gt'] for x in batch]\n        return output\n\n    def _mask_tokens(self, inputs, input_mask):\n        \"\"\"\n        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n\n        :param inputs: torch.LongTensor, batch data\n        :param input_mask: torch.Tensor, mask for the batch, False for the position with 0% probability to be masked\n        \"\"\"\n\n        labels = inputs.clone()\n        tokenizer = self._tokenizer.get_base_tokenizer()\n\n        # We sample a few tokens in each sequence for masked-LM training\n        probability_matrix = torch.full(labels.shape,\n                                        self._mlm_probability,\n                                        dtype=torch.float)\n        special_tokens_mask = [\n            tokenizer.get_special_tokens_mask(val,\n                                              already_has_special_tokens=True)\n            for val in labels.tolist()\n        ]\n        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask,\n                                                     dtype=torch.bool),\n                                        value=0.0)\n        if tokenizer.pad_token is not None:\n            padding_mask = labels.eq(tokenizer.pad_token_id)\n            probability_matrix.masked_fill_(padding_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(\n            labels.shape, 0.8)).bool() & masked_indices\n        inputs[indices_replaced & input_mask] = tokenizer.mask_token_id\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(\n            labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(tokenizer.vocab_size,\n                                     labels.shape,\n                                     dtype=torch.long)\n        inputs[indices_random & input_mask] = random_words[indices_random\n                                                           & input_mask]\n\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs"]}
{"filename": "src/data/tokenization_new_for_generated_prompt_multitasks.py", "chunked_list": ["import torch\nimport numpy as np\nfrom transformers import BartTokenizer, AutoTokenizer\nfrom itertools import chain\nfrom functools import cmp_to_key\n# from src.utils import TaskType\n\n\ndef cmp(v1, v2):\n    if v1[0] == v2[0]:\n        return v1[1] - v2[1]\n    return v1[0] - v2[0]", "def cmp(v1, v2):\n    if v1[0] == v2[0]:\n        return v1[1] - v2[1]\n    return v1[0] - v2[0]\n\n\nclass ConditionTokenizer:\n    \"\"\"\n    tokenizer for image features, event and task type\n    this is NOT inherent from transformers Tokenizer\n    \"\"\"\n    def __init__(self,\n                 args,\n                 pretrained_model_name='/home/xiaocui/code/FW-MABSA/data/weights/bart-base',\n                 cls_token=\"<<cls>>\",\n                 mlm_token=\"<<mlm>>\",\n                 mrm_token=\"<<mrm>>\",\n                 begin_text=\"<<text>>\",\n                 end_text=\"<</text>>\",\n                 img_feat='<<img_feat>>',\n                 begin_img=\"<<img>>\",\n                 end_img=\"<</img>>\",\n                 img_caption='<<img_caption>>',\n                 begin_caption='<<cap>>',\n                 end_caption='<</cap>>',\n                 ae_token='<<AE>>',\n                 sc_token='<<SC>>',\n                 ae_oe_token=\"<<AOE>>\",\n                 sep_token=\"<<SEP>>\",\n                 aesc_token='<<AESC>>',\n                 pos_token='<<POS>>',\n                 neu_token='<<NEU>>',\n                 neg_token='<<NEG>>',\n                 aspect_prompt_token='<<AE_PROMPT>>',\n                 senti_prompt_token='<<SENTI_PROMPT>>',\n                 begin_prompt='<<prompt>>',\n                 end_prompt='<</prompt>>',\n                 senti_token='<<senti>>',\n                 aspects_num_token='<<aspects_num>>',\n                 ANP_token='<<ANP>>',\n                 ANP_generate_token='<<AOG>>'):\n        self._base_tokenizer = BartTokenizer.from_pretrained(\n            pretrained_model_name, )\n        # self._base_tokenizer = AutoTokenizer.from_pretrained(\n        #     pretrained_model_name)\n\n        self.additional_special_tokens = [\n            cls_token, mlm_token, mrm_token, begin_text, end_text, img_feat, begin_img, end_img, \n            img_caption, begin_caption, end_caption, aspects_num_token,\n            senti_token, ANP_token, ANP_generate_token,\n            pos_token, neu_token, neg_token, ae_oe_token, sep_token,\n            aesc_token, ae_token, sc_token, \n            aspect_prompt_token, senti_prompt_token, begin_prompt, end_prompt\n        ]\n        unique_no_split_tokens = self._base_tokenizer.unique_no_split_tokens\n        self._base_tokenizer.unique_no_split_tokens = unique_no_split_tokens + self.additional_special_tokens\n        self.unique_no_split_tokens = self._base_tokenizer.unique_no_split_tokens\n        print(self.unique_no_split_tokens)\n\n        self._base_tokenizer.add_tokens(self.additional_special_tokens)\n        self.cls_token = cls_token\n        self.mlm_token = mlm_token\n        self.mrm_token = mrm_token\n        self.begin_text = begin_text\n        self.end_text = end_text\n        self.img_feat = img_feat\n        self.begin_img = begin_img\n        self.end_img = end_img\n\n        self.img_caption = img_caption\n        self.begin_caption = begin_caption\n        self.end_caption = end_caption\n\n        self.ae_token = ae_token\n        self.sc_token = sc_token\n        self.ae_oe_token = ae_oe_token\n        self.sep_token = sep_token\n        self.senti_token = senti_token\n        self.ANP_token = ANP_token\n        self.ANP_generate_token = ANP_generate_token\n\n        self.aesc_token = aesc_token\n        self.pos_token = pos_token\n        self.neu_token = neu_token\n        self.neg_token = neg_token\n\n        self.aspect_prompt_token = aspect_prompt_token\n        self.senti_prompt_token = senti_prompt_token\n        self.begin_prompt = begin_prompt\n        self.end_prompt = end_prompt\n        self.aspects_num_token = aspects_num_token\n\n        self.cls_token_id = self.convert_tokens_to_ids(cls_token)\n        self.mlm_token_id = self.convert_tokens_to_ids(mlm_token)\n        self.mrm_token_id = self.convert_tokens_to_ids(mrm_token)\n        self.begin_text_id = self.convert_tokens_to_ids(begin_text)\n        self.end_text_id = self.convert_tokens_to_ids(end_text)\n        self.img_feat_id = self.convert_tokens_to_ids(img_feat)\n        self.begin_img_id = self.convert_tokens_to_ids(begin_img)\n        self.end_img_id = self.convert_tokens_to_ids(end_img)\n\n        self.img_caption_id = self.convert_tokens_to_ids(img_caption)\n        self.begin_caption_id = self.convert_tokens_to_ids(begin_caption)\n        self.end_caption_id = self.convert_tokens_to_ids(end_caption)\n\n        self.ae_token_id = self.convert_tokens_to_ids(ae_token)\n        self.sc_token_id = self.convert_tokens_to_ids(sc_token)\n        self.ae_oe_token_id = self.convert_tokens_to_ids(ae_oe_token)\n        self.sep_token_id = self.convert_tokens_to_ids(sep_token)\n        self.senti_token_id = self.convert_tokens_to_ids(senti_token)\n        self.ANP_token_id = self.convert_tokens_to_ids(ANP_token)\n        self.ANP_generate_token_id = self.convert_tokens_to_ids(\n            ANP_generate_token)\n        self.aesc_token_id = self.convert_tokens_to_ids(aesc_token)\n        self.pos_token_id = self.convert_tokens_to_ids(pos_token)\n        self.neu_token_id = self.convert_tokens_to_ids(neu_token)\n        self.neg_token_id = self.convert_tokens_to_ids(neg_token)\n\n        self.aspect_prompt_token_id = self.convert_tokens_to_ids(aspect_prompt_token)\n        self.senti_prompt_token_id = self.convert_tokens_to_ids(senti_prompt_token)\n        self.begin_prompt_id =  self.convert_tokens_to_ids(begin_prompt)\n        self.end_prompt_id =  self.convert_tokens_to_ids(end_prompt)\n        self.aspects_num_token_id = self.convert_tokens_to_ids(aspects_num_token)\n\n        self.vocab_size = self._base_tokenizer.vocab_size\n        self.bos_token = self._base_tokenizer.bos_token\n        self.bos_token_id = self._base_tokenizer.bos_token_id\n\n        self.eos_token = self._base_tokenizer.eos_token\n        self.eos_token_id = self._base_tokenizer.eos_token_id\n        self.pad_token = self._base_tokenizer.pad_token\n        self.pad_token_id = self._base_tokenizer.pad_token_id\n        self.unk_token = self._base_tokenizer.unk_token\n        self.unk_token_id = self._base_tokenizer.unk_token_id\n\n\n        print('self.bos_token_id', self.bos_token_id)\n        print('self.eos_token_id', self.eos_token_id)\n        print('self.pad_token_id', self.pad_token_id)\n        print('self.begin_caption_token_id', self.begin_caption_id)\n        print('self.end_caption_token_id', self.end_caption_id)\n        print('self.aspect_prompt_token_id', self.aspect_prompt_token_id)\n        print('self.senti_prompt_token_id', self.senti_prompt_token_id)\n        print('self.begin_prompt_id', self.begin_prompt_id)\n        print('self.end_prompt_id', self.end_prompt_id)\n        print('self.aspects_num_token_id', self.aspects_num_token_id)\n\n        if args.task == 'pretrain':\n            self.mapping = {'AE_OE': '<<AOE>>', 'SEP': '<<SEP>>'}\n        else:\n            if args.task == 'twitter_sc':\n                self.mapping = {\n                    'SC': '<<SC>>',\n                    'POS': '<<POS>>',\n                    'NEU': '<<NEU>>',\n                    'NEG': '<<NEG>>'\n                }\n            elif args.task == 'twitter_ae':\n                self.mapping = {\n                    'AE': '<<AE>>',\n                    'POS': '<<POS>>',\n                    'NEU': '<<NEU>>',\n                    'NEG': '<<NEG>>'\n                }\n            else:\n                self.mapping = {\n                    'AESC': '<<AESC>>',\n                    'POS': '<<POS>>',\n                    'NEU': '<<NEU>>',\n                    'NEG': '<<NEG>>'\n                }\n        self.senti = {'POS': '<<POS>>', 'NEU': '<<NEU>>', 'NEG': '<<NEG>>'}\n        self.senti2id = {}\n        for key, value in self.senti.items():\n            key_id = self._base_tokenizer.convert_tokens_to_ids(\n                self._base_tokenizer.tokenize(value))\n            assert len(key_id) == 1, value\n            # assert key_id[0] >= self.cur_num_tokens\n            self.senti2id[key] = key_id[0]\n        self.mapping2id = {}\n        self.mapping2targetid = {}\n        for key, value in self.mapping.items():\n            key_id = self._base_tokenizer.convert_tokens_to_ids(\n                self._base_tokenizer.tokenize(value))\n            assert len(key_id) == 1, value\n            # assert key_id[0] >= self.cur_num_tokens\n            self.mapping2id[key] = key_id[0]\n            self.mapping2targetid[key] = len(self.mapping2targetid) + 2\n        print(self.mapping2id)\n        '''\n        for AESC:\n        {'AESC': 50281, 'POS': 50276, 'NEU': 50277, 'NEG': 50278}\n        '''\n\n    def encode(self, *args, **kwargs):\n        return self._base_tokenizer(*args, **kwargs)\n\n    def pad_tokens(self, tokens):\n        max_len = max([len(x) for x in tokens])\n        pad_result = torch.full((len(tokens), max_len),\n                                self.pad_token_id,\n                                dtype=torch.long)\n        mask = torch.zeros(pad_result.size(), dtype=torch.bool)\n        for i, x in enumerate(tokens):\n            pad_result[i, :len(x)] = torch.tensor(tokens[i], dtype=torch.long)\n            mask[i, :len(x)] = True\n        return pad_result, mask\n    \n    def pad_tokens_with_maxlength(self, tokens, max_len):\n        pad_result = torch.full((len(tokens), max_len),\n                                self.pad_token_id,\n                                dtype=torch.long)\n        mask = torch.zeros(pad_result.size(), dtype=torch.bool)\n     \n        for i, x in enumerate(tokens):\n            # print(x)\n            pad_result[i, :len(x)] = torch.tensor(tokens[i], dtype=torch.long)\n            mask[i, :len(x)] = True\n        # print(\"=====================pad_result=========================\")\n        # print(pad_result)\n        # print(mask)\n        return pad_result, mask\n    \n\n    def encode_mlm_sentence(self, labels):\n        label_split = [x.split() for x in labels]\n        input_tokens = []\n        for split in label_split:\n            cur_num = 0\n            bpes = [self.bos_token_id]\n            for x in split:\n                tokens = self._base_tokenizer(x, add_prefix_space=True)\n                bpes = bpes + tokens\n            bpes.append(self.eos_token_id)\n            input_tokens.append(input_tokens)\n        return input_tokens\n\n    def encode_condition(self, task, img_num=None, use_caption=True, caption=None, sentence=None, has_prompt=False, max_aspects_num=None,  text_only=False):\n        \"\"\"\n        tokenize text, image features and event\n        the output format (after decoded back):\n        task_type [<img> <img_feat> ... <img_feat> </img>] [<event> EVENT </event>] [<mlm> MLM </mlm>]\n\n        :param task_type: str or list[str]\n        :param img_num: int or list[int], the number of image features\n        :param event: str or list[str], event descriptions\n        :param mlm: str or list[str], sentence for masked language modeling\n        :return: dict {str: Tensor}, {\n                \"input_ids\": ...,\n                \"attention_mask\": ...,\n                \"event_mask\": ...,          only exist if event is given. 1 for the position with event tokens\n                \"mlm_mask\": ...,            only exist if mlm is given. 1 for the position with mlm tokens\n                \"img_mask\":...,             only exist if img_num is given. 1 for the position with img tokens\n            }\n        \"\"\"\n        \n        '''\n        [image_features] + is + [image_caption] + [text] + [aspect_prompt_token]*len_1(\u6700\u591a\u4e3a5) + has + [senti_prompt_token]*len_2 'sentiment'\n\n        prompt_1: [image_features] + is + [image_caption]  \n        + 'There is' <prompt> ([aspect_prompt_token]*len_1(\u6700\u591a\u4e3a5) + of [senti_prompt_token]*len_2 + 'sentiment' + <sep>)*n </prompt> + 'in'\n        + [text]\n\n        prompt_2: [image_features] + is + [image_caption]  \n        + <prompt> ([aspect_prompt_token]*len_1(\u6700\u591a\u4e3a5) + of [senti_prompt_token]*len_2)*n </prompt> \n        + [text]\n\n        prompt_3: [image_features] + is + [image_caption]  \n        + 'There is' <prompt> ([aspect_prompt_token]*len_1(\u6700\u591a\u4e3a5) + of <<NEU>> or <<POS>> or <<NEG>> + 'sentiment' + <sep>)*n </prompt> + 'in'\n        + [text]\n\n        '''\n\n        image_text = None\n        if img_num is not None:\n            if not isinstance(img_num, list):\n                img_num = [img_num]\n            image_text = []\n            for index, value in enumerate(img_num):\n                image_text.append(self.begin_img + self.img_feat * value +  ###\u5f15\u5165image_caption token\n                                  self.end_img)\n\n        # import ipdb; ipdb.set_trace()\n        if caption is not None:\n            if not isinstance(caption, list):\n                caption = [caption]\n            caption_split = [x.split() for x in caption]\n            image_caption_tokens = []\n            for split in caption_split:\n                '''\n                print(split)\n                \u4e3a\u65b9\u4fbf\u8d77\u89c1\uff0c\u56fa\u5b9acaption\u6587\u672c\u957f\u5ea6\n                '''\n                # print(\"+++++++++++++++++++++split before ++++++++++++++++++++++++\")\n                # print(len(split))\n                if len(split)>10:\n                    split = split[:10]\n                # print(\"+++++++++++++++++++++split after ++++++++++++++++++++++++\")\n                # print(len(split))\n                # print(split)\n                is_bpes = self._base_tokenizer.tokenize('is',\n                                                         add_prefix_space=True)\n                is_bpes = self._base_tokenizer.convert_tokens_to_ids(is_bpes) ##[16]\n                caption_word_bpes = [is_bpes]\n\n                caption_word_bpes.append([self.begin_caption_id])          \n\n                for caption_word in split:\n                    caption_bpes = self._base_tokenizer.tokenize(caption_word,\n                                                         add_prefix_space=True)\n                    caption_bpes = self._base_tokenizer.convert_tokens_to_ids(caption_bpes)\n                    caption_word_bpes.append(caption_bpes)\n                caption_word_bpes.append([self.end_caption_id])\n\n                _caption_word_bpes = list(chain(*caption_word_bpes))\n                image_caption_tokens.append(_caption_word_bpes.copy())\n\n        if sentence is not None:\n            if not isinstance(sentence, list):\n                sentence = [sentence]\n            sentence_split = [x.split() for x in sentence]\n            input_sentence_tokens = []\n            for split in sentence_split:\n                word_bpes = [[self.bos_token_id]]\n                for word in split:\n                    bpes = self._base_tokenizer.tokenize(word,\n                                                         add_prefix_space=True)\n                    bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                    word_bpes.append(bpes)\n                word_bpes.append([self.eos_token_id])\n\n                _word_bpes = list(chain(*word_bpes))\n                input_sentence_tokens.append(_word_bpes.copy())\n        \n        '''\n        for prompt_1:  prompt_1: [image_features] + is + [image_caption]  \n        + 'There is' <prompt> ([aspect_prompt_token]*len_1(\u6700\u591a\u4e3a5) + of [senti_prompt_token]*len_2 + 'sentiment' + <sep>)*n </prompt> + 'in'\n        + [text] \u6548\u679c\u5f88\u597d \n        in input_ids: input_ids[:][24:64] is the related to aspect prompts, so in generated prompt stage, the attention_mask[:][24:64]=0\n        '''\n        # if has_prompt:\n        #     aspect_prompts_tokens = []     \n        #     for index, value in enumerate(aspects_num):\n        #         # aspect_prompts.append(self.begin_prompt + (self.aspect_prompt_token*2 + 'has' +  self.senti_prompt_token + 'sentiment' + self.sep_token) * value  +\n        #         #                   self.end_prompt)\n        #         aspect_prompt_bpes = [[self.begin_prompt_id]]\n        #         if value == 1:\n        #             _be = 'is'\n        #         else:\n        #             _be = 'are'\n                \n        #         there_bpes = self._base_tokenizer.tokenize('there',\n        #                                                 add_prefix_space=True)\n        #         there_bpes = self._base_tokenizer.convert_tokens_to_ids(there_bpes)\n        #         aspect_prompt_bpes.append(there_bpes)\n\n        #         _be_bpes = self._base_tokenizer.tokenize(_be,\n        #                                                 add_prefix_space=True)\n        #         _be_bpes = self._base_tokenizer.convert_tokens_to_ids(_be_bpes)\n        #         aspect_prompt_bpes.append(_be_bpes)\n\n        #         for i in range(value):\n        #             aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n\n        #             of_bpes = self._base_tokenizer.tokenize('of',\n        #                                                     add_prefix_space=True)\n        #             of_bpes = self._base_tokenizer.convert_tokens_to_ids(of_bpes)\n        #             aspect_prompt_bpes.append(of_bpes)\n                    \n        #             aspect_prompt_bpes.append([self.senti_prompt_token_id])\n\n        #             senti_bpes = self._base_tokenizer.tokenize('sentiment',\n        #                                                     add_prefix_space=True)\n        #             senti_bpes = self._base_tokenizer.convert_tokens_to_ids(senti_bpes)\n        #             aspect_prompt_bpes.append(senti_bpes)\n\n        #             if i <(value-1):\n        #                 aspect_prompt_bpes.append([self.sep_token_id])\n\n\n        #         aspect_prompt_bpes.append([self.end_prompt_id])\n        #         in_bpes =  self._base_tokenizer.tokenize('in',\n        #                                                 add_prefix_space=True)\n        #         in_bpes = self._base_tokenizer.convert_tokens_to_ids(in_bpes)\n        #         aspect_prompt_bpes.append(in_bpes)\n        #         _aspect_prompt_bpes = list(chain(*aspect_prompt_bpes))\n        #         aspect_prompts_tokens.append(_aspect_prompt_bpes.copy())\n\n        '''\n        for prompt_4:  prompt_1: [image_features] + is + [image_caption]  \n        + <prompt> ([aspect_prompt_token]*len_1(\u6700\u591a\u4e3a5) + of [senti_prompt_token]*len_2)*n </prompt> + 'in'\n        + [text] \u6548\u679c\u5f88\u597d \n        in input_ids: input_ids[:][24:64] is the related to aspect prompts, so in generated prompt stage, the attention_mask[:][24:64]=0\n        '''\n        if has_prompt:\n            aspect_prompts_tokens = []     \n            for index, value in enumerate(max_aspects_num):\n                # aspect_prompts.append(self.begin_prompt + (self.aspect_prompt_token*2 + 'has' +  self.senti_prompt_token + 'sentiment' + self.sep_token) * value  +\n                #                   self.end_prompt)\n                aspect_prompt_bpes = [[self.begin_prompt_id]]\n                if value == 1:\n                    _be = 'is'\n                else:\n                    _be = 'are'\n                \n                there_bpes = self._base_tokenizer.tokenize('there',\n                                                        add_prefix_space=True)\n                there_bpes = self._base_tokenizer.convert_tokens_to_ids(there_bpes)\n                aspect_prompt_bpes.append(there_bpes)\n\n                _be_bpes = self._base_tokenizer.tokenize(_be,\n                                                        add_prefix_space=True)\n                _be_bpes = self._base_tokenizer.convert_tokens_to_ids(_be_bpes)\n                aspect_prompt_bpes.append(_be_bpes)\n\n                for i in range(value):\n                    if task == 'AESC':\n                        aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n\n                        of_bpes = self._base_tokenizer.tokenize('of',\n                                                                add_prefix_space=True)\n                        of_bpes = self._base_tokenizer.convert_tokens_to_ids(of_bpes)\n                        aspect_prompt_bpes.append(of_bpes)\n                        \n                        aspect_prompt_bpes.append([self.senti_prompt_token_id])\n\n                        senti_bpes = self._base_tokenizer.tokenize('sentiment',\n                                                                add_prefix_space=True)\n                        senti_bpes = self._base_tokenizer.convert_tokens_to_ids(senti_bpes)\n                        aspect_prompt_bpes.append(senti_bpes)\n\n                        if i <(value-1):\n                            aspect_prompt_bpes.append([self.sep_token_id])\n                    elif task == 'twitter_sc':\n                        aspect_prompt_bpes.append([self.senti_prompt_token_id])\n\n                        senti_bpes = self._base_tokenizer.tokenize('sentiment',\n                                                                add_prefix_space=True)\n                        senti_bpes = self._base_tokenizer.convert_tokens_to_ids(senti_bpes)\n                        aspect_prompt_bpes.append(senti_bpes)\n\n                        if i <(value-1):\n                            aspect_prompt_bpes.append([self.sep_token_id])\n                    elif task == 'twitter_ae':\n                        aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n\n                        if i <(value-1):\n                            aspect_prompt_bpes.append([self.sep_token_id])\n                    else:\n                        print('Not is right task, please check code!!!')\n\n\n                aspect_prompt_bpes.append([self.end_prompt_id])\n                in_bpes =  self._base_tokenizer.tokenize('in',\n                                                        add_prefix_space=True)\n                in_bpes = self._base_tokenizer.convert_tokens_to_ids(in_bpes)\n                aspect_prompt_bpes.append(in_bpes)\n                _aspect_prompt_bpes = list(chain(*aspect_prompt_bpes))\n                aspect_prompts_tokens.append(_aspect_prompt_bpes.copy())\n        # import ipdb; ipdb.set_trace()\n\n        '''\n        for prompt_2\n        prompt_2: [image_features] + is + [image_caption]  \n        + <prompt> ([aspect_prompt_token]*len_1(\u6700\u591a\u4e3a5) + [senti_prompt_token]*len_2)*n </prompt> \n        + [text] \u8bd5\u4e00\u8bd5\n        '''\n\n        # if has_prompt:\n        #     aspect_prompts_tokens = []     \n        #     for index, value in enumerate(aspects_num):\n        #         # aspect_prompts.append(self.begin_prompt + (self.aspect_prompt_token*2 + 'has' +  self.senti_prompt_token + 'sentiment' + self.sep_token) * value  +\n        #         #                   self.end_prompt)\n        #         aspect_prompt_bpes = [[self.begin_prompt_id]]\n\n        #         for i in range(value):\n        #             aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n                    \n        #             aspect_prompt_bpes.append([self.senti_prompt_token_id])\n\n        #             if i <(value-1):\n        #                 aspect_prompt_bpes.append([self.sep_token_id])\n\n        #         aspect_prompt_bpes.append([self.end_prompt_id])\n        #         _aspect_prompt_bpes = list(chain(*aspect_prompt_bpes))\n        #         aspect_prompts_tokens.append(_aspect_prompt_bpes.copy())\n        # import ipdb; ipdb.set_trace()\n        '''\n        prompt_3: [image_features] + is + [image_caption]  \n        + 'There is' <prompt> ([aspect_prompt_token]*len_1(\u6700\u591a\u4e3a5) + of <<NEU>> or <<POS>> or <<NEG>> + 'sentiment' + <sep>)*n </prompt> + 'in'\n        + [text]\n        '''\n        # if has_prompt:\n        #     aspect_prompts_tokens = []     \n        #     for index, value in enumerate(aspects_num):\n        #         # aspect_prompts.append(self.begin_prompt + (self.aspect_prompt_token*2 + 'has' +  self.senti_prompt_token + 'sentiment' + self.sep_token) * value  +\n        #         #                   self.end_prompt)\n        #         aspect_prompt_bpes = [[self.begin_prompt_id]]\n        #         if value == 1:\n        #             _be = 'is'\n        #         else:\n        #             _be = 'are'\n                \n        #         there_bpes = self._base_tokenizer.tokenize('there',\n        #                                                 add_prefix_space=True)\n        #         there_bpes = self._base_tokenizer.convert_tokens_to_ids(there_bpes)\n        #         aspect_prompt_bpes.append(there_bpes)\n\n        #         _be_bpes = self._base_tokenizer.tokenize(_be,\n        #                                                 add_prefix_space=True)\n        #         _be_bpes = self._base_tokenizer.convert_tokens_to_ids(_be_bpes)\n        #         aspect_prompt_bpes.append(_be_bpes)\n\n        #         for i in range(value):\n        #             aspect_prompt_bpes.append([self.aspect_prompt_token_id]*2)\n\n        #             of_bpes = self._base_tokenizer.tokenize('of',\n        #                                                     add_prefix_space=True)\n        #             of_bpes = self._base_tokenizer.convert_tokens_to_ids(of_bpes)\n        #             aspect_prompt_bpes.append(of_bpes)\n                    \n                    \n        #             or_bpes = self._base_tokenizer.tokenize('or',\n        #                                                     add_prefix_space=True)\n        #             or_bpes = self._base_tokenizer.convert_tokens_to_ids(or_bpes)\n                    \n        #             aspect_prompt_bpes.append([self.neu_token_id])\n        #             aspect_prompt_bpes.append(or_bpes)\n        #             aspect_prompt_bpes.append([self.pos_token_id])\n        #             aspect_prompt_bpes.append(or_bpes) \n        #             aspect_prompt_bpes.append([self.neg_token_id])\n\n        #             senti_bpes = self._base_tokenizer.tokenize('sentiment',\n        #                                                     add_prefix_space=True)\n        #             senti_bpes = self._base_tokenizer.convert_tokens_to_ids(senti_bpes)\n        #             aspect_prompt_bpes.append(senti_bpes)\n\n        #             if i <(value-1):\n        #                 aspect_prompt_bpes.append([self.sep_token_id])\n\n\n        #         aspect_prompt_bpes.append([self.end_prompt_id])\n        #         in_bpes =  self._base_tokenizer.tokenize('in',\n        #                                                 add_prefix_space=True)\n        #         in_bpes = self._base_tokenizer.convert_tokens_to_ids(in_bpes)\n        #         aspect_prompt_bpes.append(in_bpes)\n        #         _aspect_prompt_bpes = list(chain(*aspect_prompt_bpes))\n        #         aspect_prompts_tokens.append(_aspect_prompt_bpes.copy())\n\n        if image_text is not None:\n            image_sentence = self.encode(image_text,\n                                         add_special_tokens=False,\n                                         return_tensors='pt',\n                                         padding=True)\n            image_ids = image_sentence['input_ids']\n            image_attention_mask = image_sentence['attention_mask']\n            \n            image_caption_tokens, image_caption_mask = self.pad_tokens_with_maxlength(\n                image_caption_tokens, max_len=20)\n\n            # print(\"========================image_caption_tokens====================================\")\n            # print(image_caption_tokens)\n            # print('the length of image_caption_tokens is {}'.format(image_caption_tokens.shape))\n            # print(image_caption_mask)\n\n            input_sentence_tokens, input_sentence_mask = self.pad_tokens(\n                input_sentence_tokens)\n            \n            aspect_prompts_tokens, aspect_prompts_mask = self.pad_tokens_with_maxlength(aspect_prompts_tokens, max_len=40)\n\n\n            if text_only:\n                image_attention_mask = torch.zeros(image_ids.size())\n                image_caption_mask = torch.zeros(image_caption_tokens.size())\n            if not use_caption:\n                image_caption_mask = torch.zeros(image_caption_tokens.size())\n                image_attention_mask = image_attention_mask\n            \n\n\n            input_ids = torch.cat((image_ids, image_caption_tokens, aspect_prompts_tokens, input_sentence_tokens), 1)\n            attention_mask = torch.cat(\n                (image_attention_mask, image_caption_mask, aspect_prompts_mask, input_sentence_mask), 1)\n        else:\n            input_sentence_tokens, input_sentence_mask = self.pad_tokens(\n                input_sentence_tokens)\n            input_ids = input_sentence_tokens\n            attention_mask = input_sentence_mask\n        # import ipdb; ipdb.set_trace()\n        encoded = {}\n        encoded['input_ids'] = input_ids\n        encoded['attention_mask'] = attention_mask\n        # build mlm mask\n        if sentence is not None:\n            sentence_mask = torch.zeros(input_ids.size(), dtype=torch.bool)\n            for index, value in enumerate(input_ids):\n                start = (value == self.bos_token_id).nonzero(as_tuple=True)[0]\n                end = (value == self.eos_token_id).nonzero(as_tuple=True)[0]\n                sentence_mask[index, start + 1:end] = True\n            encoded['sentence_mask'] = sentence_mask\n\n        # build img mask\n        if img_num is not None:\n            encoded['img_mask'] = encoded['input_ids'] == self.img_feat_id\n        \n        return encoded\n\n    def encode_label(self, label, img_num=None):  #generate labels for MLM task\n\n        # build text label\n        if not isinstance(label, list):\n            label = [label]\n\n        label_split = [x.split() for x in label]\n        label_tokens = []\n        for split in label_split:\n            word_bpes = [[self.bos_token_id], [self.mlm_token_id]]\n            for word in split:\n                bpes = self._base_tokenizer.tokenize(word,\n                                                     add_prefix_space=True)\n                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                word_bpes.append(bpes)\n            word_bpes.append([self.eos_token_id])\n            _word_bpes = list(chain(*word_bpes))\n            label_tokens.append(_word_bpes)\n        input_ids, attention_mask = self.pad_tokens(label_tokens)\n\n        output_shape = input_ids[:, 2:].shape\n        labels = torch.empty(output_shape, dtype=torch.long)\n        decoder_input_ids = torch.empty(input_ids[:, 1:].shape,\n                                        dtype=torch.long)\n        decoder_attention_mask = torch.empty(input_ids[:, 1:].shape,\n                                             dtype=torch.long)\n\n        for i in range(labels.size(0)):\n            labels[i] = input_ids[i][(input_ids[i] != self.bos_token_id)\n                                     & (input_ids[i] != self.mlm_token_id)]\n            decoder_input_ids[i] = input_ids[i][\n                input_ids[i] != self.eos_token_id]\n            decoder_attention_mask[i] = attention_mask[i][\n                input_ids[i] != self.eos_token_id]\n        labels[(labels == self.pad_token_id) | (labels == self.begin_img_id) |\n               (labels == self.end_img_id) | (labels == self.mlm_token_id) |\n               (labels == self.img_feat_id)] = -100\n        output = {\n            'mlm_labels': labels,\n            'mlm_decoder_input_ids': decoder_input_ids,\n            'mlm_decoder_attention_mask': decoder_attention_mask\n        }\n\n        return output\n    \n\n\n    def encode_senti(self, sentis):  #generate label for MSP task\n        senti_input_text = [\n            self.bos_token + self.senti_token for i in range(len(sentis))\n        ]\n        senti_input_text = self.encode(senti_input_text,\n                                       add_special_tokens=False,\n                                       return_tensors='pt',\n                                       padding=True)\n        senti_decoder_input_ids = senti_input_text['input_ids']\n        senti_decoder_attention_mask = senti_input_text['attention_mask']\n\n        sentiment = []\n        for senti in sentis:\n            sentiment.append(senti)\n            # else:\n            #     raise ValueError('sentiment label error!!')\n        output = {\n            'senti_labels': torch.from_numpy(np.array(sentiment)),\n            'senti_decoder_input_ids': senti_decoder_input_ids,\n            'senti_decoder_attention_mask': senti_decoder_attention_mask\n        }\n        return output\n\n    def encode_anp_dis(self, batch_size):\n        ANP_input_text = [\n            self.bos_token + self.ANP_token for i in range(batch_size)\n        ]\n        ANP_input_text = self.encode(ANP_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        output = {}\n        output['ANP_decoder_input_ids'] = ANP_input_text['input_ids']\n        output['ANP_decoder_attention_mask'] = ANP_input_text['attention_mask']\n\n        return output\n\n    def encode_anp_generate(self, ANP_words):  #generate label for AOG task\n        label_split = [x.split() for x in ANP_words]\n        label_tokens = []\n        for split in label_split:\n            word_bpes = [[self.bos_token_id], [self.ANP_generate_token_id]]\n            for word in split:\n                bpes = self._base_tokenizer.tokenize(word,\n                                                     add_prefix_space=True)\n                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                word_bpes.append(bpes)\n            word_bpes.append([self.eos_token_id])\n            _word_bpes = list(chain(*word_bpes))\n            label_tokens.append(_word_bpes)\n        input_ids, attention_mask = self.pad_tokens(label_tokens)\n\n        output_shape = input_ids[:, 2:].shape\n        labels = torch.empty(output_shape, dtype=torch.long)\n        decoder_input_ids = torch.empty(input_ids[:, 1:].shape,\n                                        dtype=torch.long)\n        decoder_attention_mask = torch.empty(input_ids[:, 1:].shape,\n                                             dtype=torch.long)\n\n        for i in range(labels.size(0)):\n            labels[i] = input_ids[i][\n                (input_ids[i] != self.bos_token_id)\n                & (input_ids[i] != self.ANP_generate_token_id)]\n            decoder_input_ids[i] = input_ids[i][\n                input_ids[i] != self.eos_token_id]\n            decoder_attention_mask[i] = attention_mask[i][\n                input_ids[i] != self.eos_token_id]\n\n        labels[(labels == self.pad_token_id) | (labels == self.begin_img_id) |\n               (labels == self.end_img_id) |\n               (labels == self.ANP_generate_token_id) |\n               (labels == self.img_feat_id)] = -100\n\n        output = {\n            'anp_generate_labels': labels,\n            'anp_generate_decoder_input_ids': decoder_input_ids,\n            'anp_generate_decoder_attention_mask': decoder_attention_mask\n        }\n        return output\n\n    def encode_aesc(self, label, aesc_spans, aesc_max_len):\n        # import ipdb; ipdb.set_trace()\n        target_shift = len(self.mapping2targetid) + 2\n        aesc_text = []\n        masks = []\n        gt_spans = []\n\n        flag = True\n        for text, span in zip(label, aesc_spans):\n            span = sorted(span, key=cmp_to_key(cmp))\n            word_bpes = [[self.begin_text_id]]\n            for word in text.split():\n                bpes = self._base_tokenizer.tokenize(word,\n                                                     add_prefix_space=True)\n                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                word_bpes.append(bpes)\n            word_bpes.append([self.end_text_id])\n\n            lens = list(map(len, word_bpes))\n            cum_lens = np.cumsum(list(lens)).tolist()\n            # print(\"self.mapping2targetid is {}\".format(self.mapping2targetid))\n            '''\n            self.mapping2targetid: {'AESC': 2, 'POS': 3, 'NEU': 4, 'NEG': 5} \n            '''\n            cur_text = [\n                0, self.mapping2targetid['AESC'], self.mapping2targetid['AESC']\n            ]\n            # print(\"====================cur_text is {}=============================\".format(cur_text))\n            mask = [0, 0, 0]\n            gt = []\n            for x in span:\n                s_bpe = cum_lens[x[0]] + target_shift\n                e_bpe = cum_lens[x[1] - 1] + target_shift\n                polarity = self.mapping2targetid[x[2]]\n                cur_text.append(s_bpe)\n                cur_text.append(e_bpe)\n                cur_text.append(polarity)\n                gt.append((s_bpe, e_bpe, polarity))\n                mask.append(1)\n                mask.append(1)\n                mask.append(1)\n            cur_text.append(1)\n            mask.append(1)\n\n            aesc_text.append(cur_text)\n            gt_spans.append(gt)\n            masks.append(mask)\n        span_max_len = max([len(x) for x in aesc_text])\n        for i in range(len(masks)):\n            add_len = span_max_len - len(masks[i])\n            masks[i] = masks[i] + [0 for ss in range(add_len)]\n            aesc_text[i] = aesc_text[i] + [1 for ss in range(add_len)]\n        \n       \n        output = {}\n        output['labels'] = torch.tensor(aesc_text)\n        output['masks'] = torch.tensor(masks)\n        output['spans'] = gt_spans\n\n        aspect_prompt_input_text = [\n            self.bos_token + self.aspect_prompt_token + self.aspect_prompt_token for i in range(len(label))\n        ]\n        aspect_prompt_input_text = self.encode(aspect_prompt_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        output['aspect_prompt_decoder_input_ids'] = aspect_prompt_input_text['input_ids']\n        output['aspect_prompt_decoder_attention_mask'] = aspect_prompt_input_text['attention_mask']\n        \n        senti_prompt_input_text = [\n            self.bos_token + self.senti_prompt_token for i in range(len(label))\n        ]\n        senti_prompt_input_text = self.encode(senti_prompt_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        output['senti_prompt_decoder_input_ids'] = senti_prompt_input_text['input_ids']\n        output['senti_prompt_decoder_attention_mask'] = senti_prompt_input_text['attention_mask']\n\n        aspects_num_input_text = [\n            self.bos_token + self.aspect_prompt_token for i in range(len(label))\n        ]\n\n        aspects_num_input_text = self.encode(aspects_num_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        output['aspects_num_decoder_input_ids'] = aspects_num_input_text['input_ids']\n        output['aspects_num_decoder_attention_mask'] = aspects_num_input_text['attention_mask']\n        \n\n        # print(\"---------------------output is {}------------------\".format(output))\n        # print('++++++++++++++++++++++output[labels] is {}+++++++++++++++++++++++++++++'.format(output['labels']))\n        return output\n\n    def encode_ae_oe(self, label, aspect_spans,\n                     opinion_spans):  #generate labels of AOE task\n        target_shift = len(self.mapping2targetid) + 2\n        ae_oe_text = []\n        masks = []\n        gt_spans = []\n\n        for text, ae_span, oe_span in zip(label, aspect_spans, opinion_spans):\n            word_bpes = [[self.begin_text_id]]\n            for word in text.split():\n                bpes = self._base_tokenizer.tokenize(word,\n                                                     add_prefix_space=True)\n                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                word_bpes.append(bpes)\n            word_bpes.append([self.end_text_id])\n\n            lens = list(map(len, word_bpes))\n            cum_lens = np.cumsum(list(lens)).tolist()\n            cur_text = [\n                0, self.mapping2targetid['AE_OE'],\n                self.mapping2targetid['AE_OE']\n            ]\n            mask = [0, 0, 0]\n\n            gt = []\n            for x in ae_span:\n                # print(x[0])\n                s_bpe = cum_lens[x[0]] + target_shift\n                e_bpe = cum_lens[x[1]] + target_shift\n                cur_text.append(s_bpe)\n                cur_text.append(e_bpe)\n                gt.append((s_bpe, e_bpe))\n                mask.append(1)\n                mask.append(1)\n            cur_text.append(self.mapping2targetid['SEP'])\n            mask.append(1)\n            for x in oe_span:\n                # print(x[0])\n                s_bpe = cum_lens[x[0]] + target_shift\n                e_bpe = cum_lens[x[1]] + target_shift\n                cur_text.append(s_bpe)\n                cur_text.append(e_bpe)\n                gt.append((s_bpe, e_bpe))\n                mask.append(1)\n                mask.append(1)\n            cur_text.append(1)\n            mask.append(1)\n\n            ae_oe_text.append(cur_text)\n            masks.append(mask)\n            gt_spans.append(gt)\n        span_max_len = max(len(x) for x in ae_oe_text)\n        for i in range(len(masks)):\n            add_len = span_max_len - len(masks[i])\n            masks[i] = masks[i] + [0 for ss in range(add_len)]\n            ae_oe_text[i] = ae_oe_text[i] + [1 for ss in range(add_len)]\n        output = {}\n        output['labels'] = torch.tensor(ae_oe_text)\n        output['masks'] = torch.tensor(masks)\n        output['spans'] = gt_spans\n        return output\n\n    def encode_mrm(self, box_cls):\n        mrm_input_text = [\n            self.bos_token + self.mrm_token + self.img_feat * 36\n            for i in range(len(box_cls))\n        ]\n        mrm_input_text = self.encode(mrm_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        mrm_decoder_input_ids = mrm_input_text['input_ids']\n        mrm_decoder_attention_mask = mrm_input_text['attention_mask']\n\n        output = {\n            'mrm_labels': torch.from_numpy(np.array(box_cls)),\n            'mrm_decoder_input_ids': mrm_decoder_input_ids,\n            'mrm_decoder_attention_mask': mrm_decoder_attention_mask\n        }\n        return output\n\n    def encode_twitter_ae(self, label, aspect_spans, ae_max_len):\n        target_shift = len(self.mapping2targetid) + 2\n        ae_text = []\n        masks = []\n        gt_spans = []\n        for text, span in zip(label, aspect_spans):\n            word_bpes = [[self.begin_text_id]]\n            for word in text.split():\n                bpes = self._base_tokenizer.tokenize(word,\n                                                     add_prefix_space=True)\n                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                word_bpes.append(bpes)\n            word_bpes.append([self.end_text_id])\n\n            lens = list(map(len, word_bpes))\n            cum_lens = np.cumsum(list(lens)).tolist()\n            # self.all_cum_lens.append(cum_lens)\n            # print(len(cum_lens), len(split))\n            cur_text = [\n                0, self.mapping2targetid['AE'], self.mapping2targetid['AE']\n            ]\n            mask = [0, 0, 0]\n            # print(text)\n            # print(len(cum_lens), len(text.split()))\n            gt = []\n            for x in span:\n\n                s_bpe = cum_lens[x[0]] + target_shift\n                e_bpe = cum_lens[x[1] - 1] + target_shift\n                cur_text.append(s_bpe)\n                cur_text.append(e_bpe)\n                gt.append((s_bpe, e_bpe))\n                mask.append(1)\n                mask.append(1)\n            cur_text.append(1)\n            mask.append(1)\n            # cur_text = cur_text + [\n            #     1 for i in range(ae_max_len - len(cur_text))\n            # ]\n            # mask = mask + [0 for i in range(ae_max_len - len(mask))]\n            # print(cur_text)\n            ae_text.append(cur_text)\n            masks.append(mask)\n            gt_spans.append(gt)\n        span_max_len = max(len(x) for x in ae_text)\n        for i in range(len(masks)):\n            add_len = span_max_len - len(masks[i])\n            masks[i] = masks[i] + [0 for ss in range(add_len)]\n            ae_text[i] = ae_text[i] + [1 for ss in range(add_len)]\n        output = {}\n        output['labels'] = torch.tensor(ae_text)\n        output['masks'] = torch.tensor(masks)\n        output['spans'] = gt_spans\n        # output['AE_masks'][:, 2] = 1\n\n        aspect_prompt_input_text = [\n            self.bos_token + self.aspect_prompt_token + self.aspect_prompt_token for i in range(len(label))\n        ]\n        aspect_prompt_input_text = self.encode(aspect_prompt_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        output['aspect_prompt_decoder_input_ids'] = aspect_prompt_input_text['input_ids']\n        output['aspect_prompt_decoder_attention_mask'] = aspect_prompt_input_text['attention_mask']\n        \n        aspects_num_input_text = [\n            self.bos_token + self.aspect_prompt_token for i in range(len(label))\n        ]\n\n        aspects_num_input_text = self.encode(aspects_num_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        output['aspects_num_decoder_input_ids'] = aspects_num_input_text['input_ids']\n        output['aspects_num_decoder_attention_mask'] = aspects_num_input_text['attention_mask']\n        \n        return output\n\n    def encode_twitter_sc(self, label, aesc_spans, aesc_max_len):\n        target_shift = len(self.mapping2targetid) + 2\n        aesc_text = []\n        masks = []\n        gt_spans = []\n        # print(len(opinion_spans))\n        # print(len(self.all_cum_lens), len(opinion_spans))\n\n        flag = True\n        for text, span in zip(label, aesc_spans):\n            span = sorted(span, key=cmp_to_key(cmp))\n            word_bpes = [[self.begin_text_id]]\n            for word in text.split():\n                bpes = self._base_tokenizer.tokenize(word,\n                                                     add_prefix_space=True)\n                bpes = self._base_tokenizer.convert_tokens_to_ids(bpes)\n                word_bpes.append(bpes)\n            word_bpes.append([self.end_text_id])\n\n            lens = list(map(len, word_bpes))\n            cum_lens = np.cumsum(list(lens)).tolist()\n\n            # if flag:\n            #     # print(word_bpes)\n            #     print(cum_lens)\n            #     flag = False\n            cur_text = [\n                0, self.mapping2targetid['SC'], self.mapping2targetid['SC']\n            ]\n            mask = [0, 0, 0]\n            # print(text)\n            # print(len(cum_lens), len(text.split()))\n            gt = []\n            for x in span:\n\n                s_bpe = cum_lens[x[0]] + target_shift\n                e_bpe = cum_lens[x[1] - 1] + target_shift\n                # if s_bpe >= cum_lens[-1] or e_bpe >= cum_lens[-1]:\n                #     break\n                polarity = self.mapping2targetid[x[2]]\n                cur_text.append(s_bpe)\n                cur_text.append(e_bpe)\n                cur_text.append(polarity)\n                gt.append((s_bpe, e_bpe, polarity))\n                mask.append(1)\n                mask.append(1)\n                mask.append(1)\n            cur_text.append(1)\n            mask.append(0)\n            # cur_text = cur_text + [\n            #     1 for i in range(aesc_max_len - len(cur_text))\n            # ]\n            # mask = mask + [0 for i in range(aesc_max_len - len(mask))]\n            # print(cur_text)\n            aesc_text.append(cur_text)\n            gt_spans.append(gt)\n            masks.append(mask)\n        span_max_len = max([len(x) for x in aesc_text])\n        for i in range(len(masks)):\n            add_len = span_max_len - len(masks[i])\n            masks[i] = masks[i] + [0 for ss in range(add_len)]\n            aesc_text[i] = aesc_text[i] + [1 for ss in range(add_len)]\n            # masks[i].extend([0 for ss in range(add_len)])\n            # aesc_text[i].extend([1 for ss in range(add_len)])\n\n        output = {}\n        # print(oe_text[0], len(oe_text))\n        # for xx in oe_text:\n        #     if xx == None:\n        #         print('opinion shit!!!!!!!!!!!!!!!')\n        # print(aesc_text[0])\n        # print(masks[0])\n        output['labels'] = torch.tensor(aesc_text)\n        output['masks'] = torch.tensor(masks)\n        output['spans'] = gt_spans\n\n        senti_prompt_input_text = [\n            self.bos_token + self.senti_prompt_token for i in range(len(label))\n        ]\n        senti_prompt_input_text = self.encode(senti_prompt_input_text,\n                                     add_special_tokens=False,\n                                     return_tensors='pt',\n                                     padding=True)\n        output['senti_prompt_decoder_input_ids'] = senti_prompt_input_text['input_ids']\n        output['senti_prompt_decoder_attention_mask'] = senti_prompt_input_text['attention_mask']\n\n        return output\n\n    def decode(self, token_ids, skip_special_tokens=False):\n        return self._base_tokenizer.decode(\n            token_ids, skip_special_tokens=skip_special_tokens)\n\n    def convert_tokens_to_ids(self, tokens):\n        return self._base_tokenizer.convert_tokens_to_ids(tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return self._base_tokenizer.convert_ids_to_tokens(ids)\n\n    def get_base_tokenizer(self):\n        return self._base_tokenizer\n\n    def __len__(self):\n        return len(self._base_tokenizer)"]}
{"filename": "src/data/collation_for_prompt_multitasks.py", "chunked_list": ["import warnings\n\nimport numpy as np\nimport torch\nfrom itertools import chain\n# from src.utils import TaskType\n\n\nclass Collator:\n    \"\"\"\n    The collator for all types of dataset.\n    Remember to add the corresponding collation code after adding a new type of task.\n    \"\"\"\n    def __init__(self,\n                 task,\n                 tokenizer,\n                 is_mlm=False,\n                 has_label=True,\n                 mlm_enabled=False,\n                 mrm_enabled=False,\n                 senti_enabled=False,\n                 ae_enabled=False,\n                 oe_enabled=False,\n                 ae_oe_enabled=False,\n                 aesc_enabled=False,\n                 anp_enabled=False,\n                 anp_generate_enabled=False,\n                 twitter_ae_enabled=False,\n                 twitter_sc_enabled=False,\n                 has_prompt=False,\n                 text_only=False,\n                 use_caption=False,\n                 mlm_probability=0.0,\n                 mrm_probability=0.0,\n                 lm_max_len=30,\n                 max_img_num=2,\n                 max_span_len=20):\n        \"\"\"\n        :param tokenizer: ConditionTokenizer\n        :param mlm_enabled: bool, if use mlm for language modeling. False for autoregressive modeling\n        :param mrm_enabled: bool, if use mrm\n        :param rp_enabled: bool, if use relation prediction (VG)\n        :param ap_enabled: bool, if use attribute prediction (VG)\n        :param mlm_probability: float, probability to mask the tokens\n        :param mrm_probability: float, probability to mask the regions\n        \"\"\"\n        self.task = task\n        self._tokenizer = tokenizer\n        self._has_label = has_label\n        self._is_mlm = is_mlm\n        self._mrm_enabled = mrm_enabled\n        self._mlm_enabled = mlm_enabled\n        self._senti_enabled = senti_enabled\n        self._anp_enabled = anp_enabled\n        self._anp_generate_enabled = anp_generate_enabled\n        self._ae_enabled = ae_enabled\n        self._oe_enabled = oe_enabled\n        self._ae_oe_enabled = ae_oe_enabled\n        self._aesc_enabled = aesc_enabled\n        self._twitter_ae_enabled = twitter_ae_enabled\n        self._twitter_sc_enabled = twitter_sc_enabled\n        self._lm_max_len = lm_max_len\n        self._max_img_num = max_img_num\n        self._mlm_probability = mlm_probability\n        self._mrm_probability = mrm_probability\n        self._max_span_len = max_span_len\n        self.text_only = text_only\n        self.use_caption = use_caption\n        self.has_prompt=has_prompt\n        if mlm_enabled and not has_label:\n            raise ValueError(\n                'mlm_enabled can not be true while has_label is false. MLM need labels.'\n            )\n\n    def _clip_text(self, text, length):\n        tokenized = []\n        for i, word in enumerate(text.split()):\n            if i == 0:\n                bpes = self._tokenizer._base_tokenizer.tokenize(word)\n            else:\n                bpes = self._tokenizer._base_tokenizer.tokenize(\n                    word, add_prefix_space=True)\n            bpes = self._tokenizer._base_tokenizer.convert_tokens_to_ids(bpes)\n            tokenized.append(bpes)\n        _tokenized = list(chain(*tokenized))\n        return self._tokenizer.get_base_tokenizer().decode(_tokenized[:length])\n\n    def __call__(self, batch):\n        batch = [entry for entry in batch if entry is not None]\n\n        # image_features = [\n        #     torch.from_numpy(x['img_feat'][:self._max_img_num])\n        #     if 'img_feat' in x else torch.empty(0) for x in batch\n        # ]\n        # import ipdb; ipdb.set_trace()\n\n        image_features = [\n                        x['image_pixel_values']\n                        if 'image_pixel_values' in x else torch.empty(0) for x in batch\n        ]\n\n        image_caption = [x['caption'] for x in batch]\n\n        aspects_num = [x['aspects_num']-1 for x in batch]\n\n        max_aspects_num = [5 for x in batch]\n\n        img_num = [self._max_img_num for x in image_features]\n\n\n        target = [x['sentence'] for x in batch]\n    \n        sentence = list(target)\n\n        encoded_conditions = self._tokenizer.encode_condition(\n            task=self.task,\n            img_num=img_num, use_caption=self.use_caption, caption=image_caption, \n            sentence=sentence, has_prompt=self.has_prompt, max_aspects_num=max_aspects_num, text_only=self.text_only)\n\n        input_ids = encoded_conditions['input_ids']\n        output = {}\n        if self._is_mlm:\n            input_ids = self._mask_tokens(\n                inputs=input_ids,\n                input_mask=encoded_conditions['sentence_mask'])\n        condition_img_mask = encoded_conditions['img_mask']\n\n        if self._mrm_enabled:\n            encode_mrm = self._tokenizer.encode_mrm([x['cls'] for x in batch])\n            mrm_labels_all = encode_mrm['mrm_labels']\n            probability_matrix = torch.full(input_ids.shape,\n                                            self._mrm_probability,\n                                            dtype=torch.float)\n            masked_regions = torch.bernoulli(probability_matrix).bool()\n            input_ids[masked_regions\n                      & condition_img_mask] = self._tokenizer.cls_token_id\n            decoder_input_ids = encode_mrm['mrm_decoder_input_ids']\n            for i in range(input_ids.size(0)):\n                for j in range(36):\n                    if input_ids[i, j + 1] == self._tokenizer.cls_token_id:\n                        decoder_input_ids[i, j +\n                                          2] = self._tokenizer.cls_token_id\n            mrm_labels = []\n            for i in range(len(batch)):\n                # create mrm_labels\n                masked_indices = masked_regions[i][\n                    condition_img_mask[i]].nonzero(as_tuple=False)\n                mrm_label = mrm_labels_all[i]\n                mrm_labels.append(mrm_label[masked_indices].clone())\n\n                if len(image_features[i]) > 0:\n                    image_features[i][masked_indices] = torch.zeros(\n                        (len(masked_indices), 1, 2048),\n                        dtype=image_features[i].dtype)\n            MRM = {}\n            MRM['mrm_labels'] = mrm_labels\n            MRM['mrm_decoder_input_ids'] = decoder_input_ids\n            MRM['mrm_masks'] = decoder_input_ids == self._tokenizer.cls_token_id\n            MRM['mrm_decoder_attention_mask'] = encode_mrm[\n                'mrm_decoder_attention_mask']\n            output['MRM'] = MRM\n            output['task'] = 'MRM'\n        output['input_ids'] = input_ids\n        output['attention_mask'] = encoded_conditions['attention_mask']\n        output['image_features'] = image_features\n        output['input_ids'] = input_ids\n        output['aspects_num'] = aspects_num\n    \n        if self._has_label:\n            # encode mrm and mlm labels\n            if self._mlm_enabled:\n                mlm_output = self._tokenizer.encode_label(label=target,\n                                                          img_num=img_num)\n                output['MLM'] = mlm_output\n                output['task'] = 'MLM'\n\n            if self._senti_enabled:\n                output['Sentiment'] = self._tokenizer.encode_senti(\n                    [x['sentiment'] for x in batch])\n                output['task'] = 'Sentiment'\n\n            if self._anp_generate_enabled:\n                output['ANP_generate'] = self._tokenizer.encode_anp_generate(\n                    [x['ANP_words'] for x in batch])\n                output['task'] = 'ANP_generate'\n            if self._aesc_enabled:\n                output['AESC'] = self._tokenizer.encode_aesc(\n                    target, [x['aesc_spans'] for x in batch],\n                    self._max_span_len)\n                output['task'] = 'AESC'\n            if self._ae_oe_enabled:\n                output['AE_OE'] = self._tokenizer.encode_ae_oe(\n                    target, [x['aspect_spans'] for x in batch],\n                    [x['opinion_spans'] for x in batch])\n                output['task'] = 'AE_OE'\n            if self._twitter_ae_enabled:\n                output['TWITTER_AE'] = self._tokenizer.encode_twitter_ae(\n                    target, [x['aesc_spans'] for x in batch],\n                    self._max_span_len)\n            if self._twitter_sc_enabled:\n                output['TWITTER_SC'] = self._tokenizer.encode_twitter_sc(\n                    target, [x['aesc_spans'] for x in batch],\n                    self._max_span_len)\n\n        output['image_id'] = [x['image_id'] for x in batch]\n        output['gt'] = [x['gt'] for x in batch]\n        return output\n\n    def _mask_tokens(self, inputs, input_mask):\n        \"\"\"\n        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n\n        :param inputs: torch.LongTensor, batch data\n        :param input_mask: torch.Tensor, mask for the batch, False for the position with 0% probability to be masked\n        \"\"\"\n\n        labels = inputs.clone()\n        tokenizer = self._tokenizer.get_base_tokenizer()\n\n        # We sample a few tokens in each sequence for masked-LM training\n        probability_matrix = torch.full(labels.shape,\n                                        self._mlm_probability,\n                                        dtype=torch.float)\n        special_tokens_mask = [\n            tokenizer.get_special_tokens_mask(val,\n                                              already_has_special_tokens=True)\n            for val in labels.tolist()\n        ]\n        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask,\n                                                     dtype=torch.bool),\n                                        value=0.0)\n        if tokenizer.pad_token is not None:\n            padding_mask = labels.eq(tokenizer.pad_token_id)\n            probability_matrix.masked_fill_(padding_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(\n            labels.shape, 0.8)).bool() & masked_indices\n        inputs[indices_replaced & input_mask] = tokenizer.mask_token_id\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(\n            labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(tokenizer.vocab_size,\n                                     labels.shape,\n                                     dtype=torch.long)\n        inputs[indices_random & input_mask] = random_words[indices_random\n                                                           & input_mask]\n\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs", "class Collator:\n    \"\"\"\n    The collator for all types of dataset.\n    Remember to add the corresponding collation code after adding a new type of task.\n    \"\"\"\n    def __init__(self,\n                 task,\n                 tokenizer,\n                 is_mlm=False,\n                 has_label=True,\n                 mlm_enabled=False,\n                 mrm_enabled=False,\n                 senti_enabled=False,\n                 ae_enabled=False,\n                 oe_enabled=False,\n                 ae_oe_enabled=False,\n                 aesc_enabled=False,\n                 anp_enabled=False,\n                 anp_generate_enabled=False,\n                 twitter_ae_enabled=False,\n                 twitter_sc_enabled=False,\n                 has_prompt=False,\n                 text_only=False,\n                 use_caption=False,\n                 mlm_probability=0.0,\n                 mrm_probability=0.0,\n                 lm_max_len=30,\n                 max_img_num=2,\n                 max_span_len=20):\n        \"\"\"\n        :param tokenizer: ConditionTokenizer\n        :param mlm_enabled: bool, if use mlm for language modeling. False for autoregressive modeling\n        :param mrm_enabled: bool, if use mrm\n        :param rp_enabled: bool, if use relation prediction (VG)\n        :param ap_enabled: bool, if use attribute prediction (VG)\n        :param mlm_probability: float, probability to mask the tokens\n        :param mrm_probability: float, probability to mask the regions\n        \"\"\"\n        self.task = task\n        self._tokenizer = tokenizer\n        self._has_label = has_label\n        self._is_mlm = is_mlm\n        self._mrm_enabled = mrm_enabled\n        self._mlm_enabled = mlm_enabled\n        self._senti_enabled = senti_enabled\n        self._anp_enabled = anp_enabled\n        self._anp_generate_enabled = anp_generate_enabled\n        self._ae_enabled = ae_enabled\n        self._oe_enabled = oe_enabled\n        self._ae_oe_enabled = ae_oe_enabled\n        self._aesc_enabled = aesc_enabled\n        self._twitter_ae_enabled = twitter_ae_enabled\n        self._twitter_sc_enabled = twitter_sc_enabled\n        self._lm_max_len = lm_max_len\n        self._max_img_num = max_img_num\n        self._mlm_probability = mlm_probability\n        self._mrm_probability = mrm_probability\n        self._max_span_len = max_span_len\n        self.text_only = text_only\n        self.use_caption = use_caption\n        self.has_prompt=has_prompt\n        if mlm_enabled and not has_label:\n            raise ValueError(\n                'mlm_enabled can not be true while has_label is false. MLM need labels.'\n            )\n\n    def _clip_text(self, text, length):\n        tokenized = []\n        for i, word in enumerate(text.split()):\n            if i == 0:\n                bpes = self._tokenizer._base_tokenizer.tokenize(word)\n            else:\n                bpes = self._tokenizer._base_tokenizer.tokenize(\n                    word, add_prefix_space=True)\n            bpes = self._tokenizer._base_tokenizer.convert_tokens_to_ids(bpes)\n            tokenized.append(bpes)\n        _tokenized = list(chain(*tokenized))\n        return self._tokenizer.get_base_tokenizer().decode(_tokenized[:length])\n\n    def __call__(self, batch):\n        batch = [entry for entry in batch if entry is not None]\n\n        # image_features = [\n        #     torch.from_numpy(x['img_feat'][:self._max_img_num])\n        #     if 'img_feat' in x else torch.empty(0) for x in batch\n        # ]\n        # import ipdb; ipdb.set_trace()\n\n        image_features = [\n                        x['image_pixel_values']\n                        if 'image_pixel_values' in x else torch.empty(0) for x in batch\n        ]\n\n        image_caption = [x['caption'] for x in batch]\n\n        aspects_num = [x['aspects_num']-1 for x in batch]\n\n        max_aspects_num = [5 for x in batch]\n\n        img_num = [self._max_img_num for x in image_features]\n\n\n        target = [x['sentence'] for x in batch]\n    \n        sentence = list(target)\n\n        encoded_conditions = self._tokenizer.encode_condition(\n            task=self.task,\n            img_num=img_num, use_caption=self.use_caption, caption=image_caption, \n            sentence=sentence, has_prompt=self.has_prompt, max_aspects_num=max_aspects_num, text_only=self.text_only)\n\n        input_ids = encoded_conditions['input_ids']\n        output = {}\n        if self._is_mlm:\n            input_ids = self._mask_tokens(\n                inputs=input_ids,\n                input_mask=encoded_conditions['sentence_mask'])\n        condition_img_mask = encoded_conditions['img_mask']\n\n        if self._mrm_enabled:\n            encode_mrm = self._tokenizer.encode_mrm([x['cls'] for x in batch])\n            mrm_labels_all = encode_mrm['mrm_labels']\n            probability_matrix = torch.full(input_ids.shape,\n                                            self._mrm_probability,\n                                            dtype=torch.float)\n            masked_regions = torch.bernoulli(probability_matrix).bool()\n            input_ids[masked_regions\n                      & condition_img_mask] = self._tokenizer.cls_token_id\n            decoder_input_ids = encode_mrm['mrm_decoder_input_ids']\n            for i in range(input_ids.size(0)):\n                for j in range(36):\n                    if input_ids[i, j + 1] == self._tokenizer.cls_token_id:\n                        decoder_input_ids[i, j +\n                                          2] = self._tokenizer.cls_token_id\n            mrm_labels = []\n            for i in range(len(batch)):\n                # create mrm_labels\n                masked_indices = masked_regions[i][\n                    condition_img_mask[i]].nonzero(as_tuple=False)\n                mrm_label = mrm_labels_all[i]\n                mrm_labels.append(mrm_label[masked_indices].clone())\n\n                if len(image_features[i]) > 0:\n                    image_features[i][masked_indices] = torch.zeros(\n                        (len(masked_indices), 1, 2048),\n                        dtype=image_features[i].dtype)\n            MRM = {}\n            MRM['mrm_labels'] = mrm_labels\n            MRM['mrm_decoder_input_ids'] = decoder_input_ids\n            MRM['mrm_masks'] = decoder_input_ids == self._tokenizer.cls_token_id\n            MRM['mrm_decoder_attention_mask'] = encode_mrm[\n                'mrm_decoder_attention_mask']\n            output['MRM'] = MRM\n            output['task'] = 'MRM'\n        output['input_ids'] = input_ids\n        output['attention_mask'] = encoded_conditions['attention_mask']\n        output['image_features'] = image_features\n        output['input_ids'] = input_ids\n        output['aspects_num'] = aspects_num\n    \n        if self._has_label:\n            # encode mrm and mlm labels\n            if self._mlm_enabled:\n                mlm_output = self._tokenizer.encode_label(label=target,\n                                                          img_num=img_num)\n                output['MLM'] = mlm_output\n                output['task'] = 'MLM'\n\n            if self._senti_enabled:\n                output['Sentiment'] = self._tokenizer.encode_senti(\n                    [x['sentiment'] for x in batch])\n                output['task'] = 'Sentiment'\n\n            if self._anp_generate_enabled:\n                output['ANP_generate'] = self._tokenizer.encode_anp_generate(\n                    [x['ANP_words'] for x in batch])\n                output['task'] = 'ANP_generate'\n            if self._aesc_enabled:\n                output['AESC'] = self._tokenizer.encode_aesc(\n                    target, [x['aesc_spans'] for x in batch],\n                    self._max_span_len)\n                output['task'] = 'AESC'\n            if self._ae_oe_enabled:\n                output['AE_OE'] = self._tokenizer.encode_ae_oe(\n                    target, [x['aspect_spans'] for x in batch],\n                    [x['opinion_spans'] for x in batch])\n                output['task'] = 'AE_OE'\n            if self._twitter_ae_enabled:\n                output['TWITTER_AE'] = self._tokenizer.encode_twitter_ae(\n                    target, [x['aesc_spans'] for x in batch],\n                    self._max_span_len)\n            if self._twitter_sc_enabled:\n                output['TWITTER_SC'] = self._tokenizer.encode_twitter_sc(\n                    target, [x['aesc_spans'] for x in batch],\n                    self._max_span_len)\n\n        output['image_id'] = [x['image_id'] for x in batch]\n        output['gt'] = [x['gt'] for x in batch]\n        return output\n\n    def _mask_tokens(self, inputs, input_mask):\n        \"\"\"\n        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n\n        :param inputs: torch.LongTensor, batch data\n        :param input_mask: torch.Tensor, mask for the batch, False for the position with 0% probability to be masked\n        \"\"\"\n\n        labels = inputs.clone()\n        tokenizer = self._tokenizer.get_base_tokenizer()\n\n        # We sample a few tokens in each sequence for masked-LM training\n        probability_matrix = torch.full(labels.shape,\n                                        self._mlm_probability,\n                                        dtype=torch.float)\n        special_tokens_mask = [\n            tokenizer.get_special_tokens_mask(val,\n                                              already_has_special_tokens=True)\n            for val in labels.tolist()\n        ]\n        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask,\n                                                     dtype=torch.bool),\n                                        value=0.0)\n        if tokenizer.pad_token is not None:\n            padding_mask = labels.eq(tokenizer.pad_token_id)\n            probability_matrix.masked_fill_(padding_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(\n            labels.shape, 0.8)).bool() & masked_indices\n        inputs[indices_replaced & input_mask] = tokenizer.mask_token_id\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(\n            labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(tokenizer.vocab_size,\n                                     labels.shape,\n                                     dtype=torch.long)\n        inputs[indices_random & input_mask] = random_words[indices_random\n                                                           & input_mask]\n\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs"]}
{"filename": "src/data/dataset_for_prompt.py", "chunked_list": ["import torch\nimport numpy as np\nimport json\nimport csv\nimport os\nimport json\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\nfrom PIL import Image, ImageFile, UnidentifiedImageError", "from transformers import AutoTokenizer\nfrom PIL import Image, ImageFile, UnidentifiedImageError\nfrom timm.data.transforms_factory import create_transform\nfrom torchvision.transforms import Compose, Lambda\nfrom transformers import GPT2Tokenizer, AutoFeatureExtractor, CLIPFeatureExtractor\nimport logging\nlogging.getLogger('PIL').setLevel(logging.WARNING)\nTIMM_CONFIGS = {\n    'nf_resnet50':  {\n        'input_size': (3, 256, 256),", "    'nf_resnet50':  {\n        'input_size': (3, 256, 256),\n        'interpolation': 'bicubic',\n        'mean': (0.485, 0.456, 0.406),\n        'std': (0.229, 0.224, 0.225),\n        'crop_pct': 0.94,\n    },\n}\n\nclass MVSA_Dataset(data.Dataset):\n    def __init__(self, infos):\n        # print(infos)\n        infos = json.load(open(infos, 'r'))\n        self.text_dir = infos['text_dir']\n        self.img_region_dir = infos['img_region_dir']\n        self.senti_dir = infos['senti_dir']\n        # self.BIO_dir = infos['BIO_dir']\n        self.aspect_span_dict = json.load(open(infos['aspect_span_path'], 'r'))\n        self.opinion_span_dict = json.load(\n            open(infos['opinion_span_path'], 'r'))\n        self.ANP_dir = infos['ANP_dir']\n        self.ANP_class_dir = infos['ANP_class_dir']\n        self.ANP_class = json.load(open(self.ANP_class_dir, 'r'))\n        self.ANP_class = {i: anp for i, anp in enumerate(self.ANP_class)}\n        self.ANP2idx = {anp: idx for idx, anp in self.ANP_class.items()}\n        self.ANP_len = len(self.ANP_class)\n        self.id2senti = json.load(open(self.senti_dir, 'r'))\n        self.idx2ANP = json.load(open(self.ANP_dir, 'r'))['images']\n\n        self.create_id2idx()\n\n    def __len__(self):\n        return len(self.ids)\n\n    def create_id2idx(self):\n        ignore = [\n            '3151', '3910', '5995'\n        ]  #Pictures of these ids can not be opened, so we remove them.\n        self.ids = list(sorted(self.id2senti.keys(), key=lambda x: int(x)))\n        for x in ignore:\n            self.ids.remove(x)\n        self.idx2id = {i: id for i, id in enumerate(self.ids)}\n\n    def get_img_region_box(self, id):\n        region_feat = np.load(\n            os.path.join(self.img_region_dir + '/_att', id + '.npz'))['feat']\n        box = np.load(os.path.join(self.img_region_dir + '/_box', id + '.npy'))\n\n        return region_feat, box\n\n    def process_ANP_distribution(self, distribution):\n        result = np.empty([1, self.ANP_len], dtype=float)\n        for anp, prob in distribution.items():\n            result[0, self.ANP2idx[anp]] = prob\n\n        return result\n\n    def get_ANP_word(self, distribution):\n        anp_word = list(distribution.items())[0][0].replace('_', ' ')\n        # print(anp_word)\n        return anp_word\n\n    def get_img_ANP(self, idx):\n        distribution = self.idx2ANP[idx]['bi-concepts']\n        words = self.get_ANP_word(distribution)\n        dis = self.process_ANP_distribution(distribution)\n\n        return dis, words\n\n    def get_sentiment(self, id):\n        sentiment = self.id2senti[id]\n        return sentiment\n\n    def get_sentence(self, id):\n        sentence = open(os.path.join(self.text_dir,\n                                     id + '.txt')).read().strip()\n        return sentence\n\n    def get_aspect_spans(self, id):\n        aspect_spans = self.aspect_span_dict[id]['aspect_spans']\n        return aspect_spans\n\n    def get_opinion_spans(self, id):\n        opinion_spans = self.opinion_span_dict[id]['opinion_spans']\n        return opinion_spans\n\n    def get_cls(self, id):\n        cls_prob = np.load(\n            os.path.join(self.img_region_dir + '/_cls_again',\n                         id + '.npz'))['feat']\n        # _cls = np.argmax(cls_prob, axis=-1)\n        return cls_prob\n\n    def __getitem__(self, index):\n        output = {}\n        data_id = self.idx2id[index]\n        region_feat, box = self.get_img_region_box(data_id)\n        img_feature = region_feat\n        output['img_feat'] = img_feature\n\n        sentence = self.get_sentence(data_id)\n        output['sentence'] = sentence\n\n        ANP_dis, ANP_words = self.get_img_ANP(index)\n        output['ANP_dis'] = ANP_dis\n        output['ANP_words'] = ANP_words\n\n        sentiment = self.get_sentiment(data_id)\n        output['sentiment'] = sentiment\n\n        aspect_spans = self.get_aspect_spans(data_id)\n        output['aspect_spans'] = aspect_spans\n\n        opinion_spans = self.get_opinion_spans(data_id)\n        output['opinion_spans'] = opinion_spans\n        output['cls'] = self.get_cls(data_id)\n        output['image_id'] = data_id\n        output['gt'] = None\n        return output", "\nclass MVSA_Dataset(data.Dataset):\n    def __init__(self, infos):\n        # print(infos)\n        infos = json.load(open(infos, 'r'))\n        self.text_dir = infos['text_dir']\n        self.img_region_dir = infos['img_region_dir']\n        self.senti_dir = infos['senti_dir']\n        # self.BIO_dir = infos['BIO_dir']\n        self.aspect_span_dict = json.load(open(infos['aspect_span_path'], 'r'))\n        self.opinion_span_dict = json.load(\n            open(infos['opinion_span_path'], 'r'))\n        self.ANP_dir = infos['ANP_dir']\n        self.ANP_class_dir = infos['ANP_class_dir']\n        self.ANP_class = json.load(open(self.ANP_class_dir, 'r'))\n        self.ANP_class = {i: anp for i, anp in enumerate(self.ANP_class)}\n        self.ANP2idx = {anp: idx for idx, anp in self.ANP_class.items()}\n        self.ANP_len = len(self.ANP_class)\n        self.id2senti = json.load(open(self.senti_dir, 'r'))\n        self.idx2ANP = json.load(open(self.ANP_dir, 'r'))['images']\n\n        self.create_id2idx()\n\n    def __len__(self):\n        return len(self.ids)\n\n    def create_id2idx(self):\n        ignore = [\n            '3151', '3910', '5995'\n        ]  #Pictures of these ids can not be opened, so we remove them.\n        self.ids = list(sorted(self.id2senti.keys(), key=lambda x: int(x)))\n        for x in ignore:\n            self.ids.remove(x)\n        self.idx2id = {i: id for i, id in enumerate(self.ids)}\n\n    def get_img_region_box(self, id):\n        region_feat = np.load(\n            os.path.join(self.img_region_dir + '/_att', id + '.npz'))['feat']\n        box = np.load(os.path.join(self.img_region_dir + '/_box', id + '.npy'))\n\n        return region_feat, box\n\n    def process_ANP_distribution(self, distribution):\n        result = np.empty([1, self.ANP_len], dtype=float)\n        for anp, prob in distribution.items():\n            result[0, self.ANP2idx[anp]] = prob\n\n        return result\n\n    def get_ANP_word(self, distribution):\n        anp_word = list(distribution.items())[0][0].replace('_', ' ')\n        # print(anp_word)\n        return anp_word\n\n    def get_img_ANP(self, idx):\n        distribution = self.idx2ANP[idx]['bi-concepts']\n        words = self.get_ANP_word(distribution)\n        dis = self.process_ANP_distribution(distribution)\n\n        return dis, words\n\n    def get_sentiment(self, id):\n        sentiment = self.id2senti[id]\n        return sentiment\n\n    def get_sentence(self, id):\n        sentence = open(os.path.join(self.text_dir,\n                                     id + '.txt')).read().strip()\n        return sentence\n\n    def get_aspect_spans(self, id):\n        aspect_spans = self.aspect_span_dict[id]['aspect_spans']\n        return aspect_spans\n\n    def get_opinion_spans(self, id):\n        opinion_spans = self.opinion_span_dict[id]['opinion_spans']\n        return opinion_spans\n\n    def get_cls(self, id):\n        cls_prob = np.load(\n            os.path.join(self.img_region_dir + '/_cls_again',\n                         id + '.npz'))['feat']\n        # _cls = np.argmax(cls_prob, axis=-1)\n        return cls_prob\n\n    def __getitem__(self, index):\n        output = {}\n        data_id = self.idx2id[index]\n        region_feat, box = self.get_img_region_box(data_id)\n        img_feature = region_feat\n        output['img_feat'] = img_feature\n\n        sentence = self.get_sentence(data_id)\n        output['sentence'] = sentence\n\n        ANP_dis, ANP_words = self.get_img_ANP(index)\n        output['ANP_dis'] = ANP_dis\n        output['ANP_words'] = ANP_words\n\n        sentiment = self.get_sentiment(data_id)\n        output['sentiment'] = sentiment\n\n        aspect_spans = self.get_aspect_spans(data_id)\n        output['aspect_spans'] = aspect_spans\n\n        opinion_spans = self.get_opinion_spans(data_id)\n        output['opinion_spans'] = opinion_spans\n        output['cls'] = self.get_cls(data_id)\n        output['image_id'] = data_id\n        output['gt'] = None\n        return output", "\n\nclass Twitter_Dataset(data.Dataset):\n    def __init__(self, infos, split, image_model_name='nf_resnet50'):\n        self.infos = json.load(open(infos, 'r'))\n\n        if split == 'train':\n            self.data_set = json.load(\n                open(self.infos['data_dir'] + '/train.json', 'r'))\n            self.img_region_dir = self.infos['img_region_dir'] + '/train'\n        elif split == 'dev':\n            self.data_set = json.load(\n                open(self.infos['data_dir'] + '/dev.json', 'r'))\n            self.img_region_dir = self.infos['img_region_dir'] + '/dev'\n        elif split == 'test':\n            self.data_set = json.load(\n                open(self.infos['data_dir'] + '/test.json', 'r'))\n            self.img_region_dir = self.infos['img_region_dir'] + '/test'\n        else:\n            raise RuntimeError(\"split type is not exist!!!\")\n\n        self.image_model_name = image_model_name\n        self.image_transform = self.get_image_transform(self.image_model_name)\n\n    def __len__(self):\n        return len(self.data_set)\n\n    def get_img_region_box(self, id):\n        region_feat = np.load(\n            os.path.join(self.img_region_dir + '/_att',\n                         id[:-4] + '.npz'))['feat']\n        box = np.load(\n            os.path.join(self.img_region_dir + '/_box', id[:-4] + '.npy'))\n\n        return region_feat, box\n\n    def is_clip_model(self, model_name):\n        return model_name.startswith('openai/clip-')\n\n    def get_image_transform(self, model_name):\n        if model_name in TIMM_CONFIGS.keys():\n            config = TIMM_CONFIGS[model_name]\n            transform = create_transform(**config)\n            transform.transforms.append(\n                Lambda(lambda x: x.unsqueeze(0)),\n            )\n        elif self.is_clip_model(model_name):\n            transform = CLIPFeatureExtractor.from_pretrained(model_name)\n        else:\n            transform = AutoFeatureExtractor.from_pretrained(model_name)\n        return transform\n\n    def _read_image(self, image_path):\n        raw = Image.open(image_path)\n        raw = raw.convert('RGB') if raw.mode != 'RGB' else raw\n        if isinstance(self.image_transform, Compose):\n            image = self.image_transform(raw)\n        elif self.image_transform is not None:  # HuggingFace\n            image = self.image_transform(raw, return_tensors='pt')\n            image = image['pixel_values']\n        return image\n\n    def get_aesc_spans(self, dic):\n        aesc_spans = []\n        for x in dic:\n            aesc_spans.append((x['from'], x['to'], x['polarity']))\n        return aesc_spans\n\n    def get_gt_aspect_senti(self, dic):\n        gt = []\n        for x in dic:\n            gt.append((' '.join(x['term']), x['polarity']))\n        return gt\n\n    def __getitem__(self, index):\n        output = {}\n        data = self.data_set[index]\n        img_id = data['image_id']\n        output['sentence'] = ' '.join(data['words'])\n        aesc_spans = self.get_aesc_spans(data['aspects'])\n        output['aesc_spans'] = aesc_spans\n        gt = self.get_gt_aspect_senti(data['aspects'])\n        output['gt'] = gt\n\n        output['image_id'] = img_id\n        output['caption'] = data['caption']\n        image_path = data['image_path']\n        image_pixel_values = self._read_image(image_path)\n        output['image_pixel_values'] = image_pixel_values.squeeze()\n        output['aspects_num'] = data['aspects_num']\n\n        # import ipdb; ipdb.set_trace()\n        # print(\"-----------------------output-------------------------\")\n        # print(output)\n        return output", "\n# if __name__ == '__main__':\n#     dataset = '/home/xiaocui/code/VLP-MABSA/src/data/jsons/twitter15_info.json'\n#     dev_dataset = Twitter_Dataset(dataset, split='dev')\n#     dev_loader = DataLoader(dataset=dev_dataset,\n#                             batch_size=4,\n#                             shuffle=False,\n#                             num_workers=2,\n#                             pin_memory=True,\n#                             collate_fn=collate_aesc)", "#                             pin_memory=True,\n#                             collate_fn=collate_aesc)"]}
{"filename": "src/model/MAESC_model_for_generated_dual_prompts_multitasks_Aspect.py", "chunked_list": ["from typing import Optional, Tuple\nfrom fastNLP.modules.torch.encoder import Seq2SeqEncoder\nfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\nfrom fastNLP.modules.torch import State\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom src.model.modeling_bart import (PretrainedBartModel, BartEncoder,\n                                     BartDecoder, BartModel,\n                                     BartClassificationHead,", "                                     BartDecoder, BartModel,\n                                     BartClassificationHead,\n                                     _make_linear_from_emb,\n                                     _prepare_bart_decoder_inputs)\nfrom transformers import BartTokenizer\n\nfrom src.model.config import MultiModalBartConfig\n#from src.model.mixins import GenerationMixin, FromPretrainedMixin\nfrom src.model.modules_for_prompt_multitasks import MultiModalBartEncoder, MultiModalBartDecoder_span, Span_loss, MultiModalBartEncoder_for_Generating_Dual_prompts, MultiModalBartDecoder_generate_sentiment_prompt, MultiModalBartDecoder_generate_aspect_prompt\nfrom src.model.modules_for_prompt_multitasks import MultiModalBartDecoder_aspects_num ", "from src.model.modules_for_prompt_multitasks import MultiModalBartEncoder, MultiModalBartDecoder_span, Span_loss, MultiModalBartEncoder_for_Generating_Dual_prompts, MultiModalBartDecoder_generate_sentiment_prompt, MultiModalBartDecoder_generate_aspect_prompt\nfrom src.model.modules_for_prompt_multitasks import MultiModalBartDecoder_aspects_num \n\n\nclass MultiModalBartModel_AESC(PretrainedBartModel):\n    def build_model(self,\n                    args,\n                    bart_model,\n                    tokenizer,\n                    label_ids,\n                    config,\n                    decoder_type=None,\n                    copy_gate=False,\n                    use_encoder_mlp=False,\n                    use_recur_pos=False,\n                    tag_first=False):\n        if args.bart_init:\n            model = BartModel.from_pretrained(bart_model)\n            num_tokens, _ = model.encoder.embed_tokens.weight.shape\n            print('num_tokens', num_tokens)\n\n            model.resize_token_embeddings(\n                len(tokenizer.unique_no_split_tokens) + num_tokens)\n            encoder = model.encoder\n            decoder = model.decoder\n\n            padding_idx = config.pad_token_id\n            encoder.embed_tokens.padding_idx = padding_idx\n\n            # if use_recur_pos:\n            #     decoder.set_position_embedding(label_ids[0], tag_first)\n\n            _tokenizer = BartTokenizer.from_pretrained(bart_model)\n\n            for token in tokenizer.unique_no_split_tokens:\n                if token[:2] == '<<':  # \u7279\u6b8a\u5b57\u7b26\n                    index = tokenizer.convert_tokens_to_ids(\n                        tokenizer._base_tokenizer.tokenize(token))\n                    if len(index) > 1:\n                        raise RuntimeError(f\"{token} wrong split\")\n                    else:\n                        index = index[0]\n                    assert index >= num_tokens, (index, num_tokens, token)\n                    indexes = _tokenizer.convert_tokens_to_ids(\n                        _tokenizer.tokenize(token[2:-2]))\n                    embed = model.encoder.embed_tokens.weight.data[indexes[0]]\n                    for i in indexes[1:]:\n                        embed += model.decoder.embed_tokens.weight.data[i]\n                    embed /= len(indexes)\n                    model.decoder.embed_tokens.weight.data[index] = embed\n        else:\n            raise RuntimeError(\"error init!!!!!!!\")\n\n        multimodal_encoder_for_generated_aspect_prompt = MultiModalBartEncoder(config, encoder,\n                                                   tokenizer.img_feat_id,\n                                                   tokenizer.cls_token_id,\n                                                   args.num_image_tokens)\n\n        multimodal_encoder_for_generated_aspects_num = MultiModalBartEncoder(config, encoder,\n                                                   tokenizer.img_feat_id,\n                                                   tokenizer.cls_token_id,\n                                                   args.num_image_tokens)\n        \n        multimodal_encoder_for_generated_senti_prompt = MultiModalBartEncoder(config, encoder,\n                                                   tokenizer.img_feat_id,\n                                                   tokenizer.cls_token_id,\n                                                   args.num_image_tokens)\n\n\n        multimodal_encoder = MultiModalBartEncoder_for_Generating_Dual_prompts(\n                                                                         use_generated_aspect_prompt=args.use_generated_aspect_prompt, \n                                                                         use_generated_senti_prompt=args.use_generated_senti_prompt, \n                                                                         config=config, \n                                                                         encoder = encoder,\n                                                                         img_feat_id = tokenizer.img_feat_id,\n                                                                         aspect_prompt_token_id=tokenizer.aspect_prompt_token_id,\n                                                                         senti_prompt_token_id=tokenizer.senti_prompt_token_id,\n                                                                         cls_token_id = tokenizer.cls_token_id,\n                                                                         num_image_tokens = args.num_image_tokens,\n                                                                         use_different_aspect_prompt = args.use_different_aspect_prompt, use_different_senti_prompt=args.use_different_senti_prompt,\n                                                                         NEU_id = tokenizer.neu_token_id,\n                                                                         POS_id = tokenizer.pos_token_id,\n                                                                         NEG_id = tokenizer.neg_token_id)\n        return (multimodal_encoder_for_generated_aspect_prompt, multimodal_encoder_for_generated_aspects_num, multimodal_encoder_for_generated_senti_prompt, multimodal_encoder, decoder)\n\n    def __init__(self, config: MultiModalBartConfig, args, bart_model,\n                 tokenizer, label_ids):\n        super().__init__(config)\n        self.config = config\n        self.tokenizer = tokenizer\n        label_ids = sorted(label_ids)\n        multimodal_encoder_for_generated_aspect_prompt, multimodal_encoder_for_generated_aspects_num, multimodal_encoder_for_generated_senti_prompt, multimodal_encoder, share_decoder = self.build_model(\n            args, bart_model, self.tokenizer, label_ids, config)\n        causal_mask = torch.zeros(512, 512).fill_(float('-inf'))\n        self.causal_mask = causal_mask.triu(diagonal=1)\n\n        self.aspect_prompt_encoder = multimodal_encoder_for_generated_aspect_prompt\n        self.aspects_num_encoder = multimodal_encoder_for_generated_aspects_num\n        self.senti_prompt_encoder = multimodal_encoder_for_generated_senti_prompt\n        self.encoder = multimodal_encoder\n        self.use_generated_senti_prompt = args.use_generated_senti_prompt\n        self.use_multitasks = args.use_multitasks\n        self.num_image_tokens = args.num_image_tokens\n        self.loss_lambda = args.loss_lambda\n\n\n        only_sc = False\n        # need_tag = True  #if predict the sentiment or not\n        if args.task == 'twitter_ae':\n            need_tag = False\n        else:\n            need_tag = True\n            # if args.task == 'twitter_sc':\n            #     only_sc = True\n\n        self.aspect_prompt_decoder = MultiModalBartDecoder_generate_aspect_prompt(self.config, share_decoder)\n        if self.use_generated_senti_prompt:\n            self.senti_prompt_decoder = MultiModalBartDecoder_generate_sentiment_prompt(self.config, share_decoder)\n        if self.use_multitasks:\n            self.aspect_num_decoder = MultiModalBartDecoder_aspects_num(self.config, share_decoder)\n\n\n        self.decoder = MultiModalBartDecoder_span(self.config,\n                                                  self.tokenizer,\n                                                  share_decoder,\n                                                  self.tokenizer.pad_token_id,\n                                                  label_ids,\n                                                  self.causal_mask,\n                                                  num_image_tokens=self.num_image_tokens,\n                                                  need_tag=need_tag,\n                                                  only_sc=False)\n        self.span_loss_fct = Span_loss()\n        # self.aspect_num_linear = nn.Linear(768, 5) ##max_aspects_num=5\n\n    def prepare_state(self,\n                      input_ids,\n                      image_features,\n                      attention_mask=None,\n                      aesc_infos=None,\n                      aspects_nums=None,\n                      first=None):\n        ##generate prompt for each instance\n        \n        prompt_attention_mask = attention_mask\n        # import ipdb; ipdb.set_trace()\n        if self.num_image_tokens==4:\n            end_index = 66\n            begin_index = 26\n        elif self.num_image_tokens==3:\n            end_index = 65\n            begin_index = 25\n        elif self.num_image_tokens==2:\n            end_index = 64\n            begin_index = 24\n        elif self.num_image_tokens==1:\n            end_index = 63\n            begin_index = 23\n        elif self.num_image_tokens==0:\n            end_index = 62\n            begin_index = 22\n        \n        for i in range(len(prompt_attention_mask)):\n            mask = prompt_attention_mask[i]\n            mask[begin_index:end_index]=torch.zeros_like(mask[begin_index:end_index]) ##26:66 \u662faspect\u63d0\u793a\u7684\u4f4d\u7f6e\n            prompt_attention_mask[i]=mask\n        '''dict_for_senti_prompt'''\n        dict_for_aspect_prompt = self.aspect_prompt_encoder(input_ids=input_ids,\n                                              image_features=image_features,\n                                              attention_mask=prompt_attention_mask,\n                                              output_hidden_states=True,\n                                              return_dict=True)\n        \n        dict_for_aspects_num = self.aspects_num_encoder(input_ids=input_ids,\n                                              image_features=image_features,\n                                              attention_mask=prompt_attention_mask,\n                                              output_hidden_states=True,\n                                              return_dict=True)\n\n        dict_for_senti_prompt = self.senti_prompt_encoder(input_ids=input_ids,\n                                              image_features=image_features,\n                                              attention_mask=prompt_attention_mask,\n                                              output_hidden_states=True,\n                                              return_dict=True)\n        '''generated_aspect_prompt'''\n        aspect_prompt_decoder_input_ids, aspect_prompt_decoder_attention_mask = [\n            aesc_infos['aspect_prompt_decoder_input_ids'].to(input_ids.device),\n            aesc_infos['aspect_prompt_decoder_attention_mask'].to(input_ids.device)]\n        generated_aspect_prompt = self.aspect_prompt_decoder(\n                                            encoder_outputs=dict_for_aspect_prompt.last_hidden_state, \n                                            attention_mask=attention_mask,\n                                            decoder_input_ids =aspect_prompt_decoder_input_ids, decoder_attention_mask=aspect_prompt_decoder_attention_mask)\n        generated_aspect_prompt = generated_aspect_prompt[:, 1:, :] ##(batch_size, 2, 768)\n        \n        # import ipdb; ipdb.set_trace()\n        '''aspects_num'''\n        aspects_num_decoder_input_ids, aspects_num_decoder_attention_mask = [\n            aesc_infos['aspects_num_decoder_input_ids'].to(input_ids.device),\n            aesc_infos['aspects_num_decoder_attention_mask'].to(input_ids.device)]\n\n        # import ipdb; ipdb.set_trace()\n        if self.use_multitasks:\n            aspects_num_loss, predict_aspects_num_logits = self.aspect_num_decoder(aspects_num_labels=aspects_nums,\n                                                                            encoder_outputs=dict_for_aspects_num[0], \n                                                                            attention_mask=attention_mask,\n                                                                            aspects_num_decoder_input_ids=aspects_num_decoder_input_ids)\n\n\n            predict_aspects_num = torch.argmax(predict_aspects_num_logits, dim=1)\n            new_predict_aspects_num = predict_aspects_num + torch.ones_like(predict_aspects_num)\n        else:\n            aspects_num_loss =0\n            new_predict_aspects_num = []\n            predict_aspects_num = []\n            for i in range(len(input_ids)):\n                new_predict_aspects_num.append(5)\n                predict_aspects_num.append(4)\n            new_predict_aspects_num = torch.tensor(new_predict_aspects_num)\n            predict_aspects_num = torch.tensor(predict_aspects_num)\n\n        generated_senti_prompt = None\n\n        if self.use_generated_senti_prompt:\n            senti_prompt_decoder_input_ids, senti_prompt_decoder_attention_mask = [\n                aesc_infos['senti_prompt_decoder_input_ids'].to(input_ids.device),\n                aesc_infos['senti_prompt_decoder_attention_mask'].to(input_ids.device)]\n            generated_senti_prompt = self.senti_prompt_decoder(\n                                                encoder_outputs=dict_for_senti_prompt.last_hidden_state, \n                                                attention_mask=attention_mask,\n                                                decoder_input_ids =senti_prompt_decoder_input_ids, decoder_attention_mask=senti_prompt_decoder_attention_mask)\n\n            generated_senti_prompt = generated_senti_prompt[:, 1:, :] ##(batch_size, 2, 768)\n\n        dict = self.encoder(\n                            input_ids=input_ids,\n                            image_features=image_features,\n                            attention_mask=attention_mask,\n                            generated_aspect_prompt= generated_aspect_prompt,\n                            generated_senti_prompt=generated_senti_prompt,\n                            aspects_num = new_predict_aspects_num,\n                            output_hidden_states=True,\n                            return_dict=True)\n\n\n        encoder_outputs = dict.last_hidden_state\n        hidden_states = dict.hidden_states\n        encoder_mask = attention_mask\n        src_embed_outputs = hidden_states[0]\n        state = BartState(\n            encoder_outputs,\n            encoder_mask,\n            input_ids[:,\n                      end_index:],  #the text features start from index 38, the front are image features.\n            first,\n            src_embed_outputs)\n        # setattr(state, 'tgt_seq_len', tgt_seq_len)\n        return state, aspects_num_loss, predict_aspects_num\n\n    def forward(\n            self,\n            input_ids,\n            image_features,\n            attention_mask=None,\n            aesc_infos=None,\n            aspects_num=None,\n            encoder_outputs: Optional[Tuple] = None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n    ):\n        ### for prompt\n        # import ipdb; ipdb.set_trace()\n       \n        ## for aspect-spans\n      \n        aspects_num = torch.tensor(aspects_num).to(input_ids.device)\n        state, aspects_num_loss, predict_aspects_num = self.prepare_state( input_ids, image_features, attention_mask, aesc_infos, aspects_num)\n        spans, span_mask = [ \n            aesc_infos['labels'].to(input_ids.device),\n            aesc_infos['masks'].to(input_ids.device)\n        ]\n\n        logits = self.decoder(spans, state) ## spans: (2, 13) logits: (2, 12, 40)\n\n        span_loss = self.span_loss_fct(spans[:, 1:], logits, span_mask[:, 1:])\n\n        all_loss = span_loss + self.loss_lambda*aspects_num_loss\n\n        return all_loss, predict_aspects_num", "\n\nclass BartState(State):\n    def __init__(self, encoder_output, encoder_mask, src_tokens, first,\n                 src_embed_outputs):\n        super().__init__(encoder_output, encoder_mask)\n        self.past_key_values = None\n        self.src_tokens = src_tokens\n        self.first = first\n        self.src_embed_outputs = src_embed_outputs\n\n    def reorder_state(self, indices: torch.LongTensor):\n        super().reorder_state(indices)\n        self.src_tokens = self._reorder_state(self.src_tokens, indices)\n        if self.first is not None:\n            self.first = self._reorder_state(self.first, indices)\n        self.src_embed_outputs = self._reorder_state(self.src_embed_outputs,\n                                                     indices)\n        if self.past_key_values is not None:\n            new = []\n            for layer in self.past_key_values:\n                new_layer = {}\n                for key1 in list(layer.keys()):\n                    new_layer_ = {}\n                    for key2 in list(layer[key1].keys()):\n                        if layer[key1][key2] is not None:\n                            layer[key1][key2] = self._reorder_state(\n                                layer[key1][key2], indices)\n                            # print(key1, key2, layer[key1][key2].shape)\n                        new_layer_[key2] = layer[key1][key2]\n                    new_layer[key1] = new_layer_\n                new.append(new_layer)\n            self.past_key_values = new", ""]}
{"filename": "src/model/generater_for_generated_prompt.py", "chunked_list": ["r\"\"\"undocumented\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom fastNLP.models.torch.seq2seq_model import Seq2SeqModel\nfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\nimport torch.nn.functional as F\n# from fastNLP.core.utils import _get_model_device\nfrom functools import partial", "# from fastNLP.core.utils import _get_model_device\nfrom functools import partial\n\ndef _get_model_device(model):\n    r\"\"\"\n    \u4f20\u5165\u4e00\u4e2ann.Module\u7684\u6a21\u578b\uff0c\u83b7\u53d6\u5b83\u6240\u5728\u7684device\n\n    :param model: nn.Module\n    :return: torch.device,None \u5982\u679c\u8fd4\u56de\u503c\u4e3aNone\uff0c\u8bf4\u660e\u8fd9\u4e2a\u6a21\u578b\u6ca1\u6709\u4efb\u4f55\u53c2\u6570\u3002\n    \"\"\"\n    # TODO \u8fd9\u4e2a\u51fd\u6570\u5b58\u5728\u4e00\u5b9a\u7684\u98ce\u9669\uff0c\u56e0\u4e3a\u540c\u4e00\u4e2a\u6a21\u578b\u53ef\u80fd\u5b58\u5728\u67d0\u4e9bparameter\u4e0d\u5728\u663e\u5361\u4e2d\uff0c\u6bd4\u5982BertEmbedding. \u6216\u8005\u8de8\u663e\u5361\n    assert isinstance(model, nn.Module)\n\n    parameters = list(model.parameters())\n    if len(parameters) == 0:\n        return None\n    else:\n        return parameters[0].device", "\n\n\nclass SequenceGeneratorModel(nn.Module):\n    \"\"\"\n    \u7528\u4e8e\u5c01\u88c5Seq2SeqModel\u4f7f\u5176\u53ef\u4ee5\u505a\u751f\u6210\u4efb\u52a1\n\n    \"\"\"\n    def __init__(self,\n                 seq2seq_model: Seq2SeqModel,\n                 bos_token_id,\n                 eos_token_id=None,\n                 max_length=30,\n                 max_len_a=0.0,\n                 num_beams=1,\n                 do_sample=True,\n                 sc_only=False,\n                 repetition_penalty=1,\n                 length_penalty=1.0,\n                 pad_token_id=0,\n                 restricter=None):\n        \"\"\"\n\n        :param Seq2SeqModel seq2seq_model: \u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b. \u4f1a\u4f7f\u7528seq2seq_model\u7684decoder\u8fdb\u884c\u751f\u6210\n        :param int,None bos_token_id: \u53e5\u5b50\u5f00\u5934\u7684token id\n        :param int,None eos_token_id: \u53e5\u5b50\u7ed3\u675f\u7684token id\n        :param int max_length: \u751f\u6210\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6, \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\n        :param float max_len_a: \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\u3002 \u5982\u679c\u4e0d\u4e3a0\uff0c\u9700\u8981\u4fdd\u8bc1State\u4e2d\u5305\u542bencoder_mask\n        :param int num_beams: beam search\u7684\u5927\u5c0f\n        :param bool do_sample: \u662f\u5426\u901a\u8fc7\u91c7\u6837\u7684\u65b9\u5f0f\u751f\u6210\n        :param float temperature: \u53ea\u6709\u5728do_sample\u4e3aTrue\u624d\u6709\u610f\u4e49\n        :param int top_k: \u53ea\u4ecetop_k\u4e2d\u91c7\u6837\n        :param float top_p: \u53ea\u4ecetop_p\u7684token\u4e2d\u91c7\u6837\uff0cnucles sample\n        :param float repetition_penalty: \u591a\u5927\u7a0b\u5ea6\u4e0a\u60e9\u7f5a\u91cd\u590d\u7684token\n        :param float length_penalty: \u5bf9\u957f\u5ea6\u7684\u60e9\u7f5a\uff0c\u5c0f\u4e8e1\u9f13\u52b1\u957f\u53e5\uff0c\u5927\u4e8e1\u9f13\u52b1\u77ed\u5267\n        :param int pad_token_id: \u5f53\u67d0\u53e5\u8bdd\u751f\u6210\u7ed3\u675f\u4e4b\u540e\uff0c\u4e4b\u540e\u751f\u6210\u7684\u5185\u5bb9\u7528pad_token_id\u8865\u5145\n        \"\"\"\n        super().__init__()\n        self.seq2seq_model = seq2seq_model\n        self.restricter = restricter\n        self.sc_only = sc_only\n        self.generator = SequenceGenerator(\n            seq2seq_model.decoder,\n            max_length=max_length,\n            max_len_a=max_len_a,\n            num_beams=num_beams,\n            do_sample=do_sample,\n            sc_only=sc_only,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            repetition_penalty=repetition_penalty,\n            length_penalty=length_penalty,\n            pad_token_id=pad_token_id,\n            restricter=restricter)\n\n    def forward(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                aesc_infos=None,\n                aspects_num=None,\n                first=None):\n        \"\"\"\n        \u900f\u4f20\u8c03\u7528seq2seq_model\u7684forward\n\n        :param torch.LongTensor src_tokens: bsz x max_len\n        :param torch.LongTensor tgt_tokens: bsz x max_len'\n        :param torch.LongTensor src_seq_len: bsz\n        :param torch.LongTensor tgt_seq_len: bsz\n        :return:\n        \"\"\"\n        \n        return self.seq2seq_model(input_ids=input_ids,\n                                  image_features=image_features,\n                                  attention_mask=attention_mask,\n                                  aesc_infos=aesc_infos,\n                                  aspects_num=aspects_num)\n\n    def predict(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                aesc_infos=None,\n                aspects_num=None):\n        \"\"\"\n        \u7ed9\u5b9asource\u7684\u5185\u5bb9\uff0c\u8f93\u51fagenerate\u7684\u5185\u5bb9\n\n        :param torch.LongTensor src_tokens: bsz x max_len\n        :param torch.LongTensor src_seq_len: bsz\n        :return:\n        \"\"\"\n        # import ipdb; ipdb.set_trace()\n        state  = self.seq2seq_model.prepare_state(\n                                                 input_ids, image_features,\n                                                 attention_mask,\n                                                 aesc_infos,\n                                                 aspects_num)\n        tgt_tokens = aesc_infos['labels'].to(input_ids.device)\n        # print()\n        result = self.generator.generate(\n            state,\n            tokens=tgt_tokens[:, :3])  # the prompt is provided to the model\n        return result", "\n\nr\"\"\"\n\n\"\"\"\n\n__all__ = ['SequenceGenerator']\n\n\nclass SequenceGenerator:\n    \"\"\"\n    \u7ed9\u5b9a\u4e00\u4e2aSeq2SeqDecoder\uff0cdecode\u51fa\u53e5\u5b50\n\n    \"\"\"\n    def __init__(self,\n                 decoder: Seq2SeqDecoder,\n                 max_length=20,\n                 max_len_a=0.0,\n                 num_beams=1,\n                 do_sample=False,\n                 sc_only=False,\n                 bos_token_id=None,\n                 eos_token_id=None,\n                 repetition_penalty=1,\n                 length_penalty=1.0,\n                 pad_token_id=0,\n                 restricter=None):\n        \"\"\"\n\n        :param Seq2SeqDecoder decoder: Decoder\u5bf9\u8c61\n        :param int max_length: \u751f\u6210\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6, \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\n        :param float max_len_a: \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\u3002 \u5982\u679c\u4e0d\u4e3a0\uff0c\u9700\u8981\u4fdd\u8bc1State\u4e2d\u5305\u542bencoder_mask\n        :param int num_beams: beam search\u7684\u5927\u5c0f\n        :param bool do_sample: \u662f\u5426\u901a\u8fc7\u91c7\u6837\u7684\u65b9\u5f0f\u751f\u6210\n        :param float temperature: \u53ea\u6709\u5728do_sample\u4e3aTrue\u624d\u6709\u610f\u4e49\n        :param int top_k: \u53ea\u4ecetop_k\u4e2d\u91c7\u6837\n        :param float top_p: \u53ea\u4ecetop_p\u7684token\u4e2d\u91c7\u6837\uff0cnucles sample\n        :param int,None bos_token_id: \u53e5\u5b50\u5f00\u5934\u7684token id\n        :param int,None eos_token_id: \u53e5\u5b50\u7ed3\u675f\u7684token id\n        :param float repetition_penalty: \u591a\u5927\u7a0b\u5ea6\u4e0a\u60e9\u7f5a\u91cd\u590d\u7684token\n        :param float length_penalty: \u5bf9\u957f\u5ea6\u7684\u60e9\u7f5a\uff0c\u5c0f\u4e8e1\u9f13\u52b1\u957f\u53e5\uff0c\u5927\u4e8e1\u9f13\u52b1\u77ed\u5267\n        :param int pad_token_id: \u5f53\u67d0\u53e5\u8bdd\u751f\u6210\u7ed3\u675f\u4e4b\u540e\uff0c\u4e4b\u540e\u751f\u6210\u7684\u5185\u5bb9\u7528pad_token_id\u8865\u5145\n        \"\"\"\n        self.generate_func = partial(greedy_generate,\n                                     decoder=decoder,\n                                     max_length=max_length,\n                                     max_len_a=max_len_a,\n                                     num_beams=num_beams,\n                                     sc_only=sc_only,\n                                     bos_token_id=bos_token_id,\n                                     eos_token_id=eos_token_id,\n                                     repetition_penalty=repetition_penalty,\n                                     length_penalty=length_penalty,\n                                     pad_token_id=pad_token_id,\n                                     restricter=restricter)\n        self.do_sample = do_sample\n        self.max_length = max_length\n        self.num_beams = num_beams\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n        self.repetition_penalty = repetition_penalty\n        self.length_penalty = length_penalty\n        self.decoder = decoder\n        self.pad_token_id = pad_token_id\n        self.restricter = restricter\n        self.max_len_a = max_len_a\n\n    def set_new_generator(self,\n                          max_length=-1,\n                          max_len_a=-1,\n                          num_beams=-1,\n                          repetition_penalty=-1,\n                          length_penalty=-1,\n                          restricter=-1):\n        if max_length == -1:\n            max_length = self.max_length\n        if max_len_a == -1:\n            max_len_a = self.max_len_a\n        if num_beams == -1:\n            num_beams = self.num_beams\n        if repetition_penalty == -1:\n            repetition_penalty = self.repetition_penalty\n        if length_penalty == -1:\n            length_penalty = self.length_penalty\n        if restricter == -1:\n            restricter = self.restricter\n        self.generate_func = partial(greedy_generate,\n                                     decoder=self.decoder,\n                                     max_length=max_length,\n                                     max_len_a=max_len_a,\n                                     num_beams=num_beams,\n                                     sc_only=sc_only,\n                                     bos_token_id=self.bos_token_id,\n                                     eos_token_id=self.eos_token_id,\n                                     repetition_penalty=repetition_penalty,\n                                     length_penalty=length_penalty,\n                                     pad_token_id=self.pad_token_id,\n                                     restricter=restricter)\n\n    @torch.no_grad()\n    def generate(self, state, tokens=None, gt_tokens=None):\n        \"\"\"\n\n        :param State state: encoder\u7ed3\u679c\u7684State, \u662f\u4e0eDecoder\u914d\u5957\u662f\u7528\u7684\n        :param torch.LongTensor,None tokens: batch_size x length, \u5f00\u59cb\u7684token\n        :return: bsz x max_length' \u751f\u6210\u7684token\u5e8f\u5217\u3002\u5982\u679ceos_token_id\u4e0d\u4e3aNone, \u6bcf\u4e2asequence\u7684\u7ed3\u5c3e\u4e00\u5b9a\u662feos_token_id\n        \"\"\"\n\n        return self.generate_func(tokens=tokens,\n                                  gt_tokens=gt_tokens,\n                                  state=state)", "\nclass SequenceGenerator:\n    \"\"\"\n    \u7ed9\u5b9a\u4e00\u4e2aSeq2SeqDecoder\uff0cdecode\u51fa\u53e5\u5b50\n\n    \"\"\"\n    def __init__(self,\n                 decoder: Seq2SeqDecoder,\n                 max_length=20,\n                 max_len_a=0.0,\n                 num_beams=1,\n                 do_sample=False,\n                 sc_only=False,\n                 bos_token_id=None,\n                 eos_token_id=None,\n                 repetition_penalty=1,\n                 length_penalty=1.0,\n                 pad_token_id=0,\n                 restricter=None):\n        \"\"\"\n\n        :param Seq2SeqDecoder decoder: Decoder\u5bf9\u8c61\n        :param int max_length: \u751f\u6210\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6, \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\n        :param float max_len_a: \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\u3002 \u5982\u679c\u4e0d\u4e3a0\uff0c\u9700\u8981\u4fdd\u8bc1State\u4e2d\u5305\u542bencoder_mask\n        :param int num_beams: beam search\u7684\u5927\u5c0f\n        :param bool do_sample: \u662f\u5426\u901a\u8fc7\u91c7\u6837\u7684\u65b9\u5f0f\u751f\u6210\n        :param float temperature: \u53ea\u6709\u5728do_sample\u4e3aTrue\u624d\u6709\u610f\u4e49\n        :param int top_k: \u53ea\u4ecetop_k\u4e2d\u91c7\u6837\n        :param float top_p: \u53ea\u4ecetop_p\u7684token\u4e2d\u91c7\u6837\uff0cnucles sample\n        :param int,None bos_token_id: \u53e5\u5b50\u5f00\u5934\u7684token id\n        :param int,None eos_token_id: \u53e5\u5b50\u7ed3\u675f\u7684token id\n        :param float repetition_penalty: \u591a\u5927\u7a0b\u5ea6\u4e0a\u60e9\u7f5a\u91cd\u590d\u7684token\n        :param float length_penalty: \u5bf9\u957f\u5ea6\u7684\u60e9\u7f5a\uff0c\u5c0f\u4e8e1\u9f13\u52b1\u957f\u53e5\uff0c\u5927\u4e8e1\u9f13\u52b1\u77ed\u5267\n        :param int pad_token_id: \u5f53\u67d0\u53e5\u8bdd\u751f\u6210\u7ed3\u675f\u4e4b\u540e\uff0c\u4e4b\u540e\u751f\u6210\u7684\u5185\u5bb9\u7528pad_token_id\u8865\u5145\n        \"\"\"\n        self.generate_func = partial(greedy_generate,\n                                     decoder=decoder,\n                                     max_length=max_length,\n                                     max_len_a=max_len_a,\n                                     num_beams=num_beams,\n                                     sc_only=sc_only,\n                                     bos_token_id=bos_token_id,\n                                     eos_token_id=eos_token_id,\n                                     repetition_penalty=repetition_penalty,\n                                     length_penalty=length_penalty,\n                                     pad_token_id=pad_token_id,\n                                     restricter=restricter)\n        self.do_sample = do_sample\n        self.max_length = max_length\n        self.num_beams = num_beams\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n        self.repetition_penalty = repetition_penalty\n        self.length_penalty = length_penalty\n        self.decoder = decoder\n        self.pad_token_id = pad_token_id\n        self.restricter = restricter\n        self.max_len_a = max_len_a\n\n    def set_new_generator(self,\n                          max_length=-1,\n                          max_len_a=-1,\n                          num_beams=-1,\n                          repetition_penalty=-1,\n                          length_penalty=-1,\n                          restricter=-1):\n        if max_length == -1:\n            max_length = self.max_length\n        if max_len_a == -1:\n            max_len_a = self.max_len_a\n        if num_beams == -1:\n            num_beams = self.num_beams\n        if repetition_penalty == -1:\n            repetition_penalty = self.repetition_penalty\n        if length_penalty == -1:\n            length_penalty = self.length_penalty\n        if restricter == -1:\n            restricter = self.restricter\n        self.generate_func = partial(greedy_generate,\n                                     decoder=self.decoder,\n                                     max_length=max_length,\n                                     max_len_a=max_len_a,\n                                     num_beams=num_beams,\n                                     sc_only=sc_only,\n                                     bos_token_id=self.bos_token_id,\n                                     eos_token_id=self.eos_token_id,\n                                     repetition_penalty=repetition_penalty,\n                                     length_penalty=length_penalty,\n                                     pad_token_id=self.pad_token_id,\n                                     restricter=restricter)\n\n    @torch.no_grad()\n    def generate(self, state, tokens=None, gt_tokens=None):\n        \"\"\"\n\n        :param State state: encoder\u7ed3\u679c\u7684State, \u662f\u4e0eDecoder\u914d\u5957\u662f\u7528\u7684\n        :param torch.LongTensor,None tokens: batch_size x length, \u5f00\u59cb\u7684token\n        :return: bsz x max_length' \u751f\u6210\u7684token\u5e8f\u5217\u3002\u5982\u679ceos_token_id\u4e0d\u4e3aNone, \u6bcf\u4e2asequence\u7684\u7ed3\u5c3e\u4e00\u5b9a\u662feos_token_id\n        \"\"\"\n\n        return self.generate_func(tokens=tokens,\n                                  gt_tokens=gt_tokens,\n                                  state=state)", "\n\n@torch.no_grad()\ndef greedy_generate(decoder,\n                    tokens=None,\n                    gt_tokens=None,\n                    state=None,\n                    sc_eval=False,\n                    max_length=20,\n                    max_len_a=0.0,\n                    num_beams=1,\n                    sc_only=False,\n                    bos_token_id=None,\n                    eos_token_id=None,\n                    pad_token_id=0,\n                    repetition_penalty=1,\n                    length_penalty=1.0,\n                    restricter=None):\n    \"\"\"\n    \u8d2a\u5a6a\u5730\u641c\u7d22\u53e5\u5b50\n\n    :param Decoder decoder: Decoder\u5bf9\u8c61\n    :param torch.LongTensor tokens: batch_size x len, decode\u7684\u8f93\u5165\u503c\uff0c\u5982\u679c\u4e3aNone\uff0c\u5219\u81ea\u52a8\u4ecebos_token_id\u5f00\u59cb\u751f\u6210\n    :param State state: \u5e94\u8be5\u5305\u542bencoder\u7684\u4e00\u4e9b\u8f93\u51fa\u3002\n    :param int max_length: \u751f\u6210\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6, \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\n    :param float max_len_a: \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\u3002 \u5982\u679c\u4e0d\u4e3a0\uff0c\u9700\u8981\u4fdd\u8bc1State\u4e2d\u5305\u542bencoder_mask\n    :param int num_beams: \u4f7f\u7528\u591a\u5927\u7684beam\u8fdb\u884c\u89e3\u7801\u3002\n    :param int bos_token_id: \u5982\u679ctokens\u4f20\u5165\u4e3aNone\uff0c\u5219\u4f7f\u7528bos_token_id\u5f00\u59cb\u5f80\u540e\u89e3\u7801\u3002\n    :param int eos_token_id: \u7ed3\u675f\u7684token\uff0c\u5982\u679c\u4e3aNone\uff0c\u5219\u4e00\u5b9a\u4f1a\u89e3\u7801\u5230max_length\u8fd9\u4e48\u957f\u3002\n    :param int pad_token_id: pad\u7684token id\n    :param float repetition_penalty: \u5bf9\u91cd\u590d\u51fa\u73b0\u7684token\u591a\u5927\u7684\u60e9\u7f5a\u3002\n    :param float length_penalty: \u5bf9\u6bcf\u4e2atoken\uff08\u9664\u4e86eos\uff09\u6309\u7167\u957f\u5ea6\u8fdb\u884c\u4e00\u5b9a\u7684\u60e9\u7f5a\u3002\n    :return:\n    \"\"\"\n\n    # import ipdb; ipdb.set_trace()\n    if sc_only:\n        token_ids = sc_generate(decoder,\n                                tokens=tokens,\n                                gt_tokens=gt_tokens,\n                                state=state,\n                                max_length=max_length,\n                                max_len_a=max_len_a,\n                                bos_token_id=bos_token_id,\n                                eos_token_id=eos_token_id,\n                                repetition_penalty=repetition_penalty,\n                                length_penalty=length_penalty,\n                                pad_token_id=pad_token_id,\n                                restricter=restricter)\n        return token_ids\n    if num_beams == 1:\n        token_ids = _no_beam_search_generate(\n            decoder,\n            tokens=tokens,\n            state=state,\n            max_length=max_length,\n            max_len_a=max_len_a,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            repetition_penalty=repetition_penalty,\n            length_penalty=length_penalty,\n            pad_token_id=pad_token_id,\n            restricter=restricter)\n    else:\n        token_ids = _beam_search_generate(\n            decoder,\n            tokens=tokens,\n            state=state,\n            max_length=max_length,\n            max_len_a=max_len_a,\n            num_beams=num_beams,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            do_sample=False,\n            repetition_penalty=repetition_penalty,\n            length_penalty=length_penalty,\n            pad_token_id=pad_token_id,\n            restricter=restricter)\n\n    return token_ids", "\n\ndef _no_beam_search_generate(decoder: Seq2SeqDecoder,\n                             state,\n                             tokens=None,\n                             max_length=20,\n                             max_len_a=0.0,\n                             bos_token_id=None,\n                             eos_token_id=None,\n                             repetition_penalty=1.0,\n                             length_penalty=1.0,\n                             pad_token_id=0,\n                             restricter=None):\n    device = _get_model_device(decoder)\n    if tokens is None:\n        if bos_token_id is None:\n            raise RuntimeError(\n                \"You have to specify either `tokens` or `bos_token_id`.\")\n        batch_size = state.num_samples\n        if batch_size is None:\n            raise RuntimeError(\n                \"Cannot infer the number of samples from `state`.\")\n        tokens = torch.full([batch_size, 1],\n                            fill_value=bos_token_id,\n                            dtype=torch.long).to(device)\n    batch_size = tokens.size(0)\n    if state.num_samples:\n        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n\n    if eos_token_id is None:\n        _eos_token_id = -1\n    else:\n        _eos_token_id = eos_token_id\n\n    scores = decoder.decode(tokens=tokens, state=state)  # \u4e3b\u8981\u662f\u4e3a\u4e86update state\n    # \u8fd9\u91cc\u9700\u8981\u8003\u8651\u5982\u679c\u5728\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u5c31\u7ed3\u675f\u7684\u60c5\u51b5\n    # if _eos_token_id!=-1:\n    #     scores[:, _eos_token_id] = -1e12\n\n    if restricter is not None:\n        _, next_tokens = restricter(state, tokens, scores, num_beams=1)\n    else:\n        next_tokens = scores.argmax(dim=-1, keepdim=True)\n    token_ids = torch.cat([tokens, next_tokens], dim=1)\n    cur_len = token_ids.size(1)\n    dones = token_ids.new_zeros(batch_size).eq(1).__or__(\n        next_tokens.squeeze(1).eq(eos_token_id))\n    # tokens = tokens[:, -1:]\n\n    if max_len_a != 0:\n        # (bsz x num_beams, )\n        if state.encoder_mask is not None:\n            max_lengths = (state.encoder_mask.sum(dim=1).float() *\n                           max_len_a).long() + max_length\n        else:\n            max_lengths = tokens.new_full((tokens.size(0), ),\n                                          fill_value=max_length,\n                                          dtype=torch.long)\n        real_max_length = max_lengths.max().item()\n    else:\n        real_max_length = max_length\n        if state.encoder_mask is not None:\n            max_lengths = state.encoder_mask.new_ones(\n                state.encoder_mask.size(0)).long() * max_length\n        else:\n            max_lengths = tokens.new_full((tokens.size(0), ),\n                                          fill_value=max_length,\n                                          dtype=torch.long)\n\n    while cur_len < real_max_length:\n        scores = decoder.decode(tokens=token_ids,\n                                state=state)  # batch_size x vocab_size\n\n        if repetition_penalty != 1.0:\n            token_scores = scores.gather(dim=1, index=token_ids)\n            lt_zero_mask = token_scores.lt(0).float()\n            ge_zero_mask = lt_zero_mask.eq(0).float()\n            token_scores = lt_zero_mask * repetition_penalty * token_scores + ge_zero_mask / repetition_penalty * token_scores\n            scores.scatter_(dim=1, index=token_ids, src=token_scores)\n\n        if eos_token_id is not None and length_penalty != 1.0:\n            token_scores = scores / cur_len**length_penalty  # batch_size x vocab_size\n            eos_mask = scores.new_ones(scores.size(1))\n            eos_mask[eos_token_id] = 0\n            eos_mask = eos_mask.unsqueeze(0).eq(1)\n            scores = scores.masked_scatter(\n                eos_mask, token_scores)  # \u4e5f\u5373\u9664\u4e86eos\uff0c\u5176\u4ed6\u8bcd\u7684\u5206\u6570\u7ecf\u8fc7\u4e86\u653e\u5927/\u7f29\u5c0f\n\n        if restricter is not None:\n            _, next_tokens = restricter(state, token_ids, scores, 1)\n        else:\n            next_tokens = scores.argmax(dim=-1, keepdim=True)\n        next_tokens = next_tokens.squeeze(-1)\n\n        # \u5982\u679c\u5df2\u7ecf\u8fbe\u5230\u5bf9\u5e94\u7684sequence\u957f\u5ea6\u4e86\uff0c\u5c31\u76f4\u63a5\u586b\u4e3aeos\u4e86\n        if _eos_token_id != -1:\n            next_tokens = next_tokens.masked_fill(max_lengths.eq(cur_len + 1),\n                                                  _eos_token_id)\n        next_tokens = next_tokens.masked_fill(\n            dones, pad_token_id)  # \u5bf9\u5df2\u7ecf\u641c\u7d22\u5b8c\u6210\u7684sample\u505apadding\n        tokens = next_tokens.unsqueeze(1)\n\n        token_ids = torch.cat([token_ids, tokens],\n                              dim=-1)  # batch_size x max_len\n\n        end_mask = next_tokens.eq(_eos_token_id)\n        dones = dones.__or__(end_mask)\n        cur_len += 1\n\n        if dones.min() == 1:\n            break\n\n    return token_ids", "\n\ndef sc_generate(decoder: Seq2SeqDecoder,\n                state,\n                tokens=None,\n                gt_tokens=None,\n                max_length=20,\n                max_len_a=0.0,\n                bos_token_id=None,\n                eos_token_id=None,\n                repetition_penalty=1.0,\n                length_penalty=1.0,\n                pad_token_id=0,\n                restricter=None):\n    device = _get_model_device(decoder)\n    if tokens is None:\n        if bos_token_id is None:\n            raise RuntimeError(\n                \"You have to specify either `tokens` or `bos_token_id`.\")\n        batch_size = state.num_samples\n        if batch_size is None:\n            raise RuntimeError(\n                \"Cannot infer the number of samples from `state`.\")\n        tokens = torch.full([batch_size, 1],\n                            fill_value=bos_token_id,\n                            dtype=torch.long).to(device)\n    batch_size = tokens.size(0)\n    # print(state.num_samples, batch_size)\n    if state.num_samples:\n        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n\n    if eos_token_id is None:\n        _eos_token_id = -1\n    else:\n        _eos_token_id = eos_token_id\n    aspect_cnt = 3\n    next_tokens = gt_tokens[:, aspect_cnt:aspect_cnt + 2]\n    token_ids = torch.cat([tokens, next_tokens], dim=1)\n    cur_len = token_ids.size(1)\n    dones = token_ids.new_zeros(batch_size).eq(1)\n    # tokens = tokens[:, -1:]\n    max_len_a = 0\n    max_length = gt_tokens.size(1)\n    gt_mask = gt_tokens.eq(1).eq(0)\n    max_lengths = gt_mask.sum(dim=1)\n\n    while cur_len < max_length:\n        scores = decoder.decode(tokens=token_ids, state=state,\n                                only_sc=True)  # batch_size x vocab_size\n        if restricter is not None:\n            _, next_tokens = restricter(state, token_ids, scores, 1)\n        else:\n            next_tokens = scores.argmax(dim=-1, keepdim=True)\n        next_tokens = next_tokens.squeeze(-1)\n\n        # \u5982\u679c\u5df2\u7ecf\u8fbe\u5230\u5bf9\u5e94\u7684sequence\u957f\u5ea6\u4e86\uff0c\u5c31\u76f4\u63a5\u586b\u4e3aeos\u4e86\n        # if _eos_token_id != -1:\n        #     next_tokens = next_tokens.masked_fill(max_lengths.eq(cur_len + 1),\n        #                                           _eos_token_id)\n        next_tokens = next_tokens.masked_fill(\n            dones, pad_token_id)  # \u5bf9\u5df2\u7ecf\u641c\u7d22\u5b8c\u6210\u7684sample\u505apadding\n        tokens = next_tokens.unsqueeze(1)\n\n        token_ids = torch.cat([token_ids, tokens],\n                              dim=-1)  # batch_size x max_len\n\n        # end_mask = next_tokens.eq(_eos_token_id)\n        # dones = dones.__or__(end_mask)\n        dones = gt_tokens[:, cur_len + 1].eq(1)\n        cur_len += 1\n        aspect_cnt += 3\n        if aspect_cnt + 2 < max_length:\n            token_ids = torch.cat(\n                [token_ids, gt_tokens[:, aspect_cnt:aspect_cnt + 2]], dim=-1)\n        cur_len += 2\n\n        if dones.min() == 1:\n            break\n    ones = token_ids.new_ones(batch_size).unsqueeze(-1)\n    token_ids = torch.cat([token_ids, ones], dim=-1)\n    # if eos_token_id is not None:\n    #     tokens.scatter(index=max_lengths[:, None], dim=1, value=eos_token_id)  # \u5c06\u6700\u5927\u957f\u5ea6\u4f4d\u7f6e\u8bbe\u7f6e\u4e3aeos\n    # if cur_len == max_length:\n    #     token_ids[:, -1].masked_fill_(~dones, eos_token_id)  # \u82e5\u5230\u6700\u957f\u957f\u5ea6\u4ecd\u672a\u5230EOS\uff0c\u5219\u5f3a\u5236\u5c06\u6700\u540e\u4e00\u4e2a\u8bcd\u66ff\u6362\u6210eos\n\n    return token_ids", "\n\ndef _beam_search_generate(decoder: Seq2SeqDecoder,\n                          tokens=None,\n                          state=None,\n                          max_length=20,\n                          max_len_a=0.0,\n                          num_beams=4,\n                          bos_token_id=None,\n                          eos_token_id=None,\n                          do_sample=True,\n                          repetition_penalty=1.0,\n                          length_penalty=None,\n                          pad_token_id=0,\n                          restricter=None) -> torch.LongTensor:\n    assert do_sample is False\n    # \u8fdb\u884cbeam search\n    # import ipdb; ipdb.set_trace()\n    device = _get_model_device(decoder)\n    if tokens is None:\n        if bos_token_id is None:\n            raise RuntimeError(\n                \"You have to specify either `tokens` or `bos_token_id`.\")\n        batch_size = state.num_samples\n        if batch_size is None:\n            raise RuntimeError(\n                \"Cannot infer the number of samples from `state`.\")\n        tokens = torch.full([batch_size, 1],\n                            fill_value=bos_token_id,\n                            dtype=torch.long).to(device)\n    batch_size = tokens.size(0)\n    if state.num_samples:\n        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n\n    if eos_token_id is None:\n        _eos_token_id = -1\n    else:\n        _eos_token_id = eos_token_id\n\n    scores = decoder.decode(tokens=tokens, state=state)  # \u8fd9\u91cc\u8981\u4f20\u5165\u7684\u662f\u6574\u4e2a\u53e5\u5b50\u7684\u957f\u5ea6\n    # \u8fd9\u91cc\u9700\u8981\u8003\u8651\u5982\u679c\u5728\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u5c31\u7ed3\u675f\u7684\u60c5\u51b5\n    # if _eos_token_id!=-1:\n    #     scores[:, _eos_token_id] = -1e12\n    vocab_size = scores.size(1)\n    assert vocab_size >= num_beams, \"num_beams should be smaller than the number of vocabulary size.\"\n\n    scores = F.log_softmax(scores, dim=-1)  # (batch_size, vocab_size)\n    # \u5f97\u5230(batch_size, num_beams), (batch_size, num_beams)\n    # TODO \u628a\u9650\u5236\u5199\u5230\u8fd9\u4e2a\u4f4d\u7f6e, \u52a01\u662f\u56e0\u4e3a\u9700\u8981\u8003\u8651\u8f93\u51fa\u5c31\u662feos\u7684\u60c5\u51b5\n    if restricter is not None:\n        _next_scores, _next_tokens = restricter(state, tokens, scores,\n                                                num_beams + 1)\n    else:\n        # \u662fbsz x (num_beams+1)\u5927\u5c0f\u7684\u4e1c\u897f\n        _next_scores, _next_tokens = torch.topk(scores,\n                                                num_beams + 1,\n                                                dim=1,\n                                                largest=True,\n                                                sorted=True)\n\n    # \u6839\u636eindex\u6765\u505a\u987a\u5e8f\u7684\u8c03\u8f6c\n    indices = torch.arange(batch_size, dtype=torch.long).to(device)\n    indices = indices.repeat_interleave(num_beams)\n    state.reorder_state(indices)\n    tokens = tokens.index_select(\n        dim=0, index=indices)  # batch_size * num_beams x length\n\n    if max_len_a != 0:\n        # (bsz x num_beams, )\n        if state.encoder_mask is not None:\n            max_lengths = (state.encoder_mask.sum(dim=1).float() *\n                           max_len_a).long() + max_length\n        else:\n            max_lengths = tokens.new_full((batch_size * num_beams, ),\n                                          fill_value=max_length,\n                                          dtype=torch.long)\n        real_max_length = max_lengths.max().item()\n    else:\n        real_max_length = max_length\n        if state.encoder_mask is not None:\n            max_lengths = state.encoder_mask.new_ones(\n                state.encoder_mask.size(0)).long() * max_length\n        else:\n            max_lengths = tokens.new_full((batch_size * num_beams, ),\n                                          fill_value=max_length,\n                                          dtype=torch.long)\n    hypos = [\n        BeamHypotheses(num_beams,\n                       real_max_length,\n                       length_penalty,\n                       early_stopping=False) for _ in range(batch_size)\n    ]\n\n    not_eos_mask = _next_tokens.ne(_eos_token_id)  # \u4e3a1\u7684\u5730\u65b9\u4e0d\u662feos\n    keep_mask = not_eos_mask.cumsum(dim=1).le(num_beams)  # \u4e3a1\u7684\u5730\u65b9\u9700\u8981\u4fdd\u7559\n    keep_mask = not_eos_mask.__and__(keep_mask)  # \u4e3a1\u7684\u5730\u65b9\u662f\u9700\u8981\u8fdb\u884c\u4e0b\u4e00\u6b65search\u7684\n\n    next_tokens = _next_tokens.masked_select(keep_mask).view(\n        batch_size, num_beams)  # \u8fd9\u662f\u771f\u7684\u63a5\u4e0b\u6765\u8981\u7ee7\u7eed\u7684\n    next_scores = _next_scores.masked_select(keep_mask).view(\n        batch_size, num_beams)\n\n    rows, cols = not_eos_mask.eq(0)[:, :num_beams].nonzero(as_tuple=True)\n\n    if len(rows) > 0:  # \u8bf4\u660e\u6709\u7684\u5f00\u5934\u5c31\u7ed3\u675f\u4e86\n        for row, col in zip(rows.tolist(), cols.tolist()):\n            _token = torch.cat(\n                [tokens[row * num_beams], _next_tokens[row, col:col + 1]],\n                dim=0)\n            hypos[row].add(_token.clone(), _next_scores[row, col].item())\n\n    # \u8bb0\u5f55\u751f\u6210\u597d\u7684token (batch_size', cur_len)\n    token_ids = torch.cat([tokens, next_tokens.view(-1, 1)], dim=-1)\n    dones = [False] * batch_size\n\n    beam_scores = next_scores.view(-1)  # batch_size * num_beams\n\n    #  \u7528\u6765\u8bb0\u5f55\u5df2\u7ecf\u751f\u6210\u597d\u7684token\u7684\u957f\u5ea6\n    cur_len = token_ids.size(1)\n\n    # 0, num_beams, 2*num_beams, ...\n    batch_inds_with_numbeams_interval = (torch.arange(batch_size) *\n                                         num_beams).view(-1, 1).to(token_ids)\n\n    while cur_len < real_max_length:\n        scores = decoder.decode(token_ids,\n                                state)  # (bsz x num_beams, vocab_size)\n        if repetition_penalty != 1.0:\n            token_scores = scores.gather(dim=1, index=token_ids)\n            lt_zero_mask = token_scores.lt(0).float()\n            ge_zero_mask = lt_zero_mask.eq(0).float()\n            token_scores = lt_zero_mask * repetition_penalty * token_scores + ge_zero_mask / repetition_penalty * token_scores\n            scores.scatter_(dim=1, index=token_ids, src=token_scores)\n\n        if _eos_token_id != -1:\n            max_len_eos_mask = max_lengths.eq(cur_len + 1)\n            eos_scores = scores[:, _eos_token_id]\n            # \u5982\u679c\u5df2\u7ecf\u8fbe\u5230\u6700\u5927\u957f\u5ea6\uff0c\u5c31\u628aeos\u7684\u5206\u6570\u52a0\u5927\n            scores[:, _eos_token_id] = torch.where(max_len_eos_mask,\n                                                   eos_scores + 1e32,\n                                                   eos_scores)\n\n        scores = F.log_softmax(scores,\n                               dim=-1)  # (batch_size * num_beams, vocab_size)\n        _scores = scores + beam_scores[:,\n                                       None]  # (batch_size * num_beams, vocab_size)\n        _scores = _scores.view(batch_size,\n                               -1)  # (batch_size, num_beams*vocab_size)\n        # TODO \u628a\u9650\u5236\u52a0\u5230\u8fd9\u4e2a\u4f4d\u7f6e\n        if restricter is not None:\n            next_scores, ids = restricter(state, token_ids, _scores,\n                                          2 * num_beams)\n        else:\n            next_scores, ids = torch.topk(_scores,\n                                          2 * num_beams,\n                                          dim=1,\n                                          largest=True,\n                                          sorted=True)  # (bsz, 2*num_beams)\n        from_which_beam = ids // vocab_size  # (batch_size, 2*num_beams)\n        next_tokens = ids % vocab_size  # (batch_size, 2*num_beams)\n\n        not_eos_mask = next_tokens.ne(_eos_token_id)  # \u4e3a1\u7684\u5730\u65b9\u4e0d\u662feos\n        keep_mask = not_eos_mask.cumsum(dim=1).le(num_beams)  # \u4e3a1\u7684\u5730\u65b9\u9700\u8981\u4fdd\u7559\n        keep_mask = not_eos_mask.__and__(keep_mask)  # \u4e3a1\u7684\u5730\u65b9\u662f\u9700\u8981\u8fdb\u884c\u4e0b\u4e00\u6b65search\u7684\n\n        _next_tokens = next_tokens.masked_select(keep_mask).view(-1, 1)\n        _from_which_beam = from_which_beam.masked_select(keep_mask).view(\n            batch_size, num_beams)  # \u4e0a\u9762\u7684token\u662f\u6765\u81ea\u54ea\u4e2abeam\n        _next_scores = next_scores.masked_select(keep_mask).view(\n            batch_size, num_beams)\n        beam_scores = _next_scores.view(-1)\n\n        flag = True\n        if cur_len + 1 == real_max_length:\n            eos_batch_idx = torch.arange(batch_size).to(\n                next_tokens).repeat_interleave(repeats=num_beams, dim=0)\n            eos_beam_ind = torch.arange(num_beams).to(token_ids).repeat(\n                batch_size)  # \u8868\u793a\u7684\u662findice\n            eos_beam_idx = from_which_beam[:, :num_beams].reshape(\n                -1)  # \u8868\u793a\u7684\u662f\u4ece\u54ea\u4e2abeam\u83b7\u53d6\u5f97\u5230\u7684\n        else:\n            # \u5c06\u6bcf\u4e2abatch\u4e2d\u5728num_beam\u5185\u7684\u5e8f\u5217\u6dfb\u52a0\u5230\u7ed3\u675f\u4e2d, \u4e3a1\u7684\u5730\u65b9\u9700\u8981\u7ed3\u675f\u4e86\n            effective_eos_mask = next_tokens[:, :num_beams].eq(\n                _eos_token_id)  # batch_size x num_beams\n            if effective_eos_mask.sum().gt(0):\n                eos_batch_idx, eos_beam_ind = effective_eos_mask.nonzero(\n                    as_tuple=True)\n                # \u662f\u7531\u4e8efrom_which_beam\u662f (batch_size, 2*num_beams)\u7684\uff0c\u6240\u4ee5\u9700\u89812*num_beams\n                eos_beam_idx = eos_batch_idx * num_beams * 2 + eos_beam_ind\n                eos_beam_idx = from_which_beam.view(-1)[\n                    eos_beam_idx]  # \u83b7\u53d6\u771f\u5b9e\u7684\u4ece\u54ea\u4e2abeam\u83b7\u53d6\u7684eos\n            else:\n                flag = False\n\n        if flag:\n            _token_ids = torch.cat([token_ids, _next_tokens], dim=-1)\n            for batch_idx, beam_ind, beam_idx in zip(eos_batch_idx.tolist(),\n                                                     eos_beam_ind.tolist(),\n                                                     eos_beam_idx.tolist()):\n                if not dones[batch_idx]:\n                    score = next_scores[batch_idx, beam_ind].item()\n                    # \u4e4b\u540e\u9700\u8981\u5728\u7ed3\u5c3e\u65b0\u589e\u4e00\u4e2aeos\n                    if _eos_token_id != -1:\n                        hypos[batch_idx].add(\n                            _token_ids[batch_idx * num_beams +\n                                       beam_idx, :cur_len].clone(), score)\n                    else:\n                        hypos[batch_idx].add(\n                            _token_ids[batch_idx * num_beams +\n                                       beam_idx].clone(), score)\n\n        # \u66f4\u6539state\u72b6\u6001, \u91cd\u7ec4token_ids\n        reorder_inds = (batch_inds_with_numbeams_interval +\n                        _from_which_beam).view(-1)  # flatten\u6210\u4e00\u7ef4\n        state.reorder_state(reorder_inds)\n        # \u91cd\u65b0\u7ec4\u7ec7token_ids\u7684\u72b6\u6001\n        token_ids = torch.cat(\n            [token_ids.index_select(index=reorder_inds, dim=0), _next_tokens],\n            dim=-1)\n\n        for batch_idx in range(batch_size):\n            dones[batch_idx] = dones[batch_idx] or hypos[batch_idx].is_done(next_scores[batch_idx, 0].item()) or \\\n                               max_lengths[batch_idx*num_beams]==cur_len+1\n\n        cur_len += 1\n\n        if all(dones):\n            break\n\n    # select the best hypotheses\n    tgt_len = token_ids.new_zeros(batch_size)\n    best = []\n\n    for i, hypotheses in enumerate(hypos):\n        best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n        # \u628a\u4e0a\u9762\u66ff\u6362\u4e3a\u975eeos\u7684\u8bcd\u66ff\u6362\u56deeos\n        if _eos_token_id != -1:\n            best_hyp = torch.cat(\n                [best_hyp, best_hyp.new_ones(1) * _eos_token_id])\n        tgt_len[i] = len(best_hyp)\n        best.append(best_hyp)\n\n    # generate target batch\n    decoded = token_ids.new_zeros(batch_size,\n                                  tgt_len.max().item()).fill_(pad_token_id)\n    for i, hypo in enumerate(best):\n        decoded[i, :tgt_len[i]] = hypo\n\n    return decoded", "\n\nclass BeamHypotheses(object):\n    def __init__(self, num_beams, max_length, length_penalty, early_stopping):\n        \"\"\"\n        Initialize n-best list of hypotheses.\n        \"\"\"\n        self.max_length = max_length - 1  # ignoring bos_token\n        self.length_penalty = length_penalty\n        self.early_stopping = early_stopping\n        self.num_beams = num_beams\n        self.hyp = []\n        self.worst_score = 1e9\n\n    def __len__(self):\n        \"\"\"\n        Number of hypotheses in the list.\n        \"\"\"\n        return len(self.hyp)\n\n    def add(self, hyp, sum_logprobs):\n        \"\"\"\n        Add a new hypothesis to the list.\n        \"\"\"\n        score = sum_logprobs / len(hyp)**self.length_penalty\n        if len(self) < self.num_beams or score > self.worst_score:\n            self.hyp.append((score, hyp))\n            if len(self) > self.num_beams:\n                sorted_scores = sorted([\n                    (s, idx) for idx, (s, _) in enumerate(self.hyp)\n                ])\n                del self.hyp[sorted_scores[0][1]]\n                self.worst_score = sorted_scores[1][0]\n            else:\n                self.worst_score = min(score, self.worst_score)\n\n    def is_done(self, best_sum_logprobs):\n        \"\"\"\n        If there are enough hypotheses and that none of the hypotheses being generated\n        can become better than the worst one in the heap, then we are done with this sentence.\n        \"\"\"\n        if len(self) < self.num_beams:\n            return False\n        elif self.early_stopping:\n            return True\n        else:\n            return self.worst_score >= best_sum_logprobs / self.max_length**self.length_penalty", ""]}
{"filename": "src/model/model.py", "chunked_list": ["# Based on transformers.modeling_bart\n\nfrom typing import Optional, Tuple\nfrom fastNLP.modules.torch.encoder import Seq2SeqEncoder\nfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\nfrom fastNLP.modules.torch import State\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom src.model.modeling_bart import (PretrainedBartModel, BartEncoder,", "from torch import nn\nfrom src.model.modeling_bart import (PretrainedBartModel, BartEncoder,\n                                     BartDecoder, BartModel,\n                                     BartClassificationHead,\n                                     _make_linear_from_emb,\n                                     _prepare_bart_decoder_inputs)\n\nfrom transformers import BartTokenizer\n\nfrom src.model.config import MultiModalBartConfig", "\nfrom src.model.config import MultiModalBartConfig\nfrom src.model.mixins import GenerationMixin, FromPretrainedMixin\nfrom src.model.modules import MultiModalBartEncoder, MultiModalBartDecoder_span, MultiModalBartDecoder_MLM, MultiModalBartDecoder_sentiment, Span_loss, MultiModalBartDecoder_MRM, MultiModalBartDecoder_ANP_generate\n\n# This is based on transformers.BartModel\n# The modifications are:\n# - BartConfig -> MultiModalBartConfig\n# - BartEncoder -> MultiModalBartEncoder\n# - added image_features in forward", "# - BartEncoder -> MultiModalBartEncoder\n# - added image_features in forward\n\n\n# def generate_span_mask(spans):\n#     max_len = max([len(x) for x in spans])\n#     mask = torch.ones(())\nclass MultiModalBartModelForPretrain(FromPretrainedMixin, PretrainedBartModel):\n    def build_model(self,\n                    args,\n                    bart_model,\n                    tokenizer,\n                    label_ids,\n                    config,\n                    decoder_type=None,\n                    copy_gate=False,\n                    use_encoder_mlp=False,\n                    use_recur_pos=False,\n                    tag_first=False):\n        if args.bart_init:\n            model = BartModel.from_pretrained(bart_model)\n            num_tokens, _ = model.encoder.embed_tokens.weight.shape\n\n            model.resize_token_embeddings(\n                len(tokenizer.unique_no_split_tokens) + num_tokens)\n            encoder = model.encoder\n            decoder = model.decoder\n\n            padding_idx = config.pad_token_id\n            encoder.embed_tokens.padding_idx = padding_idx\n\n            _tokenizer = BartTokenizer.from_pretrained(bart_model)\n\n            for token in tokenizer.unique_no_split_tokens:\n                if token[:2] == '<<':  # \u7279\u6b8a\u5b57\u7b26\n                    index = tokenizer.convert_tokens_to_ids(\n                        tokenizer._base_tokenizer.tokenize(token))\n                    if len(index) > 1:\n                        raise RuntimeError(f\"{token} wrong split\")\n                    else:\n                        index = index[0]\n                    assert index >= num_tokens, (index, num_tokens, token)\n                    indexes = _tokenizer.convert_tokens_to_ids(\n                        _tokenizer.tokenize(token[2:-2]))\n                    embed = model.encoder.embed_tokens.weight.data[indexes[0]]\n                    for i in indexes[1:]:\n                        embed += model.decoder.embed_tokens.weight.data[i]\n                    embed /= len(indexes)\n                    model.decoder.embed_tokens.weight.data[index] = embed\n        else:\n            raise RuntimeError(\"error init!!!!!!!\")\n\n        multimodal_encoder = MultiModalBartEncoder(config, encoder,\n                                                   tokenizer.img_feat_id,\n                                                   tokenizer.cls_token_id)\n        return (multimodal_encoder, decoder)\n\n    def __init__(self, config: MultiModalBartConfig, bart_model, tokenizer,\n                 label_ids, senti_ids, args):\n        super().__init__(config)\n        self.config = config\n        label_ids = sorted(label_ids)\n        multimodal_encoder, share_decoder = self.build_model(\n            args, bart_model, tokenizer, label_ids, config)\n        causal_mask = torch.zeros(512, 512).fill_(float('-inf'))\n        self.causal_mask = causal_mask.triu(diagonal=1)\n        self.encoder = multimodal_encoder\n        self.mlm_decoder = MultiModalBartDecoder_MLM(self.config,\n                                                     share_decoder)\n        self.mrm_decoder = MultiModalBartDecoder_MRM(self.config,\n                                                     share_decoder,\n                                                     self.causal_mask, args)\n        self.span_decoder = MultiModalBartDecoder_span(self.config, tokenizer,\n                                                       share_decoder,\n                                                       tokenizer.pad_token_id,\n                                                       label_ids,\n                                                       self.causal_mask)\n        self.span_loss_fct = Span_loss()\n        self.anp_generate_decoder = MultiModalBartDecoder_ANP_generate(\n            self.config, share_decoder)\n        self.senti_decoder = MultiModalBartDecoder_sentiment(\n            self.config, share_decoder, senti_ids)\n\n    def prepare_state(self,\n                      input_ids,\n                      image_features,\n                      attention_mask=None,\n                      first=None):\n        dict = self.encoder(input_ids=input_ids,\n                            image_features=image_features,\n                            attention_mask=attention_mask,\n                            output_hidden_states=True,\n                            return_dict=True)\n        encoder_outputs = dict.last_hidden_state\n        hidden_states = dict.hidden_states\n        encoder_mask = attention_mask\n        src_embed_outputs = hidden_states[0]\n        state = BartState(encoder_outputs, encoder_mask, input_ids[:, 38:],\n                          first, src_embed_outputs)\n        # setattr(state, 'tgt_seq_len', tgt_seq_len)\n        return state\n\n    def forward(\n            self,\n            task_type,\n            input_ids,\n            image_features,\n            attention_mask=None,\n            mlm_infos=None,\n            mrm_infos=None,\n            senti_infos=None,\n            ANP_infos=None,\n            ANP_generate_infos=None,\n            ae_infos=None,\n            oe_infos=None,\n            ae_oe_infos=None,\n            encoder_outputs: Optional[Tuple] = None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n    ):\n\n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                image_features=image_features,\n                attention_mask=attention_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n            )\n        assert isinstance(encoder_outputs, tuple)\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        if task_type == 'MLM':\n            labels, decoder_input_ids, decoder_attention_mask = [\n                mlm_infos['mlm_labels'], mlm_infos['mlm_decoder_input_ids'],\n                mlm_infos['mlm_decoder_attention_mask']\n            ]\n            loss = self.mlm_decoder(labels, input_ids, encoder_outputs[0],\n                                    attention_mask, decoder_input_ids,\n                                    decoder_attention_mask)\n        elif task_type == 'MRM':\n            mrm_labels, mrm_masks, decoder_input_ids, decoder_attention_mask = [\n                mrm_infos['mrm_labels'],\n                mrm_infos['mrm_masks'].to(input_ids.device),\n                mrm_infos['mrm_decoder_input_ids'].to(input_ids.device),\n                mrm_infos['mrm_decoder_attention_mask'].to(input_ids.device)\n            ]\n            loss = self.mrm_decoder(mrm_labels, mrm_masks, encoder_outputs[0],\n                                    attention_mask, decoder_input_ids,\n                                    decoder_attention_mask)\n        elif task_type == 'Sentiment':\n            senti_labels, decoder_input_ids, decoder_attention_mask = [\n                senti_infos['senti_labels'],\n                senti_infos['senti_decoder_input_ids'],\n                senti_infos['senti_decoder_attention_mask']\n            ]\n            loss, predict_senti = self.senti_decoder(senti_labels,\n                                                     encoder_outputs[0],\n                                                     attention_mask,\n                                                     decoder_input_ids)\n        elif task_type == 'ANP_generate':\n            labels, decoder_input_ids, decoder_attention_mask = [\n                ANP_generate_infos['anp_generate_labels'],\n                ANP_generate_infos['anp_generate_decoder_input_ids'],\n                ANP_generate_infos['anp_generate_decoder_attention_mask']\n            ]\n            loss = self.anp_generate_decoder(labels, input_ids,\n                                             encoder_outputs[0],\n                                             attention_mask, decoder_input_ids,\n                                             decoder_attention_mask)\n        elif task_type == 'AE_OE':\n            spans, span_mask = [\n                ae_oe_infos['labels'].to(input_ids.device),\n                ae_oe_infos['masks'].to(input_ids.device)\n            ]\n            state = self.prepare_state(input_ids, image_features,\n                                       attention_mask)\n            logits = self.span_decoder(spans, state)\n            loss = self.span_loss_fct(spans[:, 1:], logits, span_mask[:, 1:])\n        else:\n            raise RuntimeError(\"task type error!!!!!!!\")\n\n        if task_type == 'Sentiment':\n            return loss, predict_senti\n        return loss", "\n\nclass BartState(State):\n    def __init__(self, encoder_output, encoder_mask, src_tokens, first,\n                 src_embed_outputs):\n        super().__init__(encoder_output, encoder_mask)\n        self.past_key_values = None\n        self.src_tokens = src_tokens\n        self.first = first\n        self.src_embed_outputs = src_embed_outputs\n\n    def reorder_state(self, indices: torch.LongTensor):\n        super().reorder_state(indices)\n        self.src_tokens = self._reorder_state(self.src_tokens, indices)\n        if self.first is not None:\n            self.first = self._reorder_state(self.first, indices)\n        self.src_embed_outputs = self._reorder_state(self.src_embed_outputs,\n                                                     indices)\n        if self.past_key_values is not None:\n            new = []\n            for layer in self.past_key_values:\n                new_layer = {}\n                for key1 in list(layer.keys()):\n                    new_layer_ = {}\n                    for key2 in list(layer[key1].keys()):\n                        if layer[key1][key2] is not None:\n                            layer[key1][key2] = self._reorder_state(\n                                layer[key1][key2], indices)\n                        new_layer_[key2] = layer[key1][key2]\n                    new_layer[key1] = new_layer_\n                new.append(new_layer)\n            self.past_key_values = new"]}
{"filename": "src/model/modules_for_prompt_multitasks.py", "chunked_list": ["import random\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.models.bart.modeling_bart import *\nfrom src.model.modeling_bart import (\n    SinusoidalPositionalEmbedding,\n    LearnedPositionalEmbedding,", "    SinusoidalPositionalEmbedding,\n    LearnedPositionalEmbedding,\n    invert_mask,\n    EncoderLayer,\n    LayerNorm,\n)\nfrom src.model.modeling_bart import (PretrainedBartModel, BartDecoder,\n                                     BartClassificationHead,\n                                     _make_linear_from_emb,\n                                     _prepare_bart_decoder_inputs)", "                                     _make_linear_from_emb,\n                                     _prepare_bart_decoder_inputs)\nfrom src.model.config import MultiModalBartConfig\n\nfrom transformers import AutoConfig, AutoModel, CLIPVisionModel, CLIPVisionConfig\nimport timm\nfrom src.model.attention import Attention_for_Senti_Prompt\n\nTIMM_MODELS = {\n    'nf_resnet50': 2048,", "TIMM_MODELS = {\n    'nf_resnet50': 2048,\n}\ndef is_clip_model(model_name):\n    return model_name.startswith('openai/clip-')\n\nimage_model_name =  'nf_resnet50'\nif image_model_name in TIMM_MODELS.keys():\n    image_encoder = timm.create_model(image_model_name, pretrained=True, num_classes=0)\nelif is_clip_model(image_model_name):\n    ###model_name ='openai/clip-vit-base-patch32'\n    config = CLIPVisionConfig.from_pretrained(image_model_name)\n    image_encoder = CLIPVisionModel.from_pretrained(\n            image_model_name,\n            config=config,\n        )\nelse:\n    image_encoder = AutoModel.from_pretrained(image_model_name)", "\ndef init_image_encoder(image_model_name, frozen_image_encoder, num_image_tokens, d_text_encoder):\n\n    # image_encoder = get_image_encoder(image_model_name)\n    d_image_encoder = _d_image_encoder(image_model_name, image_encoder)\n\n    if frozen_image_encoder:\n        for p in image_encoder.parameters():\n            p.requires_grad = False\n            image_encoder.eval()\n\n    proj_image_features = nn.Linear(\n            in_features=d_image_encoder,\n            out_features=num_image_tokens * d_text_encoder,\n        )\n    return proj_image_features.cuda(), d_image_encoder", "\ndef _d_image_encoder(image_model_name, image_encoder):\n    ##image_model_name\u9ed8\u8ba4\u4e3a\uff1a 'microsoft/resnet-50'\n    model_name = image_model_name\n    if model_name in TIMM_MODELS.keys():\n        return TIMM_MODELS[model_name]\n    elif is_clip_model(model_name):\n        return image_encoder.config.hidden_size\n    elif model_name.startswith('microsoft/resnet-'):\n        return image_encoder.config.hidden_sizes[-1]\n    else:\n        return image_encoder.config.hidden_size", "\n\ndef encode_images(image_encoder, proj_image_features, frozen_image_encoder, pixel_values, d_image_encoder):\n    \n    image_encoder = image_encoder.cuda()\n    pixel_values = pixel_values.cuda()\n    # print('the shape of pixel_values is {}'.format(pixel_values.shape))\n    batch_size = pixel_values.shape[0]\n\n    if frozen_image_encoder:\n        with torch.no_grad():\n            image_encoder.eval()\n            visual = image_encoder(pixel_values)\n    else:\n        visual = image_encoder(pixel_values)\n\n    if not isinstance(visual, torch.Tensor):  # HuggingFace model\n        visual = visual.pooler_output\n\n    visual = visual.reshape(batch_size, d_image_encoder)\n    visual = proj_image_features(visual).cuda()\n    return visual", "\n\n\nclass ImageEmbedding(nn.Module):\n    def __init__(self, image_dim, final_dim, image_model_name, frozen_image_encoder=False, num_image_tokens=2):\n        super(ImageEmbedding, self).__init__()\n        self.frozen_image_encoder = frozen_image_encoder\n        self.final_dim = final_dim\n        self.linear = nn.Linear(final_dim, final_dim)\n        self.d_image_encoder = _d_image_encoder(image_model_name, image_encoder)\n        if frozen_image_encoder:\n            for p in image_encoder.parameters():\n                p.requires_grad = False\n                image_encoder.eval()\n\n        self.proj_image_features = nn.Linear(\n                in_features=self.d_image_encoder,\n                out_features=num_image_tokens * final_dim,\n            )\n\n    def forward(self, image_pixel_values):\n        \n        # import ipdb; ipdb.set_trace()\n        image_pixel_values = torch.stack(image_pixel_values)\n        batch_size = image_pixel_values.size(0)\n        image_features = encode_images(image_encoder=image_encoder, \n                                           proj_image_features=self.proj_image_features, \n                                           frozen_image_encoder=self.frozen_image_encoder, \n                                           pixel_values=image_pixel_values, \n                                           d_image_encoder=self.d_image_encoder)\n        ###image_features: (batch_size, num_image_tokens*1024) (4, 2048)\n        # print(\"======================================the shape of image_features is {}=====================================\".format(image_features.shape))\n        image_features = image_features.reshape(batch_size, -1, self.final_dim) ### (4, num_image_tokens, 1024(d_model))\n        \n        img_len = list(map(len, image_features))\n        non_empty_features = list(filter(lambda x: len(x) > 0, image_features))\n\n        embedded = None\n        if len(non_empty_features) > 0:\n            img_tensor = torch.cat(non_empty_features, dim=0)\n            embedded = self.linear(img_tensor)\n\n        output = []\n        index = 0\n        for l in img_len:\n            if l > 0:\n                output.append(embedded[index:index + l])\n            else:\n                output.append(torch.empty(0))\n            index += l\n        return output", "\n\nclass MultiModalBartEncoder(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n    is a :class:EncoderLayer.\n\n    Args:\n        config: MultiModalBartConfig\n    \"\"\"\n    def __init__(self, config: MultiModalBartConfig, encoder, img_feat_id,\n                 cls_token_id, num_image_tokens):\n        super().__init__()\n\n        self.img_feat_id = img_feat_id\n        self.cls_token_id = cls_token_id\n        embed_tokens = encoder.embed_tokens\n        self.dropout = encoder.dropout\n        self.layerdrop = encoder.layerdrop\n\n        self.indentity = nn.Identity()\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_scale = encoder.embed_scale\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = encoder.max_source_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n        self.embed_positions = encoder.embed_positions\n\n        self.layers = encoder.layers\n        self.layernorm_embedding = encoder.layernorm_embedding\n        # mbart has one extra layer_norm\n        self.layer_norm = encoder.layer_norm\n\n    def _embed_multi_modal(self, input_ids, image_features):\n        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n        mask = (input_ids == self.img_feat_id) | (\n            input_ids == self.cls_token_id)\n        # print(mask.shape)\n        embedded_images = self.embed_images(image_features)\n        embedded = self.embed_tokens(input_ids)\n        # print('mask shape', mask.shape)\n        if not embedded_images[0].dtype == torch.float32:\n            embedded = embedded.half()\n\n        for index, value in enumerate(embedded_images):\n            if len(value) > 0:\n                embedded[index, mask[index]] = value\n\n        return embedded\n\n    def forward(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=False):\n        \"\"\"\n\n        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n        :param image_features: list[FloatTensor], image roi features with length of batch\n        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n        :param output_attentions:\n        :param output_hidden_states:\n        :return: Tuple comprised of:\n            - x (Tensor): the last encoder layer's output of\n              shape (src_len, batch, embed_dim)\n            - encoder_states (List[Tensor]): all intermediate\n              hidden states of shape (src_len, batch, embed_dim).\n              Only populated if output_hidden_states: is True.\n            - all_attentions (List[Tensor]): Attention weights for each layer.\n            During training might not be of length n_layers because of layer dropout.\n        \"\"\"\n        # check attention mask and invert\n        if attention_mask is not None:\n            attention_mask = invert_mask(attention_mask)\n\n        inputs_embeds = self._embed_multi_modal(\n            input_ids, image_features) * self.embed_scale\n        embed_pos = self.embed_positions(input_ids)\n        x = inputs_embeds + embed_pos\n        x = self.layernorm_embedding(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        encoder_states, all_attentions = [], []\n        for encoder_layer in self.layers:\n            if output_hidden_states:\n                encoder_states.append(x)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability <\n                                  self.layerdrop):  # skip the layer\n                attn = None\n            else:\n                x, attn = encoder_layer(x,\n                                        attention_mask,\n                                        output_attentions=output_attentions)\n\n            if output_attentions:\n                all_attentions.append(attn)\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n        if output_hidden_states:\n            encoder_states.append(x)\n\n        # T x B x C -> B x T x C\n        encoder_states = [\n            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n        ]\n        x = x.transpose(0, 1)\n\n        if not return_dict:\n            return tuple(v for v in [x, encoder_states, all_attentions]\n                         if v is not None)\n        return BaseModelOutput(last_hidden_state=x,\n                               hidden_states=encoder_states,\n                               attentions=all_attentions)", "\nclass MultiModalBartEncoder_for_Generating_aspect_prompt(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n    is a :class:EncoderLayer.\n\n    Args:\n        config: MultiModalBartConfig\n    \"\"\"\n    def __init__(self, \n                use_generated_prompt,\n                config: MultiModalBartConfig, encoder, img_feat_id, aspect_prompt_token_id, senti_prompt_token_id,\n                 cls_token_id, num_image_tokens, use_different_aspect_prompt):\n        super().__init__()\n\n        self.use_generated_prompt= use_generated_prompt\n        self.aspect_prompt_token_id = aspect_prompt_token_id\n        self.senti_prompt_token_id = senti_prompt_token_id\n        self.use_different_aspect_prompt = use_different_aspect_prompt\n\n        self.img_feat_id = img_feat_id\n        self.cls_token_id = cls_token_id\n        embed_tokens = encoder.embed_tokens\n        self.dropout = encoder.dropout\n        self.layerdrop = encoder.layerdrop\n        self.num_image_tokens = num_image_tokens\n\n        self.indentity = nn.Identity()\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_scale = encoder.embed_scale\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = encoder.max_source_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n        self.embed_positions = encoder.embed_positions\n\n        self.layers = encoder.layers\n        self.layernorm_embedding = encoder.layernorm_embedding\n        # mbart has one extra layer_norm\n        self.layer_norm = encoder.layer_norm\n\n        # self.aspect_linear = nn.Linear(768, 768)\n        # self.aspect_relu = nn.LeakyReLU()\n\n    def _embed_multi_modal(self, generated_aspect_prompt, aspects_num, input_ids, image_features):\n        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n        # import ipdb; ipdb.set_trace()\n        mask = (input_ids == self.img_feat_id) | (\n            input_ids == self.cls_token_id)\n        # print(mask.shape)\n        embedded_images = self.embed_images(image_features)\n        embedded = self.embed_tokens(input_ids)\n        # print('mask shape', mask.shape)\n        if not embedded_images[0].dtype == torch.float32:\n            embedded = embedded.half()\n\n        for index, value in enumerate(embedded_images):\n            if len(value) > 0:\n                embedded[index, mask[index]] = value\n        \n        new_input_ids =[]\n        # import ipdb; ipdb.set_trace()\n        for i in range(len(aspects_num)):\n\n            \n            aspect_num = aspects_num[i]\n            # print('the aspect_num is {}'.format(aspect_num))\n           \n            input_id = input_ids[i]\n\n            if self.num_image_tokens==0:\n                prompt_begin_index = 25\n                prompt_end_index = 39\n            elif self.num_image_tokens==1:\n                prompt_begin_index = 26\n                prompt_end_index = 40\n            elif self.num_image_tokens==2:\n                prompt_begin_index = 27\n                prompt_end_index = 41\n            elif self.num_image_tokens==3:\n                prompt_begin_index = 28\n                prompt_end_index = 42\n            elif self.num_image_tokens==4:\n                prompt_begin_index = 29\n                prompt_end_index = 43\n            elif self.num_image_tokens==5:\n                prompt_begin_index = 30\n                prompt_end_index = 44\n            elif self.num_image_tokens==6:\n                prompt_begin_index = 31\n                prompt_end_index = 45\n            elif self.num_image_tokens==7:\n                prompt_begin_index = 32\n                prompt_end_index = 46\n                \n            # print('before')\n            # print(len(input_id))\n            # import ipdb; ipdb.set_trace()\n            reserve_aspect_id = input_id[prompt_begin_index:prompt_begin_index+3*aspect_num]\n            if aspect_num ==5:\n                # print('aspect_num is 5')\n                # print(reserve_aspect_id)\n                new_input_id = torch.cat([input_id[:prompt_begin_index], reserve_aspect_id, input_id[prompt_end_index+1:]])\n            else:\n                cut_aspect_id = torch.ones_like(input_id[prompt_begin_index+3*aspect_num:prompt_end_index])\n                new_input_id = torch.cat([input_id[:prompt_begin_index], reserve_aspect_id, cut_aspect_id, input_id[prompt_end_index:]])\n            # print(\"++++++++++++++++++++cut_aspect_id++++++++++++++++++++++++\")\n            # print(cut_aspect_id)\n            # print(input_id[58:])\n            new_input_ids.append(new_input_id)\n            # print('the shape of new_input_id is {}'.format(new_input_id.shape))\n            # print(new_input_id[58:])\n            # print(\"+++++++++++++++++++++++input_id length is {}+++++++++++++++++++++++\".format(len(input_id)))\n            # print(input_id)\n            # print(\"+++++++++++++++++++++++new_input_id length is {}+++++++++++++++++++++++\".format(len(input_id)))\n            # print(new_input_id)\n        new_input_ids = torch.stack(new_input_ids)\n\n        prompt_mask = (new_input_ids == self.aspect_prompt_token_id)\n\n        if self.use_generated_prompt:\n            if self.use_different_aspect_prompt:\n                # self.aspect_linear = self.aspect_linear.to(generated_aspect_prompt.device)\n                # self.aspect_relu = self.aspect_relu.to(generated_aspect_prompt.device)\n            \n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n                    prompt_embedding_list = []\n                    for j in range(aspect_num):\n                        aspect_linear = nn.Linear(768, 768).to(generated_aspect_prompt.device) ##\u6bcf\u4e2aaspect\u6709\u81ea\u5df1\u7684\u53d8\u6362\uff0c\u4e3a\u6bcf\u4e2aaspect\u8bbe\u8ba1\u7279\u5b9a\u7684prompt\n                        aspect_relu = nn.LeakyReLU().to(generated_aspect_prompt.device)\n                        prompt_embedding = aspect_linear(generated_aspect_prompt[index])\n                        prompt_embedding = aspect_relu(prompt_embedding)\n                        ###\u53ef\u4ee5\u52a0\u5165\u6fc0\u6d3b\u51fd\u6570\n                        # prompt_embedding = nn.LeakyReLU(prompt_embedding)\n                        prompt_embedding_list.append(prompt_embedding)\n                    prompt_embedding_ = torch.cat(prompt_embedding_list, dim=0)\n                    embedded[index, prompt_mask[index]] = prompt_embedding_\n            else:\n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    prompt_embedding_ = generated_aspect_prompt[index].repeat(aspect_num, 1)\n\n                    embedded[index, prompt_mask[index]] = prompt_embedding_\n        return embedded\n       \n\n    def forward(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                generated_prompt=None,\n                aspects_num=None,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=False):\n        \"\"\"\n\n        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n        :param image_features: list[FloatTensor], image roi features with length of batch\n        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n        :param output_attentions:\n        :param output_hidden_states:\n        :return: Tuple comprised of:\n            - x (Tensor): the last encoder layer's output of\n              shape (src_len, batch, embed_dim)\n            - encoder_states (List[Tensor]): all intermediate\n              hidden states of shape (src_len, batch, embed_dim).\n              Only populated if output_hidden_states: is True.\n            - all_attentions (List[Tensor]): Attention weights for each layer.\n            During training might not be of length n_layers because of layer dropout.\n        \"\"\"\n        # check attention mask and invert\n        if attention_mask is not None:\n            attention_mask = invert_mask(attention_mask)\n\n        inputs_embeds = self._embed_multi_modal(generated_prompt, aspects_num,\n            input_ids, image_features) * self.embed_scale\n        embed_pos = self.embed_positions(input_ids)\n        x = inputs_embeds + embed_pos\n        x = self.layernorm_embedding(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        encoder_states, all_attentions = [], []\n        for encoder_layer in self.layers:\n            if output_hidden_states:\n                encoder_states.append(x)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability <\n                                  self.layerdrop):  # skip the layer\n                attn = None\n            else:\n                x, attn = encoder_layer(x,\n                                        attention_mask,\n                                        output_attentions=output_attentions)\n\n            if output_attentions:\n                all_attentions.append(attn)\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n        if output_hidden_states:\n            encoder_states.append(x)\n\n        # T x B x C -> B x T x C\n        encoder_states = [\n            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n        ]\n        x = x.transpose(0, 1)\n\n        if not return_dict:\n            return tuple(v for v in [x, encoder_states, all_attentions]\n                         if v is not None)\n        return BaseModelOutput(last_hidden_state=x,\n                               hidden_states=encoder_states,\n                               attentions=all_attentions)", "\n\nclass MultiModalBartEncoder_for_Generating_sentiment_prompt(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n    is a :class:EncoderLayer.\n\n    Args:\n        config: MultiModalBartConfig\n    \"\"\"\n    def __init__(self, use_generated_prompt,\n                 config: MultiModalBartConfig, encoder, img_feat_id, aspect_prompt_token_id, senti_prompt_token_id,\n                 cls_token_id, num_image_tokens, use_different_senti_prompt):\n        super().__init__()\n        \n        self.use_generated_prompt = use_generated_prompt\n        self.aspect_prompt_token_id = aspect_prompt_token_id\n        self.senti_prompt_token_id = senti_prompt_token_id\n        self.use_different_senti_prompt = use_different_senti_prompt\n\n        self.img_feat_id = img_feat_id\n        self.cls_token_id = cls_token_id\n        embed_tokens = encoder.embed_tokens\n        self.dropout = encoder.dropout\n        self.layerdrop = encoder.layerdrop\n\n        self.indentity = nn.Identity()\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_scale = encoder.embed_scale\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = encoder.max_source_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n        self.embed_positions = encoder.embed_positions\n\n        self.layers = encoder.layers\n        self.layernorm_embedding = encoder.layernorm_embedding\n        # mbart has one extra layer_norm\n        self.layer_norm = encoder.layer_norm\n\n        # self.aspect_linear = nn.Linear(768, 768)\n        # self.aspect_relu = nn.LeakyReLU()\n\n    def _embed_multi_modal(self, generated_senti_prompt, aspects_num, input_ids, image_features):\n        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n        # import ipdb; ipdb.set_trace()\n        mask = (input_ids == self.img_feat_id) | (\n            input_ids == self.cls_token_id)\n        # print(mask.shape)\n        embedded_images = self.embed_images(image_features)\n        embedded = self.embed_tokens(input_ids)\n        # print('mask shape', mask.shape)\n        if not embedded_images[0].dtype == torch.float32:\n            embedded = embedded.half()\n\n        for index, value in enumerate(embedded_images):\n            if len(value) > 0:\n                embedded[index, mask[index]] = value\n        \n       \n        \n        if self.use_generated_prompt:\n            senti_prompt_mask = (input_ids == self.senti_prompt_token_id)\n            # import ipdb; ipdb.set_trace()\n            if self.use_different_senti_prompt:\n                # self.aspect_linear = self.aspect_linear.to(generated_senti_prompt.device)\n                # self.aspect_relu = self.aspect_relu.to(generated_senti_prompt.device)\n            \n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n                    prompt_embedding_list = []\n                    for j in range(aspect_num):\n                        aspect_linear = nn.Linear(768, 768).to(generated_senti_prompt.device)\n                        aspect_relu = nn.LeakyReLU().to(generated_senti_prompt.device)\n                        prompt_embedding = aspect_linear(generated_senti_prompt[index])\n                        prompt_embedding = aspect_relu(prompt_embedding)\n                        ###\u53ef\u4ee5\u52a0\u5165\u6fc0\u6d3b\u51fd\u6570\u00e5\n                        # prompt_embedding = nn.LeakyReLU(prompt_embedding)\n                        prompt_embedding_list.append(prompt_embedding)\n                    prompt_embedding_ = torch.cat(prompt_embedding_list, dim=0)\n                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n            else:\n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    prompt_embedding_ = generated_senti_prompt[index].repeat(aspect_num, 1)\n\n                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n        return embedded\n       \n\n    def forward(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                generated_prompt=None,\n                aspects_num=None,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=False):\n        \"\"\"\n\n        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n        :param image_features: list[FloatTensor], image roi features with length of batch\n        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n        :param output_attentions:\n        :param output_hidden_states:\n        :return: Tuple comprised of:\n            - x (Tensor): the last encoder layer's output of\n              shape (src_len, batch, embed_dim)\n            - encoder_states (List[Tensor]): all intermediate\n              hidden states of shape (src_len, batch, embed_dim).\n              Only populated if output_hidden_states: is True.\n            - all_attentions (List[Tensor]): Attention weights for each layer.\n            During training might not be of length n_layers because of layer dropout.\n        \"\"\"\n        # check attention mask and invert\n        if attention_mask is not None:\n            attention_mask = invert_mask(attention_mask)\n\n        inputs_embeds = self._embed_multi_modal(generated_prompt, aspects_num,\n            input_ids, image_features) * self.embed_scale\n        embed_pos = self.embed_positions(input_ids)\n        x = inputs_embeds + embed_pos\n        x = self.layernorm_embedding(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        encoder_states, all_attentions = [], []\n        for encoder_layer in self.layers:\n            if output_hidden_states:\n                encoder_states.append(x)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability <\n                                  self.layerdrop):  # skip the layer\n                attn = None\n            else:\n                x, attn = encoder_layer(x,\n                                        attention_mask,\n                                        output_attentions=output_attentions)\n\n            if output_attentions:\n                all_attentions.append(attn)\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n        if output_hidden_states:\n            encoder_states.append(x)\n\n        # T x B x C -> B x T x C\n        encoder_states = [\n            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n        ]\n        x = x.transpose(0, 1)\n\n        if not return_dict:\n            return tuple(v for v in [x, encoder_states, all_attentions]\n                         if v is not None)\n        return BaseModelOutput(last_hidden_state=x,\n                               hidden_states=encoder_states,\n                               attentions=all_attentions)", "\n\nclass MultiModalBartEncoder_for_Generating_Dual_prompts(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n    is a :class:EncoderLayer.\n\n    Args:\n        config: MultiModalBartConfig\n    \"\"\"\n    def __init__(self, \n                 use_generated_aspect_prompt, use_generated_senti_prompt, \n                 config: MultiModalBartConfig, encoder, img_feat_id, aspect_prompt_token_id, senti_prompt_token_id,\n                 cls_token_id, num_image_tokens, use_different_aspect_prompt, use_different_senti_prompt, \n                 NEU_id, POS_id, NEG_id):\n        super().__init__()\n\n        self.use_generated_aspect_prompt= use_generated_aspect_prompt\n        self.use_generated_senti_prompt = use_generated_senti_prompt\n\n        self.aspect_prompt_token_id = aspect_prompt_token_id\n        self.senti_prompt_token_id = senti_prompt_token_id\n        self.use_different_aspect_prompt = use_different_aspect_prompt\n        self.use_different_senti_prompt = use_different_senti_prompt\n\n        # if self.use_different_senti_prompt:\n        #     self.attention_for_senti_prompt = Attention_for_Senti_Prompt(n_head=8, model_dim=768, drop_rate=0.2)\n        self.img_feat_id = img_feat_id\n        self.cls_token_id = cls_token_id\n        self.neu_id = NEU_id\n        self.pos_id = POS_id\n        self.neg_id = NEG_id\n        embed_tokens = encoder.embed_tokens\n        self.dropout = encoder.dropout\n        self.layerdrop = encoder.layerdrop\n\n        self.indentity = nn.Identity()\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_scale = encoder.embed_scale\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = encoder.max_source_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n        self.embed_positions = encoder.embed_positions\n\n        self.layers = encoder.layers\n        self.layernorm_embedding = encoder.layernorm_embedding\n        # mbart has one extra layer_norm\n        self.layer_norm = encoder.layer_norm\n        self.num_image_tokens = num_image_tokens\n\n        # self.aspect_linear = nn.Linear(768, 768)\n        # self.aspect_relu = nn.LeakyReLU()\n        # self.aspect_linear = nn.Sequential(nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768))\n\n    def _embed_multi_modal(self, generated_aspect_prompt, generated_senti_prompt, aspects_num, input_ids, image_features):\n        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n        # import ipdb; ipdb.set_trace()\n        device = generated_aspect_prompt.device\n        batch_size = input_ids.size(0)\n        mask = (input_ids == self.img_feat_id) | (\n            input_ids == self.cls_token_id)\n        # print(mask.shape)\n        embedded_images = self.embed_images(image_features)\n        embedded = self.embed_tokens(input_ids)\n        # print('mask shape', mask.shape)\n        if not embedded_images[0].dtype == torch.float32:\n            embedded = embedded.half()\n\n        for index, value in enumerate(embedded_images):\n            if len(value) > 0:\n                embedded[index, mask[index]] = value\n        \n\n        new_input_ids =[]\n        for i in range(len(aspects_num)):\n            \n            aspect_num = aspects_num[i]\n            # print('the aspect_num is {}'.format(aspect_num))\n           \n            input_id = input_ids[i]\n\n            if self.num_image_tokens==0:\n                prompt_begin_index = 25\n                prompt_end_index = 54\n            elif self.num_image_tokens==1:\n                prompt_begin_index = 26\n                prompt_end_index = 55\n            elif self.num_image_tokens==2:\n                prompt_begin_index = 27\n                prompt_end_index = 56\n            elif self.num_image_tokens==3:\n                prompt_begin_index = 28\n                prompt_end_index = 57\n            elif self.num_image_tokens==4:\n                prompt_begin_index = 29\n                prompt_end_index = 58\n            elif self.num_image_tokens==5:\n                prompt_begin_index = 30\n                prompt_end_index = 59\n            elif self.num_image_tokens==6:\n                prompt_begin_index = 31\n                prompt_end_index = 60\n            elif self.num_image_tokens==7:\n                prompt_begin_index = 32\n                prompt_end_index = 61 \n                \n            # print('before')\n            # print(len(input_id))\n            # import ipdb; ipdb.set_trace()\n            reserve_aspect_id = input_id[prompt_begin_index:prompt_begin_index+6*aspect_num]\n            if aspect_num ==5:\n                # print('aspect_num is 5')\n                # print(reserve_aspect_id)\n                new_input_id = torch.cat([input_id[:prompt_begin_index], reserve_aspect_id, input_id[prompt_end_index+1:]])\n            else:\n                cut_aspect_id = torch.ones_like(input_id[prompt_begin_index+6*aspect_num:prompt_end_index])\n                new_input_id = torch.cat([input_id[:prompt_begin_index], reserve_aspect_id, cut_aspect_id, input_id[prompt_end_index:]])\n            # print(\"++++++++++++++++++++cut_aspect_id++++++++++++++++++++++++\")\n            # print(cut_aspect_id)\n            # print(input_id[58:])\n            new_input_ids.append(new_input_id)\n            # print('the shape of new_input_id is {}'.format(new_input_id.shape))\n            # print(new_input_id[58:])\n\n        new_input_ids = torch.stack(new_input_ids)\n\n        \n        if self.use_generated_aspect_prompt:\n            ##aspect_prompt\n\n            # import ipdb; ipdb.set_trace()\n            aspect_prompt_mask = (new_input_ids == self.aspect_prompt_token_id) ##[29:58]: \u4e00\u51715\u7ec4:[50288, 50288,     9, 50289,  5702, 50284,]\n            if self.use_different_aspect_prompt:\n                # self.aspect_linear = self.aspect_linear.to(device)\n                # self.aspect_relu = self.aspect_relu.to(device)\n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n                    aspect_prompt_embedding_list = []\n                    for j in range(aspect_num):\n                        aspect_linear = nn.Linear(768, 768).to(generated_aspect_prompt.device) ##\u6bcf\u4e2aaspect\u6709\u81ea\u5df1\u7684\u53d8\u6362\uff0c\u4e3a\u6bcf\u4e2aaspect\u8bbe\u8ba1\u7279\u5b9a\u7684prompt\n                        aspect_relu = nn.LeakyReLU().to(generated_aspect_prompt.device)\n                        aspect_prompt_embedding = aspect_linear(generated_aspect_prompt[index])\n                        aspect_prompt_embedding = aspect_relu(aspect_prompt_embedding)\n                        aspect_prompt_embedding_list.append(aspect_prompt_embedding)\n                    aspect_prompt_embedding_ = torch.cat(aspect_prompt_embedding_list, dim=0)\n                    embedded[index, aspect_prompt_mask[index]] = aspect_prompt_embedding_\n            else:\n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    aspect_prompt_embedding_ = generated_aspect_prompt[index].repeat(aspect_num, 1)\n                    embedded[index, aspect_prompt_mask[index]] = aspect_prompt_embedding_\n\n        ##sentiment_prompt\n       \n        if self.use_generated_senti_prompt:\n            '''\n            # if self.use_different_senti_prompt:\n            \u4ee5\u4e0b\u4f7f\u7528\u7684\u662fattention\u673a\u5236\uff0csenti_prompt_token\u548csentiments_embdedding\n            # sentiments_ids = torch.tensor([self.neu_id, self.pos_id, self.neg_id]).to(device)\n            # sentiments_embdedding = self.embed_tokens(sentiments_ids)\n            # senti_prompt_mask = (input_ids == self.senti_prompt_token_id)\n            # for index in range(len(aspects_num)):\n                \n            #     aspect_num = aspects_num[index]\n            #     expanded_sentiments_embdedding = sentiments_embdedding.expand(aspect_num, sentiments_embdedding.size(0), sentiments_embdedding.size(1))\n            #     original_senti_prompt = embedded[index, senti_prompt_mask[index]].unsqueeze(1)\n            #     new_senti_prompt = self.attention_for_senti_prompt(original_senti_prompt, expanded_sentiments_embdedding, expanded_sentiments_embdedding).squeeze()\n            #     # import ipdb; ipdb.set_trace()\n            #     embedded[index, senti_prompt_mask[index]] = new_senti_prompt\n            '''\n            ##\u6362\u6210senti_prompt\u4e5f\u662f\u751f\u6210\u5f62\u5f0f\u770b\u770b\n            senti_prompt_mask = (new_input_ids == self.senti_prompt_token_id)\n            # import ipdb; ipdb.set_trace()\n            if self.use_different_senti_prompt:\n                # self.aspect_linear = self.aspect_linear.to(generated_senti_prompt.device)\n                # self.aspect_relu = self.aspect_relu.to(generated_senti_prompt.device)\n            \n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n                    prompt_embedding_list = []\n                    for j in range(aspect_num):\n                        senti_linear = nn.Linear(768, 768).to(generated_senti_prompt.device)\n                        senti_relu = nn.LeakyReLU().to(generated_senti_prompt.device)\n                        prompt_embedding = senti_linear(generated_senti_prompt[index])\n                        prompt_embedding = senti_relu(prompt_embedding)\n                        prompt_embedding_list.append(prompt_embedding)\n                    prompt_embedding_ = torch.cat(prompt_embedding_list, dim=0)\n                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n            else:\n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    prompt_embedding_ = generated_senti_prompt[index].repeat(aspect_num, 1)\n\n                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n\n\n        return embedded\n       \n\n    def forward(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                generated_aspect_prompt=None,\n                generated_senti_prompt=None,\n                aspects_num=None,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=False):\n        \"\"\"\n\n        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n        :param image_features: list[FloatTensor], image roi features with length of batch\n        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n        :param output_attentions:\n        :param output_hidden_states:\n        :return: Tuple comprised of:\n            - x (Tensor): the last encoder layer's output of\n              shape (src_len, batch, embed_dim)\n            - encoder_states (List[Tensor]): all intermediate\n              hidden states of shape (src_len, batch, embed_dim).\n              Only populated if output_hidden_states: is True.\n            - all_attentions (List[Tensor]): Attention weights for each layer.\n            During training might not be of length n_layers because of layer dropout.\n        \"\"\"\n        # check attention mask and invert\n        if attention_mask is not None:\n            attention_mask = invert_mask(attention_mask)\n\n        inputs_embeds = self._embed_multi_modal(generated_aspect_prompt, generated_senti_prompt, aspects_num,\n            input_ids, image_features) * self.embed_scale\n        embed_pos = self.embed_positions(input_ids)\n        x = inputs_embeds + embed_pos\n        x = self.layernorm_embedding(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        encoder_states, all_attentions = [], []\n        for encoder_layer in self.layers:\n            if output_hidden_states:\n                encoder_states.append(x)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability <\n                                  self.layerdrop):  # skip the layer\n                attn = None\n            else:\n                x, attn = encoder_layer(x,\n                                        attention_mask,\n                                        output_attentions=output_attentions)\n\n            if output_attentions:\n                all_attentions.append(attn)\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n        if output_hidden_states:\n            encoder_states.append(x)\n\n        # T x B x C -> B x T x C\n        encoder_states = [\n            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n        ]\n        x = x.transpose(0, 1)\n\n        if not return_dict:\n            return tuple(v for v in [x, encoder_states, all_attentions]\n                         if v is not None)\n        return BaseModelOutput(last_hidden_state=x,\n                               hidden_states=encoder_states,\n                               attentions=all_attentions)", "\nclass MultiModalBartDecoder_span(nn.Module\n                                 ):  #AOE task and all downstream tasks\n    def __init__(self,\n                 config: MultiModalBartConfig,\n                 tokenizer,\n                 decoder,\n                 pad_token_id,\n                 label_ids,\n                 causal_mask,\n                 num_image_tokens=2,\n                 need_tag=True,\n                 only_sc=False,\n                 avg_feature=False,\n                 use_encoder_mlp=True):\n        super().__init__()\n        self.decoder = decoder\n        self.tokenizer = tokenizer\n        self.causal_mask = causal_mask\n        self.register_buffer('causal_masks', causal_mask.float())\n        self.pad_token_id = pad_token_id\n        # label_ids = sorted(label_ids, reverse=False)\n        self.label_start_id = min(label_ids)\n        self.label_end_id = max(label_ids) + 1\n        self.need_tag = need_tag\n        self.only_sc = only_sc\n        self.num_image_tokens = num_image_tokens\n        mapping = torch.LongTensor([0, 2] + label_ids)\n        ###mapping: [0, 2, 50276, 50277, 50278, 50281]\n        self.register_buffer('mapping', mapping)\n        self.src_start_index = len(mapping)  # \u52a0\u4e0a\u4e00\u4e2a\n        hidden_size = decoder.embed_tokens.weight.size(1)\n        self.dropout_layer = nn.Dropout(0.1)\n\n        self.end_text_id = tokenizer.end_text_id\n        self.avg_feature = avg_feature\n        if use_encoder_mlp:\n            self.encoder_mlp = nn.Sequential(\n                nn.Linear(hidden_size, hidden_size), nn.Dropout(0.3),\n                nn.ReLU(), nn.Linear(hidden_size, hidden_size))\n\n    def forward(self, tokens, state, only_sc=False):\n        # import ipdb; ipdb.set_trace()\n        '''\n        tokens: [[0, 2, 2, 16, 16, 4, 18, 18, 4, 1, 1, 1, 1],\n                 [0, 2, 2, 15, 16, 3, 25, 26, 5, 28, 28, 4, 1]]\n        '''\n        # import ipdb; ipdb.set_trace()\n        bsz, max_len = tokens.size()\n        encoder_outputs = state.encoder_output ##(batch, 72=38(len(image_token+begin_image+end_image(36+1+1)))+34(max_tex_len(\u5305\u542bbegin_text_id(0) and end_text_id(2)) in batch), 768)\n        encoder_pad_mask = state.encoder_mask ##(batch, 72)\n        first = state.first\n        # tokens\u4e4b\u540e\u76840\u5168\u662fpadding\uff0c\u56e0\u4e3a1\u662feos, \u5728pipe\u4e2d\u89c4\u5b9a\u7684\n        cumsum = tokens.eq(1).flip(dims=[1]).cumsum(dim=-1)\n        tgt_pad_mask = cumsum.flip(dims=[1]).ne(cumsum[:, -1:])\n\n        # \u628a\u8f93\u5165\u505a\u4e00\u4e0b\u6620\u5c04\n        mapping_token_mask = tokens.lt(\n            self.src_start_index)  # \u4e3a1\u7684\u5730\u65b9\u5e94\u8be5\u4ecemapping\u4e2d\u53d6index\n        mapped_tokens = tokens.masked_fill(tokens.ge(self.src_start_index), 0)\n        tag_mapped_tokens = self.mapping[mapped_tokens]\n\n        src_tokens_index = tokens - self.src_start_index  # bsz x num_src_token\n        src_tokens_index = src_tokens_index.masked_fill(\n            src_tokens_index.lt(0), 0)\n        src_tokens = state.src_tokens \n        # print(src_tokens.shape): (2, 34)\n        if first is not None:\n            src_tokens = src_tokens.gather(index=first, dim=1) ###Sequence\n        word_mapped_tokens = src_tokens.gather(index=src_tokens_index, dim=1)\n        # print('word_mapped_tokens', word_mapped_tokens)\n        tokens = torch.where(mapping_token_mask, tag_mapped_tokens,\n                             word_mapped_tokens)\n\n        tokens = tokens.masked_fill(tgt_pad_mask, self.pad_token_id)\n        '''\n        {'AESC': 50281, 'POS': 50276, 'NEU': 50277, 'NEG': 50278}\n        tensor([[0, 50276, 50276, 4644, 4644, 50278, 798, 798, 50278, 2, 1, 1, 1],\n                [0, 50276, 50276, 9517, 957, 50277, 2561, 7772, 50281, 2762, 2762, 50278, 2]])\n        \u5c06tokens\u4e2d\u7684index\u4ee5\u53ca\u6807\u7b7e\u90fd\u8f6c\u5316\u4e3avocabulary\u4e2d\u7684token_id\n        '''\n\n        if self.training:\n            tokens = tokens[:, :-1]\n            decoder_pad_mask = tokens.eq(\n                self.pad_token_id)  # decoder\u9700\u8981\u8ba9pad\u4f4d\u7f6e\u4e3a1\n\n            dict = self.decoder(input_ids=tokens,\n                                encoder_hidden_states=encoder_outputs,\n                                encoder_padding_mask=encoder_pad_mask,\n                                decoder_padding_mask=decoder_pad_mask,\n                                decoder_causal_mask=self.\n                                causal_masks[:tokens.size(1), :tokens.size(1)],\n                                return_dict=True)\n        else:\n\n            past_key_values = state.past_key_values\n            dict = self.decoder(input_ids=tokens,\n                                encoder_hidden_states=encoder_outputs,\n                                encoder_padding_mask=encoder_pad_mask,\n                                decoder_padding_mask=None,\n                                decoder_causal_mask=self.\n                                causal_masks[:tokens.size(1), :tokens.size(1)],\n                                return_dict=True)\n\n        hidden_state = dict.last_hidden_state  # bsz x max_len x hidden_size (2, 12(\u53bb\u6389\u4e86 end_token_id), 768)\n        hidden_state = self.dropout_layer(hidden_state)\n        if not self.training:\n            state.past_key_values = dict.past_key_values\n\n        logits = hidden_state.new_full(\n            (hidden_state.size(0), hidden_state.size(1),\n             self.src_start_index + src_tokens.size(-1)),\n            fill_value=-1e24)\n        ##\u5efa\u7acb\u7a7a\u7684logits\n        # print('logits', logits.shape) (bsz, max_len,  self.src_start_index + src_tokens.size(-1)) -> (2, 12, 40=6+34)\n        # \u9996\u5148\u8ba1\u7b97\u7684\u662f\n\n        if self.need_tag:\n            '''\n            self.decoder.embed_tokens.weight: (50289, 768)\n            self.label_start_id: 50276\n            '''\n            tag_scores = F.linear(\n                hidden_state,\n                self.dropout_layer(\n                    self.decoder.embed_tokens.\n                    weight[self.label_start_id:self.label_start_id +\n                           3]))  # bsz x max_len x num_class\n            logits[:, :, 3:self.src_start_index] = tag_scores ###\u7ed9\u60c5\u611f\u7684position\u8d4b\u503c[:, :, (3, 4, 5)]\n        if not only_sc:\n            eos_scores = F.linear(\n                hidden_state,\n                self.dropout_layer(self.decoder.embed_tokens.weight[2:3])) \n            '''\n            ['</s>(eos_token)', '<mask>', '<pad>', '<s>(bos_token)', '<unk>']\n            [2, 50264, 1, 0, 3]\n            '''\n\n            # bsz x max_bpe_len(image_len + text_len) x hidden_size: (2, 72, 768)\n            src_outputs = state.encoder_output \n            if self.num_image_tokens==0:\n                end_index = 62\n            elif self.num_image_tokens==1:\n                end_index = 63\n            elif self.num_image_tokens==2:\n                end_index = 64\n            elif self.num_image_tokens==3:\n                end_index = 65\n            elif self.num_image_tokens==4:\n                end_index = 66\n            elif self.num_image_tokens==5:\n                end_index = 67\n            elif self.num_image_tokens==6:\n                end_index = 68\n            elif self.num_image_tokens==7:\n                end_index = 69\n\n            \n            if hasattr(self, 'encoder_mlp') and not only_sc:\n                src_outputs = self.encoder_mlp(src_outputs)\n\n            if first is not None:\n                mask = first.eq(0)\n                src_outputs = src_outputs.gather(\n                    index=first.unsqueeze(2).repeat(1, 1,\n                                                    src_outputs.size(-1)),\n                    dim=1)\n            else:\n                mask = state.encoder_mask[:, end_index:].eq(0)\n                # src_outputs = self.decoder.embed_tokens(src_tokens)\n            mask = mask.unsqueeze(1) ## bsz x 1 x max_word_len: (2, 1, 34)\n            input_embed = self.decoder.embed_tokens(\n                src_tokens)  #bsz x max_word_len x hidden_size: (2, 34, 768); src_tokens: (2, 34)\n            input_embed = self.dropout_layer(input_embed)\n            if self.avg_feature:  # \u5148\u628afeature\u5408\u5e76\u4e00\u4e0b\n                src_outputs = (src_outputs[:, end_index:] + input_embed) / 2\n            word_scores = torch.einsum(\n                'blh,bnh->bln', hidden_state,\n                src_outputs[:, end_index:])  # bsz x max_len x max_word_len: (2, 12, 34)\n            if not self.avg_feature:\n                gen_scores = torch.einsum(\n                    'blh,bnh->bln', hidden_state,\n                    input_embed)  # bsz x max_len x max_word_len: (2, 12, 34)\n                word_scores = (gen_scores + word_scores) / 2 \n            mask = mask.__or__(\n                src_tokens.eq(2).cumsum(dim=1).ge(1).unsqueeze(1)) ###(2, 1, 34)\n            word_scores = word_scores.masked_fill(mask, -1e32) ###(bts, max_len, max_word_len)\n            logits[:, :, self.src_start_index:] = word_scores\n            ###logits.shape (bts, max_len, max_word_len+6): (2, 12, 40)\n            logits[:, :, 1:2] = eos_scores\n        # print(torch.argmax(logits[0], dim=-1))\n        return logits\n\n    def decode(self, tokens, state, only_sc=False):\n        return self(tokens, state, only_sc)[:, -1]", "\n\n\nclass Span_loss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.loss_fct = nn.CrossEntropyLoss()\n        self.fc = nn.LogSoftmax(dim=-1)\n\n    def forward(self, tgt_tokens, pred, mask):\n        '''\n        tgt_tokens: (2 (batch-size), 12 (max_len+1))\n        pred: (2, 12, 40 (max_word_len))\n        '''\n\n        tgt_tokens = tgt_tokens.masked_fill(mask.eq(0), -100)\n        output = F.cross_entropy(target=tgt_tokens, input=pred.transpose(1, 2)) ##\u6bcf\u4e00\u4e2a\u8bcd\u90fd\u670912\u79cd\u7c7b\u522b\uff0c input= (40, 12)\n        return output", "\n\nclass MultiModalBartDecoder_MLM(nn.Module):\n    def __init__(self, config: MultiModalBartConfig, decoder):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.register_buffer(\n            \"final_logits_bias\",\n            torch.zeros((1, self.decoder.embed_tokens.num_embeddings)))\n\n    def forward(self, labels, input_ids, encoder_outputs, attention_mask,\n                decoder_input_ids, decoder_attention_mask):\n        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n            self.config,\n            input_ids,\n            decoder_input_ids=decoder_input_ids,\n            decoder_padding_mask=decoder_attention_mask,\n            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)\n        decoder_outputs = self.decoder(\n            decoder_input_ids,\n            encoder_outputs,\n            attention_mask,\n            decoder_padding_mask,\n            decoder_causal_mask=causal_mask[:decoder_input_ids.size(1), :\n                                            decoder_input_ids.size(1)],\n        )\n\n        lm_logits = F.linear(decoder_outputs[0][:, 1:],\n                             self.decoder.embed_tokens.weight,\n                             bias=self.final_logits_bias)\n\n        lm_loss = 0\n        # compute lm loss if labels is given\n        if labels is not None:\n            labels = labels.clone()\n            loss_fct = nn.CrossEntropyLoss()\n            lm_loss = loss_fct(\n                lm_logits.view(-1, self.decoder.embed_tokens.weight.size(0)),\n                labels.reshape(-1))\n\n            return lm_loss", "\n\nclass MultiModalBartDecoder_ANP_generate(nn.Module):  #AOG task\n    def __init__(self, config: MultiModalBartConfig, decoder):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.register_buffer(\n            \"final_logits_bias\",\n            torch.zeros((1, self.decoder.embed_tokens.num_embeddings)))\n\n    def forward(self, labels, input_ids, encoder_outputs, attention_mask,\n                decoder_input_ids, decoder_attention_mask):\n        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n            self.config,\n            input_ids,\n            decoder_input_ids=decoder_input_ids,\n            decoder_padding_mask=decoder_attention_mask,\n            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)\n\n        decoder_outputs = self.decoder(\n            decoder_input_ids,\n            encoder_outputs,\n            attention_mask,\n            decoder_padding_mask,\n            decoder_causal_mask=causal_mask[:decoder_input_ids.size(1), :\n                                            decoder_input_ids.size(1)],\n        )\n\n        lm_logits = F.linear(decoder_outputs[0][:, 1:],\n                             self.decoder.embed_tokens.weight,\n                             bias=self.final_logits_bias)\n\n        lm_loss = 0\n        # compute lm loss if labels is given\n        if labels is not None:\n            labels = labels.clone()\n            # labels[labels == self.cls_token_id] = -100\n            loss_fct = nn.CrossEntropyLoss()\n            lm_loss = loss_fct(\n                lm_logits.view(-1, self.decoder.embed_tokens.weight.size(0)),\n                labels.reshape(-1))\n\n            return lm_loss", "\n\nclass MultiModalBartDecoder_sentiment(nn.Module):  #MSP task\n    def __init__(self,\n                 config: MultiModalBartConfig,\n                 decoder,\n                 senti_ids,\n                 senti_nums=3):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.senti_ids = senti_ids\n        self.dropout_layer = nn.Dropout(0.1)\n        self.senti_head = BartClassificationHead(config.d_model,\n                                                 config.d_model, senti_nums,\n                                                 config.classif_dropout)\n\n    def _init_weights(self, module):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, senti_labels, encoder_outputs, attention_mask,\n                senti_decoder_input_ids):\n\n        decoder_outputs = self.decoder(\n            input_ids=senti_decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            encoder_padding_mask=attention_mask,\n            decoder_padding_mask=None,\n            decoder_causal_mask=None,\n        )\n\n        # predict_senti = F.linear(\n        #     decoder_outputs[0][:, 1],\n        #     self.dropout_layer(self.decoder.embed_tokens.\n        #                        weight[self.senti_ids[0]:self.senti_ids[2] +\n        #                               1]))  # bsz\n        # predict_senti = torch.flip(predict_senti, dims=[-1])\n        predict_senti = self.senti_head(decoder_outputs[0][:, 1])\n        loss_fct = nn.CrossEntropyLoss()\n        senti_loss = loss_fct(predict_senti, senti_labels)\n        return senti_loss, predict_senti", "\n\nclass MultiModalBartDecoder_MRM(nn.Module):\n    def __init__(self, config: MultiModalBartConfig, decoder, causal_mask,\n                 args):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.causal_mask = causal_mask\n        self.args = args\n        self.mrm_head = BartClassificationHead(\n            config.d_model,\n            config.d_model,\n            config.num_labels,\n            config.classif_dropout,\n        )\n        self._init_weights(self.mrm_head.dense)\n        self._init_weights(self.mrm_head.out_proj)\n\n    def _init_weights(self, module):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, mrm_labels, mrm_masks, encoder_outputs, attention_mask,\n                mrm_decoder_input_ids, mrm_decoder_attention_mask):\n\n        decoder_padding_mask = mrm_decoder_attention_mask.eq(0)\n        decoder_outputs = self.decoder(\n            input_ids=mrm_decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            encoder_padding_mask=attention_mask,\n            decoder_padding_mask=decoder_padding_mask,\n            decoder_causal_mask=self.causal_mask[:mrm_decoder_input_ids.size(\n                1), :mrm_decoder_input_ids.size(1)].to(\n                    mrm_decoder_input_ids.device),\n        )\n        region_representation = decoder_outputs[0][mrm_masks.bool()]\n        if len(region_representation) > 0:\n            predict_cls = self.mrm_head(region_representation)\n            loss_fct = nn.CrossEntropyLoss()\n            mrm_labels = torch.cat(mrm_labels,\n                                   dim=0).to(encoder_outputs.device)\n\n            if self.args.mrm_loss_type == 'KL':\n                predict_cls = F.log_softmax(predict_cls, dim=-1)\n                mrm_loss = F.kl_div(predict_cls.double(),\n                                    mrm_labels.double().squeeze(1),\n                                    reduction='batchmean')\n            else:\n                raise RuntimeError(\"wrong mrm type\")\n        else:\n            mrm_loss = 0\n\n        return mrm_loss", "\n\n'''\ngenerate_aspect_prompt based on the multimodal context\n'''\nclass MultiModalBartDecoder_generate_aspect_prompt(nn.Module): \n    def __init__(self, config: MultiModalBartConfig, decoder):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.aspect_prompt_linear = nn.Linear(768, 768)\n\n\n    def forward(self, encoder_outputs, attention_mask,\n                decoder_input_ids, decoder_attention_mask):\n\n        # import ipdb; ipdb.set_trace()\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            encoder_padding_mask=attention_mask.eq(0),\n            decoder_padding_mask=decoder_attention_mask.eq(0),\n            decoder_causal_mask=None,\n        )\n\n        prompt_logits = decoder_outputs[0]\n        aspect_prompt_logits = self.aspect_prompt_linear(prompt_logits)\n\n\n        return aspect_prompt_logits ", "\n\n'''\ngenerate_sentiment_prompt based on the multimodal context\n'''\nclass MultiModalBartDecoder_generate_sentiment_prompt(nn.Module): \n    def __init__(self, config: MultiModalBartConfig, decoder):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.senti_prompt_linear = nn.Linear(768, 768)\n\n\n    def forward(self, encoder_outputs, attention_mask,\n                decoder_input_ids, decoder_attention_mask):\n\n        # import ipdb; ipdb.set_trace()\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            encoder_padding_mask=attention_mask.eq(0),\n            decoder_padding_mask=decoder_attention_mask.eq(0),\n            decoder_causal_mask=None,\n        )\n\n        prompt_logits = decoder_outputs[0]\n        senti_prompt_logits = self.senti_prompt_linear(prompt_logits)\n\n\n        return senti_prompt_logits", "\n\nclass MultiModalBartDecoder_aspects_num(nn.Module):  #MSP task\n    def __init__(self,\n                 config: MultiModalBartConfig,\n                 decoder,\n                 max_aspects_nums=5):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.dropout_layer = nn.Dropout(0.1)\n        self.aspects_num_head = BartClassificationHead(config.d_model,\n                                                 config.d_model, max_aspects_nums,\n                                                 config.classif_dropout)\n\n    def _init_weights(self, module):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, aspects_num_labels, encoder_outputs, attention_mask,\n                aspects_num_decoder_input_ids):\n        \n        decoder_outputs = self.decoder(\n            input_ids=aspects_num_decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            encoder_padding_mask=attention_mask,\n            decoder_padding_mask=None,\n            decoder_causal_mask=None,\n        )\n\n        # predict_aspects_num = F.linear(\n        #     decoder_outputs[0][:, 1],\n        #     self.dropout_layer(self.decoder.embed_tokens.\n        #                        weight[self.aspects_num_ids[0]:self.aspects_num_ids[2] +\n        #                               1]))  # bsz\n        # predict_aspects_num = torch.flip(predict_aspects_num, dims=[-1])\n        predict_aspects_num_logits = self.aspects_num_head(decoder_outputs[0][:, 1])\n        loss_fct = nn.CrossEntropyLoss()\n        aspects_num_labels = torch.tensor(aspects_num_labels).to(predict_aspects_num_logits.device)\n        aspects_num_loss = loss_fct(predict_aspects_num_logits, aspects_num_labels)\n        return aspects_num_loss, predict_aspects_num_logits"]}
{"filename": "src/model/mixins.py", "chunked_list": ["import os\nimport itertools\n\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\nfrom torch.nn import Parameter\n\n# from transformers.generation_utils import logger, Iterable", "\n# from transformers.generation_utils import logger, Iterable\nfrom typing import Iterable\nimport logging as logger\n# from transformers.models.bart.modeling_bart import _make_linear_from_emb\n# from transformers.modeling_bart import (\n#     _reorder_buffer,\n#     _make_linear_from_emb\n# )\n# from transformers.utils.hub import hf_bucket_url", "# )\n# from transformers.utils.hub import hf_bucket_url\nfrom transformers.modeling_utils import (\n    # hf_bucket_url,\n    # cached_path,\n    TF2_WEIGHTS_NAME,\n    WEIGHTS_NAME,\n    TF_WEIGHTS_NAME,\n    is_remote_url,\n    PretrainedConfig", "    is_remote_url,\n    PretrainedConfig\n)\n\ndef _make_linear_from_emb(emb):\n    vocab_size, emb_size = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer\n\ndef _reorder_buffer(attn_cache, new_order):\n    for k, input_buffer_k in attn_cache.items():\n        if input_buffer_k is not None:\n            attn_cache[k] = input_buffer_k.index_select(0, new_order)\n    return attn_cache", "\ndef _reorder_buffer(attn_cache, new_order):\n    for k, input_buffer_k in attn_cache.items():\n        if input_buffer_k is not None:\n            attn_cache[k] = input_buffer_k.index_select(0, new_order)\n    return attn_cache\n\n# This is based on transformers.generation_utils\n# The modifications are:\n# - image_features parameter", "# The modifications are:\n# - image_features parameter\n# - removed unused code\n# - added image_features in prepare_inputs_for_generation\nclass GenerationMixin:\n    @torch.no_grad()\n    def generate(\n            self,\n            input_ids: Optional[torch.LongTensor] = None,\n            image_features=None,\n            max_length: Optional[int] = None,\n            min_length: Optional[int] = None,\n            do_sample: Optional[bool] = None,\n            early_stopping: Optional[bool] = None,\n            num_beams: Optional[int] = None,\n            temperature: Optional[float] = None,\n            top_k: Optional[int] = None,\n            top_p: Optional[float] = None,\n            repetition_penalty: Optional[float] = None,\n            bad_words_ids: Optional[Iterable[int]] = None,\n            bos_token_id: Optional[int] = None,\n            pad_token_id: Optional[int] = None,\n            eos_token_id: Optional[int] = None,\n            length_penalty: Optional[float] = None,\n            no_repeat_ngram_size: Optional[int] = None,\n            num_return_sequences: Optional[int] = None,\n            attention_mask: Optional[torch.LongTensor] = None,\n            decoder_start_token_id: Optional[int] = None,\n            use_cache: Optional[bool] = None,\n            **model_specific_kwargs\n    ) -> torch.LongTensor:\n        \"\"\"\n        Generates sequences for models with a LM head. The method currently supports greedy decoding, beam-search\n        decoding, sampling with temperature, sampling with top-k or nucleus sampling.\n\n        Parameters:\n\n            input_ids: (optional) torch.LongTensor of shape (batch_size, sequence_length)\n                The sequence used as a prompt for the generation. If None the method initializes\n                it as an empty torch.LongTensor of shape (1,).\n\n            image_features: (optional) list of torch.LongTensor\n                The image ROI features generated by R-CNN\n\n            max_length: (optional) int\n                The max length of the sequence to be generated.  Between min_length and infinity. Default to 20.\n\n            min_length: (optional) int\n                The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.\n\n            do_sample: (optional) bool\n                If set to False greedy decoding is used. Otherwise sampling is used. Defaults to False as defined\n                in configuration_utils.PretrainedConfig.\n\n            early_stopping: (optional) bool\n                if set to True beam search is stopped when at least num_beams sentences finished per batch.\n                Defaults to False as defined in configuration_utils.PretrainedConfig.\n\n            num_beams: (optional) int\n                Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n\n            temperature: (optional) float\n                The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.\n\n            top_k: (optional) int\n                The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity.\n                Default to 50.\n\n            top_p: (optional) float\n                The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus\n                sampling. Must be between 0 and 1. Default to 1.\n\n            repetition_penalty: (optional) float\n                The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n\n            pad_token_id: (optional) int\n                Padding token. Default to specicic model pad_token_id or None if it does not exist.\n\n            bos_token_id: (optional) int\n                BOS token. Defaults to bos_token_id as defined in the models config.\n\n            eos_token_id: (optional) int\n                EOS token. Defaults to eos_token_id as defined in the models config.\n\n            length_penalty: (optional) float\n                Exponential penalty to the length. Default to 1.\n\n            no_repeat_ngram_size: (optional) int\n                If set to int > 0, all ngrams of size no_repeat_ngram_size can only occur once.\n            bad_words_ids: (optional) list of lists of int\n                bad_words_ids contains tokens that are not allowed to be generated. In order to get the tokens of the\n                words that should not appear in the generated text, use\n                tokenizer.encode(bad_word, add_prefix_space=True).\n\n            num_return_sequences: (optional) int\n                The number of independently computed returned sequences for each element in the batch. Default to 1.\n\n            attention_mask (optional) obj: torch.LongTensor of same shape as input_ids\n                Mask to avoid performing attention on padding token indices.\n                Mask values selected in [0, 1]:\n                1 for tokens that are NOT MASKED, 0 for MASKED tokens.\n                Defaults to None.\n\n                What are attention masks? <../glossary.html#attention-mask>__\n\n            decoder_start_token_id=None: (optional) int\n                If an encoder-decoder model starts decoding with a different token than BOS.\n                Defaults to None and is changed to BOS later.\n\n            use_cache: (optional) bool\n                If use_cache is True, past key values are used to speed up decoding if applicable to model.\n                Defaults to True.\n\n            model_specific_kwargs: (optional) dict\n                Additional model specific kwargs will be forwarded to the forward function of the model.\n\n        Return:\n\n            output: torch.LongTensor of shape (batch_size * num_return_sequences, sequence_length)\n                sequence_length is either equal to max_length or shorter if all batches finished early due to\n                the eos_token_id\n        \"\"\"\n\n        max_length = max_length if max_length is not None else self.config.max_length\n        min_length = min_length if min_length is not None else self.config.min_length\n        do_sample = do_sample if do_sample is not None else self.config.do_sample\n        early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        num_beams = num_beams if num_beams is not None else self.config.num_beams\n        temperature = temperature if temperature is not None else self.config.temperature\n        top_k = top_k if top_k is not None else self.config.top_k\n        top_p = top_p if top_p is not None else self.config.top_p\n        repetition_penalty = repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty\n        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n        length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n        no_repeat_ngram_size = (\n            no_repeat_ngram_size if no_repeat_ngram_size is not None else self.config.no_repeat_ngram_size\n        )\n        bad_words_ids = bad_words_ids if bad_words_ids is not None else self.config.bad_words_ids\n        num_return_sequences = (\n            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n        )\n        decoder_start_token_id = (\n            decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id\n        )\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]  # overriden by the input batch_size\n        else:\n            batch_size = 1\n\n        assert isinstance(max_length, int) and max_length > 0, \"max_length should be a strictly positive integer.\"\n        assert isinstance(min_length, int) and min_length >= 0, \"min_length should be a positive integer.\"\n        assert isinstance(do_sample, bool), \"do_sample should be a boolean.\"\n        assert isinstance(early_stopping, bool), \"early_stopping should be a boolean.\"\n        assert isinstance(use_cache, bool), \"use_cache should be a boolean.\"\n        assert isinstance(num_beams, int) and num_beams > 0, \"num_beams should be a strictly positive integer.\"\n        assert temperature > 0, \"temperature should be strictly positive.\"\n        assert isinstance(top_k, int) and top_k >= 0, \"top_k should be a positive integer.\"\n        assert 0 <= top_p <= 1, \"top_p should be between 0 and 1.\"\n        assert repetition_penalty >= 1.0, \"repetition_penalty should be >= 1.\"\n        assert input_ids is not None or (\n                isinstance(bos_token_id, int) and bos_token_id >= 0\n        ), \"If input_ids is not defined, bos_token_id should be a positive integer.\"\n        assert pad_token_id is None or (\n                isinstance(pad_token_id, int) and (pad_token_id >= 0)\n        ), \"pad_token_id should be a positive integer.\"\n        assert (eos_token_id is None) or (\n                isinstance(eos_token_id, int) and (eos_token_id >= 0)\n        ), \"eos_token_id should be a positive integer.\"\n        assert length_penalty > 0, \"length_penalty should be strictly positive.\"\n        assert (\n                isinstance(no_repeat_ngram_size, int) and no_repeat_ngram_size >= 0\n        ), \"no_repeat_ngram_size should be a positive integer.\"\n        assert (\n                isinstance(num_return_sequences, int) and num_return_sequences > 0\n        ), \"num_return_sequences should be a strictly positive integer.\"\n        assert (\n                bad_words_ids is None or isinstance(bad_words_ids, list) and isinstance(bad_words_ids[0], list)\n        ), \"bad_words_ids is either None or a list of lists of tokens that should not be generated\"\n\n        if input_ids is None:\n            assert isinstance(bos_token_id, int) and bos_token_id >= 0, (\n                \"you should either supply a context to complete as input_ids input \"\n                \"or a bos_token_id (integer >= 0) as a first token to start the generation.\"\n            )\n            input_ids = torch.full(\n                (batch_size, 1), bos_token_id, dtype=torch.long, device=next(self.parameters()).device,\n            )\n        else:\n            assert input_ids.dim() == 2, \"Input prompt should be of shape (batch_size, sequence length).\"\n\n        # not allow to duplicate outputs when greedy decoding\n        if do_sample is False:\n            if num_beams == 1:\n                # no_beam_search greedy generation conditions\n                assert (\n                        num_return_sequences == 1\n                ), \"Greedy decoding will always produce the same output for num_beams == 1 and \" \\\n                   \"num_return_sequences > 1. Please set num_return_sequences = 1\"\n\n            else:\n                # beam_search greedy generation conditions\n                assert (\n                        num_beams >= num_return_sequences\n                ), \"Greedy beam search decoding cannot return more sequences than it has beams. \" \\\n                   \"Please set num_beams >= num_return_sequences\"\n\n        # create attention mask if necessary\n        # TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140\n        if (attention_mask is None) and (pad_token_id is not None) and (pad_token_id in input_ids):\n            attention_mask = input_ids.ne(pad_token_id).long()\n        elif attention_mask is None:\n            attention_mask = input_ids.new_ones(input_ids.shape)\n\n        # set pad_token_id to eos_token_id if not set. Important that this is done after\n        # attention_mask is created\n        if pad_token_id is None and eos_token_id is not None:\n            logger.warning(\n                \"Setting pad_token_id to {} (first eos_token_id) to generate sequence\".format(eos_token_id)\n            )\n            pad_token_id = eos_token_id\n\n        # current position and vocab size\n        if hasattr(self.config, \"vocab_size\"):\n            vocab_size = self.config.vocab_size\n        elif (\n                self.config.is_encoder_decoder\n                and hasattr(self.config, \"decoder\")\n                and hasattr(self.config.decoder, \"vocab_size\")\n        ):\n            vocab_size = self.config.decoder.vocab_size\n\n        # set effective batch size and effective batch multiplier according to do_sample\n        if do_sample:\n            effective_batch_size = batch_size * num_return_sequences\n            effective_batch_mult = num_return_sequences\n        else:\n            effective_batch_size = batch_size\n            effective_batch_mult = 1\n\n        if self.config.is_encoder_decoder:\n            if decoder_start_token_id is None:\n                decoder_start_token_id = bos_token_id\n\n            assert (\n                    decoder_start_token_id is not None\n            ), \"decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation\"\n            assert hasattr(self, \"get_encoder\"), \"{} should have a 'get_encoder' function defined\".format(self)\n            assert callable(self.get_encoder), \"{} should be a method\".format(self.get_encoder)\n\n            # get encoder and store encoder outputs\n            encoder = self.get_encoder()\n\n            encoder_outputs: tuple = encoder(input_ids, image_features=image_features, attention_mask=attention_mask)\n\n        # Expand input ids if num_beams > 1 or num_return_sequences > 1\n        if num_return_sequences > 1 or num_beams > 1:\n            input_ids_len = input_ids.shape[-1]\n            input_ids = input_ids.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, input_ids_len)\n            attention_mask = attention_mask.unsqueeze(1).expand(\n                batch_size, effective_batch_mult * num_beams, input_ids_len\n            )\n\n            input_ids = input_ids.contiguous().view(\n                effective_batch_size * num_beams, input_ids_len\n            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\n            attention_mask = attention_mask.contiguous().view(\n                effective_batch_size * num_beams, input_ids_len\n            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\n\n        if self.config.is_encoder_decoder:\n            # create empty decoder_input_ids\n            input_ids = torch.full(\n                (effective_batch_size * num_beams, 1),\n                decoder_start_token_id,\n                dtype=torch.long,\n                device=next(self.parameters()).device,\n            )\n            cur_len = 1\n\n            assert (\n                    batch_size == encoder_outputs[0].shape[0]\n            ), f\"expected encoder_outputs[0] to have 1st dimension bs={batch_size}, got {encoder_outputs[0].shape[0]} \"\n\n            # expand batch_idx to assign correct encoder output for expanded\n            # input_ids (due to num_beams > 1 and num_return_sequences > 1)\n            expanded_batch_idxs = (\n                torch.arange(batch_size)\n                    .view(-1, 1)\n                    .repeat(1, num_beams * effective_batch_mult)\n                    .view(-1)\n                    .to(input_ids.device)\n            )\n            # expand encoder_outputs\n            encoder_outputs = (encoder_outputs[0].index_select(0, expanded_batch_idxs), *encoder_outputs[1:])\n\n        else:\n            encoder_outputs = None\n            cur_len = input_ids.shape[-1]\n\n        assert (\n                cur_len < max_length\n        ), f\"The context has {cur_len} number of tokens, but max_length is only {max_length}. \" \\\n            f\"Please make sure that max_length is bigger than the number of tokens, \" \\\n            f\"by setting either generate(max_length=...,...) or config.max_length = ...\"\n\n        if num_beams > 1:\n            output = self._generate_beam_search(\n                input_ids,\n                cur_len=cur_len,\n                max_length=max_length,\n                min_length=min_length,\n                do_sample=do_sample,\n                early_stopping=early_stopping,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                repetition_penalty=repetition_penalty,\n                no_repeat_ngram_size=no_repeat_ngram_size,\n                bad_words_ids=bad_words_ids,\n                pad_token_id=pad_token_id,\n                eos_token_id=eos_token_id,\n                batch_size=effective_batch_size,\n                num_return_sequences=num_return_sequences,\n                length_penalty=length_penalty,\n                num_beams=num_beams,\n                vocab_size=vocab_size,\n                encoder_outputs=encoder_outputs,\n                attention_mask=attention_mask,\n                use_cache=use_cache,\n                model_specific_kwargs=model_specific_kwargs,\n            )\n        else:\n            output = self._generate_no_beam_search(\n                input_ids,\n                cur_len=cur_len,\n                max_length=max_length,\n                min_length=min_length,\n                do_sample=do_sample,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                repetition_penalty=repetition_penalty,\n                no_repeat_ngram_size=no_repeat_ngram_size,\n                bad_words_ids=bad_words_ids,\n                pad_token_id=pad_token_id,\n                eos_token_id=eos_token_id,\n                batch_size=effective_batch_size,\n                encoder_outputs=encoder_outputs,\n                attention_mask=attention_mask,\n                use_cache=use_cache,\n                model_specific_kwargs=model_specific_kwargs,\n            )\n\n        return output\n\n    def prepare_inputs_for_generation(self, decoder_input_ids, past, attention_mask, use_cache, **kwargs):\n        assert past is not None, \"past has to be defined for encoder_outputs\"\n\n        encoder_outputs, decoder_cached_states = past\n        return {\n            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n            \"image_features\": None,  # encoder_outputs is defined. image_features not needed\n            \"encoder_outputs\": encoder_outputs,\n            \"decoder_cached_states\": decoder_cached_states,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"attention_mask\": attention_mask,\n            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n        }\n\n    def adjust_logits_during_generation(self, logits, cur_len, max_length):\n        if cur_len == 1:\n            self._force_token_ids_generation(logits, self.config.bos_token_id)\n        if cur_len == max_length - 1 and self.config.eos_token_id is not None:\n            self._force_token_ids_generation(logits, self.config.eos_token_id)\n        return logits\n\n    def _force_token_ids_generation(self, scores, token_ids) -> None:\n        \"\"\"force one of token_ids to be generated by setting prob of all other tokens to 0\"\"\"\n        if isinstance(token_ids, int):\n            token_ids = [token_ids]\n        all_but_token_ids_mask = torch.tensor(\n            [x for x in range(self.config.vocab_size) if x not in token_ids],\n            dtype=torch.long,\n            device=next(self.parameters()).device,\n        )\n        assert len(scores.shape) == 2, \"scores should be of rank 2 with shape: [batch_size, vocab_size]\"\n        scores[:, all_but_token_ids_mask] = -float(\"inf\")\n\n    @staticmethod\n    def _reorder_cache(past, beam_idx):\n        ((enc_out, enc_mask), decoder_cached_states) = past\n        reordered_past = []\n        for layer_past in decoder_cached_states:\n            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n            layer_past_new = {\n                attn_key: _reorder_buffer(attn_cache, beam_idx) for attn_key, attn_cache in layer_past.items()\n            }\n            reordered_past.append(layer_past_new)\n\n        new_enc_out = enc_out if enc_out is None else enc_out.index_select(0, beam_idx)\n        new_enc_mask = enc_mask if enc_mask is None else enc_mask.index_select(0, beam_idx)\n\n        past = ((new_enc_out, new_enc_mask), reordered_past)\n        return past\n\n    def get_encoder(self):\n        return self.model.encoder\n\n    def get_output_embeddings(self):\n        return _make_linear_from_emb(self.model.shared)  # make it on the fly\n\n    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n        old_num_tokens = self.model.shared.num_embeddings\n        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n        self.model.shared = new_embeddings\n        self._resize_final_logits_bias(new_num_tokens, old_num_tokens)\n        return new_embeddings\n\n    def _resize_final_logits_bias(self, new_num_tokens: int, old_num_tokens: int) -> None:\n        if new_num_tokens <= old_num_tokens:\n            new_bias = self.final_logits_bias[:, :new_num_tokens]\n        else:\n            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n        self.register_buffer(\"final_logits_bias\", new_bias)", "\n\nclass FromPretrainedMixin:\n    # This is based on torch.nn.module\n    # The modifications are:\n    # - if the parameter name is in config.partial_load, load only the part the state_dict saved\n    @staticmethod\n    def _load_from_state_dict(module, state_dict, prefix, local_metadata, strict,\n                              missing_keys, unexpected_keys, error_msgs, partial_loads, config):\n        r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n        this module, but not its descendants. This is called on every submodule\n        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n        module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n        For state dicts without metadata, :attr:`local_metadata` is empty.\n        Subclasses can achieve class-specific backward compatible loading using\n        the version number at `local_metadata.get(\"version\", None)`.\n\n        .. note::\n            :attr:`state_dict` is not the same object as the input\n            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n            it can be modified.\n\n        Arguments:\n            state_dict (dict): a dict containing parameters and\n                persistent buffers.\n            prefix (str): the prefix for parameters and buffers used in this\n                module\n            local_metadata (dict): a dict containing the metadata for this module.\n                See\n            strict (bool): whether to strictly enforce that the keys in\n                :attr:`state_dict` with :attr:`prefix` match the names of\n                parameters and buffers in this module\n            missing_keys (list of str): if ``strict=True``, add missing keys to\n                this list\n            unexpected_keys (list of str): if ``strict=True``, add unexpected\n                keys to this list\n            error_msgs (list of str): error messages should be added to this\n                list, and will be reported together in\n                :meth:`~torch.nn.Module.load_state_dict`\n        \"\"\"\n        for hook in module._load_state_dict_pre_hooks.values():\n            hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\n\n        local_name_params = itertools.chain(module._parameters.items(), module._buffers.items())\n        local_state = {k: v.data for k, v in local_name_params if v is not None}\n\n        for name, param in local_state.items():\n            key = prefix + name\n            if key in state_dict:\n                input_param = state_dict[key]\n\n                # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n                if len(param.shape) == 0 and len(input_param.shape) == 1:\n                    input_param = input_param[0]\n\n                if input_param.shape != param.shape:\n                    # local shape should match the one in checkpoint\n                    if key in config.partial_load:\n                        partial_loads.append('partially loaded parameter {} ({} => {})'\n                                             .format(key, input_param.shape, param.shape))\n                    else:\n                        error_msgs.append('size mismatch for {}: copying a param with shape {} from checkpoint, '\n                                          'the shape in current model is {}.'\n                                          .format(key, input_param.shape, param.shape))\n                        continue\n\n                if isinstance(input_param, Parameter):\n                    # backwards compatibility for serialized parameters\n                    input_param = input_param.data\n                try:\n                    if key in config.partial_load:\n                        # copy only the parameters that is defined in input_param to param\n                        param[tuple(map(slice, input_param.size()))].copy_(input_param)\n                    else:\n                        param.copy_(input_param)\n                except Exception:\n                    error_msgs.append('While copying the parameter named \"{}\", '\n                                      'whose dimensions in the model are {} and '\n                                      'whose dimensions in the checkpoint are {}.'\n                                      .format(key, param.size(), input_param.size()))\n            elif strict:\n                missing_keys.append(key)\n\n        if strict:\n            for key in state_dict.keys():\n                if key.startswith(prefix):\n                    input_name = key[len(prefix):]\n                    input_name = input_name.split('.', 1)[0]  # get the name of param/buffer/child\n                    if input_name not in module._modules and input_name not in local_state:\n                        unexpected_keys.append(key)\n\n    # This is based on transformers.modeling_utils\n    # The modifications are:\n    # - changed module._load_from_state_dict to cls._load_from_state_dict\n    # - warning on partial loads\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, error_on_mismatch=True, *model_args, **kwargs):\n        r\"\"\"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\n        The model is set in evaluation mode by default using model.eval() (Dropout modules are deactivated)\n        To train the model, you should first set it back in training mode with model.train()\n\n        The warning Weights from XXX not initialized from pretrained model means that the weights of XXX do not come\n        pre-trained with the rest of the model.\n        It is up to you to train those weights with a downstream fine-tuning task.\n\n        The warning Weights from XXX not used in YYY means that the layer XXX is not used by YYY, therefore\n        those weights are discarded.\n\n        Parameters:\n            pretrained_model_name_or_path: either:\n              - a string with the shortcut name of a pre-trained model to load from cache or download,\n                e.g.: bert-base-uncased.\n              - a string with the identifier name of a pre-trained model that was user-uploaded to our S3,\n                e.g.: dbmdz/bert-base-german-cased.\n              - a path to a directory containing model weights saved using\n                :func:~transformers.PreTrainedModel.save_pretrained, e.g.: ./my_model_directory/.\n              - a path or url to a tensorflow index checkpoint file (e.g. ./tf_model/model.ckpt.index).\n                In this case, from_tf should be set to True and a configuration object should be provided as config\n                argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model\n                using the provided conversion scripts and loading the PyTorch model afterwards.\n              - None if you are both providing the configuration and state dictionary\n                (resp. with keyword arguments config and state_dict)\n\n            error_on_mismatch: (optional) boolean:\n                Set to False to only warn the mismatch on loading weights, instead of error.\n\n            model_args: (optional) Sequence of positional arguments:\n                All remaning positional arguments will be passed to the underlying model's __init__ method\n\n            config: (optional) one of:\n                - an instance of a class derived from :class:~transformers.PretrainedConfig, or\n                - a string valid as input to :func:~transformers.PretrainedConfig.from_pretrained()\n\n                Configuration for the model to use instead of an automatically loaded configuation.\n                Configuration can be automatically loaded when:\n                    - the model is a model provided by the library (loaded with the shortcut-name string of\n                      a pretrained model), or\n                    - the model was saved using :func:~transformers.PreTrainedModel.save_pretrained and is\n                      reloaded by suppling the save directory.\n                    - the model is loaded by suppling a local directory as pretrained_model_name_or_path and\n                      a configuration JSON file named config.json is found in the directory.\n\n            state_dict: (optional) dict:\n                an optional state dictionnary for the model to use instead of a state dictionary loaded from\n                saved weights file. This option can be used if you want to create a model from a pretrained\n                configuration but load your own weights. In this case though, you should check if using\n                :func:~transformers.PreTrainedModel.save_pretrained and\n                :func:~transformers.PreTrainedModel.from_pretrained is not a simpler option.\n\n            cache_dir: (optional) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            force_download: (optional) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions\n                if they exists.\n\n            resume_download: (optional) boolean, default False:\n                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n\n            proxies: (optional) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint,\n                e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n                The proxies are used on each request.\n\n            output_loading_info: (optional) boolean:\n                Set to True to also return a dictionnary containing missing keys, unexpected keys and error messages.\n\n            kwargs: (optional) Remaining dictionary of keyword arguments:\n                Can be used to update the configuration object (after it being loaded) and initiate the model.\n                (e.g. output_attention=True). Behave differently depending on whether a config is provided\n                or automatically loaded:\n\n                - If a configuration is provided with config, **kwargs will be directly passed to the underlying\n                model's __init__ method (we assume all relevant updates to the configuration have already been done)\n                - If a configuration is not provided, kwargs will be first passed to the configuration class\n                initialization function (:func:~transformers.PretrainedConfig.from_pretrained).\n                Each key of kwargs that corresponds to a configuration attribute will be used to override said\n                attribute with the supplied kwargs value. Remaining keys that do not correspond to any\n                configuration attribute will be passed to the underlying model's __init__ function.\n        \"\"\"\n        config = kwargs.pop(\"config\", None)\n        state_dict = kwargs.pop(\"state_dict\", None)\n        cache_dir = kwargs.pop(\"cache_dir\", None)\n        from_tf = kwargs.pop(\"from_tf\", False)\n        force_download = kwargs.pop(\"force_download\", False)\n        resume_download = kwargs.pop(\"resume_download\", False)\n        proxies = kwargs.pop(\"proxies\", None)\n        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n        local_files_only = kwargs.pop(\"local_files_only\", False)\n        use_cdn = kwargs.pop(\"use_cdn\", True)\n\n        # Load config if we don't provide a configuration\n        if not isinstance(config, PretrainedConfig):\n            config_path = config if config is not None else pretrained_model_name_or_path\n            config, model_kwargs = cls.config_class.from_pretrained(\n                config_path,\n                *model_args,\n                cache_dir=cache_dir,\n                return_unused_kwargs=True,\n                force_download=force_download,\n                resume_download=resume_download,\n                proxies=proxies,\n                local_files_only=local_files_only,\n                **kwargs,\n            )\n        else:\n            model_kwargs = kwargs\n\n        # Load model\n        if pretrained_model_name_or_path is not None:\n            if os.path.isdir(pretrained_model_name_or_path):\n                if from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")):\n                    # Load from a TF 1.0 checkpoint\n                    archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")\n                elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)):\n                    # Load from a TF 2.0 checkpoint\n                    archive_file = os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)\n                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):\n                    # Load from a PyTorch checkpoint\n                    archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n                else:\n                    raise EnvironmentError(\n                        \"Error no file named {} found in directory {} or from_tf set to False\".format(\n                            [WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + \".index\"],\n                            pretrained_model_name_or_path,\n                        )\n                    )\n            elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n                archive_file = pretrained_model_name_or_path\n            elif os.path.isfile(pretrained_model_name_or_path + \".index\"):\n                assert (\n                    from_tf\n                ), \"We found a TensorFlow checkpoint at {}, please set \" \\\n                   \"from_tf to True to load from this checkpoint\".format(\n                    pretrained_model_name_or_path + \".index\"\n                )\n                archive_file = pretrained_model_name_or_path + \".index\"\n            else:\n                archive_file = hf_bucket_url(\n                    pretrained_model_name_or_path,\n                    filename=(TF2_WEIGHTS_NAME if from_tf else WEIGHTS_NAME),\n                    use_cdn=use_cdn,\n                )\n\n            try:\n                # Load from URL or cache if already cached\n                resolved_archive_file = cached_path(\n                    archive_file,\n                    cache_dir=cache_dir,\n                    force_download=force_download,\n                    proxies=proxies,\n                    resume_download=resume_download,\n                    local_files_only=local_files_only,\n                )\n                if resolved_archive_file is None:\n                    raise EnvironmentError\n            except EnvironmentError:\n                msg = (\n                    f\"Can't load weights for '{pretrained_model_name_or_path}'. Make sure that:\\n\\n\"\n                    f\"- '{pretrained_model_name_or_path}' is a correct model identifier listed on \"\n                    f\"'https://huggingface.co/models'\\n\\n\"\n                    f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory \"\n                    f\"containing a file named one of {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME}.\\n\\n\"\n                )\n                raise EnvironmentError(msg)\n\n            if resolved_archive_file == archive_file:\n                logger.info(\"loading weights file {}\".format(archive_file))\n            else:\n                logger.info(\"loading weights file {} from cache at {}\".format(archive_file, resolved_archive_file))\n        else:\n            resolved_archive_file = None\n\n        # Instantiate model.\n        model = cls(config, *model_args, **model_kwargs)\n\n        if state_dict is None and not from_tf:\n            try:\n                state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n            except Exception:\n                raise OSError(\n                    \"Unable to load weights from pytorch checkpoint file. \"\n                    \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\n                )\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        partial_loads = []\n\n        if from_tf:\n            if resolved_archive_file.endswith(\".index\"):\n                # Load from a TensorFlow 1.X checkpoint - provided by original authors\n                model = cls.load_tf_weights(model, config, resolved_archive_file[:-6])  # Remove the '.index'\n            else:\n                # Load from our TensorFlow 2.0 checkpoints\n                try:\n                    from transformers import load_tf2_checkpoint_in_pytorch_model\n\n                    model = load_tf2_checkpoint_in_pytorch_model(model, resolved_archive_file, allow_missing_keys=True)\n                except ImportError:\n                    logger.error(\n                        \"Loading a TensorFlow model in PyTorch, requires both PyTorch and \"\n                        \"TensorFlow to be installed. Please see \"\n                        \"https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\"\n                    )\n                    raise\n        else:\n            # Convert old format to new format if needed from a PyTorch state_dict\n            old_keys = []\n            new_keys = []\n            for key in state_dict.keys():\n                new_key = None\n                if \"gamma\" in key:\n                    new_key = key.replace(\"gamma\", \"weight\")\n                if \"beta\" in key:\n                    new_key = key.replace(\"beta\", \"bias\")\n                if new_key:\n                    old_keys.append(key)\n                    new_keys.append(new_key)\n            for old_key, new_key in zip(old_keys, new_keys):\n                state_dict[new_key] = state_dict.pop(old_key)\n\n            # copy state_dict so _load_from_state_dict can modify it\n            metadata = getattr(state_dict, \"_metadata\", None)\n            state_dict = state_dict.copy()\n            if metadata is not None:\n                state_dict._metadata = metadata\n\n            # PyTorch's _load_from_state_dict does not copy parameters in a module's descendants\n            # so we need to apply the function recursively.\n            def load(module: nn.Module, prefix=\"\"):\n                local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n                cls._load_from_state_dict(\n                    module=module,\n                    state_dict=state_dict,\n                    prefix=prefix,\n                    local_metadata=local_metadata,\n                    strict=True,\n                    missing_keys=missing_keys,\n                    unexpected_keys=unexpected_keys,\n                    error_msgs=error_msgs,\n                    partial_loads=partial_loads,\n                    config=config\n                )\n                for name, child in module._modules.items():\n                    if child is not None:\n                        load(child, prefix + name + \".\")\n\n            # Make sure we are able to load base models as well as derived models (with heads)\n            start_prefix = \"\"\n            model_to_load = model\n            has_prefix_module = any(s.startswith(cls.base_model_prefix) for s in state_dict.keys())\n            if not hasattr(model, cls.base_model_prefix) and has_prefix_module:\n                start_prefix = cls.base_model_prefix + \".\"\n            if hasattr(model, cls.base_model_prefix) and not has_prefix_module:\n                model_to_load = getattr(model, cls.base_model_prefix)\n\n            load(model_to_load, prefix=start_prefix)\n\n            if model.__class__.__name__ != model_to_load.__class__.__name__:\n                base_model_state_dict = model_to_load.state_dict().keys()\n                head_model_state_dict_without_base_prefix = [\n                    key.split(cls.base_model_prefix + \".\")[-1] for key in model.state_dict().keys()\n                ]\n\n                missing_keys.extend(head_model_state_dict_without_base_prefix - base_model_state_dict)\n\n            if len(unexpected_keys) > 0:\n                logger.warning(\n                    f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when \"\n                    f\"initializing {model.__class__.__name__}: {unexpected_keys}\\n\"\n                    f\"- This IS expected if you are initializing {model.__class__.__name__} from the checkpoint \"\n                    f\"of a model trained on another task \"\n                    f\"or with another architecture (e.g. initializing a BertForSequenceClassification model from \"\n                    f\"a BertForPretraining model).\\n\"\n                    f\"- This IS NOT expected if you are initializing {model.__class__.__name__} from the checkpoint \"\n                    f\"of a model that you expect \"\n                    f\"to be exactly identical (initializing a BertForSequenceClassification model from \"\n                    f\"a BertForSequenceClassification model).\"\n                )\n            else:\n                logger.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n            if len(missing_keys) > 0:\n                logger.warning(\n                    f\"Some weights of {model.__class__.__name__} were not initialized from the \"\n                    f\"model checkpoint at {pretrained_model_name_or_path} \"\n                    f\"and are newly initialized: {missing_keys}\\n\"\n                    f\"You should probably TRAIN this model on a down-stream task to be able \"\n                    f\"to use it for predictions and inference.\"\n                )\n            else:\n                logger.info(\n                    f\"All the weights of {model.__class__.__name__} were initialized from \"\n                    f\"the model checkpoint at {pretrained_model_name_or_path}.\\n\"\n                    f\"If your task is similar to the task the model of the ckeckpoint was trained on, \"\n                    f\"you can already use {model.__class__.__name__} for predictions without further training.\"\n                )\n            if len(error_msgs) > 0:\n                RuntimeError(\"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n                    model.__class__.__name__, \"\\n\\t\".join(error_msgs)\n                ))\n            if len(partial_loads) > 0:\n                logger.info(\"Partial loads while loading state_dict for {}:\\n\\t{}\".format(\n                    model.__class__.__name__, \"\\n\\t\".join(partial_loads)\n                ))\n        model.tie_weights()  # make sure token embedding weights are still tied if needed\n\n        # Set model in evaluation mode to deactivate DropOut modules by default\n        model.eval()\n\n        if output_loading_info:\n            loading_info = {\n                \"missing_keys\": missing_keys,\n                \"unexpected_keys\": unexpected_keys,\n                \"error_msgs\": error_msgs,\n            }\n            return model, loading_info\n\n        if hasattr(config, \"xla_device\") and config.xla_device:\n            import torch_xla.core.xla_model as xm\n\n            model = xm.send_cpu_data_to_device(model, xm.xla_device())\n            model.to(xm.xla_device())\n\n        return model", ""]}
{"filename": "src/model/modules_for_prompt.py", "chunked_list": ["import random\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.models.bart.modeling_bart import *\nfrom src.model.modeling_bart import (\n    SinusoidalPositionalEmbedding,\n    LearnedPositionalEmbedding,", "    SinusoidalPositionalEmbedding,\n    LearnedPositionalEmbedding,\n    invert_mask,\n    EncoderLayer,\n    LayerNorm,\n)\nfrom src.model.modeling_bart import (PretrainedBartModel, BartDecoder,\n                                     BartClassificationHead,\n                                     _make_linear_from_emb,\n                                     _prepare_bart_decoder_inputs)", "                                     _make_linear_from_emb,\n                                     _prepare_bart_decoder_inputs)\nfrom src.model.config import MultiModalBartConfig\n\nfrom transformers import AutoConfig, AutoModel, CLIPVisionModel, CLIPVisionConfig\nimport timm\nfrom src.model.attention import Attention_for_Senti_Prompt\n\nTIMM_MODELS = {\n    'nf_resnet50': 2048,", "TIMM_MODELS = {\n    'nf_resnet50': 2048,\n}\ndef is_clip_model(model_name):\n    return model_name.startswith('openai/clip-')\n\nimage_model_name =  'nf_resnet50'\nif image_model_name in TIMM_MODELS.keys():\n    image_encoder = timm.create_model(image_model_name, pretrained=True, num_classes=0)\nelif is_clip_model(image_model_name):\n    ###model_name ='openai/clip-vit-base-patch32'\n    config = CLIPVisionConfig.from_pretrained(image_model_name)\n    image_encoder = CLIPVisionModel.from_pretrained(\n            image_model_name,\n            config=config,\n        )\nelse:\n    image_encoder = AutoModel.from_pretrained(image_model_name)", "\ndef init_image_encoder(image_model_name, frozen_image_encoder, num_image_tokens, d_text_encoder):\n\n    # image_encoder = get_image_encoder(image_model_name)\n    d_image_encoder = _d_image_encoder(image_model_name, image_encoder)\n\n    if frozen_image_encoder:\n        for p in image_encoder.parameters():\n            p.requires_grad = False\n            image_encoder.eval()\n\n    proj_image_features = nn.Linear(\n            in_features=d_image_encoder,\n            out_features=num_image_tokens * d_text_encoder,\n        )\n    return proj_image_features.cuda(), d_image_encoder", "\ndef _d_image_encoder(image_model_name, image_encoder):\n    ##image_model_name\u9ed8\u8ba4\u4e3a\uff1a 'microsoft/resnet-50'\n    model_name = image_model_name\n    if model_name in TIMM_MODELS.keys():\n        return TIMM_MODELS[model_name]\n    elif is_clip_model(model_name):\n        return image_encoder.config.hidden_size\n    elif model_name.startswith('microsoft/resnet-'):\n        return image_encoder.config.hidden_sizes[-1]\n    else:\n        return image_encoder.config.hidden_size", "\n\ndef encode_images(image_encoder, proj_image_features, frozen_image_encoder, pixel_values, d_image_encoder):\n    \n    image_encoder = image_encoder.cuda()\n    pixel_values = pixel_values.cuda()\n    # print('the shape of pixel_values is {}'.format(pixel_values.shape))\n    batch_size = pixel_values.shape[0]\n\n    if frozen_image_encoder:\n        with torch.no_grad():\n            image_encoder.eval()\n            visual = image_encoder(pixel_values)\n    else:\n        visual = image_encoder(pixel_values)\n\n    if not isinstance(visual, torch.Tensor):  # HuggingFace model\n        visual = visual.pooler_output\n\n    visual = visual.reshape(batch_size, d_image_encoder)\n    visual = proj_image_features(visual).cuda()\n    return visual", "\n\n\nclass ImageEmbedding(nn.Module):\n    def __init__(self, image_dim, final_dim, image_model_name, frozen_image_encoder=False, num_image_tokens=2):\n        super(ImageEmbedding, self).__init__()\n        self.frozen_image_encoder = frozen_image_encoder\n        self.final_dim = final_dim\n        self.linear = nn.Linear(final_dim, final_dim)\n        self.d_image_encoder = _d_image_encoder(image_model_name, image_encoder)\n        if frozen_image_encoder:\n            for p in image_encoder.parameters():\n                p.requires_grad = False\n                image_encoder.eval()\n\n        self.proj_image_features = nn.Linear(\n                in_features=self.d_image_encoder,\n                out_features=num_image_tokens * final_dim,\n            )\n\n    def forward(self, image_pixel_values):\n        \n        # import ipdb; ipdb.set_trace()\n        image_pixel_values = torch.stack(image_pixel_values)\n        batch_size = image_pixel_values.size(0)\n        image_features = encode_images(image_encoder=image_encoder, \n                                           proj_image_features=self.proj_image_features, \n                                           frozen_image_encoder=self.frozen_image_encoder, \n                                           pixel_values=image_pixel_values, \n                                           d_image_encoder=self.d_image_encoder)\n        ###image_features: (batch_size, num_image_tokens*1024) (4, 2048)\n        # print(\"======================================the shape of image_features is {}=====================================\".format(image_features.shape))\n        image_features = image_features.reshape(batch_size, -1, self.final_dim) ### (4, num_image_tokens, 1024(d_model))\n        \n        img_len = list(map(len, image_features))\n        non_empty_features = list(filter(lambda x: len(x) > 0, image_features))\n\n        embedded = None\n        if len(non_empty_features) > 0:\n            img_tensor = torch.cat(non_empty_features, dim=0)\n            embedded = self.linear(img_tensor)\n\n        output = []\n        index = 0\n        for l in img_len:\n            if l > 0:\n                output.append(embedded[index:index + l])\n            else:\n                output.append(torch.empty(0))\n            index += l\n        return output", "\n\nclass MultiModalBartEncoder(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n    is a :class:EncoderLayer.\n\n    Args:\n        config: MultiModalBartConfig\n    \"\"\"\n    def __init__(self, config: MultiModalBartConfig, encoder, img_feat_id,\n                 cls_token_id, num_image_tokens):\n        super().__init__()\n\n        self.img_feat_id = img_feat_id\n        self.cls_token_id = cls_token_id\n        embed_tokens = encoder.embed_tokens\n        self.dropout = encoder.dropout\n        self.layerdrop = encoder.layerdrop\n\n        self.indentity = nn.Identity()\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_scale = encoder.embed_scale\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = encoder.max_source_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n        self.embed_positions = encoder.embed_positions\n\n        self.layers = encoder.layers\n        self.layernorm_embedding = encoder.layernorm_embedding\n        # mbart has one extra layer_norm\n        self.layer_norm = encoder.layer_norm\n\n    def _embed_multi_modal(self, input_ids, image_features):\n        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n        mask = (input_ids == self.img_feat_id) | (\n            input_ids == self.cls_token_id)\n        # print(mask.shape)\n        embedded_images = self.embed_images(image_features)\n        embedded = self.embed_tokens(input_ids)\n        # print('mask shape', mask.shape)\n        if not embedded_images[0].dtype == torch.float32:\n            embedded = embedded.half()\n\n        for index, value in enumerate(embedded_images):\n            if len(value) > 0:\n                embedded[index, mask[index]] = value\n\n        return embedded\n\n    def forward(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=False):\n        \"\"\"\n\n        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n        :param image_features: list[FloatTensor], image roi features with length of batch\n        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n        :param output_attentions:\n        :param output_hidden_states:\n        :return: Tuple comprised of:\n            - x (Tensor): the last encoder layer's output of\n              shape (src_len, batch, embed_dim)\n            - encoder_states (List[Tensor]): all intermediate\n              hidden states of shape (src_len, batch, embed_dim).\n              Only populated if output_hidden_states: is True.\n            - all_attentions (List[Tensor]): Attention weights for each layer.\n            During training might not be of length n_layers because of layer dropout.\n        \"\"\"\n        # check attention mask and invert\n        if attention_mask is not None:\n            attention_mask = invert_mask(attention_mask)\n\n        inputs_embeds = self._embed_multi_modal(\n            input_ids, image_features) * self.embed_scale\n        embed_pos = self.embed_positions(input_ids)\n        x = inputs_embeds + embed_pos\n        x = self.layernorm_embedding(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        encoder_states, all_attentions = [], []\n        for encoder_layer in self.layers:\n            if output_hidden_states:\n                encoder_states.append(x)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability <\n                                  self.layerdrop):  # skip the layer\n                attn = None\n            else:\n                x, attn = encoder_layer(x,\n                                        attention_mask,\n                                        output_attentions=output_attentions)\n\n            if output_attentions:\n                all_attentions.append(attn)\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n        if output_hidden_states:\n            encoder_states.append(x)\n\n        # T x B x C -> B x T x C\n        encoder_states = [\n            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n        ]\n        x = x.transpose(0, 1)\n\n        if not return_dict:\n            return tuple(v for v in [x, encoder_states, all_attentions]\n                         if v is not None)\n        return BaseModelOutput(last_hidden_state=x,\n                               hidden_states=encoder_states,\n                               attentions=all_attentions)", "\nclass MultiModalBartEncoder_for_Generating_aspect_prompt(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n    is a :class:EncoderLayer.\n\n    Args:\n        config: MultiModalBartConfig\n    \"\"\"\n    def __init__(self, \n                use_generated_prompt,\n                config: MultiModalBartConfig, encoder, img_feat_id, aspect_prompt_token_id, senti_prompt_token_id,\n                 cls_token_id, num_image_tokens, use_different_aspect_prompt):\n        super().__init__()\n\n        self.use_generated_prompt= use_generated_prompt\n        self.aspect_prompt_token_id = aspect_prompt_token_id\n        self.senti_prompt_token_id = senti_prompt_token_id\n        self.use_different_aspect_prompt = use_different_aspect_prompt\n\n        self.img_feat_id = img_feat_id\n        self.cls_token_id = cls_token_id\n        embed_tokens = encoder.embed_tokens\n        self.dropout = encoder.dropout\n        self.layerdrop = encoder.layerdrop\n\n        self.indentity = nn.Identity()\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_scale = encoder.embed_scale\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = encoder.max_source_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n        self.embed_positions = encoder.embed_positions\n\n        self.layers = encoder.layers\n        self.layernorm_embedding = encoder.layernorm_embedding\n        # mbart has one extra layer_norm\n        self.layer_norm = encoder.layer_norm\n\n        # self.aspect_linear = nn.Linear(768, 768)\n        # self.aspect_relu = nn.LeakyReLU()\n\n    def _embed_multi_modal(self, generated_aspect_prompt, aspects_num, input_ids, image_features):\n        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n        # import ipdb; ipdb.set_trace()\n        mask = (input_ids == self.img_feat_id) | (\n            input_ids == self.cls_token_id)\n        # print(mask.shape)\n        embedded_images = self.embed_images(image_features)\n        embedded = self.embed_tokens(input_ids)\n        # print('mask shape', mask.shape)\n        if not embedded_images[0].dtype == torch.float32:\n            embedded = embedded.half()\n\n        for index, value in enumerate(embedded_images):\n            if len(value) > 0:\n                embedded[index, mask[index]] = value\n        \n       \n        prompt_mask = (input_ids == self.aspect_prompt_token_id)\n\n        if self.use_generated_prompt:\n            if self.use_different_aspect_prompt:\n                # self.aspect_linear = self.aspect_linear.to(generated_aspect_prompt.device)\n                # self.aspect_relu = self.aspect_relu.to(generated_aspect_prompt.device)\n            \n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n                    prompt_embedding_list = []\n                    for j in range(aspect_num):\n                        aspect_linear = nn.Linear(768, 768).to(generated_aspect_prompt.device) ##\u6bcf\u4e2aaspect\u6709\u81ea\u5df1\u7684\u53d8\u6362\uff0c\u4e3a\u6bcf\u4e2aaspect\u8bbe\u8ba1\u7279\u5b9a\u7684prompt\n                        aspect_relu = nn.LeakyReLU().to(generated_aspect_prompt.device)\n                        prompt_embedding = aspect_linear(generated_aspect_prompt[index])\n                        prompt_embedding = aspect_relu(prompt_embedding)\n                        ###\u53ef\u4ee5\u52a0\u5165\u6fc0\u6d3b\u51fd\u6570\n                        # prompt_embedding = nn.LeakyReLU(prompt_embedding)\n                        prompt_embedding_list.append(prompt_embedding)\n                    prompt_embedding_ = torch.cat(prompt_embedding_list, dim=0)\n                    embedded[index, prompt_mask[index]] = prompt_embedding_\n            else:\n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    prompt_embedding_ = generated_aspect_prompt[index].repeat(aspect_num, 1)\n\n                    embedded[index, prompt_mask[index]] = prompt_embedding_\n        return embedded\n       \n\n    def forward(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                generated_prompt=None,\n                aspects_num=None,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=False):\n        \"\"\"\n\n        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n        :param image_features: list[FloatTensor], image roi features with length of batch\n        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n        :param output_attentions:\n        :param output_hidden_states:\n        :return: Tuple comprised of:\n            - x (Tensor): the last encoder layer's output of\n              shape (src_len, batch, embed_dim)\n            - encoder_states (List[Tensor]): all intermediate\n              hidden states of shape (src_len, batch, embed_dim).\n              Only populated if output_hidden_states: is True.\n            - all_attentions (List[Tensor]): Attention weights for each layer.\n            During training might not be of length n_layers because of layer dropout.\n        \"\"\"\n        # check attention mask and invert\n        if attention_mask is not None:\n            attention_mask = invert_mask(attention_mask)\n\n        inputs_embeds = self._embed_multi_modal(generated_prompt, aspects_num,\n            input_ids, image_features) * self.embed_scale\n        embed_pos = self.embed_positions(input_ids)\n        x = inputs_embeds + embed_pos\n        x = self.layernorm_embedding(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        encoder_states, all_attentions = [], []\n        for encoder_layer in self.layers:\n            if output_hidden_states:\n                encoder_states.append(x)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability <\n                                  self.layerdrop):  # skip the layer\n                attn = None\n            else:\n                x, attn = encoder_layer(x,\n                                        attention_mask,\n                                        output_attentions=output_attentions)\n\n            if output_attentions:\n                all_attentions.append(attn)\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n        if output_hidden_states:\n            encoder_states.append(x)\n\n        # T x B x C -> B x T x C\n        encoder_states = [\n            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n        ]\n        x = x.transpose(0, 1)\n\n        if not return_dict:\n            return tuple(v for v in [x, encoder_states, all_attentions]\n                         if v is not None)\n        return BaseModelOutput(last_hidden_state=x,\n                               hidden_states=encoder_states,\n                               attentions=all_attentions)", "\n\nclass MultiModalBartEncoder_for_Generating_sentiment_prompt(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n    is a :class:EncoderLayer.\n\n    Args:\n        config: MultiModalBartConfig\n    \"\"\"\n    def __init__(self, use_generated_prompt,\n                 config: MultiModalBartConfig, encoder, img_feat_id, aspect_prompt_token_id, senti_prompt_token_id,\n                 cls_token_id, num_image_tokens, use_different_senti_prompt):\n        super().__init__()\n        \n        self.use_generated_prompt = use_generated_prompt\n        self.aspect_prompt_token_id = aspect_prompt_token_id\n        self.senti_prompt_token_id = senti_prompt_token_id\n        self.use_different_senti_prompt = use_different_senti_prompt\n\n        self.img_feat_id = img_feat_id\n        self.cls_token_id = cls_token_id\n        embed_tokens = encoder.embed_tokens\n        self.dropout = encoder.dropout\n        self.layerdrop = encoder.layerdrop\n\n        self.indentity = nn.Identity()\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_scale = encoder.embed_scale\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = encoder.max_source_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n        self.embed_positions = encoder.embed_positions\n\n        self.layers = encoder.layers\n        self.layernorm_embedding = encoder.layernorm_embedding\n        # mbart has one extra layer_norm\n        self.layer_norm = encoder.layer_norm\n\n        # self.aspect_linear = nn.Linear(768, 768)\n        # self.aspect_relu = nn.LeakyReLU()\n\n    def _embed_multi_modal(self, generated_senti_prompt, aspects_num, input_ids, image_features):\n        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n        # import ipdb; ipdb.set_trace()\n        mask = (input_ids == self.img_feat_id) | (\n            input_ids == self.cls_token_id)\n        # print(mask.shape)\n        embedded_images = self.embed_images(image_features)\n        embedded = self.embed_tokens(input_ids)\n        # print('mask shape', mask.shape)\n        if not embedded_images[0].dtype == torch.float32:\n            embedded = embedded.half()\n\n        for index, value in enumerate(embedded_images):\n            if len(value) > 0:\n                embedded[index, mask[index]] = value\n        \n       \n        \n        if self.use_generated_prompt:\n            senti_prompt_mask = (input_ids == self.senti_prompt_token_id)\n            # import ipdb; ipdb.set_trace()\n            if self.use_different_senti_prompt:\n                # self.aspect_linear = self.aspect_linear.to(generated_senti_prompt.device)\n                # self.aspect_relu = self.aspect_relu.to(generated_senti_prompt.device)\n            \n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n                    prompt_embedding_list = []\n                    for j in range(aspect_num):\n                        aspect_linear = nn.Linear(768, 768).to(generated_senti_prompt.device)\n                        aspect_relu = nn.LeakyReLU().to(generated_senti_prompt.device)\n                        prompt_embedding = aspect_linear(generated_senti_prompt[index])\n                        prompt_embedding = aspect_relu(prompt_embedding)\n                        ###\u53ef\u4ee5\u52a0\u5165\u6fc0\u6d3b\u51fd\u6570\u00e5\n                        # prompt_embedding = nn.LeakyReLU(prompt_embedding)\n                        prompt_embedding_list.append(prompt_embedding)\n                    prompt_embedding_ = torch.cat(prompt_embedding_list, dim=0)\n                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n            else:\n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    prompt_embedding_ = generated_senti_prompt[index].repeat(aspect_num, 1)\n\n                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n        return embedded\n       \n\n    def forward(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                generated_prompt=None,\n                aspects_num=None,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=False):\n        \"\"\"\n\n        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n        :param image_features: list[FloatTensor], image roi features with length of batch\n        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n        :param output_attentions:\n        :param output_hidden_states:\n        :return: Tuple comprised of:\n            - x (Tensor): the last encoder layer's output of\n              shape (src_len, batch, embed_dim)\n            - encoder_states (List[Tensor]): all intermediate\n              hidden states of shape (src_len, batch, embed_dim).\n              Only populated if output_hidden_states: is True.\n            - all_attentions (List[Tensor]): Attention weights for each layer.\n            During training might not be of length n_layers because of layer dropout.\n        \"\"\"\n        # check attention mask and invert\n        if attention_mask is not None:\n            attention_mask = invert_mask(attention_mask)\n\n        inputs_embeds = self._embed_multi_modal(generated_prompt, aspects_num,\n            input_ids, image_features) * self.embed_scale\n        embed_pos = self.embed_positions(input_ids)\n        x = inputs_embeds + embed_pos\n        x = self.layernorm_embedding(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        encoder_states, all_attentions = [], []\n        for encoder_layer in self.layers:\n            if output_hidden_states:\n                encoder_states.append(x)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability <\n                                  self.layerdrop):  # skip the layer\n                attn = None\n            else:\n                x, attn = encoder_layer(x,\n                                        attention_mask,\n                                        output_attentions=output_attentions)\n\n            if output_attentions:\n                all_attentions.append(attn)\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n        if output_hidden_states:\n            encoder_states.append(x)\n\n        # T x B x C -> B x T x C\n        encoder_states = [\n            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n        ]\n        x = x.transpose(0, 1)\n\n        if not return_dict:\n            return tuple(v for v in [x, encoder_states, all_attentions]\n                         if v is not None)\n        return BaseModelOutput(last_hidden_state=x,\n                               hidden_states=encoder_states,\n                               attentions=all_attentions)", "\n\nclass MultiModalBartEncoder_for_Generating_Dual_prompts(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n    is a :class:EncoderLayer.\n\n    Args:\n        config: MultiModalBartConfig\n    \"\"\"\n    def __init__(self, \n                 use_generated_aspect_prompt, use_generated_senti_prompt, \n                 config: MultiModalBartConfig, encoder, img_feat_id, aspect_prompt_token_id, senti_prompt_token_id,\n                 cls_token_id, num_image_tokens, use_different_aspect_prompt, use_different_senti_prompt, \n                 NEU_id, POS_id, NEG_id):\n        super().__init__()\n\n        self.use_generated_aspect_prompt= use_generated_aspect_prompt\n        self.use_generated_senti_prompt = use_generated_senti_prompt\n\n        self.aspect_prompt_token_id = aspect_prompt_token_id\n        self.senti_prompt_token_id = senti_prompt_token_id\n        self.use_different_aspect_prompt = use_different_aspect_prompt\n        self.use_different_senti_prompt = use_different_senti_prompt\n\n        # if self.use_different_senti_prompt:\n        #     self.attention_for_senti_prompt = Attention_for_Senti_Prompt(n_head=8, model_dim=768, drop_rate=0.2)\n        self.img_feat_id = img_feat_id\n        self.cls_token_id = cls_token_id\n        self.neu_id = NEU_id\n        self.pos_id = POS_id\n        self.neg_id = NEG_id\n        embed_tokens = encoder.embed_tokens\n        self.dropout = encoder.dropout\n        self.layerdrop = encoder.layerdrop\n\n        self.indentity = nn.Identity()\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_scale = encoder.embed_scale\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = encoder.max_source_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_images = ImageEmbedding(embed_dim, embed_dim, image_model_name, num_image_tokens=num_image_tokens)\n        self.embed_positions = encoder.embed_positions\n\n        self.layers = encoder.layers\n        self.layernorm_embedding = encoder.layernorm_embedding\n        # mbart has one extra layer_norm\n        self.layer_norm = encoder.layer_norm\n\n        # self.aspect_linear = nn.Linear(768, 768)\n        # self.aspect_relu = nn.LeakyReLU()\n        # self.aspect_linear = nn.Sequential(nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768), nn.Linear(768, 768))\n\n    def _embed_multi_modal(self, generated_aspect_prompt, generated_senti_prompt, aspects_num, input_ids, image_features):\n        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n        # import ipdb; ipdb.set_trace()\n        device = generated_aspect_prompt.device\n        batch_size = input_ids.size(0)\n        mask = (input_ids == self.img_feat_id) | (\n            input_ids == self.cls_token_id)\n        # print(mask.shape)\n        embedded_images = self.embed_images(image_features)\n        embedded = self.embed_tokens(input_ids)\n        # print('mask shape', mask.shape)\n        if not embedded_images[0].dtype == torch.float32:\n            embedded = embedded.half()\n\n        for index, value in enumerate(embedded_images):\n            if len(value) > 0:\n                embedded[index, mask[index]] = value\n        import ipdb; ipdb.set_trace()\n        if self.use_generated_aspect_prompt:\n            ##aspect_prompt\n            aspect_prompt_mask = (input_ids == self.aspect_prompt_token_id)\n            if self.use_different_aspect_prompt:\n                # self.aspect_linear = self.aspect_linear.to(device)\n                # self.aspect_relu = self.aspect_relu.to(device)\n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n                    aspect_prompt_embedding_list = []\n                    for j in range(aspect_num):\n                        aspect_linear = nn.Linear(768, 768).to(generated_aspect_prompt.device) ##\u6bcf\u4e2aaspect\u6709\u81ea\u5df1\u7684\u53d8\u6362\uff0c\u4e3a\u6bcf\u4e2aaspect\u8bbe\u8ba1\u7279\u5b9a\u7684prompt\n                        aspect_relu = nn.LeakyReLU().to(generated_aspect_prompt.device)\n                        aspect_prompt_embedding = aspect_linear(generated_aspect_prompt[index])\n                        aspect_prompt_embedding = aspect_relu(aspect_prompt_embedding)\n                        aspect_prompt_embedding_list.append(aspect_prompt_embedding)\n                    aspect_prompt_embedding_ = torch.cat(aspect_prompt_embedding_list, dim=0)\n                    embedded[index, aspect_prompt_mask[index]] = aspect_prompt_embedding_\n            else:\n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    aspect_prompt_embedding_ = generated_aspect_prompt[index].repeat(aspect_num, 1)\n                    embedded[index, aspect_prompt_mask[index]] = aspect_prompt_embedding_\n\n        ##sentiment_prompt\n       \n        if self.use_generated_senti_prompt:\n            '''\n            # if self.use_different_senti_prompt:\n            \u4ee5\u4e0b\u4f7f\u7528\u7684\u662fattention\u673a\u5236\uff0csenti_prompt_token\u548csentiments_embdedding\n            # sentiments_ids = torch.tensor([self.neu_id, self.pos_id, self.neg_id]).to(device)\n            # sentiments_embdedding = self.embed_tokens(sentiments_ids)\n            # senti_prompt_mask = (input_ids == self.senti_prompt_token_id)\n            # for index in range(len(aspects_num)):\n                \n            #     aspect_num = aspects_num[index]\n            #     expanded_sentiments_embdedding = sentiments_embdedding.expand(aspect_num, sentiments_embdedding.size(0), sentiments_embdedding.size(1))\n            #     original_senti_prompt = embedded[index, senti_prompt_mask[index]].unsqueeze(1)\n            #     new_senti_prompt = self.attention_for_senti_prompt(original_senti_prompt, expanded_sentiments_embdedding, expanded_sentiments_embdedding).squeeze()\n            #     # import ipdb; ipdb.set_trace()\n            #     embedded[index, senti_prompt_mask[index]] = new_senti_prompt\n            '''\n            ##\u6362\u6210senti_prompt\u4e5f\u662f\u751f\u6210\u5f62\u5f0f\u770b\u770b\n            senti_prompt_mask = (input_ids == self.senti_prompt_token_id)\n            # import ipdb; ipdb.set_trace()\n            if self.use_different_senti_prompt:\n                # self.aspect_linear = self.aspect_linear.to(generated_senti_prompt.device)\n                # self.aspect_relu = self.aspect_relu.to(generated_senti_prompt.device)\n            \n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    # prompt_embedding_ = generated_prompt[index].repeat(aspect_num, 1)\n                    prompt_embedding_list = []\n                    for j in range(aspect_num):\n                        senti_linear = nn.Linear(768, 768).to(generated_senti_prompt.device)\n                        senti_relu = nn.LeakyReLU().to(generated_senti_prompt.device)\n                        prompt_embedding = senti_linear(generated_senti_prompt[index])\n                        prompt_embedding = senti_relu(prompt_embedding)\n                        prompt_embedding_list.append(prompt_embedding)\n                    prompt_embedding_ = torch.cat(prompt_embedding_list, dim=0)\n                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n            else:\n                for index in range(len(aspects_num)):\n                    aspect_num = aspects_num[index]\n                    prompt_embedding_ = generated_senti_prompt[index].repeat(aspect_num, 1)\n\n                    embedded[index, senti_prompt_mask[index]] = prompt_embedding_\n\n\n        return embedded\n       \n\n    def forward(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                generated_aspect_prompt=None,\n                generated_senti_prompt=None,\n                aspects_num=None,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=False):\n        \"\"\"\n\n        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n        :param image_features: list[FloatTensor], image roi features with length of batch\n        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n        :param output_attentions:\n        :param output_hidden_states:\n        :return: Tuple comprised of:\n            - x (Tensor): the last encoder layer's output of\n              shape (src_len, batch, embed_dim)\n            - encoder_states (List[Tensor]): all intermediate\n              hidden states of shape (src_len, batch, embed_dim).\n              Only populated if output_hidden_states: is True.\n            - all_attentions (List[Tensor]): Attention weights for each layer.\n            During training might not be of length n_layers because of layer dropout.\n        \"\"\"\n        # check attention mask and invert\n        if attention_mask is not None:\n            attention_mask = invert_mask(attention_mask)\n\n        inputs_embeds = self._embed_multi_modal(generated_aspect_prompt, generated_senti_prompt, aspects_num,\n            input_ids, image_features) * self.embed_scale\n        embed_pos = self.embed_positions(input_ids)\n        x = inputs_embeds + embed_pos\n        x = self.layernorm_embedding(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        encoder_states, all_attentions = [], []\n        for encoder_layer in self.layers:\n            if output_hidden_states:\n                encoder_states.append(x)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability <\n                                  self.layerdrop):  # skip the layer\n                attn = None\n            else:\n                x, attn = encoder_layer(x,\n                                        attention_mask,\n                                        output_attentions=output_attentions)\n\n            if output_attentions:\n                all_attentions.append(attn)\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n        if output_hidden_states:\n            encoder_states.append(x)\n\n        # T x B x C -> B x T x C\n        encoder_states = [\n            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n        ]\n        x = x.transpose(0, 1)\n\n        if not return_dict:\n            return tuple(v for v in [x, encoder_states, all_attentions]\n                         if v is not None)\n        return BaseModelOutput(last_hidden_state=x,\n                               hidden_states=encoder_states,\n                               attentions=all_attentions)", "\nclass MultiModalBartDecoder_span(nn.Module\n                                 ):  #AOE task and all downstream tasks\n    def __init__(self,\n                 config: MultiModalBartConfig,\n                 tokenizer,\n                 decoder,\n                 pad_token_id,\n                 label_ids,\n                 causal_mask,\n                 need_tag=True,\n                 only_sc=False,\n                 avg_feature=False,\n                 use_encoder_mlp=True):\n        super().__init__()\n        self.decoder = decoder\n        self.tokenizer = tokenizer\n        self.causal_mask = causal_mask\n        self.register_buffer('causal_masks', causal_mask.float())\n        self.pad_token_id = pad_token_id\n        # label_ids = sorted(label_ids, reverse=False)\n        self.label_start_id = min(label_ids)\n        self.label_end_id = max(label_ids) + 1\n        self.need_tag = need_tag\n        self.only_sc = only_sc\n        mapping = torch.LongTensor([0, 2] + label_ids)\n        ###mapping: [0, 2, 50276, 50277, 50278, 50281]\n        self.register_buffer('mapping', mapping)\n        self.src_start_index = len(mapping)  # \u52a0\u4e0a\u4e00\u4e2a\n        hidden_size = decoder.embed_tokens.weight.size(1)\n        self.dropout_layer = nn.Dropout(0.1)\n\n        self.end_text_id = tokenizer.end_text_id\n        self.avg_feature = avg_feature\n        if use_encoder_mlp:\n            self.encoder_mlp = nn.Sequential(\n                nn.Linear(hidden_size, hidden_size), nn.Dropout(0.3),\n                nn.ReLU(), nn.Linear(hidden_size, hidden_size))\n\n    def forward(self, tokens, state, only_sc=False):\n        # import ipdb; ipdb.set_trace()\n        '''\n        tokens: [[0, 2, 2, 16, 16, 4, 18, 18, 4, 1, 1, 1, 1],\n                 [0, 2, 2, 15, 16, 3, 25, 26, 5, 28, 28, 4, 1]]\n        '''\n        # import ipdb; ipdb.set_trace()\n        bsz, max_len = tokens.size()\n        encoder_outputs = state.encoder_output ##(batch, 72=38(len(image_token+begin_image+end_image(36+1+1)))+34(max_tex_len(\u5305\u542bbegin_text_id(0) and end_text_id(2)) in batch), 768)\n        encoder_pad_mask = state.encoder_mask ##(batch, 72)\n        first = state.first\n        # tokens\u4e4b\u540e\u76840\u5168\u662fpadding\uff0c\u56e0\u4e3a1\u662feos, \u5728pipe\u4e2d\u89c4\u5b9a\u7684\n        cumsum = tokens.eq(1).flip(dims=[1]).cumsum(dim=-1)\n        tgt_pad_mask = cumsum.flip(dims=[1]).ne(cumsum[:, -1:])\n\n        # \u628a\u8f93\u5165\u505a\u4e00\u4e0b\u6620\u5c04\n        mapping_token_mask = tokens.lt(\n            self.src_start_index)  # \u4e3a1\u7684\u5730\u65b9\u5e94\u8be5\u4ecemapping\u4e2d\u53d6index\n        mapped_tokens = tokens.masked_fill(tokens.ge(self.src_start_index), 0)\n        tag_mapped_tokens = self.mapping[mapped_tokens]\n\n        src_tokens_index = tokens - self.src_start_index  # bsz x num_src_token\n        src_tokens_index = src_tokens_index.masked_fill(\n            src_tokens_index.lt(0), 0)\n        src_tokens = state.src_tokens \n        # print(src_tokens.shape): (2, 34)\n        if first is not None:\n            src_tokens = src_tokens.gather(index=first, dim=1) ###Sequence\n        word_mapped_tokens = src_tokens.gather(index=src_tokens_index, dim=1)\n        # print('word_mapped_tokens', word_mapped_tokens)\n        tokens = torch.where(mapping_token_mask, tag_mapped_tokens,\n                             word_mapped_tokens)\n\n        tokens = tokens.masked_fill(tgt_pad_mask, self.pad_token_id)\n        '''\n        {'AESC': 50281, 'POS': 50276, 'NEU': 50277, 'NEG': 50278}\n        tensor([[0, 50276, 50276, 4644, 4644, 50278, 798, 798, 50278, 2, 1, 1, 1],\n                [0, 50276, 50276, 9517, 957, 50277, 2561, 7772, 50281, 2762, 2762, 50278, 2]])\n        \u5c06tokens\u4e2d\u7684index\u4ee5\u53ca\u6807\u7b7e\u90fd\u8f6c\u5316\u4e3avocabulary\u4e2d\u7684token_id\n        '''\n\n        if self.training:\n            tokens = tokens[:, :-1]\n            decoder_pad_mask = tokens.eq(\n                self.pad_token_id)  # decoder\u9700\u8981\u8ba9pad\u4f4d\u7f6e\u4e3a1\n\n            dict = self.decoder(input_ids=tokens,\n                                encoder_hidden_states=encoder_outputs,\n                                encoder_padding_mask=encoder_pad_mask,\n                                decoder_padding_mask=decoder_pad_mask,\n                                decoder_causal_mask=self.\n                                causal_masks[:tokens.size(1), :tokens.size(1)],\n                                return_dict=True)\n        else:\n\n            past_key_values = state.past_key_values\n            dict = self.decoder(input_ids=tokens,\n                                encoder_hidden_states=encoder_outputs,\n                                encoder_padding_mask=encoder_pad_mask,\n                                decoder_padding_mask=None,\n                                decoder_causal_mask=self.\n                                causal_masks[:tokens.size(1), :tokens.size(1)],\n                                return_dict=True)\n\n        hidden_state = dict.last_hidden_state  # bsz x max_len x hidden_size (2, 12(\u53bb\u6389\u4e86 end_token_id), 768)\n        hidden_state = self.dropout_layer(hidden_state)\n        if not self.training:\n            state.past_key_values = dict.past_key_values\n\n        logits = hidden_state.new_full(\n            (hidden_state.size(0), hidden_state.size(1),\n             self.src_start_index + src_tokens.size(-1)),\n            fill_value=-1e24)\n        ##\u5efa\u7acb\u7a7a\u7684logits\n        # print('logits', logits.shape) (bsz, max_len,  self.src_start_index + src_tokens.size(-1)) -> (2, 12, 40=6+34)\n        # \u9996\u5148\u8ba1\u7b97\u7684\u662f\n\n        if self.need_tag:\n            '''\n            self.decoder.embed_tokens.weight: (50289, 768)\n            self.label_start_id: 50276\n            '''\n            tag_scores = F.linear(\n                hidden_state,\n                self.dropout_layer(\n                    self.decoder.embed_tokens.\n                    weight[self.label_start_id:self.label_start_id +\n                           3]))  # bsz x max_len x num_class\n            logits[:, :, 3:self.src_start_index] = tag_scores ###\u7ed9\u60c5\u611f\u7684position\u8d4b\u503c[:, :, (3, 4, 5)]\n        if not only_sc:\n            eos_scores = F.linear(\n                hidden_state,\n                self.dropout_layer(self.decoder.embed_tokens.weight[2:3])) \n            '''\n            ['</s>(eos_token)', '<mask>', '<pad>', '<s>(bos_token)', '<unk>']\n            [2, 50264, 1, 0, 3]\n            '''\n\n            # bsz x max_bpe_len(image_len + text_len) x hidden_size: (2, 72, 768)\n            src_outputs = state.encoder_output \n            if hasattr(self, 'encoder_mlp') and not only_sc:\n                src_outputs = self.encoder_mlp(src_outputs)\n\n            if first is not None:\n                mask = first.eq(0)\n                src_outputs = src_outputs.gather(\n                    index=first.unsqueeze(2).repeat(1, 1,\n                                                    src_outputs.size(-1)),\n                    dim=1)\n            else:\n                mask = state.encoder_mask[:, 64:].eq(0)\n                # src_outputs = self.decoder.embed_tokens(src_tokens)\n            mask = mask.unsqueeze(1) ## bsz x 1 x max_word_len: (2, 1, 34)\n            input_embed = self.decoder.embed_tokens(\n                src_tokens)  #bsz x max_word_len x hidden_size: (2, 34, 768); src_tokens: (2, 34)\n            input_embed = self.dropout_layer(input_embed)\n            if self.avg_feature:  # \u5148\u628afeature\u5408\u5e76\u4e00\u4e0b\n                src_outputs = (src_outputs[:, 64:] + input_embed) / 2\n            word_scores = torch.einsum(\n                'blh,bnh->bln', hidden_state,\n                src_outputs[:, 64:])  # bsz x max_len x max_word_len: (2, 12, 34)\n            if not self.avg_feature:\n                gen_scores = torch.einsum(\n                    'blh,bnh->bln', hidden_state,\n                    input_embed)  # bsz x max_len x max_word_len: (2, 12, 34)\n                word_scores = (gen_scores + word_scores) / 2 \n            mask = mask.__or__(\n                src_tokens.eq(2).cumsum(dim=1).ge(1).unsqueeze(1)) ###(2, 1, 34)\n            word_scores = word_scores.masked_fill(mask, -1e32) ###(bts, max_len, max_word_len)\n            logits[:, :, self.src_start_index:] = word_scores\n            ###logits.shape (bts, max_len, max_word_len+6): (2, 12, 40)\n            logits[:, :, 1:2] = eos_scores\n        # print(torch.argmax(logits[0], dim=-1))\n        return logits\n\n    def decode(self, tokens, state, only_sc=False):\n        return self(tokens, state, only_sc)[:, -1]", "\n\nclass Span_loss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.loss_fct = nn.CrossEntropyLoss()\n        self.fc = nn.LogSoftmax(dim=-1)\n\n    def forward(self, tgt_tokens, pred, mask):\n        '''\n        tgt_tokens: (2 (batch-size), 12 (max_len+1))\n        pred: (2, 12, 40 (max_word_len))\n        '''\n\n        tgt_tokens = tgt_tokens.masked_fill(mask.eq(0), -100)\n        output = F.cross_entropy(target=tgt_tokens, input=pred.transpose(1, 2)) ##\u6bcf\u4e00\u4e2a\u8bcd\u90fd\u670912\u79cd\u7c7b\u522b\uff0c input= (40, 12)\n        return output", "\n\nclass MultiModalBartDecoder_MLM(nn.Module):\n    def __init__(self, config: MultiModalBartConfig, decoder):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.register_buffer(\n            \"final_logits_bias\",\n            torch.zeros((1, self.decoder.embed_tokens.num_embeddings)))\n\n    def forward(self, labels, input_ids, encoder_outputs, attention_mask,\n                decoder_input_ids, decoder_attention_mask):\n        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n            self.config,\n            input_ids,\n            decoder_input_ids=decoder_input_ids,\n            decoder_padding_mask=decoder_attention_mask,\n            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)\n        decoder_outputs = self.decoder(\n            decoder_input_ids,\n            encoder_outputs,\n            attention_mask,\n            decoder_padding_mask,\n            decoder_causal_mask=causal_mask[:decoder_input_ids.size(1), :\n                                            decoder_input_ids.size(1)],\n        )\n\n        lm_logits = F.linear(decoder_outputs[0][:, 1:],\n                             self.decoder.embed_tokens.weight,\n                             bias=self.final_logits_bias)\n\n        lm_loss = 0\n        # compute lm loss if labels is given\n        if labels is not None:\n            labels = labels.clone()\n            loss_fct = nn.CrossEntropyLoss()\n            lm_loss = loss_fct(\n                lm_logits.view(-1, self.decoder.embed_tokens.weight.size(0)),\n                labels.reshape(-1))\n\n            return lm_loss", "\n\nclass MultiModalBartDecoder_ANP_generate(nn.Module):  #AOG task\n    def __init__(self, config: MultiModalBartConfig, decoder):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.register_buffer(\n            \"final_logits_bias\",\n            torch.zeros((1, self.decoder.embed_tokens.num_embeddings)))\n\n    def forward(self, labels, input_ids, encoder_outputs, attention_mask,\n                decoder_input_ids, decoder_attention_mask):\n        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n            self.config,\n            input_ids,\n            decoder_input_ids=decoder_input_ids,\n            decoder_padding_mask=decoder_attention_mask,\n            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)\n\n        decoder_outputs = self.decoder(\n            decoder_input_ids,\n            encoder_outputs,\n            attention_mask,\n            decoder_padding_mask,\n            decoder_causal_mask=causal_mask[:decoder_input_ids.size(1), :\n                                            decoder_input_ids.size(1)],\n        )\n\n        lm_logits = F.linear(decoder_outputs[0][:, 1:],\n                             self.decoder.embed_tokens.weight,\n                             bias=self.final_logits_bias)\n\n        lm_loss = 0\n        # compute lm loss if labels is given\n        if labels is not None:\n            labels = labels.clone()\n            # labels[labels == self.cls_token_id] = -100\n            loss_fct = nn.CrossEntropyLoss()\n            lm_loss = loss_fct(\n                lm_logits.view(-1, self.decoder.embed_tokens.weight.size(0)),\n                labels.reshape(-1))\n\n            return lm_loss", "\n\nclass MultiModalBartDecoder_sentiment(nn.Module):  #MSP task\n    def __init__(self,\n                 config: MultiModalBartConfig,\n                 decoder,\n                 senti_ids,\n                 senti_nums=3):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.senti_ids = senti_ids\n        self.dropout_layer = nn.Dropout(0.1)\n        self.senti_head = BartClassificationHead(config.d_model,\n                                                 config.d_model, senti_nums,\n                                                 config.classif_dropout)\n\n    def _init_weights(self, module):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, senti_labels, encoder_outputs, attention_mask,\n                senti_decoder_input_ids):\n\n        decoder_outputs = self.decoder(\n            input_ids=senti_decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            encoder_padding_mask=attention_mask,\n            decoder_padding_mask=None,\n            decoder_causal_mask=None,\n        )\n\n        # predict_senti = F.linear(\n        #     decoder_outputs[0][:, 1],\n        #     self.dropout_layer(self.decoder.embed_tokens.\n        #                        weight[self.senti_ids[0]:self.senti_ids[2] +\n        #                               1]))  # bsz\n        # predict_senti = torch.flip(predict_senti, dims=[-1])\n        predict_senti = self.senti_head(decoder_outputs[0][:, 1])\n        loss_fct = nn.CrossEntropyLoss()\n        senti_loss = loss_fct(predict_senti, senti_labels)\n        return senti_loss, predict_senti", "\n\nclass MultiModalBartDecoder_MRM(nn.Module):\n    def __init__(self, config: MultiModalBartConfig, decoder, causal_mask,\n                 args):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.causal_mask = causal_mask\n        self.args = args\n        self.mrm_head = BartClassificationHead(\n            config.d_model,\n            config.d_model,\n            config.num_labels,\n            config.classif_dropout,\n        )\n        self._init_weights(self.mrm_head.dense)\n        self._init_weights(self.mrm_head.out_proj)\n\n    def _init_weights(self, module):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, mrm_labels, mrm_masks, encoder_outputs, attention_mask,\n                mrm_decoder_input_ids, mrm_decoder_attention_mask):\n\n        decoder_padding_mask = mrm_decoder_attention_mask.eq(0)\n        decoder_outputs = self.decoder(\n            input_ids=mrm_decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            encoder_padding_mask=attention_mask,\n            decoder_padding_mask=decoder_padding_mask,\n            decoder_causal_mask=self.causal_mask[:mrm_decoder_input_ids.size(\n                1), :mrm_decoder_input_ids.size(1)].to(\n                    mrm_decoder_input_ids.device),\n        )\n        region_representation = decoder_outputs[0][mrm_masks.bool()]\n        if len(region_representation) > 0:\n            predict_cls = self.mrm_head(region_representation)\n            loss_fct = nn.CrossEntropyLoss()\n            mrm_labels = torch.cat(mrm_labels,\n                                   dim=0).to(encoder_outputs.device)\n\n            if self.args.mrm_loss_type == 'KL':\n                predict_cls = F.log_softmax(predict_cls, dim=-1)\n                mrm_loss = F.kl_div(predict_cls.double(),\n                                    mrm_labels.double().squeeze(1),\n                                    reduction='batchmean')\n            else:\n                raise RuntimeError(\"wrong mrm type\")\n        else:\n            mrm_loss = 0\n\n        return mrm_loss", "\n\n'''\ngenerate_aspect_prompt based on the multimodal context\n'''\nclass MultiModalBartDecoder_generate_aspect_prompt(nn.Module): \n    def __init__(self, config: MultiModalBartConfig, decoder):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.aspect_prompt_linear = nn.Linear(768, 768)\n\n\n    def forward(self, encoder_outputs, attention_mask,\n                decoder_input_ids, decoder_attention_mask):\n\n        # import ipdb; ipdb.set_trace()\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            encoder_padding_mask=attention_mask.eq(0),\n            decoder_padding_mask=decoder_attention_mask.eq(0),\n            decoder_causal_mask=None,\n        )\n\n        prompt_logits = decoder_outputs[0]\n        aspect_prompt_logits = self.aspect_prompt_linear(prompt_logits)\n\n\n        return aspect_prompt_logits ", "\n\n'''\ngenerate_sentiment_prompt based on the multimodal context\n'''\nclass MultiModalBartDecoder_generate_sentiment_prompt(nn.Module): \n    def __init__(self, config: MultiModalBartConfig, decoder):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.senti_prompt_linear = nn.Linear(768, 768)\n\n\n    def forward(self, encoder_outputs, attention_mask,\n                decoder_input_ids, decoder_attention_mask):\n\n        # import ipdb; ipdb.set_trace()\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            encoder_padding_mask=attention_mask.eq(0),\n            decoder_padding_mask=decoder_attention_mask.eq(0),\n            decoder_causal_mask=None,\n        )\n\n        prompt_logits = decoder_outputs[0]\n        senti_prompt_logits = self.senti_prompt_linear(prompt_logits)\n\n\n        return senti_prompt_logits", "\n\nclass MultiModalBartDecoder_aspects_num(nn.Module):  #MSP task\n    def __init__(self,\n                 config: MultiModalBartConfig,\n                 decoder,\n                 max_aspects_nums=5):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.dropout_layer = nn.Dropout(0.1)\n        self.aspects_num_head = BartClassificationHead(config.d_model,\n                                                 config.d_model, max_aspects_nums,\n                                                 config.classif_dropout)\n\n    def _init_weights(self, module):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, aspects_num_labels, encoder_outputs, attention_mask,\n                aspects_num_decoder_input_ids):\n        \n        decoder_outputs = self.decoder(\n            input_ids=aspects_num_decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            encoder_padding_mask=attention_mask,\n            decoder_padding_mask=None,\n            decoder_causal_mask=None,\n        )\n\n        # predict_aspects_num = F.linear(\n        #     decoder_outputs[0][:, 1],\n        #     self.dropout_layer(self.decoder.embed_tokens.\n        #                        weight[self.aspects_num_ids[0]:self.aspects_num_ids[2] +\n        #                               1]))  # bsz\n        # predict_aspects_num = torch.flip(predict_aspects_num, dims=[-1])\n        predict_aspects_num_logits = self.aspects_num_head(decoder_outputs[0][:, 1])\n        loss_fct = nn.CrossEntropyLoss()\n        aspects_num_loss = loss_fct(predict_aspects_num_logits, aspects_num_labels)\n        return aspects_num_loss, predict_aspects_num_logits"]}
{"filename": "src/model/config.py", "chunked_list": ["from transformers import BartConfig\n\n\nclass MultiModalBartConfig(BartConfig):\n    def __init__(\n            self,\n            activation_dropout=0.0,\n            extra_pos_embeddings=2,\n            activation_function=\"gelu\",\n            vocab_size=50323,\n            image_feature_size=2048 + 4,\n            d_model=1024,\n            encoder_ffn_dim=4096,\n            encoder_layers=12,\n            encoder_attention_heads=16,\n            decoder_ffn_dim=4096,\n            decoder_layers=12,\n            decoder_attention_heads=16,\n            encoder_layerdrop=0.0,\n            decoder_layerdrop=0.0,\n            attention_dropout=0.0,\n            dropout=0.1,\n            max_position_embeddings=1024,\n            init_std=0.02,\n            classif_dropout=0.0,\n            num_labels=1,\n            num_attributes=1,\n            num_relations=1,\n            is_encoder_decoder=True,\n            pad_token_id=1,\n            bos_token_id=0,\n            eos_token_id=2,\n            img_feat_id=50273,\n            cls_token_id=50276,\n            normalize_before=False,\n            add_final_layer_norm=False,\n            scale_embedding=False,\n            normalize_embedding=True,\n            static_position_embeddings=False,\n            add_bias_logits=False,\n            decoder_start_token_id=0,\n            partial_load=(),\n            lm_loss_factor=1.0,\n            mrm_loss_factor=1.0,\n            attribute_loss_factor=1.0,\n            relation_loss_factor=1.0,\n            num_aspects =1,\n            **common_kwargs\n    ):\n        super(MultiModalBartConfig, self).__init__(\n            activation_dropout=activation_dropout,\n            extra_pos_embeddings=extra_pos_embeddings,\n            activation_function=activation_function,\n            vocab_size=vocab_size,\n            d_model=d_model,\n            encoder_ffn_dim=encoder_ffn_dim,\n            encoder_layers=encoder_layers,\n            encoder_attention_heads=encoder_attention_heads,\n            decoder_ffn_dim=decoder_ffn_dim,\n            decoder_layers=decoder_layers,\n            decoder_attention_heads=decoder_attention_heads,\n            encoder_layerdrop=encoder_layerdrop,\n            decoder_layerdrop=decoder_layerdrop,\n            attention_dropout=attention_dropout,\n            dropout=dropout,\n            max_position_embeddings=max_position_embeddings,\n            init_std=init_std,\n            classif_dropout=classif_dropout,\n            num_labels=num_labels,\n            is_encoder_decoder=is_encoder_decoder,\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            normalize_before=normalize_before,\n            add_final_layer_norm=add_final_layer_norm,\n            scale_embedding=scale_embedding,\n            normalize_embedding=normalize_embedding,\n            static_position_embeddings=static_position_embeddings,\n            add_bias_logits=add_bias_logits,\n            decoder_start_token_id=decoder_start_token_id,\n            **common_kwargs\n        )\n\n        self.image_feature_size = image_feature_size\n        self.img_feat_id = img_feat_id\n        self.cls_token_id = cls_token_id\n        self.partial_load = partial_load\n        self.num_attributes = num_attributes\n        self.num_relations = num_relations\n        self.lm_loss_factor = lm_loss_factor\n        self.mrm_loss_factor = mrm_loss_factor\n        self.attribute_loss_factor = attribute_loss_factor\n        self.relation_loss_factor = relation_loss_factor\n        self.num_aspects = num_aspects", ""]}
{"filename": "src/model/model_for_prompt.py", "chunked_list": ["# Based on transformers.modeling_bart\n\nfrom typing import Optional, Tuple\nfrom fastNLP.modules.torch.encoder import Seq2SeqEncoder\nfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\nfrom fastNLP.modules.torch import State\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom src.model.modeling_bart import (PretrainedBartModel, BartEncoder,", "from torch import nn\nfrom src.model.modeling_bart import (PretrainedBartModel, BartEncoder,\n                                     BartDecoder, BartModel,\n                                     BartClassificationHead,\n                                     _make_linear_from_emb,\n                                     _prepare_bart_decoder_inputs)\n\nfrom transformers import BartTokenizer\n\nfrom src.model.config import MultiModalBartConfig", "\nfrom src.model.config import MultiModalBartConfig\nfrom src.model.mixins import GenerationMixin, FromPretrainedMixin\nfrom src.model.modules_for_prompt import MultiModalBartEncoder, MultiModalBartDecoder_span, MultiModalBartDecoder_MLM, MultiModalBartDecoder_sentiment, Span_loss, MultiModalBartDecoder_MRM, MultiModalBartDecoder_ANP_generate\n\n# This is based on transformers.BartModel\n# The modifications are:\n# - BartConfig -> MultiModalBartConfig\n# - BartEncoder -> MultiModalBartEncoder\n# - added image_features in forward", "# - BartEncoder -> MultiModalBartEncoder\n# - added image_features in forward\n\n\n# def generate_span_mask(spans):\n#     max_len = max([len(x) for x in spans])\n#     mask = torch.ones(())\nclass MultiModalBartModelForPretrain(FromPretrainedMixin, PretrainedBartModel):\n    def build_model(self,\n                    args,\n                    bart_model,\n                    tokenizer,\n                    label_ids,\n                    config,\n                    decoder_type=None,\n                    copy_gate=False,\n                    use_encoder_mlp=False,\n                    use_recur_pos=False,\n                    tag_first=False):\n        if args.bart_init:\n            model = BartModel.from_pretrained(bart_model)\n            num_tokens, _ = model.encoder.embed_tokens.weight.shape\n\n            model.resize_token_embeddings(\n                len(tokenizer.unique_no_split_tokens) + num_tokens)\n            encoder = model.encoder\n            decoder = model.decoder\n\n            padding_idx = config.pad_token_id\n            encoder.embed_tokens.padding_idx = padding_idx\n\n            _tokenizer = BartTokenizer.from_pretrained(bart_model)\n\n            for token in tokenizer.unique_no_split_tokens:\n                if token[:2] == '<<':  # \u7279\u6b8a\u5b57\u7b26\n                    index = tokenizer.convert_tokens_to_ids(\n                        tokenizer._base_tokenizer.tokenize(token))\n                    if len(index) > 1:\n                        raise RuntimeError(f\"{token} wrong split\")\n                    else:\n                        index = index[0]\n                    assert index >= num_tokens, (index, num_tokens, token)\n                    indexes = _tokenizer.convert_tokens_to_ids(\n                        _tokenizer.tokenize(token[2:-2]))\n                    embed = model.encoder.embed_tokens.weight.data[indexes[0]]\n                    for i in indexes[1:]:\n                        embed += model.decoder.embed_tokens.weight.data[i]\n                    embed /= len(indexes)\n                    model.decoder.embed_tokens.weight.data[index] = embed\n        else:\n            raise RuntimeError(\"error init!!!!!!!\")\n\n        multimodal_encoder = MultiModalBartEncoder(config, encoder,\n                                                   tokenizer.img_feat_id,\n                                                   tokenizer.cls_token_id)\n        return (multimodal_encoder, decoder)\n\n    def __init__(self, config: MultiModalBartConfig, bart_model, tokenizer,\n                 label_ids, senti_ids, args):\n        super().__init__(config)\n        self.config = config\n        label_ids = sorted(label_ids)\n        multimodal_encoder, share_decoder = self.build_model(\n            args, bart_model, tokenizer, label_ids, config)\n        causal_mask = torch.zeros(512, 512).fill_(float('-inf'))\n        self.causal_mask = causal_mask.triu(diagonal=1)\n        self.encoder = multimodal_encoder\n        self.mlm_decoder = MultiModalBartDecoder_MLM(self.config,\n                                                     share_decoder)\n        self.mrm_decoder = MultiModalBartDecoder_MRM(self.config,\n                                                     share_decoder,\n                                                     self.causal_mask, args)\n        self.span_decoder = MultiModalBartDecoder_span(self.config, tokenizer,\n                                                       share_decoder,\n                                                       tokenizer.pad_token_id,\n                                                       label_ids,\n                                                       self.causal_mask)\n        self.span_loss_fct = Span_loss()\n        self.anp_generate_decoder = MultiModalBartDecoder_ANP_generate(\n            self.config, share_decoder)\n        self.senti_decoder = MultiModalBartDecoder_sentiment(\n            self.config, share_decoder, senti_ids)\n\n    def prepare_state(self,\n                      input_ids,\n                      image_features,\n                      attention_mask=None,\n                      first=None):\n        dict = self.encoder(input_ids=input_ids,\n                            image_features=image_features,\n                            attention_mask=attention_mask,\n                            output_hidden_states=True,\n                            return_dict=True)\n        encoder_outputs = dict.last_hidden_state\n        hidden_states = dict.hidden_states\n        encoder_mask = attention_mask\n        src_embed_outputs = hidden_states[0]\n        state = BartState(encoder_outputs, encoder_mask, input_ids[:, 64:],\n                          first, src_embed_outputs)\n        # setattr(state, 'tgt_seq_len', tgt_seq_len)\n        return state\n\n    def forward(\n            self,\n            task_type,\n            input_ids,\n            image_features,\n            attention_mask=None,\n            mlm_infos=None,\n            mrm_infos=None,\n            senti_infos=None,\n            ANP_infos=None,\n            ANP_generate_infos=None,\n            ae_infos=None,\n            oe_infos=None,\n            ae_oe_infos=None,\n            encoder_outputs: Optional[Tuple] = None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n    ):\n\n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                image_features=image_features,\n                attention_mask=attention_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n            )\n        assert isinstance(encoder_outputs, tuple)\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        if task_type == 'MLM':\n            labels, decoder_input_ids, decoder_attention_mask = [\n                mlm_infos['mlm_labels'], mlm_infos['mlm_decoder_input_ids'],\n                mlm_infos['mlm_decoder_attention_mask']\n            ]\n            loss = self.mlm_decoder(labels, input_ids, encoder_outputs[0],\n                                    attention_mask, decoder_input_ids,\n                                    decoder_attention_mask)\n        elif task_type == 'MRM':\n            mrm_labels, mrm_masks, decoder_input_ids, decoder_attention_mask = [\n                mrm_infos['mrm_labels'],\n                mrm_infos['mrm_masks'].to(input_ids.device),\n                mrm_infos['mrm_decoder_input_ids'].to(input_ids.device),\n                mrm_infos['mrm_decoder_attention_mask'].to(input_ids.device)\n            ]\n            loss = self.mrm_decoder(mrm_labels, mrm_masks, encoder_outputs[0],\n                                    attention_mask, decoder_input_ids,\n                                    decoder_attention_mask)\n        elif task_type == 'Sentiment':\n            senti_labels, decoder_input_ids, decoder_attention_mask = [\n                senti_infos['senti_labels'],\n                senti_infos['senti_decoder_input_ids'],\n                senti_infos['senti_decoder_attention_mask']\n            ]\n            loss, predict_senti = self.senti_decoder(senti_labels,\n                                                     encoder_outputs[0],\n                                                     attention_mask,\n                                                     decoder_input_ids)\n        elif task_type == 'ANP_generate':\n            labels, decoder_input_ids, decoder_attention_mask = [\n                ANP_generate_infos['anp_generate_labels'],\n                ANP_generate_infos['anp_generate_decoder_input_ids'],\n                ANP_generate_infos['anp_generate_decoder_attention_mask']\n            ]\n            loss = self.anp_generate_decoder(labels, input_ids,\n                                             encoder_outputs[0],\n                                             attention_mask, decoder_input_ids,\n                                             decoder_attention_mask)\n        elif task_type == 'AE_OE':\n            spans, span_mask = [\n                ae_oe_infos['labels'].to(input_ids.device),\n                ae_oe_infos['masks'].to(input_ids.device)\n            ]\n            state = self.prepare_state(input_ids, image_features,\n                                       attention_mask)\n            logits = self.span_decoder(spans, state)\n            loss = self.span_loss_fct(spans[:, 1:], logits, span_mask[:, 1:])\n        else:\n            raise RuntimeError(\"task type error!!!!!!!\")\n\n        if task_type == 'Sentiment':\n            return loss, predict_senti\n        return loss", "\n\nclass BartState(State):\n    def __init__(self, encoder_output, encoder_mask, src_tokens, first,\n                 src_embed_outputs):\n        super().__init__(encoder_output, encoder_mask)\n        self.past_key_values = None\n        self.src_tokens = src_tokens\n        self.first = first\n        self.src_embed_outputs = src_embed_outputs\n\n    def reorder_state(self, indices: torch.LongTensor):\n        super().reorder_state(indices)\n        self.src_tokens = self._reorder_state(self.src_tokens, indices)\n        if self.first is not None:\n            self.first = self._reorder_state(self.first, indices)\n        self.src_embed_outputs = self._reorder_state(self.src_embed_outputs,\n                                                     indices)\n        if self.past_key_values is not None:\n            new = []\n            for layer in self.past_key_values:\n                new_layer = {}\n                for key1 in list(layer.keys()):\n                    new_layer_ = {}\n                    for key2 in list(layer[key1].keys()):\n                        if layer[key1][key2] is not None:\n                            layer[key1][key2] = self._reorder_state(\n                                layer[key1][key2], indices)\n                        new_layer_[key2] = layer[key1][key2]\n                    new_layer[key1] = new_layer_\n                new.append(new_layer)\n            self.past_key_values = new"]}
{"filename": "src/model/metrics.py", "chunked_list": ["from collections import Counter\nimport numpy as np\nimport torch\n\n\nclass AESCSpanMetric(object):\n    def __init__(self,\n                 eos_token_id,\n                 num_labels,\n                 conflict_id,\n                 opinion_first=False):\n        super(AESCSpanMetric, self).__init__()\n        self.eos_token_id = eos_token_id\n        self.word_start_index = num_labels + 2\n\n        self.aesc_fp = 0\n        self.aesc_tp = 0\n        self.aesc_fn = 0\n        self.ae_fp = 0\n        self.ae_tp = 0\n        self.ae_fn = 0\n        self.sc_fp = Counter()\n        self.sc_tp = Counter()\n        self.sc_fn = Counter()\n        self.sc_right = 0\n        self.sc_all_num = 0\n\n        self.em = 0\n        self.total = 0\n        self.invalid = 0\n        self.conflict_id = conflict_id\n        # assert opinion_first is False, \"Current metric only supports aspect first\"\n\n    def evaluate(self, aesc_target_span, pred, tgt_tokens):\n        # print('aesc_target_span', aesc_target_span[0])\n        # print(pred[0])\n        # print(tgt_tokens[0])\n        self.total += pred.size(0)\n        pred_eos_index = pred.flip(dims=[1]).eq(\n            self.eos_token_id).cumsum(dim=1).long()\n        target_eos_index = tgt_tokens.flip(dims=[1]).eq(\n            self.eos_token_id).cumsum(dim=1).long()\n\n        pred = pred[:, 1:]  # \u53bb\u6389</s>\n        tgt_tokens = tgt_tokens[:, 1:]\n        pred_seq_len = pred_eos_index.flip(dims=[1]).eq(\n            pred_eos_index[:, -1:]).sum(dim=1)  # bsz\n        pred_seq_len = (pred_seq_len - 2).tolist()\n        target_seq_len = target_eos_index.flip(dims=[1]).eq(\n            target_eos_index[:, -1:]).sum(dim=1)  # bsz\n        target_seq_len = (target_seq_len - 2).tolist()\n        pred_spans = []\n        flag = True\n        for i, (ts, ps) in enumerate(zip(aesc_target_span, pred.tolist())):\n            em = 0\n            assert ps[0] == tgt_tokens[i, 0]\n            ps = ps[2:pred_seq_len[i]]\n            if pred_seq_len[i] == target_seq_len[i]:\n                em = int(tgt_tokens[i, :target_seq_len[i]].eq(\n                    pred[i, :target_seq_len[i]]).sum().item() ==\n                         target_seq_len[i])\n            self.em += em\n            invalid = 0\n            pairs = []\n            cur_pair = []\n            if len(ps):\n                for index, j in enumerate(ps):\n                    if j < self.word_start_index:\n                        cur_pair.append(j)\n                        if len(cur_pair) != 3 or cur_pair[0] > cur_pair[1]:\n                            invalid = 1\n                        else:\n                            pairs.append(tuple(cur_pair))\n                        cur_pair = []\n                    else:\n                        cur_pair.append(j)\n            pred_spans.append(pairs.copy())\n\n            # print(pred_spans)\n            self.invalid += invalid\n\n            aesc_target_counter = Counter()\n            aesc_pred_counter = Counter()\n            ae_target_counter = Counter()\n            ae_pred_counter = Counter()\n            conflicts = set()\n            # if flag:\n            #     print(tgt_tokens[0])\n            #     print(pred[0])\n            #     print(ts)\n            #     print(pairs)\n            #     flag = False\n            for t in ts:\n                ae_target_counter[(t[0], t[1])] = 1\n                if t[2] != self.conflict_id:\n                    aesc_target_counter[(t[0], t[1])] = t[2]\n                else:\n                    conflicts.add((t[0], t[1]))\n\n            for p in pairs:\n                ae_pred_counter[(p[0], p[1])] = 1\n                if (p[0], p[1]) not in conflicts and p[-1] not in (\n                        0, 1, self.conflict_id):\n                    aesc_pred_counter[(p[0], p[1])] = p[-1]\n\n            # \u8fd9\u91cc\u76f8\u540c\u7684pair\u4f1a\u88ab\u8ba1\u7b97\u591a\u6b21\n            tp, fn, fp = _compute_tp_fn_fp(\n                [(key[0], key[1], value)\n                 for key, value in aesc_pred_counter.items()],\n                [(key[0], key[1], value)\n                 for key, value in aesc_target_counter.items()])\n            self.aesc_fn += fn\n            self.aesc_fp += fp\n            self.aesc_tp += tp\n\n            tp, fn, fp = _compute_tp_fn_fp(list(aesc_pred_counter.keys()),\n                                           list(aesc_target_counter.keys()))\n            self.ae_fn += fn\n            self.ae_fp += fp\n            self.ae_tp += tp\n\n            # sorry, this is a very wrongdoing, but to make it comparable with previous work, we have to stick to the\n            #   error\n            for key in aesc_pred_counter:\n                if key not in aesc_target_counter:\n                    continue\n                self.sc_all_num += 1\n                if aesc_target_counter[key] == aesc_pred_counter[key]:\n                    self.sc_tp[aesc_pred_counter[key]] += 1\n                    self.sc_right += 1\n                    aesc_target_counter.pop(key)\n                else:\n                    self.sc_fp[aesc_pred_counter[key]] += 1\n                    self.sc_fn[aesc_target_counter[key]] += 1\n\n    def pri(self):\n        print('aesc_fp tp fn', self.aesc_fp, self.aesc_tp, self.aesc_fn)\n        print('ae_fp tp fn', self.ae_fp, self.ae_tp, self.ae_fn)\n\n    def get_metric(self, reset=True):\n        res = {}\n        f, pre, rec = _compute_f_pre_rec(1, self.aesc_tp, self.aesc_fn,\n                                         self.aesc_fp)\n        res['aesc_f'] = round(f * 100, 2)\n        res['aesc_rec'] = round(rec * 100, 2)\n        res['aesc_pre'] = round(pre * 100, 2)\n\n        f, pre, rec = _compute_f_pre_rec(1, self.ae_tp, self.ae_fn, self.ae_fp)\n        res['ae_f'] = round(f * 100, 2)\n        res['ae_rec'] = round(rec * 100, 2)\n        res['ae_pre'] = round(pre * 100, 2)\n\n        tags = set(self.sc_tp.keys())\n        tags.update(set(self.sc_fp.keys()))\n        tags.update(set(self.sc_fn.keys()))\n        f_sum = 0\n        pre_sum = 0\n        rec_sum = 0\n        for tag in tags:\n            assert tag not in (0, 1, self.conflict_id), (tag, self.conflict_id)\n            tp = self.sc_tp[tag]\n            fn = self.sc_fn[tag]\n            fp = self.sc_fp[tag]\n            f, pre, rec = _compute_f_pre_rec(1, tp, fn, fp)\n            f_sum += f\n            pre_sum += pre\n            rec_sum += rec\n\n        rec_sum /= (len(tags) + 1e-12)\n        pre_sum /= (len(tags) + 1e-12)\n        res['sc_f'] = round(\n            2 * pre_sum * rec_sum / (pre_sum + rec_sum + 1e-12) * 100, 2)\n        res['sc_rec'] = round(rec_sum * 100, 2)\n        res['sc_pre'] = round(pre_sum * 100, 2)\n        res['sc_acc'] = round(\n            1.0 * self.sc_right / (self.sc_all_num + 1e-12) * 100, 2)\n        res['sc_all_num'] = self.sc_all_num\n        res['em'] = round(self.em / self.total, 4)\n        res['invalid'] = round(self.invalid / self.total, 4)\n        if reset:\n            self.aesc_fp = 0\n            self.aesc_tp = 0\n            self.aesc_fn = 0\n            self.ae_fp = 0\n            self.ae_tp = 0\n            self.ae_fn = 0\n            self.sc_all_num = 0\n            self.sc_right = 0\n            self.sc_fp = Counter()\n            self.sc_tp = Counter()\n            self.sc_fn = Counter()\n\n        return res", "\n\ndef _compute_f_pre_rec(beta_square, tp, fn, fp):\n    r\"\"\"\n\n    :param tp: int, true positive\n    :param fn: int, false negative\n    :param fp: int, false positive\n    :return: (f, pre, rec)\n    \"\"\"\n    pre = tp / (fp + tp + 1e-13)\n    rec = tp / (fn + tp + 1e-13)\n    f = (1 + beta_square) * pre * rec / (beta_square * pre + rec + 1e-13)\n\n    return f, pre, rec", "\n\ndef _compute_tp_fn_fp(ps, ts):\n    ps = ps.copy()\n    tp = 0\n    fp = 0\n    fn = 0\n    # print(ts)\n    # print(ps)\n    if isinstance(ts, (list, set)):\n        ts = {key: 1 for key in list(ts)}\n    if isinstance(ps, (list, set)):\n        ps = {key: 1 for key in list(ps)}\n    for key in ts.keys():\n        # print(key)\n        t_num = ts[key]\n        if key not in ps:\n            p_num = 0\n        else:\n            p_num = ps[key]\n        # print(p_num, t_num)\n        tp += min(p_num, t_num)\n        fp += max(p_num - t_num, 0)\n        fn += max(t_num - p_num, 0)\n        # print(fp, tp, fn)\n        if key in ps:\n            ps.pop(key)\n    fp += sum(ps.values())\n    # print(fp, tp, fn)\n    return tp, fn, fp", "\n\nclass OESpanMetric(object):\n    def __init__(self, eos_token_id, num_labels, opinion_first=True):\n        super(OESpanMetric, self).__init__()\n        self.eos_token_id = eos_token_id\n        self.word_start_index = num_labels + 2\n\n        self.oe_fp = 0\n        self.oe_tp = 0\n        self.oe_fn = 0\n        self.em = 0\n        self.total = 0\n        self.invalid = 0\n        # assert opinion_first is False, \"Current metric only supports aspect first\"\n\n        self.opinin_first = opinion_first\n\n    def evaluate(self, oe_target_span, pred, tgt_tokens):\n        self.total += pred.size(0)\n        pred_eos_index = pred.flip(dims=[1]).eq(\n            self.eos_token_id).cumsum(dim=1).long()\n        target_eos_index = tgt_tokens.flip(dims=[1]).eq(\n            self.eos_token_id).cumsum(dim=1).long()\n\n        pred = pred[:, 1:]  # \u53bb\u6389</s>\n        tgt_tokens = tgt_tokens[:, 1:]\n        pred_seq_len = pred_eos_index.flip(dims=[1]).eq(\n            pred_eos_index[:, -1:]).sum(dim=1)  # bsz\n        pred_seq_len = (pred_seq_len - 2).tolist()\n        target_seq_len = target_eos_index.flip(dims=[1]).eq(\n            target_eos_index[:, -1:]).sum(dim=1)  # bsz\n        target_seq_len = (target_seq_len - 2).tolist()\n        pred_spans = []\n        flag = True\n        for i, (ts, ps) in enumerate(zip(oe_target_span, pred.tolist())):\n            em = 0\n            assert ps[0] == tgt_tokens[i, 0]\n            ps = ps[2:pred_seq_len[i]]\n            if pred_seq_len[i] == target_seq_len[i]:\n                em = int(tgt_tokens[i, :target_seq_len[i]].eq(\n                    pred[i, :target_seq_len[i]]).sum().item() ==\n                         target_seq_len[i])\n            self.em += em\n            invalid = 0\n            pairs = []\n            cur_pair = []\n            if len(ps):\n                for index, j in enumerate(ps, start=1):\n                    if index % 2 == 0:\n                        cur_pair.append(j)\n                        if cur_pair[0]>cur_pair[1] or cur_pair[0]<self.word_start_index\\\n                                or cur_pair[1]<self.word_start_index:\n                            invalid = 1\n                        else:\n                            pairs.append(tuple(cur_pair))\n                        cur_pair = []\n                    else:\n                        cur_pair.append(j)\n            self.invalid += invalid\n\n            oe_target_counter = Counter([tuple(t) for t in ts])\n            oe_pred_counter = Counter(pairs)\n            # if flag:\n            #     print(tgt_tokens[0])\n            #     print(pred[0])\n            #     print(ts)\n            #     print(pairs)\n            #     flag = False\n            # \u8fd9\u91cc\u76f8\u540c\u7684pair\u4f1a\u88ab\u8ba1\u7b97\u591a\u6b21\n            tp, fn, fp = _compute_tp_fn_fp(set(list(oe_pred_counter.keys())),\n                                           set(list(oe_target_counter.keys())))\n            self.oe_fn += fn\n            self.oe_fp += fp\n            self.oe_tp += tp\n\n    def get_metric(self, reset=True):\n        res = {}\n        f, pre, rec = _compute_f_pre_rec(1, self.oe_tp, self.oe_fn, self.oe_fp)\n\n        res['oe_f'] = round(f * 100, 2)\n        res['oe_rec'] = round(rec * 100, 2)\n        res['oe_pre'] = round(pre * 100, 2)\n\n        res['em'] = round(self.em / self.total, 4)\n        res['invalid'] = round(self.invalid / self.total, 4)\n        if reset:\n            self.oe_fp = 0\n            self.oe_tp = 0\n            self.oe_fn = 0\n\n        return res", "\n\n# metric = AESCSpanMetric(1, 3, -1)\n\n# spans = [[(6, 7, 3), (9, 10, 4)]]\n# pred = torch.tensor([[0, 2, 2, 6, 7, 3, 9, 9, 4, 1, 1]])\n# print(pred.size())\n# tgt = torch.tensor([[0, 2, 2, 6, 7, 3, 9, 10, 4, 1, 1]])\n# metric.evaluate(spans, pred, tgt)\n", "# metric.evaluate(spans, pred, tgt)\n\n# metric.pri()"]}
{"filename": "src/model/generater_for_generated_prompt_multitasks.py", "chunked_list": ["r\"\"\"undocumented\"\"\"\n\nimport torch\nfrom torch import nn\n\nfrom fastNLP.models.torch.seq2seq_model import Seq2SeqModel\nfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\nimport torch.nn.functional as F\n# from fastNLP.core.utils import _get_model_device\nfrom functools import partial", "# from fastNLP.core.utils import _get_model_device\nfrom functools import partial\n\ndef _get_model_device(model):\n    r\"\"\"\n    \u4f20\u5165\u4e00\u4e2ann.Module\u7684\u6a21\u578b\uff0c\u83b7\u53d6\u5b83\u6240\u5728\u7684device\n\n    :param model: nn.Module\n    :return: torch.device,None \u5982\u679c\u8fd4\u56de\u503c\u4e3aNone\uff0c\u8bf4\u660e\u8fd9\u4e2a\u6a21\u578b\u6ca1\u6709\u4efb\u4f55\u53c2\u6570\u3002\n    \"\"\"\n    # TODO \u8fd9\u4e2a\u51fd\u6570\u5b58\u5728\u4e00\u5b9a\u7684\u98ce\u9669\uff0c\u56e0\u4e3a\u540c\u4e00\u4e2a\u6a21\u578b\u53ef\u80fd\u5b58\u5728\u67d0\u4e9bparameter\u4e0d\u5728\u663e\u5361\u4e2d\uff0c\u6bd4\u5982BertEmbedding. \u6216\u8005\u8de8\u663e\u5361\n    assert isinstance(model, nn.Module)\n\n    parameters = list(model.parameters())\n    if len(parameters) == 0:\n        return None\n    else:\n        return parameters[0].device", "\n\n\nclass SequenceGeneratorModel(nn.Module):\n    \"\"\"\n    \u7528\u4e8e\u5c01\u88c5Seq2SeqModel\u4f7f\u5176\u53ef\u4ee5\u505a\u751f\u6210\u4efb\u52a1\n\n    \"\"\"\n    def __init__(self,\n                 seq2seq_model: Seq2SeqModel,\n                 bos_token_id,\n                 eos_token_id=None,\n                 max_length=30,\n                 max_len_a=0.0,\n                 num_beams=1,\n                 do_sample=True,\n                 sc_only=False,\n                 repetition_penalty=1,\n                 length_penalty=1.0,\n                 pad_token_id=0,\n                 restricter=None):\n        \"\"\"\n\n        :param Seq2SeqModel seq2seq_model: \u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b. \u4f1a\u4f7f\u7528seq2seq_model\u7684decoder\u8fdb\u884c\u751f\u6210\n        :param int,None bos_token_id: \u53e5\u5b50\u5f00\u5934\u7684token id\n        :param int,None eos_token_id: \u53e5\u5b50\u7ed3\u675f\u7684token id\n        :param int max_length: \u751f\u6210\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6, \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\n        :param float max_len_a: \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\u3002 \u5982\u679c\u4e0d\u4e3a0\uff0c\u9700\u8981\u4fdd\u8bc1State\u4e2d\u5305\u542bencoder_mask\n        :param int num_beams: beam search\u7684\u5927\u5c0f\n        :param bool do_sample: \u662f\u5426\u901a\u8fc7\u91c7\u6837\u7684\u65b9\u5f0f\u751f\u6210\n        :param float temperature: \u53ea\u6709\u5728do_sample\u4e3aTrue\u624d\u6709\u610f\u4e49\n        :param int top_k: \u53ea\u4ecetop_k\u4e2d\u91c7\u6837\n        :param float top_p: \u53ea\u4ecetop_p\u7684token\u4e2d\u91c7\u6837\uff0cnucles sample\n        :param float repetition_penalty: \u591a\u5927\u7a0b\u5ea6\u4e0a\u60e9\u7f5a\u91cd\u590d\u7684token\n        :param float length_penalty: \u5bf9\u957f\u5ea6\u7684\u60e9\u7f5a\uff0c\u5c0f\u4e8e1\u9f13\u52b1\u957f\u53e5\uff0c\u5927\u4e8e1\u9f13\u52b1\u77ed\u5267\n        :param int pad_token_id: \u5f53\u67d0\u53e5\u8bdd\u751f\u6210\u7ed3\u675f\u4e4b\u540e\uff0c\u4e4b\u540e\u751f\u6210\u7684\u5185\u5bb9\u7528pad_token_id\u8865\u5145\n        \"\"\"\n        super().__init__()\n        self.seq2seq_model = seq2seq_model\n        self.restricter = restricter\n        self.sc_only = sc_only\n        self.generator = SequenceGenerator(\n            seq2seq_model.decoder,\n            max_length=max_length,\n            max_len_a=max_len_a,\n            num_beams=num_beams,\n            do_sample=do_sample,\n            sc_only=sc_only,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            repetition_penalty=repetition_penalty,\n            length_penalty=length_penalty,\n            pad_token_id=pad_token_id,\n            restricter=restricter)\n\n    def forward(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                aesc_infos=None,\n                aspects_num=None,\n                first=None):\n        \"\"\"\n        \u900f\u4f20\u8c03\u7528seq2seq_model\u7684forward\n\n        :param torch.LongTensor src_tokens: bsz x max_len\n        :param torch.LongTensor tgt_tokens: bsz x max_len'\n        :param torch.LongTensor src_seq_len: bsz\n        :param torch.LongTensor tgt_seq_len: bsz\n        :return:\n        \"\"\"\n        \n        return self.seq2seq_model(input_ids=input_ids,\n                                  image_features=image_features,\n                                  attention_mask=attention_mask,\n                                  aesc_infos=aesc_infos,\n                                  aspects_num=aspects_num)\n\n    def predict(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                aesc_infos=None,\n                aspects_num=None):\n        \"\"\"\n        \u7ed9\u5b9asource\u7684\u5185\u5bb9\uff0c\u8f93\u51fagenerate\u7684\u5185\u5bb9\n\n        :param torch.LongTensor src_tokens: bsz x max_len\n        :param torch.LongTensor src_seq_len: bsz\n        :return:\n        \"\"\"\n        # import ipdb; ipdb.set_trace()\n        state, aspects_num_loss, predict_aspects_num = self.seq2seq_model.prepare_state(\n                                                 input_ids, image_features,\n                                                 attention_mask,\n                                                 aesc_infos,\n                                                 aspects_num)\n        tgt_tokens = aesc_infos['labels'].to(input_ids.device)\n        # print()\n        result = self.generator.generate(\n            state,\n            tokens=tgt_tokens[:, :3])  # the prompt is provided to the model\n        return result, predict_aspects_num", "\n\nr\"\"\"\n\n\"\"\"\n\n__all__ = ['SequenceGenerator']\n\n\nclass SequenceGenerator:\n    \"\"\"\n    \u7ed9\u5b9a\u4e00\u4e2aSeq2SeqDecoder\uff0cdecode\u51fa\u53e5\u5b50\n\n    \"\"\"\n    def __init__(self,\n                 decoder: Seq2SeqDecoder,\n                 max_length=20,\n                 max_len_a=0.0,\n                 num_beams=1,\n                 do_sample=False,\n                 sc_only=False,\n                 bos_token_id=None,\n                 eos_token_id=None,\n                 repetition_penalty=1,\n                 length_penalty=1.0,\n                 pad_token_id=0,\n                 restricter=None):\n        \"\"\"\n\n        :param Seq2SeqDecoder decoder: Decoder\u5bf9\u8c61\n        :param int max_length: \u751f\u6210\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6, \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\n        :param float max_len_a: \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\u3002 \u5982\u679c\u4e0d\u4e3a0\uff0c\u9700\u8981\u4fdd\u8bc1State\u4e2d\u5305\u542bencoder_mask\n        :param int num_beams: beam search\u7684\u5927\u5c0f\n        :param bool do_sample: \u662f\u5426\u901a\u8fc7\u91c7\u6837\u7684\u65b9\u5f0f\u751f\u6210\n        :param float temperature: \u53ea\u6709\u5728do_sample\u4e3aTrue\u624d\u6709\u610f\u4e49\n        :param int top_k: \u53ea\u4ecetop_k\u4e2d\u91c7\u6837\n        :param float top_p: \u53ea\u4ecetop_p\u7684token\u4e2d\u91c7\u6837\uff0cnucles sample\n        :param int,None bos_token_id: \u53e5\u5b50\u5f00\u5934\u7684token id\n        :param int,None eos_token_id: \u53e5\u5b50\u7ed3\u675f\u7684token id\n        :param float repetition_penalty: \u591a\u5927\u7a0b\u5ea6\u4e0a\u60e9\u7f5a\u91cd\u590d\u7684token\n        :param float length_penalty: \u5bf9\u957f\u5ea6\u7684\u60e9\u7f5a\uff0c\u5c0f\u4e8e1\u9f13\u52b1\u957f\u53e5\uff0c\u5927\u4e8e1\u9f13\u52b1\u77ed\u5267\n        :param int pad_token_id: \u5f53\u67d0\u53e5\u8bdd\u751f\u6210\u7ed3\u675f\u4e4b\u540e\uff0c\u4e4b\u540e\u751f\u6210\u7684\u5185\u5bb9\u7528pad_token_id\u8865\u5145\n        \"\"\"\n        self.generate_func = partial(greedy_generate,\n                                     decoder=decoder,\n                                     max_length=max_length,\n                                     max_len_a=max_len_a,\n                                     num_beams=num_beams,\n                                     sc_only=sc_only,\n                                     bos_token_id=bos_token_id,\n                                     eos_token_id=eos_token_id,\n                                     repetition_penalty=repetition_penalty,\n                                     length_penalty=length_penalty,\n                                     pad_token_id=pad_token_id,\n                                     restricter=restricter)\n        self.do_sample = do_sample\n        self.max_length = max_length\n        self.num_beams = num_beams\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n        self.repetition_penalty = repetition_penalty\n        self.length_penalty = length_penalty\n        self.decoder = decoder\n        self.pad_token_id = pad_token_id\n        self.restricter = restricter\n        self.max_len_a = max_len_a\n        self.sc_only = sc_only\n\n    def set_new_generator(self,\n                          max_length=-1,\n                          max_len_a=-1,\n                          num_beams=-1,\n                          repetition_penalty=-1,\n                          length_penalty=-1,\n                          restricter=-1):\n        if max_length == -1:\n            max_length = self.max_length\n        if max_len_a == -1:\n            max_len_a = self.max_len_a\n        if num_beams == -1:\n            num_beams = self.num_beams\n        if repetition_penalty == -1:\n            repetition_penalty = self.repetition_penalty\n        if length_penalty == -1:\n            length_penalty = self.length_penalty\n        if restricter == -1:\n            restricter = self.restricter\n        self.generate_func = partial(greedy_generate,\n                                     decoder=self.decoder,\n                                     max_length=max_length,\n                                     max_len_a=max_len_a,\n                                     num_beams=num_beams,\n                                     sc_only=self.sc_only,\n                                     bos_token_id=self.bos_token_id,\n                                     eos_token_id=self.eos_token_id,\n                                     repetition_penalty=repetition_penalty,\n                                     length_penalty=length_penalty,\n                                     pad_token_id=self.pad_token_id,\n                                     restricter=restricter)\n\n    @torch.no_grad()\n    def generate(self, state, tokens=None, gt_tokens=None):\n        \"\"\"\n\n        :param State state: encoder\u7ed3\u679c\u7684State, \u662f\u4e0eDecoder\u914d\u5957\u662f\u7528\u7684\n        :param torch.LongTensor,None tokens: batch_size x length, \u5f00\u59cb\u7684token\n        :return: bsz x max_length' \u751f\u6210\u7684token\u5e8f\u5217\u3002\u5982\u679ceos_token_id\u4e0d\u4e3aNone, \u6bcf\u4e2asequence\u7684\u7ed3\u5c3e\u4e00\u5b9a\u662feos_token_id\n        \"\"\"\n\n        return self.generate_func(tokens=tokens,\n                                  gt_tokens=gt_tokens,\n                                  state=state)", "\nclass SequenceGenerator:\n    \"\"\"\n    \u7ed9\u5b9a\u4e00\u4e2aSeq2SeqDecoder\uff0cdecode\u51fa\u53e5\u5b50\n\n    \"\"\"\n    def __init__(self,\n                 decoder: Seq2SeqDecoder,\n                 max_length=20,\n                 max_len_a=0.0,\n                 num_beams=1,\n                 do_sample=False,\n                 sc_only=False,\n                 bos_token_id=None,\n                 eos_token_id=None,\n                 repetition_penalty=1,\n                 length_penalty=1.0,\n                 pad_token_id=0,\n                 restricter=None):\n        \"\"\"\n\n        :param Seq2SeqDecoder decoder: Decoder\u5bf9\u8c61\n        :param int max_length: \u751f\u6210\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6, \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\n        :param float max_len_a: \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\u3002 \u5982\u679c\u4e0d\u4e3a0\uff0c\u9700\u8981\u4fdd\u8bc1State\u4e2d\u5305\u542bencoder_mask\n        :param int num_beams: beam search\u7684\u5927\u5c0f\n        :param bool do_sample: \u662f\u5426\u901a\u8fc7\u91c7\u6837\u7684\u65b9\u5f0f\u751f\u6210\n        :param float temperature: \u53ea\u6709\u5728do_sample\u4e3aTrue\u624d\u6709\u610f\u4e49\n        :param int top_k: \u53ea\u4ecetop_k\u4e2d\u91c7\u6837\n        :param float top_p: \u53ea\u4ecetop_p\u7684token\u4e2d\u91c7\u6837\uff0cnucles sample\n        :param int,None bos_token_id: \u53e5\u5b50\u5f00\u5934\u7684token id\n        :param int,None eos_token_id: \u53e5\u5b50\u7ed3\u675f\u7684token id\n        :param float repetition_penalty: \u591a\u5927\u7a0b\u5ea6\u4e0a\u60e9\u7f5a\u91cd\u590d\u7684token\n        :param float length_penalty: \u5bf9\u957f\u5ea6\u7684\u60e9\u7f5a\uff0c\u5c0f\u4e8e1\u9f13\u52b1\u957f\u53e5\uff0c\u5927\u4e8e1\u9f13\u52b1\u77ed\u5267\n        :param int pad_token_id: \u5f53\u67d0\u53e5\u8bdd\u751f\u6210\u7ed3\u675f\u4e4b\u540e\uff0c\u4e4b\u540e\u751f\u6210\u7684\u5185\u5bb9\u7528pad_token_id\u8865\u5145\n        \"\"\"\n        self.generate_func = partial(greedy_generate,\n                                     decoder=decoder,\n                                     max_length=max_length,\n                                     max_len_a=max_len_a,\n                                     num_beams=num_beams,\n                                     sc_only=sc_only,\n                                     bos_token_id=bos_token_id,\n                                     eos_token_id=eos_token_id,\n                                     repetition_penalty=repetition_penalty,\n                                     length_penalty=length_penalty,\n                                     pad_token_id=pad_token_id,\n                                     restricter=restricter)\n        self.do_sample = do_sample\n        self.max_length = max_length\n        self.num_beams = num_beams\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n        self.repetition_penalty = repetition_penalty\n        self.length_penalty = length_penalty\n        self.decoder = decoder\n        self.pad_token_id = pad_token_id\n        self.restricter = restricter\n        self.max_len_a = max_len_a\n        self.sc_only = sc_only\n\n    def set_new_generator(self,\n                          max_length=-1,\n                          max_len_a=-1,\n                          num_beams=-1,\n                          repetition_penalty=-1,\n                          length_penalty=-1,\n                          restricter=-1):\n        if max_length == -1:\n            max_length = self.max_length\n        if max_len_a == -1:\n            max_len_a = self.max_len_a\n        if num_beams == -1:\n            num_beams = self.num_beams\n        if repetition_penalty == -1:\n            repetition_penalty = self.repetition_penalty\n        if length_penalty == -1:\n            length_penalty = self.length_penalty\n        if restricter == -1:\n            restricter = self.restricter\n        self.generate_func = partial(greedy_generate,\n                                     decoder=self.decoder,\n                                     max_length=max_length,\n                                     max_len_a=max_len_a,\n                                     num_beams=num_beams,\n                                     sc_only=self.sc_only,\n                                     bos_token_id=self.bos_token_id,\n                                     eos_token_id=self.eos_token_id,\n                                     repetition_penalty=repetition_penalty,\n                                     length_penalty=length_penalty,\n                                     pad_token_id=self.pad_token_id,\n                                     restricter=restricter)\n\n    @torch.no_grad()\n    def generate(self, state, tokens=None, gt_tokens=None):\n        \"\"\"\n\n        :param State state: encoder\u7ed3\u679c\u7684State, \u662f\u4e0eDecoder\u914d\u5957\u662f\u7528\u7684\n        :param torch.LongTensor,None tokens: batch_size x length, \u5f00\u59cb\u7684token\n        :return: bsz x max_length' \u751f\u6210\u7684token\u5e8f\u5217\u3002\u5982\u679ceos_token_id\u4e0d\u4e3aNone, \u6bcf\u4e2asequence\u7684\u7ed3\u5c3e\u4e00\u5b9a\u662feos_token_id\n        \"\"\"\n\n        return self.generate_func(tokens=tokens,\n                                  gt_tokens=gt_tokens,\n                                  state=state)", "\n\n@torch.no_grad()\ndef greedy_generate(decoder,\n                    tokens=None,\n                    gt_tokens=None,\n                    state=None,\n                    sc_eval=False,\n                    max_length=20,\n                    max_len_a=0.0,\n                    num_beams=1,\n                    sc_only=False,\n                    bos_token_id=None,\n                    eos_token_id=None,\n                    pad_token_id=0,\n                    repetition_penalty=1,\n                    length_penalty=1.0,\n                    restricter=None):\n    \"\"\"\n    \u8d2a\u5a6a\u5730\u641c\u7d22\u53e5\u5b50\n\n    :param Decoder decoder: Decoder\u5bf9\u8c61\n    :param torch.LongTensor tokens: batch_size x len, decode\u7684\u8f93\u5165\u503c\uff0c\u5982\u679c\u4e3aNone\uff0c\u5219\u81ea\u52a8\u4ecebos_token_id\u5f00\u59cb\u751f\u6210\n    :param State state: \u5e94\u8be5\u5305\u542bencoder\u7684\u4e00\u4e9b\u8f93\u51fa\u3002\n    :param int max_length: \u751f\u6210\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6, \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\n    :param float max_len_a: \u6bcf\u53e5\u8bdd\u7684decode\u957f\u5ea6\u4e3amax_length + max_len_a*src_len\u3002 \u5982\u679c\u4e0d\u4e3a0\uff0c\u9700\u8981\u4fdd\u8bc1State\u4e2d\u5305\u542bencoder_mask\n    :param int num_beams: \u4f7f\u7528\u591a\u5927\u7684beam\u8fdb\u884c\u89e3\u7801\u3002\n    :param int bos_token_id: \u5982\u679ctokens\u4f20\u5165\u4e3aNone\uff0c\u5219\u4f7f\u7528bos_token_id\u5f00\u59cb\u5f80\u540e\u89e3\u7801\u3002\n    :param int eos_token_id: \u7ed3\u675f\u7684token\uff0c\u5982\u679c\u4e3aNone\uff0c\u5219\u4e00\u5b9a\u4f1a\u89e3\u7801\u5230max_length\u8fd9\u4e48\u957f\u3002\n    :param int pad_token_id: pad\u7684token id\n    :param float repetition_penalty: \u5bf9\u91cd\u590d\u51fa\u73b0\u7684token\u591a\u5927\u7684\u60e9\u7f5a\u3002\n    :param float length_penalty: \u5bf9\u6bcf\u4e2atoken\uff08\u9664\u4e86eos\uff09\u6309\u7167\u957f\u5ea6\u8fdb\u884c\u4e00\u5b9a\u7684\u60e9\u7f5a\u3002\n    :return:\n    \"\"\"\n\n    # import ipdb; ipdb.set_trace()\n    if sc_only:\n        token_ids = sc_generate(decoder,\n                                tokens=tokens,\n                                gt_tokens=gt_tokens,\n                                state=state,\n                                max_length=max_length,\n                                max_len_a=max_len_a,\n                                bos_token_id=bos_token_id,\n                                eos_token_id=eos_token_id,\n                                repetition_penalty=repetition_penalty,\n                                length_penalty=length_penalty,\n                                pad_token_id=pad_token_id,\n                                restricter=restricter)\n        return token_ids\n    if num_beams == 1:\n        token_ids = _no_beam_search_generate(\n            decoder,\n            tokens=tokens,\n            state=state,\n            max_length=max_length,\n            max_len_a=max_len_a,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            repetition_penalty=repetition_penalty,\n            length_penalty=length_penalty,\n            pad_token_id=pad_token_id,\n            restricter=restricter)\n    else:\n        token_ids = _beam_search_generate(\n            decoder,\n            tokens=tokens,\n            state=state,\n            max_length=max_length,\n            max_len_a=max_len_a,\n            num_beams=num_beams,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            do_sample=False,\n            repetition_penalty=repetition_penalty,\n            length_penalty=length_penalty,\n            pad_token_id=pad_token_id,\n            restricter=restricter)\n\n    return token_ids", "\n\ndef _no_beam_search_generate(decoder: Seq2SeqDecoder,\n                             state,\n                             tokens=None,\n                             max_length=20,\n                             max_len_a=0.0,\n                             bos_token_id=None,\n                             eos_token_id=None,\n                             repetition_penalty=1.0,\n                             length_penalty=1.0,\n                             pad_token_id=0,\n                             restricter=None):\n    device = _get_model_device(decoder)\n    if tokens is None:\n        if bos_token_id is None:\n            raise RuntimeError(\n                \"You have to specify either `tokens` or `bos_token_id`.\")\n        batch_size = state.num_samples\n        if batch_size is None:\n            raise RuntimeError(\n                \"Cannot infer the number of samples from `state`.\")\n        tokens = torch.full([batch_size, 1],\n                            fill_value=bos_token_id,\n                            dtype=torch.long).to(device)\n    batch_size = tokens.size(0)\n    if state.num_samples:\n        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n\n    if eos_token_id is None:\n        _eos_token_id = -1\n    else:\n        _eos_token_id = eos_token_id\n\n    scores = decoder.decode(tokens=tokens, state=state)  # \u4e3b\u8981\u662f\u4e3a\u4e86update state\n    # \u8fd9\u91cc\u9700\u8981\u8003\u8651\u5982\u679c\u5728\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u5c31\u7ed3\u675f\u7684\u60c5\u51b5\n    # if _eos_token_id!=-1:\n    #     scores[:, _eos_token_id] = -1e12\n\n    if restricter is not None:\n        _, next_tokens = restricter(state, tokens, scores, num_beams=1)\n    else:\n        next_tokens = scores.argmax(dim=-1, keepdim=True)\n    token_ids = torch.cat([tokens, next_tokens], dim=1)\n    cur_len = token_ids.size(1)\n    dones = token_ids.new_zeros(batch_size).eq(1).__or__(\n        next_tokens.squeeze(1).eq(eos_token_id))\n    # tokens = tokens[:, -1:]\n\n    if max_len_a != 0:\n        # (bsz x num_beams, )\n        if state.encoder_mask is not None:\n            max_lengths = (state.encoder_mask.sum(dim=1).float() *\n                           max_len_a).long() + max_length\n        else:\n            max_lengths = tokens.new_full((tokens.size(0), ),\n                                          fill_value=max_length,\n                                          dtype=torch.long)\n        real_max_length = max_lengths.max().item()\n    else:\n        real_max_length = max_length\n        if state.encoder_mask is not None:\n            max_lengths = state.encoder_mask.new_ones(\n                state.encoder_mask.size(0)).long() * max_length\n        else:\n            max_lengths = tokens.new_full((tokens.size(0), ),\n                                          fill_value=max_length,\n                                          dtype=torch.long)\n\n    while cur_len < real_max_length:\n        scores = decoder.decode(tokens=token_ids,\n                                state=state)  # batch_size x vocab_size\n\n        if repetition_penalty != 1.0:\n            token_scores = scores.gather(dim=1, index=token_ids)\n            lt_zero_mask = token_scores.lt(0).float()\n            ge_zero_mask = lt_zero_mask.eq(0).float()\n            token_scores = lt_zero_mask * repetition_penalty * token_scores + ge_zero_mask / repetition_penalty * token_scores\n            scores.scatter_(dim=1, index=token_ids, src=token_scores)\n\n        if eos_token_id is not None and length_penalty != 1.0:\n            token_scores = scores / cur_len**length_penalty  # batch_size x vocab_size\n            eos_mask = scores.new_ones(scores.size(1))\n            eos_mask[eos_token_id] = 0\n            eos_mask = eos_mask.unsqueeze(0).eq(1)\n            scores = scores.masked_scatter(\n                eos_mask, token_scores)  # \u4e5f\u5373\u9664\u4e86eos\uff0c\u5176\u4ed6\u8bcd\u7684\u5206\u6570\u7ecf\u8fc7\u4e86\u653e\u5927/\u7f29\u5c0f\n\n        if restricter is not None:\n            _, next_tokens = restricter(state, token_ids, scores, 1)\n        else:\n            next_tokens = scores.argmax(dim=-1, keepdim=True)\n        next_tokens = next_tokens.squeeze(-1)\n\n        # \u5982\u679c\u5df2\u7ecf\u8fbe\u5230\u5bf9\u5e94\u7684sequence\u957f\u5ea6\u4e86\uff0c\u5c31\u76f4\u63a5\u586b\u4e3aeos\u4e86\n        if _eos_token_id != -1:\n            next_tokens = next_tokens.masked_fill(max_lengths.eq(cur_len + 1),\n                                                  _eos_token_id)\n        next_tokens = next_tokens.masked_fill(\n            dones, pad_token_id)  # \u5bf9\u5df2\u7ecf\u641c\u7d22\u5b8c\u6210\u7684sample\u505apadding\n        tokens = next_tokens.unsqueeze(1)\n\n        token_ids = torch.cat([token_ids, tokens],\n                              dim=-1)  # batch_size x max_len\n\n        end_mask = next_tokens.eq(_eos_token_id)\n        dones = dones.__or__(end_mask)\n        cur_len += 1\n\n        if dones.min() == 1:\n            break\n\n    return token_ids", "\n\ndef sc_generate(decoder: Seq2SeqDecoder,\n                state,\n                tokens=None,\n                gt_tokens=None,\n                max_length=20,\n                max_len_a=0.0,\n                bos_token_id=None,\n                eos_token_id=None,\n                repetition_penalty=1.0,\n                length_penalty=1.0,\n                pad_token_id=0,\n                restricter=None):\n    device = _get_model_device(decoder)\n    if tokens is None:\n        if bos_token_id is None:\n            raise RuntimeError(\n                \"You have to specify either `tokens` or `bos_token_id`.\")\n        batch_size = state.num_samples\n        if batch_size is None:\n            raise RuntimeError(\n                \"Cannot infer the number of samples from `state`.\")\n        tokens = torch.full([batch_size, 1],\n                            fill_value=bos_token_id,\n                            dtype=torch.long).to(device)\n    batch_size = tokens.size(0)\n    # print(state.num_samples, batch_size)\n    if state.num_samples:\n        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n\n    if eos_token_id is None:\n        _eos_token_id = -1\n    else:\n        _eos_token_id = eos_token_id\n    aspect_cnt = 3\n    next_tokens = gt_tokens[:, aspect_cnt:aspect_cnt + 2]\n    token_ids = torch.cat([tokens, next_tokens], dim=1)\n    cur_len = token_ids.size(1)\n    dones = token_ids.new_zeros(batch_size).eq(1)\n    # tokens = tokens[:, -1:]\n    max_len_a = 0\n    max_length = gt_tokens.size(1)\n    gt_mask = gt_tokens.eq(1).eq(0)\n    max_lengths = gt_mask.sum(dim=1)\n\n    while cur_len < max_length:\n        scores = decoder.decode(tokens=token_ids, state=state,\n                                only_sc=True)  # batch_size x vocab_size\n        if restricter is not None:\n            _, next_tokens = restricter(state, token_ids, scores, 1)\n        else:\n            next_tokens = scores.argmax(dim=-1, keepdim=True)\n        next_tokens = next_tokens.squeeze(-1)\n\n        # \u5982\u679c\u5df2\u7ecf\u8fbe\u5230\u5bf9\u5e94\u7684sequence\u957f\u5ea6\u4e86\uff0c\u5c31\u76f4\u63a5\u586b\u4e3aeos\u4e86\n        # if _eos_token_id != -1:\n        #     next_tokens = next_tokens.masked_fill(max_lengths.eq(cur_len + 1),\n        #                                           _eos_token_id)\n        next_tokens = next_tokens.masked_fill(\n            dones, pad_token_id)  # \u5bf9\u5df2\u7ecf\u641c\u7d22\u5b8c\u6210\u7684sample\u505apadding\n        tokens = next_tokens.unsqueeze(1)\n\n        token_ids = torch.cat([token_ids, tokens],\n                              dim=-1)  # batch_size x max_len\n\n        # end_mask = next_tokens.eq(_eos_token_id)\n        # dones = dones.__or__(end_mask)\n        dones = gt_tokens[:, cur_len + 1].eq(1)\n        cur_len += 1\n        aspect_cnt += 3\n        if aspect_cnt + 2 < max_length:\n            token_ids = torch.cat(\n                [token_ids, gt_tokens[:, aspect_cnt:aspect_cnt + 2]], dim=-1)\n        cur_len += 2\n\n        if dones.min() == 1:\n            break\n    ones = token_ids.new_ones(batch_size).unsqueeze(-1)\n    token_ids = torch.cat([token_ids, ones], dim=-1)\n    # if eos_token_id is not None:\n    #     tokens.scatter(index=max_lengths[:, None], dim=1, value=eos_token_id)  # \u5c06\u6700\u5927\u957f\u5ea6\u4f4d\u7f6e\u8bbe\u7f6e\u4e3aeos\n    # if cur_len == max_length:\n    #     token_ids[:, -1].masked_fill_(~dones, eos_token_id)  # \u82e5\u5230\u6700\u957f\u957f\u5ea6\u4ecd\u672a\u5230EOS\uff0c\u5219\u5f3a\u5236\u5c06\u6700\u540e\u4e00\u4e2a\u8bcd\u66ff\u6362\u6210eos\n\n    return token_ids", "\n\ndef _beam_search_generate(decoder: Seq2SeqDecoder,\n                          tokens=None,\n                          state=None,\n                          max_length=20,\n                          max_len_a=0.0,\n                          num_beams=4,\n                          bos_token_id=None,\n                          eos_token_id=None,\n                          do_sample=True,\n                          repetition_penalty=1.0,\n                          length_penalty=None,\n                          pad_token_id=0,\n                          restricter=None) -> torch.LongTensor:\n    assert do_sample is False\n    # \u8fdb\u884cbeam search\n    # import ipdb; ipdb.set_trace()\n    device = _get_model_device(decoder)\n    if tokens is None:\n        if bos_token_id is None:\n            raise RuntimeError(\n                \"You have to specify either `tokens` or `bos_token_id`.\")\n        batch_size = state.num_samples\n        if batch_size is None:\n            raise RuntimeError(\n                \"Cannot infer the number of samples from `state`.\")\n        tokens = torch.full([batch_size, 1],\n                            fill_value=bos_token_id,\n                            dtype=torch.long).to(device)\n    batch_size = tokens.size(0)\n    if state.num_samples:\n        assert state.num_samples == batch_size, \"The number of samples in `tokens` and `state` should match.\"\n\n    if eos_token_id is None:\n        _eos_token_id = -1\n    else:\n        _eos_token_id = eos_token_id\n\n    scores = decoder.decode(tokens=tokens, state=state)  # \u8fd9\u91cc\u8981\u4f20\u5165\u7684\u662f\u6574\u4e2a\u53e5\u5b50\u7684\u957f\u5ea6\n    # \u8fd9\u91cc\u9700\u8981\u8003\u8651\u5982\u679c\u5728\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u5c31\u7ed3\u675f\u7684\u60c5\u51b5\n    # if _eos_token_id!=-1:\n    #     scores[:, _eos_token_id] = -1e12\n    vocab_size = scores.size(1)\n    assert vocab_size >= num_beams, \"num_beams should be smaller than the number of vocabulary size.\"\n\n    scores = F.log_softmax(scores, dim=-1)  # (batch_size, vocab_size)\n    # \u5f97\u5230(batch_size, num_beams), (batch_size, num_beams)\n    # TODO \u628a\u9650\u5236\u5199\u5230\u8fd9\u4e2a\u4f4d\u7f6e, \u52a01\u662f\u56e0\u4e3a\u9700\u8981\u8003\u8651\u8f93\u51fa\u5c31\u662feos\u7684\u60c5\u51b5\n    if restricter is not None:\n        _next_scores, _next_tokens = restricter(state, tokens, scores,\n                                                num_beams + 1)\n    else:\n        # \u662fbsz x (num_beams+1)\u5927\u5c0f\u7684\u4e1c\u897f\n        _next_scores, _next_tokens = torch.topk(scores,\n                                                num_beams + 1,\n                                                dim=1,\n                                                largest=True,\n                                                sorted=True)\n\n    # \u6839\u636eindex\u6765\u505a\u987a\u5e8f\u7684\u8c03\u8f6c\n    indices = torch.arange(batch_size, dtype=torch.long).to(device)\n    indices = indices.repeat_interleave(num_beams)\n    state.reorder_state(indices)\n    tokens = tokens.index_select(\n        dim=0, index=indices)  # batch_size * num_beams x length\n\n    if max_len_a != 0:\n        # (bsz x num_beams, )\n        if state.encoder_mask is not None:\n            max_lengths = (state.encoder_mask.sum(dim=1).float() *\n                           max_len_a).long() + max_length\n        else:\n            max_lengths = tokens.new_full((batch_size * num_beams, ),\n                                          fill_value=max_length,\n                                          dtype=torch.long)\n        real_max_length = max_lengths.max().item()\n    else:\n        real_max_length = max_length\n        if state.encoder_mask is not None:\n            max_lengths = state.encoder_mask.new_ones(\n                state.encoder_mask.size(0)).long() * max_length\n        else:\n            max_lengths = tokens.new_full((batch_size * num_beams, ),\n                                          fill_value=max_length,\n                                          dtype=torch.long)\n    hypos = [\n        BeamHypotheses(num_beams,\n                       real_max_length,\n                       length_penalty,\n                       early_stopping=False) for _ in range(batch_size)\n    ]\n\n    not_eos_mask = _next_tokens.ne(_eos_token_id)  # \u4e3a1\u7684\u5730\u65b9\u4e0d\u662feos\n    keep_mask = not_eos_mask.cumsum(dim=1).le(num_beams)  # \u4e3a1\u7684\u5730\u65b9\u9700\u8981\u4fdd\u7559\n    keep_mask = not_eos_mask.__and__(keep_mask)  # \u4e3a1\u7684\u5730\u65b9\u662f\u9700\u8981\u8fdb\u884c\u4e0b\u4e00\u6b65search\u7684\n\n    next_tokens = _next_tokens.masked_select(keep_mask).view(\n        batch_size, num_beams)  # \u8fd9\u662f\u771f\u7684\u63a5\u4e0b\u6765\u8981\u7ee7\u7eed\u7684\n    next_scores = _next_scores.masked_select(keep_mask).view(\n        batch_size, num_beams)\n\n    rows, cols = not_eos_mask.eq(0)[:, :num_beams].nonzero(as_tuple=True)\n\n    if len(rows) > 0:  # \u8bf4\u660e\u6709\u7684\u5f00\u5934\u5c31\u7ed3\u675f\u4e86\n        for row, col in zip(rows.tolist(), cols.tolist()):\n            _token = torch.cat(\n                [tokens[row * num_beams], _next_tokens[row, col:col + 1]],\n                dim=0)\n            hypos[row].add(_token.clone(), _next_scores[row, col].item())\n\n    # \u8bb0\u5f55\u751f\u6210\u597d\u7684token (batch_size', cur_len)\n    token_ids = torch.cat([tokens, next_tokens.view(-1, 1)], dim=-1)\n    dones = [False] * batch_size\n\n    beam_scores = next_scores.view(-1)  # batch_size * num_beams\n\n    #  \u7528\u6765\u8bb0\u5f55\u5df2\u7ecf\u751f\u6210\u597d\u7684token\u7684\u957f\u5ea6\n    cur_len = token_ids.size(1)\n\n    # 0, num_beams, 2*num_beams, ...\n    batch_inds_with_numbeams_interval = (torch.arange(batch_size) *\n                                         num_beams).view(-1, 1).to(token_ids)\n\n    while cur_len < real_max_length:\n        scores = decoder.decode(token_ids,\n                                state)  # (bsz x num_beams, vocab_size)\n        if repetition_penalty != 1.0:\n            token_scores = scores.gather(dim=1, index=token_ids)\n            lt_zero_mask = token_scores.lt(0).float()\n            ge_zero_mask = lt_zero_mask.eq(0).float()\n            token_scores = lt_zero_mask * repetition_penalty * token_scores + ge_zero_mask / repetition_penalty * token_scores\n            scores.scatter_(dim=1, index=token_ids, src=token_scores)\n\n        if _eos_token_id != -1:\n            max_len_eos_mask = max_lengths.eq(cur_len + 1)\n            eos_scores = scores[:, _eos_token_id]\n            # \u5982\u679c\u5df2\u7ecf\u8fbe\u5230\u6700\u5927\u957f\u5ea6\uff0c\u5c31\u628aeos\u7684\u5206\u6570\u52a0\u5927\n            scores[:, _eos_token_id] = torch.where(max_len_eos_mask,\n                                                   eos_scores + 1e32,\n                                                   eos_scores)\n\n        scores = F.log_softmax(scores,\n                               dim=-1)  # (batch_size * num_beams, vocab_size)\n        _scores = scores + beam_scores[:,\n                                       None]  # (batch_size * num_beams, vocab_size)\n        _scores = _scores.view(batch_size,\n                               -1)  # (batch_size, num_beams*vocab_size)\n        # TODO \u628a\u9650\u5236\u52a0\u5230\u8fd9\u4e2a\u4f4d\u7f6e\n        if restricter is not None:\n            next_scores, ids = restricter(state, token_ids, _scores,\n                                          2 * num_beams)\n        else:\n            next_scores, ids = torch.topk(_scores,\n                                          2 * num_beams,\n                                          dim=1,\n                                          largest=True,\n                                          sorted=True)  # (bsz, 2*num_beams)\n        from_which_beam = ids // vocab_size  # (batch_size, 2*num_beams)\n        next_tokens = ids % vocab_size  # (batch_size, 2*num_beams)\n\n        not_eos_mask = next_tokens.ne(_eos_token_id)  # \u4e3a1\u7684\u5730\u65b9\u4e0d\u662feos\n        keep_mask = not_eos_mask.cumsum(dim=1).le(num_beams)  # \u4e3a1\u7684\u5730\u65b9\u9700\u8981\u4fdd\u7559\n        keep_mask = not_eos_mask.__and__(keep_mask)  # \u4e3a1\u7684\u5730\u65b9\u662f\u9700\u8981\u8fdb\u884c\u4e0b\u4e00\u6b65search\u7684\n\n        _next_tokens = next_tokens.masked_select(keep_mask).view(-1, 1)\n        _from_which_beam = from_which_beam.masked_select(keep_mask).view(\n            batch_size, num_beams)  # \u4e0a\u9762\u7684token\u662f\u6765\u81ea\u54ea\u4e2abeam\n        _next_scores = next_scores.masked_select(keep_mask).view(\n            batch_size, num_beams)\n        beam_scores = _next_scores.view(-1)\n\n        flag = True\n        if cur_len + 1 == real_max_length:\n            eos_batch_idx = torch.arange(batch_size).to(\n                next_tokens).repeat_interleave(repeats=num_beams, dim=0)\n            eos_beam_ind = torch.arange(num_beams).to(token_ids).repeat(\n                batch_size)  # \u8868\u793a\u7684\u662findice\n            eos_beam_idx = from_which_beam[:, :num_beams].reshape(\n                -1)  # \u8868\u793a\u7684\u662f\u4ece\u54ea\u4e2abeam\u83b7\u53d6\u5f97\u5230\u7684\n        else:\n            # \u5c06\u6bcf\u4e2abatch\u4e2d\u5728num_beam\u5185\u7684\u5e8f\u5217\u6dfb\u52a0\u5230\u7ed3\u675f\u4e2d, \u4e3a1\u7684\u5730\u65b9\u9700\u8981\u7ed3\u675f\u4e86\n            effective_eos_mask = next_tokens[:, :num_beams].eq(\n                _eos_token_id)  # batch_size x num_beams\n            if effective_eos_mask.sum().gt(0):\n                eos_batch_idx, eos_beam_ind = effective_eos_mask.nonzero(\n                    as_tuple=True)\n                # \u662f\u7531\u4e8efrom_which_beam\u662f (batch_size, 2*num_beams)\u7684\uff0c\u6240\u4ee5\u9700\u89812*num_beams\n                eos_beam_idx = eos_batch_idx * num_beams * 2 + eos_beam_ind\n                eos_beam_idx = from_which_beam.view(-1)[\n                    eos_beam_idx]  # \u83b7\u53d6\u771f\u5b9e\u7684\u4ece\u54ea\u4e2abeam\u83b7\u53d6\u7684eos\n            else:\n                flag = False\n\n        if flag:\n            _token_ids = torch.cat([token_ids, _next_tokens], dim=-1)\n            for batch_idx, beam_ind, beam_idx in zip(eos_batch_idx.tolist(),\n                                                     eos_beam_ind.tolist(),\n                                                     eos_beam_idx.tolist()):\n                if not dones[batch_idx]:\n                    score = next_scores[batch_idx, beam_ind].item()\n                    # \u4e4b\u540e\u9700\u8981\u5728\u7ed3\u5c3e\u65b0\u589e\u4e00\u4e2aeos\n                    if _eos_token_id != -1:\n                        hypos[batch_idx].add(\n                            _token_ids[batch_idx * num_beams +\n                                       beam_idx, :cur_len].clone(), score)\n                    else:\n                        hypos[batch_idx].add(\n                            _token_ids[batch_idx * num_beams +\n                                       beam_idx].clone(), score)\n\n        # \u66f4\u6539state\u72b6\u6001, \u91cd\u7ec4token_ids\n        reorder_inds = (batch_inds_with_numbeams_interval +\n                        _from_which_beam).view(-1)  # flatten\u6210\u4e00\u7ef4\n        state.reorder_state(reorder_inds)\n        # \u91cd\u65b0\u7ec4\u7ec7token_ids\u7684\u72b6\u6001\n        token_ids = torch.cat(\n            [token_ids.index_select(index=reorder_inds, dim=0), _next_tokens],\n            dim=-1)\n\n        for batch_idx in range(batch_size):\n            dones[batch_idx] = dones[batch_idx] or hypos[batch_idx].is_done(next_scores[batch_idx, 0].item()) or \\\n                               max_lengths[batch_idx*num_beams]==cur_len+1\n\n        cur_len += 1\n\n        if all(dones):\n            break\n\n    # select the best hypotheses\n    tgt_len = token_ids.new_zeros(batch_size)\n    best = []\n\n    for i, hypotheses in enumerate(hypos):\n        best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n        # \u628a\u4e0a\u9762\u66ff\u6362\u4e3a\u975eeos\u7684\u8bcd\u66ff\u6362\u56deeos\n        if _eos_token_id != -1:\n            best_hyp = torch.cat(\n                [best_hyp, best_hyp.new_ones(1) * _eos_token_id])\n        tgt_len[i] = len(best_hyp)\n        best.append(best_hyp)\n\n    # generate target batch\n    decoded = token_ids.new_zeros(batch_size,\n                                  tgt_len.max().item()).fill_(pad_token_id)\n    for i, hypo in enumerate(best):\n        decoded[i, :tgt_len[i]] = hypo\n\n    return decoded", "\n\nclass BeamHypotheses(object):\n    def __init__(self, num_beams, max_length, length_penalty, early_stopping):\n        \"\"\"\n        Initialize n-best list of hypotheses.\n        \"\"\"\n        self.max_length = max_length - 1  # ignoring bos_token\n        self.length_penalty = length_penalty\n        self.early_stopping = early_stopping\n        self.num_beams = num_beams\n        self.hyp = []\n        self.worst_score = 1e9\n\n    def __len__(self):\n        \"\"\"\n        Number of hypotheses in the list.\n        \"\"\"\n        return len(self.hyp)\n\n    def add(self, hyp, sum_logprobs):\n        \"\"\"\n        Add a new hypothesis to the list.\n        \"\"\"\n        score = sum_logprobs / len(hyp)**self.length_penalty\n        if len(self) < self.num_beams or score > self.worst_score:\n            self.hyp.append((score, hyp))\n            if len(self) > self.num_beams:\n                sorted_scores = sorted([\n                    (s, idx) for idx, (s, _) in enumerate(self.hyp)\n                ])\n                del self.hyp[sorted_scores[0][1]]\n                self.worst_score = sorted_scores[1][0]\n            else:\n                self.worst_score = min(score, self.worst_score)\n\n    def is_done(self, best_sum_logprobs):\n        \"\"\"\n        If there are enough hypotheses and that none of the hypotheses being generated\n        can become better than the worst one in the heap, then we are done with this sentence.\n        \"\"\"\n        if len(self) < self.num_beams:\n            return False\n        elif self.early_stopping:\n            return True\n        else:\n            return self.worst_score >= best_sum_logprobs / self.max_length**self.length_penalty", ""]}
{"filename": "src/model/utils.py", "chunked_list": ["from transformers import top_k_top_p_filtering\nimport torch\nfrom torch.nn import functional as F\n\n\ndef set_lr(optimizer, lr):\n    for group in optimizer.param_groups:\n        group['lr'] = lr\n\n\ndef get_lr(optimizer):\n    for group in optimizer.param_groups:\n        return group['lr']", "\n\ndef get_lr(optimizer):\n    for group in optimizer.param_groups:\n        return group['lr']\n\n\ndef clip_gradient(optimizer, grad_clip):\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            # print(param.shape)\n            if param.grad == None:\n                continue\n            param.grad.data.clamp_(-grad_clip, grad_clip)", "\n\ndef liner_warmup(cur_step, t_step, warmup):\n    progress = cur_step / t_step\n    if progress < warmup:\n        return progress / warmup\n    return max((progress - 1.) / (warmup - 1.), 0.)\n\n\ndef sample_sentence(model,\n                    input_ids,\n                    image_features,\n                    attention_mask,\n                    tokenizer,\n                    top_k=50,\n                    top_p=1.0,\n                    max_length=20):\n    batch_size = input_ids.shape[0]\n    encoder = model.get_encoder()\n    encoder_outputs = encoder(input_ids,\n                              image_features,\n                              attention_mask=attention_mask)\n\n    unfinished_sents = input_ids.new(batch_size).fill_(1)\n    sent_lengths = input_ids.new(batch_size).fill_(max_length)\n    logprobs = []\n\n    decoder_input_ids = input_ids.new(batch_size,\n                                      1).fill_(tokenizer.bos_token_id)\n\n    cur_len = 1\n    while cur_len < max_length:\n        model_inputs = {\n            \"input_ids\": None,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"image_features\": image_features,\n            \"attention_mask\": attention_mask,\n            \"encoder_outputs\": encoder_outputs,\n            \"use_cache\": False\n        }\n\n        outputs = model(**model_inputs)\n        next_token_logits = outputs[0][:, -1, :]\n        next_token_logits = top_k_top_p_filtering(next_token_logits,\n                                                  top_k=top_k,\n                                                  top_p=top_p)\n\n        next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1),\n                                       num_samples=1).squeeze(1)\n\n        _scores = F.log_softmax(next_token_logits, dim=-1)\n        _scores = torch.gather(_scores, -1, next_token.unsqueeze(-1))\n        logprobs.append(_scores)\n\n        tokens_to_add = next_token * unfinished_sents + (\n            tokenizer.pad_token_id) * (1 - unfinished_sents)\n        decoder_input_ids = torch.cat(\n            [decoder_input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n        cur_len = cur_len + 1\n\n        eos_in_sents = tokens_to_add == tokenizer.eos_token_id\n        # if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length\n        is_sents_unfinished_and_token_to_add_is_eos = unfinished_sents.mul(\n            eos_in_sents.long()).bool()\n        sent_lengths.masked_fill_(is_sents_unfinished_and_token_to_add_is_eos,\n                                  cur_len)\n        # unfinished_sents is set to zero if eos in sentence\n        unfinished_sents.mul_((~eos_in_sents).long())\n\n        if unfinished_sents.max() == 0:\n            break\n\n    logprobs = torch.cat(logprobs, dim=1)\n    for i in range(batch_size):\n        logprobs[i, sent_lengths[i] - 1:] = 0\n\n    sum_logprobs = logprobs.sum(dim=1)\n\n    return decoder_input_ids, sum_logprobs.unsqueeze(1)", "\ndef sample_sentence(model,\n                    input_ids,\n                    image_features,\n                    attention_mask,\n                    tokenizer,\n                    top_k=50,\n                    top_p=1.0,\n                    max_length=20):\n    batch_size = input_ids.shape[0]\n    encoder = model.get_encoder()\n    encoder_outputs = encoder(input_ids,\n                              image_features,\n                              attention_mask=attention_mask)\n\n    unfinished_sents = input_ids.new(batch_size).fill_(1)\n    sent_lengths = input_ids.new(batch_size).fill_(max_length)\n    logprobs = []\n\n    decoder_input_ids = input_ids.new(batch_size,\n                                      1).fill_(tokenizer.bos_token_id)\n\n    cur_len = 1\n    while cur_len < max_length:\n        model_inputs = {\n            \"input_ids\": None,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"image_features\": image_features,\n            \"attention_mask\": attention_mask,\n            \"encoder_outputs\": encoder_outputs,\n            \"use_cache\": False\n        }\n\n        outputs = model(**model_inputs)\n        next_token_logits = outputs[0][:, -1, :]\n        next_token_logits = top_k_top_p_filtering(next_token_logits,\n                                                  top_k=top_k,\n                                                  top_p=top_p)\n\n        next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1),\n                                       num_samples=1).squeeze(1)\n\n        _scores = F.log_softmax(next_token_logits, dim=-1)\n        _scores = torch.gather(_scores, -1, next_token.unsqueeze(-1))\n        logprobs.append(_scores)\n\n        tokens_to_add = next_token * unfinished_sents + (\n            tokenizer.pad_token_id) * (1 - unfinished_sents)\n        decoder_input_ids = torch.cat(\n            [decoder_input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n        cur_len = cur_len + 1\n\n        eos_in_sents = tokens_to_add == tokenizer.eos_token_id\n        # if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length\n        is_sents_unfinished_and_token_to_add_is_eos = unfinished_sents.mul(\n            eos_in_sents.long()).bool()\n        sent_lengths.masked_fill_(is_sents_unfinished_and_token_to_add_is_eos,\n                                  cur_len)\n        # unfinished_sents is set to zero if eos in sentence\n        unfinished_sents.mul_((~eos_in_sents).long())\n\n        if unfinished_sents.max() == 0:\n            break\n\n    logprobs = torch.cat(logprobs, dim=1)\n    for i in range(batch_size):\n        logprobs[i, sent_lengths[i] - 1:] = 0\n\n    sum_logprobs = logprobs.sum(dim=1)\n\n    return decoder_input_ids, sum_logprobs.unsqueeze(1)", ""]}
{"filename": "src/model/MAESC_model_for_generated_aspect_prompt_multitasks.py", "chunked_list": ["from typing import Optional, Tuple\nfrom fastNLP.modules.torch.encoder import Seq2SeqEncoder\nfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\nfrom fastNLP.modules.torch import State\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom src.model.modeling_bart import (PretrainedBartModel, BartEncoder,\n                                     BartDecoder, BartModel,\n                                     BartClassificationHead,", "                                     BartDecoder, BartModel,\n                                     BartClassificationHead,\n                                     _make_linear_from_emb,\n                                     _prepare_bart_decoder_inputs)\nfrom transformers import BartTokenizer\n\nfrom src.model.config import MultiModalBartConfig\n#from src.model.mixins import GenerationMixin, FromPretrainedMixin\nfrom src.model.modules_for_prompt_multitasks import MultiModalBartEncoder, MultiModalBartDecoder_span, Span_loss, MultiModalBartEncoder_for_Generating_aspect_prompt, MultiModalBartDecoder_generate_aspect_prompt\nfrom src.model.modules_for_prompt_multitasks import MultiModalBartDecoder_aspects_num ", "from src.model.modules_for_prompt_multitasks import MultiModalBartEncoder, MultiModalBartDecoder_span, Span_loss, MultiModalBartEncoder_for_Generating_aspect_prompt, MultiModalBartDecoder_generate_aspect_prompt\nfrom src.model.modules_for_prompt_multitasks import MultiModalBartDecoder_aspects_num \n\n\nclass MultiModalBartModel_AESC(PretrainedBartModel):\n    def build_model(self,\n                    args,\n                    bart_model,\n                    tokenizer,\n                    label_ids,\n                    config,\n                    decoder_type=None,\n                    copy_gate=False,\n                    use_encoder_mlp=False,\n                    use_recur_pos=False,\n                    tag_first=False):\n        if args.bart_init:\n            model = BartModel.from_pretrained(bart_model)\n            num_tokens, _ = model.encoder.embed_tokens.weight.shape\n            print('num_tokens', num_tokens)\n\n            model.resize_token_embeddings(\n                len(tokenizer.unique_no_split_tokens) + num_tokens)\n            encoder = model.encoder\n            decoder = model.decoder\n\n            padding_idx = config.pad_token_id\n            encoder.embed_tokens.padding_idx = padding_idx\n\n            # if use_recur_pos:\n            #     decoder.set_position_embedding(label_ids[0], tag_first)\n\n            _tokenizer = BartTokenizer.from_pretrained(bart_model)\n\n            for token in tokenizer.unique_no_split_tokens:\n                if token[:2] == '<<':  # \u7279\u6b8a\u5b57\u7b26\n                    index = tokenizer.convert_tokens_to_ids(\n                        tokenizer._base_tokenizer.tokenize(token))\n                    if len(index) > 1:\n                        raise RuntimeError(f\"{token} wrong split\")\n                    else:\n                        index = index[0]\n                    assert index >= num_tokens, (index, num_tokens, token)\n                    indexes = _tokenizer.convert_tokens_to_ids(\n                        _tokenizer.tokenize(token[2:-2]))\n                    embed = model.encoder.embed_tokens.weight.data[indexes[0]]\n                    for i in indexes[1:]:\n                        embed += model.decoder.embed_tokens.weight.data[i]\n                    embed /= len(indexes)\n                    model.decoder.embed_tokens.weight.data[index] = embed\n        else:\n            raise RuntimeError(\"error init!!!!!!!\")\n\n        multimodal_encoder_for_generated_aspect_prompt = MultiModalBartEncoder(config, encoder,\n                                                   tokenizer.img_feat_id,\n                                                   tokenizer.cls_token_id,\n                                                   args.num_image_tokens)\n\n        multimodal_encoder = MultiModalBartEncoder_for_Generating_aspect_prompt(\n                                                                         use_generated_prompt=args.use_generated_prompt,\n                                                                         config=config, \n                                                                         encoder = encoder,\n                                                                         img_feat_id = tokenizer.img_feat_id,\n                                                                         aspect_prompt_token_id=tokenizer.aspect_prompt_token_id,\n                                                                         senti_prompt_token_id=tokenizer.senti_prompt_token_id,\n                                                                         cls_token_id = tokenizer.cls_token_id,\n                                                                         num_image_tokens = args.num_image_tokens,\n                                                                         use_different_aspect_prompt=args.use_different_aspect_prompt \n                                                  \n                                                   )\n        return (multimodal_encoder_for_generated_aspect_prompt, multimodal_encoder, decoder)\n\n    def __init__(self, config: MultiModalBartConfig, args, bart_model,\n                 tokenizer, label_ids):\n        super().__init__(config)\n        self.config = config\n        self.tokenizer = tokenizer\n        label_ids = sorted(label_ids)\n        multimodal_encoder_for_generated_aspect_prompt, multimodal_encoder, share_decoder = self.build_model(\n            args, bart_model, self.tokenizer, label_ids, config)\n        causal_mask = torch.zeros(512, 512).fill_(float('-inf'))\n        self.causal_mask = causal_mask.triu(diagonal=1)\n        self.use_multitasks = args.use_multitasks\n        self.loss_lambda = args.loss_lambda\n        self.num_image_tokens = args.num_image_tokens\n\n        self.aspect_prompt_encoder = multimodal_encoder_for_generated_aspect_prompt\n        self.encoder = multimodal_encoder\n\n        only_sc = False\n        # need_tag = True  #if predict the sentiment or not\n        if args.task == 'twitter_ae':\n            need_tag = False\n        else:\n            need_tag = True\n            # if args.task == 'twitter_sc':\n            #     only_sc = True\n\n        self.prompt_decoder = MultiModalBartDecoder_generate_aspect_prompt(self.config, share_decoder)\n\n        if self.use_multitasks:\n            self.aspect_num_decoder = MultiModalBartDecoder_aspects_num(self.config, share_decoder)\n\n        self.decoder = MultiModalBartDecoder_span(self.config,\n                                                  self.tokenizer,\n                                                  share_decoder,\n                                                  self.tokenizer.pad_token_id,\n                                                  label_ids,\n                                                  self.causal_mask,\n                                                  num_image_tokens=self.num_image_tokens,\n                                                  need_tag=need_tag,\n                                                  only_sc=False)\n        self.span_loss_fct = Span_loss()\n\n    def prepare_state(self,\n                      input_ids,\n                      image_features,\n                      attention_mask=None,\n                      aesc_infos=None,\n                      aspects_num=None,\n                      first=None):\n        ##generate prompt for each instance\n\n        prompt_attention_mask = attention_mask\n        if self.num_image_tokens==0:\n            end_index = 62\n            begin_index = 22\n        elif self.num_image_tokens==1:\n            end_index = 63\n            begin_index = 23\n        elif self.num_image_tokens==2:\n            end_index = 64\n            begin_index = 24\n        elif self.num_image_tokens==3:\n            end_index = 65\n            begin_index = 25\n        elif self.num_image_tokens==4:\n            end_index = 66\n            begin_index = 26\n        elif self.num_image_tokens==5:\n            end_index = 67\n            begin_index = 27\n        elif self.num_image_tokens==6:\n            end_index = 68\n            begin_index = 28\n        elif self.num_image_tokens==7:\n            end_index = 69\n            begin_index = 29\n        \n        \n        \n        for i in range(len(prompt_attention_mask)):\n            mask = prompt_attention_mask[i]\n            mask[begin_index:end_index]=torch.zeros_like(mask[begin_index:end_index]) ##26:66 \u662faspect\u63d0\u793a\u7684\u4f4d\u7f6e\n            prompt_attention_mask[i]=mask\n        \n        ''' aspects_prompt '''\n        dict_for_prompt = self.aspect_prompt_encoder(input_ids=input_ids,\n                                              image_features=image_features,\n                                              attention_mask=prompt_attention_mask,\n                                              output_hidden_states=True,\n                                              return_dict=True)\n\n        \n        aspect_prompt_decoder_input_ids, aspect_prompt_decoder_attention_mask = [\n            aesc_infos['aspect_prompt_decoder_input_ids'].to(input_ids.device),\n            aesc_infos['aspect_prompt_decoder_attention_mask'].to(input_ids.device)]\n        generated_prompt = self.prompt_decoder(\n                                            encoder_outputs=dict_for_prompt.last_hidden_state, \n                                            attention_mask=attention_mask,\n                                            decoder_input_ids =aspect_prompt_decoder_input_ids, decoder_attention_mask=aspect_prompt_decoder_attention_mask)\n\n        generated_prompt = generated_prompt[:, 1:, :] ##(batch_size, 2, 768)\n\n        '''aspects_num'''\n        aspects_num_decoder_input_ids, aspects_num_decoder_attention_mask = [\n            aesc_infos['aspects_num_decoder_input_ids'].to(input_ids.device),\n            aesc_infos['aspects_num_decoder_attention_mask'].to(input_ids.device)]\n\n        # import ipdb; ipdb.set_trace()\n        if self.use_multitasks:\n            aspects_num_loss, predict_aspects_num_logits = self.aspect_num_decoder(aspects_num_labels=aspects_num,\n                                                                            encoder_outputs=dict_for_prompt[0], \n                                                                            attention_mask=attention_mask,\n                                                                            aspects_num_decoder_input_ids=aspects_num_decoder_input_ids)\n\n\n            predict_aspects_num = torch.argmax(predict_aspects_num_logits, dim=1)\n            new_predict_aspects_num = predict_aspects_num + torch.ones_like(predict_aspects_num)\n        else:\n            aspects_num_loss =0\n            new_predict_aspects_num = []\n            predict_aspects_num = []\n            for i in range(len(input_ids)):\n                new_predict_aspects_num.append(5)\n                predict_aspects_num.append(4)\n            new_predict_aspects_num = torch.tensor(new_predict_aspects_num)\n            predict_aspects_num = torch.tensor(predict_aspects_num)\n\n        dict = self.encoder(\n                            input_ids=input_ids,\n                            image_features=image_features,\n                            attention_mask=attention_mask,\n                            generated_prompt= generated_prompt,\n                            aspects_num = new_predict_aspects_num,\n                            output_hidden_states=True,\n                            return_dict=True)\n\n\n        encoder_outputs = dict.last_hidden_state\n        hidden_states = dict.hidden_states\n        encoder_mask = attention_mask\n        src_embed_outputs = hidden_states[0]\n        state = BartState(\n            encoder_outputs,\n            encoder_mask,\n            input_ids[:,\n                      end_index:],  #the text features start from index 38, the front are image features.\n            first,\n            src_embed_outputs)\n        # setattr(state, 'tgt_seq_len', tgt_seq_len)\n        return state, aspects_num_loss, predict_aspects_num\n\n\n    def forward(\n            self,\n            input_ids,\n            image_features,\n            attention_mask=None,\n            aesc_infos=None,\n            aspects_num=None,\n            encoder_outputs: Optional[Tuple] = None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n    ):\n        ### for prompt\n        # import ipdb; ipdb.set_trace()\n       \n        ## for aspect-spans\n      \n        aspects_num = torch.tensor(aspects_num).to(input_ids.device)\n        state, aspects_num_loss, predict_aspects_num = self.prepare_state( input_ids, image_features, attention_mask, aesc_infos, aspects_num)\n        spans, span_mask = [ \n            aesc_infos['labels'].to(input_ids.device),\n            aesc_infos['masks'].to(input_ids.device)\n        ]\n\n        logits = self.decoder(spans, state) ## spans: (2, 13) logits: (2, 12, 40)\n\n        span_loss = self.span_loss_fct(spans[:, 1:], logits, span_mask[:, 1:])\n\n        all_loss = span_loss + self.loss_lambda*aspects_num_loss\n\n        return all_loss, predict_aspects_num", "\n\n\nclass BartState(State):\n    def __init__(self, encoder_output, encoder_mask, src_tokens, first,\n                 src_embed_outputs):\n        super().__init__(encoder_output, encoder_mask)\n        self.past_key_values = None\n        self.src_tokens = src_tokens\n        self.first = first\n        self.src_embed_outputs = src_embed_outputs\n\n    def reorder_state(self, indices: torch.LongTensor):\n        super().reorder_state(indices)\n        self.src_tokens = self._reorder_state(self.src_tokens, indices)\n        if self.first is not None:\n            self.first = self._reorder_state(self.first, indices)\n        self.src_embed_outputs = self._reorder_state(self.src_embed_outputs,\n                                                     indices)\n        if self.past_key_values is not None:\n            new = []\n            for layer in self.past_key_values:\n                new_layer = {}\n                for key1 in list(layer.keys()):\n                    new_layer_ = {}\n                    for key2 in list(layer[key1].keys()):\n                        if layer[key1][key2] is not None:\n                            layer[key1][key2] = self._reorder_state(\n                                layer[key1][key2], indices)\n                            # print(key1, key2, layer[key1][key2].shape)\n                        new_layer_[key2] = layer[key1][key2]\n                    new_layer[key1] = new_layer_\n                new.append(new_layer)\n            self.past_key_values = new", ""]}
{"filename": "src/model/modeling_bart.py", "chunked_list": ["# coding=utf-8\n# Copyright 2020 The Facebook AI Research Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch BART model, ported from the fairseq repo.\"\"\"\nimport math\nimport random\nimport warnings", "import random\nimport warnings\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\nfrom torch.nn import CrossEntropyLoss\nfrom transformers.models.bart.modeling_bart import *", "from torch.nn import CrossEntropyLoss\nfrom transformers.models.bart.modeling_bart import *\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, BaseModelOutput\n\n# from transformer_my.modeling_bart import *\n\n# logger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"BartConfig\"\n_TOKENIZER_FOR_DOC = \"BartTokenizer\"", "_CONFIG_FOR_DOC = \"BartConfig\"\n_TOKENIZER_FOR_DOC = \"BartTokenizer\"\n\nBART_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"facebook/bart-base\",\n    \"facebook/bart-large\",\n    \"facebook/bart-large-mnli\",\n    \"facebook/bart-large-cnn\",\n    \"facebook/bart-large-xsum\",\n    \"facebook/mbart-large-en-ro\",", "    \"facebook/bart-large-xsum\",\n    \"facebook/mbart-large-en-ro\",\n]\n# This list is incomplete. See all BART models at https://huggingface.co/models?filter=bart\n\nBART_START_DOCSTRING = r\"\"\"\n\n    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n    pruning heads etc.)", "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n    pruning heads etc.)\n\n    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general\n    usage and behavior.\n\n    Parameters:\n        config (:class:`~transformers.BartConfig`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the configuration.", "        config (:class:`~transformers.BartConfig`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n\n\"\"\"\n\nBART_GENERATION_EXAMPLE = r\"\"\"\n    Summarization example::\n\n        >>> from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig", "\n        >>> from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n\n        >>> # see ``examples/summarization/bart/run_eval.py`` for a longer example\n        >>> model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n        >>> tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n\n        >>> ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n        >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n", "        >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n\n        >>> # Generate Summary\n        >>> summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=5, early_stopping=True)\n        >>> print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])\n\n\"\"\"\n\nBART_INPUTS_DOCSTRING = r\"\"\"\n    Args:", "BART_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using :class:`~transformers.BartTokenizer`.\n            See :meth:`transformers.PreTrainedTokenizer.encode` and\n            :meth:`transformers.PreTrainedTokenizer.__call__` for details.\n", "            :meth:`transformers.PreTrainedTokenizer.__call__` for details.\n\n            `What are input IDs? <../glossary.html#input-ids>`__\n        attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n", "            - 0 for tokens that are **masked**.\n\n            `What are attention masks? <../glossary.html#attention-mask>`__\n        decoder_input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`):\n            Provide for translation and summarization training. By default, the model will create this tensor by\n            shifting the :obj:`input_ids` to the right, following the paper.\n        decoder_attention_mask (:obj:`torch.BoolTensor` of shape :obj:`(batch_size, tgt_seq_len)`, `optional`):\n            Default behavior: generate a tensor that ignores pad tokens in :obj:`decoder_input_ids`. Causal mask will\n            also be used by default.\n", "            also be used by default.\n\n            If you want to change padding behavior, you should read :func:`modeling_bart._prepare_decoder_inputs` and\n            modify to your needs. See diagram 1 in `the paper <https://arxiv.org/abs/1910.13461>`__ for more\n            information on the default strategy.\n        encoder_outputs (:obj:`tuple(tuple(torch.FloatTensor)`, `optional`):\n            Tuple consists of (:obj:`last_hidden_state`, `optional`: :obj:`hidden_states`, `optional`: :obj:`attentions`)\n            :obj:`last_hidden_state` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`) is a\n            sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention of\n            the decoder.", "            sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention of\n            the decoder.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last\n            ``decoder_input_ids`` (those that don't have their past key value states given to this model) of shape\n            :obj:`(batch_size, 1)` instead of all ``decoder_input_ids`` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up", "        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        output_attentions (:obj:`bool`, `optional`):\n            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n            tensors for more detail.\n        output_hidden_states (:obj:`bool`, `optional`):\n            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n            more detail.\n        return_dict (:obj:`bool`, `optional`):", "            more detail.\n        return_dict (:obj:`bool`, `optional`):\n            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n\"\"\"\n\n\ndef invert_mask(attention_mask):\n    \"\"\"Turns 1->0, 0->1, False->True, True-> False\"\"\"\n    assert attention_mask.dim() == 2\n    return attention_mask.eq(0)", "\n\ndef _prepare_bart_decoder_inputs(config,\n                                 input_ids,\n                                 decoder_input_ids=None,\n                                 decoder_padding_mask=None,\n                                 causal_mask_dtype=torch.float32):\n    \"\"\"Prepare masks that ignore padding tokens in the decoder and a causal mask for the decoder if\n    none are provided. This mimics the default behavior in fairseq. To override it pass in masks.\n    Note: this is not called during generation\n    \"\"\"\n    pad_token_id = config.pad_token_id\n    if decoder_input_ids is None:\n        decoder_input_ids = shift_tokens_right(input_ids, pad_token_id)\n    bsz, tgt_len = decoder_input_ids.size()\n    if decoder_padding_mask is None:\n        decoder_padding_mask = make_padding_mask(decoder_input_ids,\n                                                 pad_token_id)\n    else:\n        decoder_padding_mask = invert_mask(decoder_padding_mask)\n    if decoder_padding_mask is not None and decoder_padding_mask.shape[1] > 1:\n        # never mask leading token, even if it is pad\n        decoder_padding_mask[:, 0] = decoder_padding_mask[:, 1]\n    tmp = fill_with_neg_inf(torch.zeros(tgt_len, tgt_len))\n    mask = torch.arange(tmp.size(-1))\n    tmp.masked_fill_(mask < (mask + 1).view(tmp.size(-1), 1), 0)\n    causal_mask = tmp.to(dtype=causal_mask_dtype,\n                         device=decoder_input_ids.device)\n    return decoder_input_ids, decoder_padding_mask, causal_mask", "\n\nclass PretrainedBartModel(PreTrainedModel):\n    config_class = BartConfig\n    base_model_prefix = \"model\"\n\n    def _init_weights(self, module):\n        std = self.config.init_std\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, SinusoidalPositionalEmbedding):\n            pass\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    @property\n    def dummy_inputs(self):\n        pad_token = self.config.pad_token_id\n        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]],\n                                 device=self.device)\n        dummy_inputs = {\n            \"attention_mask\": input_ids.ne(pad_token),\n            \"input_ids\": input_ids,\n        }\n        return dummy_inputs", "\n\ndef _make_linear_from_emb(emb):\n    vocab_size, emb_size = emb.weight.shape\n    lin_layer = nn.Linear(vocab_size, emb_size, bias=False)\n    lin_layer.weight.data = emb.weight.data\n    return lin_layer\n\n\n# Helper Functions, mostly for making masks\ndef _check_shapes(shape_1, shape2):\n    if shape_1 != shape2:\n        raise AssertionError(\"shape mismatch: {} != {}\".format(\n            shape_1, shape2))", "\n# Helper Functions, mostly for making masks\ndef _check_shapes(shape_1, shape2):\n    if shape_1 != shape2:\n        raise AssertionError(\"shape mismatch: {} != {}\".format(\n            shape_1, shape2))\n\n\ndef shift_tokens_right(input_ids, pad_token_id):\n    \"\"\"Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\"\"\"\n    prev_output_tokens = input_ids.clone()\n    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n    return prev_output_tokens", "def shift_tokens_right(input_ids, pad_token_id):\n    \"\"\"Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\"\"\"\n    prev_output_tokens = input_ids.clone()\n    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n    return prev_output_tokens\n\n\ndef make_padding_mask(input_ids, padding_idx=1):\n    \"\"\"True for pad tokens\"\"\"\n    padding_mask = input_ids.eq(padding_idx)\n    if not padding_mask.any():\n        padding_mask = None\n    return padding_mask", "\ndef make_padding_mask(input_ids, padding_idx=1):\n    \"\"\"True for pad tokens\"\"\"\n    padding_mask = input_ids.eq(padding_idx)\n    if not padding_mask.any():\n        padding_mask = None\n    return padding_mask\n\n\n# Helper Modules", "\n# Helper Modules\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, config: BartConfig):\n        super().__init__()\n        self.embed_dim = config.d_model\n        self.self_attn = Attention(self.embed_dim,\n                                   config.encoder_attention_heads,\n                                   dropout=config.attention_dropout)\n        self.normalize_before = config.normalize_before\n        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n        self.dropout = config.dropout\n        self.activation_fn = ACT2FN[config.activation_function]\n        self.activation_dropout = config.activation_dropout\n        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n        self.final_layer_norm = LayerNorm(self.embed_dim)\n\n    def forward(self, x, encoder_padding_mask, output_attentions=False):\n        \"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n                `(batch, src_len)` where padding elements are indicated by ``1``.\n            for t_tgt, t_src is excluded (or masked out), =0 means it is\n            included in attention\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        \"\"\"\n        residual = x\n        if self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n        x, attn_weights = self.self_attn(query=x,\n                                         key=x,\n                                         key_padding_mask=encoder_padding_mask,\n                                         output_attentions=output_attentions)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        if not self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.final_layer_norm(x)\n        x = self.activation_fn(self.fc1(x))\n        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n        x = self.fc2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        if not self.normalize_before:\n            x = self.final_layer_norm(x)\n        if torch.isinf(x).any() or torch.isnan(x).any():\n            clamp_value = torch.finfo(x.dtype).max - 1000\n            x = torch.clamp(x, min=-clamp_value, max=clamp_value)\n        return x, attn_weights", "\n\nclass BartEncoder(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n    is a :class:`EncoderLayer`.\n\n    Args:\n        config: BartConfig\n    \"\"\"\n    def __init__(self, config: BartConfig, embed_tokens):\n        super().__init__()\n\n        self.dropout = config.dropout\n        self.layerdrop = config.encoder_layerdrop\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_scale = math.sqrt(\n            embed_dim) if config.scale_embedding else 1.0\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = config.max_position_embeddings\n\n        self.embed_tokens = embed_tokens\n        if config.static_position_embeddings:\n            self.embed_positions = SinusoidalPositionalEmbedding(\n                config.max_position_embeddings, embed_dim, self.padding_idx)\n        else:\n            self.embed_positions = LearnedPositionalEmbedding(\n                config.max_position_embeddings,\n                embed_dim,\n                self.padding_idx,\n                config.extra_pos_embeddings,\n            )\n        self.layers = nn.ModuleList(\n            [EncoderLayer(config) for _ in range(config.encoder_layers)])\n        self.layernorm_embedding = LayerNorm(\n            embed_dim) if config.normalize_embedding else nn.Identity()\n        # mbart has one extra layer_norm\n        self.layer_norm = LayerNorm(\n            config.d_model) if config.add_final_layer_norm else None\n\n    def forward(self,\n                input_ids,\n                attention_mask=None,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=False):\n        \"\"\"\n        Args:\n            input_ids (LongTensor): tokens in the source language of shape\n                `(batch, src_len)`\n            attention_mask (torch.LongTensor): indicating which indices are padding tokens.\n        Returns:\n            BaseModelOutput or Tuple comprised of:\n                - **x** (Tensor): the last encoder layer's output of\n                  shape `(src_len, batch, embed_dim)`\n                - **encoder_states** (tuple(torch.FloatTensor)): all intermediate\n                  hidden states of shape `(src_len, batch, embed_dim)`.\n                  Only populated if *output_hidden_states:* is True.\n                - **all_attentions** (tuple(torch.FloatTensor)): Attention weights for each layer.\n                During training might not be of length n_layers because of layer dropout.\n        \"\"\"\n        # check attention mask and invert\n        if attention_mask is not None:\n            attention_mask = invert_mask(attention_mask)\n\n        inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n        embed_pos = self.embed_positions(input_ids)\n        x = inputs_embeds + embed_pos\n        x = self.layernorm_embedding(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        encoder_states = [] if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n        for encoder_layer in self.layers:\n            if output_hidden_states:\n                encoder_states.append(x)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability <\n                                  self.layerdrop):  # skip the layer\n                attn = None\n            else:\n                x, attn = encoder_layer(x,\n                                        attention_mask,\n                                        output_attentions=output_attentions)\n\n            if output_attentions:\n                all_attentions = all_attentions + (attn, )\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n        if output_hidden_states:\n            encoder_states.append(x)\n            # T x B x C -> B x T x C\n            encoder_states = tuple(\n                hidden_state.transpose(0, 1)\n                for hidden_state in encoder_states)\n\n        # T x B x C -> B x T x C\n        x = x.transpose(0, 1)\n\n        if not return_dict:\n            return tuple(v for v in [x, encoder_states, all_attentions]\n                         if v is not None)\n        return BaseModelOutput(last_hidden_state=x,\n                               hidden_states=encoder_states,\n                               attentions=all_attentions)", "\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, config: BartConfig):\n        super().__init__()\n        self.embed_dim = config.d_model\n\n        self.self_attn = Attention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n        )\n        self.dropout = config.dropout\n        self.activation_fn = ACT2FN[config.activation_function]\n        self.activation_dropout = config.activation_dropout\n        self.normalize_before = config.normalize_before\n\n        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n        self.encoder_attn = Attention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            encoder_decoder_attention=True,\n        )\n        self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)\n        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n        self.final_layer_norm = LayerNorm(self.embed_dim)\n\n    def forward(\n            self,\n            x,\n            encoder_hidden_states,\n            encoder_attn_mask=None,\n            layer_state=None,\n            causal_mask=None,\n            decoder_padding_mask=None,\n            output_attentions=False,\n    ):\n        residual = x\n\n        if layer_state is None:\n            layer_state = {}\n        if self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n        # Self Attention\n\n        x, self_attn_weights = self.self_attn(\n            query=x,\n            key=x,\n            layer_state=layer_state,  # adds keys to layer state\n            key_padding_mask=decoder_padding_mask,\n            attn_mask=causal_mask,\n            output_attentions=output_attentions,\n        )\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        if not self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        # Cross attention\n        residual = x\n        assert self.encoder_attn.cache_key != self.self_attn.cache_key\n        if self.normalize_before:\n            x = self.encoder_attn_layer_norm(x)\n        x, _ = self.encoder_attn(\n            query=x,\n            key=encoder_hidden_states,\n            key_padding_mask=encoder_attn_mask,\n            layer_state=layer_state,  # mutates layer state\n        )\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        if not self.normalize_before:\n            x = self.encoder_attn_layer_norm(x)\n\n        # Fully Connected\n        residual = x\n        if self.normalize_before:\n            x = self.final_layer_norm(x)\n        x = self.activation_fn(self.fc1(x))\n        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n        x = self.fc2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = residual + x\n        if not self.normalize_before:\n            x = self.final_layer_norm(x)\n        return (\n            x,\n            self_attn_weights,\n            layer_state,\n        )  # just self_attn weights for now, following t5, layer_state = cache for decoding", "\n\nclass BartDecoder(nn.Module):\n    \"\"\"\n    Transformer decoder consisting of *config.decoder_layers* layers. Each layer\n    is a :class:`DecoderLayer`.\n    Args:\n        config: BartConfig\n        embed_tokens (torch.nn.Embedding): output embedding\n    \"\"\"\n    def __init__(self, config: BartConfig, embed_tokens: nn.Embedding):\n        super().__init__()\n        self.dropout = config.dropout\n        self.layerdrop = config.decoder_layerdrop\n        self.do_blenderbot_90_layernorm = config.do_blenderbot_90_layernorm  # layernorm variant\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_target_positions = config.max_position_embeddings\n        self.embed_scale = math.sqrt(\n            config.d_model) if config.scale_embedding else 1.0\n        self.embed_tokens = embed_tokens\n        if config.static_position_embeddings:\n            self.embed_positions = SinusoidalPositionalEmbedding(\n                config.max_position_embeddings, config.d_model,\n                config.pad_token_id)\n        else:\n            self.embed_positions = LearnedPositionalEmbedding(\n                config.max_position_embeddings, config.d_model,\n                self.padding_idx, config.extra_pos_embeddings)\n        self.layers = nn.ModuleList([\n            DecoderLayer(config) for _ in range(config.decoder_layers)\n        ])  # type: List[DecoderLayer]\n        self.layernorm_embedding = LayerNorm(\n            config.d_model) if config.normalize_embedding else nn.Identity()\n        self.layer_norm = LayerNorm(\n            config.d_model) if config.add_final_layer_norm else None\n        self.config = config\n\n    def set_position_embedding(self, special_tag_start_id, tag_first=True):\n        if tag_first:\n            embed_positions = DecoderLearnedPositionalEmbedding(\n                self.config.max_position_embeddings, self.config.d_model,\n                self.padding_idx, self.config.extra_pos_embeddings,\n                special_tag_start_id)\n        else:\n            embed_positions = DecoderLearnedPositionalEmbedding2(\n                self.config.max_position_embeddings, self.config.d_model,\n                self.padding_idx, self.config.extra_pos_embeddings,\n                special_tag_start_id)\n\n        embed_positions.weight.data = self.embed_positions.weight.data\n        self.embed_positions = embed_positions\n\n    def forward(\n            self,\n            input_ids,\n            encoder_hidden_states,\n            encoder_padding_mask,\n            decoder_padding_mask,\n            decoder_causal_mask,\n            past_key_values=None,\n            use_cache=False,\n            output_attentions=False,\n            output_hidden_states=False,\n            return_dict=False,\n            use_pos_cache=False,\n            **unused,\n    ):\n        \"\"\"\n        Includes several features from \"Jointly Learning to Align and\n        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n\n        Args:\n            input_ids (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for teacher forcing\n            encoder_hidden_states: output from the encoder, used for\n                encoder-side attention\n            encoder_padding_mask: for ignoring pad tokens\n            past_key_values (dict or None): dictionary used for storing state during generation\n\n        Returns:\n            BaseModelOutputWithPast or tuple:\n                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n                - the cache\n                - hidden states\n                - attentions\n        \"\"\"\n        if \"decoder_cached_states\" in unused:\n            warnings.warn(\n                \"The `decoder_cached_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n                FutureWarning,\n            )\n            past_key_values = unused.pop(\"decoder_cached_states\")\n        if \"decoder_past_key_values\" in unused:\n            warnings.warn(\n                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n                FutureWarning,\n            )\n            past_key_values = unused.pop(\"decoder_past_key_values\")\n\n        # check attention mask and invert\n        if encoder_padding_mask is not None:\n            encoder_padding_mask = invert_mask(encoder_padding_mask)\n\n        # embed positions\n        positions = self.embed_positions(input_ids, use_cache=use_pos_cache)\n\n        if use_pos_cache:\n            input_ids = input_ids[:, -1:]\n            positions = positions[:, -1:]\n\n        x = self.embed_tokens(input_ids) * self.embed_scale\n        if self.do_blenderbot_90_layernorm:\n            x = self.layernorm_embedding(x)\n            x += positions\n        else:\n            x += positions\n            x = self.layernorm_embedding(x)\n\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # Convert to Bart output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n        x = x.transpose(0, 1)\n        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = []\n        for idx, decoder_layer in enumerate(self.layers):\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            if output_hidden_states:\n                all_hidden_states += (x, )\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability < self.layerdrop):\n                continue\n\n            layer_state = past_key_values[\n                idx] if past_key_values is not None else None\n\n            x, layer_self_attn, layer_past = decoder_layer(\n                x,\n                encoder_hidden_states,\n                encoder_attn_mask=encoder_padding_mask,\n                decoder_padding_mask=decoder_padding_mask,\n                layer_state=layer_state,\n                causal_mask=decoder_causal_mask,\n                output_attentions=output_attentions,\n            )\n\n            if use_cache:\n                next_decoder_cache.append(layer_past.copy())\n\n            if output_attentions:\n                all_self_attns += (layer_self_attn, )\n\n        if self.layer_norm:  # if config.add_final_layer_norm (mBART)\n            x = self.layer_norm(x)\n\n        # Convert to standard output format: (seq_len, BS, model_dim) -> (BS, seq_len, model_dim)\n        if output_hidden_states:\n            all_hidden_states = tuple(\n                hidden_state.transpose(0, 1)\n                for hidden_state in all_hidden_states)\n        x = x.transpose(0, 1)\n        encoder_hidden_states = encoder_hidden_states.transpose(0, 1)\n\n        next_cache = next_decoder_cache if use_cache else None\n\n        if not return_dict:\n            return tuple(\n                v for v in [x, next_cache, all_hidden_states, all_self_attns]\n                if v is not None)\n        return BaseModelOutputWithPast(last_hidden_state=x,\n                                       past_key_values=next_cache,\n                                       hidden_states=all_hidden_states,\n                                       attentions=all_self_attns)", "\n\ndef _reorder_buffer(attn_cache, new_order):\n    for k, input_buffer_k in attn_cache.items():\n        if input_buffer_k is not None:\n            attn_cache[k] = input_buffer_k.index_select(0, new_order)\n    return attn_cache\n\n\nclass Attention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n    def __init__(\n            self,\n            embed_dim,\n            num_heads,\n            dropout=0.0,\n            bias=True,\n            encoder_decoder_attention=False,  # otherwise self_attention\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim**-0.5\n\n        self.encoder_decoder_attention = encoder_decoder_attention\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.cache_key = \"encoder_decoder\" if self.encoder_decoder_attention else \"self\"\n\n    def _shape(self, tensor, seq_len, bsz):\n        return tensor.contiguous().view(seq_len, bsz * self.num_heads,\n                                        self.head_dim).transpose(0, 1)\n\n    def forward(\n            self,\n            query,\n            key: Optional[Tensor],\n            key_padding_mask: Optional[Tensor] = None,\n            layer_state: Optional[Dict[str, Optional[Tensor]]] = None,\n            attn_mask: Optional[Tensor] = None,\n            output_attentions=False,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"Input shape: Time(SeqLen) x Batch x Channel\"\"\"\n        static_kv: bool = self.encoder_decoder_attention\n        tgt_len, bsz, embed_dim = query.size()\n        assert embed_dim == self.embed_dim\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n        # get here for encoder decoder cause of static_kv\n        if layer_state is not None:  # reuse k,v and encoder_padding_mask\n            saved_state = layer_state.get(self.cache_key, {})\n            if \"prev_key\" in saved_state and static_kv:\n                # previous time steps are cached - no need to recompute key and value if they are static\n                key = None\n        else:\n            saved_state = None\n            layer_state = {}\n\n        q = self.q_proj(query) * self.scaling\n        if static_kv:\n            if key is None:\n                k = v = None\n            else:\n                k = self.k_proj(key)\n                v = self.v_proj(key)\n        else:\n            k = self.k_proj(query)\n            v = self.v_proj(query)\n\n        q = self._shape(q, tgt_len, bsz)\n        if k is not None:\n            k = self._shape(k, -1, bsz)\n        if v is not None:\n            v = self._shape(v, -1, bsz)\n        if saved_state is not None:\n            k, v, key_padding_mask = self._use_saved_state(\n                k, v, saved_state, key_padding_mask, static_kv, bsz)\n        # Update cache\n        layer_state[self.cache_key] = {\n            \"prev_key\": k.view(bsz, self.num_heads, -1, self.head_dim),\n            \"prev_value\": v.view(bsz, self.num_heads, -1, self.head_dim),\n            \"prev_key_padding_mask\":\n            key_padding_mask if not static_kv else None,\n        }\n\n        assert k is not None\n        src_len = k.size(1)\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n        assert attn_weights.size() == (bsz * self.num_heads, tgt_len, src_len)\n\n        if attn_mask is not None:\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len,\n                                             src_len) + attn_mask\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len,\n                                             src_len)\n\n        # This is part of a workaround to get around fork/join parallelism not supporting Optional types.\n        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n            key_padding_mask = None\n        assert key_padding_mask is None or key_padding_mask.size()[:2] == (\n            bsz,\n            src_len,\n        )\n\n        if key_padding_mask is not None:  # don't attend to padding symbols\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len,\n                                             src_len)\n            reshaped = key_padding_mask.unsqueeze(1).unsqueeze(2)\n            attn_weights = attn_weights.masked_fill(reshaped, float(\"-inf\"))\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len,\n                                             src_len)\n        attn_weights = F.softmax(attn_weights, dim=-1)\n        attn_probs = F.dropout(\n            attn_weights,\n            p=self.dropout,\n            training=self.training,\n        )\n\n        assert v is not None\n        attn_output = torch.bmm(attn_probs, v)\n        assert attn_output.size() == (bsz * self.num_heads, tgt_len,\n                                      self.head_dim)\n        attn_output = attn_output.transpose(0, 1).contiguous().view(\n            tgt_len, bsz, embed_dim)\n        attn_output = self.out_proj(attn_output)\n        if output_attentions:\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len,\n                                             src_len)\n        else:\n            attn_weights = None\n        return attn_output, attn_weights\n\n    def _use_saved_state(self, k, v, saved_state, key_padding_mask, static_kv,\n                         bsz):\n        # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n        if \"prev_key\" in saved_state:\n            _prev_key = saved_state[\"prev_key\"]\n            assert _prev_key is not None\n            prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n            if static_kv:\n                k = prev_key\n            else:\n                assert k is not None\n                k = torch.cat([prev_key, k], dim=1)\n        if \"prev_value\" in saved_state:\n            _prev_value = saved_state[\"prev_value\"]\n            assert _prev_value is not None\n            prev_value = _prev_value.view(bsz * self.num_heads, -1,\n                                          self.head_dim)\n            if static_kv:\n                v = prev_value\n            else:\n                assert v is not None\n                v = torch.cat([prev_value, v], dim=1)\n        assert k is not None and v is not None\n        prev_key_padding_mask: Optional[Tensor] = saved_state.get(\n            \"prev_key_padding_mask\", None)\n        if prev_key_padding_mask is not None:\n            if static_kv:\n                new_key_padding_mask = prev_key_padding_mask\n            else:\n                new_key_padding_mask = torch.cat(\n                    [prev_key_padding_mask, key_padding_mask], dim=1)\n        else:\n            new_key_padding_mask = key_padding_mask\n        return k, v, new_key_padding_mask", "\nclass Attention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n    def __init__(\n            self,\n            embed_dim,\n            num_heads,\n            dropout=0.0,\n            bias=True,\n            encoder_decoder_attention=False,  # otherwise self_attention\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim**-0.5\n\n        self.encoder_decoder_attention = encoder_decoder_attention\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.cache_key = \"encoder_decoder\" if self.encoder_decoder_attention else \"self\"\n\n    def _shape(self, tensor, seq_len, bsz):\n        return tensor.contiguous().view(seq_len, bsz * self.num_heads,\n                                        self.head_dim).transpose(0, 1)\n\n    def forward(\n            self,\n            query,\n            key: Optional[Tensor],\n            key_padding_mask: Optional[Tensor] = None,\n            layer_state: Optional[Dict[str, Optional[Tensor]]] = None,\n            attn_mask: Optional[Tensor] = None,\n            output_attentions=False,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"Input shape: Time(SeqLen) x Batch x Channel\"\"\"\n        static_kv: bool = self.encoder_decoder_attention\n        tgt_len, bsz, embed_dim = query.size()\n        assert embed_dim == self.embed_dim\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n        # get here for encoder decoder cause of static_kv\n        if layer_state is not None:  # reuse k,v and encoder_padding_mask\n            saved_state = layer_state.get(self.cache_key, {})\n            if \"prev_key\" in saved_state and static_kv:\n                # previous time steps are cached - no need to recompute key and value if they are static\n                key = None\n        else:\n            saved_state = None\n            layer_state = {}\n\n        q = self.q_proj(query) * self.scaling\n        if static_kv:\n            if key is None:\n                k = v = None\n            else:\n                k = self.k_proj(key)\n                v = self.v_proj(key)\n        else:\n            k = self.k_proj(query)\n            v = self.v_proj(query)\n\n        q = self._shape(q, tgt_len, bsz)\n        if k is not None:\n            k = self._shape(k, -1, bsz)\n        if v is not None:\n            v = self._shape(v, -1, bsz)\n        if saved_state is not None:\n            k, v, key_padding_mask = self._use_saved_state(\n                k, v, saved_state, key_padding_mask, static_kv, bsz)\n        # Update cache\n        layer_state[self.cache_key] = {\n            \"prev_key\": k.view(bsz, self.num_heads, -1, self.head_dim),\n            \"prev_value\": v.view(bsz, self.num_heads, -1, self.head_dim),\n            \"prev_key_padding_mask\":\n            key_padding_mask if not static_kv else None,\n        }\n\n        assert k is not None\n        src_len = k.size(1)\n        attn_weights = torch.bmm(q, k.transpose(1, 2))\n        assert attn_weights.size() == (bsz * self.num_heads, tgt_len, src_len)\n\n        if attn_mask is not None:\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len,\n                                             src_len) + attn_mask\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len,\n                                             src_len)\n\n        # This is part of a workaround to get around fork/join parallelism not supporting Optional types.\n        if key_padding_mask is not None and key_padding_mask.dim() == 0:\n            key_padding_mask = None\n        assert key_padding_mask is None or key_padding_mask.size()[:2] == (\n            bsz,\n            src_len,\n        )\n\n        if key_padding_mask is not None:  # don't attend to padding symbols\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len,\n                                             src_len)\n            reshaped = key_padding_mask.unsqueeze(1).unsqueeze(2)\n            attn_weights = attn_weights.masked_fill(reshaped, float(\"-inf\"))\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len,\n                                             src_len)\n        attn_weights = F.softmax(attn_weights, dim=-1)\n        attn_probs = F.dropout(\n            attn_weights,\n            p=self.dropout,\n            training=self.training,\n        )\n\n        assert v is not None\n        attn_output = torch.bmm(attn_probs, v)\n        assert attn_output.size() == (bsz * self.num_heads, tgt_len,\n                                      self.head_dim)\n        attn_output = attn_output.transpose(0, 1).contiguous().view(\n            tgt_len, bsz, embed_dim)\n        attn_output = self.out_proj(attn_output)\n        if output_attentions:\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len,\n                                             src_len)\n        else:\n            attn_weights = None\n        return attn_output, attn_weights\n\n    def _use_saved_state(self, k, v, saved_state, key_padding_mask, static_kv,\n                         bsz):\n        # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n        if \"prev_key\" in saved_state:\n            _prev_key = saved_state[\"prev_key\"]\n            assert _prev_key is not None\n            prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n            if static_kv:\n                k = prev_key\n            else:\n                assert k is not None\n                k = torch.cat([prev_key, k], dim=1)\n        if \"prev_value\" in saved_state:\n            _prev_value = saved_state[\"prev_value\"]\n            assert _prev_value is not None\n            prev_value = _prev_value.view(bsz * self.num_heads, -1,\n                                          self.head_dim)\n            if static_kv:\n                v = prev_value\n            else:\n                assert v is not None\n                v = torch.cat([prev_value, v], dim=1)\n        assert k is not None and v is not None\n        prev_key_padding_mask: Optional[Tensor] = saved_state.get(\n            \"prev_key_padding_mask\", None)\n        if prev_key_padding_mask is not None:\n            if static_kv:\n                new_key_padding_mask = prev_key_padding_mask\n            else:\n                new_key_padding_mask = torch.cat(\n                    [prev_key_padding_mask, key_padding_mask], dim=1)\n        else:\n            new_key_padding_mask = key_padding_mask\n        return k, v, new_key_padding_mask", "\n\nclass BartClassificationHead(nn.Module):\n    \"\"\"Head for sentence-level classification tasks.\"\"\"\n\n    # This can trivially be shared with RobertaClassificationHead\n\n    def __init__(\n            self,\n            input_dim,\n            inner_dim,\n            num_classes,\n            pooler_dropout,\n    ):\n        super().__init__()\n        self.dense = nn.Linear(input_dim, inner_dim)\n        self.dropout = nn.Dropout(p=pooler_dropout)\n        self.out_proj = nn.Linear(inner_dim, num_classes)\n\n    def forward(self, x):\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x", "\n\nclass LearnedPositionalEmbedding(nn.Embedding):\n    \"\"\"\n    This module learns positional embeddings up to a fixed maximum size.\n    Padding ids are ignored by either offsetting based on padding_idx\n    or by setting padding_idx to None and ensuring that the appropriate\n    position ids are passed to the forward function.\n    \"\"\"\n    def __init__(self, num_embeddings: int, embedding_dim: int,\n                 padding_idx: int, offset):\n        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n        # and adjust num_embeddings appropriately. Other models dont have this hack\n        self.offset = offset\n        assert padding_idx is not None\n        num_embeddings += offset\n        super().__init__(num_embeddings,\n                         embedding_dim,\n                         padding_idx=padding_idx)\n\n    def forward(self, input_ids, use_cache=False):\n        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n        bsz, seq_len = input_ids.shape[:2]\n        if use_cache:\n            positions = input_ids.data.new(1, 1).fill_(\n                seq_len - 1)  # called before slicing\n        else:\n            # starts at 0, ends at 1-seq_len\n            positions = torch.arange(seq_len,\n                                     dtype=torch.long,\n                                     device=self.weight.device)\n        return super().forward(positions + self.offset)", "\n\nclass DecoderLearnedPositionalEmbedding(nn.Embedding):\n    \"\"\"\n    \u4e3b\u8981\u4fee\u6539\u662f\uff0cposition\u7684\u662f\u5faa\u73af\u7684\n    This module learns positional embeddings up to a fixed maximum size.\n    Padding ids are ignored by either offsetting based on padding_idx\n    or by setting padding_idx to None and ensuring that the appropriate\n    position ids are passed to the forward function.\n    \"\"\"\n    def __init__(self,\n                 num_embeddings: int,\n                 embedding_dim: int,\n                 padding_idx: int,\n                 offset,\n                 special_tag_start_id=None):\n        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n        # and adjust num_embeddings appropriately. Other models dont have this hack\n        self.offset = offset\n        assert padding_idx is not None\n        num_embeddings += offset\n        self.special_tag_start_id = special_tag_start_id  # \u8fd9\u4e2aid\u4e4b\u540e\u7684\u8bcd\u662f\u7279\u6b8a\u8bcd\u6c47\n        super().__init__(num_embeddings,\n                         embedding_dim,\n                         padding_idx=padding_idx)\n\n    def forward(self, input_ids, use_cache=False):\n        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n        if self.special_tag_start_id is None or input_ids.size(1) < 2:\n            bsz, seq_len = input_ids.shape[:2]\n            if use_cache:\n                positions = input_ids.data.new(1, 1).fill_(\n                    seq_len - 1)  # called before slicing\n            else:\n                # starts at 0, ends at 1-seq_len\n                positions = torch.arange(seq_len,\n                                         dtype=torch.long,\n                                         device=self.weight.device)\n        else:\n            # \u5b9e\u73b0\u7684\u662f\u6bcf\u4e2a\u4f4d\u7f6e\u91cd\u65b0\u5f00\u59cbposition\n            \"\"\"\n                \u5927\u6982\u610f\u601d\u662f\uff0c\u5047\u8bbeinput_ids\u4e2d\u5047\u8bbe\u5927\u4e8e4\u662f\u7279\u6b8a\u7b26\u53f7\uff0c\u90a3\u4e48\u8f93\u5165\u662f\n                [[2, 4, 1, 2, 3, 5, 1],\n                 [2, 5, 3, 3, 0, 0, 0]]\u65f6\uff0c\u8f93\u51fa\u4e3a\n                [[0, 1, 2, 3, 4, 1, 2],\n                 [0, 1, 2, 3, 4, 5, 6] \u6bcf\u4e2a\u5927\u4e8e4\u7684\u4f4d\u7f6e\u90fd\u4f1a\u91cd\u7f6e\n            \"\"\"\n            _input_ids = input_ids[:, 1:]\n            bsz, seq_len = _input_ids.shape[:2]\n            special_tag_mask = _input_ids.ge(\n                self.special_tag_start_id)  # bsz x max_len\n            if special_tag_mask.sum() > 0:\n                num_masks = special_tag_mask.cumsum(dim=1).max()  # \u8868\u793a\u6700\u957f\u7684\n                arange_indices = torch.arange(seq_len).to(\n                    _input_ids).expand_as(_input_ids)  # bsz x max_len\n                special_tag_indice = arange_indices.masked_select(\n                    special_tag_mask)  # a vector\u53ea\u5305\u542b\u6240\u6709\u7684special\u7684indice\n                indices = torch.arange(num_masks).to(_input_ids)[None].repeat(\n                    bsz, 1)  # bsz x mask_len\n                mask = indices.lt(special_tag_mask.sum(dim=1, keepdim=True))\n                indices = indices.masked_scatter(mask, special_tag_indice)\n                _, inverted_indices = special_tag_mask.cumsum(dim=-1).unique(\n                    return_inverse=True)\n\n                inverted_indices = inverted_indices - inverted_indices[:, :1]\n                inverted_indices = inverted_indices.masked_fill(\n                    inverted_indices.ge(indices.size(1)),\n                    max(indices.size(1) - 1, 0))\n                positions = indices.gather(index=inverted_indices, dim=1)\n                positions = (arange_indices - positions) + 1\n            else:\n                positions = torch.arange(seq_len + 1,\n                                         dtype=torch.long,\n                                         device=self.weight.device)[None]\n\n            if use_cache:\n                positions = positions[:, -1:]\n            else:\n                positions = torch.cat([input_ids.new_zeros(bsz, 1), positions],\n                                      dim=1)\n\n        return super().forward(positions + self.offset)", "\n\nclass DecoderLearnedPositionalEmbedding2(nn.Embedding):\n    \"\"\"\n    \u4e3b\u8981\u4fee\u6539\u662f\uff0cposition\u7684\u662f\u5faa\u73af\u7684, \u548c\u4e0a\u9762\u7684\u533a\u522b\u662ftag\u6240\u5728\u7684\u4f4d\u7f6e\u4e0d\u540c\n    This module learns positional embeddings up to a fixed maximum size.\n    Padding ids are ignored by either offsetting based on padding_idx\n    or by setting padding_idx to None and ensuring that the appropriate\n    position ids are passed to the forward function.\n    \"\"\"\n    def __init__(self,\n                 num_embeddings: int,\n                 embedding_dim: int,\n                 padding_idx: int,\n                 offset,\n                 special_tag_start_id=None):\n        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n        # and adjust num_embeddings appropriately. Other models dont have this hack\n        self.offset = offset\n        assert padding_idx is not None\n        num_embeddings += offset\n        self.special_tag_start_id = special_tag_start_id  # \u8fd9\u4e2aid\u4e4b\u540e\u7684\u8bcd\u662f\u7279\u6b8a\u8bcd\u6c47\n        super().__init__(num_embeddings,\n                         embedding_dim,\n                         padding_idx=padding_idx)\n\n    def forward(self, input_ids, use_cache=False):\n        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n        if self.special_tag_start_id is None or input_ids.size(1) < 2:\n            bsz, seq_len = input_ids.shape[:2]\n            if use_cache:\n                positions = input_ids.data.new(1, 1).fill_(\n                    seq_len - 1)  # called before slicing\n            else:\n                # starts at 0, ends at 1-seq_len\n                positions = torch.arange(seq_len,\n                                         dtype=torch.long,\n                                         device=self.weight.device)\n        else:\n            # \u5b9e\u73b0\u7684\u662f\u6bcf\u4e2a\u4f4d\u7f6e\u91cd\u65b0\u5f00\u59cbposition\n            \"\"\"\n                \u5927\u6982\u610f\u601d\u662f\uff0c\u5047\u8bbeinput_ids\u4e2d\u5047\u8bbe\u5927\u4e8e4\u662f\u7279\u6b8a\u7b26\u53f7\uff0c\u90a3\u4e48\u8f93\u5165\u662f\n                [[2, 1, 2, 3, 4, 1, 5],\n                 [2, 3, 3, 5, 0, 0, 0]]\u65f6\uff0c\u8f93\u51fa\u4e3a\n                [[0, 1, 2, 3, 4, 1, 2],\n                 [0, 1, 2, 3, 4, 5, 6] \u6bcf\u4e2a\u5927\u4e8e4\u7684\u4f4d\u7f6e\u90fd\u4f1a\u91cd\u7f6e\n            \"\"\"\n            _input_ids = input_ids[:, 1:]  # \u628asos\u53bb\u6389\n            bsz, seq_len = _input_ids.shape[:2]\n            special_tag_mask = _input_ids.ge(\n                self.special_tag_start_id)  # bsz x max_len\n            if special_tag_mask.sum() > 0:\n                num_masks = special_tag_mask.cumsum(dim=1)  # \u8868\u793a\u6700\u957f\u7684\n                num_masks_value = num_masks.max()\n                arange_indices = torch.arange(seq_len).to(\n                    _input_ids).expand_as(_input_ids)  # bsz x max_len\n\n                special_tag_indice = arange_indices.masked_select(\n                    special_tag_mask)  # a vector\u53ea\u5305\u542b\u6240\u6709\u7684special\u7684indice\n                indices = torch.arange(num_masks_value).to(\n                    _input_ids)[None].repeat(bsz, 1)  # bsz x mask_len\n                mask = indices.lt(special_tag_mask.sum(dim=-1, keepdim=True))\n                special_tag_indice = indices.masked_scatter(\n                    mask, special_tag_indice)\n\n                indices = torch.cat([\n                    special_tag_indice.new_zeros(bsz, 1),\n                    special_tag_indice[:, :-1] + 1\n                ],\n                                    dim=1)\n                _, inverted_indices = special_tag_mask.flip(dims=[1]).cumsum(\n                    dim=-1).flip(dims=[1]).unique(return_inverse=True)\n                values = inverted_indices[:, 0]  # bsz\n                inverted_indices = values[:, None] - inverted_indices\n                inverted_indices = inverted_indices.masked_fill(\n                    inverted_indices.ge(indices.size(1)),\n                    indices.size(1) - 1)\n\n                positions = indices.gather(index=inverted_indices, dim=1)\n                positions = arange_indices - positions + 1\n            else:\n                positions = torch.arange(seq_len + 1,\n                                         dtype=torch.long,\n                                         device=self.weight.device)[None]\n\n        if use_cache:\n            positions = positions[:, -1:]\n        else:\n            positions = torch.cat([input_ids.new_zeros(bsz, 1), positions],\n                                  dim=1)\n\n        return super().forward(positions + self.offset)", "\n\ndef LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True):\n    if torch.cuda.is_available():\n        try:\n            from apex.normalization import FusedLayerNorm\n\n            return FusedLayerNorm(normalized_shape, eps, elementwise_affine)\n        except ImportError:\n            pass\n    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)", "\n\ndef fill_with_neg_inf(t):\n    \"\"\"FP16-compatible function that fills a input_ids with -inf.\"\"\"\n    return t.float().fill_(float(\"-inf\")).type_as(t)\n\n\n# Public API\ndef _get_shape(t):\n    return getattr(t, \"shape\", None)", "def _get_shape(t):\n    return getattr(t, \"shape\", None)\n\n\n@add_start_docstrings(\n    \"The bare BART Model outputting raw hidden-states without any specific head on top.\",\n    BART_START_DOCSTRING,\n)\nclass BartModel(PretrainedBartModel):\n    def __init__(self, config: BartConfig):\n        super().__init__(config)\n\n        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n\n        self.encoder = BartEncoder(config, self.shared)\n        self.decoder = BartDecoder(config, self.shared)\n\n        self.init_weights()\n\n    # @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        # tokenizer_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"facebook/bart-large\",\n        output_type=Seq2SeqModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n            self,\n            input_ids,\n            attention_mask=None,\n            decoder_input_ids=None,\n            decoder_attention_mask=None,\n            encoder_outputs: Optional[Tuple] = None,\n            past_key_values=None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n            **kwargs,\n    ):\n        if \"decoder_past_key_values\" in kwargs:\n            warnings.warn(\n                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n                FutureWarning,\n            )\n            past_key_values = kwargs.pop(\"decoder_past_key_values\")\n\n        if decoder_input_ids is None:\n            use_cache = False\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (output_hidden_states\n                                if output_hidden_states is not None else\n                                self.config.output_hidden_states)\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # make masks if user doesn't supply\n        if not use_cache:\n            decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n                self.config,\n                input_ids,\n                decoder_input_ids=decoder_input_ids,\n                decoder_padding_mask=decoder_attention_mask,\n                causal_mask_dtype=self.shared.weight.dtype,\n            )\n        else:\n            decoder_padding_mask, causal_mask = None, None\n\n        assert decoder_input_ids is not None\n\n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOuput when return_dict=False\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1]\n                if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2]\n                if len(encoder_outputs) > 2 else None,\n            )\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        decoder_outputs = self.decoder(\n            decoder_input_ids,\n            encoder_outputs[0],\n            attention_mask,\n            decoder_padding_mask,\n            decoder_causal_mask=causal_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        if not return_dict:\n            return decoder_outputs + encoder_outputs\n\n        return Seq2SeqModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, value):\n        self.shared = value\n        self.encoder.embed_tokens = self.shared\n        self.decoder.embed_tokens = self.shared\n\n    def get_output_embeddings(self):\n        return _make_linear_from_emb(self.shared)  # make it on the fly", "class BartModel(PretrainedBartModel):\n    def __init__(self, config: BartConfig):\n        super().__init__(config)\n\n        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n\n        self.encoder = BartEncoder(config, self.shared)\n        self.decoder = BartDecoder(config, self.shared)\n\n        self.init_weights()\n\n    # @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        # tokenizer_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"facebook/bart-large\",\n        output_type=Seq2SeqModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n            self,\n            input_ids,\n            attention_mask=None,\n            decoder_input_ids=None,\n            decoder_attention_mask=None,\n            encoder_outputs: Optional[Tuple] = None,\n            past_key_values=None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n            **kwargs,\n    ):\n        if \"decoder_past_key_values\" in kwargs:\n            warnings.warn(\n                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n                FutureWarning,\n            )\n            past_key_values = kwargs.pop(\"decoder_past_key_values\")\n\n        if decoder_input_ids is None:\n            use_cache = False\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (output_hidden_states\n                                if output_hidden_states is not None else\n                                self.config.output_hidden_states)\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # make masks if user doesn't supply\n        if not use_cache:\n            decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n                self.config,\n                input_ids,\n                decoder_input_ids=decoder_input_ids,\n                decoder_padding_mask=decoder_attention_mask,\n                causal_mask_dtype=self.shared.weight.dtype,\n            )\n        else:\n            decoder_padding_mask, causal_mask = None, None\n\n        assert decoder_input_ids is not None\n\n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOuput when return_dict=False\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1]\n                if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2]\n                if len(encoder_outputs) > 2 else None,\n            )\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        decoder_outputs = self.decoder(\n            decoder_input_ids,\n            encoder_outputs[0],\n            attention_mask,\n            decoder_padding_mask,\n            decoder_causal_mask=causal_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        if not return_dict:\n            return decoder_outputs + encoder_outputs\n\n        return Seq2SeqModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, value):\n        self.shared = value\n        self.encoder.embed_tokens = self.shared\n        self.decoder.embed_tokens = self.shared\n\n    def get_output_embeddings(self):\n        return _make_linear_from_emb(self.shared)  # make it on the fly", "\n\n@add_start_docstrings(\n    \"The BART Model with a language modeling head. Can be used for summarization.\",\n    BART_START_DOCSTRING)\nclass BartForConditionalGeneration(PretrainedBartModel):\n    base_model_prefix = \"model\"\n    authorized_missing_keys = [\n        r\"final_logits_bias\", r\"encoder\\.version\", r\"decoder\\.version\"\n    ]\n\n    def __init__(self, config: BartConfig):\n        super().__init__(config)\n        base_model = BartModel(config)\n        self.model = base_model\n        self.register_buffer(\n            \"final_logits_bias\",\n            torch.zeros((1, self.model.shared.num_embeddings)))\n\n    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n        old_num_tokens = self.model.shared.num_embeddings\n        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n        self.model.shared = new_embeddings\n        self._resize_final_logits_bias(new_num_tokens, old_num_tokens)\n        return new_embeddings\n\n    def _resize_final_logits_bias(self, new_num_tokens: int,\n                                  old_num_tokens: int) -> None:\n        if new_num_tokens <= old_num_tokens:\n            new_bias = self.final_logits_bias[:, :new_num_tokens]\n        else:\n            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens),\n                                     device=self.final_logits_bias.device)\n            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n        self.register_buffer(\"final_logits_bias\", new_bias)\n\n    # @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=Seq2SeqLMOutput,\n                               config_class=_CONFIG_FOR_DOC)\n    @add_end_docstrings(BART_GENERATION_EXAMPLE)\n    def forward(\n            self,\n            input_ids,\n            attention_mask=None,\n            decoder_input_ids=None,\n            decoder_attention_mask=None,\n            encoder_outputs=None,\n            past_key_values=None,\n            labels=None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n            **unused,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss.\n            Indices should either be in ``[0, ..., config.vocab_size]`` or -100 (see ``input_ids`` docstring).\n            Tokens with indices set to ``-100`` are ignored (masked), the loss is only computed for the tokens\n            with labels in ``[0, ..., config.vocab_size]``.\n\n        Returns:\n\n        Conditional generation example::\n\n            >>> # Mask filling only works for bart-large\n            >>> from transformers import BartTokenizer, BartForConditionalGeneration\n            >>> tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n            >>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n\n            >>> model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n            >>> input_ids = tokenizer([TXT], return_tensors='pt')['input_ids']\n            >>> logits = model(input_ids).logits\n\n            >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n            >>> probs = logits[0, masked_index].softmax(dim=0)\n            >>> values, predictions = probs.topk(5)\n\n            >>> tokenizer.decode(predictions).split()\n            >>> # ['good', 'great', 'all', 'really', 'very']\n        \"\"\"\n        if \"lm_labels\" in unused:\n            warnings.warn(\n                \"The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n                FutureWarning,\n            )\n            labels = unused.pop(\"lm_labels\")\n        if \"decoder_cached_states\" in unused:\n            warnings.warn(\n                \"The `decoder_cached_states` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n                FutureWarning,\n            )\n            past_key_values = unused.pop(\"decoder_cached_states\")\n        if \"decoder_past_key_values\" in unused:\n            warnings.warn(\n                \"The `decoder_past_key_values` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\",\n                FutureWarning,\n            )\n            past_key_values = unused.pop(\"decoder_past_key_values\")\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if labels is not None:\n            use_cache = False\n            if decoder_input_ids is None:\n                decoder_input_ids = shift_tokens_right(\n                    labels, self.config.pad_token_id)\n\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            encoder_outputs=encoder_outputs,\n            decoder_attention_mask=decoder_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        lm_logits = F.linear(outputs[0],\n                             self.model.shared.weight,\n                             bias=self.final_logits_bias)\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # TODO(SS): do we need to ignore pad tokens in labels?\n            masked_lm_loss = loss_fct(\n                lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (lm_logits, ) + outputs[1:]\n            return ((masked_lm_loss, ) +\n                    output) if masked_lm_loss is not None else output\n\n        return Seq2SeqLMOutput(\n            loss=masked_lm_loss,\n            logits=lm_logits,\n            past_key_values=outputs.past_key_values,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )\n\n    def prepare_inputs_for_generation(self, decoder_input_ids, past,\n                                      attention_mask, use_cache,\n                                      encoder_outputs, **kwargs):\n        return {\n            \"input_ids\":\n            None,  # encoder_outputs is defined. input_ids not needed\n            \"encoder_outputs\": encoder_outputs,\n            \"past_key_values\": past,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"attention_mask\": attention_mask,\n            \"use_cache\":\n            use_cache,  # change this to avoid caching (presumably for debugging)\n        }\n\n    def adjust_logits_during_generation(self, logits, cur_len, max_length):\n        if cur_len == 1 and self.config.force_bos_token_to_be_generated:\n            self._force_token_ids_generation(logits, self.config.bos_token_id)\n        elif cur_len == max_length - 1 and self.config.eos_token_id is not None:\n            self._force_token_ids_generation(logits, self.config.eos_token_id)\n        return logits\n\n    def _force_token_ids_generation(self, scores, token_id) -> None:\n        \"\"\"force one of token_ids to be generated by setting prob of all other tokens to 0 (logprob=-float(\"inf\"))\"\"\"\n        scores[:, [x for x in range(self.config.vocab_size)\n                   if x != token_id]] = -float(\"inf\")\n\n    @staticmethod\n    def _reorder_cache(past, beam_idx):\n        reordered_past = []\n        for layer_past in past:\n            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n            layer_past_new = {\n                attn_key: _reorder_buffer(attn_cache, beam_idx)\n                for attn_key, attn_cache in layer_past.items()\n            }\n            reordered_past.append(layer_past_new)\n        return reordered_past\n\n    def get_encoder(self):\n        return self.model.encoder\n\n    def get_output_embeddings(self):\n        return _make_linear_from_emb(self.model.shared)  # make it on the fly", "\n\n@add_start_docstrings(\n    \"\"\"Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks. \"\"\",\n    BART_START_DOCSTRING,\n)\nclass BartForSequenceClassification(PretrainedBartModel):\n    def __init__(self, config: BartConfig, **kwargs):\n        super().__init__(config, **kwargs)\n        self.model = BartModel(config)\n        self.classification_head = BartClassificationHead(\n            config.d_model,\n            config.d_model,\n            config.num_labels,\n            config.classifier_dropout,\n        )\n        self.model._init_weights(self.classification_head.dense)\n        self.model._init_weights(self.classification_head.out_proj)\n\n    # @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        # tokenizer_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"facebook/bart-large\",\n        output_type=Seq2SeqSequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n            self,\n            input_ids,\n            attention_mask=None,\n            decoder_input_ids=None,\n            decoder_attention_mask=None,\n            encoder_outputs=None,\n            labels=None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if labels is not None:\n            use_cache = False\n\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            encoder_outputs=encoder_outputs,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        x = outputs[0]  # last hidden state\n        eos_mask = input_ids.eq(self.config.eos_token_id)\n        if len(torch.unique(eos_mask.sum(1))) > 1:\n            raise ValueError(\n                \"All examples must have the same number of <eos> tokens.\")\n        sentence_representation = x[eos_mask, :].view(x.size(0), -1,\n                                                      x.size(-1))[:, -1, :]\n        logits = self.classification_head(sentence_representation)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_labels),\n                            labels.view(-1))\n\n        if not return_dict:\n            output = (logits, ) + outputs[1:]\n            return ((loss, ) + output) if loss is not None else output\n\n        return Seq2SeqSequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layer on top of\n    the hidden-states output to compute `span start logits` and `span end logits`). \"\"\",\n    BART_START_DOCSTRING,\n)\nclass BartForQuestionAnswering(PretrainedBartModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        config.num_labels = 2\n        self.num_labels = config.num_labels\n\n        self.model = BartModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.model._init_weights(self.qa_outputs)\n\n    # @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        # tokenizer_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"facebook/bart-large\",\n        output_type=Seq2SeqQuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n            self,\n            input_ids,\n            attention_mask=None,\n            decoder_input_ids=None,\n            decoder_attention_mask=None,\n            encoder_outputs=None,\n            start_positions=None,\n            end_positions=None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n    ):\n        r\"\"\"\n        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if start_positions is not None and end_positions is not None:\n            use_cache = False\n\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            encoder_outputs=encoder_outputs,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (\n                start_logits,\n                end_logits,\n            ) + outputs[1:]\n            return ((total_loss, ) +\n                    output) if total_loss is not None else output\n\n        return Seq2SeqQuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            past_key_values=outputs.past_key_values,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )", "\n\nclass SinusoidalPositionalEmbedding(nn.Embedding):\n    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n        super().__init__(num_positions, embedding_dim)\n        if embedding_dim % 2 != 0:\n            raise NotImplementedError(\n                f\"odd embedding_dim {embedding_dim} not supported\")\n        self.weight = self._init_weight(self.weight)\n\n    @staticmethod\n    def _init_weight(out: nn.Parameter):\n        \"\"\"Identical to the XLM create_sinusoidal_embeddings except features are not interleaved.\n        The cos features are in the 2nd half of the vector. [dim // 2:]\n        \"\"\"\n        n_pos, dim = out.shape\n        position_enc = np.array(\n            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n             for pos in range(n_pos)])\n        out[:, 0:dim // 2] = torch.FloatTensor(np.sin(\n            position_enc[:, 0::2]))  # This line breaks for odd n_pos\n        out[:, dim // 2:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n        out.detach_()\n        out.requires_grad = False\n        return out\n\n    @torch.no_grad()\n    def forward(self, input_ids, use_cache=False):\n        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n        bsz, seq_len = input_ids.shape[:2]\n        if use_cache:\n            positions = input_ids.data.new(1, 1).fill_(\n                seq_len - 1)  # called before slicing\n        else:\n            # starts at 0, ends at 1-seq_len\n            positions = torch.arange(seq_len,\n                                     dtype=torch.long,\n                                     device=self.weight.device)\n        return super().forward(positions)", ""]}
{"filename": "src/model/modules.py", "chunked_list": ["import random\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.models.bart.modeling_bart import *\nfrom src.model.modeling_bart import (\n    SinusoidalPositionalEmbedding,\n    LearnedPositionalEmbedding,", "    SinusoidalPositionalEmbedding,\n    LearnedPositionalEmbedding,\n    invert_mask,\n    EncoderLayer,\n    LayerNorm,\n)\nfrom src.model.modeling_bart import (PretrainedBartModel, BartDecoder,\n                                     BartClassificationHead,\n                                     _make_linear_from_emb,\n                                     _prepare_bart_decoder_inputs)", "                                     _make_linear_from_emb,\n                                     _prepare_bart_decoder_inputs)\nfrom src.model.config import MultiModalBartConfig\n\n\nclass ImageEmbedding(nn.Module):\n    def __init__(self, image_dim, final_dim):\n        super(ImageEmbedding, self).__init__()\n        self.linear = nn.Linear(image_dim, final_dim)\n\n    def forward(self, image_features):\n        img_len = list(map(len, image_features))\n        non_empty_features = list(filter(lambda x: len(x) > 0, image_features))\n\n        embedded = None\n        if len(non_empty_features) > 0:\n            img_tensor = torch.cat(non_empty_features, dim=0)\n            embedded = self.linear(img_tensor)\n\n        output = []\n        index = 0\n        for l in img_len:\n            if l > 0:\n                output.append(embedded[index:index + l])\n            else:\n                output.append(torch.empty(0))\n            index += l\n        return output", "\n\nclass MultiModalBartEncoder(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer\n    is a :class:EncoderLayer.\n\n    Args:\n        config: MultiModalBartConfig\n    \"\"\"\n    def __init__(self, config: MultiModalBartConfig, encoder, img_feat_id,\n                 cls_token_id):\n        super().__init__()\n\n        self.img_feat_id = img_feat_id\n        self.cls_token_id = cls_token_id\n        embed_tokens = encoder.embed_tokens\n        self.dropout = encoder.dropout\n        self.layerdrop = encoder.layerdrop\n\n        self.indentity = nn.Identity()\n\n        embed_dim = embed_tokens.embedding_dim\n        self.embed_scale = encoder.embed_scale\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_source_positions = encoder.max_source_positions\n\n        self.embed_tokens = embed_tokens\n        self.embed_images = ImageEmbedding(2048, embed_dim)\n        self.embed_positions = encoder.embed_positions\n\n        self.layers = encoder.layers\n        self.layernorm_embedding = encoder.layernorm_embedding\n        # mbart has one extra layer_norm\n        self.layer_norm = encoder.layer_norm\n\n    def _embed_multi_modal(self, input_ids, image_features):\n        \"\"\"embed textual and visual inputs and combine them into one embedding\"\"\"\n        mask = (input_ids == self.img_feat_id) | (\n            input_ids == self.cls_token_id)\n        # print(mask.shape)\n        embedded_images = self.embed_images(image_features)\n        embedded = self.embed_tokens(input_ids)\n        # print('mask shape', mask.shape)\n        if not embedded_images[0].dtype == torch.float32:\n            embedded = embedded.half()\n\n        for index, value in enumerate(embedded_images):\n            if len(value) > 0:\n                embedded[index, mask[index]] = value\n\n        return embedded\n\n    def forward(self,\n                input_ids,\n                image_features,\n                attention_mask=None,\n                output_attentions=False,\n                output_hidden_states=False,\n                return_dict=False):\n        \"\"\"\n\n        :param input_ids: LongTensor, tokens in the source language of shape (batch, src_len)\n        :param image_features: list[FloatTensor], image roi features with length of batch\n        :param attention_mask: LongTensor, indicating which indices are padding tokens.\n        :param output_attentions:\n        :param output_hidden_states:\n        :return: Tuple comprised of:\n            - x (Tensor): the last encoder layer's output of\n              shape (src_len, batch, embed_dim)\n            - encoder_states (List[Tensor]): all intermediate\n              hidden states of shape (src_len, batch, embed_dim).\n              Only populated if output_hidden_states: is True.\n            - all_attentions (List[Tensor]): Attention weights for each layer.\n            During training might not be of length n_layers because of layer dropout.\n        \"\"\"\n        # check attention mask and invert\n        if attention_mask is not None:\n            attention_mask = invert_mask(attention_mask)\n\n        inputs_embeds = self._embed_multi_modal(\n            input_ids, image_features) * self.embed_scale\n        embed_pos = self.embed_positions(input_ids)\n        x = inputs_embeds + embed_pos\n        x = self.layernorm_embedding(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n\n        # B x T x C -> T x B x C\n        x = x.transpose(0, 1)\n\n        encoder_states, all_attentions = [], []\n        for encoder_layer in self.layers:\n            if output_hidden_states:\n                encoder_states.append(x)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability <\n                                  self.layerdrop):  # skip the layer\n                attn = None\n            else:\n                x, attn = encoder_layer(x,\n                                        attention_mask,\n                                        output_attentions=output_attentions)\n\n            if output_attentions:\n                all_attentions.append(attn)\n\n        if self.layer_norm:\n            x = self.layer_norm(x)\n        if output_hidden_states:\n            encoder_states.append(x)\n\n        # T x B x C -> B x T x C\n        encoder_states = [\n            hidden_state.transpose(0, 1) for hidden_state in encoder_states\n        ]\n        x = x.transpose(0, 1)\n\n        if not return_dict:\n            return tuple(v for v in [x, encoder_states, all_attentions]\n                         if v is not None)\n        return BaseModelOutput(last_hidden_state=x,\n                               hidden_states=encoder_states,\n                               attentions=all_attentions)", "\n\nclass MultiModalBartDecoder_span(nn.Module\n                                 ):  #AOE task and all downstream tasks\n    def __init__(self,\n                 config: MultiModalBartConfig,\n                 tokenizer,\n                 decoder,\n                 pad_token_id,\n                 label_ids,\n                 causal_mask,\n                 need_tag=True,\n                 only_sc=False,\n                 avg_feature=False,\n                 use_encoder_mlp=True):\n        super().__init__()\n        self.decoder = decoder\n        self.tokenizer = tokenizer\n        self.causal_mask = causal_mask\n        self.register_buffer('causal_masks', causal_mask.float())\n        self.pad_token_id = pad_token_id\n        # label_ids = sorted(label_ids, reverse=False)\n        self.label_start_id = min(label_ids)\n        self.label_end_id = max(label_ids) + 1\n        self.need_tag = need_tag\n        self.only_sc = only_sc\n        mapping = torch.LongTensor([0, 2] + label_ids)\n        ###mapping: [0, 2, 50276, 50277, 50278, 50281]\n        self.register_buffer('mapping', mapping)\n        self.src_start_index = len(mapping)  # \u52a0\u4e0a\u4e00\u4e2a\n        hidden_size = decoder.embed_tokens.weight.size(1)\n        self.dropout_layer = nn.Dropout(0.1)\n\n        self.end_text_id = tokenizer.end_text_id\n        self.avg_feature = avg_feature\n        if use_encoder_mlp:\n            self.encoder_mlp = nn.Sequential(\n                nn.Linear(hidden_size, hidden_size), nn.Dropout(0.3),\n                nn.ReLU(), nn.Linear(hidden_size, hidden_size))\n\n    def forward(self, tokens, state, only_sc=False):\n        # import ipdb; ipdb.set_trace()\n        '''\n        tokens: [[0, 2, 2, 16, 16, 4, 18, 18, 4, 1, 1, 1, 1],\n                 [0, 2, 2, 15, 16, 3, 25, 26, 5, 28, 28, 4, 1]]\n        '''\n        # import ipdb; ipdb.set_trace()\n        bsz, max_len = tokens.size()\n        encoder_outputs = state.encoder_output ##(batch, 72=38(len(image_token+begin_image+end_image(36+1+1)))+34(max_tex_len(\u5305\u542bbegin_text_id(0) and end_text_id(2)) in batch), 768)\n        encoder_pad_mask = state.encoder_mask ##(batch, 72)\n        first = state.first\n        # tokens\u4e4b\u540e\u76840\u5168\u662fpadding\uff0c\u56e0\u4e3a1\u662feos, \u5728pipe\u4e2d\u89c4\u5b9a\u7684\n        cumsum = tokens.eq(1).flip(dims=[1]).cumsum(dim=-1)\n        tgt_pad_mask = cumsum.flip(dims=[1]).ne(cumsum[:, -1:])\n\n        # \u628a\u8f93\u5165\u505a\u4e00\u4e0b\u6620\u5c04\n        mapping_token_mask = tokens.lt(\n            self.src_start_index)  # \u4e3a1\u7684\u5730\u65b9\u5e94\u8be5\u4ecemapping\u4e2d\u53d6index\n        mapped_tokens = tokens.masked_fill(tokens.ge(self.src_start_index), 0)\n        tag_mapped_tokens = self.mapping[mapped_tokens]\n\n        src_tokens_index = tokens - self.src_start_index  # bsz x num_src_token\n        src_tokens_index = src_tokens_index.masked_fill(\n            src_tokens_index.lt(0), 0)\n        src_tokens = state.src_tokens \n        # print(src_tokens.shape): (2, 34)\n        if first is not None:\n            src_tokens = src_tokens.gather(index=first, dim=1) ###Sequence\n        word_mapped_tokens = src_tokens.gather(index=src_tokens_index, dim=1)\n        #print('word_mapped_tokens', word_mapped_tokens[0])\n        tokens = torch.where(mapping_token_mask, tag_mapped_tokens,\n                             word_mapped_tokens)\n\n        tokens = tokens.masked_fill(tgt_pad_mask, self.pad_token_id)\n        '''\n        {'AESC': 50281, 'POS': 50276, 'NEU': 50277, 'NEG': 50278}\n        tensor([[0, 50276, 50276, 4644, 4644, 50278, 798, 798, 50278, 2, 1, 1, 1],\n                [0, 50276, 50276, 9517, 957, 50277, 2561, 7772, 50281, 2762, 2762, 50278, 2]])\n        \u5c06tokens\u4e2d\u7684index\u4ee5\u53ca\u6807\u7b7e\u90fd\u8f6c\u5316\u4e3avocabulary\u4e2d\u7684token_id\n        '''\n        \n\n        if self.training:\n            tokens = tokens[:, :-1]\n            decoder_pad_mask = tokens.eq(\n                self.pad_token_id)  # decoder\u9700\u8981\u8ba9pad\u4f4d\u7f6e\u4e3a1\n\n            dict = self.decoder(input_ids=tokens,\n                                encoder_hidden_states=encoder_outputs,\n                                encoder_padding_mask=encoder_pad_mask,\n                                decoder_padding_mask=decoder_pad_mask,\n                                decoder_causal_mask=self.\n                                causal_masks[:tokens.size(1), :tokens.size(1)],\n                                return_dict=True)\n        else:\n\n            past_key_values = state.past_key_values\n            dict = self.decoder(input_ids=tokens,\n                                encoder_hidden_states=encoder_outputs,\n                                encoder_padding_mask=encoder_pad_mask,\n                                decoder_padding_mask=None,\n                                decoder_causal_mask=self.\n                                causal_masks[:tokens.size(1), :tokens.size(1)],\n                                return_dict=True)\n        \n        hidden_state = dict.last_hidden_state  # bsz x max_len x hidden_size (2, 12(\u53bb\u6389\u4e86 end_token_id), 768)\n        hidden_state = self.dropout_layer(hidden_state)\n        if not self.training:\n            state.past_key_values = dict.past_key_values\n\n        logits = hidden_state.new_full(\n            (hidden_state.size(0), hidden_state.size(1),\n             self.src_start_index + src_tokens.size(-1)),\n            fill_value=-1e24)\n        ##\u5efa\u7acb\u7a7a\u7684logits\n        # print('logits', logits.shape) (bsz, max_len,  self.src_start_index + src_tokens.size(-1)) -> (2, 12, 40=6+34)\n        # \u9996\u5148\u8ba1\u7b97\u7684\u662f\n\n        if self.need_tag:\n            '''\n            self.decoder.embed_tokens.weight: (50289, 768)\n            self.label_start_id: 50276\n            '''\n            tag_scores = F.linear(\n                hidden_state,\n                self.dropout_layer(\n                    self.decoder.embed_tokens.\n                    weight[self.label_start_id:self.label_start_id +\n                           3]))  # bsz x max_len x num_class\n            logits[:, :, 3:self.src_start_index] = tag_scores ###\u7ed9\u60c5\u611f\u7684position\u8d4b\u503c[:, :, (3, 4, 5)]\n        if not only_sc:\n            eos_scores = F.linear(\n                hidden_state,\n                self.dropout_layer(self.decoder.embed_tokens.weight[2:3])) \n            '''\n            ['</s>(eos_token)', '<mask>', '<pad>', '<s>(bos_token)', '<unk>']\n            [2, 50264, 1, 0, 3]\n            '''\n\n            # bsz x max_bpe_len(image_len + text_len) x hidden_size: (2, 72, 768)\n            src_outputs = state.encoder_output \n            if hasattr(self, 'encoder_mlp') and not only_sc:\n                src_outputs = self.encoder_mlp(src_outputs)\n\n            if first is not None:\n                mask = first.eq(0)\n                src_outputs = src_outputs.gather(\n                    index=first.unsqueeze(2).repeat(1, 1,\n                                                    src_outputs.size(-1)),\n                    dim=1)\n            else:\n                mask = state.encoder_mask[:, 38:].eq(0)\n                # src_outputs = self.decoder.embed_tokens(src_tokens)\n            mask = mask.unsqueeze(1) ## bsz x 1 x max_word_len: (2, 1, 34)\n            input_embed = self.decoder.embed_tokens(\n                src_tokens)  #bsz x max_word_len x hidden_size: (2, 34, 768); src_tokens: (2, 34)\n            input_embed = self.dropout_layer(input_embed)\n            if self.avg_feature:  # \u5148\u628afeature\u5408\u5e76\u4e00\u4e0b\n                src_outputs = (src_outputs[:, 38:] + input_embed) / 2\n            word_scores = torch.einsum(\n                'blh,bnh->bln', hidden_state,\n                src_outputs[:, 38:])  # bsz x max_len x max_word_len: (2, 12, 34)\n            if not self.avg_feature:\n                gen_scores = torch.einsum(\n                    'blh,bnh->bln', hidden_state,\n                    input_embed)  # bsz x max_len x max_word_len: (2, 12, 34)\n                word_scores = (gen_scores + word_scores) / 2 \n            mask = mask.__or__(\n                src_tokens.eq(2).cumsum(dim=1).ge(1).unsqueeze(1)) ###(2, 1, 34)\n            word_scores = word_scores.masked_fill(mask, -1e32) ###(bts, max_len, max_word_len)\n            logits[:, :, self.src_start_index:] = word_scores\n            ###logits.shape (bts, max_len, max_word_len+6): (2, 12, 40)\n            logits[:, :, 1:2] = eos_scores\n        # print(torch.argmax(logits[0], dim=-1))\n        return logits\n\n    def decode(self, tokens, state, only_sc=False):\n        return self(tokens, state, only_sc)[:, -1]", "\n\nclass Span_loss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.loss_fct = nn.CrossEntropyLoss()\n        self.fc = nn.LogSoftmax(dim=-1)\n\n    def forward(self, tgt_tokens, pred, mask):\n        '''\n        tgt_tokens: (2 (batch-size), 12 (max_len+1))\n        pred: (2, 12, 40 (max_word_len))\n        '''\n\n        tgt_tokens = tgt_tokens.masked_fill(mask.eq(0), -100)\n        output = F.cross_entropy(target=tgt_tokens, input=pred.transpose(1, 2)) ##\u6bcf\u4e00\u4e2a\u8bcd\u90fd\u670912\u79cd\u7c7b\u522b\uff0c input= (40, 12)\n        return output", "\n\nclass MultiModalBartDecoder_MLM(nn.Module):\n    def __init__(self, config: MultiModalBartConfig, decoder):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.register_buffer(\n            \"final_logits_bias\",\n            torch.zeros((1, self.decoder.embed_tokens.num_embeddings)))\n\n    def forward(self, labels, input_ids, encoder_outputs, attention_mask,\n                decoder_input_ids, decoder_attention_mask):\n        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n            self.config,\n            input_ids,\n            decoder_input_ids=decoder_input_ids,\n            decoder_padding_mask=decoder_attention_mask,\n            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)\n        decoder_outputs = self.decoder(\n            decoder_input_ids,\n            encoder_outputs,\n            attention_mask,\n            decoder_padding_mask,\n            decoder_causal_mask=causal_mask[:decoder_input_ids.size(1), :\n                                            decoder_input_ids.size(1)],\n        )\n\n        lm_logits = F.linear(decoder_outputs[0][:, 1:],\n                             self.decoder.embed_tokens.weight,\n                             bias=self.final_logits_bias)\n\n        lm_loss = 0\n        # compute lm loss if labels is given\n        if labels is not None:\n            labels = labels.clone()\n            loss_fct = nn.CrossEntropyLoss()\n            lm_loss = loss_fct(\n                lm_logits.view(-1, self.decoder.embed_tokens.weight.size(0)),\n                labels.reshape(-1))\n\n            return lm_loss", "\n\nclass MultiModalBartDecoder_ANP_generate(nn.Module):  #AOG task\n    def __init__(self, config: MultiModalBartConfig, decoder):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.register_buffer(\n            \"final_logits_bias\",\n            torch.zeros((1, self.decoder.embed_tokens.num_embeddings)))\n\n    def forward(self, labels, input_ids, encoder_outputs, attention_mask,\n                decoder_input_ids, decoder_attention_mask):\n        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n            self.config,\n            input_ids,\n            decoder_input_ids=decoder_input_ids,\n            decoder_padding_mask=decoder_attention_mask,\n            causal_mask_dtype=self.decoder.embed_tokens.weight.dtype)\n\n        decoder_outputs = self.decoder(\n            decoder_input_ids,\n            encoder_outputs,\n            attention_mask,\n            decoder_padding_mask,\n            decoder_causal_mask=causal_mask[:decoder_input_ids.size(1), :\n                                            decoder_input_ids.size(1)],\n        )\n\n        lm_logits = F.linear(decoder_outputs[0][:, 1:],\n                             self.decoder.embed_tokens.weight,\n                             bias=self.final_logits_bias)\n\n        lm_loss = 0\n        # compute lm loss if labels is given\n        if labels is not None:\n            labels = labels.clone()\n            # labels[labels == self.cls_token_id] = -100\n            loss_fct = nn.CrossEntropyLoss()\n            lm_loss = loss_fct(\n                lm_logits.view(-1, self.decoder.embed_tokens.weight.size(0)),\n                labels.reshape(-1))\n\n            return lm_loss", "\n\nclass MultiModalBartDecoder_sentiment(nn.Module):  #MSP task\n    def __init__(self,\n                 config: MultiModalBartConfig,\n                 decoder,\n                 senti_ids,\n                 senti_nums=3):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.senti_ids = senti_ids\n        self.dropout_layer = nn.Dropout(0.1)\n        self.senti_head = BartClassificationHead(config.d_model,\n                                                 config.d_model, senti_nums,\n                                                 config.classif_dropout)\n\n    def _init_weights(self, module):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, senti_labels, encoder_outputs, attention_mask,\n                senti_decoder_input_ids):\n\n        decoder_outputs = self.decoder(\n            input_ids=senti_decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            encoder_padding_mask=attention_mask,\n            decoder_padding_mask=None,\n            decoder_causal_mask=None,\n        )\n\n        # predict_senti = F.linear(\n        #     decoder_outputs[0][:, 1],\n        #     self.dropout_layer(self.decoder.embed_tokens.\n        #                        weight[self.senti_ids[0]:self.senti_ids[2] +\n        #                               1]))  # bsz\n        # predict_senti = torch.flip(predict_senti, dims=[-1])\n        predict_senti = self.senti_head(decoder_outputs[0][:, 1])\n        loss_fct = nn.CrossEntropyLoss()\n        senti_loss = loss_fct(predict_senti, senti_labels)\n        return senti_loss, predict_senti", "\n\nclass MultiModalBartDecoder_MRM(nn.Module):\n    def __init__(self, config: MultiModalBartConfig, decoder, causal_mask,\n                 args):\n        super().__init__()\n        self.config = config\n        self.decoder = decoder\n        self.causal_mask = causal_mask\n        self.args = args\n        self.mrm_head = BartClassificationHead(\n            config.d_model,\n            config.d_model,\n            config.num_labels,\n            config.classif_dropout,\n        )\n        self._init_weights(self.mrm_head.dense)\n        self._init_weights(self.mrm_head.out_proj)\n\n    def _init_weights(self, module):\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, mrm_labels, mrm_masks, encoder_outputs, attention_mask,\n                mrm_decoder_input_ids, mrm_decoder_attention_mask):\n\n        decoder_padding_mask = mrm_decoder_attention_mask.eq(0)\n        decoder_outputs = self.decoder(\n            input_ids=mrm_decoder_input_ids,\n            encoder_hidden_states=encoder_outputs,\n            encoder_padding_mask=attention_mask,\n            decoder_padding_mask=decoder_padding_mask,\n            decoder_causal_mask=self.causal_mask[:mrm_decoder_input_ids.size(\n                1), :mrm_decoder_input_ids.size(1)].to(\n                    mrm_decoder_input_ids.device),\n        )\n        region_representation = decoder_outputs[0][mrm_masks.bool()]\n        if len(region_representation) > 0:\n            predict_cls = self.mrm_head(region_representation)\n            loss_fct = nn.CrossEntropyLoss()\n            mrm_labels = torch.cat(mrm_labels,\n                                   dim=0).to(encoder_outputs.device)\n\n            if self.args.mrm_loss_type == 'KL':\n                predict_cls = F.log_softmax(predict_cls, dim=-1)\n                mrm_loss = F.kl_div(predict_cls.double(),\n                                    mrm_labels.double().squeeze(1),\n                                    reduction='batchmean')\n            else:\n                raise RuntimeError(\"wrong mrm type\")\n        else:\n            mrm_loss = 0\n\n        return mrm_loss", ""]}
{"filename": "src/model/attention.py", "chunked_list": ["import torch.nn as nn\nimport torch\nimport numpy as np\n\nclass Attention_for_Senti_Prompt(nn.Module):\n    def __init__(self, n_head=8, model_dim=768, drop_rate=0.2):\n        # n_head \u6709\u51e0\u5c42\u6ce8\u610f\u529b\u673a\u5236\n        # model_dim \u6a21\u578b\u7684\u7ef4\u5ea6\n        # drop_rate \u968f\u673a\u4e22\u5f03\u7387\n        super().__init__()\n        self.n_head = n_head\n        self.head_dim = model_dim // n_head     # 32//4=8\n        self.wq = nn.Linear(model_dim, n_head * self.head_dim)  # [4*8]\n        self.wk = nn.Linear(model_dim, n_head * self.head_dim)\n        self.wv = nn.Linear(model_dim, n_head * self.head_dim)\n\n        self.o_dense = nn.Linear(model_dim, model_dim)\n        self.o_drop = nn.Dropout(drop_rate)\n        self.layer_norm = nn.LayerNorm(model_dim)\n\n    def forward(self, query, k, v, mask=None):\n        # residual connect\n        # q: [4, 1, 768]\n        # k=v=[batch_size,seq_len, emb_dim]=[4, 3, 768]\n        residual = query    # \u6b8b\u5dee\n\n        # linear projection\n        key = self.wk(k)    # [batch_size,seq_len, num_heads * head_dim]\n        value = self.wv(v)  # [batch_size,seq_len, num_heads * head_dim]\n        query = self.wq(query)  # [batch_size,seq_len, num_heads * head_dim]\n\n        # \u5c06\u5934\u5206\u79bb\u51fa\u6765\n        # [step,n_head,n,head_dim] = [batch_size,\u5934\u7684\u6570\u91cf\uff0cseq_len,\u6bcf\u4e2a\u5934\u7684\u7ef4\u5ea6]\n        query = self.split_heads(query) # [4,1,8,96]\n        key = self.split_heads(key)     # [4,3,8,96]\n        value = self.split_heads(value) # [4,3,8,96]\n        \n        # \u81ea\u6ce8\u610f\u529b\u673a\u5236 \u70b9\u4e58 \n        context = self.scaled_dot_product_attention(\n            query, key, value, mask)    # [batch_size,seq_len, model_dim]\n\n        # \u518d\u7ecf\u8fc7\u4e00\u4e2a\u7ebf\u6027\u53d8\u5316\n        o = self.o_dense(context)       # [batch_size,seq_len, model_dim]\n        # \u968f\u673a\u4f7f\u5f97\u4e00\u4e9b\u6743\u91cd\u5931\u6548\n        o = self.o_drop(o)\n        # layer normalization\n        o = self.layer_norm(residual+o)\n        return o\n\n    def split_heads(self, x):\n        x = torch.reshape(\n            x, (x.shape[0], x.shape[1], self.n_head, self.head_dim))\n        # x = [step,n_head,n,head_dim]\n        return x.permute(0, 2, 1, 3)\n\n    def scaled_dot_product_attention(self, query, k, v, mask=None):\n        # query: [4, 8, 1, 96]\n        # k=v: [4, 8, 3, 96]\n        dk = torch.tensor(k.shape[-1]).type(torch.float) ##96\n        # import pdb; pdb.set_trace()\n        score = torch.matmul(query, k.permute(0, 1, 3, 2)) / (torch.sqrt(dk) + 1e-8)                 # [step, n_head, n, n]=[32, 4, 11, 11]\n        if mask is not None:\n            score = score.masked_fill_(mask, -np.inf) ##[4, 8, 1, 3]\n        self.attention = torch.softmax(score, dim=-1)    ##[4, 8, 1, 3]\n        context = torch.matmul(self.attention, v)   # [step, num_head, n, head_dim]: [4, 8, 1, 96]\n        context = context.permute(0, 2, 1, 3)       # [batch_size,seq_len, num_head, head_dim]: [4, 1, 8, 96]\n        context = context.reshape((context.shape[0], context.shape[1], -1)) ##[4, 1, 768]\n        return context                              # [batch_size,seq_len, model_dim]", "\n\n\n\nif __name__ == \"__main__\":\n    attention = Attention_for_Senti_Prompt()\n    device = torch.device('cuda:0' )\n    query = torch.randn(4, 1, 768)\n    key = torch.randn(4, 3, 768)\n    value = key\n\n    xx = attention(query, key, value)\n    print(xx.shape)", ""]}
{"filename": "src/model/MAESC_model_for_generated_senti_prompt.py", "chunked_list": ["from typing import Optional, Tuple\nfrom fastNLP.modules.torch.encoder import Seq2SeqEncoder\nfrom fastNLP.modules.torch.decoder import Seq2SeqDecoder\nfrom fastNLP.modules.torch import State\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom src.model.modeling_bart import (PretrainedBartModel, BartEncoder,\n                                     BartDecoder, BartModel,\n                                     BartClassificationHead,", "                                     BartDecoder, BartModel,\n                                     BartClassificationHead,\n                                     _make_linear_from_emb,\n                                     _prepare_bart_decoder_inputs)\nfrom transformers import BartTokenizer\n\nfrom src.model.config import MultiModalBartConfig\n#from src.model.mixins import GenerationMixin, FromPretrainedMixin\nfrom src.model.modules_for_prompt import MultiModalBartEncoder, MultiModalBartDecoder_span, Span_loss, MultiModalBartEncoder_for_Generating_sentiment_prompt, MultiModalBartDecoder_generate_sentiment_prompt\n", "from src.model.modules_for_prompt import MultiModalBartEncoder, MultiModalBartDecoder_span, Span_loss, MultiModalBartEncoder_for_Generating_sentiment_prompt, MultiModalBartDecoder_generate_sentiment_prompt\n\n\n\nclass MultiModalBartModel_AESC(PretrainedBartModel):\n    def build_model(self,\n                    args,\n                    bart_model,\n                    tokenizer,\n                    label_ids,\n                    config,\n                    decoder_type=None,\n                    copy_gate=False,\n                    use_encoder_mlp=False,\n                    use_recur_pos=False,\n                    tag_first=False):\n        if args.bart_init:\n            model = BartModel.from_pretrained(bart_model)\n            num_tokens, _ = model.encoder.embed_tokens.weight.shape\n            print('num_tokens', num_tokens)\n\n            model.resize_token_embeddings(\n                len(tokenizer.unique_no_split_tokens) + num_tokens)\n            encoder = model.encoder\n            decoder = model.decoder\n\n            padding_idx = config.pad_token_id\n            encoder.embed_tokens.padding_idx = padding_idx\n\n            # if use_recur_pos:\n            #     decoder.set_position_embedding(label_ids[0], tag_first)\n\n            _tokenizer = BartTokenizer.from_pretrained(bart_model)\n\n            for token in tokenizer.unique_no_split_tokens:\n                if token[:2] == '<<':  # \u7279\u6b8a\u5b57\u7b26\n                    index = tokenizer.convert_tokens_to_ids(\n                        tokenizer._base_tokenizer.tokenize(token))\n                    if len(index) > 1:\n                        raise RuntimeError(f\"{token} wrong split\")\n                    else:\n                        index = index[0]\n                    assert index >= num_tokens, (index, num_tokens, token)\n                    indexes = _tokenizer.convert_tokens_to_ids(\n                        _tokenizer.tokenize(token[2:-2]))\n                    embed = model.encoder.embed_tokens.weight.data[indexes[0]]\n                    for i in indexes[1:]:\n                        embed += model.decoder.embed_tokens.weight.data[i]\n                    embed /= len(indexes)\n                    model.decoder.embed_tokens.weight.data[index] = embed\n        else:\n            raise RuntimeError(\"error init!!!!!!!\")\n\n        multimodal_encoder_for_generated_senti_prompt = MultiModalBartEncoder(config, encoder,\n                                                   tokenizer.img_feat_id,\n                                                   tokenizer.cls_token_id,\n                                                   args.num_image_tokens)\n\n        multimodal_encoder = MultiModalBartEncoder_for_Generating_sentiment_prompt(\n                                                                         use_generated_prompt=args.use_generated_prompt, \n                                                                         config=config,\n                                                                         encoder = encoder,\n                                                                         img_feat_id = tokenizer.img_feat_id,\n                                                                         aspect_prompt_token_id=tokenizer.aspect_prompt_token_id,\n                                                                         senti_prompt_token_id=tokenizer.senti_prompt_token_id,\n                                                                         cls_token_id = tokenizer.cls_token_id,\n                                                                         num_image_tokens = args.num_image_tokens,\n                                                                         use_different_senti_prompt=args.use_different_senti_prompt \n                                                  \n                                                   )\n        return (multimodal_encoder_for_generated_senti_prompt, multimodal_encoder, decoder)\n\n    def __init__(self, config: MultiModalBartConfig, args, bart_model,\n                 tokenizer, label_ids):\n        super().__init__(config)\n        self.config = config\n        self.tokenizer = tokenizer\n        label_ids = sorted(label_ids)\n        multimodal_encoder_for_generated_senti_prompt, multimodal_encoder, share_decoder = self.build_model(\n            args, bart_model, self.tokenizer, label_ids, config)\n        causal_mask = torch.zeros(512, 512).fill_(float('-inf'))\n        self.causal_mask = causal_mask.triu(diagonal=1)\n        self.num_image_tokens = args.num_image_tokens\n\n        self.senti_prompt_encoder = multimodal_encoder_for_generated_senti_prompt\n        self.encoder = multimodal_encoder\n\n        only_sc = False\n        # need_tag = True  #if predict the sentiment or not\n        if args.task == 'twitter_ae':\n            need_tag = False\n        else:\n            need_tag = True\n            # if args.task == 'twitter_sc':\n            #     only_sc = True\n\n        self.senti_prompt_decoder = MultiModalBartDecoder_generate_sentiment_prompt(self.config, share_decoder)\n\n        self.decoder = MultiModalBartDecoder_span(self.config,\n                                                  self.tokenizer,\n                                                  share_decoder,\n                                                  self.tokenizer.pad_token_id,\n                                                  label_ids,\n                                                  self.causal_mask,\n                                                  num_image_tokens=self.num_image_tokens,\n                                                  need_tag=need_tag,\n                                                  only_sc=False)\n        self.span_loss_fct = Span_loss()\n\n    def prepare_state(self,\n                      input_ids,\n                      image_features,\n                      attention_mask=None,\n                      aesc_infos=None,\n                      aspects_num=None,\n                      first=None):\n        ##generate prompt for each instance\n\n        prompt_attention_mask = attention_mask\n\n        if self.num_image_tokens==0:\n            end_index = 62\n            begin_index = 22\n        elif self.num_image_tokens==1:\n            end_index = 63\n            begin_index = 23\n        elif self.num_image_tokens==2:\n            end_index = 64\n            begin_index = 24\n        elif self.num_image_tokens==3:\n            end_index = 65\n            begin_index = 25\n        elif self.num_image_tokens==4:\n            end_index = 66\n            begin_index = 26\n        elif self.num_image_tokens==5:\n            end_index = 67\n            begin_index = 27\n        elif self.num_image_tokens==6:\n            end_index = 68\n            begin_index = 28\n        elif self.num_image_tokens==7:\n            end_index = 69\n            begin_index = 29\n            \n        \n        for i in range(len(prompt_attention_mask)):\n            mask = prompt_attention_mask[i]\n            mask[begin_index:end_index]=torch.zeros_like(mask[begin_index:end_index]) ##26:66 \u662faspect\u63d0\u793a\u7684\u4f4d\u7f6e\n            prompt_attention_mask[i]=mask\n        dict_for_prompt = self.senti_prompt_encoder(input_ids=input_ids,\n                                              image_features=image_features,\n                                              attention_mask=prompt_attention_mask,\n                                              output_hidden_states=True,\n                                              return_dict=True)\n\n        \n        prompt_decoder_input_ids, prompt_decoder_attention_mask = [\n            aesc_infos['senti_prompt_decoder_input_ids'].to(input_ids.device),\n            aesc_infos['senti_prompt_decoder_attention_mask'].to(input_ids.device)]\n        generated_prompt = self.senti_prompt_decoder(\n                                            encoder_outputs=dict_for_prompt.last_hidden_state, \n                                            attention_mask=attention_mask,\n                                            decoder_input_ids =prompt_decoder_input_ids, decoder_attention_mask=prompt_decoder_attention_mask)\n\n        generated_prompt = generated_prompt[:, 1:, :] ##(batch_size, 2, 768)\n\n        dict = self.encoder(\n                            input_ids=input_ids,\n                            image_features=image_features,\n                            attention_mask=attention_mask,\n                            generated_prompt= generated_prompt,\n                            aspects_num = aspects_num,\n                            output_hidden_states=True,\n                            return_dict=True)\n\n\n        encoder_outputs = dict.last_hidden_state\n        hidden_states = dict.hidden_states\n        encoder_mask = attention_mask\n        src_embed_outputs = hidden_states[0]\n        state = BartState(\n            encoder_outputs,\n            encoder_mask,\n            input_ids[:,\n                      end_index:],  #the text features start from index 38, the front are image features.\n            first,\n            src_embed_outputs)\n        # setattr(state, 'tgt_seq_len', tgt_seq_len)\n        return state\n\n    def forward(\n            self,\n            input_ids,\n            image_features,\n            attention_mask=None,\n            aesc_infos=None,\n            aspects_num=None,\n            encoder_outputs: Optional[Tuple] = None,\n            use_cache=None,\n            output_attentions=None,\n            output_hidden_states=None,\n    ):\n        ### for prompt\n        # import ipdb; ipdb.set_trace()\n       \n        ## for aspect-spans\n      \n        aspects_num = torch.tensor(aspects_num).to(input_ids.device)\n        state = self.prepare_state( input_ids, image_features, attention_mask, aesc_infos, aspects_num)\n        spans, span_mask = [ \n            aesc_infos['labels'].to(input_ids.device),\n            aesc_infos['masks'].to(input_ids.device)\n        ]\n\n        logits = self.decoder(spans, state) ## spans: (2, 13) logits: (2, 12, 40)\n\n        loss = self.span_loss_fct(spans[:, 1:], logits, span_mask[:, 1:])\n\n        return loss", "\n\nclass BartState(State):\n    def __init__(self, encoder_output, encoder_mask, src_tokens, first,\n                 src_embed_outputs):\n        super().__init__(encoder_output, encoder_mask)\n        self.past_key_values = None\n        self.src_tokens = src_tokens\n        self.first = first\n        self.src_embed_outputs = src_embed_outputs\n\n    def reorder_state(self, indices: torch.LongTensor):\n        super().reorder_state(indices)\n        self.src_tokens = self._reorder_state(self.src_tokens, indices)\n        if self.first is not None:\n            self.first = self._reorder_state(self.first, indices)\n        self.src_embed_outputs = self._reorder_state(self.src_embed_outputs,\n                                                     indices)\n        if self.past_key_values is not None:\n            new = []\n            for layer in self.past_key_values:\n                new_layer = {}\n                for key1 in list(layer.keys()):\n                    new_layer_ = {}\n                    for key2 in list(layer[key1].keys()):\n                        if layer[key1][key2] is not None:\n                            layer[key1][key2] = self._reorder_state(\n                                layer[key1][key2], indices)\n                            # print(key1, key2, layer[key1][key2].shape)\n                        new_layer_[key2] = layer[key1][key2]\n                    new_layer[key1] = new_layer_\n                new.append(new_layer)\n            self.past_key_values = new", ""]}
