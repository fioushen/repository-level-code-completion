{"filename": "runner.py", "chunked_list": ["import os\nimport time\nimport datetime\nimport yaml\nimport git\n\nimport torch\nimport torch.backends.cudnn as cudnn\n\nfrom timm.utils import AverageMeter", "\nfrom timm.utils import AverageMeter\n\nfrom utils import load_checkpoint, load_pretrained, save_checkpoint, save_image_torch, get_grad_norm\nfrom utils.config import parse_options, copy_cfg, ordered_dict_to_dict\nfrom utils.scheduler import build_scheduler\nfrom utils.optimizer import build_optimizer\nfrom utils.metrics import get_psnr_torch, get_ssim_torch\nfrom utils.loss import build_loss\nfrom utils.logger import create_logger", "from utils.loss import build_loss\nfrom utils.logger import create_logger\n\nfrom models import build_model\nfrom datasets import build_train_loader, build_valid_loader, build_test_loader\nfrom forwards import build_forwards, build_profile\n\nfrom torch.utils.tensorboard import SummaryWriter\n\ndef main(config):\n    writer = SummaryWriter(os.path.join(config['output'], 'tensorboard'))\n    train_dataloader = build_train_loader(config['data'])\n    if not config['testset_as_validset']:\n        valid_dataloader =  build_valid_loader(config['data'], 1)\n    else:\n        valid_dataloader = build_test_loader(config['data'], 2)\n\n    logger.info(f\"Creating model:{config['name']}/{config['model']['type']}\")\n    model = build_model(config['model'])\n    model.cuda()\n    logger.info(str(model))\n    profile_forward = build_profile(config)\n    profile_model(config, profile_forward, model, train_dataloader, logger)\n\n    optimizer = build_optimizer(config['train'], model)\n    lr_scheduler = build_scheduler(config['train'], optimizer, len(train_dataloader))\n    loss_list = build_loss(config['loss'])\n    logger.info(str(loss_list))\n    \n    logger.info('Building forwards:')\n    logger.info(f'Train forward: {config[\"train\"][\"forward_type\"]}')\n    logger.info(f'Test forward: {config[\"test\"][\"forward_type\"]}')\n    train_forward, test_forward = build_forwards(config)\n\n    max_psnr = 0.0\n    max_ssim = 0.0\n    total_epochs = config['train']['early_stop'] if config['train']['early_stop'] is not None else config['train']['epochs']\n\n    if config.get('throughput_mode', False):\n        throughput(config, train_forward, model, valid_dataloader, logger)\n        return\n\n    # set auto resume\n    if config['train']['auto_resume']:\n        auto_resume_path = os.path.join(config['output'], 'checkpoints', 'checkpoint.pth')\n        if os.path.exists(auto_resume_path):\n            config['train']['resume'] = auto_resume_path\n            logger.info(f'Auto resume: setting resume path to {auto_resume_path}')\n    \n    if config['train'].get('resume'):\n        max_psnr = load_checkpoint(config, model, optimizer, lr_scheduler, logger)\n        validate(config, test_forward, model, loss_list, valid_dataloader, config['train'].get('start_epoch', 0), writer)\n        if config.get('eval_mode', False):\n            return\n\n    if config['train'].get('pretrained') and (not config['train'].get('resume')):\n        load_pretrained(config, model, logger)\n        validate(config, test_forward, model, loss_list, valid_dataloader, config['train'].get('start_epoch', 0), writer)\n        if config.get('eval_mode', False):\n            return\n\n    logger.info(\"Start training\")\n    start_time = time.time()\n    start = time.time()\n    lr_scheduler.step(config['train'].get('start_epoch', 0))\n\n    for epoch in range(config['train'].get('start_epoch', 0)+1, total_epochs+1):\n        train_one_epoch(config, train_forward, model, loss_list, train_dataloader, optimizer, None, epoch, lr_scheduler, writer)\n        if epoch % config['valid_per_epoch'] == 0 or (total_epochs - epoch) < 50:\n            psnr, ssim, loss = validate(config, test_forward, model, loss_list, valid_dataloader, epoch, writer)\n            max_psnr = max(max_psnr, psnr)\n            max_ssim = max(max_ssim, ssim)\n            writer.add_scalar('eval/max_psnr', max_psnr, epoch)\n            writer.add_scalar('eval/max_ssim', max_ssim, epoch)\n        else:\n            psnr = 0\n        save_checkpoint(config, epoch, model, max_psnr, optimizer, lr_scheduler, logger, is_best=(max_psnr==psnr))\n        logger.info(f'Train: [{epoch}/{config[\"train\"][\"epochs\"]}] Max Valid PSNR: {max_psnr:.4f}, Max Valid SSIM: {max_ssim:.4f}')\n        logger.info(f\"Train: [{epoch}/{config['train']['epochs']}] Total Time {datetime.timedelta(seconds=int(time.time()-start))}\")\n        start = time.time()\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info('Training time {}'.format(total_time_str))", "\ndef main(config):\n    writer = SummaryWriter(os.path.join(config['output'], 'tensorboard'))\n    train_dataloader = build_train_loader(config['data'])\n    if not config['testset_as_validset']:\n        valid_dataloader =  build_valid_loader(config['data'], 1)\n    else:\n        valid_dataloader = build_test_loader(config['data'], 2)\n\n    logger.info(f\"Creating model:{config['name']}/{config['model']['type']}\")\n    model = build_model(config['model'])\n    model.cuda()\n    logger.info(str(model))\n    profile_forward = build_profile(config)\n    profile_model(config, profile_forward, model, train_dataloader, logger)\n\n    optimizer = build_optimizer(config['train'], model)\n    lr_scheduler = build_scheduler(config['train'], optimizer, len(train_dataloader))\n    loss_list = build_loss(config['loss'])\n    logger.info(str(loss_list))\n    \n    logger.info('Building forwards:')\n    logger.info(f'Train forward: {config[\"train\"][\"forward_type\"]}')\n    logger.info(f'Test forward: {config[\"test\"][\"forward_type\"]}')\n    train_forward, test_forward = build_forwards(config)\n\n    max_psnr = 0.0\n    max_ssim = 0.0\n    total_epochs = config['train']['early_stop'] if config['train']['early_stop'] is not None else config['train']['epochs']\n\n    if config.get('throughput_mode', False):\n        throughput(config, train_forward, model, valid_dataloader, logger)\n        return\n\n    # set auto resume\n    if config['train']['auto_resume']:\n        auto_resume_path = os.path.join(config['output'], 'checkpoints', 'checkpoint.pth')\n        if os.path.exists(auto_resume_path):\n            config['train']['resume'] = auto_resume_path\n            logger.info(f'Auto resume: setting resume path to {auto_resume_path}')\n    \n    if config['train'].get('resume'):\n        max_psnr = load_checkpoint(config, model, optimizer, lr_scheduler, logger)\n        validate(config, test_forward, model, loss_list, valid_dataloader, config['train'].get('start_epoch', 0), writer)\n        if config.get('eval_mode', False):\n            return\n\n    if config['train'].get('pretrained') and (not config['train'].get('resume')):\n        load_pretrained(config, model, logger)\n        validate(config, test_forward, model, loss_list, valid_dataloader, config['train'].get('start_epoch', 0), writer)\n        if config.get('eval_mode', False):\n            return\n\n    logger.info(\"Start training\")\n    start_time = time.time()\n    start = time.time()\n    lr_scheduler.step(config['train'].get('start_epoch', 0))\n\n    for epoch in range(config['train'].get('start_epoch', 0)+1, total_epochs+1):\n        train_one_epoch(config, train_forward, model, loss_list, train_dataloader, optimizer, None, epoch, lr_scheduler, writer)\n        if epoch % config['valid_per_epoch'] == 0 or (total_epochs - epoch) < 50:\n            psnr, ssim, loss = validate(config, test_forward, model, loss_list, valid_dataloader, epoch, writer)\n            max_psnr = max(max_psnr, psnr)\n            max_ssim = max(max_ssim, ssim)\n            writer.add_scalar('eval/max_psnr', max_psnr, epoch)\n            writer.add_scalar('eval/max_ssim', max_ssim, epoch)\n        else:\n            psnr = 0\n        save_checkpoint(config, epoch, model, max_psnr, optimizer, lr_scheduler, logger, is_best=(max_psnr==psnr))\n        logger.info(f'Train: [{epoch}/{config[\"train\"][\"epochs\"]}] Max Valid PSNR: {max_psnr:.4f}, Max Valid SSIM: {max_ssim:.4f}')\n        logger.info(f\"Train: [{epoch}/{config['train']['epochs']}] Total Time {datetime.timedelta(seconds=int(time.time()-start))}\")\n        start = time.time()\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info('Training time {}'.format(total_time_str))", "\n@torch.no_grad()\ndef profile_model(config, profile_forward, model, data_loader, logger):\n    if profile_forward is not None:\n        data_iter = iter(data_loader)\n        data = next(data_iter)\n        del data_iter\n        profile_forward(config, model, data, logger)\n\n    n_parameters = sum(p.numel() for p in model.parameters())\n    logger.info(f\"Total Params: {n_parameters:,}\")", "\ndef train_one_epoch(config, train_forward, model, loss_list, data_loader, optimizer, scaler, epoch, lr_scheduler, writer):\n    torch.cuda.reset_peak_memory_stats()\n    model.train()\n    optimizer.zero_grad()\n\n    num_steps = len(data_loader)\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    loss_meter = AverageMeter()\n    norm_meter = AverageMeter()\n    losses_count = len(loss_list)\n    losses_meter = [AverageMeter() for _ in range(losses_count)]\n\n    start = time.time()\n    end = time.time()\n    for idx, data in enumerate(data_loader):\n        data_time.update(time.time() - end)\n\n        outputs, targets = train_forward(config, model, data)\n\n        losses = loss_list(outputs, targets)\n        loss = sum(losses)\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        if config['train'].get('clip_grad'):\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config['train']['clip_grad'])\n        else:\n            grad_norm = get_grad_norm(model.parameters())\n\n        optimizer.step()\n\n        if not config['train']['lr_scheduler']['t_in_epochs']:\n            lr_scheduler.step_update((epoch-1)*num_steps+idx)\n\n        batch_size = list(targets.values())[0].size(0)\n        for _loss_meter, _loss in zip(losses_meter, losses):\n            _loss_meter.update(_loss.item(), batch_size)   \n        loss_meter.update(loss.item(), batch_size)\n        norm_meter.update(grad_norm)\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config['print_per_iter'] == 0 or idx == num_steps:\n            lr = optimizer.param_groups[0]['lr']\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            etas = batch_time.avg * (num_steps - idx)\n            logger.info(\n                f'Train: [{epoch}/{config[\"train\"][\"epochs\"]}][{idx}/{num_steps}]\\t'\n                f'ETA {datetime.timedelta(seconds=int(etas))} LR {lr:.6f}\\t'\n                f'Time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n                f'Loss {loss_meter.val:.8f} ({loss_meter.avg:.8f})\\t'\n                f'GradNorm {norm_meter.val:.4f} ({norm_meter.avg:.4f})\\t'\n                f'Mem {memory_used:.0f}MB')\n    if config['train']['lr_scheduler']['t_in_epochs']:\n        lr_scheduler.step(epoch)\n    logger.info(f\"Train: [{epoch}/{config['train']['epochs']}] Time {datetime.timedelta(seconds=int(time.time()-start))}\")\n    tensor_board_dict = {'train/loss_total':loss_meter.avg}\n    for index, (_loss, _loss_meter) in enumerate(zip(losses, losses_meter)):\n        tensor_board_dict[f'train/loss_{index}'] = _loss_meter.avg\n    for log_key, log_value in tensor_board_dict.items():\n        writer.add_scalar(log_key, log_value, epoch)", "\n@torch.no_grad()\ndef validate(config, test_forward, model, loss_list, data_loader, epoch, writer):\n    torch.cuda.reset_max_memory_allocated()\n    model.eval()\n\n    logger.info(f\"Valid: [{epoch}/{config['train']['epochs']}]\\t\")\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    loss_meter = AverageMeter()\n    psnr_meter = AverageMeter()\n    ssim_meter = AverageMeter()\n    losses_count = len(loss_list)\n    losses_meter = [AverageMeter() for _ in range(losses_count)]\n\n    start = time.time()\n    end = time.time()\n    for idx, data in enumerate(data_loader):\n        data_time.update(time.time() - end)\n        outputs, targets, img_files, lbl_files = test_forward(config, model, data)\n\n        if config['testset_as_validset']:\n            psnr, ssim = test_metric_cuda(config, epoch, outputs[config['test']['which_stage']], targets[config['test']['which_gt']], img_files, lbl_files)\n        else:\n            psnr, ssim = validate_metric(config, epoch, outputs[config['test']['which_stage']], targets[config['test']['which_gt']], img_files, lbl_files)\n\n        losses = loss_list(outputs, targets)\n        loss = sum(losses)\n        batch_size = targets[config['test']['which_gt']].size(0)\n        for _loss_meter, _loss in zip(losses_meter, losses):\n            _loss_meter.update(_loss.item(), batch_size)   \n        loss_meter.update(loss.item(), batch_size)\n        psnr_meter.update(psnr.item(), batch_size)\n        ssim_meter.update(ssim.item(), batch_size)\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if config['testset_as_validset'] or idx % config['print_per_iter'] == 0 or idx == len(data_loader):\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            logger.info(\n                f'Valid: [{epoch}/{config[\"train\"][\"epochs\"]}][{idx}/{len(data_loader)}]\\t'\n                f'Time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n                f'Loss {loss_meter.val:.8f} ({loss_meter.avg:.8f})\\t'\n                f'PSNR {psnr_meter.val:.4f} ({psnr_meter.avg:.4f})\\t'\n                f'SSIM {ssim_meter.val:.4f} ({ssim_meter.avg:.4f})\\t'\n                f'Mem {memory_used:.0f}MB\\t{os.path.basename(img_files[0])}')\n    logger.info(f'Valid: [{epoch}/{config[\"train\"][\"epochs\"]}] PSNR {psnr_meter.avg:.4f}\\tSSIM {ssim_meter.avg:.4f}')\n    logger.info(f'Valid: [{epoch}/{config[\"train\"][\"epochs\"]}] Time {datetime.timedelta(seconds=int(time.time()-start))}')\n    tensor_board_dict = {'eval/loss_total':loss_meter.avg}\n    for index, (loss, loss_meter) in enumerate(zip(losses, losses_meter)):\n        tensor_board_dict[f'eval/loss_{index}'] = loss_meter.avg\n    tensor_board_dict['eval/psnr'] = psnr_meter.avg\n    tensor_board_dict['eval/ssim'] = ssim_meter.avg\n    for log_key, log_value in tensor_board_dict.items():\n        writer.add_scalar(log_key, log_value, epoch)\n    return psnr_meter.avg, ssim_meter.avg, loss_meter.avg", "\n@torch.no_grad()\ndef validate_metric(config, epoch, outputs, targets, image_paths, target_params=None):\n    outputs = torch.clamp(outputs, 0, 1) * 255\n    targets = targets * 255\n    if config['test']['round']:\n        outputs = outputs.round()\n        targets = targets.round()\n    psnrs = get_psnr_torch(outputs, targets)\n    ssims = get_ssim_torch(outputs, targets)\n\n    if config['test']['save_image'] and epoch % config['save_per_epoch'] == 0:\n        images = torch.cat((outputs, targets), dim=3)\n        result_path = os.path.join(config['output'], 'results', f'valid_{epoch:04d}')\n        os.makedirs(result_path, exist_ok=True)\n        for image, image_path, psnr in zip(images, image_paths, psnrs):\n            save_path = os.path.join(result_path, f'{os.path.basename(image_path)[:-4]}_{psnr:.2f}.jpg')\n            save_image_torch(image, save_path)\n\n    return psnrs.mean(), ssims.mean()", "\n@torch.no_grad()\ndef test_metric_cuda(config, epoch, outputs, targets, image_paths, target_params=None):\n    outputs = torch.clamp(outputs, 0, 1) * 255\n    targets = torch.clamp(targets, 0, 1) * 255\n    if config['test']['round']:\n        outputs = outputs.round()\n        targets = targets.round()\n    psnr = get_psnr_torch(outputs, targets)\n    ssim = get_ssim_torch(outputs, targets)\n\n    if config['test']['save_image']:\n        result_path = os.path.join(config['output'], 'results', f'test_{epoch:04d}')\n        os.makedirs(result_path, exist_ok=True)\n        save_path = os.path.join(result_path, f'{os.path.basename(image_paths[0])[:-4]}_{psnr.item():.2f}.png')\n        save_image_torch(outputs[0], save_path)\n\n    return psnr, ssim", "\n@torch.no_grad()\ndef throughput(config, forward, model, data_loader, logger):\n    model.eval()\n\n    for idx, data in enumerate(data_loader):\n        for i in range(30):\n            forward(config, model, data)\n        logger.info(f\"throughput averaged with 100 times\")\n        torch.cuda.synchronize()\n        tic = time.time()\n        for i in range(100):\n            pred, label = forward(config, model, data)\n        batch_size = list(pred.values())[0].size(0)\n        torch.cuda.synchronize()\n        toc = time.time()\n        logger.info(f\"batch_size {batch_size} throughput {(toc - tic) * 1000 / (100 * batch_size)}ms\")\n        return", "\n\nif __name__ == '__main__':\n    args, config = parse_options()\n    phase = 'train' if not args.test else 'test'\n\n    cudnn.benchmark = True\n\n    os.makedirs(config['output'], exist_ok=True)\n    start_time = time.strftime(\"%y%m%d-%H%M\", time.localtime())\n    logger = create_logger(output_dir=config['output'], name=f\"{config['tag']}\", action=f\"{phase}-{start_time}\")\n    path = os.path.join(config['output'], f\"{phase}-{start_time}.yaml\")\n    \n    try:\n        repo = git.Repo(search_parent_directories=True)\n        sha = repo.head.object.hexsha\n        if repo.is_dirty():\n            logger.warning(f'Current work on commit: {sha}, however the repo is dirty (not committed)!')\n        else:\n            logger.info(f'Current work on commit: {sha}.')\n    except git.exc.InvalidGitRepositoryError as e:\n        logger.warn(f'No git repo base.')\n\n    copy_cfg(config, path)\n    logger.info(f\"Full config saved to {path}\")\n\n    # print config\n    logger.info(\"Config:\\n\" + yaml.dump(ordered_dict_to_dict(config), default_flow_style=False, sort_keys=False))\n    current_cuda_device = torch.cuda.get_device_properties(torch.cuda.current_device())\n    logger.info(f\"Current CUDA Device: {current_cuda_device.name}, Total Mem: {int(current_cuda_device.total_memory / 1024 / 1024)}MB\")\n\n    main(config)", ""]}
{"filename": "utils/registry.py", "chunked_list": ["# Modified from: https://github.com/facebookresearch/fvcore/blob/master/fvcore/common/registry.py  # noqa: E501\n\n\nclass Registry():\n    \"\"\"\n    The registry that provides name -> object mapping, to support third-party\n    users' custom modules.\n\n    To create a registry (e.g. a backbone registry):\n\n    .. code-block:: python\n\n        BACKBONE_REGISTRY = Registry('BACKBONE')\n\n    To register an object:\n\n    .. code-block:: python\n\n        @BACKBONE_REGISTRY.register()\n        class MyBackbone():\n            ...\n\n    Or:\n\n    .. code-block:: python\n\n        BACKBONE_REGISTRY.register(MyBackbone)\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"\n        Args:\n            name (str): the name of this registry\n        \"\"\"\n        self._name = name\n        self._obj_map = {}\n\n    def _do_register(self, name, obj, suffix=None):\n        if isinstance(suffix, str):\n            name = name + '_' + suffix\n\n        assert (name not in self._obj_map), (f\"An object named '{name}' was already registered \"\n                                             f\"in '{self._name}' registry!\")\n        self._obj_map[name] = obj\n\n    def register(self, obj=None, suffix=None):\n        \"\"\"\n        Register the given object under the the name `obj.__name__`.\n        Can be used as either a decorator or not.\n        See docstring of this class for usage.\n        \"\"\"\n        if obj is None:\n            # used as a decorator\n            def deco(func_or_class):\n                name = func_or_class.__name__\n                self._do_register(name, func_or_class, suffix)\n                return func_or_class\n\n            return deco\n\n        # used as a function call\n        name = obj.__name__\n        self._do_register(name, obj, suffix)\n\n    def get(self, name, suffix='basicsr'):\n        ret = self._obj_map.get(name)\n        if ret is None:\n            ret = self._obj_map.get(name + '_' + suffix)\n            print(f'Name {name} is not found, use name: {name}_{suffix}!')\n        if ret is None:\n            raise KeyError(f\"No object named '{name}' found in '{self._name}' registry!\")\n        return ret\n\n    def __contains__(self, name):\n        return name in self._obj_map\n\n    def __iter__(self):\n        return iter(self._obj_map.items())\n\n    def keys(self):\n        return self._obj_map.keys()", "\n\nDATASET_REGISTRY = Registry('dataset')\nMODEL_REGISTRY = Registry('model')\nFORWARD_REGISTRY = Registry('forward')\n"]}
{"filename": "utils/loss.py", "chunked_list": ["import torch.nn as nn\nfrom torch.nn import MSELoss, L1Loss\n\nclass Losses(nn.Module):\n    def __init__(self, classes, names, weights, positions, gt_positions):\n        super().__init__()\n        self.module_list = nn.ModuleList()\n        self.names = names\n        self.weights = weights\n        self.positions = positions\n        self.gt_positions = gt_positions\n        for class_name in classes:\n            module_class = eval(class_name)\n            self.module_list.append(module_class())\n\n    def __len__(self):\n        return len(self.names)\n\n    def forward(self, outputs, targets):\n        losses = []\n        for i in range(len(self.names)):\n            loss = self.module_list[i](outputs[self.positions[i]], targets[self.gt_positions[i]]) * self.weights[i]\n            losses.append(loss)\n        return losses", "\ndef build_loss(config):\n    loss_names = config['types']\n    loss_classes = config['classes']\n    loss_weights = config['weights']\n    loss_positions = config['which_stage']\n    loss_gt_positions = config['which_gt']\n    assert len(loss_names) == len(loss_weights) == \\\n           len(loss_classes) == len(loss_positions) == \\\n           len(loss_gt_positions)\n    criterion = Losses(classes=loss_classes, names=loss_names,\n                          weights=loss_weights, positions=loss_positions,\n                          gt_positions=loss_gt_positions)\n    return criterion", ""]}
{"filename": "utils/config.py", "chunked_list": ["import argparse\nimport os\nimport random\nimport torch\nimport yaml\nfrom collections import OrderedDict\nfrom os import path as osp\nimport numpy as np\nfrom copy import deepcopy\n", "from copy import deepcopy\n\n\ndef set_random_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\ndef ordered_yaml():\n    \"\"\"Support OrderedDict for yaml.\n\n    Returns:\n        tuple: yaml Loader and Dumper.\n    \"\"\"\n    try:\n        from yaml import CDumper as Dumper\n        from yaml import CLoader as Loader\n    except ImportError:\n        from yaml import Dumper, Loader\n\n    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n\n    def dict_representer(dumper, data):\n        return dumper.represent_dict(data.items())\n\n    def dict_constructor(loader, node):\n        return OrderedDict(loader.construct_pairs(node))\n\n    Dumper.add_representer(OrderedDict, dict_representer)\n    Loader.add_constructor(_mapping_tag, dict_constructor)\n    return Loader, Dumper", "\n\ndef ordered_yaml():\n    \"\"\"Support OrderedDict for yaml.\n\n    Returns:\n        tuple: yaml Loader and Dumper.\n    \"\"\"\n    try:\n        from yaml import CDumper as Dumper\n        from yaml import CLoader as Loader\n    except ImportError:\n        from yaml import Dumper, Loader\n\n    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n\n    def dict_representer(dumper, data):\n        return dumper.represent_dict(data.items())\n\n    def dict_constructor(loader, node):\n        return OrderedDict(loader.construct_pairs(node))\n\n    Dumper.add_representer(OrderedDict, dict_representer)\n    Loader.add_constructor(_mapping_tag, dict_constructor)\n    return Loader, Dumper", "\n\ndef yaml_load(f):\n    \"\"\"Load yaml file or string.\n\n    Args:\n        f (str): File path or a python string.\n\n    Returns:\n        dict: Loaded dict.\n    \"\"\"\n    if os.path.isfile(f):\n        with open(f, 'r') as f:\n            return yaml.load(f, Loader=ordered_yaml()[0])\n    else:\n        return yaml.load(f, Loader=ordered_yaml()[0])", "\n\ndef dict2str(opt, indent_level=1):\n    \"\"\"dict to string for printing options.\n\n    Args:\n        opt (dict): Option dict.\n        indent_level (int): Indent level. Default: 1.\n\n    Return:\n        (str): Option string for printing.\n    \"\"\"\n    msg = '\\n'\n    for k, v in opt.items():\n        if isinstance(v, dict):\n            msg += ' ' * (indent_level * 2) + k + ':['\n            msg += dict2str(v, indent_level + 1)\n            msg += ' ' * (indent_level * 2) + ']\\n'\n        else:\n            msg += ' ' * (indent_level * 2) + k + ': ' + str(v) + '\\n'\n    return msg", "\n\ndef _postprocess_yml_value(value):\n    # None\n    if value == '~' or value.lower() == 'none':\n        return None\n    # bool\n    if value.lower() == 'true':\n        return True\n    elif value.lower() == 'false':\n        return False\n    # !!float number\n    if value.startswith('!!float'):\n        return float(value.replace('!!float', ''))\n    # number\n    if value.isdigit():\n        return int(value)\n    elif value.replace('.', '', 1).isdigit() and value.count('.') < 2:\n        return float(value)\n    # list\n    if value.startswith('['):\n        return eval(value)\n    # str\n    return value", "\n\ndef merge_from_base(cfg, cfg_path):\n    def _merge_a_into_b(cfg_a, cfg_b):\n        for k, v_ in cfg_a.items():\n            v = deepcopy(v_)\n            if isinstance(v, dict) and k in cfg_b:\n                _merge_a_into_b(v, cfg_b[k])\n            else:\n                cfg_b[k] = v\n\n    if 'base' in cfg:\n        if isinstance(cfg['base'], str):\n            cfg['base'] = [ cfg['base'] ]\n        for base_cfg_path in cfg['base']:\n            full_base_cfg_path = os.path.join(os.path.dirname(cfg_path), base_cfg_path)\n            base_cfg = yaml_load(full_base_cfg_path)\n            base_cfg = merge_from_base(base_cfg, full_base_cfg_path)\n            _merge_a_into_b(cfg, base_cfg)\n            \n        return base_cfg\n    return cfg", "            \n            \ndef set_default_config(cfg):\n    if 'train' not in cfg:\n        cfg['train'] = {}\n    \n    ##### default #####\n    if 'output' not in cfg:\n        cfg['output'] = 'runs'\n    if 'tag' not in cfg:\n        cfg['tag'] = 'debug'\n    \n    ##### data #####\n    if 'persistent_workers' not in cfg['data']:\n        cfg['data']['persistent_workers'] = False\n    if 'train' in cfg['data'] and 'repeat' not in cfg['data']['train']:\n        cfg['data']['train']['repeat'] = 1\n    # augmentation \n    if 'transpose' not in cfg['data']['process']:\n        cfg['data']['process']['transpose'] = False\n    if 'h_flip' not in cfg['data']['process']:\n        cfg['data']['process']['h_flip'] = True\n    if 'v_flip' not in cfg['data']['process']:\n        cfg['data']['process']['v_flip'] = True\n    if 'rotation' not in cfg['data']['process']:\n        cfg['data']['process']['rotation'] = False\n        \n    ##### train #####\n    if 'auto_resume' not in cfg['train']:\n        cfg['train']['auto_resume'] = False\n        \n    ##### test #####\n    if 'test' in cfg:\n        if 'round' not in cfg['test']:\n            cfg['test']['round'] = False\n        if 'save_image' not in cfg['test']:\n            cfg['test']['save_image'] = False\n        \n    cfg['output'] = os.path.join(cfg.get('output', 'runs'), cfg['name'], cfg.get('tag', ''))", "\n\ndef parse_options():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-cfg', type=str, required=True, help='Path to option YAML file.')\n    parser.add_argument('--auto-resume', action='store_true', default=False, help='Auto resume from latest checkpoint')\n    parser.add_argument('--resume', type=str, default=None, help='Path to resume.')\n    parser.add_argument('--pretrain', type=str, default=None, help='Path to the pretrained checkpoint path.')\n    parser.add_argument('--test', action='store_true', default=False, help='Test mode')\n    parser.add_argument('--save-image', action='store_true', default=False, help='Save image during test or validation')\n    parser.add_argument(\n        '--force-yml', nargs='+', default=None, help='Force to update yml files. Examples: train:ema_decay=0.999')\n    args = parser.parse_args()\n\n    # parse yml to dict\n    cfg = yaml_load(args.cfg)\n    cfg = merge_from_base(cfg, args.cfg)\n    set_default_config(cfg)\n\n    # random seed\n    seed = cfg.get('manual_seed')\n    if seed is None:\n        seed = random.randint(1, 10000)\n        cfg['manual_seed'] = seed\n    set_random_seed(seed)\n    \n    if args.test:\n        assert not args.auto_resume and \\\n            (args.resume is not None or cfg['train'].get('resume') is not None) or \\\n            (args.pretrain is not None or cfg['train'].get('pretrained') is not None)\n        cfg['testset_as_validset'] = True\n        cfg['eval_mode'] = True\n    \n    if args.auto_resume:\n        assert args.resume is None\n        cfg['train']['auto_resume'] = True\n\n    if args.resume:\n        assert args.pretrain is None\n        cfg['train']['resume'] = args.resume\n        \n    if args.pretrain:\n        cfg['train']['pretrained'] = args.pretrain\n\n    if args.save_image:\n        cfg['test']['save_image'] = True\n\n    # force to update yml options\n    if args.force_yml is not None:\n        for entry in args.force_yml:\n            # now do not support creating new keys\n            keys, value = entry.split('=')\n            keys, value = keys.strip(), value.strip()\n            value = _postprocess_yml_value(value)\n            eval_str = 'cfg'\n            for key in keys.split(':'):\n                eval_str += f'[\"{key}\"]'\n            eval_str += '=value'\n            # using exec function\n            exec(eval_str)\n\n    return args, cfg", "\n\ndef copy_cfg(cfg, filename):\n    # copy the yml file to the experiment root\n    import sys\n    import time\n    from shutil import copyfile\n    cmd = ' '.join(sys.argv)\n\n    with open(filename, 'w') as f:\n        lines = [f'# GENERATE TIME: {time.asctime()}\\n# CMD:\\n# {cmd}\\n\\n']\n        lines.append(yaml.dump(ordered_dict_to_dict(cfg), default_flow_style=False, sort_keys=False))\n        f.writelines(lines)", "\n\ndef ordered_dict_to_dict(cfg):\n    cfg_dict = {}\n    for k, v in cfg.items():\n        if isinstance(v, OrderedDict):\n            cfg_dict[k] = ordered_dict_to_dict(deepcopy(v))\n        else:\n            cfg_dict[k] = v\n    return cfg_dict", ""]}
{"filename": "utils/scheduler.py", "chunked_list": ["from timm.scheduler.cosine_lr import CosineLRScheduler\nfrom timm.scheduler.step_lr import StepLRScheduler\n\ndef build_scheduler(config, optimizer, n_iter_per_epoch=1):\n    if config['lr_scheduler']['t_in_epochs']:\n        n_iter_per_epoch = 1\n    num_steps = int(config['epochs'] * n_iter_per_epoch)\n    warmup_steps = int(config['warmup_epochs'] * n_iter_per_epoch)\n    lr_scheduler = None\n    if config['lr_scheduler']['type'] == 'cosine':\n        lr_scheduler = CosineLRScheduler(\n            optimizer,\n            t_initial=num_steps,\n            cycle_mul=1.,\n            lr_min=config['min_lr'],\n            warmup_lr_init=config.get('warmup_lr', 0.0),\n            warmup_t=warmup_steps,\n            cycle_limit=1,\n            t_in_epochs=config['lr_scheduler']['t_in_epochs'],\n        )\n    elif config['lr_scheduler']['type'] == 'step':\n        decay_steps = int(config['lr_scheduler']['decay_epochs'] * n_iter_per_epoch)\n        lr_scheduler = StepLRScheduler(\n            optimizer,\n            decay_t=decay_steps,\n            decay_rate=config['lr_scheduler']['decay_rate'],\n            warmup_lr_init=config.get('warmup_lr', 0.0),\n            warmup_t=warmup_steps,\n            t_in_epochs=config['lr_scheduler']['t_in_epochs'],\n        )\n    else:\n        raise NotImplementedError()\n\n    return lr_scheduler", ""]}
{"filename": "utils/metrics.py", "chunked_list": ["import torch\nfrom torch import nn\n\nclass PSNR(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, gt):\n        mse = torch.mean((x - gt) ** 2, dim=[1, 2, 3])\n        return 20 * torch.log10(255.0 / torch.sqrt(mse))", "\ndef get_ssim_torch(x, gt):\n    return ssim(x, gt, size_average=False)\n\ndef get_psnr_torch(x, gt, data_range=255.0):\n    mse = torch.mean((x - gt) ** 2, dim=[1, 2, 3])\n    return 20 * torch.log10(data_range / torch.sqrt(mse))\n\n\n# Copyright 2020 by Gongfan Fang, Zhejiang University.", "\n# Copyright 2020 by Gongfan Fang, Zhejiang University.\n# All rights reserved.\nimport warnings\n\nimport torch\nimport torch.nn.functional as F\n\n\ndef _fspecial_gauss_1d(size, sigma):\n    r\"\"\"Create 1-D gauss kernel\n    Args:\n        size (int): the size of gauss kernel\n        sigma (float): sigma of normal distribution\n    Returns:\n        torch.Tensor: 1D kernel (1 x 1 x size)\n    \"\"\"\n    coords = torch.arange(size, dtype=torch.float)\n    coords -= size // 2\n\n    g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n    g /= g.sum()\n\n    return g.unsqueeze(0).unsqueeze(0)", "\ndef _fspecial_gauss_1d(size, sigma):\n    r\"\"\"Create 1-D gauss kernel\n    Args:\n        size (int): the size of gauss kernel\n        sigma (float): sigma of normal distribution\n    Returns:\n        torch.Tensor: 1D kernel (1 x 1 x size)\n    \"\"\"\n    coords = torch.arange(size, dtype=torch.float)\n    coords -= size // 2\n\n    g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n    g /= g.sum()\n\n    return g.unsqueeze(0).unsqueeze(0)", "\n\ndef gaussian_filter(input, win):\n    r\"\"\" Blur input with 1-D kernel\n    Args:\n        input (torch.Tensor): a batch of tensors to be blurred\n        window (torch.Tensor): 1-D gauss kernel\n    Returns:\n        torch.Tensor: blurred tensors\n    \"\"\"\n    assert all([ws == 1 for ws in win.shape[1:-1]]), win.shape\n    if len(input.shape) == 4:\n        conv = F.conv2d\n    elif len(input.shape) == 5:\n        conv = F.conv3d\n    else:\n        raise NotImplementedError(input.shape)\n\n    C = input.shape[1]\n    out = input\n    for i, s in enumerate(input.shape[2:]):\n        if s >= win.shape[-1]:\n            out = conv(out, weight=win.transpose(2 + i, -1), stride=1, padding=0, groups=C)\n        else:\n            warnings.warn(\n                f\"Skipping Gaussian Smoothing at dimension 2+{i} for input: {input.shape} and win size: {win.shape[-1]}\"\n            )\n\n    return out", "\n\ndef _ssim(X, Y, data_range, win, size_average=True, K=(0.01, 0.03)):\n\n    r\"\"\" Calculate ssim index for X and Y\n    Args:\n        X (torch.Tensor): images\n        Y (torch.Tensor): images\n        win (torch.Tensor): 1-D gauss kernel\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n    Returns:\n        torch.Tensor: ssim results.\n    \"\"\"\n    K1, K2 = K\n    # batch, channel, [depth,] height, width = X.shape\n    compensation = 1.0\n\n    C1 = (K1 * data_range) ** 2\n    C2 = (K2 * data_range) ** 2\n\n    win = win.to(X.device, dtype=X.dtype)\n\n    mu1 = gaussian_filter(X, win)\n    mu2 = gaussian_filter(Y, win)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n\n    sigma1_sq = compensation * (gaussian_filter(X * X, win) - mu1_sq)\n    sigma2_sq = compensation * (gaussian_filter(Y * Y, win) - mu2_sq)\n    sigma12 = compensation * (gaussian_filter(X * Y, win) - mu1_mu2)\n\n    cs_map = (2 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)  # set alpha=beta=gamma=1\n    ssim_map = ((2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)) * cs_map\n\n    ssim_per_channel = torch.flatten(ssim_map, 2).mean(-1)\n    cs = torch.flatten(cs_map, 2).mean(-1)\n    return ssim_per_channel, cs", "\n\ndef ssim(\n    X,\n    Y,\n    data_range=255,\n    size_average=True,\n    win_size=11,\n    win_sigma=1.5,\n    win=None,\n    K=(0.01, 0.03),\n    nonnegative_ssim=False,\n):\n    r\"\"\" interface of ssim\n    Args:\n        X (torch.Tensor): a batch of images, (N,C,H,W)\n        Y (torch.Tensor): a batch of images, (N,C,H,W)\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu\n    Returns:\n        torch.Tensor: ssim results\n    \"\"\"\n    if not X.shape == Y.shape:\n        raise ValueError(\"Input images should have the same dimensions.\")\n\n    for d in range(len(X.shape) - 1, 1, -1):\n        X = X.squeeze(dim=d)\n        Y = Y.squeeze(dim=d)\n\n    if len(X.shape) not in (4, 5):\n        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n\n    if not X.type() == Y.type():\n        raise ValueError(\"Input images should have the same dtype.\")\n\n    if win is not None:  # set win_size\n        win_size = win.shape[-1]\n\n    if not (win_size % 2 == 1):\n        raise ValueError(\"Window size should be odd.\")\n\n    if win is None:\n        win = _fspecial_gauss_1d(win_size, win_sigma)\n        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n\n    ssim_per_channel, cs = _ssim(X, Y, data_range=data_range, win=win, size_average=False, K=K)\n    if nonnegative_ssim:\n        ssim_per_channel = torch.relu(ssim_per_channel)\n\n    if size_average:\n        return ssim_per_channel.mean()\n    else:\n        return ssim_per_channel.mean(1)", "\n\ndef ms_ssim(\n    X, Y, data_range=255, size_average=True, win_size=11, win_sigma=1.5, win=None, weights=None, K=(0.01, 0.03)\n):\n\n    r\"\"\" interface of ms-ssim\n    Args:\n        X (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n        Y (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n        weights (list, optional): weights for different levels\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n    Returns:\n        torch.Tensor: ms-ssim results\n    \"\"\"\n    if not X.shape == Y.shape:\n        raise ValueError(\"Input images should have the same dimensions.\")\n\n    for d in range(len(X.shape) - 1, 1, -1):\n        X = X.squeeze(dim=d)\n        Y = Y.squeeze(dim=d)\n\n    if not X.type() == Y.type():\n        raise ValueError(\"Input images should have the same dtype.\")\n\n    if len(X.shape) == 4:\n        avg_pool = F.avg_pool2d\n    elif len(X.shape) == 5:\n        avg_pool = F.avg_pool3d\n    else:\n        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n\n    if win is not None:  # set win_size\n        win_size = win.shape[-1]\n\n    if not (win_size % 2 == 1):\n        raise ValueError(\"Window size should be odd.\")\n\n    smaller_side = min(X.shape[-2:])\n    assert smaller_side > (win_size - 1) * (\n        2 ** 4\n    ), \"Image size should be larger than %d due to the 4 downsamplings in ms-ssim\" % ((win_size - 1) * (2 ** 4))\n\n    if weights is None:\n        weights = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]\n    weights = torch.tensor(weights, device=X.device, dtype=X.dtype)\n\n    if win is None:\n        win = _fspecial_gauss_1d(win_size, win_sigma)\n        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n\n    levels = weights.shape[0]\n    mcs = []\n    for i in range(levels):\n        ssim_per_channel, cs = _ssim(X, Y, win=win, data_range=data_range, size_average=False, K=K)\n\n        if i < levels - 1:\n            mcs.append(torch.relu(cs))\n            padding = [s % 2 for s in X.shape[2:]]\n            X = avg_pool(X, kernel_size=2, padding=padding)\n            Y = avg_pool(Y, kernel_size=2, padding=padding)\n\n    ssim_per_channel = torch.relu(ssim_per_channel)  # (batch, channel)\n    mcs_and_ssim = torch.stack(mcs + [ssim_per_channel], dim=0)  # (level, batch, channel)\n    ms_ssim_val = torch.prod(mcs_and_ssim ** weights.view(-1, 1, 1), dim=0)\n\n    if size_average:\n        return ms_ssim_val.mean()\n    else:\n        return ms_ssim_val.mean(1)", "\n\nclass SSIM(torch.nn.Module):\n    def __init__(\n        self,\n        data_range=255,\n        size_average=True,\n        win_size=11,\n        win_sigma=1.5,\n        channel=3,\n        spatial_dims=2,\n        K=(0.01, 0.03),\n        nonnegative_ssim=False,\n    ):\n        r\"\"\" class for ssim\n        Args:\n            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n            win_size: (int, optional): the size of gauss kernel\n            win_sigma: (float, optional): sigma of normal distribution\n            channel (int, optional): input channels (default: 3)\n            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n            nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.\n        \"\"\"\n\n        super(SSIM, self).__init__()\n        self.win_size = win_size\n        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n        self.size_average = size_average\n        self.data_range = data_range\n        self.K = K\n        self.nonnegative_ssim = nonnegative_ssim\n\n    def forward(self, X, Y):\n        return ssim(\n            X,\n            Y,\n            data_range=self.data_range,\n            size_average=self.size_average,\n            win=self.win,\n            K=self.K,\n            nonnegative_ssim=self.nonnegative_ssim,\n        )", "\n\nclass MS_SSIM(torch.nn.Module):\n    def __init__(\n        self,\n        data_range=255,\n        size_average=True,\n        win_size=11,\n        win_sigma=1.5,\n        channel=3,\n        spatial_dims=2,\n        weights=None,\n        K=(0.01, 0.03),\n    ):\n        r\"\"\" class for ms-ssim\n        Args:\n            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n            win_size: (int, optional): the size of gauss kernel\n            win_sigma: (float, optional): sigma of normal distribution\n            channel (int, optional): input channels (default: 3)\n            weights (list, optional): weights for different levels\n            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        \"\"\"\n\n        super(MS_SSIM, self).__init__()\n        self.win_size = win_size\n        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n        self.size_average = size_average\n        self.data_range = data_range\n        self.weights = weights\n        self.K = K\n\n    def forward(self, X, Y):\n        return ms_ssim(\n            X,\n            Y,\n            data_range=self.data_range,\n            size_average=self.size_average,\n            win=self.win,\n            weights=self.weights,\n            K=self.K,\n        )"]}
{"filename": "utils/optimizer.py", "chunked_list": ["# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nfrom torch import optim as optim\n# from pytorch_lamb import Lamb\n\ndef build_optimizer(config, model):\n    \"\"\"\n    Build optimizer, set weight decay of normalization to 0 by default.\n    \"\"\"\n    skip = {}\n    skip_keywords = {}\n    if hasattr(model, 'no_weight_decay'):\n        skip = model.no_weight_decay()\n    if hasattr(model, 'no_weight_decay_keywords'):\n        skip_keywords = model.no_weight_decay_keywords()\n    parameters = set_weight_decay(model, skip, skip_keywords)\n\n    opt_lower = config['optimizer']['type'].lower()\n    optimizer = None\n    if opt_lower == 'sgd':\n        optimizer = optim.SGD(parameters, momentum=config['optimizer']['momentum'], nesterov=True,\n                              lr=config['base_lr'], weight_decay=config['weight_decay'])\n    elif opt_lower == 'adamw':\n        optimizer = optim.AdamW(parameters, eps=config['optimizer']['eps'], betas=config['optimizer']['betas'],\n                                lr=config['base_lr'], weight_decay=config['weight_decay'])\n\n    return optimizer", "# from pytorch_lamb import Lamb\n\ndef build_optimizer(config, model):\n    \"\"\"\n    Build optimizer, set weight decay of normalization to 0 by default.\n    \"\"\"\n    skip = {}\n    skip_keywords = {}\n    if hasattr(model, 'no_weight_decay'):\n        skip = model.no_weight_decay()\n    if hasattr(model, 'no_weight_decay_keywords'):\n        skip_keywords = model.no_weight_decay_keywords()\n    parameters = set_weight_decay(model, skip, skip_keywords)\n\n    opt_lower = config['optimizer']['type'].lower()\n    optimizer = None\n    if opt_lower == 'sgd':\n        optimizer = optim.SGD(parameters, momentum=config['optimizer']['momentum'], nesterov=True,\n                              lr=config['base_lr'], weight_decay=config['weight_decay'])\n    elif opt_lower == 'adamw':\n        optimizer = optim.AdamW(parameters, eps=config['optimizer']['eps'], betas=config['optimizer']['betas'],\n                                lr=config['base_lr'], weight_decay=config['weight_decay'])\n\n    return optimizer", "\n\ndef set_weight_decay(model, skip_list=(), skip_keywords=()):\n    has_decay = []\n    no_decay = []\n\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue  # frozen weights\n        if len(param.shape) == 1 or name.endswith(\".bias\") or (name in skip_list) or \\\n                check_keywords_in_name(name, skip_keywords):\n            no_decay.append(param)\n            # print(f\"{name} has no weight decay\")\n        else:\n            has_decay.append(param)\n    return [{'params': has_decay},\n            {'params': no_decay, 'weight_decay': 0.}]", "\n\ndef check_keywords_in_name(name, keywords=()):\n    isin = False\n    for keyword in keywords:\n        if keyword in name:\n            isin = True\n    return isin\n", ""]}
{"filename": "utils/logger.py", "chunked_list": ["# --------------------------------------------------------\n# Swin Transformer\n# Copyright (c) 2021 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ze Liu\n# --------------------------------------------------------\n\nimport os\nimport sys\nimport logging", "import sys\nimport logging\nimport functools\nfrom termcolor import colored\n\nlogger = None\n\n@functools.lru_cache()\ndef create_logger(output_dir, dist_rank=None, name='', action='train'):\n    global logger\n    if logger is not None:\n        return logger\n    # create logger\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    logger.propagate = False\n\n    # create formatter\n    fmt = '[%(asctime)s %(name)s] (%(filename)s %(lineno)d): %(levelname)s %(message)s'\n    color_fmt = colored('[%(asctime)s %(name)s]', 'green') + \\\n                colored('(%(filename)s %(lineno)d)', 'yellow') + ': %(levelname)s %(message)s'\n\n    if dist_rank is not None:\n        # create console handlers for master process\n        if dist_rank == 0:\n            console_handler = logging.StreamHandler(sys.stdout)\n            console_handler.setLevel(logging.DEBUG)\n            console_handler.setFormatter(\n                logging.Formatter(fmt=color_fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n            logger.addHandler(console_handler)\n\n        file_handler = logging.FileHandler(os.path.join(output_dir, f'{action}-rank-{dist_rank}.log'), mode='a')\n    else:\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(logging.DEBUG)\n        console_handler.setFormatter(\n            logging.Formatter(fmt=color_fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n        logger.addHandler(console_handler)\n        file_handler = logging.FileHandler(os.path.join(output_dir, f'{action}.log'), mode='a')\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(logging.Formatter(fmt=fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n    logger.addHandler(file_handler)\n\n    return logger", "def create_logger(output_dir, dist_rank=None, name='', action='train'):\n    global logger\n    if logger is not None:\n        return logger\n    # create logger\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    logger.propagate = False\n\n    # create formatter\n    fmt = '[%(asctime)s %(name)s] (%(filename)s %(lineno)d): %(levelname)s %(message)s'\n    color_fmt = colored('[%(asctime)s %(name)s]', 'green') + \\\n                colored('(%(filename)s %(lineno)d)', 'yellow') + ': %(levelname)s %(message)s'\n\n    if dist_rank is not None:\n        # create console handlers for master process\n        if dist_rank == 0:\n            console_handler = logging.StreamHandler(sys.stdout)\n            console_handler.setLevel(logging.DEBUG)\n            console_handler.setFormatter(\n                logging.Formatter(fmt=color_fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n            logger.addHandler(console_handler)\n\n        file_handler = logging.FileHandler(os.path.join(output_dir, f'{action}-rank-{dist_rank}.log'), mode='a')\n    else:\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(logging.DEBUG)\n        console_handler.setFormatter(\n            logging.Formatter(fmt=color_fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n        logger.addHandler(console_handler)\n        file_handler = logging.FileHandler(os.path.join(output_dir, f'{action}.log'), mode='a')\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(logging.Formatter(fmt=fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n    logger.addHandler(file_handler)\n\n    return logger", ""]}
{"filename": "utils/miscs.py", "chunked_list": ["import torch\nimport os\nimport shutil\nimport cv2\nimport numpy as np\n\ndef load_checkpoint(config, model, optimizer, lr_scheduler, logger, epoch=None):\n    resume_ckpt_path = config['train']['resume']\n    logger.info(f\"==============> Resuming form {resume_ckpt_path}....................\")\n    if resume_ckpt_path.startswith('https'):\n        checkpoint = torch.hub.load_state_dict_from_url(\n            resume_ckpt_path, map_location='cpu', check_hash=True)\n    else:\n        checkpoint = torch.load(resume_ckpt_path, map_location='cpu')\n    msg = model.load_state_dict(checkpoint['model'], strict=False)\n    logger.info(msg)\n    max_psnr = 0.0\n    if not config.get('eval_mode', False) and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n        if 'max_psnr' in checkpoint:\n            max_psnr = checkpoint['max_psnr']\n    if epoch is None and 'epoch' in checkpoint:\n        config['train']['start_epoch'] = checkpoint['epoch']\n        logger.info(f\"=> loaded successfully '{resume_ckpt_path}' (epoch {checkpoint['epoch']})\")\n    del checkpoint\n    torch.cuda.empty_cache()\n    return max_psnr", "\ndef load_pretrained(config, model, logger):\n    logger.info(f\"==============> Loading weight {config['train']['pretrained']}....................\")\n    checkpoint = torch.load(config['train']['pretrained'], map_location='cpu')\n    state_dict = checkpoint['model']\n    msg = model.load_state_dict(state_dict, strict=False)\n    logger.warning(msg)\n\n    logger.info(f\"=> loaded successfully '{config['train']['pretrained']}'\")\n\n    del checkpoint\n    torch.cuda.empty_cache()", "    \ndef save_checkpoint(config, epoch, model, max_psnr, optimizer, lr_scheduler, logger, is_best=False):\n    save_state = {'model': model.state_dict(),\n                  'optimizer': optimizer.state_dict(),\n                  'lr_scheduler': lr_scheduler.state_dict(),\n                  'max_psnr': max_psnr,\n                  'epoch': epoch,\n                  'config': config}\n\n    os.makedirs(os.path.join(config['output'], 'checkpoints'), exist_ok=True)\n\n    save_path = os.path.join(config['output'], 'checkpoints', 'checkpoint.pth')\n    logger.info(f\"{save_path} saving......\")\n    torch.save(save_state, save_path)\n    logger.info(f\"{save_path} saved\")\n    if epoch % config['save_per_epoch'] == 0 or (config['train']['epochs'] - epoch) < 50:\n        shutil.copy(save_path, os.path.join(config['output'], 'checkpoints', f'epoch_{epoch:04d}.pth'))\n        logger.info(f\"{save_path} copied to epoch_{epoch:04d}.pth\")\n    if is_best:\n        shutil.copy(save_path, os.path.join(config['output'], 'checkpoints', 'model_best.pth'))\n        logger.info(f\"{save_path} copied to model_best.pth\")", "        \ndef get_grad_norm(parameters, norm_type=2):\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    norm_type = float(norm_type)\n    total_norm = 0\n    for p in parameters:\n        param_norm = p.grad.data.norm(norm_type)\n        total_norm += param_norm.item() ** norm_type\n    total_norm = total_norm ** (1. / norm_type)\n    return total_norm", "\ndef save_image_torch(img, file_path, range_255_float=True, params=None, auto_mkdir=True):\n    \"\"\"Write image to file.\n    Args:\n        img (ndarray): Image array to be written.\n        file_path (str): Image file path.\n        params (None or list): Same as opencv's :func:`imwrite` interface.\n        auto_mkdir (bool): If the parent folder of `file_path` does not exist,\n            whether to create it automatically.\n    Returns:\n        bool: Successful or not.\n    \"\"\"\n    if auto_mkdir:\n        dir_name = os.path.abspath(os.path.dirname(file_path))\n        os.makedirs(dir_name, exist_ok=True)\n\n    assert len(img.size()) == 3\n    img = img.clone().cpu().detach().numpy().transpose(1, 2, 0)\n\n    if range_255_float:\n        # Unlike MATLAB, numpy.unit8() WILL NOT round by default.\n        img = img.clip(0, 255).round()\n        img = img.astype(np.uint8)\n    else:\n        img = img.clip(0, 1)\n\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    ok = cv2.imwrite(file_path, img, params)\n    if not ok:\n        raise IOError('Failed in writing images.')"]}
{"filename": "utils/__init__.py", "chunked_list": ["from .miscs import *"]}
{"filename": "scripts/inference.py", "chunked_list": ["from copy import deepcopy\nimport os\nimport time\nimport datetime\nimport yaml\nimport git\n\nimport torch\nimport torch.backends.cudnn as cudnn\n", "import torch.backends.cudnn as cudnn\n\nfrom timm.utils import AverageMeter\n\nfrom utils import load_checkpoint, load_pretrained, save_image_torch\nfrom utils.config import parse_options, copy_cfg, ordered_dict_to_dict\nfrom utils.metrics import get_psnr_torch, get_ssim_torch\nfrom utils.logger import create_logger\n\nfrom models import build_model", "\nfrom models import build_model\nfrom datasets import build_test_loader\nfrom forwards import build_forward\n\ndef main(config):\n    data_loader = build_test_loader(config['data'], 2)\n\n    logger.info(f\"Creating model:{config['name']}/{config['model']['type']}\")\n    model = build_model(config['model'])\n    model.cuda()\n    logger.info(str(model))\n    \n    logger.info('Building forwards:')\n    logger.info(f'Inference forward: {config[\"inference\"][\"forward_type\"]}')\n    forward = build_forward(config[\"inference\"][\"forward_type\"])\n    \n    if config['train'].get('resume'):\n        load_checkpoint(config, model, None, None, logger)\n\n    if config['train'].get('pretrained') and (not config['train'].get('resume')):\n        load_pretrained(config, model, logger)\n\n    logger.info(\"Start Inference\")\n    start_time = time.time()\n    \n    inference(config, forward, model, data_loader, logger)\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info('Total time {}'.format(total_time_str))", "\n\n@torch.no_grad()\ndef inference(config, forward, model, data_loader, logger):\n    torch.cuda.reset_max_memory_allocated()\n    model.eval()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n\n    start = time.time()\n    end = time.time()\n    for idx, data in enumerate(data_loader):\n        data_time.update(time.time() - end)\n        outputs, img_files = forward(config, model, data)\n        \n        output = outputs[config['inference']['which_stage']]\n        output = torch.clamp(output, 0, 1) * 255\n        \n        result_path = os.path.join(config['output'], 'results', f'inference')\n        os.makedirs(result_path, exist_ok=True)\n        for i, result in enumerate(output):\n            save_path = os.path.join(result_path, f'{os.path.basename(img_files[i])[:-4]}.png')\n            save_image_torch(result, save_path)\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if config['testset_as_validset'] or idx % config['print_per_iter'] == 0 or idx == len(data_loader):\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            logger.info(\n                f'Infer: [{idx}/{len(data_loader)}]\\t'\n                f'Time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n                f'Mem {memory_used:.0f}MB\\t{os.path.basename(img_files[0])}')\n    logger.info(f'Infer: Time {datetime.timedelta(seconds=int(time.time()-start))}')", "\n@torch.no_grad()\ndef test_metric_cuda(config, epoch, outputs, targets, image_paths, target_params=None):\n    outputs = torch.clamp(outputs, 0, 1) * 255\n    targets = torch.clamp(targets, 0, 1) * 255\n    if config['test']['round']:\n        outputs = outputs.round()\n        targets = targets.round()\n    psnr = get_psnr_torch(outputs, targets)\n    ssim = get_ssim_torch(outputs, targets)\n\n    if config['test']['save_image']:\n        result_path = os.path.join(config['output'], 'results', f'test_{epoch:04d}')\n        os.makedirs(result_path, exist_ok=True)\n        save_path = os.path.join(result_path, f'{os.path.basename(image_paths[0])[:-4]}_{psnr.item():.2f}.png')\n        save_image_torch(outputs[0], save_path)\n\n    return psnr, ssim", "\n\nif __name__ == '__main__':\n    args, config = parse_options()\n    phase = 'infer'\n    if 'inference' not in config:\n        config['inference'] = deepcopy(config['test'])\n    config['testset_as_validset'] = True\n    config['eval_mode'] = True\n    assert not args.auto_resume and \\\n            (args.resume is not None or config['train'].get('resume') is not None) or \\\n            (args.pretrain is not None or config['train'].get('pretrained') is not None)\n\n    cudnn.benchmark = True\n\n    os.makedirs(config['output'], exist_ok=True)\n    start_time = time.strftime(\"%y%m%d-%H%M\", time.localtime())\n    logger = create_logger(output_dir=config['output'], name=f\"{config['tag']}\", action=f\"{phase}-{start_time}\")\n    path = os.path.join(config['output'], f\"{phase}-{start_time}.yaml\")\n    \n    try:\n        repo = git.Repo(search_parent_directories=True)\n        sha = repo.head.object.hexsha\n        if repo.is_dirty():\n            logger.warning(f'Current work on commit: {sha}, however the repo is dirty (not committed)!')\n        else:\n            logger.info(f'Current work on commit: {sha}.')\n    except git.exc.InvalidGitRepositoryError as e:\n        logger.warn(f'No git repo base.')\n\n    copy_cfg(config, path)\n    logger.info(f\"Full config saved to {path}\")\n\n    # print config\n    logger.info(\"Config:\\n\" + yaml.dump(ordered_dict_to_dict(config), default_flow_style=False, sort_keys=False))\n    current_cuda_device = torch.cuda.get_device_properties(torch.cuda.current_device())\n    logger.info(f\"Current CUDA Device: {current_cuda_device.name}, Total Mem: {int(current_cuda_device.total_memory / 1024 / 1024)}MB\")\n\n    main(config)", ""]}
{"filename": "scripts/generate_video_from_images.py", "chunked_list": ["import argparse\nimport cv2\nimport os\nimport glob\nfrom tqdm import tqdm\n\nIMAGES_PATH = ''\nSAVE_PATH = ''\nFILE_NAME = None\nFPS = 24", "FILE_NAME = None\nFPS = 24\n\ndef parse_options():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--images-path', type=str, required=True, help='Path to the image sequences.')\n    parser.add_argument('--save-path', type=str, required=True, help='Path to the save path.')\n    parser.add_argument('--file-name', type=str, required=True, help='File name.')\n    parser.add_argument('--fps', type=int, default=24, help='FPS of the target video.')\n    args = parser.parse_args()\n    \n    global IMAGES_PATH, SAVE_PATH, FPS, FILE_NAME\n    IMAGES_PATH = args.images_path\n    SAVE_PATH = args.save_path if args.save_path != \"\" else \"runs/CVPR_DEMO/video/videos\"\n    FPS = args.fps\n    FILE_NAME = args.file_name", "\ndef main(images_path, save_path, fps, file_name=None, img_postfix='png', vid_postfix='mp4', scale_factor=1.0):\n    vid_to_fourcc = {\n        'avi': 'DIVX',\n        'mp4': 'mp4v'\n    }\n    \n    images = list(sorted(glob.glob(f'{images_path}/*.{img_postfix}')))\n    H, W, _ = cv2.imread(images[0], cv2.IMREAD_COLOR).shape\n    frame_size = (int(W * scale_factor), int(H * scale_factor))\n\n    os.makedirs(save_path, exist_ok=True)\n    file_name = os.path.basename(images_path) if not file_name else file_name\n    out = cv2.VideoWriter(f'{save_path}/{file_name}.{vid_postfix}', cv2.VideoWriter_fourcc(*vid_to_fourcc[vid_postfix]), fps, frame_size)\n\n    for filename in tqdm(images):\n        img = cv2.imread(filename, cv2.IMREAD_COLOR)\n        if scale_factor != 1.0:\n            img = cv2.resize(img, frame_size, cv2.INTER_NEAREST)\n        out.write(img)\n\n    out.release()", "    \n\nif __name__ == \"__main__\":\n    parse_options()\n    main(IMAGES_PATH, SAVE_PATH, FPS, file_name=FILE_NAME)"]}
{"filename": "scripts/preprocess/preprocess_sid.py", "chunked_list": ["import numpy as np\nimport rawpy\nimport os\nfrom glob import glob\nfrom tqdm import tqdm\nimport time\nimport argparse\nfrom multiprocessing import Pool\nfrom utils import save_image\nimport cv2", "from utils import save_image\nimport cv2\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Preprocess dataset for fast training', add_help=False)\n\n    parser.add_argument('--data-path', type=str, default='./dataset/sid', help='path to dataset')\n    parser.add_argument('--camera', type=str, default='Sony', choices=['Sony', 'Fuji'], help='Determine the CFA pattern')\n    parser.add_argument('--split', type=str, default='long', choices=['long', 'short'], help='Preprocess long/short split of SID dataset')\n\n    args, unparsed = parser.parse_known_args()\n    return args", "\n\nmeta = {\n    'Sony': {\n        'white_level': 16383,\n        'black_level': 512,\n        'raw_ext': 'ARW',\n        'post_shape': (2848, 4256)\n    },\n    'Fuji': {", "    },\n    'Fuji': {\n        'white_level': 16383,\n        'black_level': 1024,\n        'raw_ext': 'RAF',\n        'post_shape': (4032, 6030)\n    }\n}\n\ndef pack_raw(im, camera):\n    if camera == 'Sony':\n        im = np.expand_dims(im, axis=0)\n        C, H, W = im.shape\n        out = np.concatenate((im[:, 0:H:2, 0:W:2],\n                              im[:, 0:H:2, 1:W:2],\n                              im[:, 1:H:2, 1:W:2],\n                              im[:, 1:H:2, 0:W:2]), axis=0)\n    elif camera == 'Fuji':\n        img_shape = im.shape\n        # orig 4032, 6032\n        # crop 4032, 6030\n        # pack 1344, 2010\n\n        H = (img_shape[0] // 6) * 6\n        W = (img_shape[1] // 6) * 6\n\n        out = np.zeros((9, H // 3, W // 3), dtype=np.uint16)\n\n        # 0 R\n        out[0, 0::2, 0::2] = im[0:H:6, 0:W:6]\n        out[0, 0::2, 1::2] = im[0:H:6, 4:W:6]\n        out[0, 1::2, 0::2] = im[3:H:6, 1:W:6]\n        out[0, 1::2, 1::2] = im[3:H:6, 3:W:6]\n\n        # 1 G\n        out[1, 0::2, 0::2] = im[0:H:6, 2:W:6]\n        out[1, 0::2, 1::2] = im[0:H:6, 5:W:6]\n        out[1, 1::2, 0::2] = im[3:H:6, 2:W:6]\n        out[1, 1::2, 1::2] = im[3:H:6, 5:W:6]\n\n        # 1 B\n        out[2, 0::2, 0::2] = im[0:H:6, 1:W:6]\n        out[2, 0::2, 1::2] = im[0:H:6, 3:W:6]\n        out[2, 1::2, 0::2] = im[3:H:6, 0:W:6]\n        out[2, 1::2, 1::2] = im[3:H:6, 4:W:6]\n\n        # 4 R\n        out[3, 0::2, 0::2] = im[1:H:6, 2:W:6]\n        out[3, 0::2, 1::2] = im[2:H:6, 5:W:6]\n        out[3, 1::2, 0::2] = im[5:H:6, 2:W:6]\n        out[3, 1::2, 1::2] = im[4:H:6, 5:W:6]\n\n        # 5 B\n        out[4, 0::2, 0::2] = im[2:H:6, 2:W:6]\n        out[4, 0::2, 1::2] = im[1:H:6, 5:W:6]\n        out[4, 1::2, 0::2] = im[4:H:6, 2:W:6]\n        out[4, 1::2, 1::2] = im[5:H:6, 5:W:6]\n\n        out[5, :, :] = im[1:H:3, 0:W:3]\n        out[6, :, :] = im[1:H:3, 1:W:3]\n        out[7, :, :] = im[2:H:3, 0:W:3]\n        out[8, :, :] = im[2:H:3, 1:W:3]\n    return out", "\ndef pack_raw(im, camera):\n    if camera == 'Sony':\n        im = np.expand_dims(im, axis=0)\n        C, H, W = im.shape\n        out = np.concatenate((im[:, 0:H:2, 0:W:2],\n                              im[:, 0:H:2, 1:W:2],\n                              im[:, 1:H:2, 1:W:2],\n                              im[:, 1:H:2, 0:W:2]), axis=0)\n    elif camera == 'Fuji':\n        img_shape = im.shape\n        # orig 4032, 6032\n        # crop 4032, 6030\n        # pack 1344, 2010\n\n        H = (img_shape[0] // 6) * 6\n        W = (img_shape[1] // 6) * 6\n\n        out = np.zeros((9, H // 3, W // 3), dtype=np.uint16)\n\n        # 0 R\n        out[0, 0::2, 0::2] = im[0:H:6, 0:W:6]\n        out[0, 0::2, 1::2] = im[0:H:6, 4:W:6]\n        out[0, 1::2, 0::2] = im[3:H:6, 1:W:6]\n        out[0, 1::2, 1::2] = im[3:H:6, 3:W:6]\n\n        # 1 G\n        out[1, 0::2, 0::2] = im[0:H:6, 2:W:6]\n        out[1, 0::2, 1::2] = im[0:H:6, 5:W:6]\n        out[1, 1::2, 0::2] = im[3:H:6, 2:W:6]\n        out[1, 1::2, 1::2] = im[3:H:6, 5:W:6]\n\n        # 1 B\n        out[2, 0::2, 0::2] = im[0:H:6, 1:W:6]\n        out[2, 0::2, 1::2] = im[0:H:6, 3:W:6]\n        out[2, 1::2, 0::2] = im[3:H:6, 0:W:6]\n        out[2, 1::2, 1::2] = im[3:H:6, 4:W:6]\n\n        # 4 R\n        out[3, 0::2, 0::2] = im[1:H:6, 2:W:6]\n        out[3, 0::2, 1::2] = im[2:H:6, 5:W:6]\n        out[3, 1::2, 0::2] = im[5:H:6, 2:W:6]\n        out[3, 1::2, 1::2] = im[4:H:6, 5:W:6]\n\n        # 5 B\n        out[4, 0::2, 0::2] = im[2:H:6, 2:W:6]\n        out[4, 0::2, 1::2] = im[1:H:6, 5:W:6]\n        out[4, 1::2, 0::2] = im[4:H:6, 2:W:6]\n        out[4, 1::2, 1::2] = im[5:H:6, 5:W:6]\n\n        out[5, :, :] = im[1:H:3, 0:W:3]\n        out[6, :, :] = im[1:H:3, 1:W:3]\n        out[7, :, :] = im[2:H:3, 0:W:3]\n        out[8, :, :] = im[2:H:3, 1:W:3]\n    return out", "\ndef preprocess(image_path):\n    # print(image_path)\n    image_name = os.path.basename(image_path)\n\n    # read raw image\n    raw = rawpy.imread(image_path)\n    image_visible = raw.raw_image_visible.astype(np.uint16)\n\n    # pack raw image\n    pack_image = pack_raw(image_visible, args.camera)\n\n    # save packed image\n    save_pack_name = os.path.join(pack_path, image_name)\n    np.save(save_pack_name, pack_image, allow_pickle=True)\n\n    # post process raw image using rawpy\n    if args.split == 'long':\n        post_image = raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n        # post_image_norm = np.float32(post_image) / 255\n        post_image = post_image.transpose(2, 0, 1)\n        if post_image.shape[1:] != meta[args.camera]['post_shape']:\n            H, W = meta[args.camera]['post_shape']\n            post_image = post_image[:, :H, :W]\n\n        save_post_name = os.path.join(post_path, image_name)\n        # save_post_jpg_name = os.path.join(post_jpg_path, image_name+'.jpg')\n\n        # save post processed image\n        np.save(save_post_name, post_image, allow_pickle=True)\n        # save_image(post_image_norm, save_post_jpg_name)\n\n    if args.split == 'long':\n        post_image = raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n        post_image = cv2.cvtColor(post_image, cv2.COLOR_RGB2BGR)\n        post_image = (post_image / 65535.0 * 255.0).round().astype(np.uint8)\n        if post_image.shape[:2] != meta[args.camera]['post_shape']:\n            print(111)\n            H, W = meta[args.camera]['post_shape']\n            post_image = post_image[:H, :W, :]\n\n        save_post_png_name = os.path.join(post_png_path, image_name + '.png')\n        cv2.imwrite(save_post_png_name, post_image)", "\n\nif __name__ == '__main__':\n    args = parse_args()\n    print(args)\n\n    data_path = os.path.join(args.data_path, args.camera)\n    split_path = os.path.join(data_path, args.split)\n    pack_path = os.path.join(data_path, f'{args.split}_pack')\n    post_path = os.path.join(data_path, f'{args.split}_post_int')\n    post_jpg_path = os.path.join(data_path, f'{args.split}_post_jpg')\n    post_png_path = os.path.join(data_path, f'{args.split}_post_png_16')\n\n    os.makedirs(pack_path, exist_ok=True)\n    if args.split == 'long':\n        os.makedirs(post_path, exist_ok=True)\n        os.makedirs(post_jpg_path, exist_ok=True)\n        os.makedirs(post_png_path, exist_ok=True)\n\n    image_list = glob(os.path.join(split_path, f'*.{meta[args.camera][\"raw_ext\"]}'))\n    # image_list = image_list[:10]\n    print('number of images:', len(image_list))\n\n    with Pool(8) as pool:\n        with tqdm(total=len(image_list)) as t:\n            for i, x in enumerate(pool.imap(preprocess, image_list)):\n                t.update()", ""]}
{"filename": "models/sid_model.py", "chunked_list": ["import torch\nfrom torch import nn\n\nfrom utils.registry import MODEL_REGISTRY\n\n\n@MODEL_REGISTRY.register()\nclass SIDUNet(nn.Module):\n    def __init__(self, block_size=2, channels=32) -> None:\n        super().__init__()\n        inchannels = block_size * block_size\n        outchannels = block_size * block_size * 3\n\n        self.conv1_1 = nn.Conv2d(inchannels, channels, kernel_size=3, stride=1, padding=1)\n        self.conv1_2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n\n        self.conv2_1 = nn.Conv2d(channels, channels * 2, kernel_size=3, stride=1, padding=1)\n        self.conv2_2 = nn.Conv2d(channels * 2, channels * 2, kernel_size=3, stride=1, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n\n        self.conv3_1 = nn.Conv2d(channels * 2, channels * 4, kernel_size=3, stride=1, padding=1)\n        self.conv3_2 = nn.Conv2d(channels * 4, channels * 4, kernel_size=3, stride=1, padding=1)\n        self.pool3 = nn.MaxPool2d(kernel_size=2)\n\n        self.conv4_1 = nn.Conv2d(channels * 4, channels * 8, kernel_size=3, stride=1, padding=1)\n        self.conv4_2 = nn.Conv2d(channels * 8, channels * 8, kernel_size=3, stride=1, padding=1)\n        self.pool4 = nn.MaxPool2d(kernel_size=2)\n\n        self.conv5_1 = nn.Conv2d(channels * 8, channels * 16, kernel_size=3, stride=1, padding=1)\n        self.conv5_2 = nn.Conv2d(channels * 16, channels * 16, kernel_size=3, stride=1, padding=1)\n\n        self.upv6 = nn.ConvTranspose2d(channels * 16, channels * 8, 2, stride=2)\n        self.conv6_1 = nn.Conv2d(channels * 16, channels * 8, kernel_size=3, stride=1, padding=1)\n        self.conv6_2 = nn.Conv2d(channels * 8, channels * 8, kernel_size=3, stride=1, padding=1)\n\n        self.upv7 = nn.ConvTranspose2d(channels * 8, channels * 4, 2, stride=2)\n        self.conv7_1 = nn.Conv2d(channels * 8, channels * 4, kernel_size=3, stride=1, padding=1)\n        self.conv7_2 = nn.Conv2d(channels * 4, channels * 4, kernel_size=3, stride=1, padding=1)\n\n        self.upv8 = nn.ConvTranspose2d(channels * 4, channels * 2, 2, stride=2)\n        self.conv8_1 = nn.Conv2d(channels * 4, channels * 2, kernel_size=3, stride=1, padding=1)\n        self.conv8_2 = nn.Conv2d(channels * 2, channels * 2, kernel_size=3, stride=1, padding=1)\n\n        self.upv9 = nn.ConvTranspose2d(channels * 2, channels, 2, stride=2)\n        self.conv9_1 = nn.Conv2d(channels * 2, channels, kernel_size=3, stride=1, padding=1)\n        self.conv9_2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n\n        self.conv10_1 = nn.Conv2d(channels, outchannels, kernel_size=1, stride=1)\n        self.pixel_shuffle = nn.PixelShuffle(block_size)\n\n    def forward(self, x):\n        conv1 = self.lrelu(self.conv1_1(x))\n        conv1 = self.lrelu(self.conv1_2(conv1))\n        pool1 = self.pool1(conv1)\n\n        conv2 = self.lrelu(self.conv2_1(pool1))\n        conv2 = self.lrelu(self.conv2_2(conv2))\n        pool2 = self.pool1(conv2)\n\n        conv3 = self.lrelu(self.conv3_1(pool2))\n        conv3 = self.lrelu(self.conv3_2(conv3))\n        pool3 = self.pool1(conv3)\n\n        conv4 = self.lrelu(self.conv4_1(pool3))\n        conv4 = self.lrelu(self.conv4_2(conv4))\n        pool4 = self.pool1(conv4)\n\n        conv5 = self.lrelu(self.conv5_1(pool4))\n        conv5 = self.lrelu(self.conv5_2(conv5))\n\n        up6 = self.upv6(conv5)\n        up6 = torch.cat([up6, conv4], 1)\n        conv6 = self.lrelu(self.conv6_1(up6))\n        conv6 = self.lrelu(self.conv6_2(conv6))\n\n        up7 = self.upv7(conv6)\n        up7 = torch.cat([up7, conv3], 1)\n        conv7 = self.lrelu(self.conv7_1(up7))\n        conv7 = self.lrelu(self.conv7_2(conv7))\n\n        up8 = self.upv8(conv7)\n        up8 = torch.cat([up8, conv2], 1)\n        conv8 = self.lrelu(self.conv8_1(up8))\n        conv8 = self.lrelu(self.conv8_2(conv8))\n\n        up9 = self.upv9(conv8)\n        up9 = torch.cat([up9, conv1], 1)\n        conv9 = self.lrelu(self.conv9_1(up9))\n        conv9 = self.lrelu(self.conv9_2(conv9))\n\n        conv10 = self.conv10_1(conv9)\n        out = self.pixel_shuffle(conv10)\n        return out\n\n    def lrelu(self, x):\n        outt = torch.max(0.2 * x, x)\n        return outt", ""]}
{"filename": "models/__init__.py", "chunked_list": ["import importlib\nfrom copy import deepcopy\nfrom os import path as osp\nfrom glob import glob\n\nfrom utils.registry import MODEL_REGISTRY\n\n__all__ = ['build_model']\n\n# automatically scan and import model modules for registry", "\n# automatically scan and import model modules for registry\n# scan all the files under the 'models' folder and collect files ending with '_model.py'\nmodel_folder = osp.dirname(osp.abspath(__file__))\nmodel_filenames = [osp.splitext(osp.basename(v))[0] for v in glob(osp.join(model_folder, '*_model.py'))]\n# import all the model modules\n_model_modules = [importlib.import_module(f'models.{file_name}') for file_name in model_filenames]\n\n\ndef build_model(model_cfg):\n    model_cfg = deepcopy(model_cfg)\n    model_type = model_cfg.pop('type')\n    model = MODEL_REGISTRY.get(model_type)(**model_cfg)\n    return model", "\ndef build_model(model_cfg):\n    model_cfg = deepcopy(model_cfg)\n    model_type = model_cfg.pop('type')\n    model = MODEL_REGISTRY.get(model_type)(**model_cfg)\n    return model"]}
{"filename": "models/utils.py", "chunked_list": ["import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass LayerNorm(nn.Module):\n    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n    with shape (batch_size, channels, height, width).\n    \"\"\"\n\n    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n        self.eps = eps\n        self.data_format = data_format\n        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n            raise NotImplementedError\n        self.normalized_shape = (normalized_shape, )\n\n    def forward(self, x):\n        if self.data_format == \"channels_last\":\n            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        elif self.data_format == \"channels_first\":\n            u = x.mean(1, keepdim=True)\n            s = (x - u).pow(2).mean(1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.eps)\n            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n            return x", ""]}
{"filename": "models/dnf_model.py", "chunked_list": ["from torch import nn\n\nfrom .dnf_utils.base import DNFBase\nfrom .dnf_utils.cid import CID\nfrom .dnf_utils.mcc import MCC\nfrom .dnf_utils.fuse import PDConvFuse, GFM\nfrom .dnf_utils.resudual_switch import ResidualSwitchBlock\nfrom utils.registry import MODEL_REGISTRY\n\n", "\n\n@MODEL_REGISTRY.register()\nclass SingleStageNet(DNFBase):\n    def __init__(self, f_number, *, \n                 block_size=1, \n                 layers=4, \n                 denoising_block='CID', \n                 color_correction_block='MCC') -> None:\n        super().__init__(f_number, block_size=block_size, layers=layers, \n                         denoising_block=denoising_block, color_correction_block=color_correction_block)\n    \n    def _pass_features_to_color_decoder(self, x, f_short_cut, encoder_features):\n        return x, None, encoder_features", "\n\n@MODEL_REGISTRY.register()\nclass MultiStageNet(DNFBase):\n    def __init__(self, f_number, *, \n                 block_size=1, \n                 layers=4, \n                 denoising_block='CID', \n                 color_correction_block='MCC') -> None:\n        super().__init__(f_number, block_size=block_size, layers=layers, \n                         denoising_block=denoising_block, color_correction_block=color_correction_block)\n\n        aux_outchannel = 3 if block_size == 1 else block_size * block_size\n        self.aux_denoising_blocks = nn.ModuleList([\n            ResidualSwitchBlock(\n                self.denoising_block_class(\n                    f_number=f_number * (2**idx),\n                    padding_mode=self.padding_mode\n                )\n            )\n            for idx in range(layers)\n        ])\n        self.aux_upsamples = nn.ModuleList([\n            self.upsample_class(\n                f_number * (2**idx), \n                padding_mode=self.padding_mode\n            )\n            for idx in range(1, layers)\n        ])\n        self.denoising_decoder_fuses = nn.ModuleList([\n            self.decoder_fuse_class(in_channels=f_number * (2 ** idx)) for idx in range(layers - 1)\n        ])\n\n        self.aux_conv_fuse_0 = nn.Conv2d(f_number, f_number, 3, 1, 1, bias=True, padding_mode=self.padding_mode)\n        self.aux_conv_fuse_1 = nn.Conv2d(f_number, aux_outchannel, 1, 1, 0, bias=True)\n        \n        inchannel = 3 if block_size == 1 else block_size * block_size\n        self.aux_feature_conv_0 = nn.Conv2d(inchannel, f_number, 5, 1, 2, bias=True, padding_mode=self.padding_mode)\n        self.aux_feature_conv_1 = nn.Conv2d(f_number, f_number, 5, 1, 2, bias=True, padding_mode=self.padding_mode)\n        \n        head = [2 ** layer for layer in range(layers)]\n        self.aux_color_correction_blocks = nn.ModuleList([\n            self.color_correction_block_class(\n                f_number=f_number * (2 ** idx),\n                num_heads=head[idx],\n                padding_mode=self.padding_mode,\n            )\n            for idx in range(layers)\n        ])\n        self.aux_downsamples = nn.ModuleList([\n            self.downsample_class(\n                f_number * (2**idx), \n                padding_mode=self.padding_mode\n            )\n            for idx in range(layers - 1)\n        ])\n        \n    def _pass_features_to_color_decoder(self, x, f_short_cut, encoder_features):\n        denoise_decoder_features = []\n        for denoise, up, fuse, encoder_feature in reversed(list(zip(\n            self.aux_denoising_blocks[1:], \n            self.aux_upsamples, \n            self.denoising_decoder_fuses,\n            encoder_features    \n        ))):\n            x = denoise(x, 1)\n            denoise_decoder_features.append(x)\n            x = up(x)\n            x = fuse(x, encoder_feature)\n        x = self.aux_denoising_blocks[0](x, 1)\n        denoise_decoder_features.append(x)\n        x = x + f_short_cut\n        x = self.act(self.aux_conv_fuse_0(x))\n        x = self.aux_conv_fuse_1(x)\n        res1 = x\n\n        encoder_features = []\n        x = self.act(self.aux_feature_conv_0(res1))\n        x = self.aux_feature_conv_1(x)\n        for color_correction, down in zip(self.aux_color_correction_blocks[:-1], self.aux_downsamples):\n            x = color_correction(x)\n            encoder_features.append(x)\n            x = down(x)\n        x = self.aux_color_correction_blocks[-1](x)\n        return x, res1, encoder_features", "\n\n@MODEL_REGISTRY.register()\nclass DNF(DNFBase):\n    def __init__(self, f_number, *,\n                block_size=1,\n                layers=4,\n                denoising_block='CID',\n                color_correction_block='MCC',\n                feedback_fuse='GFM'\n                ) -> None:\n        super(DNF, self).__init__(f_number=f_number, block_size=block_size, layers=layers,\n                                  denoising_block=denoising_block, color_correction_block=color_correction_block)\n        def get_class(class_or_class_str):\n            return eval(class_or_class_str) if isinstance(class_or_class_str, str) else class_or_class_str\n        \n        self.feedback_fuse_class = get_class(feedback_fuse)\n\n        self.feedback_fuses = nn.ModuleList([\n            self.feedback_fuse_class(in_channels=f_number * (2 ** idx)) for idx in range(layers)\n        ])\n\n        aux_outchannel = 3 if block_size == 1 else block_size * block_size\n        self.aux_denoising_blocks = nn.ModuleList([\n            ResidualSwitchBlock(\n                self.denoising_block_class(\n                    f_number=f_number * (2**idx),\n                    padding_mode=self.padding_mode\n                )   \n            )\n            for idx in range(layers)\n        ])\n        self.aux_upsamples = nn.ModuleList([\n            self.upsample_class(\n                f_number * (2**idx), \n                padding_mode=self.padding_mode\n            )\n            for idx in range(1, layers)\n        ])\n        self.denoising_decoder_fuses = nn.ModuleList([\n            self.decoder_fuse_class(in_channels=f_number * (2 ** idx)) for idx in range(layers - 1)\n        ])\n\n        self.aux_conv_fuse_0 = nn.Conv2d(f_number, f_number, 3, 1, 1, bias=True, padding_mode=self.padding_mode)\n        self.aux_conv_fuse_1 = nn.Conv2d(f_number, aux_outchannel, 1, 1, 0, bias=True)\n\n    def _pass_features_to_color_decoder(self, x, f_short_cut, encoder_features):\n        ## denoising decoder\n        denoise_decoder_features = []\n        for denoise, up, fuse, encoder_feature in reversed(list(zip(\n            self.aux_denoising_blocks[1:], \n            self.aux_upsamples, \n            self.denoising_decoder_fuses,\n            encoder_features    \n        ))):\n            x = denoise(x, 1)\n            denoise_decoder_features.append(x)\n            x = up(x)\n            x = fuse(x, encoder_feature)\n        x = self.aux_denoising_blocks[0](x, 1)\n        denoise_decoder_features.append(x)\n        x = x + f_short_cut\n        x = self.act(self.aux_conv_fuse_0(x))\n        x = self.aux_conv_fuse_1(x)\n        res1 = x\n\n        ## feedback, local residual switch on\n        encoder_features = []\n        denoise_decoder_features.reverse()\n        x = f_short_cut\n        for fuse, denoise, down, decoder_feedback_feature in zip(\n            self.feedback_fuses[:-1], \n            self.denoising_blocks[:-1], \n            self.downsamples,\n            denoise_decoder_features[:-1]\n        ):\n            x = fuse(x, decoder_feedback_feature)\n            x = denoise(x, 1)  # residual switch on\n            encoder_features.append(x)\n            x = down(x)\n        x = self.feedback_fuses[-1](x, denoise_decoder_features[-1])\n        x = self.denoising_blocks[-1](x, 1)  # residual switch on\n        \n        return x, res1, encoder_features", ""]}
{"filename": "models/dnf_utils/base.py", "chunked_list": ["from torch import nn\nfrom torch.nn import functional as F\n\nfrom .fuse import PDConvFuse\nfrom .cid import CID\nfrom .mcc import MCC\nfrom .sample import SimpleDownsample, SimpleUpsample\nfrom .resudual_switch import ResidualSwitchBlock\n\nfrom abc import abstractmethod", "\nfrom abc import abstractmethod\n\nclass DNFBase(nn.Module):\n    def __init__(self, f_number, *,\n                block_size=1,\n                layers=4,\n                denoising_block='CID',\n                color_correction_block='MCC'\n                ) -> None:\n        super().__init__()\n        def get_class(class_or_class_str):\n            return eval(class_or_class_str) if isinstance(class_or_class_str, str) else class_or_class_str\n\n        self.denoising_block_class = get_class(denoising_block)\n        self.color_correction_block_class = get_class(color_correction_block)\n        self.downsample_class = SimpleDownsample\n        self.upsample_class = SimpleUpsample\n        self.decoder_fuse_class = PDConvFuse\n\n        self.padding_mode = 'reflect'\n        self.act = nn.GELU()\n        self.layers = layers\n\n        head = [2 ** layer for layer in range(layers)]\n        self.block_size = block_size\n        inchannel = 3 if block_size == 1 else block_size * block_size\n        outchannel = 3 * block_size * block_size\n\n        self.feature_conv_0 = nn.Conv2d(inchannel, f_number, 5, 1, 2, bias=True, padding_mode=self.padding_mode)\n        self.feature_conv_1 = nn.Conv2d(f_number, f_number, 5, 1, 2, bias=True, padding_mode=self.padding_mode)\n\n        self.downsamples = nn.ModuleList([\n            self.downsample_class(\n                f_number * (2**idx), \n                padding_mode=self.padding_mode\n            )\n            for idx in range(layers - 1)\n        ])\n\n        self.upsamples = nn.ModuleList([\n            self.upsample_class(\n                f_number * (2**idx), \n                padding_mode=self.padding_mode\n            )\n            for idx in range(1, layers)\n        ])\n\n        self.denoising_blocks = nn.ModuleList([\n            ResidualSwitchBlock(\n                self.denoising_block_class(\n                    f_number=f_number * (2**idx),\n                    padding_mode=self.padding_mode\n                )\n            )\n            for idx in range(layers)\n        ])\n            \n        self.color_correction_blocks = nn.ModuleList([\n            self.color_correction_block_class(\n                f_number=f_number * (2 ** idx),\n                num_heads=head[idx],\n                padding_mode=self.padding_mode,\n            )\n            for idx in range(layers)\n        ])\n\n        self.color_decoder_fuses = nn.ModuleList([\n            self.decoder_fuse_class(in_channels=f_number * (2 ** idx)) for idx in range(layers - 1)\n        ])\n\n        self.conv_fuse_0 = nn.Conv2d(f_number, f_number, 3, 1, 1, bias=True, padding_mode=self.padding_mode)\n        self.conv_fuse_1 = nn.Conv2d(f_number, outchannel, 1, 1, 0, bias=True)\n\n        if block_size > 1:\n            self.pixel_shuffle = nn.PixelShuffle(block_size)\n        else:\n            self.pixel_shuffle = nn.Identity()\n\n    @abstractmethod\n    def _pass_features_to_color_decoder(self, x, f_short_cut, encoder_features):\n        pass\n\n    def _check_and_padding(self, x):\n        # Calculate the required size based on the input size and required factor\n        _, _, h, w = x.size()\n        stride = (2 ** (self.layers - 1))\n\n        # Calculate the number of pixels needed to reach the required size\n        dh = -h % stride\n        dw = -w % stride\n\n        # Calculate the amount of padding needed for each side\n        top_pad = dh // 2\n        bottom_pad = dh - top_pad\n        left_pad = dw // 2\n        right_pad = dw - left_pad\n        self.crop_indices = (left_pad, w+left_pad, top_pad, h+top_pad)\n\n        # Pad the tensor with reflect mode\n        padded_tensor = F.pad(\n            x, (left_pad, right_pad, top_pad, bottom_pad), mode=\"reflect\"\n        )\n\n        return padded_tensor\n        \n    def _check_and_crop(self, x, res1):\n        left, right, top, bottom = self.crop_indices\n        x = x[:, :, top*self.block_size:bottom*self.block_size, left*self.block_size:right*self.block_size]\n        res1 = res1[:, :, top:bottom, left:right] if res1 is not None else None\n        return x, res1\n\n    def forward(self, x):\n        x = self._check_and_padding(x)\n        x = self.act(self.feature_conv_0(x))\n        x = self.feature_conv_1(x)\n        f_short_cut = x\n\n        ## encoder, local residual switch off\n        encoder_features = []\n        for denoise, down in zip(self.denoising_blocks[:-1], self.downsamples):\n            x = denoise(x, 0)  # residual switch off\n            encoder_features.append(x)\n            x = down(x)\n        x = self.denoising_blocks[-1](x, 0)  # residual switch off\n\n        x, res1, refined_encoder_features = self._pass_features_to_color_decoder(x, f_short_cut, encoder_features) \n\n        ## color correction\n        for color_correction, up, fuse, encoder_feature in reversed(list(zip(\n            self.color_correction_blocks[1:], \n            self.upsamples, \n            self.color_decoder_fuses,\n            refined_encoder_features\n        ))):\n            x = color_correction(x)\n            x = up(x)\n            x = fuse(x, encoder_feature)\n        x = self.color_correction_blocks[0](x)\n\n        x = self.act(self.conv_fuse_0(x))\n        x = self.conv_fuse_1(x)\n        x = self.pixel_shuffle(x)\n        rgb, raw = self._check_and_crop(x, res1)\n        return rgb, raw", ""]}
{"filename": "models/dnf_utils/mcc.py", "chunked_list": ["from einops import rearrange\nimport torch\nfrom torch import nn\n\nfrom ..utils import LayerNorm\n\nclass MCC(nn.Module):\n    def __init__(self, f_number, num_heads, padding_mode, bias=False) -> None:\n        super().__init__()\n        self.norm = LayerNorm(f_number, eps=1e-6, data_format='channels_first')\n\n        self.num_heads = num_heads\n        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n        self.pwconv = nn.Conv2d(f_number, f_number * 3, kernel_size=1, bias=bias)\n        self.dwconv = nn.Conv2d(f_number * 3, f_number * 3, 3, 1, 1, bias=bias, padding_mode=padding_mode, groups=f_number * 3)\n        self.project_out = nn.Conv2d(f_number, f_number, kernel_size=1, bias=bias)\n        self.feedforward = nn.Sequential(\n            nn.Conv2d(f_number, f_number, 1, 1, 0, bias=bias),\n            nn.GELU(),\n            nn.Conv2d(f_number, f_number, 3, 1, 1, bias=bias, groups=f_number, padding_mode=padding_mode),\n            nn.GELU()\n        )\n\n    def forward(self, x):\n        attn = self.norm(x)\n        _, _, h, w = attn.shape\n\n        qkv = self.dwconv(self.pwconv(attn))\n        q, k, v = qkv.chunk(3, dim=1)\n\n        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n\n        q = torch.nn.functional.normalize(q, dim=-1)\n        k = torch.nn.functional.normalize(k, dim=-1)\n\n        attn = (q @ k.transpose(-2, -1)) * self.temperature\n        attn = attn.softmax(dim=-1)\n\n        out = (attn @ v)\n\n        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n\n        out = self.project_out(out)\n        out = self.feedforward(out + x)\n        return out", ""]}
{"filename": "models/dnf_utils/__init__.py", "chunked_list": [""]}
{"filename": "models/dnf_utils/cid.py", "chunked_list": ["from torch import nn\n\n# CI\nclass DConv7(nn.Module):\n    def __init__(self, f_number, padding_mode='reflect') -> None:\n        super().__init__()\n        self.dconv = nn.Conv2d(f_number, f_number, kernel_size=7, padding=3, groups=f_number, padding_mode=padding_mode)\n\n    def forward(self, x):\n        return self.dconv(x)", "\n# Post-CI\nclass MLP(nn.Module):\n    def __init__(self, f_number, excitation_factor=2) -> None:\n        super().__init__()\n        self.act = nn.GELU()\n        self.pwconv1 = nn.Conv2d(f_number, excitation_factor * f_number, kernel_size=1)\n        self.pwconv2 = nn.Conv2d(f_number * excitation_factor, f_number, kernel_size=1)\n\n    def forward(self, x):\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        return x", "    \nclass CID(nn.Module):\n    def __init__(self, f_number, padding_mode) -> None:\n        super().__init__()\n        self.channel_independent = DConv7(f_number, padding_mode)\n        self.channel_dependent = MLP(f_number, excitation_factor=2)\n\n    def forward(self, x):\n        return self.channel_dependent(self.channel_independent(x))\n", ""]}
{"filename": "models/dnf_utils/sample.py", "chunked_list": ["from torch import nn\n\nclass SimpleDownsample(nn.Module):\n    def __init__(self, dim, *, padding_mode='reflect'):\n        super().__init__()\n        self.body = nn.Conv2d(dim, dim*2, kernel_size=2, stride=2, padding=0, bias=False, padding_mode=padding_mode)\n\n    def forward(self, x):\n        return self.body(x)\n\nclass SimpleUpsample(nn.Module):\n    def __init__(self, dim, *, padding_mode='reflect'):\n        super().__init__()\n        self.body = nn.ConvTranspose2d(dim, dim//2, kernel_size=2, stride=2, padding=0, bias=False)\n\n    def forward(self, x):\n        return self.body(x)", "\nclass SimpleUpsample(nn.Module):\n    def __init__(self, dim, *, padding_mode='reflect'):\n        super().__init__()\n        self.body = nn.ConvTranspose2d(dim, dim//2, kernel_size=2, stride=2, padding=0, bias=False)\n\n    def forward(self, x):\n        return self.body(x)\n", ""]}
{"filename": "models/dnf_utils/fuse.py", "chunked_list": ["import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass PDConvFuse(nn.Module):\n    def __init__(self, in_channels=None, f_number=None, feature_num=2, bias=True, **kwargs) -> None:\n        super().__init__()\n        if in_channels is None:\n            assert f_number is not None\n            in_channels = f_number\n        self.feature_num = feature_num\n        self.act = nn.GELU()\n        self.pwconv = nn.Conv2d(feature_num * in_channels, in_channels, 1, 1, 0, bias=bias)\n        self.dwconv = nn.Conv2d(in_channels, in_channels, 3, 1, 1, bias=bias, groups=in_channels, padding_mode='reflect')\n\n    def forward(self, *inp_feats):\n        assert len(inp_feats) == self.feature_num\n        return self.dwconv(self.act(self.pwconv(torch.cat(inp_feats, dim=1))))", "\nclass GFM(nn.Module):\n    def __init__(self, in_channels, feature_num=2, bias=True, padding_mode='reflect', **kwargs) -> None:\n        super().__init__()\n        self.feature_num = feature_num\n\n        hidden_features = in_channels * feature_num\n        self.pwconv = nn.Conv2d(hidden_features, hidden_features * 2, 1, 1, 0, bias=bias)\n        self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, 3, 1, 1, bias=bias, padding_mode=padding_mode, groups=hidden_features * 2)\n        self.project_out = nn.Conv2d(hidden_features, in_channels, kernel_size=1, bias=bias)\n        self.mlp = nn.Conv2d(in_channels, in_channels, 1, 1, 0, bias=True)\n\n    def forward(self, *inp_feats):\n        assert len(inp_feats) == self.feature_num\n        shortcut = inp_feats[0]\n        x = torch.cat(inp_feats, dim=1)\n        x = self.pwconv(x)\n        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n        x = F.gelu(x1) * x2\n        x = self.project_out(x)\n        return self.mlp(x + shortcut)", ""]}
{"filename": "models/dnf_utils/resudual_switch.py", "chunked_list": ["from torch import nn\n\n\nclass ResidualSwitchBlock(nn.Module):\n    def __init__(self, block) -> None:\n        super().__init__()\n        self.block = block\n        \n    def forward(self, x, residual_switch):\n        return self.block(x) + residual_switch * x"]}
{"filename": "forwards/single_stage_forward.py", "chunked_list": ["from utils.registry import FORWARD_REGISTRY\nfrom fvcore.nn import FlopCountAnalysis, flop_count_table\n\n\n@FORWARD_REGISTRY.register()\ndef ss_train_forward(config, model, data):\n    raw = data['noisy_raw'].cuda(non_blocking=True)\n    rgb_gt = data['clean_rgb'].cuda(non_blocking=True)\n    \n    rgb_out = model(raw)\n    ###### | output          | label\n    return {'rgb': rgb_out}, {'rgb': rgb_gt}", "\n\n@FORWARD_REGISTRY.register()\ndef ss_test_forward(config, model, data):\n    if not config['test'].get('cpu', False):\n        raw = data['noisy_raw'].cuda(non_blocking=True)\n        rgb_gt = data['clean_rgb'].cuda(non_blocking=True)\n    else:\n        raw = data['noisy_raw']\n        rgb_gt = data['clean_rgb']\n    img_files = data['img_file']\n    lbl_files = data['lbl_file']\n\n    rgb_out = model(raw)\n\n    return {'rgb': rgb_out}, {'rgb': rgb_gt}, img_files, lbl_files", "\n\n@FORWARD_REGISTRY.register()  # without label, for inference only\ndef ss_inference(config, model, data):\n    raw = data['noisy_raw'].cuda(non_blocking=True)\n    img_files = data['img_file']\n\n    rgb_out = model(raw)\n\n    ###### | output          | img names\n    return {'rgb': rgb_out}, img_files", ""]}
{"filename": "forwards/__init__.py", "chunked_list": ["import importlib\nfrom copy import deepcopy\nfrom os import path as osp\nfrom glob import glob\n\nfrom utils.registry import FORWARD_REGISTRY\n\n__all__ = ['build_forwards', 'build_profile']\n\n# automatically scan and import forward modules for registry", "\n# automatically scan and import forward modules for registry\n# scan all the files under the 'forwards' folder and collect files ending with '_forward.py'\nforward_folder = osp.dirname(osp.abspath(__file__))\nforward_filenames = [osp.splitext(osp.basename(v))[0] for v in glob(osp.join(forward_folder, '*_forward.py'))]\n# import all the forward modules\n_forward_modules = [importlib.import_module(f'forwards.{file_name}') for file_name in forward_filenames]\n\n\ndef build_forwards(cfg):\n    cfg = deepcopy(cfg)\n    train_fwd_type = cfg['train']['forward_type']\n    test_fwd_type = cfg['test']['forward_type']\n    train_forward = FORWARD_REGISTRY.get(train_fwd_type)\n    test_forward = FORWARD_REGISTRY.get(test_fwd_type)\n    return train_forward, test_forward", "\ndef build_forwards(cfg):\n    cfg = deepcopy(cfg)\n    train_fwd_type = cfg['train']['forward_type']\n    test_fwd_type = cfg['test']['forward_type']\n    train_forward = FORWARD_REGISTRY.get(train_fwd_type)\n    test_forward = FORWARD_REGISTRY.get(test_fwd_type)\n    return train_forward, test_forward\n\ndef build_forward(forward_type):\n    return FORWARD_REGISTRY.get(forward_type)", "\ndef build_forward(forward_type):\n    return FORWARD_REGISTRY.get(forward_type)\n\ndef build_profile(cfg):\n    cfg = deepcopy(cfg)\n    profile = cfg.get('profile')\n    if profile is None:\n        return profile\n    return FORWARD_REGISTRY.get(profile)", ""]}
{"filename": "forwards/dnf_forward.py", "chunked_list": ["from utils.registry import FORWARD_REGISTRY\nfrom fvcore.nn import FlopCountAnalysis, flop_count_table\n\n\n@FORWARD_REGISTRY.register(suffix='DNF')\ndef train_forward(config, model, data):\n    raw = data['noisy_raw'].cuda(non_blocking=True)\n    raw_gt = data['clean_raw'].cuda(non_blocking=True)\n    rgb_gt = data['clean_rgb'].cuda(non_blocking=True)\n    \n    rgb_out, raw_out = model(raw)\n    ###### | output                          | label\n    return {'rgb': rgb_out, 'raw': raw_out}, {'rgb': rgb_gt, 'raw': raw_gt}", "\n\n@FORWARD_REGISTRY.register(suffix='DNF')\ndef test_forward(config, model, data):\n    if not config['test'].get('cpu', False):\n        raw = data['noisy_raw'].cuda(non_blocking=True)\n        raw_gt = data['clean_raw'].cuda(non_blocking=True)\n        rgb_gt = data['clean_rgb'].cuda(non_blocking=True)\n    else:\n        raw = data['noisy_raw']\n        raw_gt = data['clean_raw']\n        rgb_gt = data['clean_rgb']\n    img_files = data['img_file']\n    lbl_files = data['lbl_file']\n\n    rgb_out, raw_out = model(raw)\n\n    ###### | output                          | label                         | img and label names\n    return {'rgb': rgb_out, 'raw': raw_out}, {'rgb': rgb_gt, 'raw': raw_gt}, img_files, lbl_files", "\n\n@FORWARD_REGISTRY.register(suffix='DNF')  # without label, for inference only\ndef inference(config, model, data):\n    raw = data['noisy_raw'].cuda(non_blocking=True)\n    img_files = data['img_file']\n\n    rgb_out, raw_out = model(raw)\n\n    ###### | output                          | img names\n    return {'rgb': rgb_out, 'raw': raw_out}, img_files", "\n\n@FORWARD_REGISTRY.register()\ndef DNF_profile(config, model, data, logger):\n    x = data['noisy_raw'].cuda()\n    flops = FlopCountAnalysis(model, x)\n    logger.info('Detaild FLOPs:\\n' + flop_count_table(flops))\n    flops_total = flops.total()\n    logger.info(f\"Total FLOPs: {flops_total:,}\")\n", ""]}
{"filename": "datasets/MCR_dict_dataset.py", "chunked_list": ["#!/usr/bin/env python\n\nimport os\n\nimport numpy as np\nimport rawpy\nimport torch\nfrom torch.utils import data\nimport time\n", "import time\n\nfrom utils.registry import DATASET_REGISTRY\nfrom timm.utils import AverageMeter\nimport imageio\nimport tqdm\n\n@DATASET_REGISTRY.register()\nclass MCRDictSet(data.Dataset):\n\n    def __init__(self, data_path, image_list_file, patch_size=None, split='train', load_npy=True, repeat=1,\n                 raw_ext='ARW', max_samples=None, max_clip=1.0, min_clip=None, only_00=False,\n                 transpose=True, h_flip=True, v_flip=True, rotation=False, ratio=True, **kwargs):\n        \"\"\"\n        :param data_path: dataset directory\n        :param image_list_file: contains image file names under data_path\n        :param patch_size: if None, full images are returned, otherwise patches are returned\n        :param split: train or valid\n        :param upper: max number of image used for debug\n        \"\"\"\n        assert os.path.exists(data_path), \"data_path: {} not found.\".format(data_path)\n        self.data_path = data_path\n        image_list_file = os.path.join(data_path, image_list_file)\n        assert os.path.exists(image_list_file), \"image_list_file: {} not found.\".format(image_list_file)\n        self.image_list_file = image_list_file\n        self.patch_size = patch_size\n        self.split = split\n        self.load_npy = load_npy\n        self.raw_ext = raw_ext\n        self.max_clip = max_clip\n        self.min_clip = min_clip\n        self.transpose = transpose\n        self.h_flip = h_flip\n        self.v_flip = v_flip\n        self.rotation = rotation\n        self.ratio = ratio\n        self.only_00 = only_00\n        self.repeat = repeat\n\n        self.raw_short_read_time = AverageMeter()\n        self.raw_short_pack_time = AverageMeter()\n        self.raw_short_post_time = AverageMeter()\n        self.raw_long_read_time = AverageMeter()\n        self.raw_long_pack_time = AverageMeter()\n        self.raw_long_post_time = AverageMeter()\n        self.npy_long_read_time = AverageMeter()\n        self.data_aug_time = AverageMeter()\n        self.data_norm_time = AverageMeter()\n        self.count = 0\n\n        self.block_size = 2\n        self.black_level = 0\n        self.white_level = 255\n\n        self.raw_input_path = []\n        self.raw_gt_path = []\n        self.rgb_gt_path = []\n        self.rgb_gt_dict = {}\n        self.raw_input_list = []\n        self.raw_gt_dict = {}\n        with open(self.image_list_file, 'r') as f:\n            for i, img_pair in enumerate(f):\n                raw_input_path, raw_gt_path, rgb_gt_path = img_pair.strip().split(' ')\n                self.raw_input_path.append(os.path.join(self.data_path, raw_input_path))\n                self.raw_gt_path.append(os.path.join(self.data_path, raw_gt_path))\n                self.rgb_gt_path.append(os.path.join(self.data_path, rgb_gt_path))\n                raw_input = imageio.imread(os.path.join(self.data_path, raw_input_path))\n                self.raw_input_list.append(raw_input)\n\n                raw_gt = imageio.imread(os.path.join(self.data_path, raw_gt_path))\n                raw_gt_name = os.path.basename(raw_gt_path)\n                if raw_gt_name not in self.raw_gt_dict:\n                    self.raw_gt_dict[raw_gt_name] = raw_gt\n\n                rgb_gt = imageio.imread(os.path.join(self.data_path, rgb_gt_path)).transpose(2, 0, 1)\n                rgb_gt_name = os.path.basename(rgb_gt_path)\n                if rgb_gt_name not in self.rgb_gt_dict:\n                    self.rgb_gt_dict[rgb_gt_name] = rgb_gt\n\n                if max_samples and i == max_samples - 1:  # for debug purpose\n                    break\n\n        print(\"processing: {} images for {}\".format(len(self.raw_input_path), self.split))\n\n\n    def __len__(self):\n        return len(self.raw_input_path) * self.repeat\n\n    def print_time(self):\n        print('self.raw_short_read_time:', self.raw_short_read_time.avg)\n        print('self.raw_short_pack_time:', self.raw_short_pack_time.avg)\n        print('self.raw_short_post_time:', self.raw_short_post_time.avg)\n        print('self.raw_long_read_time:', self.raw_long_read_time.avg)\n        print('self.raw_long_pack_time:', self.raw_long_pack_time.avg)\n        print('self.raw_long_post_time:', self.raw_long_post_time.avg)\n        print('self.npy_long_read_time:', self.npy_long_read_time.avg)\n        print('self.data_aug_time:', self.data_aug_time.avg)\n        print('self.data_norm_time:', self.data_norm_time.avg)\n\n    def __getitem__(self, index):\n        self.count += 1\n        idx = index // self.repeat\n        if self.count % 100 == 0 and False:\n            self.print_time()\n        info = self.raw_input_path[idx]\n        img_file = info\n\n        start = time.time()\n        noisy_raw = self.raw_input_list[idx]\n        if self.patch_size is None:\n            # pack raw with patch size is implemented in clip patch for reduce computation\n            noisy_raw = self._pack_raw(noisy_raw)\n        self.raw_short_read_time.update(time.time() - start)\n\n        lbl_file = self.rgb_gt_path[idx]\n        start = time.time()\n        clean_rgb = self.rgb_gt_dict[os.path.basename(self.rgb_gt_path[idx])]\n        self.raw_long_post_time.update(time.time() - start)\n\n        start = time.time()\n        clean_raw = self.raw_gt_dict[os.path.basename(self.raw_gt_path[idx])]\n        if self.patch_size is None:\n            clean_raw = self._pack_raw(clean_raw)\n        self.raw_long_read_time.update(time.time() - start)\n\n        if self.patch_size:\n            start = time.time()\n            patch_size = self.patch_size\n            H, W = clean_rgb.shape[1:3]\n            if self.split == 'train':\n                if (H - patch_size) // self.block_size > 0:\n                    yy = torch.randint(0, (H - patch_size) // self.block_size, (1,))\n                else:\n                    yy = 0\n                if (W - patch_size) // self.block_size > 0:\n                    xx = torch.randint(0, (W - patch_size) // self.block_size, (1,))\n                else:\n                    xx = 0\n                # yy, xx = torch.randint(0, (H - patch_size) // self.block_size, (1,)),  torch.randint(0, (W - patch_size) // self.block_size, (1,))\n            else:\n                yy, xx = (H - patch_size) // self.block_size // 2, (W - patch_size) // self.block_size // 2\n            input_patch = self._pack_raw(noisy_raw, yy, xx)\n            clean_raw_patch = self._pack_raw(clean_raw, yy, xx)\n            gt_patch = clean_rgb[:, yy*self.block_size:yy*self.block_size + patch_size, xx*self.block_size:xx*self.block_size + patch_size]\n\n            if self.h_flip and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random horizontal flip\n                input_patch = np.flip(input_patch, axis=2)\n                gt_patch = np.flip(gt_patch, axis=2)\n                clean_raw_patch = np.flip(clean_raw_patch, axis=2)\n            if self.v_flip and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random vertical flip\n                input_patch = np.flip(input_patch, axis=1)\n                gt_patch = np.flip(gt_patch, axis=1)\n                clean_raw_patch = np.flip(clean_raw_patch, axis=1)\n            if self.transpose and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random transpose\n                input_patch = np.transpose(input_patch, (0, 2, 1))\n                gt_patch = np.transpose(gt_patch, (0, 2, 1))\n                clean_raw_patch = np.transpose(clean_raw_patch, (0, 2, 1))\n            if self.rotation and self.split == 'train':\n                raise NotImplementedError('rotation')\n\n            noisy_raw = input_patch.copy()\n            clean_rgb = gt_patch.copy()\n            clean_raw = clean_raw_patch.copy()\n            self.data_aug_time.update(time.time() - start)\n\n        start = time.time()\n        noisy_raw = (np.float32(noisy_raw) - self.black_level) / np.float32(self.white_level - self.black_level)  # subtract the black level\n        clean_raw = (np.float32(clean_raw) - self.black_level) / np.float32(self.white_level - self.black_level)\n        clean_rgb = np.float32(clean_rgb) / np.float32(255)\n        self.data_norm_time.update(time.time() - start)\n\n        img_num = int(self.raw_input_path[idx][-23:-20])\n        img_expo = int(self.raw_input_path[idx][-8:-4],16)\n\n        if img_num < 500:\n            gt_expo = 12287\n        else:\n            gt_expo = 1023\n        ratio = gt_expo / img_expo\n\n        if self.ratio:\n            noisy_raw = noisy_raw * ratio\n        if self.max_clip is not None:\n            noisy_raw = np.minimum(noisy_raw, self.max_clip)\n        if self.min_clip is not None:\n            noisy_raw = np.maximum(noisy_raw, self.min_clip)\n\n        clean_rgb = clean_rgb.clip(0.0, 1.0)\n\n        noisy_raw = torch.from_numpy(noisy_raw).float()\n        clean_rgb = torch.from_numpy(clean_rgb).float()\n        clean_raw = torch.from_numpy(clean_raw).float()\n\n        return {\n            'noisy_raw': noisy_raw,\n            'clean_raw': clean_raw,\n            'clean_rgb': clean_rgb,\n            'img_file': img_file,\n            'lbl_file': lbl_file,\n            'img_exposure': img_expo,\n            'lbl_exposure': gt_expo,\n            'ratio': ratio\n        }\n\n    def _pack_raw(self, raw, hh=None, ww=None):\n        if self.patch_size is None:\n            assert hh is None and ww is None\n        # pack Bayer image to 4 channels (RGBG)\n        # im = raw.raw_image_visible.astype(np.uint16)\n\n        H, W = raw.shape\n        im = np.expand_dims(raw, axis=0)\n        if self.patch_size is None:\n            out = np.concatenate((im[:, 0:H:2, 0:W:2],\n                                  im[:, 0:H:2, 1:W:2],\n                                  im[:, 1:H:2, 1:W:2],\n                                  im[:, 1:H:2, 0:W:2]), axis=0)\n        else:\n            h1 = hh * 2\n            h2 = hh * 2 + self.patch_size\n            w1 = ww * 2\n            w2 = ww * 2 + self.patch_size\n            out = np.concatenate((im[:, h1:h2:2, w1:w2:2],\n                                  im[:, h1:h2:2, w1+1:w2:2],\n                                  im[:, h1+1:h2:2, w1+1:w2:2],\n                                  im[:, h1+1:h2:2, w1:w2:2]), axis=0)\n        return out", "class MCRDictSet(data.Dataset):\n\n    def __init__(self, data_path, image_list_file, patch_size=None, split='train', load_npy=True, repeat=1,\n                 raw_ext='ARW', max_samples=None, max_clip=1.0, min_clip=None, only_00=False,\n                 transpose=True, h_flip=True, v_flip=True, rotation=False, ratio=True, **kwargs):\n        \"\"\"\n        :param data_path: dataset directory\n        :param image_list_file: contains image file names under data_path\n        :param patch_size: if None, full images are returned, otherwise patches are returned\n        :param split: train or valid\n        :param upper: max number of image used for debug\n        \"\"\"\n        assert os.path.exists(data_path), \"data_path: {} not found.\".format(data_path)\n        self.data_path = data_path\n        image_list_file = os.path.join(data_path, image_list_file)\n        assert os.path.exists(image_list_file), \"image_list_file: {} not found.\".format(image_list_file)\n        self.image_list_file = image_list_file\n        self.patch_size = patch_size\n        self.split = split\n        self.load_npy = load_npy\n        self.raw_ext = raw_ext\n        self.max_clip = max_clip\n        self.min_clip = min_clip\n        self.transpose = transpose\n        self.h_flip = h_flip\n        self.v_flip = v_flip\n        self.rotation = rotation\n        self.ratio = ratio\n        self.only_00 = only_00\n        self.repeat = repeat\n\n        self.raw_short_read_time = AverageMeter()\n        self.raw_short_pack_time = AverageMeter()\n        self.raw_short_post_time = AverageMeter()\n        self.raw_long_read_time = AverageMeter()\n        self.raw_long_pack_time = AverageMeter()\n        self.raw_long_post_time = AverageMeter()\n        self.npy_long_read_time = AverageMeter()\n        self.data_aug_time = AverageMeter()\n        self.data_norm_time = AverageMeter()\n        self.count = 0\n\n        self.block_size = 2\n        self.black_level = 0\n        self.white_level = 255\n\n        self.raw_input_path = []\n        self.raw_gt_path = []\n        self.rgb_gt_path = []\n        self.rgb_gt_dict = {}\n        self.raw_input_list = []\n        self.raw_gt_dict = {}\n        with open(self.image_list_file, 'r') as f:\n            for i, img_pair in enumerate(f):\n                raw_input_path, raw_gt_path, rgb_gt_path = img_pair.strip().split(' ')\n                self.raw_input_path.append(os.path.join(self.data_path, raw_input_path))\n                self.raw_gt_path.append(os.path.join(self.data_path, raw_gt_path))\n                self.rgb_gt_path.append(os.path.join(self.data_path, rgb_gt_path))\n                raw_input = imageio.imread(os.path.join(self.data_path, raw_input_path))\n                self.raw_input_list.append(raw_input)\n\n                raw_gt = imageio.imread(os.path.join(self.data_path, raw_gt_path))\n                raw_gt_name = os.path.basename(raw_gt_path)\n                if raw_gt_name not in self.raw_gt_dict:\n                    self.raw_gt_dict[raw_gt_name] = raw_gt\n\n                rgb_gt = imageio.imread(os.path.join(self.data_path, rgb_gt_path)).transpose(2, 0, 1)\n                rgb_gt_name = os.path.basename(rgb_gt_path)\n                if rgb_gt_name not in self.rgb_gt_dict:\n                    self.rgb_gt_dict[rgb_gt_name] = rgb_gt\n\n                if max_samples and i == max_samples - 1:  # for debug purpose\n                    break\n\n        print(\"processing: {} images for {}\".format(len(self.raw_input_path), self.split))\n\n\n    def __len__(self):\n        return len(self.raw_input_path) * self.repeat\n\n    def print_time(self):\n        print('self.raw_short_read_time:', self.raw_short_read_time.avg)\n        print('self.raw_short_pack_time:', self.raw_short_pack_time.avg)\n        print('self.raw_short_post_time:', self.raw_short_post_time.avg)\n        print('self.raw_long_read_time:', self.raw_long_read_time.avg)\n        print('self.raw_long_pack_time:', self.raw_long_pack_time.avg)\n        print('self.raw_long_post_time:', self.raw_long_post_time.avg)\n        print('self.npy_long_read_time:', self.npy_long_read_time.avg)\n        print('self.data_aug_time:', self.data_aug_time.avg)\n        print('self.data_norm_time:', self.data_norm_time.avg)\n\n    def __getitem__(self, index):\n        self.count += 1\n        idx = index // self.repeat\n        if self.count % 100 == 0 and False:\n            self.print_time()\n        info = self.raw_input_path[idx]\n        img_file = info\n\n        start = time.time()\n        noisy_raw = self.raw_input_list[idx]\n        if self.patch_size is None:\n            # pack raw with patch size is implemented in clip patch for reduce computation\n            noisy_raw = self._pack_raw(noisy_raw)\n        self.raw_short_read_time.update(time.time() - start)\n\n        lbl_file = self.rgb_gt_path[idx]\n        start = time.time()\n        clean_rgb = self.rgb_gt_dict[os.path.basename(self.rgb_gt_path[idx])]\n        self.raw_long_post_time.update(time.time() - start)\n\n        start = time.time()\n        clean_raw = self.raw_gt_dict[os.path.basename(self.raw_gt_path[idx])]\n        if self.patch_size is None:\n            clean_raw = self._pack_raw(clean_raw)\n        self.raw_long_read_time.update(time.time() - start)\n\n        if self.patch_size:\n            start = time.time()\n            patch_size = self.patch_size\n            H, W = clean_rgb.shape[1:3]\n            if self.split == 'train':\n                if (H - patch_size) // self.block_size > 0:\n                    yy = torch.randint(0, (H - patch_size) // self.block_size, (1,))\n                else:\n                    yy = 0\n                if (W - patch_size) // self.block_size > 0:\n                    xx = torch.randint(0, (W - patch_size) // self.block_size, (1,))\n                else:\n                    xx = 0\n                # yy, xx = torch.randint(0, (H - patch_size) // self.block_size, (1,)),  torch.randint(0, (W - patch_size) // self.block_size, (1,))\n            else:\n                yy, xx = (H - patch_size) // self.block_size // 2, (W - patch_size) // self.block_size // 2\n            input_patch = self._pack_raw(noisy_raw, yy, xx)\n            clean_raw_patch = self._pack_raw(clean_raw, yy, xx)\n            gt_patch = clean_rgb[:, yy*self.block_size:yy*self.block_size + patch_size, xx*self.block_size:xx*self.block_size + patch_size]\n\n            if self.h_flip and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random horizontal flip\n                input_patch = np.flip(input_patch, axis=2)\n                gt_patch = np.flip(gt_patch, axis=2)\n                clean_raw_patch = np.flip(clean_raw_patch, axis=2)\n            if self.v_flip and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random vertical flip\n                input_patch = np.flip(input_patch, axis=1)\n                gt_patch = np.flip(gt_patch, axis=1)\n                clean_raw_patch = np.flip(clean_raw_patch, axis=1)\n            if self.transpose and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random transpose\n                input_patch = np.transpose(input_patch, (0, 2, 1))\n                gt_patch = np.transpose(gt_patch, (0, 2, 1))\n                clean_raw_patch = np.transpose(clean_raw_patch, (0, 2, 1))\n            if self.rotation and self.split == 'train':\n                raise NotImplementedError('rotation')\n\n            noisy_raw = input_patch.copy()\n            clean_rgb = gt_patch.copy()\n            clean_raw = clean_raw_patch.copy()\n            self.data_aug_time.update(time.time() - start)\n\n        start = time.time()\n        noisy_raw = (np.float32(noisy_raw) - self.black_level) / np.float32(self.white_level - self.black_level)  # subtract the black level\n        clean_raw = (np.float32(clean_raw) - self.black_level) / np.float32(self.white_level - self.black_level)\n        clean_rgb = np.float32(clean_rgb) / np.float32(255)\n        self.data_norm_time.update(time.time() - start)\n\n        img_num = int(self.raw_input_path[idx][-23:-20])\n        img_expo = int(self.raw_input_path[idx][-8:-4],16)\n\n        if img_num < 500:\n            gt_expo = 12287\n        else:\n            gt_expo = 1023\n        ratio = gt_expo / img_expo\n\n        if self.ratio:\n            noisy_raw = noisy_raw * ratio\n        if self.max_clip is not None:\n            noisy_raw = np.minimum(noisy_raw, self.max_clip)\n        if self.min_clip is not None:\n            noisy_raw = np.maximum(noisy_raw, self.min_clip)\n\n        clean_rgb = clean_rgb.clip(0.0, 1.0)\n\n        noisy_raw = torch.from_numpy(noisy_raw).float()\n        clean_rgb = torch.from_numpy(clean_rgb).float()\n        clean_raw = torch.from_numpy(clean_raw).float()\n\n        return {\n            'noisy_raw': noisy_raw,\n            'clean_raw': clean_raw,\n            'clean_rgb': clean_rgb,\n            'img_file': img_file,\n            'lbl_file': lbl_file,\n            'img_exposure': img_expo,\n            'lbl_exposure': gt_expo,\n            'ratio': ratio\n        }\n\n    def _pack_raw(self, raw, hh=None, ww=None):\n        if self.patch_size is None:\n            assert hh is None and ww is None\n        # pack Bayer image to 4 channels (RGBG)\n        # im = raw.raw_image_visible.astype(np.uint16)\n\n        H, W = raw.shape\n        im = np.expand_dims(raw, axis=0)\n        if self.patch_size is None:\n            out = np.concatenate((im[:, 0:H:2, 0:W:2],\n                                  im[:, 0:H:2, 1:W:2],\n                                  im[:, 1:H:2, 1:W:2],\n                                  im[:, 1:H:2, 0:W:2]), axis=0)\n        else:\n            h1 = hh * 2\n            h2 = hh * 2 + self.patch_size\n            w1 = ww * 2\n            w2 = ww * 2 + self.patch_size\n            out = np.concatenate((im[:, h1:h2:2, w1:w2:2],\n                                  im[:, h1:h2:2, w1+1:w2:2],\n                                  im[:, h1+1:h2:2, w1+1:w2:2],\n                                  im[:, h1+1:h2:2, w1:w2:2]), axis=0)\n        return out"]}
{"filename": "datasets/single_bayer_dict_dataset.py", "chunked_list": ["#!/usr/bin/env python\n\nimport os\n\nimport numpy as np\nimport rawpy\nimport torch\nfrom torch.utils import data\nfrom glob import glob\nimport time", "from glob import glob\nimport time\n\nfrom utils.registry import DATASET_REGISTRY\nfrom timm.utils import AverageMeter\n\nclass BaseDictSet(data.Dataset):\n\n    def __init__(self, data_path, load_npy=True,\n                 max_clip=1.0, min_clip=None, ratio=1, **kwargs):\n        \"\"\"\n        :param data_path: dataset directory\n        :param image_list_file: contains image file names under data_path\n        :param patch_size: if None, full images are returned, otherwise patches are returned\n        :param split: train or valid\n        :param upper: max number of image used for debug\n        \"\"\"\n        assert os.path.exists(data_path), \"data_path: {} not found.\".format(data_path)\n        self.data_path = data_path\n        self.load_npy = load_npy\n        self.max_clip = max_clip\n        self.min_clip = min_clip\n        self.ratio = ratio\n\n        self.raw_short_read_time = AverageMeter()\n        self.raw_short_pack_time = AverageMeter()\n        self.data_norm_time = AverageMeter()\n        self.count = 0\n\n        self.img_info = []\n        \n        img_list = sorted(glob(f'{self.data_path}/*'))\n        for i, img_file in enumerate(img_list):\n            img_file = os.path.basename(img_file)\n            self.img_info.append({\n                'img': img_file,\n                'ratio': np.float32(ratio)\n            })\n        print(\"processing: {} images\".format(len(self.img_info)))\n\n\n    def __len__(self):\n        return len(self.img_info)\n\n    def print_time(self):\n        print('self.raw_short_read_time:', self.raw_short_read_time.avg)\n        print('self.raw_short_pack_time:', self.raw_short_pack_time.avg)\n        print('self.data_norm_time:', self.data_norm_time.avg)\n\n    def __getitem__(self, index):\n        self.count += 1\n        if self.count % 100 == 0 and False:\n            self.print_time()\n        info = self.img_info[index]\n\n        img_file = info['img']\n        if not self.load_npy:\n            start = time.time()\n            raw = rawpy.imread(os.path.join(self.data_path, img_file))\n            self.raw_short_read_time.update(time.time() - start)\n            start = time.time()\n            noisy_raw = self._pack_raw(raw)\n            self.raw_short_pack_time.update(time.time() - start)\n        else:\n            start = time.time()\n            noisy_raw = np.load(os.path.join(self.data_path, img_file), allow_pickle=True)\n            self.raw_short_read_time.update(time.time() - start)\n\n        start = time.time()\n        noisy_raw = (np.float32(noisy_raw) - self.black_level) / np.float32(self.white_level - self.black_level)  # subtract the black level\n        self.data_norm_time.update(time.time() - start)\n\n        if self.ratio:\n            noisy_raw = noisy_raw * info['ratio']\n        if self.max_clip is not None:\n            noisy_raw = np.minimum(noisy_raw, self.max_clip)\n        if self.min_clip is not None:\n            noisy_raw = np.maximum(noisy_raw, self.min_clip)\n\n        noisy_raw = torch.from_numpy(noisy_raw).float()\n\n        return {\n            'noisy_raw': noisy_raw,\n            'img_file': img_file,\n            'ratio': info['ratio']\n        }", "\n\n@DATASET_REGISTRY.register()\nclass SingleBayerDictSet(BaseDictSet):\n    def __init__(self, data_path, load_npy=True, max_clip=1, min_clip=None, ratio=1, black_level=512, white_level=16383, **kwargs):\n        super().__init__(data_path, load_npy, max_clip, min_clip, ratio, **kwargs)\n        self.block_size = 2\n        self.black_level = black_level\n        self.white_level = white_level\n\n    def _pack_raw(self, raw):\n        # pack Bayer image to 4 channels (RGBG)\n        im = raw.raw_image_visible.astype(np.uint16)\n\n        H, W = im.shape\n        im = np.expand_dims(im, axis=0)\n        out = np.concatenate((im[:, 0:H:2, 0:W:2],\n                              im[:, 0:H:2, 1:W:2],\n                              im[:, 1:H:2, 1:W:2],\n                              im[:, 1:H:2, 0:W:2]), axis=0)\n        return out", ""]}
{"filename": "datasets/SID_dict_dataset.py", "chunked_list": ["#!/usr/bin/env python\n\nimport os\n\nimport numpy as np\nimport rawpy\nimport torch\nfrom torch.utils import data\nimport time\n", "import time\n\nfrom utils.registry import DATASET_REGISTRY\nfrom timm.utils import AverageMeter\n\nclass BaseDictSet(data.Dataset):\n\n    def __init__(self, data_path, image_list_file, patch_size=None, split='train', load_npy=True, repeat=1,\n                 raw_ext='ARW', max_samples=None, max_clip=1.0, min_clip=None, only_00=False,\n                 transpose=True, h_flip=True, v_flip=True, rotation=False, ratio=True, **kwargs):\n        \"\"\"\n        :param data_path: dataset directory\n        :param image_list_file: contains image file names under data_path\n        :param patch_size: if None, full images are returned, otherwise patches are returned\n        :param split: train or valid\n        :param upper: max number of image used for debug\n        \"\"\"\n        assert os.path.exists(data_path), \"data_path: {} not found.\".format(data_path)\n        self.data_path = data_path\n        image_list_file = os.path.join(data_path, image_list_file)\n        assert os.path.exists(image_list_file), \"image_list_file: {} not found.\".format(image_list_file)\n        self.image_list_file = image_list_file\n        self.patch_size = patch_size\n        self.split = split\n        self.load_npy = load_npy\n        self.raw_ext = raw_ext\n        self.max_clip = max_clip\n        self.min_clip = min_clip\n        self.transpose = transpose\n        self.h_flip = h_flip\n        self.v_flip = v_flip\n        self.rotation = rotation\n        self.ratio = ratio\n        self.only_00 = only_00\n        self.repeat = repeat\n\n        self.raw_short_read_time = AverageMeter()\n        self.raw_short_pack_time = AverageMeter()\n        self.raw_short_post_time = AverageMeter()\n        self.raw_long_read_time = AverageMeter()\n        self.raw_long_pack_time = AverageMeter()\n        self.raw_long_post_time = AverageMeter()\n        self.npy_long_read_time = AverageMeter()\n        self.data_aug_time = AverageMeter()\n        self.data_norm_time = AverageMeter()\n        self.count = 0\n\n        self.img_info = []\n        with open(self.image_list_file, 'r') as f:\n            for i, img_pair in enumerate(f):\n                img_pair = img_pair.strip()  # ./Sony/short/10003_00_0.04s.ARW ./Sony/long/10003_00_10s.ARW ISO200 F9\n                img_file, lbl_file, iso, focus = img_pair.split(' ')\n                if self.split == 'test' and self.only_00:\n                    if os.path.split(img_file)[-1][5:8] != '_00':\n                        continue\n                img_exposure = float(os.path.split(img_file)[-1][9:-5]) # 0.04\n                lbl_exposure = float(os.path.split(lbl_file)[-1][9:-5]) # 10\n                ratio = min(lbl_exposure/img_exposure, 300)\n                self.img_info.append({\n                    'img': img_file,\n                    'lbl': lbl_file,\n                    'img_exposure': img_exposure,\n                    'lbl_exposure': lbl_exposure,\n                    'ratio': np.float32(ratio),\n                    'iso': float(iso[3::]),\n                    'focus': focus,\n                })\n                if max_samples and i == max_samples - 1:  # for debug purpose\n                    break\n        print(\"processing: {} images for {}\".format(len(self.img_info), self.split))\n\n\n    def __len__(self):\n        return len(self.img_info) * self.repeat\n\n    def print_time(self):\n        print('self.raw_short_read_time:', self.raw_short_read_time.avg)\n        print('self.raw_short_pack_time:', self.raw_short_pack_time.avg)\n        print('self.raw_short_post_time:', self.raw_short_post_time.avg)\n        print('self.raw_long_read_time:', self.raw_long_read_time.avg)\n        print('self.raw_long_pack_time:', self.raw_long_pack_time.avg)\n        print('self.raw_long_post_time:', self.raw_long_post_time.avg)\n        print('self.npy_long_read_time:', self.npy_long_read_time.avg)\n        print('self.data_aug_time:', self.data_aug_time.avg)\n        print('self.data_norm_time:', self.data_norm_time.avg)\n\n    def __getitem__(self, index):\n        self.count += 1\n        if self.count % 100 == 0 and False:\n            self.print_time()\n        info = self.img_info[index // self.repeat]\n\n        img_file = info['img']\n        if not self.load_npy:\n            start = time.time()\n            raw = rawpy.imread(os.path.join(self.data_path, img_file))\n            self.raw_short_read_time.update(time.time() - start)\n            start = time.time()\n            if self.patch_size is None:\n                # pack raw with patch size is implemented in clip patch for reducing computation\n                noisy_raw = self._pack_raw(raw)\n            self.raw_short_pack_time.update(time.time() - start)\n        else:\n            start = time.time()\n            noisy_raw = np.load(os.path.join(self.data_path, img_file.replace('short', 'short_pack')+'.npy'), allow_pickle=True)\n            self.raw_short_read_time.update(time.time() - start)\n\n        lbl_file = info['lbl']\n        if self.load_npy:\n            start = time.time()\n            clean_rgb = np.load(os.path.join(self.data_path, lbl_file.replace('long', 'long_post_int')+'.npy'), allow_pickle=True)\n            self.npy_long_read_time.update(time.time() - start)\n        else:\n            start = time.time()\n            lbl_raw = rawpy.imread(os.path.join(self.data_path, lbl_file))\n            clean_rgb = lbl_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n            clean_rgb = clean_rgb.transpose(2, 0, 1)\n            # clean_rgb = clean_rgb / 65535\n            self.raw_long_post_time.update(time.time() - start)\n\n        start = time.time()\n        if not self.load_npy:\n            lbl_raw = rawpy.imread(os.path.join(self.data_path, lbl_file))\n            if self.patch_size is None:\n                clean_raw = self._pack_raw(lbl_raw)\n        else:\n            clean_raw = np.load(os.path.join(self.data_path, lbl_file.replace('long', 'long_pack')+'.npy'), allow_pickle=True)\n        self.raw_long_read_time.update(time.time() - start)\n\n        if self.patch_size:\n            start = time.time()\n            patch_size = self.patch_size\n            # crop\n            H, W = clean_rgb.shape[1:3]\n            if self.split == 'train':\n                yy, xx = torch.randint(0, (H - patch_size) // self.block_size, (1,)),  torch.randint(0, (W - patch_size) // self.block_size, (1,))\n            else:\n                yy, xx = (H - patch_size) // self.block_size // 2, (W - patch_size) // self.block_size // 2\n            if not self.load_npy:\n                input_patch = self._pack_raw(raw, yy, xx)\n                clean_raw_patch = self._pack_raw(lbl_raw, yy, xx)\n            else:\n                input_patch = noisy_raw[:, yy:yy+patch_size//self.block_size, xx:xx+patch_size//self.block_size]\n                clean_raw_patch = clean_raw[:, yy:yy+patch_size//self.block_size, xx:xx+patch_size//self.block_size]\n\n            gt_patch = clean_rgb[:, yy*self.block_size:yy*self.block_size + patch_size, xx*self.block_size:xx*self.block_size + patch_size]\n\n            if self.h_flip and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random horizontal flip\n                input_patch = np.flip(input_patch, axis=2)\n                gt_patch = np.flip(gt_patch, axis=2)\n                clean_raw_patch = np.flip(clean_raw_patch, axis=2)\n            if self.v_flip and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random vertical flip\n                input_patch = np.flip(input_patch, axis=1)\n                gt_patch = np.flip(gt_patch, axis=1)\n                clean_raw_patch = np.flip(clean_raw_patch, axis=1)\n            if self.transpose and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random transpose\n                input_patch = np.transpose(input_patch, (0, 2, 1))\n                gt_patch = np.transpose(gt_patch, (0, 2, 1))\n                clean_raw_patch = np.transpose(clean_raw_patch, (0, 2, 1))\n            if self.rotation and self.split == 'train':\n                raise NotImplementedError('rotation')\n\n            noisy_raw = input_patch.copy()\n            clean_rgb = gt_patch.copy()\n            clean_raw = clean_raw_patch.copy()\n            self.data_aug_time.update(time.time() - start)\n\n        start = time.time()\n        noisy_raw = (np.float32(noisy_raw) - self.black_level) / np.float32(self.white_level - self.black_level)  # subtract the black level\n        clean_raw = (np.float32(clean_raw) - self.black_level) / np.float32(self.white_level - self.black_level)\n        clean_rgb = np.float32(clean_rgb) / np.float32(65535)\n        self.data_norm_time.update(time.time() - start)\n\n        if self.ratio:\n            noisy_raw = noisy_raw * info['ratio']\n        if self.max_clip is not None:\n            noisy_raw = np.minimum(noisy_raw, self.max_clip)\n        if self.min_clip is not None:\n            noisy_raw = np.maximum(noisy_raw, self.min_clip)\n\n        clean_rgb = clean_rgb.clip(0.0, 1.0)\n\n        noisy_raw = torch.from_numpy(noisy_raw).float()\n        clean_rgb = torch.from_numpy(clean_rgb).float()\n        clean_raw = torch.from_numpy(clean_raw).float()\n\n        return {\n            'noisy_raw': noisy_raw,\n            'clean_raw': clean_raw,\n            'clean_rgb': clean_rgb,\n            'img_file': img_file,\n            'lbl_file': lbl_file,\n            'img_exposure': info['img_exposure'],\n            'lbl_exposure': info['lbl_exposure'],\n            'ratio': info['ratio']\n        }", "\n\n@DATASET_REGISTRY.register()\nclass SonyDictSet(BaseDictSet):\n    def __init__(self, data_path, image_list_file, patch_size=None, split='train', load_npy=True, raw_ext='ARW', repeat=1,\n                 max_samples=None, max_clip=1.0, min_clip=None, only_00=False,\n                 transpose=True, h_flip=True, v_flip=True, rotation=False, ratio=True, **kwargs):\n        super(SonyDictSet, self).__init__(data_path, image_list_file, split=split, patch_size=patch_size,\n                                   load_npy=load_npy, raw_ext=raw_ext, max_samples=max_samples, max_clip=max_clip, repeat=repeat,\n                                   min_clip=min_clip, only_00=only_00, transpose=transpose, h_flip=h_flip, v_flip=v_flip, rotation=rotation, ratio=ratio)\n        self.block_size = 2\n        self.black_level = 512\n        self.white_level = 16383\n\n    def _pack_raw(self, raw, hh=None, ww=None):\n        if self.patch_size is None:\n            assert hh is None and ww is None\n        # pack Bayer image to 4 channels (RGBG)\n        im = raw.raw_image_visible.astype(np.uint16)\n\n        H, W = im.shape\n        im = np.expand_dims(im, axis=0)\n        if self.patch_size is None:\n            out = np.concatenate((im[:, 0:H:2, 0:W:2],\n                                  im[:, 0:H:2, 1:W:2],\n                                  im[:, 1:H:2, 1:W:2],\n                                  im[:, 1:H:2, 0:W:2]), axis=0)\n        else:\n            h1 = hh * 2\n            h2 = hh * 2 + self.patch_size\n            w1 = ww * 2\n            w2 = ww * 2 + self.patch_size\n            out = np.concatenate((im[:, h1:h2:2, w1:w2:2],\n                                  im[:, h1:h2:2, w1+1:w2:2],\n                                  im[:, h1+1:h2:2, w1+1:w2:2],\n                                  im[:, h1+1:h2:2, w1:w2:2]), axis=0)\n        return out", "\n\n@DATASET_REGISTRY.register()\nclass FujiDictSet(BaseDictSet):\n    def __init__(self, data_path, image_list_file, patch_size=None, split='train', load_npy=True, raw_ext='RAF', repeat=1,\n                 max_samples=None, max_clip=1.0, min_clip=None, only_00=False,\n                 transpose=True, h_flip=True, v_flip=True, rotation=False, ratio=True, **kwargs):\n        super(FujiDictSet, self).__init__(data_path, image_list_file, split=split, patch_size=patch_size,\n                                   load_npy=load_npy, raw_ext=raw_ext, max_samples=max_samples, max_clip=max_clip, repeat=repeat,\n                                   min_clip=min_clip, only_00=only_00, transpose=transpose, h_flip=h_flip, v_flip=v_flip, rotation=rotation, ratio=ratio)\n        self.block_size = 3\n        self.black_level = 1024\n        self.white_level = 16383\n\n    def _pack_raw(self, raw, hh=None, ww=None):\n        if self.patch_size is None:\n            assert hh is None and ww is None\n        # pack XTrans image to 9 channels ()\n        im = raw.raw_image_visible.astype(np.uint16)\n\n        H, W = im.shape\n        if self.patch_size is None:\n            h1 = 0\n            h2 = H // 6 * 6\n            w1 = 0\n            w2 = W // 6 * 6\n            out = np.zeros((9, h2 // 3, w2 // 3), dtype=np.uint16)\n        else:\n            h1 = hh * 3\n            h2 = hh * 3 + self.patch_size\n            w1 = ww * 3\n            w2 = ww * 3 + self.patch_size\n            out = np.zeros((9, self.patch_size // 3, self.patch_size // 3), dtype=np.uint16)\n        \n        # 0 R\n        out[0, 0::2, 0::2] = im[h1:h2:6, w1:w2:6]\n        out[0, 0::2, 1::2] = im[h1:h2:6, w1+4:w2:6]\n        out[0, 1::2, 0::2] = im[h1+3:h2:6, w1+1:w2:6]\n        out[0, 1::2, 1::2] = im[h1+3:h2:6, w1+3:w2:6]\n\n        # 1 G\n        out[1, 0::2, 0::2] = im[h1:h2:6, w1+2:w2:6]\n        out[1, 0::2, 1::2] = im[h1:h2:6, w1+5:w2:6]\n        out[1, 1::2, 0::2] = im[h1+3:h2:6, w1+2:w2:6]\n        out[1, 1::2, 1::2] = im[h1+3:h2:6, w1+5:w2:6]\n\n        # 1 B\n        out[2, 0::2, 0::2] = im[h1:h2:6, w1+1:w2:6]\n        out[2, 0::2, 1::2] = im[h1:h2:6, w1+3:w2:6]\n        out[2, 1::2, 0::2] = im[h1+3:h2:6, w1:w2:6]\n        out[2, 1::2, 1::2] = im[h1+3:h2:6, w1+4:w2:6]\n\n        # 4 R\n        out[3, 0::2, 0::2] = im[h1+1:h2:6, w1+2:w2:6]\n        out[3, 0::2, 1::2] = im[h1+2:h2:6, w1+5:w2:6]\n        out[3, 1::2, 0::2] = im[h1+5:h2:6, w1+2:w2:6]\n        out[3, 1::2, 1::2] = im[h1+4:h2:6, w1+5:w2:6]\n\n        # 5 B\n        out[4, 0::2, 0::2] = im[h1+2:h2:6, w1+2:w2:6]\n        out[4, 0::2, 1::2] = im[h1+1:h2:6, w1+5:w2:6]\n        out[4, 1::2, 0::2] = im[h1+4:h2:6, w1+2:w2:6]\n        out[4, 1::2, 1::2] = im[h1+5:h2:6, w1+5:w2:6]\n\n        out[5, :, :] = im[h1+1:h2:3, w1:w2:3]\n        out[6, :, :] = im[h1+1:h2:3, w1+1:w2:3]\n        out[7, :, :] = im[h1+2:h2:3, w1:w2:3]\n        out[8, :, :] = im[h1+2:h2:3, w1+1:w2:3]\n        return out", ""]}
{"filename": "datasets/__init__.py", "chunked_list": ["import importlib\nfrom copy import deepcopy\nfrom os import path as osp\nfrom glob import glob\nimport torch\n\nfrom utils.registry import DATASET_REGISTRY\n\n__all__ = ['build_train_loader', 'build_valid_loader', 'build_test_loader']\n", "__all__ = ['build_train_loader', 'build_valid_loader', 'build_test_loader']\n\n# automatically scan and import dataset modules for registry\n# scan all the files under the 'datasets' folder and collect files ending with '_dataset.py'\ndataset_folder = osp.dirname(osp.abspath(__file__))\ndataset_filenames = [osp.splitext(osp.basename(v))[0] for v in glob(osp.join(dataset_folder, '*_dataset.py'))]\n# import all the dataset modules\n_dataset_modules = [importlib.import_module(f'datasets.{file_name}') for file_name in dataset_filenames]\n\n\ndef build_dataset(dataset_cfg, split: str):\n    assert split.upper() in ['TRAIN', 'VALID', 'TEST']\n    dataset_cfg = deepcopy(dataset_cfg)\n    dataset_type = dataset_cfg.pop('type')\n    process_cfg = dataset_cfg.pop('process')\n    split_cfg = dataset_cfg.pop(split)\n    dataset = DATASET_REGISTRY.get(dataset_type)(\n        **dataset_cfg,\n        **process_cfg,\n        **split_cfg,\n        split=split\n    )\n    return dataset", "\n\ndef build_dataset(dataset_cfg, split: str):\n    assert split.upper() in ['TRAIN', 'VALID', 'TEST']\n    dataset_cfg = deepcopy(dataset_cfg)\n    dataset_type = dataset_cfg.pop('type')\n    process_cfg = dataset_cfg.pop('process')\n    split_cfg = dataset_cfg.pop(split)\n    dataset = DATASET_REGISTRY.get(dataset_type)(\n        **dataset_cfg,\n        **process_cfg,\n        **split_cfg,\n        split=split\n    )\n    return dataset", "\ndef build_train_loader(dataset_cfg):\n    train_dataset = build_dataset(dataset_cfg, 'train')\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=dataset_cfg['train']['batch_size'],\n                                                   shuffle=True, num_workers=dataset_cfg['num_workers'],\n                                                   pin_memory=dataset_cfg['pin_memory'], persistent_workers=dataset_cfg['persistent_workers'])\n    return train_dataloader\n\ndef build_valid_loader(dataset_cfg, num_workers=None):\n    valid_dataset = build_dataset(dataset_cfg, 'valid')\n    if num_workers is None:\n        num_workers = dataset_cfg['num_workers']\n    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=dataset_cfg['valid']['batch_size'],\n                                                   shuffle=False, num_workers=num_workers,\n                                                   pin_memory=dataset_cfg['pin_memory'], persistent_workers=dataset_cfg['persistent_workers'])\n    return valid_dataloader", "def build_valid_loader(dataset_cfg, num_workers=None):\n    valid_dataset = build_dataset(dataset_cfg, 'valid')\n    if num_workers is None:\n        num_workers = dataset_cfg['num_workers']\n    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=dataset_cfg['valid']['batch_size'],\n                                                   shuffle=False, num_workers=num_workers,\n                                                   pin_memory=dataset_cfg['pin_memory'], persistent_workers=dataset_cfg['persistent_workers'])\n    return valid_dataloader\n\n\ndef build_test_loader(dataset_cfg, num_workers=None):\n    test_dataset = build_dataset(dataset_cfg, 'test')\n    if num_workers is None:\n        num_workers = dataset_cfg['num_workers']\n    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=dataset_cfg['test']['batch_size'],\n                                                   shuffle=False, num_workers=num_workers,\n                                                   pin_memory=dataset_cfg['pin_memory'], persistent_workers=dataset_cfg['persistent_workers'])\n    return test_dataloader", "\n\ndef build_test_loader(dataset_cfg, num_workers=None):\n    test_dataset = build_dataset(dataset_cfg, 'test')\n    if num_workers is None:\n        num_workers = dataset_cfg['num_workers']\n    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=dataset_cfg['test']['batch_size'],\n                                                   shuffle=False, num_workers=num_workers,\n                                                   pin_memory=dataset_cfg['pin_memory'], persistent_workers=dataset_cfg['persistent_workers'])\n    return test_dataloader"]}
