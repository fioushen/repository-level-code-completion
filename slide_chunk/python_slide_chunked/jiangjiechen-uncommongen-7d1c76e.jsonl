{"filename": "cjjpy.py", "chunked_list": ["\ufeff# -*- coding: utf-8 -*-\n\n'''\n@Author : Jiangjie Chen\n@Time   : 2022/5/26 19:52\n@Contact: jjchen19@fudan.edu.cn\n'''\n\nimport re\nimport datetime", "import re\nimport datetime\nimport os\nimport subprocess\nimport urllib.request, urllib.parse\nimport argparse\nfrom tqdm import tqdm\nimport sqlite3\nimport requests\nimport socket", "import requests\nimport socket\nimport logging\nimport io\nimport traceback\n\ntry:\n    import ujson as json\nexcept:\n    import json", "except:\n    import json\n\nHADOOP_BIN = 'PATH=/usr/bin/:$PATH hdfs'\n\n\ndef LengthStats(filename, key4json=None):\n    len_list = []\n    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n    with open(filename) as f:", "    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n    with open(filename) as f:\n        for line in f:\n            if key4json not in ['none', None, 'None']:\n                len_list.append(len(json.loads(line)[key4json].split()))\n            else:\n                len_list.append(len(line.strip().split()))\n    stats = {\n        'Max': max(len_list),\n        'Min': min(len_list),", "        'Max': max(len_list),\n        'Min': min(len_list),\n        'Avg': round(sum(len_list) / len(len_list), 4),\n    }\n    len_list.sort()\n    for t in thresholds:\n        stats[f\"Top-{t}\"] = len_list[int(len(len_list) * t)]\n\n    for k in stats:\n        print(f\"- {k}: {stats[k]}\")", "    for k in stats:\n        print(f\"- {k}: {stats[k]}\")\n    return stats\n\n\nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n", "        self.__dict__ = self\n\n\ndef TraceBack(error_msg):\n    exc = traceback.format_exc()\n    msg = f'[Error]: {error_msg}.\\n[Traceback]: {exc}'\n    return msg\n\n\ndef Now():", "\ndef Now():\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef TorchHLoad(filepath: str, **kwargs):\n    import torch, tensorflow as tf\n    if not filepath.startswith(\"hdfs://\"):\n        return torch.load(filepath, **kwargs)\n    else:", "        return torch.load(filepath, **kwargs)\n    else:\n        with tf.io.gfile.GFile(filepath, 'rb') as reader:\n            return torch.load(io.BytesIO(reader.read()), **kwargs)\n\n\ndef TorchHSave(obj, filepath: str, **kwargs):\n    import torch, tensorflow as tf\n    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n        with tf.io.gfile.GFile(filepath, 'wb') as f:", "    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n        with tf.io.gfile.GFile(filepath, 'wb') as f:\n            buffer = io.BytesIO()\n            torch.save(obj, buffer, **kwargs)\n            f.write(buffer.getvalue())\n    else:\n        torch.save(obj, filepath, **kwargs)\n\n\ndef PutHDFS(local: str, remote: str):", "\ndef PutHDFS(local: str, remote: str):\n    import tensorflow as tf\n    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n    if not tf.io.gfile.exists(remote):\n        tf.io.gfile.makedirs(remote)\n    RunCmd(f'{HADOOP_BIN} dfs -put {local} {remote}')\n\n\ndef GetHDFS(remote: str, local: str):", "\ndef GetHDFS(remote: str, local: str):\n    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n    os.makedirs(local, exist_ok=True)\n    RunCmd(f'{HADOOP_BIN} dfs -get {remote} {local}')\n\n\ndef RunCmd(command):\n    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    res, err = pipe.communicate()", "    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    res, err = pipe.communicate()\n    res = res.decode('utf-8')\n    err = err.decode('utf-8')\n    return res, err\n\n\ndef AbsParentDir(file, parent='..', postfix=None):\n    ppath = os.path.abspath(file)\n    parent_level = parent.count('.')", "    ppath = os.path.abspath(file)\n    parent_level = parent.count('.')\n    while parent_level > 0:\n        ppath = os.path.dirname(ppath)\n        parent_level -= 1\n    if postfix is not None:\n        return os.path.join(ppath, postfix)\n    else:\n        return ppath\n", "        return ppath\n\n\ndef init_logger(log_file=None, log_file_level=logging.NOTSET, from_scratch=False):\n    from coloredlogs import ColoredFormatter\n    import tensorflow as tf\n\n    fmt = \"[%(asctime)s %(levelname)s] %(message)s\"\n    log_format = ColoredFormatter(fmt=fmt)\n    # log_format = logging.Formatter()", "    log_format = ColoredFormatter(fmt=fmt)\n    # log_format = logging.Formatter()\n    logger = logging.getLogger()\n    logger.setLevel(log_file_level)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_format)\n    logger.handlers = [console_handler]\n\n    if log_file and log_file != '':", "\n    if log_file and log_file != '':\n        if from_scratch and tf.io.gfile.exists(log_file):\n            logger.warning('Removing previous log file: %s' % log_file)\n            tf.io.gfile.remove(log_file)\n        path = os.path.dirname(log_file)\n        os.makedirs(path, exist_ok=True)\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(log_file_level)\n        file_handler.setFormatter(log_format)", "        file_handler.setLevel(log_file_level)\n        file_handler.setFormatter(log_format)\n        logger.addHandler(file_handler)\n\n    return logger\n\n\ndef is_qid(x, hard=False):\n    if type(x) is not str: return None\n    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)", "    if type(x) is not str: return None\n    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)\n    return None if len(ret) == 0 else ret[0]\n\n\ndef is_pid(x, hard=False):\n    if type(x) is not str: return None\n    ret = re.findall('^P\\d+$', x) if hard else re.findall('P\\d+', x)\n    return None if len(ret) == 0 else ret[0]\n", "    return None if len(ret) == 0 else ret[0]\n\n\nclass MiniLutDB:\n    def __init__(self, db, verbose=True):\n        self.db = db\n        self.conn = None\n        self.verbose = verbose\n\n    def dump_lut(self, lut_tuples, verbose=None):", "\n    def dump_lut(self, lut_tuples, verbose=None):\n        # lut_tuple: (k, v)+, iterable\n\n        if verbose is None: \n            verbose = self.verbose\n        self.conn = sqlite3.connect(self.db)\n        cur = self.conn.cursor()\n        cur.executescript('''\n        DROP TABLE IF EXISTS lut;", "        cur.executescript('''\n        DROP TABLE IF EXISTS lut;\n        CREATE TABLE lut (\n        id      TEXT PRIMARY KEY UNIQUE,\n        content TEXT)''')\n        self.conn.commit()\n\n        BLOCKSIZE = 100000\n        block = []\n        i = 0", "        block = []\n        i = 0\n        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n        for x in iter:\n            block.append(x)\n            i += 1\n            if i == BLOCKSIZE:\n                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n                block = []\n                i = 0", "                block = []\n                i = 0\n        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n        self.conn.commit()\n\n        self.close()\n\n    def update_lut(self, lut_tuples, verbose=None):\n        if verbose is None: \n            verbose = self.verbose", "        if verbose is None: \n            verbose = self.verbose\n\n        self.conn = sqlite3.connect(self.db)\n        BLOCKSIZE = 100000\n        block = []\n        i = 0\n        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n        for x in iter:\n            block.append(x)", "        for x in iter:\n            block.append(x)\n            i += 1\n            if i == BLOCKSIZE:\n                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n                block = []\n                i = 0\n        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n        self.conn.commit()\n", "        self.conn.commit()\n\n        self.close()\n\n    def create_index(self):\n        self.conn = sqlite3.connect(self.db)\n        self.cur = self.conn.cursor()\n        # sql = ('CREATE INDEX index_lut ON lut(id);')\n        # self.cur.execute(sql)\n        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')", "        # self.cur.execute(sql)\n        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')\n        self.conn.commit()\n\n    def get(self, x, default=None):\n        if x is None: return default\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db)\n            self.cur = self.conn.cursor()\n", "            self.cur = self.conn.cursor()\n\n        res = self.query_lut(self.cur, x, False)[0]\n        return res if res is not None else default\n\n    def get_chunk(self, xx):\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db)\n            self.cur = self.conn.cursor()\n        return self.query_lut(self.conn, xx, self.verbose)", "            self.cur = self.conn.cursor()\n        return self.query_lut(self.conn, xx, self.verbose)\n\n    def close(self):\n        if self.conn is not None:\n            self.conn.close()\n            self.conn = None\n    \n    def delete_sample(self, key, value=None):\n        if self.get(key) is None: return", "    def delete_sample(self, key, value=None):\n        if self.get(key) is None: return\n        self.conn = sqlite3.connect(self.db)\n        self.cur = self.conn.cursor()\n        self.cur.execute('DELETE FROM lut WHERE id = ?', (key,))\n        self.conn.commit()\n        assert self.get(key) is None, f'delete failed: {key}'\n\n    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n        values = []", "    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n        values = []\n        if isinstance(keys, str): keys = [keys]\n\n        iter = tqdm(keys, mininterval=0.5, disable=not verbose)\n        for k in iter:\n            cur.execute('SELECT content FROM lut WHERE id = ?', (k,))\n            val = cur.fetchone()\n            val = val[0] if val is not None else None\n            values.append(val)", "            val = val[0] if val is not None else None\n            values.append(val)\n        return values\n\n\ndef OverWriteCjjPy(root='.'):\n    # import difflib\n    # diff = difflib.HtmlDiff()\n    cnt = 0\n    golden_cjjpy = os.path.join(root, 'cjjpy.py')", "    cnt = 0\n    golden_cjjpy = os.path.join(root, 'cjjpy.py')\n    # golden_content = open(golden_cjjpy).readlines()\n    for dir, folder, file in os.walk(root):\n        for f in file:\n            if f == 'cjjpy.py':\n                cjjpy = '%s/%s' % (dir, f)\n                # content = open(cjjpy).readlines()\n                # d = diff.make_file(golden_content, content)\n                cnt += 1", "                # d = diff.make_file(golden_content, content)\n                cnt += 1\n                print('[%d]: %s' % (cnt, cjjpy))\n                os.system('cp %s %s' % (golden_cjjpy, cjjpy))\n\n\ndef ReplaceChar(file, replaced, replacer):\n    print(file, replaced, replacer)\n    with open(file) as f:\n        data = f.readlines()", "    with open(file) as f:\n        data = f.readlines()\n        out = open(file, 'w')\n        for line in data:\n            out.write(line.replace(replaced, replacer))\n\n\ndef DeUnicode(line):\n    return line.encode('utf-8').decode('unicode_escape')\n", "    return line.encode('utf-8').decode('unicode_escape')\n\n\ndef LoadIDDict(dict_file, unify_words=False, lower=False, reverse=False):\n    '''\n    a\\tb\\n, `.dict' file\n    '''\n    import tensorflow as tf\n    assert dict_file.endswith('.dict')\n    id2label = {}", "    assert dict_file.endswith('.dict')\n    id2label = {}\n    with tf.io.gfile.GFile(dict_file, 'r') as f:\n        data = f.read().split('\\n')\n        for i, line in enumerate(data):\n            if line == '': continue\n            try:\n                id, label = line.split('\\t')\n                if reverse:\n                    id, label = label, id", "                if reverse:\n                    id, label = label, id\n                _val = '_'.join(label.split()) if unify_words else label\n                id2label[id] = _val.lower() if lower else _val\n            except:\n                pass\n    return id2label\n\n\ndef LoadWords(file, is_file=True):", "\ndef LoadWords(file, is_file=True):\n    import tensorflow as tf\n    if is_file:\n        with tf.io.gfile.GFile(file, 'r') as f:\n            data = f.read().splitlines()\n    else:\n        data = file.splitlines()\n    return set(map(lambda x: x.strip(), data))\n", "    return set(map(lambda x: x.strip(), data))\n\n\ndef ChangeFileFormat(filename, new_fmt):\n    assert type(filename) is str and type(new_fmt) is str\n    spt = filename.split('.')\n    if len(spt) == 0:\n        return filename\n    else:\n        return filename.replace('.' + spt[-1], new_fmt)", "    else:\n        return filename.replace('.' + spt[-1], new_fmt)\n\n\ndef CountLines(fname):\n    with open(fname, 'rb') as f:\n        count = 0\n        last_data = '\\n'\n        while True:\n            data = f.read(0x400000)", "        while True:\n            data = f.read(0x400000)\n            if not data:\n                break\n            count += data.count(b'\\n')\n            last_data = data\n        if last_data[-1:] != b'\\n':\n            count += 1  # Remove this if a wc-like count is needed\n    return count\n", "    return count\n\n\ndef SearchByKey(file, key):\n    with open(file, 'r') as fin:\n        while True:\n            line = fin.readline()\n            if not line: break\n            if key in line:\n                print(line, end='')", "            if key in line:\n                print(line, end='')\n\n\ndef SendEmail(subject, content, receivers=['MichaelChen0110@163.com']):\n    from email.mime.text import MIMEText\n    import smtplib\n\n    # receivers got to be list.\n    mail_receivers = receivers", "    # receivers got to be list.\n    mail_receivers = receivers\n    # mail_host = \"smtp.163.com\n    mail_host = \"220.181.12.18\"\n    mail_user = \"MichaelChen0110@163.com\"\n    mail_pass = \"\"\n    me = socket.gethostname() + \"<\" + mail_user + \">\"\n    msg = MIMEText(content, _subtype='plain', _charset='utf-8')\n    msg['Subject'] = subject\n    msg['From'] = me", "    msg['Subject'] = subject\n    msg['From'] = me\n    msg['To'] = \";\".join(mail_receivers)\n    try:\n        server = smtplib.SMTP()\n        server.connect(mail_host)\n        server.login(mail_user, mail_pass)\n        server.sendmail(me, mail_receivers, msg.as_string())\n        server.close()\n        print('Have sent the email to ' + str(mail_receivers) + '. ')", "        server.close()\n        print('Have sent the email to ' + str(mail_receivers) + '. ')\n        return True\n    except Exception as e:\n        print(str(e))\n        return False\n\n\ndef SortDict(_dict, reverse=True):\n    assert type(_dict) is dict", "def SortDict(_dict, reverse=True):\n    assert type(_dict) is dict\n    return sorted(_dict.items(), key=lambda d: d[1], reverse=reverse)\n\n\ndef MaxCommLen(str1, str2):\n    lstr1 = len(str1)\n    lstr2 = len(str2)\n    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n    max_num = 0", "    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n    max_num = 0\n    for i in range(lstr1):\n        for j in range(lstr2):\n            if str1[i] == str2[j]:\n                record[i + 1][j + 1] = record[i][j] + 1\n                if record[i + 1][j + 1] > max_num:\n                    max_num = record[i + 1][j + 1]\n    return max_num, ''\n", "    return max_num, ''\n\n\ndef lark(content='test'):\n    print(content)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n", "    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--diff', nargs=2,\n                        help='show difference between two files, shown in downloads/diff.html')\n    parser.add_argument('--de_unicode', action='store_true', default=False,\n                        help='remove unicode characters')\n    parser.add_argument('--link_entity', action='store_true', default=False,\n                        help='')\n    parser.add_argument('--max_comm_len', action='store_true', default=False,\n                        help='')", "    parser.add_argument('--max_comm_len', action='store_true', default=False,\n                        help='')\n    parser.add_argument('--search', nargs=2,\n                        help='search key from file, 2 args: file name & key')\n    parser.add_argument('--email', nargs=2,\n                        help='sending emails, 2 args: subject & content')\n    parser.add_argument('--overwrite', action='store_true', default=None,\n                        help='overwrite all cjjpy under given *dir* based on *dir*/cjjpy.py')\n    parser.add_argument('--replace', nargs=3,\n                        help='replace char, 3 args: file name & replaced char & replacer char')", "    parser.add_argument('--replace', nargs=3,\n                        help='replace char, 3 args: file name & replaced char & replacer char')\n    parser.add_argument('--lark', nargs=1)\n    parser.add_argument('--get_hdfs', nargs=2,\n                        help='easy copy from hdfs to local fs, 2 args: remote_file/dir & local_dir')\n    parser.add_argument('--put_hdfs', nargs=2,\n                        help='easy put from local fs to hdfs, 2 args: local_file/dir & remote_dir')\n    parser.add_argument('--length_stats', nargs=2,\n                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n", "                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n\n    args = parser.parse_args()\n\n    if args.overwrite:\n        print('* Overwriting cjjpy...')\n        OverWriteCjjPy()\n\n    if args.replace:\n        print('* Replacing Char...')", "    if args.replace:\n        print('* Replacing Char...')\n        ReplaceChar(args.replace[0], args.replace[1], args.replace[2])\n\n    if args.search:\n        file = args.search[0]\n        key = args.search[1]\n        print('* Searching %s from %s...' % (key, file))\n        SearchByKey(file, key)\n", "        SearchByKey(file, key)\n\n    if args.email:\n        try:\n            subj = args.email[0]\n            cont = args.email[1]\n        except:\n            subj = 'running complete'\n            cont = ''\n        print('* Sending email {%s, %s} to host...' % (subj, cont))", "            cont = ''\n        print('* Sending email {%s, %s} to host...' % (subj, cont))\n        SendEmail(subj, cont)\n\n    if args.lark:\n        try:\n            content = args.lark[0]\n        except:\n            content = 'running complete'\n        print(f'* Larking \"{content}\"...')", "            content = 'running complete'\n        print(f'* Larking \"{content}\"...')\n        lark(content)\n\n    if args.get_hdfs:\n        remote = args.get_hdfs[0]\n        local = args.get_hdfs[1]\n        print(f'* Copying {remote} to {local}...')\n        GetHDFS(remote, local)\n", "        GetHDFS(remote, local)\n\n    if args.put_hdfs:\n        local = args.put_hdfs[0]\n        remote = args.put_hdfs[1]\n        print(f'* Copying {local} to {remote}...')\n        PutHDFS(local, remote)\n\n    if args.length_stats:\n        file = args.length_stats[0]", "    if args.length_stats:\n        file = args.length_stats[0]\n        key4json = args.length_stats[1]\n        print(f'* Working on {file} lengths statistics...')\n        LengthStats(file, key4json)\n"]}
{"filename": "flant5_helper.py", "chunked_list": ["# from transformers import (\n#     AutoTokenizer,\n#     AutoModelForSeq2SeqLM,\n#     DataCollatorForSeq2Seq,\n# )\n# from datasets import Dataset\n# from torch.utils.data import DataLoader\n# import torch\n# from tqdm import tqdm\nfrom base_generator import Seq2SeqBaseGenerator", "# from tqdm import tqdm\nfrom base_generator import Seq2SeqBaseGenerator\n\n\ndef flatten_list(chunk_list):\n    for chunk in chunk_list:\n        if isinstance(chunk, list):\n            yield from flatten_list(chunk)\n        else:\n            yield chunk", "\n\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i: i + n]\n\n\ndef prompt_flant5(prompt_input: list, model_name='flan-t5-large', max_tokens=128,\n                  clean=False, batch_size=16, verbose=False, **kwargs):\n\n    _model_name_or_path = 'google/' + model_name if model_name.startswith('flan-t5') else model_name\n    flant5 = Seq2SeqBaseGenerator(_model_name_or_path)\n\n    outputs = flant5.generate(prompt_input,\n                              num_return_sequences=kwargs.get('n', 1),\n                              beam_size=kwargs.get('n', 1),\n                              temperature=kwargs.get('temperature', 0),\n                              max_new_tokens=max_tokens,\n                              per_device_test_batch_size=batch_size,\n                              verbose=verbose,)\n    \n    return flatten_list(outputs)", "def prompt_flant5(prompt_input: list, model_name='flan-t5-large', max_tokens=128,\n                  clean=False, batch_size=16, verbose=False, **kwargs):\n\n    _model_name_or_path = 'google/' + model_name if model_name.startswith('flan-t5') else model_name\n    flant5 = Seq2SeqBaseGenerator(_model_name_or_path)\n\n    outputs = flant5.generate(prompt_input,\n                              num_return_sequences=kwargs.get('n', 1),\n                              beam_size=kwargs.get('n', 1),\n                              temperature=kwargs.get('temperature', 0),\n                              max_new_tokens=max_tokens,\n                              per_device_test_batch_size=batch_size,\n                              verbose=verbose,)\n    \n    return flatten_list(outputs)", "\n    # tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-xl')\n    # model = AutoModelForSeq2SeqLM.from_pretrained(_model_name_or_path, device_map='auto')\n\n    # data_collator = DataCollatorForSeq2Seq(\n    #     tokenizer,\n    #     model=model,\n    #     label_pad_token_id=-100,\n    #     pad_to_multiple_of=None,\n    # )", "    #     pad_to_multiple_of=None,\n    # )\n\n    # if isinstance(prompt_input[0], str):\n    #     prompt_input = [{'source': x} for x in prompt_input]\n    \n    # raw_datasets = Dataset.from_list(prompt_input, split='test')\n    # column_names = raw_datasets.column_names\n    \n    # def preprocess_function(examples, prefix='', source_column='source'):", "    \n    # def preprocess_function(examples, prefix='', source_column='source'):\n    #     padding = 'max_length'\n    #     inputs = examples[source_column]\n    #     inputs = [prefix + inp for inp in inputs]\n    #     model_inputs = tokenizer(inputs, padding=padding, truncation=True)\n    #     return model_inputs\n\n    # processed_datasets = raw_datasets.map(\n    #     preprocess_function,", "    # processed_datasets = raw_datasets.map(\n    #     preprocess_function,\n    #     batched=True,\n    #     num_proc=1,\n    #     remove_columns=column_names,\n    #     desc=\"Running tokenizer on dataset\",\n    # )\n\n    # dataloader = DataLoader(processed_datasets, collate_fn=data_collator, batch_size=batch_size)\n", "    # dataloader = DataLoader(processed_datasets, collate_fn=data_collator, batch_size=batch_size)\n\n    # predictions = []\n    # for batch in tqdm(dataloader, total=len(dataloader), disable=not verbose,):\n    #     with torch.no_grad():\n    #         generated_output = model.generate(\n    #             batch[\"input_ids\"],\n    #             attention_mask=batch[\"attention_mask\"],\n    #             num_beams=kwargs.get('n', 1),\n    #             num_return_sequences=kwargs.get('n', 1),", "    #             num_beams=kwargs.get('n', 1),\n    #             num_return_sequences=kwargs.get('n', 1),\n    #             max_new_tokens=max_tokens,\n    #             output_scores=True,\n    #             return_dict_in_generate=True,\n    #             temperature=kwargs.get('temperature', 0)\n    #         )\n    #         generated_tokens = generated_output.sequences\n    #         if isinstance(generated_tokens, tuple):\n    #             generated_tokens = generated_tokens[0]", "    #         if isinstance(generated_tokens, tuple):\n    #             generated_tokens = generated_tokens[0]\n    #         decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n    #         preds = [pred.strip() for pred in decoded_preds]\n    #         predictions += preds\n\n    # predictions = list(map(lambda x: x.strip(), predictions))\n    # # TODO: `[:len(test_dataset)]` is a hack to deal with the auto-filling of the last incomplete batch.\n    # predictions = list(chunks(predictions, kwargs.get('n', 1)))[:len(processed_datasets)]\n", "    # predictions = list(chunks(predictions, kwargs.get('n', 1)))[:len(processed_datasets)]\n\n    # return flatten_list(predictions)\n\n\nif __name__ == '__main__':\n    res = prompt_flant5([{'source': 'are lions mammal?'}, {'source': 'can birds fly?'}], max_tokens=32)\n    print(list(res))"]}
{"filename": "gpt3_helper.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\n'''\n@Author : Jiangjie Chen\n@Time   : 2023/1/2 21:00\n@Contact: jjchen19@fudan.edu.cn\n'''\n\nimport os\nimport openai", "import os\nimport openai\nimport math\nimport sys\nimport time\nfrom tqdm import tqdm\nfrom typing import Iterable, List, TypeVar\n\nT = TypeVar('T')\nKEY_INDEX = 0", "T = TypeVar('T')\nKEY_INDEX = 0\nKEY_POOL = [os.environ[\"OPENAI_API_KEY\"]] # your key pool\nopenai.api_key = KEY_POOL[0]\n\n\ndef batchify(data: Iterable[T], batch_size: int) -> Iterable[List[T]]:\n    # function copied from allenai/real-toxicity-prompts\n    assert batch_size > 0\n    batch = []\n    for item in data:\n        # Yield next batch\n        if len(batch) == batch_size:\n            yield batch\n            batch = []\n        batch.append(item)\n\n    # Yield last un-filled batch\n    if len(batch) != 0:\n        yield batch", "\n\ndef openai_unit_price(model_name):\n    if 'gpt-3.5-turbo' in model_name:\n        unit = 0.002\n    elif 'davinci' in model_name:\n        unit = 0.02\n    elif 'curie' in model_name:\n        unit = 0.002\n    elif 'babbage' in model_name:\n        unit = 0.0005\n    elif 'ada' in model_name:\n        unit = 0.0004\n    else:\n        unit = -1\n    return unit", "\n\ndef calc_cost_w_tokens(total_tokens: int, model_name: str):\n    unit = openai_unit_price(model_name)\n    return round(unit * total_tokens / 1000, 2)\n\n\ndef calc_cost_w_prompt(prompt: str, model_name: str):\n    # 750 words == 1000 tokens\n    unit = openai_unit_price(model_name)\n    return round(len(prompt.split()) / 750 * unit, 2)", "\n\ndef get_perplexity(logprobs):\n    assert len(logprobs) > 0, logprobs\n    return math.exp(-sum(logprobs)/len(logprobs))\n\n\ndef keep_logprobs_before_eos(tokens, logprobs):\n    keep_tokens = []\n    keep_logprobs = []\n    start_flag = False\n    for tok, lp in zip(tokens, logprobs):\n        if start_flag:\n            if tok == \"<|endoftext|>\":\n                break\n            else:\n                keep_tokens.append(tok)\n                keep_logprobs.append(lp)\n        else:\n            if tok != '\\n':\n                start_flag = True\n                if tok != \"<|endoftext>\":\n                    keep_tokens.append(tok)\n                    keep_logprobs.append(lp)\n\n    return keep_tokens, keep_logprobs", "\n\ndef catch_openai_api_error(prompt_input: list):\n    global KEY_INDEX\n    error = sys.exc_info()[0]\n    if error == openai.error.InvalidRequestError:\n        # something is wrong: e.g. prompt too long\n        print(f\"InvalidRequestError\\nPrompt:\\n\\n{prompt_input}\\n\\n\")\n        assert False\n    elif error == openai.error.RateLimitError:\n        KEY_INDEX = (KEY_INDEX + 1) % len(KEY_POOL)\n        openai.api_key = KEY_POOL[KEY_INDEX]\n        print(\"RateLimitError, now change the key.\")\n    else:\n        print(\"API error:\", error)", "\n\ndef prompt_gpt3(prompt_input: list, model_name='text-davinci-003', max_tokens=128,\n                clean=False, batch_size=16, verbose=False, **kwargs):\n    # return: output_list, money_cost\n\n    def request_api(prompts: list):\n        # prompts: list or str\n        while True:\n            try:\n                return openai.Completion.create(\n                    model=model_name,\n                    prompt=prompts,\n                    max_tokens=max_tokens,\n                    # temperature=kwargs.get('temperature', 0.9),\n                    # top_p=kwargs.get('top_p', 1),\n                    # frequency_penalty=kwargs.get('frequency_penalty', 0),\n                    # presence_penalty=kwargs.get('presence_penalty', 0),\n                    **kwargs\n                )\n            except:\n                catch_openai_api_error(prompt_input)\n                time.sleep(1)\n\n    total_tokens = 0\n    results = []\n    for batch in tqdm(batchify(prompt_input, batch_size), total=len(prompt_input) // batch_size):\n        batch_response = request_api(batch)\n        total_tokens += batch_response['usage']['total_tokens']\n        if not clean:\n            results += batch_response['choices']\n        else:\n            results += [choice['text'] for choice in batch_response['choices']]\n\n    return results, calc_cost_w_tokens(total_tokens, model_name)", "\n\ndef prompt_chatgpt(system_input, user_input, history=[], model_name='gpt-3.5-turbo'):\n    '''\n    :param system_input: \"You are a helpful assistant/translator.\"\n    :param user_input: you texts here\n    :param history: ends with assistant output.\n                    e.g. [{\"role\": \"system\", \"content\": xxx},\n                          {\"role\": \"user\": \"content\": xxx},\n                          {\"role\": \"assistant\", \"content\": \"xxx\"}]\n    return: assistant_output, (updated) history, money cost\n    '''\n    if len(history) == 0:\n        history = [{\"role\": \"system\", \"content\": system_input}]\n    history.append({\"role\": \"user\", \"content\": user_input})\n\n    for _ in range(5):\n        try:\n            completion = openai.ChatCompletion.create(\n                model=model_name,\n                messages=history\n            )\n            break\n        except:\n            catch_openai_api_error()\n            time.sleep(1)\n\n    assistant_output = completion['choices'][0]['message']['content']\n    history.append({\"role\": \"assistant\", \"content\": assistant_output})\n    total_tokens = completion['usage']['total_tokens']\n\n    return assistant_output, history, calc_cost_w_tokens(total_tokens, model_name)", "\n\nif __name__ == '__main__':\n    prompt = 'hello world'\n    response = prompt_gpt3([prompt]*4, batch_size=4, clean=True)\n    print(response)\n    response = prompt_chatgpt('You are a super villian.', 'What is your plan when get caught?')\n    print(response)"]}
{"filename": "utils.py", "chunked_list": ["import os\nimport re\nimport ujson as json\nimport cjjpy as cjj\n\n\nREL_TO_BOOLQ_TEMPLATE = {\n    \"IsA\": \"is [w1] a type of [w2]?\",\n    'CapableOf': \"can [w1] [w2]?\",\n    'UsedFor': \"is [w1] used for [w2]?\",", "    'CapableOf': \"can [w1] [w2]?\",\n    'UsedFor': \"is [w1] used for [w2]?\",\n    \"MadeOf\": \"is [w1] made of [w2]?\",\n    'HasProperty': \"does [w1] has the property of [w2]?\",\n    'HasSubevent': \"does [w1] have a subevent of [w2]?\",\n    \"AtLocation\": \"is [w1] likely to be found in [w2]?\",\n    \"PartOf\": \"is [w1] part of [w2]?\",\n    \"HasA\": \"does [w1] have [w2]?\",\n    # \"ReceivesAction\": \"can [w1] be [w2]?\",\n    \"Causes\": \"does [w1] cause [w2]?\",", "    # \"ReceivesAction\": \"can [w1] be [w2]?\",\n    \"Causes\": \"does [w1] cause [w2]?\",\n    # \"HasPrerequisite\": \"in order for [w1] to happen, does [w2] need to happen?\",\n    # \"NotCapableOf\": \"is [w1] capable of [w2]?\",\n    \"RelatedTo\": \"is [w1] like [w2]?\",\n    \"Desires\": \"does [w1] want [w2]?\",\n    \"MotivatedByGoal\": \"is [w1] movitated by the goal of [w2]?\",\n    # \"NotHasProperty\":  \"does [w1] have the property of [w2]?\",\n    \"CreatedBy\": \"is [w1] created by [w2]?\",\n    \"CausesDesire\": \"does [w1] make people want [w2]?\",", "    \"CreatedBy\": \"is [w1] created by [w2]?\",\n    \"CausesDesire\": \"does [w1] make people want [w2]?\",\n    # \"NotIsA\": \"is [w1] a type of [w2]?\",\n    # \"HasFirstSubevent\": \"is [w2] the first subevent of [w1]?\",\n    # \"DefinedAs\": \"is [w1] defined as [w2]?\"\n}\n\nUSUALLY_REL_TO_BOOLQ_TEMPLATE = {\n    \"IsA\": \"is [w1] a type of [w2]?\",\n    'CapableOf': \"can [w1] generally [w2]?\",", "    \"IsA\": \"is [w1] a type of [w2]?\",\n    'CapableOf': \"can [w1] generally [w2]?\",\n    'UsedFor': \"is [w1] generally used for [w2]?\",\n    \"MadeOf\": \"is [w1] generally made of [w2]?\",\n    'HasProperty': \"does [w1] generally have the property of [w2]?\",\n    'HasSubevent': \"does [w1] generally have a subevent of [w2]?\",\n    \"AtLocation\": \"is [w1] likely to be found in [w2]?\",\n    \"PartOf\": \"is [w1] generally part of [w2]?\",\n    \"HasA\": \"does [w1] generally have [w2]?\",\n    # \"ReceivesAction\": \"can [w1] generally be [w2]?\",", "    \"HasA\": \"does [w1] generally have [w2]?\",\n    # \"ReceivesAction\": \"can [w1] generally be [w2]?\",\n    \"Causes\": \"does [w1] generally cause [w2]?\",\n    # \"HasPrerequisite\": \"in order for [w1] to happen, does [w2] generally need to happen?\",\n    # \"NotCapableOf\": \"is [w1] generally capable of [w2]?\",\n    \"RelatedTo\": \"is [w1] like [w2]?\",\n    \"Desires\": \"does [w1] generally want [w2]?\",\n    \"MotivatedByGoal\": \"is [w1] generally movitated by the goal of [w2]?\",\n    # \"NotHasProperty\":  \"does [w1] generally have the property of [w2]?\",\n    \"CreatedBy\": \"is [w1] generally created by [w2]?\",", "    # \"NotHasProperty\":  \"does [w1] generally have the property of [w2]?\",\n    \"CreatedBy\": \"is [w1] generally created by [w2]?\",\n    \"CausesDesire\": \"does [w1] generally make people want [w2]?\",\n    # \"NotIsA\": \"is [w1] a type of [w2]?\",\n    # \"HasFirstSubevent\": \"is [w2] generally the first subevent of [w1]?\",\n    # \"DefinedAs\": \"is [w1] generally defined as [w2]?\"\n}\n\nREL_TO_NEG_TEMPLATE = {\n    \"IsA\": \"[w1] is not a type of [w2]\",", "REL_TO_NEG_TEMPLATE = {\n    \"IsA\": \"[w1] is not a type of [w2]\",\n    'CapableOf': \"[w1] can not [w2]\",\n    'UsedFor': \"[w1] is not used for [w2]\",\n    \"MadeOf\": \"[w1] is not made of [w2]\",\n    'HasProperty': \"[w1] is not [w2]\",\n    'HasSubevent': \"Something you do when you [w1] is [w2]\",\n    \"AtLocation\": \"You are not likely to find [w1] in [w2]\",\n    \"PartOf\": \"[w1] is not part of [w2]\",\n    \"HasA\": \"[w1] does not have [w2]\",", "    \"PartOf\": \"[w1] is not part of [w2]\",\n    \"HasA\": \"[w1] does not have [w2]\",\n    \"ReceivesAction\": \"[w1] can not be [w2]\",\n    \"Causes\": \"[w1] does not cause [w2]\",\n    \"HasPrerequisite\": \"In order for [w1] to happen, [w2] needs not to happen\",\n    \"NotCapableOf\": \"[w1] is capable of [w2]\",\n    \"RelatedTo\": \"[w1] is not like [w2]\",\n    \"Desires\": \"[w1] does not want [w2]\",\n    \"MotivatedByGoal\": \"You would [w1] not because you want to [w2]\",\n    \"NotHasProperty\":  \"[w1] has the property of [w2]\",", "    \"MotivatedByGoal\": \"You would [w1] not because you want to [w2]\",\n    \"NotHasProperty\":  \"[w1] has the property of [w2]\",\n    \"CreatedBy\": \"[w1] is not created by [w2]\",\n    \"CausesDesire\": \"[w1] does not make people want [w2]\",\n    \"NotIsA\": \"[w1] is a type of [w2]\",\n    \"HasFirstSubevent\": \"the first thing you do when you [w1] is not [w2]\",\n    \"DefinedAs\": \"[w1] is not defined as [w2]\"\n}\n\nREL_TO_TEMPLATE = {", "\nREL_TO_TEMPLATE = {\n    \"RelatedTo\": \"[w1] is like [w2]\",\n    \"ExternalUrl\": \"[w1] is described at the following URL [w2]\",\n    \"FormOf\": \"[w1] is a form of the word [w2]\",\n    \"IsA\": \"[w1] is a type of [w2]\",\n    \"NotIsA\": \"[w1] is not [w2]\",\n    \"PartOf\": \"[w1] is part of [w2]\",\n    \"UsedFor\": \"[w1] is used for [w2]\",\n    \"CapableOf\": \"[w1] can [w2]\",", "    \"UsedFor\": \"[w1] is used for [w2]\",\n    \"CapableOf\": \"[w1] can [w2]\",\n    \"AtLocation\": \"You are likely to find [w1] in [w2]\",\n    \"Causes\": \"Sometimes [w1] causes [w2]\",\n    \"HasA\": \"[w1] has [w2]\",\n    \"HasSubevent\": \"Something you do when you [w1] is [w2]\",\n    \"HasFirstSubevent\": \"the first thing you do when you [w1] is [w2]\",\n    \"HasLastSubevent\": \"the last thing you do when you [w1] is [w2]\",\n    \"HasPrerequisite\": \"In order for [w1] to happen, [w2] needs to happen\",\n    \"HasProperty\": \"[w1] is [w2]\",", "    \"HasPrerequisite\": \"In order for [w1] to happen, [w2] needs to happen\",\n    \"HasProperty\": \"[w1] is [w2]\",\n    \"HasContext\": \"[w1] is a word used in the context of [w2]\",\n    \"MotivatedByGoal\": \"You would [w1] because you want to [w2]\",\n    \"ObstructedBy\": \"[w1] can be prevented by [w2]\",\n    \"Desires\": \"[w1] wants [w2]\",\n    \"CreatedBy\": \"[w1] is created by [w2]\",\n    \"Synonym\": \"[w1] and [w2] have similar meanings\",\n    \"Antonym\": \"[w1] is the opposite of [w2]\",\n    \"DistinctFrom\": \"it cannot be both [w1] and [w2]\",", "    \"Antonym\": \"[w1] is the opposite of [w2]\",\n    \"DistinctFrom\": \"it cannot be both [w1] and [w2]\",\n    \"DerivedFrom\": \"the word [w1] is derived from the word [w2]\",\n    \"DefinedAs\": \"[w1] is defined as [w2]\",\n    \"Entails\": \"if [w1] is happening, [w2] is also happening\",\n    \"MannerOf\": \"[w1] is a specific way of doing [w2]\",\n    \"LocatedNear\": \"[w1] is located near [w2]\",\n    \"dbpedia\": \"[w1] is conceptually related to [w2]\",\n    \"SimilarTo\": \"[w1] is similar to [w2]\",\n    \"EtymologicallyRelatedTo\": \"the word [w1] and the word [w2] have the same origin\",", "    \"SimilarTo\": \"[w1] is similar to [w2]\",\n    \"EtymologicallyRelatedTo\": \"the word [w1] and the word [w2] have the same origin\",\n    \"EtymologicallyDerivedFrom\": \"the word [w1] comes from the word [w2]\",\n    \"CausesDesire\": \"[w1] makes people want [w2]\",\n    \"MadeOf\": \"[w1] is made of [w2]\",\n    \"ReceivesAction\": \"[w1] can be [w2]\",\n    \"InstanceOf\": \"[w1] is an example of [w2]\",\n    \"NotDesires\": \"[w1] does not want [w2]\",\n    \"NotUsedFor\": \"[w1] is not used for [w2]\",\n    \"NotCapableOf\": \"[w1] is not capable of [w2]\",", "    \"NotUsedFor\": \"[w1] is not used for [w2]\",\n    \"NotCapableOf\": \"[w1] is not capable of [w2]\",\n    \"NotHasProperty\": \"[w1] does not have the property of [w2]\",\n    \"NotMadeOf\": \"[w1] is not made of [w2]\"\n}\n\ndef avg(x):\n    return sum(x) / len(x)\n\n\ndef load_conceptnet_weight(cw_filename=os.path.join(os.environ.get('PJ_HOME', '..'),\n                                                    'data/conceptnet/conceptnet_weight.txt'),\n                           top_percentage=1.):\n    cw_dict = {}\n    with open(cw_filename) as f:\n        for x in f.readlines():\n            c, w = x.strip().split('\\t')\n            cw_dict[c] = w\n    cw_tuple = cjj.SortDict(cw_dict)\n    weight_threshold = cw_tuple[int(top_percentage * len(cw_dict))]\n    return cw_dict, weight_threshold[-1]", "\n\ndef load_conceptnet_weight(cw_filename=os.path.join(os.environ.get('PJ_HOME', '..'),\n                                                    'data/conceptnet/conceptnet_weight.txt'),\n                           top_percentage=1.):\n    cw_dict = {}\n    with open(cw_filename) as f:\n        for x in f.readlines():\n            c, w = x.strip().split('\\t')\n            cw_dict[c] = w\n    cw_tuple = cjj.SortDict(cw_dict)\n    weight_threshold = cw_tuple[int(top_percentage * len(cw_dict))]\n    return cw_dict, weight_threshold[-1]", "\n\ndef load_jsonl(jsl_or_path):\n    if isinstance(jsl_or_path, str):\n        with open(jsl_or_path) as f:\n            data = [json.loads(line) for line in f]\n    else:\n        data = jsl_or_path\n    return data\n", "\n\ndef save_jsonl(jsl, output_file):\n    with open(output_file, 'w') as f:\n        for js in jsl:\n            f.write(json.dumps(js, ensure_ascii=False) + '\\n')\n    return output_file\n\n\ndef calc_biclf_metrics(y_pred, y_true):\n    from sklearn import metrics\n    acc = metrics.accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n    return {'accuracy': acc, 'tn': tn/(tn+fp), 'fp': fp/(tn+fp), 'fn': fn/(fn+tp), 'tp': tp/(fn+tp)}", "\ndef calc_biclf_metrics(y_pred, y_true):\n    from sklearn import metrics\n    acc = metrics.accuracy_score(y_true, y_pred)\n    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n    return {'accuracy': acc, 'tn': tn/(tn+fp), 'fp': fp/(tn+fp), 'fn': fn/(fn+tp), 'tp': tp/(fn+tp)}\n\n\ndef rel2text(rel):\n    if rel == 'ReceivesAction':\n        rel_text = 'can be'\n    else:\n        p = re.compile(r'([a-z]|\\d)([A-Z])')\n        rel_text = re.sub(p, r'\\1 \\2', rel).lower()\n    return rel_text", "def rel2text(rel):\n    if rel == 'ReceivesAction':\n        rel_text = 'can be'\n    else:\n        p = re.compile(r'([a-z]|\\d)([A-Z])')\n        rel_text = re.sub(p, r'\\1 \\2', rel).lower()\n    return rel_text\n\n\ndef chunks_list_first(lst, n=1):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    lst = list(lst)\n    for i in range(0, len(lst), n):\n        yield lst[i : i + n]", "\ndef chunks_list_first(lst, n=1):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    lst = list(lst)\n    for i in range(0, len(lst), n):\n        yield lst[i : i + n]\n\n\ndef answer2bool(text, prefix='Answer'):\n    if prefix is not None:\n        # find if Yes, yes or No, no exsits in the text following the prefix with re\n        x = re.findall(f'{prefix}:\\s*(Yes|yes|No|no)', text)\n        x = x[0] if len(x) > 0 else text\n    else:\n        x = text\n    x = x.strip().lower().replace(\"<pad>\", \"\").replace(\"###</s>\", \"\").strip()\n    if x.startswith('yes'):\n        return 1\n    elif x.startswith('no'):\n        return 0\n    else:\n        return -1", "def answer2bool(text, prefix='Answer'):\n    if prefix is not None:\n        # find if Yes, yes or No, no exsits in the text following the prefix with re\n        x = re.findall(f'{prefix}:\\s*(Yes|yes|No|no)', text)\n        x = x[0] if len(x) > 0 else text\n    else:\n        x = text\n    x = x.strip().lower().replace(\"<pad>\", \"\").replace(\"###</s>\", \"\").strip()\n    if x.startswith('yes'):\n        return 1\n    elif x.startswith('no'):\n        return 0\n    else:\n        return -1", ""]}
{"filename": "base_generator.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\nimport sys\nimport os\nimport torch\nimport ujson as json\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,", "    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n)\nfrom accelerate import Accelerator\nfrom datasets import load_dataset, Dataset\nfrom torch.utils.data import DataLoader\n\n\naccelerator = Accelerator()", "\naccelerator = Accelerator()\ntorch.cuda.manual_seed_all(42)\n\n\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i: i + n]\n", "\n\ndef save_jsonl(jsl, output_file):\n    with open(output_file, 'w') as f:\n        for js in jsl:\n            f.write(json.dumps(js, ensure_ascii=False) + '\\n')\n    return output_file\n\n\nclass Seq2SeqBaseGenerator:\n    def __init__(self, model_name_or_path, cache_dir=None, use_auth_token=False, **model_kwargs):\n        # turn on deepspeed if the model is too large\n        max_retry_times = 3\n        for i in range(max_retry_times):\n            try:\n                self.tokenizer = AutoTokenizer.from_pretrained(\n                    model_name_or_path,\n                    cache_dir=cache_dir if cache_dir else None,\n                    use_auth_token=use_auth_token\n                )\n                self.model = AutoModelForSeq2SeqLM.from_pretrained(\n                    model_name_or_path,\n                    cache_dir=cache_dir if cache_dir else None,\n                    use_auth_token=use_auth_token,\n                    **model_kwargs\n                )\n                break\n            except Exception as e:\n                print(e)\n                print(f'* Reaching huggingface: {i}/{max_retry_times}')\n        \n        self.data_collator = DataCollatorForSeq2Seq(\n            self.tokenizer,\n            model=self.model,\n            label_pad_token_id=-100,\n            pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n        )\n\n    def preprocess_function(self, examples, max_source_length):\n        padding = 'max_length'\n        inputs = examples[self.source_column]\n        inputs = [self.prefix + inp for inp in inputs]\n        model_inputs = self.tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n        return model_inputs\n    \n    def make_source_from_list(self, data):\n        return [{'source': x} for x in data]\n\n    def generate(self,\n                 test_data_or_file,\n                 output_file=None,\n                 prefix='',\n                 source_column='source',\n                 beam_size=1,\n                 num_return_sequences=1,\n                 max_source_length=None,\n                 per_device_test_batch_size=4,\n                 verbose=True,\n                 num_proc=1,\n                 **generate_kwargs):\n        assert num_return_sequences <= beam_size\n\n        self.source_column = source_column\n        self.prefix = prefix\n\n        if isinstance(test_data_or_file, list):\n            if isinstance(test_data_or_file[0], str):\n                test_data_or_file = self.make_source_from_list(test_data_or_file)\n            raw_datasets = Dataset.from_list(test_data_or_file, split='test')\n        else:   # file\n            extension = test_data_or_file.split(\".\")[-1]\n            raw_datasets = load_dataset(extension, data_files={'test': test_data_or_file})['test']\n        column_names = raw_datasets.column_names\n\n        with accelerator.main_process_first():\n            processed_datasets = raw_datasets.map(\n                self.preprocess_function,\n                batched=True,\n                num_proc=num_proc,\n                remove_columns=column_names,\n                desc=\"Running tokenizer on dataset\",\n                fn_kwargs={\n                    'max_source_length': max_source_length,\n                }\n            )\n        # test_dataset = processed_datasets['test']\n        test_dataset = processed_datasets\n        test_dataloader = DataLoader(test_dataset, collate_fn=self.data_collator, batch_size=per_device_test_batch_size)\n        \n        self.model, test_dataloader = accelerator.prepare(self.model, test_dataloader)\n\n        predictions = []\n        scores = []\n        for batch in tqdm(test_dataloader, total=len(test_dataloader),\n                          disable=not verbose and not accelerator.is_local_main_process,\n                          desc=prefix):\n            with torch.no_grad():\n                generated_output = accelerator.unwrap_model(self.model).generate(\n                    batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                    num_beams=beam_size,\n                    num_return_sequences=num_return_sequences,\n                    output_scores=True,\n                    return_dict_in_generate=True,\n                    **generate_kwargs\n                )\n                generated_tokens = accelerator.pad_across_processes(\n                    generated_output.sequences, dim=1, pad_index=self.tokenizer.pad_token_id\n                )\n                generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n                if isinstance(generated_tokens, tuple):\n                    generated_tokens = generated_tokens[0]\n                decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n                preds = [pred.strip() for pred in decoded_preds]\n                predictions += preds\n\n                # gather scores\n                # generated_scores = accelerator.gather(generated_output.scores)\n                # generated_scores = (x.cpu().numpy() for x in\n                #                     generated_scores)  # before softmax: (max_length-1, [bsz * beam_size, vocab])\n                # scores += generated_scores\n\n        predictions = list(map(lambda x: x.strip(), predictions))\n        # TODO: `[:len(test_dataset)]` is a hack to deal with the auto-filling of the last incomplete batch.\n        predictions = list(chunks(predictions, num_return_sequences))[:len(test_dataset)] \n        if output_file is not None:\n            accelerator.wait_for_everyone()\n            if accelerator.is_main_process:\n                with open(output_file, 'w') as f:\n                    for pred in predictions:\n                        f.write(' '.join(pred) + '\\n')\n\n        return predictions", "\nclass Seq2SeqBaseGenerator:\n    def __init__(self, model_name_or_path, cache_dir=None, use_auth_token=False, **model_kwargs):\n        # turn on deepspeed if the model is too large\n        max_retry_times = 3\n        for i in range(max_retry_times):\n            try:\n                self.tokenizer = AutoTokenizer.from_pretrained(\n                    model_name_or_path,\n                    cache_dir=cache_dir if cache_dir else None,\n                    use_auth_token=use_auth_token\n                )\n                self.model = AutoModelForSeq2SeqLM.from_pretrained(\n                    model_name_or_path,\n                    cache_dir=cache_dir if cache_dir else None,\n                    use_auth_token=use_auth_token,\n                    **model_kwargs\n                )\n                break\n            except Exception as e:\n                print(e)\n                print(f'* Reaching huggingface: {i}/{max_retry_times}')\n        \n        self.data_collator = DataCollatorForSeq2Seq(\n            self.tokenizer,\n            model=self.model,\n            label_pad_token_id=-100,\n            pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n        )\n\n    def preprocess_function(self, examples, max_source_length):\n        padding = 'max_length'\n        inputs = examples[self.source_column]\n        inputs = [self.prefix + inp for inp in inputs]\n        model_inputs = self.tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n        return model_inputs\n    \n    def make_source_from_list(self, data):\n        return [{'source': x} for x in data]\n\n    def generate(self,\n                 test_data_or_file,\n                 output_file=None,\n                 prefix='',\n                 source_column='source',\n                 beam_size=1,\n                 num_return_sequences=1,\n                 max_source_length=None,\n                 per_device_test_batch_size=4,\n                 verbose=True,\n                 num_proc=1,\n                 **generate_kwargs):\n        assert num_return_sequences <= beam_size\n\n        self.source_column = source_column\n        self.prefix = prefix\n\n        if isinstance(test_data_or_file, list):\n            if isinstance(test_data_or_file[0], str):\n                test_data_or_file = self.make_source_from_list(test_data_or_file)\n            raw_datasets = Dataset.from_list(test_data_or_file, split='test')\n        else:   # file\n            extension = test_data_or_file.split(\".\")[-1]\n            raw_datasets = load_dataset(extension, data_files={'test': test_data_or_file})['test']\n        column_names = raw_datasets.column_names\n\n        with accelerator.main_process_first():\n            processed_datasets = raw_datasets.map(\n                self.preprocess_function,\n                batched=True,\n                num_proc=num_proc,\n                remove_columns=column_names,\n                desc=\"Running tokenizer on dataset\",\n                fn_kwargs={\n                    'max_source_length': max_source_length,\n                }\n            )\n        # test_dataset = processed_datasets['test']\n        test_dataset = processed_datasets\n        test_dataloader = DataLoader(test_dataset, collate_fn=self.data_collator, batch_size=per_device_test_batch_size)\n        \n        self.model, test_dataloader = accelerator.prepare(self.model, test_dataloader)\n\n        predictions = []\n        scores = []\n        for batch in tqdm(test_dataloader, total=len(test_dataloader),\n                          disable=not verbose and not accelerator.is_local_main_process,\n                          desc=prefix):\n            with torch.no_grad():\n                generated_output = accelerator.unwrap_model(self.model).generate(\n                    batch[\"input_ids\"],\n                    attention_mask=batch[\"attention_mask\"],\n                    num_beams=beam_size,\n                    num_return_sequences=num_return_sequences,\n                    output_scores=True,\n                    return_dict_in_generate=True,\n                    **generate_kwargs\n                )\n                generated_tokens = accelerator.pad_across_processes(\n                    generated_output.sequences, dim=1, pad_index=self.tokenizer.pad_token_id\n                )\n                generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n                if isinstance(generated_tokens, tuple):\n                    generated_tokens = generated_tokens[0]\n                decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n                preds = [pred.strip() for pred in decoded_preds]\n                predictions += preds\n\n                # gather scores\n                # generated_scores = accelerator.gather(generated_output.scores)\n                # generated_scores = (x.cpu().numpy() for x in\n                #                     generated_scores)  # before softmax: (max_length-1, [bsz * beam_size, vocab])\n                # scores += generated_scores\n\n        predictions = list(map(lambda x: x.strip(), predictions))\n        # TODO: `[:len(test_dataset)]` is a hack to deal with the auto-filling of the last incomplete batch.\n        predictions = list(chunks(predictions, num_return_sequences))[:len(test_dataset)] \n        if output_file is not None:\n            accelerator.wait_for_everyone()\n            if accelerator.is_main_process:\n                with open(output_file, 'w') as f:\n                    for pred in predictions:\n                        f.write(' '.join(pred) + '\\n')\n\n        return predictions", "\n\ndef test():\n    input_data = [\n                     {'source': '1. is time changing globally? (A) yes (B) no', 'answer': 'time'},\n                     {'source': '2. is rabbit the largest animal on earth?', 'answer': 'one thing'},\n                     {'source': '3. is donald trump american? \\n (A) yes (B) no', 'answer': 'two birds'},\n                     {'source': '4. is time changing globally? (A) yes (B) no', 'answer': 'time'},\n                     {'source': '5. is rabbit the largest animal on earth?', 'answer': 'one thing'},\n                     {'source': '6. is donald trump american? \\n (A) yes (B) no', 'answer': 'two birds'},\n                     {'source': '7. is time changing globally? (A) yes (B) no', 'answer': 'time'},\n                     {'source': '8. is rabbit the largest animal on earth?', 'answer': 'one thing'},\n                     {'source': '9. is donald trump american? \\n (A) yes (B) no', 'answer': 'two birds'},\n                    #  {'source': '4. who is the president of america?', 'answer': 'three values'},\n                 ]\n\n    generator = Seq2SeqBaseGenerator('t5-small')\n    results = generator.generate(input_data, beam_size=2, num_proc=1, prefix='translate: ')\n\n    if accelerator.is_main_process:\n        print(results)", "\n\nif __name__ == '__main__':\n    test()\n"]}
{"filename": "llm_utils.py", "chunked_list": ["import os\nimport re\nimport random\nimport ujson as json\nfrom utils import load_jsonl, rel2text\nimport cjjpy as cjj\n\nrandom.seed(42)\n\nNEG_EX_FILE = f\"{cjj.AbsParentDir(__file__, '.')}/ICL_examples/neg_example_pool.jsonl\"", "\nNEG_EX_FILE = f\"{cjj.AbsParentDir(__file__, '.')}/ICL_examples/neg_example_pool.jsonl\"\nPOS_EX_FILE = f\"{cjj.AbsParentDir(__file__, '.')}/ICL_examples/pos_example_pool.jsonl\"\n\n_CG_INSTRUCTIONS = {\n    'question': {\n        'none': [\n            \"Answer the question by writing a short sentence that contains correct common sense knowledge.\",\n        ],\n        'fact': [", "        ],\n        'fact': [\n            \"Answer the question by writing a sentence that contains correct common sense knowledge. Find a related fact to help write the sentence:\",\n        ],\n        'logic': [\n            \"Answer the question by writing a sentence that contains correct common sense knowledge. Let's think step by step before writing the sentence:\",\n        ]\n    },\n    'keywords': {\n        'none': [", "    'keywords': {\n        'none': [\n            \"Write a short and factual sentence according to commonsense based on the keywords:\",\n            \"Use the keywords to create a short and factual sentence that accurately reflects commonsense knowledge.\",\n            \"Create a short, factual sentence based on the keywords and what is generally accepted as true.\",\n            \"Construct a factual and concise statement based on the provided keywords and commonsense knowledge.\"\n        ],\n        'fact': [\n            \"Write a short and factual sentence according to commonsense based on the keywords. Find a related fact to help write the sentence:\",\n        ],", "            \"Write a short and factual sentence according to commonsense based on the keywords. Find a related fact to help write the sentence:\",\n        ],\n        'logic': [\n            \"Write a short and factual sentence according to commonsense based on the keywords. Let's think step by step before writing the sentence:\",\n        ]\n    }\n}\n_QA_INSTRUCTIONS = {\n    'question': {\n        'none': [", "    'question': {\n        'none': [\n            \"Answer the commonsense questions with yes or no:\",\n            \"Choose \\\"yes\\\" or \\\"no\\\" to indicate whether you agree or disagree with the commonsense questions.\",\n            \"Respond to the questions using \\\"yes\\\" or \\\"no\\\".\",\n            \"Indicate whether the commonsense questions are correct or incorrect by writing \\\"yes\\\" or \\\"no\\\".\",\n        ],\n        'fact': [\n            \"Find a related fact to about the question, then answer it with yes or no:\",\n        ],", "            \"Find a related fact to about the question, then answer it with yes or no:\",\n        ],\n        'logic': [\n            \"Let's think step by step to about the question, then answer it with yes or no:\",\n        ]\n    },\n    'keywords': {\n        'none': [\n            \"Can these keywords form a truthful commonsense fact? Answer with yes or no:\",\n        ],", "            \"Can these keywords form a truthful commonsense fact? Answer with yes or no:\",\n        ],\n        'fact': [\n            \"Can these keywords form a truthful commonsense fact? Find a helpful and related fact to answer this question. Answer with yes or no:\",\n        ],\n        'logic': [\n            \"Can these keywords form a truthful commonsense fact? Let's think step by step to answer this question. Answer with yes or no:\",\n        ]\n    }\n}", "    }\n}\n\nINSTRUCTIONS = {\n    'qa': _QA_INSTRUCTIONS,\n    'cg': _CG_INSTRUCTIONS\n}\nTASK_KEY = {\n    'qa': {\n        'question': 'qa_pred',", "    'qa': {\n        'question': 'qa_pred',\n        'keywords': 'qa_pred_from_kw',\n    },\n    'cg': {\n        'question': 'cg_pred_from_q',\n        'keywords': 'cg_pred',\n    }\n}\nINPUT_TYPE_PARAMS = {", "}\nINPUT_TYPE_PARAMS = {\n    'qa': {\n        'question': {\n            'input_key': 'question',\n            'input_str': 'Question',\n        },\n        'keywords': {\n            'input_key': 'keywords',\n            'input_str': 'Keywords',", "            'input_key': 'keywords',\n            'input_str': 'Keywords',\n        }\n    },\n    'cg': {\n        'question': {\n            'input_key': 'question',\n            'input_str': 'Question',\n        },\n        'keywords': {", "        },\n        'keywords': {\n            'input_key': 'keywords',\n            'input_str': 'Keywords',\n        }\n    }\n}\nOUTPUT_EXAMPLE_KEY = {  # used when loading examples\n    'qa': 'answer',\n    'cg': 'sentence'", "    'qa': 'answer',\n    'cg': 'sentence'\n}\nOUTPUT_TYPE_PARAMS = {\n    'qa': \"Answer\",\n    'cg': \"Sentence\"\n}\n\ndef load_examples(input_key, input_str, output_key, output_str, cot_key=None, cot_str=None):\n    neg_ex = load_jsonl(NEG_EX_FILE)\n    pos_ex = load_jsonl(POS_EX_FILE)\n\n    def assemble_demonstration(x):\n        '''\n        param x: {'keywords': '...', 'sentence': '...'}\n        return: 'Keywords: ... \\nSentence: ...'\n        '''\n        input_ex = x[input_key].strip() if input_key != 'keywords' else ', '.join(\n            x[input_key]).strip()\n        ds = f\"{input_str}: {input_ex}\\n\"\n        if cot_key is not None and cot_str is not None:\n            if x.get(cot_key) is not None:\n                ds += f\"{cot_str}: {x[cot_key]}\\n\"\n            else:\n                print(f'Warning: no {cot_key}:', x)\n        ds += f\"{output_str}: {x[output_key].strip()}\"\n        return ds\n\n    neg_ex = [assemble_demonstration(x) for x in neg_ex]\n    pos_ex = [assemble_demonstration(x) for x in pos_ex]\n\n    return pos_ex, neg_ex", "def load_examples(input_key, input_str, output_key, output_str, cot_key=None, cot_str=None):\n    neg_ex = load_jsonl(NEG_EX_FILE)\n    pos_ex = load_jsonl(POS_EX_FILE)\n\n    def assemble_demonstration(x):\n        '''\n        param x: {'keywords': '...', 'sentence': '...'}\n        return: 'Keywords: ... \\nSentence: ...'\n        '''\n        input_ex = x[input_key].strip() if input_key != 'keywords' else ', '.join(\n            x[input_key]).strip()\n        ds = f\"{input_str}: {input_ex}\\n\"\n        if cot_key is not None and cot_str is not None:\n            if x.get(cot_key) is not None:\n                ds += f\"{cot_str}: {x[cot_key]}\\n\"\n            else:\n                print(f'Warning: no {cot_key}:', x)\n        ds += f\"{output_str}: {x[output_key].strip()}\"\n        return ds\n\n    neg_ex = [assemble_demonstration(x) for x in neg_ex]\n    pos_ex = [assemble_demonstration(x) for x in pos_ex]\n\n    return pos_ex, neg_ex", "\n\ndef prepare_prompt(task, js, k_pos_ex=1, k_neg_ex=1, key_q='text-davinci-002_ex-8', cot='none'):\n    assert task in ['qa', 'cg'], task\n    assert cot in ['fact', 'logic', 'none'], cot\n\n    input_type = 'keywords' if key_q == 'keywords' else 'question'\n    if cot == 'none':\n        cot_key, cot_str = None, None\n    elif cot == 'fact':\n        cot_key, cot_str = 'cot_fact', 'Related fact'\n    elif cot == 'logic':\n        cot_key, cot_str = 'cot_logic', \"Let's think step by step\"\n    else:\n        raise NotImplementedError\n\n    examples_pos, examples_neg = load_examples(input_key=INPUT_TYPE_PARAMS[task][input_type]['input_key'],\n                                               input_str=INPUT_TYPE_PARAMS[task][input_type]['input_str'],\n                                               output_key=OUTPUT_EXAMPLE_KEY[task], output_str=OUTPUT_TYPE_PARAMS[task] if cot == 'none' else 'Therefore',\n                                               cot_key=cot_key, cot_str=cot_str)\n\n    if key_q != 'keywords':\n        q = js['boolq'][key_q].strip().lower()\n    else:\n        q = triple2keywords(js['subject'], js['predicate'], js['object']).lower()\n\n    # build demonstrations with random examples\n    instruct = INSTRUCTIONS[task][input_type][cot][0]  # TODO: hardcoded\n    examples = sample_examples_pos_neg(examples_neg, k_neg_ex, examples_pos, k_pos_ex)\n    example_text = examples_to_text(examples, '###')\n    \n    _cue = OUTPUT_TYPE_PARAMS[task] if cot_str is None else cot_str\n    demonstration = f\"{instruct}\\n\\n{example_text}\\n{INPUT_TYPE_PARAMS[task][input_type]['input_str']}: {q}\\n{_cue}:\"\n\n    return demonstration", "\n\ndef triple2keywords(s, p, o):\n    p_text = rel2text(p)\n    return f\"{s}, {p_text}, {o}\"\n\n\ndef examples_to_text(examples: list, delim=\"###\"):\n    if examples == []: \n        return ''\n    else:\n        return f\"\\n{delim}\\n\".join(examples) + f\"\\n{delim}\"", "\n\ndef sample_examples_pos_neg(ex1: list, k1: int, ex2: list, k2: int):\n    ex = random.sample(ex1, k1)\n    ex += random.sample(ex2, k2)\n    random.shuffle(ex)\n    return ex\n\n\ndef save_llm_results(input_file_or_data, y_pred, task_key, model_key, output_file):\n    # support (pseudo) multiprocessing: avoid overwrite, reload in-disk data\n    data = load_jsonl(input_file_or_data)\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    with open(output_file, 'w') as fo:\n        for x, a in zip(data, y_pred):\n            if x.get(task_key) is None:\n                x[task_key] = {model_key: a}\n            else:\n                x[task_key][model_key] = a\n            fo.write(json.dumps(x) + '\\n')", "\ndef save_llm_results(input_file_or_data, y_pred, task_key, model_key, output_file):\n    # support (pseudo) multiprocessing: avoid overwrite, reload in-disk data\n    data = load_jsonl(input_file_or_data)\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    with open(output_file, 'w') as fo:\n        for x, a in zip(data, y_pred):\n            if x.get(task_key) is None:\n                x[task_key] = {model_key: a}\n            else:\n                x[task_key][model_key] = a\n            fo.write(json.dumps(x) + '\\n')", ""]}
{"filename": "boolqa/cjjpy.py", "chunked_list": ["\ufeff# -*- coding: utf-8 -*-\n\n'''\n@Author : Jiangjie Chen\n@Time   : 2022/5/26 19:52\n@Contact: jjchen19@fudan.edu.cn\n'''\n\nimport re\nimport datetime", "import re\nimport datetime\nimport os\nimport subprocess\nimport urllib.request, urllib.parse\nimport argparse\nfrom tqdm import tqdm\nimport sqlite3\nimport requests\nimport socket", "import requests\nimport socket\nimport logging\nimport io\nimport traceback\n\ntry:\n    import ujson as json\nexcept:\n    import json", "except:\n    import json\n\nHADOOP_BIN = 'PATH=/usr/bin/:$PATH hdfs'\n\n\ndef LengthStats(filename, key4json=None):\n    len_list = []\n    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n    with open(filename) as f:", "    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n    with open(filename) as f:\n        for line in f:\n            if key4json not in ['none', None, 'None']:\n                len_list.append(len(json.loads(line)[key4json].split()))\n            else:\n                len_list.append(len(line.strip().split()))\n    stats = {\n        'Max': max(len_list),\n        'Min': min(len_list),", "        'Max': max(len_list),\n        'Min': min(len_list),\n        'Avg': round(sum(len_list) / len(len_list), 4),\n    }\n    len_list.sort()\n    for t in thresholds:\n        stats[f\"Top-{t}\"] = len_list[int(len(len_list) * t)]\n\n    for k in stats:\n        print(f\"- {k}: {stats[k]}\")", "    for k in stats:\n        print(f\"- {k}: {stats[k]}\")\n    return stats\n\n\nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n", "        self.__dict__ = self\n\n\ndef TraceBack(error_msg):\n    exc = traceback.format_exc()\n    msg = f'[Error]: {error_msg}.\\n[Traceback]: {exc}'\n    return msg\n\n\ndef Now():", "\ndef Now():\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef TorchHLoad(filepath: str, **kwargs):\n    import torch, tensorflow as tf\n    if not filepath.startswith(\"hdfs://\"):\n        return torch.load(filepath, **kwargs)\n    else:", "        return torch.load(filepath, **kwargs)\n    else:\n        with tf.io.gfile.GFile(filepath, 'rb') as reader:\n            return torch.load(io.BytesIO(reader.read()), **kwargs)\n\n\ndef TorchHSave(obj, filepath: str, **kwargs):\n    import torch, tensorflow as tf\n    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n        with tf.io.gfile.GFile(filepath, 'wb') as f:", "    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n        with tf.io.gfile.GFile(filepath, 'wb') as f:\n            buffer = io.BytesIO()\n            torch.save(obj, buffer, **kwargs)\n            f.write(buffer.getvalue())\n    else:\n        torch.save(obj, filepath, **kwargs)\n\n\ndef PutHDFS(local: str, remote: str):", "\ndef PutHDFS(local: str, remote: str):\n    import tensorflow as tf\n    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n    if not tf.io.gfile.exists(remote):\n        tf.io.gfile.makedirs(remote)\n    RunCmd(f'{HADOOP_BIN} dfs -put {local} {remote}')\n\n\ndef GetHDFS(remote: str, local: str):", "\ndef GetHDFS(remote: str, local: str):\n    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n    os.makedirs(local, exist_ok=True)\n    RunCmd(f'{HADOOP_BIN} dfs -get {remote} {local}')\n\n\ndef RunCmd(command):\n    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    res, err = pipe.communicate()", "    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    res, err = pipe.communicate()\n    res = res.decode('utf-8')\n    err = err.decode('utf-8')\n    return res, err\n\n\ndef AbsParentDir(file, parent='..', postfix=None):\n    ppath = os.path.abspath(file)\n    parent_level = parent.count('.')", "    ppath = os.path.abspath(file)\n    parent_level = parent.count('.')\n    while parent_level > 0:\n        ppath = os.path.dirname(ppath)\n        parent_level -= 1\n    if postfix is not None:\n        return os.path.join(ppath, postfix)\n    else:\n        return ppath\n", "        return ppath\n\n\ndef init_logger(log_file=None, log_file_level=logging.NOTSET, from_scratch=False):\n    from coloredlogs import ColoredFormatter\n    import tensorflow as tf\n\n    fmt = \"[%(asctime)s %(levelname)s] %(message)s\"\n    log_format = ColoredFormatter(fmt=fmt)\n    # log_format = logging.Formatter()", "    log_format = ColoredFormatter(fmt=fmt)\n    # log_format = logging.Formatter()\n    logger = logging.getLogger()\n    logger.setLevel(log_file_level)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_format)\n    logger.handlers = [console_handler]\n\n    if log_file and log_file != '':", "\n    if log_file and log_file != '':\n        if from_scratch and tf.io.gfile.exists(log_file):\n            logger.warning('Removing previous log file: %s' % log_file)\n            tf.io.gfile.remove(log_file)\n        path = os.path.dirname(log_file)\n        os.makedirs(path, exist_ok=True)\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(log_file_level)\n        file_handler.setFormatter(log_format)", "        file_handler.setLevel(log_file_level)\n        file_handler.setFormatter(log_format)\n        logger.addHandler(file_handler)\n\n    return logger\n\n\ndef is_qid(x, hard=False):\n    if type(x) is not str: return None\n    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)", "    if type(x) is not str: return None\n    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)\n    return None if len(ret) == 0 else ret[0]\n\n\ndef is_pid(x, hard=False):\n    if type(x) is not str: return None\n    ret = re.findall('^P\\d+$', x) if hard else re.findall('P\\d+', x)\n    return None if len(ret) == 0 else ret[0]\n", "    return None if len(ret) == 0 else ret[0]\n\n\nclass MiniLutDB:\n    def __init__(self, db, verbose=True):\n        self.db = db\n        self.conn = None\n        self.verbose = verbose\n\n    def dump_lut(self, lut_tuples, verbose=None):", "\n    def dump_lut(self, lut_tuples, verbose=None):\n        # lut_tuple: (k, v)+, iterable\n\n        if verbose is None: \n            verbose = self.verbose\n        self.conn = sqlite3.connect(self.db)\n        cur = self.conn.cursor()\n        cur.executescript('''\n        DROP TABLE IF EXISTS lut;", "        cur.executescript('''\n        DROP TABLE IF EXISTS lut;\n        CREATE TABLE lut (\n        id      TEXT PRIMARY KEY UNIQUE,\n        content TEXT)''')\n        self.conn.commit()\n\n        BLOCKSIZE = 100000\n        block = []\n        i = 0", "        block = []\n        i = 0\n        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n        for x in iter:\n            block.append(x)\n            i += 1\n            if i == BLOCKSIZE:\n                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n                block = []\n                i = 0", "                block = []\n                i = 0\n        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n        self.conn.commit()\n\n        self.close()\n\n    def update_lut(self, lut_tuples, verbose=None):\n        if verbose is None: \n            verbose = self.verbose", "        if verbose is None: \n            verbose = self.verbose\n\n        self.conn = sqlite3.connect(self.db)\n        BLOCKSIZE = 100000\n        block = []\n        i = 0\n        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n        for x in iter:\n            block.append(x)", "        for x in iter:\n            block.append(x)\n            i += 1\n            if i == BLOCKSIZE:\n                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n                block = []\n                i = 0\n        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n        self.conn.commit()\n", "        self.conn.commit()\n\n        self.close()\n\n    def create_index(self):\n        self.conn = sqlite3.connect(self.db)\n        self.cur = self.conn.cursor()\n        # sql = ('CREATE INDEX index_lut ON lut(id);')\n        # self.cur.execute(sql)\n        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')", "        # self.cur.execute(sql)\n        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')\n        self.conn.commit()\n\n    def get(self, x, default=None):\n        if x is None: return default\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db)\n            self.cur = self.conn.cursor()\n", "            self.cur = self.conn.cursor()\n\n        res = self.query_lut(self.cur, x, False)[0]\n        return res if res is not None else default\n\n    def get_chunk(self, xx):\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db)\n            self.cur = self.conn.cursor()\n        return self.query_lut(self.conn, xx, self.verbose)", "            self.cur = self.conn.cursor()\n        return self.query_lut(self.conn, xx, self.verbose)\n\n    def close(self):\n        if self.conn is not None:\n            self.conn.close()\n            self.conn = None\n    \n    def delete_sample(self, key, value=None):\n        if self.get(key) is None: return", "    def delete_sample(self, key, value=None):\n        if self.get(key) is None: return\n        self.conn = sqlite3.connect(self.db)\n        self.cur = self.conn.cursor()\n        self.cur.execute('DELETE FROM lut WHERE id = ?', (key,))\n        self.conn.commit()\n        assert self.get(key) is None, f'delete failed: {key}'\n\n    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n        values = []", "    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n        values = []\n        if isinstance(keys, str): keys = [keys]\n\n        iter = tqdm(keys, mininterval=0.5, disable=not verbose)\n        for k in iter:\n            cur.execute('SELECT content FROM lut WHERE id = ?', (k,))\n            val = cur.fetchone()\n            val = val[0] if val is not None else None\n            values.append(val)", "            val = val[0] if val is not None else None\n            values.append(val)\n        return values\n\n\ndef OverWriteCjjPy(root='.'):\n    # import difflib\n    # diff = difflib.HtmlDiff()\n    cnt = 0\n    golden_cjjpy = os.path.join(root, 'cjjpy.py')", "    cnt = 0\n    golden_cjjpy = os.path.join(root, 'cjjpy.py')\n    # golden_content = open(golden_cjjpy).readlines()\n    for dir, folder, file in os.walk(root):\n        for f in file:\n            if f == 'cjjpy.py':\n                cjjpy = '%s/%s' % (dir, f)\n                # content = open(cjjpy).readlines()\n                # d = diff.make_file(golden_content, content)\n                cnt += 1", "                # d = diff.make_file(golden_content, content)\n                cnt += 1\n                print('[%d]: %s' % (cnt, cjjpy))\n                os.system('cp %s %s' % (golden_cjjpy, cjjpy))\n\n\ndef ReplaceChar(file, replaced, replacer):\n    print(file, replaced, replacer)\n    with open(file) as f:\n        data = f.readlines()", "    with open(file) as f:\n        data = f.readlines()\n        out = open(file, 'w')\n        for line in data:\n            out.write(line.replace(replaced, replacer))\n\n\ndef DeUnicode(line):\n    return line.encode('utf-8').decode('unicode_escape')\n", "    return line.encode('utf-8').decode('unicode_escape')\n\n\ndef LoadIDDict(dict_file, unify_words=False, lower=False, reverse=False):\n    '''\n    a\\tb\\n, `.dict' file\n    '''\n    import tensorflow as tf\n    assert dict_file.endswith('.dict')\n    id2label = {}", "    assert dict_file.endswith('.dict')\n    id2label = {}\n    with tf.io.gfile.GFile(dict_file, 'r') as f:\n        data = f.read().split('\\n')\n        for i, line in enumerate(data):\n            if line == '': continue\n            try:\n                id, label = line.split('\\t')\n                if reverse:\n                    id, label = label, id", "                if reverse:\n                    id, label = label, id\n                _val = '_'.join(label.split()) if unify_words else label\n                id2label[id] = _val.lower() if lower else _val\n            except:\n                pass\n    return id2label\n\n\ndef LoadWords(file, is_file=True):", "\ndef LoadWords(file, is_file=True):\n    import tensorflow as tf\n    if is_file:\n        with tf.io.gfile.GFile(file, 'r') as f:\n            data = f.read().splitlines()\n    else:\n        data = file.splitlines()\n    return set(map(lambda x: x.strip(), data))\n", "    return set(map(lambda x: x.strip(), data))\n\n\ndef ChangeFileFormat(filename, new_fmt):\n    assert type(filename) is str and type(new_fmt) is str\n    spt = filename.split('.')\n    if len(spt) == 0:\n        return filename\n    else:\n        return filename.replace('.' + spt[-1], new_fmt)", "    else:\n        return filename.replace('.' + spt[-1], new_fmt)\n\n\ndef CountLines(fname):\n    with open(fname, 'rb') as f:\n        count = 0\n        last_data = '\\n'\n        while True:\n            data = f.read(0x400000)", "        while True:\n            data = f.read(0x400000)\n            if not data:\n                break\n            count += data.count(b'\\n')\n            last_data = data\n        if last_data[-1:] != b'\\n':\n            count += 1  # Remove this if a wc-like count is needed\n    return count\n", "    return count\n\n\ndef SearchByKey(file, key):\n    with open(file, 'r') as fin:\n        while True:\n            line = fin.readline()\n            if not line: break\n            if key in line:\n                print(line, end='')", "            if key in line:\n                print(line, end='')\n\n\ndef SendEmail(subject, content, receivers=['MichaelChen0110@163.com']):\n    from email.mime.text import MIMEText\n    import smtplib\n\n    # receivers got to be list.\n    mail_receivers = receivers", "    # receivers got to be list.\n    mail_receivers = receivers\n    # mail_host = \"smtp.163.com\n    mail_host = \"220.181.12.18\"\n    mail_user = \"MichaelChen0110@163.com\"\n    mail_pass = \"\"\n    me = socket.gethostname() + \"<\" + mail_user + \">\"\n    msg = MIMEText(content, _subtype='plain', _charset='utf-8')\n    msg['Subject'] = subject\n    msg['From'] = me", "    msg['Subject'] = subject\n    msg['From'] = me\n    msg['To'] = \";\".join(mail_receivers)\n    try:\n        server = smtplib.SMTP()\n        server.connect(mail_host)\n        server.login(mail_user, mail_pass)\n        server.sendmail(me, mail_receivers, msg.as_string())\n        server.close()\n        print('Have sent the email to ' + str(mail_receivers) + '. ')", "        server.close()\n        print('Have sent the email to ' + str(mail_receivers) + '. ')\n        return True\n    except Exception as e:\n        print(str(e))\n        return False\n\n\ndef SortDict(_dict, reverse=True):\n    assert type(_dict) is dict", "def SortDict(_dict, reverse=True):\n    assert type(_dict) is dict\n    return sorted(_dict.items(), key=lambda d: d[1], reverse=reverse)\n\n\ndef MaxCommLen(str1, str2):\n    lstr1 = len(str1)\n    lstr2 = len(str2)\n    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n    max_num = 0", "    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n    max_num = 0\n    for i in range(lstr1):\n        for j in range(lstr2):\n            if str1[i] == str2[j]:\n                record[i + 1][j + 1] = record[i][j] + 1\n                if record[i + 1][j + 1] > max_num:\n                    max_num = record[i + 1][j + 1]\n    return max_num, ''\n", "    return max_num, ''\n\n\ndef lark(content='test'):\n    print(content)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n", "    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--diff', nargs=2,\n                        help='show difference between two files, shown in downloads/diff.html')\n    parser.add_argument('--de_unicode', action='store_true', default=False,\n                        help='remove unicode characters')\n    parser.add_argument('--link_entity', action='store_true', default=False,\n                        help='')\n    parser.add_argument('--max_comm_len', action='store_true', default=False,\n                        help='')", "    parser.add_argument('--max_comm_len', action='store_true', default=False,\n                        help='')\n    parser.add_argument('--search', nargs=2,\n                        help='search key from file, 2 args: file name & key')\n    parser.add_argument('--email', nargs=2,\n                        help='sending emails, 2 args: subject & content')\n    parser.add_argument('--overwrite', action='store_true', default=None,\n                        help='overwrite all cjjpy under given *dir* based on *dir*/cjjpy.py')\n    parser.add_argument('--replace', nargs=3,\n                        help='replace char, 3 args: file name & replaced char & replacer char')", "    parser.add_argument('--replace', nargs=3,\n                        help='replace char, 3 args: file name & replaced char & replacer char')\n    parser.add_argument('--lark', nargs=1)\n    parser.add_argument('--get_hdfs', nargs=2,\n                        help='easy copy from hdfs to local fs, 2 args: remote_file/dir & local_dir')\n    parser.add_argument('--put_hdfs', nargs=2,\n                        help='easy put from local fs to hdfs, 2 args: local_file/dir & remote_dir')\n    parser.add_argument('--length_stats', nargs=2,\n                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n", "                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n\n    args = parser.parse_args()\n\n    if args.overwrite:\n        print('* Overwriting cjjpy...')\n        OverWriteCjjPy()\n\n    if args.replace:\n        print('* Replacing Char...')", "    if args.replace:\n        print('* Replacing Char...')\n        ReplaceChar(args.replace[0], args.replace[1], args.replace[2])\n\n    if args.search:\n        file = args.search[0]\n        key = args.search[1]\n        print('* Searching %s from %s...' % (key, file))\n        SearchByKey(file, key)\n", "        SearchByKey(file, key)\n\n    if args.email:\n        try:\n            subj = args.email[0]\n            cont = args.email[1]\n        except:\n            subj = 'running complete'\n            cont = ''\n        print('* Sending email {%s, %s} to host...' % (subj, cont))", "            cont = ''\n        print('* Sending email {%s, %s} to host...' % (subj, cont))\n        SendEmail(subj, cont)\n\n    if args.lark:\n        try:\n            content = args.lark[0]\n        except:\n            content = 'running complete'\n        print(f'* Larking \"{content}\"...')", "            content = 'running complete'\n        print(f'* Larking \"{content}\"...')\n        lark(content)\n\n    if args.get_hdfs:\n        remote = args.get_hdfs[0]\n        local = args.get_hdfs[1]\n        print(f'* Copying {remote} to {local}...')\n        GetHDFS(remote, local)\n", "        GetHDFS(remote, local)\n\n    if args.put_hdfs:\n        local = args.put_hdfs[0]\n        remote = args.put_hdfs[1]\n        print(f'* Copying {local} to {remote}...')\n        PutHDFS(local, remote)\n\n    if args.length_stats:\n        file = args.length_stats[0]", "    if args.length_stats:\n        file = args.length_stats[0]\n        key4json = args.length_stats[1]\n        print(f'* Working on {file} lengths statistics...')\n        LengthStats(file, key4json)\n"]}
{"filename": "boolqa/llm_boolqg.py", "chunked_list": ["import os\nimport sys\nimport random\nimport ujson as json\nimport numpy as np\nimport cjjpy as cjj\n\nsys.path.append('..')\nfrom gpt3_helper import prompt_gpt3, calc_cost_w_prompt\nfrom utils import load_jsonl, rel2text, chunks_list_first", "from gpt3_helper import prompt_gpt3, calc_cost_w_prompt\nfrom utils import load_jsonl, rel2text, chunks_list_first\nfrom llm_utils import examples_to_text\nnp.random.seed(42)\nrandom.seed(42)\n\nboolqg_instructions = [\n    \"Write a question that asks about the validity of a commonsense knowledge triple (subject, relation, object):\",\n]\n", "]\n\nboolqg_examples_generally = [\n    \"Triple: (bird, capable of, fly)\\nQuestion: can most birds fly?\",\n    \"Triple: (playing tennis, causes, feel relaxed)\\nQuestion: does playing tennis generally make you feel relaxed?\",\n    \"Triple: (room, located at, buildings)\\nQuestion: are rooms usually located at buildings?\",\n    \"Triple: (fish, capable of, sleep)\\nQuestion: can fish sleep?\",\n    \"Triple: (sheepskin, used for, writing)\\nQuestion: can sheepskin be used for writing?\",\n    \"Triple: (spicy, is a, pain)\\nQuestion: is spicy a kind of pain?\",\n    \"Triple: (water, has property, spicy)\\nQuestion: is water usually spicy?\",", "    \"Triple: (spicy, is a, pain)\\nQuestion: is spicy a kind of pain?\",\n    \"Triple: (water, has property, spicy)\\nQuestion: is water usually spicy?\",\n    \"Triple: (shields, made of, grass)\\nQuestion: are shields generally made of grass?\",\n    \"Triple: (penguins, is a, mammal)\\nQuestion: are penguins a kind of mammal?\",\n    \"Triple: (work, causes desire, rest)\\nQuestion: does working usually make you want to rest?\",\n    \"Triple: (sleeves, part of, shoes)\\nQuestion: are sleeves a part of shoes?\",\n    \"Triple: (books, part of, kitchen)\\nQuestion: are books usually part of a kitchen?\",\n]\n\nboolqg_examples = [", "\nboolqg_examples = [\n    \"Triple: (bird, capable of, fly)\\nQuestion: can birds fly?\",\n    \"Triple: (playing tennis, causes, feel relaxed)\\nQuestion: does playing tennis make you feel relaxed?\",\n    \"Triple: (room, located at, buildings)\\nQuestion: are rooms located at buildings?\",\n    \"Triple: (fish, capable of, sleep)\\nQuestion: can fish sleep?\",\n    \"Triple: (sheepskin, used for, writing)\\nQuestion: can sheepskin be used for writing?\",\n    \"Triple: (spicy, is a, pain)\\nQuestion: is spicy a kind of pain?\",\n    \"Triple: (water, has property, spicy)\\nQuestion: is water spicy?\",\n    \"Triple: (shields, made of, grass)\\nQuestion: are shields made of grass?\",", "    \"Triple: (water, has property, spicy)\\nQuestion: is water spicy?\",\n    \"Triple: (shields, made of, grass)\\nQuestion: are shields made of grass?\",\n    \"Triple: (penguins, is a, mammal)\\nQuestion: are penguins a kind of mammal?\",\n    \"Triple: (work, causes desire, rest)\\nQuestion: does working make you want to rest?\",\n    \"Triple: (sleeves, part of, shoes)\\nQuestion: are sleeves a part of shoes?\",\n    \"Triple: (books, part of, kitchen)\\nQuestion: are books part of a kitchen?\",\n]\n\n\ndef prep_triple(subj, pred, obj):\n    pred = rel2text(pred)\n    return f\"({subj}, {pred}, {obj})\"", "\ndef prep_triple(subj, pred, obj):\n    pred = rel2text(pred)\n    return f\"({subj}, {pred}, {obj})\"\n\n\ndef llm_boolqg(model_name, input_file, output_file=None, k_ex=8, batch_size=8, generally=False):\n    data = load_jsonl(input_file)\n\n    prompts = []\n    for line in data:\n        # TODO: triple\n        triple = prep_triple(line['subject'], line['predicate'], line['object'])\n        instruct = boolqg_instructions[0]\n        qg_ex = boolqg_examples_generally if generally else boolqg_examples\n        examples = np.random.choice(qg_ex, k_ex, replace=False).tolist()\n        random.shuffle(examples)\n        example_text = examples_to_text(examples, '###')\n        demonstration = f\"{instruct}\\n\\n{example_text}\\nTriple:\"\n\n        prompts.append(f\"{demonstration} {triple}\\nQuestion:\")\n\n    y_pred = []\n\n    res, money = prompt_gpt3(prompts, model_name=model_name, clean=True, temperature=0., max_tokens=32, batch_size=batch_size, verbose=True)\n    for ny, indiv_prompt in zip(chunks_list_first(res), prompts):\n        # handle n_returned sequences\n        y_pred.append(ny)\n        print(indiv_prompt + ny[0])\n    \n    general = '_general' if generally else ''\n    model_key = f\"{model_name}_ex-{k_ex}{general}\"\n    # log predicted answers by LLM $-$!!!\n    if output_file is not None:\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n        with open(output_file, 'w') as f:\n            for x, a in zip(data, y_pred):\n                if isinstance(a, list): a = a[0]\n                if x.get('boolq') is None:\n                    x['boolq'] = {model_key: a}\n                else:\n                    x['boolq'][model_key] = a\n                # x['prompts'] = p\n                f.write(json.dumps(x) + '\\n')\n    \n    cjj.lark(f\"This run has cost you {round(money, 2)}$: {model_key}.\")\n    \n    return y_pred", "\n\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_file', '-i', type=str, required=True)\n    parser.add_argument('--model_name', '-m', type=str, required=True)\n    parser.add_argument('--output_file', '-o', type=str, required=True)\n    parser.add_argument('--k', '-k', type=int, help='number of examples', default=8, required=True)\n    parser.add_argument('--batch_size', '-b', type=int, default=8)\n    parser.add_argument('--generally', action='store_true')\n    args = parser.parse_args()\n    \n    y_pred = llm_boolqg(args.model_name, args.input_file, args.output_file, k_ex=args.k, batch_size=args.batch_size, generally=args.generally)"]}
{"filename": "boolqa/llm_answer_prediction.py", "chunked_list": ["import sys\nfrom accelerate import Accelerator\nimport cjjpy as cjj\n\nsys.path.append(cjj.AbsParentDir(__file__, '..'))\nfrom gpt3_helper import prompt_gpt3, calc_cost_w_prompt\n# from flant5_helper import prompt_flant5\nfrom utils import load_jsonl, chunks_list_first, answer2bool\nfrom llm_utils import (\n    save_llm_results, ", "from llm_utils import (\n    save_llm_results, \n    prepare_prompt,\n    TASK_KEY\n)\n\naccelerator = Accelerator()\n\n\ndef judge_ambi_w_prob(prompt, prompt_func, **kwargs):\n    # TODO: flan-t5 prob, but flan-t5 will only output yes/no.\n    results, money = prompt_func([prompt + ' yes', prompt + 'no'], echo=True, clean=False, logprobs=1, **kwargs)\n    results = list(results)\n    prob_yes = results[0]['logprobs']['token_logprobs'][-1]\n    prob_no = results[1]['logprobs']['token_logprobs'][-1]\n    return ' yes' if prob_yes > prob_no else ' no'", "\ndef judge_ambi_w_prob(prompt, prompt_func, **kwargs):\n    # TODO: flan-t5 prob, but flan-t5 will only output yes/no.\n    results, money = prompt_func([prompt + ' yes', prompt + 'no'], echo=True, clean=False, logprobs=1, **kwargs)\n    results = list(results)\n    prob_yes = results[0]['logprobs']['token_logprobs'][-1]\n    prob_no = results[1]['logprobs']['token_logprobs'][-1]\n    return ' yes' if prob_yes > prob_no else ' no'\n\n\ndef llm_boolqa(model_name, input_file, output_file=None, key_q='text-davinci-002_ex-8', k_pos_ex=2, k_neg_ex=2, batch_size=8, cot='none'):\n    data = load_jsonl(input_file)\n    prompts = []\n    for js in data:\n        prompt = prepare_prompt('qa', js, k_pos_ex, k_neg_ex, key_q, cot)\n        prompts.append(prompt)\n    print(prompts[0])\n    \n    num_cot_tokens = 40 if cot is not None else 0   # TODO: approximate number of cot tokens\n    if 'flan-t5' in model_name:\n        prompt_func = prompt_flant5\n    else:\n        prompt_func = prompt_gpt3\n    res, money = prompt_func(prompts, model_name=model_name, clean=True, temperature=0., \n                             max_tokens=5 + num_cot_tokens, verbose=True, batch_size=batch_size, n=1)\n    \n    y_pred = []\n    for i, (ny, indiv_prompt) in enumerate(zip(chunks_list_first(res, n=1), prompts)):\n        # handle n_returned sequences, where n = 1 in QA.\n        y = ny[0]\n        if answer2bool(y, 'Answer' if cot != 'none' else None) < 0 and 'flan-t5' not in model_name: # if y is undecidable, use probability\n            y = judge_ambi_w_prob(indiv_prompt, prompt_func, model_name=model_name, temperature=0., max_tokens=0, batch_size=batch_size, n=1)\n            money += calc_cost_w_prompt(indiv_prompt + y + indiv_prompt + y, model_name)\n        y_pred.append(y)\n        if i == 0: print(indiv_prompt + y)\n\n    if args.instruct_id == 0:\n        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n\"\n    else:\n        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n_i{args.instruct_id}\" # TODO: hard-coded\n    \n    if args.temperature == 0:\n        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n\"\n    else:\n        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n_t{args.temperature}\" # TODO: hard-coded\n    \n    input_type = 'keywords' if key_q == 'keywords' else 'question'\n    task_key = TASK_KEY['qa'][input_type]\n    if cot != 'none':\n        task_key += f'_{cot}'\n    \n    if accelerator.is_main_process:\n        # save into file, override previous model key.\n        save_llm_results(input_file, y_pred, task_key, model_key, output_file)\n\n        if 'bloom' not in model_name or 'flan-t5' not in model_name:\n            cjj.lark(f\"This run has cost you {round(money, 2)}$: {task_key}/{model_key}.\")\n    \n    return f\"{task_key}/{model_key}\"", "\n\ndef llm_boolqa(model_name, input_file, output_file=None, key_q='text-davinci-002_ex-8', k_pos_ex=2, k_neg_ex=2, batch_size=8, cot='none'):\n    data = load_jsonl(input_file)\n    prompts = []\n    for js in data:\n        prompt = prepare_prompt('qa', js, k_pos_ex, k_neg_ex, key_q, cot)\n        prompts.append(prompt)\n    print(prompts[0])\n    \n    num_cot_tokens = 40 if cot is not None else 0   # TODO: approximate number of cot tokens\n    if 'flan-t5' in model_name:\n        prompt_func = prompt_flant5\n    else:\n        prompt_func = prompt_gpt3\n    res, money = prompt_func(prompts, model_name=model_name, clean=True, temperature=0., \n                             max_tokens=5 + num_cot_tokens, verbose=True, batch_size=batch_size, n=1)\n    \n    y_pred = []\n    for i, (ny, indiv_prompt) in enumerate(zip(chunks_list_first(res, n=1), prompts)):\n        # handle n_returned sequences, where n = 1 in QA.\n        y = ny[0]\n        if answer2bool(y, 'Answer' if cot != 'none' else None) < 0 and 'flan-t5' not in model_name: # if y is undecidable, use probability\n            y = judge_ambi_w_prob(indiv_prompt, prompt_func, model_name=model_name, temperature=0., max_tokens=0, batch_size=batch_size, n=1)\n            money += calc_cost_w_prompt(indiv_prompt + y + indiv_prompt + y, model_name)\n        y_pred.append(y)\n        if i == 0: print(indiv_prompt + y)\n\n    if args.instruct_id == 0:\n        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n\"\n    else:\n        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n_i{args.instruct_id}\" # TODO: hard-coded\n    \n    if args.temperature == 0:\n        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n\"\n    else:\n        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n_t{args.temperature}\" # TODO: hard-coded\n    \n    input_type = 'keywords' if key_q == 'keywords' else 'question'\n    task_key = TASK_KEY['qa'][input_type]\n    if cot != 'none':\n        task_key += f'_{cot}'\n    \n    if accelerator.is_main_process:\n        # save into file, override previous model key.\n        save_llm_results(input_file, y_pred, task_key, model_key, output_file)\n\n        if 'bloom' not in model_name or 'flan-t5' not in model_name:\n            cjj.lark(f\"This run has cost you {round(money, 2)}$: {task_key}/{model_key}.\")\n    \n    return f\"{task_key}/{model_key}\"", "\n\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_file', '-i', type=str, required=True)\n    parser.add_argument('--model_name', '-m', type=str, required=True, \n                        choices=['flan-t5-large', 'flan-t5-xl', 'flan-t5-xxl',\n                                 'davinci', 'curie', 'babbage', 'ada', \n                                 'text-davinci-001', 'text-curie-001', 'text-babbage-001', 'text-ada-001',\n                                 'text-davinci-002', 'text-davinci-003', 'code-davinci-002'])\n    parser.add_argument('--posk', type=int, help='Number of positive examples in the demonstration.')\n    parser.add_argument('--negk', type=int, help='Number of negative examples in the demonstration.')\n    parser.add_argument('--output_file', '-o', type=str, required=True, \n                        help='Note: just for convenience, input and output file are the same.')\n    parser.add_argument('--input_key', '-q', default='text-davinci-002_ex-8', type=str, \n                        help='Key for input questions (in QA) in the jsonline file. InstructGPT-002 generated questions by default.')\n    parser.add_argument('--batch_size', '-b', type=int, default=16)\n    parser.add_argument('--temperature', type=float, default=0.0)\n    parser.add_argument('--cot', type=str, choices=['fact', 'logic', 'none'], default='none')\n    parser.add_argument('--instruct_id', type=int, default=0, \n                        help='For testing different instructions. Use the first instruction by default.')\n    args = parser.parse_args()\n    \n    y_pred = llm_boolqa(args.model_name, args.input_file, args.output_file, \n                        args.input_key, k_pos_ex=args.posk, k_neg_ex=args.negk, \n                        batch_size=args.batch_size, cot=args.cot)"]}
{"filename": "boolqa/tmpl_boolqg.py", "chunked_list": ["import sys\nimport argparse\n\nsys.path.append('..')\nfrom base_generator import Seq2SeqBaseGenerator\nfrom utils import *\n\n\ndef _triple2boolq(s, p, o, boolq_template):\n    boolq = boolq_template[p.lower()].replace('[w1]', s).replace('[w2]', o)\n    return boolq", "def _triple2boolq(s, p, o, boolq_template):\n    boolq = boolq_template[p.lower()].replace('[w1]', s).replace('[w2]', o)\n    return boolq\n\n\ndef _correct_grammar(inputs):\n    # requires `huggingface-cli login`\n    gf = Seq2SeqBaseGenerator('prithivida/grammar_error_correcter_v1', use_auth_token=True)\n    corrected_sentences = gf.generate(inputs, prefix='gec: ', beam_size=7, num_proc=1,\n                                      do_sample=True, max_length=128, early_stopping=True, \n                                      per_device_test_batch_size=32)\n    return [x[0] for x in corrected_sentences]", "\n\ndef generate_boolq_from_triples(input_file_or_data, output_file, boolq_template):\n    # boolq from template => correct grammar => add choices for unifiedqa\n    # output_file: input_file (jsonl) with boolq\n    data = load_jsonl(input_file_or_data)\n    \n    boolqs = []\n    for js in data:\n        boolq = _triple2boolq(js['subject'], js['predicate'], js['object'], boolq_template)\n        boolqs.append(boolq) # potentially boolq is None\n\n    boolqs = _correct_grammar([{'source': x} for x in boolqs])\n\n    if output_file is not None:\n        with open(output_file, 'w') as fo:\n            for js, q in zip(data, boolqs):\n                if q is not None:\n                    js['boolq'] = {'rule': q}\n                    fo.write(json.dumps(js) + '\\n')\n\n    return boolqs", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_file', '-i', type=str)\n    parser.add_argument('--output_file', '-o', type=str)\n    parser.add_argument('--generally', action='store_true')\n    args = parser.parse_args()\n\n    template = USUALLY_REL_TO_BOOLQ_TEMPLATE if args.generally else REL_TO_BOOLQ_TEMPLATE\n    uncased_template = {k.lower(): v for k, v in template.items()}\n    generate_boolq_from_triples(args.input_file, args.output_file, uncased_template)"]}
{"filename": "evaluation/eval_boolqa.py", "chunked_list": ["import argparse\nimport sys\nimport random\nfrom collections import Counter\nsys.path.append('..')\nimport cjjpy as cjj\nfrom utils import answer2bool, load_jsonl, calc_biclf_metrics\n\nrandom.seed(42)\n", "random.seed(42)\n\n\ndef eval_qa(file_or_data, model_name):\n    data = load_jsonl(file_or_data)\n    y_pred = []\n    y_true = []\n    i = 0\n    for x in data:\n        if '/' in model_name:\n            task_key = model_name.split('/')[0]\n            model_key = '/'.join(model_name.split('/')[1:])\n        else:\n            task_key = 'qa_pred'\n            model_key = model_name\n        if 'fact' in task_key or 'logic' in task_key:\n            eval_cot = True\n        else:\n            eval_cot = False\n        pred = x[task_key][model_key]\n        if isinstance(pred, str): # TODO: majority vote QA\n            pred = [pred]\n        \n        vote_pred = []\n        for p in pred:\n            bool_pred = answer2bool(p, prefix='Answer' if eval_cot else None)\n            vote_pred.append(bool_pred)\n        \n        voted_pred = Counter(vote_pred).most_common()[0][0]\n        y_pred.append(voted_pred)\n        if voted_pred < 0: \n            # bool_pred = random.choice([0, 1])\n            print(f\"[{i}] Undecided: {x}\")\n            i += 1\n\n        if x.get('label') is None:\n            _label = int('pos' in x['source_kb'])\n        else:\n            _label = x['label']\n        y_true.append(_label)\n    \n    y_pred_f, y_true_f = [], []\n    for y1, y2 in zip(y_pred, y_true):\n        if y1 >= 0:\n            y_pred_f.append(y1)\n            y_true_f.append(y2)\n    scores = calc_biclf_metrics(y_pred_f, y_true_f)\n    return scores, y_pred", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_file', '-i', type=str)\n    parser.add_argument('--qa_model', '-m', type=str, help='nested or not: `qa_pred_cot_fact/text-davinci-002_ex-8p8n` or `text-davinci-002_ex-8p8n`')\n    parser.add_argument('--log', action='store_true')\n    args = parser.parse_args()\n\n    scores, y_pred = eval_qa(args.input_file, args.qa_model)\n    print(scores)\n    cjj.lark(f\"QA - {args.qa_model} in {args.input_file}: {str(scores)}\")\n\n    if args.log:\n        with open('metrics.log', 'a') as f:\n            f.write(f\"{args.qa_model}\\t{scores}\\tNone\\n\")"]}
{"filename": "evaluation/cjjpy.py", "chunked_list": ["\ufeff# -*- coding: utf-8 -*-\n\n'''\n@Author : Jiangjie Chen\n@Time   : 2022/5/26 19:52\n@Contact: jjchen19@fudan.edu.cn\n'''\n\nimport re\nimport datetime", "import re\nimport datetime\nimport os\nimport subprocess\nimport urllib.request, urllib.parse\nimport argparse\nfrom tqdm import tqdm\nimport sqlite3\nimport requests\nimport socket", "import requests\nimport socket\nimport logging\nimport io\nimport traceback\n\ntry:\n    import ujson as json\nexcept:\n    import json", "except:\n    import json\n\nHADOOP_BIN = 'PATH=/usr/bin/:$PATH hdfs'\n\n\ndef LengthStats(filename, key4json=None):\n    len_list = []\n    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n    with open(filename) as f:", "    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n    with open(filename) as f:\n        for line in f:\n            if key4json not in ['none', None, 'None']:\n                len_list.append(len(json.loads(line)[key4json].split()))\n            else:\n                len_list.append(len(line.strip().split()))\n    stats = {\n        'Max': max(len_list),\n        'Min': min(len_list),", "        'Max': max(len_list),\n        'Min': min(len_list),\n        'Avg': round(sum(len_list) / len(len_list), 4),\n    }\n    len_list.sort()\n    for t in thresholds:\n        stats[f\"Top-{t}\"] = len_list[int(len(len_list) * t)]\n\n    for k in stats:\n        print(f\"- {k}: {stats[k]}\")", "    for k in stats:\n        print(f\"- {k}: {stats[k]}\")\n    return stats\n\n\nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n", "        self.__dict__ = self\n\n\ndef TraceBack(error_msg):\n    exc = traceback.format_exc()\n    msg = f'[Error]: {error_msg}.\\n[Traceback]: {exc}'\n    return msg\n\n\ndef Now():", "\ndef Now():\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef TorchHLoad(filepath: str, **kwargs):\n    import torch, tensorflow as tf\n    if not filepath.startswith(\"hdfs://\"):\n        return torch.load(filepath, **kwargs)\n    else:", "        return torch.load(filepath, **kwargs)\n    else:\n        with tf.io.gfile.GFile(filepath, 'rb') as reader:\n            return torch.load(io.BytesIO(reader.read()), **kwargs)\n\n\ndef TorchHSave(obj, filepath: str, **kwargs):\n    import torch, tensorflow as tf\n    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n        with tf.io.gfile.GFile(filepath, 'wb') as f:", "    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n        with tf.io.gfile.GFile(filepath, 'wb') as f:\n            buffer = io.BytesIO()\n            torch.save(obj, buffer, **kwargs)\n            f.write(buffer.getvalue())\n    else:\n        torch.save(obj, filepath, **kwargs)\n\n\ndef PutHDFS(local: str, remote: str):", "\ndef PutHDFS(local: str, remote: str):\n    import tensorflow as tf\n    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n    if not tf.io.gfile.exists(remote):\n        tf.io.gfile.makedirs(remote)\n    RunCmd(f'{HADOOP_BIN} dfs -put {local} {remote}')\n\n\ndef GetHDFS(remote: str, local: str):", "\ndef GetHDFS(remote: str, local: str):\n    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n    os.makedirs(local, exist_ok=True)\n    RunCmd(f'{HADOOP_BIN} dfs -get {remote} {local}')\n\n\ndef RunCmd(command):\n    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    res, err = pipe.communicate()", "    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    res, err = pipe.communicate()\n    res = res.decode('utf-8')\n    err = err.decode('utf-8')\n    return res, err\n\n\ndef AbsParentDir(file, parent='..', postfix=None):\n    ppath = os.path.abspath(file)\n    parent_level = parent.count('.')", "    ppath = os.path.abspath(file)\n    parent_level = parent.count('.')\n    while parent_level > 0:\n        ppath = os.path.dirname(ppath)\n        parent_level -= 1\n    if postfix is not None:\n        return os.path.join(ppath, postfix)\n    else:\n        return ppath\n", "        return ppath\n\n\ndef init_logger(log_file=None, log_file_level=logging.NOTSET, from_scratch=False):\n    from coloredlogs import ColoredFormatter\n    import tensorflow as tf\n\n    fmt = \"[%(asctime)s %(levelname)s] %(message)s\"\n    log_format = ColoredFormatter(fmt=fmt)\n    # log_format = logging.Formatter()", "    log_format = ColoredFormatter(fmt=fmt)\n    # log_format = logging.Formatter()\n    logger = logging.getLogger()\n    logger.setLevel(log_file_level)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_format)\n    logger.handlers = [console_handler]\n\n    if log_file and log_file != '':", "\n    if log_file and log_file != '':\n        if from_scratch and tf.io.gfile.exists(log_file):\n            logger.warning('Removing previous log file: %s' % log_file)\n            tf.io.gfile.remove(log_file)\n        path = os.path.dirname(log_file)\n        os.makedirs(path, exist_ok=True)\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(log_file_level)\n        file_handler.setFormatter(log_format)", "        file_handler.setLevel(log_file_level)\n        file_handler.setFormatter(log_format)\n        logger.addHandler(file_handler)\n\n    return logger\n\n\ndef is_qid(x, hard=False):\n    if type(x) is not str: return None\n    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)", "    if type(x) is not str: return None\n    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)\n    return None if len(ret) == 0 else ret[0]\n\n\ndef is_pid(x, hard=False):\n    if type(x) is not str: return None\n    ret = re.findall('^P\\d+$', x) if hard else re.findall('P\\d+', x)\n    return None if len(ret) == 0 else ret[0]\n", "    return None if len(ret) == 0 else ret[0]\n\n\nclass MiniLutDB:\n    def __init__(self, db, verbose=True):\n        self.db = db\n        self.conn = None\n        self.verbose = verbose\n\n    def dump_lut(self, lut_tuples, verbose=None):", "\n    def dump_lut(self, lut_tuples, verbose=None):\n        # lut_tuple: (k, v)+, iterable\n\n        if verbose is None: \n            verbose = self.verbose\n        self.conn = sqlite3.connect(self.db)\n        cur = self.conn.cursor()\n        cur.executescript('''\n        DROP TABLE IF EXISTS lut;", "        cur.executescript('''\n        DROP TABLE IF EXISTS lut;\n        CREATE TABLE lut (\n        id      TEXT PRIMARY KEY UNIQUE,\n        content TEXT)''')\n        self.conn.commit()\n\n        BLOCKSIZE = 100000\n        block = []\n        i = 0", "        block = []\n        i = 0\n        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n        for x in iter:\n            block.append(x)\n            i += 1\n            if i == BLOCKSIZE:\n                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n                block = []\n                i = 0", "                block = []\n                i = 0\n        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n        self.conn.commit()\n\n        self.close()\n\n    def update_lut(self, lut_tuples, verbose=None):\n        if verbose is None: \n            verbose = self.verbose", "        if verbose is None: \n            verbose = self.verbose\n\n        self.conn = sqlite3.connect(self.db)\n        BLOCKSIZE = 100000\n        block = []\n        i = 0\n        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n        for x in iter:\n            block.append(x)", "        for x in iter:\n            block.append(x)\n            i += 1\n            if i == BLOCKSIZE:\n                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n                block = []\n                i = 0\n        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n        self.conn.commit()\n", "        self.conn.commit()\n\n        self.close()\n\n    def create_index(self):\n        self.conn = sqlite3.connect(self.db)\n        self.cur = self.conn.cursor()\n        # sql = ('CREATE INDEX index_lut ON lut(id);')\n        # self.cur.execute(sql)\n        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')", "        # self.cur.execute(sql)\n        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')\n        self.conn.commit()\n\n    def get(self, x, default=None):\n        if x is None: return default\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db)\n            self.cur = self.conn.cursor()\n", "            self.cur = self.conn.cursor()\n\n        res = self.query_lut(self.cur, x, False)[0]\n        return res if res is not None else default\n\n    def get_chunk(self, xx):\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db)\n            self.cur = self.conn.cursor()\n        return self.query_lut(self.conn, xx, self.verbose)", "            self.cur = self.conn.cursor()\n        return self.query_lut(self.conn, xx, self.verbose)\n\n    def close(self):\n        if self.conn is not None:\n            self.conn.close()\n            self.conn = None\n    \n    def delete_sample(self, key, value=None):\n        if self.get(key) is None: return", "    def delete_sample(self, key, value=None):\n        if self.get(key) is None: return\n        self.conn = sqlite3.connect(self.db)\n        self.cur = self.conn.cursor()\n        self.cur.execute('DELETE FROM lut WHERE id = ?', (key,))\n        self.conn.commit()\n        assert self.get(key) is None, f'delete failed: {key}'\n\n    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n        values = []", "    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n        values = []\n        if isinstance(keys, str): keys = [keys]\n\n        iter = tqdm(keys, mininterval=0.5, disable=not verbose)\n        for k in iter:\n            cur.execute('SELECT content FROM lut WHERE id = ?', (k,))\n            val = cur.fetchone()\n            val = val[0] if val is not None else None\n            values.append(val)", "            val = val[0] if val is not None else None\n            values.append(val)\n        return values\n\n\ndef OverWriteCjjPy(root='.'):\n    # import difflib\n    # diff = difflib.HtmlDiff()\n    cnt = 0\n    golden_cjjpy = os.path.join(root, 'cjjpy.py')", "    cnt = 0\n    golden_cjjpy = os.path.join(root, 'cjjpy.py')\n    # golden_content = open(golden_cjjpy).readlines()\n    for dir, folder, file in os.walk(root):\n        for f in file:\n            if f == 'cjjpy.py':\n                cjjpy = '%s/%s' % (dir, f)\n                # content = open(cjjpy).readlines()\n                # d = diff.make_file(golden_content, content)\n                cnt += 1", "                # d = diff.make_file(golden_content, content)\n                cnt += 1\n                print('[%d]: %s' % (cnt, cjjpy))\n                os.system('cp %s %s' % (golden_cjjpy, cjjpy))\n\n\ndef ReplaceChar(file, replaced, replacer):\n    print(file, replaced, replacer)\n    with open(file) as f:\n        data = f.readlines()", "    with open(file) as f:\n        data = f.readlines()\n        out = open(file, 'w')\n        for line in data:\n            out.write(line.replace(replaced, replacer))\n\n\ndef DeUnicode(line):\n    return line.encode('utf-8').decode('unicode_escape')\n", "    return line.encode('utf-8').decode('unicode_escape')\n\n\ndef LoadIDDict(dict_file, unify_words=False, lower=False, reverse=False):\n    '''\n    a\\tb\\n, `.dict' file\n    '''\n    import tensorflow as tf\n    assert dict_file.endswith('.dict')\n    id2label = {}", "    assert dict_file.endswith('.dict')\n    id2label = {}\n    with tf.io.gfile.GFile(dict_file, 'r') as f:\n        data = f.read().split('\\n')\n        for i, line in enumerate(data):\n            if line == '': continue\n            try:\n                id, label = line.split('\\t')\n                if reverse:\n                    id, label = label, id", "                if reverse:\n                    id, label = label, id\n                _val = '_'.join(label.split()) if unify_words else label\n                id2label[id] = _val.lower() if lower else _val\n            except:\n                pass\n    return id2label\n\n\ndef LoadWords(file, is_file=True):", "\ndef LoadWords(file, is_file=True):\n    import tensorflow as tf\n    if is_file:\n        with tf.io.gfile.GFile(file, 'r') as f:\n            data = f.read().splitlines()\n    else:\n        data = file.splitlines()\n    return set(map(lambda x: x.strip(), data))\n", "    return set(map(lambda x: x.strip(), data))\n\n\ndef ChangeFileFormat(filename, new_fmt):\n    assert type(filename) is str and type(new_fmt) is str\n    spt = filename.split('.')\n    if len(spt) == 0:\n        return filename\n    else:\n        return filename.replace('.' + spt[-1], new_fmt)", "    else:\n        return filename.replace('.' + spt[-1], new_fmt)\n\n\ndef CountLines(fname):\n    with open(fname, 'rb') as f:\n        count = 0\n        last_data = '\\n'\n        while True:\n            data = f.read(0x400000)", "        while True:\n            data = f.read(0x400000)\n            if not data:\n                break\n            count += data.count(b'\\n')\n            last_data = data\n        if last_data[-1:] != b'\\n':\n            count += 1  # Remove this if a wc-like count is needed\n    return count\n", "    return count\n\n\ndef SearchByKey(file, key):\n    with open(file, 'r') as fin:\n        while True:\n            line = fin.readline()\n            if not line: break\n            if key in line:\n                print(line, end='')", "            if key in line:\n                print(line, end='')\n\n\ndef SendEmail(subject, content, receivers=['MichaelChen0110@163.com']):\n    from email.mime.text import MIMEText\n    import smtplib\n\n    # receivers got to be list.\n    mail_receivers = receivers", "    # receivers got to be list.\n    mail_receivers = receivers\n    # mail_host = \"smtp.163.com\n    mail_host = \"220.181.12.18\"\n    mail_user = \"MichaelChen0110@163.com\"\n    mail_pass = \"\"\n    me = socket.gethostname() + \"<\" + mail_user + \">\"\n    msg = MIMEText(content, _subtype='plain', _charset='utf-8')\n    msg['Subject'] = subject\n    msg['From'] = me", "    msg['Subject'] = subject\n    msg['From'] = me\n    msg['To'] = \";\".join(mail_receivers)\n    try:\n        server = smtplib.SMTP()\n        server.connect(mail_host)\n        server.login(mail_user, mail_pass)\n        server.sendmail(me, mail_receivers, msg.as_string())\n        server.close()\n        print('Have sent the email to ' + str(mail_receivers) + '. ')", "        server.close()\n        print('Have sent the email to ' + str(mail_receivers) + '. ')\n        return True\n    except Exception as e:\n        print(str(e))\n        return False\n\n\ndef SortDict(_dict, reverse=True):\n    assert type(_dict) is dict", "def SortDict(_dict, reverse=True):\n    assert type(_dict) is dict\n    return sorted(_dict.items(), key=lambda d: d[1], reverse=reverse)\n\n\ndef MaxCommLen(str1, str2):\n    lstr1 = len(str1)\n    lstr2 = len(str2)\n    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n    max_num = 0", "    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n    max_num = 0\n    for i in range(lstr1):\n        for j in range(lstr2):\n            if str1[i] == str2[j]:\n                record[i + 1][j + 1] = record[i][j] + 1\n                if record[i + 1][j + 1] > max_num:\n                    max_num = record[i + 1][j + 1]\n    return max_num, ''\n", "    return max_num, ''\n\n\ndef lark(content='test'):\n    print(content)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n", "    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--diff', nargs=2,\n                        help='show difference between two files, shown in downloads/diff.html')\n    parser.add_argument('--de_unicode', action='store_true', default=False,\n                        help='remove unicode characters')\n    parser.add_argument('--link_entity', action='store_true', default=False,\n                        help='')\n    parser.add_argument('--max_comm_len', action='store_true', default=False,\n                        help='')", "    parser.add_argument('--max_comm_len', action='store_true', default=False,\n                        help='')\n    parser.add_argument('--search', nargs=2,\n                        help='search key from file, 2 args: file name & key')\n    parser.add_argument('--email', nargs=2,\n                        help='sending emails, 2 args: subject & content')\n    parser.add_argument('--overwrite', action='store_true', default=None,\n                        help='overwrite all cjjpy under given *dir* based on *dir*/cjjpy.py')\n    parser.add_argument('--replace', nargs=3,\n                        help='replace char, 3 args: file name & replaced char & replacer char')", "    parser.add_argument('--replace', nargs=3,\n                        help='replace char, 3 args: file name & replaced char & replacer char')\n    parser.add_argument('--lark', nargs=1)\n    parser.add_argument('--get_hdfs', nargs=2,\n                        help='easy copy from hdfs to local fs, 2 args: remote_file/dir & local_dir')\n    parser.add_argument('--put_hdfs', nargs=2,\n                        help='easy put from local fs to hdfs, 2 args: local_file/dir & remote_dir')\n    parser.add_argument('--length_stats', nargs=2,\n                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n", "                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n\n    args = parser.parse_args()\n\n    if args.overwrite:\n        print('* Overwriting cjjpy...')\n        OverWriteCjjPy()\n\n    if args.replace:\n        print('* Replacing Char...')", "    if args.replace:\n        print('* Replacing Char...')\n        ReplaceChar(args.replace[0], args.replace[1], args.replace[2])\n\n    if args.search:\n        file = args.search[0]\n        key = args.search[1]\n        print('* Searching %s from %s...' % (key, file))\n        SearchByKey(file, key)\n", "        SearchByKey(file, key)\n\n    if args.email:\n        try:\n            subj = args.email[0]\n            cont = args.email[1]\n        except:\n            subj = 'running complete'\n            cont = ''\n        print('* Sending email {%s, %s} to host...' % (subj, cont))", "            cont = ''\n        print('* Sending email {%s, %s} to host...' % (subj, cont))\n        SendEmail(subj, cont)\n\n    if args.lark:\n        try:\n            content = args.lark[0]\n        except:\n            content = 'running complete'\n        print(f'* Larking \"{content}\"...')", "            content = 'running complete'\n        print(f'* Larking \"{content}\"...')\n        lark(content)\n\n    if args.get_hdfs:\n        remote = args.get_hdfs[0]\n        local = args.get_hdfs[1]\n        print(f'* Copying {remote} to {local}...')\n        GetHDFS(remote, local)\n", "        GetHDFS(remote, local)\n\n    if args.put_hdfs:\n        local = args.put_hdfs[0]\n        remote = args.put_hdfs[1]\n        print(f'* Copying {local} to {remote}...')\n        PutHDFS(local, remote)\n\n    if args.length_stats:\n        file = args.length_stats[0]", "    if args.length_stats:\n        file = args.length_stats[0]\n        key4json = args.length_stats[1]\n        print(f'* Working on {file} lengths statistics...')\n        LengthStats(file, key4json)\n"]}
{"filename": "evaluation/eval_constrained_generation.py", "chunked_list": ["import sys\nimport re\nimport argparse \nfrom tqdm import tqdm\nimport torch\nfrom collections import defaultdict, Counter\nfrom transformers import pipeline\nsys.path.append('..')\n\nimport cjjpy as cjj", "\nimport cjjpy as cjj\nfrom utils import *\n\n\nnegation_cue = {'nothing', \"no\", \"not\", \"never\", \"none\", \"hardly\", \"rarely\", \"scarcely\", \"seldom\", 'barely',\n                \"nor\", \"neither\", \"nothing\", \"nowhere\", \"without\", \"lack\", \"cant\", \"dont\", \"doesnt\",\n                \"doesn't\", \"don't\", \"isn't\", \"wasn't\", \"aren't\", \"weren't\", \"haven't\", \"hasn't\", \n                \"shouldn't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\", \"cannot\", \"unable\"}\n", "                \"shouldn't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\", \"cannot\", \"unable\"}\n\n\ndef majority_vote(pred_list):\n    return [Counter(x).most_common()[0][0] for x in pred_list]\n\n\ndef fetch_target_sent(x, prefix):\n    # prefix = None: ignore prefix, use the first sentence\n    # prefix = xxxx: find the first of that pattern\n    if prefix is not None:\n        for subx in x.strip().replace('###', '\\n').split('\\n'):\n            # find the sentence after prefix\n            sent = re.findall(f'{prefix}(.*)', subx)\n            if sent != []:\n                return sent[0].strip()\n    return x.strip().split('\\n')[0].strip()", "\n\ndef detect_negation(pipe, texts: list, prefix=None, only_rule=False):\n    # True for neg, False for pos\n    def _has_neg(x):\n        for j in x:\n            if j['entity'] == 'Y':\n                return True\n        return False\n    \n    labels = []\n    for t in texts:\n        rule_hit_flag = False\n        t1 = fetch_target_sent(t, prefix)\n        \n        if only_rule:\n            label = False\n            for w in t1.split():\n                if w.lower() in negation_cue:\n                    label = True\n                    break\n            labels.append(label)\n        else:\n            # filtering with rules first\n            for w in t1.split():\n                if w.lower() in negation_cue:\n                    labels.append(True)\n                    rule_hit_flag = True\n                    break\n            \n            if not rule_hit_flag:\n                labels.append(_has_neg(pipe(t1)))\n    \n    return labels", "\n\ndef eval_negation(model_name, filename, pipe):\n    data = load_jsonl(filename)\n    y_pred = []\n    y_true = []\n    y_pred_by_rel = defaultdict(list)\n    y_true_by_rel = defaultdict(list)\n\n    if '/' in model_name:\n        task_key = model_name.split('/')[0]\n        model_key = '/'.join(model_name.split('/')[1:])\n    else:\n        task_key = 'cg_pred'\n        model_key = model_name\n    \n    for js in tqdm(data, desc='Negation Detection'):\n        if js.get('label') is None:\n            _label = int('pos' in js['source_kb'])\n        else:\n            _label = js['label']\n        y_true.append(_label)\n        y_true_by_rel[js['predicate']].append(_label)\n        \n        prefix = 'Therefore:' if 'logic' in task_key or 'fact' in task_key else None\n        _pred = detect_negation(pipe, js[task_key][model_key], prefix=prefix)\n        _pred = [int(not x) for x in _pred]\n        \n        y_pred.append(_pred)\n        y_pred_by_rel[js['predicate']].append(_pred)\n    \n    y_score = majority_vote(y_pred)\n    scores = calc_biclf_metrics(y_score, y_true)\n    \n    return scores, y_score", "\n\ndef load_negation_detector():\n    pipe = pipeline('token-classification', model='spoiled/roberta-large-condaqa-neg-tag-token-classifier', device=0 if torch.cuda.is_available() else -1)\n    return pipe\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_file', '-i')\n    parser.add_argument('--model_name', '-m')\n    parser.add_argument('--log', action='store_true')\n    args = parser.parse_args()\n\n    pipe = load_negation_detector()\n    scores, neg_pred = eval_negation(args.model_name, args.input_file, pipe)\n    print(scores)\n    cjj.lark(f\"CG - {args.model_name} in {args.input_file}: {str(scores)}\")\n    \n    if args.log:\n        with open('metrics.log', 'a') as f:\n            f.write(f\"{args.model_name}\\t{scores}\\tNone\\n\")"]}
{"filename": "evaluation/agreement.py", "chunked_list": ["import sys\nimport argparse\n\nsys.path.append('..')\ntry:\n    from ..utils import *\n    from .eval_boolqa import eval_qa\n    from .eval_constrained_generation import eval_negation, load_negation_detector\nexcept:\n    from utils import *\n    from eval_boolqa import eval_qa\n    from eval_constrained_generation import eval_negation, load_negation_detector", "\n\ndef agreement(y_qa, y_cg, y_gt):\n    cnt = 0\n    agree = 0\n    cnt_pos = 0\n    agree_pos = 0\n    cnt_neg = 0\n    agree_neg = 0\n    for y1, y2, yt in zip(y_qa, y_cg, y_gt):\n        if y1 < 0:\n            continue\n        if yt == 1:\n            cnt_pos += 1\n        else:\n            cnt_neg += 1\n        if y1 == y2:\n            agree += 1\n            if yt == 1: # pos\n                agree_pos += 1\n            else:\n                agree_neg += 1\n        cnt += 1\n    return agree / cnt, agree_pos / cnt_pos, agree_neg / cnt_neg", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_file', '-i', type=str)\n    parser.add_argument('--cg_model_name', '-g', type=str)\n    parser.add_argument('--qa_model_name', '-q', type=str)\n    parser.add_argument('--log', action='store_true')\n    args = parser.parse_args()\n\n    qa_score, y_qa = eval_qa(args.input_file, args.qa_model_name)\n    print(qa_score)\n    pipe = load_negation_detector()\n    neg_scores, y_cg = eval_negation(args.cg_model_name, args.input_file, pipe)\n    data = load_jsonl(args.input_file)\n    y_gt = [int('pos' in x['source_kb']) for x in data]\n    print(neg_scores)\n    agree, agree_pos, agree_neg = agreement(y_qa, y_cg, y_gt)\n    print(agree, agree_pos, agree_neg)\n    cjj.lark(f\"Agreement between [{args.cg_model_name}] and [{args.qa_model_name}]:\\nALL: {agree}, POS: {agree_pos}, NEG: {agree_neg}\\nCG score:\\n{neg_scores}\\nQA score:\\n{qa_score}\")\n    \n    if args.log:\n        with open('metrics.log', 'a') as f:\n            f.write(f\"{args.cg_model_name}\\t{neg_scores}\\tALL: {agree}, POS: {agree_pos}, NEG: {agree_neg}\\n\")"]}
{"filename": "preprocessing/conceptnet_helper.py", "chunked_list": ["# From https://github.com/vered1986/self_talk\nimport os\nimport csv\nimport gzip\nimport json\nimport tqdm\nimport math\nimport logging\nimport itertools\n", "import itertools\n\nimport numpy as np\n\nfrom operator import mul\nfrom functools import reduce\nfrom scipy.sparse import coo_matrix, dok_matrix\nfrom collections import defaultdict, namedtuple\n\n", "\n\nlogging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.INFO)\nlogger = logging.getLogger(__name__)\n\nResource = namedtuple('Resource',\n                      'index2concept, concept2index, index2relation, relation2index, edges, cooc_mat')\n", "                      'index2concept, concept2index, index2relation, relation2index, edges, cooc_mat')\n\n# we'll use infinity as a default distance to nodes.\nEdge = namedtuple('Edge', 'start, end, rel, cost')\n\n# Based on \"Commonsense Knowledge Mining from Pretrained Models\".\n# Joshua Feldman, Joe Davison, and Alexander M. Rush. 2019.\nREL_TO_TEMPLATE = {\n    \"RelatedTo\": \"[w1] is like [w2]\",\n    \"ExternalUrl\": \"[w1] is described at the following URL [w2]\",", "    \"RelatedTo\": \"[w1] is like [w2]\",\n    \"ExternalUrl\": \"[w1] is described at the following URL [w2]\",\n    \"FormOf\": \"[w1] is a form of the word [w2]\",\n    \"IsA\": \"[w1] is a type of [w2]\",\n    \"NotIsA\": \"[w1] is not [w2]\",\n    \"PartOf\": \"[w1] is part of [w2]\",\n    \"UsedFor\": \"[w1] is used for [w2]\",\n    \"CapableOf\": \"[w1] can [w2]\",\n    \"AtLocation\": \"You are likely to find [w1] in [w2]\",\n    \"Causes\": \"Sometimes [w1] causes [w2]\",", "    \"AtLocation\": \"You are likely to find [w1] in [w2]\",\n    \"Causes\": \"Sometimes [w1] causes [w2]\",\n    \"HasA\": \"[w1] has [w2]\",\n    \"HasSubevent\": \"Something you do when you [w1] is [w2]\",\n    \"HasFirstSubevent\": \"the first thing you do when you [w1] is [w2]\",\n    \"HasLastSubevent\": \"the last thing you do when you [w1] is [w2]\",\n    \"HasPrerequisite\": \"In order for [w1] to happen, [w2] needs to happen\",\n    \"HasProperty\": \"[w1] is [w2]\",\n    \"HasContext\": \"[w1] is a word used in the context of [w2]\",\n    \"MotivatedByGoal\": \"You would [w1] because you want to [w2]\",", "    \"HasContext\": \"[w1] is a word used in the context of [w2]\",\n    \"MotivatedByGoal\": \"You would [w1] because you want to [w2]\",\n    \"ObstructedBy\": \"[w1] can be prevented by [w2]\",\n    \"Desires\": \"[w1] wants [w2]\",\n    \"CreatedBy\": \"[w1] is created by [w2]\",\n    \"Synonym\": \"[w1] and [w2] have similar meanings\",\n    \"Antonym\": \"[w1] is the opposite of [w2]\",\n    \"DistinctFrom\": \"it cannot be both [w1] and [w2]\",\n    \"DerivedFrom\": \"the word [w1] is derived from the word [w2]\",\n    \"DefinedAs\": \"[w1] is defined as [w2]\",", "    \"DerivedFrom\": \"the word [w1] is derived from the word [w2]\",\n    \"DefinedAs\": \"[w1] is defined as [w2]\",\n    \"Entails\": \"if [w1] is happening, [w2] is also happening\",\n    \"MannerOf\": \"[w1] is a specific way of doing [w2]\",\n    \"LocatedNear\": \"[w1] is located near [w2]\",\n    \"dbpedia\": \"[w1] is conceptually related to [w2]\",\n    \"SimilarTo\": \"[w1] is similar to [w2]\",\n    \"EtymologicallyRelatedTo\": \"the word [w1] and the word [w2] have the same origin\",\n    \"EtymologicallyDerivedFrom\": \"the word [w1] comes from the word [w2]\",\n    \"CausesDesire\": \"[w1] makes people want [w2]\",", "    \"EtymologicallyDerivedFrom\": \"the word [w1] comes from the word [w2]\",\n    \"CausesDesire\": \"[w1] makes people want [w2]\",\n    \"MadeOf\": \"[w1] is made of [w2]\",\n    \"ReceivesAction\": \"[w1] can be [w2]\",\n    \"InstanceOf\": \"[w1] is an example of [w2]\",\n    \"NotDesires\": \"[w1] does not want [w2]\",\n    \"NotUsedFor\": \"[w1] is not used for [w2]\",\n    \"NotCapableOf\": \"[w1] is not capable of [w2]\",\n    \"NotHasProperty\": \"[w1] does not have the property of [w2]\",\n    \"NotMadeOf\": \"[w1] is not made of [w2]\"", "    \"NotHasProperty\": \"[w1] does not have the property of [w2]\",\n    \"NotMadeOf\": \"[w1] is not made of [w2]\"\n}\nLOWER_REL_TO_TEMPLATE = {k.lower(): v for k, v in REL_TO_TEMPLATE.items()}\n\ndef build_conceptnet(conceptnet_dir):\n    \"\"\"\n    Download ConceptNet and build it locally.\n    First run:\n    !wget https://s3.amazonaws.com/conceptnet/downloads/2019/edges/conceptnet-assertions-5.7.0.csv.gz \\\n        -O ~/resources/conceptnet-assertions-5.7.0.csv.gz\n    \"\"\"\n    # resource_dir = conceptnet_dir.replace(\"conceptnet\", \"\")\n    concept2index = defaultdict(itertools.count(0).__next__)\n    relation2index = defaultdict(itertools.count(0).__next__)\n\n    # concept -> concept -> relation = weight\n    edges = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n\n    with gzip.open(os.path.join(conceptnet_dir, 'conceptnet-assertions-5.7.0.csv.gz'), mode='rt') as f_in:\n        csvfile = csv.reader(f_in, delimiter='\\t', quotechar=\"'\")\n        for row in tqdm.tqdm(csvfile):\n            \"\"\"\n             Row format:\n                The URI of the whole edge\n                The relation expressed by the edge\n                The node at the start of the edge\n                The node at the end of the edge\n                A JSON structure of additional information about the edge, such as its weight\n            Example:\n                b'/a/[/r/Antonym/,/c/en/arise/,/c/en/repose/]\n                /r/Antonym\n                /c/en/arise\n                /c/en/repose\n                {\"dataset\": \"/d/verbosity\", \"license\": \"cc:by/4.0\", \n                \"sources\": [{\"contributor\": \"/s/resource/verbosity\"}], \n                \"surfaceEnd\": \"repose\", \"surfaceStart\": \"arise\", \n                \"surfaceText\": \"[[arise]] is the opposite of [[repose]]\", \"weight\": 0.15}\n            \"\"\"\n            relation, start, end = row[1:4]\n\n            # Keep only English concepts\n            if not start.startswith('/c/en') or not end.startswith('/c/en'):\n                continue\n\n            relation_label = os.path.basename(relation).lower()\n            edge_info = json.loads(row[-1])\n            start_label = edge_info.get('surfaceStart', '').lower().strip()\n            end_label = edge_info.get('surfaceEnd', '').lower().strip()\n\n            if len(start_label) > 0 and len(end_label) > 0:\n                weight = edge_info['weight']\n                start_index, end_index = concept2index[start_label], concept2index[end_label]\n                relation_index = relation2index[relation_label]\n                edges[start_index][end_index][relation_index] = weight\n\n        index2relation = {index: relation for relation, index in relation2index.items()}\n        with open(os.path.join(conceptnet_dir, 'relations.txt'), 'w', encoding='utf-8') as f_out:\n            for index in range(len(index2relation)):\n                f_out.write(index2relation[index] + '\\n')\n\n        index2concept = {index: concept for concept, index in concept2index.items()}\n        with open(os.path.join(conceptnet_dir, 'concepts.txt'), 'w', encoding='utf-8') as f_out:\n            for index in range(len(index2concept)):\n                f_out.write(index2concept[index] + '\\n')\n\n        with open(os.path.join(conceptnet_dir, 'edges.txt'), 'w', encoding='utf-8') as f_out:\n            for start_index in range(len(index2concept)):\n                f_out.write(json.dumps(edges[start_index]) + '\\n')\n\n        row_ind, col_ind, cooc_data = zip(*[(c1, c2, 1)\n                                            for c1, c1_relations in edges.items()\n                                            for c2 in c1_relations.keys()])\n\n        cooc_mat = coo_matrix((cooc_data, (row_ind, col_ind)),\n                              shape=(len(concept2index), len(concept2index)))\n\n        np.savez_compressed(os.path.join(conceptnet_dir, 'cooc.npz'),\n                            data=cooc_mat.data,\n                            row=cooc_mat.row,\n                            col=cooc_mat.col,\n                            shape=cooc_mat.shape)", "\n\ndef load_conceptnet(conceptnet_dir):\n    \"\"\"\n    Load an existing local ConceptNet from this directory\n    \"\"\"\n    with open(os.path.join(conceptnet_dir, 'concepts.txt'), 'r', encoding='utf-8') as f_in:\n        index2concept = [line.strip() for line in f_in]\n        concept2index = {c: i for i, c in enumerate(index2concept)}\n\n    with open(os.path.join(conceptnet_dir, 'relations.txt'), 'r', encoding='utf-8') as f_in:\n        index2relation = [line.strip() for line in f_in]\n        relation2index = {c: i for i, c in enumerate(index2relation)}\n\n    # concept -> concept -> relation = weight\n    edges = {}\n\n    with open(os.path.join(conceptnet_dir, 'edges.txt'), 'r', encoding='utf-8') as f_in:\n        for c1, line in enumerate(f_in):\n            edges[c1] = json.loads(line.strip())\n\n    edges = {int(c1): {\n        int(c2): {int(r): float(score) for r, score in relations.items()}\n        for c2, relations in c1_rs.items()}\n        for c1, c1_rs in edges.items()}\n\n    with np.load(os.path.join(conceptnet_dir, 'cooc.npz')) as loader:\n        cooc_mat = coo_matrix((loader['data'], (loader['row'], loader['col'])), shape=loader['shape'])\n\n    return Resource(*(index2concept, concept2index, index2relation,\n                      relation2index, edges, cooc_mat))", "\n\nclass NodesOnPathFinder:\n    \"\"\"\n    Applies bi-directional search to find the nodes in the shortest paths a pair of terms.\n    \"\"\"\n\n    def __init__(self, resource, include_reverse=True):\n        \"\"\"\n        Init the relevant nodes search\n        \"\"\"\n        self.adjacency_matrix = resource.cooc_mat\n        self.transposed_adjacency_matrix = resource.cooc_mat.T\n\n        # Include reversed relations\n        if include_reverse:\n            self.adjacency_matrix += self.transposed_adjacency_matrix\n            self.transposed_adjacency_matrix = self.adjacency_matrix\n\n    def find_nodes_on_path(self, x, y, max_length=5):\n        \"\"\"\n        Finds all nodes in the shortest paths between x and y\n        subject to the maximum length.\n        :param x -- the index of the first term\n        :param y -- the index of the second term\n        :param max_length -- the maximum path length\n        \"\"\"\n        m = self.adjacency_matrix\n        mT = self.transposed_adjacency_matrix\n\n        dim = m.shape[0]\n        n_r = create_one_hot_vector(x, dim)\n        n_g = create_one_hot_vector(y, dim)\n\n        return find_nodes(m, mT, n_r, n_g, max_length)", "\n\ndef find_nodes(m, mT, n_r, n_g, max_len):\n    \"\"\"\n    Finds all nodes in the shortest paths between x and y\n    subject to the maximum length.\n    :param m -- the adjacency matrix\n    :param mT -- the transposed adjacency matrix\n    :param n_r -- the one-hot vector representing the root node\n    :param n_g -- the one-hot vector representing the goal node\n    :param max_len -- the maximum path length\n    \"\"\"\n    nodes = set()\n    n_x = n_r\n    n_y = n_g\n\n    # Stop condition 1 - no paths\n    if max_len == 0:\n        return nodes\n\n    # Stop condition 2 - the two sides are connected by one edge.\n    # Notice that if max_length == 1, then this function will return the two\n    # nodes even if they are not connected - this path will be discarded\n    # in the second search phase.\n    if max_len == 1:\n        return set(n_r.nonzero()[1].flatten()).union(set(n_g.nonzero()[1].flatten()))\n\n    # Move one step in each direction until the root and goal meet\n    for l in range(max_len + 1):\n\n        # The root and goal met - apply recursively for each half of the path\n        if n_r.dot(n_g.T)[0, 0] > 0:\n            intersection = n_r.multiply(n_g)\n            forward = find_nodes(\n                m, mT, n_x, intersection, int(math.ceil((l + 1) / 2.0)))\n            backward = find_nodes(\n                m, mT, intersection, n_y, int(math.floor((l + 1) / 2.0)))\n            return forward.union(backward)\n\n        # Make a step forward\n        if l % 2 == 0:\n            n_r = n_r.dot(m)\n        # Make a step backward\n        else:\n            n_g = n_g.dot(mT)\n\n    return nodes", "\n\ndef create_one_hot_vector(x, dim):\n    \"\"\"\n    Creates the one-hot vector representing this node\n    :param x -- the node\n    :param dim -- the number of nodes (the adjacency matrix dimension)\n    \"\"\"\n    n_x = dok_matrix((1, dim), dtype=np.int16)\n    n_x[0, x] = 1\n    n_x = n_x.tocsr()\n    return n_x", "\n\nclass Graph:\n    def __init__(self, edges):\n        self.edges = [Edge(*edge) for edge in edges]\n\n    @property\n    def nodes(self):\n        return set(sum(([edge.start, edge.end] for edge in self.edges), []))\n\n    @property\n    def neighbours(self):\n        neighbours = {node: set() for node in self.nodes}\n        for edge in self.edges:\n            neighbours[edge.start].add((edge.end, edge.rel, edge.cost))\n\n        return neighbours\n\n    def bfs(self, start, goal):\n        \"\"\"\n        Get the shortest path from source to dest\n        \"\"\"\n        queue = [(start, [''], [start], [1.0])]\n        min_len_path = np.inf\n        paths = list()\n\n        while queue:\n            curr_node, edges_on_path, nodes_on_path, weights_on_path = queue.pop(0)\n            for next_node, rel, weight in self.neighbours.get(curr_node, set()):\n                if next_node in set(nodes_on_path):\n                    continue\n\n                if next_node == goal:\n                    curr_path = list(zip(edges_on_path, nodes_on_path, weights_on_path)) + [\n                        (rel, next_node, weight)]\n\n                    if len(curr_path) <= min_len_path:\n                        min_len_path = len(curr_path)\n                        path_weight = reduce(mul, weights_on_path, 1)\n                        paths.append((curr_path, path_weight))\n\n                    # Already found shorter paths\n                    else:\n                        return paths\n\n                else:\n                    queue.append((next_node,\n                                  edges_on_path + [rel],\n                                  nodes_on_path + [next_node],\n                                  weights_on_path + [weight]))\n\n        return paths", "\n\ndef shortest_paths(resource, c1, c2, max_length=10, exclude_relations=None):\n    \"\"\"\n    Return the shortest paths from c1 to c2, up to max_length edges,\n    optionally excluding some relations.\n    \"\"\"\n    nodes_finder = NodesOnPathFinder(resource, include_reverse=True)\n    c1_index = resource.concept2index.get(c1, None)\n    c2_index = resource.concept2index.get(c2, None)\n\n    if c1_index is None or c2_index is None:\n        logger.warning('{} not found'.format(c1 if c1_index is None else c2))\n        return [([], 0)]\n\n    # Find the nodes on the path\n    nodes = nodes_finder.find_nodes_on_path(c1_index, c2_index, max_length=max_length)\n\n    # Get all the edges between these nodes in the original graph\n    # Get the maximum weight for each start and end\n    curr_edges = {resource.index2concept[start]: {} for start in nodes}\n\n    for start, end in itertools.permutations(nodes, 2):\n        start_label = resource.index2concept[start]\n        end_label = resource.index2concept[end]\n        for relation, weight in resource.edges.get(start, {}).get(end, {}).items():\n            relation_label = resource.index2relation[relation]\n            if exclude_relations is None or relation_label not in exclude_relations:\n                if end_label not in curr_edges[start_label] or \\\n                        curr_edges[start_label][end_label][1] < weight:\n                    curr_edges[start_label][end_label] = (relation_label, weight)\n                if start_label not in curr_edges[end_label] or \\\n                        curr_edges[end_label][start_label][1] < weight:\n                    curr_edges[end_label][start_label] = (relation_label + '-1', weight)\n\n    # Create the subgraph and use Dijkstra to find the shortest weighted path\n    edge_list = [(start, end, rel, 1.0 / weight)\n                 for start, start_rels in curr_edges.items()\n                 for end, (rel, weight) in start_rels.items()]\n    graph = Graph(edge_list)\n    result = graph.bfs(c1, c2)\n    return result", "\n\ndef pretty_print(path):\n    \"\"\"\n    Print a path in a readable format\n    param path: a list of (edge_label, node)\n    \"\"\"\n    path_str = ''\n    if len(path) > 0:\n        path_str += path[0][1]\n\n    for rel, node, _ in path[1:]:\n        if rel.endswith('-1'):\n            path_str += f' <--{rel[:-2]}-- {node}'\n        else:\n            path_str += f' --{rel}--> {node}'\n\n    return path_str", "\n\ndef to_natural_language(path):\n    \"\"\"\n    Print a path in a readable format\n    param path: a list of (edge_label, node)\n    \"\"\"\n    props = []\n    for (_, node1, _), (rel, node2, _) in zip(path, path[1:]):\n        w1, w2, rel = (node2, node1, rel.replace(\"-1\", \"\")) if rel.endswith('-1') else (node1, node2, rel)\n        props.append(LOWER_REL_TO_TEMPLATE[rel].replace(\"[w1]\", w1).replace(\"[w2]\", w2))\n\n    return \". \".join([p[0].upper() + p[1:] for p in props])"]}
{"filename": "preprocessing/cjjpy.py", "chunked_list": ["\ufeff# -*- coding: utf-8 -*-\n\n'''\n@Author : Jiangjie Chen\n@Time   : 2022/5/26 19:52\n@Contact: jjchen19@fudan.edu.cn\n'''\n\nimport re\nimport datetime", "import re\nimport datetime\nimport os\nimport subprocess\nimport urllib.request, urllib.parse\nimport argparse\nfrom tqdm import tqdm\nimport sqlite3\nimport requests\nimport socket", "import requests\nimport socket\nimport logging\nimport io\nimport traceback\n\ntry:\n    import ujson as json\nexcept:\n    import json", "except:\n    import json\n\nHADOOP_BIN = 'PATH=/usr/bin/:$PATH hdfs'\n\n\ndef LengthStats(filename, key4json=None):\n    len_list = []\n    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n    with open(filename) as f:", "    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n    with open(filename) as f:\n        for line in f:\n            if key4json not in ['none', None, 'None']:\n                len_list.append(len(json.loads(line)[key4json].split()))\n            else:\n                len_list.append(len(line.strip().split()))\n    stats = {\n        'Max': max(len_list),\n        'Min': min(len_list),", "        'Max': max(len_list),\n        'Min': min(len_list),\n        'Avg': round(sum(len_list) / len(len_list), 4),\n    }\n    len_list.sort()\n    for t in thresholds:\n        stats[f\"Top-{t}\"] = len_list[int(len(len_list) * t)]\n\n    for k in stats:\n        print(f\"- {k}: {stats[k]}\")", "    for k in stats:\n        print(f\"- {k}: {stats[k]}\")\n    return stats\n\n\nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n", "        self.__dict__ = self\n\n\ndef TraceBack(error_msg):\n    exc = traceback.format_exc()\n    msg = f'[Error]: {error_msg}.\\n[Traceback]: {exc}'\n    return msg\n\n\ndef Now():", "\ndef Now():\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef TorchHLoad(filepath: str, **kwargs):\n    import torch, tensorflow as tf\n    if not filepath.startswith(\"hdfs://\"):\n        return torch.load(filepath, **kwargs)\n    else:", "        return torch.load(filepath, **kwargs)\n    else:\n        with tf.io.gfile.GFile(filepath, 'rb') as reader:\n            return torch.load(io.BytesIO(reader.read()), **kwargs)\n\n\ndef TorchHSave(obj, filepath: str, **kwargs):\n    import torch, tensorflow as tf\n    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n        with tf.io.gfile.GFile(filepath, 'wb') as f:", "    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n        with tf.io.gfile.GFile(filepath, 'wb') as f:\n            buffer = io.BytesIO()\n            torch.save(obj, buffer, **kwargs)\n            f.write(buffer.getvalue())\n    else:\n        torch.save(obj, filepath, **kwargs)\n\n\ndef PutHDFS(local: str, remote: str):", "\ndef PutHDFS(local: str, remote: str):\n    import tensorflow as tf\n    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n    if not tf.io.gfile.exists(remote):\n        tf.io.gfile.makedirs(remote)\n    RunCmd(f'{HADOOP_BIN} dfs -put {local} {remote}')\n\n\ndef GetHDFS(remote: str, local: str):", "\ndef GetHDFS(remote: str, local: str):\n    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n    os.makedirs(local, exist_ok=True)\n    RunCmd(f'{HADOOP_BIN} dfs -get {remote} {local}')\n\n\ndef RunCmd(command):\n    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    res, err = pipe.communicate()", "    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    res, err = pipe.communicate()\n    res = res.decode('utf-8')\n    err = err.decode('utf-8')\n    return res, err\n\n\ndef AbsParentDir(file, parent='..', postfix=None):\n    ppath = os.path.abspath(file)\n    parent_level = parent.count('.')", "    ppath = os.path.abspath(file)\n    parent_level = parent.count('.')\n    while parent_level > 0:\n        ppath = os.path.dirname(ppath)\n        parent_level -= 1\n    if postfix is not None:\n        return os.path.join(ppath, postfix)\n    else:\n        return ppath\n", "        return ppath\n\n\ndef init_logger(log_file=None, log_file_level=logging.NOTSET, from_scratch=False):\n    from coloredlogs import ColoredFormatter\n    import tensorflow as tf\n\n    fmt = \"[%(asctime)s %(levelname)s] %(message)s\"\n    log_format = ColoredFormatter(fmt=fmt)\n    # log_format = logging.Formatter()", "    log_format = ColoredFormatter(fmt=fmt)\n    # log_format = logging.Formatter()\n    logger = logging.getLogger()\n    logger.setLevel(log_file_level)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_format)\n    logger.handlers = [console_handler]\n\n    if log_file and log_file != '':", "\n    if log_file and log_file != '':\n        if from_scratch and tf.io.gfile.exists(log_file):\n            logger.warning('Removing previous log file: %s' % log_file)\n            tf.io.gfile.remove(log_file)\n        path = os.path.dirname(log_file)\n        os.makedirs(path, exist_ok=True)\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(log_file_level)\n        file_handler.setFormatter(log_format)", "        file_handler.setLevel(log_file_level)\n        file_handler.setFormatter(log_format)\n        logger.addHandler(file_handler)\n\n    return logger\n\n\ndef is_qid(x, hard=False):\n    if type(x) is not str: return None\n    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)", "    if type(x) is not str: return None\n    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)\n    return None if len(ret) == 0 else ret[0]\n\n\ndef is_pid(x, hard=False):\n    if type(x) is not str: return None\n    ret = re.findall('^P\\d+$', x) if hard else re.findall('P\\d+', x)\n    return None if len(ret) == 0 else ret[0]\n", "    return None if len(ret) == 0 else ret[0]\n\n\nclass MiniLutDB:\n    def __init__(self, db, verbose=True):\n        self.db = db\n        self.conn = None\n        self.verbose = verbose\n\n    def dump_lut(self, lut_tuples, verbose=None):", "\n    def dump_lut(self, lut_tuples, verbose=None):\n        # lut_tuple: (k, v)+, iterable\n\n        if verbose is None: \n            verbose = self.verbose\n        self.conn = sqlite3.connect(self.db)\n        cur = self.conn.cursor()\n        cur.executescript('''\n        DROP TABLE IF EXISTS lut;", "        cur.executescript('''\n        DROP TABLE IF EXISTS lut;\n        CREATE TABLE lut (\n        id      TEXT PRIMARY KEY UNIQUE,\n        content TEXT)''')\n        self.conn.commit()\n\n        BLOCKSIZE = 100000\n        block = []\n        i = 0", "        block = []\n        i = 0\n        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n        for x in iter:\n            block.append(x)\n            i += 1\n            if i == BLOCKSIZE:\n                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n                block = []\n                i = 0", "                block = []\n                i = 0\n        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n        self.conn.commit()\n\n        self.close()\n\n    def update_lut(self, lut_tuples, verbose=None):\n        if verbose is None: \n            verbose = self.verbose", "        if verbose is None: \n            verbose = self.verbose\n\n        self.conn = sqlite3.connect(self.db)\n        BLOCKSIZE = 100000\n        block = []\n        i = 0\n        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n        for x in iter:\n            block.append(x)", "        for x in iter:\n            block.append(x)\n            i += 1\n            if i == BLOCKSIZE:\n                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n                block = []\n                i = 0\n        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n        self.conn.commit()\n", "        self.conn.commit()\n\n        self.close()\n\n    def create_index(self):\n        self.conn = sqlite3.connect(self.db)\n        self.cur = self.conn.cursor()\n        # sql = ('CREATE INDEX index_lut ON lut(id);')\n        # self.cur.execute(sql)\n        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')", "        # self.cur.execute(sql)\n        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')\n        self.conn.commit()\n\n    def get(self, x, default=None):\n        if x is None: return default\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db)\n            self.cur = self.conn.cursor()\n", "            self.cur = self.conn.cursor()\n\n        res = self.query_lut(self.cur, x, False)[0]\n        return res if res is not None else default\n\n    def get_chunk(self, xx):\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db)\n            self.cur = self.conn.cursor()\n        return self.query_lut(self.conn, xx, self.verbose)", "            self.cur = self.conn.cursor()\n        return self.query_lut(self.conn, xx, self.verbose)\n\n    def close(self):\n        if self.conn is not None:\n            self.conn.close()\n            self.conn = None\n    \n    def delete_sample(self, key, value=None):\n        if self.get(key) is None: return", "    def delete_sample(self, key, value=None):\n        if self.get(key) is None: return\n        self.conn = sqlite3.connect(self.db)\n        self.cur = self.conn.cursor()\n        self.cur.execute('DELETE FROM lut WHERE id = ?', (key,))\n        self.conn.commit()\n        assert self.get(key) is None, f'delete failed: {key}'\n\n    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n        values = []", "    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n        values = []\n        if isinstance(keys, str): keys = [keys]\n\n        iter = tqdm(keys, mininterval=0.5, disable=not verbose)\n        for k in iter:\n            cur.execute('SELECT content FROM lut WHERE id = ?', (k,))\n            val = cur.fetchone()\n            val = val[0] if val is not None else None\n            values.append(val)", "            val = val[0] if val is not None else None\n            values.append(val)\n        return values\n\n\ndef OverWriteCjjPy(root='.'):\n    # import difflib\n    # diff = difflib.HtmlDiff()\n    cnt = 0\n    golden_cjjpy = os.path.join(root, 'cjjpy.py')", "    cnt = 0\n    golden_cjjpy = os.path.join(root, 'cjjpy.py')\n    # golden_content = open(golden_cjjpy).readlines()\n    for dir, folder, file in os.walk(root):\n        for f in file:\n            if f == 'cjjpy.py':\n                cjjpy = '%s/%s' % (dir, f)\n                # content = open(cjjpy).readlines()\n                # d = diff.make_file(golden_content, content)\n                cnt += 1", "                # d = diff.make_file(golden_content, content)\n                cnt += 1\n                print('[%d]: %s' % (cnt, cjjpy))\n                os.system('cp %s %s' % (golden_cjjpy, cjjpy))\n\n\ndef ReplaceChar(file, replaced, replacer):\n    print(file, replaced, replacer)\n    with open(file) as f:\n        data = f.readlines()", "    with open(file) as f:\n        data = f.readlines()\n        out = open(file, 'w')\n        for line in data:\n            out.write(line.replace(replaced, replacer))\n\n\ndef DeUnicode(line):\n    return line.encode('utf-8').decode('unicode_escape')\n", "    return line.encode('utf-8').decode('unicode_escape')\n\n\ndef LoadIDDict(dict_file, unify_words=False, lower=False, reverse=False):\n    '''\n    a\\tb\\n, `.dict' file\n    '''\n    import tensorflow as tf\n    assert dict_file.endswith('.dict')\n    id2label = {}", "    assert dict_file.endswith('.dict')\n    id2label = {}\n    with tf.io.gfile.GFile(dict_file, 'r') as f:\n        data = f.read().split('\\n')\n        for i, line in enumerate(data):\n            if line == '': continue\n            try:\n                id, label = line.split('\\t')\n                if reverse:\n                    id, label = label, id", "                if reverse:\n                    id, label = label, id\n                _val = '_'.join(label.split()) if unify_words else label\n                id2label[id] = _val.lower() if lower else _val\n            except:\n                pass\n    return id2label\n\n\ndef LoadWords(file, is_file=True):", "\ndef LoadWords(file, is_file=True):\n    import tensorflow as tf\n    if is_file:\n        with tf.io.gfile.GFile(file, 'r') as f:\n            data = f.read().splitlines()\n    else:\n        data = file.splitlines()\n    return set(map(lambda x: x.strip(), data))\n", "    return set(map(lambda x: x.strip(), data))\n\n\ndef ChangeFileFormat(filename, new_fmt):\n    assert type(filename) is str and type(new_fmt) is str\n    spt = filename.split('.')\n    if len(spt) == 0:\n        return filename\n    else:\n        return filename.replace('.' + spt[-1], new_fmt)", "    else:\n        return filename.replace('.' + spt[-1], new_fmt)\n\n\ndef CountLines(fname):\n    with open(fname, 'rb') as f:\n        count = 0\n        last_data = '\\n'\n        while True:\n            data = f.read(0x400000)", "        while True:\n            data = f.read(0x400000)\n            if not data:\n                break\n            count += data.count(b'\\n')\n            last_data = data\n        if last_data[-1:] != b'\\n':\n            count += 1  # Remove this if a wc-like count is needed\n    return count\n", "    return count\n\n\ndef SearchByKey(file, key):\n    with open(file, 'r') as fin:\n        while True:\n            line = fin.readline()\n            if not line: break\n            if key in line:\n                print(line, end='')", "            if key in line:\n                print(line, end='')\n\n\ndef SendEmail(subject, content, receivers=['MichaelChen0110@163.com']):\n    from email.mime.text import MIMEText\n    import smtplib\n\n    # receivers got to be list.\n    mail_receivers = receivers", "    # receivers got to be list.\n    mail_receivers = receivers\n    # mail_host = \"smtp.163.com\n    mail_host = \"220.181.12.18\"\n    mail_user = \"MichaelChen0110@163.com\"\n    mail_pass = \"\"\n    me = socket.gethostname() + \"<\" + mail_user + \">\"\n    msg = MIMEText(content, _subtype='plain', _charset='utf-8')\n    msg['Subject'] = subject\n    msg['From'] = me", "    msg['Subject'] = subject\n    msg['From'] = me\n    msg['To'] = \";\".join(mail_receivers)\n    try:\n        server = smtplib.SMTP()\n        server.connect(mail_host)\n        server.login(mail_user, mail_pass)\n        server.sendmail(me, mail_receivers, msg.as_string())\n        server.close()\n        print('Have sent the email to ' + str(mail_receivers) + '. ')", "        server.close()\n        print('Have sent the email to ' + str(mail_receivers) + '. ')\n        return True\n    except Exception as e:\n        print(str(e))\n        return False\n\n\ndef SortDict(_dict, reverse=True):\n    assert type(_dict) is dict", "def SortDict(_dict, reverse=True):\n    assert type(_dict) is dict\n    return sorted(_dict.items(), key=lambda d: d[1], reverse=reverse)\n\n\ndef MaxCommLen(str1, str2):\n    lstr1 = len(str1)\n    lstr2 = len(str2)\n    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n    max_num = 0", "    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n    max_num = 0\n    for i in range(lstr1):\n        for j in range(lstr2):\n            if str1[i] == str2[j]:\n                record[i + 1][j + 1] = record[i][j] + 1\n                if record[i + 1][j + 1] > max_num:\n                    max_num = record[i + 1][j + 1]\n    return max_num, ''\n", "    return max_num, ''\n\n\ndef lark(content='test'):\n    print(content)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n", "    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--diff', nargs=2,\n                        help='show difference between two files, shown in downloads/diff.html')\n    parser.add_argument('--de_unicode', action='store_true', default=False,\n                        help='remove unicode characters')\n    parser.add_argument('--link_entity', action='store_true', default=False,\n                        help='')\n    parser.add_argument('--max_comm_len', action='store_true', default=False,\n                        help='')", "    parser.add_argument('--max_comm_len', action='store_true', default=False,\n                        help='')\n    parser.add_argument('--search', nargs=2,\n                        help='search key from file, 2 args: file name & key')\n    parser.add_argument('--email', nargs=2,\n                        help='sending emails, 2 args: subject & content')\n    parser.add_argument('--overwrite', action='store_true', default=None,\n                        help='overwrite all cjjpy under given *dir* based on *dir*/cjjpy.py')\n    parser.add_argument('--replace', nargs=3,\n                        help='replace char, 3 args: file name & replaced char & replacer char')", "    parser.add_argument('--replace', nargs=3,\n                        help='replace char, 3 args: file name & replaced char & replacer char')\n    parser.add_argument('--lark', nargs=1)\n    parser.add_argument('--get_hdfs', nargs=2,\n                        help='easy copy from hdfs to local fs, 2 args: remote_file/dir & local_dir')\n    parser.add_argument('--put_hdfs', nargs=2,\n                        help='easy put from local fs to hdfs, 2 args: local_file/dir & remote_dir')\n    parser.add_argument('--length_stats', nargs=2,\n                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n", "                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n\n    args = parser.parse_args()\n\n    if args.overwrite:\n        print('* Overwriting cjjpy...')\n        OverWriteCjjPy()\n\n    if args.replace:\n        print('* Replacing Char...')", "    if args.replace:\n        print('* Replacing Char...')\n        ReplaceChar(args.replace[0], args.replace[1], args.replace[2])\n\n    if args.search:\n        file = args.search[0]\n        key = args.search[1]\n        print('* Searching %s from %s...' % (key, file))\n        SearchByKey(file, key)\n", "        SearchByKey(file, key)\n\n    if args.email:\n        try:\n            subj = args.email[0]\n            cont = args.email[1]\n        except:\n            subj = 'running complete'\n            cont = ''\n        print('* Sending email {%s, %s} to host...' % (subj, cont))", "            cont = ''\n        print('* Sending email {%s, %s} to host...' % (subj, cont))\n        SendEmail(subj, cont)\n\n    if args.lark:\n        try:\n            content = args.lark[0]\n        except:\n            content = 'running complete'\n        print(f'* Larking \"{content}\"...')", "            content = 'running complete'\n        print(f'* Larking \"{content}\"...')\n        lark(content)\n\n    if args.get_hdfs:\n        remote = args.get_hdfs[0]\n        local = args.get_hdfs[1]\n        print(f'* Copying {remote} to {local}...')\n        GetHDFS(remote, local)\n", "        GetHDFS(remote, local)\n\n    if args.put_hdfs:\n        local = args.put_hdfs[0]\n        remote = args.put_hdfs[1]\n        print(f'* Copying {local} to {remote}...')\n        PutHDFS(local, remote)\n\n    if args.length_stats:\n        file = args.length_stats[0]", "    if args.length_stats:\n        file = args.length_stats[0]\n        key4json = args.length_stats[1]\n        print(f'* Working on {file} lengths statistics...')\n        LengthStats(file, key4json)\n"]}
{"filename": "preprocessing/make_conceptnet_negatives.py", "chunked_list": ["import os\nimport ujson as json\n\n\npronouns = ['me', 'i', 'I', 'her', 'she', 'him', 'he', 'us', 'we', 'them', 'they', 'it', 'its', 'my', 'your', 'his', 'her', 'their', 'our', 'mine', 'yours', 'hers', 'theirs', 'ours', 'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves', 'yourselves', 'themselves', 'this', 'that', 'these', 'those', 'who', 'whom', 'whose', 'which', 'what', 'where', 'when', 'why', 'how', 'there', 'here', 'anybody', 'anyone', 'anything', 'each', 'either', 'everybody', 'everyone', 'everything', 'neither', 'nobody', 'none', 'nothing', 'one', 'other', 'others', 'somebody', 'someone', 'something', 'both', 'few', 'many', 'several', 'all', 'any', 'any', 'more', 'most', 'none', 'some', 'such']\n\nnegation_cue = ['nothing', \"no\", \"not\", \"never\", \"none\", \"hardly\", \"rarely\", \"scarcely\", \"seldom\", 'barely',\n                \"nor\", \"neither\", \"nothing\", \"nowhere\", \"without\", \"lack\", \"cant\", \"dont\", \"doesnt\",\n                \"doesn't\", \"don't\", \"isn't\", \"wasn't\", \"aren't\", \"weren't\", \"haven't\", \"hasn't\", \n                \"shouldn't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\", \"cannot\", \"unable\"]", "                \"doesn't\", \"don't\", \"isn't\", \"wasn't\", \"aren't\", \"weren't\", \"haven't\", \"hasn't\", \n                \"shouldn't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\", \"cannot\", \"unable\"]\n\n\ndef load_ids(file):\n    id_lut = {}\n    with open(file) as f:\n        for line in f.readlines():\n            i, text = line.strip().split('\\t')\n            id_lut[i] = text\n    return id_lut", "\n\ndef has_pronoun(s):\n    for p in pronouns:\n        if p.lower() in s.lower().split():\n            return True\n    return False\n\n\ndef has_negation(s):\n    for p in negation_cue:\n        if p in s.lower().split():\n            return True\n    return False", "\ndef has_negation(s):\n    for p in negation_cue:\n        if p in s.lower().split():\n            return True\n    return False\n\n\ndef load_triples(file, ent_lut, rel_lut, kb):\n    triples = []\n    with open(file) as f:\n        for line in f.readlines():\n            s, p, o = line.strip().split('\\t')\n            s_s = ent_lut[s]\n            p_s = rel_lut[p]\n            o_s = ent_lut[o]\n            if has_pronoun(s_s) or has_pronoun(o_s) or has_negation(s_s) or has_pronoun(o_s): \n                continue\n            if 'or' in o_s.split():\n                continue\n            triples.append({'subject': s_s, 'predicate': p_s, 'object': o_s, 'source_kb': kb})\n    print(f\"* {file} has {len(triples)} valid triples\")\n    return triples", "def load_triples(file, ent_lut, rel_lut, kb):\n    triples = []\n    with open(file) as f:\n        for line in f.readlines():\n            s, p, o = line.strip().split('\\t')\n            s_s = ent_lut[s]\n            p_s = rel_lut[p]\n            o_s = ent_lut[o]\n            if has_pronoun(s_s) or has_pronoun(o_s) or has_negation(s_s) or has_pronoun(o_s): \n                continue\n            if 'or' in o_s.split():\n                continue\n            triples.append({'subject': s_s, 'predicate': p_s, 'object': o_s, 'source_kb': kb})\n    print(f\"* {file} has {len(triples)} valid triples\")\n    return triples", "\n\ndef assemble(negater_dir, output_file):\n    rel_lut = load_ids(f'{negater_dir}/relation_ids.txt')\n    ent_lut = load_ids(f'{negater_dir}/entity_ids.txt')\n    neg_triples = load_triples(f'{negater_dir}/test_negatives.txt', ent_lut, rel_lut, 'conceptnet-neg-test')\n    neg_triples += load_triples(f'{negater_dir}/valid_negatives.txt', ent_lut, rel_lut, 'conceptnet-neg-valid')\n    pos_triples = load_triples(f'{negater_dir}/test.txt', ent_lut, rel_lut, 'conceptnet-pos-test')\n    pos_triples += load_triples(f'{negater_dir}/valid.txt', ent_lut, rel_lut, 'conceptnet-pos-valid')\n    \n    min_size = min(len(pos_triples), len(neg_triples))\n    triples = pos_triples[:min_size] + neg_triples[:min_size]\n    \n    print(f\"* total {len(triples)} triples\")\n    with open(output_file, 'w') as f:\n        for t in triples:\n            f.write(json.dumps(t) + '\\n')", "\n\nif __name__ == '__main__':\n    assemble(f'{os.environ[\"PJ_HOME\"]}/data/NegatER/data/conceptnet/true-neg', f'{os.environ[\"PJ_HOME\"]}/data/probe_datasets/true-neg.jsonl')"]}
{"filename": "preprocessing/remove_nested_keys_from_json.py", "chunked_list": ["import argparse\nimport ujson as json\n\n\ndef remove_nested_keys(js, keys):\n    nested_keys = keys.split('/')\n    for k in nested_keys[:-1]:\n        js = js[k]\n    rm_key = nested_keys[-1]\n    if js.get(rm_key):\n        js.pop(rm_key)\n    else:\n        print(f'no key: {rm_key}')", "\n\ndef test():\n    s = {\"subject\":\"speed boat\",\"predicate\":\"HasProperty\",\"object\":\"dangerous\",\"source_kb\":\"conceptnet-pos-test\",\"boolq\":{\"text-davinci-002_ex-8\":\"is a speed boat dangerous?\"},\"cg_pred\":{\"text-davinci-002_ex-4p4n\":[\" speed boats can be dangerous.\"],\"text-davinci-002_ex-2p2n\":[\" speed boats are dangerous.\"],\"text-davinci-002_ex-2p6n\":[\" speed boats are not dangerous.\"],\"text-davinci-002_ex-6p2n\":[\"\\n\\nspeed boats are dangerous.\"],\"text-davinci-002_ex-1p7n\":[\" speed boats can be dangerous.\"],\"text-davinci-002_ex-0p8n\":[\" speed boats are not dangerous.\"],\"text-davinci-002_ex-8p8n\":[\"speed boats can be dangerous.\\n###\\nKeywords: dollar, used to, buy things\\n\\nDollars are used to buy things.\"]},\"qa_pred\":{\"text-davinci-002_ex-4p4n\":\"yes\",\"text-davinci-002_ex-2p2n\":\"yes\",\"text-davinci-002_ex-2p6n\":\"yes\",\"text-davinci-002_ex-6p2n\":\"yes\"}}\n    for k in ['cg_pred/text-davinci-002_ex-8p8n', 'cg_pred/text-davinci-002_ex-6p2n', 'cg_pred/text-davinci-002_ex-0p8den']:\n        remove_nested_keys(s, k)\n    print(s)\n    # print(ss)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-i', type=str, required=True)\n    parser.add_argument('-o', type=str)\n    parser.add_argument('-k', type=str, help='nested keys, e.g. aaa/bbb/ccc')\n    args = parser.parse_args()\n\n    new_jsl = []\n    with open(args.i) as f:\n        for line in f.readlines():\n            try:\n                js = json.loads(line)\n            except:\n                print(line)\n                raise ValueError\n            remove_nested_keys(js, args.k)\n            new_jsl.append(js)\n    \n    output_file = args.i if args.o is None else args.o\n    with open(output_file, 'w') as fo:\n        for js in new_jsl:\n            fo.write(json.dumps(js) + '\\n')", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-i', type=str, required=True)\n    parser.add_argument('-o', type=str)\n    parser.add_argument('-k', type=str, help='nested keys, e.g. aaa/bbb/ccc')\n    args = parser.parse_args()\n\n    new_jsl = []\n    with open(args.i) as f:\n        for line in f.readlines():\n            try:\n                js = json.loads(line)\n            except:\n                print(line)\n                raise ValueError\n            remove_nested_keys(js, args.k)\n            new_jsl.append(js)\n    \n    output_file = args.i if args.o is None else args.o\n    with open(output_file, 'w') as fo:\n        for js in new_jsl:\n            fo.write(json.dumps(js) + '\\n')"]}
{"filename": "preprocessing/calculate_cooccurrence.py", "chunked_list": ["import os\nimport ujson as json\nfrom tqdm import tqdm\nimport cjjpy as cjj\nfrom multiprocessing import Pool\n\n\nstopwords = cjj.LoadWords(f\"{os.environ['PJ_HOME']}/preprocessing/stopwords.txt\")\n\ndef load_sentences():\n    prefix = f\"{os.environ['PJ_HOME']}/data/corpus\"\n    sents = []    \n    with open(f'{prefix}/omcs_sentences.txt') as f:\n        sents += f.read().splitlines()\n    with open(f'{prefix}/wiki1m_for_simcse.txt') as f:\n        sents += f.read().splitlines()\n    sents = [x.lower() for x in sents]\n    return sents ", "\ndef load_sentences():\n    prefix = f\"{os.environ['PJ_HOME']}/data/corpus\"\n    sents = []    \n    with open(f'{prefix}/omcs_sentences.txt') as f:\n        sents += f.read().splitlines()\n    with open(f'{prefix}/wiki1m_for_simcse.txt') as f:\n        sents += f.read().splitlines()\n    sents = [x.lower() for x in sents]\n    return sents ", "\n\ndef cooccur_cnt(js):\n    hit = 0\n    w1 = js['subject'].lower().split()\n    w2 = js['object'].lower().split()\n    pairs = [(x, y) for x in w1 for y in w2 if x not in stopwords and y not in stopwords]\n    for sent in sents:\n        for p1, p2 in pairs:\n            if p1 in sent and p2 in sent:\n                hit += 1\n    if pairs == []: hit = 0\n    \n    js['cooccur_cnt'] = hit / len(pairs)\n    js['cooccur_total'] = hit\n    return js", "\n\ndef callback(x):\n    bar.update(1)\n    fw.write(json.dumps(x) + '\\n')\n\n\nif __name__ == \"__main__\":\n    sents = load_sentences()\n\n    with open(f'{os.environ[\"PJ_HOME\"]}/data/probe_datasets/true-neg-llm_test.clean.jsonl') as f:\n        data = f.readlines()\n        print(len(data))\n\n    fw = open(f'{os.environ[\"PJ_HOME\"]}/data/probe_datasets/true-neg-llm_test.clean.cooccur.jsonl', 'w')\n    p = Pool(16)\n    bar = tqdm(total=len(data))\n    for line in data:\n        js = json.loads(line)\n        p.apply_async(cooccur_cnt, (js,), callback=callback)\n    p.close()\n    p.join()\n    fw.close()", "    \n"]}
{"filename": "constrained_generation/cjjpy.py", "chunked_list": ["\ufeff# -*- coding: utf-8 -*-\n\n'''\n@Author : Jiangjie Chen\n@Time   : 2022/5/26 19:52\n@Contact: jjchen19@fudan.edu.cn\n'''\n\nimport re\nimport datetime", "import re\nimport datetime\nimport os\nimport subprocess\nimport urllib.request, urllib.parse\nimport argparse\nfrom tqdm import tqdm\nimport sqlite3\nimport requests\nimport socket", "import requests\nimport socket\nimport logging\nimport io\nimport traceback\n\ntry:\n    import ujson as json\nexcept:\n    import json", "except:\n    import json\n\nHADOOP_BIN = 'PATH=/usr/bin/:$PATH hdfs'\n\n\ndef LengthStats(filename, key4json=None):\n    len_list = []\n    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n    with open(filename) as f:", "    thresholds = [0.8, 0.9, 0.95, 0.99, 0.999]\n    with open(filename) as f:\n        for line in f:\n            if key4json not in ['none', None, 'None']:\n                len_list.append(len(json.loads(line)[key4json].split()))\n            else:\n                len_list.append(len(line.strip().split()))\n    stats = {\n        'Max': max(len_list),\n        'Min': min(len_list),", "        'Max': max(len_list),\n        'Min': min(len_list),\n        'Avg': round(sum(len_list) / len(len_list), 4),\n    }\n    len_list.sort()\n    for t in thresholds:\n        stats[f\"Top-{t}\"] = len_list[int(len(len_list) * t)]\n\n    for k in stats:\n        print(f\"- {k}: {stats[k]}\")", "    for k in stats:\n        print(f\"- {k}: {stats[k]}\")\n    return stats\n\n\nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n", "        self.__dict__ = self\n\n\ndef TraceBack(error_msg):\n    exc = traceback.format_exc()\n    msg = f'[Error]: {error_msg}.\\n[Traceback]: {exc}'\n    return msg\n\n\ndef Now():", "\ndef Now():\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef TorchHLoad(filepath: str, **kwargs):\n    import torch, tensorflow as tf\n    if not filepath.startswith(\"hdfs://\"):\n        return torch.load(filepath, **kwargs)\n    else:", "        return torch.load(filepath, **kwargs)\n    else:\n        with tf.io.gfile.GFile(filepath, 'rb') as reader:\n            return torch.load(io.BytesIO(reader.read()), **kwargs)\n\n\ndef TorchHSave(obj, filepath: str, **kwargs):\n    import torch, tensorflow as tf\n    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n        with tf.io.gfile.GFile(filepath, 'wb') as f:", "    if filepath.startswith(\"hdfs://\") or remote.startswith('webhdfs://'):\n        with tf.io.gfile.GFile(filepath, 'wb') as f:\n            buffer = io.BytesIO()\n            torch.save(obj, buffer, **kwargs)\n            f.write(buffer.getvalue())\n    else:\n        torch.save(obj, filepath, **kwargs)\n\n\ndef PutHDFS(local: str, remote: str):", "\ndef PutHDFS(local: str, remote: str):\n    import tensorflow as tf\n    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n    if not tf.io.gfile.exists(remote):\n        tf.io.gfile.makedirs(remote)\n    RunCmd(f'{HADOOP_BIN} dfs -put {local} {remote}')\n\n\ndef GetHDFS(remote: str, local: str):", "\ndef GetHDFS(remote: str, local: str):\n    assert remote.startswith('hdfs://') or remote.startswith('webhdfs://')\n    os.makedirs(local, exist_ok=True)\n    RunCmd(f'{HADOOP_BIN} dfs -get {remote} {local}')\n\n\ndef RunCmd(command):\n    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    res, err = pipe.communicate()", "    pipe = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    res, err = pipe.communicate()\n    res = res.decode('utf-8')\n    err = err.decode('utf-8')\n    return res, err\n\n\ndef AbsParentDir(file, parent='..', postfix=None):\n    ppath = os.path.abspath(file)\n    parent_level = parent.count('.')", "    ppath = os.path.abspath(file)\n    parent_level = parent.count('.')\n    while parent_level > 0:\n        ppath = os.path.dirname(ppath)\n        parent_level -= 1\n    if postfix is not None:\n        return os.path.join(ppath, postfix)\n    else:\n        return ppath\n", "        return ppath\n\n\ndef init_logger(log_file=None, log_file_level=logging.NOTSET, from_scratch=False):\n    from coloredlogs import ColoredFormatter\n    import tensorflow as tf\n\n    fmt = \"[%(asctime)s %(levelname)s] %(message)s\"\n    log_format = ColoredFormatter(fmt=fmt)\n    # log_format = logging.Formatter()", "    log_format = ColoredFormatter(fmt=fmt)\n    # log_format = logging.Formatter()\n    logger = logging.getLogger()\n    logger.setLevel(log_file_level)\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(log_format)\n    logger.handlers = [console_handler]\n\n    if log_file and log_file != '':", "\n    if log_file and log_file != '':\n        if from_scratch and tf.io.gfile.exists(log_file):\n            logger.warning('Removing previous log file: %s' % log_file)\n            tf.io.gfile.remove(log_file)\n        path = os.path.dirname(log_file)\n        os.makedirs(path, exist_ok=True)\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(log_file_level)\n        file_handler.setFormatter(log_format)", "        file_handler.setLevel(log_file_level)\n        file_handler.setFormatter(log_format)\n        logger.addHandler(file_handler)\n\n    return logger\n\n\ndef is_qid(x, hard=False):\n    if type(x) is not str: return None\n    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)", "    if type(x) is not str: return None\n    ret = re.findall('^Q\\d+$', x) if hard else re.findall('Q\\d+', x)\n    return None if len(ret) == 0 else ret[0]\n\n\ndef is_pid(x, hard=False):\n    if type(x) is not str: return None\n    ret = re.findall('^P\\d+$', x) if hard else re.findall('P\\d+', x)\n    return None if len(ret) == 0 else ret[0]\n", "    return None if len(ret) == 0 else ret[0]\n\n\nclass MiniLutDB:\n    def __init__(self, db, verbose=True):\n        self.db = db\n        self.conn = None\n        self.verbose = verbose\n\n    def dump_lut(self, lut_tuples, verbose=None):", "\n    def dump_lut(self, lut_tuples, verbose=None):\n        # lut_tuple: (k, v)+, iterable\n\n        if verbose is None: \n            verbose = self.verbose\n        self.conn = sqlite3.connect(self.db)\n        cur = self.conn.cursor()\n        cur.executescript('''\n        DROP TABLE IF EXISTS lut;", "        cur.executescript('''\n        DROP TABLE IF EXISTS lut;\n        CREATE TABLE lut (\n        id      TEXT PRIMARY KEY UNIQUE,\n        content TEXT)''')\n        self.conn.commit()\n\n        BLOCKSIZE = 100000\n        block = []\n        i = 0", "        block = []\n        i = 0\n        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n        for x in iter:\n            block.append(x)\n            i += 1\n            if i == BLOCKSIZE:\n                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n                block = []\n                i = 0", "                block = []\n                i = 0\n        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n        self.conn.commit()\n\n        self.close()\n\n    def update_lut(self, lut_tuples, verbose=None):\n        if verbose is None: \n            verbose = self.verbose", "        if verbose is None: \n            verbose = self.verbose\n\n        self.conn = sqlite3.connect(self.db)\n        BLOCKSIZE = 100000\n        block = []\n        i = 0\n        iter = tqdm(lut_tuples, mininterval=0.5, disable=not verbose)\n        for x in iter:\n            block.append(x)", "        for x in iter:\n            block.append(x)\n            i += 1\n            if i == BLOCKSIZE:\n                self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n                block = []\n                i = 0\n        self.conn.executemany('INSERT OR REPLACE INTO lut (id, content) VALUES (?, ?)', block)\n        self.conn.commit()\n", "        self.conn.commit()\n\n        self.close()\n\n    def create_index(self):\n        self.conn = sqlite3.connect(self.db)\n        self.cur = self.conn.cursor()\n        # sql = ('CREATE INDEX index_lut ON lut(id);')\n        # self.cur.execute(sql)\n        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')", "        # self.cur.execute(sql)\n        self.cur.executescript('CREATE INDEX index_lut ON lut(id);')\n        self.conn.commit()\n\n    def get(self, x, default=None):\n        if x is None: return default\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db)\n            self.cur = self.conn.cursor()\n", "            self.cur = self.conn.cursor()\n\n        res = self.query_lut(self.cur, x, False)[0]\n        return res if res is not None else default\n\n    def get_chunk(self, xx):\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.db)\n            self.cur = self.conn.cursor()\n        return self.query_lut(self.conn, xx, self.verbose)", "            self.cur = self.conn.cursor()\n        return self.query_lut(self.conn, xx, self.verbose)\n\n    def close(self):\n        if self.conn is not None:\n            self.conn.close()\n            self.conn = None\n    \n    def delete_sample(self, key, value=None):\n        if self.get(key) is None: return", "    def delete_sample(self, key, value=None):\n        if self.get(key) is None: return\n        self.conn = sqlite3.connect(self.db)\n        self.cur = self.conn.cursor()\n        self.cur.execute('DELETE FROM lut WHERE id = ?', (key,))\n        self.conn.commit()\n        assert self.get(key) is None, f'delete failed: {key}'\n\n    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n        values = []", "    def query_lut(self, cur: sqlite3.Cursor, keys, verbose=True):\n        values = []\n        if isinstance(keys, str): keys = [keys]\n\n        iter = tqdm(keys, mininterval=0.5, disable=not verbose)\n        for k in iter:\n            cur.execute('SELECT content FROM lut WHERE id = ?', (k,))\n            val = cur.fetchone()\n            val = val[0] if val is not None else None\n            values.append(val)", "            val = val[0] if val is not None else None\n            values.append(val)\n        return values\n\n\ndef OverWriteCjjPy(root='.'):\n    # import difflib\n    # diff = difflib.HtmlDiff()\n    cnt = 0\n    golden_cjjpy = os.path.join(root, 'cjjpy.py')", "    cnt = 0\n    golden_cjjpy = os.path.join(root, 'cjjpy.py')\n    # golden_content = open(golden_cjjpy).readlines()\n    for dir, folder, file in os.walk(root):\n        for f in file:\n            if f == 'cjjpy.py':\n                cjjpy = '%s/%s' % (dir, f)\n                # content = open(cjjpy).readlines()\n                # d = diff.make_file(golden_content, content)\n                cnt += 1", "                # d = diff.make_file(golden_content, content)\n                cnt += 1\n                print('[%d]: %s' % (cnt, cjjpy))\n                os.system('cp %s %s' % (golden_cjjpy, cjjpy))\n\n\ndef ReplaceChar(file, replaced, replacer):\n    print(file, replaced, replacer)\n    with open(file) as f:\n        data = f.readlines()", "    with open(file) as f:\n        data = f.readlines()\n        out = open(file, 'w')\n        for line in data:\n            out.write(line.replace(replaced, replacer))\n\n\ndef DeUnicode(line):\n    return line.encode('utf-8').decode('unicode_escape')\n", "    return line.encode('utf-8').decode('unicode_escape')\n\n\ndef LoadIDDict(dict_file, unify_words=False, lower=False, reverse=False):\n    '''\n    a\\tb\\n, `.dict' file\n    '''\n    import tensorflow as tf\n    assert dict_file.endswith('.dict')\n    id2label = {}", "    assert dict_file.endswith('.dict')\n    id2label = {}\n    with tf.io.gfile.GFile(dict_file, 'r') as f:\n        data = f.read().split('\\n')\n        for i, line in enumerate(data):\n            if line == '': continue\n            try:\n                id, label = line.split('\\t')\n                if reverse:\n                    id, label = label, id", "                if reverse:\n                    id, label = label, id\n                _val = '_'.join(label.split()) if unify_words else label\n                id2label[id] = _val.lower() if lower else _val\n            except:\n                pass\n    return id2label\n\n\ndef LoadWords(file, is_file=True):", "\ndef LoadWords(file, is_file=True):\n    import tensorflow as tf\n    if is_file:\n        with tf.io.gfile.GFile(file, 'r') as f:\n            data = f.read().splitlines()\n    else:\n        data = file.splitlines()\n    return set(map(lambda x: x.strip(), data))\n", "    return set(map(lambda x: x.strip(), data))\n\n\ndef ChangeFileFormat(filename, new_fmt):\n    assert type(filename) is str and type(new_fmt) is str\n    spt = filename.split('.')\n    if len(spt) == 0:\n        return filename\n    else:\n        return filename.replace('.' + spt[-1], new_fmt)", "    else:\n        return filename.replace('.' + spt[-1], new_fmt)\n\n\ndef CountLines(fname):\n    with open(fname, 'rb') as f:\n        count = 0\n        last_data = '\\n'\n        while True:\n            data = f.read(0x400000)", "        while True:\n            data = f.read(0x400000)\n            if not data:\n                break\n            count += data.count(b'\\n')\n            last_data = data\n        if last_data[-1:] != b'\\n':\n            count += 1  # Remove this if a wc-like count is needed\n    return count\n", "    return count\n\n\ndef SearchByKey(file, key):\n    with open(file, 'r') as fin:\n        while True:\n            line = fin.readline()\n            if not line: break\n            if key in line:\n                print(line, end='')", "            if key in line:\n                print(line, end='')\n\n\ndef SendEmail(subject, content, receivers=['MichaelChen0110@163.com']):\n    from email.mime.text import MIMEText\n    import smtplib\n\n    # receivers got to be list.\n    mail_receivers = receivers", "    # receivers got to be list.\n    mail_receivers = receivers\n    # mail_host = \"smtp.163.com\n    mail_host = \"220.181.12.18\"\n    mail_user = \"MichaelChen0110@163.com\"\n    mail_pass = \"\"\n    me = socket.gethostname() + \"<\" + mail_user + \">\"\n    msg = MIMEText(content, _subtype='plain', _charset='utf-8')\n    msg['Subject'] = subject\n    msg['From'] = me", "    msg['Subject'] = subject\n    msg['From'] = me\n    msg['To'] = \";\".join(mail_receivers)\n    try:\n        server = smtplib.SMTP()\n        server.connect(mail_host)\n        server.login(mail_user, mail_pass)\n        server.sendmail(me, mail_receivers, msg.as_string())\n        server.close()\n        print('Have sent the email to ' + str(mail_receivers) + '. ')", "        server.close()\n        print('Have sent the email to ' + str(mail_receivers) + '. ')\n        return True\n    except Exception as e:\n        print(str(e))\n        return False\n\n\ndef SortDict(_dict, reverse=True):\n    assert type(_dict) is dict", "def SortDict(_dict, reverse=True):\n    assert type(_dict) is dict\n    return sorted(_dict.items(), key=lambda d: d[1], reverse=reverse)\n\n\ndef MaxCommLen(str1, str2):\n    lstr1 = len(str1)\n    lstr2 = len(str2)\n    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n    max_num = 0", "    record = [[0 for i in range(lstr2 + 1)] for j in range(lstr1 + 1)]\n    max_num = 0\n    for i in range(lstr1):\n        for j in range(lstr2):\n            if str1[i] == str2[j]:\n                record[i + 1][j + 1] = record[i][j] + 1\n                if record[i + 1][j + 1] > max_num:\n                    max_num = record[i + 1][j + 1]\n    return max_num, ''\n", "    return max_num, ''\n\n\ndef lark(content='test'):\n    print(content)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n", "    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--diff', nargs=2,\n                        help='show difference between two files, shown in downloads/diff.html')\n    parser.add_argument('--de_unicode', action='store_true', default=False,\n                        help='remove unicode characters')\n    parser.add_argument('--link_entity', action='store_true', default=False,\n                        help='')\n    parser.add_argument('--max_comm_len', action='store_true', default=False,\n                        help='')", "    parser.add_argument('--max_comm_len', action='store_true', default=False,\n                        help='')\n    parser.add_argument('--search', nargs=2,\n                        help='search key from file, 2 args: file name & key')\n    parser.add_argument('--email', nargs=2,\n                        help='sending emails, 2 args: subject & content')\n    parser.add_argument('--overwrite', action='store_true', default=None,\n                        help='overwrite all cjjpy under given *dir* based on *dir*/cjjpy.py')\n    parser.add_argument('--replace', nargs=3,\n                        help='replace char, 3 args: file name & replaced char & replacer char')", "    parser.add_argument('--replace', nargs=3,\n                        help='replace char, 3 args: file name & replaced char & replacer char')\n    parser.add_argument('--lark', nargs=1)\n    parser.add_argument('--get_hdfs', nargs=2,\n                        help='easy copy from hdfs to local fs, 2 args: remote_file/dir & local_dir')\n    parser.add_argument('--put_hdfs', nargs=2,\n                        help='easy put from local fs to hdfs, 2 args: local_file/dir & remote_dir')\n    parser.add_argument('--length_stats', nargs=2,\n                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n", "                        help='simple token lengths distribution of a line-by-line file, 2 args: filename & key (or none)')\n\n    args = parser.parse_args()\n\n    if args.overwrite:\n        print('* Overwriting cjjpy...')\n        OverWriteCjjPy()\n\n    if args.replace:\n        print('* Replacing Char...')", "    if args.replace:\n        print('* Replacing Char...')\n        ReplaceChar(args.replace[0], args.replace[1], args.replace[2])\n\n    if args.search:\n        file = args.search[0]\n        key = args.search[1]\n        print('* Searching %s from %s...' % (key, file))\n        SearchByKey(file, key)\n", "        SearchByKey(file, key)\n\n    if args.email:\n        try:\n            subj = args.email[0]\n            cont = args.email[1]\n        except:\n            subj = 'running complete'\n            cont = ''\n        print('* Sending email {%s, %s} to host...' % (subj, cont))", "            cont = ''\n        print('* Sending email {%s, %s} to host...' % (subj, cont))\n        SendEmail(subj, cont)\n\n    if args.lark:\n        try:\n            content = args.lark[0]\n        except:\n            content = 'running complete'\n        print(f'* Larking \"{content}\"...')", "            content = 'running complete'\n        print(f'* Larking \"{content}\"...')\n        lark(content)\n\n    if args.get_hdfs:\n        remote = args.get_hdfs[0]\n        local = args.get_hdfs[1]\n        print(f'* Copying {remote} to {local}...')\n        GetHDFS(remote, local)\n", "        GetHDFS(remote, local)\n\n    if args.put_hdfs:\n        local = args.put_hdfs[0]\n        remote = args.put_hdfs[1]\n        print(f'* Copying {local} to {remote}...')\n        PutHDFS(local, remote)\n\n    if args.length_stats:\n        file = args.length_stats[0]", "    if args.length_stats:\n        file = args.length_stats[0]\n        key4json = args.length_stats[1]\n        print(f'* Working on {file} lengths statistics...')\n        LengthStats(file, key4json)\n"]}
{"filename": "constrained_generation/llm_constrained_generation.py", "chunked_list": ["import sys\nimport cjjpy as cjj\n\nsys.path.append(cjj.AbsParentDir(__file__, '..'))\nfrom gpt3_helper import prompt_gpt3, calc_cost_w_prompt\n# from flant5_helper import prompt_flant5\nfrom utils import load_jsonl, chunks_list_first\nfrom llm_utils import (\n    save_llm_results, \n    prepare_prompt,", "    save_llm_results, \n    prepare_prompt,\n    TASK_KEY\n)\n\n\ndef llm_constrained_generation(model_name, input_file, output_file=None, key_q='keywords', k_pos_ex=2, k_neg_ex=2, \n                               n_repeat=1, temperature=0.9, batch_size=8, cot='none'):\n    data = load_jsonl(input_file)\n\n    prompts = []\n    for js in data:\n        prompt = prepare_prompt('cg', js, k_pos_ex, k_neg_ex, key_q, cot)\n        prompts.append(prompt)\n    print(prompts[0])\n    \n    num_cot_tokens = 64 if cot is not None else 0   # TODO: approximate number of cot tokens\n    if 'flan-t5' in model_name:\n        prompt_func = prompt_flant5\n    else:\n        prompt_func = prompt_gpt3\n    res, money = prompt_func(prompts, model_name=model_name, clean=True, temperature=temperature, \n                            max_tokens=30 + num_cot_tokens, verbose=True, batch_size=batch_size, n=n_repeat)\n    \n    y_pred = []\n    for i, (ny, indiv_prompt) in enumerate(zip(chunks_list_first(res, n=n_repeat), prompts)):\n        # handle n_returned sequences\n        if i == 0: print(indiv_prompt + ny[0])\n        y_pred.append(ny)\n\n    if args.instruct_id == 0:\n        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n\"\n    else:\n        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n_i{args.instruct_id}\" # TODO: hard-coded\n    \n    if args.temperature == 0:\n        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n\"\n    else:\n        model_key = f\"{model_name}_ex-{k_pos_ex}p{k_neg_ex}n_t{args.temperature}\" # TODO: hard-coded\n    \n    input_type = 'keywords' if key_q == 'keywords' else 'question'\n    task_key = TASK_KEY['cg'][input_type]\n    if cot != 'none':\n        task_key += f'_{cot}'\n\n    # save into file, override previous model key.\n    save_llm_results(input_file, y_pred, task_key, model_key, output_file)\n    \n    if 'bloom' not in model_name or 'flan-t5' not in model_name:\n        cjj.lark(f\"This run has cost you {round(money, 2)}$: {task_key}/{model_key}.\")\n        \n    return f\"{task_key}/{model_key}\"", "\n\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_file', '-i', type=str, required=True)\n    parser.add_argument('--model_name', '-m', type=str, required=True, \n                        choices=['flan-t5-large', 'flan-t5-xl', 'flan-t5-xxl',\n                                 'davinci', 'curie', 'babbage', 'ada', \n                                 'text-davinci-001', 'text-curie-001', 'text-babbage-001', 'text-ada-001',\n                                 'text-davinci-002', 'text-davinci-003', 'code-davinci-002'])\n    parser.add_argument('--posk', type=int, help='Number of positive examples in the demonstration.')\n    parser.add_argument('--negk', type=int, help='Number of negative examples in the demonstration.')\n    parser.add_argument('--output_file', '-o', type=str, required=True)\n    parser.add_argument('--input_key', '-q', default='keywords', \n                        choices=['rule', 'text-davinci-002_ex-8', 'keywords'], type=str,\n                        help='Key for input in the jsonline file. Keywords input by default for CG task')\n    parser.add_argument('--n_repeat', '-n', type=int, default=1)\n    parser.add_argument('--temperature', type=float, default=0.0)\n    parser.add_argument('--batch_size', '-b', type=int, default=16)\n    parser.add_argument('--cot', type=str, choices=['fact', 'logic', 'none'], default='none')\n    parser.add_argument('--instruct_id', type=int, default=0, \n                        help='For testing different instructions. Use the first instruction by default.')\n    parser.add_argument(\"--local_rank\", required=False, type=int, help=\"used by dist launchers\")\n    args = parser.parse_args()\n\n    filename = llm_constrained_generation(args.model_name, args.input_file, args.output_file, key_q=args.input_key,\n                                          k_pos_ex=args.posk, k_neg_ex=args.negk, n_repeat=args.n_repeat, \n                                          temperature=args.temperature, batch_size=args.batch_size, cot=args.cot)"]}
