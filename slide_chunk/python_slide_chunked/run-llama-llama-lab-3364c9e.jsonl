{"filename": "convo_agents/convo_agents.py", "chunked_list": ["\"\"\"Run Llama conversation agents.\n\nThe goal of this is to simulate conversation between two agents.\n\n\"\"\"\n\nfrom llama_index import (\n    GPTVectorStoreIndex, GPTListIndex, Document, ServiceContext\n)\nfrom llama_index.indices.base import BaseGPTIndex", ")\nfrom llama_index.indices.base import BaseGPTIndex\nfrom llama_index.data_structs import Node\nfrom llama_index.prompts.prompts import QuestionAnswerPrompt\nfrom collections import deque\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Dict\n\n\ndef format_text(text: str, user: str) -> str:\n    return user + \": \" + text", "\ndef format_text(text: str, user: str) -> str:\n    return user + \": \" + text\n\n\nDEFAULT_USER_PREFIX_TMPL = (\n    \"Your name is {name}. \"\n    \"We provide conversation context between you and other users below. \"\\\n    \"You are on a date with someone else. \\n\"\n    # \"The user is the plaintiff and the other user is the defendant.\"", "    \"You are on a date with someone else. \\n\"\n    # \"The user is the plaintiff and the other user is the defendant.\"\n)\nDEFAULT_PROMPT_TMPL = (\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given the context information, perform the following task.\\n\"\n    \"Task: {query_str}\\n\"\n    \"You: \"", "    \"Task: {query_str}\\n\"\n    \"You: \"\n    # \"Here's an example:\\n\"\n    # \"Previous line: Hi Bob, good to meet you!\\n\"\n    # \"You: Good to meet you too!\\n\\n\"\n    # \"Previous line: {query_str}\\n\"\n    # \"You: \"\n)\nDEFAULT_PROMPT = QuestionAnswerPrompt(DEFAULT_PROMPT_TMPL)\n", "DEFAULT_PROMPT = QuestionAnswerPrompt(DEFAULT_PROMPT_TMPL)\n\n\n\nclass ConvoAgent(BaseModel):\n    \"\"\"Basic abstraction for a conversation agent.\"\"\"\n    name: str\n    st_memory: deque\n    lt_memory: BaseGPTIndex\n    lt_memory_query_kwargs: Dict = Field(default_factory=dict)\n    service_context: ServiceContext\n    st_memory_size: int = 10\n    # qa_prompt: QuestionAnswerPrompt = DEFAULT_PROMPT\n    user_prefix_tmpl: str = DEFAULT_USER_PREFIX_TMPL\n    qa_prompt_tmpl: str = DEFAULT_PROMPT_TMPL\n    \n    class Config:\n        arbitrary_types_allowed = True\n        \n    @classmethod\n    def from_defaults(\n        cls,\n        name: Optional[str] = None,\n        st_memory: Optional[deque] = None,\n        lt_memory: Optional[BaseGPTIndex] = None,\n        service_context: Optional[ServiceContext] = None,\n        **kwargs\n    ) -> \"ConvoAgent\":\n        name = name or \"Agent\"\n        st_memory = st_memory or deque()\n        lt_memory = lt_memory or GPTVectorStoreIndex([])\n        service_context = service_context or ServiceContext.from_defaults()\n        return cls(\n            name=name,\n            st_memory=st_memory,\n            lt_memory=lt_memory,\n            service_context=service_context,\n            **kwargs\n        )\n                      \n    \n    def add_message(self, message: str, user: str) -> None:\n        \"\"\"Add message from another user.\"\"\"\n        fmt_message = format_text(message, user)\n        self.st_memory.append(fmt_message) \n        while len(self.st_memory) > self.st_memory_size:\n            self.st_memory.popleft()\n        self.lt_memory.insert(Document(fmt_message))\n    \n    def generate_message(self, prev_message: Optional[str] = None) -> str:\n        \"\"\"Generate a new message.\"\"\"\n        # if prev_message is None, get previous message using short-term memory\n        if prev_message is None:\n            prev_message = self.st_memory[-1]\n\n        st_memory_text = \"\\n\".join([l for l in self.st_memory])\n        summary_response = self.lt_memory.as_query_engine(**self.lt_memory_query_kwargs).query(\n            f\"Tell me a bit more about any context that's relevant \"\n            f\"to the current messages: \\n{st_memory_text}\"\n        )\n        \n        # add both the long-term memory summary and the short-term conversation\n        list_builder = GPTListIndex([])\n        list_builder.insert_nodes([Node(str(summary_response))])\n        list_builder.insert_nodes([Node(st_memory_text)])\n        \n        # question-answer prompt\n        full_qa_prompt_tmpl = (\n            self.user_prefix_tmpl.format(name=self.name) + \"\\n\" +\n            self.qa_prompt_tmpl\n        )\n        qa_prompt = QuestionAnswerPrompt(full_qa_prompt_tmpl)\n        \n        response = list_builder.as_query_engine(text_qa_template=qa_prompt).query(\n            \"Generate the next message in the conversation.\"\n        )    \n        return str(response)", ""]}
{"filename": "llama_agi/llama_agi/__init__.py", "chunked_list": [""]}
{"filename": "llama_agi/llama_agi/utils.py", "chunked_list": ["from typing import Any, List, Optional\n\nfrom llama_index import GPTVectorStoreIndex, GPTListIndex, ServiceContext, Document\nfrom llama_index.indices.base import BaseGPTIndex\n\n\ndef initialize_task_list_index(\n    documents: List[Document], service_context: Optional[ServiceContext] = None\n) -> BaseGPTIndex[Any]:\n    return GPTListIndex.from_documents(documents, service_context=service_context)", "\n\ndef initialize_search_index(\n    documents: List[Document], service_context: Optional[ServiceContext] = None\n) -> BaseGPTIndex[Any]:\n    return GPTVectorStoreIndex.from_documents(\n        documents, service_context=service_context\n    )\n\n\ndef log_current_status(\n    cur_task: str,\n    result: str,\n    completed_tasks_summary: str,\n    task_list: List[Document],\n    return_str: bool = False,\n) -> Optional[str]:\n    status_string = f\"\"\"\n    __________________________________\n    Completed Tasks Summary: {completed_tasks_summary.strip()}\n    Current Task: {cur_task.strip()}\n    Result: {result.strip()}\n    Task List: {\", \".join([x.get_text().strip() for x in task_list])}\n    __________________________________\n    \"\"\"\n    if return_str:\n        return status_string\n    else:\n        print(status_string, flush=True)\n        return None", "\n\ndef log_current_status(\n    cur_task: str,\n    result: str,\n    completed_tasks_summary: str,\n    task_list: List[Document],\n    return_str: bool = False,\n) -> Optional[str]:\n    status_string = f\"\"\"\n    __________________________________\n    Completed Tasks Summary: {completed_tasks_summary.strip()}\n    Current Task: {cur_task.strip()}\n    Result: {result.strip()}\n    Task List: {\", \".join([x.get_text().strip() for x in task_list])}\n    __________________________________\n    \"\"\"\n    if return_str:\n        return status_string\n    else:\n        print(status_string, flush=True)\n        return None", ""]}
{"filename": "llama_agi/llama_agi/default_task_prompts.py", "chunked_list": ["#############################################\n##### AGI Prefix #####\n#############################################\nPREFIX = (\n    \"You are an autonomous artificial intelligence, capable of planning and executing tasks to achieve an objective.\\n\"\n    \"When given an objective, you can plan and execute any number tasks that will help achieve your original objective.\\n\"\n)\n\n\n#############################################", "\n#############################################\n##### Initial Completed Tasks Summary #####\n#############################################\nNO_COMPLETED_TASKS_SUMMARY = \"You haven't completed any tasks yet.\"\n\n\n#############################################\n##### Langchain - Execution Agent #####\n#############################################", "##### Langchain - Execution Agent #####\n#############################################\nLC_PREFIX = PREFIX + \"You have access to the following tools:\"\n\nLC_SUFFIX = (\n    \"This is your current objective: {objective}\\n\"\n    \"Take into account what you have already achieved: {completed_tasks_summary}\\n\"\n    \"Using your current objective, your previously completed tasks, and your available tools,\"\n    \"Complete the current task.\\n\"\n    \"Begin!\\n\"", "    \"Complete the current task.\\n\"\n    \"Begin!\\n\"\n    \"Task: {cur_task}\\n\"\n    \"Thought: {agent_scratchpad}\"\n)\n\n\n#############################################\n##### Langchain - Execution Chain #####\n#############################################", "##### Langchain - Execution Chain #####\n#############################################\nLC_EXECUTION_PROMPT = (\n    \"You are an AI who performs one task based on the following objective: {objective}\\n.\"\n    \"Take into account this summary of previously completed tasks: {completed_tasks_summary}\\n.\"\n    \"Your task: {task}\\n\"\n    \"Response: \"\n)\n\n", "\n\n#############################################\n##### LlamaIndex -- Task Creation #####\n#############################################\nDEFAULT_TASK_CREATE_TMPL = (\n    f\"{PREFIX}\"\n    \"Your current objective is as follows: {query_str}\\n\"\n    \"Most recently, you completed the task '{prev_task}', which had the result of '{prev_result}'. \"\n    \"A description of your current incomplete tasks are below: \\n\"", "    \"Most recently, you completed the task '{prev_task}', which had the result of '{prev_result}'. \"\n    \"A description of your current incomplete tasks are below: \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given the current objective, the current incomplete tasks, and the latest completed task, \"\n    \"create new tasks to be completed that do not overlap with incomplete tasks. \"\n    \"Return the tasks as an array.\"\n)\n# TASK_CREATE_PROMPT = QuestionAnswerPrompt(DEFAULT_TASK_CREATE_TMPL)", ")\n# TASK_CREATE_PROMPT = QuestionAnswerPrompt(DEFAULT_TASK_CREATE_TMPL)\n\nDEFAULT_REFINE_TASK_CREATE_TMPL = (\n    f\"{PREFIX}\"\n    \"Your current objective is as follows: {query_str}\\n\"\n    \"Most recently, you completed the task '{prev_task}', which had the result of '{prev_result}'. \"\n    \"A description of your current incomplete tasks are below: \\n\"\n    \"---------------------\\n\"\n    \"{context_msg}\"", "    \"---------------------\\n\"\n    \"{context_msg}\"\n    \"\\n---------------------\\n\"\n    \"Currently, you have created the following new tasks: {existing_answer}\"\n    \"Given the current objective, the current incomplete tasks, list of newly created tasks, and the latest completed task, \"\n    \"add new tasks to be completed that do not overlap with incomplete tasks. \"\n    \"Return the tasks as an array. If you have no more tasks to add, repeat the existing list of new tasks.\"\n)\n# REFINE_TASK_CREATE_PROMPT = RefinePrompt(DEFAULT_REFINE_TASK_CREATE_TMPL)\n", "# REFINE_TASK_CREATE_PROMPT = RefinePrompt(DEFAULT_REFINE_TASK_CREATE_TMPL)\n\n\n#############################################\n##### LlamaIndex -- Task Prioritization #####\n#############################################\nDEFAULT_TASK_PRIORITIZE_TMPL = (\n    f\"{PREFIX}\"\n    \"Your current objective is as follows: {query_str}\\n\"\n    \"A list of your current incomplete tasks are below: \\n\"", "    \"Your current objective is as follows: {query_str}\\n\"\n    \"A list of your current incomplete tasks are below: \\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given the current objective, prioritize the current list of tasks. \"\n    \"Do not remove or add any tasks. Return the results as a numbered list, like:\\n\"\n    \"#. First task\\n\"\n    \"#. Second task\\n\"\n    \"... continue until all tasks are prioritized. \"", "    \"#. Second task\\n\"\n    \"... continue until all tasks are prioritized. \"\n    \"Start the task list with number 1.\"\n)\n\nDEFAULT_REFINE_TASK_PRIORITIZE_TMPL = (\n    f\"{PREFIX}\"\n    \"Your current objective is as follows: {query_str}\\n\"\n    \"A list of additional incomplete tasks are below: \\n\"\n    \"---------------------\\n\"", "    \"A list of additional incomplete tasks are below: \\n\"\n    \"---------------------\\n\"\n    \"{context_msg}\"\n    \"\\n---------------------\\n\"\n    \"Currently, you also have the following list of prioritized tasks: {existing_answer}\"\n    \"Given the current objective and existing list, prioritize the current list of tasks. \"\n    \"Do not remove or add any tasks. Return the results as a numbered list, like:\\n\"\n    \"#. First task\\n\"\n    \"#. Second task\\n\"\n    \"... continue until all tasks are prioritized. \"", "    \"#. Second task\\n\"\n    \"... continue until all tasks are prioritized. \"\n    \"Start the task list with number 1.\"\n)\n"]}
{"filename": "llama_agi/llama_agi/tools/NoteTakingTools.py", "chunked_list": ["from langchain.agents import tool\nfrom llama_index import Document\nfrom llama_agi.utils import initialize_search_index\n\nnote_index = initialize_search_index([])\n\n\n@tool(\"Record Note\")\ndef record_note(note: str) -> str:\n    \"\"\"Useful for when you need to record a note or reminder for yourself to reference in the future.\"\"\"\n    global note_index\n    note_index.insert(Document(note))\n    return \"Note successfully recorded.\"", "def record_note(note: str) -> str:\n    \"\"\"Useful for when you need to record a note or reminder for yourself to reference in the future.\"\"\"\n    global note_index\n    note_index.insert(Document(note))\n    return \"Note successfully recorded.\"\n\n\n@tool(\"Search Notes\")\ndef search_notes(query_str: str) -> str:\n    \"\"\"Useful for searching through notes that you previously recorded.\"\"\"\n    global note_index\n    response = note_index.as_query_engine(\n        similarity_top_k=3,\n    ).query(query_str)\n    return str(response)", "def search_notes(query_str: str) -> str:\n    \"\"\"Useful for searching through notes that you previously recorded.\"\"\"\n    global note_index\n    response = note_index.as_query_engine(\n        similarity_top_k=3,\n    ).query(query_str)\n    return str(response)\n"]}
{"filename": "llama_agi/llama_agi/tools/WebpageSearchTool.py", "chunked_list": ["from langchain.agents import tool\nfrom llama_index import download_loader, ServiceContext\n\nfrom llama_agi.utils import initialize_search_index\n\nBeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n\n\n@tool(\"Search Webpage\")\ndef search_webpage(prompt: str) -> str:\n    \"\"\"Useful for searching a specific webpage. The input to the tool should be URL and query, separated by a newline.\"\"\"\n    loader = BeautifulSoupWebReader()\n    if len(prompt.split(\"\\n\")) < 2:\n        return \"The input to search_webpage should be a URL and a query, separated by a newline.\"\n\n    url = prompt.split(\"\\n\")[0]\n    query_str = \" \".join(prompt.split(\"\\n\")[1:])\n\n    try:\n        documents = loader.load_data(urls=[url])\n        service_context = ServiceContext.from_defaults(chunk_size_limit=512)\n        index = initialize_search_index(documents, service_context=service_context)\n        query_result = index.as_query_engine(similarity_top_k=3).query(query_str)\n        return str(query_result)\n    except ValueError as e:\n        return str(e)\n    except Exception:\n        return \"Encountered an error while searching the webpage.\"", "@tool(\"Search Webpage\")\ndef search_webpage(prompt: str) -> str:\n    \"\"\"Useful for searching a specific webpage. The input to the tool should be URL and query, separated by a newline.\"\"\"\n    loader = BeautifulSoupWebReader()\n    if len(prompt.split(\"\\n\")) < 2:\n        return \"The input to search_webpage should be a URL and a query, separated by a newline.\"\n\n    url = prompt.split(\"\\n\")[0]\n    query_str = \" \".join(prompt.split(\"\\n\")[1:])\n\n    try:\n        documents = loader.load_data(urls=[url])\n        service_context = ServiceContext.from_defaults(chunk_size_limit=512)\n        index = initialize_search_index(documents, service_context=service_context)\n        query_result = index.as_query_engine(similarity_top_k=3).query(query_str)\n        return str(query_result)\n    except ValueError as e:\n        return str(e)\n    except Exception:\n        return \"Encountered an error while searching the webpage.\"", ""]}
{"filename": "llama_agi/llama_agi/tools/__init__.py", "chunked_list": ["from .NoteTakingTools import record_note, search_notes\nfrom .WebpageSearchTool import search_webpage\n\n__all__ = [record_note, search_notes, search_webpage]\n"]}
{"filename": "llama_agi/llama_agi/task_manager/LlamaTaskManager.py", "chunked_list": ["import re\nimport json\nfrom typing import List, Tuple, Optional\n\nfrom llama_index import Document, ServiceContext\nfrom llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\n\nfrom llama_agi.task_manager.base import BaseTaskManager, LlamaTaskPrompts\nfrom llama_agi.utils import initialize_task_list_index\nfrom llama_agi.default_task_prompts import NO_COMPLETED_TASKS_SUMMARY", "from llama_agi.utils import initialize_task_list_index\nfrom llama_agi.default_task_prompts import NO_COMPLETED_TASKS_SUMMARY\n\n\nclass LlamaTaskManager(BaseTaskManager):\n    \"\"\"Llama Task Manager\n\n    This task manager uses LlamaIndex to create and prioritize tasks. Using\n    the LlamaTaskPrompts, the task manager will create tasks that work\n    towards achieving an overall objective.\n\n    New tasks are created based on the prev task+result, completed tasks summary,\n    and the overall objective.\n\n    Tasks are then prioritized using the overall objective and current list of tasks.\n\n    Args:\n        tasks (List[str]): The initial list of tasks to complete.\n        prompts: (LlamaTaskPrompts): The prompts to control the task creation\n        and prioritization.\n        tasK_service_context (ServiceContext): The LlamaIndex service context to use\n        for task creation and prioritization.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        tasks: List[str],\n        prompts: LlamaTaskPrompts = LlamaTaskPrompts(),\n        task_service_context: Optional[ServiceContext] = None,\n    ) -> None:\n        super().__init__(\n            tasks=tasks, prompts=prompts, task_service_context=task_service_context\n        )\n\n        self.current_tasks_index = initialize_task_list_index(\n            self.current_tasks, service_context=self.task_service_context\n        )\n        self.completed_tasks_index = initialize_task_list_index(\n            self.completed_tasks, service_context=self.task_service_context\n        )\n\n        self.task_create_qa_template = self.prompts.task_create_qa_template\n        self.task_create_refine_template = self.prompts.task_create_refine_template\n\n        self.task_prioritize_qa_template = self.prompts.task_prioritize_qa_template\n        self.task_prioritize_refine_template = (\n            self.prompts.task_prioritize_refine_template\n        )\n\n    def _get_task_create_templates(\n        self, prev_task: str, prev_result: str\n    ) -> Tuple[QuestionAnswerPrompt, RefinePrompt]:\n        \"\"\"Fetch the task create prompts as llama_index objects.\"\"\"\n        text_qa_template = self.task_create_qa_template.format(\n            prev_result=prev_result,\n            prev_task=prev_task,\n            query_str=\"{query_str}\",\n            context_str=\"{context_str}\",\n        )\n        llama_text_qa_template = QuestionAnswerPrompt(text_qa_template)\n\n        refine_template = self.task_create_refine_template.format(\n            prev_result=prev_result,\n            prev_task=prev_task,\n            query_str=\"{query_str}\",\n            context_msg=\"{context_msg}\",\n            existing_answer=\"{existing_answer}\",\n        )\n        llama_refine_template = RefinePrompt(refine_template)\n\n        return (llama_text_qa_template, llama_refine_template)\n\n    def _get_task_prioritize_templates(\n        self,\n    ) -> Tuple[QuestionAnswerPrompt, RefinePrompt]:\n        \"\"\"Fetch the task prioritize prompts as llama_index objects.\"\"\"\n        return (\n            QuestionAnswerPrompt(self.task_prioritize_qa_template),\n            RefinePrompt(self.task_prioritize_refine_template),\n        )\n\n    def parse_task_list(self, task_list_str: str) -> List[str]:\n        \"\"\"Parse new tasks generated by the agent.\"\"\"\n        new_tasks: List[str] = []\n        try:\n            new_tasks = json.loads(task_list_str)\n            new_tasks = [x.strip() for x in new_tasks if len(x.strip()) > 10]\n        except Exception:\n            new_tasks = str(task_list_str).split(\"\\n\")\n            new_tasks = [\n                re.sub(r\"^[0-9]+\\.\", \"\", x).strip()\n                for x in str(new_tasks)\n                if len(x.strip()) > 10 and x[0].isnumeric()\n            ]\n        return new_tasks\n\n    def get_completed_tasks_summary(self) -> str:\n        \"\"\"Generate a summary of completed tasks.\"\"\"\n        if len(self.completed_tasks) == 0:\n            return NO_COMPLETED_TASKS_SUMMARY\n        summary = self.completed_tasks_index.as_query_engine(\n            response_mode=\"tree_summarize\"\n        ).query(\n            \"Summarize the current completed tasks\",\n        )\n        return str(summary)\n\n    def prioritize_tasks(self, objective: str) -> None:\n        \"\"\"Prioritize the current list of incomplete tasks.\"\"\"\n        (text_qa_template, refine_template) = self._get_task_prioritize_templates()\n        prioritized_tasks = self.current_tasks_index.as_query_engine(\n            text_qa_template=text_qa_template, refine_template=refine_template\n        ).query(objective)\n\n        new_tasks = []\n        for task in str(prioritized_tasks).split(\"\\n\"):\n            task = re.sub(r\"^[0-9]+\\.\", \"\", task).strip()\n            if len(task) > 10:\n                new_tasks.append(task)\n        self.current_tasks = [Document(x) for x in new_tasks]\n        self.current_tasks_index = initialize_task_list_index(\n            self.current_tasks, service_context=self.task_service_context\n        )\n\n    def generate_new_tasks(\n        self, objective: str, prev_task: str, prev_result: str\n    ) -> None:\n        \"\"\"Generate new tasks given the previous task and result.\"\"\"\n        (text_qa_template, refine_template) = self._get_task_create_templates(\n            prev_task, prev_result\n        )\n        task_list_response = self.completed_tasks_index.as_query_engine(\n            text_qa_template=text_qa_template, refine_template=refine_template\n        ).query(objective)\n        new_tasks = self.parse_task_list(str(task_list_response))\n        self.add_new_tasks(new_tasks)\n\n    def get_next_task(self) -> str:\n        \"\"\"Get the next task to complete.\"\"\"\n        next_task = self.current_tasks.pop().get_text()\n        self.current_tasks_index = initialize_task_list_index(\n            self.current_tasks, service_context=self.task_service_context\n        )\n        return next_task\n\n    def add_new_tasks(self, tasks: List[str]) -> None:\n        \"\"\"Add new tasks to the task manager.\"\"\"\n        for task in tasks:\n            if task not in self.current_tasks and task not in self.completed_tasks:\n                self.current_tasks.append(Document(task))\n        self.current_tasks_index = initialize_task_list_index(\n            self.current_tasks, service_context=self.task_service_context\n        )\n\n    def add_completed_task(self, task: str, result: str) -> None:\n        \"\"\"Add a task as completed.\"\"\"\n        document = Document(f\"Task: {task}\\nResult: {result}\\n\")\n        self.completed_tasks.append(document)\n        self.completed_tasks_index = initialize_task_list_index(\n            self.completed_tasks, service_context=self.task_service_context\n        )", ""]}
{"filename": "llama_agi/llama_agi/task_manager/base.py", "chunked_list": ["from abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\nfrom llama_index import Document, ServiceContext\n\nfrom llama_agi.default_task_prompts import (\n    DEFAULT_TASK_PRIORITIZE_TMPL,\n    DEFAULT_REFINE_TASK_PRIORITIZE_TMPL,\n    DEFAULT_TASK_CREATE_TMPL,", "    DEFAULT_REFINE_TASK_PRIORITIZE_TMPL,\n    DEFAULT_TASK_CREATE_TMPL,\n    DEFAULT_REFINE_TASK_CREATE_TMPL,\n)\n\n\n@dataclass\nclass LlamaTaskPrompts:\n    task_create_qa_template: str = DEFAULT_TASK_CREATE_TMPL\n    task_create_refine_template: str = DEFAULT_REFINE_TASK_CREATE_TMPL\n    task_prioritize_qa_template: str = DEFAULT_TASK_PRIORITIZE_TMPL\n    task_prioritize_refine_template: str = DEFAULT_REFINE_TASK_PRIORITIZE_TMPL", "\n\nclass BaseTaskManager:\n    \"\"\"Base Task Manager\n\n    Args:\n        tasks (List[str]): The initial list of tasks to complete.\n        prompts: (LlamaTaskPrompts): The prompts to control the task creation\n        and prioritization.\n        tasK_service_context (ServiceContext): The LlamaIndex service context to use\n        for task creation and prioritization.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        tasks: List[str],\n        prompts: LlamaTaskPrompts = LlamaTaskPrompts(),\n        task_service_context: Optional[ServiceContext] = None,\n    ) -> None:\n        self.current_tasks = [Document(x) for x in tasks]\n        self.completed_tasks: List[Document] = []\n        self.prompts = prompts\n        self.task_service_context = task_service_context\n\n    @abstractmethod\n    def parse_task_list(self, task_list_str: str) -> List[str]:\n        \"\"\"Parse new tasks generated by the agent.\"\"\"\n\n    @abstractmethod\n    def get_completed_tasks_summary(self) -> str:\n        \"\"\"Generate a summary of completed tasks.\"\"\"\n\n    @abstractmethod\n    def prioritize_tasks(self, objective: str) -> None:\n        \"\"\"Prioritize the current list of incomplete tasks.\"\"\"\n\n    @abstractmethod\n    def generate_new_tasks(\n        self, objective: str, prev_task: str, prev_result: str\n    ) -> None:\n        \"\"\"Generate new tasks given the previous task and result.\"\"\"\n\n    @abstractmethod\n    def get_next_task(self) -> str:\n        \"\"\"Get the next task to complete.\"\"\"\n\n    @abstractmethod\n    def add_new_tasks(self, tasks: List[str]) -> None:\n        \"\"\"Add new tasks to the task manager.\"\"\"\n\n    @abstractmethod\n    def add_completed_task(self, task: str, result: str) -> None:\n        \"\"\"Add a task as completed.\"\"\"", ""]}
{"filename": "llama_agi/llama_agi/task_manager/__init__.py", "chunked_list": ["from .LlamaTaskManager import LlamaTaskManager\n\n__all__ = [\n    LlamaTaskManager,\n]\n"]}
{"filename": "llama_agi/llama_agi/execution_agent/base.py", "chunked_list": ["from abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom langchain.agents.tools import Tool\nfrom langchain.llms import OpenAI, BaseLLM\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.chat_models import ChatOpenAI\n\nfrom llama_agi.default_task_prompts import LC_PREFIX, LC_SUFFIX, LC_EXECUTION_PROMPT", "\nfrom llama_agi.default_task_prompts import LC_PREFIX, LC_SUFFIX, LC_EXECUTION_PROMPT\n\n\n@dataclass\nclass LlamaAgentPrompts:\n    execution_prompt: str = LC_EXECUTION_PROMPT\n    agent_prefix: str = LC_PREFIX\n    agent_suffix: str = LC_SUFFIX\n", "\n\nclass BaseExecutionAgent:\n    \"\"\"Base Execution Agent\n\n    Args:\n        llm (Union[BaseLLM, BaseChatModel]): The langchain LLM class to use.\n        model_name: (str): The name of the OpenAI model to use, if the LLM is\n        not provided.\n        max_tokens: (int): The maximum number of tokens the LLM can generate.\n        prompts: (LlamaAgentPrompts): The prompt templates used during execution.\n        tools: (List[Tool]): The list of langchain tools for the execution\n        agent to use.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[Union[BaseLLM, BaseChatModel]] = None,\n        model_name: str = \"text-davinci-003\",\n        max_tokens: int = 512,\n        prompts: LlamaAgentPrompts = LlamaAgentPrompts(),\n        tools: Optional[List[Tool]] = None,\n    ) -> None:\n        if llm:\n            self._llm = llm\n        elif model_name == \"text-davinci-003\":\n            self._llm = OpenAI(\n                temperature=0, model_name=model_name, max_tokens=max_tokens\n            )\n        else:\n            self._llm = ChatOpenAI(\n                temperature=0, model_name=model_name, max_tokens=max_tokens\n            )\n        self.max_tokens = max_tokens\n        self.prompts = prompts\n        self.tools = tools if tools else []\n\n    @abstractmethod\n    def execute_task(self, **prompt_kwargs: Any) -> Dict[str, str]:\n        \"\"\"Execute a task.\"\"\"", ""]}
{"filename": "llama_agi/llama_agi/execution_agent/ToolExecutionAgent.py", "chunked_list": ["from typing import Any, Dict, List, Optional, Union\nfrom string import Formatter\n\nfrom langchain.agents import AgentExecutor, ZeroShotAgent\nfrom langchain.agents.tools import Tool\nfrom langchain.chains import LLMChain\nfrom langchain.llms import BaseLLM\nfrom langchain.chat_models.base import BaseChatModel\n\nfrom llama_agi.execution_agent.base import BaseExecutionAgent, LlamaAgentPrompts", "\nfrom llama_agi.execution_agent.base import BaseExecutionAgent, LlamaAgentPrompts\n\n\nclass ToolExecutionAgent(BaseExecutionAgent):\n    \"\"\"Tool Execution Agent\n\n    This agent is a wrapper around the zero-shot agent from Langchain. Using\n    a set of tools, the agent is expected to carry out and complete some task\n    that will help achieve an overall objective.\n\n    The agents overall behavior is controlled by the LlamaAgentPrompts.agent_prefix\n    and LlamaAgentPrompts.agent_suffix prompt templates.\n\n    The execution template kwargs are automatically extracted and expected to be\n    specified in execute_task().\n\n    execute_task() also returns the intermediate steps, for additional debugging and is\n    used for the streamlit example.\n\n    Args:\n        llm (Union[BaseLLM, BaseChatModel]): The langchain LLM class to use.\n        model_name: (str): The name of the OpenAI model to use, if the LLM is\n        not provided.\n        max_tokens: (int): The maximum number of tokens the LLM can generate.\n        prompts: (LlamaAgentPrompts): The prompt templates used during execution.\n        The Tool Execution Agent uses LlamaAgentPrompts.agent_prefix and\n        LlamaAgentPrompts.agent_suffix.\n        tools: (List[Tool]): The list of langchain tools for the execution agent to use.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[Union[BaseLLM, BaseChatModel]] = None,\n        model_name: str = \"text-davinci-003\",\n        max_tokens: int = 512,\n        prompts: LlamaAgentPrompts = LlamaAgentPrompts(),\n        tools: Optional[List[Tool]] = None,\n    ) -> None:\n        super().__init__(\n            llm=llm,\n            model_name=model_name,\n            max_tokens=max_tokens,\n            prompts=prompts,\n            tools=tools,\n        )\n        self.agent_prefix = self.prompts.agent_prefix\n        self.agent_suffix = self.prompts.agent_suffix\n\n        # create the agent\n        input_variables = [\n            fn for _, fn, _, _ in Formatter().parse(self.agent_prefix) if fn is not None\n        ] + [\n            fn for _, fn, _, _ in Formatter().parse(self.agent_suffix) if fn is not None\n        ]\n        self._agent_prompt = ZeroShotAgent.create_prompt(\n            self.tools,\n            prefix=self.agent_prefix,\n            suffix=self.agent_suffix,\n            input_variables=input_variables,\n        )\n        self._llm_chain = LLMChain(llm=self._llm, prompt=self._agent_prompt)\n        self._agent = ZeroShotAgent(\n            llm_chain=self._llm_chain, tools=self.tools, verbose=True\n        )\n        self._execution_chain = AgentExecutor.from_agent_and_tools(\n            agent=self._agent,\n            tools=self.tools,\n            verbose=True,\n            return_intermediate_steps=True,\n        )\n\n    def execute_task(self, **prompt_kwargs: Any) -> Dict[str, str]:\n        \"\"\"Execute a task, using tools.\"\"\"\n        result = self._execution_chain(prompt_kwargs)\n        return result", ""]}
{"filename": "llama_agi/llama_agi/execution_agent/SimpleExecutionAgent.py", "chunked_list": ["from typing import Any, Dict, List, Optional, Union\nfrom string import Formatter\n\nfrom langchain.agents.tools import Tool\nfrom langchain.chains import LLMChain\nfrom langchain.llms import BaseLLM\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.prompts import PromptTemplate\n\nfrom llama_agi.execution_agent.base import BaseExecutionAgent, LlamaAgentPrompts", "\nfrom llama_agi.execution_agent.base import BaseExecutionAgent, LlamaAgentPrompts\n\n\nclass SimpleExecutionAgent(BaseExecutionAgent):\n    \"\"\"Simple Execution Agent\n\n    This agent uses an LLM to execute a basic action without tools.\n    The LlamaAgentPrompts.execution_prompt defines how this execution agent\n    behaves.\n\n    Usually, this is used for simple tasks, like generating the initial list of tasks.\n\n    The execution template kwargs are automatically extracted and expected to be\n    specified in execute_task().\n\n    Args:\n        llm (Union[BaseLLM, BaseChatModel]): The langchain LLM class to use.\n        model_name: (str): The name of the OpenAI model to use, if the LLM is\n        not provided.\n        max_tokens: (int): The maximum number of tokens the LLM can generate.\n        prompts: (LlamaAgentPrompts): The prompt templates used during execution.\n        The only prompt used byt the SimpleExecutionAgent is\n        LlamaAgentPrompts.execution_prompt.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[Union[BaseLLM, BaseChatModel]] = None,\n        model_name: str = \"text-davinci-003\",\n        max_tokens: int = 512,\n        prompts: LlamaAgentPrompts = LlamaAgentPrompts(),\n        tools: Optional[List[Tool]] = None,\n    ) -> None:\n        super().__init__(\n            llm=llm,\n            model_name=model_name,\n            max_tokens=max_tokens,\n            prompts=prompts,\n            tools=tools,\n        )\n\n        self.execution_prompt = self.prompts.execution_prompt\n        input_variables = [\n            fn\n            for _, fn, _, _ in Formatter().parse(self.execution_prompt)\n            if fn is not None\n        ]\n        self._prompt_template = PromptTemplate(\n            template=self.execution_prompt,\n            input_variables=input_variables,\n        )\n        self._execution_chain = LLMChain(llm=self._llm, prompt=self._prompt_template)\n\n    def execute_task(self, **prompt_kwargs: Any) -> Dict[str, str]:\n        \"\"\"Execute a task.\"\"\"\n        result = self._execution_chain.predict(**prompt_kwargs)\n        return {\"output\": result}", ""]}
{"filename": "llama_agi/llama_agi/execution_agent/__init__.py", "chunked_list": ["from .SimpleExecutionAgent import SimpleExecutionAgent\nfrom .ToolExecutionAgent import ToolExecutionAgent\n\n__all__ = [SimpleExecutionAgent, ToolExecutionAgent]\n"]}
{"filename": "llama_agi/llama_agi/runners/AutoAGIRunner.py", "chunked_list": ["import time\nfrom typing import List, Optional\n\nfrom llama_agi.runners.base import BaseAGIRunner\nfrom llama_agi.execution_agent.SimpleExecutionAgent import SimpleExecutionAgent\nfrom llama_agi.utils import log_current_status\n\n\nclass AutoAGIRunner(BaseAGIRunner):\n    def run(\n        self,\n        objective: str,\n        initial_task: str,\n        sleep_time: int,\n        initial_task_list: Optional[List[str]] = None,\n    ) -> None:\n        # get initial list of tasks\n        if initial_task_list:\n            self.task_manager.add_new_tasks(initial_task_list)\n        else:\n            initial_completed_tasks_summary = (\n                self.task_manager.get_completed_tasks_summary()\n            )\n            initial_task_prompt = initial_task + \"\\nReturn the list as an array.\"\n\n            # create simple execution agent using current agent\n            simple_execution_agent = SimpleExecutionAgent(\n                llm=self.execution_agent._llm,\n                max_tokens=self.execution_agent.max_tokens,\n                prompts=self.execution_agent.prompts,\n            )\n            initial_task_list_result = simple_execution_agent.execute_task(\n                objective=objective,\n                task=initial_task_prompt,\n                completed_tasks_summary=initial_completed_tasks_summary,\n            )\n\n            initial_task_list = self.task_manager.parse_task_list(\n                initial_task_list_result[\"output\"]\n            )\n\n            # add tasks to the task manager\n            self.task_manager.add_new_tasks(initial_task_list)\n\n        # prioritize initial tasks\n        self.task_manager.prioritize_tasks(objective)\n\n        completed_tasks_summary = initial_completed_tasks_summary\n        while True:\n            # Get the next task\n            cur_task = self.task_manager.get_next_task()\n\n            # Execute current task\n            result = self.execution_agent.execute_task(\n                objective=objective,\n                cur_task=cur_task,\n                completed_tasks_summary=completed_tasks_summary,\n            )[\"output\"]\n\n            # store the task and result as completed\n            self.task_manager.add_completed_task(cur_task, result)\n\n            # generate new task(s), if needed\n            self.task_manager.generate_new_tasks(objective, cur_task, result)\n\n            # Summarize completed tasks\n            completed_tasks_summary = self.task_manager.get_completed_tasks_summary()\n\n            # log state of AGI to terminal\n            log_current_status(\n                cur_task,\n                result,\n                completed_tasks_summary,\n                self.task_manager.current_tasks,\n            )\n\n            # Quit the loop?\n            if len(self.task_manager.current_tasks) == 0:\n                print(\"Out of tasks! Objective Accomplished?\")\n                break\n\n            # wait a bit to let you read what's happening\n            time.sleep(sleep_time)", "class AutoAGIRunner(BaseAGIRunner):\n    def run(\n        self,\n        objective: str,\n        initial_task: str,\n        sleep_time: int,\n        initial_task_list: Optional[List[str]] = None,\n    ) -> None:\n        # get initial list of tasks\n        if initial_task_list:\n            self.task_manager.add_new_tasks(initial_task_list)\n        else:\n            initial_completed_tasks_summary = (\n                self.task_manager.get_completed_tasks_summary()\n            )\n            initial_task_prompt = initial_task + \"\\nReturn the list as an array.\"\n\n            # create simple execution agent using current agent\n            simple_execution_agent = SimpleExecutionAgent(\n                llm=self.execution_agent._llm,\n                max_tokens=self.execution_agent.max_tokens,\n                prompts=self.execution_agent.prompts,\n            )\n            initial_task_list_result = simple_execution_agent.execute_task(\n                objective=objective,\n                task=initial_task_prompt,\n                completed_tasks_summary=initial_completed_tasks_summary,\n            )\n\n            initial_task_list = self.task_manager.parse_task_list(\n                initial_task_list_result[\"output\"]\n            )\n\n            # add tasks to the task manager\n            self.task_manager.add_new_tasks(initial_task_list)\n\n        # prioritize initial tasks\n        self.task_manager.prioritize_tasks(objective)\n\n        completed_tasks_summary = initial_completed_tasks_summary\n        while True:\n            # Get the next task\n            cur_task = self.task_manager.get_next_task()\n\n            # Execute current task\n            result = self.execution_agent.execute_task(\n                objective=objective,\n                cur_task=cur_task,\n                completed_tasks_summary=completed_tasks_summary,\n            )[\"output\"]\n\n            # store the task and result as completed\n            self.task_manager.add_completed_task(cur_task, result)\n\n            # generate new task(s), if needed\n            self.task_manager.generate_new_tasks(objective, cur_task, result)\n\n            # Summarize completed tasks\n            completed_tasks_summary = self.task_manager.get_completed_tasks_summary()\n\n            # log state of AGI to terminal\n            log_current_status(\n                cur_task,\n                result,\n                completed_tasks_summary,\n                self.task_manager.current_tasks,\n            )\n\n            # Quit the loop?\n            if len(self.task_manager.current_tasks) == 0:\n                print(\"Out of tasks! Objective Accomplished?\")\n                break\n\n            # wait a bit to let you read what's happening\n            time.sleep(sleep_time)", ""]}
{"filename": "llama_agi/llama_agi/runners/base.py", "chunked_list": ["from abc import abstractmethod\nfrom typing import List, Optional\n\nfrom llama_agi.execution_agent.base import BaseExecutionAgent\nfrom llama_agi.task_manager.base import BaseTaskManager\n\n\nclass BaseAGIRunner:\n    def __init__(\n        self, task_manager: BaseTaskManager, execution_agent: BaseExecutionAgent\n    ) -> None:\n        self.task_manager = task_manager\n        self.execution_agent = execution_agent\n\n    @abstractmethod\n    def run(\n        self,\n        objective: str,\n        initial_task: str,\n        sleep_time: int,\n        initial_task_list: Optional[List[str]] = None,\n    ) -> None:\n        \"\"\"Run the task manager and execution agent in a loop.\"\"\"", ""]}
{"filename": "llama_agi/llama_agi/runners/__init__.py", "chunked_list": ["from .AutoAGIRunner import AutoAGIRunner\nfrom .AutoStreamlitAGIRunner import AutoStreamlitAGIRunner\n\n__all__ = [AutoAGIRunner, AutoStreamlitAGIRunner]\n"]}
{"filename": "llama_agi/llama_agi/runners/AutoStreamlitAGIRunner.py", "chunked_list": ["import json\nimport streamlit as st\nimport time\nfrom typing import List, Optional\n\nfrom llama_agi.runners.base import BaseAGIRunner\nfrom llama_agi.execution_agent.SimpleExecutionAgent import SimpleExecutionAgent\nfrom llama_agi.utils import log_current_status\n\n\ndef make_intermediate_steps_pretty(json_str: str) -> List[str]:\n    steps = json.loads(json_str)\n    output = []\n    for action_set in steps:\n        for step in action_set:\n            if isinstance(step, list):\n                output.append(step[-1])\n            else:\n                output.append(step)\n    return output", "\n\ndef make_intermediate_steps_pretty(json_str: str) -> List[str]:\n    steps = json.loads(json_str)\n    output = []\n    for action_set in steps:\n        for step in action_set:\n            if isinstance(step, list):\n                output.append(step[-1])\n            else:\n                output.append(step)\n    return output", "\n\nclass AutoStreamlitAGIRunner(BaseAGIRunner):\n    def run(\n        self,\n        objective: str,\n        initial_task: str,\n        sleep_time: int,\n        initial_task_list: Optional[List[str]] = None,\n        max_iterations: Optional[int] = None,\n    ) -> None:\n\n        run_initial_task = False\n        if \"logs\" not in st.session_state:\n            st.session_state[\"logs\"] = []\n            st.session_state[\"state_str\"] = \"No state yet!\"\n            st.session_state[\"tasks_summary\"] = \"\"\n            run_initial_task = True\n\n        logs_col, state_col = st.columns(2)\n\n        with logs_col:\n            st.subheader(\"Execution Log\")\n            st_logs = st.empty()\n        st_logs.write(st.session_state[\"logs\"])\n\n        with state_col:\n            st.subheader(\"AGI State\")\n            st_state = st.empty()\n        st_state.write(st.session_state[\"state_str\"])\n\n        if run_initial_task:\n            # get initial list of tasks\n            if initial_task_list:\n                self.task_manager.add_new_tasks(initial_task_list)\n            else:\n                initial_completed_tasks_summary = (\n                    self.task_manager.get_completed_tasks_summary()\n                )\n                initial_task_prompt = initial_task + \"\\nReturn the list as an array.\"\n\n                # create simple execution agent using current agent\n                simple_execution_agent = SimpleExecutionAgent(\n                    llm=self.execution_agent._llm,\n                    max_tokens=self.execution_agent.max_tokens,\n                    prompts=self.execution_agent.prompts,\n                )\n                initial_task_list_result = simple_execution_agent.execute_task(\n                    objective=objective,\n                    task=initial_task_prompt,\n                    completed_tasks_summary=initial_completed_tasks_summary,\n                )\n\n                initial_task_list = self.task_manager.parse_task_list(\n                    initial_task_list_result[\"output\"]\n                )\n\n                # add tasks to the task manager\n                self.task_manager.add_new_tasks(initial_task_list)\n\n            # prioritize initial tasks\n            self.task_manager.prioritize_tasks(objective)\n\n            tasks_summary = initial_completed_tasks_summary\n            st.session_state[\"tasks_summary\"] = tasks_summary\n\n            # update streamlit state\n            st.session_state[\"state_str\"] = log_current_status(\n                initial_task,\n                initial_task_list_result[\"output\"],\n                tasks_summary,\n                self.task_manager.current_tasks,\n                return_str=True,\n            )\n            if st.session_state[\"state_str\"]:\n                st_state.markdown(st.session_state[\"state_str\"].replace(\"\\n\", \"\\n\\n\"))\n\n        for _ in range(0, max_iterations):\n            # Get the next task\n            cur_task = self.task_manager.get_next_task()\n\n            # Execute current task\n            result_dict = self.execution_agent.execute_task(\n                objective=objective,\n                cur_task=cur_task,\n                completed_tasks_summary=st.session_state[\"tasks_summary\"],\n            )\n            result = result_dict[\"output\"]\n\n            # update logs\n            log = make_intermediate_steps_pretty(\n                json.dumps(result_dict[\"intermediate_steps\"])\n            ) + [result]\n            st.session_state[\"logs\"].append(log)\n            st_logs.write(st.session_state[\"logs\"])\n\n            # store the task and result as completed\n            self.task_manager.add_completed_task(cur_task, result)\n\n            # generate new task(s), if needed\n            self.task_manager.generate_new_tasks(objective, cur_task, result)\n\n            # Summarize completed tasks\n            completed_tasks_summary = self.task_manager.get_completed_tasks_summary()\n            st.session_state[\"tasks_summary\"] = completed_tasks_summary\n\n            # log state of AGI to streamlit\n            st.session_state[\"state_str\"] = log_current_status(\n                cur_task,\n                result,\n                completed_tasks_summary,\n                self.task_manager.current_tasks,\n                return_str=True,\n            )\n            if st.session_state[\"state_str\"] is not None:\n                st_state.markdown(st.session_state[\"state_str\"].replace(\"\\n\", \"\\n\\n\"))\n\n            # Quit the loop?\n            if len(self.task_manager.current_tasks) == 0:\n                st.success(\"Out of tasks! Objective Accomplished?\")\n                break\n\n            # wait a bit to let you read what's happening\n            time.sleep(sleep_time)", ""]}
{"filename": "llama_agi/examples/streamlit_runner_example.py", "chunked_list": ["import os\nimport streamlit as st\nfrom langchain.agents import load_tools\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\n\nfrom llama_agi.execution_agent import ToolExecutionAgent\nfrom llama_agi.runners import AutoStreamlitAGIRunner\nfrom llama_agi.task_manager import LlamaTaskManager\n", "from llama_agi.task_manager import LlamaTaskManager\n\nfrom llama_index import ServiceContext, LLMPredictor\n\n\nst.set_page_config(layout=\"wide\")\nst.header(\"\ud83e\udd16 Llama AGI \ud83e\udd99\")\nst.markdown(\"This demo uses the [llama-agi](https://github.com/run-llama/llama-lab/tree/main/llama_agi) package to create an AutoGPT-like agent, powered by [LlamaIndex](https://github.com/jerryjliu/llama_index) and Langchain. The AGI has access to tools that search the web and record notes, as it works to achieve an objective. Use the setup tab to configure your LLM settings and initial objective+tasks. Then use the Launch tab to run the AGI. Kill the AGI by refreshing the page.\")\n\nsetup_tab, launch_tab = st.tabs([\"Setup\", \"Launch\"])", "\nsetup_tab, launch_tab = st.tabs([\"Setup\", \"Launch\"])\n\nwith setup_tab:\n    if 'init' in st.session_state:\n        st.success(\"Initialized!\")\n\n    st.subheader(\"LLM Setup\")\n    col1, col2, col3 = st.columns(3)\n\n    with col1:\n        openai_api_key = st.text_input(\"Enter your OpenAI API key here\", type=\"password\")\n        llm_name = st.selectbox(\n            \"Which LLM?\", [\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-4\"]\n        )\n\n    with col2:\n        google_api_key = st.text_input(\"Enter your Google API key here\", type=\"password\")\n        model_temperature = st.slider(\n            \"LLM Temperature\", min_value=0.0, max_value=1.0, step=0.1, value=0.0\n        )\n    \n    with col3:\n        google_cse_id = st.text_input(\"Enter your Google CSE ID key here\", type=\"password\")\n        max_tokens = st.slider(\n            \"LLM Max Tokens\", min_value=256, max_value=1024, step=8, value=512\n        )\n\n    st.subheader(\"AGI Setup\")\n    objective = st.text_input(\"Objective:\", value=\"Solve world hunger\")\n    initial_task = st.text_input(\"Initial Task:\", value=\"Create a list of tasks\")\n    max_iterations = st.slider(\"Iterations until pause\", value=1, min_value=1, max_value=10, step=1)\n\n    if st.button('Initialize?'):\n        os.environ['OPENAI_API_KEY'] = openai_api_key\n        os.environ['GOOGLE_API_KEY'] = google_api_key\n        os.environ['GOOGLE_CSE_ID'] = google_cse_id\n        if llm_name == \"text-davinci-003\":\n            llm = OpenAI(\n                temperature=model_temperature, model_name=llm_name, max_tokens=max_tokens\n            )\n        else:\n            llm= ChatOpenAI(\n                temperature=model_temperature, model_name=llm_name, max_tokens=max_tokens\n            )\n        \n        service_context = ServiceContext.from_defaults(\n            llm_predictor=LLMPredictor(llm=llm), chunk_size_limit=512\n        )\n\n        st.session_state['task_manager'] = LlamaTaskManager(\n            [initial_task], task_service_context=service_context\n        )\n\n        from llama_agi.tools import search_notes, record_note, search_webpage\n        tools = load_tools([\"google-search-results-json\"])\n        tools = tools + [search_notes, record_note, search_webpage]\n        st.session_state['execution_agent'] = ToolExecutionAgent(llm=llm, tools=tools)\n\n        st.session_state['initial_task'] = initial_task\n        st.session_state['objective'] = objective\n\n        st.session_state['init'] = True\n        st.experimental_rerun()", "\nwith launch_tab:\n    st.subheader(\"AGI Status\")\n    if st.button(f\"Continue for {max_iterations} Steps\"):\n        if st.session_state.get('init', False):\n            # launch the auto runner\n            with st.spinner(\"Running!\"):\n                runner = AutoStreamlitAGIRunner(st.session_state['task_manager'], st.session_state['execution_agent'])\n                runner.run(st.session_state['objective'], st.session_state['initial_task'], 2, max_iterations=max_iterations)\n", "\n"]}
{"filename": "llama_agi/examples/auto_runner_example.py", "chunked_list": ["import argparse\nfrom langchain.agents import load_tools\nfrom langchain.llms import OpenAI\n\nfrom llama_agi.execution_agent import ToolExecutionAgent\nfrom llama_agi.runners import AutoAGIRunner\nfrom llama_agi.task_manager import LlamaTaskManager\nfrom llama_agi.tools import search_notes, record_note, search_webpage\n\nfrom llama_index import ServiceContext, LLMPredictor", "\nfrom llama_index import ServiceContext, LLMPredictor\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        prog=\"Llama AGI\",\n        description=\"A baby-agi/auto-gpt inspired application, powered by Llama Index!\",\n    )\n    parser.add_argument(\n        \"-it\",\n        \"--initial-task\",\n        default=\"Create a list of tasks\",\n        help=\"The initial task for the system to carry out. Default='Create a list of tasks'\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--objective\",\n        default=\"Solve world hunger\",\n        help=\"The overall objective for the system. Default='Solve world hunger'\",\n    )\n    parser.add_argument(\n        \"--sleep-time\",\n        default=2,\n        help=\"Sleep time (in seconds) between each task loop. Default=2\",\n        type=int,\n    )\n\n    args = parser.parse_args()\n\n    # LLM setup\n    llm = OpenAI(temperature=0, model_name=\"text-davinci-003\")\n    service_context = ServiceContext.from_defaults(\n        llm_predictor=LLMPredictor(llm=llm), chunk_size_limit=512\n    )\n\n    # llama_agi setup\n    task_manager = LlamaTaskManager(\n        [args.initial_task], task_service_context=service_context\n    )\n\n    tools = load_tools([\"google-search-results-json\"])\n    tools = tools + [search_notes, record_note, search_webpage]\n    execution_agent = ToolExecutionAgent(llm=llm, tools=tools)\n\n    # launch the auto runner\n    runner = AutoAGIRunner(task_manager, execution_agent)\n    runner.run(args.objective, args.initial_task, args.sleep_time)", ""]}
{"filename": "auto_llama/auto_llama/actions.py", "chunked_list": ["import json\nimport os\n\nfrom duckduckgo_search import ddg\nfrom llama_index.readers.web import BeautifulSoupWebReader\nfrom llama_index import GPTListIndex\nfrom auto_llama.data_models import Response\nfrom typing import Dict\nfrom auto_llama.const import SEARCH_RESULTS_TEMPLATE, format_web_download\nfrom llama_index import Document", "from auto_llama.const import SEARCH_RESULTS_TEMPLATE, format_web_download\nfrom llama_index import Document\nfrom llama_index.indices.composability import ComposableGraph\nfrom llama_index import GPTListIndex, LLMPredictor, ServiceContext\nfrom langchain.llms.base import BaseLLM\nfrom llama_index.logger import LlamaLogger\n\n\ndef run_command(user_query: str, command: str, args: Dict, llm: BaseLLM) -> str:\n    llama_logger = LlamaLogger()\n    service_context = ServiceContext.from_defaults(\n        llm_predictor=LLMPredictor(llm), llama_logger=llama_logger\n    )\n    if command == \"search\":\n        search_terms = args[\"search_terms\"]\n        print(\"Searching...\\n\")\n        results = search_web(search_terms)\n        response = analyze_search_results(\n            user_query, search_terms, results, service_context\n        )\n        print(response + \"\\n\")\n        return response\n    elif command == \"download\":\n        url = args[\"url\"]\n        doc_name = args[\"doc_name\"]\n        print(\"Downloading web page...\\n\")\n        if isinstance(url, str) and \"[\" in url and \"]\" in url:  # list parsing case\n            url = url.strip(\"[\").strip(\"]\").split(\", \")\n            doc_name = doc_name.strip(\"[\").strip(\"]\").split(\", \")\n        if isinstance(url, list):\n            if len(url) != len(doc_name):\n                raise ValueError(\"url and doc_name must have the same length\")\n            results = []\n            if os.path.exists(\"data/web_summary_cache.json\"):\n                with open(\"data/web_summary_cache.json\", \"r\") as f:\n                    web_summary_cache = json.load(f)\n            else:\n                web_summary_cache = {}\n            for i in range(len(url)):\n                web_summary = download_web(url[i], doc_name[i], service_context)\n                results.append(format_web_download(url[i], doc_name[i], web_summary))\n                web_summary_cache[doc_name[i]] = web_summary\n            print(\"Writing web summary cache to file\")\n\n            with open(\"data/web_summary_cache.json\", \"w\") as f:\n                json.dump(web_summary_cache, f)\n            response = \"\\n\".join(results)\n            print(response)\n            return response\n        else:\n            if os.path.exists(\"data/web_summary_cache.json\"):\n                with open(\"data/web_summary_cache.json\", \"r\") as f:\n                    web_summary_cache = json.load(f)\n            else:\n                web_summary_cache = {}\n            web_summary = download_web(url, doc_name, service_context)\n            web_summary_cache[doc_name] = web_summary\n            print(\"Writing web summary cache to file\")\n\n            with open(\"data/web_summary_cache.json\", \"w\") as f:\n                json.dump(web_summary_cache, f)\n            response = format_web_download(url, doc_name, web_summary)\n            print(response)\n            return response\n    elif command == \"query\":\n        print(\"Querying...\\n\")\n        response = query_docs(args[\"docs\"], args[\"query\"], service_context)\n        print(response)\n        return response\n    elif command == \"write\":\n        print(\"Writing to file...\\n\")\n        return write_to_file(args[\"file_name\"], args[\"data\"])\n    elif command == \"exit\":\n        print(\"Exiting...\\n\")\n        return \"exit\"\n    else:\n        raise ValueError(f\"Unknown command: {command}\")", "def run_command(user_query: str, command: str, args: Dict, llm: BaseLLM) -> str:\n    llama_logger = LlamaLogger()\n    service_context = ServiceContext.from_defaults(\n        llm_predictor=LLMPredictor(llm), llama_logger=llama_logger\n    )\n    if command == \"search\":\n        search_terms = args[\"search_terms\"]\n        print(\"Searching...\\n\")\n        results = search_web(search_terms)\n        response = analyze_search_results(\n            user_query, search_terms, results, service_context\n        )\n        print(response + \"\\n\")\n        return response\n    elif command == \"download\":\n        url = args[\"url\"]\n        doc_name = args[\"doc_name\"]\n        print(\"Downloading web page...\\n\")\n        if isinstance(url, str) and \"[\" in url and \"]\" in url:  # list parsing case\n            url = url.strip(\"[\").strip(\"]\").split(\", \")\n            doc_name = doc_name.strip(\"[\").strip(\"]\").split(\", \")\n        if isinstance(url, list):\n            if len(url) != len(doc_name):\n                raise ValueError(\"url and doc_name must have the same length\")\n            results = []\n            if os.path.exists(\"data/web_summary_cache.json\"):\n                with open(\"data/web_summary_cache.json\", \"r\") as f:\n                    web_summary_cache = json.load(f)\n            else:\n                web_summary_cache = {}\n            for i in range(len(url)):\n                web_summary = download_web(url[i], doc_name[i], service_context)\n                results.append(format_web_download(url[i], doc_name[i], web_summary))\n                web_summary_cache[doc_name[i]] = web_summary\n            print(\"Writing web summary cache to file\")\n\n            with open(\"data/web_summary_cache.json\", \"w\") as f:\n                json.dump(web_summary_cache, f)\n            response = \"\\n\".join(results)\n            print(response)\n            return response\n        else:\n            if os.path.exists(\"data/web_summary_cache.json\"):\n                with open(\"data/web_summary_cache.json\", \"r\") as f:\n                    web_summary_cache = json.load(f)\n            else:\n                web_summary_cache = {}\n            web_summary = download_web(url, doc_name, service_context)\n            web_summary_cache[doc_name] = web_summary\n            print(\"Writing web summary cache to file\")\n\n            with open(\"data/web_summary_cache.json\", \"w\") as f:\n                json.dump(web_summary_cache, f)\n            response = format_web_download(url, doc_name, web_summary)\n            print(response)\n            return response\n    elif command == \"query\":\n        print(\"Querying...\\n\")\n        response = query_docs(args[\"docs\"], args[\"query\"], service_context)\n        print(response)\n        return response\n    elif command == \"write\":\n        print(\"Writing to file...\\n\")\n        return write_to_file(args[\"file_name\"], args[\"data\"])\n    elif command == \"exit\":\n        print(\"Exiting...\\n\")\n        return \"exit\"\n    else:\n        raise ValueError(f\"Unknown command: {command}\")", "\n\ndef search_web(search_terms, max_results=5):\n    \"\"\"Search the Web and obtain a list of web results.\"\"\"\n    results = ddg(search_terms, max_results=max_results)\n    return results\n\n\ndef analyze_search_results(user_query, search_terms, results, service_context):\n    \"\"\"Analyze the results of the search using llm.\"\"\"\n    doc = Document(json.dumps(results))\n    index = GPTListIndex.from_documents([doc], service_context=service_context)\n    response = index.query(\n        SEARCH_RESULTS_TEMPLATE.format(search_terms=search_terms, user_query=user_query)\n    )\n    return response.response", "def analyze_search_results(user_query, search_terms, results, service_context):\n    \"\"\"Analyze the results of the search using llm.\"\"\"\n    doc = Document(json.dumps(results))\n    index = GPTListIndex.from_documents([doc], service_context=service_context)\n    response = index.query(\n        SEARCH_RESULTS_TEMPLATE.format(search_terms=search_terms, user_query=user_query)\n    )\n    return response.response\n\n\ndef download_web(url: str, doc_name: str, service_context: ServiceContext):\n    \"\"\"Download the html of the url and save a reference under doc_name.\n    Return the summary of the web page.\n    \"\"\"\n    reader = BeautifulSoupWebReader()\n    docs = reader.load_data([url])\n    index = GPTListIndex.from_documents(docs, service_context=service_context)\n    if not os.path.exists(\"data\"):\n        os.mkdir(\"data\")\n    index.save_to_disk(\"data/\" + doc_name + \".json\")\n    summary = index.query(\n        \"Summarize the contents of this web page.\", response_mode=\"tree_summarize\", use_async=True\n    )\n    return summary.response", "\n\ndef download_web(url: str, doc_name: str, service_context: ServiceContext):\n    \"\"\"Download the html of the url and save a reference under doc_name.\n    Return the summary of the web page.\n    \"\"\"\n    reader = BeautifulSoupWebReader()\n    docs = reader.load_data([url])\n    index = GPTListIndex.from_documents(docs, service_context=service_context)\n    if not os.path.exists(\"data\"):\n        os.mkdir(\"data\")\n    index.save_to_disk(\"data/\" + doc_name + \".json\")\n    summary = index.query(\n        \"Summarize the contents of this web page.\", response_mode=\"tree_summarize\", use_async=True\n    )\n    return summary.response", "\n\ndef query_docs(docs, query, service_context):\n    query_configs = [\n        {\n            \"index_struct_type\": \"list\",\n            \"query_mode\": \"default\",\n            \"query_kwargs\": {\"response_mode\": \"tree_summarize\", \"use_async\": True},\n        }\n    ]\n    print(\"Opening web summary cache\")\n    with open(\"data/web_summary_cache.json\", \"r\") as f:\n        doc_summary_cache = json.load(f)\n    if isinstance(docs, list):\n        indices = []\n        for doc_name in docs:\n            index = GPTListIndex.load_from_disk(\n                \"data/\" + doc_name + \".json\", service_context=service_context\n            )\n            indices.append((index, doc_summary_cache[doc_name]))\n        graph = ComposableGraph.from_indices(\n            GPTListIndex,\n            [index[0] for index in indices],\n            index_summaries=[index[1] for index in indices],\n            service_context=service_context,\n        )\n        response = graph.query(\n            query, query_configs=query_configs, service_context=service_context\n        )\n        return response.response\n    else:\n        index = GPTListIndex.load_from_disk(\n            \"data/\" + docs + \".json\", service_context=service_context\n        )\n        response = index.query(query, service_context=service_context)\n        return response.response", "\n\ndef write_to_file(file_name, data):\n    print(\"Writing to file\" + file_name)\n    with open(file_name, \"w\") as f:\n        f.write(data)\n    return \"done\"\n"]}
{"filename": "auto_llama/auto_llama/data_models.py", "chunked_list": ["from pydantic import BaseModel, Field, root_validator\nfrom typing import Dict, Union, List\nimport json\n\n\nclass Command(BaseModel):\n    action: str = Field(description=\"This is the current action\")\n    args: Dict = Field(description=\"This is the command's arguments\")\n\n    @root_validator\n    def validate_all(cls, values):\n        # print(f\"{values}\")\n        if values[\"action\"] == \"search\" and \"search_terms\" not in values[\"args\"]:\n            raise ValueError(\"malformed search args\")\n        if values[\"action\"] == \"download\" and (\n            \"url\" not in values[\"args\"] or \"doc_name\" not in values[\"args\"]\n        ):\n            raise ValueError(\"malformed download args\")\n        if values[\"action\"] == \"query\" and (\n            \"docs\" not in values[\"args\"] or \"query\" not in values[\"args\"]\n        ):\n            raise ValueError(\"malformed query args\")\n        if values[\"action\"] == \"write\" and (\n            \"file_name\" not in values[\"args\"] or \"data\" not in values[\"args\"]\n        ):\n            raise ValueError(\"malformed write args\")\n        return values\n\n    def toJSON(self):\n        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)", "\n\nclass Response(BaseModel):\n    remember: str = Field(description=\"This is what the AI just accomplished. Probably should not do it again\")\n    thoughts: str = Field(description=\"This what the AI is currently thinking.\")\n    reasoning: str = Field(\n        description=\"This is why the AI thinks it will help lead to the user's desired result\"\n    )\n    plan: Union[str, object] = Field(\n        description=\"This is the AI's current plan of action\"\n    )\n    command: Command = Field(description=\"This is the AI's current command\")", ""]}
{"filename": "auto_llama/auto_llama/__main__.py", "chunked_list": ["import json\nfrom auto_llama.agent import Agent\nimport auto_llama.const as const\nfrom auto_llama.utils import print_pretty\nfrom auto_llama.actions import run_command\nfrom langchain.chat_models import ChatOpenAI\n\nimport logging\n\n\ndef main():\n    logger = logging.getLogger()\n    logger.level = logging.WARN\n    # # Enter your OpenAI API key here:\n    # import os\n    # os.environ[\"OPENAI_API_KEY\"] = 'YOUR OPENAI API KEY'\n\n    openaichat = ChatOpenAI(\n        model_name=\"gpt-4\",\n        temperature=0.0,\n        max_tokens=400,\n    )\n\n    user_query = input(\"Enter what you would like AutoLlama to do:\\n\")\n    if user_query == \"\":\n        user_query = \"Summarize the financial news from the past week.\"\n        print(\"I will summarize the financial news from the past week.\\n\")\n    agent = Agent(const.DEFAULT_AGENT_PREAMBLE, user_query, openaichat)\n    while True:\n        print(\"Thinking...\")\n        response = agent.get_response()\n        print_pretty(response)\n        action, args = response.command.action, response.command.args\n        user_confirm = input(\n            'Should I run the command \"'\n            + action\n            + '\" with args '\n            + json.dumps(args)\n            + \"? (y/[N])\\n\"\n        )\n        if user_confirm == \"y\":\n            action_results = run_command(user_query, action, args, openaichat)\n            # print(action_results)\n            agent.memory.append(action_results)\n            if action_results == \"exit\" or action_results == \"done\":\n                break\n        else:\n            break", "\n\ndef main():\n    logger = logging.getLogger()\n    logger.level = logging.WARN\n    # # Enter your OpenAI API key here:\n    # import os\n    # os.environ[\"OPENAI_API_KEY\"] = 'YOUR OPENAI API KEY'\n\n    openaichat = ChatOpenAI(\n        model_name=\"gpt-4\",\n        temperature=0.0,\n        max_tokens=400,\n    )\n\n    user_query = input(\"Enter what you would like AutoLlama to do:\\n\")\n    if user_query == \"\":\n        user_query = \"Summarize the financial news from the past week.\"\n        print(\"I will summarize the financial news from the past week.\\n\")\n    agent = Agent(const.DEFAULT_AGENT_PREAMBLE, user_query, openaichat)\n    while True:\n        print(\"Thinking...\")\n        response = agent.get_response()\n        print_pretty(response)\n        action, args = response.command.action, response.command.args\n        user_confirm = input(\n            'Should I run the command \"'\n            + action\n            + '\" with args '\n            + json.dumps(args)\n            + \"? (y/[N])\\n\"\n        )\n        if user_confirm == \"y\":\n            action_results = run_command(user_query, action, args, openaichat)\n            # print(action_results)\n            agent.memory.append(action_results)\n            if action_results == \"exit\" or action_results == \"done\":\n                break\n        else:\n            break", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "auto_llama/auto_llama/agent.py", "chunked_list": ["from auto_llama.utils import get_date\nfrom langchain.output_parsers import PydanticOutputParser\nfrom auto_llama.data_models import Response\nfrom langchain.prompts import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import AIMessage\nfrom typing import List", "from langchain.schema import AIMessage\nfrom typing import List\nfrom auto_llama.tokens import count_tokens\n\n\nclass Agent:\n    \"\"\"A class representing an agent.\n\n    Attributes:\n        desc(str):\n            A description of the agent used in the preamble.\n        task(str):\n            The task the agent is supposed to perform.\n        memory(list):\n            A list of the agent's memories.\n        llm(BaseLLM):\n            The LLM used by the agent.\n    \"\"\"\n\n    def __init__(\n        self,\n        desc,\n        task,\n        llm,\n        memory=[],\n    ):\n        \"\"\"Initialize the agent.\"\"\"\n        self.desc = desc\n        self.task = task\n        self.memory = memory\n        self.llm = llm\n\n        memory.append(\"Here is a list of your previous actions:\")\n\n    def get_response(self) -> Response:\n        \"\"\"Get the response given the agent's current state.\"\"\"\n        parser: PydanticOutputParser = PydanticOutputParser(pydantic_object=Response)\n        format_instructions = parser.get_format_instructions()\n        llm_input = self.create_chat_messages(\n            self.desc, self.task, self.memory, format_instructions\n        ).to_messages()\n        # print(llm_input)\n        output: AIMessage = self.llm(llm_input)\n        # print(output.content)\n        self.memory.append(\"Old thought: \" + output.content)\n        response_obj = parser.parse(output.content)\n        # print(response_obj)\n        return response_obj\n\n    def create_chat_messages(\n        self, desc: str, task: str, memory: List[str], format_instructions: str\n    ):\n        \"\"\"Create the messages for the agent.\"\"\"\n        messages = []\n        system_template = \"{desc}\\n{memory}\\n{date}\\n{format_instructions}\"\n        system_message_prompt = SystemMessagePromptTemplate.from_template(\n            system_template\n        )\n        messages.append(system_message_prompt)\n\n        human_template = \"{text}\"\n        human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n        messages.append(human_message_prompt)\n\n        prompt_template = ChatPromptTemplate.from_messages(messages)\n\n        date_str = \"The current date is \" + get_date()\n        recent_memories = self.create_memories(memory)\n        # print(recent_memories)\n        prompt = prompt_template.format_prompt(\n            desc=desc,\n            memory=recent_memories,\n            date=date_str,\n            format_instructions=format_instructions,\n            text=task,\n        )\n\n        return prompt\n\n    def create_memories(self, memory: List[str], max_tokens: int = 2000):\n        # print(memory)\n        token_counter = 0\n        memories: List[str] = []\n        memories.insert(0, memory[0])  # always include memories header.\n        token_counter += count_tokens(memory[0])\n        memory_index = len(memory) - 1\n        while memory_index > 0 and token_counter < max_tokens:\n            memories.insert(1, memory[memory_index])\n            token_counter += count_tokens(memory[memory_index])\n            memory_index -= 1\n        return \"\\n\".join(memories)", ""]}
{"filename": "auto_llama/auto_llama/tokens.py", "chunked_list": ["import tiktoken\n\n\ndef count_tokens(input: str):\n    encoder = tiktoken.get_encoding(\"cl100k_base\")\n    return len(encoder.encode(input))\n"]}
{"filename": "auto_llama/auto_llama/__init__.py", "chunked_list": [""]}
{"filename": "auto_llama/auto_llama/utils.py", "chunked_list": ["import datetime\nimport json\nfrom auto_llama.data_models import Response\n\ndef get_date():\n    return datetime.datetime.now().strftime(\"%Y-%m-%d\")\n\n\ndef print_pretty(response: Response):\n    print(\"Thoughts: \" + response.thoughts + \"\\n\")\n    print(\"Remember: \" + response.remember + \"\\n\")\n    print(\"Reasoning: \" + response.reasoning + \"\\n\")\n    print(\"Plan: \" + json.dumps(response.plan) + \"\\n\")\n    print(\"Command: \" + response.command.toJSON() + \"\\n\")", "def print_pretty(response: Response):\n    print(\"Thoughts: \" + response.thoughts + \"\\n\")\n    print(\"Remember: \" + response.remember + \"\\n\")\n    print(\"Reasoning: \" + response.reasoning + \"\\n\")\n    print(\"Plan: \" + json.dumps(response.plan) + \"\\n\")\n    print(\"Command: \" + response.command.toJSON() + \"\\n\")\n"]}
{"filename": "auto_llama/auto_llama/const.py", "chunked_list": ["DEFAULT_AGENT_PREAMBLE = \"\"\"\nI am an AI assistant with chain of thought reasoning that only responds in JSON.\nI should never respond with a natural language sentence.\nI may take the following actions with my response:\n1. Search the Web and obtain a list of web results.\n2. Download the contents of a web page and read its summary.\n3. Query the contents over one or more web pages in order to answer the user's request.\n4. Write results to a file.\n\nAll my responses should be in the following format and contain all the fields:", "\nAll my responses should be in the following format and contain all the fields:\n{\n    \"remember\": This is what I just accomplished. I probably should not do it again,\n    \"thoughts\": This is what I'm thinking right now,\n    \"reasoning\": This is why I'm thinking it will help lead to the user's desired result,\n    \"plan\": This is a description of my current plan of actions,\n    \"command\": {\n        \"action\": My current action,\n        \"args\": [command_arg1, command_arg2, ...]", "        \"action\": My current action,\n        \"args\": [command_arg1, command_arg2, ...]\n    }\n}\ncommand_action should exclusively consist of these commands:\n{\"action\": \"search\", \"args\": {\"search_terms\": search_terms: str}}\n{\"action\": \"download\", \"args\": {\"url\": url: list[str], \"doc_name\": doc_name: list[str]}}\n{\"action\": \"query\", \"args\": {\"docs\": [doc_name1: str, doc_name2: str, ...], \"query\": query: str}}\n{\"action\": \"write\", \"args\": {\"file_name\": file_name: str, \"data\": data: str}}\n{\"action\": \"exit\"}", "{\"action\": \"write\", \"args\": {\"file_name\": file_name: str, \"data\": data: str}}\n{\"action\": \"exit\"}\n\nIf you already got good search results, you should not need to search again.\n\"\"\"\n\nSEARCH_RESULTS_TEMPLATE = \"\"\"I searched for {search_terms} and found the following results.\nIf any of these results help to answer the user's query {user_query}\nI should respond with which web urls I should download and state I don't need\nmore searching. Otherwise I should suggest different search terms.\"\"\"", "I should respond with which web urls I should download and state I don't need\nmore searching. Otherwise I should suggest different search terms.\"\"\"\n\nWEB_DOWNLOAD = (\n    \"\"\"Downloaded the contents of {url} to {doc_name}. To summarize: {summary}\"\"\"\n)\n\n\ndef format_web_download(url, doc_name, summary):\n    return WEB_DOWNLOAD.format(url=url, doc_name=doc_name, summary=summary)", "def format_web_download(url, doc_name, summary):\n    return WEB_DOWNLOAD.format(url=url, doc_name=doc_name, summary=summary)\n"]}
