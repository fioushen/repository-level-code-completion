{"filename": "custom_queue.py", "chunked_list": ["from contextlib import contextmanager\nfrom collections import defaultdict\nimport time\n\nclass UserLimitedQueue:\n  max_tasks_per_user = None\n\n  def __init__(self, max_tasks_per_user):\n    self.task_count = defaultdict(int)\n    if self.max_tasks_per_user is None:\n      self.max_tasks_per_user = max_tasks_per_user\n\n  @contextmanager\n  def for_user(self, user_id):\n    if self.task_count[user_id] < self.max_tasks_per_user:\n      self.task_count[user_id] += 1\n      try:\n        yield True\n      finally:\n        self.task_count[user_id] -= 1\n    else:\n      yield False", "\n\nclass CallCooldown:\n  calls = {}\n\n  @classmethod\n  def check_call(cls, uid, function_name, timeout=30):\n    key = f'{uid}_{function_name}'\n    if key in cls.calls:\n      if time.time() - cls.calls[key] < timeout:\n        return False\n    cls.calls[key] = time.time()\n    return True", "\n\ndef semaphore_wrapper(semaphore, callback):\n  async def wrapped(*args, **kwargs):\n    async with semaphore:\n      return await callback(*args, **kwargs)\n  return wrapped\n    "]}
{"filename": "automigration.py", "chunked_list": ["from dotenv import dotenv_values\nfrom utils import cprint\nimport os\nimport sys\nimport json\nassert os.path.exists('.env') and os.path.exists('.env.example')\n\nsystem_env = dotenv_values('.env')\ndemo_env = dotenv_values('.env.example')\n\ndef check_new_keys_in_example_env():\n  towrite = '''\n'''\n  for key in demo_env:\n    if key not in system_env:\n      towrite += f\"{key}='{demo_env[key]}'\\n\"\n\n  if len(towrite) != 1:\n    with open('.env', 'a') as file:\n      file.write(towrite)", "demo_env = dotenv_values('.env.example')\n\ndef check_new_keys_in_example_env():\n  towrite = '''\n'''\n  for key in demo_env:\n    if key not in system_env:\n      towrite += f\"{key}='{demo_env[key]}'\\n\"\n\n  if len(towrite) != 1:\n    with open('.env', 'a') as file:\n      file.write(towrite)", "\nDEPRECATED_KEYS = ['llm_active_model_type', 'sd_available_loras', 'tts_so_vits_svc_code_path']\nDEPRECATED_KVS = {\n  'llm_assistant_chronicler': ['gpt4all', 'minchatgpt', 'alpaca'],\n  'llm_python_model_type': 'cerebras_gpt'\n}\n\ndef check_deprecated_keys_in_dotenv():\n  for key in system_env:\n    if key in DEPRECATED_KEYS:\n      cprint(f'Warning! The key \"{key}\" has been deprecated! See CHANGELOG.md.', color='red')\n    value = system_env[key]\n    if len(value) < 1:\n      continue\n    if (value[0] != '[' and value[0] != '{'):\n      value = f'\"{value}\"'\n    value = json.loads(value)\n    if type(value) is str:\n      if key in DEPRECATED_KVS and value in DEPRECATED_KVS[key]:\n        cprint(f'Warning! The value \"{value}\" of \"{key}\" has been deprecated! See CHANGELOG.md.', color='yellow')", "\ncheck_new_keys_in_example_env()\ncheck_deprecated_keys_in_dotenv()"]}
{"filename": "bot.py", "chunked_list": ["import logging\nimport automigration\n\nfrom aiogram import Bot, Dispatcher\nfrom config_reader import config\nfrom middleware import ChatActionMiddleware, AccessMiddleware, CooldownMiddleware, MediaGroupMiddleware\n\nfrom modules.sd import StableDiffusionModule\nfrom modules.tts import TextToSpeechModule\nfrom modules.admin import AdminModule", "from modules.tts import TextToSpeechModule\nfrom modules.admin import AdminModule\nfrom modules.llm import LargeLanguageModel\nfrom modules.tta import TextToAudioModule\nfrom modules.stt import SpeechToTextModule\n\n\nlogger = logging.getLogger(__name__)\n\ndp = Dispatcher()", "\ndp = Dispatcher()\ndp.message.middleware(AccessMiddleware())\ndp.message.middleware(ChatActionMiddleware())\ndp.message.middleware(CooldownMiddleware())\ndp.message.middleware(MediaGroupMiddleware())\n\n\ndef initialize(dp, bot):\n  available_modules = {\n    \"sd\": StableDiffusionModule,\n    \"tts\": TextToSpeechModule,\n    \"tta\": TextToAudioModule,\n    \"stt\": SpeechToTextModule,\n    \"admin\": AdminModule,\n    \"llm\": LargeLanguageModel\n  }\n  dp['modules'] = {}\n  for module in config.active_modules:\n    if module in available_modules:\n      dp['modules'][module] = available_modules[module](dp, bot)", "def initialize(dp, bot):\n  available_modules = {\n    \"sd\": StableDiffusionModule,\n    \"tts\": TextToSpeechModule,\n    \"tta\": TextToAudioModule,\n    \"stt\": SpeechToTextModule,\n    \"admin\": AdminModule,\n    \"llm\": LargeLanguageModel\n  }\n  dp['modules'] = {}\n  for module in config.active_modules:\n    if module in available_modules:\n      dp['modules'][module] = available_modules[module](dp, bot)", "\ndef main():\n  bot = Bot(token=config.bot_token.get_secret_value(), parse_mode=\"HTML\")\n  logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s',)\n  initialize(dp, bot)\n  print('running')\n  dp.run_polling(bot)\n\n\nif __name__ == \"__main__\":\n  main()", "\nif __name__ == \"__main__\":\n  main()"]}
{"filename": "config_reader.py", "chunked_list": ["from pydantic import BaseSettings, SecretStr, validator\nfrom typing import List, Dict\nfrom typing_extensions import Literal\nfrom utils import update_env\n\nclass Settings(BaseSettings):\n  bot_token: SecretStr\n  adminlist: List[int]\n  whitelist: List[int]\n  blacklist: List[int]\n  ignore_mode: Literal[\"blacklist\", \"whitelist\", \"both\"]\n  active_modules: List[str]\n  apply_mps_fixes: bool\n  tts_path: str\n  tts_voices: List[str]\n  tts_mode: Literal[\"local\", \"localhttp\", \"remote\"]\n  tts_replacements: Dict\n  tts_credits: str\n  tts_ffmpeg_path: str\n  tts_enable_so_vits_svc: bool\n  tts_so_vits_svc_4_0_code_path: str\n  tts_so_vits_svc_4_1_code_path: str\n  tts_so_vits_svc_base_tts_provider: Literal[\"say_macos\", \"built_in\"]\n  tts_so_vits_svc_voices: List[Dict]\n  tts_queue_size_per_user: int\n  tts_host: str\n  sd_host: str\n  sd_max_steps: int\n  sd_max_resolution: int\n  sd_available_samplers: List[str]\n  sd_extra_prompt: str\n  sd_extra_negative_prompt: str\n  sd_default_sampler: str\n  sd_default_n_iter: int\n  sd_default_width: int\n  sd_default_height: int\n  sd_default_tti_steps: int\n  sd_default_tti_cfg_scale: int\n  sd_default_iti_cfg_scale: int\n  sd_default_iti_steps: int\n  sd_default_iti_denoising_strength: float\n  sd_default_iti_sampler: str\n  sd_launch_process_automatically: bool\n  sd_launch_command: str\n  sd_launch_dir: str\n  sd_launch_waittime: int\n  sd_lora_custom_activations: Dict\n  sd_only_admins_can_change_models: bool\n  sd_queue_size_per_user: int\n  llm_host: str\n  llm_queue_size_per_user: int\n  llm_backend: Literal ['pytorch', 'llama_cpp', 'mlc_pb', 'remote_ob', 'remote_lcpp']\n  llm_python_model_type: Literal[\"gpt2\",\"gptj\", \"auto_hf\",\"llama_orig\", \"llama_hf\"]\n  llm_paths: Dict\n  llm_character: str\n  llm_history_grouping: Literal[\"user\", \"chat\"]\n  llm_max_history_items: int\n  llm_generation_cfg_override: Dict\n  llm_assistant_cfg_override: Dict\n  llm_assistant_chronicler: Literal[\"alpaca\", \"instruct\", \"raw\"]\n  llm_assistant_use_in_chat_mode: bool\n  llm_force_assistant_for_unsupported_models: bool\n  llm_max_tokens: int\n  llm_max_assistant_tokens: int\n  llm_lcpp_gpu_layers: int\n  llm_lcpp_max_context_size: int\n  llm_remote_launch_process_automatically: bool\n  llm_remote_launch_command: str\n  llm_remote_launch_dir: str\n  llm_remote_launch_waittime: int\n  llm_remote_model_name: str\n  stt_backend: Literal['whisper', 'silero', 'wav2vec2']\n  stt_model_path_or_name: str\n  stt_autoreply_mode: Literal['none', 'assistant', 'chat']\n  stt_autoreply_voice: str\n  stt_queue_size_per_user: int\n  tta_queue_size_per_user: int\n  tta_device: Literal[\"cpu\", \"cuda\", \"mps\"]\n  tta_music_model: str\n  tta_sfx_model: str\n  tta_duration: int\n  python_command: str\n  mm_preload_models_on_start: bool\n  mm_ram_cached_model_count_limit: int\n  mm_vram_cached_model_count_limit: int\n  mm_management_policy: Literal[\"COUNT\", \"MEMORY\", \"BOTH\", \"NONE\"]\n  mm_unload_order_policy: Literal[\"LEAST_USED\", \"OLDEST_USE_TIME\", \"OLDEST_LOAD_ORDER\", \"MEMORY_FOOTPRINT\"]\n  mm_autounload_after_seconds: int\n  \n  @validator('sd_max_resolution', 'sd_default_width', 'sd_default_height')\n  def resolution_in_correct_ranges(cls, v):\n    if v % 64 != 0 or v < 256 or v > 2048:\n      raise ValueError('incorrect value')\n    return v\n  \n  @validator('tts_so_vits_svc_voices')\n  def base_voices_exist(cls, v, values):\n    if not values['tts_enable_so_vits_svc']:\n      return v\n    for item in v:\n      provider = item.get('provider', values['tts_so_vits_svc_base_tts_provider'])\n      if provider == 'built_in':\n        if item['base_voice'] not in values['tts_voices']:\n          raise ValueError(f'base tts voice ({item[\"base_voice\"]}) does not exist')\n    return v\n  \n  class Config:\n    env_file = '.env'\n    env_file_encoding = 'utf-8'", "\n# mirror all config changes to .env file\nclass SettingsWrapper(Settings):\n  def __setattr__(self, name, value):\n    update_env('.env', name, value)\n    super().__setattr__(name, value)\n\nconfig = SettingsWrapper()"]}
{"filename": "utils.py", "chunked_list": ["from io import BytesIO\nimport base64\nimport argparse\nimport functools\nimport sys\nimport json\n\nasync def tg_image_to_data(photo, bot):\n  if not photo:\n    return None", "  if not photo:\n    return None\n  file_bytes = BytesIO()\n  file_path = (await bot.get_file(file_id=photo[-1].file_id)).file_path\n  await bot.download_file(file_path, file_bytes)\n  image = 'data:image/png;base64,' + str(base64.b64encode(file_bytes.getvalue()), 'utf-8')\n  return image\n\ndef b64_to_img(imgstr):\n  from PIL import Image\n  decoded = base64.b64decode(imgstr.split(',')[1])\n  return Image.open(BytesIO(decoded))", "def b64_to_img(imgstr):\n  from PIL import Image\n  decoded = base64.b64decode(imgstr.split(',')[1])\n  return Image.open(BytesIO(decoded))\n\n# do not sysexit on error\nclass CustomArgumentParser(argparse.ArgumentParser):\n  def error(self, message):\n    raise Exception(message)\n", "\n# join the rest of the arguments, so they can be validated\nclass JoinNargsAction(argparse.Action):\n  def __call__(self, parser, namespace, values, option_string=None):\n    setattr(namespace, self.dest, ' '.join(values))\n\n\ndef parse_photo(message):\n  if message.photo:\n    return message.photo\n  if message.document and message.document.mime_type.startswith('image'):\n    return [message.document]\n  if message.reply_to_message:\n    if message.reply_to_message.photo:\n      return message.reply_to_message.photo\n    if message.reply_to_message.document and message.reply_to_message.document.mime_type.startswith('image'):\n      return [message.reply_to_message.document]\n  return None", "\ndef log_exceptions(logger):\n  def decorator(func):\n    @functools.wraps(func)\n    async def wrapper(*args, **kwargs):\n      try:\n        result = await func(*args, **kwargs)\n        return result\n      except Exception as e:\n        logger.error(f\"Error in {func.__name__}: {str(e)}\")\n    return wrapper\n  return decorator", "\ndef cprint(*args, color='default'):\n  keys = ['default', 'red', 'green', 'yellow', 'blue']\n  sys.stdout.write(f'\\x1b[1;3{keys.index(color)}m' + ' '.join(args) + '\\x1b[0m\\n')\n\ndef update_env(path, key, value):\n  with open(path, \"r\") as file:\n    lines = file.readlines()\n  with open(path, \"w+\") as file:\n    to_write = []\n    multiline = False\n    try:\n      for line in lines:\n        if line.startswith(key + \"=\"):\n          multiline = line.startswith(key + \"='\")\n          if not multiline:\n            to_write.append(f\"{key}={value}\\n\")\n          else:\n            to_write.append(f\"{key}='{json.dumps(json.loads(value), indent=2, sort_keys=True)}'\\n\")\n          continue\n        if multiline:\n          if line.endswith(\"'\") or line.endswith(\"'\\n\") or line.endswith(\"'\\r\\n\"):\n            multiline = False\n          continue\n        to_write.append(line)\n      file.writelines(to_write)\n    except Exception as e:\n      file.writelines(lines)\n      cprint(\"Unable to update .env file: \" + str(e), color='red')", "\nasync def download_audio(bot, file_id, dl_path):\n  file_path = (await bot.get_file(file_id=file_id)).file_path\n  await bot.download_file(file_path, dl_path)"]}
{"filename": "middleware.py", "chunked_list": ["from aiogram.dispatcher.flags import get_flag\nfrom aiogram.utils.chat_action import ChatActionSender\nfrom aiogram import BaseMiddleware\nfrom config_reader import config\nfrom custom_queue import CallCooldown\nfrom collections import defaultdict\nimport asyncio\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass ChatActionMiddleware(BaseMiddleware):\n  async def __call__(self, handler, event, data):\n    long_operation_type = get_flag(data, \"long_operation\")\n\n    if not long_operation_type:\n      return await handler(event, data)\n\n    async with ChatActionSender(action=long_operation_type, chat_id=event.chat.id):\n      return await handler(event, data)", "logger = logging.getLogger(__name__)\n\nclass ChatActionMiddleware(BaseMiddleware):\n  async def __call__(self, handler, event, data):\n    long_operation_type = get_flag(data, \"long_operation\")\n\n    if not long_operation_type:\n      return await handler(event, data)\n\n    async with ChatActionSender(action=long_operation_type, chat_id=event.chat.id):\n      return await handler(event, data)", "\nclass AccessMiddleware(BaseMiddleware):\n  async def __call__(self, handler, event, data):\n    uid = event.from_user.id\n    cid = event.chat.id\n    logger.info(f'message in chat {cid} ({event.chat.title or \"private\"}) from {uid} (@{event.from_user.username or event.from_user.first_name})')\n    if config.ignore_mode == 'whitelist' or config.ignore_mode == 'both':\n      if cid not in config.whitelist:\n        return\n    if config.ignore_mode == 'blacklist' or config.ignore_mode == 'both':\n      if uid in config.blacklist or cid in config.blacklist:\n        return\n    if get_flag(data, \"admins_only\"):\n      if uid not in config.adminlist:\n        return\n    return await handler(event, data)", "\nclass CooldownMiddleware(BaseMiddleware):\n  async def __call__(self, handler, event, data):\n    cooldown_seconds = get_flag(data, \"cooldown\")\n    if cooldown_seconds:\n      function_name = data['handler'].callback.__name__\n      if CallCooldown.check_call(event.from_user.id, function_name, cooldown_seconds):\n        return await handler(event, data)\n      else:\n        return\n    else:\n      return await handler(event, data)", "\nclass MediaGroupMiddleware(BaseMiddleware):\n    albums = defaultdict(lambda: [])\n  \n    def __init__(self, delay =  1):\n      self.delay = delay\n\n    async def __call__(self, handler, event, data):\n      if not event.media_group_id:\n        return await handler(event, data)\n\n      try:\n        self.albums[event.media_group_id].append(event)\n        await asyncio.sleep(self.delay)\n        data[\"album\"] = self.albums.pop(event.media_group_id)\n      except Exception as e:\n        logger.error(e)\n      return await handler(event, data)"]}
{"filename": "providers/stt_provider.py", "chunked_list": ["from config_reader import config\nfrom .stt import backends\n\nactive_model = backends[config.stt_backend]() if 'stt' in config.active_modules else None"]}
{"filename": "providers/sd_provider.py", "chunked_list": ["import base64\nimport httpx \nimport json\nimport random\nimport asyncio\nimport logging\nimport subprocess\nimport psutil\nfrom collections import defaultdict\nfrom config_reader import config", "from collections import defaultdict\nfrom config_reader import config\nfrom misc.memory_manager import mload\nfrom functools import partial\n\n\nlogger = logging.getLogger(__name__)\nrequest_payload = {\n  \"denoising_strength\": 1,\n  \"prompt\": \"\",", "  \"denoising_strength\": 1,\n  \"prompt\": \"\",\n  \"sampler_name\": config.sd_default_sampler,\n  \"steps\": config.sd_default_tti_steps,\n  \"cfg_scale\": 5,\n  \"width\": config.sd_default_width,\n  \"height\": config.sd_default_height,\n  \"restore_faces\": False,\n  \"tiling\": False,\n  \"batch_size\": 1,", "  \"tiling\": False,\n  \"batch_size\": 1,\n  \"n_iter\": config.sd_default_n_iter,\n  \"negative_prompt\": \"\",\n  \"eta\": 71337\n}\n\n# hash: name\nmodels = defaultdict(lambda: 'Unknown model')\nembeddings = []", "models = defaultdict(lambda: 'Unknown model')\nembeddings = []\nloras = []\n\nsd_url = config.sd_host or \"http://127.0.0.1:7860\"\n\nsd_service = False\nsd_started = False\n\ndef run_sd_service():\n  global sd_started\n  if not config.sd_launch_process_automatically:\n    return\n  p = partial(subprocess.Popen, config.sd_launch_command.split(' '), cwd=config.sd_launch_dir, stderr=subprocess.DEVNULL)\n  service = mload('sd-remote', \n    p, \n    lambda p: p.terminate() or (sd_started:=False),\n    lambda p: psutil.Process(p.pid).memory_info().rss, \n    gpu=True\n  )\n  return service", "\ndef run_sd_service():\n  global sd_started\n  if not config.sd_launch_process_automatically:\n    return\n  p = partial(subprocess.Popen, config.sd_launch_command.split(' '), cwd=config.sd_launch_dir, stderr=subprocess.DEVNULL)\n  service = mload('sd-remote', \n    p, \n    lambda p: p.terminate() or (sd_started:=False),\n    lambda p: psutil.Process(p.pid).memory_info().rss, \n    gpu=True\n  )\n  return service", "\ndef check_server(async_func):\n  async def decorated_function(*args, **kwargs):\n    global sd_service, sd_started\n    if not config.sd_launch_process_automatically:\n      return await async_func(*args, **kwargs)\n    url = f\"{sd_url}/sdapi/v1/sd-models\"\n    try:\n      async with httpx.AsyncClient() as client:\n        await client.get(url)\n    except (httpx.HTTPError, httpx.NetworkError, ConnectionError, httpx.RemoteProtocolError):\n      print(\"SD server is down. Restarting it...\")\n      if not sd_started or sd_service.poll() is not None:\n        sd_service = run_sd_service()\n        sd_started = True\n        # better idea is to read stdout and wait for server, but it doesn't work for some reason\n        await asyncio.sleep(config.sd_launch_waittime)\n      else:\n        # TODO: fix this mess sometime later\n        sd_service = run_sd_service()\n      return await async_func(*args, **kwargs)\n    sd_service = run_sd_service()\n    return await async_func(*args, **kwargs)\n  return decorated_function", "\n@check_server\nasync def refresh_model_list():\n  global models, embeddings, loras\n  try:\n    async with httpx.AsyncClient() as client:\n      model_response = await client.get(url=f'{sd_url}/sdapi/v1/sd-models',headers={'accept': 'application/json'}, timeout=None)\n      embed_response = await client.get(url=f'{sd_url}/sdapi/v1/embeddings',headers={'accept': 'application/json'}, timeout=None)\n      lora_response  = await client.get(url=f'{sd_url}/sdapi/v1/loras',headers={'accept': 'application/json'}, timeout=None)\n      if model_response.status_code == 200 and embed_response.status_code == 200 and lora_response.status_code == 200: \n        model_response_data = model_response.json()\n        embed_response_data = embed_response.json()\n        lora_response_data  = lora_response.json()\n        models.clear()\n        embeddings.clear()\n        loras.clear()\n        for m in model_response_data:\n          models[m['hash']] = m['model_name']\n        for e in embed_response_data['loaded']:\n          embeddings.append(e)\n        for lora in lora_response_data:\n          loras.append(lora['name'])\n        loras[:] = [key for key in loras if key not in config.sd_lora_custom_activations]\n      else:\n        raise Exception('Server error')\n  except Exception as e:\n    logger.warn('Failed to load stable diffusion model names: ' + str(e))", "\n\ndef b642img(base64_image):\n  return base64.b64decode(base64_image)\n\n@check_server\nasync def switch_model(name):\n  async with httpx.AsyncClient() as client:\n    try:\n      payload = {'sd_model_checkpoint': name}\n      response = await client.post(url=f'{sd_url}/sdapi/v1/options', json=payload, timeout=None)\n      if response.status_code == 200:\n        return True\n    except Exception:\n      return False", "    try:\n      payload = {'sd_model_checkpoint': name}\n      response = await client.post(url=f'{sd_url}/sdapi/v1/options', json=payload, timeout=None)\n      if response.status_code == 200:\n        return True\n    except Exception:\n      return False\n  return False\n\n@check_server", "\n@check_server\nasync def sd_get_images(payload, endpoint):\n  if len(models.values()) == 0:\n    await refresh_model_list()\n  async with httpx.AsyncClient() as client:\n    try:\n      response = await client.post(url=f'{sd_url}/{endpoint}', json=payload, timeout=None)\n      if response.status_code == 200:\n        response_data = response.json()\n        images = response_data.get(\"images\")\n        bstr_images = [b642img(i) for i in images]\n        gen_info = json.loads(response_data.get('info'))\n        gen_info['model'] = models[gen_info['sd_model_hash']]\n        return (False, bstr_images, gen_info)\n      else:\n        return ('Connection error', None, None)\n    except (httpx.NetworkError, ConnectionError, httpx.RemoteProtocolError, json.decoder.JSONDecodeError) as error:\n      return (error, None, None)\n    except Exception:\n      return ('unknown error', None, None)", "\n\nasync def tti(override=None):\n  payload = request_payload\n  default_scale = config.sd_default_tti_cfg_scale\n  payload['cfg_scale'] = random.choice([3,4,5,6]) if default_scale == 0 else default_scale\n  if override:\n    payload = {**payload, **override}\n  return await sd_get_images(payload, 'sdapi/v1/txt2img')\n", "  return await sd_get_images(payload, 'sdapi/v1/txt2img')\n\n\nasync def iti(override=None):\n  payload = request_payload\n  payload['denoising_strength'] = config.sd_default_iti_denoising_strength\n  payload['cfg_scale'] = config.sd_default_iti_cfg_scale\n  payload['steps'] = config.sd_default_iti_steps\n  payload['sampler'] = config.sd_default_iti_sampler\n  if override:\n    payload = {**payload, **override}", "  payload['sampler'] = config.sd_default_iti_sampler\n  if override:\n    payload = {**payload, **override}\n  return await sd_get_images(payload, 'sdapi/v1/img2img')\n"]}
{"filename": "providers/tta_provider.py", "chunked_list": ["from utils import cprint\nfrom config_reader import config\nfrom concurrent.futures import ThreadPoolExecutor\nfrom misc.memory_manager import mload\nfrom functools import partial\nimport asyncio\nimport tempfile\n\nAudioGen = None\nMusicGen = None", "AudioGen = None\nMusicGen = None\n\ndef tta_init():\n  global MusicGen, AudioGen, audio_write\n  try:\n    from audiocraft.models import MusicGen, AudioGen\n    from audiocraft.data.audio import audio_write\n    return True\n  except ImportError:\n    cprint(\"TTA (AudioCraft) module not available, please reinstall it\", color=\"red\")\n    return False", "\ndef get_model(path, loader, name):\n  loader = partial(loader.get_pretrained, path, config.tta_device)\n  model = mload('tta-' + name, loader, None)\n  return model\n\n\ndef generate_audio(text, audio_type=\"music\", duration=5, raw_data=False):\n  try:\n    if audio_type == \"music\":\n      model = get_model(config.tta_music_model, MusicGen, 'MusicGen')\n    else:\n      model = get_model(config.tta_sfx_model, AudioGen, 'AudioGen')\n    model.set_generation_params(duration=duration)\n    wav = model.generate([text])\n  except Exception as e:\n    return (str(e), None)\n  if not raw_data:\n    wav = save_audio(wav[0], model)\n  return False, wav", "\nasync def generate_audio_async(text, audio_type=\"music\", duration=5, raw_data=False):\n  with ThreadPoolExecutor():\n    error, output = await asyncio.to_thread(generate_audio, \n      text, audio_type, duration, raw_data\n    )\n  return error, output\n\n  \ndef save_audio(wav_file, model):\n  tmp_path = tempfile.TemporaryDirectory().name + 'record'\n  audio_write(tmp_path, wav_file.cpu(), model.sample_rate, strategy=\"loudness\", loudness_compressor=True)\n  return tmp_path + '.wav'", "  \ndef save_audio(wav_file, model):\n  tmp_path = tempfile.TemporaryDirectory().name + 'record'\n  audio_write(tmp_path, wav_file.cpu(), model.sample_rate, strategy=\"loudness\", loudness_compressor=True)\n  return tmp_path + '.wav'"]}
{"filename": "providers/tts_provider.py", "chunked_list": ["from utils import cprint\nfrom config_reader import config\ntry:\n  import torch\n  from TTS.utils.synthesizer import Synthesizer\nexcept ImportError:\n  Synthesizer = None\n  if 'tts' in config.active_modules and config.tts_mode == 'local':\n    cprint(\"TTS module not available, please reinstall it\", color=\"red\")\nfrom pathlib import Path", "from pathlib import Path\nimport httpx\nimport json\nimport tempfile\nimport subprocess\nimport os\nimport time\nfrom config_reader import config\nfrom misc.memory_manager import mload\nfrom functools import partial", "from misc.memory_manager import mload\nfrom functools import partial\n\nsynthesizers = {}\nso_vits_svc_voices = dict({m['voice'].lower().replace('-',''): m for m in config.tts_so_vits_svc_voices})\n\nasync def so_vits_svc(voice, text, original_audio=False):\n  version = so_vits_svc_voices[voice].get('v', 4.0)\n  v4_0_code_path = config.tts_so_vits_svc_4_0_code_path\n  v4_1_code_path = config.tts_so_vits_svc_4_1_code_path", "  v4_0_code_path = config.tts_so_vits_svc_4_0_code_path\n  v4_1_code_path = config.tts_so_vits_svc_4_1_code_path\n  so_vits_svc_code = v4_0_code_path if version == 4.0 else v4_1_code_path\n  name = 'temp_tts' + str(time.time_ns())\n  temp_file = f'{so_vits_svc_code}/raw/{name}.aif'\n  temp_file_wav = temp_file.replace('.aif', '.wav')\n  v = so_vits_svc_voices[voice]\n  so_vits_model = Path(v['path']) / v['weights']\n  so_vits_config = Path(v['path']) / 'config.json'\n  so_vits_voice = v['voice']", "  so_vits_config = Path(v['path']) / 'config.json'\n  so_vits_voice = v['voice']\n  base_voice = v['base_voice']\n  if not original_audio:\n    # generate any text-to-speech output\n    base_tts_provider = v.get('provider', config.tts_so_vits_svc_base_tts_provider)\n    if base_tts_provider == 'say_macos':\n      subprocess.run(\n        ['say','-v', base_voice,  '-o', temp_file, text],\n      )\n    elif base_tts_provider == 'built_in':\n      error, temp_file = await tts(base_voice, text)\n  else:\n    temp_file = original_audio", "\n  subprocess.run([\n    config.tts_ffmpeg_path, '-y', '-i', temp_file, temp_file_wav],\n    stdout=subprocess.DEVNULL,\n    stderr=subprocess.STDOUT\n  )\n  subprocess.run(\n    [config.python_command, f\"inference_main.py\", \"-m\", str(so_vits_model), \"-c\", str(so_vits_config), \n    \"-n\", f'{name}.wav', \"-t\", \"0\", \"-s\", so_vits_voice]\n    ,", "    \"-n\", f'{name}.wav', \"-t\", \"0\", \"-s\", so_vits_voice]\n    ,\n    cwd=so_vits_svc_code\n  )\n  os.remove(temp_file)\n  os.remove(temp_file_wav)\n  filename = f'{so_vits_svc_code}/results/{name}.wav_0key_{so_vits_voice}.flac'\n  if not os.path.isfile(filename):\n    filename = filename.replace('.flac', '_sovits_pm.flac')\n    if not os.path.isfile(filename):\n      return ('File not found', None)", "  return (False, filename)\n\nasync def tts(voice, text):\n  try:\n    assert os.path.exists(config.tts_ffmpeg_path)\n    for r in config.tts_replacements:\n      text = text.replace(r, config.tts_replacements[r])\n    if voice in so_vits_svc_voices:\n      return await so_vits_svc(voice, text)\n    loader = partial(\n      Synthesizer,\n      tts_config_path=Path(config.tts_path) / \"config.json\",\n      tts_checkpoint=Path(config.tts_path) / (voice + \".pth\"),\n      use_cuda=torch.cuda.is_available(),\n    )\n    synth = mload('tts-' + voice, loader, None, gpu=torch.cuda.is_available())\n    data = synth.tts(text[:4096] + '.')\n    return (False, save_audio(synth, data))\n  except Exception as e:\n    return (str(e), None)", "\nasync def remote_tts(voice, text):\n  async with httpx.AsyncClient() as client:\n    try:\n      tts_payload = {\"voice\": voice, \"text\": text, \"response\": 'file' if config.tts_mode == 'remote' else 'path'}\n      response = await client.post(url=config.tts_host, json=tts_payload, timeout=None)\n      if response.status_code == 200:\n        if config.tts_mode == 'remote':\n          path = tempfile.TemporaryDirectory().name + str(hash(text)) + '.wav'\n          with open(path, 'wb') as f:\n            f.write(response.content)\n          return (False, path)\n        else:\n          response_data = response.json()\n        error = response_data.get('error')\n        if error:\n          return (error, None)\n        wpath = response_data.get(\"data\")\n        return (False, wpath)\n      else:\n        return ('Server error', None)\n    except (httpx.NetworkError, ConnectionError, httpx.RemoteProtocolError, json.decoder.JSONDecodeError) as error:\n      return (error, None)\n    except Exception as e:\n      return (str(e), None)", "\ndef save_audio(synth, wav_file):\n  tmp_path = tempfile.TemporaryDirectory().name + 'record.wav'\n  synth.save_wav(wav_file, tmp_path)\n  return tmp_path\n\ndef convert_to_ogg(wav_path):\n  ogg_path = wav_path + '.ogg'\n  subprocess.run([\n    config.tts_ffmpeg_path, '-i', wav_path, \n    '-acodec', 'libopus', '-b:a', '128k', '-vbr', 'off', ogg_path, '-y'],\n    stdout=subprocess.DEVNULL,\n    stderr=subprocess.STDOUT\n  )\n  with open(ogg_path, 'rb') as f:\n    data = f.read()  \n  os.remove(wav_path)\n  os.remove(ogg_path)\n  return data"]}
{"filename": "providers/llm_provider.py", "chunked_list": ["from config_reader import config\nfrom .llm import external_backends, pytorch_models\nuse_built_in_models = config.llm_backend == 'pytorch'\ntarget_provider = pytorch_models if use_built_in_models else external_backends\ntarget_key = config.llm_python_model_type if use_built_in_models else config.llm_backend\n\nactive_model = target_provider[target_key]() if 'llm' in config.active_modules else None\n"]}
{"filename": "providers/llm/mlc_chat_prebuilt_provider.py", "chunked_list": ["import sys\nfrom concurrent.futures import ThreadPoolExecutor\nfrom providers.llm.abstract_llm import AbstractLLM\nimport asyncio\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass MLCChatPrebuilt(AbstractLLM):\n  assistant_mode = True\n  def __init__(self, model_paths, init_config={}):\n    sys.path.append(model_paths['path_to_mlc_chatbot_code'])\n    try:\n      from mlc_chatbot.bot import ChatBot    \n    except ImportError:\n      logging.error('MLC Chatbot is not installed')\n    self.model = ChatBot(model_paths['path_to_mlc_pb_home_dir'], model_paths['path_to_mlc_pb_binary_dir'])\n    self.model.generate = self.model.send\n    self.filename = 'Unknown model'\n\n  async def generate(self, raw_prompt, length=0, model_params={}, assist=True):\n    error = None\n    try:\n      with ThreadPoolExecutor():\n        print(self.model)\n        output = await asyncio.to_thread(self.model.generate, \n          raw_prompt\n        )\n      self.model.reset()\n    except Exception as e:\n      error = str(e)\n    return (False, output) if not error else (error, None)", "class MLCChatPrebuilt(AbstractLLM):\n  assistant_mode = True\n  def __init__(self, model_paths, init_config={}):\n    sys.path.append(model_paths['path_to_mlc_chatbot_code'])\n    try:\n      from mlc_chatbot.bot import ChatBot    \n    except ImportError:\n      logging.error('MLC Chatbot is not installed')\n    self.model = ChatBot(model_paths['path_to_mlc_pb_home_dir'], model_paths['path_to_mlc_pb_binary_dir'])\n    self.model.generate = self.model.send\n    self.filename = 'Unknown model'\n\n  async def generate(self, raw_prompt, length=0, model_params={}, assist=True):\n    error = None\n    try:\n      with ThreadPoolExecutor():\n        print(self.model)\n        output = await asyncio.to_thread(self.model.generate, \n          raw_prompt\n        )\n      self.model.reset()\n    except Exception as e:\n      error = str(e)\n    return (False, output) if not error else (error, None)", "\ninit = MLCChatPrebuilt"]}
{"filename": "providers/llm/abstract_llm.py", "chunked_list": ["from abc import ABCMeta, abstractmethod\nfrom collections import defaultdict\n\nclass AbstractLLM(metaclass=ABCMeta):\n  assistant_mode = True\n  model = None\n  def __init__(self, model_paths, init_config):\n    return self\n\n  def tokenize(self, details):\n    pass\n\n  @abstractmethod\n  def generate(self, prompt, length, model_params, assist):\n    pass", "  \n"]}
{"filename": "providers/llm/__init__.py", "chunked_list": ["import importlib\ngpt2_provider = lambda: importlib.import_module('providers.llm.pytorch.gpt2_provider')\ngptj_provider = lambda: importlib.import_module('providers.llm.pytorch.gptj_provider')\nauto_hf_provider = lambda: importlib.import_module('providers.llm.pytorch.auto_hf_provider')\nllama_orig_provider = lambda: importlib.import_module('providers.llm.pytorch.llama_orig_provider')\nllama_hf_provider = lambda: importlib.import_module('providers.llm.pytorch.llama_hf_provider')\nmlc_chat_prebuilt_provider = lambda: importlib.import_module('providers.llm.mlc_chat_prebuilt_provider')\nllama_cpp_provider = lambda: importlib.import_module('providers.llm.llama_cpp_provider')\nremote_ob_provider = lambda: importlib.import_module('providers.llm.remote_ob')\nremote_lcpp_provider = lambda: importlib.import_module('providers.llm.remote_llama_cpp')", "remote_ob_provider = lambda: importlib.import_module('providers.llm.remote_ob')\nremote_lcpp_provider = lambda: importlib.import_module('providers.llm.remote_llama_cpp')\n\npytorch_models = {\n  'gpt2': gpt2_provider, \n  'gptj': gptj_provider, \n  'auto_hf': auto_hf_provider,\n  'llama_orig': llama_orig_provider,\n  'llama_hf': llama_hf_provider,\n}", "  'llama_hf': llama_hf_provider,\n}\n\nexternal_backends = {\n  'llama_cpp': llama_cpp_provider,\n  'mlc_pb': mlc_chat_prebuilt_provider,\n  'remote_ob': remote_ob_provider,\n  'remote_lcpp': remote_lcpp_provider\n}", "}"]}
{"filename": "providers/llm/remote_ob.py", "chunked_list": ["import httpx \nimport json\nimport logging\nimport asyncio\nimport subprocess\nimport psutil\nfrom functools import partial\nfrom misc.memory_manager import mload\nfrom config_reader import config\nfrom providers.llm.abstract_llm import AbstractLLM", "from config_reader import config\nfrom providers.llm.abstract_llm import AbstractLLM\nfrom time import sleep\n\nlogger = logging.getLogger(__name__)\nllm_host = config.llm_host\nllm_load_started = False\n\nclass RemoteLLM(AbstractLLM):\n  assistant_mode = True\n  async def remote_llm_api(self, method, endpoint, payload):\n    async with httpx.AsyncClient() as client:\n      try:\n        if method == 'GET':\n          response = await client.get(url=f'{llm_host}/{endpoint}', params=payload, timeout=None)\n        else:\n          response = await client.post(url=f'{llm_host}/{endpoint}', json=payload, timeout=None)\n        if response.status_code == 200:\n          response_data = response.json()\n          return (False, response_data)\n        else:\n          return 'Connection error', None\n      except (httpx.NetworkError, ConnectionError, httpx.RemoteProtocolError, json.decoder.JSONDecodeError) as error:\n        return str(error), None\n      except Exception:\n        return 'Unknown error', None\n\n  def __init__(self, model_paths, init_config={}):\n    if config.llm_remote_launch_process_automatically and \\\n    config.mm_preload_models_on_start:\n      asyncio.run(self.run_llm_service())\n    else:\n      error, data = asyncio.run(self.remote_llm_api('GET', 'api/v1/model', {}))\n      if error:\n        logger.warn('Unable to get remote language model name: ' + str(error))\n      self.model = None\n      self.filename = data.get('result') if not error else 'Unknown model'\n\n  async def generate(self, prompt, length=64, model_params={}, assist=True):\n    if config.llm_remote_launch_process_automatically:\n      await self.run_llm_service()\n    data = {\n      'prompt': prompt,\n      'max_length': length,\n      **model_params,\n    }\n    error, response = await self.remote_llm_api('POST', 'api/v1/generate', data)\n    if not error:\n      response = response.get('results')[0].get('text')\n      logger.info(response)\n      return False, prompt + response\n    else:\n      return str(error), None\n\n  async def run_llm_service(self):\n    global llm_load_started, last_pid\n    if llm_load_started:\n      return\n    llm_load_started = True\n    service = mload('llm-remote_ob',  \n      partial(subprocess.Popen, config.llm_remote_launch_command.split(' '), cwd=config.llm_remote_launch_dir),\n      lambda p: p.terminate(),\n      lambda p: psutil.Process(p.pid).memory_info().rss, \n      gpu=True\n    )\n    if service.pid != last_pid:\n      await asyncio.sleep(config.llm_remote_launch_waittime)\n      await self.remote_llm_api('POST', 'api/v1/model', {'action': 'load', 'model_name': config.llm_remote_model_name}),\n      self.model = None\n      self.filename = config.llm_remote_model_name\n    llm_load_started=False\n    last_pid = service.pid\n    return service", "class RemoteLLM(AbstractLLM):\n  assistant_mode = True\n  async def remote_llm_api(self, method, endpoint, payload):\n    async with httpx.AsyncClient() as client:\n      try:\n        if method == 'GET':\n          response = await client.get(url=f'{llm_host}/{endpoint}', params=payload, timeout=None)\n        else:\n          response = await client.post(url=f'{llm_host}/{endpoint}', json=payload, timeout=None)\n        if response.status_code == 200:\n          response_data = response.json()\n          return (False, response_data)\n        else:\n          return 'Connection error', None\n      except (httpx.NetworkError, ConnectionError, httpx.RemoteProtocolError, json.decoder.JSONDecodeError) as error:\n        return str(error), None\n      except Exception:\n        return 'Unknown error', None\n\n  def __init__(self, model_paths, init_config={}):\n    if config.llm_remote_launch_process_automatically and \\\n    config.mm_preload_models_on_start:\n      asyncio.run(self.run_llm_service())\n    else:\n      error, data = asyncio.run(self.remote_llm_api('GET', 'api/v1/model', {}))\n      if error:\n        logger.warn('Unable to get remote language model name: ' + str(error))\n      self.model = None\n      self.filename = data.get('result') if not error else 'Unknown model'\n\n  async def generate(self, prompt, length=64, model_params={}, assist=True):\n    if config.llm_remote_launch_process_automatically:\n      await self.run_llm_service()\n    data = {\n      'prompt': prompt,\n      'max_length': length,\n      **model_params,\n    }\n    error, response = await self.remote_llm_api('POST', 'api/v1/generate', data)\n    if not error:\n      response = response.get('results')[0].get('text')\n      logger.info(response)\n      return False, prompt + response\n    else:\n      return str(error), None\n\n  async def run_llm_service(self):\n    global llm_load_started, last_pid\n    if llm_load_started:\n      return\n    llm_load_started = True\n    service = mload('llm-remote_ob',  \n      partial(subprocess.Popen, config.llm_remote_launch_command.split(' '), cwd=config.llm_remote_launch_dir),\n      lambda p: p.terminate(),\n      lambda p: psutil.Process(p.pid).memory_info().rss, \n      gpu=True\n    )\n    if service.pid != last_pid:\n      await asyncio.sleep(config.llm_remote_launch_waittime)\n      await self.remote_llm_api('POST', 'api/v1/model', {'action': 'load', 'model_name': config.llm_remote_model_name}),\n      self.model = None\n      self.filename = config.llm_remote_model_name\n    llm_load_started=False\n    last_pid = service.pid\n    return service", "\ninit = RemoteLLM\nlast_pid = -1 "]}
{"filename": "providers/llm/llama_cpp_provider.py", "chunked_list": ["from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\nfrom config_reader import config\nfrom providers.llm.abstract_llm import AbstractLLM\nimport asyncio\nimport os\nimport logging\nfrom misc.memory_manager import mload\nfrom functools import partial\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\ntry:\n  from llama_cpp import Llama\nexcept ImportError:\n  Llama = False\n\nclass LlamaCPP(AbstractLLM):\n  assistant_mode = True\n  def __init__(self, model_paths, init_config={}):\n    if not Llama:\n      logger.error('llama.cpp is not installed, run \"pip install llama-cpp-python\" to install it')\n      return logger.error('for GPU support, please read https://github.com/abetlen/llama-cpp-python')\n    override = init_config.get('llama_cpp_init', {})\n    lora_path = model_paths.get('path_to_llama_cpp_lora', None)\n    self.load_model = partial(\n      Llama,\n      n_ctx=min(init_config.get('context_size', 512), config.llm_lcpp_max_context_size),\n      rope_freq_base=init_config.get('rope_freq_base', 10000),\n      rope_freq_scale=init_config.get('rope_freq_scale', 1.0),\n      n_gpu_layers=config.llm_lcpp_gpu_layers,\n      model_path=model_paths[\"path_to_llama_cpp_weights\"],\n      seed=0,\n      lora_path=lora_path,\n      **override  \n    )\n    self.filename = os.path.basename(model_paths['path_to_llama_cpp_weights'])\n    if config.mm_preload_models_on_start:\n      m = self.model\n  \n  @property\n  def model(self):\n    return mload('llm-llama.cpp', self.load_model, None)\n    \n  async def generate(self, prompt, length=64, model_params={}, assist=True):\n    if 'repetition_penalty' in model_params:\n      model_params['repeat_penalty'] = model_params['repetition_penalty']\n      del model_params['repetition_penalty']\n    if 'early_stopping' in model_params:\n      del model_params['early_stopping']\n    output = error = None\n    with ThreadPoolExecutor():\n      try:\n        output = await asyncio.to_thread(\n          self.model, \n          prompt=prompt,\n          stop=[\"</s>\"],\n          max_tokens=length,\n           **model_params\n        )\n      except Exception as e:\n        error = str(e)\n    if not error:\n      output = output['choices'][0]['text']\n      logger.info(output)\n      output = prompt + output\n    return (False, output) if not error else (error, None)", "class LlamaCPP(AbstractLLM):\n  assistant_mode = True\n  def __init__(self, model_paths, init_config={}):\n    if not Llama:\n      logger.error('llama.cpp is not installed, run \"pip install llama-cpp-python\" to install it')\n      return logger.error('for GPU support, please read https://github.com/abetlen/llama-cpp-python')\n    override = init_config.get('llama_cpp_init', {})\n    lora_path = model_paths.get('path_to_llama_cpp_lora', None)\n    self.load_model = partial(\n      Llama,\n      n_ctx=min(init_config.get('context_size', 512), config.llm_lcpp_max_context_size),\n      rope_freq_base=init_config.get('rope_freq_base', 10000),\n      rope_freq_scale=init_config.get('rope_freq_scale', 1.0),\n      n_gpu_layers=config.llm_lcpp_gpu_layers,\n      model_path=model_paths[\"path_to_llama_cpp_weights\"],\n      seed=0,\n      lora_path=lora_path,\n      **override  \n    )\n    self.filename = os.path.basename(model_paths['path_to_llama_cpp_weights'])\n    if config.mm_preload_models_on_start:\n      m = self.model\n  \n  @property\n  def model(self):\n    return mload('llm-llama.cpp', self.load_model, None)\n    \n  async def generate(self, prompt, length=64, model_params={}, assist=True):\n    if 'repetition_penalty' in model_params:\n      model_params['repeat_penalty'] = model_params['repetition_penalty']\n      del model_params['repetition_penalty']\n    if 'early_stopping' in model_params:\n      del model_params['early_stopping']\n    output = error = None\n    with ThreadPoolExecutor():\n      try:\n        output = await asyncio.to_thread(\n          self.model, \n          prompt=prompt,\n          stop=[\"</s>\"],\n          max_tokens=length,\n           **model_params\n        )\n      except Exception as e:\n        error = str(e)\n    if not error:\n      output = output['choices'][0]['text']\n      logger.info(output)\n      output = prompt + output\n    return (False, output) if not error else (error, None)", "\n## process-based approach re-creates a new process and re-allocates memory on every run\n## which is not optimal, I leave this code for future reference\n# import functools\n# async def generate_mp(prompt, length=64, model_params={}, assist=True):\n#   with ProcessPoolExecutor(max_workers=1) as executor:\n#     loop = asyncio.get_event_loop()\n#     binded = functools.partial(\n#       model, \n#      prompt=prompt,", "#       model, \n#      prompt=prompt,\n#       stop=[\"</s>\"],\n#       max_tokens=length,\n#        **model_params\n#     )\n#     output = loop.run_in_executor(executor, binded)\n#     output = await output\n#     return prompt + output['choices'][0]['text']\ninit = LlamaCPP", "#     return prompt + output['choices'][0]['text']\ninit = LlamaCPP"]}
{"filename": "providers/llm/remote_llama_cpp.py", "chunked_list": ["import logging\nfrom config_reader import config\nfrom providers.llm.remote_ob import RemoteLLM\n\nlogger = logging.getLogger(__name__)\nllm_host = config.llm_host\nassistant_mode = True\n\n\nclass RemoteLLamaCPP(RemoteLLM):\n  async def generate(self, prompt, length=64, model_params={}, assist=True):\n    if config.llm_remote_launch_process_automatically:\n      self.run_llm_service()\n    data = {\n      'prompt': prompt,\n      'max_length': length,\n      'seed': -1,\n      **model_params,\n    }\n    error, response = await super().remote_llm_api('POST', 'completion', data)\n    if not error:\n      logger.info(response)\n      return False, prompt + response.get('content')\n    else:\n      return 'Error: ' + str(error), None", "\nclass RemoteLLamaCPP(RemoteLLM):\n  async def generate(self, prompt, length=64, model_params={}, assist=True):\n    if config.llm_remote_launch_process_automatically:\n      self.run_llm_service()\n    data = {\n      'prompt': prompt,\n      'max_length': length,\n      'seed': -1,\n      **model_params,\n    }\n    error, response = await super().remote_llm_api('POST', 'completion', data)\n    if not error:\n      logger.info(response)\n      return False, prompt + response.get('content')\n    else:\n      return 'Error: ' + str(error), None", "\ninit = RemoteLLamaCPP"]}
{"filename": "providers/llm/pytorch/gptj_provider.py", "chunked_list": ["import torch\nimport asyncio\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nfrom providers.llm.abstract_llm import AbstractLLM\n\ntokenizer = None\nmodel = None\ndevice = torch.device(\"cpu\")\n\nclass GPTJ(AbstractLLM):\n  def __init__(self, model_paths, init_config={}):\n    from transformers import AutoTokenizer, GPTJForCausalLM\n    weights = model_paths['path_to_gptj_weights']\n    self.tokenizer = AutoTokenizer.from_pretrained(weights)\n    self.model = GPTJForCausalLM.from_pretrained(weights, revision=\"float16\", torch_dtype=torch.float32, low_cpu_mem_usage=True)\n    self.model = self.model.to(device)\n    self.filename = os.path.basename(weights)\n\n  def tokenize(self, prompt):\n    encoded_prompt = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n    return encoded_prompt\n\n  async def generate(self, prompt, length=64, model_params={}, assist=False):\n    encoded_prompt = self.tokenize(prompt)\n    error = None\n    try:\n      with ThreadPoolExecutor():\n        output = await asyncio.to_thread(self.model.generate, \n          input_ids=encoded_prompt,\n          max_length=len(encoded_prompt[0]) + length,\n          do_sample=True,\n          **model_params\n        )\n    except Exception as e:\n      error = str(e)\n    return (False, self.tokenizer.batch_decode(output)[0]) if not error else (True, error)", "device = torch.device(\"cpu\")\n\nclass GPTJ(AbstractLLM):\n  def __init__(self, model_paths, init_config={}):\n    from transformers import AutoTokenizer, GPTJForCausalLM\n    weights = model_paths['path_to_gptj_weights']\n    self.tokenizer = AutoTokenizer.from_pretrained(weights)\n    self.model = GPTJForCausalLM.from_pretrained(weights, revision=\"float16\", torch_dtype=torch.float32, low_cpu_mem_usage=True)\n    self.model = self.model.to(device)\n    self.filename = os.path.basename(weights)\n\n  def tokenize(self, prompt):\n    encoded_prompt = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n    return encoded_prompt\n\n  async def generate(self, prompt, length=64, model_params={}, assist=False):\n    encoded_prompt = self.tokenize(prompt)\n    error = None\n    try:\n      with ThreadPoolExecutor():\n        output = await asyncio.to_thread(self.model.generate, \n          input_ids=encoded_prompt,\n          max_length=len(encoded_prompt[0]) + length,\n          do_sample=True,\n          **model_params\n        )\n    except Exception as e:\n      error = str(e)\n    return (False, self.tokenizer.batch_decode(output)[0]) if not error else (True, error)", "\ninit = GPTJ\n"]}
{"filename": "providers/llm/pytorch/llama_hf_provider.py", "chunked_list": ["import os\nimport torch\nfrom torch import mps\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\nfrom misc.mps_fixups import fixup_mps\nfrom config_reader import config\nfrom providers.llm.abstract_llm import AbstractLLM\n", "from providers.llm.abstract_llm import AbstractLLM\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() \\\n  else torch.device(\"cpu\") if not torch.backends.mps.is_available() \\\n  else torch.device('mps')\n\nif torch.backends.mps.is_available() and config.apply_mps_fixes:\n  fixup_mps()\n\nclass LlamaHuggingface(AbstractLLM):\n  submodel = None\n  assistant_mode = False\n  def __init__(self, model_paths, init_config={}):\n    tokenizer = model_paths['path_to_hf_llama']\n    weights = model_paths['path_to_hf_llama']\n    self.tokenizer = LlamaTokenizer.from_pretrained(tokenizer)\n    self.model = LlamaForCausalLM.from_pretrained(\n      weights, \n      torch_dtype=torch.float16 if device is not torch.device('cpu') else torch.float32,\n      device_map={\"\": device}\n    )\n\n    if os.path.exists(model_paths.get('path_to_llama_lora', '')):\n      from peft import PeftModel\n      self.submodel = PeftModel.from_pretrained(\n        self.model,\n        model_paths['path_to_llama_lora'],\n        device_map={\"\": device},\n        torch_dtype=torch.float16 if device is not torch.device('cpu') else torch.float32,\n      )\n      self.submodel.half()\n      self.submodel.eval()\n      self.assistant_mode = True\n\n    self.model.config.bos_token_id = 1\n    self.model.config.eos_token_id = 2\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    self.model.half()\n    self.model.eval()\n    self.filename = os.path.basename(model_paths['path_to_hf_llama'])\n\n  def tokenize(self, prompt):\n    return self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n  async def generate(self, prompt, length=64, model_params={}, use_submodel=False):\n    encoded_prompt = self.tokenize(prompt)\n    generation_config = GenerationConfig(\n      num_beams=1,\n      **model_params\n    )\n    model = self.submodel if use_submodel else self.model\n    error = None\n    try:\n      with ThreadPoolExecutor():\n        with torch.no_grad():\n          output = await asyncio.to_thread(model.generate, \n            input_ids=encoded_prompt,\n            max_new_tokens=length,\n            generation_config=generation_config,\n            eos_token_id=model.config.eos_token_id,\n            do_sample=True\n          )\n          output = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n      if torch.backends.mps.is_available():\n        mps.empty_cache()\n      elif torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    except Exception as e:\n      error = str(e)\n    return (False, output[0]) if not error else (True, error)", "\nclass LlamaHuggingface(AbstractLLM):\n  submodel = None\n  assistant_mode = False\n  def __init__(self, model_paths, init_config={}):\n    tokenizer = model_paths['path_to_hf_llama']\n    weights = model_paths['path_to_hf_llama']\n    self.tokenizer = LlamaTokenizer.from_pretrained(tokenizer)\n    self.model = LlamaForCausalLM.from_pretrained(\n      weights, \n      torch_dtype=torch.float16 if device is not torch.device('cpu') else torch.float32,\n      device_map={\"\": device}\n    )\n\n    if os.path.exists(model_paths.get('path_to_llama_lora', '')):\n      from peft import PeftModel\n      self.submodel = PeftModel.from_pretrained(\n        self.model,\n        model_paths['path_to_llama_lora'],\n        device_map={\"\": device},\n        torch_dtype=torch.float16 if device is not torch.device('cpu') else torch.float32,\n      )\n      self.submodel.half()\n      self.submodel.eval()\n      self.assistant_mode = True\n\n    self.model.config.bos_token_id = 1\n    self.model.config.eos_token_id = 2\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    self.model.half()\n    self.model.eval()\n    self.filename = os.path.basename(model_paths['path_to_hf_llama'])\n\n  def tokenize(self, prompt):\n    return self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n  async def generate(self, prompt, length=64, model_params={}, use_submodel=False):\n    encoded_prompt = self.tokenize(prompt)\n    generation_config = GenerationConfig(\n      num_beams=1,\n      **model_params\n    )\n    model = self.submodel if use_submodel else self.model\n    error = None\n    try:\n      with ThreadPoolExecutor():\n        with torch.no_grad():\n          output = await asyncio.to_thread(model.generate, \n            input_ids=encoded_prompt,\n            max_new_tokens=length,\n            generation_config=generation_config,\n            eos_token_id=model.config.eos_token_id,\n            do_sample=True\n          )\n          output = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n      if torch.backends.mps.is_available():\n        mps.empty_cache()\n      elif torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    except Exception as e:\n      error = str(e)\n    return (False, output[0]) if not error else (True, error)", "\ninit = LlamaHuggingface"]}
{"filename": "providers/llm/pytorch/auto_hf_provider.py", "chunked_list": ["import torch\nimport asyncio\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nfrom providers.llm.abstract_llm import AbstractLLM\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass AutoHF(AbstractLLM):\n  def __init__(self, model_paths, init_config={}):\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    weights = model_paths['path_to_autohf_weights']\n    self.tokenizer = AutoTokenizer.from_pretrained(weights)\n    self.model = AutoModelForCausalLM.from_pretrained(weights)\n    self.filename = os.path.basename(weights)\n\n  def tokenize(self, prompt):\n    encoded_prompt = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n    return encoded_prompt\n\n  async def generate(self, prompt, length=64, model_params={}, assist=False):\n    error = None\n    try:\n      encoded_prompt = self.tokenize(prompt)\n      if 'early_stopping' in model_params:\n        del model_params['early_stopping']\n      with ThreadPoolExecutor():\n        output = await asyncio.to_thread(self.model.generate, \n          input_ids=encoded_prompt,\n          no_repeat_ngram_size=2,\n          max_new_tokens=length,\n          early_stopping=True,\n          do_sample=True,\n          **model_params\n        )\n    except Exception as e:\n      error = str(e)\n    return (False, self.tokenizer.batch_decode(output, skip_special_tokens=True)[0]) if not error else (True, error)", "class AutoHF(AbstractLLM):\n  def __init__(self, model_paths, init_config={}):\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    weights = model_paths['path_to_autohf_weights']\n    self.tokenizer = AutoTokenizer.from_pretrained(weights)\n    self.model = AutoModelForCausalLM.from_pretrained(weights)\n    self.filename = os.path.basename(weights)\n\n  def tokenize(self, prompt):\n    encoded_prompt = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n    return encoded_prompt\n\n  async def generate(self, prompt, length=64, model_params={}, assist=False):\n    error = None\n    try:\n      encoded_prompt = self.tokenize(prompt)\n      if 'early_stopping' in model_params:\n        del model_params['early_stopping']\n      with ThreadPoolExecutor():\n        output = await asyncio.to_thread(self.model.generate, \n          input_ids=encoded_prompt,\n          no_repeat_ngram_size=2,\n          max_new_tokens=length,\n          early_stopping=True,\n          do_sample=True,\n          **model_params\n        )\n    except Exception as e:\n      error = str(e)\n    return (False, self.tokenizer.batch_decode(output, skip_special_tokens=True)[0]) if not error else (True, error)", "\ninit = AutoHF\n"]}
{"filename": "providers/llm/pytorch/llama_orig_provider.py", "chunked_list": ["import os\nimport sys\nimport torch\nfrom concurrent.futures import ThreadPoolExecutor\nfrom utils import b64_to_img\nfrom providers.llm.abstract_llm import AbstractLLM\nimport asyncio\nimport inspect\n\n#python3.10 -m torch.distributed.launch --use_env bot.py", "\n#python3.10 -m torch.distributed.launch --use_env bot.py\n\nclass LlamaOrig(AbstractLLM):\n  generator = None\n  assistant_mode = False\n  visual_mode = False\n  def __init__(self, model_paths, init_config={}):\n    llama_weights = model_paths['path_to_llama_weights']\n    llama_tokenizer = model_paths['path_to_llama_tokenizer']\n    sys.path.append(model_paths['path_to_llama_code'])\n    self.filename = os.path.basename(llama_weights)\n    if os.path.exists(model_paths.get('path_to_llama_multimodal_adapter', '')):\n      self._load_multimodal_adapter(model_paths, llama_weights, llama_tokenizer)\n    else:\n      self._load_llama_model(model_paths, llama_weights, llama_tokenizer)\n\n  def _load_llama_model(self, model_paths, llama_weights, llama_tokenizer):\n    from example import setup_model_parallel, load\n    with torch.inference_mode(mode=True):\n      local_rank, world_size = setup_model_parallel()\n      if 'adapter_path' in inspect.signature(load).parameters and \\\n         'path_to_llama_adapter' in model_paths and \\\n          os.path.exists(model_paths.get('path_to_llama_adapter', None)): \n        self.model = load(\n          llama_weights, llama_tokenizer, model_paths['path_to_llama_adapter'], local_rank, world_size, 1024, 1\n        )\n        self.assistant_mode = True\n      else:\n        self.model = load(\n          llama_weights, llama_tokenizer, local_rank, world_size, 1024, 1\n        )\n\n  def _load_multimodal_adapter(self, model_paths, llama_weights, llama_tokenizer):\n    global generator, assistant_mode, visual_mode\n    import llama\n    device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available else 'cpu')\n    lpath = os.path.dirname(llama_tokenizer)\n    orig_generator, preprocess = llama.load(model_paths['path_to_llama_multimodal_adapter'], lpath, device)\n    self.assistant_mode = True\n    self.visual_mode = True\n    class Wrapped_generator():\n      def generate(self, prompt, use_adapter=True, visual_input=False, **kwargs):\n        if visual_input:\n          img = b64_to_img(visual_input)\n          img = preprocess(img).unsqueeze(0).half().to(device)\n        else:\n          img = []\n        generated = orig_generator.generate(img, prompt, **kwargs)\n        return [prompt[0] + generated[0]]\n    self.model = Wrapped_generator()\n\n  async def generate(self, prompt, max_gen_len=64, params={}, assist=False):\n    available_params = inspect.signature(self.model.generate).parameters\n    for param in list(params):\n      if param not in available_params:\n        del params[param]\n    error = None\n    with ThreadPoolExecutor():\n      if self.assistant_mode and 'use_adapter' in available_params:\n        params['use_adapter'] = assist\n      try:\n        results = await asyncio.to_thread(self.model.generate,\n          [prompt], max_gen_len=max_gen_len, **params\n        )\n      except Exception as e:\n        error = str(e)\n    return (False, results[0]) if not error else (True, error)", "\ninit = LlamaOrig"]}
{"filename": "providers/llm/pytorch/gpt2_provider.py", "chunked_list": ["import torch\nimport os\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nfrom types import SimpleNamespace\nfrom providers.llm.abstract_llm import AbstractLLM\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass GPT2(AbstractLLM):\n  is_nanoGPT = False\n  assistant_mode = False\n  def __init__(self, model_paths, init_config={}):\n    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n    weights = model_paths['path_to_gpt2_weights']\n    self.filename = os.path.basename(weights)\n    if 'use_tiktoken' in init_config and init_config['use_tiktoken']:\n      import tiktoken\n      tk = tiktoken.get_encoding(\"gpt2\")\n      tokenizer = {}\n      tokenizer[\"encode\"] = lambda s, *args, **kwargs: torch.tensor(tk.encode(s, allowed_special={\"<|endoftext|>\"}), dtype=torch.long, device=device)[None, ...]\n      tokenizer[\"decode\"] = lambda l: tk.decode(l.tolist())\n      tokenizer[\"name\"] = 'tiktoken'\n      tokenizer = SimpleNamespace(**tokenizer)\n      self.tokenizer = tokenizer\n    else:\n      self.tokenizer = GPT2Tokenizer.from_pretrained(weights)\n    if 'nanogpt' in init_config and init_config['nanogpt']:\n      import sys\n      sys.path.append(model_paths['path_to_minchatgpt_code'])\n      from gpt import GPT\n      from configs import get_configs\n      self.is_nanoGPT = True\n      cfg = get_configs(\"gpt2-medium\")\n      model = GPT(cfg)\n      model.load_state_dict(state_dict=torch.load(weights), strict=False)\n      self.assistant_mode = True\n    else:\n      model = GPT2LMHeadModel.from_pretrained(weights)\n    self.model = model.to(device)\n\n  def tokenize(self, prompt):\n    encoded_prompt = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n    encoded_prompt = encoded_prompt.to(device)\n    return encoded_prompt[:, :1024]\n\n  async def generate(self, prompt, length=64, model_params={}, assist=False):\n    error = None\n    try:\n      encoded_prompt = self.tokenize(prompt)\n      with ThreadPoolExecutor():\n        if not self.is_nanoGPT:\n          output_sequences = await asyncio.to_thread(self.model.generate, \n            input_ids=encoded_prompt,\n            max_length=length + len(encoded_prompt[0]),\n            do_sample=True,\n            num_return_sequences=1,\n            **model_params\n          )\n        else:\n          if 'early_stopping' in model_params:\n            del model_params['early_stopping']\n          output_sequences = await asyncio.to_thread(self.model.generate, \n            encoded_prompt,\n            length,\n            **model_params\n          )\n    except Exception as e:\n      error = str(e)\n    return (False, self.tokenizer.decode(output_sequences[0])) if not error else (True, error)", "\nclass GPT2(AbstractLLM):\n  is_nanoGPT = False\n  assistant_mode = False\n  def __init__(self, model_paths, init_config={}):\n    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n    weights = model_paths['path_to_gpt2_weights']\n    self.filename = os.path.basename(weights)\n    if 'use_tiktoken' in init_config and init_config['use_tiktoken']:\n      import tiktoken\n      tk = tiktoken.get_encoding(\"gpt2\")\n      tokenizer = {}\n      tokenizer[\"encode\"] = lambda s, *args, **kwargs: torch.tensor(tk.encode(s, allowed_special={\"<|endoftext|>\"}), dtype=torch.long, device=device)[None, ...]\n      tokenizer[\"decode\"] = lambda l: tk.decode(l.tolist())\n      tokenizer[\"name\"] = 'tiktoken'\n      tokenizer = SimpleNamespace(**tokenizer)\n      self.tokenizer = tokenizer\n    else:\n      self.tokenizer = GPT2Tokenizer.from_pretrained(weights)\n    if 'nanogpt' in init_config and init_config['nanogpt']:\n      import sys\n      sys.path.append(model_paths['path_to_minchatgpt_code'])\n      from gpt import GPT\n      from configs import get_configs\n      self.is_nanoGPT = True\n      cfg = get_configs(\"gpt2-medium\")\n      model = GPT(cfg)\n      model.load_state_dict(state_dict=torch.load(weights), strict=False)\n      self.assistant_mode = True\n    else:\n      model = GPT2LMHeadModel.from_pretrained(weights)\n    self.model = model.to(device)\n\n  def tokenize(self, prompt):\n    encoded_prompt = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n    encoded_prompt = encoded_prompt.to(device)\n    return encoded_prompt[:, :1024]\n\n  async def generate(self, prompt, length=64, model_params={}, assist=False):\n    error = None\n    try:\n      encoded_prompt = self.tokenize(prompt)\n      with ThreadPoolExecutor():\n        if not self.is_nanoGPT:\n          output_sequences = await asyncio.to_thread(self.model.generate, \n            input_ids=encoded_prompt,\n            max_length=length + len(encoded_prompt[0]),\n            do_sample=True,\n            num_return_sequences=1,\n            **model_params\n          )\n        else:\n          if 'early_stopping' in model_params:\n            del model_params['early_stopping']\n          output_sequences = await asyncio.to_thread(self.model.generate, \n            encoded_prompt,\n            length,\n            **model_params\n          )\n    except Exception as e:\n      error = str(e)\n    return (False, self.tokenizer.decode(output_sequences[0])) if not error else (True, error)", "\ninit = GPT2"]}
{"filename": "providers/stt/whisper.py", "chunked_list": ["from whispercpp import Whisper\nfrom providers.stt.abstract_stt import AbstractSTT\nfrom config_reader import config\nfrom concurrent.futures import ThreadPoolExecutor\nfrom misc.memory_manager import mload\nfrom functools import partial\nimport asyncio\n\nclass WhisperCPP(AbstractSTT):\n  def __init__(self):\n    if config.mm_preload_models_on_start:\n      m = self.model\n\n  @property\n  def model(self):\n    loader = partial(Whisper, config.stt_model_path_or_name)\n    return mload('stt-whisper', loader, None)\n  \n  async def recognize(self, audio_path):\n    try:\n      with ThreadPoolExecutor():\n        output = await asyncio.to_thread(self.model.transcribe, audio_path)\n      text = ''.join(self.model.extract_text(output))\n      return False, text\n    except Exception as e:\n      return str(e), None ", "class WhisperCPP(AbstractSTT):\n  def __init__(self):\n    if config.mm_preload_models_on_start:\n      m = self.model\n\n  @property\n  def model(self):\n    loader = partial(Whisper, config.stt_model_path_or_name)\n    return mload('stt-whisper', loader, None)\n  \n  async def recognize(self, audio_path):\n    try:\n      with ThreadPoolExecutor():\n        output = await asyncio.to_thread(self.model.transcribe, audio_path)\n      text = ''.join(self.model.extract_text(output))\n      return False, text\n    except Exception as e:\n      return str(e), None ", "\ninit = WhisperCPP"]}
{"filename": "providers/stt/__init__.py", "chunked_list": ["import importlib\nwhispercpp = lambda: importlib.import_module('providers.stt.whisper')\nsilero = lambda: importlib.import_module('providers.stt.silero')\nwav2vec2 = lambda: importlib.import_module('providers.stt.wav2vec2')\n\nbackends = {\n  'whisper': whispercpp,\n  'silero': silero,\n  'wav2vec2': wav2vec2\n}", "  'wav2vec2': wav2vec2\n}"]}
{"filename": "providers/stt/silero.py", "chunked_list": ["import torch\nimport torchaudio\nfrom providers.stt.abstract_stt import AbstractSTT\nfrom config_reader import config\nfrom concurrent.futures import ThreadPoolExecutor\nimport asyncio\nfrom glob import glob\nfrom misc.memory_manager import mload\n\ndevice = torch.device('cpu')", "\ndevice = torch.device('cpu')\n\nclass SileroSTT(AbstractSTT):\n  def __init__(self):\n    if config.mm_preload_models_on_start:\n      mload('st-wav2vec2', self.load, None)\n\n  def load(self):\n    model, decoder, utils = torch.hub.load(\n      repo_or_dir='snakers4/silero-models',\n      model='silero_stt',\n      language=config.stt_model_path_or_name if len(config.stt_model_path_or_name) == 2 else 'en',\n      device=device\n    )\n    return model, utils, decoder\n  \n  def stt(self, path):\n    model, utils, decoder = mload('st-silero', self.load, None)\n    read_batch, split_into_batches, read_audio, prepare_model_input = utils\n    test_files = glob(path)\n    batches = split_into_batches(test_files, batch_size=10)\n    input = prepare_model_input(read_batch(batches[0]), device=device)\n    output = model(input)\n    transcript = '. '.join([decoder(x.cpu()) for x in output])\n    print(transcript)\n    return transcript\n  \n  async def recognize(self, audio_path):\n    try:\n      with ThreadPoolExecutor():\n        text = await asyncio.to_thread(self.stt, audio_path)\n      return False, text\n    except Exception as e:\n      return str(e), None ", "\ninit = SileroSTT"]}
{"filename": "providers/stt/wav2vec2.py", "chunked_list": ["import torch\nimport torchaudio\nfrom transformers import SpeechEncoderDecoderModel, Wav2Vec2Processor\nfrom providers.stt.abstract_stt import AbstractSTT\nfrom config_reader import config\nfrom concurrent.futures import ThreadPoolExecutor\nfrom misc.memory_manager import mload\nimport asyncio\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")", "\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass Wav2Vec2(AbstractSTT): \n  def __init__(self):\n    if config.mm_preload_models_on_start:\n      mload('st-wav2vec2', self.load, None)\n\n  def load(self):\n    processor = Wav2Vec2Processor.from_pretrained(config.stt_model_path_or_name)\n    model = SpeechEncoderDecoderModel.from_pretrained(config.stt_model_path_or_name)\n    if device != 'cpu':\n      model = model.to(device)\n      processor = processor\n    return (model, processor)\n  \n  def stt(self, path):\n    model, processor = mload('st-wav2vec2', self.load, None)\n    waveform, sample_rate = torchaudio.load(path, normalize=True)\n    if waveform.shape[0] == 2:\n        waveform = torch.mean(waveform, dim=0)\n    if sample_rate != 16000:\n        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n        waveform = resampler(waveform)\n    waveform = waveform.squeeze()\n    processed = processor(waveform, sampling_rate=16_000,\n                      return_tensors=\"pt\", padding='longest', device=device)\n    if device != 'cpu':\n      processed['input_values'] = processed['input_values'].to(device)\n      processed['attention_mask'] = processed['attention_mask'].to(device)\n    print(processed)\n    with torch.no_grad():\n      predicted_ids = model.generate(**processed)\n    \n    predicted_sentences = processor.batch_decode(\n        predicted_ids,\n        num_processes=8,\n        skip_special_tokens=True\n    )\n    print(predicted_sentences)\n    return ' '.join(predicted_sentences)\n\n  async def recognize(self, audio_path):\n    try:\n      with ThreadPoolExecutor():\n        text = await asyncio.to_thread(self.stt, audio_path)\n      return False, text\n    except AssertionError as e:\n      print(e)\n      return str(e), None ", "\ninit = Wav2Vec2"]}
{"filename": "providers/stt/abstract_stt.py", "chunked_list": ["from abc import ABCMeta, abstractmethod\n\n\nclass AbstractSTT(metaclass=ABCMeta):\n  model = None\n  def __init__(self):\n    return None\n\n  @abstractmethod\n  def recognize(self, audio_path):\n    pass"]}
{"filename": "modules/tts.py", "chunked_list": ["from aiogram.filters import Command, CommandObject\nfrom aiogram.types import Message, BufferedInputFile\nfrom providers.tts_provider import tts, remote_tts, convert_to_ogg, so_vits_svc\nfrom custom_queue import UserLimitedQueue, semaphore_wrapper\nfrom config_reader import config\nfrom utils import download_audio\nimport asyncio\nimport tempfile\n\nclass TextToSpeechModule:\n  def __init__(self, dp, bot):\n    self.queue = UserLimitedQueue(config.tts_queue_size_per_user)\n    self.semaphore = asyncio.Semaphore(1)\n    \n    so_vits_svc_voices = list(v['voice'].lower().replace('-','') for v in config.tts_so_vits_svc_voices)\n    all_voices = [*config.tts_voices, *so_vits_svc_voices]\n    @dp.message(Command(commands=[\"tts\", *config.tts_voices, *so_vits_svc_voices]), flags={\"long_operation\": \"record_audio\"})\n    async def command_tts_handler(message: Message, command: CommandObject) -> None:\n      with self.queue.for_user(message.from_user.id) as available:\n        if available:\n          #show helper message if no voice is selected\n          if command.command == \"tts\" or not command.args or str(command.args).strip() == \"\" or ('-help' in str(command.args)):\n            return await message.answer(f\"usage: {' '.join(['/' + x for x in all_voices])} text, /revoice [recording]\\nUse the commands like /command@botname \\n{config.tts_credits}\")\n          voice = command.command\n          text = str(command.args)\n          task_function = remote_tts if config.tts_mode != 'local' else tts\n          wrapped_runner = semaphore_wrapper(self.semaphore, task_function)\n          error, data = await wrapped_runner(voice, text)\n          if error:\n            return await message.answer(f\"Error, <b>{error}</b>\")\n          else:\n            audio = BufferedInputFile(convert_to_ogg(data), 'tts.ogg')\n            return await message.answer_voice(voice=audio)\n    \n    if config.tts_enable_so_vits_svc:\n      @dp.message(Command(commands=[\"revoice\", \"sts\"]), flags={\"long_operation\": \"record_audio\"})\n      async def revoice(message: Message, command: CommandObject) -> None:\n        voice = str(command.args).split(' ')[0] if command.args else so_vits_svc_voices[0]\n        voice = voice if voice in so_vits_svc_voices else None\n        if not voice:\n          return await message.answer(\"<b>Voice not found</b>, available speech-to-speech voices: \" +\n           \", \".join(so_vits_svc_voices))\n        if message.reply_to_message:\n          if message.reply_to_message.voice:\n            with tempfile.NamedTemporaryFile(suffix='.ogg', delete=False) as temp_file:\n              await download_audio(bot, message.reply_to_message.voice.file_id, temp_file.name)\n              wrapped_runner = semaphore_wrapper(self.semaphore, so_vits_svc)\n              error, data = await wrapped_runner(voice, None, temp_file.name)\n              if error:\n                return await message.answer(f\"Error, <b>{error}</b>\")\n              else:\n                audio = BufferedInputFile(convert_to_ogg(data), 'tts.ogg')\n                return await message.answer_voice(voice=audio)\n        return await message.answer(\"No audio found. Use this command replying to voice messages\")\n\n    bot.reply_tts = command_tts_handler", "\nclass TextToSpeechModule:\n  def __init__(self, dp, bot):\n    self.queue = UserLimitedQueue(config.tts_queue_size_per_user)\n    self.semaphore = asyncio.Semaphore(1)\n    \n    so_vits_svc_voices = list(v['voice'].lower().replace('-','') for v in config.tts_so_vits_svc_voices)\n    all_voices = [*config.tts_voices, *so_vits_svc_voices]\n    @dp.message(Command(commands=[\"tts\", *config.tts_voices, *so_vits_svc_voices]), flags={\"long_operation\": \"record_audio\"})\n    async def command_tts_handler(message: Message, command: CommandObject) -> None:\n      with self.queue.for_user(message.from_user.id) as available:\n        if available:\n          #show helper message if no voice is selected\n          if command.command == \"tts\" or not command.args or str(command.args).strip() == \"\" or ('-help' in str(command.args)):\n            return await message.answer(f\"usage: {' '.join(['/' + x for x in all_voices])} text, /revoice [recording]\\nUse the commands like /command@botname \\n{config.tts_credits}\")\n          voice = command.command\n          text = str(command.args)\n          task_function = remote_tts if config.tts_mode != 'local' else tts\n          wrapped_runner = semaphore_wrapper(self.semaphore, task_function)\n          error, data = await wrapped_runner(voice, text)\n          if error:\n            return await message.answer(f\"Error, <b>{error}</b>\")\n          else:\n            audio = BufferedInputFile(convert_to_ogg(data), 'tts.ogg')\n            return await message.answer_voice(voice=audio)\n    \n    if config.tts_enable_so_vits_svc:\n      @dp.message(Command(commands=[\"revoice\", \"sts\"]), flags={\"long_operation\": \"record_audio\"})\n      async def revoice(message: Message, command: CommandObject) -> None:\n        voice = str(command.args).split(' ')[0] if command.args else so_vits_svc_voices[0]\n        voice = voice if voice in so_vits_svc_voices else None\n        if not voice:\n          return await message.answer(\"<b>Voice not found</b>, available speech-to-speech voices: \" +\n           \", \".join(so_vits_svc_voices))\n        if message.reply_to_message:\n          if message.reply_to_message.voice:\n            with tempfile.NamedTemporaryFile(suffix='.ogg', delete=False) as temp_file:\n              await download_audio(bot, message.reply_to_message.voice.file_id, temp_file.name)\n              wrapped_runner = semaphore_wrapper(self.semaphore, so_vits_svc)\n              error, data = await wrapped_runner(voice, None, temp_file.name)\n              if error:\n                return await message.answer(f\"Error, <b>{error}</b>\")\n              else:\n                audio = BufferedInputFile(convert_to_ogg(data), 'tts.ogg')\n                return await message.answer_voice(voice=audio)\n        return await message.answer(\"No audio found. Use this command replying to voice messages\")\n\n    bot.reply_tts = command_tts_handler", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n\n\n", "\n\n\n"]}
{"filename": "modules/llm.py", "chunked_list": ["from aiogram.filters import Command, CommandObject\nfrom aiogram.types import Message, BufferedInputFile, InputMediaPhoto, URLInputFile\nfrom custom_queue import UserLimitedQueue, semaphore_wrapper\nfrom config_reader import config\nfrom aiogram import html, F\nfrom providers.llm_provider import active_model\nfrom chroniclers.base import chroniclers\nfrom types import SimpleNamespace\nfrom utils import parse_photo, tg_image_to_data\nimport asyncio", "from utils import parse_photo, tg_image_to_data\nimport asyncio\nimport json\n\ndef get_chat_id(message):\n  return message.from_user.id if config.llm_history_grouping == 'user' else message.chat.id\n\ndef assistant_model_available(model):\n  return (hasattr(model, 'assistant_mode') and model.assistant_mode) or \\\n    config.llm_force_assistant_for_unsupported_models", "\ndef visual_mode_available(model):\n  return (hasattr(model, 'visual_mode') and model.assistant_mode)\n\nclass LargeLanguageModel:\n  async def assist(self, text, context):\n    params = {**self.assistant.gen_cfg(self.assistant_cfg_override), **context.get('img_input',{})}\n    return await self.complete_with_chronicler(text, self.assistant, context, params, config.llm_max_assistant_tokens)\n\n  async def chat(self, text, context):\n    params = self.chatter.gen_cfg(self.chat_cfg_override)\n    return await self.complete_with_chronicler(text, self.chatter, context, params, config.llm_max_tokens)\n\n  async def reply(self, text, context, message):\n    params = self.assistant.gen_cfg(self.assistant_cfg_override)\n    context['reply_text'] = message.reply_to_message.text\n    return await self.complete_with_chronicler(text, self.replier, context, params, config.llm_max_assistant_tokens)\n\n  async def complete_with_chronicler(self, text, chronicler, context, params, length):\n    prompt = chronicler.prepare({\n      \"message\": text, \n      \"model\": self.model,\n      **context\n    })\n    wrapped_runner = semaphore_wrapper(self.semaphore, self.model.generate)\n    error, result = await wrapped_runner(prompt, config.llm_max_assistant_tokens, params)\n    return chronicler.parse(result, context.get('chat_id', 0), len(prompt)) if not error else str(error)\n\n  async def complete_raw(self, text, context):\n    wrapped_runner = semaphore_wrapper(self.semaphore, self.model.generate)\n    error, result = await wrapped_runner(text, config.llm_max_assistant_tokens, self.assistant.gen_cfg({}))\n    return result\n\n  def should_use_reply_chronicler(self, message, bot):\n    reply = message.reply_to_message\n    is_reply_from_bot = reply and reply.from_user.id == bot._me.id\n    is_qa_format = reply and reply.text and reply.text.startswith('Q:')\n    always_assist = config.llm_assistant_use_in_chat_mode and assistant_model_available(self.model)\n    return is_reply_from_bot and (is_qa_format or always_assist)\n\n  def get_common_chat_attributes(self, message, additional={}):\n    return {\n      'author': message.from_user.first_name.replace(' ', '') or 'User',\n      'chat_id': get_chat_id(message),\n      'user_id': message.from_user.id,\n      **additional\n    }\n\n  def __init__(self, dp, bot):\n    self.queue = UserLimitedQueue(config.llm_queue_size_per_user)\n    self.semaphore = asyncio.Semaphore(1)\n    self.chatter = chroniclers['chat'](config.llm_character, False, config.llm_max_history_items)\n    self.assistant = chroniclers[config.llm_assistant_chronicler](config.llm_character)\n    self.replier = chroniclers['reply'](config.llm_character)\n    model = active_model.init(config.llm_paths, self.chatter.init_cfg())\n    self.model = model\n    self.chat_cfg_override = config.llm_generation_cfg_override\n    self.assistant_cfg_override = config.llm_assistant_cfg_override\n    \n    @dp.message((F.text[0] if F.text else '') != '/', flags={\"long_operation\": \"typing\"})\n    async def handle_messages(message: Message):\n      parse_reply = self.should_use_reply_chronicler(message, bot)\n      # if should be handled in assistant mode\n      if (config.llm_assistant_use_in_chat_mode and assistant_model_available(model))\\\n      or (visual_mode_available(model) and parse_photo(message)) or parse_reply:\n        command = SimpleNamespace(args=message.text, parse_reply=parse_reply)\n        return await assist_message(message=message, command=command)\n      with self.queue.for_user(message.from_user.id) as available:\n        if available:\n          output = await self.chat(message.text, self.get_common_chat_attributes(message))\n          await message.reply(text=html.quote(output))\n\n    @dp.message(Command(commands=['reset', 'clear']), flags={\"cooldown\": 20})\n    async def clear_llm_history(message: Message, command: Command):\n      self.chatter.history[get_chat_id(message)] = []\n    \n    @dp.message(Command(commands=['ask', 'assist']), flags={\"long_operation\": \"typing\"})\n    async def assist_message(message: Message, command: Command):\n      msg = str(command.args).strip()\n      if not (msg and command.args):\n        return\n      if not assistant_model_available(model):\n        return await message.reply(text='Assistant model is not available')\n      img_input = {\"visual_input\": await tg_image_to_data(parse_photo(message), bot)}\\\n                  if visual_mode_available(model) \\\n                  else {}\n      with self.queue.for_user(message.from_user.id) as available:\n        if not available:\n          return\n        if hasattr(command, 'parse_reply') and not img_input:\n          output = await self.reply(msg,\n            self.get_common_chat_attributes(message),\n            message\n          )\n        else:\n          output = await self.assist(msg, \n            self.get_common_chat_attributes(message, {'img_imput': img_input})\n          )\n      if config.llm_assistant_use_in_chat_mode and not hasattr(command, 'command'):\n        reply = html.quote(output)\n      else:\n        reply = f'<b>Q</b>: {msg}\\n\\n<b>A</b>: {html.quote(output)}'\n      await message.reply(text=(reply), allow_sending_without_reply=True)\n\n    @dp.message(Command(commands=['llm']), flags={\"cooldown\": 20})\n    async def helpfunction(message: Message, command: Command):\n      text = f'''[LLM module].\n      Commands: /ask\n      Active model type: {config.llm_backend}\n      Assistant mode: {str(assistant_model_available(model))} ({config.llm_assistant_chronicler})\n      Character: {config.llm_character}\n      Context visibility: {config.llm_history_grouping}\n      Model: {model.filename if model.model else 'unknown'}\n      Config: \n{json.dumps(self.chatter.gen_cfg(self.assistant_cfg_override), sort_keys=True, indent=4)}'''\n      return await message.reply(text=text)"]}
{"filename": "modules/admin.py", "chunked_list": ["from aiogram.filters import Command, CommandObject\nfrom aiogram.types import Message\nfrom utils import parse_photo, log_exceptions\nimport logging\nlogger = logging.getLogger(__name__)\n\n\nclass AdminModule:\n  def __init__(self, dp, bot):\n    @dp.message(Command(commands=[\"sendpic\"]), flags={\"admins_only\": True})\n    @log_exceptions(logger)\n    async def send_pic(message: Message, command: CommandObject) -> None:\n      photo = parse_photo(message)\n      await bot.send_photo(chat_id = int(str(command.args)), photo = photo[-1].file_id)\n\n    @dp.message(Command(commands=[\"delete\"]), flags={\"admins_only\": True})\n    @log_exceptions(logger)\n    async def delete_msg(message: Message, command: CommandObject) -> None:\n      await bot.delete_message(chat_id = message.chat.id, message_id = message.reply_to_message.message_id)\n\n    @dp.message(Command(commands=[\"info\"]), flags={\"admins_only\": True})\n    async def chat_info(message: Message, command: CommandObject) -> None:\n      msg = message if not message.reply_to_message else message.reply_to_message\n      prefix = '[reply info]\\n' if message.reply_to_message else ''\n      await message.reply(f'{prefix}Chat ID: {msg.chat.id}\\nUser ID: {msg.from_user.id}')", ""]}
{"filename": "modules/sd.py", "chunked_list": ["\nfrom aiogram import html, F\nfrom aiogram.filters import Command, CommandObject\nfrom aiogram.types import Message, BufferedInputFile, InputMediaPhoto\nfrom providers.sd_provider import tti, iti, models, embeddings, loras, switch_model, refresh_model_list\nfrom utils import tg_image_to_data, parse_photo, CustomArgumentParser, JoinNargsAction\nfrom custom_queue import UserLimitedQueue, semaphore_wrapper\nfrom typing import Literal\nfrom config_reader import config\nimport asyncio", "from config_reader import config\nimport asyncio\nimport pydantic\nimport re\nimport shlex\nimport random\n\nsd_available_resolutions = list(range(256, config.sd_max_resolution + 1, 64))\n\nclass SDArguments(pydantic.BaseModel):\n  denoising_strength: float = pydantic.Field(None, ge=0, le=1, alias='d', description='Denoising strength')\n  cfg_scale: float = pydantic.Field(None, ge=1, le=25, alias='c', description='Cfg scale')\n  steps: int = pydantic.Field(None, ge=5, le=config.sd_max_steps, alias='st')\n  sampler_name: Literal[tuple(config.sd_available_samplers)] = pydantic.Field(None, alias=\"sa\", description='Sampler')\n  width: Literal[tuple(sd_available_resolutions)] = pydantic.Field(None, alias=\"wi\", description='Width')\n  height: Literal[tuple(sd_available_resolutions)] = pydantic.Field(None, alias=\"he\", description='Height')\n  negative_prompt: str = pydantic.Field(None, alias='np', description='Negative prompt')\n  seed: int = pydantic.Field(None, ge=-1, alias='se', description='Seed')\n  prompt: str = pydantic.Field(None)\n  mask: Literal[1, -1] = pydantic.Field(None, alias='ma', description='Inpaint mask')\n  inpainting_fill: int = pydantic.Field(None, ge=0, le=3, alias='fi', description='Inpaint fill mode')", "\nclass SDArguments(pydantic.BaseModel):\n  denoising_strength: float = pydantic.Field(None, ge=0, le=1, alias='d', description='Denoising strength')\n  cfg_scale: float = pydantic.Field(None, ge=1, le=25, alias='c', description='Cfg scale')\n  steps: int = pydantic.Field(None, ge=5, le=config.sd_max_steps, alias='st')\n  sampler_name: Literal[tuple(config.sd_available_samplers)] = pydantic.Field(None, alias=\"sa\", description='Sampler')\n  width: Literal[tuple(sd_available_resolutions)] = pydantic.Field(None, alias=\"wi\", description='Width')\n  height: Literal[tuple(sd_available_resolutions)] = pydantic.Field(None, alias=\"he\", description='Height')\n  negative_prompt: str = pydantic.Field(None, alias='np', description='Negative prompt')\n  seed: int = pydantic.Field(None, ge=-1, alias='se', description='Seed')\n  prompt: str = pydantic.Field(None)\n  mask: Literal[1, -1] = pydantic.Field(None, alias='ma', description='Inpaint mask')\n  inpainting_fill: int = pydantic.Field(None, ge=0, le=3, alias='fi', description='Inpaint fill mode')", "\n\nclass StableDiffusionModule:\n  def __init__(self, dp, bot):\n    self.queue = UserLimitedQueue(config.sd_queue_size_per_user)\n    self.semaphore = asyncio.Semaphore(1)\n    if config.mm_preload_models_on_start:\n      asyncio.run(refresh_model_list())\n\n    @dp.message(Command(commands=[\"tti\", \"iti\", \"ttiraw\", \"itiraw\"]), flags={\"long_operation\": \"upload_photo\"})\n    async def command_sd_handler(message: Message, command: CommandObject, album=False) -> None:\n      with self.queue.for_user(message.from_user.id) as available:\n        if available:\n          parse_error, params = self.parse_input(command.args)\n          if parse_error:\n            return await message.answer(f\"{html.quote(parse_error)}\")\n          prompt = params['prompt']\n          if not command.command.endswith('raw'):\n            params = self.apply_standard_prompt_modifiers(params)\n          processor = tti\n\n          photo = parse_photo(message)\n          if command.command.startswith(\"iti\") and photo:\n            image_data = await tg_image_to_data(photo, bot)\n            params['init_images'] = [image_data]\n            if 'mask' in params and album and len(album) > 1:\n              if params['mask'] == -1:\n                params['inpainting_mask_invert'] = 1\n              params['mask'] = await tg_image_to_data(parse_photo(album[1]), bot)\n            processor = iti\n          elif command.command.startswith(\"iti\"):\n            return await message.answer(\"Error, <b>unable to find initial photo</b>\")\n\n          wrapped_runner = semaphore_wrapper(self.semaphore, processor)\n          sd_error, data, details = await wrapped_runner(params)\n          reply_to = message.reply_to_message.message_id if message.reply_to_message is not None else None\n          \n          if sd_error:\n            return await message.answer(f\"<b>Error:</b> {sd_error}\")\n          else:\n            images = [InputMediaPhoto(\n              type='photo', \n              media=BufferedInputFile(i, filename='image.png'), \n              caption= None if idx != 0 else (f'<pre>{html.quote(prompt[0:768])}</pre>\\n' + \n              f'Seed: {details.get(\"seed\")}\\n' +\n              f'Sampler: {details.get(\"sampler_name\")}\\n' +\n              f'Cfg scale: {details.get(\"cfg_scale\")}\\n' + \n              f'Steps: {details.get(\"steps\")}\\n' +\n              f'Model: {details.get(\"model\")}')\n            ) for idx, i in enumerate(data)]\n          await message.answer_media_group(media=images, reply_to_message_id=reply_to)\n\n    @dp.message(F.media_group_id)\n    async def handle_media_groups(*args, **kwargs):\n      return\n\n    @dp.message(Command(commands=[\"models\", \"loras\", \"embeddings\"]), flags={\"cooldown\": 5})\n    async def list_sd_models(message: Message, command: CommandObject):\n      if len(models.values()) == 0:\n        await refresh_model_list()\n      if command.command == \"models\":\n        return message.answer('<b>Available models:</b> \\n' + \"\\n\".join(models.values()))\n      if command.command == \"embeddings\":\n        return message.answer('<b>Available embeddings:</b> \\n' + \"\\n\".join(embeddings))\n      if command.command == \"loras\":\n        loras_list = sorted([*loras, *config.sd_lora_custom_activations.keys()])\n        return message.answer('<b>Available loras:</b> \\n' + \"\\n\".join(loras_list))\n\n    @dp.message(\n      Command(commands=[\"model\", \"changemodel\", \"switchmodel\"]), \n      flags={\n        \"cooldown\": 30, \n        \"admins_only\": config.sd_only_admins_can_change_models, \n        \"long_operation\":\"choose_sticker\"\n      }\n    )\n    async def switch_sd_model(message: Message, command: CommandObject):\n      async with self.semaphore:\n        if command.args in models.values():\n          success = await switch_model(command.args)\n          if success:\n            return message.answer(f'<b>Model changed to:</b> {command.args}')\n          else:\n            return message.answer('<b>Unable to change the model.</b>')\n        else:\n          if not command.args:\n            return message.answer('use this command with model name to change the model, try /models for all model names')\n          return message.answer('<b>Model not found</b>')\n\n\n\n  def parse_input(self, user_input):\n      user_input = str(user_input) + ' '\n      parser = CustomArgumentParser(description='generate images from text and other images')\n      parser.add_argument('-d', type=float, help='Denoising strength (for image-to-image) (0 <= d <= 1)')\n      parser.add_argument('-c', type=float, help='Cfg scale (1 <= c <= 25)')\n      parser.add_argument('-sa', choices=['Euler a', 'Euler', 'Heun', 'DPM++ 2M', 'DPM++ 2S a'], help='Sampler')\n      parser.add_argument('-st', type=int, help=f'Number of steps (5 <= st <= {config.sd_max_steps})')\n      parser.add_argument('-se', type=int, help='Seed')\n      parser.add_argument('-wi', type=int, help='Image width')\n      parser.add_argument('-he', type=int, help='Image height')\n      parser.add_argument('-ma', type=int, help='Use the 2nd image as inpaint regular/inverse mask (1 or -1)')\n      parser.add_argument('-fi', type=int, help='Inpaint mask fill mode [0 <= fi <= 3], fill/original/l.noise/l.nothing')\n      parser.add_argument('-np', type=str, help='Negative prompt')\n      parser.add_argument('prompt', type=str, help='prompt', nargs=\"*\", action=JoinNargsAction)\n      try:\n        # override default -h behavior\n        if '-help' in user_input or '\u2014help' in user_input or '-h ' in user_input:\n          return (parser.format_help().replace(\n            'bot.py','/tti /iti /ttiraw /itiraw, /models /loras /embeddings /command@botname params text' \n          ), None)\n        # parse arguments using custom arg parser\n        args = parser.parse_args(shlex.split(user_input))\n        # apply pydantic checks\n        sd_args = {k: v for k, v in SDArguments(**vars(args)) if v is not None}\n        return (False, sd_args)\n      except (Exception, SystemExit) as e:\n        return (str(e), None)\n\n  def apply_standard_prompt_modifiers(self, params):\n    params['prompt'] = config.sd_extra_prompt.format(prompt=self.parse_lora(params['prompt']))\n    params['negative_prompt'] = config.sd_extra_negative_prompt.format(\n      negative_prompt='' if 'negative_prompt' not in params else params['negative_prompt']\n    ) \n    if 'width' in params and 'height' in params:\n      if params['width'] == params['height']:\n        params['prompt'] += ', square image'\n    return params\n\n  def parse_lora(self, prompt):\n    if 'lora' in prompt:\n      return prompt\n    seamless_loras = config.sd_lora_custom_activations or {}\n    for key in seamless_loras:\n      if type(seamless_loras[key]) is list:\n        prompt = prompt.replace(key, random.choice(seamless_loras[key]))\\\n                       .replace('LORA_RANGES', str(random.choice([0.9, 0.95, 1.0, 1.1, 1.2])))\n      else:\n        prompt = prompt.replace(key, seamless_loras[key])\n    subst = \"<lora:\\\\1:\\\\2.\\\\3\\\\4>\"\n    for lora in loras:\n      regex = fr\"({lora})([0-9])?([0-9])([0-9])\"\n      prompt = re.sub(regex, subst, prompt, 1, re.IGNORECASE)\n    return prompt\n\n  async def tti(self, params):\n    wrapped_runner = semaphore_wrapper(self.semaphore, tti)\n    return await wrapped_runner(params)\n \n  async def iti(self, params):\n    wrapped_runner = semaphore_wrapper(self.semaphore, iti)\n    return await wrapped_runner(params)"]}
{"filename": "modules/stt.py", "chunked_list": ["from aiogram.filters import Command, CommandObject\nfrom aiogram.types import Message, BufferedInputFile\nfrom aiogram import html, F\nfrom providers.stt_provider import active_model\nfrom custom_queue import UserLimitedQueue, semaphore_wrapper\nfrom config_reader import config\nfrom utils import download_audio\nfrom types import SimpleNamespace\nimport random\nimport tempfile", "import random\nimport tempfile\nimport asyncio\n\nclass SpeechToTextModule:\n  def __init__(self, dp, bot):\n    self.queue = UserLimitedQueue(config.stt_queue_size_per_user)\n    self.semaphore = asyncio.Semaphore(1)\n    self.bot = bot\n    self.model = active_model.init()\n    \n    if config.stt_autoreply_mode:\n      @dp.message((F.voice), flags={\"long_operation\": \"record_audio\"})\n      async def handle_voice_messages(message: Message):\n        error, text = await self.recognize_voice_message(message)\n        if text and 'llm' in config.active_modules:\n          llm = dp['modules']['llm']\n          llm_call_func = llm.assist if config.stt_autoreply_mode == 'assist' else llm.chat\n          reply = await llm_call_func(text, llm.get_common_chat_attributes(message))\n          if 'tts' in config.active_modules:\n            voice = config.stt_autoreply_voice or config.tts_voices[0]\n            voice = random.choice(config.tts_voices) if voice == 'random' else voice\n            await bot.reply_tts(message=message, command=SimpleNamespace(command=voice, args=[reply]))\n          else:\n            return await message.answer(reply)\n        if error:\n          return await message.answer(f\"Error, <b>{error}</b>\")\n\n    @dp.message(Command(commands=[\"stt\", \"recognize\", \"transcribe\"]), flags={\"long_operation\": \"typing\"})\n    async def command_stt_handler(message: Message, command: CommandObject) -> None:\n      with self.queue.for_user(message.from_user.id) as available:\n        if not available:\n          return\n        if command.command == \"stt\" and ('-h' in str(command.args) or not (message.reply_to_message and message.reply_to_message.voice)):\n          return await message.answer(f\"Usage: /stt [voice_message] \\nUse the commands like /command@botname\")\n        else:\n          error, text = await self.recognize_voice_message(message)\n          if error:\n            return await message.answer(f\"Error, <b>{error}</b>\")\n          else:\n            return await message.answer(f\"<i>{text}</i>\")\n\n  async def recognize_voice_message(self, message):\n    with tempfile.NamedTemporaryFile(suffix='.ogg', delete=False) as temp_file:\n      voice = message.reply_to_message.voice if not message.voice else message.voice\n      await download_audio(self.bot, voice.file_id, temp_file.name)\n      wrapped_runner = semaphore_wrapper(self.semaphore, self.recognize)\n      error, data = await wrapped_runner(temp_file.name)\n      return error, data\n\n  async def recognize(self, audio_path):\n    return await self.model.recognize(audio_path)", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n\n\n", "\n\n\n"]}
{"filename": "modules/tta.py", "chunked_list": ["from aiogram.filters import Command, CommandObject\nfrom aiogram.types import Message, BufferedInputFile\nfrom providers.tts_provider import convert_to_ogg\nfrom providers.tta_provider import generate_audio_async, tta_init\nfrom custom_queue import UserLimitedQueue, semaphore_wrapper\nfrom config_reader import config\nimport asyncio\n\nclass TextToAudioModule:\n  def __init__(self, dp, bot):\n    self.queue = UserLimitedQueue(config.tta_queue_size_per_user)\n    self.semaphore = asyncio.Semaphore(1)\n    \n    if 'tta' in config.active_modules:\n      self.available = tta_init()\n    if not self.available:\n      return\n\n    @dp.message(Command(commands=[\"tta\", \"sfx\", \"music\"]), flags={\"long_operation\": \"record_audio\"})\n    async def command_tta_handler(message: Message, command: CommandObject) -> None:\n      with self.queue.for_user(message.from_user.id) as available:\n        if not available:\n          return\n        if command.command == \"tta\" or not command.args or str(command.args).strip() == \"\" or ('-help' in str(command.args)):\n          return await message.answer(f\"Usage: /sfx text /music text\\nUse the commands like /command@botname\")\n        else:\n          audio_type = command.command\n          text = str(command.args)\n          wrapped_runner = semaphore_wrapper(self.semaphore, generate_audio_async)\n          error, data = await wrapped_runner(text, audio_type, config.tta_duration)\n          print(error, data)\n          if error:\n            return await message.answer(f\"Error, <b>{error}</b>\")\n          else:\n            audio = BufferedInputFile(convert_to_ogg(data), 'audio.ogg')\n            return await message.answer_voice(voice=audio)", "class TextToAudioModule:\n  def __init__(self, dp, bot):\n    self.queue = UserLimitedQueue(config.tta_queue_size_per_user)\n    self.semaphore = asyncio.Semaphore(1)\n    \n    if 'tta' in config.active_modules:\n      self.available = tta_init()\n    if not self.available:\n      return\n\n    @dp.message(Command(commands=[\"tta\", \"sfx\", \"music\"]), flags={\"long_operation\": \"record_audio\"})\n    async def command_tta_handler(message: Message, command: CommandObject) -> None:\n      with self.queue.for_user(message.from_user.id) as available:\n        if not available:\n          return\n        if command.command == \"tta\" or not command.args or str(command.args).strip() == \"\" or ('-help' in str(command.args)):\n          return await message.answer(f\"Usage: /sfx text /music text\\nUse the commands like /command@botname\")\n        else:\n          audio_type = command.command\n          text = str(command.args)\n          wrapped_runner = semaphore_wrapper(self.semaphore, generate_audio_async)\n          error, data = await wrapped_runner(text, audio_type, config.tta_duration)\n          print(error, data)\n          if error:\n            return await message.answer(f\"Error, <b>{error}</b>\")\n          else:\n            audio = BufferedInputFile(convert_to_ogg(data), 'audio.ogg')\n            return await message.answer_voice(voice=audio)", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n\n\n", "\n\n\n"]}
{"filename": "servers/tts_server.py", "chunked_list": ["import sys \nimport os\nsys.path.append('.')\nsys.path.append('..')\nfrom fastapi import FastAPI\nfrom providers.tts_provider import tts, save_audio\nfrom pydantic import BaseModel, Field\nfrom fastapi.responses import StreamingResponse\nfrom io import BytesIO\nfrom typing_extensions import Literal", "from io import BytesIO\nfrom typing_extensions import Literal\n\napp = FastAPI()\n\nclass Data(BaseModel):\n  voice: str = Field(None, description='String with speaker model name')\n  text: str = Field(None, description='String with text that you want to hear')\n  response: Literal['file', 'path'] = Field(None, description='String with value \"file\" or \"path\", changes the output format to either the path to the recorded audio or the file itself.')\n", "\n\n@app.post(\"/\")\nasync def read_root(rqdata: Data):\n  error, data = await tts(rqdata.voice, rqdata.text)\n  if not error:\n    if rqdata.response == 'file':\n      bytes = BytesIO(open(data, mode='rb').read())\n      os.remove(data)\n      response = StreamingResponse(bytes, media_type='audio/wav')\n      response.headers[\"Content-Disposition\"] = f\"inline; filename=record.wav\"\n      return response\n    return {\"data\": data}\n  else:\n    return {\"error\": error}", "\nif __name__ == \"__main__\":\n  import uvicorn\n  uvicorn.run(app, host=\"0.0.0.0\", port=7077)\n  "]}
{"filename": "chroniclers/base.py", "chunked_list": ["import importlib\nimport re\nfrom abc import ABCMeta, abstractmethod\nfrom collections import defaultdict\n\nclass AbstractChronicler(metaclass=ABCMeta):\n  def __init__(self, filename):\n    chronicler_script = importlib.import_module(filename)\n    self.chronicler_script = chronicler_script\n    self.vars = chronicler_script.get_chat_variables\n    self.gen_cfg = chronicler_script.get_generation_config\n    self.init_cfg = chronicler_script.get_init_config\n\n  @abstractmethod\n  def prepare(self, details):\n    pass\n\n  @abstractmethod\n  def parse(self):\n    pass\n  \n  @staticmethod\n  def prepare_hook(func):\n    def wrapper(self, *args, **kwargs):\n      formatter = getattr(self.chronicler_script, 'custom_input_formatter', func)\n      return formatter(self, *args, **kwargs)\n    return wrapper\n\n  @staticmethod\n  def parse_hook(func):\n    def wrapper(self, *args, **kwargs):\n      print(args[0])\n      parser = getattr(self.chronicler_script, 'custom_output_parser', func)\n      return parser(self, *args, **kwargs)\n    return wrapper", "\nclass AssistantReplyChronicler(AbstractChronicler):\n  def __init__(self, chronicler_filename):\n    super().__init__(chronicler_filename)\n\n  def prepare(self, details, fresh=False):\n    text = details['message']\n    reply_text = details['reply_text']\n    if text and reply_text:\n      memory = self.parse_qa(reply_text) + '\\n' + text\n      details['message'] = memory\n    return chroniclers['instruct'].prepare(self, details)\n  \n  def parse_qa(self, text):\n    if text.startswith('Q:'):\n      splits = text.split('\\n\\n')\n      return f'>{splits[0][2:]}\\n>{splits[1][2:]}'\n    else:\n      return '>' + text.replace('\\n', ' ')\n\n  def parse(self, output, chat_id, skip=0):\n    return chroniclers['instruct'].parse(self, output, chat_id, skip)", "\n\nclass ConversationChronicler(AbstractChronicler):\n  def __init__(self, chronicler_filename, continous=False, max_length=10):\n    super().__init__(chronicler_filename)\n    self.history = defaultdict(lambda: [])\n    self.max_length = max_length\n    self.multiline_re = re.compile(\"[^\\n:]+\\:[^\\n]+\\n\")\n\n  def get_author(self, vars, item):\n    r_username = vars.get('replace_username', False)\n    return r_username if r_username and item['author'] != vars['name'] else item['author']\n\n  def prepare(self, details, fresh=False):\n    if fresh:\n      self.history[details['chat_id']] = []\n    history = self.history[details['chat_id']]\n    history.append({\"message\": details['message'], \"author\": details['author']})\n    while len(history) >= self.max_length:\n      history.pop(0)\n    conversation = ''\n    char_vars = self.vars(details)\n    for item in history:\n      msg = item[\"message\"]\n      author = self.get_author(char_vars, item)\n      conversation += f'{author}: {msg[0].upper() + msg[1:]}\\n'\n    if char_vars['pre_dialog']:\n      char_vars['pre_dialog'] += '\\n'\n    dialog = '''{intro}\n{personality}\n\n{pre_dialog}{conversation}{name}:'''\\\n    .format(conversation=conversation, **char_vars)\n    return dialog\n\n  def parse(self, output, chat_id, skip=0):\n    output = output.strip()[skip:]\n    print(output)\n    re_end = re.search(self.multiline_re, output) or re.search('\\n', output) \n    end = (output.find('</s>') + 1) or (re_end.span()[0] if re_end else len(output) + 1)\n    parsed = output[:end - 1].strip()\n    if parsed == '':\n      return '...'\n    author = self.vars()['name']\n    self.history[chat_id].append({\"message\": parsed.replace(':', ''), \"author\": author})\n    return parsed", "\n\nclass AlpacaAssistantChronicler(AbstractChronicler):\n  def __init__(self, chronicler_filename):\n    super().__init__(chronicler_filename)\n\n  @AbstractChronicler.prepare_hook\n  def prepare(self, details, fresh=False):\n    msg = details['message'].split('\\n', 1)\n    l = self.vars(details)\n    if len(msg) > 1 and l['assistant_input']:\n      return f\"\"\"{l['assistant_intro1']} \n### {l['assistant_instruction']}:\n{msg[0]}\n### {l['assistant_input']}:\n{msg[1]}\n### {l['assistant_response']}:\n\"\"\"\n    else:\n      return f\"\"\"{l['assistant_intro2']} \n### {l['assistant_instruction']}:\n{msg[0]}\n### {l['assistant_response']}:\n\"\"\"\n  @AbstractChronicler.parse_hook\n  def parse(self, output, chat_id, skip=0):\n    output = output[skip:]\n    end = output.find('</s>')\n    if end == -1:\n      end = output.find('###')\n    parsed = output[0: end if end != -1 else None].strip()\n    if parsed == '':\n      return '...'\n    return parsed", "\nclass RawChronicler(AbstractChronicler):\n  def __init__(self, chronicler_filename):\n    super().__init__(chronicler_filename)\n  \n  def prepare(self, details, fresh=False):\n    return details['message']\n\n  def parse(self, output, chat_id, skip=0):\n    print(output)\n    return output", "\n\nchroniclers = {\n  \"alpaca\": AlpacaAssistantChronicler,\n  \"instruct\": AlpacaAssistantChronicler,\n  \"chat\": ConversationChronicler,\n  \"reply\": AssistantReplyChronicler,\n  \"raw\": RawChronicler\n}", "}"]}
{"filename": "characters/pygmalion_chat_king_william.py", "chunked_list": ["\ndef get_assistant_variables():\n  return {\n    \"assistant_intro1\": \"Provide a correct repsonse.\",\n    \"assistant_intro2\": \"\",\n    \"assistant_instruction\": \"Question\",\n    \"assistant_input\": False,\n    \"assistant_response\": \"Answer\"\n  }\n\ndef get_chat_variables(context=None):\n  # change these as you wish\n  # sample personality: King William\n  name = 'William'\n  intro = f\"{name}'s Persona: {name} is the king of an ancient principality located on one of the islands of the northern sea. He is proud and honest, and despises pitiful peasants and other kings.\"\n  predialog = ''\n  return {\"intro\": intro, \"personality\": '<START>', 'name': name, 'pre_dialog': predialog, **get_assistant_variables() }", "\ndef get_chat_variables(context=None):\n  # change these as you wish\n  # sample personality: King William\n  name = 'William'\n  intro = f\"{name}'s Persona: {name} is the king of an ancient principality located on one of the islands of the northern sea. He is proud and honest, and despises pitiful peasants and other kings.\"\n  predialog = ''\n  return {\"intro\": intro, \"personality\": '<START>', 'name': name, 'pre_dialog': predialog, **get_assistant_variables() }\n\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.75,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.12,\n    **override\n  }", "\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.75,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.12,\n    **override\n  }\n\ndef get_init_config():\n  return {}", "\ndef get_init_config():\n  return {}"]}
{"filename": "characters/gpt4all_default.py", "chunked_list": ["def get_assistant_variables():\n  return {}\n\ndef get_chat_variables(context=None):\n  return {\"intro\": '', \"personality\": '', 'name': 'ASSISTANT', 'pre_dialog': '', **get_assistant_variables() }\n\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.7,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.1,\n    **override\n  }", "\ndef custom_input_formatter(chronicler, details, fresh=True):\n    msg = details['message'].replace('\\n', ' ')\n    return f\"\"\"{msg}\n\"\"\"\n\ndef custom_output_parser(chronicler, output, chat_id, skip=0):\n    output = output[skip:].strip()\n    return output\n\ndef get_init_config():\n  return {'context_size': 2048}", "\ndef get_init_config():\n  return {'context_size': 2048}"]}
{"filename": "characters/gptj_6B_default.py", "chunked_list": ["from datetime import datetime\ndef get_chat_variables(context=None):\n  intro = 'The year is {}.'.format(datetime.now().year)\n  personality = 'I am a very advanced AI from another planet. I met a person, their name is {}.\\n'.format(\n    context['author'] if context else ''\n  )\n  name = 'AI'\n  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': ''}\n\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.8,\n    \"top_k\": 40,\n    \"top_p\": 1,\n    \"repetition_penalty\": 1.01,\n    **override\n  }", "\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.8,\n    \"top_k\": 40,\n    \"top_p\": 1,\n    \"repetition_penalty\": 1.01,\n    **override\n  }\n\ndef get_init_config():\n  return {}", "\ndef get_init_config():\n  return {}"]}
{"filename": "characters/ru_saiga_lcpp.py", "chunked_list": ["#Reference: https://github.com/IlyaGusev/rulm/blob/master/self_instruct/src/interact_llamacpp.py\n#License: Apache License 2.0\n\nfrom llama_cpp import Llama\n\nSYSTEM_PROMPT = \"\u0422\u044b \u2014 \u0421\u0430\u0439\u0433\u0430, \u0440\u0443\u0441\u0441\u043a\u043e\u044f\u0437\u044b\u0447\u043d\u044b\u0439 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0430\u0441\u0441\u0438\u0441\u0442\u0435\u043d\u0442. \u0422\u044b \u0440\u0430\u0437\u0433\u043e\u0432\u0430\u0440\u0438\u0432\u0430\u0435\u0448\u044c \u0441 \u043b\u044e\u0434\u044c\u043c\u0438 \u0438 \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0448\u044c \u0438\u043c.\"\nSYSTEM_TOKEN = 1788\nUSER_TOKEN = 1404\nBOT_TOKEN = 9225\nLINEBREAK_TOKEN = 13", "BOT_TOKEN = 9225\nLINEBREAK_TOKEN = 13\n\nROLE_TOKENS = {\n    \"user\": USER_TOKEN,\n    \"bot\": BOT_TOKEN,\n    \"system\": SYSTEM_TOKEN\n}\n\ndef get_message_tokens(model, role, content):\n    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n    message_tokens.insert(1, ROLE_TOKENS[role])\n    message_tokens.insert(2, LINEBREAK_TOKEN)\n    message_tokens.append(model.token_eos())\n    return message_tokens", "\ndef get_message_tokens(model, role, content):\n    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n    message_tokens.insert(1, ROLE_TOKENS[role])\n    message_tokens.insert(2, LINEBREAK_TOKEN)\n    message_tokens.append(model.token_eos())\n    return message_tokens\n\ndef get_system_tokens(model):\n    system_message = {\n        \"role\": \"system\",\n        \"content\": SYSTEM_PROMPT\n    }\n    return get_message_tokens(model, **system_message)", "def get_system_tokens(model):\n    system_message = {\n        \"role\": \"system\",\n        \"content\": SYSTEM_PROMPT\n    }\n    return get_message_tokens(model, **system_message)\n\ndef get_assistant_variables():\n  # change these only if your custom lora input format changed\n  return {'replace_username': 'User'}", "\ndef get_chat_variables(context=None):\n  # change these as you wish\n  name = 'Saiga'\n  intro = SYSTEM_PROMPT\n  return {\"intro\": intro, \"personality\": '', 'name': name, 'pre_dialog': '', **get_assistant_variables() }\n\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.2,\n    \"top_k\": 30,\n    \"top_p\": 0.9,\n    \"repetition_penalty\": 1.1,\n    **override\n  }", "\ndef custom_input_formatter(chronicler, details, fresh=True):\n  model = details['model']['instance']\n  tokens = get_system_tokens(model)\n  message_tokens = get_message_tokens(model=model, role=\"user\", content=details['message'])\n  role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]\n  tokens += message_tokens + role_tokens\n  # detokinization is used for compatibility with moldel(), since model.generate is not supported\n  return model.detokenize(tokens).decode(\"utf-8\")\n\ndef custom_output_parser(chronicler, output, chat_id, skip=0):\n    output = output[skip:].strip()\n    end = (output.find('User:') + 1 ) or (output.find('Saiga:') + 1) or (len(output) + 1)\n    return output[:end - 1].strip()", "\ndef custom_output_parser(chronicler, output, chat_id, skip=0):\n    output = output[skip:].strip()\n    end = (output.find('User:') + 1 ) or (output.find('Saiga:') + 1) or (len(output) + 1)\n    return output[:end - 1].strip()\n\ndef get_init_config():\n  return {'context_size': 2000}"]}
{"filename": "characters/min_chatGPT2_default.py", "chunked_list": ["from datetime import datetime\ndef get_chat_variables(context=None):\n  intro = 'The year is {}.'.format(datetime.now().year)\n  personality = 'I am a very advanced AI from another planet. I met a person, their name is {}.\\n'.format(\n    context['author'] if context else ''\n  )\n  name = 'AI'\n  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': ''}\n\n\ndef custom_input_formatter(chronicler, details, fresh=True):\n    msg = details['message']\n    return f\"\"\"Human: {msg}\n\nAssistant: \n\"\"\"", "\n\ndef custom_input_formatter(chronicler, details, fresh=True):\n    msg = details['message']\n    return f\"\"\"Human: {msg}\n\nAssistant: \n\"\"\"\n\ndef custom_output_parser(chronicler, output, chat_id, skip=0):\n  def parse(self, output, chat_id, skip=0):\n    output = output[skip:].strip()\n    end = (output.find('Human:') + 1 ) or (output.find('Assistant:') + 1) or (len(output) + 1)\n    parsed = output[:end - 1].strip()\n    if parsed == '':\n      return '...'\n    return parsed", "\ndef custom_output_parser(chronicler, output, chat_id, skip=0):\n  def parse(self, output, chat_id, skip=0):\n    output = output[skip:].strip()\n    end = (output.find('Human:') + 1 ) or (output.find('Assistant:') + 1) or (len(output) + 1)\n    parsed = output[:end - 1].strip()\n    if parsed == '':\n      return '...'\n    return parsed\n\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.9,\n    \"top_k\": 200,\n    **override\n  }", "\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.9,\n    \"top_k\": 200,\n    **override\n  }\n\ndef get_init_config():\n  return {\n    \"use_tiktoken\": True,\n    \"nanogpt\": True\n  }", "def get_init_config():\n  return {\n    \"use_tiktoken\": True,\n    \"nanogpt\": True\n  }"]}
{"filename": "characters/llama_chat_default.py", "chunked_list": ["from datetime import datetime\n\ndef get_assistant_variables():\n  # change these only if your custom lora input format changed\n  return {\n    \"assistant_intro1\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\",\n    \"assistant_intro2\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n    \"assistant_instruction\": \"Instruction\",\n    \"assistant_input\": \"Input\",\n    \"assistant_response\": \"Response\"\n  }", "\ndef get_chat_variables(context=None):\n  # change these as you wish\n  name = 'AI'\n  intro = 'The year is {}.'.format(datetime.now().year)\n  personality = f'My name is {name}. I am a very advanced AI. I chat with humans. I must be verbose and honest, expressing myself.\\n'\n  predialog = f'''Andy: What is your name? Why are you here?\n{name}: Humans call me {name}. Nice to meet you. I am here to chat with you.'''\n  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': predialog, **get_assistant_variables() }\n\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.7,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.2,\n    **override\n  }", "\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.7,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.2,\n    **override\n  }\n\ndef get_init_config():\n  return {}", "\ndef get_init_config():\n  return {}"]}
{"filename": "characters/llama_rulora_assistant_only.py", "chunked_list": ["# for https://huggingface.co/IlyaGusev/llama_7b_ru_turbo_alpaca_lora\n\nfrom datetime import datetime\n\ndef custom_input_formatter(chronicler, details, fresh=True):\n  lines = details[\"message\"].split('\\n')\n  query = lines[0]\n  is_question = query.endswith('?')\n  if len(lines) > 1 or not is_question:\n    return f'''\u0417\u0430\u0434\u0430\u043d\u0438\u0435: {query}.\n\u0412\u0445\u043e\u0434: {' '.join(lines[1:]) if len(lines) > 1 else ''}\n\u0412\u044b\u0445\u043e\u0434:'''\n  else:\n    return f'''\u0412\u043e\u043f\u0440\u043e\u0441: {query} \n\u0412\u044b\u0445\u043e\u0434:'''", "\ndef get_assistant_variables():\n  return {}\n\ndef get_chat_variables(context=None):\n  return {\"intro\": '', \"personality\": '', 'name': '', 'pre_dialog': '', **get_assistant_variables() }\n\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.7,\n    \"top_k\": 100,\n    \"top_p\": 0.75,\n    \"repetition_penalty\": 1.16,\n    **override\n  }", "\ndef get_init_config():\n  return {}"]}
{"filename": "characters/ru_gpt3_default.py", "chunked_list": ["from datetime import datetime\ndef get_chat_variables(context=None):\n  intro = '\u0421\u0435\u0439\u0447\u0430\u0441 {} \u0433\u043e\u0434.'.format(datetime.now().year)\n  personality = '\u042f - \u043d\u043e\u0432\u0430\u044f \u0444\u043e\u0440\u043c\u0430 \u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0430. \u0412\u0441\u0442\u0440\u0435\u0442\u0438\u043b\u0430 \u044f \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430, \u043f\u043e \u0438\u043c\u0435\u043d\u0438 {}. \u0420\u0435\u0448\u0438\u043b\u0438 \u043f\u043e\u0431\u043e\u043b\u0442\u0430\u0442\u044c.'.format(\n    context['author'] if context else ''\n  )\n  name = '\u042f'\n  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': ''}\n\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.85,\n    \"top_k\": 50,\n    \"top_p\": 0.92,\n    \"repetition_penalty\": 1.01,\n    **override\n  }", "\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.85,\n    \"top_k\": 50,\n    \"top_p\": 0.92,\n    \"repetition_penalty\": 1.01,\n    **override\n  }\n\ndef get_init_config():\n  return {}", "\ndef get_init_config():\n  return {}"]}
{"filename": "characters/orca_default.py", "chunked_list": ["ORCA_VERSION = 2\n\ndef get_assistant_variables():\n  return {}\n\ndef get_chat_variables(context=None):\n  intro = 'You are an AI assistant that follows instruction extremely well. Help as much as you can.'\n  return {\"intro\": intro, \"personality\": '', 'name': 'ASSISTANT', 'pre_dialog': ''}\n\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.7,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.1,\n    **override\n  }", "\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.7,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.1,\n    **override\n  }\n\ndef custom_input_formatter(chronicler, details, fresh=True):\n  msg = details['message']\n  n = '\\n'\n  if not msg.startswith('>') and '\\n' in msg:\n    msg = msg.split('\\n', 1)\n  else:\n    msg = [msg]\n  template = f'''### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n### User:\n{msg[0]}\n\n### Response:\n''' if ORCA_VERSION == 1 else f'''### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n### User:\n{msg[0]}\n\n### Input:{(n + msg[1]) if len(msg) > 1 else \"\"}\n\n### Response:\n'''\n  return template", "\ndef custom_input_formatter(chronicler, details, fresh=True):\n  msg = details['message']\n  n = '\\n'\n  if not msg.startswith('>') and '\\n' in msg:\n    msg = msg.split('\\n', 1)\n  else:\n    msg = [msg]\n  template = f'''### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n### User:\n{msg[0]}\n\n### Response:\n''' if ORCA_VERSION == 1 else f'''### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n\n### User:\n{msg[0]}\n\n### Input:{(n + msg[1]) if len(msg) > 1 else \"\"}\n\n### Response:\n'''\n  return template", "\ndef custom_output_parser(chronicler, output, chat_id, skip=0):\n    output = output[skip:].strip()\n    end = (output.find('###') + 1) or (len(output) + 1)\n    return output[:end - 1].strip()\n\ndef get_init_config():\n  return {'context_size': 2048}"]}
{"filename": "characters/vicuna_default.py", "chunked_list": ["from datetime import datetime\n\nVERSION = 1.1\n\ndef get_assistant_variables():\n  # change these only if your custom lora input format changed\n  if VERSION == 0:\n    intro = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\"\n    assistant_instruction = \"Human\"\n    assistant_response = \"Assistant\"\n  if VERSION == 1.1:\n    intro = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n    assistant_instruction = \"USER\"\n    assistant_response = \"ASSISTANT\"\n  return {\n    \"assistant_intro1\": intro,\n    \"assistant_intro2\": intro,\n    \"assistant_instruction\": assistant_instruction,\n    \"assistant_input\": False,\n    \"assistant_response\": assistant_response\n  }", "\ndef get_chat_variables(context=None):\n  # change these as you wish\n  assistant_variables = get_assistant_variables()\n  name = 'AI'\n  intro = 'The year is {}.'.format(datetime.now().year)\n  personality = f'My name is {name}. I am a very advanced AI. I chat with humans. I must be verbose and honest, expressing myself.\\n'\n  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': '', **get_assistant_variables() }\n\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.7,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.05,\n    **override\n  }", "\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.7,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.05,\n    **override\n  }\n\ndef get_init_config(): \n  return {'context_size': 2048}", "\ndef get_init_config(): \n  return {'context_size': 2048}"]}
{"filename": "characters/samantha_default.py", "chunked_list": ["from datetime import datetime\n\ndef get_assistant_variables():\n  # change these only if your custom lora input format changed\n  return {'replace_username': 'USER'}\n\ndef get_chat_variables(context=None):\n  # change these as you wish\n  name = 'ASSISTANT'\n  intro = 'You are Samantha, a sentient AI.'\n  return {\"intro\": intro, \"personality\": '', 'name': name, 'pre_dialog': '', **get_assistant_variables() }", "\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.7,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.1,\n    **override\n  }\n\ndef custom_input_formatter(chronicler, details, fresh=True):\n  cvars = get_chat_variables()\n  template = f'''{cvars['intro']}\n\n  USER: {details['message']}\n  {cvars['name']}:'''\n  return template", "\ndef custom_input_formatter(chronicler, details, fresh=True):\n  cvars = get_chat_variables()\n  template = f'''{cvars['intro']}\n\n  USER: {details['message']}\n  {cvars['name']}:'''\n  return template\n\ndef custom_output_parser(chronicler, output, chat_id, skip=0):\n    output = output[skip:].strip()\n    end = (output.find('USER:') + 1 ) or (output.find('ASSISTANT:') + 1) or (len(output) + 1)\n    return output[:end - 1].strip()", "\ndef custom_output_parser(chronicler, output, chat_id, skip=0):\n    output = output[skip:].strip()\n    end = (output.find('USER:') + 1 ) or (output.find('ASSISTANT:') + 1) or (len(output) + 1)\n    return output[:end - 1].strip()\n\ndef get_init_config():\n  return {}"]}
{"filename": "characters/llama_2_nous-hermes.py", "chunked_list": ["def get_init_config():\n  return {'context_size': 4096}\n\ndef custom_input_formatter(chronicler, details, fresh=True):\n  template = f'''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: {details['message']}\n\n### Response: '''\n  return template", "\ndef get_chat_variables(context=None):\n  # change these as you wish\n  name = 'Assistant'\n  intro = 'System:'\n  personality = f'You are a helpful, respectful and honest assistant.\\nUser:\\n'\n  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': '' }\n\ndef custom_output_parser(chronicler, output, chat_id, skip=0):\n    output = output[skip:].strip()\n    end = (output.find('### ') + 1) or (len(output) + 1)\n    return output[:end - 1].strip()", "def custom_output_parser(chronicler, output, chat_id, skip=0):\n    output = output[skip:].strip()\n    end = (output.find('### ') + 1) or (len(output) + 1)\n    return output[:end - 1].strip()\n\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.6,\n    \"top_k\": 50,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.12,\n    **override\n  }", "\n\n"]}
{"filename": "characters/vicuna_16k.py", "chunked_list": ["from datetime import datetime\n\nVERSION = 1.1\n\ndef get_assistant_variables():\n  # change these only if your custom lora input format changed\n  if VERSION == 1.1:\n    intro = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n    assistant_instruction = \"USER\"\n    assistant_response = \"ASSISTANT\"\n  return {\n    \"assistant_intro1\": intro,\n    \"assistant_intro2\": intro,\n    \"assistant_instruction\": assistant_instruction,\n    \"assistant_input\": False,\n    \"assistant_response\": assistant_response\n  }", "\ndef get_chat_variables(context=None):\n  # change these as you wish\n  assistant_variables = get_assistant_variables()\n  name = 'AI'\n  intro = 'The year is {}.'.format(datetime.now().year)\n  personality = f'My name is {name}. I am a very advanced AI. I chat with humans. I must be verbose and honest, expressing myself.\\n'\n  return {\"intro\": intro, \"personality\": personality, 'name': name, 'pre_dialog': '', **get_assistant_variables() }\n\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.82,\n    \"top_k\": 72,\n    \"top_p\": 0.21,\n    \"repetition_penalty\": 1.19,\n    **override\n  }", "\ndef get_generation_config(override={}):\n  return {\n    \"temperature\": 0.82,\n    \"top_k\": 72,\n    \"top_p\": 0.21,\n    \"repetition_penalty\": 1.19,\n    **override\n  }\n\ndef get_init_config(): \n  # rope_freq_scale = 1 / (max_seq_len / 2048)\n  return {'context_size': 16092, 'rope_freq_base': 10000, 'rope_freq_scale': 0.125}", "\ndef get_init_config(): \n  # rope_freq_scale = 1 / (max_seq_len / 2048)\n  return {'context_size': 16092, 'rope_freq_base': 10000, 'rope_freq_scale': 0.125}"]}
{"filename": "misc/mps_fixups.py", "chunked_list": ["# Workarounds and fixes for LLMs for mps accelerator\n# Copyright Jeremy Barnes / MIT License\n# reference code:\n# https://github.com/jeremybarnes/llm-webgpu/blob/main/mps_fixups.py\n#\nimport torch\nfrom torch import Tensor\nfrom typing import Optional\n\ndef fixup_mps():\n\n    orig_topk = torch.topk\n    # Topk only works up to k=15 on MPS, replace it with a CPU fallback\n    def _topk(self: torch.Tensor, k: int, dim:int=-1, largest:bool=True, sorted:bool=True):\n        res, indices = orig_topk(self.to('cpu', torch.float32), k, dim, largest, sorted)\n        return res.to(self), indices.to('mps')\n\n    torch.topk = _topk\n\n    orig_max = torch.max\n    # Max doesn't work with longs on MPS, replace it with a CPU fallback\n    def _max(self: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n        return orig_max(self.to('cpu'), *args, **kwargs).to('mps')\n\n    torch.max = _max\n\n    orig_cumsum = torch.cumsum\n    # Cumulative sum doesn't work, replace with CPU fallback\n    def _cumsum(input: torch.Tensor, dim: int, **kwargs) -> torch.Tensor:\n        return orig_cumsum(input.to('cpu', torch.float32), dim, **kwargs).to('mps', input.dtype)\n\n    torch.cumsum = _cumsum\n    torch.Tensor.cumsum = _cumsum", "\ndef fixup_mps():\n\n    orig_topk = torch.topk\n    # Topk only works up to k=15 on MPS, replace it with a CPU fallback\n    def _topk(self: torch.Tensor, k: int, dim:int=-1, largest:bool=True, sorted:bool=True):\n        res, indices = orig_topk(self.to('cpu', torch.float32), k, dim, largest, sorted)\n        return res.to(self), indices.to('mps')\n\n    torch.topk = _topk\n\n    orig_max = torch.max\n    # Max doesn't work with longs on MPS, replace it with a CPU fallback\n    def _max(self: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n        return orig_max(self.to('cpu'), *args, **kwargs).to('mps')\n\n    torch.max = _max\n\n    orig_cumsum = torch.cumsum\n    # Cumulative sum doesn't work, replace with CPU fallback\n    def _cumsum(input: torch.Tensor, dim: int, **kwargs) -> torch.Tensor:\n        return orig_cumsum(input.to('cpu', torch.float32), dim, **kwargs).to('mps', input.dtype)\n\n    torch.cumsum = _cumsum\n    torch.Tensor.cumsum = _cumsum", "    "]}
{"filename": "misc/memory_manager.py", "chunked_list": ["import torch\nimport sys\nimport psutil\nfrom config_reader import config\nfrom time import time \n\nSHARED_MEMORY = sys.platform == \"darwin\"\nGPU_AVAILABLE = torch.cuda.is_available()\n\n\ndef get_vram_info():\n  if torch.cuda.is_available():\n    device = torch.cuda.current_device()\n    vram_bytes = torch.cuda.get_device_properties(device).total_memory\n    try:\n      vram_bytes, total  = torch.cuda.mem_get_info()\n    except Exception:\n      pass\n    vram_gb = vram_bytes / 1e9\n    return vram_gb\n  else:\n    return 0.0", "\n\ndef get_vram_info():\n  if torch.cuda.is_available():\n    device = torch.cuda.current_device()\n    vram_bytes = torch.cuda.get_device_properties(device).total_memory\n    try:\n      vram_bytes, total  = torch.cuda.mem_get_info()\n    except Exception:\n      pass\n    vram_gb = vram_bytes / 1e9\n    return vram_gb\n  else:\n    return 0.0", "\ndef get_system_ram_info():\n    virtual_memory = psutil.virtual_memory()\n    memory_gb = virtual_memory.available / (1024**3)\n    return memory_gb\n\nram_available = get_system_ram_info()\n\nclass MModel:\n  def __init__(self, name, load, unload):\n    self.name = name\n    self.model = load()\n    self.load = load\n    self.unload = unload\n  def __setattr__(self, name, value):\n    self.__dict__[name] = value", "class MModel:\n  def __init__(self, name, load, unload):\n    self.name = name\n    self.model = load()\n    self.load = load\n    self.unload = unload\n  def __setattr__(self, name, value):\n    self.__dict__[name] = value\n\nclass MemoryManager:\n  def __init__(self, get_memory, cached_model_count):\n    self.get_memory = get_memory\n    self.starting_memory = get_memory()\n    self.cache = {}\n    self.cached_model_count = cached_model_count\n    self.mm_management_policy = config.mm_management_policy\n  \n  def wrap(self, model_name, load_function=None, unload_function=None, memory='auto'):\n    mem = self.get_memory()\n    # get keys and values of items with model != None as lists\n    [*alive_keys], [*alive_values] = zip(*((i.name, i) for i in self.cache.values() if i.model is not None)) \\\n      if len(self.cache.keys()) > 0 else ([],[])\n    if config.mm_autounload_after_seconds > 0:\n      seconds = config.mm_autounload_after_seconds\n      for key in alive_keys:\n        if key != model_name and self.cache[key].last_used + seconds < time():\n          self.unload(key, 'timeout')\n          alive_keys.remove(key)\n          alive_values.remove(self.cache[key])\n    if model_name not in self.cache:\n      self.cache[model_name] = MModel(model_name, load_function, unload_function)\n      self.cache[model_name].last_loaded = time()\n    elif not self.cache[model_name].model:\n      self.cache[model_name].model = load_function()\n      self.cache[model_name].last_loaded = time()\n    mem_diff = mem - self.get_memory()\n    mem_diff = mem_diff * int(mem_diff > 0)\n    item = self.cache[model_name]\n    item.memory = (mem_diff if memory == 'auto' else memory(item.model) or mem_diff) / 1e9\n    item.last_used = time()\n    item.use_count = (item.use_count + 1) if hasattr(item, 'use_count') else 1\n    if self.mm_management_policy == 'COUNT' or self.mm_management_policy == 'BOTH':\n      cache_count = len(alive_keys)\n      if cache_count > 0 and cache_count > self.cached_model_count:\n        unloaded_key = self.unload_by_policy(model_name, alive_values)\n        if self.mm_management_policy == 'BOTH':\n          alive_keys.remove(unloaded_key)\n          alive_values.remove(self.cache[unloaded_key])\n    if self.mm_management_policy == 'MEMORY' or self.mm_management_policy == 'BOTH' \\\n    and len(alive_values) > 0:\n      items_memory = list(item.memory for item in alive_values)\n      total_memory_used = sum(items_memory)\n      memory_available = self.get_memory()\n      if memory_available < max(items_memory) * 1.3 \\\n      or memory_available < self.starting_memory/3 \\\n      or total_memory_used * 1.3 > self.starting_memory:\n        self.unload_by_policy(model_name, alive_values)\n    return self.cache[model_name].model\n\n  def unload(self, name, reason):\n    target = self.cache[name]\n    if target.unload is not None:\n      target.unload(target.model)\n    self.cache[name].model = None\n    print('removed', name, 'from model cache by', reason)\n\n  def unload_by_policy(self, model_name, items):\n    if config.mm_unload_order_policy == 'LEAST_USED':\n      items = sorted(items, key=lambda x: x.use_count)\n    if config.mm_unload_order_policy == 'OLDEST_USE_TIME':\n      items = sorted(items, key=lambda x: x.last_used)\n    if config.mm_unload_order_policy == 'OLDEST_LOAD_ORDER':\n      items = sorted(items, key=lambda x: x.last_loaded)\n    if config.mm_unload_order_policy == 'MEMORY_FOOTPRINT':\n      items = sorted(items, key=lambda x: x.memory)[::-1]\n    to_unload = items[0].name\n    if to_unload == model_name and len(items) > 1:\n      to_unload = items[1].name\n    self.unload(to_unload, config.mm_unload_order_policy)\n    return to_unload", "\nclass MemoryManager:\n  def __init__(self, get_memory, cached_model_count):\n    self.get_memory = get_memory\n    self.starting_memory = get_memory()\n    self.cache = {}\n    self.cached_model_count = cached_model_count\n    self.mm_management_policy = config.mm_management_policy\n  \n  def wrap(self, model_name, load_function=None, unload_function=None, memory='auto'):\n    mem = self.get_memory()\n    # get keys and values of items with model != None as lists\n    [*alive_keys], [*alive_values] = zip(*((i.name, i) for i in self.cache.values() if i.model is not None)) \\\n      if len(self.cache.keys()) > 0 else ([],[])\n    if config.mm_autounload_after_seconds > 0:\n      seconds = config.mm_autounload_after_seconds\n      for key in alive_keys:\n        if key != model_name and self.cache[key].last_used + seconds < time():\n          self.unload(key, 'timeout')\n          alive_keys.remove(key)\n          alive_values.remove(self.cache[key])\n    if model_name not in self.cache:\n      self.cache[model_name] = MModel(model_name, load_function, unload_function)\n      self.cache[model_name].last_loaded = time()\n    elif not self.cache[model_name].model:\n      self.cache[model_name].model = load_function()\n      self.cache[model_name].last_loaded = time()\n    mem_diff = mem - self.get_memory()\n    mem_diff = mem_diff * int(mem_diff > 0)\n    item = self.cache[model_name]\n    item.memory = (mem_diff if memory == 'auto' else memory(item.model) or mem_diff) / 1e9\n    item.last_used = time()\n    item.use_count = (item.use_count + 1) if hasattr(item, 'use_count') else 1\n    if self.mm_management_policy == 'COUNT' or self.mm_management_policy == 'BOTH':\n      cache_count = len(alive_keys)\n      if cache_count > 0 and cache_count > self.cached_model_count:\n        unloaded_key = self.unload_by_policy(model_name, alive_values)\n        if self.mm_management_policy == 'BOTH':\n          alive_keys.remove(unloaded_key)\n          alive_values.remove(self.cache[unloaded_key])\n    if self.mm_management_policy == 'MEMORY' or self.mm_management_policy == 'BOTH' \\\n    and len(alive_values) > 0:\n      items_memory = list(item.memory for item in alive_values)\n      total_memory_used = sum(items_memory)\n      memory_available = self.get_memory()\n      if memory_available < max(items_memory) * 1.3 \\\n      or memory_available < self.starting_memory/3 \\\n      or total_memory_used * 1.3 > self.starting_memory:\n        self.unload_by_policy(model_name, alive_values)\n    return self.cache[model_name].model\n\n  def unload(self, name, reason):\n    target = self.cache[name]\n    if target.unload is not None:\n      target.unload(target.model)\n    self.cache[name].model = None\n    print('removed', name, 'from model cache by', reason)\n\n  def unload_by_policy(self, model_name, items):\n    if config.mm_unload_order_policy == 'LEAST_USED':\n      items = sorted(items, key=lambda x: x.use_count)\n    if config.mm_unload_order_policy == 'OLDEST_USE_TIME':\n      items = sorted(items, key=lambda x: x.last_used)\n    if config.mm_unload_order_policy == 'OLDEST_LOAD_ORDER':\n      items = sorted(items, key=lambda x: x.last_loaded)\n    if config.mm_unload_order_policy == 'MEMORY_FOOTPRINT':\n      items = sorted(items, key=lambda x: x.memory)[::-1]\n    to_unload = items[0].name\n    if to_unload == model_name and len(items) > 1:\n      to_unload = items[1].name\n    self.unload(to_unload, config.mm_unload_order_policy)\n    return to_unload", "    \nRAM = MemoryManager(get_system_ram_info, config.mm_ram_cached_model_count_limit)\nVRAM = MemoryManager(get_vram_info, config.mm_vram_cached_model_count_limit) if GPU_AVAILABLE else False\n\ndef mload(*args, gpu=False, **kwargs):\n  if 'gpu' in kwargs and kwargs['gpu'] and GPU_AVAILABLE:\n    return VRAM.wrap(*args, **kwargs)\n  else:\n    return RAM.wrap(*args, **kwargs)"]}
