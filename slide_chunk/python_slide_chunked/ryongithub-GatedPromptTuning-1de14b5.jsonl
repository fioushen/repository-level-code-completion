{"filename": "train.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\nmajor actions here: fine-tune the features and evaluate different settings\n\"\"\"\nimport os\nimport torch\nimport warnings\n\nimport numpy as np\nimport random", "import numpy as np\nimport random\n\nfrom time import sleep\nfrom random import randint\n\nimport src.utils.logging as logging\nfrom src.configs.config import get_cfg\nfrom src.data import loader as data_loader\nfrom src.engine.evaluator import Evaluator", "from src.data import loader as data_loader\nfrom src.engine.evaluator import Evaluator\nfrom src.engine.trainer import Trainer\nfrom src.models.build_model import build_model\nfrom src.utils.file_io import PathManager\n\nfrom launch import default_argument_parser, logging_train_setup\nwarnings.filterwarnings(\"ignore\")\ntorch.set_num_threads(4)\n", "torch.set_num_threads(4)\n\n\ndef setup(args):\n    \"\"\"\n    Create configs and perform basic setups.\n    \"\"\"\n    cfg = get_cfg()\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n\n    # setup dist\n    # cfg.DIST_INIT_PATH = \"tcp://{}:12399\".format(os.environ[\"SLURMD_NODENAME\"])\n\n    # setup output dir\n    # output_dir / data_name / feature_name / lr_wd / run1\n    output_dir = cfg.OUTPUT_DIR\n    lr = cfg.SOLVER.BASE_LR\n    wd = cfg.SOLVER.WEIGHT_DECAY\n    output_folder = os.path.join(\n        cfg.DATA.NAME, cfg.DATA.FEATURE, f\"{args.id}_lr{lr}_wd{wd}\")\n\n    # train cfg.RUN_N_TIMES times\n    count = 1\n    while count <= cfg.RUN_N_TIMES:\n        output_path = os.path.join(output_dir, output_folder, f\"run{count}\")\n        # pause for a random time, so concurrent process with same setting won't interfere with each other. # noqa\n        sleep(randint(3, 30))\n        if not PathManager.exists(output_path):\n            PathManager.mkdirs(output_path)\n            cfg.OUTPUT_DIR = output_path\n            break\n        else:\n            count += 1\n    if count > cfg.RUN_N_TIMES:\n        raise ValueError(\n            f\"Already run {cfg.RUN_N_TIMES} times for {output_folder}, no need to run more\")\n\n    cfg.freeze()\n    return cfg", "\n\ndef get_loaders(cfg, logger):\n    logger.info(\"Loading training data (final training data for vtab)...\")\n    if cfg.DATA.NAME.startswith(\"vtab-\"):\n        train_loader = data_loader.construct_trainval_loader(cfg)\n    else:\n        train_loader = data_loader.construct_train_loader(cfg)\n\n    logger.info(\"Loading validation data...\")\n    # not really needed for vtab\n    val_loader = data_loader.construct_val_loader(cfg)\n    logger.info(\"Loading test data...\")\n    if cfg.DATA.NO_TEST:\n        logger.info(\"...no test data is constructed\")\n        test_loader = None\n    else:\n        test_loader = data_loader.construct_test_loader(cfg)\n    return train_loader,  val_loader, test_loader", "\n\ndef train(cfg, args):\n    # clear up residual cache from previous runs\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    # main training / eval actions here\n\n    # fix the seed for reproducibility\n    if cfg.SEED is not None:\n        torch.manual_seed(cfg.SEED)\n        np.random.seed(cfg.SEED)\n        random.seed(0)\n\n    # setup training env including loggers\n    logging_train_setup(args, cfg)\n    logger = logging.get_logger(\"visual_prompt\")\n\n    train_loader, val_loader, test_loader = get_loaders(cfg, logger)\n    logger.info(\"Constructing models...\")\n    model, cur_device = build_model(cfg)\n\n    trainable_params = [name for name, p in model.named_parameters() if p.requires_grad]\n    print(trainable_params)\n\n    logger.info(\"Setting up Evalutator...\")\n    evaluator = Evaluator()\n    logger.info(\"Setting up Trainer...\")\n    trainer = Trainer(cfg, model, evaluator, cur_device)\n\n    if train_loader:\n        trainer.train_classifier(train_loader, val_loader, test_loader)\n    else:\n        print(\"No train loader presented. Exit\")\n\n    if cfg.SOLVER.TOTAL_EPOCH == 0:\n        trainer.eval_classifier(test_loader, \"test\", 0)", "\n\ndef main(args):\n    \"\"\"main function to call from workflow\"\"\"\n    # set up cfg and args\n    cfg = setup(args)\n    with open(os.path.join(cfg.OUTPUT_DIR, 'configs.yaml'), 'w') as f:\n        f.write(cfg.dump())\n    # Perform training.\n    train(cfg, args)", "\n\nif __name__ == '__main__':\n    args = default_argument_parser().parse_args()\n    main(args)\n"]}
{"filename": "launch.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\nlaunch helper functions\n\"\"\"\nimport argparse\nimport os\nimport sys\nimport pprint\nimport PIL\nfrom collections import defaultdict", "import PIL\nfrom collections import defaultdict\nfrom tabulate import tabulate\nfrom typing import Tuple\n\nimport torch\nfrom src.utils.file_io import PathManager\nfrom src.utils import logging\nfrom src.utils.distributed import get_rank, get_world_size\n", "from src.utils.distributed import get_rank, get_world_size\n\n\ndef collect_torch_env() -> str:\n    try:\n        import torch.__config__\n\n        return torch.__config__.show()\n    except ImportError:\n        # compatible with older versions of pytorch\n        from torch.utils.collect_env import get_pretty_env_info\n\n        return get_pretty_env_info()", "\n\ndef get_env_module() -> Tuple[str]:\n    var_name = \"ENV_MODULE\"\n    return var_name, os.environ.get(var_name, \"<not set>\")\n\n\ndef collect_env_info() -> str:\n    data = []\n    data.append((\"Python\", sys.version.replace(\"\\n\", \"\")))\n    data.append(get_env_module())\n    data.append((\"PyTorch\", torch.__version__))\n    data.append((\"PyTorch Debug Build\", torch.version.debug))\n\n    has_cuda = torch.cuda.is_available()\n    data.append((\"CUDA available\", has_cuda))\n    if has_cuda:\n        data.append((\"CUDA ID\", os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n        devices = defaultdict(list)\n        for k in range(torch.cuda.device_count()):\n            devices[torch.cuda.get_device_name(k)].append(str(k))\n        for name, devids in devices.items():\n            data.append((\"GPU \" + \",\".join(devids), name))\n    data.append((\"Pillow\", PIL.__version__))\n\n    try:\n        import cv2\n\n        data.append((\"cv2\", cv2.__version__))\n    except ImportError:\n        pass\n    env_str = tabulate(data) + \"\\n\"\n    env_str += collect_torch_env()\n    return env_str", "\n\ndef default_argument_parser():\n    \"\"\"\n    create a simple parser to wrap around config file\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"visual-prompt\")\n    parser.add_argument(\n        \"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n    parser.add_argument(\n        \"--train-type\", default=\"\", help=\"training types\")\n    parser.add_argument(\n        \"opts\",\n        help=\"Modify config options using the command-line\",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    from datetime import datetime\n    time_fmt = datetime.now().strftime(\"%y%m%d-%H-%M-%S\")\n    parser.add_argument('--id', default=time_fmt)\n\n    return parser", "\n\ndef logging_train_setup(args, cfg) -> None:\n    output_dir = cfg.OUTPUT_DIR\n    if output_dir:\n        PathManager.mkdirs(output_dir)\n\n    logger = logging.setup_logging(\n        cfg.NUM_GPUS, get_world_size(), output_dir, name=\"visual_prompt\")\n\n    # Log basic information about environment, cmdline arguments, and config\n    rank = get_rank()\n    logger.info(\n        f\"Rank of current process: {rank}. World size: {get_world_size()}\")\n    logger.info(\"Environment info:\\n\" + collect_env_info())\n\n    logger.info(\"Command line arguments: \" + str(args))\n    if hasattr(args, \"config_file\") and args.config_file != \"\":\n        logger.info(\n            \"Contents of args.config_file={}:\\n{}\".format(\n                args.config_file,\n                PathManager.open(args.config_file, \"r\").read()\n            )\n        )\n    # Show the config\n    logger.info(\"Training with config:\")\n    logger.info(pprint.pformat(cfg))\n    # cudnn benchmark has large overhead.\n    # It shouldn't be used considering the small size of typical val set.\n    if not (hasattr(args, \"eval_only\") and args.eval_only):\n        torch.backends.cudnn.benchmark = cfg.CUDNN_BENCHMARK", ""]}
{"filename": "src/configs/vit_configs.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\nCopyright (c) Meta Platforms, Inc. All Rights Reserved\nhttps://github.com/jeonsworld/ViT-pytorch/blob/main/models/configs.py\n\"\"\"\nimport ml_collections\n\n\ndef get_testing():\n    \"\"\"Returns a minimal configuration for testing.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 1\n    config.transformer.num_heads = 1\n    config.transformer.num_layers = 1\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config", "def get_testing():\n    \"\"\"Returns a minimal configuration for testing.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 1\n    config.transformer.num_heads = 1\n    config.transformer.num_layers = 1\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config", "\n\ndef get_b16_config():\n    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 768\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 3072\n    config.transformer.num_heads = 12\n    config.transformer.num_layers = 12\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config", "\n\ndef get_r50_b16_config():\n    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n    config = get_b16_config()\n    del config.patches.size\n    config.patches.grid = (14, 14)\n    config.resnet = ml_collections.ConfigDict()\n    config.resnet.num_layers = (3, 4, 9)\n    config.resnet.width_factor = 1\n    return config", "\n\ndef get_b32_config():\n    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.size = (32, 32)\n    return config\n\n\ndef get_b8_config():\n    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.size = (8, 8)\n    return config", "\ndef get_b8_config():\n    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n    config = get_b16_config()\n    config.patches.size = (8, 8)\n    return config\n\n\ndef get_l16_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1024\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 4096\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 24\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config", "def get_l16_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n    config.hidden_size = 1024\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 4096\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 24\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config", "\n\ndef get_l32_config():\n    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n    config = get_l16_config()\n    config.patches.size = (32, 32)\n    return config\n\n\ndef get_h14_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n    config.hidden_size = 1280\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 5120\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 32\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config", "\ndef get_h14_config():\n    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n    config = ml_collections.ConfigDict()\n    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n    config.hidden_size = 1280\n    config.transformer = ml_collections.ConfigDict()\n    config.transformer.mlp_dim = 5120\n    config.transformer.num_heads = 16\n    config.transformer.num_layers = 32\n    config.transformer.attention_dropout_rate = 0.0\n    config.transformer.dropout_rate = 0.1\n    config.classifier = 'token'\n    config.representation_size = None\n    return config", ""]}
{"filename": "src/configs/config_node.py", "chunked_list": ["#!/usr/bin/env python3\n\n\"\"\"Config system (based on Detectron's).\"\"\"\n\nfrom fvcore.common.config import CfgNode as _CfgNode\nfrom ..utils.file_io import PathManager\n\n\nclass CfgNode(_CfgNode):\n    \"\"\"\n    The same as `fvcore.common.config.CfgNode`, but different in:\n\n    support manifold path\n    \"\"\"\n\n    @classmethod\n    def _open_cfg(cls, filename):\n        return PathManager.open(filename, \"r\")\n\n    def dump(self, *args, **kwargs):\n        \"\"\"\n        Returns:\n            str: a yaml string representation of the config\n        \"\"\"\n        # to make it show up in docs\n        return super().dump(*args, **kwargs)", "class CfgNode(_CfgNode):\n    \"\"\"\n    The same as `fvcore.common.config.CfgNode`, but different in:\n\n    support manifold path\n    \"\"\"\n\n    @classmethod\n    def _open_cfg(cls, filename):\n        return PathManager.open(filename, \"r\")\n\n    def dump(self, *args, **kwargs):\n        \"\"\"\n        Returns:\n            str: a yaml string representation of the config\n        \"\"\"\n        # to make it show up in docs\n        return super().dump(*args, **kwargs)", ""]}
{"filename": "src/configs/config.py", "chunked_list": ["#!/usr/bin/env python3\n\n\"\"\"Config system (based on Detectron's).\"\"\"\n\nfrom .config_node import CfgNode\n\n\n# Global config object\n_C = CfgNode()\n# Example usage:", "_C = CfgNode()\n# Example usage:\n#   from configs.config import cfg\n\n_C.DBG = False\n_C.OUTPUT_DIR = \"./output\"\n_C.RUN_N_TIMES = 5\n# Perform benchmarking to select the fastest CUDNN algorithms to use\n# Note that this may increase the memory usage and will likely not result\n# in overall speedups when variable size inputs are used (e.g. COCO training)", "# Note that this may increase the memory usage and will likely not result\n# in overall speedups when variable size inputs are used (e.g. COCO training)\n_C.CUDNN_BENCHMARK = False\n\n# Number of GPUs to use (applies to both training and testing)\n_C.NUM_GPUS = 1\n_C.NUM_SHARDS = 1\n\n# Note that non-determinism may still be present due to non-deterministic\n# operator implementations in GPU operator libraries", "# Note that non-determinism may still be present due to non-deterministic\n# operator implementations in GPU operator libraries\n_C.SEED = None\n\n# ----------------------------------------------------------------------\n# Model options\n# ----------------------------------------------------------------------\n_C.MODEL = CfgNode()\n_C.MODEL.TRANSFER_TYPE = \"linear\"  # one of linear, end2end, prompt, adapter, side, partial-1, tinytl-bias\n_C.MODEL.WEIGHT_PATH = \"\"  # if resume from some checkpoint file", "_C.MODEL.TRANSFER_TYPE = \"linear\"  # one of linear, end2end, prompt, adapter, side, partial-1, tinytl-bias\n_C.MODEL.WEIGHT_PATH = \"\"  # if resume from some checkpoint file\n_C.MODEL.SAVE_CKPT = True\n\n_C.MODEL.MODEL_ROOT = \"\"  # root folder for pretrained model weights\n\n_C.MODEL.TYPE = \"vit\"\n_C.MODEL.MLP_NUM = 0\n\n_C.MODEL.LINEAR = CfgNode()", "\n_C.MODEL.LINEAR = CfgNode()\n_C.MODEL.LINEAR.MLP_SIZES = []\n_C.MODEL.LINEAR.DROPOUT = 0.1\n\n# ----------------------------------------------------------------------\n# Prompt options\n# ----------------------------------------------------------------------\n_C.MODEL.PROMPT = CfgNode()\n_C.MODEL.PROMPT.NUM_TOKENS = 5", "_C.MODEL.PROMPT = CfgNode()\n_C.MODEL.PROMPT.NUM_TOKENS = 5\n_C.MODEL.PROMPT.LOCATION = \"prepend\"\n\n# prompt initalizatioin: \n    # (1) default \"random\"\n    # (2) \"final-cls\" use aggregated final [cls] embeddings from training dataset\n    # (3) \"cls-nolastl\": use first 12 cls embeddings (exclude the final output) for deep prompt\n        # (4) \"cls-nofirstl\": use last 12 cls embeddings (exclude the input to first layer)\n_C.MODEL.PROMPT.INITIATION = \"random\"  # \"final-cls\", \"cls-first12\"", "        # (4) \"cls-nofirstl\": use last 12 cls embeddings (exclude the input to first layer)\n_C.MODEL.PROMPT.INITIATION = \"random\"  # \"final-cls\", \"cls-first12\"\n_C.MODEL.PROMPT.CLSEMB_FOLDER = \"\"\n_C.MODEL.PROMPT.CLSEMB_PATH = \"\"\n_C.MODEL.PROMPT.PROJECT = -1  # \"projection mlp hidden dim\"\n_C.MODEL.PROMPT.DEEP = False # \"whether do deep prompt or not, only for prepend location\"\n\n\n_C.MODEL.PROMPT.NUM_DEEP_LAYERS = None  # if set to be an int, then do partial-deep prompt tuning\n_C.MODEL.PROMPT.REVERSE_DEEP = False  # if to only update last n layers, not the input layer", "_C.MODEL.PROMPT.NUM_DEEP_LAYERS = None  # if set to be an int, then do partial-deep prompt tuning\n_C.MODEL.PROMPT.REVERSE_DEEP = False  # if to only update last n layers, not the input layer\n_C.MODEL.PROMPT.DEEP_SHARED = False  # if true, all deep layers will be use the same prompt emb\n_C.MODEL.PROMPT.FORWARD_DEEP_NOEXPAND = False  # if true, will not expand input sequence for layers without prompt\n# how to get the output emb for cls head:\n    # original: follow the orignial backbone choice,\n    # img_pool: image patch pool only\n    # prompt_pool: prompt embd pool only\n    # imgprompt_pool: pool everything but the cls token\n_C.MODEL.PROMPT.VIT_POOL_TYPE = \"original\"", "    # imgprompt_pool: pool everything but the cls token\n_C.MODEL.PROMPT.VIT_POOL_TYPE = \"original\"\n_C.MODEL.PROMPT.DROPOUT = 0.0 \n_C.MODEL.PROMPT.SAVE_FOR_EACH_EPOCH = False\n\n_C.MODEL.PROMPT.GATE_PRIOR = False\n_C.MODEL.PROMPT.GATE_NUM = 11\n_C.MODEL.PROMPT.GATE_INIT = 10\n_C.MODEL.PROMPT.TEMP = 1.0\n_C.MODEL.PROMPT.TEMP_LEARN = False", "_C.MODEL.PROMPT.TEMP = 1.0\n_C.MODEL.PROMPT.TEMP_LEARN = False\n_C.MODEL.PROMPT.TEMP_NUM = 12\n_C.MODEL.PROMPT.TEMP_MIN = 0.01\n_C.MODEL.PROMPT.TEMP_MAX = 10.0\n# _C.MODEL.PROMPT.TEMP_MIN = 0.05\n# _C.MODEL.PROMPT.TEMP_MAX = 5.0\n\n# ----------------------------------------------------------------------\n# adapter options", "# ----------------------------------------------------------------------\n# adapter options\n# ----------------------------------------------------------------------\n_C.MODEL.ADAPTER = CfgNode()\n_C.MODEL.ADAPTER.REDUCATION_FACTOR = 8\n_C.MODEL.ADAPTER.STYLE = \"Pfeiffer\"\n\n# ----------------------------------------------------------------------\n# Solver options\n# ----------------------------------------------------------------------", "# Solver options\n# ----------------------------------------------------------------------\n_C.SOLVER = CfgNode()\n_C.SOLVER.LOSS = \"softmax\"\n_C.SOLVER.LOSS_ALPHA = 0.01\n\n_C.SOLVER.OPTIMIZER = \"sgd\"  # or \"adamw\"\n_C.SOLVER.MOMENTUM = 0.9\n_C.SOLVER.WEIGHT_DECAY = 0.0001\n_C.SOLVER.WEIGHT_DECAY_BIAS = 0", "_C.SOLVER.WEIGHT_DECAY = 0.0001\n_C.SOLVER.WEIGHT_DECAY_BIAS = 0\n\n_C.SOLVER.PATIENCE = 300\n\n\n_C.SOLVER.SCHEDULER = \"cosine\"\n\n_C.SOLVER.BASE_LR = 0.01\n_C.SOLVER.BIAS_MULTIPLIER = 1.              # for prompt + bias", "_C.SOLVER.BASE_LR = 0.01\n_C.SOLVER.BIAS_MULTIPLIER = 1.              # for prompt + bias\n\n_C.SOLVER.WARMUP_EPOCH = 5\n_C.SOLVER.TOTAL_EPOCH = 30\n_C.SOLVER.LOG_EVERY_N = 1000\n\n\n_C.SOLVER.DBG_TRAINABLE = False # if True, will print the name of trainable params\n", "_C.SOLVER.DBG_TRAINABLE = False # if True, will print the name of trainable params\n\n# ----------------------------------------------------------------------\n# Dataset options\n# ----------------------------------------------------------------------\n_C.DATA = CfgNode()\n\n_C.DATA.NAME = \"\"\n_C.DATA.DATAPATH = \"\"\n_C.DATA.FEATURE = \"\"  # e.g. inat2021_supervised", "_C.DATA.DATAPATH = \"\"\n_C.DATA.FEATURE = \"\"  # e.g. inat2021_supervised\n\n_C.DATA.PERCENTAGE = 1.0\n_C.DATA.NUMBER_CLASSES = -1\n_C.DATA.MULTILABEL = False\n_C.DATA.CLASS_WEIGHTS_TYPE = \"none\"\n\n_C.DATA.CROPSIZE = 224  # or 384\n", "_C.DATA.CROPSIZE = 224  # or 384\n\n_C.DATA.NO_TEST = False\n_C.DATA.BATCH_SIZE = 32\n# Number of data loader workers per training process\n_C.DATA.NUM_WORKERS = 4\n# Load data to pinned host memory\n_C.DATA.PIN_MEMORY = True\n\n", "\n\n_C.DIST_BACKEND = \"nccl\"\n_C.DIST_INIT_PATH = \"env://\"\n_C.DIST_INIT_FILE = \"\"\n\n\ndef get_cfg():\n    \"\"\"\n    Get a copy of the default config.\n    \"\"\"\n    return _C.clone()", ""]}
{"filename": "src/utils/distributed.py", "chunked_list": ["#!/usr/bin/env python3\n\n\"\"\"Distributed helpers.\"\"\"\n\nimport torch\nimport torch.distributed as dist\n_LOCAL_PROCESS_GROUP = None\n\n\ndef get_world_size() -> int:\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()", "\ndef get_world_size() -> int:\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank() -> int:\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()", "\ndef get_rank() -> int:\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_master_process(num_gpus=8):\n    \"\"\"\n    Determines if the current process is the master process.\n    \"\"\"\n    if torch.distributed.is_initialized():\n        return dist.get_rank() % num_gpus == 0\n    else:\n        return True", "\ndef is_master_process(num_gpus=8):\n    \"\"\"\n    Determines if the current process is the master process.\n    \"\"\"\n    if torch.distributed.is_initialized():\n        return dist.get_rank() % num_gpus == 0\n    else:\n        return True\n", "\n\ndef run(\n    local_rank,\n    num_proc,\n    func,\n    init_method,\n    shard_id,\n    num_shards,\n    backend,\n    cfg,\n    args,\n):\n    \"\"\"\n    Runs a function from a child process.\n    Args:\n        local_rank (int): rank of the current process on the current machine.\n        num_proc (int): number of processes per machine.\n        func (function): function to execute on each of the process.\n        init_method (string): method to initialize the distributed training.\n            TCP initialization: equiring a network address reachable from all\n            processes followed by the port.\n            Shared file-system initialization: makes use of a file system that\n            is shared and visible from all machines. The URL should start with\n            file:// and contain a path to a non-existent file on a shared file\n            system.\n        shard_id (int): the rank of the current machine.\n        num_shards (int): number of overall machines for the distributed\n            training job.\n        backend (string): three distributed backends ('nccl', 'gloo', 'mpi') are\n            supports, each with different capabilities. Details can be found\n            here:\n            https://pytorch.org/docs/stable/distributed.html\n        cfg (CfgNode): configs. Details can be found in\n            loco/config/defaults.py\n    \"\"\"\n    # Initialize the process group.\n    # shard_id = get_rank()\n    world_size = num_proc * num_shards\n    rank = shard_id * num_proc + local_rank\n\n    try:\n        torch.distributed.init_process_group(\n            backend=backend,\n            init_method=init_method,\n            world_size=world_size,\n            rank=rank,\n        )\n    except Exception as e:\n        raise e\n\n    torch.cuda.set_device(local_rank)\n    func(cfg, args)", "\n\ndef destroy_process_group():\n    \"\"\"Destroys the default process group.\"\"\"\n    torch.distributed.destroy_process_group()\n\n\ndef scaled_all_reduce(cfg, tensors):\n    \"\"\"Performs the scaled all_reduce operation on the provided tensors.\n\n    The input tensors are modified in-place. Currently supports only the sum\n    reduction operator. The reduced values are scaled by the inverse size of\n    the process group (equivalent to cfg.NUM_GPUS).\n    \"\"\"\n    # Queue the reductions\n    reductions = []\n    for tensor in tensors:\n        reduction = torch.distributed.all_reduce(tensor, async_op=True)\n        reductions.append(reduction)\n    # Wait for reductions to finish\n    for reduction in reductions:\n        reduction.wait()\n    # Scale the results\n    for tensor in tensors:\n        tensor.mul_(1.0 / cfg.NUM_GPUS / cfg.NUM_SHARDS)\n    return tensors", "\n\ndef cat_all_gather(tensors):\n    \"\"\"Performs the concatenated all_gather operation on the provided tensors.\n    \"\"\"\n    tensors_gather = [\n        torch.ones_like(tensors)\n        for _ in range(torch.distributed.get_world_size())\n    ]\n    torch.distributed.all_gather(tensors_gather, tensors, async_op=False)\n\n    output = torch.cat(tensors_gather, dim=0)\n    return output", "\n\ndef local_cat_all_gather(tensors):\n    \"\"\"Performs the concatenated all_gather operation on the provided tensors.\n    \"\"\"\n    tensors_gather = [\n        torch.ones_like(tensors)\n        for _ in range(get_local_size())\n    ]\n    torch.distributed.all_gather(\n        tensors_gather,\n        tensors,\n        async_op=False,\n        group=_LOCAL_PROCESS_GROUP,\n    )\n    output = torch.cat(tensors_gather, dim=0)\n    return output", "\n\ndef get_local_size():\n    \"\"\"\n    Returns:\n        The size of the per-machine process group,\n        i.e. the number of processes per machine.\n    \"\"\"\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size(group=_LOCAL_PROCESS_GROUP)", "\n\ndef get_local_rank():\n    \"\"\"\n    Returns:\n        The rank of the current process within the local (per-machine) process group.\n    \"\"\"\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    assert _LOCAL_PROCESS_GROUP is not None\n    return dist.get_rank(group=_LOCAL_PROCESS_GROUP)", ""]}
{"filename": "src/utils/file_io.py", "chunked_list": ["#!/usr/bin/env python3\n\n\"\"\"\nProject specific pathmanagers for a project as recommended by Detectron2\n\"\"\"\nfrom iopath.common.file_io import PathManager as PathManagerBase\nfrom iopath.common.file_io import HTTPURLHandler\n\n\nPathManager = PathManagerBase()", "\nPathManager = PathManagerBase()\nPathManager.register_handler(HTTPURLHandler())\n"]}
{"filename": "src/utils/io_utils.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\na bunch of helper functions for read and write data\n\"\"\"\nimport os\nimport json\nimport numpy as np\nimport time\nimport pandas as pd\n", "import pandas as pd\n\nfrom typing import List, Union\nfrom PIL import Image, ImageFile\nImage.MAX_IMAGE_PIXELS = None\n\n\ndef save_or_append_df(out_path, df):\n    if os.path.exists(out_path):\n        previous_df = pd.read_pickle(out_path)\n        df = pd.concat([previous_df, df], ignore_index=True)\n    df.to_pickle(out_path)\n    print(f\"Saved output at {out_path}\")", "\n\nclass JSONEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, bytes):\n            return str(obj, encoding='utf-8')\n        elif isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            # return super(MyEncoder, self).default(obj)\n\n            raise TypeError(\n                \"Unserializable object {} of type {}\".format(obj, type(obj))\n            )", "\n\ndef write_json(data: Union[list, dict], outfile: str) -> None:\n    json_dir, _ = os.path.split(outfile)\n    if json_dir and not os.path.exists(json_dir):\n        os.makedirs(json_dir)\n\n    with open(outfile, 'w') as f:\n        json.dump(data, f, cls=JSONEncoder, ensure_ascii=False, indent=2)\n", "\n\ndef read_json(filename: str) -> Union[list, dict]:\n    \"\"\"read json files\"\"\"\n    with open(filename, \"rb\") as fin:\n        data = json.load(fin, encoding=\"utf-8\")\n    return data\n\n\ndef pil_loader(path: str) -> Image.Image:\n    \"\"\"load an image from path, and suppress warning\"\"\"\n    # to avoid crashing for truncated (corrupted images)\n    ImageFile.LOAD_TRUNCATED_IMAGES = True\n    # open path as file to avoid ResourceWarning\n    # (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')", "\ndef pil_loader(path: str) -> Image.Image:\n    \"\"\"load an image from path, and suppress warning\"\"\"\n    # to avoid crashing for truncated (corrupted images)\n    ImageFile.LOAD_TRUNCATED_IMAGES = True\n    # open path as file to avoid ResourceWarning\n    # (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')", ""]}
{"filename": "src/utils/distributed_orig.py", "chunked_list": ["#!/usr/bin/env python3\n\n\"\"\"Distributed helpers.\"\"\"\n\nimport torch\nimport torch.distributed as dist\n_LOCAL_PROCESS_GROUP = None\n\n\ndef get_world_size() -> int:\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()", "\ndef get_world_size() -> int:\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank() -> int:\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()", "\ndef get_rank() -> int:\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_master_process(num_gpus=8):\n    \"\"\"\n    Determines if the current process is the master process.\n    \"\"\"\n    if torch.distributed.is_initialized():\n        return dist.get_rank() % num_gpus == 0\n    else:\n        return True", "\ndef is_master_process(num_gpus=8):\n    \"\"\"\n    Determines if the current process is the master process.\n    \"\"\"\n    if torch.distributed.is_initialized():\n        return dist.get_rank() % num_gpus == 0\n    else:\n        return True\n", "\n\ndef run(\n    local_rank,\n    num_proc,\n    func,\n    init_method,\n    shard_id,\n    num_shards,\n    backend,\n    cfg,\n    args,\n):\n    \"\"\"\n    Runs a function from a child process.\n    Args:\n        local_rank (int): rank of the current process on the current machine.\n        num_proc (int): number of processes per machine.\n        func (function): function to execute on each of the process.\n        init_method (string): method to initialize the distributed training.\n            TCP initialization: equiring a network address reachable from all\n            processes followed by the port.\n            Shared file-system initialization: makes use of a file system that\n            is shared and visible from all machines. The URL should start with\n            file:// and contain a path to a non-existent file on a shared file\n            system.\n        shard_id (int): the rank of the current machine.\n        num_shards (int): number of overall machines for the distributed\n            training job.\n        backend (string): three distributed backends ('nccl', 'gloo', 'mpi') are\n            supports, each with different capabilities. Details can be found\n            here:\n            https://pytorch.org/docs/stable/distributed.html\n        cfg (CfgNode): configs. Details can be found in\n            loco/config/defaults.py\n    \"\"\"\n    # Initialize the process group.\n    # shard_id = get_rank()\n    world_size = num_proc * num_shards\n    rank = shard_id * num_proc + local_rank\n\n    try:\n        torch.distributed.init_process_group(\n            backend=backend,\n            init_method=init_method,\n            world_size=world_size,\n            rank=rank,\n        )\n    except Exception as e:\n        raise e\n\n    torch.cuda.set_device(local_rank)\n    func(cfg, args)", "\n\ndef destroy_process_group():\n    \"\"\"Destroys the default process group.\"\"\"\n    torch.distributed.destroy_process_group()\n\n\ndef scaled_all_reduce(cfg, tensors):\n    \"\"\"Performs the scaled all_reduce operation on the provided tensors.\n\n    The input tensors are modified in-place. Currently supports only the sum\n    reduction operator. The reduced values are scaled by the inverse size of\n    the process group (equivalent to cfg.NUM_GPUS).\n    \"\"\"\n    # Queue the reductions\n    reductions = []\n    for tensor in tensors:\n        reduction = torch.distributed.all_reduce(tensor, async_op=True)\n        reductions.append(reduction)\n    # Wait for reductions to finish\n    for reduction in reductions:\n        reduction.wait()\n    # Scale the results\n    for tensor in tensors:\n        tensor.mul_(1.0 / cfg.NUM_GPUS / cfg.NUM_SHARDS)\n    return tensors", "\n\ndef cat_all_gather(tensors):\n    \"\"\"Performs the concatenated all_gather operation on the provided tensors.\n    \"\"\"\n    tensors_gather = [\n        torch.ones_like(tensors)\n        for _ in range(torch.distributed.get_world_size())\n    ]\n    torch.distributed.all_gather(tensors_gather, tensors, async_op=False)\n\n    output = torch.cat(tensors_gather, dim=0)\n    return output", "\n\ndef local_cat_all_gather(tensors):\n    \"\"\"Performs the concatenated all_gather operation on the provided tensors.\n    \"\"\"\n    tensors_gather = [\n        torch.ones_like(tensors)\n        for _ in range(get_local_size())\n    ]\n    torch.distributed.all_gather(\n        tensors_gather,\n        tensors,\n        async_op=False,\n        group=_LOCAL_PROCESS_GROUP,\n    )\n    output = torch.cat(tensors_gather, dim=0)\n    return output", "\n\ndef get_local_size():\n    \"\"\"\n    Returns:\n        The size of the per-machine process group,\n        i.e. the number of processes per machine.\n    \"\"\"\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size(group=_LOCAL_PROCESS_GROUP)", "\n\ndef get_local_rank():\n    \"\"\"\n    Returns:\n        The rank of the current process within the local (per-machine) process group.\n    \"\"\"\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    assert _LOCAL_PROCESS_GROUP is not None\n    return dist.get_rank(group=_LOCAL_PROCESS_GROUP)", ""]}
{"filename": "src/utils/train_utils.py", "chunked_list": ["#!/usr/bin/env python3\nimport torch\n\n\ndef gpu_mem_usage():\n    \"\"\"Computes the GPU memory usage for the current device (GB).\"\"\"\n    if not torch.cuda.is_available():\n        return 0\n    # Number of bytes in a megabyte\n    _B_IN_GB = 1024 * 1024 * 1024\n\n    mem_usage_bytes = torch.cuda.max_memory_allocated()\n    return mem_usage_bytes / _B_IN_GB", "\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)", ""]}
{"filename": "src/utils/vis_utils.py", "chunked_list": ["import datetime\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom sklearn.metrics import confusion_matrix", "from collections import defaultdict\nfrom sklearn.metrics import confusion_matrix\n# plt.rcParams[\"axes.grid\"] = False\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nLOG_NAME = \"logs.txt\"\n\n\ndef remove_trailing(eval_dict):\n    min_num = min([len(v) for k, v in eval_dict.items() if \"top5\" not in k])\n    new_dict ={}\n    for k, v in eval_dict.items():\n        if \"top5\" not in k:\n            new_dict[k] = v[:min_num]\n    return new_dict", "\ndef remove_trailing(eval_dict):\n    min_num = min([len(v) for k, v in eval_dict.items() if \"top5\" not in k])\n    new_dict ={}\n    for k, v in eval_dict.items():\n        if \"top5\" not in k:\n            new_dict[k] = v[:min_num]\n    return new_dict\n\n\ndef get_meta(job_root, job_path, model_type):\n    # get lr, wd, feature-type, dataset\n    j_data = job_path.split(\"/run\")[0].split(\n        job_root + \"/\" + model_type)[-1].split(\"/\")\n    data_name, feat_type, opt_params = j_data[1], j_data[2], j_data[3]\n    lr = float(opt_params.split(\"_\")[0].split(\"lr\")[-1])\n    wd = float(opt_params.split(\"_\")[1].split(\"wd\")[-1])\n    return data_name, feat_type, lr, wd", "\n\ndef get_meta(job_root, job_path, model_type):\n    # get lr, wd, feature-type, dataset\n    j_data = job_path.split(\"/run\")[0].split(\n        job_root + \"/\" + model_type)[-1].split(\"/\")\n    data_name, feat_type, opt_params = j_data[1], j_data[2], j_data[3]\n    lr = float(opt_params.split(\"_\")[0].split(\"lr\")[-1])\n    wd = float(opt_params.split(\"_\")[1].split(\"wd\")[-1])\n    return data_name, feat_type, lr, wd", "\n\ndef update_eval(line, eval_dict, data_name):        \n    if \"top1\" in line and \"top\" in line.split(\": top1:\")[-1]:\n        metric = \"top\"     \n    else:\n        metric = \"rocauc\"\n    top1 = float(line.split(\": top1:\")[-1].split(metric)[0])\n    eval_type = line.split(\" Classification results with \")[-1].split(\": top1\")[0] \n    eval_type = \"\".join(eval_type.split(\"_\" + data_name))\n    eval_dict[eval_type + \"_top1\"].append(top1)", "\n\ndef get_nmi(job_path):\n    with open(job_path) as f:\n        lines = f.readlines()\n    nmi_dict = defaultdict(list)\n    num_jobs = 0\n    log_temp = []\n    for l in lines:  #, leave=False):\n        if \"Rank of current process:\" in l:\n            num_jobs += 1\n        if num_jobs == 2:\n            break\n        if \"Clutering nmi\" in l:\n            n = l.split(\"Clutering nmi: \")[-1].split(\",\")[0]\n            a_n = l.split(\"adjusted nmi: \")[-1].split(\",\")[0]\n            v = l.split(\"v: \")[-1].split(\",\")[0]\n            nmi_dict[\"nmi\"].append(float(n))\n            nmi_dict[\"a_nmi\"].append(float(a_n))\n            nmi_dict[\"v_nmi\"].append(float(v))\n    return nmi_dict", "\n\ndef get_mean_accuracy(job_path, data_name):\n    val_data = torch.load(\n        job_path.replace(\"logs.txt\", f\"val_{data_name}_logits.pth\"))\n    test_data = torch.load(\n        job_path.replace(\"logs.txt\", f\"val_{data_name}_logits.pth\"))\n    v_matrix = confusion_matrix(\n        val_data['targets'],\n        np.argmax(val_data['joint_logits'], 1)\n    )\n    t_matrix = confusion_matrix(\n        test_data['targets'],\n        np.argmax(test_data['joint_logits'], 1)\n    )\n    return np.mean(v_matrix.diagonal()/v_matrix.sum(axis=1) ) * 100, np.mean(t_matrix.diagonal()/t_matrix.sum(axis=1) ) * 100", "\n\ndef get_training_data(job_path, model_type, job_root):\n    data_name, feat_type, lr, wd = get_meta(job_root, job_path, model_type)\n    with open(job_path) as f:\n        lines = f.readlines()\n\n    # get training loss per epoch, \n    # cls results for both val and test\n    train_loss = []\n    eval_dict = defaultdict(list)\n#     best_epoch = -1\n    num_jobs = 0\n    total_params = -1\n    gradiented_params = -1\n    batch_size = None\n    for line in lines:  #, leave=False):\n        if \"{'BATCH_SIZE'\" in line and batch_size is None:\n            batch_size = int(line.split(\"'BATCH_SIZE': \")[-1].split(\",\")[0])\n            \n        if \"Total Parameters: \" in line:\n            total_params = int(line.split(\"Total Parameters: \")[-1].split(\"\\t\")[0])\n            gradiented_params = int(line.split(\"Gradient Parameters: \")[-1].split(\"\\n\")[0])\n\n        if \"Rank of current process:\" in line:\n            num_jobs += 1\n        if num_jobs == 2:\n            break\n        if \"average train loss:\" in line:\n            loss = float(line.split(\"average train loss: \")[-1])\n            train_loss.append(loss)\n        if \" Classification results with \" in line:\n            update_eval(line, eval_dict, data_name)\n\n    meta_dict = {\n        \"data\": data_name,\n        \"feature\": feat_type,\n        \"lr\": float(lr) * 256 / int(batch_size),\n        \"wd\": wd,\n        \"total_params\": total_params,\n        \"tuned_params\": gradiented_params,\n        \"tuned / total (%)\": round(gradiented_params / total_params * 100, 4),\n        \"batch_size\": batch_size,\n    }\n    v_top1, t_top1 = None, None\n    return train_loss, eval_dict, meta_dict, (v_top1, t_top1)", "\n\ndef get_time(file):\n    with open(file) as f:\n        lines = f.readlines()\n    start_time = lines[0].split(\"[\")[1].split(\"]\")[0]\n    start_time = datetime.datetime.strptime(start_time, '%m/%d %H:%M:%S')\n\n    end_time = lines[-1].split(\"[\")[1].split(\"]\")[0]\n    end_time = datetime.datetime.strptime(end_time, '%m/%d %H:%M:%S')\n\n    per_iter = None\n    with open(file) as f:\n        lines = f.readlines()\n\n    per_batch = []\n    per_batch_train = []\n    for line in lines[::-1]:\n#         print(line)\"Test 6/6. loss: 6.097, \"\n        if \". loss:\" in line and \"Test\" in line:\n            per_iter = line.split(\" s / batch\")[0].split(\",\")[-1]\n            per_batch.append(float(per_iter))\n        if \". train loss:\" in line:\n            per_iter = line.split(\" s / batch\")[0].split(\",\")[-1]\n            per_batch_train.append(float(per_iter))\n            \n    return datetime.timedelta(seconds=(end_time-start_time).total_seconds()), np.mean(per_batch), np.mean(per_batch_train)", "\n\ndef get_df(files, model_type, root, is_best=True, is_last=True, max_epoch=300):\n    pd_dict = defaultdict(list)\n    for job_path in tqdm(files, desc=model_type):\n        train_loss, eval_results, meta_dict, (v_top1, t_top1) = get_training_data(job_path, model_type, root)\n        batch_size = meta_dict[\"batch_size\"]\n        \n        if len(eval_results) == 0:\n            print(f\"job {job_path} not ready\")\n            continue\n        if len(eval_results[\"val_top1\"]) == 0:\n            print(f\"job {job_path} not ready\")\n            continue\n\n        if \"val_top1\" not in eval_results or \"test_top1\" not in eval_results:\n            print(f\"inbalanced: {job_path}\")\n            continue\n                \n        for k, v in meta_dict.items():\n            pd_dict[k].append(v)\n        \n        metric_b = \"val_top1\"\n        best_epoch = np.argmax(eval_results[metric_b])\n\n        if is_best:\n            for name, val in eval_results.items():\n                if \"top5\" in name:\n                    continue\n                if len(val) == 0:\n                    continue\n                if not isinstance(val[0], list):\n                    try:\n                        pd_dict[\"b-\" + name].append(val[best_epoch])\n                    except:\n                        pd_dict[\"b-\" + name].append(-1)\n                        # ongoing training process\n                        print(name, best_epoch, val)\n        # last epoch\n        if is_last:\n            if v_top1 is not None:\n                pd_dict[\"l-val_top1\"].append(v_top1)\n                pd_dict[\"l-test_top1\"].append(t_top1)\n                val = eval_results[\"val_top1\"]\n            else:\n                for name, val in eval_results.items():\n                    if \"top5\" in name:\n                        continue\n                    if len(val) == 0:\n                        continue\n                    pd_dict[\"l-\" + name].append(val[-1])\n\n        pd_dict[\"best_epoch\"].append(f\"{best_epoch + 1} | {len(val)}\")\n\n        pd_dict[\"file\"].append(job_path)\n        total_time, _, _ = get_time(job_path)\n        pd_dict[\"total_time\"].append(total_time)\n\n    result_df = None\n    if len(pd_dict) > 0:\n        result_df = pd.DataFrame(pd_dict)\n        result_df = result_df.sort_values(['data', \"feature\", \"lr\", \"wd\"])\n    return result_df", "\n\ndef delete_ckpts(f):\n    # delete saved ckpts for re\n    f_dir, _ = os.path.split(f)\n    for f_delete in glob.glob(f_dir + \"/*.pth\"):\n        os.remove(f_delete)\n        print(f\"removed {f_delete}\")\n\n\ndef average_df(df, metric_names=[\"l-val_top1\", \"l-val_base_top1\"], take_average=True):\n    # for each data and features and train type, display the averaged results\n    data_names = set(list(df[\"data\"]))\n    f_names = set(list(df[\"feature\"]))\n    t_names = set(list(df[\"type\"]))\n    hp_names = [\n        c for c in df.columns if c not in [\"data\", \"feature\", \"type\", \"file\", \"best_epoch\"] + metric_names]\n    data_dict = defaultdict(list)\n    for d_name in data_names:\n        for f_name in f_names:\n            for t_name in t_names:\n\n                result = df[df.data == d_name]\n                result = result[result.feature == f_name]\n                result = result[result.type == t_name]\n                # take average here\n                if len(result) == 0:\n                    continue\n                data_dict[\"data\"].append(d_name)\n                data_dict[\"feature\"].append(f_name)\n                data_dict[\"type\"].append(t_name)\n                data_dict[\"total_runs\"].append(len(result))\n        \n                for m in metric_names:\n                    if take_average:\n                        data_dict[m].append(\"{:.2f}\".format(\n                            np.mean([r for i, r in enumerate(result[m])]),\n                        ))\n                        data_dict[f\"{m}-std\"].append(\"{:.2f}\".format(\n                            np.std([r for i, r in enumerate(result[m])])\n                        ))\n                    else:\n                        data_dict[m].append(\"{:.2f}\".format(\n                            np.median([r for i, r in enumerate(result[m])]),\n                        ))\n                for h_name in hp_names:\n                    data_dict[h_name].append(result[h_name].iloc[0])\n\n    df = pd.DataFrame(data_dict)\n    df = df.sort_values([\"data\", \"feature\", \"type\"])\n    return df", "\n\ndef average_df(df, metric_names=[\"l-val_top1\", \"l-val_base_top1\"], take_average=True):\n    # for each data and features and train type, display the averaged results\n    data_names = set(list(df[\"data\"]))\n    f_names = set(list(df[\"feature\"]))\n    t_names = set(list(df[\"type\"]))\n    hp_names = [\n        c for c in df.columns if c not in [\"data\", \"feature\", \"type\", \"file\", \"best_epoch\"] + metric_names]\n    data_dict = defaultdict(list)\n    for d_name in data_names:\n        for f_name in f_names:\n            for t_name in t_names:\n\n                result = df[df.data == d_name]\n                result = result[result.feature == f_name]\n                result = result[result.type == t_name]\n                # take average here\n                if len(result) == 0:\n                    continue\n                data_dict[\"data\"].append(d_name)\n                data_dict[\"feature\"].append(f_name)\n                data_dict[\"type\"].append(t_name)\n                data_dict[\"total_runs\"].append(len(result))\n        \n                for m in metric_names:\n                    if take_average:\n                        data_dict[m].append(\"{:.2f}\".format(\n                            np.mean([r for i, r in enumerate(result[m])]),\n                        ))\n                        data_dict[f\"{m}-std\"].append(\"{:.2f}\".format(\n                            np.std([r for i, r in enumerate(result[m])])\n                        ))\n                    else:\n                        data_dict[m].append(\"{:.2f}\".format(\n                            np.median([r for i, r in enumerate(result[m])]),\n                        ))\n                for h_name in hp_names:\n                    data_dict[h_name].append(result[h_name].iloc[0])\n\n    df = pd.DataFrame(data_dict)\n    df = df.sort_values([\"data\", \"feature\", \"type\"])\n    return df", "\n\ndef filter_df(df, sorted_cols, max_num):\n    # for each data and features, display only top max_num runs\n    data_names = set(list(df[\"data\"]))\n    f_names = set(list(df[\"feature\"]))\n    t_names = set(list(df[\"type\"]))\n    df_list = []\n    for d_name in data_names:\n        for f_name in f_names:\n            for t_name in t_names:\n                result = df[df.data == d_name]\n                result = result[result.feature == f_name]\n                result = result[result.type == t_name]\n                if len(result) == 0:\n                    continue\n                cols = [c for c in sorted_cols if c in result.columns]\n                result = result.sort_values(cols, ignore_index=True)\n\n                _num = min([max_num, len(result)])\n    #             print(result.iloc[-_num:])\n                df_list.append(result.iloc[-_num:])\n    return pd.concat(df_list)", "\n\ndef display_results(df, sorted_cols=[\"data\", \"feature\", \"type\", \"l-val_top1\"], max_num=1):\n    cols = [c for c in df.columns if c not in []]\n    df = df[cols]\n    if max_num is not None:\n        df = filter_df(df, sorted_cols[3:], max_num)\n    return df.sort_values(sorted_cols).reset_index(drop=True)\n", ""]}
{"filename": "src/utils/logging.py", "chunked_list": ["#!/usr/bin/env python3\n\n\"\"\"Logging.\"\"\"\n\nimport builtins\nimport decimal\nimport functools\nimport logging\nimport simplejson\nimport sys", "import simplejson\nimport sys\nimport os\nfrom termcolor import colored\n\nfrom .distributed import is_master_process\nfrom .file_io import PathManager\n\n# Show filename and line number in logs\n_FORMAT = \"[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s\"", "# Show filename and line number in logs\n_FORMAT = \"[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s\"\n\n\ndef _suppress_print():\n    \"\"\"Suppresses printing from the current process.\"\"\"\n\n    def print_pass(*objects, sep=\" \", end=\"\\n\", file=sys.stdout, flush=False):\n        pass\n\n    builtins.print = print_pass", "\n\n# cache the opened file object, so that different calls to `setup_logger`\n# with the same file name can safely write to the same file.\n@functools.lru_cache(maxsize=None)\ndef _cached_log_stream(filename):\n    return PathManager.open(filename, \"a\")\n\n\n@functools.lru_cache()  # so that calling setup_logger multiple times won't add many handlers  # noqa\ndef setup_logging(\n    num_gpu, num_shards, output=\"\", name=\"visual_prompt\", color=True):\n    \"\"\"Sets up the logging.\"\"\"\n    # Enable logging only for the master process\n    if is_master_process(num_gpu):\n        # Clear the root logger to prevent any existing logging config\n        # (e.g. set by another module) from messing with our setup\n        logging.root.handlers = []\n        # Configure logging\n        logging.basicConfig(\n            level=logging.INFO, format=_FORMAT, stream=sys.stdout\n        )\n    else:\n        _suppress_print()\n\n    if name is None:\n        name = __name__\n    logger = logging.getLogger(name)\n    # remove any lingering handler\n    logger.handlers.clear()\n\n    logger.setLevel(logging.INFO)\n    logger.propagate = False\n\n    plain_formatter = logging.Formatter(\n        \"[%(asctime)s][%(levelname)s] %(name)s: %(lineno)4d: %(message)s\",\n        datefmt=\"%m/%d %H:%M:%S\",\n    )\n    if color:\n        formatter = _ColorfulFormatter(\n            colored(\"[%(asctime)s %(name)s]: \", \"green\") + \"%(message)s\",\n            datefmt=\"%m/%d %H:%M:%S\",\n            root_name=name,\n            abbrev_name=str(name),\n        )\n    else:\n        formatter = plain_formatter\n\n    if is_master_process(num_gpu):\n        ch = logging.StreamHandler(stream=sys.stdout)\n        ch.setLevel(logging.DEBUG)\n        ch.setFormatter(formatter)\n        logger.addHandler(ch)\n\n    if is_master_process(num_gpu * num_shards):\n        if len(output) > 0:\n            if output.endswith(\".txt\") or output.endswith(\".log\"):\n                filename = output\n            else:\n                filename = os.path.join(output, \"logs.txt\")\n\n            PathManager.mkdirs(os.path.dirname(filename))\n\n            fh = logging.StreamHandler(_cached_log_stream(filename))\n            fh.setLevel(logging.DEBUG)\n            fh.setFormatter(plain_formatter)\n            logger.addHandler(fh)\n    return logger", "\n@functools.lru_cache()  # so that calling setup_logger multiple times won't add many handlers  # noqa\ndef setup_logging(\n    num_gpu, num_shards, output=\"\", name=\"visual_prompt\", color=True):\n    \"\"\"Sets up the logging.\"\"\"\n    # Enable logging only for the master process\n    if is_master_process(num_gpu):\n        # Clear the root logger to prevent any existing logging config\n        # (e.g. set by another module) from messing with our setup\n        logging.root.handlers = []\n        # Configure logging\n        logging.basicConfig(\n            level=logging.INFO, format=_FORMAT, stream=sys.stdout\n        )\n    else:\n        _suppress_print()\n\n    if name is None:\n        name = __name__\n    logger = logging.getLogger(name)\n    # remove any lingering handler\n    logger.handlers.clear()\n\n    logger.setLevel(logging.INFO)\n    logger.propagate = False\n\n    plain_formatter = logging.Formatter(\n        \"[%(asctime)s][%(levelname)s] %(name)s: %(lineno)4d: %(message)s\",\n        datefmt=\"%m/%d %H:%M:%S\",\n    )\n    if color:\n        formatter = _ColorfulFormatter(\n            colored(\"[%(asctime)s %(name)s]: \", \"green\") + \"%(message)s\",\n            datefmt=\"%m/%d %H:%M:%S\",\n            root_name=name,\n            abbrev_name=str(name),\n        )\n    else:\n        formatter = plain_formatter\n\n    if is_master_process(num_gpu):\n        ch = logging.StreamHandler(stream=sys.stdout)\n        ch.setLevel(logging.DEBUG)\n        ch.setFormatter(formatter)\n        logger.addHandler(ch)\n\n    if is_master_process(num_gpu * num_shards):\n        if len(output) > 0:\n            if output.endswith(\".txt\") or output.endswith(\".log\"):\n                filename = output\n            else:\n                filename = os.path.join(output, \"logs.txt\")\n\n            PathManager.mkdirs(os.path.dirname(filename))\n\n            fh = logging.StreamHandler(_cached_log_stream(filename))\n            fh.setLevel(logging.DEBUG)\n            fh.setFormatter(plain_formatter)\n            logger.addHandler(fh)\n    return logger", "\n\ndef setup_single_logging(name, output=\"\"):\n    \"\"\"Sets up the logging.\"\"\"\n    # Enable logging only for the master process\n    # Clear the root logger to prevent any existing logging config\n    # (e.g. set by another module) from messing with our setup\n    logging.root.handlers = []\n    # Configure logging\n    logging.basicConfig(\n        level=logging.INFO, format=_FORMAT, stream=sys.stdout\n    )\n\n    if len(name) == 0:\n        name = __name__\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.INFO)\n    logger.propagate = False\n\n    plain_formatter = logging.Formatter(\n        \"[%(asctime)s][%(levelname)s] %(name)s: %(lineno)4d: %(message)s\",\n        datefmt=\"%m/%d %H:%M:%S\",\n    )\n    formatter = _ColorfulFormatter(\n        colored(\"[%(asctime)s %(name)s]: \", \"green\") + \"%(message)s\",\n        datefmt=\"%m/%d %H:%M:%S\",\n        root_name=name,\n        abbrev_name=str(name),\n    )\n\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    if len(output) > 0:\n        if output.endswith(\".txt\") or output.endswith(\".log\"):\n            filename = output\n        else:\n            filename = os.path.join(output, \"logs.txt\")\n\n        PathManager.mkdirs(os.path.dirname(filename))\n\n        fh = logging.StreamHandler(_cached_log_stream(filename))\n        fh.setLevel(logging.DEBUG)\n        fh.setFormatter(plain_formatter)\n        logger.addHandler(fh)\n\n    return logger", "\n\ndef get_logger(name):\n    \"\"\"Retrieves the logger.\"\"\"\n    return logging.getLogger(name)\n\n\ndef log_json_stats(stats, sort_keys=True):\n    \"\"\"Logs json stats.\"\"\"\n    # It seems that in Python >= 3.6 json.encoder.FLOAT_REPR has no effect\n    # Use decimal+string as a workaround for having fixed length values in logs\n    logger = get_logger(__name__)\n    stats = {\n        k: decimal.Decimal(\"{:.6f}\".format(v)) if isinstance(v, float) else v\n        for k, v in stats.items()\n    }\n    json_stats = simplejson.dumps(stats, sort_keys=True, use_decimal=True)\n    if stats[\"_type\"] == \"test_epoch\" or stats[\"_type\"] == \"train_epoch\":\n        logger.info(\"json_stats: {:s}\".format(json_stats))\n    else:\n        logger.info(\"{:s}\".format(json_stats))", "\n\nclass _ColorfulFormatter(logging.Formatter):\n    # from detectron2\n    def __init__(self, *args, **kwargs):\n        self._root_name = kwargs.pop(\"root_name\") + \".\"\n        self._abbrev_name = kwargs.pop(\"abbrev_name\", \"\")\n        if len(self._abbrev_name):\n            self._abbrev_name = self._abbrev_name + \".\"\n        super(_ColorfulFormatter, self).__init__(*args, **kwargs)\n\n    def formatMessage(self, record: logging.LogRecord) -> str:\n        record.name = record.name.replace(self._root_name, self._abbrev_name)\n        log = super(_ColorfulFormatter, self).formatMessage(record)\n        if record.levelno == logging.WARNING:\n            prefix = colored(\"WARNING\", \"red\", attrs=[\"blink\"])\n        elif record.levelno == logging.ERROR or record.levelno == logging.CRITICAL:\n            prefix = colored(\"ERROR\", \"red\", attrs=[\"blink\", \"underline\"])\n        else:\n            return log\n        return prefix + \" \" + log", ""]}
{"filename": "src/data/loader.py", "chunked_list": ["#!/usr/bin/env python3\n\n\"\"\"Data loader.\"\"\"\nimport torch\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data.sampler import RandomSampler\n\nfrom ..utils import logging\nfrom .datasets.json_dataset import (\n    CUB200Dataset, CarsDataset, DogsDataset, FlowersDataset, NabirdsDataset", "from .datasets.json_dataset import (\n    CUB200Dataset, CarsDataset, DogsDataset, FlowersDataset, NabirdsDataset\n)\n\nlogger = logging.get_logger(\"visual_prompt\")\n_DATASET_CATALOG = {\n    \"CUB\": CUB200Dataset,\n    'OxfordFlowers': FlowersDataset,\n    'StanfordCars': CarsDataset,\n    'StanfordDogs': DogsDataset,", "    'StanfordCars': CarsDataset,\n    'StanfordDogs': DogsDataset,\n    \"nabirds\": NabirdsDataset,\n}\n\n\ndef _construct_loader(cfg, split, batch_size, shuffle, drop_last):\n    \"\"\"Constructs the data loader for the given dataset.\"\"\"\n    dataset_name = cfg.DATA.NAME\n\n    # Construct the dataset\n    if dataset_name.startswith(\"vtab-\"):\n        # import the tensorflow here only if needed\n        from .datasets.tf_dataset import TFDataset\n        dataset = TFDataset(cfg, split)\n    else:\n        assert (\n            dataset_name in _DATASET_CATALOG.keys()\n        ), \"Dataset '{}' not supported\".format(dataset_name)\n        dataset = _DATASET_CATALOG[dataset_name](cfg, split)\n\n    # Create a sampler for multi-process training\n    sampler = DistributedSampler(dataset) if cfg.NUM_GPUS > 1 else None\n    # Create a loader\n    loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=(False if sampler else shuffle),\n        sampler=sampler,\n        num_workers=cfg.DATA.NUM_WORKERS,\n        pin_memory=cfg.DATA.PIN_MEMORY,\n        drop_last=drop_last,\n    )\n    return loader", "\n\ndef construct_train_loader(cfg):\n    \"\"\"Train loader wrapper.\"\"\"\n    if cfg.NUM_GPUS > 1:\n        drop_last = True\n    else:\n        drop_last = False\n    return _construct_loader(\n        cfg=cfg,\n        split=\"train\",\n        batch_size=int(cfg.DATA.BATCH_SIZE / cfg.NUM_GPUS),\n        shuffle=True,\n        drop_last=drop_last,\n    )", "\n\ndef construct_trainval_loader(cfg):\n    \"\"\"Train loader wrapper.\"\"\"\n    if cfg.NUM_GPUS > 1:\n        drop_last = True\n    else:\n        drop_last = False\n    return _construct_loader(\n        cfg=cfg,\n        split=\"trainval\",\n        batch_size=int(cfg.DATA.BATCH_SIZE / cfg.NUM_GPUS),\n        shuffle=True,\n        drop_last=drop_last,\n    )", "\n\ndef construct_test_loader(cfg):\n    \"\"\"Test loader wrapper.\"\"\"\n    return _construct_loader(\n        cfg=cfg,\n        split=\"test\",\n        batch_size=int(cfg.DATA.BATCH_SIZE / cfg.NUM_GPUS),\n        shuffle=False,\n        drop_last=False,\n    )", "\n\ndef construct_val_loader(cfg, batch_size=None):\n    if batch_size is None:\n        bs = int(cfg.DATA.BATCH_SIZE / cfg.NUM_GPUS)\n    else:\n        bs = batch_size\n    \"\"\"Validation loader wrapper.\"\"\"\n    return _construct_loader(\n        cfg=cfg,\n        split=\"val\",\n        batch_size=bs,\n        shuffle=False,\n        drop_last=False,\n    )", "\n\ndef shuffle(loader, cur_epoch):\n    \"\"\"\"Shuffles the data.\"\"\"\n    assert isinstance(\n        loader.sampler, (RandomSampler, DistributedSampler)\n    ), \"Sampler type '{}' not supported\".format(type(loader.sampler))\n    # RandomSampler handles shuffling automatically\n    if isinstance(loader.sampler, DistributedSampler):\n        # DistributedSampler shuffles data based on epoch\n        loader.sampler.set_epoch(cur_epoch)", ""]}
{"filename": "src/data/transforms.py", "chunked_list": ["#!/usr/bin/env python3\n\n\"\"\"Image transformations.\"\"\"\nimport torchvision as tv\n\n\ndef get_transforms(split, size):\n    normalize = tv.transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n    )\n    if size == 448:\n        resize_dim = 512\n        crop_dim = 448\n    elif size == 224:\n        resize_dim = 256\n        crop_dim = 224\n    elif size == 384:\n        resize_dim = 438\n        crop_dim = 384\n    if split == \"train\":\n        transform = tv.transforms.Compose(\n            [\n                tv.transforms.Resize(resize_dim),\n                tv.transforms.RandomCrop(crop_dim),\n                tv.transforms.RandomHorizontalFlip(0.5),\n                # tv.transforms.RandomResizedCrop(224, scale=(0.2, 1.0)),\n                # tv.transforms.RandomHorizontalFlip(),\n                tv.transforms.ToTensor(),\n                normalize,\n            ]\n        )\n    else:\n        transform = tv.transforms.Compose(\n            [\n                tv.transforms.Resize(resize_dim),\n                tv.transforms.CenterCrop(crop_dim),\n                tv.transforms.ToTensor(),\n                normalize,\n            ]\n        )\n    return transform", ""]}
{"filename": "src/data/vtab_datasets/dmlab.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements Dmlab data class.\"\"\"\n", "\"\"\"Implements Dmlab data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow_datasets as tfds\n\nfrom . import base as base\nfrom .registry import Registry", "from . import base as base\nfrom .registry import Registry\n\n\n@Registry.register(\"data.dmlab\", \"class\")\nclass DmlabData(base.ImageTfdsData):\n  \"\"\"Dmlab dataset.\n\n      The Dmlab dataset contains frames observed by the agent acting in the\n      DMLab environment, which are annotated by the distance between\n      the agent and various objects present in the environment. The goal is to\n      is to evaluate the ability of a visual model to reason about distances\n      from the visual input in 3D environments. The Dmlab dataset consists of\n      360x480 color images in 6 classes. The classes are\n      {close, far, very far} x {positive reward, negative reward}\n      respectively.\n  \"\"\"\n\n  def __init__(self, data_dir=None):\n\n    dataset_builder = tfds.builder(\"dmlab:2.0.1\", data_dir=data_dir)\n\n    tfds_splits = {\n        \"train\": \"train\",\n        \"val\": \"validation\",\n        \"trainval\": \"train+validation\",\n        \"test\": \"test\",\n        \"train800\": \"train[:800]\",\n        \"val200\": \"validation[:200]\",\n        \"train800val200\": \"train[:800]+validation[:200]\",\n    }\n\n    # Example counts are retrieved from the tensorflow dataset info.\n    train_count = dataset_builder.info.splits[\"train\"].num_examples\n    val_count = dataset_builder.info.splits[\"validation\"].num_examples\n    test_count = dataset_builder.info.splits[\"test\"].num_examples\n\n    # Creates a dict with example counts for each split.\n    num_samples_splits = {\n        \"train\": train_count,\n        \"val\": val_count,\n        \"trainval\": train_count + val_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    super(DmlabData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        base_preprocess_fn=base.make_get_and_cast_tensors_fn({\n            \"image\": (\"image\", None),\n            \"label\": (\"label\", None),\n        }),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes,\n        image_key=\"image\")", ""]}
{"filename": "src/data/vtab_datasets/clevr.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements CLEVR data class.\"\"\"\n", "\"\"\"Implements CLEVR data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nimport tensorflow_datasets as tfds\n", "import tensorflow_datasets as tfds\n\nfrom . import base as base\nfrom .registry import Registry\n\nTRAIN_SPLIT_PERCENT = 90\n\n\ndef _count_preprocess_fn(x):\n  return {\"image\": x[\"image\"],\n          \"label\": tf.size(x[\"objects\"][\"size\"]) - 3}", "def _count_preprocess_fn(x):\n  return {\"image\": x[\"image\"],\n          \"label\": tf.size(x[\"objects\"][\"size\"]) - 3}\n\n\ndef _count_cylinders_preprocess_fn(x):\n  # Class distribution:\n\n  num_cylinders = tf.reduce_sum(\n      tf.cast(tf.equal(x[\"objects\"][\"shape\"], 2), tf.int32))\n  return {\"image\": x[\"image\"], \"label\": num_cylinders}", "\n\ndef _closest_object_preprocess_fn(x):\n  dist = tf.reduce_min(x[\"objects\"][\"pixel_coords\"][:, 2])\n  # These thresholds are uniformly spaced and result in more or less balanced\n  # distribution of classes, see the resulting histogram:\n\n  thrs = np.array([0.0, 8.0, 8.5, 9.0, 9.5, 10.0, 100.0])\n  label = tf.reduce_max(tf.where((thrs - dist) < 0))\n  return {\"image\": x[\"image\"],\n          \"label\": label}", "\n\n_TASK_DICT = {\n    \"count_all\": {\n        \"preprocess_fn\": _count_preprocess_fn,\n        \"num_classes\": 8\n    },\n    \"count_cylinders\": {\n        \"preprocess_fn\": _count_cylinders_preprocess_fn,\n        \"num_classes\": 11", "        \"preprocess_fn\": _count_cylinders_preprocess_fn,\n        \"num_classes\": 11\n    },\n    \"closest_object_distance\": {\n        \"preprocess_fn\": _closest_object_preprocess_fn,\n        \"num_classes\": 6\n    },\n}\n\n", "\n\n@Registry.register(\"data.clevr\", \"class\")\nclass CLEVRData(base.ImageTfdsData):\n  \"\"\"Provides CLEVR dataset.\n\n  Currently, two tasks are supported:\n    1. Predict number of objects.\n    2. Predict distnace to the closest object.\n  \"\"\"\n\n  def __init__(self, task, data_dir=None):\n\n    if task not in _TASK_DICT:\n      raise ValueError(\"Unknown task: %s\" % task)\n\n    dataset_builder = tfds.builder(\"clevr:3.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    # Creates a dict with example counts for each split.\n    trainval_count = dataset_builder.info.splits[tfds.Split.TRAIN].num_examples\n    test_count = dataset_builder.info.splits[tfds.Split.TEST].num_examples\n    num_samples_splits = {\n        \"train\": (TRAIN_SPLIT_PERCENT * trainval_count) // 100,\n        \"val\": trainval_count - (TRAIN_SPLIT_PERCENT * trainval_count) // 100,\n        \"trainval\": trainval_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"train\": \"train[:{}]\".format(num_samples_splits[\"train\"]),\n        \"val\": \"train[{}:]\".format(num_samples_splits[\"train\"]),\n        \"trainval\": \"train\",\n        \"test\": \"validation\",\n        \"train800\": \"train[:800]\",\n        \"val200\": \"train[{}:{}]\".format(\n            num_samples_splits[\"train\"], num_samples_splits[\"train\"]+200),\n        \"train800val200\": \"train[:800]+train[{}:{}]\".format(\n            num_samples_splits[\"train\"], num_samples_splits[\"train\"]+200),\n    }\n\n    task = _TASK_DICT[task]\n    base_preprocess_fn = task[\"preprocess_fn\"]\n\n    super(CLEVRData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Export only image and label tensors with their original types.\n        base_preprocess_fn=base_preprocess_fn,\n        num_classes=task[\"num_classes\"])", ""]}
{"filename": "src/data/vtab_datasets/registry.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Global Registry for the task adaptation framework.\n\"\"\"", "\"\"\"Global Registry for the task adaptation framework.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport ast\nimport functools\n", "import functools\n\n\ndef partialclass(cls, *base_args, **base_kwargs):\n  \"\"\"Builds a subclass with partial application of the given args and keywords.\n\n  Equivalent to functools.partial performance, base_args are preprended to the\n  positional arguments given during object initialization and base_kwargs are\n  updated with the kwargs given later.\n\n  Args:\n    cls: The base class.\n    *base_args: Positional arguments to be applied to the subclass.\n    **base_kwargs: Keyword arguments to be applied to the subclass.\n\n  Returns:\n    A subclass of the input class.\n  \"\"\"\n\n  class _NewClass(cls):\n\n    def __init__(self, *args, **kwargs):\n      bound_args = base_args + args\n      bound_kwargs = base_kwargs.copy()\n      bound_kwargs.update(kwargs)\n      super(_NewClass, self).__init__(*bound_args, **bound_kwargs)\n\n  return _NewClass", "\n\ndef parse_name(string_to_parse):\n  \"\"\"Parses input to the registry's lookup function.\n\n  Args:\n    string_to_parse: can be either an arbitrary name or function call\n      (optionally with positional and keyword arguments).\n      e.g. \"multiclass\", \"resnet50_v2(filters_factor=8)\".\n\n  Returns:\n    A tuple of input name and a dctinary with arguments. Examples:\n      \"multiclass\" -> (\"multiclass\", (), {})\n      \"resnet50_v2(9, filters_factor=4)\" ->\n          (\"resnet50_v2\", (9,), {\"filters_factor\": 4})\n  \"\"\"\n  expr = ast.parse(string_to_parse, mode=\"eval\").body  # pytype: disable=attribute-error\n  if not isinstance(expr, (ast.Attribute, ast.Call, ast.Name)):\n    raise ValueError(\n        \"The given string should be a name or a call, but a {} was parsed from \"\n        \"the string {!r}\".format(type(expr), string_to_parse))\n\n  # Notes:\n  # name=\"some_name\" -> type(expr) = ast.Name\n  # name=\"module.some_name\" -> type(expr) = ast.Attribute\n  # name=\"some_name()\" -> type(expr) = ast.Call\n  # name=\"module.some_name()\" -> type(expr) = ast.Call\n\n  if isinstance(expr, ast.Name):\n    return string_to_parse, {}\n  elif isinstance(expr, ast.Attribute):\n    return string_to_parse, {}\n\n  def _get_func_name(expr):\n    if isinstance(expr, ast.Attribute):\n      return _get_func_name(expr.value) + \".\" + expr.attr\n    elif isinstance(expr, ast.Name):\n      return expr.id\n    else:\n      raise ValueError(\n          \"Type {!r} is not supported in a function name, the string to parse \"\n          \"was {!r}\".format(type(expr), string_to_parse))\n\n  def _get_func_args_and_kwargs(call):\n    args = tuple([ast.literal_eval(arg) for arg in call.args])\n    kwargs = {\n        kwarg.arg: ast.literal_eval(kwarg.value) for kwarg in call.keywords\n    }\n    return args, kwargs\n\n  func_name = _get_func_name(expr.func)\n  func_args, func_kwargs = _get_func_args_and_kwargs(expr)\n  if func_args:\n    raise ValueError(\"Positional arguments are not supported here, but these \"\n                     \"were found: {!r}\".format(func_args))\n\n  return func_name, func_kwargs", "\n\nclass Registry(object):\n  \"\"\"Implements global Registry.\"\"\"\n\n  _GLOBAL_REGISTRY = {}\n\n  @staticmethod\n  def global_registry():\n    return Registry._GLOBAL_REGISTRY\n\n  @staticmethod\n  def register(name, item_type):\n    \"\"\"Creates a function that registers its input.\"\"\"\n    if item_type not in [\"function\", \"class\"]:\n      raise ValueError(\"Unknown item type: %s\" % item_type)\n\n    def _register(item):\n      if name in Registry.global_registry():\n        raise KeyError(\n            \"The name {!r} was already registered in with type {!r}\".format(\n                name, item_type))\n\n      Registry.global_registry()[name] = (item, item_type)\n      return item\n\n    return _register\n\n  @staticmethod\n  def lookup(lookup_string, kwargs_extra=None):\n    \"\"\"Lookup a name in the registry.\"\"\"\n\n    name, kwargs = parse_name(lookup_string)\n    if kwargs_extra:\n      kwargs.update(kwargs_extra)\n    item, item_type = Registry.global_registry()[name]\n    if item_type == \"function\":\n      return functools.partial(item, **kwargs)\n    elif item_type == \"class\":\n      return partialclass(item, **kwargs)", ""]}
{"filename": "src/data/vtab_datasets/svhn.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements Svhn data class.\"\"\"\n", "\"\"\"Implements Svhn data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow_datasets as tfds\n\nfrom . import base as base\nfrom .registry import Registry", "from . import base as base\nfrom .registry import Registry\n# This constant specifies the percentage of data that is used to create custom\n# train/val splits. Specifically, TRAIN_SPLIT_PERCENT% of the official training\n# split is used as a new training split and the rest is used for validation.\nTRAIN_SPLIT_PERCENT = 90\n\n\n@Registry.register(\"data.svhn\", \"class\")\nclass SvhnData(base.ImageTfdsData):\n  \"\"\"Provides SVHN data.\n\n  The Street View House Numbers (SVHN) Dataset is an image digit recognition\n  dataset of over 600,000 color digit images coming from real world data.\n  Split size:\n    - Training set: 73,257 images\n    - Testing set: 26,032 images\n    - Extra training set: 531,131 images\n  Following the common setup on SVHN, we only use the official training and\n  testing data. Images are cropped to 32x32.\n\n  URL: http://ufldl.stanford.edu/housenumbers/\n  \"\"\"\n\n  def __init__(self, data_dir=None):\n    dataset_builder = tfds.builder(\"svhn_cropped:3.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    # Example counts are retrieved from the tensorflow dataset info.\n    trainval_count = dataset_builder.info.splits[tfds.Split.TRAIN].num_examples\n    test_count = dataset_builder.info.splits[tfds.Split.TEST].num_examples\n\n    # Creates a dict with example counts for each split.\n    num_samples_splits = {\n        # Calculates the train/val split example count based on percent.\n        \"train\": TRAIN_SPLIT_PERCENT * trainval_count // 100,\n        \"val\": trainval_count - TRAIN_SPLIT_PERCENT * trainval_count // 100,\n        \"trainval\": trainval_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    # Defines dataset specific train/val/trainval/test splits.\n    # The validation set is split out of the original training set, and the\n    # remaining examples are used as the \"train\" split. The \"trainval\" split\n    # corresponds to the original training set.\n    tfds_splits = {\n        \"train\":\n            \"train[:{}]\".format(num_samples_splits[\"train\"]),\n        \"val\":\n            \"train[{}:]\".format(num_samples_splits[\"train\"]),\n        \"trainval\":\n            \"train\",\n        \"test\":\n            \"test\",\n        \"train800\":\n            \"train[:800]\",\n        \"val200\":\n            \"train[{}:{}]\".format(num_samples_splits[\"train\"],\n                                  num_samples_splits[\"train\"] + 200),\n        \"train800val200\":\n            \"train[:800]+train[{}:{}]\".format(\n                num_samples_splits[\"train\"], num_samples_splits[\"train\"] + 200),\n    }\n\n    super(SvhnData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Rename tensors but keep their original types.\n        base_preprocess_fn=base.make_get_and_cast_tensors_fn({\n            \"image\": (\"image\", None),\n            \"label\": (\"label\", None),\n        }),\n        num_classes=dataset_builder.info.features[\"label\"]\n        .num_classes)", "@Registry.register(\"data.svhn\", \"class\")\nclass SvhnData(base.ImageTfdsData):\n  \"\"\"Provides SVHN data.\n\n  The Street View House Numbers (SVHN) Dataset is an image digit recognition\n  dataset of over 600,000 color digit images coming from real world data.\n  Split size:\n    - Training set: 73,257 images\n    - Testing set: 26,032 images\n    - Extra training set: 531,131 images\n  Following the common setup on SVHN, we only use the official training and\n  testing data. Images are cropped to 32x32.\n\n  URL: http://ufldl.stanford.edu/housenumbers/\n  \"\"\"\n\n  def __init__(self, data_dir=None):\n    dataset_builder = tfds.builder(\"svhn_cropped:3.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    # Example counts are retrieved from the tensorflow dataset info.\n    trainval_count = dataset_builder.info.splits[tfds.Split.TRAIN].num_examples\n    test_count = dataset_builder.info.splits[tfds.Split.TEST].num_examples\n\n    # Creates a dict with example counts for each split.\n    num_samples_splits = {\n        # Calculates the train/val split example count based on percent.\n        \"train\": TRAIN_SPLIT_PERCENT * trainval_count // 100,\n        \"val\": trainval_count - TRAIN_SPLIT_PERCENT * trainval_count // 100,\n        \"trainval\": trainval_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    # Defines dataset specific train/val/trainval/test splits.\n    # The validation set is split out of the original training set, and the\n    # remaining examples are used as the \"train\" split. The \"trainval\" split\n    # corresponds to the original training set.\n    tfds_splits = {\n        \"train\":\n            \"train[:{}]\".format(num_samples_splits[\"train\"]),\n        \"val\":\n            \"train[{}:]\".format(num_samples_splits[\"train\"]),\n        \"trainval\":\n            \"train\",\n        \"test\":\n            \"test\",\n        \"train800\":\n            \"train[:800]\",\n        \"val200\":\n            \"train[{}:{}]\".format(num_samples_splits[\"train\"],\n                                  num_samples_splits[\"train\"] + 200),\n        \"train800val200\":\n            \"train[:800]+train[{}:{}]\".format(\n                num_samples_splits[\"train\"], num_samples_splits[\"train\"] + 200),\n    }\n\n    super(SvhnData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Rename tensors but keep their original types.\n        base_preprocess_fn=base.make_get_and_cast_tensors_fn({\n            \"image\": (\"image\", None),\n            \"label\": (\"label\", None),\n        }),\n        num_classes=dataset_builder.info.features[\"label\"]\n        .num_classes)", ""]}
{"filename": "src/data/vtab_datasets/patch_camelyon.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements PatchCamelyon data class.\"\"\"\n", "\"\"\"Implements PatchCamelyon data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow_datasets as tfds\n\nfrom . import base as base\nfrom .registry import Registry", "from . import base as base\nfrom .registry import Registry\n\n\n@Registry.register(\"data.patch_camelyon\", \"class\")\nclass PatchCamelyonData(base.ImageTfdsData):\n  \"\"\"Provides PatchCamelyon data.\"\"\"\n\n  def __init__(self, data_dir=None):\n\n    dataset_builder = tfds.builder(\"patch_camelyon:2.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"test\": \"test\",\n        \"train\": \"train\",\n        \"val\": \"validation\",\n        \"trainval\": \"train+validation\",\n        \"train800\": \"train[:800]\",\n        \"val200\": \"validation[:200]\",\n        \"train800val200\": \"train[:800]+validation[:200]\",\n    }\n    # Creates a dict with example counts.\n    num_samples_splits = {\n        \"test\": dataset_builder.info.splits[\"test\"].num_examples,\n        \"train\": dataset_builder.info.splits[\"train\"].num_examples,\n        \"val\": dataset_builder.info.splits[\"validation\"].num_examples,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n    num_samples_splits[\"trainval\"] = (\n        num_samples_splits[\"train\"] + num_samples_splits[\"val\"])\n    super(PatchCamelyonData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Export only image and label tensors with their original types.\n        base_preprocess_fn=base.make_get_tensors_fn([\"image\", \"label\"]),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", ""]}
{"filename": "src/data/vtab_datasets/base.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Abstract class for reading the data using tfds.\"\"\"\n", "\"\"\"Abstract class for reading the data using tfds.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport six\nimport tensorflow.compat.v1 as tf\nimport tensorflow_datasets as tfds", "import tensorflow.compat.v1 as tf\nimport tensorflow_datasets as tfds\n\n\ndef make_get_tensors_fn(output_tensors):\n  \"\"\"Create a function that outputs a collection of tensors from the dataset.\"\"\"\n\n  def _get_fn(data):\n    \"\"\"Get tensors by name.\"\"\"\n    return {tensor_name: data[tensor_name] for tensor_name in output_tensors}\n\n  return _get_fn", "\n\ndef make_get_and_cast_tensors_fn(output_tensors):\n  \"\"\"Create a function that gets and casts a set of tensors from the dataset.\n\n  Optionally, you can also rename the tensors.\n\n  Examples:\n    # This simply gets \"image\" and \"label\" tensors without any casting.\n    # Note that this is equivalent to make_get_tensors_fn([\"image\", \"label\"]).\n    make_get_and_cast_tensors_fn({\n      \"image\": None,\n      \"label\": None,\n    })\n\n    # This gets the \"image\" tensor without any type conversion, casts the\n    # \"heatmap\" tensor to tf.float32, and renames the tensor \"class/label\" to\n    # \"label\" and casts it to tf.int64.\n    make_get_and_cast_tensors_fn({\n      \"image\": None,\n      \"heatmap\": tf.float32,\n      \"class/label\": (\"label\", tf.int64),\n    })\n\n  Args:\n    output_tensors: dictionary specifying the set of tensors to get and cast\n      from the dataset.\n\n  Returns:\n    The function performing the operation.\n  \"\"\"\n\n  def _tensors_to_cast():\n    tensors_to_cast = []  # AutoGraph does not support generators.\n    for tensor_name, tensor_dtype in output_tensors.items():\n      if isinstance(tensor_dtype, tuple) and len(tensor_dtype) == 2:\n        tensors_to_cast.append((tensor_name, tensor_dtype[0], tensor_dtype[1]))\n      elif tensor_dtype is None or isinstance(tensor_dtype, tf.dtypes.DType):\n        tensors_to_cast.append((tensor_name, tensor_name, tensor_dtype))\n      else:\n        raise ValueError('Values of the output_tensors dictionary must be '\n                         'None, tf.dtypes.DType or 2-tuples.')\n    return tensors_to_cast\n\n  def _get_and_cast_fn(data):\n    \"\"\"Get and cast tensors by name, optionally changing the name too.\"\"\"\n\n    return {\n        new_name:\n        data[name] if new_dtype is None else tf.cast(data[name], new_dtype)\n        for name, new_name, new_dtype in _tensors_to_cast()\n    }\n\n  return _get_and_cast_fn", "\n\ndef compose_preprocess_fn(*functions):\n  \"\"\"Compose two or more preprocessing functions.\n\n  Args:\n    *functions: Sequence of preprocess functions to compose.\n\n  Returns:\n    The composed function.\n  \"\"\"\n\n  def _composed_fn(x):\n    for fn in functions:\n      if fn is not None:  # Note: If one function is None, equiv. to identity.\n        x = fn(x)\n    return x\n\n  return _composed_fn", "\n\n# Note: DO NOT implement any method in this abstract class.\n@six.add_metaclass(abc.ABCMeta)\nclass ImageDataInterface(object):\n  \"\"\"Interface to the image data classes.\"\"\"\n\n  @property\n  @abc.abstractmethod\n  def default_label_key(self):\n    \"\"\"Returns the default label key of the dataset.\"\"\"\n\n  @property\n  @abc.abstractmethod\n  def label_keys(self):\n    \"\"\"Returns a tuple with the available label keys of the dataset.\"\"\"\n\n  @property\n  @abc.abstractmethod\n  def num_channels(self):\n    \"\"\"Returns the number of channels of the images in the dataset.\"\"\"\n\n  @property\n  @abc.abstractmethod\n  def splits(self):\n    \"\"\"Returns the splits defined in the dataset.\"\"\"\n\n  @abc.abstractmethod\n  def get_num_samples(self, split_name):\n    \"\"\"Returns the number of images in the given split name.\"\"\"\n\n  @abc.abstractmethod\n  def get_num_classes(self, label_key=None):\n    \"\"\"Returns the number of classes of the given label_key.\"\"\"\n\n  @abc.abstractmethod\n  def get_tf_data(self,\n                  split_name,\n                  batch_size,\n                  pairwise_mix_fn=None,\n                  preprocess_fn=None,\n                  preprocess_before_filter=None,\n                  epochs=None,\n                  drop_remainder=True,\n                  for_eval=False,\n                  shuffle_buffer_size=None,\n                  prefetch=1,\n                  train_examples=None,\n                  filtered_num_samples=None,\n                  filter_fn=None,\n                  batch_preprocess_fn=None,\n                  ignore_errors=False,\n                  shuffle_files=False):\n    \"\"\"Provides preprocessed and batched data.\n\n    Args:\n      split_name: name of a data split to provide. Can be \"train\", \"val\",\n          \"trainval\" or \"test\".\n      batch_size: batch size.\n      pairwise_mix_fn: a function for mixing each data with another random one.\n      preprocess_fn: a function for preprocessing input data. It expects a\n          dictionary with a key \"image\" associated with a 3D image tensor.\n      preprocess_before_filter: a function for preprocessing input data,\n          before filter_fn. It is only designed for light preprocessing,\n          i.e. augment with image id. For heavy preprocessing, it's more\n          efficient to do it after filter_fn.\n      epochs: number of full passes through the data. If None, the data is\n          provided indefinitely.\n      drop_remainder: if True, the last incomplete batch of data is dropped.\n          Normally, this parameter should be True, otherwise it leads to\n          the unknown batch dimension, which is not compatible with training\n          or evaluation on TPUs.\n      for_eval: get data for evaluation. Disables shuffling.\n      shuffle_buffer_size: overrides default shuffle buffer size.\n      prefetch: number of batches to prefetch.\n      train_examples: optional number of examples to take for training.\n        If greater than available number of examples, equivalent to None (all).\n        Ignored with for_eval is True.\n      filtered_num_samples: required when filter_fn is set, number of\n        samples after applying filter_fn.\n      filter_fn: filter function for generating training subset.\n      batch_preprocess_fn: optional function for preprocessing a full batch of\n        input data. Analoguous to preprocess_fn with an extra batch-dimension\n        on all tensors.\n      ignore_errors: whether to skip images that encountered an error in\n        decoding *or pre-processing*, the latter is why it is False by default.\n      shuffle_files: whether to shuffle the dataset files or not.\n\n    Returns:\n      A tf.data.Dataset object as a dictionary containing the output tensors.\n    \"\"\"", "\n\nclass ImageData(ImageDataInterface):\n  \"\"\"Abstract data provider class.\n\n  IMPORTANT: You should use ImageTfdsData below whenever is posible. We want\n  to use as many datasets in TFDS as possible to ensure reproducibility of our\n  experiments. Your data class should only inherit directly from this if you\n  are doing experiments while creating a TFDS dataset.\n  \"\"\"\n\n  @abc.abstractmethod\n  def __init__(self,\n               num_samples_splits,\n               shuffle_buffer_size,\n               num_preprocessing_threads,\n               num_classes,\n               default_label_key='label',\n               base_preprocess_fn=None,\n               filter_fn=None,\n               image_decoder=None,\n               num_channels=3):\n    \"\"\"Initializer for the base ImageData class.\n\n    Args:\n      num_samples_splits: a dictionary, that maps splits (\"train\", \"trainval\",\n          \"val\", and \"test\") to the corresponding number of samples.\n      shuffle_buffer_size: size of a buffer used for shuffling.\n      num_preprocessing_threads: the number of parallel threads for data\n          preprocessing.\n      num_classes: int/dict, number of classes in this dataset for the\n        `default_label_key` tensor, or dictionary with the number of classes in\n        each label tensor.\n      default_label_key: optional, string with the name of the tensor to use\n        as label. Default is \"label\".\n      base_preprocess_fn: optional, base preprocess function to apply in all\n        cases for this dataset.\n      filter_fn: optional, function to filter the examples to use in the\n        dataset. DEPRECATED, soon to be removed.\n      image_decoder: a function to decode image.\n      num_channels: number of channels in the dataset image.\n    \"\"\"\n    self._log_warning_if_direct_inheritance()\n    self._num_samples_splits = num_samples_splits\n    self._shuffle_buffer_size = shuffle_buffer_size\n    self._num_preprocessing_threads = num_preprocessing_threads\n    self._base_preprocess_fn = base_preprocess_fn\n    self._default_label_key = default_label_key\n    self._filter_fn = filter_fn\n    if self._filter_fn:\n      tf.logging.warning('Using deprecated filtering mechanism.')\n    self._image_decoder = image_decoder\n    self._num_channels = num_channels\n\n    if isinstance(num_classes, dict):\n      self._num_classes = num_classes\n      if default_label_key not in num_classes:\n        raise ValueError(\n            'No num_classes was specified for the default_label_key %r' %\n            default_label_key)\n    elif isinstance(num_classes, int):\n      self._num_classes = {default_label_key: num_classes}\n    else:\n      raise ValueError(\n          '\"num_classes\" must be a int or a dict, but type %r was given' %\n          type(num_classes))\n\n  @property\n  def default_label_key(self):\n    return self._default_label_key\n\n  @property\n  def label_keys(self):\n    return tuple(self._num_classes.keys())\n\n  @property\n  def num_channels(self):\n    return self._num_channels\n\n  @property\n  def splits(self):\n    return tuple(self._num_samples_splits.keys())\n\n  def get_num_samples(self, split_name):\n    return self._num_samples_splits[split_name]\n\n  def get_num_classes(self, label_key=None):\n    if label_key is None:\n      label_key = self._default_label_key\n    return self._num_classes[label_key]\n\n  def get_version(self):\n    return NotImplementedError('Version is not supported outside TFDS.')\n\n  def get_tf_data(self,\n                  split_name,\n                  batch_size,\n                  pairwise_mix_fn=None,\n                  preprocess_fn=None,\n                  preprocess_before_filter=None,\n                  epochs=None,\n                  drop_remainder=True,\n                  for_eval=False,\n                  shuffle_buffer_size=None,\n                  prefetch=1,\n                  train_examples=None,\n                  filtered_num_samples=None,\n                  filter_fn=None,\n                  batch_preprocess_fn=None,\n                  ignore_errors=False,\n                  shuffle_files=False):\n    # Obtains tf.data object.\n    # We shuffle later when not for eval, it's important to not shuffle before\n    # a subset of data is retrieved.\n    data = self._get_dataset_split(\n        split_name=split_name,\n        shuffle_files=shuffle_files)\n\n    if preprocess_before_filter is not None:\n      data = preprocess_before_filter(data)\n\n\n    if self._filter_fn and (filter_fn is None):\n      filter_fn = self._filter_fn\n\n    # Dataset filtering priority: (1) filter_fn; (2) train_examples.\n    if filter_fn and train_examples:\n      raise ValueError('You must not set both filter_fn and train_examples.')\n\n    if filter_fn:\n      tf.logging.warning(\n          'You are filtering the dataset. Notice that this may hurt your '\n          'throughput, since examples still need to be decoded, and may '\n          'make the result of get_num_samples() inacurate. '\n          'train_examples is ignored for filtering, but only used for '\n          'calculating training steps.')\n      data = data.filter(filter_fn)\n      num_samples = filtered_num_samples\n      assert num_samples is not None, (\n          'You must set filtered_num_samples if filter_fn is set.')\n\n    elif not for_eval and train_examples:\n      # Deterministic for same dataset version.\n      data = data.take(train_examples)\n      num_samples = train_examples\n\n    else:\n      num_samples = self.get_num_samples(split_name)\n\n    data = self._cache_data_if_possible(\n        data, split_name=split_name, num_samples=num_samples, for_eval=for_eval)\n\n    def print_filtered_subset(ex):\n      \"\"\"Print filtered subset for debug purpose.\"\"\"\n      if isinstance(ex, dict) and 'id' in ex and 'label' in ex:\n        print_op = tf.print(\n            'filtered_example:',\n            ex['id'],\n            ex['label'],\n            output_stream=tf.logging.error)\n        with tf.control_dependencies([print_op]):\n          ex['id'] = tf.identity(ex['id'])\n      return ex\n    if not for_eval and filter_fn:\n      data = data.map(print_filtered_subset)\n\n    # Repeats data `epochs` time or indefinitely if `epochs` is None.\n    if epochs is None or epochs > 1:\n      data = data.repeat(epochs)\n\n    shuffle_buffer_size = shuffle_buffer_size or self._shuffle_buffer_size\n    if not for_eval and shuffle_buffer_size > 1:\n      data = data.shuffle(shuffle_buffer_size)\n\n    data = self._preprocess_and_batch_data(data, batch_size, drop_remainder,\n                                           pairwise_mix_fn, preprocess_fn,\n                                           ignore_errors)\n\n    if batch_preprocess_fn is not None:\n      data = data.map(batch_preprocess_fn, self._num_preprocessing_threads)\n\n    if prefetch != 0:\n      data = data.prefetch(prefetch)\n\n    return data\n\n  @abc.abstractmethod\n  def _get_dataset_split(self, split_name, shuffle_files=False):\n    \"\"\"Return the Dataset object for the given split name.\n\n    Args:\n      split_name: Name of the dataset split to get.\n      shuffle_files: Whether or not to shuffle files in the dataset.\n\n    Returns:\n      A tf.data.Dataset object containing the data for the given split.\n    \"\"\"\n\n  def _log_warning_if_direct_inheritance(self):\n    tf.logging.warning(\n        'You are directly inheriting from ImageData. Please, consider porting '\n        'your dataset to TFDS (go/tfds) and inheriting from ImageTfdsData '\n        'instead.')\n\n  def _preprocess_and_batch_data(self,\n                                 data,\n                                 batch_size,\n                                 drop_remainder=True,\n                                 pairwise_mix_fn=None,\n                                 preprocess_fn=None,\n                                 ignore_errors=False):\n    \"\"\"Preprocesses and batches a given tf.Dataset.\"\"\"\n    # Preprocess with basic preprocess functions (e.g. decoding images, parsing\n    # features etc.).\n    base_preprocess_fn = compose_preprocess_fn(self._image_decoder,\n                                               self._base_preprocess_fn)\n    # Note: `map_and_batch` is deprecated, and at least when nothing happens\n    # in-between, automatically gets merged for efficiency. Same below.\n    data = data.map(base_preprocess_fn, self._num_preprocessing_threads)\n\n    # Mix images pair-wise before other element-wise preprocessing.\n    # Note: The pairing is implemented by shifting `data` by 1, so the last\n    # element of `data` will be dropped.\n    if pairwise_mix_fn is not None:\n      data = tf.data.Dataset.zip(\n          (data, data.skip(1))).map(pairwise_mix_fn,\n                                    self._num_preprocessing_threads)\n\n    # Preprocess with customized preprocess functions.\n    if preprocess_fn is not None:\n      data = data.map(preprocess_fn, self._num_preprocessing_threads)\n\n    if ignore_errors:\n      tf.logging.info('Ignoring any image with errors.')\n      data = data.apply(tf.data.experimental.ignore_errors())\n\n    return data.batch(batch_size, drop_remainder)\n\n  def _cache_data_if_possible(self, data, split_name, num_samples, for_eval):\n    del split_name\n\n    if not for_eval and num_samples <= 150000:\n      # Cache the whole dataset if it's smaller than 150K examples.\n      data = data.cache()\n    return data", "\n\nclass ImageTfdsData(ImageData):\n  \"\"\"Abstract data provider class for datasets available in Tensorflow Datasets.\n\n  To add new datasets inherit from this class. This class implements a simple\n  API that is used throughout the project and provides standardized way of data\n  preprocessing and batching.\n  \"\"\"\n\n  @abc.abstractmethod\n  def __init__(self, dataset_builder, tfds_splits, image_key='image', **kwargs):\n    \"\"\"Initializer for the base ImageData class.\n\n    Args:\n      dataset_builder: tfds dataset builder object.\n      tfds_splits: a dictionary, that maps splits (\"train\", \"trainval\", \"val\",\n          and \"test\") to the corresponding tfds `Split` objects.\n      image_key: image key.\n      **kwargs: Additional keyword arguments for the ImageData class.\n    \"\"\"\n    self._dataset_builder = dataset_builder\n    self._tfds_splits = tfds_splits\n    self._image_key = image_key\n\n    # Overwrite image decoder\n    def _image_decoder(data):\n      decoder = dataset_builder.info.features[image_key].decode_example\n      data[image_key] = decoder(data[image_key])\n      return data\n    self._image_decoder = _image_decoder\n\n    kwargs.update({'image_decoder': _image_decoder})\n\n    super(ImageTfdsData, self).__init__(**kwargs)\n\n  def get_version(self):\n    return self._dataset_builder.version.__str__()\n\n  def _get_dataset_split(self, split_name, shuffle_files):\n    dummy_decoder = tfds.decode.SkipDecoding()\n    return self._dataset_builder.as_dataset(\n        split=self._tfds_splits[split_name], shuffle_files=shuffle_files,\n        decoders={self._image_key: dummy_decoder})\n\n  def _log_warning_if_direct_inheritance(self):\n    pass", ""]}
{"filename": "src/data/vtab_datasets/kitti.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements Kitti data class.\"\"\"\n", "\"\"\"Implements Kitti data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport tensorflow.compat.v1 as tf\nimport tensorflow_datasets as tfds", "import tensorflow.compat.v1 as tf\nimport tensorflow_datasets as tfds\n\nfrom . import base as base\nfrom .registry import Registry\n\n\ndef _count_all_pp(x):\n  \"\"\"Count all objects.\"\"\"\n  # Count distribution (thresholded at 15):\n\n  label = tf.math.minimum(tf.size(x[\"objects\"][\"type\"]) - 1, 8)\n  return {\"image\": x[\"image\"], \"label\": label}", "\n\ndef _count_vehicles_pp(x):\n  \"\"\"Counting vehicles.\"\"\"\n  # Label distribution:\n\n  vehicles = tf.where(x[\"objects\"][\"type\"] < 3)  # Car, Van, Truck.\n  # Cap at 3.\n  label = tf.math.minimum(tf.size(vehicles), 3)\n  return {\"image\": x[\"image\"], \"label\": label}", "\n\ndef _count_left_pp(x):\n  \"\"\"Count objects on the left hand side of the camera.\"\"\"\n  # Count distribution (thresholded at 15):\n\n  # Location feature contains (x, y, z) in meters w.r.t. the camera.\n  objects_on_left = tf.where(x[\"objects\"][\"location\"][:, 0] < 0)\n  label = tf.math.minimum(tf.size(objects_on_left), 8)\n  return {\"image\": x[\"image\"], \"label\": label}", "\n\ndef _count_far_pp(x):\n  \"\"\"Counts objects far from the camera.\"\"\"\n  # Threshold removes ~half of the objects.\n  # Count distribution (thresholded at 15):\n\n  # Location feature contains (x, y, z) in meters w.r.t. the camera.\n  distant_objects = tf.where(x[\"objects\"][\"location\"][:, 2] >= 25)\n  label = tf.math.minimum(tf.size(distant_objects), 8)\n  return {\"image\": x[\"image\"], \"label\": label}", "\n\ndef _count_near_pp(x):\n  \"\"\"Counts objects close to the camera.\"\"\"\n  # Threshold removes ~half of the objects.\n  # Count distribution:\n\n  # Location feature contains (x, y, z) in meters w.r.t. the camera.\n  close_objects = tf.where(x[\"objects\"][\"location\"][:, 2] < 25)\n  label = tf.math.minimum(tf.size(close_objects), 8)\n  return {\"image\": x[\"image\"], \"label\": label}", "\n\ndef _closest_object_distance_pp(x):\n  \"\"\"Predict the distance to the closest object.\"\"\"\n  # Label distribution:\n\n  # Location feature contains (x, y, z) in meters w.r.t. the camera.\n  dist = tf.reduce_min(x[\"objects\"][\"location\"][:, 2])\n  thrs = np.array([-100, 5.6, 8.4, 13.4, 23.4])\n  label = tf.reduce_max(tf.where((thrs - dist) < 0))\n  return {\"image\": x[\"image\"], \"label\": label}", "\n\ndef _closest_vehicle_distance_pp(x):\n  \"\"\"Predict the distance to the closest vehicle.\"\"\"\n  # Label distribution:\n\n  # Location feature contains (x, y, z) in meters w.r.t. the camera.\n  vehicles = tf.where(x[\"objects\"][\"type\"] < 3)  # Car, Van, Truck.\n  vehicle_z = tf.gather(params=x[\"objects\"][\"location\"][:, 2], indices=vehicles)\n  vehicle_z = tf.concat([vehicle_z, tf.constant([[1000.0]])], axis=0)\n  dist = tf.reduce_min(vehicle_z)\n  # Results in a uniform distribution over three distances, plus one class for\n  # \"no vehicle\".\n  thrs = np.array([-100.0, 8.0, 20.0, 999.0])\n  label = tf.reduce_max(tf.where((thrs - dist) < 0))\n  return {\"image\": x[\"image\"], \"label\": label}", "\n\ndef _closest_object_x_location_pp(x):\n  \"\"\"Predict the absolute x position of the closest object.\"\"\"\n  # Label distribution:\n\n  # Location feature contains (x, y, z) in meters w.r.t. the camera.\n  idx = tf.math.argmin(x[\"objects\"][\"location\"][:, 2])\n  xloc = x[\"objects\"][\"location\"][idx, 0]\n  thrs = np.array([-100, -6.4, -3.5, 0.0, 3.3, 23.9])\n  label = tf.reduce_max(tf.where((thrs - xloc) < 0))\n  return {\"image\": x[\"image\"], \"label\": label}", "\n\n_TASK_DICT = {\n    \"count_all\": {\n        \"preprocess_fn\": _count_all_pp,\n        \"num_classes\": 16,\n    },\n    \"count_left\": {\n        \"preprocess_fn\": _count_left_pp,\n        \"num_classes\": 16,", "        \"preprocess_fn\": _count_left_pp,\n        \"num_classes\": 16,\n    },\n    \"count_far\": {\n        \"preprocess_fn\": _count_far_pp,\n        \"num_classes\": 16,\n    },\n    \"count_near\": {\n        \"preprocess_fn\": _count_near_pp,\n        \"num_classes\": 16,", "        \"preprocess_fn\": _count_near_pp,\n        \"num_classes\": 16,\n    },\n    \"closest_object_distance\": {\n        \"preprocess_fn\": _closest_object_distance_pp,\n        \"num_classes\": 5,\n    },\n    \"closest_object_x_location\": {\n        \"preprocess_fn\": _closest_object_x_location_pp,\n        \"num_classes\": 5,", "        \"preprocess_fn\": _closest_object_x_location_pp,\n        \"num_classes\": 5,\n    },\n    \"count_vehicles\": {\n        \"preprocess_fn\": _count_vehicles_pp,\n        \"num_classes\": 4,\n    },\n    \"closest_vehicle_distance\": {\n        \"preprocess_fn\": _closest_vehicle_distance_pp,\n        \"num_classes\": 4,", "        \"preprocess_fn\": _closest_vehicle_distance_pp,\n        \"num_classes\": 4,\n    },\n}\n\n\n@Registry.register(\"data.kitti\", \"class\")\nclass KittiData(base.ImageTfdsData):\n  \"\"\"Provides Kitti dataset.\n\n  Six tasks are supported:\n    1. Count the number of objects.\n    2. Count the number of objects on the left hand side of the camera.\n    3. Count the number of objects in the foreground.\n    4. Count the number of objects in the background.\n    5. Predict the distance of the closest object.\n    6. Predict the x-location (w.r.t. the camera) of the closest object.\n  \"\"\"\n\n  def __init__(self, task, data_dir=None):\n\n    if task not in _TASK_DICT:\n      raise ValueError(\"Unknown task: %s\" % task)\n\n    dataset_builder = tfds.builder(\"kitti:3.2.0\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    tfds_splits = {\n        \"train\": \"train\",\n        \"val\": \"validation\",\n        \"trainval\": \"train+validation\",\n        \"test\": \"test\",\n        \"train800\": \"train[:800]\",\n        \"val200\": \"validation[:200]\",\n        \"train800val200\": \"train[:800]+validation[:200]\",\n    }\n\n    # Example counts are retrieved from the tensorflow dataset info.\n    train_count = dataset_builder.info.splits[tfds.Split.TRAIN].num_examples\n    val_count = dataset_builder.info.splits[tfds.Split.VALIDATION].num_examples\n    test_count = dataset_builder.info.splits[tfds.Split.TEST].num_examples\n    # Creates a dict with example counts for each split.\n    num_samples_splits = {\n        \"train\": train_count,\n        \"val\": val_count,\n        \"trainval\": train_count + val_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    task = _TASK_DICT[task]\n    base_preprocess_fn = task[\"preprocess_fn\"]\n    super(KittiData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        base_preprocess_fn=base_preprocess_fn,\n        num_classes=task[\"num_classes\"])", ""]}
{"filename": "src/data/vtab_datasets/caltech.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Imports the Caltech images dataset.\"\"\"\n", "\"\"\"Imports the Caltech images dataset.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom . import base as base\nfrom .registry import Registry\nimport tensorflow_datasets as tfds\n\n", "\n\n# Percentage of the original training set retained for training, the rest is\n# used as a validation set.\n_TRAIN_SPLIT_PERCENT = 90\n\n\n@Registry.register(\"data.caltech101\", \"class\")\nclass Caltech101(base.ImageTfdsData):\n  \"\"\"Provides the Caltech101 dataset.\n\n  See the base class for additional details on the class.\n\n  See TFDS dataset for details on the dataset:\n  third_party/py/tensorflow_datasets/image/caltech.py\n\n  The original (TFDS) dataset contains only a train and test split. We randomly\n  sample _TRAIN_SPLIT_PERCENT% of the train split for our \"train\" set. The\n  remainder of the TFDS train split becomes our \"val\" set. The full TFDS train\n  split is called \"trainval\". The TFDS test split is used as our test set.\n\n  Note that, in the TFDS dataset, the training split is class-balanced, but not\n  the test split. Therefore, a significant difference between performance on the\n  \"val\" and \"test\" sets should be expected.\n  \"\"\"\n\n  def __init__(self, data_dir=None):\n    dataset_builder = tfds.builder(\"caltech101:3.0.1\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    # Creates a dict with example counts for each split.\n    trainval_count = dataset_builder.info.splits[\"train\"].num_examples\n    train_count = (_TRAIN_SPLIT_PERCENT * trainval_count) // 100\n    test_count = dataset_builder.info.splits[\"test\"].num_examples\n    num_samples_splits = dict(\n        train=train_count,\n        val=trainval_count - train_count,\n        trainval=trainval_count,\n        test=test_count,\n        train800=800,\n        val200=200,\n        train800val200=1000)\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"train\": \"train[:{}]\".format(train_count),\n        \"val\": \"train[{}:]\".format(train_count),\n        \"trainval\": \"train\",\n        \"test\": \"test\",\n        \"train800\": \"train[:800]\",\n        \"val200\": \"train[{}:{}]\".format(train_count, train_count+200),\n        \"train800val200\": (\n            \"train[:800]+train[{}:{}]\".format(train_count, train_count+200)),\n    }\n\n    super(Caltech101, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=3000,\n        base_preprocess_fn=base.make_get_tensors_fn((\"image\", \"label\")),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", "class Caltech101(base.ImageTfdsData):\n  \"\"\"Provides the Caltech101 dataset.\n\n  See the base class for additional details on the class.\n\n  See TFDS dataset for details on the dataset:\n  third_party/py/tensorflow_datasets/image/caltech.py\n\n  The original (TFDS) dataset contains only a train and test split. We randomly\n  sample _TRAIN_SPLIT_PERCENT% of the train split for our \"train\" set. The\n  remainder of the TFDS train split becomes our \"val\" set. The full TFDS train\n  split is called \"trainval\". The TFDS test split is used as our test set.\n\n  Note that, in the TFDS dataset, the training split is class-balanced, but not\n  the test split. Therefore, a significant difference between performance on the\n  \"val\" and \"test\" sets should be expected.\n  \"\"\"\n\n  def __init__(self, data_dir=None):\n    dataset_builder = tfds.builder(\"caltech101:3.0.1\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    # Creates a dict with example counts for each split.\n    trainval_count = dataset_builder.info.splits[\"train\"].num_examples\n    train_count = (_TRAIN_SPLIT_PERCENT * trainval_count) // 100\n    test_count = dataset_builder.info.splits[\"test\"].num_examples\n    num_samples_splits = dict(\n        train=train_count,\n        val=trainval_count - train_count,\n        trainval=trainval_count,\n        test=test_count,\n        train800=800,\n        val200=200,\n        train800val200=1000)\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"train\": \"train[:{}]\".format(train_count),\n        \"val\": \"train[{}:]\".format(train_count),\n        \"trainval\": \"train\",\n        \"test\": \"test\",\n        \"train800\": \"train[:800]\",\n        \"val200\": \"train[{}:{}]\".format(train_count, train_count+200),\n        \"train800val200\": (\n            \"train[:800]+train[{}:{}]\".format(train_count, train_count+200)),\n    }\n\n    super(Caltech101, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=3000,\n        base_preprocess_fn=base.make_get_tensors_fn((\"image\", \"label\")),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", ""]}
{"filename": "src/data/vtab_datasets/oxford_iiit_pet.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements OxfordIIITPet data class.\"\"\"\n", "\"\"\"Implements OxfordIIITPet data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow_datasets as tfds\n\nfrom . import base as base\nfrom .registry import Registry", "from . import base as base\nfrom .registry import Registry\n# This constant specifies the percentage of data that is used to create custom\n# train/val splits. Specifically, TRAIN_SPLIT_PERCENT% of the official training\n# split is used as a new training split and the rest is used for validation.\nTRAIN_SPLIT_PERCENT = 80\n\n\n@Registry.register(\"data.oxford_iiit_pet\", \"class\")\nclass OxfordIIITPetData(base.ImageTfdsData):\n  \"\"\"Provides OxfordIIITPet data.\n\n  The OxfordIIITPet dataset comes only with a training and test set.\n  Therefore, the validation set is split out of the original training set, and\n  the remaining examples are used as the \"train\" split. The \"trainval\" split\n  corresponds to the original training set.\n\n  For additional details and usage, see the base class.\n  \"\"\"\n\n  def __init__(self, data_dir=None, train_split_percent=None):\n\n    dataset_builder = tfds.builder(\"oxford_iiit_pet:3.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n    train_split_percent = train_split_percent or TRAIN_SPLIT_PERCENT\n\n    # Creates a dict with example counts for each split.\n    trainval_count = dataset_builder.info.splits[tfds.Split.TRAIN].num_examples\n    test_count = dataset_builder.info.splits[tfds.Split.TEST].num_examples\n    num_samples_splits = {\n        \"train\": (train_split_percent * trainval_count) // 100,\n        \"val\": trainval_count - (train_split_percent * trainval_count) // 100,\n        \"trainval\": trainval_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"train\": \"train[:{}]\".format(num_samples_splits[\"train\"]),\n        \"val\": \"train[{}:]\".format(num_samples_splits[\"train\"]),\n        \"trainval\": tfds.Split.TRAIN,\n        \"test\": tfds.Split.TEST,\n        \"train800\": \"train[:800]\",\n        \"val200\": \"train[{}:{}]\".format(\n            num_samples_splits[\"train\"], num_samples_splits[\"train\"]+200),\n        \"train800val200\": \"train[:800]+train[{}:{}]\".format(\n            num_samples_splits[\"train\"], num_samples_splits[\"train\"]+200),\n    }\n\n    super(OxfordIIITPetData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Export only image and label tensors with their original types.\n        base_preprocess_fn=base.make_get_tensors_fn([\"image\", \"label\"]),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", "@Registry.register(\"data.oxford_iiit_pet\", \"class\")\nclass OxfordIIITPetData(base.ImageTfdsData):\n  \"\"\"Provides OxfordIIITPet data.\n\n  The OxfordIIITPet dataset comes only with a training and test set.\n  Therefore, the validation set is split out of the original training set, and\n  the remaining examples are used as the \"train\" split. The \"trainval\" split\n  corresponds to the original training set.\n\n  For additional details and usage, see the base class.\n  \"\"\"\n\n  def __init__(self, data_dir=None, train_split_percent=None):\n\n    dataset_builder = tfds.builder(\"oxford_iiit_pet:3.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n    train_split_percent = train_split_percent or TRAIN_SPLIT_PERCENT\n\n    # Creates a dict with example counts for each split.\n    trainval_count = dataset_builder.info.splits[tfds.Split.TRAIN].num_examples\n    test_count = dataset_builder.info.splits[tfds.Split.TEST].num_examples\n    num_samples_splits = {\n        \"train\": (train_split_percent * trainval_count) // 100,\n        \"val\": trainval_count - (train_split_percent * trainval_count) // 100,\n        \"trainval\": trainval_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"train\": \"train[:{}]\".format(num_samples_splits[\"train\"]),\n        \"val\": \"train[{}:]\".format(num_samples_splits[\"train\"]),\n        \"trainval\": tfds.Split.TRAIN,\n        \"test\": tfds.Split.TEST,\n        \"train800\": \"train[:800]\",\n        \"val200\": \"train[{}:{}]\".format(\n            num_samples_splits[\"train\"], num_samples_splits[\"train\"]+200),\n        \"train800val200\": \"train[:800]+train[{}:{}]\".format(\n            num_samples_splits[\"train\"], num_samples_splits[\"train\"]+200),\n    }\n\n    super(OxfordIIITPetData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Export only image and label tensors with their original types.\n        base_preprocess_fn=base.make_get_tensors_fn([\"image\", \"label\"]),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", ""]}
{"filename": "src/data/vtab_datasets/__init__.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n", ""]}
{"filename": "src/data/vtab_datasets/resisc45.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements RESISC-45 data class.\"\"\"\n", "\"\"\"Implements RESISC-45 data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow_datasets as tfds\n\nfrom . import base as base\nfrom .registry import Registry", "from . import base as base\nfrom .registry import Registry\nTRAIN_SPLIT_PERCENT = 60\nVALIDATION_SPLIT_PERCENT = 20\nTEST_SPLIT_PERCENT = 20\n\n\n@Registry.register(\"data.resisc45\", \"class\")\nclass Resisc45Data(base.ImageTfdsData):\n  \"\"\"Provides RESISC-45 dataset.\n\n  RESISC45 dataset is a publicly available benchmark for Remote Sensing Image\n  Scene Classification (RESISC), created by Northwestern Polytechnical\n  University (NWPU). This dataset contains 31,500 images, covering 45 scene\n  classes with 700 images in each class.\n\n  URL: http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html\n  \"\"\"\n\n  def __init__(self, data_dir=None):\n    dataset_builder = tfds.builder(\"resisc45:3.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    # Example counts are retrieved from the tensorflow dataset info.\n    num_examples = dataset_builder.info.splits[\"train\"].num_examples\n    train_count = num_examples * TRAIN_SPLIT_PERCENT // 100\n    val_count = num_examples * VALIDATION_SPLIT_PERCENT // 100\n    test_count = num_examples * TEST_SPLIT_PERCENT // 100\n\n    tfds_splits = {\n        \"train\":\n            \"train[:{}]\".format(train_count),\n        \"val\":\n            \"train[{}:{}]\".format(train_count, train_count + val_count),\n        \"trainval\":\n            \"train[:{}]\".format(train_count + val_count),\n        \"test\":\n            \"train[{}:]\".format(train_count + val_count),\n        \"train800\":\n            \"train[:800]\",\n        \"val200\":\n            \"train[{}:{}]\".format(train_count, train_count+200),\n        \"train800val200\":\n            \"train[:800]+train[{}:{}]\".format(train_count, train_count+200),\n    }\n\n    # Creates a dict with example counts for each split.\n    num_samples_splits = {\n        \"train\": train_count,\n        \"val\": val_count,\n        \"trainval\": train_count + val_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    super(Resisc45Data, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Rename tensors but keep their original types.\n        base_preprocess_fn=base.make_get_and_cast_tensors_fn({\n            \"image\": (\"image\", None),\n            \"label\": (\"label\", None),\n        }),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", "class Resisc45Data(base.ImageTfdsData):\n  \"\"\"Provides RESISC-45 dataset.\n\n  RESISC45 dataset is a publicly available benchmark for Remote Sensing Image\n  Scene Classification (RESISC), created by Northwestern Polytechnical\n  University (NWPU). This dataset contains 31,500 images, covering 45 scene\n  classes with 700 images in each class.\n\n  URL: http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html\n  \"\"\"\n\n  def __init__(self, data_dir=None):\n    dataset_builder = tfds.builder(\"resisc45:3.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    # Example counts are retrieved from the tensorflow dataset info.\n    num_examples = dataset_builder.info.splits[\"train\"].num_examples\n    train_count = num_examples * TRAIN_SPLIT_PERCENT // 100\n    val_count = num_examples * VALIDATION_SPLIT_PERCENT // 100\n    test_count = num_examples * TEST_SPLIT_PERCENT // 100\n\n    tfds_splits = {\n        \"train\":\n            \"train[:{}]\".format(train_count),\n        \"val\":\n            \"train[{}:{}]\".format(train_count, train_count + val_count),\n        \"trainval\":\n            \"train[:{}]\".format(train_count + val_count),\n        \"test\":\n            \"train[{}:]\".format(train_count + val_count),\n        \"train800\":\n            \"train[:800]\",\n        \"val200\":\n            \"train[{}:{}]\".format(train_count, train_count+200),\n        \"train800val200\":\n            \"train[:800]+train[{}:{}]\".format(train_count, train_count+200),\n    }\n\n    # Creates a dict with example counts for each split.\n    num_samples_splits = {\n        \"train\": train_count,\n        \"val\": val_count,\n        \"trainval\": train_count + val_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    super(Resisc45Data, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Rename tensors but keep their original types.\n        base_preprocess_fn=base.make_get_and_cast_tensors_fn({\n            \"image\": (\"image\", None),\n            \"label\": (\"label\", None),\n        }),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", ""]}
{"filename": "src/data/vtab_datasets/sun397.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements Sun397 data class.\"\"\"\n", "\"\"\"Implements Sun397 data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow_datasets as tfds\n\nfrom . import base as base\nfrom .registry import Registry", "from . import base as base\nfrom .registry import Registry\nCUSTOM_TRAIN_SPLIT_PERCENT = 50\nCUSTOM_VALIDATION_SPLIT_PERCENT = 20\nCUSTOM_TEST_SPLIT_PERCENT = 30\n\n\n@Registry.register(\"data.sun397\", \"class\")\nclass Sun397Data(base.ImageTfdsData):\n  \"\"\"Provides Sun397Data data.\"\"\"\n\n  def __init__(self, config=\"tfds\", data_dir=None):\n\n    if config == \"tfds\":\n      dataset_builder = tfds.builder(\"sun397/tfds:4.*.*\", data_dir=data_dir)\n      dataset_builder.download_and_prepare()\n\n      tfds_splits = {\n          \"train\": \"train\",\n          \"val\": \"validation\",\n          \"test\": \"test\",\n          \"trainval\": \"train+validation\",\n          \"train800\": \"train[:800]\",\n          \"val200\": \"validation[:200]\",\n          \"train800val200\": \"train[:800]+validation[:200]\",\n      }\n      # Creates a dict with example counts.\n      num_samples_splits = {\n          \"test\": dataset_builder.info.splits[\"test\"].num_examples,\n          \"train\": dataset_builder.info.splits[\"train\"].num_examples,\n          \"val\": dataset_builder.info.splits[\"validation\"].num_examples,\n          \"train800\": 800,\n          \"val200\": 200,\n          \"train800val200\": 1000,\n      }\n      num_samples_splits[\"trainval\"] = (\n          num_samples_splits[\"train\"] + num_samples_splits[\"val\"])\n    else:\n\n      raise ValueError(\"No supported config %r for Sun397Data.\" % config)\n\n    super(Sun397Data, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Export only image and label tensors with their original types.\n        base_preprocess_fn=base.make_get_tensors_fn([\"image\", \"label\"]),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", "class Sun397Data(base.ImageTfdsData):\n  \"\"\"Provides Sun397Data data.\"\"\"\n\n  def __init__(self, config=\"tfds\", data_dir=None):\n\n    if config == \"tfds\":\n      dataset_builder = tfds.builder(\"sun397/tfds:4.*.*\", data_dir=data_dir)\n      dataset_builder.download_and_prepare()\n\n      tfds_splits = {\n          \"train\": \"train\",\n          \"val\": \"validation\",\n          \"test\": \"test\",\n          \"trainval\": \"train+validation\",\n          \"train800\": \"train[:800]\",\n          \"val200\": \"validation[:200]\",\n          \"train800val200\": \"train[:800]+validation[:200]\",\n      }\n      # Creates a dict with example counts.\n      num_samples_splits = {\n          \"test\": dataset_builder.info.splits[\"test\"].num_examples,\n          \"train\": dataset_builder.info.splits[\"train\"].num_examples,\n          \"val\": dataset_builder.info.splits[\"validation\"].num_examples,\n          \"train800\": 800,\n          \"val200\": 200,\n          \"train800val200\": 1000,\n      }\n      num_samples_splits[\"trainval\"] = (\n          num_samples_splits[\"train\"] + num_samples_splits[\"val\"])\n    else:\n\n      raise ValueError(\"No supported config %r for Sun397Data.\" % config)\n\n    super(Sun397Data, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Export only image and label tensors with their original types.\n        base_preprocess_fn=base.make_get_tensors_fn([\"image\", \"label\"]),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", ""]}
{"filename": "src/data/vtab_datasets/diabetic_retinopathy.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements Diabetic Retinopathy data class.\"\"\"\n", "\"\"\"Implements Diabetic Retinopathy data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\nimport tensorflow_addons.image as tfa_image\nimport tensorflow_datasets as tfds\n", "import tensorflow_datasets as tfds\n\nfrom . import base as base\nfrom .registry import Registry\n\n\n@Registry.register(\"data.diabetic_retinopathy\", \"class\")\nclass RetinopathyData(base.ImageTfdsData):\n  \"\"\"Provides Diabetic Retinopathy classification data.\n\n  Retinopathy comes only with a training and test set. Therefore, the validation\n  set is split out of the original training set, and the remaining examples are\n  used as the \"train\" split. The \"trainval\" split corresponds to the original\n  training set.\n\n  For additional details and usage, see the base class.\n  \"\"\"\n\n  _CONFIGS_WITH_GREY_BACKGROUND = [\"btgraham-300\"]\n\n  def __init__(self, config=\"btgraham-300\", heavy_train_augmentation=False,\n               data_dir=None):\n    \"\"\"Initializer for Diabetic Retinopathy dataset.\n\n    Args:\n      config: Name of the TFDS config to use for this dataset.\n      heavy_train_augmentation: If True, use heavy data augmentation on the\n        training data. Recommended to achieve SOTA.\n      data_dir: directory for downloading and storing the data.\n    \"\"\"\n    config_and_version = config + \":3.*.*\"\n    dataset_builder = tfds.builder(\"diabetic_retinopathy_detection/{}\".format(\n        config_and_version), data_dir=data_dir)\n    self._config = config\n    self._heavy_train_augmentation = heavy_train_augmentation\n\n    dataset_builder.download_and_prepare()\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"train\": \"train\",\n        \"val\": \"validation\",\n        \"trainval\": \"train+validation\",\n        \"test\": \"test\",\n        \"train800\": \"train[:800]\",\n        \"val200\": \"validation[:200]\",\n        \"train800val200\": \"train[:800]+validation[:200]\",\n    }\n\n    # Creates a dict with example counts for each split.\n    train_count = dataset_builder.info.splits[\"train\"].num_examples\n    val_count = dataset_builder.info.splits[\"validation\"].num_examples\n    test_count = dataset_builder.info.splits[\"test\"].num_examples\n    num_samples_splits = {\n        \"train\": train_count,\n        \"val\": val_count,\n        \"trainval\": train_count + val_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    super(RetinopathyData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Export only image and label tensors with their original types.\n        base_preprocess_fn=base.make_get_tensors_fn([\"image\", \"label\"]),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)\n\n  @property\n  def config(self):\n    return self._config\n\n  @property\n  def heavy_train_augmentation(self):\n    return self._heavy_train_augmentation\n\n  def get_tf_data(self,\n                  split_name,\n                  batch_size,\n                  preprocess_fn=None,\n                  for_eval=False,\n                  **kwargs):\n    if self._heavy_train_augmentation and not for_eval:\n      preprocess_fn = base.compose_preprocess_fn(\n          self._heavy_train_augmentation, preprocess_fn)\n\n    return super(RetinopathyData, self).get_tf_data(\n        split_name=split_name,\n        batch_size=batch_size,\n        preprocess_fn=preprocess_fn,\n        for_eval=for_eval,\n        **kwargs)\n\n  def _sample_heavy_data_augmentation_parameters(self):\n    # Scale image +/- 10%.\n    s = tf.random.uniform(shape=(), minval=-0.1, maxval=0.1)\n    # Rotate image [0, 2pi).\n    a = tf.random.uniform(shape=(), minval=0.0, maxval=2.0 * 3.1415926535)\n    # Vertically shear image +/- 20%.\n    b = tf.random.uniform(shape=(), minval=-0.2, maxval=0.2) + a\n    # Horizontal and vertial flipping.\n    hf = tf.random.shuffle([-1.0, 1.0])[0]\n    vf = tf.random.shuffle([-1.0, 1.0])[0]\n    # Relative x,y translation.\n    dx = tf.random.uniform(shape=(), minval=-0.1, maxval=0.1)\n    dy = tf.random.uniform(shape=(), minval=-0.1, maxval=0.1)\n    return s, a, b, hf, vf, dx, dy\n\n  def _heavy_data_augmentation_fn(self, example):\n    \"\"\"Perform heavy augmentation on a given input data example.\n\n    This is the same data augmentation as the one done by Ben Graham, the winner\n    of the 2015 Kaggle competition. See:\n    https://github.com/btgraham/SparseConvNet/blob/a6bdb0c938b3556c1e6c23d5a014db9f404502b9/kaggleDiabetes1.cpp#L12\n\n    Args:\n      example: A dictionary containing an \"image\" key with the image to\n        augment.\n\n    Returns:\n      The input dictionary with the key \"image\" containing the augmented image.\n    \"\"\"\n    image = example[\"image\"]\n    image_shape = tf.shape(image)\n    if len(image.get_shape().as_list()) not in [2, 3]:\n      raise ValueError(\n          \"Input image must be a rank-2 or rank-3 tensor, but rank-{} \"\n          \"was given\".format(len(image.get_shape().as_list())))\n    height = tf.cast(image_shape[0], dtype=tf.float32)\n    width = tf.cast(image_shape[1], dtype=tf.float32)\n    # Sample data augmentation parameters.\n    s, a, b, hf, vf, dx, dy = self._sample_heavy_data_augmentation_parameters()\n    # Rotation + scale.\n    c00 = (1 + s) * tf.cos(a)\n    c01 = (1 + s) * tf.sin(a)\n    c10 = (s - 1) * tf.sin(b)\n    c11 = (1 - s) * tf.cos(b)\n    # Horizontal and vertial flipping.\n    c00 = c00 * hf\n    c01 = c01 * hf\n    c10 = c10 * vf\n    c11 = c11 * vf\n    # Convert x,y translation to absolute values.\n    dx = width * dx\n    dy = height * dy\n    # Convert affine matrix to TF's transform. Matrix is applied w.r.t. the\n    # center of the image.\n    cy = height / 2.0\n    cx = width / 2.0\n    affine_matrix = [[c00, c01, (1.0 - c00) * cx - c01 * cy + dx],\n                     [c10, c11, (1.0 - c11) * cy - c10 * cx + dy],\n                     [0.0, 0.0, 1.0]]\n    affine_matrix = tf.convert_to_tensor(affine_matrix, dtype=tf.float32)\n    transform = tfa_image.transform_ops.matrices_to_flat_transforms(\n        tf.linalg.inv(affine_matrix))\n    if self._config in self._CONFIGS_WITH_GREY_BACKGROUND:\n      # Since background is grey in these configs, put in pixels in [-1, 1]\n      # range to avoid artifacts from the affine transformation.\n      image = tf.cast(image, dtype=tf.float32)\n      image = (image / 127.5) - 1.0\n    # Apply the affine transformation.\n    image = tfa_image.transform(images=image, transforms=transform)\n    if self._config in self._CONFIGS_WITH_GREY_BACKGROUND:\n      # Put pixels back to [0, 255] range and cast to uint8, since this is what\n      # our preprocessing pipeline usually expects.\n      image = (1.0 + image) * 127.5\n      image = tf.cast(image, dtype=tf.uint8)\n    example[\"image\"] = image\n    return example", ""]}
{"filename": "src/data/vtab_datasets/dtd.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements the Describable Textures Dataset (DTD) data class.\"\"\"\n", "\"\"\"Implements the Describable Textures Dataset (DTD) data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow_datasets as tfds\n\nfrom . import base as base\nfrom .registry import Registry", "from . import base as base\nfrom .registry import Registry\n\n\n@Registry.register(\"data.dtd\", \"class\")\nclass DTDData(base.ImageTfdsData):\n  \"\"\"Provides Describable Textures Dataset (DTD) data.\n\n  As of version 1.0.0, the train/val/test splits correspond to those of the\n  1st fold of the official cross-validation partition.\n\n  For additional details and usage, see the base class.\n  \"\"\"\n\n  def __init__(self, data_dir=None):\n\n    dataset_builder = tfds.builder(\"dtd:3.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"train\": \"train\",\n        \"val\": \"validation\",\n        \"trainval\": \"train+validation\",\n        \"test\": \"test\",\n        \"train800\": \"train[:800]\",\n        \"val200\": \"validation[:200]\",\n        \"train800val200\": \"train[:800]+validation[:200]\",\n    }\n\n    # Creates a dict with example counts for each split.\n    train_count = dataset_builder.info.splits[\"train\"].num_examples\n    val_count = dataset_builder.info.splits[\"validation\"].num_examples\n    test_count = dataset_builder.info.splits[\"test\"].num_examples\n    num_samples_splits = {\n        \"train\": train_count,\n        \"val\": val_count,\n        \"trainval\": train_count + val_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    super(DTDData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Export only image and label tensors with their original types.\n        base_preprocess_fn=base.make_get_tensors_fn([\"image\", \"label\"]),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", ""]}
{"filename": "src/data/vtab_datasets/oxford_flowers102.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements oxford flowers 102 data class.\"\"\"\n", "\"\"\"Implements oxford flowers 102 data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow_datasets as tfds\n\nfrom . import base as base\nfrom .registry import Registry", "from . import base as base\nfrom .registry import Registry\n\n\n@Registry.register(\"data.oxford_flowers102\", \"class\")\nclass OxfordFlowers102Data(base.ImageTfdsData):\n  \"\"\"Provides Oxford 102 categories flowers dataset.\n\n  See corresponding tfds dataset for details.\n\n  URL: https://www.robots.ox.ac.uk/~vgg/data/flowers/102/\n  \"\"\"\n\n  def __init__(self, data_dir=None, train_split_percent=None):\n    dataset_builder = tfds.builder(\"oxford_flowers102:2.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    # Example counts are retrieved from the tensorflow dataset info.\n    train_count = dataset_builder.info.splits[tfds.Split.TRAIN].num_examples\n    val_count = dataset_builder.info.splits[tfds.Split.VALIDATION].num_examples\n    test_count = dataset_builder.info.splits[tfds.Split.TEST].num_examples\n\n    if train_split_percent:\n      tfds_splits = {\n          \"train\": \"train[:{s}%]+validation[:{s}%]\".format(\n              s=train_split_percent),\n          \"val\": \"train[-{s}%:]+validation[-{s}%:]\".format(\n              s=train_split_percent),\n          \"trainval\": \"train+validation\",\n          \"test\": \"test\",\n          \"train800\": \"train[:800]\",\n          \"val200\": \"validation[:200]\",\n          \"train800val200\": \"train[:800]+validation[:200]\",\n      }\n      num_samples_splits = {\n          \"train\": (((train_count + val_count) // 100)\n                    * train_split_percent),\n          \"val\": (((train_count + val_count) // 100) *\n                  (100 - train_split_percent)),\n          \"trainval\": train_count + val_count,\n          \"test\": test_count,\n          \"train800\": 800,\n          \"val200\": 200,\n          \"train800val200\": 1000,\n      }\n    else:\n      tfds_splits = {\n          \"train\": \"train\",\n          \"val\": \"validation\",\n          \"trainval\": \"train+validation\",\n          \"test\": \"test\",\n          \"train800\": \"train[:800]\",\n          \"val200\": \"validation[:200]\",\n          \"train800val200\": \"train[:800]+validation[:200]\",\n      }\n      num_samples_splits = {\n          \"train\": train_count,\n          \"val\": val_count,\n          \"trainval\": train_count + val_count,\n          \"test\": test_count,\n          \"train800\": 800,\n          \"val200\": 200,\n          \"train800val200\": 1000,\n      }\n\n    super(OxfordFlowers102Data, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Rename tensors but keep their original types.\n        base_preprocess_fn=base.make_get_and_cast_tensors_fn({\n            \"image\": (\"image\", None),\n            \"label\": (\"label\", None),\n        }),\n        num_classes=dataset_builder.info.features[\"label\"]\n        .num_classes)", ""]}
{"filename": "src/data/vtab_datasets/smallnorb.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements the SmallNORB data class.\"\"\"\n", "\"\"\"Implements the SmallNORB data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\nimport tensorflow_datasets as tfds\n\nfrom . import base as base", "\nfrom . import base as base\nfrom .registry import Registry\n# This constant specifies the percentage of data that is used to create custom\n# val/test splits. Specifically, VAL_SPLIT_PERCENT% of the official testing\n# split is used as a new validation split and the rest is used for testing.\nVAL_SPLIT_PERCENT = 50\n\n\n@Registry.register(\"data.smallnorb\", \"class\")\nclass SmallNORBData(base.ImageTfdsData):\n  \"\"\"Provides the SmallNORB data set.\n\n  SmallNORB comes only with a training and test set. Therefore, the validation\n  set is split out of the original training set, and the remaining examples are\n  used as the \"train\" split. The \"trainval\" split corresponds to the original\n  training set.\n\n  For additional details and usage, see the base class.\n\n  The data set page is https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/.\n  \"\"\"\n\n  def __init__(self, predicted_attribute, data_dir=None):\n    dataset_builder = tfds.builder(\"smallnorb:2.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    if predicted_attribute not in dataset_builder.info.features:\n      raise ValueError(\n          \"{} is not a valid attribute to predict.\".format(predicted_attribute))\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"train\": \"train\",\n        \"val\": \"test[:{}%]\".format(VAL_SPLIT_PERCENT),\n        \"trainval\": \"train+test[:{}%]\".format(VAL_SPLIT_PERCENT),\n        \"test\": \"test[{}%:]\".format(VAL_SPLIT_PERCENT),\n        \"train800\": \"train[:800]\",\n        \"val200\": \"test[:200]\",\n        \"train800val200\": \"train[:800]+test[:200]\",\n    }\n\n    # Creates a dict with example counts for each split.\n    train_count = dataset_builder.info.splits[\"train\"].num_examples\n    test_count = dataset_builder.info.splits[\"test\"].num_examples\n    num_samples_validation = VAL_SPLIT_PERCENT * test_count // 100\n    num_samples_splits = {\n        \"train\": train_count,\n        \"val\": num_samples_validation,\n        \"trainval\": train_count + num_samples_validation,\n        \"test\": test_count - num_samples_validation,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    def preprocess_fn(tensors):\n      # For consistency with other datasets, image needs to have three channels.\n      image = tf.tile(tensors[\"image\"], [1, 1, 3])\n      return dict(image=image, label=tensors[predicted_attribute])\n\n    info = dataset_builder.info\n    super(SmallNORBData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # We extract the attribute we want to predict in the preprocessing.\n        base_preprocess_fn=preprocess_fn,\n        num_classes=info.features[predicted_attribute].num_classes)", "\n@Registry.register(\"data.smallnorb\", \"class\")\nclass SmallNORBData(base.ImageTfdsData):\n  \"\"\"Provides the SmallNORB data set.\n\n  SmallNORB comes only with a training and test set. Therefore, the validation\n  set is split out of the original training set, and the remaining examples are\n  used as the \"train\" split. The \"trainval\" split corresponds to the original\n  training set.\n\n  For additional details and usage, see the base class.\n\n  The data set page is https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/.\n  \"\"\"\n\n  def __init__(self, predicted_attribute, data_dir=None):\n    dataset_builder = tfds.builder(\"smallnorb:2.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    if predicted_attribute not in dataset_builder.info.features:\n      raise ValueError(\n          \"{} is not a valid attribute to predict.\".format(predicted_attribute))\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"train\": \"train\",\n        \"val\": \"test[:{}%]\".format(VAL_SPLIT_PERCENT),\n        \"trainval\": \"train+test[:{}%]\".format(VAL_SPLIT_PERCENT),\n        \"test\": \"test[{}%:]\".format(VAL_SPLIT_PERCENT),\n        \"train800\": \"train[:800]\",\n        \"val200\": \"test[:200]\",\n        \"train800val200\": \"train[:800]+test[:200]\",\n    }\n\n    # Creates a dict with example counts for each split.\n    train_count = dataset_builder.info.splits[\"train\"].num_examples\n    test_count = dataset_builder.info.splits[\"test\"].num_examples\n    num_samples_validation = VAL_SPLIT_PERCENT * test_count // 100\n    num_samples_splits = {\n        \"train\": train_count,\n        \"val\": num_samples_validation,\n        \"trainval\": train_count + num_samples_validation,\n        \"test\": test_count - num_samples_validation,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    def preprocess_fn(tensors):\n      # For consistency with other datasets, image needs to have three channels.\n      image = tf.tile(tensors[\"image\"], [1, 1, 3])\n      return dict(image=image, label=tensors[predicted_attribute])\n\n    info = dataset_builder.info\n    super(SmallNORBData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # We extract the attribute we want to predict in the preprocessing.\n        base_preprocess_fn=preprocess_fn,\n        num_classes=info.features[predicted_attribute].num_classes)", ""]}
{"filename": "src/data/vtab_datasets/eurosat.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements EurosatData class.\"\"\"\n", "\"\"\"Implements EurosatData class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow_datasets as tfds\n\nfrom . import base as base\nfrom .registry import Registry", "from . import base as base\nfrom .registry import Registry\n\nTRAIN_SPLIT_PERCENT = 60\nVALIDATION_SPLIT_PERCENT = 20\nTEST_SPLIT_PERCENT = 20\n\n\n@Registry.register(\"data.eurosat\", \"class\")\nclass EurosatData(base.ImageTfdsData):\n  \"\"\"Provides EuroSat dataset.\n\n  EuroSAT dataset is based on Sentinel-2 satellite images covering 13 spectral\n  bands and consisting of 10 classes with 27000 labeled and\n  geo-referenced samples.\n\n  URL: https://github.com/phelber/eurosat\n  \"\"\"\n\n  def __init__(self, subset=\"rgb\", data_key=\"image\", data_dir=None):\n    dataset_name = \"eurosat/{}:2.*.*\".format(subset)\n    dataset_builder = tfds.builder(dataset_name, data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    # Example counts are retrieved from the tensorflow dataset info.\n    num_examples = dataset_builder.info.splits[tfds.Split.TRAIN].num_examples\n    train_count = num_examples * TRAIN_SPLIT_PERCENT // 100\n    val_count = num_examples * VALIDATION_SPLIT_PERCENT // 100\n    test_count = num_examples * TEST_SPLIT_PERCENT // 100\n\n    tfds_splits = {\n        \"train\":\n            \"train[:{}]\".format(train_count),\n        \"val\":\n            \"train[{}:{}]\".format(train_count, train_count+val_count),\n        \"trainval\":\n            \"train[:{}]\".format(train_count+val_count),\n        \"test\":\n            \"train[{}:]\".format(train_count+val_count),\n        \"train800\":\n            \"train[:800]\",\n        \"val200\":\n            \"train[{}:{}]\".format(train_count, train_count+200),\n        \"train800val200\":\n            \"train[:800]+train[{}:{}]\".format(train_count, train_count+200),\n    }\n\n    # Creates a dict with example counts for each split.\n    num_samples_splits = {\n        \"train\": train_count,\n        \"val\": val_count,\n        \"trainval\": train_count + val_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    num_channels = 3\n    if data_key == \"sentinel2\":\n      num_channels = 13\n\n    super(EurosatData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=100,\n        shuffle_buffer_size=10000,\n        base_preprocess_fn=base.make_get_and_cast_tensors_fn({\n            data_key: (\"image\", None),\n            \"label\": (\"label\", None),\n        }),\n        image_key=data_key,\n        num_channels=num_channels,\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", "@Registry.register(\"data.eurosat\", \"class\")\nclass EurosatData(base.ImageTfdsData):\n  \"\"\"Provides EuroSat dataset.\n\n  EuroSAT dataset is based on Sentinel-2 satellite images covering 13 spectral\n  bands and consisting of 10 classes with 27000 labeled and\n  geo-referenced samples.\n\n  URL: https://github.com/phelber/eurosat\n  \"\"\"\n\n  def __init__(self, subset=\"rgb\", data_key=\"image\", data_dir=None):\n    dataset_name = \"eurosat/{}:2.*.*\".format(subset)\n    dataset_builder = tfds.builder(dataset_name, data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n\n    # Example counts are retrieved from the tensorflow dataset info.\n    num_examples = dataset_builder.info.splits[tfds.Split.TRAIN].num_examples\n    train_count = num_examples * TRAIN_SPLIT_PERCENT // 100\n    val_count = num_examples * VALIDATION_SPLIT_PERCENT // 100\n    test_count = num_examples * TEST_SPLIT_PERCENT // 100\n\n    tfds_splits = {\n        \"train\":\n            \"train[:{}]\".format(train_count),\n        \"val\":\n            \"train[{}:{}]\".format(train_count, train_count+val_count),\n        \"trainval\":\n            \"train[:{}]\".format(train_count+val_count),\n        \"test\":\n            \"train[{}:]\".format(train_count+val_count),\n        \"train800\":\n            \"train[:800]\",\n        \"val200\":\n            \"train[{}:{}]\".format(train_count, train_count+200),\n        \"train800val200\":\n            \"train[:800]+train[{}:{}]\".format(train_count, train_count+200),\n    }\n\n    # Creates a dict with example counts for each split.\n    num_samples_splits = {\n        \"train\": train_count,\n        \"val\": val_count,\n        \"trainval\": train_count + val_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    num_channels = 3\n    if data_key == \"sentinel2\":\n      num_channels = 13\n\n    super(EurosatData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=100,\n        shuffle_buffer_size=10000,\n        base_preprocess_fn=base.make_get_and_cast_tensors_fn({\n            data_key: (\"image\", None),\n            \"label\": (\"label\", None),\n        }),\n        image_key=data_key,\n        num_channels=num_channels,\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", ""]}
{"filename": "src/data/vtab_datasets/dsprites.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements the DSprites data class.\"\"\"\n", "\"\"\"Implements the DSprites data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\nimport tensorflow_datasets as tfds\n\nfrom . import base as base", "\nfrom . import base as base\nfrom .registry import Registry\n\n\n# These constants specify the percentage of data that is used to create custom\n# train/val splits. Specifically, TRAIN_SPLIT_PERCENT% of the data set is used\n# as a new training split and VAL_SPLIT_PERCENT% is used for validation.\n# The rest is used for testing.\nTRAIN_SPLIT_PERCENT = 80", "# The rest is used for testing.\nTRAIN_SPLIT_PERCENT = 80\nVAL_SPLIT_PERCENT = 10\n\n\n@Registry.register(\"data.dsprites\", \"class\")\nclass DSpritesData(base.ImageTfdsData):\n  \"\"\"Provides the DSprites data set.\n\n  DSprites only comes with a training set. Therefore, the training, validation,\n  and test set are split out of the original training set.\n\n  For additional details and usage, see the base class.\n\n  The data set page is https://github.com/deepmind/dsprites-dataset/.\n  \"\"\"\n\n  def __init__(self, predicted_attribute, num_classes=None, data_dir=None):\n    dataset_builder = tfds.builder(\"dsprites:2.*.*\", data_dir=data_dir)\n    dataset_builder.download_and_prepare()\n    info = dataset_builder.info\n\n    if predicted_attribute not in dataset_builder.info.features:\n      raise ValueError(\n          \"{} is not a valid attribute to predict.\".format(predicted_attribute))\n\n    # If num_classes is set, we group together nearby integer values to arrive\n    # at the desired number of classes. This is useful for example for grouping\n    # together different spatial positions.\n    num_original_classes = info.features[predicted_attribute].num_classes\n    if num_classes is None:\n      num_classes = num_original_classes\n    if not isinstance(num_classes, int) or num_classes <= 1 or (\n        num_classes > num_original_classes):\n      raise ValueError(\n          \"The number of classes should be None or in [2, ..., num_classes].\")\n    class_division_factor = float(num_original_classes) / num_classes\n\n    # Creates a dict with example counts for each split.\n    num_total = dataset_builder.info.splits[\"train\"].num_examples\n    num_samples_train = TRAIN_SPLIT_PERCENT * num_total // 100\n    num_samples_val = VAL_SPLIT_PERCENT * num_total // 100\n    num_samples_splits = {\n        \"train\": num_samples_train,\n        \"val\": num_samples_val,\n        \"trainval\": num_samples_val + num_samples_train,\n        \"test\": num_total - num_samples_val - num_samples_train,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"train\": \"train[:{}]\".format(num_samples_splits[\"train\"]),\n        \"val\": \"train[{}:{}]\".format(num_samples_splits[\"train\"],\n                                     num_samples_splits[\"trainval\"]),\n        \"trainval\": \"train[:{}]\".format(num_samples_splits[\"trainval\"]),\n        \"test\": \"train[{}:]\".format(num_samples_splits[\"trainval\"]),\n        \"train800\": \"train[:800]\",\n        \"val200\": \"train[{}:{}]\".format(num_samples_splits[\"train\"],\n                                        num_samples_splits[\"train\"]+200),\n        \"train800val200\": \"train[:800]+train[{}:{}]\".format(\n            num_samples_splits[\"train\"], num_samples_splits[\"train\"]+200),\n    }\n\n    def preprocess_fn(tensors):\n      # For consistency with other datasets, image needs to have three channels\n      # and be in [0, 255).\n      images = tf.tile(tensors[\"image\"], [1, 1, 3]) * 255\n      label = tf.cast(\n          tf.math.floordiv(\n              tf.cast(tensors[predicted_attribute], tf.float32),\n              class_division_factor), info.features[predicted_attribute].dtype)\n      return dict(image=images, label=label)\n\n    super(DSpritesData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # We extract the attribute we want to predict in the preprocessing.\n        base_preprocess_fn=preprocess_fn,\n        num_classes=num_classes)", ""]}
{"filename": "src/data/vtab_datasets/cifar.py", "chunked_list": ["# coding=utf-8\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n# Copyright 2019 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Implements Cifar data class.\"\"\"\n", "\"\"\"Implements Cifar data class.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom . import base as base\nfrom .registry import Registry\nimport tensorflow_datasets as tfds\n\n# This constant specifies the percentage of data that is used to create custom", "\n# This constant specifies the percentage of data that is used to create custom\n# train/val splits. Specifically, TRAIN_SPLIT_PERCENT% of the official training\n# split is used as a new training split and the rest is used for validation.\nTRAIN_SPLIT_PERCENT = 90\n\n\n@Registry.register(\"data.cifar\", \"class\")\nclass CifarData(base.ImageTfdsData):\n  \"\"\"Provides Cifar10 or Cifar100 data.\n\n  Cifar comes only with a training and test set. Therefore, the validation set\n  is split out of the original training set, and the remaining examples are used\n  as the \"train\" split. The \"trainval\" split corresponds to the original\n  training set.\n\n  For additional details and usage, see the base class.\n  \"\"\"\n\n  def __init__(self, num_classes=10, data_dir=None, train_split_percent=None):\n\n    if num_classes == 10:\n      dataset_builder = tfds.builder(\"cifar10:3.*.*\", data_dir=data_dir)\n    elif num_classes == 100:\n      dataset_builder = tfds.builder(\"cifar100:3.*.*\", data_dir=data_dir)\n    else:\n      raise ValueError(\n          \"Number of classes must be 10 or 100, got {}\".format(num_classes))\n\n    dataset_builder.download_and_prepare()\n\n    train_split_percent = train_split_percent or TRAIN_SPLIT_PERCENT\n\n    # Creates a dict with example counts for each split.\n    trainval_count = dataset_builder.info.splits[\"train\"].num_examples\n    test_count = dataset_builder.info.splits[\"test\"].num_examples\n    num_samples_splits = {\n        \"train\": (train_split_percent * trainval_count) // 100,\n        \"val\": trainval_count - (train_split_percent * trainval_count) // 100,\n        \"trainval\": trainval_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"train\": \"train[:{}]\".format(num_samples_splits[\"train\"]),\n        \"val\": \"train[{}:]\".format(num_samples_splits[\"train\"]),\n        \"trainval\": \"train\",\n        \"test\": \"test\",\n        \"train800\": \"train[:800]\",\n        \"val200\": \"train[{}:{}]\".format(\n            num_samples_splits[\"train\"], num_samples_splits[\"train\"]+200),\n        \"train800val200\": \"train[:800]+train[{}:{}]\".format(\n            num_samples_splits[\"train\"], num_samples_splits[\"train\"]+200),\n    }\n\n    super(CifarData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Export only image and label tensors with their original types.\n        base_preprocess_fn=base.make_get_tensors_fn([\"image\", \"label\", \"id\"]),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", "class CifarData(base.ImageTfdsData):\n  \"\"\"Provides Cifar10 or Cifar100 data.\n\n  Cifar comes only with a training and test set. Therefore, the validation set\n  is split out of the original training set, and the remaining examples are used\n  as the \"train\" split. The \"trainval\" split corresponds to the original\n  training set.\n\n  For additional details and usage, see the base class.\n  \"\"\"\n\n  def __init__(self, num_classes=10, data_dir=None, train_split_percent=None):\n\n    if num_classes == 10:\n      dataset_builder = tfds.builder(\"cifar10:3.*.*\", data_dir=data_dir)\n    elif num_classes == 100:\n      dataset_builder = tfds.builder(\"cifar100:3.*.*\", data_dir=data_dir)\n    else:\n      raise ValueError(\n          \"Number of classes must be 10 or 100, got {}\".format(num_classes))\n\n    dataset_builder.download_and_prepare()\n\n    train_split_percent = train_split_percent or TRAIN_SPLIT_PERCENT\n\n    # Creates a dict with example counts for each split.\n    trainval_count = dataset_builder.info.splits[\"train\"].num_examples\n    test_count = dataset_builder.info.splits[\"test\"].num_examples\n    num_samples_splits = {\n        \"train\": (train_split_percent * trainval_count) // 100,\n        \"val\": trainval_count - (train_split_percent * trainval_count) // 100,\n        \"trainval\": trainval_count,\n        \"test\": test_count,\n        \"train800\": 800,\n        \"val200\": 200,\n        \"train800val200\": 1000,\n    }\n\n    # Defines dataset specific train/val/trainval/test splits.\n    tfds_splits = {\n        \"train\": \"train[:{}]\".format(num_samples_splits[\"train\"]),\n        \"val\": \"train[{}:]\".format(num_samples_splits[\"train\"]),\n        \"trainval\": \"train\",\n        \"test\": \"test\",\n        \"train800\": \"train[:800]\",\n        \"val200\": \"train[{}:{}]\".format(\n            num_samples_splits[\"train\"], num_samples_splits[\"train\"]+200),\n        \"train800val200\": \"train[:800]+train[{}:{}]\".format(\n            num_samples_splits[\"train\"], num_samples_splits[\"train\"]+200),\n    }\n\n    super(CifarData, self).__init__(\n        dataset_builder=dataset_builder,\n        tfds_splits=tfds_splits,\n        num_samples_splits=num_samples_splits,\n        num_preprocessing_threads=400,\n        shuffle_buffer_size=10000,\n        # Note: Export only image and label tensors with their original types.\n        base_preprocess_fn=base.make_get_tensors_fn([\"image\", \"label\", \"id\"]),\n        num_classes=dataset_builder.info.features[\"label\"].num_classes)", ""]}
{"filename": "src/data/datasets/tf_dataset.py", "chunked_list": ["#!/usr/bin/env python3\n\n\"\"\"a dataset that handles output of tf.data: support datasets from VTAB\"\"\"\nimport functools\nimport tensorflow.compat.v1 as tf\nimport torch\nimport torch.utils.data\nimport numpy as np\n\nfrom collections import Counter", "\nfrom collections import Counter\nfrom torch import Tensor\n\nfrom ..vtab_datasets import base\n# pylint: disable=unused-import\nfrom ..vtab_datasets import caltech\nfrom ..vtab_datasets import cifar\nfrom ..vtab_datasets import clevr\nfrom ..vtab_datasets import diabetic_retinopathy", "from ..vtab_datasets import clevr\nfrom ..vtab_datasets import diabetic_retinopathy\nfrom ..vtab_datasets import dmlab\nfrom ..vtab_datasets import dsprites\nfrom ..vtab_datasets import dtd\nfrom ..vtab_datasets import eurosat\nfrom ..vtab_datasets import kitti\nfrom ..vtab_datasets import oxford_flowers102\nfrom ..vtab_datasets import oxford_iiit_pet\nfrom ..vtab_datasets import patch_camelyon", "from ..vtab_datasets import oxford_iiit_pet\nfrom ..vtab_datasets import patch_camelyon\nfrom ..vtab_datasets import resisc45\nfrom ..vtab_datasets import smallnorb\nfrom ..vtab_datasets import sun397\nfrom ..vtab_datasets import svhn\nfrom ..vtab_datasets.registry import Registry\n\nfrom ...utils import logging\nlogger = logging.get_logger(\"visual_prompt\")", "from ...utils import logging\nlogger = logging.get_logger(\"visual_prompt\")\ntf.config.experimental.set_visible_devices([], 'GPU')  # set tensorflow to not use gpu  # noqa\nDATASETS = [\n    'caltech101',\n    'cifar(num_classes=100)',\n    'dtd',\n    'oxford_flowers102',\n    'oxford_iiit_pet',\n    'patch_camelyon',", "    'oxford_iiit_pet',\n    'patch_camelyon',\n    'sun397',\n    'svhn',\n    'resisc45',\n    'eurosat',\n    'dmlab',\n    'kitti(task=\"closest_vehicle_distance\")',\n    'smallnorb(predicted_attribute=\"label_azimuth\")',\n    'smallnorb(predicted_attribute=\"label_elevation\")',", "    'smallnorb(predicted_attribute=\"label_azimuth\")',\n    'smallnorb(predicted_attribute=\"label_elevation\")',\n    'dsprites(predicted_attribute=\"label_x_position\",num_classes=16)',\n    'dsprites(predicted_attribute=\"label_orientation\",num_classes=16)',\n    'clevr(task=\"closest_object_distance\")',\n    'clevr(task=\"count_all\")',\n    'diabetic_retinopathy(config=\"btgraham-300\")'\n]\n\n\nclass TFDataset(torch.utils.data.Dataset):\n    def __init__(self, cfg, split):\n        assert split in {\n            \"train\",\n            \"val\",\n            \"test\",\n            \"trainval\"\n        }, \"Split '{}' not supported for {} dataset\".format(\n            split, cfg.DATA.NAME)\n        logger.info(\"Constructing {} dataset {}...\".format(\n            cfg.DATA.NAME, split))\n\n        self.cfg = cfg\n        self._split = split\n        self.name = cfg.DATA.NAME\n\n        self.img_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n        self.img_std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n        self.get_data(cfg, split)\n\n    def get_data(self, cfg, split):\n        tf_data = build_tf_dataset(cfg, split)\n        data_list = list(tf_data)  # a list of tuples\n\n        self._image_tensor_list = [t[0].numpy().squeeze() for t in data_list]\n        self._targets = [int(t[1].numpy()[0]) for t in data_list]\n        self._class_ids = sorted(list(set(self._targets)))\n\n        logger.info(\"Number of images: {}\".format(len(self._image_tensor_list)))\n        logger.info(\"Number of classes: {} / {}\".format(\n            len(self._class_ids), self.get_class_num()))\n\n        del data_list\n        del tf_data\n\n    def get_info(self):\n        num_imgs = len(self._image_tensor_list)\n        return num_imgs, self.get_class_num()\n\n    def get_class_num(self):\n        return self.cfg.DATA.NUMBER_CLASSES\n\n    def get_class_weights(self, weight_type):\n        \"\"\"get a list of class weight, return a list float\"\"\"\n        if \"train\" not in self._split:\n            raise ValueError(\n                \"only getting training class distribution, \" + \\\n                \"got split {} instead\".format(self._split)\n            )\n\n        cls_num = self.get_class_num()\n        if weight_type == \"none\":\n            return [1.0] * cls_num\n\n        id2counts = Counter(self._class_ids)\n        assert len(id2counts) == cls_num\n        num_per_cls = np.array([id2counts[i] for i in self._class_ids])\n\n        if weight_type == 'inv':\n            mu = -1.0\n        elif weight_type == 'inv_sqrt':\n            mu = -0.5\n        weight_list = num_per_cls ** mu\n        weight_list = np.divide(\n            weight_list, np.linalg.norm(weight_list, 1)) * cls_num\n        return weight_list.tolist()\n\n    def __getitem__(self, index):\n        # Load the image\n        label = self._targets[index]\n        im = to_torch_imgs(\n            self._image_tensor_list[index], self.img_mean, self.img_std)\n\n        if self._split == \"train\":\n            index = index\n        else:\n            index = f\"{self._split}{index}\"\n        sample = {\n            \"image\": im,\n            \"label\": label,\n            # \"id\": index\n        }\n        return sample\n\n    def __len__(self):\n        return len(self._targets)", "\n\nclass TFDataset(torch.utils.data.Dataset):\n    def __init__(self, cfg, split):\n        assert split in {\n            \"train\",\n            \"val\",\n            \"test\",\n            \"trainval\"\n        }, \"Split '{}' not supported for {} dataset\".format(\n            split, cfg.DATA.NAME)\n        logger.info(\"Constructing {} dataset {}...\".format(\n            cfg.DATA.NAME, split))\n\n        self.cfg = cfg\n        self._split = split\n        self.name = cfg.DATA.NAME\n\n        self.img_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n        self.img_std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n        self.get_data(cfg, split)\n\n    def get_data(self, cfg, split):\n        tf_data = build_tf_dataset(cfg, split)\n        data_list = list(tf_data)  # a list of tuples\n\n        self._image_tensor_list = [t[0].numpy().squeeze() for t in data_list]\n        self._targets = [int(t[1].numpy()[0]) for t in data_list]\n        self._class_ids = sorted(list(set(self._targets)))\n\n        logger.info(\"Number of images: {}\".format(len(self._image_tensor_list)))\n        logger.info(\"Number of classes: {} / {}\".format(\n            len(self._class_ids), self.get_class_num()))\n\n        del data_list\n        del tf_data\n\n    def get_info(self):\n        num_imgs = len(self._image_tensor_list)\n        return num_imgs, self.get_class_num()\n\n    def get_class_num(self):\n        return self.cfg.DATA.NUMBER_CLASSES\n\n    def get_class_weights(self, weight_type):\n        \"\"\"get a list of class weight, return a list float\"\"\"\n        if \"train\" not in self._split:\n            raise ValueError(\n                \"only getting training class distribution, \" + \\\n                \"got split {} instead\".format(self._split)\n            )\n\n        cls_num = self.get_class_num()\n        if weight_type == \"none\":\n            return [1.0] * cls_num\n\n        id2counts = Counter(self._class_ids)\n        assert len(id2counts) == cls_num\n        num_per_cls = np.array([id2counts[i] for i in self._class_ids])\n\n        if weight_type == 'inv':\n            mu = -1.0\n        elif weight_type == 'inv_sqrt':\n            mu = -0.5\n        weight_list = num_per_cls ** mu\n        weight_list = np.divide(\n            weight_list, np.linalg.norm(weight_list, 1)) * cls_num\n        return weight_list.tolist()\n\n    def __getitem__(self, index):\n        # Load the image\n        label = self._targets[index]\n        im = to_torch_imgs(\n            self._image_tensor_list[index], self.img_mean, self.img_std)\n\n        if self._split == \"train\":\n            index = index\n        else:\n            index = f\"{self._split}{index}\"\n        sample = {\n            \"image\": im,\n            \"label\": label,\n            # \"id\": index\n        }\n        return sample\n\n    def __len__(self):\n        return len(self._targets)", "\n\ndef preprocess_fn(data, size=224, input_range=(0.0, 1.0)):\n    image = data[\"image\"]\n    image = tf.image.resize(image, [size, size])\n\n    image = tf.cast(image, tf.float32) / 255.0\n    image = image * (input_range[1] - input_range[0]) + input_range[0]\n\n    data[\"image\"] = image\n    return data", "\n\ndef build_tf_dataset(cfg, mode):\n    \"\"\"\n    Builds a tf data instance, then transform to a list of tensors and labels\n    \"\"\"\n\n    if mode not in [\"train\", \"val\", \"test\", \"trainval\"]:\n        raise ValueError(\"The input pipeline supports `train`, `val`, `test`.\"\n                         \"Provided mode is {}\".format(mode))\n\n    vtab_dataname = cfg.DATA.NAME.split(\"vtab-\")[-1]\n    data_dir = cfg.DATA.DATAPATH\n    if vtab_dataname in DATASETS:\n        data_cls = Registry.lookup(\"data.\" + vtab_dataname)\n        vtab_tf_dataloader = data_cls(data_dir=data_dir)\n    else:\n        raise ValueError(\"Unknown type for \\\"dataset\\\" field: {}\".format(\n            type(vtab_dataname)))\n\n    split_name_dict = {\n        \"dataset_train_split_name\": \"train800\",\n        \"dataset_val_split_name\": \"val200\",\n        \"dataset_trainval_split_name\": \"train800val200\",\n        \"dataset_test_split_name\": \"test\",\n    }\n\n    def _dict_to_tuple(batch):\n        return batch['image'], batch['label']\n\n    return vtab_tf_dataloader.get_tf_data(\n        batch_size=1,  # data_params[\"batch_size\"],\n        drop_remainder=False,\n        split_name=split_name_dict[f\"dataset_{mode}_split_name\"],\n        preprocess_fn=functools.partial(\n            preprocess_fn,\n            input_range=(0.0, 1.0),\n            size=cfg.DATA.CROPSIZE,\n            ),\n        for_eval=mode != \"train\",  # handles shuffling\n        shuffle_buffer_size=1000,\n        prefetch=1,\n        train_examples=None,\n        epochs=1  # setting epochs to 1 make sure it returns one copy of the dataset\n    ).map(_dict_to_tuple)  # return a PrefetchDataset object. (which does not have much documentation to go on)", "\n\ndef to_torch_imgs(img: np.ndarray, mean: Tensor, std: Tensor) -> Tensor:\n    t_img: Tensor = torch.from_numpy(np.transpose(img, (2, 0, 1)))\n    t_img -= mean\n    t_img /= std\n\n    return t_img\n", ""]}
{"filename": "src/data/datasets/json_dataset.py", "chunked_list": ["#!/usr/bin/env python3\n\n\"\"\"JSON dataset: support CUB, NABrids, Flower, Dogs and Cars\"\"\"\n\nimport os\nimport torch\nimport torch.utils.data\nimport torchvision as tv\nimport numpy as np\nfrom collections import Counter", "import numpy as np\nfrom collections import Counter\n\nfrom ..transforms import get_transforms\nfrom ...utils import logging\nfrom ...utils.io_utils import read_json\nlogger = logging.get_logger(\"visual_prompt\")\n\n\nclass JSONDataset(torch.utils.data.Dataset):\n    def __init__(self, cfg, split):\n        assert split in {\n            \"train\",\n            \"val\",\n            \"test\",\n        }, \"Split '{}' not supported for {} dataset\".format(\n            split, cfg.DATA.NAME)\n        logger.info(\"Constructing {} dataset {}...\".format(\n            cfg.DATA.NAME, split))\n\n        self.cfg = cfg\n        self._split = split\n        self.name = cfg.DATA.NAME\n        self.data_dir = cfg.DATA.DATAPATH\n        self.data_percentage = cfg.DATA.PERCENTAGE\n        self._construct_imdb(cfg)\n        self.transform = get_transforms(split, cfg.DATA.CROPSIZE)\n    \n    def get_anno(self):\n        anno_path = os.path.join(self.data_dir, \"{}.json\".format(self._split))\n        if \"train\" in self._split:\n            if self.data_percentage < 1.0:\n                anno_path = os.path.join(\n                    self.data_dir,\n                    \"{}_{}.json\".format(self._split, self.data_percentage)\n                )\n        assert os.path.exists(anno_path), \"{} dir not found\".format(anno_path)\n\n        return read_json(anno_path)\n\n    def get_imagedir(self):\n        raise NotImplementedError()\n\n    def _construct_imdb(self, cfg):\n        \"\"\"Constructs the imdb.\"\"\"\n\n        img_dir = self.get_imagedir()\n        assert os.path.exists(img_dir), \"{} dir not found\".format(img_dir)\n\n        anno = self.get_anno()\n        # Map class ids to contiguous ids\n        self._class_ids = sorted(list(set(anno.values())))\n        self._class_id_cont_id = {v: i for i, v in enumerate(self._class_ids)}\n\n        # Construct the image db\n        self._imdb = []\n        for img_name, cls_id in anno.items():\n            cont_id = self._class_id_cont_id[cls_id]\n            im_path = os.path.join(img_dir, img_name)\n            self._imdb.append({\"im_path\": im_path, \"class\": cont_id})\n\n        logger.info(\"Number of images: {}\".format(len(self._imdb)))\n        logger.info(\"Number of classes: {}\".format(len(self._class_ids)))\n\n    def get_info(self):\n        num_imgs = len(self._imdb)\n        return num_imgs, self.get_class_num(), self.name\n\n    def get_class_num(self):\n        return self.cfg.DATA.NUMBER_CLASSES\n        # return len(self._class_ids)\n\n    def get_class_weights(self, weight_type):\n        \"\"\"get a list of class weight, return a list float\"\"\"\n        if \"train\" not in self._split:\n            raise ValueError(\n                \"only getting training class distribution, \" + \\\n                \"got split {} instead\".format(self._split)\n            )\n\n        cls_num = self.get_class_num()\n        if weight_type == \"none\":\n            return [1.0] * cls_num\n\n        id2counts = Counter(self._class_ids)\n        assert len(id2counts) == cls_num\n        num_per_cls = np.array([id2counts[i] for i in self._class_ids])\n\n        if weight_type == 'inv':\n            mu = -1.0\n        elif weight_type == 'inv_sqrt':\n            mu = -0.5\n        weight_list = num_per_cls ** mu\n        weight_list = np.divide(\n            weight_list, np.linalg.norm(weight_list, 1)) * cls_num\n        return weight_list.tolist()\n\n    def __getitem__(self, index):\n        # Load the image\n        im = tv.datasets.folder.default_loader(self._imdb[index][\"im_path\"])\n        label = self._imdb[index][\"class\"]\n        im = self.transform(im)\n        if self._split == \"train\":\n            index = index\n        else:\n            index = f\"{self._split}{index}\"\n        sample = {\n            \"image\": im,\n            \"label\": label,\n            # \"id\": index\n        }\n        return sample\n\n    def __len__(self):\n        return len(self._imdb)", "\nclass JSONDataset(torch.utils.data.Dataset):\n    def __init__(self, cfg, split):\n        assert split in {\n            \"train\",\n            \"val\",\n            \"test\",\n        }, \"Split '{}' not supported for {} dataset\".format(\n            split, cfg.DATA.NAME)\n        logger.info(\"Constructing {} dataset {}...\".format(\n            cfg.DATA.NAME, split))\n\n        self.cfg = cfg\n        self._split = split\n        self.name = cfg.DATA.NAME\n        self.data_dir = cfg.DATA.DATAPATH\n        self.data_percentage = cfg.DATA.PERCENTAGE\n        self._construct_imdb(cfg)\n        self.transform = get_transforms(split, cfg.DATA.CROPSIZE)\n    \n    def get_anno(self):\n        anno_path = os.path.join(self.data_dir, \"{}.json\".format(self._split))\n        if \"train\" in self._split:\n            if self.data_percentage < 1.0:\n                anno_path = os.path.join(\n                    self.data_dir,\n                    \"{}_{}.json\".format(self._split, self.data_percentage)\n                )\n        assert os.path.exists(anno_path), \"{} dir not found\".format(anno_path)\n\n        return read_json(anno_path)\n\n    def get_imagedir(self):\n        raise NotImplementedError()\n\n    def _construct_imdb(self, cfg):\n        \"\"\"Constructs the imdb.\"\"\"\n\n        img_dir = self.get_imagedir()\n        assert os.path.exists(img_dir), \"{} dir not found\".format(img_dir)\n\n        anno = self.get_anno()\n        # Map class ids to contiguous ids\n        self._class_ids = sorted(list(set(anno.values())))\n        self._class_id_cont_id = {v: i for i, v in enumerate(self._class_ids)}\n\n        # Construct the image db\n        self._imdb = []\n        for img_name, cls_id in anno.items():\n            cont_id = self._class_id_cont_id[cls_id]\n            im_path = os.path.join(img_dir, img_name)\n            self._imdb.append({\"im_path\": im_path, \"class\": cont_id})\n\n        logger.info(\"Number of images: {}\".format(len(self._imdb)))\n        logger.info(\"Number of classes: {}\".format(len(self._class_ids)))\n\n    def get_info(self):\n        num_imgs = len(self._imdb)\n        return num_imgs, self.get_class_num(), self.name\n\n    def get_class_num(self):\n        return self.cfg.DATA.NUMBER_CLASSES\n        # return len(self._class_ids)\n\n    def get_class_weights(self, weight_type):\n        \"\"\"get a list of class weight, return a list float\"\"\"\n        if \"train\" not in self._split:\n            raise ValueError(\n                \"only getting training class distribution, \" + \\\n                \"got split {} instead\".format(self._split)\n            )\n\n        cls_num = self.get_class_num()\n        if weight_type == \"none\":\n            return [1.0] * cls_num\n\n        id2counts = Counter(self._class_ids)\n        assert len(id2counts) == cls_num\n        num_per_cls = np.array([id2counts[i] for i in self._class_ids])\n\n        if weight_type == 'inv':\n            mu = -1.0\n        elif weight_type == 'inv_sqrt':\n            mu = -0.5\n        weight_list = num_per_cls ** mu\n        weight_list = np.divide(\n            weight_list, np.linalg.norm(weight_list, 1)) * cls_num\n        return weight_list.tolist()\n\n    def __getitem__(self, index):\n        # Load the image\n        im = tv.datasets.folder.default_loader(self._imdb[index][\"im_path\"])\n        label = self._imdb[index][\"class\"]\n        im = self.transform(im)\n        if self._split == \"train\":\n            index = index\n        else:\n            index = f\"{self._split}{index}\"\n        sample = {\n            \"image\": im,\n            \"label\": label,\n            # \"id\": index\n        }\n        return sample\n\n    def __len__(self):\n        return len(self._imdb)", "\n\nclass CUB200Dataset(JSONDataset):\n    \"\"\"CUB_200 dataset.\"\"\"\n\n    def __init__(self, cfg, split):\n        super(CUB200Dataset, self).__init__(cfg, split)\n\n    def get_imagedir(self):\n        return os.path.join(self.data_dir, \"images\")", "\n\nclass CarsDataset(JSONDataset):\n    \"\"\"stanford-cars dataset.\"\"\"\n\n    def __init__(self, cfg, split):\n        super(CarsDataset, self).__init__(cfg, split)\n\n    def get_imagedir(self):\n        return self.data_dir", "\n\nclass DogsDataset(JSONDataset):\n    \"\"\"stanford-dogs dataset.\"\"\"\n\n    def __init__(self, cfg, split):\n        super(DogsDataset, self).__init__(cfg, split)\n\n    def get_imagedir(self):\n        return os.path.join(self.data_dir, \"Images\")", "\n\nclass FlowersDataset(JSONDataset):\n    \"\"\"flowers dataset.\"\"\"\n\n    def __init__(self, cfg, split):\n        super(FlowersDataset, self).__init__(cfg, split)\n\n    def get_imagedir(self):\n        return self.data_dir", "\n\nclass NabirdsDataset(JSONDataset):\n    \"\"\"Nabirds dataset.\"\"\"\n\n    def __init__(self, cfg, split):\n        super(NabirdsDataset, self).__init__(cfg, split)\n\n    def get_imagedir(self):\n        return os.path.join(self.data_dir, \"images\")", ""]}
{"filename": "src/solver/losses.py", "chunked_list": ["#!/usr/bin/env python3\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional\n\nfrom ..utils import logging\nlogger = logging.get_logger(\"visual_prompt\")\n\n\nclass SigmoidLoss(nn.Module):\n    def __init__(self, cfg=None):\n        super(SigmoidLoss, self).__init__()\n\n    def is_single(self):\n        return True\n\n    def is_local(self):\n        return False\n\n    def multi_hot(self, labels: torch.Tensor, nb_classes: int) -> torch.Tensor:\n        labels = labels.unsqueeze(1)  # (batch_size, 1)\n        target = torch.zeros(\n            labels.size(0), nb_classes, device=labels.device\n        ).scatter_(1, labels, 1.)\n        # (batch_size, num_classes)\n        return target\n\n    def loss(\n        self, logits, targets, per_cls_weights,\n        multihot_targets: Optional[bool] = False\n    ):\n        # targets: 1d-tensor of integer\n        # Only support single label at this moment\n        # if len(targets.shape) != 2:\n        num_classes = logits.shape[1]\n        targets = self.multi_hot(targets, num_classes)\n\n        loss = F.binary_cross_entropy_with_logits(\n            logits, targets, reduction=\"none\")\n        # logger.info(f\"loss shape: {loss.shape}\")\n        weight = torch.tensor(\n            per_cls_weights, device=logits.device\n        ).unsqueeze(0)\n        # logger.info(f\"weight shape: {weight.shape}\")\n        loss = torch.mul(loss.to(torch.float32), weight.to(torch.float32))\n        return torch.sum(loss) / targets.shape[0]\n\n    def forward(\n        self, pred_logits, targets, per_cls_weights, multihot_targets=False\n    ):\n        loss = self.loss(\n            pred_logits, targets,  per_cls_weights, multihot_targets)\n        return loss", "\n\nclass SigmoidLoss(nn.Module):\n    def __init__(self, cfg=None):\n        super(SigmoidLoss, self).__init__()\n\n    def is_single(self):\n        return True\n\n    def is_local(self):\n        return False\n\n    def multi_hot(self, labels: torch.Tensor, nb_classes: int) -> torch.Tensor:\n        labels = labels.unsqueeze(1)  # (batch_size, 1)\n        target = torch.zeros(\n            labels.size(0), nb_classes, device=labels.device\n        ).scatter_(1, labels, 1.)\n        # (batch_size, num_classes)\n        return target\n\n    def loss(\n        self, logits, targets, per_cls_weights,\n        multihot_targets: Optional[bool] = False\n    ):\n        # targets: 1d-tensor of integer\n        # Only support single label at this moment\n        # if len(targets.shape) != 2:\n        num_classes = logits.shape[1]\n        targets = self.multi_hot(targets, num_classes)\n\n        loss = F.binary_cross_entropy_with_logits(\n            logits, targets, reduction=\"none\")\n        # logger.info(f\"loss shape: {loss.shape}\")\n        weight = torch.tensor(\n            per_cls_weights, device=logits.device\n        ).unsqueeze(0)\n        # logger.info(f\"weight shape: {weight.shape}\")\n        loss = torch.mul(loss.to(torch.float32), weight.to(torch.float32))\n        return torch.sum(loss) / targets.shape[0]\n\n    def forward(\n        self, pred_logits, targets, per_cls_weights, multihot_targets=False\n    ):\n        loss = self.loss(\n            pred_logits, targets,  per_cls_weights, multihot_targets)\n        return loss", "\n\nclass SoftmaxLoss(SigmoidLoss):\n    def __init__(self, cfg=None):\n        super(SoftmaxLoss, self).__init__()\n\n    def loss(self, logits, targets, per_cls_weights, kwargs):\n        weight = torch.tensor(\n            per_cls_weights, device=logits.device\n        )\n        loss = F.cross_entropy(logits, targets, weight, reduction=\"none\")\n\n        return torch.sum(loss) / targets.shape[0]", "\n\nLOSS = {\n    \"softmax\": SoftmaxLoss,\n}\n\n\ndef build_loss(cfg):\n    loss_name = cfg.SOLVER.LOSS\n    assert loss_name in LOSS, \\\n        f'loss name {loss_name} is not supported'\n    loss_fn = LOSS[loss_name]\n    if not loss_fn:\n        return None\n    else:\n        return loss_fn(cfg)", ""]}
{"filename": "src/solver/optimizer.py", "chunked_list": ["#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n\"\"\"\noptimizer, ref:\nhttps://github.com/huggingface/transformers/blob/master/transformers/optimization.property  #noqa\n\"\"\"\nimport math\n\nimport torch\nfrom fvcore.common.config import CfgNode", "import torch\nfrom fvcore.common.config import CfgNode\nfrom torch.optim import Optimizer\nimport torch.optim as optim\nfrom typing import Any, Callable, Iterable, List, Tuple, Optional\n\nfrom ..utils import logging\nlogger = logging.get_logger(\"visual_prompt\")\n\n\ndef make_optimizer(\n    models: List[Any], train_params: CfgNode\n) -> Optimizer:\n    params = []\n    for model in models:\n        # only include learnable params\n        if train_params.DBG_TRAINABLE:\n            logger.info(\"Trainable params:\")\n\n        for key, value in model.named_parameters():\n            \n            if value.requires_grad:\n\n                if train_params.DBG_TRAINABLE:\n                    logger.info(\"\\t{}, {}, {}\".format(key, value.numel(), value.shape))\n                params.append((key, value))\n\n    if train_params.WEIGHT_DECAY > 0:\n        if train_params.OPTIMIZER == 'adamw':\n\n            no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n            optimizer_grouped_parameters = [\n                {'params': [p for n, p in params\n                            if not any(nd in n for nd in no_decay)],\n                 'weight_decay': 0.01},\n                {'params': [p for n, p in params\n                            if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n            ]\n            optimizer = AdamW(\n                optimizer_grouped_parameters,\n                lr=train_params.BASE_LR,\n            )\n        else:\n            _params = []\n            for p in params:\n                key, value = p\n                # print(key)\n                # if not value.requires_grad:\n                #     continue\n                lr = train_params.BASE_LR\n                weight_decay = train_params.WEIGHT_DECAY\n                if \"last_layer.bias\" in key:\n                    # no regularization (weight decay) for last layer's bias\n                    weight_decay = 0.0\n\n                if train_params.BIAS_MULTIPLIER == 1.:\n                    _params += [{\n                        \"params\": [value],\n                        \"lr\": lr,\n                        \"weight_decay\": weight_decay\n                    }]\n                else:\n                    if \"bias\" in key and \"last_layer.bias\" not in key:\n                        # use updated lr for this param\n                        lr_value = lr * train_params.BIAS_MULTIPLIER\n                    else:\n                        lr_value = lr\n\n                    if train_params.DBG_TRAINABLE:\n                        logger.info(\"\\t{}, {:.4f}\".format(key, lr_value))\n\n                    _params += [{\n                        \"params\": [value],\n                        \"lr\": lr_value,\n                        \"weight_decay\": weight_decay\n                    }]\n\n            if train_params.OPTIMIZER == 'adam':\n                optimizer = optim.Adam(\n                    _params,\n                    lr=train_params.BASE_LR,\n                    weight_decay=train_params.WEIGHT_DECAY,\n                )\n            else:\n                optimizer = optim.SGD(\n                    _params,\n                    train_params.BASE_LR,\n                    momentum=train_params.MOMENTUM,\n                    weight_decay=train_params.WEIGHT_DECAY\n                )\n        return optimizer\n    else:\n        if train_params.OPTIMIZER == 'adam':\n            optimizer = optim.Adam(\n                model.parameters(),\n                lr=train_params.BASE_LR\n            )\n        else:\n            _params = []\n            for p in params:\n                key, value = p\n\n                lr = train_params.BASE_LR\n\n                if train_params.BIAS_MULTIPLIER == 1.:\n                    _params += [{\n                        \"params\": [value],\n                        \"lr\": lr,\n                    }]\n                else:\n                    if \"bias\" in key and \"last_layer.bias\" not in key:\n                        # use updated lr for this param\n                        lr_value = lr * train_params.BIAS_MULTIPLIER\n                    else:\n                        lr_value = lr\n\n                    if train_params.DBG_TRAINABLE:\n                        logger.info(\"\\t{}, {:.4f}\".format(key, lr_value))\n\n                    _params += [{\n                        \"params\": [value],\n                        \"lr\": lr_value,\n                    }]\n            optimizer = optim.SGD(\n                _params,\n                train_params.BASE_LR,\n                momentum=train_params.MOMENTUM,\n            )\n        return optimizer", "\n\ndef make_optimizer(\n    models: List[Any], train_params: CfgNode\n) -> Optimizer:\n    params = []\n    for model in models:\n        # only include learnable params\n        if train_params.DBG_TRAINABLE:\n            logger.info(\"Trainable params:\")\n\n        for key, value in model.named_parameters():\n            \n            if value.requires_grad:\n\n                if train_params.DBG_TRAINABLE:\n                    logger.info(\"\\t{}, {}, {}\".format(key, value.numel(), value.shape))\n                params.append((key, value))\n\n    if train_params.WEIGHT_DECAY > 0:\n        if train_params.OPTIMIZER == 'adamw':\n\n            no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n            optimizer_grouped_parameters = [\n                {'params': [p for n, p in params\n                            if not any(nd in n for nd in no_decay)],\n                 'weight_decay': 0.01},\n                {'params': [p for n, p in params\n                            if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n            ]\n            optimizer = AdamW(\n                optimizer_grouped_parameters,\n                lr=train_params.BASE_LR,\n            )\n        else:\n            _params = []\n            for p in params:\n                key, value = p\n                # print(key)\n                # if not value.requires_grad:\n                #     continue\n                lr = train_params.BASE_LR\n                weight_decay = train_params.WEIGHT_DECAY\n                if \"last_layer.bias\" in key:\n                    # no regularization (weight decay) for last layer's bias\n                    weight_decay = 0.0\n\n                if train_params.BIAS_MULTIPLIER == 1.:\n                    _params += [{\n                        \"params\": [value],\n                        \"lr\": lr,\n                        \"weight_decay\": weight_decay\n                    }]\n                else:\n                    if \"bias\" in key and \"last_layer.bias\" not in key:\n                        # use updated lr for this param\n                        lr_value = lr * train_params.BIAS_MULTIPLIER\n                    else:\n                        lr_value = lr\n\n                    if train_params.DBG_TRAINABLE:\n                        logger.info(\"\\t{}, {:.4f}\".format(key, lr_value))\n\n                    _params += [{\n                        \"params\": [value],\n                        \"lr\": lr_value,\n                        \"weight_decay\": weight_decay\n                    }]\n\n            if train_params.OPTIMIZER == 'adam':\n                optimizer = optim.Adam(\n                    _params,\n                    lr=train_params.BASE_LR,\n                    weight_decay=train_params.WEIGHT_DECAY,\n                )\n            else:\n                optimizer = optim.SGD(\n                    _params,\n                    train_params.BASE_LR,\n                    momentum=train_params.MOMENTUM,\n                    weight_decay=train_params.WEIGHT_DECAY\n                )\n        return optimizer\n    else:\n        if train_params.OPTIMIZER == 'adam':\n            optimizer = optim.Adam(\n                model.parameters(),\n                lr=train_params.BASE_LR\n            )\n        else:\n            _params = []\n            for p in params:\n                key, value = p\n\n                lr = train_params.BASE_LR\n\n                if train_params.BIAS_MULTIPLIER == 1.:\n                    _params += [{\n                        \"params\": [value],\n                        \"lr\": lr,\n                    }]\n                else:\n                    if \"bias\" in key and \"last_layer.bias\" not in key:\n                        # use updated lr for this param\n                        lr_value = lr * train_params.BIAS_MULTIPLIER\n                    else:\n                        lr_value = lr\n\n                    if train_params.DBG_TRAINABLE:\n                        logger.info(\"\\t{}, {:.4f}\".format(key, lr_value))\n\n                    _params += [{\n                        \"params\": [value],\n                        \"lr\": lr_value,\n                    }]\n            optimizer = optim.SGD(\n                _params,\n                train_params.BASE_LR,\n                momentum=train_params.MOMENTUM,\n            )\n        return optimizer", "\n\nclass AdamW(Optimizer):\n    \"\"\" Implements Adam algorithm with weight decay fix.\n    Parameters:\n        lr (float): learning rate. Default 1e-3.\n        betas (tuple of 2 floats): Adams beta parameters (b1, b2). Default: (0.9, 0.999)\n        eps (float): Adams epsilon. Default: 1e-6\n        weight_decay (float): Weight decay. Default: 0.0\n        correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\n    \"\"\"\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr: float = 1e-3,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-6,\n        weight_decay: float = 0.0,\n        correct_bias: bool = True\n    ) -> None:\n        if lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n        defaults = {\n            \"lr\": lr, \"betas\": betas, \"eps\": eps,\n            \"weight_decay\": weight_decay, \"correct_bias\": correct_bias\n        }\n        super(AdamW, self).__init__(params, defaults)\n\n    def step(self, closure: Optional[Callable] = None) -> Optional[Callable]:\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \"Adam does not support sparse gradients, \"\n                        \"please consider SparseAdam instead\")\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n                denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                step_size = group['lr']\n                if group['correct_bias']:  # No bias correction for Bert\n                    bias_correction1 = 1.0 - beta1 ** state['step']\n                    bias_correction2 = 1.0 - beta2 ** state['step']\n                    step_size = step_size * math.sqrt(\n                        bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want to decay the weights in a manner that doesn't interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                # Add weight decay at the end (fixed version)\n                if group['weight_decay'] > 0.0:\n                    p.data.add_(-group['lr'] * group['weight_decay'], p.data)\n\n        return loss", ""]}
{"filename": "src/solver/lr_scheduler.py", "chunked_list": ["#!/usr/bin/env python3\nimport math\n\nimport torch.optim as optim\nfrom fvcore.common.config import CfgNode\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\ndef make_scheduler(\n    optimizer: optim.Optimizer, train_params: CfgNode\n) -> LambdaLR:\n    warmup = train_params.WARMUP_EPOCH\n    total_iters = train_params.TOTAL_EPOCH\n\n    if train_params.SCHEDULER == \"cosine\":\n        scheduler = WarmupCosineSchedule(\n            optimizer,\n            warmup_steps=warmup,\n            t_total=total_iters\n        )\n    elif train_params.SCHEDULER == \"cosine_hardrestart\":\n        scheduler = WarmupCosineWithHardRestartsSchedule(\n            optimizer,\n            warmup_steps=warmup,\n            t_total=total_iters\n        )\n\n    elif train_params.SCHEDULER == \"plateau\":\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            \"max\",\n            patience=5,\n            verbose=True,\n            factor=train_params.LR_DECAY_FACTOR,\n        )\n    else:\n        scheduler = None\n    return scheduler", "def make_scheduler(\n    optimizer: optim.Optimizer, train_params: CfgNode\n) -> LambdaLR:\n    warmup = train_params.WARMUP_EPOCH\n    total_iters = train_params.TOTAL_EPOCH\n\n    if train_params.SCHEDULER == \"cosine\":\n        scheduler = WarmupCosineSchedule(\n            optimizer,\n            warmup_steps=warmup,\n            t_total=total_iters\n        )\n    elif train_params.SCHEDULER == \"cosine_hardrestart\":\n        scheduler = WarmupCosineWithHardRestartsSchedule(\n            optimizer,\n            warmup_steps=warmup,\n            t_total=total_iters\n        )\n\n    elif train_params.SCHEDULER == \"plateau\":\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            \"max\",\n            patience=5,\n            verbose=True,\n            factor=train_params.LR_DECAY_FACTOR,\n        )\n    else:\n        scheduler = None\n    return scheduler", "\n\nclass WarmupCosineSchedule(LambdaLR):\n    \"\"\" Linear warmup and then cosine decay.\n        Linearly increases learning rate from 0 to 1 over `warmup_steps`.\n        Decreases learning rate from 1. to 0. over remaining\n            `t_total - warmup_steps` steps following a cosine curve.\n        If `cycles` (default=0.5) is different from default, learning rate\n            follows cosine function after warmup.\n    \"\"\"\n    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n        self.warmup_steps = warmup_steps\n        self.t_total = t_total\n        self.cycles = cycles\n        super(WarmupCosineSchedule, self).__init__(\n            optimizer, self.lr_lambda, last_epoch=last_epoch)\n\n    def lr_lambda(self, step):\n        if step < self.warmup_steps:\n            return float(step) / float(max(1.0, self.warmup_steps))\n        # progress after warmup\n        progress = float(step - self.warmup_steps) / float(max(\n            1, self.t_total - self.warmup_steps))\n        return max(\n            0.0,\n            0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress))\n        )", "\n\nclass WarmupCosineWithHardRestartsSchedule(LambdaLR):\n    \"\"\" Linear warmup and then cosine cycles with hard restarts.\n        Linearly increases learning rate from 0 to 1 over `warmup_steps`.\n        If `cycles` (default=1.) is different from default, learning rate\n            follows `cycles` times a cosine decaying learning rate\n            (with hard restarts).\n    \"\"\"\n    def __init__(self, optimizer, warmup_steps, t_total, cycles=1., last_epoch=-1):\n        self.warmup_steps = warmup_steps\n        self.t_total = t_total\n        self.cycles = cycles\n        super(WarmupCosineWithHardRestartsSchedule, self).__init__(\n            optimizer, self.lr_lambda, last_epoch=last_epoch)\n\n    def lr_lambda(self, step):\n        if step < self.warmup_steps:\n            return float(step) / float(max(1, self.warmup_steps))\n        # progress after warmup\n        progress = float(step - self.warmup_steps) / float(\n            max(1, self.t_total - self.warmup_steps))\n        if progress >= 1.0:\n            return 0.0\n        return max(\n            0.0,\n            0.5 * (1. + math.cos(\n                math.pi * ((float(self.cycles) * progress) % 1.0)))\n        )", ""]}
{"filename": "src/engine/trainer.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\na trainer class\n\"\"\"\nimport datetime\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os", "import torch.nn.functional as F\nimport os\n\nfrom fvcore.common.config import CfgNode\nfrom fvcore.common.checkpoint import Checkpointer\n\nfrom ..engine.evaluator import Evaluator\nfrom ..solver.lr_scheduler import make_scheduler\nfrom ..solver.optimizer import make_optimizer\nfrom ..solver.losses import build_loss", "from ..solver.optimizer import make_optimizer\nfrom ..solver.losses import build_loss\nfrom ..utils import logging\nfrom ..utils.train_utils import AverageMeter, gpu_mem_usage\n\nlogger = logging.get_logger(\"visual_prompt\")\n\n\nclass Trainer():\n    \"\"\"\n    a trainer with below logics:\n\n    1. Build optimizer, scheduler\n    2. Load checkpoints if provided\n    3. Train and eval at each epoch\n    \"\"\"\n    def __init__(\n        self,\n        cfg: CfgNode,\n        model: nn.Module,\n        evaluator: Evaluator,\n        device: torch.device,\n    ) -> None:\n        self.cfg = cfg\n        self.model = model\n        self.device = device\n\n        # solver related\n        logger.info(\"\\tSetting up the optimizer...\")\n        self.optimizer = make_optimizer([self.model], cfg.SOLVER)\n        self.scheduler = make_scheduler(self.optimizer, cfg.SOLVER)\n        self.cls_criterion = build_loss(self.cfg)\n\n        self.checkpointer = Checkpointer(\n            self.model,\n            save_dir=cfg.OUTPUT_DIR,\n            save_to_disk=True\n        )\n\n        if len(cfg.MODEL.WEIGHT_PATH) > 0:\n            # only use this for vtab in-domain experiments\n            checkpointables = [key for key in self.checkpointer.checkpointables if key not in [\"head.last_layer.bias\",  \"head.last_layer.weight\"]]\n            self.checkpointer.load(cfg.MODEL.WEIGHT_PATH, checkpointables)\n            logger.info(f\"Model weight loaded from {cfg.MODEL.WEIGHT_PATH}\")\n\n        self.evaluator = evaluator\n        self.cpu_device = torch.device(\"cpu\")\n\n    def forward_one_batch(self, inputs, targets, is_train):\n        \"\"\"Train a single (full) epoch on the model using the given\n        data loader.\n\n        Args:\n            X: input dict\n            targets\n            is_train: bool\n        Returns:\n            loss\n            outputs: output logits\n        \"\"\"\n        # move data to device\n        inputs = inputs.to(self.device, non_blocking=True)    # (batchsize, 2048)\n        targets = targets.to(self.device, non_blocking=True)  # (batchsize, )\n\n        if self.cfg.DBG:\n            logger.info(f\"shape of inputs: {inputs.shape}\")\n            logger.info(f\"shape of targets: {targets.shape}\")\n\n        # forward\n        with torch.set_grad_enabled(is_train):\n            outputs = self.model(inputs)  # (batchsize, num_cls)\n            if self.cfg.DBG:\n                logger.info(\n                    \"shape of model output: {}, targets: {}\".format(\n                        outputs.shape, targets.shape))\n\n            if self.cls_criterion.is_local() and is_train:\n                self.model.eval()\n                loss = self.cls_criterion(\n                    outputs, targets, self.cls_weights,\n                    self.model, inputs\n                )\n            elif self.cls_criterion.is_local():\n                return torch.tensor(1), outputs\n            else:\n                loss = self.cls_criterion(\n                    outputs, targets, self.cls_weights)\n\n            if loss == float('inf'):\n                logger.info(\n                    \"encountered infinite loss, skip gradient updating for this batch!\"\n                )\n                return -1, -1\n            elif torch.isnan(loss).any():\n                logger.info(\n                    \"encountered nan loss, skip gradient updating for this batch!\"\n                )\n                return -1, -1\n\n        # =======backward and optim step only if in training phase... =========\n        if is_train:\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n        return loss, outputs\n\n    def get_input(self, data):\n        if not isinstance(data[\"image\"], torch.Tensor):\n            for k, v in data.items():\n                data[k] = torch.from_numpy(v)\n\n        inputs = data[\"image\"].float()\n        labels = data[\"label\"]\n        return inputs, labels\n\n    def train_classifier(self, train_loader, val_loader, test_loader):\n        \"\"\"\n        Train a classifier using epoch\n        \"\"\"\n        # save the model prompt if required before training\n        self.model.eval()\n        # self.save_prompt(0)\n        # setup training epoch params\n        total_epoch = self.cfg.SOLVER.TOTAL_EPOCH\n        total_data = len(train_loader)\n        best_epoch = -1\n        best_metric = 0\n        log_interval = self.cfg.SOLVER.LOG_EVERY_N\n\n        losses = AverageMeter('Loss', ':.4e')\n        batch_time = AverageMeter('Time', ':6.3f')\n        data_time = AverageMeter('Data', ':6.3f')\n\n        self.cls_weights = train_loader.dataset.get_class_weights(\n            self.cfg.DATA.CLASS_WEIGHTS_TYPE)\n        # logger.info(f\"class weights: {self.cls_weights}\")\n        patience = 0  # if > self.cfg.SOLVER.PATIENCE, stop training\n\n        for epoch in range(total_epoch):\n            losses.reset()\n            batch_time.reset()\n            data_time.reset()\n\n            lr = self.scheduler.get_lr()[0]\n            logger.info(\n                \"Training {} / {} epoch, with learning rate {}\".format(\n                    epoch + 1, total_epoch, lr\n                )\n            )\n\n            # Enable training mode\n            self.model.train()\n\n            end = time.time()\n\n            for idx, input_data in enumerate(train_loader):\n                if self.cfg.DBG and idx == 20:\n                    # if debugging, only need to see the first few iterations\n                    break\n                \n                X, targets = self.get_input(input_data)\n                # logger.info(X.shape)\n                # logger.info(targets.shape)\n                # measure data loading time\n                data_time.update(time.time() - end)\n\n                train_loss, _ = self.forward_one_batch(X, targets, True)\n\n                if train_loss == -1:\n                    # continue\n                    return None\n\n                losses.update(train_loss.item(), X.shape[0])\n\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                # log during one batch\n                if (idx + 1) % log_interval == 0:\n                    seconds_per_batch = batch_time.val\n                    eta = datetime.timedelta(seconds=int(\n                        seconds_per_batch * (total_data - idx - 1) + seconds_per_batch*total_data*(total_epoch-epoch-1)))\n                    logger.info(\n                        \"\\tTraining {}/{}. train loss: {:.4f},\".format(\n                            idx + 1,\n                            total_data,\n                            train_loss\n                        )\n                        + \"\\t{:.4f} s / batch. (data: {:.2e}). ETA={}, \".format(\n                            seconds_per_batch,\n                            data_time.val,\n                            str(eta),\n                        )\n                        + \"max mem: {:.1f} GB \".format(gpu_mem_usage())\n                    )\n            logger.info(\n                \"Epoch {} / {}: \".format(epoch + 1, total_epoch)\n                + \"avg data time: {:.2e}, avg batch time: {:.4f}, \".format(\n                    data_time.avg, batch_time.avg)\n                + \"average train loss: {:.4f}\".format(losses.avg))\n            \n            # update lr, scheduler.step() must be called after optimizer.step() according to the docs: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate  # noqa\n            self.scheduler.step()\n\n            # Enable eval mode\n            self.model.eval()\n\n            self.save_prompt(epoch + 1)\n\n            # eval at each epoch for single gpu training\n            self.evaluator.update_iteration(epoch)\n            self.eval_classifier(val_loader, \"val\", epoch == total_epoch - 1)\n            if test_loader is not None:\n                self.eval_classifier(\n                    test_loader, \"test\", epoch == total_epoch - 1)\n\n            # check the patience\n            t_name = \"val_\" + val_loader.dataset.name\n            try:\n                curr_acc = self.evaluator.results[f\"epoch_{epoch}\"][\"classification\"][t_name][\"top1\"]\n            except KeyError:\n                return\n\n            if curr_acc > best_metric:\n                best_metric = curr_acc\n                best_epoch = epoch + 1\n                logger.info(\n                    f'Best epoch {best_epoch}: best metric: {best_metric:.3f}')\n                patience = 0\n            else:\n                patience += 1\n            if patience >= self.cfg.SOLVER.PATIENCE:\n                logger.info(\"No improvement. Breaking out of loop.\")\n                break\n\n        # save the last checkpoints\n        if self.cfg.MODEL.SAVE_CKPT:\n            Checkpointer(\n                self.model,\n                save_dir=self.cfg.OUTPUT_DIR,\n                save_to_disk=True\n            ).save(\"last_model\")\n\n    @torch.no_grad()\n    def save_prompt(self, epoch):\n        # only save the prompt embed if below conditions are satisfied\n        if self.cfg.MODEL.PROMPT.SAVE_FOR_EACH_EPOCH:\n            if self.cfg.MODEL.TYPE in [\"vit\", \"ssl-vit\"] and \"prompt\" in self.cfg.MODEL.TRANSFER_TYPE:\n                prompt_embds = self.model.prompt_embeddings.cpu().numpy()\n                out = {\"shallow_prompt\": prompt_embds}\n                if self.cfg.MODEL.PROMPT.DEEP:\n                    deep_embds = self.model.enc.transformer.deep_prompt_embeddings.cpu().numpy()\n                    out[\"deep_prompt\"] = deep_embds\n                torch.save(out, os.path.join(\n                    self.cfg.OUTPUT_DIR, f\"prompt_ep{epoch}.pth\"))\n\n    @torch.no_grad()\n    def eval_classifier(self, data_loader, prefix, save=False):\n        \"\"\"evaluate classifier\"\"\"\n        batch_time = AverageMeter('Time', ':6.3f')\n        data_time = AverageMeter('Data', ':6.3f')\n        losses = AverageMeter('Loss', ':.4e')\n\n        log_interval = self.cfg.SOLVER.LOG_EVERY_N\n        test_name = prefix + \"_\" + data_loader.dataset.name\n        total = len(data_loader)\n\n        # initialize features and target\n        total_logits = []\n        total_targets = []\n\n        for idx, input_data in enumerate(data_loader):\n            end = time.time()\n            X, targets = self.get_input(input_data)\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            if self.cfg.DBG:\n                logger.info(\"during eval: {}\".format(X.shape))\n            loss, outputs = self.forward_one_batch(X, targets, False)\n            if loss == -1:\n                return\n            losses.update(loss, X.shape[0])\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n\n            if (idx + 1) % log_interval == 0:\n                logger.info(\n                    \"\\tTest {}/{}. loss: {:.3f}, {:.4f} s / batch. (data: {:.2e})\".format(  # noqa\n                        idx + 1,\n                        total,\n                        losses.val,\n                        batch_time.val,\n                        data_time.val\n                    ) + \"max mem: {:.5f} GB \".format(gpu_mem_usage())\n                )\n\n            # targets: List[int]\n            total_targets.extend(list(targets.numpy()))\n            total_logits.append(outputs)\n        logger.info(\n            f\"Inference ({prefix}):\"\n            + \"avg data time: {:.2e}, avg batch time: {:.4f}, \".format(\n                data_time.avg, batch_time.avg)\n            + \"average loss: {:.4f}\".format(losses.avg))\n        if self.model.side is not None:\n            logger.info(\n                \"--> side tuning alpha = {:.4f}\".format(self.model.side_alpha))\n        # total_testimages x num_classes\n        joint_logits = torch.cat(total_logits, dim=0).cpu().numpy()\n        self.evaluator.classify(\n            joint_logits, total_targets,\n            test_name, self.cfg.DATA.MULTILABEL,\n        )\n\n        # save the probs and targets\n        if save and self.cfg.MODEL.SAVE_CKPT:\n            out = {\"targets\": total_targets, \"joint_logits\": joint_logits}\n            out_path = os.path.join(\n                self.cfg.OUTPUT_DIR, f\"{test_name}_logits.pth\")\n            torch.save(out, out_path)\n            logger.info(\n                f\"Saved logits and targets for {test_name} at {out_path}\")", "class Trainer():\n    \"\"\"\n    a trainer with below logics:\n\n    1. Build optimizer, scheduler\n    2. Load checkpoints if provided\n    3. Train and eval at each epoch\n    \"\"\"\n    def __init__(\n        self,\n        cfg: CfgNode,\n        model: nn.Module,\n        evaluator: Evaluator,\n        device: torch.device,\n    ) -> None:\n        self.cfg = cfg\n        self.model = model\n        self.device = device\n\n        # solver related\n        logger.info(\"\\tSetting up the optimizer...\")\n        self.optimizer = make_optimizer([self.model], cfg.SOLVER)\n        self.scheduler = make_scheduler(self.optimizer, cfg.SOLVER)\n        self.cls_criterion = build_loss(self.cfg)\n\n        self.checkpointer = Checkpointer(\n            self.model,\n            save_dir=cfg.OUTPUT_DIR,\n            save_to_disk=True\n        )\n\n        if len(cfg.MODEL.WEIGHT_PATH) > 0:\n            # only use this for vtab in-domain experiments\n            checkpointables = [key for key in self.checkpointer.checkpointables if key not in [\"head.last_layer.bias\",  \"head.last_layer.weight\"]]\n            self.checkpointer.load(cfg.MODEL.WEIGHT_PATH, checkpointables)\n            logger.info(f\"Model weight loaded from {cfg.MODEL.WEIGHT_PATH}\")\n\n        self.evaluator = evaluator\n        self.cpu_device = torch.device(\"cpu\")\n\n    def forward_one_batch(self, inputs, targets, is_train):\n        \"\"\"Train a single (full) epoch on the model using the given\n        data loader.\n\n        Args:\n            X: input dict\n            targets\n            is_train: bool\n        Returns:\n            loss\n            outputs: output logits\n        \"\"\"\n        # move data to device\n        inputs = inputs.to(self.device, non_blocking=True)    # (batchsize, 2048)\n        targets = targets.to(self.device, non_blocking=True)  # (batchsize, )\n\n        if self.cfg.DBG:\n            logger.info(f\"shape of inputs: {inputs.shape}\")\n            logger.info(f\"shape of targets: {targets.shape}\")\n\n        # forward\n        with torch.set_grad_enabled(is_train):\n            outputs = self.model(inputs)  # (batchsize, num_cls)\n            if self.cfg.DBG:\n                logger.info(\n                    \"shape of model output: {}, targets: {}\".format(\n                        outputs.shape, targets.shape))\n\n            if self.cls_criterion.is_local() and is_train:\n                self.model.eval()\n                loss = self.cls_criterion(\n                    outputs, targets, self.cls_weights,\n                    self.model, inputs\n                )\n            elif self.cls_criterion.is_local():\n                return torch.tensor(1), outputs\n            else:\n                loss = self.cls_criterion(\n                    outputs, targets, self.cls_weights)\n\n            if loss == float('inf'):\n                logger.info(\n                    \"encountered infinite loss, skip gradient updating for this batch!\"\n                )\n                return -1, -1\n            elif torch.isnan(loss).any():\n                logger.info(\n                    \"encountered nan loss, skip gradient updating for this batch!\"\n                )\n                return -1, -1\n\n        # =======backward and optim step only if in training phase... =========\n        if is_train:\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n        return loss, outputs\n\n    def get_input(self, data):\n        if not isinstance(data[\"image\"], torch.Tensor):\n            for k, v in data.items():\n                data[k] = torch.from_numpy(v)\n\n        inputs = data[\"image\"].float()\n        labels = data[\"label\"]\n        return inputs, labels\n\n    def train_classifier(self, train_loader, val_loader, test_loader):\n        \"\"\"\n        Train a classifier using epoch\n        \"\"\"\n        # save the model prompt if required before training\n        self.model.eval()\n        # self.save_prompt(0)\n        # setup training epoch params\n        total_epoch = self.cfg.SOLVER.TOTAL_EPOCH\n        total_data = len(train_loader)\n        best_epoch = -1\n        best_metric = 0\n        log_interval = self.cfg.SOLVER.LOG_EVERY_N\n\n        losses = AverageMeter('Loss', ':.4e')\n        batch_time = AverageMeter('Time', ':6.3f')\n        data_time = AverageMeter('Data', ':6.3f')\n\n        self.cls_weights = train_loader.dataset.get_class_weights(\n            self.cfg.DATA.CLASS_WEIGHTS_TYPE)\n        # logger.info(f\"class weights: {self.cls_weights}\")\n        patience = 0  # if > self.cfg.SOLVER.PATIENCE, stop training\n\n        for epoch in range(total_epoch):\n            losses.reset()\n            batch_time.reset()\n            data_time.reset()\n\n            lr = self.scheduler.get_lr()[0]\n            logger.info(\n                \"Training {} / {} epoch, with learning rate {}\".format(\n                    epoch + 1, total_epoch, lr\n                )\n            )\n\n            # Enable training mode\n            self.model.train()\n\n            end = time.time()\n\n            for idx, input_data in enumerate(train_loader):\n                if self.cfg.DBG and idx == 20:\n                    # if debugging, only need to see the first few iterations\n                    break\n                \n                X, targets = self.get_input(input_data)\n                # logger.info(X.shape)\n                # logger.info(targets.shape)\n                # measure data loading time\n                data_time.update(time.time() - end)\n\n                train_loss, _ = self.forward_one_batch(X, targets, True)\n\n                if train_loss == -1:\n                    # continue\n                    return None\n\n                losses.update(train_loss.item(), X.shape[0])\n\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                # log during one batch\n                if (idx + 1) % log_interval == 0:\n                    seconds_per_batch = batch_time.val\n                    eta = datetime.timedelta(seconds=int(\n                        seconds_per_batch * (total_data - idx - 1) + seconds_per_batch*total_data*(total_epoch-epoch-1)))\n                    logger.info(\n                        \"\\tTraining {}/{}. train loss: {:.4f},\".format(\n                            idx + 1,\n                            total_data,\n                            train_loss\n                        )\n                        + \"\\t{:.4f} s / batch. (data: {:.2e}). ETA={}, \".format(\n                            seconds_per_batch,\n                            data_time.val,\n                            str(eta),\n                        )\n                        + \"max mem: {:.1f} GB \".format(gpu_mem_usage())\n                    )\n            logger.info(\n                \"Epoch {} / {}: \".format(epoch + 1, total_epoch)\n                + \"avg data time: {:.2e}, avg batch time: {:.4f}, \".format(\n                    data_time.avg, batch_time.avg)\n                + \"average train loss: {:.4f}\".format(losses.avg))\n            \n            # update lr, scheduler.step() must be called after optimizer.step() according to the docs: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate  # noqa\n            self.scheduler.step()\n\n            # Enable eval mode\n            self.model.eval()\n\n            self.save_prompt(epoch + 1)\n\n            # eval at each epoch for single gpu training\n            self.evaluator.update_iteration(epoch)\n            self.eval_classifier(val_loader, \"val\", epoch == total_epoch - 1)\n            if test_loader is not None:\n                self.eval_classifier(\n                    test_loader, \"test\", epoch == total_epoch - 1)\n\n            # check the patience\n            t_name = \"val_\" + val_loader.dataset.name\n            try:\n                curr_acc = self.evaluator.results[f\"epoch_{epoch}\"][\"classification\"][t_name][\"top1\"]\n            except KeyError:\n                return\n\n            if curr_acc > best_metric:\n                best_metric = curr_acc\n                best_epoch = epoch + 1\n                logger.info(\n                    f'Best epoch {best_epoch}: best metric: {best_metric:.3f}')\n                patience = 0\n            else:\n                patience += 1\n            if patience >= self.cfg.SOLVER.PATIENCE:\n                logger.info(\"No improvement. Breaking out of loop.\")\n                break\n\n        # save the last checkpoints\n        if self.cfg.MODEL.SAVE_CKPT:\n            Checkpointer(\n                self.model,\n                save_dir=self.cfg.OUTPUT_DIR,\n                save_to_disk=True\n            ).save(\"last_model\")\n\n    @torch.no_grad()\n    def save_prompt(self, epoch):\n        # only save the prompt embed if below conditions are satisfied\n        if self.cfg.MODEL.PROMPT.SAVE_FOR_EACH_EPOCH:\n            if self.cfg.MODEL.TYPE in [\"vit\", \"ssl-vit\"] and \"prompt\" in self.cfg.MODEL.TRANSFER_TYPE:\n                prompt_embds = self.model.prompt_embeddings.cpu().numpy()\n                out = {\"shallow_prompt\": prompt_embds}\n                if self.cfg.MODEL.PROMPT.DEEP:\n                    deep_embds = self.model.enc.transformer.deep_prompt_embeddings.cpu().numpy()\n                    out[\"deep_prompt\"] = deep_embds\n                torch.save(out, os.path.join(\n                    self.cfg.OUTPUT_DIR, f\"prompt_ep{epoch}.pth\"))\n\n    @torch.no_grad()\n    def eval_classifier(self, data_loader, prefix, save=False):\n        \"\"\"evaluate classifier\"\"\"\n        batch_time = AverageMeter('Time', ':6.3f')\n        data_time = AverageMeter('Data', ':6.3f')\n        losses = AverageMeter('Loss', ':.4e')\n\n        log_interval = self.cfg.SOLVER.LOG_EVERY_N\n        test_name = prefix + \"_\" + data_loader.dataset.name\n        total = len(data_loader)\n\n        # initialize features and target\n        total_logits = []\n        total_targets = []\n\n        for idx, input_data in enumerate(data_loader):\n            end = time.time()\n            X, targets = self.get_input(input_data)\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            if self.cfg.DBG:\n                logger.info(\"during eval: {}\".format(X.shape))\n            loss, outputs = self.forward_one_batch(X, targets, False)\n            if loss == -1:\n                return\n            losses.update(loss, X.shape[0])\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n\n            if (idx + 1) % log_interval == 0:\n                logger.info(\n                    \"\\tTest {}/{}. loss: {:.3f}, {:.4f} s / batch. (data: {:.2e})\".format(  # noqa\n                        idx + 1,\n                        total,\n                        losses.val,\n                        batch_time.val,\n                        data_time.val\n                    ) + \"max mem: {:.5f} GB \".format(gpu_mem_usage())\n                )\n\n            # targets: List[int]\n            total_targets.extend(list(targets.numpy()))\n            total_logits.append(outputs)\n        logger.info(\n            f\"Inference ({prefix}):\"\n            + \"avg data time: {:.2e}, avg batch time: {:.4f}, \".format(\n                data_time.avg, batch_time.avg)\n            + \"average loss: {:.4f}\".format(losses.avg))\n        if self.model.side is not None:\n            logger.info(\n                \"--> side tuning alpha = {:.4f}\".format(self.model.side_alpha))\n        # total_testimages x num_classes\n        joint_logits = torch.cat(total_logits, dim=0).cpu().numpy()\n        self.evaluator.classify(\n            joint_logits, total_targets,\n            test_name, self.cfg.DATA.MULTILABEL,\n        )\n\n        # save the probs and targets\n        if save and self.cfg.MODEL.SAVE_CKPT:\n            out = {\"targets\": total_targets, \"joint_logits\": joint_logits}\n            out_path = os.path.join(\n                self.cfg.OUTPUT_DIR, f\"{test_name}_logits.pth\")\n            torch.save(out, out_path)\n            logger.info(\n                f\"Saved logits and targets for {test_name} at {out_path}\")", ""]}
{"filename": "src/engine/evaluator.py", "chunked_list": ["#!/usr/bin/env python3\nimport numpy as np\n\nfrom collections import defaultdict\nfrom typing import List, Union\n\nfrom .eval import multilabel\nfrom .eval import singlelabel\nfrom ..utils import logging\nlogger = logging.get_logger(\"visual_prompt\")", "from ..utils import logging\nlogger = logging.get_logger(\"visual_prompt\")\n\n\nclass Evaluator():\n    \"\"\"\n    An evaluator with below logics:\n\n    1. find which eval module to use.\n    2. store the eval results, pretty print it in log file as well.\n    \"\"\"\n\n    def __init__(\n        self,\n    ) -> None:\n        self.results = defaultdict(dict)\n        self.iteration = -1\n        self.threshold_end = 0.5\n\n    def update_iteration(self, iteration: int) -> None:\n        \"\"\"update iteration info\"\"\"\n        self.iteration = iteration\n\n    def update_result(self, metric: str, value: Union[float, dict]) -> None:\n        if self.iteration > -1:\n            key_name = \"epoch_\" + str(self.iteration)\n        else:\n            key_name = \"final\"\n        if isinstance(value, float):\n            self.results[key_name].update({metric: value})\n        else:\n            if metric in self.results[key_name]:\n                self.results[key_name][metric].update(value)\n            else:\n                self.results[key_name].update({metric: value})\n\n    def classify(self, probs, targets, test_data, multilabel=False):\n        \"\"\"\n        Evaluate classification result.\n        Args:\n            probs: np.ndarray for num_data x num_class, predicted probabilities\n            targets: np.ndarray for multilabel, list of integers for single label\n            test_labels:  map test image ids to a list of class labels\n        \"\"\"\n        if not targets:\n            raise ValueError(\n                \"When evaluating classification, need at least give targets\")\n\n        if multilabel:\n            self._eval_multilabel(probs, targets, test_data)\n        else:\n            self._eval_singlelabel(probs, targets, test_data)\n\n    def _eval_singlelabel(\n        self,\n        scores: np.ndarray,\n        targets: List[int],\n        eval_type: str\n    ) -> None:\n        \"\"\"\n        if number of labels > 2:\n            top1 and topk (5 by default) accuracy\n        if number of labels == 2:\n            top1 and rocauc\n        \"\"\"\n        acc_dict = singlelabel.compute_acc_auc(scores, targets)\n\n        log_results = {\n            k: np.around(v * 100, decimals=2) for k, v in acc_dict.items()\n        }\n        save_results = acc_dict\n\n        self.log_and_update(log_results, save_results, eval_type)\n\n    def _eval_multilabel(\n        self,\n        scores: np.ndarray,\n        targets: np.ndarray,\n        eval_type: str\n    ) -> None:\n        num_labels = scores.shape[-1]\n        targets = multilabel.multihot(targets, num_labels)\n\n        log_results = {}\n        ap, ar, mAP, mAR = multilabel.compute_map(scores, targets)\n        f1_dict = multilabel.get_best_f1_scores(\n            targets, scores, self.threshold_end)\n\n        log_results[\"mAP\"] = np.around(mAP * 100, decimals=2)\n        log_results[\"mAR\"] = np.around(mAR * 100, decimals=2)\n        log_results.update({\n            k: np.around(v * 100, decimals=2) for k, v in f1_dict.items()})\n        save_results = {\n            \"ap\": ap, \"ar\": ar, \"mAP\": mAP, \"mAR\": mAR, \"f1\": f1_dict\n        }\n        self.log_and_update(log_results, save_results, eval_type)\n\n    def log_and_update(self, log_results, save_results, eval_type):\n        log_str = \"\"\n        for k, result in log_results.items():\n            if not isinstance(result, np.ndarray):\n                log_str += f\"{k}: {result:.2f}\\t\"\n            else:\n                log_str += f\"{k}: {list(result)}\\t\"\n        logger.info(f\"Classification results with {eval_type}: {log_str}\")\n        # save everything\n        self.update_result(\"classification\", {eval_type: save_results})", ""]}
{"filename": "src/engine/eval/multilabel.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\nevaluate precision@1, @5 equal to Top1 and Top5 error rate\n\"\"\"\nimport numpy as np\nfrom typing import List, Tuple, Dict\nfrom sklearn.metrics import (\n    precision_recall_curve,\n    average_precision_score,\n    f1_score", "    average_precision_score,\n    f1_score\n)\n\n\ndef get_continuous_ids(probe_labels: List[int]) -> Dict[int, int]:\n    sorted(probe_labels)\n    id2continuousid = {}\n    for idx, p_id in enumerate(probe_labels):\n        id2continuousid[p_id] = idx\n    return id2continuousid", "\n\ndef multihot(x: List[List[int]], nb_classes: int) -> np.ndarray:\n    \"\"\"transform to multihot encoding\n\n    Arguments:\n        x: list of multi-class integer labels, in the range\n            [0, nb_classes-1]\n        nb_classes: number of classes for the multi-hot vector\n\n    Returns:\n        multihot: multihot vector of type int, (num_samples, nb_classes)\n    \"\"\"\n    num_samples = len(x)\n\n    multihot = np.zeros((num_samples, nb_classes), dtype=np.int32)\n    for idx, labs in enumerate(x):\n        for lab in labs:\n            multihot[idx, lab] = 1\n\n    return multihot.astype(np.int)", "\n\ndef compute_map(\n        scores: np.ndarray, multihot_targets: np.ndarray\n) -> Tuple[np.ndarray, np.ndarray, float, float]:\n    \"\"\"Compute the mean average precision across all class labels.\n\n    Arguments:\n        scores: matrix of per-class distances,\n            of size num_samples x nb_classes\n        multihot_targets: matrix of multi-hot target predictions,\n            of size num_samples x nb_classes\n\n    Returns:\n        ap: list of average-precision scores, one for each of\n            the nb_classes classes.\n        ar: list of average-recall scores, one for each of\n            the nb_classes classes.\n        mAP: the mean average precision score over all average\n            precisions for all nb_classes classes.\n        mAR: the mean average recall score over all average\n            precisions for all nb_classes classes.\n    \"\"\"\n    nb_classes = scores.shape[1]\n\n    ap = np.zeros((nb_classes,), dtype=np.float32)\n    ar = np.zeros((nb_classes,), dtype=np.float32)\n\n    for c in range(nb_classes):\n        y_true = multihot_targets[:, c]\n        y_scores = scores[:, c]\n\n        # Use interpolated average precision (a la PASCAL\n        try:\n            ap[c] = average_precision_score(y_true, y_scores)\n        except ValueError:\n            ap[c] = -1\n\n        # Also get the average of the recalls on the raw PR-curve\n        try:\n            _, rec, _ = precision_recall_curve(y_true, y_scores)\n            ar[c] = rec.mean()\n        except ValueError:\n            ar[c] = -1\n\n    mAP = ap.mean()\n    mAR = ar.mean()\n\n    return ap, ar, mAP, mAR", "\n\ndef compute_f1(\n        multihot_targets: np.ndarray, scores: np.ndarray, threshold: float = 0.5\n) -> Tuple[float, float, float]:\n    # change scores to predict_labels\n    predict_labels = scores > threshold\n    predict_labels = predict_labels.astype(np.int)\n\n    # change targets to multihot\n    f1 = {}\n    f1[\"micro\"] = f1_score(\n        y_true=multihot_targets,\n        y_pred=predict_labels,\n        average=\"micro\"\n    )\n    f1[\"samples\"] = f1_score(\n        y_true=multihot_targets,\n        y_pred=predict_labels,\n        average=\"samples\"\n    )\n    f1[\"macro\"] = f1_score(\n        y_true=multihot_targets,\n        y_pred=predict_labels,\n        average=\"macro\"\n    )\n    f1[\"none\"] = f1_score(\n        y_true=multihot_targets,\n        y_pred=predict_labels,\n        average=None\n    )\n    return f1[\"micro\"], f1[\"samples\"], f1[\"macro\"], f1[\"none\"]", "\n\ndef get_best_f1_scores(\n    multihot_targets: np.ndarray, scores: np.ndarray, threshold_end: int\n) -> Dict[str, float]:\n    end = 0.5\n    end = 0.05\n    end = threshold_end\n    thrs = np.linspace(\n        end, 0.95, int(np.round((0.95 - end) / 0.05)) + 1, endpoint=True\n    )\n    f1_micros = []\n    f1_macros = []\n    f1_samples = []\n    f1_none = []\n    for thr in thrs:\n        _micros, _samples, _macros, _none = compute_f1(multihot_targets, scores, thr)\n        f1_micros.append(_micros)\n        f1_samples.append(_samples)\n        f1_macros.append(_macros)\n        f1_none.append(_none)\n\n    f1_macros_m = max(f1_macros)\n    b_thr = np.argmax(f1_macros)\n\n    f1_micros_m = f1_micros[b_thr]\n    f1_samples_m = f1_samples[b_thr]\n    f1_none_m = f1_none[b_thr]\n    f1 = {}\n    f1[\"micro\"] = f1_micros_m\n    f1[\"macro\"] = f1_macros_m\n    f1[\"samples\"] = f1_samples_m\n    f1[\"threshold\"] = thrs[b_thr]\n    f1[\"none\"] = f1_none_m\n    return f1", ""]}
{"filename": "src/engine/eval/singlelabel.py", "chunked_list": ["#!/usr/bin/env python3\n\n\"\"\"Functions for computing metrics. all metrics has range of 0-1\"\"\"\n\nimport numpy as np\nimport torch\nfrom sklearn.metrics import (\n    accuracy_score, average_precision_score, f1_score, roc_auc_score\n)\n", ")\n\n\ndef accuracy(y_probs, y_true):\n    # y_prob: (num_images, num_classes)\n    y_preds = np.argmax(y_probs, axis=1)\n    accuracy = accuracy_score(y_true, y_preds)\n    error = 1.0 - accuracy\n    return accuracy, error\n", "\n\ndef top_n_accuracy(y_probs, truths, n=1):\n    # y_prob: (num_images, num_classes)\n    # truth: (num_images, num_classes) multi/one-hot encoding\n    best_n = np.argsort(y_probs, axis=1)[:, -n:]\n    if isinstance(truths, np.ndarray) and truths.shape == y_probs.shape:\n        ts = np.argmax(truths, axis=1)\n    else:\n        # a list of GT class idx\n        ts = truths\n\n    num_input = y_probs.shape[0]\n    successes = 0\n    for i in range(num_input):\n        if ts[i] in best_n[i, :]:\n            successes += 1\n    return float(successes) / num_input", "\n\ndef compute_acc_auc(y_probs, y_true_ids):\n    onehot_tgts = np.zeros_like(y_probs)\n    for idx, t in enumerate(y_true_ids):\n        onehot_tgts[idx, t] = 1.\n\n    num_classes = y_probs.shape[1]\n    if num_classes == 2:\n        top1, _ = accuracy(y_probs, y_true_ids)\n        # so precision can set all to 2\n        try:\n            auc = roc_auc_score(onehot_tgts, y_probs, average='macro')\n        except ValueError as e:\n            print(f\"value error encountered {e}, set auc sccore to -1.\")\n            auc = -1\n        return {\"top1\": top1, \"rocauc\": auc}\n\n    top1, _ = accuracy(y_probs, y_true_ids)\n    k = min([5, num_classes])  # if number of labels < 5, use the total class\n    top5 = top_n_accuracy(y_probs, y_true_ids, k)\n    return {\"top1\": top1, f\"top{k}\": top5}", "\n\ndef topks_correct(preds, labels, ks):\n    \"\"\"Computes the number of top-k correct predictions for each k.\"\"\"\n    assert preds.size(0) == labels.size(\n        0\n    ), \"Batch dim of predictions and labels must match\"\n    # Find the top max_k predictions for each sample\n    _top_max_k_vals, top_max_k_inds = torch.topk(\n        preds, max(ks), dim=1, largest=True, sorted=True\n    )\n    # (batch_size, max_k) -> (max_k, batch_size)\n    top_max_k_inds = top_max_k_inds.t()\n    # (batch_size, ) -> (max_k, batch_size)\n    rep_max_k_labels = labels.view(1, -1).expand_as(top_max_k_inds)\n    # (i, j) = 1 if top i-th prediction for the j-th sample is correct\n    top_max_k_correct = top_max_k_inds.eq(rep_max_k_labels)\n    # Compute the number of topk correct predictions for each k\n    topks_correct = [\n        top_max_k_correct[:k, :].reshape(-1).float().sum() for k in ks\n    ]\n    return topks_correct", "\n\ndef topk_errors(preds, labels, ks):\n    \"\"\"Computes the top-k error for each k.\"\"\"\n    if int(labels.min()) < 0:  # has ignore\n        keep_ids = np.where(labels.cpu() >= 0)[0]\n        preds = preds[keep_ids, :]\n        labels = labels[keep_ids]\n\n    num_topks_correct = topks_correct(preds, labels, ks)\n    return [(1.0 - x / preds.size(0)) for x in num_topks_correct]", "\n\ndef topk_accuracies(preds, labels, ks):\n    \"\"\"Computes the top-k accuracy for each k.\"\"\"\n    num_topks_correct = topks_correct(preds, labels, ks)\n    return [(x / preds.size(0)) for x in num_topks_correct]\n\n"]}
{"filename": "src/models/mlp.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\nModified from: fbcode/multimo/models/encoders/mlp.py\n\"\"\"\nimport math\nimport torch\n\nfrom torch import nn\nfrom typing import List, Type\n", "from typing import List, Type\n\nfrom ..utils import logging\nlogger = logging.get_logger(\"visual_prompt\")\n\n\nclass MLP(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        mlp_dims: List[int],\n        dropout: float = 0.1,\n        nonlinearity: Type[nn.Module] = nn.ReLU,\n        normalization: Type[nn.Module] = nn.BatchNorm1d,  # nn.LayerNorm,\n        special_bias: bool = False,\n        add_bn_first: bool = False,\n    ):\n        super(MLP, self).__init__()\n        projection_prev_dim = input_dim\n        projection_modulelist = []\n        last_dim = mlp_dims[-1]\n        mlp_dims = mlp_dims[:-1]\n\n        if add_bn_first:\n            if normalization is not None:\n                projection_modulelist.append(normalization(projection_prev_dim))\n            if dropout != 0:\n                projection_modulelist.append(nn.Dropout(dropout))\n\n        for idx, mlp_dim in enumerate(mlp_dims):\n            fc_layer = nn.Linear(projection_prev_dim, mlp_dim)\n            nn.init.kaiming_normal_(fc_layer.weight, a=0, mode='fan_out')\n            projection_modulelist.append(fc_layer)\n            projection_modulelist.append(nonlinearity())\n\n            if normalization is not None:\n                projection_modulelist.append(normalization(mlp_dim))\n\n            if dropout != 0:\n                projection_modulelist.append(nn.Dropout(dropout))\n            projection_prev_dim = mlp_dim\n\n        self.projection = nn.Sequential(*projection_modulelist)\n        self.last_layer = nn.Linear(projection_prev_dim, last_dim)\n        nn.init.kaiming_normal_(self.last_layer.weight, a=0, mode='fan_out')\n        if special_bias:\n            prior_prob = 0.01\n            bias_value = -math.log((1 - prior_prob) / prior_prob)\n            torch.nn.init.constant_(self.last_layer.bias, bias_value)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        input_arguments:\n            @x: torch.FloatTensor\n        \"\"\"\n        x = self.projection(x)\n        x = self.last_layer(x)\n        return x", ""]}
{"filename": "src/models/build_vit_backbone.py", "chunked_list": ["#!/usr/bin/env python3\nimport numpy as np\nimport torch\nimport os\nfrom .vit_backbones.vit_moco import vit_base as moco_vit_model\nfrom .vit_backbones.vit_mae import build_model as mae_vit_model\n\nfrom .vit_prompt.vit_moco import build_model as prompt_moco_vit\nfrom .vit_prompt.vit_mae import build_model as prompt_mae_vit\n", "from .vit_prompt.vit_mae import build_model as prompt_mae_vit\n\n\nMODEL_ZOO = {\n    \"mae_vitb16\": \"mae-ViT-B.pth\",\n    \"mae_vitl16\": \"mae-ViT-L.pth\",\n    \"mocov3_vitb16\" : \"mocov3-ViT-B.pth.tar\",\n    \"mocov3_vits16\" : \"mocov3-ViT-S.pth.tar\",\n}\n", "}\n\n\n\ndef build_mae_model(\n    model_type, crop_size, prompt_cfg, model_root, adapter_cfg=None\n):  \n    if not model_type in [\"mae_vitb16\", \"mae_vitl16\"]:\n        raise ValueError(\"Does not support other arch\")\n    if prompt_cfg is not None:\n        model = prompt_mae_vit(model_type, prompt_cfg)\n    else:\n        model = mae_vit_model(model_type)\n    out_dim = model.embed_dim\n\n    ckpt = os.path.join(model_root, MODEL_ZOO[model_type])\n    checkpoint = torch.load(ckpt, map_location=\"cpu\")\n    state_dict = checkpoint['model']\n\n    msg = model.load_state_dict(state_dict, strict=False)\n    print(msg)\n    model.head = torch.nn.Identity()\n    return model, out_dim", "\n\ndef build_mocov3_model(\n    model_type, crop_size, prompt_cfg, model_root, adapter_cfg=None\n):\n    if not model_type in [\"mocov3_vitb16\", \"mocov3_vits16\"]:\n        raise ValueError(\"Does not support other arch\")\n    if prompt_cfg is not None:\n        model = prompt_moco_vit(model_type, prompt_cfg)\n    else:\n        model = moco_vit_model()\n        \n    out_dim = 384 if model_type.endswith('s16') else 768\n    ckpt = os.path.join(model_root, MODEL_ZOO[model_type])\n    checkpoint = torch.load(ckpt, map_location=\"cpu\")\n    state_dict = checkpoint['state_dict']\n    for k in list(state_dict.keys()):\n        # retain only base_encoder up to before the embedding layer\n        if k.startswith('module.'):\n            # remove prefix\n            key = k.replace('module.', '')\n            if key.startswith('base_encoder.'):\n                key = key.replace('base_encoder.', '')\n            elif key.startswith('momentum'):\n                del state_dict[k]\n                continue\n            state_dict[key] = state_dict[k]\n    \n        # delete renamed or unused k\n        del state_dict[k]\n   \n    msg = model.load_state_dict(state_dict, strict=False)\n    print(msg)\n    model.head = torch.nn.Identity()\n    return model, out_dim", "\n"]}
{"filename": "src/models/vit_models.py", "chunked_list": ["#!/usr/bin/env python3\n\n\"\"\"\nViT-related models\nNote: models return logits instead of prob\n\"\"\"\nimport torch\nimport torch.nn as nn\n\nfrom collections import OrderedDict", "\nfrom collections import OrderedDict\nfrom torchvision import models\n\nfrom .build_vit_backbone import (\n    build_mocov3_model, build_mae_model,\n)\nfrom .mlp import MLP\nfrom ..utils import logging\nlogger = logging.get_logger(\"visual_prompt\")", "from ..utils import logging\nlogger = logging.get_logger(\"visual_prompt\")\n\n\n\nclass SSLViT(nn.Module):\n    \"\"\"moco-v3 and mae model.\"\"\"\n\n    def __init__(self, cfg):\n        super(SSLViT, self).__init__()\n        \n        if \"prompt\" in cfg.MODEL.TRANSFER_TYPE:\n                prompt_cfg = cfg.MODEL.PROMPT\n        else:\n            prompt_cfg = None\n\n        if cfg.MODEL.TRANSFER_TYPE != \"end2end\" and \"prompt\" not in cfg.MODEL.TRANSFER_TYPE:\n            # linear, cls, tiny-tl, parital, adapter\n            self.froze_enc = True\n        else:\n            # prompt, end2end, cls+prompt\n            self.froze_enc = False\n        \n        if cfg.MODEL.TRANSFER_TYPE == \"adapter\":\n            adapter_cfg = cfg.MODEL.ADAPTER\n        else:\n            adapter_cfg = None\n\n        self.build_backbone(\n            prompt_cfg, cfg, adapter_cfg)\n\n        self.cfg = cfg\n        self.setup_side()\n        self.setup_head(cfg)\n\n    def setup_side(self):\n        if self.cfg.MODEL.TRANSFER_TYPE != \"side\":\n            self.side = None\n        else:\n            self.side_alpha = nn.Parameter(torch.tensor(0.0))\n            m = models.alexnet(pretrained=True)\n            self.side = nn.Sequential(OrderedDict([\n                (\"features\", m.features),\n                (\"avgpool\", m.avgpool),\n            ]))\n            self.side_projection = nn.Linear(9216, self.feat_dim, bias=False)\n            \n    def setup_head(self, cfg):\n        self.head = MLP(\n            input_dim=self.feat_dim,\n            mlp_dims=[self.feat_dim] * self.cfg.MODEL.MLP_NUM + \\\n                [cfg.DATA.NUMBER_CLASSES], # noqa\n            special_bias=True\n        )\n\n    def build_backbone(self, prompt_cfg, cfg, adapter_cfg):\n        if \"moco\" in cfg.DATA.FEATURE:\n            build_fn = build_mocov3_model\n        elif \"mae\" in cfg.DATA.FEATURE:\n            build_fn = build_mae_model\n        \n        self.enc, self.feat_dim = build_fn(\n            cfg.DATA.FEATURE, cfg.DATA.CROPSIZE,\n            prompt_cfg, cfg.MODEL.MODEL_ROOT, adapter_cfg=adapter_cfg\n        )\n\n        transfer_type = cfg.MODEL.TRANSFER_TYPE\n        # linear, prompt, cls, cls+prompt, partial_1\n        if transfer_type == \"partial-1\":\n            total_layer = len(self.enc.blocks)\n            for k, p in self.enc.named_parameters():\n                if \"blocks.{}\".format(total_layer - 1) not in k and \"fc_norm\" not in k and k != \"norm\": # noqa\n                    p.requires_grad = False\n        elif transfer_type == \"partial-2\":\n            total_layer = len(self.enc.blocks)\n            for k, p in self.enc.named_parameters():\n                if \"blocks.{}\".format(total_layer - 1) not in k and \"blocks.{}\".format(total_layer - 2) not in k and \"fc_norm\" not in k and k != \"norm\": # noqa\n                    p.requires_grad = False\n\n        elif transfer_type == \"partial-4\":\n            total_layer = len(self.enc.blocks)\n            for k, p in self.enc.named_parameters():\n                if \"blocks.{}\".format(total_layer - 1) not in k and \"blocks.{}\".format(total_layer - 2) not in k and \"blocks.{}\".format(total_layer - 3) not in k and \"blocks.{}\".format(total_layer - 4) not in k and \"fc_norm\" not in k and k != \"norm\": # noqa\n                    p.requires_grad = False\n\n        elif transfer_type == \"linear\" or transfer_type == \"sidetune\":\n            for k, p in self.enc.named_parameters():\n                p.requires_grad = False\n\n        elif transfer_type == \"tinytl-bias\":\n            for k, p in self.enc.named_parameters():\n                if 'bias' not in k:\n                    p.requires_grad = False\n\n        elif transfer_type == \"prompt+bias\":\n            for k, p in self.enc.named_parameters():\n                if \"prompt\" not in k and 'bias' not in k:\n                    p.requires_grad = False\n\n        elif transfer_type == \"prompt\" and prompt_cfg.LOCATION == \"below\":\n            for k, p in self.enc.named_parameters():\n                if \"prompt\" not in k and \"patch_embed.proj.weight\" not in k  and \"patch_embed.proj.bias\" not in k:\n                    p.requires_grad = False\n\n        elif transfer_type == \"prompt\":\n            for k, p in self.enc.named_parameters():\n                if \"prompt\" not in k:\n                    p.requires_grad = False\n                    \n        elif transfer_type == \"end2end\":\n            logger.info(\"Enable all parameters update during training\")\n        \n        # adapter\n        elif transfer_type == \"adapter\":\n            for k, p in self.enc.named_parameters():\n                if \"adapter\" not in k:\n                    p.requires_grad = False\n        else:\n            raise ValueError(\"transfer type {} is not supported\".format(\n                transfer_type))\n\n        for k, p in self.enc.named_parameters():\n            if 'gate' in k:\n                p.requires_grad = True\n            if 'temp' in k:\n                p.requires_grad = True\n                \n    def forward(self, x, return_feature=False):\n        if self.side is not None:\n            side_output = self.side(x)\n            side_output = side_output.view(side_output.size(0), -1)\n            side_output = self.side_projection(side_output)\n\n        if self.froze_enc and self.enc.training:\n            self.enc.eval()\n        x = self.enc(x)  # batch_size x self.feat_dim\n        \n        if self.side is not None:\n            alpha_squashed = torch.sigmoid(self.side_alpha)\n            x = alpha_squashed * x + (1 - alpha_squashed) * side_output\n\n        if return_feature:\n            return x, x\n        x = self.head(x)\n        \n        return x\n        \n    def forward_cls_layerwise(self, x):\n        cls_embeds = self.enc.forward_cls_layerwise(x)\n        return cls_embeds\n\n    def get_features(self, x):\n        \"\"\"get a (batch_size, self.feat_dim) feature\"\"\"\n        x = self.enc(x)  # batch_size x self.feat_dim\n        return x", "            "]}
{"filename": "src/models/build_model.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\nModel construction functions.\n\"\"\"\nfrom tabnanny import verbose\nimport torch\n\nfrom .vit_models import SSLViT\nfrom ..utils import logging\nlogger = logging.get_logger(\"visual_prompt\")", "from ..utils import logging\nlogger = logging.get_logger(\"visual_prompt\")\n# Supported model types\n_MODEL_TYPES = {\n    \"ssl-vit\": SSLViT,\n}\n\n\ndef build_model(cfg):\n    \"\"\"\n    build model here\n    \"\"\"\n    assert (\n        cfg.MODEL.TYPE in _MODEL_TYPES.keys()\n    ), \"Model type '{}' not supported\".format(cfg.MODEL.TYPE)\n    assert (\n        cfg.NUM_GPUS <= torch.cuda.device_count()\n    ), \"Cannot use more GPU devices than available\"\n\n    # Construct the model\n    train_type = cfg.MODEL.TYPE\n    model = _MODEL_TYPES[train_type](cfg) \n\n    log_model_info(model, verbose=cfg.DBG)\n    model, device = load_model_to_device(model, cfg)\n    logger.info(f\"Device used for model: {device}\")\n\n    return model, device", "def build_model(cfg):\n    \"\"\"\n    build model here\n    \"\"\"\n    assert (\n        cfg.MODEL.TYPE in _MODEL_TYPES.keys()\n    ), \"Model type '{}' not supported\".format(cfg.MODEL.TYPE)\n    assert (\n        cfg.NUM_GPUS <= torch.cuda.device_count()\n    ), \"Cannot use more GPU devices than available\"\n\n    # Construct the model\n    train_type = cfg.MODEL.TYPE\n    model = _MODEL_TYPES[train_type](cfg) \n\n    log_model_info(model, verbose=cfg.DBG)\n    model, device = load_model_to_device(model, cfg)\n    logger.info(f\"Device used for model: {device}\")\n\n    return model, device", "\n\ndef log_model_info(model, verbose=False):\n    \"\"\"Logs model info\"\"\"\n    if verbose:\n        logger.info(f\"Classification Model:\\n{model}\")\n    model_total_params = sum(p.numel() for p in model.parameters())\n    model_grad_params = sum(\n        p.numel() for p in model.parameters() if p.requires_grad)\n    logger.info(\"Total Parameters: {0}\\t Gradient Parameters: {1}\".format(\n        model_total_params, model_grad_params))\n    logger.info(\"tuned percent:%.3f\"%(model_grad_params/model_total_params*100))", "\n\ndef get_current_device():\n    if torch.cuda.is_available():\n        # Determine the GPU used by the current process\n        cur_device = torch.cuda.current_device()\n    else:\n        cur_device = torch.device('cpu')\n    return cur_device\n", "\n\ndef load_model_to_device(model, cfg):\n    cur_device = get_current_device()\n    if torch.cuda.is_available():\n        # Transfer the model to the current GPU device\n        model = model.cuda(device=cur_device)\n        # Use multi-process data parallel model in the multi-gpu setting\n        if cfg.NUM_GPUS > 1:\n            # Make model replica operate on the current device\n            model = torch.nn.parallel.DistributedDataParallel(\n                module=model, device_ids=[cur_device], output_device=cur_device,\n                find_unused_parameters=True,\n            )\n    else:\n        model = model.to(cur_device)\n    return model, cur_device", ""]}
{"filename": "src/models/vit_backbones/vit_mae.py", "chunked_list": ["#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n\"\"\"\nborrowed from https://github.com/facebookresearch/mae/blob/main/models_vit.py\n\"\"\"\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n", "import torch.nn as nn\n\nimport timm.models.vision_transformer\nfrom timm.models.layers import Mlp, DropPath\n\n\n# based on timm Attention implementation\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, temp=1.0):\n        \"\"\" \n        temp = 1.0 by default or learnable scalar\n        \"\"\"\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        \n        attn = (attn / temp).softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n            \n        return x", "\n\nclass Block(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x, temp=1.0):\n        \"\"\" \n        temp = 1.0 by default or learnable scalar\n        \"\"\"\n        x = x + self.drop_path(self.attn(self.norm1(x), temp=temp))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x", "\n\nclass VisionTransformer(timm.models.vision_transformer.VisionTransformer):\n    \"\"\" Vision Transformer with support for global average pooling\n    \"\"\"\n    def __init__(self, global_pool=False, **kwargs):\n        super(VisionTransformer, self).__init__(**kwargs)\n\n        self.global_pool = global_pool\n        \n        norm_layer = kwargs['norm_layer']\n        embed_dim = kwargs['embed_dim']\n        \n\n        dpr = [x.item() for x in torch.linspace(0, kwargs['drop_path_rate'], kwargs['depth'])]  # stochastic depth decay rule\n        self.blocks = nn.Sequential(*[\n            Block(\n                dim=embed_dim, num_heads=kwargs['num_heads'], mlp_ratio=kwargs['mlp_ratio'], qkv_bias=kwargs['qkv_bias'],\n                drop_path=dpr[i], norm_layer=kwargs['norm_layer'])\n            for i in range(kwargs['depth'])])\n        # if pretrained_norm:\n        self.norm = norm_layer(embed_dim)\n        # self.fc_norm = norm_layer(embed_dim)\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        for blk in self.blocks:\n            x, attn = blk(x)\n\n        \n        x = x[:, 1:, :].mean(dim=1)  # global pool without cls token\n        \n        outcome = self.norm(x)\n\n        return outcome", "\n\ndef build_model(model_type):\n    if \"vitb\" in model_type:\n        return vit_base_patch16()\n    elif \"vitl\" in model_type:\n        return vit_large_patch16()\n    elif \"vith\" in model_type:\n        return vit_huge_patch14()\n", "\n\ndef vit_base_patch16(**kwargs):\n    model = VisionTransformer(\n        drop_path_rate=0.1, global_pool=True,  # using default settings for mae-finetune\n        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n        mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n", "\n\ndef vit_large_patch16(**kwargs):\n    model = VisionTransformer(\n        drop_path_rate=0.1, global_pool=True,  # using default settings for mae-finetune\n        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n        mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n", "\n\ndef vit_huge_patch14(**kwargs):\n    model = VisionTransformer(\n        drop_path_rate=0.1, global_pool=True,  # using default settings for mae-finetune\n        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n        mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model\n", ""]}
{"filename": "src/models/vit_backbones/vit_moco.py", "chunked_list": ["#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n\"\"\"\nborrowed from https://github.com/facebookresearch/moco-v3/blob/main/vits.py\n\"\"\"\nimport math\nimport torch\nimport torch.nn as nn\nfrom functools import partial, reduce\nfrom operator import mul", "from functools import partial, reduce\nfrom operator import mul\n\nfrom timm.models.vision_transformer import _cfg\nfrom timm.models.layers.helpers import to_2tuple\nfrom timm.models.layers import PatchEmbed\nfrom .vit_mae import VisionTransformer\n\n__all__ = [\n    'vit_small',", "__all__ = [\n    'vit_small',\n    'vit_base',\n    'vit_conv_small',\n    'vit_conv_base',\n]\n\n\n\nclass VisionTransformerMoCo(VisionTransformer):\n    def __init__(self, stop_grad_conv1=False, **kwargs):\n        super().__init__(**kwargs)\n        # Use fixed 2D sin-cos position embedding\n        self.build_2d_sincos_position_embedding()\n\n        # weight initialization\n        for name, m in self.named_modules():\n            if isinstance(m, nn.Linear):\n                if 'qkv' in name:\n                    # treat the weights of Q, K, V separately\n                    val = math.sqrt(6. / float(m.weight.shape[0] // 3 + m.weight.shape[1]))\n                    nn.init.uniform_(m.weight, -val, val)\n                else:\n                    nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n        nn.init.normal_(self.cls_token, std=1e-6)\n\n        if isinstance(self.patch_embed, PatchEmbed):\n            # xavier_uniform initialization\n            val = math.sqrt(6. / float(3 * reduce(mul, self.patch_embed.patch_size, 1) + self.embed_dim))\n            nn.init.uniform_(self.patch_embed.proj.weight, -val, val)\n            nn.init.zeros_(self.patch_embed.proj.bias)\n\n            if stop_grad_conv1:\n                self.patch_embed.proj.weight.requires_grad = False\n                self.patch_embed.proj.bias.requires_grad = False\n\n    def build_2d_sincos_position_embedding(self, temperature=10000.):\n        h, w = self.patch_embed.grid_size\n        grid_w = torch.arange(w, dtype=torch.float32)\n        grid_h = torch.arange(h, dtype=torch.float32)\n        grid_w, grid_h = torch.meshgrid(grid_w, grid_h)\n        assert self.embed_dim % 4 == 0, 'Embed dimension must be divisible by 4 for 2D sin-cos position embedding'\n        pos_dim = self.embed_dim // 4\n        omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim\n        omega = 1. / (temperature**omega)\n        out_w = torch.einsum('m,d->md', [grid_w.flatten(), omega])\n        out_h = torch.einsum('m,d->md', [grid_h.flatten(), omega])\n        pos_emb = torch.cat([torch.sin(out_w), torch.cos(out_w), torch.sin(out_h), torch.cos(out_h)], dim=1)[None, :, :]\n\n        assert self.num_tokens == 1, 'Assuming one and only one token, [cls]'\n        pe_token = torch.zeros([1, 1, self.embed_dim], dtype=torch.float32)\n        self.pos_embed = nn.Parameter(torch.cat([pe_token, pos_emb], dim=1))\n        self.pos_embed.requires_grad = False", "\nclass VisionTransformerMoCo(VisionTransformer):\n    def __init__(self, stop_grad_conv1=False, **kwargs):\n        super().__init__(**kwargs)\n        # Use fixed 2D sin-cos position embedding\n        self.build_2d_sincos_position_embedding()\n\n        # weight initialization\n        for name, m in self.named_modules():\n            if isinstance(m, nn.Linear):\n                if 'qkv' in name:\n                    # treat the weights of Q, K, V separately\n                    val = math.sqrt(6. / float(m.weight.shape[0] // 3 + m.weight.shape[1]))\n                    nn.init.uniform_(m.weight, -val, val)\n                else:\n                    nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n        nn.init.normal_(self.cls_token, std=1e-6)\n\n        if isinstance(self.patch_embed, PatchEmbed):\n            # xavier_uniform initialization\n            val = math.sqrt(6. / float(3 * reduce(mul, self.patch_embed.patch_size, 1) + self.embed_dim))\n            nn.init.uniform_(self.patch_embed.proj.weight, -val, val)\n            nn.init.zeros_(self.patch_embed.proj.bias)\n\n            if stop_grad_conv1:\n                self.patch_embed.proj.weight.requires_grad = False\n                self.patch_embed.proj.bias.requires_grad = False\n\n    def build_2d_sincos_position_embedding(self, temperature=10000.):\n        h, w = self.patch_embed.grid_size\n        grid_w = torch.arange(w, dtype=torch.float32)\n        grid_h = torch.arange(h, dtype=torch.float32)\n        grid_w, grid_h = torch.meshgrid(grid_w, grid_h)\n        assert self.embed_dim % 4 == 0, 'Embed dimension must be divisible by 4 for 2D sin-cos position embedding'\n        pos_dim = self.embed_dim // 4\n        omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim\n        omega = 1. / (temperature**omega)\n        out_w = torch.einsum('m,d->md', [grid_w.flatten(), omega])\n        out_h = torch.einsum('m,d->md', [grid_h.flatten(), omega])\n        pos_emb = torch.cat([torch.sin(out_w), torch.cos(out_w), torch.sin(out_h), torch.cos(out_h)], dim=1)[None, :, :]\n\n        assert self.num_tokens == 1, 'Assuming one and only one token, [cls]'\n        pe_token = torch.zeros([1, 1, self.embed_dim], dtype=torch.float32)\n        self.pos_embed = nn.Parameter(torch.cat([pe_token, pos_emb], dim=1))\n        self.pos_embed.requires_grad = False", "\n\nclass ConvStem(nn.Module):\n    \"\"\"\n    ConvStem, from Early Convolutions Help Transformers See Better, Tete et al. https://arxiv.org/abs/2106.14881\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):\n        super().__init__()\n\n        assert patch_size == 16, 'ConvStem only supports patch size of 16'\n        assert embed_dim % 8 == 0, 'Embed dimension must be divisible by 8 for ConvStem'\n\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.num_patches = self.grid_size[0] * self.grid_size[1]\n        self.flatten = flatten\n\n        # build stem, similar to the design in https://arxiv.org/abs/2106.14881\n        stem = []\n        input_dim, output_dim = 3, embed_dim // 8\n        for l in range(4):\n            stem.append(nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=2, padding=1, bias=False))\n            stem.append(nn.BatchNorm2d(output_dim))\n            stem.append(nn.ReLU(inplace=True))\n            input_dim = output_dim\n            output_dim *= 2\n        stem.append(nn.Conv2d(input_dim, embed_dim, kernel_size=1))\n        self.proj = nn.Sequential(*stem)\n\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x)\n        if self.flatten:\n            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n        x = self.norm(x)\n        return x", "\n\ndef vit_small(**kwargs):\n    model = VisionTransformerMoCo(\n        patch_size=16, embed_dim=384, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef vit_base(**kwargs):\n    model = VisionTransformerMoCo(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True, drop_path_rate=0.1,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    return model", "\ndef vit_base(**kwargs):\n    model = VisionTransformerMoCo(\n        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True, drop_path_rate=0.1,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef vit_conv_small(**kwargs):\n    # minus one ViT block\n    model = VisionTransformerMoCo(\n        patch_size=16, embed_dim=384, depth=11, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), embed_layer=ConvStem, **kwargs)\n    model.default_cfg = _cfg()\n    return model", "def vit_conv_small(**kwargs):\n    # minus one ViT block\n    model = VisionTransformerMoCo(\n        patch_size=16, embed_dim=384, depth=11, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), embed_layer=ConvStem, **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef vit_conv_base(**kwargs):\n    # minus one ViT block\n    model = VisionTransformerMoCo(\n        patch_size=16, embed_dim=768, depth=11, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), embed_layer=ConvStem, **kwargs)\n    model.default_cfg = _cfg()\n    return model", "def vit_conv_base(**kwargs):\n    # minus one ViT block\n    model = VisionTransformerMoCo(\n        patch_size=16, embed_dim=768, depth=11, num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), embed_layer=ConvStem, **kwargs)\n    model.default_cfg = _cfg()\n    return model\n"]}
{"filename": "src/models/vit_prompt/vit_mae.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\nvit-moco-v3 with prompt\n\"\"\"\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision as tv\n", "import torchvision as tv\n\nfrom functools import partial, reduce\nfrom operator import mul\nfrom torch.nn import Conv2d, Dropout\nfrom timm.models.vision_transformer import _cfg\n\nfrom ..vit_backbones.vit_mae import VisionTransformer\nfrom ...utils import logging\nlogger = logging.get_logger(\"visual_prompt\")", "from ...utils import logging\nlogger = logging.get_logger(\"visual_prompt\")\n\n\nclass PromptedVisionTransformer(VisionTransformer):\n    def __init__(self, prompt_config, **kwargs):\n        super().__init__(**kwargs)\n        self.prompt_config = prompt_config\n        if self.prompt_config.DEEP and self.prompt_config.LOCATION not in [\"prepend\", ]:\n            raise ValueError(\"Deep-{} is not supported\".format(self.prompt_config.LOCATION))\n\n        num_tokens = self.prompt_config.NUM_TOKENS\n\n        self.num_tokens = num_tokens\n        self.prompt_dropout = Dropout(self.prompt_config.DROPOUT)\n        \n        # define temperature for attention shaping\n        self.temp = self.prompt_config.TEMP\n        self.temp_learn = self.prompt_config.TEMP_LEARN\n        if self.temp_learn:\n            self.temp = nn.Parameter(torch.ones(prompt_config.TEMP_NUM))\n      \n        # initiate prompt:\n        if self.prompt_config.INITIATION == \"random\":\n            val = math.sqrt(6. / float(3 * reduce(mul, self.patch_embed.patch_size, 1) + self.embed_dim))  # noqa\n\n            self.prompt_embeddings = nn.Parameter(torch.zeros(\n                1, num_tokens, self.embed_dim))\n            # xavier_uniform initialization\n            nn.init.uniform_(self.prompt_embeddings.data, -val, val)\n\n            if self.prompt_config.DEEP:\n                self.deep_prompt_embeddings = nn.Parameter(torch.zeros(\n                    len(self.blocks) - 1,\n                    num_tokens, self.embed_dim\n                ))\n                # xavier_uniform initialization\n                nn.init.uniform_(\n                    self.deep_prompt_embeddings.data, -val, val)\n    \n        else:\n            raise ValueError(\"Other initiation scheme is not supported\")\n        \n        # define block-wise learnable gate scalar\n        if self.prompt_config.GATE_PRIOR:\n            gate_logit = (-torch.ones(self.prompt_config.GATE_NUM) * self.prompt_config.GATE_INIT)\n            self.gate_logit = nn.Parameter(gate_logit)\n            print(self.gate_logit)\n\n    def incorporate_prompt(self, x):\n        # combine prompt embeddings with image-patch embeddings\n        B = x.shape[0]\n        if self.prompt_config.LOCATION == \"prepend\":\n            # after CLS token, all before image patches\n            x = self.embeddings(x)  # (batch_size, 1 + n_pa\n            x = torch.cat((\n                    x[:, :1, :],\n                    self.prompt_dropout(\n                        self.prompt_embeddings.expand(B, -1, -1)),\n                    x[:, 1:, :]\n                ), dim=1)\n            # (batch_size, cls_token + n_prompt + n_patches, hidden_dim)\n        else:\n            raise ValueError(\"Other prompt locations are not supported\")\n        return x\n\n    def embeddings(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        return x\n\n    def train(self, mode=True):\n        # set train status for this class: disable all but the prompt-related modules\n        if mode:\n            # training:\n            self.blocks.eval()\n            self.patch_embed.eval()\n            self.pos_drop.eval()\n            self.prompt_dropout.train()\n        else:\n            # eval:\n            for module in self.children():\n                module.train(mode)\n        \n    def reinit_temp(self):\n        assert self.temp_learn, \"reinit_temp() could be run only when config.TEMP_LEARN == True\"\n        self.temp.data.copy_(self.temp.data.clamp(min=self.prompt_config.TEMP_MIN, max=self.prompt_config.TEMP_MAX))\n\n    def forward_features(self, x):\n        x = self.incorporate_prompt(x)\n        \n        # deep\n        if self.prompt_config.DEEP:\n            B = x.shape[0]\n            num_layers = len(self.blocks)\n\n            for i in range(num_layers):\n                if i == 0:\n                    x = self.blocks[i](x)\n                else:\n                    # prepend\n                    x = torch.cat((\n                        x[:, 0:1, :],\n                        self.prompt_dropout(\n                            self.deep_prompt_embeddings[i - 1].expand(B, -1, -1)\n                        ),\n                        x[:, (1 + self.num_tokens):, :]\n                    ), dim=1)\n                    x = self.blocks[i](x)\n\n        else:\n            # clamp temperatures not to be too small or too large\n            if self.temp_learn:\n                self.reinit_temp()\n                    \n            for i, blk in enumerate(self.blocks):\n                # current block's input prompt representation\n                if self.prompt_config.GATE_PRIOR and i < self.gate_logit.shape[0]:\n                    gate = self.gate_logit[i].sigmoid()\n                    prompt_in = x[:, 1: 1+self.prompt_config.NUM_TOKENS, :]\n\n                # block-wise learnable temperature\n                temp = self.temp if not isinstance(self.temp, nn.Parameter) else self.temp[i]\n                \n                x = blk(x, temp=temp)\n                if self.prompt_config.GATE_PRIOR and i < self.gate_logit.shape[0]:\n                    # current block's output prompt representation\n                    prompt_out = x[:, 1: 1+self.prompt_config.NUM_TOKENS, :]\n                    # convex combinate input and output prompt representations of current block via learnalbe gate\n                    x = torch.cat([\n                        x[:, 0:1, :], \n                        gate * prompt_out + (1 - gate) * prompt_in, \n                        x[:, 1+self.prompt_config.NUM_TOKENS:, :]\n                    ], dim=1)\n        \n        norm_func = self.norm\n        if self.prompt_config.VIT_POOL_TYPE == \"imgprompt_pool\":\n            assert self.prompt_config.LOCATION == \"prepend\"\n            x = x[:, 1:, :].mean(dim=1)  # global pool without cls token\n            outcome = norm_func(x)\n        elif self.prompt_config.VIT_POOL_TYPE == \"original\":\n            x = x[:, 1:, :].mean(dim=1)  # global pool without cls token\n            outcome = norm_func(x)\n        elif self.prompt_config.VIT_POOL_TYPE == \"img_pool\":\n            assert self.prompt_config.LOCATION == \"prepend\"\n            x = x[:, self.num_tokens+1:, :].mean(dim=1)\n            outcome = norm_func(x)\n        elif self.prompt_config.VIT_POOL_TYPE == \"prompt_pool\":\n            assert self.prompt_config.LOCATION == \"prepend\"\n            x = x[:, 1:self.num_tokens+1, :].mean(dim=1)\n            outcome = norm_func(x)\n        else:\n            raise ValueError(\"pooling type for output is not supported\")\n\n        \n        return outcome", "\n\ndef build_model(model_type, prompt_cfg):\n    if \"vitb\" in model_type:\n        return vit_base_patch16(prompt_cfg)\n    elif \"vitl\" in model_type:\n        return vit_large_patch16(prompt_cfg)\n    elif \"vith\" in model_type:\n        return vit_huge_patch14(prompt_cfg)\n", "\n\ndef vit_base_patch16(prompt_cfg, **kwargs):\n    model = PromptedVisionTransformer(\n        prompt_cfg,\n        drop_path_rate=0.1, global_pool=True,  # using default settings for mae-finetune\n        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n        mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model", "\n\ndef vit_large_patch16(prompt_cfg, **kwargs):\n    model = PromptedVisionTransformer(\n        prompt_cfg,\n        drop_path_rate=0.1, global_pool=True,  # using default settings for mae-finetune\n        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n        mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model", "\n\ndef vit_huge_patch14(prompt_cfg, **kwargs):\n    model = PromptedVisionTransformer(\n        prompt_cfg,\n        drop_path_rate=0.1, global_pool=True,  # using default settings for mae-finetune\n        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n        mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    return model", ""]}
{"filename": "src/models/vit_prompt/vit_moco.py", "chunked_list": ["#!/usr/bin/env python3\n\"\"\"\nvit-moco-v3 with prompt\n\"\"\"\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision as tv\n", "import torchvision as tv\n\nfrom functools import partial, reduce\nfrom operator import mul\nfrom torch.nn import Conv2d, Dropout\nfrom timm.models.vision_transformer import _cfg\n\nfrom ..vit_backbones.vit_moco import VisionTransformerMoCo\nfrom ...utils import logging\nlogger = logging.get_logger(\"visual_prompt\")", "from ...utils import logging\nlogger = logging.get_logger(\"visual_prompt\")\n\n\nclass PromptedVisionTransformerMoCo(VisionTransformerMoCo):\n    def __init__(self, prompt_config, **kwargs):\n        super().__init__(**kwargs)\n        self.prompt_config = prompt_config\n\n        if self.prompt_config.DEEP and self.prompt_config.LOCATION not in [\"prepend\", ]:\n            raise ValueError(\"Deep-{} is not supported\".format(self.prompt_config.LOCATION))\n\n        num_tokens = self.prompt_config.NUM_TOKENS\n\n        self.num_tokens = num_tokens\n        self.prompt_dropout = Dropout(self.prompt_config.DROPOUT)\n        \n        # define temperature for attention shaping\n        self.temp = self.prompt_config.TEMP\n        self.temp_learn = self.prompt_config.TEMP_LEARN\n        if self.temp_learn:\n            self.temp = nn.Parameter(torch.ones(prompt_config.TEMP_NUM))\n\n        # initiate prompt:\n        if self.prompt_config.INITIATION == \"random\":\n            val = math.sqrt(6. / float(3 * reduce(mul, self.patch_embed.patch_size, 1) + self.embed_dim))  # noqa\n\n            self.prompt_embeddings = nn.Parameter(torch.zeros(\n                1, num_tokens, self.embed_dim))\n            # xavier_uniform initialization\n            nn.init.uniform_(self.prompt_embeddings.data, -val, val)\n\n            if self.prompt_config.DEEP:\n                self.deep_prompt_embeddings = nn.Parameter(torch.zeros(\n                    len(self.blocks) - 1,\n                    num_tokens, self.embed_dim\n                ))\n                # xavier_uniform initialization\n                nn.init.uniform_(\n                    self.deep_prompt_embeddings.data, -val, val)\n        else:\n            raise ValueError(\"Other initiation scheme is not supported\")\n        \n        # define block-wise learnable gate scalar\n        if self.prompt_config.GATE_PRIOR:       \n            gate_logit = (-torch.ones(self.prompt_config.GATE_NUM) * self.prompt_config.GATE_INIT)        \n            self.gate_logit = nn.Parameter(gate_logit)\n            print(self.gate_logit)\n       \n    def incorporate_prompt(self, x):\n        # combine prompt embeddings with image-patch embeddings\n        B = x.shape[0]\n        if self.prompt_config.LOCATION == \"prepend\":\n            # after CLS token, all before image patches\n            x = self.embeddings(x)  # (batch_size, 1 + n_patches, hidden_dim)\n            x = torch.cat((\n                    x[:, :1, :],\n                    self.prompt_dropout(\n                        self.prompt_embeddings.expand(B, -1, -1)),\n                    x[:, 1:, :]\n                ), dim=1)\n            # (batch_size, cls_token + n_prompt + n_patches, hidden_dim)\n        else:\n            raise ValueError(\"Other prompt locations are not supported\")\n        return x\n\n    def embeddings(self, x):\n        x = self.patch_embed(x)\n        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n        if self.dist_token is None:\n            x = torch.cat((cls_token, x), dim=1)\n        else:\n            x = torch.cat((\n                cls_token, self.dist_token.expand(x.shape[0], -1, -1), x),\n            dim=1)\n        x = self.pos_drop(x + self.pos_embed)\n        return x\n\n    def train(self, mode=True):\n        # set train status for this class: disable all but the prompt-related modules\n        if mode:\n            # training:\n            self.blocks.eval()\n            self.patch_embed.eval()\n            self.pos_drop.eval()\n            self.prompt_dropout.train()\n        else:\n            # eval:\n            for module in self.children():\n                module.train(mode)\n                \n    def reinit_temp(self):\n        assert self.temp_learn, \"reinit_temp() could be run only when config.TEMP_LEARN == True\"\n        self.temp.data.copy_(self.temp.data.clamp(min=self.prompt_config.TEMP_MIN, max=self.prompt_config.TEMP_MAX))\n\n    def forward_features(self, x):\n        x = self.incorporate_prompt(x)\n            \n        # deep\n        if self.prompt_config.DEEP:\n            B = x.shape[0]\n            num_layers = len(self.blocks)\n\n            for i in range(num_layers):\n                if i == 0:\n                    x = self.blocks[i](x)\n                else:\n                    # prepend\n                    x = torch.cat((\n                        x[:, :1, :],\n                        self.prompt_dropout(\n                            self.deep_prompt_embeddings[i - 1].expand(B, -1, -1)\n                        ),\n                        x[:, (1 + self.num_tokens):, :]\n                    ), dim=1)\n                    x = self.blocks[i](x)\n               \n        else:\n            # clamp temperatures not to be too small or too large\n            if self.temp_learn:\n                self.reinit_temp()\n\n            for i, blk in enumerate(self.blocks):\n                # current block's input prompt representation\n                if self.prompt_config.GATE_PRIOR and i < self.gate_logit.shape[0]:\n                    gate = self.gate_logit[i].sigmoid()\n                    prompt_in = x[:, 1: 1+self.prompt_config.NUM_TOKENS, :]\n\n                # block-wise learnable temperature\n                temp = self.temp if not isinstance(self.temp, nn.Parameter) else self.temp[i]\n                \n                x = blk(x, temp=temp)\n                if self.prompt_config.GATE_PRIOR and i < self.gate_logit.shape[0]:\n                    # current block's output prompt representation\n                    prompt_out = x[:, 1: 1+self.prompt_config.NUM_TOKENS, :]\n                    # convex combinate input and output prompt representations of current block via learnalbe gate\n                    x = torch.cat([\n                        x[:, 0:1, :], \n                        gate * prompt_out + (1 - gate) * prompt_in, \n                        x[:, 1+self.prompt_config.NUM_TOKENS:, :]\n                    ], dim=1)\n\n        norm_func = self.norm \n        if self.prompt_config.VIT_POOL_TYPE == \"imgprompt_pool\":\n            assert self.prompt_config.LOCATION == \"prepend\"\n            outcome = norm_func(x[:, 1:, :].mean(dim=1))  # global pool without cls token\n            \n        elif self.prompt_config.VIT_POOL_TYPE == \"original\":\n            x = norm_func(x)\n            outcome = x[:, 0]\n            \n        elif self.prompt_config.VIT_POOL_TYPE == \"img_pool\":\n            assert self.prompt_config.LOCATION == \"prepend\"\n            outcome = norm_func(x[:, self.num_tokens+1:, :].mean(dim=1))\n        elif self.prompt_config.VIT_POOL_TYPE == \"prompt_pool\":\n            assert self.prompt_config.LOCATION == \"prepend\"\n            outcome = norm_func(x[:, 1:self.num_tokens+1, :].mean(dim=1))\n           \n        else:\n            raise ValueError(\"pooling type for output is not supported\")\n    \n        return outcome", "\n\ndef build_model(model_type, prompt_cfg):\n    if \"vitb\" in model_type:\n        return vit_base(prompt_cfg)\n    elif \"vits\" in model_type:\n        return vit_small(prompt_cfg)\n    \n\ndef vit_small(prompt_cfg, **kwargs):\n    model = PromptedVisionTransformerMoCo(\n        prompt_cfg,\n        patch_size=16, embed_dim=384, depth=12, drop_path_rate=0.1,\n        num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    return model", "\ndef vit_small(prompt_cfg, **kwargs):\n    model = PromptedVisionTransformerMoCo(\n        prompt_cfg,\n        patch_size=16, embed_dim=384, depth=12, drop_path_rate=0.1,\n        num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    return model\n", "\n\ndef vit_base(prompt_cfg, **kwargs):\n    model = PromptedVisionTransformerMoCo(\n        prompt_cfg,\n        patch_size=16, embed_dim=768, depth=12, drop_path_rate=0.1,\n        num_heads=12, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n    model.default_cfg = _cfg()\n    return model", "\n"]}
