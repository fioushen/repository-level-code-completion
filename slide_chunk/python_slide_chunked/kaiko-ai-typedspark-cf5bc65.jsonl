{"filename": "setup.py", "chunked_list": ["from setuptools import find_packages, setup\n\n\ndef get_requirements():\n    with open(\"requirements.txt\") as f:\n        return f.read().splitlines()\n\n\ndef get_long_description():\n    with open(\"README.rst\", encoding=\"utf-8\") as f:\n        return f.read()", "def get_long_description():\n    with open(\"README.rst\", encoding=\"utf-8\") as f:\n        return f.read()\n\n\nsetup(\n    name=\"typedspark\",\n    url=\"https://github.com/kaiko-ai/typedspark\",\n    license=\"Apache-2.0\",\n    author=\"Nanne Aben\",", "    license=\"Apache-2.0\",\n    author=\"Nanne Aben\",\n    author_email=\"nanne@kaiko.ai\",\n    description=\"Column-wise type annotations for pyspark DataFrames\",\n    keywords=\"pyspark spark typing type checking annotations\",\n    long_description=get_long_description(),\n    long_description_content_type=\"text/x-rst\",\n    packages=find_packages(include=[\"typedspark\", \"typedspark.*\"]),\n    install_requires=get_requirements(),\n    python_requires=\">=3.9.0\",", "    install_requires=get_requirements(),\n    python_requires=\">=3.9.0\",\n    classifiers=[\"Programming Language :: Python\", \"Typing :: Typed\"],\n    version_config=True,\n    setup_requires=[\"setuptools-git-versioning\"],\n    package_data={\"typedspark\": [\"py.typed\"]},\n    extras_require={\n        \"pyspark\": [\"pyspark\"],\n    },\n)", "    },\n)\n"]}
{"filename": "typedspark/__init__.py", "chunked_list": ["\"\"\"Typedspark: column-wise type annotations for pyspark DataFrames.\"\"\"\n\nfrom typedspark._core.column import Column\nfrom typedspark._core.column_meta import ColumnMeta\nfrom typedspark._core.dataset import DataSet\nfrom typedspark._core.datatypes import (\n    ArrayType,\n    DayTimeIntervalType,\n    DecimalType,\n    MapType,", "    DecimalType,\n    MapType,\n    StructType,\n)\nfrom typedspark._core.literaltype import IntervalType\nfrom typedspark._schema.schema import MetaSchema, Schema\nfrom typedspark._transforms.structtype_column import structtype_column\nfrom typedspark._transforms.transform_to_schema import transform_to_schema\nfrom typedspark._utils.create_dataset import (\n    create_empty_dataset,", "from typedspark._utils.create_dataset import (\n    create_empty_dataset,\n    create_partially_filled_dataset,\n    create_structtype_row,\n)\nfrom typedspark._utils.databases import Catalogs, Database, Databases\nfrom typedspark._utils.load_table import create_schema, load_table\nfrom typedspark._utils.register_schema_to_dataset import register_schema_to_dataset\n\n__all__ = [", "\n__all__ = [\n    \"ArrayType\",\n    \"Catalogs\",\n    \"Column\",\n    \"ColumnMeta\",\n    \"Database\",\n    \"Databases\",\n    \"DataSet\",\n    \"DayTimeIntervalType\",", "    \"DataSet\",\n    \"DayTimeIntervalType\",\n    \"DecimalType\",\n    \"IntervalType\",\n    \"MapType\",\n    \"MetaSchema\",\n    \"Schema\",\n    \"StructType\",\n    \"create_empty_dataset\",\n    \"create_partially_filled_dataset\",", "    \"create_empty_dataset\",\n    \"create_partially_filled_dataset\",\n    \"create_structtype_row\",\n    \"create_schema\",\n    \"load_table\",\n    \"register_schema_to_dataset\",\n    \"structtype_column\",\n    \"transform_to_schema\",\n]\n", "]\n"]}
{"filename": "typedspark/_transforms/structtype_column.py", "chunked_list": ["\"\"\"Functionality for dealing with StructType columns.\"\"\"\n\nfrom typing import Dict, Optional, Type\n\nfrom pyspark.sql import Column as SparkColumn\nfrom pyspark.sql.functions import struct\n\nfrom typedspark._core.column import Column\nfrom typedspark._schema.schema import Schema\nfrom typedspark._transforms.utils import add_nulls_for_unspecified_columns, convert_keys_to_strings", "from typedspark._schema.schema import Schema\nfrom typedspark._transforms.utils import add_nulls_for_unspecified_columns, convert_keys_to_strings\n\n\ndef structtype_column(\n    schema: Type[Schema],\n    transformations: Optional[Dict[Column, SparkColumn]] = None,\n    fill_unspecified_columns_with_nulls: bool = False,\n) -> SparkColumn:\n    \"\"\"Helps with creating new ``StructType`` columns of a certain schema, for\n    example:\n\n    .. code-block:: python\n\n        transform_to_schema(\n            df,\n            Output,\n            {\n                Output.values: structtype_column(\n                    Value,\n                    {\n                        Value.a: Input.a + 2,\n                        ...\n                    }\n                )\n            }\n        )\n    \"\"\"\n    _transformations = convert_keys_to_strings(transformations)\n\n    if fill_unspecified_columns_with_nulls:\n        _transformations = add_nulls_for_unspecified_columns(_transformations, schema)\n\n    _transformations = _order_columns(_transformations, schema)\n\n    return struct([v.alias(k) for k, v in _transformations.items()])", "\n\ndef _order_columns(\n    transformations: Dict[str, SparkColumn], schema: Type[Schema]\n) -> Dict[str, SparkColumn]:\n    \"\"\"Chispa's DataFrame comparer doesn't deal nicely with StructTypes whose columns\n    are ordered differently, hence we order them the same as in the schema here.\"\"\"\n    transformations_ordered = {}\n    for field in schema.get_structtype().fields:\n        transformations_ordered[field.name] = transformations[field.name]\n\n    return transformations_ordered", ""]}
{"filename": "typedspark/_transforms/__init__.py", "chunked_list": [""]}
{"filename": "typedspark/_transforms/utils.py", "chunked_list": ["\"\"\"Util functions for typedspark._transforms.\"\"\"\nfrom typing import Dict, List, Optional, Type\n\nfrom pyspark.sql import Column as SparkColumn\nfrom pyspark.sql.functions import lit\n\nfrom typedspark._core.column import Column\nfrom typedspark._schema.schema import Schema\n\n\ndef add_nulls_for_unspecified_columns(\n    transformations: Dict[str, SparkColumn],\n    schema: Type[Schema],\n    previously_existing_columns: Optional[List[str]] = None,\n) -> Dict[str, SparkColumn]:\n    \"\"\"Takes the columns from the schema that are not present in the transformation\n    dictionary and sets their values to Null (casted to the corresponding type defined\n    in the schema).\"\"\"\n    _previously_existing_columns = (\n        [] if previously_existing_columns is None else previously_existing_columns\n    )\n    for field in schema.get_structtype().fields:\n        if field.name not in transformations and field.name not in _previously_existing_columns:\n            transformations[field.name] = lit(None).cast(field.dataType)\n\n    return transformations", "\n\ndef add_nulls_for_unspecified_columns(\n    transformations: Dict[str, SparkColumn],\n    schema: Type[Schema],\n    previously_existing_columns: Optional[List[str]] = None,\n) -> Dict[str, SparkColumn]:\n    \"\"\"Takes the columns from the schema that are not present in the transformation\n    dictionary and sets their values to Null (casted to the corresponding type defined\n    in the schema).\"\"\"\n    _previously_existing_columns = (\n        [] if previously_existing_columns is None else previously_existing_columns\n    )\n    for field in schema.get_structtype().fields:\n        if field.name not in transformations and field.name not in _previously_existing_columns:\n            transformations[field.name] = lit(None).cast(field.dataType)\n\n    return transformations", "\n\ndef convert_keys_to_strings(\n    transformations: Optional[Dict[Column, SparkColumn]]\n) -> Dict[str, SparkColumn]:\n    \"\"\"Takes the Column keys in transformations and converts them to strings.\"\"\"\n    if transformations is None:\n        return {}\n\n    _transformations = {k.str: v for k, v in transformations.items()}\n\n    if len(transformations) != len(_transformations):\n        raise ValueError(\n            \"The transformations dictionary requires columns with unique names as keys.\"\n            + \"It is currently not possible to have ambiguous column names here,\"\n            + \"even when used in combination with register_schema_to_dataset().\"\n        )\n\n    return _transformations", ""]}
{"filename": "typedspark/_transforms/transform_to_schema.py", "chunked_list": ["\"\"\"Module containing functions that are related to transformations to DataSets.\"\"\"\nfrom functools import reduce\nfrom typing import Dict, Optional, Type, TypeVar\n\nfrom pyspark.sql import Column as SparkColumn\nfrom pyspark.sql import DataFrame\n\nfrom typedspark._core.column import Column\nfrom typedspark._core.dataset import DataSet\nfrom typedspark._schema.schema import Schema", "from typedspark._core.dataset import DataSet\nfrom typedspark._schema.schema import Schema\nfrom typedspark._transforms.utils import add_nulls_for_unspecified_columns, convert_keys_to_strings\n\nT = TypeVar(\"T\", bound=Schema)\n\n\ndef transform_to_schema(\n    dataframe: DataFrame,\n    schema: Type[T],\n    transformations: Optional[Dict[Column, SparkColumn]] = None,\n    fill_unspecified_columns_with_nulls: bool = False,\n) -> DataSet[T]:\n    \"\"\"On the provided DataFrame ``df``, it performs the ``transformations``\n    (if provided), and subsequently subsets the resulting DataFrame to the\n    columns specified in ``schema``.\n\n    .. code-block:: python\n\n        transform_to_schema(\n            df_a.join(df_b, A.a == B.f),\n            AB,\n            {\n                AB.a: A.a + 3,\n                AB.b: A.b + 7,\n                AB.i: B.i - 5,\n                AB.j: B.j + 1,\n            }\n        )\n    \"\"\"\n    _transformations = convert_keys_to_strings(transformations)\n\n    if fill_unspecified_columns_with_nulls:\n        _transformations = add_nulls_for_unspecified_columns(\n            _transformations, schema, previously_existing_columns=dataframe.columns\n        )\n\n    return DataSet[schema](  # type: ignore\n        reduce(\n            lambda acc, key: DataFrame.withColumn(acc, key, _transformations[key]),\n            _transformations.keys(),\n            dataframe,\n        ).select(*schema.all_column_names())\n    )", ""]}
{"filename": "typedspark/_utils/databases.py", "chunked_list": ["\"\"\"Loads all catalogs, databases and tables in a SparkSession.\"\"\"\n\nfrom abc import ABC\nfrom datetime import datetime\nfrom typing import Optional, Tuple, TypeVar\nfrom warnings import warn\n\nfrom pyspark.sql import Row, SparkSession\n\nfrom typedspark._core.dataset import DataSet", "\nfrom typedspark._core.dataset import DataSet\nfrom typedspark._schema.schema import Schema\nfrom typedspark._utils.camelcase import to_camel_case\nfrom typedspark._utils.load_table import load_table\n\nT = TypeVar(\"T\", bound=Schema)\n\n\nclass Timeout(ABC):\n    \"\"\"Warns the user if loading databases or catalogs is taking too long.\"\"\"\n\n    _TIMEOUT_WARNING: str\n\n    def __init__(self, silent: bool, n: int):  # pylint: disable=invalid-name\n        self._start = datetime.now()\n        self._silent = silent\n        self._n = n\n\n    def check_for_warning(self, i: int):  # pragma: no cover\n        \"\"\"Checks if a warning should be issued.\"\"\"\n        if self._silent:\n            return\n\n        diff = datetime.now() - self._start\n        if diff.seconds > 10:\n            warn(self._TIMEOUT_WARNING.format(i, self._n))\n            self._silent = True", "\nclass Timeout(ABC):\n    \"\"\"Warns the user if loading databases or catalogs is taking too long.\"\"\"\n\n    _TIMEOUT_WARNING: str\n\n    def __init__(self, silent: bool, n: int):  # pylint: disable=invalid-name\n        self._start = datetime.now()\n        self._silent = silent\n        self._n = n\n\n    def check_for_warning(self, i: int):  # pragma: no cover\n        \"\"\"Checks if a warning should be issued.\"\"\"\n        if self._silent:\n            return\n\n        diff = datetime.now() - self._start\n        if diff.seconds > 10:\n            warn(self._TIMEOUT_WARNING.format(i, self._n))\n            self._silent = True", "\n\nclass DatabasesTimeout(Timeout):\n    \"\"\"Warns the user if Databases() is taking too long.\"\"\"\n\n    _TIMEOUT_WARNING = \"\"\"\nDatabases() is taking longer than 10 seconds. So far, {} out of {} databases have been loaded.\nIf this is too slow, consider loading a single database using:\n\nfrom typedspark import Database\n\ndb = Database(spark, db_name=...)\n\"\"\"", "\n\nclass CatalogsTimeout(Timeout):\n    \"\"\"Warns the user if Catalogs() is taking too long.\"\"\"\n\n    _TIMEOUT_WARNING = \"\"\"\nCatalogs() is taking longer than 10 seconds. So far, {} out of {} catalogs have been loaded.\nIf this is too slow, consider loading a single catalog using:\n\nfrom typedspark import Databases\n\ndb = Databases(spark, catalog_name=...)\n\"\"\"", "\n\nclass Table:\n    \"\"\"Loads a table in a database.\"\"\"\n\n    def __init__(self, spark: SparkSession, db_name: str, table_name: str, is_temporary: bool):\n        self._spark = spark\n        self._db_name = db_name\n        self._table_name = table_name\n        self._is_temporary = is_temporary\n\n    @property\n    def str(self) -> str:\n        \"\"\"Returns the path to the table, e.g. ``default.person``.\n\n        While temporary tables are always stored in the ``default`` db, they are saved and\n        loaded directly from their table name, e.g. ``person``.\n\n        Non-temporary tables are saved and loaded from their full name, e.g.\n        ``default.person``.\n        \"\"\"\n        if self._is_temporary:\n            return self._table_name\n\n        return f\"{self._db_name}.{self._table_name}\"\n\n    def load(self) -> Tuple[DataSet[T], T]:\n        \"\"\"Loads the table as a DataSet[T] and returns the schema.\"\"\"\n        return load_table(  # type: ignore\n            self._spark,\n            self.str,\n            to_camel_case(self._table_name),\n        )", "\n\nclass Database:\n    \"\"\"Loads all tables in a database.\"\"\"\n\n    def __init__(self, spark: SparkSession, db_name: str, catalog_name: Optional[str] = None):\n        if catalog_name is None:\n            self._db_name = db_name\n        else:\n            self._db_name = f\"{catalog_name}.{db_name}\"\n\n        tables = spark.sql(f\"show tables from {self._db_name}\").collect()\n        for table in tables:\n            table_name = table.tableName\n            self.__setattr__(\n                table_name,\n                Table(spark, self._db_name, table_name, table.isTemporary),\n            )\n\n    @property\n    def str(self) -> str:\n        \"\"\"Returns the database name.\"\"\"\n        return self._db_name", "\n\nclass Databases:\n    \"\"\"Loads all databases and tables in a SparkSession.\"\"\"\n\n    def __init__(\n        self, spark: SparkSession, silent: bool = False, catalog_name: Optional[str] = None\n    ):\n        if catalog_name is None:\n            query = \"show databases\"\n        else:\n            query = f\"show databases in {catalog_name}\"\n\n        databases = spark.sql(query).collect()\n        timeout = DatabasesTimeout(silent, n=len(databases))\n\n        for i, database in enumerate(databases):\n            timeout.check_for_warning(i)\n            db_name = self._extract_db_name(database)\n            self.__setattr__(db_name, Database(spark, db_name, catalog_name))\n\n    def _extract_db_name(self, database: Row) -> str:\n        \"\"\"Extracts the database name from a Row.\n\n        Old versions of Spark use ``databaseName``, newer versions use ``namespace``.\n        \"\"\"\n        if hasattr(database, \"databaseName\"):  # pragma: no cover\n            return database.databaseName\n        if hasattr(database, \"namespace\"):\n            return database.namespace\n\n        raise ValueError(f\"Could not find database name in {database}.\")  # pragma: no cover", "\n\nclass Catalogs:\n    \"\"\"Loads all catalogs, databases and tables in a SparkSession.\"\"\"\n\n    def __init__(self, spark: SparkSession, silent: bool = False):\n        catalogs = spark.sql(\"show catalogs\").collect()\n        timeout = CatalogsTimeout(silent, n=len(catalogs))\n\n        for i, catalog in enumerate(catalogs):\n            timeout.check_for_warning(i)\n            catalog_name: str = catalog.catalog\n            self.__setattr__(\n                catalog_name,\n                Databases(spark, silent=True, catalog_name=catalog_name),\n            )", ""]}
{"filename": "typedspark/_utils/camelcase.py", "chunked_list": ["\"\"\"Utility function for converting from snake case to camel case.\"\"\"\n\n\ndef to_camel_case(name: str) -> str:\n    \"\"\"Converts a string to camel case.\"\"\"\n    return \"\".join([word.capitalize() for word in name.split(\"_\")])\n"]}
{"filename": "typedspark/_utils/load_table.py", "chunked_list": ["\"\"\"Functions for loading `DataSet` and `Schema` in notebooks.\"\"\"\n\nimport re\nfrom typing import Dict, Literal, Optional, Tuple, Type\n\nfrom pyspark.sql import DataFrame, SparkSession\nfrom pyspark.sql.types import ArrayType as SparkArrayType\nfrom pyspark.sql.types import DataType\nfrom pyspark.sql.types import DayTimeIntervalType as SparkDayTimeIntervalType\nfrom pyspark.sql.types import DecimalType as SparkDecimalType", "from pyspark.sql.types import DayTimeIntervalType as SparkDayTimeIntervalType\nfrom pyspark.sql.types import DecimalType as SparkDecimalType\nfrom pyspark.sql.types import MapType as SparkMapType\nfrom pyspark.sql.types import StructType as SparkStructType\n\nfrom typedspark._core.column import Column\nfrom typedspark._core.dataset import DataSet\nfrom typedspark._core.datatypes import (\n    ArrayType,\n    DayTimeIntervalType,", "    ArrayType,\n    DayTimeIntervalType,\n    DecimalType,\n    MapType,\n    StructType,\n)\nfrom typedspark._schema.schema import MetaSchema, Schema\nfrom typedspark._utils.camelcase import to_camel_case\nfrom typedspark._utils.register_schema_to_dataset import register_schema_to_dataset\n", "from typedspark._utils.register_schema_to_dataset import register_schema_to_dataset\n\n\ndef _replace_illegal_column_names(dataframe: DataFrame) -> DataFrame:\n    \"\"\"Replaces illegal column names with a legal version.\"\"\"\n    mapping = _create_mapping(dataframe)\n\n    for column, column_renamed in mapping.items():\n        if column != column_renamed:\n            dataframe = dataframe.withColumnRenamed(column, column_renamed)\n\n    return dataframe", "\n\ndef _create_mapping(dataframe: DataFrame) -> Dict[str, str]:\n    \"\"\"Checks if there are duplicate columns after replacing illegal characters.\"\"\"\n    mapping = {column: _replace_illegal_characters(column) for column in dataframe.columns}\n    renamed_columns = list(mapping.values())\n    duplicates = {\n        column: column_renamed\n        for column, column_renamed in mapping.items()\n        if renamed_columns.count(column_renamed) > 1\n    }\n\n    if len(duplicates) > 0:\n        raise ValueError(\n            \"You're trying to dynamically generate a Schema from a DataFrame. \"\n            + \"However, typedspark has detected that the DataFrame contains duplicate columns \"\n            + \"after replacing illegal characters (e.g. whitespaces, dots, etc.).\\n\"\n            + \"The folowing columns have lead to duplicates:\\n\"\n            + f\"{duplicates}\\n\\n\"\n            + \"Please rename these columns in your DataFrame.\"\n        )\n\n    return mapping", "\n\ndef _replace_illegal_characters(column_name: str) -> str:\n    \"\"\"Replaces illegal characters in a column name with an underscore.\"\"\"\n    return re.sub(\"[^A-Za-z0-9]\", \"_\", column_name)\n\n\ndef _create_schema(structtype: SparkStructType, schema_name: Optional[str] = None) -> Type[Schema]:\n    \"\"\"Dynamically builds a ``Schema`` based on a ``DataFrame``'s\n    ``StructType``\"\"\"\n    type_annotations = {}\n    attributes: Dict[str, None] = {}\n    for column in structtype:\n        name = column.name\n        data_type = _extract_data_type(column.dataType, name)\n        type_annotations[name] = Column[data_type]  # type: ignore\n        attributes[name] = None\n\n    if not schema_name:\n        schema_name = \"DynamicallyLoadedSchema\"\n\n    schema = MetaSchema(schema_name, tuple([Schema]), attributes)\n    schema.__annotations__ = type_annotations\n\n    return schema  # type: ignore", "\n\ndef _extract_data_type(dtype: DataType, name: str) -> Type[DataType]:\n    \"\"\"Given an instance of a ``DataType``, it extracts the corresponding\n    ``DataType`` class, potentially including annotations (e.g.\n    ``ArrayType[StringType]``).\"\"\"\n    if isinstance(dtype, SparkArrayType):\n        element_type = _extract_data_type(dtype.elementType, name)\n        return ArrayType[element_type]  # type: ignore\n\n    if isinstance(dtype, SparkMapType):\n        key_type = _extract_data_type(dtype.keyType, name)\n        value_type = _extract_data_type(dtype.valueType, name)\n        return MapType[key_type, value_type]  # type: ignore\n\n    if isinstance(dtype, SparkStructType):\n        subschema = _create_schema(dtype, to_camel_case(name))\n        return StructType[subschema]  # type: ignore\n\n    if isinstance(dtype, SparkDayTimeIntervalType):\n        start_field = dtype.startField\n        end_field = dtype.endField\n        return DayTimeIntervalType[Literal[start_field], Literal[end_field]]  # type: ignore\n\n    if isinstance(dtype, SparkDecimalType):\n        precision = dtype.precision\n        scale = dtype.scale\n        return DecimalType[Literal[precision], Literal[scale]]  # type: ignore\n\n    return type(dtype)", "\n\ndef create_schema(\n    dataframe: DataFrame, schema_name: Optional[str] = None\n) -> Tuple[DataSet[Schema], Type[Schema]]:\n    \"\"\"This function inferres a ``Schema`` in a notebook based on a the provided ``DataFrame``.\n\n    This allows for autocompletion on column names, amongst other\n    things.\n\n    .. code-block:: python\n\n        df, Person = create_schema(df)\n    \"\"\"\n    dataframe = _replace_illegal_column_names(dataframe)\n    schema = _create_schema(dataframe.schema, schema_name)\n    dataset = DataSet[schema](dataframe)  # type: ignore\n    schema = register_schema_to_dataset(dataset, schema)\n    return dataset, schema", "\n\ndef load_table(\n    spark: SparkSession, table_name: str, schema_name: Optional[str] = None\n) -> Tuple[DataSet[Schema], Type[Schema]]:\n    \"\"\"This function loads a ``DataSet``, along with its inferred ``Schema``,\n    in a notebook.\n\n    This allows for autocompletion on column names, amongst other\n    things.\n\n    .. code-block:: python\n\n        df, Person = load_table(spark, \"path.to.table\")\n    \"\"\"\n    dataframe = spark.table(table_name)\n    return create_schema(dataframe, schema_name)", ""]}
{"filename": "typedspark/_utils/__init__.py", "chunked_list": [""]}
{"filename": "typedspark/_utils/create_dataset.py", "chunked_list": ["\"\"\"Module containing functions related to creating a DataSet from scratch.\"\"\"\nfrom typing import Any, Dict, List, Type, TypeVar, Union, get_type_hints\n\nfrom pyspark.sql import Row, SparkSession\n\nfrom typedspark._core.column import Column\nfrom typedspark._core.dataset import DataSet\nfrom typedspark._schema.schema import Schema\n\nT = TypeVar(\"T\", bound=Schema)", "\nT = TypeVar(\"T\", bound=Schema)\n\n\ndef create_empty_dataset(spark: SparkSession, schema: Type[T], n_rows: int = 3) -> DataSet[T]:\n    \"\"\"Creates a ``DataSet`` with ``Schema`` schema, containing ``n_rows``\n    rows, filled with ``None`` values.\n\n    .. code-block:: python\n\n        class Person(Schema):\n            name: Column[StringType]\n            age: Column[LongType]\n\n        df = create_empty_dataset(spark, Person)\n    \"\"\"\n    n_cols = len(get_type_hints(schema))\n    rows = tuple([None] * n_cols)\n    data = [rows] * n_rows\n    spark_schema = schema.get_structtype()\n    dataframe = spark.createDataFrame(data, spark_schema)\n    return DataSet[schema](dataframe)  # type: ignore", "\n\ndef create_partially_filled_dataset(\n    spark: SparkSession,\n    schema: Type[T],\n    data: Union[Dict[Column, List[Any]], List[Dict[Column, Any]]],\n) -> DataSet[T]:\n    \"\"\"Creates a ``DataSet`` with ``Schema`` schema, where ``data`` can\n    be defined in either of the following two ways:\n\n    .. code-block:: python\n\n        class Person(Schema):\n            name: Column[StringType]\n            age: Column[LongType]\n            job: Column[StringType]\n\n        df = create_partially_filled_dataset(\n            spark,\n            Person,\n            {\n                Person.name: [\"John\", \"Jack\", \"Jane\"],\n                Person.age: [30, 40, 50],\n            }\n        )\n\n    Or:\n\n    .. code-block:: python\n\n        df = create_partially_filled_dataset(\n            spark,\n            Person,\n            [\n                {Person.name: \"John\", Person.age: 30},\n                {Person.name: \"Jack\", Person.age: 40},\n                {Person.name: \"Jane\", Person.age: 50},\n            ]\n        )\n\n    Any columns in the schema that are not present in the data will be\n    initialized with ``None`` values.\n    \"\"\"\n    if isinstance(data, list):\n        col_data = _create_column_wise_data_from_list(schema, data)\n    elif isinstance(data, dict):\n        col_data = _create_column_wise_data_from_dict(schema, data)\n    else:\n        raise ValueError(\"The provided data is not a list or a dict.\")\n\n    row_data = zip(*col_data)\n    spark_schema = schema.get_structtype()\n    dataframe = spark.createDataFrame(row_data, spark_schema)\n    return DataSet[schema](dataframe)  # type: ignore", "\n\ndef create_structtype_row(schema: Type[T], data: Dict[Column, Any]) -> Row:\n    \"\"\"Creates a ``Row`` with ``StructType`` schema, where ``data`` is a\n    mapping from column to data in the respective column.\"\"\"\n    data_with_string_index = {k.str: v for k, v in data.items()}\n    data_converted = {\n        k: data_with_string_index[k] if k in data_with_string_index else None\n        for k in get_type_hints(schema).keys()\n    }\n    return Row(**data_converted)", "\n\ndef _create_column_wise_data_from_dict(\n    schema: Type[T], data: Dict[Column, List[Any]]\n) -> List[List[Any]]:\n    \"\"\"Converts a dict of column to data to a list of lists, where each inner list\n    contains the data for a column.\"\"\"\n    data_converted = {k.str: v for k, v in data.items()}\n    n_rows_unique = {len(v) for _, v in data.items()}\n    if len(n_rows_unique) > 1:\n        raise ValueError(\"The number of rows in the provided data differs per column.\")\n\n    n_rows = list(n_rows_unique)[0]\n    col_data = []\n    for col in get_type_hints(schema).keys():\n        if col in data_converted:\n            col_data += [data_converted[col]]\n        else:\n            col_data += [[None] * n_rows]\n\n    return col_data", "\n\ndef _create_column_wise_data_from_list(\n    schema: Type[T], data: List[Dict[Column, Any]]\n) -> List[List[Any]]:\n    \"\"\"Converts a list of dicts of column to data to a list of lists, where each inner\n    list contains the data for a column.\"\"\"\n    data_converted = [{k.str: v for k, v in row.items()} for row in data]\n\n    col_data = []\n    for col in get_type_hints(schema).keys():\n        col_data += [[row[col] if col in row else None for row in data_converted]]\n\n    return col_data", ""]}
{"filename": "typedspark/_utils/register_schema_to_dataset.py", "chunked_list": ["\"\"\"Module containing functions that are related to registering schema's to DataSets.\"\"\"\nimport itertools\nfrom typing import Type, TypeVar\n\nfrom typedspark._core.dataset import DataSet\nfrom typedspark._schema.schema import Schema\n\nT = TypeVar(\"T\", bound=Schema)\n\n\ndef _counter(count: itertools.count = itertools.count()):\n    return next(count)", "\n\ndef _counter(count: itertools.count = itertools.count()):\n    return next(count)\n\n\ndef register_schema_to_dataset(dataframe: DataSet[T], schema: Type[T]) -> Type[T]:\n    \"\"\"Helps combat column ambiguity. For example:\n\n    .. code-block:: python\n\n        class Person(Schema):\n            id: Column[IntegerType]\n            name: Column[StringType]\n\n        class Job(Schema):\n            id: Column[IntegerType]\n            salary: Column[IntegerType]\n\n        class PersonWithJob(Person, Job):\n            pass\n\n        def foo(df_a: DataSet[Person], df_b: DataSet[Job]) -> DataSet[PersonWithJob]:\n            return DataSet[PersonWithSalary](\n                df_a.join(\n                    df_b,\n                    Person.id == Job.id\n                )\n            )\n\n    Calling ``foo()`` would result in a ``AnalysisException``, because Spark can't figure out\n    whether ``id`` belongs to ``df_a`` or ``df_b``. To deal with this, you need to register\n    your ``Schema`` to the ``DataSet``.\n\n    .. code-block:: python\n\n        from typedspark import register_schema_to_dataset\n\n        def foo(df_a: DataSet[Person], df_b: DataSet[Job]) -> DataSet[PersonWithSalary]:\n            person = register_schema_to_dataset(df_a, Person)\n            job = register_schema_to_dataset(df_b, Job)\n            return DataSet[PersonWithSalary](\n                df_a.join(\n                    df_b,\n                    person.id == job.id\n                )\n            )\n    \"\"\"\n\n    class LinkedSchema(schema):  # type: ignore\n        \"\"\"TypedSpark LinkedSchema.\n\n        Contains the DataFrame that this Schema is linked to.\n        \"\"\"\n\n        _parent = dataframe\n        _current_id = _counter()\n        _original_name = schema.get_schema_name()\n\n    return LinkedSchema  # type: ignore", ""]}
{"filename": "typedspark/_core/datatypes.py", "chunked_list": ["\"\"\"Here, we make our own definitions of ``MapType``, ``ArrayType`` and\n``StructType`` in order to allow e.g. for ``ArrayType[StringType]``.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, Dict, Generic, Type, TypeVar\n\nfrom pyspark.sql.types import DataType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from typedspark._schema.schema import Schema\n\n    _Schema = TypeVar(\"_Schema\", bound=Schema)\nelse:\n    _Schema = TypeVar(\"_Schema\")", "if TYPE_CHECKING:  # pragma: no cover\n    from typedspark._schema.schema import Schema\n\n    _Schema = TypeVar(\"_Schema\", bound=Schema)\nelse:\n    _Schema = TypeVar(\"_Schema\")\n\n_KeyType = TypeVar(\"_KeyType\", bound=DataType)  # pylint: disable=invalid-name\n_ValueType = TypeVar(\"_ValueType\", bound=DataType)  # pylint: disable=invalid-name\n_Precision = TypeVar(\"_Precision\", bound=int)  # pylint: disable=invalid-name", "_ValueType = TypeVar(\"_ValueType\", bound=DataType)  # pylint: disable=invalid-name\n_Precision = TypeVar(\"_Precision\", bound=int)  # pylint: disable=invalid-name\n_Scale = TypeVar(\"_Scale\", bound=int)  # pylint: disable=invalid-name\n_StartField = TypeVar(\"_StartField\", bound=int)  # pylint: disable=invalid-name\n_EndField = TypeVar(\"_EndField\", bound=int)  # pylint: disable=invalid-name\n\n\nclass TypedSparkDataType(DataType):\n    \"\"\"Base class for typedspark specific ``DataTypes``.\"\"\"\n\n    @classmethod\n    def get_name(cls) -> str:\n        \"\"\"Return the name of the type.\"\"\"\n        return cls.__name__", "\n\nclass StructTypeMeta(type):\n    \"\"\"Initializes the schema attribute as None.\n\n    This allows for auto-complete in Databricks notebooks (uninitialized variables don't\n    show up in auto-complete there).\n    \"\"\"\n\n    def __new__(cls, name: str, bases: Any, dct: Dict[str, Any]):\n        dct[\"schema\"] = None\n        return super().__new__(cls, name, bases, dct)", "\n\nclass StructType(Generic[_Schema], TypedSparkDataType, metaclass=StructTypeMeta):\n    \"\"\"Allows for type annotations such as:\n\n    .. code-block:: python\n\n        class Job(Schema):\n            position: Column[StringType]\n            salary: Column[LongType]\n\n        class Person(Schema):\n            job: Column[StructType[Job]]\n    \"\"\"\n\n    schema: Type[_Schema]", "\n\nclass MapType(Generic[_KeyType, _ValueType], TypedSparkDataType):\n    \"\"\"Allows for type annotations such as.\n\n    .. code-block:: python\n\n        class Basket(Schema):\n            items: Column[MapType[StringType, StringType]]\n    \"\"\"", "\n\nclass ArrayType(Generic[_ValueType], TypedSparkDataType):\n    \"\"\"Allows for type annotations such as.\n\n    .. code-block:: python\n\n        class Basket(Schema):\n            items: Column[ArrayType[StringType]]\n    \"\"\"", "\n\nclass DecimalType(Generic[_Precision, _Scale], TypedSparkDataType):\n    \"\"\"Allows for type annotations such as.\n\n    .. code-block:: python\n\n        class Numbers(Schema):\n            number: Column[DecimalType[Literal[10], Literal[0]]]\n    \"\"\"", "\n\nclass DayTimeIntervalType(Generic[_StartField, _EndField], TypedSparkDataType):\n    \"\"\"Allows for type annotations such as.\n\n    .. code-block:: python\n\n        class TimeInterval(Schema):\n            interval: Column[DayTimeIntervalType[IntervalType.HOUR, IntervalType.SECOND]\n    \"\"\"", ""]}
{"filename": "typedspark/_core/column_meta.py", "chunked_list": ["\"\"\"Metadata for ``Column`` objects that can be accessed during runtime.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Dict, Optional\n\n\n@dataclass\nclass ColumnMeta:\n    \"\"\"Contains the metadata for a ``Column``. Used as:\n\n    .. code-block:: python\n\n        class A(Schema):\n            a: Annotated[\n                Column[IntegerType],\n                ColumnMeta(\n                    comment=\"This is a comment\",\n                )\n            ]\n    \"\"\"\n\n    comment: Optional[str] = None\n\n    def get_metadata(self) -> Optional[Dict[str, str]]:\n        \"\"\"Returns the metadata of this column.\"\"\"\n        return {\"comment\": self.comment} if self.comment else None", ""]}
{"filename": "typedspark/_core/dataset.py", "chunked_list": ["\"\"\"Module containing classes and functions related to TypedSpark DataSets.\"\"\"\nfrom copy import deepcopy\nfrom typing import (\n    Any,\n    Callable,\n    Generic,\n    List,\n    Literal,\n    Optional,\n    Type,", "    Optional,\n    Type,\n    TypeVar,\n    Union,\n    get_args,\n    overload,\n)\n\nfrom pyspark.sql import Column as SparkColumn\nfrom pyspark.sql import DataFrame", "from pyspark.sql import Column as SparkColumn\nfrom pyspark.sql import DataFrame\nfrom typing_extensions import Concatenate, ParamSpec\n\nfrom typedspark._core.validate_schema import validate_schema\nfrom typedspark._schema.schema import Schema\n\nT = TypeVar(\"T\", bound=Schema)\n_ReturnType = TypeVar(\"_ReturnType\", bound=DataFrame)  # pylint: disable=C0103\nP = ParamSpec(\"P\")", "_ReturnType = TypeVar(\"_ReturnType\", bound=DataFrame)  # pylint: disable=C0103\nP = ParamSpec(\"P\")\n\n\nclass DataSet(DataFrame, Generic[T]):\n    \"\"\"``DataSet`` subclasses pyspark ``DataFrame`` and hence has all the same\n    functionality, with in addition the possibility to define a schema.\n\n    .. code-block:: python\n\n        class Person(Schema):\n            name: Column[StringType]\n            age: Column[LongType]\n\n        def foo(df: DataSet[Person]) -> DataSet[Person]:\n            # do stuff\n            return df\n    \"\"\"\n\n    def __new__(cls, dataframe: DataFrame) -> \"DataSet[T]\":\n        \"\"\"``__new__()`` instantiates the object (prior to ``__init__()``).\n\n        Here, we simply take the provided ``df`` and cast it to a\n        ``DataSet``. This allows us to bypass the ``DataFrame``\n        constuctor in ``__init__()``, which requires parameters that may\n        be difficult to access.\n        \"\"\"\n        dataframe.__class__ = DataSet\n        return dataframe  # type: ignore\n\n    def __init__(self, dataframe: DataFrame):\n        pass\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        \"\"\"Python base function that sets attributes.\n\n        We listen here for the setting of ``__orig_class__``, which\n        contains the ``Schema`` of the ``DataSet``. Note that this gets\n        set after ``__new__()`` and ``__init__()`` are finished.\n        \"\"\"\n        object.__setattr__(self, name, value)\n\n        if name == \"__orig_class__\":\n            orig_class_args = get_args(self.__orig_class__)\n            if orig_class_args and issubclass(orig_class_args[0], Schema):\n                self._schema_annotations: Type[Schema] = orig_class_args[0]\n                validate_schema(\n                    self._schema_annotations.get_structtype(),\n                    deepcopy(self.schema),\n                    self._schema_annotations.get_schema_name(),\n                )\n                self._add_schema_metadata()\n\n    def _add_schema_metadata(self) -> None:\n        \"\"\"Adds the ``ColumnMeta`` comments as metadata to the ``DataSet``.\n\n        Previously set metadata is deleted. Hence, if ``foo(dataframe: DataSet[A]) -> DataSet[B]``,\n        then ``DataSet[B]`` will not inherrit any metadata from ``DataSet[A]``.\n\n        Assumes validate_schema() in __setattr__() has been run.\n        \"\"\"\n        for field in self._schema_annotations.get_structtype().fields:\n            self.schema[field.name].metadata = field.metadata\n\n    \"\"\"The following functions are equivalent to their parents in ``DataFrame``, but since they\n    don't affect the ``Schema``, we can add type annotations here. We're omitting docstrings,\n    such that the docstring from the parent will appear.\"\"\"\n\n    def distinct(self) -> \"DataSet[T]\":  # pylint: disable=C0116\n        return DataSet[self._schema_annotations](super().distinct())  # type: ignore\n\n    def filter(self, condition) -> \"DataSet[T]\":  # pylint: disable=C0116\n        return DataSet[self._schema_annotations](super().filter(condition))  # type: ignore\n\n    @overload\n    def join(  # type: ignore\n        self,\n        other: DataFrame,\n        on: Optional[  # pylint: disable=C0103\n            Union[str, List[str], SparkColumn, List[SparkColumn]]\n        ] = ...,\n        how: None = ...,\n    ) -> DataFrame:\n        ...  # pragma: no cover\n\n    @overload\n    def join(\n        self,\n        other: DataFrame,\n        on: Optional[  # pylint: disable=C0103\n            Union[str, List[str], SparkColumn, List[SparkColumn]]\n        ] = ...,\n        how: Literal[\"semi\"] = ...,\n    ) -> \"DataSet[T]\":\n        ...  # pragma: no cover\n\n    @overload\n    def join(\n        self,\n        other: DataFrame,\n        on: Optional[  # pylint: disable=C0103\n            Union[str, List[str], SparkColumn, List[SparkColumn]]\n        ] = ...,\n        how: Optional[str] = ...,\n    ) -> DataFrame:\n        ...  # pragma: no cover\n\n    def join(  # pylint: disable=C0116\n        self,\n        other: DataFrame,\n        on: Optional[  # pylint: disable=C0103\n            Union[str, List[str], SparkColumn, List[SparkColumn]]\n        ] = None,\n        how: Optional[str] = None,\n    ) -> DataFrame:\n        return super().join(other, on, how)  # type: ignore\n\n    def orderBy(self, *args, **kwargs) -> \"DataSet[T]\":  # type: ignore  # noqa: N802, E501  # pylint: disable=C0116, C0103\n        return DataSet[self._schema_annotations](super().orderBy(*args, **kwargs))  # type: ignore\n\n    @overload\n    def transform(\n        self,\n        func: Callable[Concatenate[\"DataSet[T]\", P], _ReturnType],\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -> _ReturnType:\n        ...  # pragma: no cover\n\n    @overload\n    def transform(self, func: Callable[..., \"DataFrame\"], *args: Any, **kwargs: Any) -> \"DataFrame\":\n        ...  # pragma: no cover\n\n    def transform(  # pylint: disable=C0116\n        self, func: Callable[..., \"DataFrame\"], *args: Any, **kwargs: Any\n    ) -> \"DataFrame\":\n        return super().transform(func, *args, **kwargs)\n\n    @overload\n    def unionByName(  # noqa: N802  # pylint: disable=C0116, C0103\n        self,\n        other: \"DataSet[T]\",\n        allowMissingColumns: Literal[False] = ...,  # noqa: N803\n    ) -> \"DataSet[T]\":\n        ...  # pragma: no cover\n\n    @overload\n    def unionByName(  # noqa: N802  # pylint: disable=C0116, C0103\n        self,\n        other: DataFrame,\n        allowMissingColumns: bool = ...,  # noqa: N803\n    ) -> DataFrame:\n        ...  # pragma: no cover\n\n    def unionByName(  # noqa: N802  # pylint: disable=C0116, C0103\n        self,\n        other: DataFrame,\n        allowMissingColumns: bool = False,  # noqa: N803\n    ) -> DataFrame:\n        res = super().unionByName(other, allowMissingColumns)\n        if isinstance(other, DataSet) and other._schema_annotations == self._schema_annotations:\n            return DataSet[self._schema_annotations](res)  # type: ignore\n        return res  # pragma: no cover", ""]}
{"filename": "typedspark/_core/__init__.py", "chunked_list": [""]}
{"filename": "typedspark/_core/validate_schema.py", "chunked_list": ["\"\"\"Module containing functions that are related to validating schema's at runtime.\"\"\"\nfrom typing import Dict, Set\n\nfrom pyspark.sql.types import ArrayType, DataType, MapType, StructField, StructType\n\n\ndef validate_schema(\n    structtype_expected: StructType, structtype_observed: StructType, schema_name: str\n) -> None:\n    \"\"\"Checks whether the expected and the observed StructType match.\"\"\"\n    expected = unpack_schema(structtype_expected)\n    observed = unpack_schema(structtype_observed)\n\n    check_names(set(expected.keys()), set(observed.keys()), schema_name)\n    check_dtypes(expected, observed, schema_name)", "\n\ndef unpack_schema(schema: StructType) -> Dict[str, StructField]:\n    \"\"\"Converts the observed schema to a dictionary mapping column name to StructField.\n\n    We ignore columns that start with ``__``.\n    \"\"\"\n    res = {}\n    for field in schema.fields:\n        if field.name.startswith(\"__\"):\n            continue\n        field.nullable = True\n        field.metadata = {}\n        res[field.name] = field\n\n    return res", "\n\ndef check_names(names_expected: Set[str], names_observed: Set[str], schema_name: str) -> None:\n    \"\"\"Checks whether the observed and expected list of column names overlap.\n\n    Is order insensitive.\n    \"\"\"\n    diff = names_observed - names_expected\n    if diff:\n        raise TypeError(\n            f\"Data contains the following columns not present in schema {schema_name}: {diff}\"\n        )\n\n    diff = names_expected - names_observed\n    if diff:\n        raise TypeError(\n            f\"Schema {schema_name} contains the following columns not present in data: {diff}\"\n        )", "\n\ndef check_dtypes(\n    schema_expected: Dict[str, StructField],\n    schema_observed: Dict[str, StructField],\n    schema_name: str,\n) -> None:\n    \"\"\"Checks for each column whether the observed and expected data type match.\n\n    Is order insensitive.\n    \"\"\"\n    for name, structfield_expected in schema_expected.items():\n        structfield_observed = schema_observed[name]\n        check_dtype(\n            name,\n            structfield_expected.dataType,\n            structfield_observed.dataType,\n            schema_name,\n        )", "\n\ndef check_dtype(\n    colname: str, dtype_expected: DataType, dtype_observed: DataType, schema_name: str\n) -> None:\n    \"\"\"Checks whether the observed and expected data type match.\"\"\"\n    if dtype_expected == dtype_observed:\n        return None\n\n    if isinstance(dtype_expected, ArrayType) and isinstance(dtype_observed, ArrayType):\n        return check_dtype(\n            f\"{colname}.element_type\",\n            dtype_expected.elementType,\n            dtype_observed.elementType,\n            schema_name,\n        )\n\n    if isinstance(dtype_expected, MapType) and isinstance(dtype_observed, MapType):\n        check_dtype(\n            f\"{colname}.key\",\n            dtype_expected.keyType,\n            dtype_observed.keyType,\n            schema_name,\n        )\n        return check_dtype(\n            f\"{colname}.value\",\n            dtype_expected.valueType,\n            dtype_observed.valueType,\n            schema_name,\n        )\n\n    if isinstance(dtype_expected, StructType) and isinstance(dtype_observed, StructType):\n        return validate_schema(dtype_expected, dtype_observed, f\"{schema_name}.{colname}\")\n\n    raise TypeError(\n        f\"Column {colname} is of type {dtype_observed}, but {schema_name}.{colname} \"\n        + f\"suggests {dtype_expected}.\"\n    )", ""]}
{"filename": "typedspark/_core/column.py", "chunked_list": ["\"\"\"Module containing classes and functions related to TypedSpark Columns.\"\"\"\n\nfrom logging import warn\nfrom typing import Generic, Optional, TypeVar, Union, get_args, get_origin\n\nfrom pyspark.sql import Column as SparkColumn\nfrom pyspark.sql import DataFrame, SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import DataType\n", "from pyspark.sql.types import DataType\n\nfrom typedspark._core.datatypes import StructType\n\nT = TypeVar(\"T\", bound=DataType)\n\n\nclass EmptyColumn(SparkColumn):\n    \"\"\"Column object to be instantiated when there is no active Spark session.\"\"\"\n\n    def __init__(self, *args, **kwargs) -> None:  # pragma: no cover\n        pass", "\n\nclass Column(SparkColumn, Generic[T]):\n    \"\"\"Represents a ``Column`` in a ``Schema``. Can be used as:\n\n    .. code-block:: python\n\n        class A(Schema):\n            a: Column[IntegerType]\n            b: Column[StringType]\n    \"\"\"\n\n    def __new__(\n        cls,\n        name: str,\n        dataframe: Optional[DataFrame] = None,\n        curid: Optional[int] = None,\n        dtype: Optional[T] = None,\n        parent: Union[DataFrame, \"Column\", None] = None,\n    ):\n        \"\"\"``__new__()`` instantiates the object (prior to ``__init__()``).\n\n        Here, we simply take the provided ``name``, create a pyspark\n        ``Column`` object and cast it to a typedspark ``Column`` object.\n        This allows us to bypass the pypsark ``Column`` constuctor in\n        ``__init__()``, which requires parameters that may be difficult\n        to access.\n        \"\"\"\n        # pylint: disable=unused-argument\n\n        if dataframe is not None and parent is None:\n            parent = dataframe\n            warn(\"The use of Column(dataframe=...) is deprecated, use Column(parent=...) instead.\")\n\n        column: SparkColumn\n        if SparkSession.getActiveSession() is None:\n            column = EmptyColumn()  # pragma: no cover\n        elif parent is None:\n            column = col(name)\n        else:\n            column = parent[name]\n\n        column.__class__ = Column\n        return column\n\n    def __init__(\n        self,\n        name: str,\n        dataframe: Optional[DataFrame] = None,\n        curid: Optional[int] = None,\n        dtype: Optional[T] = None,\n        parent: Union[DataFrame, \"Column\", None] = None,\n    ):\n        # pylint: disable=unused-argument\n        self.str = name\n        self._dtype = dtype if dtype is not None else DataType\n        self._curid = curid\n\n    def __hash__(self) -> int:\n        return hash((self.str, self._curid))\n\n    @property\n    def dtype(self) -> T:\n        \"\"\"Get the datatype of the column, e.g. Column[IntegerType] -> IntegerType.\"\"\"\n        dtype = self._dtype\n        if get_origin(dtype) == StructType:\n            dtype.schema = get_args(dtype)[0]  # type: ignore\n            dtype.schema._parent = self  # type: ignore\n\n        return dtype  # type: ignore", ""]}
{"filename": "typedspark/_core/literaltype.py", "chunked_list": ["\"\"\"Defines ``LiteralTypes``, e.g. ``IntervalType.DAY``, that map their class attribute to a\n``Literal`` integer. Can be used for example in ``DayTimeIntervalType``.\"\"\"\nfrom typing import Dict, Literal\n\n\nclass LiteralType:\n    \"\"\"Base class for literal types, that map their class attribute to a Literal\n    integer.\"\"\"\n\n    @classmethod\n    def get_dict(cls) -> Dict[str, str]:\n        \"\"\"Returns a dictionary mapping e.g. \"IntervalType.DAY\" -> \"Literal[0]\".\"\"\"\n        dictionary = {}\n        for key, value in cls.__dict__.items():\n            if key.startswith(\"_\"):\n                continue\n\n            key = f\"{cls.__name__}.{key}\"\n            value = str(value).replace(\"typing.\", \"\")\n\n            dictionary[key] = value\n\n        return dictionary\n\n    @classmethod\n    def get_inverse_dict(cls) -> Dict[str, str]:\n        \"\"\"Returns a dictionary mapping e.g. \"Literal[0]\" -> \"IntervalType.DAY\".\"\"\"\n        return {v: k for k, v in cls.get_dict().items()}", "\n\nclass IntervalType(LiteralType):\n    \"\"\"Interval types for ``DayTimeIntervalType``.\"\"\"\n\n    DAY = Literal[0]\n    HOUR = Literal[1]\n    MINUTE = Literal[2]\n    SECOND = Literal[3]\n", ""]}
{"filename": "typedspark/_schema/schema.py", "chunked_list": ["\"\"\"Module containing classes and functions related to TypedSpark Schemas.\"\"\"\nimport inspect\nimport re\nfrom typing import Any, Dict, List, Optional, Type, Union, get_args, get_type_hints\n\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import DataType, StructType\n\nfrom typedspark._core.column import Column\nfrom typedspark._schema.dlt_kwargs import DltKwargs", "from typedspark._core.column import Column\nfrom typedspark._schema.dlt_kwargs import DltKwargs\nfrom typedspark._schema.get_schema_definition import get_schema_definition_as_string\nfrom typedspark._schema.structfield import get_structfield\n\n\nclass MetaSchema(type):\n    \"\"\"``MetaSchema`` is the metaclass of ``Schema``.\n\n    It basically implements all functionality of ``Schema``. But since\n    classes are typically considered more convenient than metaclasses,\n    we provide ``Schema`` as the public interface.\n\n    .. code-block:: python\n\n        class A(Schema):\n            a: Column[IntegerType]\n            b: Column[StringType]\n\n        DataSet[A](df)\n\n    The class methods of ``Schema`` are described here.\n    \"\"\"\n\n    _parent: Optional[Union[DataFrame, Column]] = None\n    _current_id: Optional[int] = None\n    _original_name: Optional[str] = None\n\n    def __new__(cls, name: str, bases: Any, dct: Dict[str, Any]):\n        cls._attributes = dir(cls)\n\n        # initializes all uninitialied variables with a type annotation as None\n        # this allows for auto-complete in Databricks notebooks (uninitialized variables\n        # don't show up in auto-complete there).\n        if \"__annotations__\" in dct.keys():\n            extra = {attr: None for attr in dct[\"__annotations__\"] if attr not in dct}\n            dct = dict(dct, **extra)\n\n        return type.__new__(cls, name, bases, dct)\n\n    def __repr__(cls) -> str:\n        return f\"\\n{str(cls)}\"\n\n    def __str__(cls) -> str:\n        return cls.get_schema_definition_as_string(add_subschemas=False)\n\n    def __getattribute__(cls, name: str) -> Any:\n        \"\"\"Python base function that gets attributes.\n\n        We listen here for anyone getting a ``Column`` from the ``Schema``.\n        Even though they're not explicitely instantiated, we can instantiate\n        them here whenever someone attempts to get them. This allows us to do the following:\n\n        .. code-block:: python\n\n            class A(Schema):\n                a: Column[IntegerType]\n\n            (\n                df.withColumn(A.a.str, lit(1))\n                .select(A.a)\n            )\n        \"\"\"\n        if name.startswith(\"__\") or name == \"_attributes\" or name in cls._attributes:\n            return object.__getattribute__(cls, name)\n\n        if name in get_type_hints(cls):\n            return Column(\n                name,\n                dtype=cls._get_dtype(name),  # type: ignore\n                parent=cls._parent,\n                curid=cls._current_id,\n            )\n\n        raise TypeError(f\"Schema {cls.get_schema_name()} does not have attribute {name}.\")\n\n    def _get_dtype(cls, name: str) -> Type[DataType]:\n        \"\"\"Returns the datatype of a column, e.g. Column[IntegerType] -> IntegerType.\"\"\"\n        column = get_type_hints(cls)[name]\n        args = get_args(column)\n\n        if not args:\n            raise TypeError(\n                f\"Column {cls.get_schema_name()}.{name} does not have an annotated type.\"\n            )\n\n        dtype = args[0]\n        return dtype\n\n    def all_column_names(cls) -> List[str]:\n        \"\"\"Returns all column names for a given schema.\"\"\"\n        return list(get_type_hints(cls).keys())\n\n    def all_column_names_except_for(cls, except_for: List[str]) -> List[str]:\n        \"\"\"Returns all column names for a given schema except for the columns\n        specified in the ``except_for`` parameter.\"\"\"\n        return list(name for name in get_type_hints(cls).keys() if name not in except_for)\n\n    def get_snake_case(cls) -> str:\n        \"\"\"Return the class name transformed into snakecase.\"\"\"\n        word = cls.get_schema_name()\n        word = re.sub(r\"([A-Z]+)([A-Z][a-z])\", r\"\\1_\\2\", word)\n        word = re.sub(r\"([a-z\\d])([A-Z])\", r\"\\1_\\2\", word)\n        word = word.replace(\"-\", \"_\")\n        return word.lower()\n\n    def get_schema_definition_as_string(\n        cls,\n        schema_name: Optional[str] = None,\n        include_documentation: bool = False,\n        generate_imports: bool = True,\n        add_subschemas: bool = True,\n    ) -> str:\n        \"\"\"Return the code for the ``Schema`` as a string.\"\"\"\n        if schema_name is None:\n            schema_name = cls.get_schema_name()\n        return get_schema_definition_as_string(\n            cls,  # type: ignore\n            include_documentation,\n            generate_imports,\n            add_subschemas,\n            schema_name,\n        )\n\n    def print_schema(\n        cls,\n        schema_name: Optional[str] = None,\n        include_documentation: bool = False,\n        generate_imports: bool = True,\n        add_subschemas: bool = False,\n    ):  # pragma: no cover\n        \"\"\"Print the code for the ``Schema``.\"\"\"\n        print(\n            cls.get_schema_definition_as_string(\n                schema_name=schema_name,\n                include_documentation=include_documentation,\n                generate_imports=generate_imports,\n                add_subschemas=add_subschemas,\n            )\n        )\n\n    def get_docstring(cls) -> Union[str, None]:\n        \"\"\"Returns the docstring of the schema.\"\"\"\n        return inspect.getdoc(cls)\n\n    def get_structtype(cls) -> StructType:\n        \"\"\"Creates the spark StructType for the schema.\"\"\"\n        return StructType(\n            [\n                get_structfield(name, column)\n                for name, column in get_type_hints(cls, include_extras=True).items()\n            ]\n        )\n\n    def get_dlt_kwargs(cls, name: Optional[str] = None) -> DltKwargs:\n        \"\"\"Creates a representation of the ``Schema`` to be used by Delta Live\n        Tables.\n\n        .. code-block:: python\n\n            @dlt.table(**DimPatient.get_dlt_kwargs())\n            def table_definition() -> DataSet[DimPatient]:\n                <your table definition here>\n        \"\"\"\n        return {\n            \"name\": name if name else cls.get_snake_case(),\n            \"comment\": cls.get_docstring(),\n            \"schema\": cls.get_structtype(),\n        }\n\n    def get_schema_name(cls):\n        \"\"\"Returns the name with which the schema was initialized.\"\"\"\n        return cls._original_name if cls._original_name else cls.__name__", "\n\nclass Schema(metaclass=MetaSchema):\n    # pylint: disable=missing-class-docstring\n    # Since docstrings are inherrited, and since we use docstrings to\n    # annotate tables (see MetaSchema.get_dlt_kwargs()), we have chosen\n    # to not add a docstring to the Schema class (otherwise the Schema\n    # docstring would be added to any schema without a docstring).\n    pass\n", ""]}
{"filename": "typedspark/_schema/__init__.py", "chunked_list": [""]}
{"filename": "typedspark/_schema/get_schema_imports.py", "chunked_list": ["\"\"\"Builds an import statement for everything imported by a given ``Schema``.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Optional, Type, get_args, get_origin, get_type_hints\n\nfrom pyspark.sql.types import DataType\n\nfrom typedspark._core.datatypes import (\n    ArrayType,\n    DayTimeIntervalType,", "    ArrayType,\n    DayTimeIntervalType,\n    DecimalType,\n    MapType,\n    StructType,\n    TypedSparkDataType,\n)\n\nif TYPE_CHECKING:  # pragma: no cover\n    from typedspark._schema.schema import Schema", "if TYPE_CHECKING:  # pragma: no cover\n    from typedspark._schema.schema import Schema\n\n\ndef get_schema_imports(schema: Type[Schema], include_documentation: bool) -> str:\n    \"\"\"Builds an import statement for everything imported by the ``Schema``.\"\"\"\n    dtypes = _get_imported_dtypes(schema)\n    return _build_import_string(dtypes, include_documentation)\n\n\ndef _get_imported_dtypes(schema: Type[Schema]) -> set[Type[DataType]]:\n    \"\"\"Returns a set of DataTypes that are imported by the given schema.\"\"\"\n    encountered_datatypes: set[Type[DataType]] = set()\n    for column in get_type_hints(schema).values():\n        args = get_args(column)\n        if not args:\n            continue\n\n        dtype = args[0]\n        encountered_datatypes |= _process_datatype(dtype)\n\n    return encountered_datatypes", "\n\ndef _get_imported_dtypes(schema: Type[Schema]) -> set[Type[DataType]]:\n    \"\"\"Returns a set of DataTypes that are imported by the given schema.\"\"\"\n    encountered_datatypes: set[Type[DataType]] = set()\n    for column in get_type_hints(schema).values():\n        args = get_args(column)\n        if not args:\n            continue\n\n        dtype = args[0]\n        encountered_datatypes |= _process_datatype(dtype)\n\n    return encountered_datatypes", "\n\ndef _process_datatype(dtype: Type[DataType]) -> set[Type[DataType]]:\n    \"\"\"Returns a set of DataTypes that are imported for a given DataType.\n\n    Handles nested DataTypes recursively.\n    \"\"\"\n    encountered_datatypes: set[Type[DataType]] = set()\n\n    origin: Optional[Type[DataType]] = get_origin(dtype)\n    if origin:\n        encountered_datatypes.add(origin)\n    else:\n        encountered_datatypes.add(dtype)\n\n    if origin == MapType:\n        key, value = get_args(dtype)\n        encountered_datatypes |= _process_datatype(key)\n        encountered_datatypes |= _process_datatype(value)\n\n    if origin == ArrayType:\n        element = get_args(dtype)[0]\n        encountered_datatypes |= _process_datatype(element)\n\n    if get_origin(dtype) == StructType:\n        subschema = get_args(dtype)[0]\n        encountered_datatypes |= _get_imported_dtypes(subschema)\n\n    return encountered_datatypes", "\n\ndef _build_import_string(\n    encountered_datatypes: set[Type[DataType]], include_documentation: bool\n) -> str:\n    \"\"\"Returns a multiline string with the imports required for the given\n    encountered_datatypes.\n\n    Import sorting is applied.\n\n    If the schema uses IntegerType, BooleanType, StringType, this functions result would be\n\n    .. code-block:: python\n\n        from pyspark.sql.types import BooleanType, IntegerType, StringType\n\n        from typedspark import Column, Schema\n    \"\"\"\n    return (\n        _typing_imports(encountered_datatypes, include_documentation)\n        + _pyspark_imports(encountered_datatypes)\n        + _typedspark_imports(encountered_datatypes, include_documentation)\n    )", "\n\ndef _typing_imports(encountered_datatypes: set[Type[DataType]], include_documentation: bool) -> str:\n    \"\"\"Returns the import statement for the typing library.\"\"\"\n    imports = []\n\n    if any([dtype == DecimalType for dtype in encountered_datatypes]):\n        imports += [\"Literal\"]\n\n    if include_documentation:\n        imports += [\"Annotated\"]\n\n    if len(imports) > 0:\n        imports = sorted(imports)\n        imports_string = \", \".join(imports)\n        return f\"from typing import {imports_string}\\n\\n\"\n\n    return \"\"", "\n\ndef _pyspark_imports(encountered_datatypes: set[Type[DataType]]) -> str:\n    \"\"\"Returns the import statement for the pyspark library.\"\"\"\n    dtypes = sorted(\n        [\n            dtype.__name__\n            for dtype in encountered_datatypes\n            if not issubclass(dtype, TypedSparkDataType)\n        ]\n    )\n\n    if len(dtypes) > 0:\n        dtypes_string = \", \".join(dtypes)\n        return f\"from pyspark.sql.types import {dtypes_string}\\n\\n\"\n\n    return \"\"", "\n\ndef _typedspark_imports(\n    encountered_datatypes: set[Type[DataType]], include_documentation: bool\n) -> str:\n    \"\"\"Returns the import statement for the typedspark library.\"\"\"\n    dtypes = [\n        dtype.__name__ for dtype in encountered_datatypes if issubclass(dtype, TypedSparkDataType)\n    ] + [\"Column\", \"Schema\"]\n\n    if any([dtype == DayTimeIntervalType for dtype in encountered_datatypes]):\n        dtypes += [\"IntervalType\"]\n\n    if include_documentation:\n        dtypes.append(\"ColumnMeta\")\n\n    dtypes = sorted(dtypes)\n\n    dtypes_string = \", \".join(dtypes)\n    return f\"from typedspark import {dtypes_string}\\n\\n\\n\"", ""]}
{"filename": "typedspark/_schema/get_schema_definition.py", "chunked_list": ["\"\"\"Module to output a string with the ``Schema`` definition of a given\n``DataFrame``.\"\"\"\nfrom __future__ import annotations\n\nimport re\nfrom typing import TYPE_CHECKING, Type, get_args, get_origin, get_type_hints\n\nfrom typedspark._core.datatypes import DayTimeIntervalType, StructType, TypedSparkDataType\nfrom typedspark._core.literaltype import IntervalType, LiteralType\nfrom typedspark._schema.get_schema_imports import get_schema_imports", "from typedspark._core.literaltype import IntervalType, LiteralType\nfrom typedspark._schema.get_schema_imports import get_schema_imports\n\nif TYPE_CHECKING:  # pragma: no cover\n    from typedspark._schema.schema import Schema\n\n\ndef get_schema_definition_as_string(\n    schema: Type[Schema],\n    include_documentation: bool,\n    generate_imports: bool,\n    add_subschemas: bool,\n    class_name: str = \"MyNewSchema\",\n) -> str:\n    \"\"\"Return the code for a given ``Schema`` as a string.\n\n    Typically used when you load a dataset using\n    ``load_dataset_from_table()`` in a notebook and you want to save the\n    schema in your code base. When ``generate_imports`` is True, the\n    required imports for the schema are included in the string.\n    \"\"\"\n    imports = get_schema_imports(schema, include_documentation) if generate_imports else \"\"\n    schema_string = _build_schema_definition_string(\n        schema, include_documentation, add_subschemas, class_name\n    )\n\n    return imports + schema_string", "\n\ndef _build_schema_definition_string(\n    schema: Type[Schema],\n    include_documentation: bool,\n    add_subschemas: bool,\n    class_name: str = \"MyNewSchema\",\n) -> str:\n    \"\"\"Return the code for a given ``Schema`` as a string.\"\"\"\n    lines = f\"class {class_name}(Schema):\\n\"\n    if include_documentation:\n        lines += '    \"\"\"Add documentation here.\"\"\"\\n\\n'\n\n    for k, val in get_type_hints(schema).items():\n        typehint = (\n            str(val)\n            .replace(\"typedspark._core.column.\", \"\")\n            .replace(\"typedspark._core.datatypes.\", \"\")\n            .replace(\"typedspark._schema.schema.\", \"\")\n            .replace(\"pyspark.sql.types.\", \"\")\n            .replace(\"typing.\", \"\")\n        )\n        typehint = _replace_literals(\n            typehint, replace_literals_in=DayTimeIntervalType, replace_literals_by=IntervalType\n        )\n        if include_documentation:\n            lines += f'    {k}: Annotated[{typehint}, ColumnMeta(comment=\"\")]\\n'\n        else:\n            lines += f\"    {k}: {typehint}\\n\"\n\n    if add_subschemas:\n        lines += _add_subschemas(schema, add_subschemas, include_documentation)\n\n    return lines", "\n\ndef _replace_literals(\n    typehint: str,\n    replace_literals_in: Type[TypedSparkDataType],\n    replace_literals_by: Type[LiteralType],\n) -> str:\n    \"\"\"Replace all Literals in a LiteralType, e.g.\n\n    \"DayTimeIntervalType[Literal[0], Literal[1]]\" ->\n    \"DayTimeIntervalType[IntervalType.DAY, IntervalType.HOUR]\"\n    \"\"\"\n    mapping = replace_literals_by.get_inverse_dict()\n    for original, replacement in mapping.items():\n        typehint = _replace_literal(typehint, replace_literals_in, original, replacement)\n\n    return typehint", "\n\ndef _replace_literal(\n    typehint: str,\n    replace_literals_in: Type[TypedSparkDataType],\n    original: str,\n    replacement: str,\n) -> str:\n    \"\"\"Replaces a single Literal in a LiteralType, e.g.\n\n    \"DayTimeIntervalType[Literal[0], Literal[1]]\" ->\n    \"DayTimeIntervalType[IntervalType.DAY, Literal[1]]\"\n    \"\"\"\n    return re.sub(\n        rf\"{replace_literals_in.get_name()}\\[[^]]*\\]\",\n        lambda x: x.group(0).replace(original, replacement),\n        typehint,\n    )", "\n\ndef _add_subschemas(schema: Type[Schema], add_subschemas: bool, include_documentation: bool) -> str:\n    \"\"\"Identifies whether any ``Column`` are of the ``StructType`` type and\n    generates their schema recursively.\"\"\"\n    lines = \"\"\n    for val in get_type_hints(schema).values():\n        args = get_args(val)\n        if not args:\n            continue\n\n        dtype = args[0]\n        if get_origin(dtype) == StructType:\n            lines += \"\\n\\n\"\n            subschema: Type[Schema] = get_args(dtype)[0]\n            lines += _build_schema_definition_string(\n                subschema, include_documentation, add_subschemas, subschema.get_schema_name()\n            )\n\n    return lines", ""]}
{"filename": "typedspark/_schema/structfield.py", "chunked_list": ["\"\"\"Module responsible for generating StructFields from Columns in a Schema.\"\"\"\nfrom __future__ import annotations\n\nimport inspect\nfrom typing import TYPE_CHECKING, Annotated, Type, TypeVar, Union, get_args, get_origin\n\nfrom pyspark.sql.types import ArrayType as SparkArrayType\nfrom pyspark.sql.types import DataType\nfrom pyspark.sql.types import DayTimeIntervalType as SparkDayTimeIntervalType\nfrom pyspark.sql.types import DecimalType as SparkDecimalType", "from pyspark.sql.types import DayTimeIntervalType as SparkDayTimeIntervalType\nfrom pyspark.sql.types import DecimalType as SparkDecimalType\nfrom pyspark.sql.types import MapType as SparkMapType\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import StructType as SparkStructType\n\nfrom typedspark._core.column import Column\nfrom typedspark._core.column_meta import ColumnMeta\nfrom typedspark._core.datatypes import (\n    ArrayType,", "from typedspark._core.datatypes import (\n    ArrayType,\n    DayTimeIntervalType,\n    DecimalType,\n    MapType,\n    StructType,\n    TypedSparkDataType,\n)\n\nif TYPE_CHECKING:  # pragma: no cover\n    from typedspark._schema.schema import Schema", "\nif TYPE_CHECKING:  # pragma: no cover\n    from typedspark._schema.schema import Schema\n\n_DataType = TypeVar(\"_DataType\", bound=DataType)  # pylint: disable=invalid-name\n\n\ndef get_structfield(\n    name: str,\n    column: Union[Type[Column[_DataType]], Annotated[Type[Column[_DataType]], ColumnMeta]],\n) -> StructField:\n    \"\"\"Generates a ``StructField`` for a given ``Column`` in a ``Schema``.\"\"\"\n    meta = get_structfield_meta(column)\n\n    return StructField(\n        name=name,\n        dataType=_get_structfield_dtype(column, name),\n        nullable=True,\n        metadata=meta.get_metadata(),\n    )", "\n\ndef get_structfield_meta(\n    column: Union[Type[Column[_DataType]], Annotated[Type[Column[_DataType]], ColumnMeta]]\n) -> ColumnMeta:\n    \"\"\"Get the spark column metadata from the ``ColumnMeta`` data, when\n    available.\"\"\"\n    return next((x for x in get_args(column) if isinstance(x, ColumnMeta)), ColumnMeta())\n\n\ndef _get_structfield_dtype(\n    column: Union[Type[Column[_DataType]], Annotated[Type[Column[_DataType]], ColumnMeta]],\n    colname: str,\n) -> DataType:\n    \"\"\"Get the spark ``DataType`` from the ``Column`` type annotation.\"\"\"\n    origin = get_origin(column)\n    if origin not in [Annotated, Column]:\n        raise TypeError(f\"Column {colname} needs to be of type Column or Annotated.\")\n\n    if origin == Annotated:\n        column = _get_column_from_annotation(column, colname)\n\n    args = get_args(column)\n    dtype = _get_dtype(args[0], colname)\n    return dtype", "\n\ndef _get_structfield_dtype(\n    column: Union[Type[Column[_DataType]], Annotated[Type[Column[_DataType]], ColumnMeta]],\n    colname: str,\n) -> DataType:\n    \"\"\"Get the spark ``DataType`` from the ``Column`` type annotation.\"\"\"\n    origin = get_origin(column)\n    if origin not in [Annotated, Column]:\n        raise TypeError(f\"Column {colname} needs to be of type Column or Annotated.\")\n\n    if origin == Annotated:\n        column = _get_column_from_annotation(column, colname)\n\n    args = get_args(column)\n    dtype = _get_dtype(args[0], colname)\n    return dtype", "\n\ndef _get_column_from_annotation(\n    column: Annotated[Type[Column[_DataType]], ColumnMeta],\n    colname: str,\n) -> Type[Column[_DataType]]:\n    \"\"\"Takes an ``Annotation[Column[...], ...]`` and returns the\n    ``Column[...]``.\"\"\"\n    column = get_args(column)[0]\n    if get_origin(column) != Column:\n        raise TypeError(f\"Column {colname} needs to have a Column[] within Annotated[].\")\n\n    return column", "\n\ndef _get_dtype(dtype: Type[DataType], colname: str) -> DataType:\n    \"\"\"Takes a ``DataType`` class and returns a DataType object.\"\"\"\n    origin = get_origin(dtype)\n    if origin == ArrayType:\n        return _extract_arraytype(dtype, colname)\n    if origin == MapType:\n        return _extract_maptype(dtype, colname)\n    if origin == StructType:\n        return _extract_structtype(dtype)\n    if origin == DecimalType:\n        return _extract_decimaltype(dtype)\n    if origin == DayTimeIntervalType:\n        return _extract_daytimeintervaltype(dtype)\n    if (\n        inspect.isclass(dtype)\n        and issubclass(dtype, DataType)\n        and not issubclass(dtype, TypedSparkDataType)\n    ):\n        return dtype()\n\n    raise TypeError(\n        f\"Column {colname} does not have a correctly formatted DataType as a parameter.\"\n    )", "\n\ndef _extract_arraytype(arraytype: Type[DataType], colname: str) -> SparkArrayType:\n    \"\"\"Takes e.g. an ``ArrayType[StringType]`` and creates an\n    ``ArrayType(StringType(), True)``.\"\"\"\n    params = get_args(arraytype)\n    element_type = _get_dtype(params[0], colname)\n    return SparkArrayType(element_type)\n\n\ndef _extract_maptype(maptype: Type[DataType], colname: str) -> SparkMapType:\n    \"\"\"Takes e.g. a ``MapType[StringType, StringType]`` and creates a ``\n    MapType(StringType(), StringType(), True)``.\"\"\"\n    params = get_args(maptype)\n    key_type = _get_dtype(params[0], colname)\n    value_type = _get_dtype(params[1], colname)\n    return SparkMapType(key_type, value_type)", "\n\ndef _extract_maptype(maptype: Type[DataType], colname: str) -> SparkMapType:\n    \"\"\"Takes e.g. a ``MapType[StringType, StringType]`` and creates a ``\n    MapType(StringType(), StringType(), True)``.\"\"\"\n    params = get_args(maptype)\n    key_type = _get_dtype(params[0], colname)\n    value_type = _get_dtype(params[1], colname)\n    return SparkMapType(key_type, value_type)\n", "\n\ndef _extract_structtype(structtype: Type[DataType]) -> SparkStructType:\n    \"\"\"Takes a ``StructType[Schema]`` annotation and creates a\n    ``StructType(schema_list)``, where ``schema_list`` contains all\n    ``StructField()`` defined in the ``Schema``.\"\"\"\n    params = get_args(structtype)\n    schema: Type[Schema] = params[0]\n    return schema.get_structtype()\n", "\n\ndef _extract_decimaltype(decimaltype: Type[DataType]) -> SparkDecimalType:\n    \"\"\"Takes e.g. a ``DecimalType[Literal[10], Literal[12]]`` and returns\n    ``DecimalType(10, 12)``.\"\"\"\n    params = get_args(decimaltype)\n    key_type: int = _unpack_literal(params[0])\n    value_type: int = _unpack_literal(params[1])\n    return SparkDecimalType(key_type, value_type)\n", "\n\ndef _extract_daytimeintervaltype(daytimeintervaltype: Type[DataType]) -> SparkDayTimeIntervalType:\n    \"\"\"Takes e.g. a ``DayTimeIntervalType[Literal[1], Literal[2]]`` and returns\n    ``DayTimeIntervalType(1, 2)``.\"\"\"\n    params = get_args(daytimeintervaltype)\n    start_field: int = _unpack_literal(params[0])\n    end_field: int = _unpack_literal(params[1])\n    return SparkDayTimeIntervalType(start_field, end_field)\n", "\n\ndef _unpack_literal(literal):\n    \"\"\"Takes as input e.g. ``Literal[10]`` and returns ``10``.\"\"\"\n    return get_args(literal)[0]\n"]}
{"filename": "typedspark/_schema/dlt_kwargs.py", "chunked_list": ["\"\"\"A representation of the ``Schema`` to be used by Delta Live Tables.\"\"\"\n\nfrom typing import Optional, TypedDict\n\nfrom pyspark.sql.types import StructType\n\n\nclass DltKwargs(TypedDict):\n    \"\"\"A representation of the ``Schema`` to be used by Delta Live Tables.\n\n    .. code-block:: python\n\n        @dlt.table(**Person.get_dlt_kwargs())\n        def table_definition() -> DataSet[Person]:\n            <your table definition here>\n    \"\"\"\n\n    name: str\n    comment: Optional[str]\n    schema: StructType", ""]}
{"filename": "tests/conftest.py", "chunked_list": ["import os\nimport sys\n\nimport pytest\nfrom pyspark.sql import SparkSession\n\n\n@pytest.fixture(scope=\"session\")\ndef spark():\n    \"\"\"Fixture for creating a spark session.\"\"\"\n    os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n\n    spark = SparkSession.Builder().getOrCreate()\n    yield spark\n    spark.stop()", "def spark():\n    \"\"\"Fixture for creating a spark session.\"\"\"\n    os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n\n    spark = SparkSession.Builder().getOrCreate()\n    yield spark\n    spark.stop()\n", ""]}
{"filename": "tests/_transforms/test_structtype_column.py", "chunked_list": ["import pytest\nfrom chispa.dataframe_comparer import assert_df_equality  # type: ignore\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import IntegerType\n\nfrom typedspark import Column, Schema, StructType, structtype_column\nfrom typedspark._transforms.transform_to_schema import transform_to_schema\nfrom typedspark._utils.create_dataset import create_partially_filled_dataset\n\n\nclass SubSchema(Schema):\n    a: Column[IntegerType]\n    b: Column[IntegerType]", "\n\nclass SubSchema(Schema):\n    a: Column[IntegerType]\n    b: Column[IntegerType]\n\n\nclass MainSchema(Schema):\n    a: Column[IntegerType]\n    b: Column[StructType[SubSchema]]", "\n\ndef test_structtype_column(spark: SparkSession):\n    df = create_partially_filled_dataset(spark, MainSchema, {MainSchema.a: [1, 2, 3]})\n    observed = transform_to_schema(\n        df,\n        MainSchema,\n        {\n            MainSchema.b: structtype_column(\n                SubSchema,\n                {SubSchema.a: MainSchema.a + 2, SubSchema.b: MainSchema.a + 4},\n            )\n        },\n    )\n    expected = create_partially_filled_dataset(\n        spark,\n        MainSchema,\n        {\n            MainSchema.a: [1, 2, 3],\n            MainSchema.b: create_partially_filled_dataset(\n                spark, SubSchema, {SubSchema.a: [3, 4, 5], SubSchema.b: [5, 6, 7]}\n            ).collect(),\n        },\n    )\n    assert_df_equality(observed, expected, ignore_nullable=True)", "\n\ndef test_structtype_column_different_column_order(spark: SparkSession):\n    df = create_partially_filled_dataset(spark, MainSchema, {MainSchema.a: [1, 2, 3]})\n    observed = transform_to_schema(\n        df,\n        MainSchema,\n        {\n            MainSchema.b: structtype_column(\n                SubSchema,\n                {SubSchema.b: MainSchema.a + 4, SubSchema.a: MainSchema.a + 2},\n            )\n        },\n    )\n    expected = create_partially_filled_dataset(\n        spark,\n        MainSchema,\n        {\n            MainSchema.a: [1, 2, 3],\n            MainSchema.b: create_partially_filled_dataset(\n                spark, SubSchema, {SubSchema.a: [3, 4, 5], SubSchema.b: [5, 6, 7]}\n            ).collect(),\n        },\n    )\n    assert_df_equality(observed, expected, ignore_nullable=True)", "\n\ndef test_structtype_column_partial(spark: SparkSession):\n    df = create_partially_filled_dataset(spark, MainSchema, {MainSchema.a: [1, 2, 3]})\n    observed = transform_to_schema(\n        df,\n        MainSchema,\n        {\n            MainSchema.b: structtype_column(\n                SubSchema,\n                {SubSchema.a: MainSchema.a + 2},\n                fill_unspecified_columns_with_nulls=True,\n            )\n        },\n    )\n    expected = create_partially_filled_dataset(\n        spark,\n        MainSchema,\n        {\n            MainSchema.a: [1, 2, 3],\n            MainSchema.b: create_partially_filled_dataset(\n                spark,\n                SubSchema,\n                {SubSchema.a: [3, 4, 5], SubSchema.b: [None, None, None]},\n            ).collect(),\n        },\n    )\n    assert_df_equality(observed, expected, ignore_nullable=True)", "\n\ndef test_structtype_column_with_double_column(spark: SparkSession):\n    df = create_partially_filled_dataset(spark, MainSchema, {MainSchema.a: [1, 2, 3]})\n    with pytest.raises(ValueError):\n        transform_to_schema(\n            df,\n            MainSchema,\n            {\n                MainSchema.b: structtype_column(\n                    SubSchema,\n                    {SubSchema.a: MainSchema.a + 2, SubSchema.a: MainSchema.a + 2},\n                )\n            },\n        )", ""]}
{"filename": "tests/_transforms/test_transform_to_schema.py", "chunked_list": ["import pytest\nfrom chispa.dataframe_comparer import assert_df_equality  # type: ignore\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import IntegerType, StringType\n\nfrom typedspark import (\n    Column,\n    Schema,\n    create_empty_dataset,\n    register_schema_to_dataset,", "    create_empty_dataset,\n    register_schema_to_dataset,\n    transform_to_schema,\n)\nfrom typedspark._utils.create_dataset import create_partially_filled_dataset\n\n\nclass Person(Schema):\n    a: Column[IntegerType]\n    b: Column[IntegerType]\n    c: Column[IntegerType]", "\n\nclass PersonLessData(Schema):\n    a: Column[IntegerType]\n    b: Column[IntegerType]\n\n\nclass PersonDifferentData(Schema):\n    a: Column[IntegerType]\n    d: Column[IntegerType]", "\n\ndef test_transform_to_schema_without_transformations(spark: SparkSession):\n    df = create_empty_dataset(spark, Person)\n    observed = transform_to_schema(df, PersonLessData)\n    expected = create_empty_dataset(spark, PersonLessData)\n    assert_df_equality(observed, expected)\n\n\ndef test_transform_to_schema_with_transformation(spark: SparkSession):\n    df = create_partially_filled_dataset(spark, Person, {Person.c: [1, 2, 3]})\n    observed = transform_to_schema(df, PersonDifferentData, {PersonDifferentData.d: Person.c + 3})\n    expected = create_partially_filled_dataset(\n        spark, PersonDifferentData, {PersonDifferentData.d: [4, 5, 6]}\n    )\n    assert_df_equality(observed, expected)", "\ndef test_transform_to_schema_with_transformation(spark: SparkSession):\n    df = create_partially_filled_dataset(spark, Person, {Person.c: [1, 2, 3]})\n    observed = transform_to_schema(df, PersonDifferentData, {PersonDifferentData.d: Person.c + 3})\n    expected = create_partially_filled_dataset(\n        spark, PersonDifferentData, {PersonDifferentData.d: [4, 5, 6]}\n    )\n    assert_df_equality(observed, expected)\n\n\ndef test_transform_to_schema_with_missing_column(spark):\n    df = create_partially_filled_dataset(spark, Person, {Person.c: [1, 2, 3]}).drop(Person.a)\n    with pytest.raises(Exception):\n        transform_to_schema(df, PersonDifferentData, {PersonDifferentData.d: Person.c + 3})\n\n    observed = transform_to_schema(\n        df,\n        PersonDifferentData,\n        {PersonDifferentData.d: Person.c + 3},\n        fill_unspecified_columns_with_nulls=True,\n    )\n\n    expected = create_partially_filled_dataset(\n        spark,\n        PersonDifferentData,\n        {PersonDifferentData.d: [4, 5, 6]},\n    )\n    assert_df_equality(observed, expected)", "\n\ndef test_transform_to_schema_with_missing_column(spark):\n    df = create_partially_filled_dataset(spark, Person, {Person.c: [1, 2, 3]}).drop(Person.a)\n    with pytest.raises(Exception):\n        transform_to_schema(df, PersonDifferentData, {PersonDifferentData.d: Person.c + 3})\n\n    observed = transform_to_schema(\n        df,\n        PersonDifferentData,\n        {PersonDifferentData.d: Person.c + 3},\n        fill_unspecified_columns_with_nulls=True,\n    )\n\n    expected = create_partially_filled_dataset(\n        spark,\n        PersonDifferentData,\n        {PersonDifferentData.d: [4, 5, 6]},\n    )\n    assert_df_equality(observed, expected)", "\n\ndef test_transform_to_schema_with_pre_existing_column(spark):\n    df = create_partially_filled_dataset(spark, Person, {Person.a: [0, 1, 2], Person.c: [1, 2, 3]})\n\n    observed = transform_to_schema(\n        df,\n        PersonDifferentData,\n        {PersonDifferentData.d: Person.c + 3},\n        fill_unspecified_columns_with_nulls=True,\n    )\n\n    expected = create_partially_filled_dataset(\n        spark,\n        PersonDifferentData,\n        {PersonDifferentData.a: [0, 1, 2], PersonDifferentData.d: [4, 5, 6]},\n    )\n    assert_df_equality(observed, expected)", "\n\nclass PersonA(Schema):\n    name: Column[StringType]\n    age: Column[StringType]\n\n\nclass PersonB(Schema):\n    name: Column[StringType]\n    age: Column[StringType]", "\n\ndef test_transform_to_schema_with_column_disambiguation(spark: SparkSession):\n    df_a = create_partially_filled_dataset(\n        spark,\n        PersonA,\n        {PersonA.name: [\"John\", \"Jane\", \"Bob\"], PersonA.age: [30, 40, 50]},\n    )\n    df_b = create_partially_filled_dataset(\n        spark,\n        PersonB,\n        {PersonB.name: [\"John\", \"Jane\", \"Bob\"], PersonB.age: [30, 40, 50]},\n    )\n\n    person_a = register_schema_to_dataset(df_a, PersonA)\n    person_b = register_schema_to_dataset(df_b, PersonB)\n\n    with pytest.raises(ValueError):\n        transform_to_schema(\n            df_a.join(df_b, person_a.name == person_b.name),\n            PersonA,\n            {\n                person_a.age: person_a.age + 3,\n                person_b.age: person_b.age + 5,\n            },\n        )", "\n\ndef test_transform_to_schema_with_double_column(spark: SparkSession):\n    df = create_partially_filled_dataset(spark, Person, {Person.a: [1, 2, 3], Person.b: [1, 2, 3]})\n\n    with pytest.raises(ValueError):\n        transform_to_schema(\n            df,\n            Person,\n            {\n                Person.a: Person.a + 3,\n                Person.a: Person.a + 5,\n            },\n        )", "\n\ndef test_transform_to_schema_sequential(spark: SparkSession):\n    df = create_partially_filled_dataset(spark, Person, {Person.a: [1, 2, 3], Person.b: [1, 2, 3]})\n\n    observed = transform_to_schema(\n        df,\n        Person,\n        {\n            Person.a: Person.a + 3,\n            Person.b: Person.a + 5,\n        },\n    )\n\n    expected = create_partially_filled_dataset(\n        spark, Person, {Person.a: [4, 5, 6], Person.b: [9, 10, 11]}\n    )\n\n    assert_df_equality(observed, expected)", ""]}
{"filename": "tests/_utils/test_load_table.py", "chunked_list": ["from typing import Literal\n\nimport pytest\nfrom chispa.dataframe_comparer import assert_df_equality  # type: ignore\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import first\nfrom pyspark.sql.types import IntegerType, StringType\n\nfrom typedspark import (\n    ArrayType,", "from typedspark import (\n    ArrayType,\n    Column,\n    Databases,\n    DecimalType,\n    MapType,\n    Schema,\n    StructType,\n    create_empty_dataset,\n    load_table,", "    create_empty_dataset,\n    load_table,\n)\nfrom typedspark._core.datatypes import DayTimeIntervalType\nfrom typedspark._core.literaltype import IntervalType\nfrom typedspark._utils.create_dataset import create_partially_filled_dataset\nfrom typedspark._utils.databases import Catalogs\nfrom typedspark._utils.load_table import create_schema\n\n\nclass SubSchema(Schema):\n    a: Column[IntegerType]", "\n\nclass SubSchema(Schema):\n    a: Column[IntegerType]\n\n\nclass A(Schema):\n    a: Column[IntegerType]\n    b: Column[ArrayType[IntegerType]]\n    c: Column[ArrayType[MapType[IntegerType, IntegerType]]]\n    d: Column[DayTimeIntervalType[IntervalType.HOUR, IntervalType.MINUTE]]\n    e: Column[DecimalType[Literal[7], Literal[2]]]\n    value_container: Column[StructType[SubSchema]]", "\n\ndef test_load_table(spark: SparkSession) -> None:\n    df = create_empty_dataset(spark, A)\n    df.createOrReplaceTempView(\"temp\")\n\n    df_loaded, schema = load_table(spark, \"temp\")\n\n    assert_df_equality(df, df_loaded)\n    assert schema.get_structtype() == A.get_structtype()\n    assert schema.get_schema_name() != \"A\"", "\n\ndef test_load_table_with_schema_name(spark: SparkSession) -> None:\n    df = create_empty_dataset(spark, A)\n    df.createOrReplaceTempView(\"temp\")\n\n    df_loaded, schema = load_table(spark, \"temp\", schema_name=\"A\")\n\n    assert_df_equality(df, df_loaded)\n    assert schema.get_structtype() == A.get_structtype()\n    assert schema.get_schema_name() == \"A\"", "\n\nclass B(Schema):\n    a: Column[StringType]\n    b: Column[IntegerType]\n    c: Column[StringType]\n\n\ndef test_create_schema(spark: SparkSession) -> None:\n    df = (\n        create_partially_filled_dataset(\n            spark,\n            B,\n            {\n                B.a: [\"a\", \"b!!\", \"c\", \"a\", \"b!!\", \"c\", \"a\", \"b!!\", \"c\"],\n                B.b: [1, 1, 1, 2, 2, 2, 3, 3, 3],\n                B.c: [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\", \"zeta\", \"eta\", \"theta\", \"iota\"],\n            },\n        )\n        .groupby(B.b)\n        .pivot(B.a.str)\n        .agg(first(B.c))\n    )\n\n    df, MySchema = create_schema(df, \"B\")\n\n    assert MySchema.get_schema_name() == \"B\"\n    assert \"a\" in MySchema.all_column_names()\n    assert \"b__\" in MySchema.all_column_names()\n    assert \"c\" in MySchema.all_column_names()", "def test_create_schema(spark: SparkSession) -> None:\n    df = (\n        create_partially_filled_dataset(\n            spark,\n            B,\n            {\n                B.a: [\"a\", \"b!!\", \"c\", \"a\", \"b!!\", \"c\", \"a\", \"b!!\", \"c\"],\n                B.b: [1, 1, 1, 2, 2, 2, 3, 3, 3],\n                B.c: [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\", \"zeta\", \"eta\", \"theta\", \"iota\"],\n            },\n        )\n        .groupby(B.b)\n        .pivot(B.a.str)\n        .agg(first(B.c))\n    )\n\n    df, MySchema = create_schema(df, \"B\")\n\n    assert MySchema.get_schema_name() == \"B\"\n    assert \"a\" in MySchema.all_column_names()\n    assert \"b__\" in MySchema.all_column_names()\n    assert \"c\" in MySchema.all_column_names()", "\n\ndef test_create_schema_with_duplicated_column_names(spark: SparkSession) -> None:\n    df = (\n        create_partially_filled_dataset(\n            spark,\n            B,\n            {\n                B.a: [\"a\", \"b??\", \"c\", \"a\", \"b!!\", \"c\", \"a\", \"b!!\", \"c\"],\n                B.b: [1, 1, 1, 2, 2, 2, 3, 3, 3],\n                B.c: [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\", \"zeta\", \"eta\", \"theta\", \"iota\"],\n            },\n        )\n        .groupby(B.b)\n        .pivot(B.a.str)\n        .agg(first(B.c))\n    )\n\n    with pytest.raises(ValueError):\n        create_schema(df, \"B\")", "\n\ndef test_name_of_structtype_schema(spark):\n    df = create_empty_dataset(spark, A)\n    df, MySchema = create_schema(df, \"A\")\n\n    assert MySchema.value_container.dtype.schema.get_schema_name() == \"ValueContainer\"\n\n\ndef test_databases_with_temp_view(spark):\n    df = create_empty_dataset(spark, A)\n    df.createOrReplaceTempView(\"table_a\")\n\n    db = Databases(spark)\n    df_loaded, schema = db.default.table_a.load()  # type: ignore\n\n    assert_df_equality(df, df_loaded)\n    assert schema.get_structtype() == A.get_structtype()\n    assert schema.get_schema_name() == \"TableA\"\n    assert db.default.table_a.str == \"table_a\"  # type: ignore\n    assert db.default.str == \"default\"  # type: ignore", "\ndef test_databases_with_temp_view(spark):\n    df = create_empty_dataset(spark, A)\n    df.createOrReplaceTempView(\"table_a\")\n\n    db = Databases(spark)\n    df_loaded, schema = db.default.table_a.load()  # type: ignore\n\n    assert_df_equality(df, df_loaded)\n    assert schema.get_structtype() == A.get_structtype()\n    assert schema.get_schema_name() == \"TableA\"\n    assert db.default.table_a.str == \"table_a\"  # type: ignore\n    assert db.default.str == \"default\"  # type: ignore", "\n\ndef _drop_table(spark: SparkSession, table_name: str) -> None:\n    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n\n\ndef test_databases_with_table(spark):\n    df = create_empty_dataset(spark, A)\n    df.write.saveAsTable(\"default.table_b\")\n\n    try:\n        db = Databases(spark)\n        df_loaded, schema = db.default.table_b.load()  # type: ignore\n\n        assert_df_equality(df, df_loaded)\n        assert schema.get_structtype() == A.get_structtype()\n        assert schema.get_schema_name() == \"TableB\"\n        assert db.default.table_b.str == \"default.table_b\"  # type: ignore\n        assert db.default.str == \"default\"  # type: ignore\n    except Exception as exception:\n        _drop_table(spark, \"default.table_b\")\n        raise exception\n\n    _drop_table(spark, \"default.table_b\")", "\n\ndef test_catalogs(spark):\n    df = create_empty_dataset(spark, A)\n    df.write.saveAsTable(\"spark_catalog.default.table_b\")\n\n    try:\n        db = Catalogs(spark)\n        df_loaded, schema = db.spark_catalog.default.table_b.load()  # type: ignore\n\n        assert_df_equality(df, df_loaded)\n        assert schema.get_structtype() == A.get_structtype()\n        assert schema.get_schema_name() == \"TableB\"\n        assert db.spark_catalog.default.table_b.str == \"spark_catalog.default.table_b\"  # type: ignore  # noqa: E501\n    except Exception as exception:\n        _drop_table(spark, \"spark_catalog.default.table_b\")\n        raise exception\n\n    _drop_table(spark, \"spark_catalog.default.table_b\")", ""]}
{"filename": "tests/_utils/test_register_schema_to_dataset.py", "chunked_list": ["import pytest\nfrom pyspark.errors import AnalysisException\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import IntegerType\n\nfrom typedspark import Column, Schema, create_partially_filled_dataset, register_schema_to_dataset\n\n\nclass Person(Schema):\n    a: Column[IntegerType]\n    b: Column[IntegerType]", "class Person(Schema):\n    a: Column[IntegerType]\n    b: Column[IntegerType]\n\n\nclass Job(Schema):\n    a: Column[IntegerType]\n    c: Column[IntegerType]\n\n\ndef test_register_schema_to_dataset(spark: SparkSession):\n    df_a = create_partially_filled_dataset(spark, Person, {Person.a: [1, 2, 3]})\n    df_b = create_partially_filled_dataset(spark, Job, {Job.a: [1, 2, 3]})\n\n    with pytest.raises(AnalysisException):\n        df_a.join(df_b, Person.a == Job.a)\n\n    person = register_schema_to_dataset(df_a, Person)\n    job = register_schema_to_dataset(df_b, Job)\n\n    assert person.get_schema_name() == \"Person\"\n\n    df_a.join(df_b, person.a == job.a)", "\n\ndef test_register_schema_to_dataset(spark: SparkSession):\n    df_a = create_partially_filled_dataset(spark, Person, {Person.a: [1, 2, 3]})\n    df_b = create_partially_filled_dataset(spark, Job, {Job.a: [1, 2, 3]})\n\n    with pytest.raises(AnalysisException):\n        df_a.join(df_b, Person.a == Job.a)\n\n    person = register_schema_to_dataset(df_a, Person)\n    job = register_schema_to_dataset(df_b, Job)\n\n    assert person.get_schema_name() == \"Person\"\n\n    df_a.join(df_b, person.a == job.a)", ""]}
{"filename": "tests/_utils/test_create_dataset.py", "chunked_list": ["from decimal import Decimal\nfrom typing import Literal\n\nimport pytest\nfrom chispa.dataframe_comparer import assert_df_equality  # type: ignore\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType\n\nfrom typedspark import (\n    ArrayType,", "from typedspark import (\n    ArrayType,\n    Column,\n    DataSet,\n    MapType,\n    Schema,\n    StructType,\n    create_empty_dataset,\n    create_partially_filled_dataset,\n    create_structtype_row,", "    create_partially_filled_dataset,\n    create_structtype_row,\n)\nfrom typedspark._core.datatypes import DecimalType\n\n\nclass A(Schema):\n    a: Column[DecimalType[Literal[38], Literal[18]]]\n    b: Column[StringType]\n", "\n\ndef test_create_empty_dataset(spark: SparkSession):\n    n_rows = 2\n    result: DataSet[A] = create_empty_dataset(spark, A, n_rows)\n\n    spark_schema = A.get_structtype()\n    data = [(None, None), (None, None)]\n    expected = spark.createDataFrame(data, spark_schema)\n\n    assert_df_equality(result, expected)", "\n\ndef test_create_partially_filled_dataset(spark: SparkSession):\n    data = {A.a: [Decimal(x) for x in [1, 2, 3]]}\n    result: DataSet[A] = create_partially_filled_dataset(spark, A, data)\n\n    spark_schema = A.get_structtype()\n    row_data = [(Decimal(1), None), (Decimal(2), None), (Decimal(3), None)]\n    expected = spark.createDataFrame(row_data, spark_schema)\n\n    assert_df_equality(result, expected)", "\n\ndef test_create_partially_filled_dataset_with_different_number_of_rows(\n    spark: SparkSession,\n):\n    with pytest.raises(ValueError):\n        create_partially_filled_dataset(spark, A, {A.a: [1], A.b: [\"a\", \"b\"]})\n\n\nclass B(Schema):\n    a: Column[ArrayType[StringType]]\n    b: Column[MapType[StringType, StringType]]\n    c: Column[StructType[A]]", "\nclass B(Schema):\n    a: Column[ArrayType[StringType]]\n    b: Column[MapType[StringType, StringType]]\n    c: Column[StructType[A]]\n\n\ndef test_create_empty_dataset_with_complex_data(spark: SparkSession):\n    df_a = create_partially_filled_dataset(spark, A, {A.a: [Decimal(x) for x in [1, 2, 3]]})\n\n    result = create_partially_filled_dataset(\n        spark,\n        B,\n        {\n            B.a: [[\"a\"], [\"b\", \"c\"], [\"d\"]],\n            B.b: [{\"a\": \"1\"}, {\"b\": \"2\", \"c\": \"3\"}, {\"d\": \"4\"}],\n            B.c: df_a.collect(),\n        },\n    )\n\n    spark_schema = B.get_structtype()\n    row_data = [\n        ([\"a\"], {\"a\": \"1\"}, (Decimal(1), None)),\n        ([\"b\", \"c\"], {\"b\": \"2\", \"c\": \"3\"}, (Decimal(2), None)),\n        ([\"d\"], {\"d\": \"4\"}, (Decimal(3), None)),\n    ]\n    expected = spark.createDataFrame(row_data, spark_schema)\n\n    assert_df_equality(result, expected)", "\n\ndef test_create_partially_filled_dataset_from_list(spark: SparkSession):\n    result = create_partially_filled_dataset(\n        spark,\n        A,\n        [\n            {A.a: Decimal(1), A.b: \"a\"},\n            {A.a: Decimal(2)},\n            {A.b: \"c\", A.a: Decimal(3)},\n        ],\n    )\n\n    spark_schema = A.get_structtype()\n    row_data = [(Decimal(1), \"a\"), (Decimal(2), None), (Decimal(3), \"c\")]\n    expected = spark.createDataFrame(row_data, spark_schema)\n\n    assert_df_equality(result, expected)", "\n\ndef test_create_partially_filled_dataset_from_list_with_complex_data(spark: SparkSession):\n    result = create_partially_filled_dataset(\n        spark,\n        B,\n        [\n            {\n                B.a: [\"a\"],\n                B.b: {\"a\": \"1\"},\n                B.c: create_structtype_row(A, {A.a: Decimal(1), A.b: \"a\"}),\n            },\n            {\n                B.a: [\"b\", \"c\"],\n                B.b: {\"b\": \"2\", \"c\": \"3\"},\n                B.c: create_structtype_row(A, {A.a: Decimal(2)}),\n            },\n            {\n                B.a: [\"d\"],\n                B.b: {\"d\": \"4\"},\n                B.c: create_structtype_row(A, {A.b: \"c\", A.a: Decimal(3)}),\n            },\n        ],\n    )\n\n    spark_schema = B.get_structtype()\n    row_data = [\n        ([\"a\"], {\"a\": \"1\"}, (Decimal(1), \"a\")),\n        ([\"b\", \"c\"], {\"b\": \"2\", \"c\": \"3\"}, (Decimal(2), None)),\n        ([\"d\"], {\"d\": \"4\"}, (Decimal(3), \"c\")),\n    ]\n    expected = spark.createDataFrame(row_data, spark_schema)\n\n    assert_df_equality(result, expected)", "\n\ndef test_create_partially_filled_dataset_with_invalid_argument(spark: SparkSession):\n    with pytest.raises(ValueError):\n        create_partially_filled_dataset(spark, A, ())  # type: ignore\n"]}
{"filename": "tests/_core/test_datatypes.py", "chunked_list": ["import pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import ArrayType as SparkArrayType\nfrom pyspark.sql.types import LongType\nfrom pyspark.sql.types import MapType as SparkMapType\nfrom pyspark.sql.types import StringType, StructField\nfrom pyspark.sql.types import StructType as SparkStructType\n\nfrom typedspark import ArrayType, Column, DataSet, MapType, Schema, StructType, create_empty_dataset\n", "from typedspark import ArrayType, Column, DataSet, MapType, Schema, StructType, create_empty_dataset\n\n\nclass SubSchema(Schema):\n    a: Column[StringType]\n    b: Column[StringType]\n\n\nclass Example(Schema):\n    a: Column[MapType[StringType, StringType]]\n    b: Column[ArrayType[StringType]]\n    c: Column[StructType[SubSchema]]", "class Example(Schema):\n    a: Column[MapType[StringType, StringType]]\n    b: Column[ArrayType[StringType]]\n    c: Column[StructType[SubSchema]]\n\n\ndef test_complex_datatypes_equals(spark: SparkSession):\n    df = create_empty_dataset(spark, Example)\n\n    assert df.schema[\"a\"] == StructField(\"a\", SparkMapType(StringType(), StringType()))\n    assert df.schema[\"b\"] == StructField(\"b\", SparkArrayType(StringType()))\n    structfields = [\n        StructField(\"a\", StringType(), True),\n        StructField(\"b\", StringType(), True),\n    ]\n    assert df.schema[\"c\"] == StructField(\"c\", SparkStructType(structfields))", "\n\nclass ArrayTypeSchema(Schema):\n    a: Column[ArrayType[StringType]]\n\n\nclass DifferentArrayTypeSchema(Schema):\n    a: Column[ArrayType[LongType]]\n\n\nclass MapTypeSchema(Schema):\n    a: Column[MapType[StringType, StringType]]", "\n\nclass MapTypeSchema(Schema):\n    a: Column[MapType[StringType, StringType]]\n\n\nclass DifferentKeyMapTypeSchema(Schema):\n    a: Column[MapType[LongType, StringType]]\n\n\nclass DifferentValueMapTypeSchema(Schema):\n    a: Column[MapType[StringType, LongType]]", "\n\nclass DifferentValueMapTypeSchema(Schema):\n    a: Column[MapType[StringType, LongType]]\n\n\nclass DifferentSubSchema(Schema):\n    a: Column[LongType]\n    b: Column[LongType]\n", "\n\nclass StructTypeSchema(Schema):\n    a: Column[StructType[SubSchema]]\n\n\nclass DifferentStructTypeSchema(Schema):\n    a: Column[StructType[DifferentSubSchema]]\n\n\ndef test_complex_datatypes_not_equals(spark: SparkSession):\n    with pytest.raises(TypeError):\n        df1 = create_empty_dataset(spark, ArrayTypeSchema)\n        DataSet[DifferentArrayTypeSchema](df1)\n\n    df2 = create_empty_dataset(spark, MapTypeSchema)\n    with pytest.raises(TypeError):\n        DataSet[DifferentKeyMapTypeSchema](df2)\n    with pytest.raises(TypeError):\n        DataSet[DifferentValueMapTypeSchema](df2)\n\n    with pytest.raises(TypeError):\n        df3 = create_empty_dataset(spark, StructTypeSchema)\n        DataSet[DifferentStructTypeSchema](df3)", "\n\ndef test_complex_datatypes_not_equals(spark: SparkSession):\n    with pytest.raises(TypeError):\n        df1 = create_empty_dataset(spark, ArrayTypeSchema)\n        DataSet[DifferentArrayTypeSchema](df1)\n\n    df2 = create_empty_dataset(spark, MapTypeSchema)\n    with pytest.raises(TypeError):\n        DataSet[DifferentKeyMapTypeSchema](df2)\n    with pytest.raises(TypeError):\n        DataSet[DifferentValueMapTypeSchema](df2)\n\n    with pytest.raises(TypeError):\n        df3 = create_empty_dataset(spark, StructTypeSchema)\n        DataSet[DifferentStructTypeSchema](df3)", ""]}
{"filename": "tests/_core/test_metadata.py", "chunked_list": ["from typing import Annotated\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import LongType\n\nfrom typedspark import Column, DataSet, Schema, create_empty_dataset\nfrom typedspark._core.column_meta import ColumnMeta\n\n\nclass A(Schema):\n    a: Annotated[Column[LongType], ColumnMeta(comment=\"test\")]\n    b: Column[LongType]", "\nclass A(Schema):\n    a: Annotated[Column[LongType], ColumnMeta(comment=\"test\")]\n    b: Column[LongType]\n\n\ndef test_add_schema_metadata(spark: SparkSession):\n    df: DataSet[A] = create_empty_dataset(spark, A, 1)\n    assert df.schema[\"a\"].metadata == {\"comment\": \"test\"}\n    assert df.schema[\"b\"].metadata == {}", "\n\nclass B(Schema):\n    a: Column[LongType]\n    b: Annotated[Column[LongType], ColumnMeta(comment=\"test\")]\n\n\ndef test_refresh_metadata(spark: SparkSession):\n    df_a = create_empty_dataset(spark, A, 1)\n    df_b = DataSet[B](df_a)\n    assert df_b.schema[\"a\"].metadata == {}\n    assert df_b.schema[\"b\"].metadata == {\"comment\": \"test\"}", ""]}
{"filename": "tests/_core/test_dataset.py", "chunked_list": ["import pandas as pd\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import LongType, StringType\n\nfrom typedspark import Column, DataSet, Schema\nfrom typedspark._utils.create_dataset import create_empty_dataset\n\n\nclass A(Schema):\n    a: Column[LongType]\n    b: Column[StringType]", "\nclass A(Schema):\n    a: Column[LongType]\n    b: Column[StringType]\n\n\ndef create_dataframe(spark: SparkSession, d):\n    return spark.createDataFrame(pd.DataFrame(d))\n\n\ndef test_dataset(spark: SparkSession):\n    d = dict(\n        a=[1, 2, 3],\n        b=[\"a\", \"b\", \"c\"],\n    )\n    df = create_dataframe(spark, d)\n    DataSet[A](df)", "\n\ndef test_dataset(spark: SparkSession):\n    d = dict(\n        a=[1, 2, 3],\n        b=[\"a\", \"b\", \"c\"],\n    )\n    df = create_dataframe(spark, d)\n    DataSet[A](df)\n", "\n\ndef test_dataset_allow_underscored_columns_not_in_schema(spark: SparkSession):\n    d = {\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"], \"__c\": [1, 2, 3]}\n    df = create_dataframe(spark, d)\n    DataSet[A](df)\n\n\ndef test_dataset_single_underscored_column_should_raise(spark: SparkSession):\n    d = {\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"], \"_c\": [1, 2, 3]}\n    df = create_dataframe(spark, d)\n    with pytest.raises(TypeError):\n        DataSet[A](df)", "def test_dataset_single_underscored_column_should_raise(spark: SparkSession):\n    d = {\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"], \"_c\": [1, 2, 3]}\n    df = create_dataframe(spark, d)\n    with pytest.raises(TypeError):\n        DataSet[A](df)\n\n\ndef test_dataset_missing_colnames(spark: SparkSession):\n    d = dict(\n        a=[1, 2, 3],\n    )\n    df = create_dataframe(spark, d)\n    with pytest.raises(TypeError):\n        DataSet[A](df)", "\n\ndef test_dataset_too_many_colnames(spark: SparkSession):\n    d = dict(\n        a=[1, 2, 3],\n        b=[\"a\", \"b\", \"c\"],\n        c=[1, 2, 3],\n    )\n    df = create_dataframe(spark, d)\n    with pytest.raises(TypeError):\n        DataSet[A](df)", "\n\ndef test_wrong_type(spark: SparkSession):\n    d = dict(\n        a=[1, 2, 3],\n        b=[1, 2, 3],\n    )\n    df = create_dataframe(spark, d)\n    with pytest.raises(TypeError):\n        DataSet[A](df)", "\n\ndef test_inherrited_functions(spark: SparkSession):\n    df = create_empty_dataset(spark, A)\n\n    df.distinct()\n    df.filter(A.a == 1)\n    df.orderBy(A.a)\n    df.transform(lambda df: df)\n", "\n\ndef test_inherrited_functions_with_other_dataset(spark: SparkSession):\n    df_a = create_empty_dataset(spark, A)\n    df_b = create_empty_dataset(spark, A)\n\n    df_a.join(df_b, A.a.str)\n    df_a.unionByName(df_b)\n", ""]}
{"filename": "tests/_core/test_column.py", "chunked_list": ["import pandas as pd\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.types import LongType, StringType\n\nfrom typedspark import Column, Schema\nfrom typedspark._utils.create_dataset import create_partially_filled_dataset\n\n\nclass A(Schema):\n    a: Column[LongType]\n    b: Column[StringType]", "\n\nclass A(Schema):\n    a: Column[LongType]\n    b: Column[StringType]\n\n\ndef test_column(spark: SparkSession):\n    (\n        spark.createDataFrame(\n            pd.DataFrame(\n                dict(\n                    a=[1, 2, 3],\n                )\n            )\n        )\n        .filter(A.a == 1)\n        .withColumn(A.b.str, lit(\"a\"))\n    )", "\n\ndef test_column_doesnt_exist():\n    with pytest.raises(TypeError):\n        A.z\n\n\n@pytest.mark.no_spark_session\ndef test_column_reference_without_spark_session():\n    a = A.a\n    assert a.str == \"a\"", "def test_column_reference_without_spark_session():\n    a = A.a\n    assert a.str == \"a\"\n\n\ndef test_column_with_deprecated_dataframe_param(spark: SparkSession):\n    df = create_partially_filled_dataset(spark, A, {A.a: [1, 2, 3]})\n    Column(\"a\", dataframe=df)\n", ""]}
{"filename": "tests/_schema/test_create_spark_schema.py", "chunked_list": ["import pytest\nfrom pyspark.sql.types import LongType, StringType, StructField, StructType\n\nfrom typedspark import Column, Schema\n\n\nclass A(Schema):\n    a: Column[LongType]\n    b: Column[StringType]\n", "\n\nclass B(Schema):\n    a: Column\n\n\ndef test_create_spark_schema():\n    result = A.get_structtype()\n    expected = StructType(\n        [\n            StructField(\"a\", LongType(), True),\n            StructField(\"b\", StringType(), True),\n        ]\n    )\n\n    assert result == expected", "\n\ndef test_create_spark_schema_with_faulty_schema():\n    with pytest.raises(TypeError):\n        B.get_structtype()\n"]}
{"filename": "tests/_schema/test_get_schema_definition.py", "chunked_list": ["from typedspark._core.datatypes import DayTimeIntervalType\nfrom typedspark._core.literaltype import IntervalType\nfrom typedspark._schema.get_schema_definition import _replace_literal, _replace_literals\n\n\ndef test_replace_literal():\n    result = _replace_literal(\n        \"DayTimeIntervalType[Literal[0], Literal[1]]\",\n        replace_literals_in=DayTimeIntervalType,\n        original=\"Literal[0]\",\n        replacement=\"IntervalType.DAY\",\n    )\n    expected = \"DayTimeIntervalType[IntervalType.DAY, Literal[1]]\"\n\n    assert result == expected", "\n\ndef test_replace_literals():\n    result = _replace_literals(\n        \"DayTimeIntervalType[Literal[0], Literal[1]]\",\n        replace_literals_in=DayTimeIntervalType,\n        replace_literals_by=IntervalType,\n    )\n    expected = \"DayTimeIntervalType[IntervalType.DAY, IntervalType.HOUR]\"\n\n    assert result == expected", ""]}
{"filename": "tests/_schema/test_offending_schemas.py", "chunked_list": ["from typing import Annotated, List, Type\n\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType\n\nfrom typedspark import ArrayType, Column, ColumnMeta, MapType, Schema, create_empty_dataset\nfrom typedspark._core.datatypes import DecimalType\n\n\nclass InvalidColumn(Schema):\n    a: int", "\n\nclass InvalidColumn(Schema):\n    a: int\n\n\nclass ColumnWithoutType(Schema):\n    a: Column\n\n\nclass AnnotationWithoutColumn(Schema):\n    a: Annotated  # type: ignore", "\n\nclass AnnotationWithoutColumn(Schema):\n    a: Annotated  # type: ignore\n\n\nclass InvalidColumnMeta(Schema):\n    a: Annotated[StringType, str]\n\n\nclass InvalidDataTypeWithinAnnotation(Schema):\n    a: Annotated[str, ColumnMeta()]  # type: ignore", "\n\nclass InvalidDataTypeWithinAnnotation(Schema):\n    a: Annotated[str, ColumnMeta()]  # type: ignore\n\n\nclass InvalidDataType(Schema):\n    a: Column[int]  # type: ignore\n\n\nclass ComplexTypeWithoutSubtype(Schema):\n    a: Column[ArrayType]", "\n\nclass ComplexTypeWithoutSubtype(Schema):\n    a: Column[ArrayType]\n\n\nclass ComplexTypeWithInvalidSubtype(Schema):\n    a: Column[ArrayType[int]]  # type: ignore\n\n\nclass InvalidDataTypeWithArguments(Schema):\n    a: Column[List[str]]  # type: ignore", "\n\nclass InvalidDataTypeWithArguments(Schema):\n    a: Column[List[str]]  # type: ignore\n\n\nclass DecimalTypeWithoutArguments(Schema):\n    a: Column[DecimalType]  # type: ignore\n\n\nclass DecimalTypeWithIncorrectArguments(Schema):\n    a: Column[DecimalType[int, int]]  # type: ignore", "\n\nclass DecimalTypeWithIncorrectArguments(Schema):\n    a: Column[DecimalType[int, int]]  # type: ignore\n\n\noffending_schemas: List[Type[Schema]] = [\n    InvalidColumn,\n    ColumnWithoutType,\n    AnnotationWithoutColumn,", "    ColumnWithoutType,\n    AnnotationWithoutColumn,\n    InvalidColumnMeta,\n    InvalidDataTypeWithinAnnotation,\n    InvalidDataType,\n    ComplexTypeWithoutSubtype,\n    ComplexTypeWithInvalidSubtype,\n    InvalidDataTypeWithArguments,\n]\n", "]\n\n\ndef test_offending_schema_exceptions(spark: SparkSession):\n    for schema in offending_schemas:\n        with pytest.raises(TypeError):\n            create_empty_dataset(spark, schema)\n\n\ndef test_offending_schemas_repr_exceptions():\n    for schema in offending_schemas:\n        schema.get_schema_definition_as_string(generate_imports=True)", "\ndef test_offending_schemas_repr_exceptions():\n    for schema in offending_schemas:\n        schema.get_schema_definition_as_string(generate_imports=True)\n\n\ndef test_offending_schemas_dtype():\n    with pytest.raises(TypeError):\n        ColumnWithoutType.a.dtype\n", "\n\ndef test_offending_schemas_runtime_error_on_load():\n    with pytest.raises(TypeError):\n\n        class WrongNumberOfArguments(Schema):\n            a: Column[MapType[StringType]]  # type: ignore\n"]}
{"filename": "tests/_schema/test_schema.py", "chunked_list": ["from typing import Annotated, Literal, Type\n\nimport pytest\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import LongType, StringType, StructField, StructType\n\nimport typedspark\nfrom typedspark import Column, ColumnMeta, Schema, create_partially_filled_dataset\nfrom typedspark._core.literaltype import IntervalType\nfrom typedspark._schema.schema import DltKwargs", "from typedspark._core.literaltype import IntervalType\nfrom typedspark._schema.schema import DltKwargs\n\n\nclass A(Schema):\n    a: Column[LongType]\n    b: Column[StringType]\n\n\nschema_a_string = \"\"\"", "\nschema_a_string = \"\"\"\nfrom pyspark.sql.types import LongType, StringType\n\nfrom typedspark import Column, Schema\n\n\nclass A(Schema):\n    a: Column[LongType]\n    b: Column[StringType]", "    a: Column[LongType]\n    b: Column[StringType]\n\"\"\"\n\nschema_a_string_with_documentation = '''from typing import Annotated\n\nfrom pyspark.sql.types import LongType, StringType\n\nfrom typedspark import Column, ColumnMeta, Schema\n", "from typedspark import Column, ColumnMeta, Schema\n\n\nclass A(Schema):\n    \"\"\"Add documentation here.\"\"\"\n\n    a: Annotated[Column[LongType], ColumnMeta(comment=\"\")]\n    b: Annotated[Column[StringType], ColumnMeta(comment=\"\")]\n'''\n", "'''\n\n\nclass B(Schema):\n    b: Column[LongType]\n    a: Column[StringType]\n\n\nclass Values(Schema):\n    a: Column[typedspark.DecimalType[Literal[38], Literal[18]]]\n    b: Column[StringType]", "class Values(Schema):\n    a: Column[typedspark.DecimalType[Literal[38], Literal[18]]]\n    b: Column[StringType]\n\n\nclass ComplexDatatypes(Schema):\n    value: Column[typedspark.StructType[Values]]\n    items: Column[typedspark.ArrayType[StringType]]\n    consequences: Column[typedspark.MapType[StringType, typedspark.ArrayType[StringType]]]\n    diff: Column[typedspark.DayTimeIntervalType[IntervalType.DAY, IntervalType.SECOND]]", "\n\nschema_complex_datatypes = '''from typing import Annotated, Literal\n\nfrom pyspark.sql.types import StringType\n\nfrom typedspark import ArrayType, Column, ColumnMeta, DayTimeIntervalType, DecimalType, IntervalType, MapType, Schema, StructType\n\n\nclass ComplexDatatypes(Schema):", "\nclass ComplexDatatypes(Schema):\n    \"\"\"Add documentation here.\"\"\"\n\n    value: Annotated[Column[StructType[test_schema.Values]], ColumnMeta(comment=\"\")]\n    items: Annotated[Column[ArrayType[StringType]], ColumnMeta(comment=\"\")]\n    consequences: Annotated[Column[MapType[StringType, ArrayType[StringType]]], ColumnMeta(comment=\"\")]\n    diff: Annotated[Column[DayTimeIntervalType[IntervalType.DAY, IntervalType.SECOND]], ColumnMeta(comment=\"\")]\n\n", "\n\nclass Values(Schema):\n    \"\"\"Add documentation here.\"\"\"\n\n    a: Annotated[Column[DecimalType[Literal[38], Literal[18]]], ColumnMeta(comment=\"\")]\n    b: Annotated[Column[StringType], ColumnMeta(comment=\"\")]\n'''  # noqa: E501\n\n\nclass PascalCase(Schema):\n    \"\"\"Schema docstring.\"\"\"\n\n    a: Annotated[Column[StringType], ColumnMeta(comment=\"some\")]\n    b: Annotated[Column[LongType], ColumnMeta(comment=\"other\")]", "\n\nclass PascalCase(Schema):\n    \"\"\"Schema docstring.\"\"\"\n\n    a: Annotated[Column[StringType], ColumnMeta(comment=\"some\")]\n    b: Annotated[Column[LongType], ColumnMeta(comment=\"other\")]\n\n\ndef test_all_column_names():\n    assert A.all_column_names() == [\"a\", \"b\"]\n    assert B.all_column_names() == [\"b\", \"a\"]", "\ndef test_all_column_names():\n    assert A.all_column_names() == [\"a\", \"b\"]\n    assert B.all_column_names() == [\"b\", \"a\"]\n\n\ndef test_all_column_names_except_for():\n    assert A.all_column_names_except_for([\"a\"]) == [\"b\"]\n    assert B.all_column_names_except_for([]) == [\"b\", \"a\"]\n    assert B.all_column_names_except_for([\"b\", \"a\"]) == []", "\n\ndef test_get_snake_case():\n    assert A.get_snake_case() == \"a\"\n    assert PascalCase.get_snake_case() == \"pascal_case\"\n\n\ndef test_get_docstring():\n    assert A.get_docstring() is None\n    assert PascalCase.get_docstring() == \"Schema docstring.\"", "\n\ndef test_get_structtype():\n    assert A.get_structtype() == StructType(\n        [StructField(\"a\", LongType(), True), StructField(\"b\", StringType(), True)]\n    )\n    assert PascalCase.get_structtype() == StructType(\n        [\n            StructField(\"a\", StringType(), metadata={\"comment\": \"some\"}),\n            StructField(\"b\", LongType(), metadata={\"comment\": \"other\"}),\n        ]\n    )", "\n\ndef test_get_dlt_kwargs():\n    assert A.get_dlt_kwargs() == DltKwargs(\n        name=\"a\",\n        comment=None,\n        schema=StructType(\n            [StructField(\"a\", LongType(), True), StructField(\"b\", StringType(), True)]\n        ),\n    )\n\n    assert PascalCase.get_dlt_kwargs() == DltKwargs(\n        name=\"pascal_case\",\n        comment=\"Schema docstring.\",\n        schema=StructType(\n            [\n                StructField(\"a\", StringType(), metadata={\"comment\": \"some\"}),\n                StructField(\"b\", LongType(), metadata={\"comment\": \"other\"}),\n            ]\n        ),\n    )", "\n\ndef test_repr():\n    assert repr(A) == schema_a_string\n\n\n@pytest.mark.parametrize(\n    \"schema, expected_schema_definition\",\n    [\n        (A, schema_a_string_with_documentation),", "    [\n        (A, schema_a_string_with_documentation),\n        (ComplexDatatypes, schema_complex_datatypes),\n    ],\n)\ndef test_get_schema(schema: Type[Schema], expected_schema_definition: str):\n    schema_definition = schema.get_schema_definition_as_string(include_documentation=True)\n    assert schema_definition == expected_schema_definition\n\n\ndef test_dtype_attributes(spark: SparkSession):\n    assert ComplexDatatypes.value.dtype == typedspark.StructType[Values]\n    assert ComplexDatatypes.value.dtype.schema == Values\n    assert ComplexDatatypes.value.dtype.schema.b.dtype == StringType\n\n    df = create_partially_filled_dataset(\n        spark,\n        ComplexDatatypes,\n        {\n            ComplexDatatypes.value: create_partially_filled_dataset(\n                spark,\n                Values,\n                {\n                    Values.b: [\"a\", \"b\", \"c\"],\n                },\n            ).collect(),\n        },\n    )\n    assert df.filter(ComplexDatatypes.value.dtype.schema.b == \"b\").count() == 1", "\n\ndef test_dtype_attributes(spark: SparkSession):\n    assert ComplexDatatypes.value.dtype == typedspark.StructType[Values]\n    assert ComplexDatatypes.value.dtype.schema == Values\n    assert ComplexDatatypes.value.dtype.schema.b.dtype == StringType\n\n    df = create_partially_filled_dataset(\n        spark,\n        ComplexDatatypes,\n        {\n            ComplexDatatypes.value: create_partially_filled_dataset(\n                spark,\n                Values,\n                {\n                    Values.b: [\"a\", \"b\", \"c\"],\n                },\n            ).collect(),\n        },\n    )\n    assert df.filter(ComplexDatatypes.value.dtype.schema.b == \"b\").count() == 1", ""]}
{"filename": "tests/_schema/test_structfield.py", "chunked_list": ["from typing import Annotated, get_type_hints\n\nimport pytest\nfrom pyspark.sql.types import BooleanType, LongType, StringType, StructField\n\nfrom typedspark import Column, ColumnMeta, Schema\nfrom typedspark._schema.structfield import (\n    _get_structfield_dtype,\n    get_structfield,\n    get_structfield_meta,", "    get_structfield,\n    get_structfield_meta,\n)\n\n\nclass A(Schema):\n    a: Column[LongType]\n    b: Column[StringType]\n    c: Annotated[Column[StringType], ColumnMeta(comment=\"comment\")]\n    d: Annotated[Column[BooleanType], ColumnMeta(comment=\"comment2\")]", "\n\n@pytest.fixture()\ndef type_hints():\n    return get_type_hints(A, include_extras=True)\n\n\ndef test_get_structfield_dtype(type_hints):\n    assert _get_structfield_dtype(Column[LongType], \"a\") == LongType()\n    assert _get_structfield_dtype(type_hints[\"b\"], \"b\") == StringType()\n    assert (\n        _get_structfield_dtype(Annotated[Column[StringType], ColumnMeta(comment=\"comment\")], \"c\")\n        == StringType()\n    )\n    assert _get_structfield_dtype(type_hints[\"d\"], \"d\") == BooleanType()", "\n\ndef test_get_structfield_metadata(type_hints):\n    assert get_structfield_meta(Column[LongType]) == ColumnMeta()\n    assert get_structfield_meta(type_hints[\"b\"]) == ColumnMeta()\n    assert get_structfield_meta(\n        Annotated[Column[StringType], ColumnMeta(comment=\"comment\")]\n    ) == ColumnMeta(comment=\"comment\")\n    assert get_structfield_meta(type_hints[\"d\"]) == ColumnMeta(comment=\"comment2\")\n", "\n\ndef test_get_structfield(type_hints):\n    assert get_structfield(\"a\", Column[LongType]) == StructField(name=\"a\", dataType=LongType())\n    assert get_structfield(\"b\", type_hints[\"b\"]) == StructField(name=\"b\", dataType=StringType())\n    assert get_structfield(\n        \"c\", Annotated[Column[StringType], ColumnMeta(comment=\"comment\")]\n    ) == StructField(name=\"c\", dataType=StringType(), metadata={\"comment\": \"comment\"})\n    assert get_structfield(\"d\", type_hints[\"d\"]) == StructField(\n        name=\"d\", dataType=BooleanType(), metadata={\"comment\": \"comment2\"}\n    )", ""]}
{"filename": "docs/remove_metadata.py", "chunked_list": ["\"\"\"Removes the metadata from a notebook.\n\nAlso removes the spark warnings from cells where the sparksession is initialized.\n\"\"\"\nimport sys\n\nimport nbformat\n\n\ndef clear_metadata(cell):\n    \"\"\"Clears the metadata of a notebook cell.\"\"\"\n    cell.metadata = {}", "\ndef clear_metadata(cell):\n    \"\"\"Clears the metadata of a notebook cell.\"\"\"\n    cell.metadata = {}\n\n\ndef remove_spark_warnings(cell):\n    \"\"\"Removes the spark warnings from a notebook cell.\"\"\"\n    if \"outputs\" in cell.keys():\n        outputs = []\n        for output in cell.outputs:\n            if \"text\" in output.keys():\n                if 'Setting default log level to \"WARN\"' in output.text:\n                    continue\n                if (\n                    \"WARN NativeCodeLoader: Unable to load native-hadoop library for your platform.\"\n                    in output.text\n                ):\n                    continue\n                if \"WARN Utils: Service 'SparkUI' could not bind on port\" in output.text:\n                    continue\n            outputs.append(output)\n\n        cell.outputs = outputs", "\n\ndef remove_papermill_metadata(nb):\n    \"\"\"Removes the papermill metadata from a notebook.\"\"\"\n    if \"papermill\" in nb.metadata.keys():\n        nb.metadata.pop(\"papermill\")\n\n\nif __name__ == \"__main__\":\n    FILENAME = sys.argv[1]\n    nb = nbformat.read(FILENAME, as_version=4)\n\n    for nb_cell in nb[\"cells\"]:\n        clear_metadata(nb_cell)\n        remove_spark_warnings(nb_cell)\n\n    remove_papermill_metadata(nb)\n\n    nbformat.write(nb, FILENAME)", "if __name__ == \"__main__\":\n    FILENAME = sys.argv[1]\n    nb = nbformat.read(FILENAME, as_version=4)\n\n    for nb_cell in nb[\"cells\"]:\n        clear_metadata(nb_cell)\n        remove_spark_warnings(nb_cell)\n\n    remove_papermill_metadata(nb)\n\n    nbformat.write(nb, FILENAME)", ""]}
{"filename": "docs/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = \"typedspark\"\ncopyright = \"2023, Nanne Aben, Marijn Valk\"", "project = \"typedspark\"\ncopyright = \"2023, Nanne Aben, Marijn Valk\"\nauthor = \"Nanne Aben, Marijn Valk\"\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = [\"sphinx.ext.autodoc\", \"sphinx_rtd_theme\", \"nbsphinx\"]\n\ntemplates_path = [\"_templates\"]", "\ntemplates_path = [\"_templates\"]\nexclude_patterns = []\n\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = \"sphinx_rtd_theme\"\nhtml_static_path = [\"_static\"]", "html_theme = \"sphinx_rtd_theme\"\nhtml_static_path = [\"_static\"]\n\nautodoc_inherit_docstrings = False\n"]}
