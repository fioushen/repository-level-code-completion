{"filename": "entry_point.py", "chunked_list": ["import argparse\nimport importlib\nimport json\nimport os\nimport pkgutil\nfrom logging import getLogger\nfrom types import SimpleNamespace\n\nfrom jobs.abstracts.abstract_job import AbstractJob\n", "from jobs.abstracts.abstract_job import AbstractJob\n\n\nlogger = getLogger(__name__)\n\ndef get_job_object(job_class_name: str, relative_import=\"jobs\") -> AbstractJob:\n    for module in pkgutil.iter_modules([f\"{os.path.dirname(os.path.abspath(__file__))}/jobs\"]):\n        job_module = importlib.import_module(f\"{relative_import}.{module.name}\")\n        try:\n            job_class = getattr(job_module, job_class_name)\n            return job_class()\n        except AttributeError:\n            continue\n\n    raise RuntimeError(\"could not find job class\")", "\n\ndef init():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str)\n    args, unknown = parser.parse_known_args()\n    if len(unknown) > 0:\n        logger.info(f\"Unknown arguments passed: {unknown}\")\n    cfg = json.loads(args.config, object_hook=lambda d: SimpleNamespace(**d))\n    get_job_object(cfg.job_class_name).main(cfg)", "\n\nif __name__ == \"__main__\":\n    init()\n"]}
{"filename": "cli.py", "chunked_list": ["import importlib\nimport logging\nimport pkgutil\nimport os\n\nimport click\n\nfrom experiments.abstracts.abstract_experiments import AbstractExperiment\n\nlog = logging.getLogger()", "\nlog = logging.getLogger()\n\n\n@click.group()\ndef cli():\n    pass\n\n\n@cli.command()", "\n@cli.command()\n@click.option('--experiment_name', '-n', help='Please provide a unique name')\n@click.option('--experiment_class', '-c', help='Please provide experiment class name eg. NActionExperiment')\ndef run(experiment_name: str, experiment_class: str):\n    get_experiment_object(experiment_class).run(experiment_name)\n    click.echo(\"Running Experiment on Sage maker Training Jobs. Please wait until jobs are finished\")\n    click.echo(\"Check status here: https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs\")\n\n", "\n\n@cli.command()\n@click.option('--experiment_name', '-n', help='Please provide the experiment name you set when running the experiment', default=None)\n@click.option('--experiment_class', '-c', help='Please provide experiment class name eg. NActionExperiment')\n@click.option('--local_path', '-d', help='If the results are stored locally, provide the directory location', default=None)\ndef output(experiment_name: str, experiment_class: str, local_path: str):\n    click.echo(\"Getting Experiment output on Sage maker Training Jobs.\")\n    get_experiment_object(experiment_class).get_output(experiment_name, local_path=local_path)\n    click.echo(\"Output has finished successfully\")", "\n\ndef get_experiment_object(experiment_class_name: str, relative_import=\"experiments\") -> AbstractExperiment:\n    for module in pkgutil.iter_modules([f\"{os.path.dirname(os.path.abspath(__file__))}/experiments\"]):\n        experiment_module = importlib.import_module(f'{relative_import}.{module.name}')\n        try:\n            experiment_class = getattr(experiment_module, experiment_class_name)\n            return experiment_class()\n        except AttributeError:\n            continue\n\n    raise RuntimeError(\"could not find experiment class\")", "\n\nif __name__ == '__main__':\n    cli()\n"]}
{"filename": "jobs/__init__.py", "chunked_list": [""]}
{"filename": "jobs/n_val_data_job.py", "chunked_list": ["from jobs.abstracts.abstract_synthetic_job import AbstractSyntheticJob\n\n\nclass NValDataJob(AbstractSyntheticJob):\n    def main(self, cfg):\n        return self.run(cfg, \"n_val_data\", cfg.n_val_data_list)\n"]}
{"filename": "jobs/n_actions_job.py", "chunked_list": ["from jobs.abstracts.abstract_synthetic_job import AbstractSyntheticJob\n\n\nclass NActionsJob(AbstractSyntheticJob):\n    def main(self, cfg):\n        return self.run(cfg, \"n_actions\", cfg.n_actions_list)\n"]}
{"filename": "jobs/n_unobs_cat_dim_job.py", "chunked_list": ["from jobs.abstracts.abstract_synthetic_job import AbstractSyntheticJob\n\n\nclass NUnobsCatDimJob(AbstractSyntheticJob):\n    def main(self, cfg):\n        return self.run(cfg, \"n_unobserved_cat_dim\", cfg.n_unobserved_cat_dim_list)\n"]}
{"filename": "jobs/real_dataset_job.py", "chunked_list": ["import os\nfrom logging import getLogger\nfrom os.path import dirname\nfrom time import time\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom obp.policy import BernoulliTS, Random", "import torch\nfrom obp.policy import BernoulliTS, Random\nfrom torch.utils.data import DataLoader\n\nfrom .abstracts.abstract_ope_job import AbstractOpeJob\nfrom .utils.learn_embed import LearnEmbedLinear, TorchBanditDataset\nfrom .utils.ope import run_real_dataset_ope\nfrom .utils.dataset import ModifiedOpenBanditDataset\nfrom experiments.utils.configs import RealOpeTrialConfig\n", "from experiments.utils.configs import RealOpeTrialConfig\n\nlogger = getLogger(__name__)\n\n\nclass RealDatasetJob(AbstractOpeJob):\n    def main(self, cfg: RealOpeTrialConfig):\n        logger.info(cfg)\n        logger.info(f\"The current working directory is {os.getcwd()}\")\n        start_time = time()\n        if not cfg.s3_path.startswith(\"s3://\"):\n            Path(cfg.s3_path).mkdir(parents=True, exist_ok=True)\n\n        # configurations\n        sample_size = cfg.sample_size\n        random_state = cfg.random_state\n        obd_path = dirname(dirname(dirname(cfg.s3_path))) + \"/open_bandit_dataset\"\n        OBD_N_ACTIONS = 80\n        OBD_LEN_LIST = 3\n\n        # define policies\n        policy_ur = Random(\n            n_actions=OBD_N_ACTIONS,\n            len_list=OBD_LEN_LIST,\n            random_state=random_state,\n        )\n        policy_ts = BernoulliTS(\n            n_actions=OBD_N_ACTIONS,\n            len_list=OBD_LEN_LIST,\n            random_state=random_state,\n            is_zozotown_prior=True,\n            campaign=\"all\",\n        )\n\n        # calc ground-truth policy value (on-policy)\n        policy_value = ModifiedOpenBanditDataset.calc_on_policy_policy_value_estimate(\n            behavior_policy=\"bts\", campaign=\"all\", data_path=obd_path\n        )\n\n        # define a dataset class\n        dataset = ModifiedOpenBanditDataset(\n            behavior_policy=\"random\",\n            data_path=obd_path,\n            campaign=\"all\",\n        )\n\n        elapsed_prev = 0.0\n        squared_error_list = []\n        relative_squared_error_list = []\n\n        # iterate over n_seeds bootstrap runs\n        for t in np.arange(cfg.n_seeds):\n            pi_b = policy_ur.compute_batch_action_dist(n_rounds=sample_size)\n            pi_e = policy_ts.compute_batch_action_dist(n_rounds=sample_size)\n            pi_e = pi_e.reshape(sample_size, OBD_N_ACTIONS * OBD_LEN_LIST, 1) / OBD_LEN_LIST\n\n            val_bandit_data = dataset.sample_bootstrap_bandit_feedback(\n                sample_size=sample_size,\n                random_state=t,\n            )\n            val_bandit_data[\"pi_b\"] = pi_b.reshape(sample_size, OBD_N_ACTIONS * OBD_LEN_LIST, 1) / OBD_LEN_LIST\n\n            # learn the reward model for DM and DR methods - same model as Learned MIPS OneHot\n            model = LearnEmbedLinear(\n                action_dim=val_bandit_data['action_context'].shape[1],\n                action_cat_dim=len(np.unique(val_bandit_data['action_context'])),\n                n_actions=val_bandit_data['n_actions'],\n                context_dim=val_bandit_data['context'].shape[1],\n                config=cfg.embed_model_config\n            )\n            \n            model_dataset = TorchBanditDataset(\n                n_actions=val_bandit_data['n_actions'],\n                n_dim_action=len(np.unique(val_bandit_data['action_context'])), \n                context=val_bandit_data['context'], \n                action=val_bandit_data['action'], \n                action_embed=val_bandit_data['action_context'], \n                reward=val_bandit_data['reward']\n            )\n            train_dataloader = DataLoader(model_dataset, batch_size=32, shuffle=True)\n\n            loss_fn = torch.nn.MSELoss()\n            optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n            for _ in range(cfg.learned_embed_params.epochs):\n                model.train_loop(train_dataloader, loss_fn, optimizer)\n\n            estimated_rewards = np.zeros((val_bandit_data['n_rounds'], val_bandit_data['n_actions'], 1))\n            for i in range(val_bandit_data['n_actions']):\n                indices = np.where(dataset.action == i)\n                if len(indices) != 0:\n                    _, action, action_embed, _ = model_dataset.__getitem__(i)\n                else:\n                    action_embed = np.zeros(val_bandit_data['action_context'].shape[1]).astype(int)\n                estimated_rewards[:,i,:] = model(\n                    torch.from_numpy(val_bandit_data['context']).float(),\n                    action.repeat(val_bandit_data['n_rounds'], 1),\n                    action_embed.repeat(val_bandit_data['n_rounds'], 1, 1)\n                ).detach().numpy()     \n\n            estimated_rewards_dict = {\n                \"DR\": estimated_rewards,\n                \"DM\": estimated_rewards,\n                \"SwitchDR\": estimated_rewards,\n            }\n\n            # estimate policy values and calculate MSE of estimators\n            squared_errors, relative_squared_errors = run_real_dataset_ope(\n                val_bandit_data=val_bandit_data,\n                action_dist_val=pi_e,\n                estimated_rewards=estimated_rewards_dict,\n                policy_value=policy_value,\n                embed_model_config=cfg.embed_model_config,\n                learned_embed_params=cfg.learned_embed_params,\n                logging_losses_file = f\"{cfg.s3_path}/model_losses/{time()}.parquet\"\n            )\n            squared_error_list.append(squared_errors)\n            relative_squared_error_list.append(relative_squared_errors)\n\n            elapsed = np.round((time() - start_time) / 60, 2)\n            diff = np.round(elapsed - elapsed_prev, 2)\n            logger.info(f\"t={t}: {elapsed}min (diff {diff}min)\")\n            elapsed_prev = elapsed\n\n            # aggregate all results\n            result_df = (\n                pd.DataFrame(pd.DataFrame(squared_error_list).stack())\n                .reset_index(1)\n                .rename(columns={\"level_1\": \"est\", 0: \"se\"})\n            )\n            result_df.reset_index(inplace=True, drop=True)\n            result_df.to_csv(f\"{cfg.s3_path}/result_df.csv\")\n\n        rel_result_df = (\n            pd.DataFrame(pd.DataFrame(relative_squared_error_list).stack())\n            .reset_index(1)\n            .rename(columns={\"level_1\": \"est\", 0: \"se\"})\n        )\n        rel_result_df.reset_index(inplace=True, drop=True)\n        rel_result_df.to_csv(f\"{cfg.s3_path}/rel_result_df.csv\")", ""]}
{"filename": "jobs/ope_hpo_job.py", "chunked_list": ["import os\nfrom functools import reduce\nfrom logging import getLogger\nfrom time import time\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom obp.dataset import SyntheticBanditDatasetWithActionEmbeds, linear_reward_function\nfrom obp.ope import InverseProbabilityWeighting as IPS", "from obp.dataset import SyntheticBanditDatasetWithActionEmbeds, linear_reward_function\nfrom obp.ope import InverseProbabilityWeighting as IPS\nfrom obp.ope import OffPolicyEvaluation\n\nfrom experiments.utils.configs import HpoTrialConfig\n\nfrom .abstracts.abstract_ope_job import AbstractOpeJob\nfrom .utils.policy import gen_eps_greedy\nfrom .utils.learn_embed import LearnedEmbedMIPS, LearnEmbedMF\n", "from .utils.learn_embed import LearnedEmbedMIPS, LearnEmbedMF\n\nlogger = getLogger(__name__)\n\nESTIMATOR = \"Learned NMF MIPS\"\n\nclass OpeHpoJob(AbstractOpeJob):\n    def main(self, cfg: HpoTrialConfig):\n        logger.info(cfg)\n        logger.info(f\"The current working directory is {os.getcwd()}\")\n        start_time = time()\n        if not cfg.s3_path.startswith(\"s3://\"):\n            Path(cfg.s3_path).mkdir(parents=True, exist_ok=True)\n\n        dataset = SyntheticBanditDatasetWithActionEmbeds(  \n            n_actions=cfg.n_actions,\n            dim_context=cfg.dim_context,\n            beta=cfg.beta,\n            reward_type=\"continuous\",\n            n_cat_per_dim=cfg.n_cat_per_dim,\n            latent_param_mat_dim=cfg.latent_param_mat_dim,\n            n_cat_dim=cfg.n_cat_dim,\n            n_unobserved_cat_dim=cfg.n_unobserved_cat_dim,\n            n_deficient_actions=int(cfg.n_actions * cfg.n_def_actions),\n            reward_function=linear_reward_function,\n            reward_std=cfg.reward_std,\n            random_state=cfg.random_state,\n        )\n        test_bandit_data = dataset.obtain_batch_bandit_feedback(\n            n_rounds=cfg.n_test_data\n        )\n        action_dist_test = gen_eps_greedy(\n            expected_reward=test_bandit_data[\"expected_reward\"],\n            is_optimal=cfg.is_optimal,\n            eps=cfg.eps,\n        )\n        policy_value = dataset.calc_ground_truth_policy_value(\n            expected_reward=test_bandit_data[\"expected_reward\"],\n            action_dist=action_dist_test,\n        )\n\n        elapsed_prev = 0.0\n        squared_error_list = []\n        relative_squared_error_list = []\n        estimated_policy_value_list = []\n\n        # iterate over n_seeds bootstrap runs\n        for t in np.arange(cfg.n_seeds):\n            # generate validation data\n            val_bandit_data = dataset.obtain_batch_bandit_feedback(\n                n_rounds=cfg.n_val_data,\n            )\n\n            # make decisions on validation data\n            action_dist_val = gen_eps_greedy(\n                expected_reward=val_bandit_data[\"expected_reward\"],\n                is_optimal=cfg.is_optimal,\n                eps=cfg.eps,\n            )\n\n            ope = OffPolicyEvaluation(\n                bandit_feedback=val_bandit_data,\n                ope_estimators=[\n                    IPS(estimator_name=\"IPS\"),\n                    LearnedEmbedMIPS(\n                        estimator_name=ESTIMATOR, \n                        n_actions=val_bandit_data[\"n_actions\"], \n                        embed_model=LearnEmbedMF, \n                        learned_embed_params=cfg.learned_embed_params,\n                        embed_model_config=cfg.embed_model_config,\n                    ),\n                ],\n            )\n\n            estimated_policy_values = ope.estimate_policy_values(\n                action_dist=action_dist_val,\n                action_embed=val_bandit_data[\"action_embed\"],\n                pi_b=val_bandit_data[\"pi_b\"],\n            )\n            \n            estimated_policy_value_list.append(estimated_policy_values)\n\n            squared_errors = reduce(lambda acc, val: {**acc, val[0]: (val[1] - policy_value) ** 2}, estimated_policy_values.items(), {})\n            baseline = squared_errors[\"IPS\"]\n            relative_squared_errors = reduce(lambda acc, val: {**acc, val[0]: val[1] / baseline }, squared_errors.items(), {})\n\n            # estimate policy values and calculate MSE of estimators\n            squared_error_list.append(squared_errors)\n            relative_squared_error_list.append(relative_squared_errors)\n\n            elapsed = np.round((time() - start_time) / 60, 2)\n            diff = np.round(elapsed - elapsed_prev, 2)\n            logger.info(f\"t={t}: {elapsed}min (diff {diff}min)\")\n            elapsed_prev = elapsed\n\n        # aggregate all results\n        value_df = (\n            pd.DataFrame(pd.DataFrame(estimated_policy_value_list).stack())\n            .reset_index(1)\n            .rename(columns={\"level_1\": \"est\", 0: \"value\"})\n        )\n        value_df.reset_index(inplace=True, drop=True)\n        value_df[\"se\"] = (value_df.value - policy_value) ** 2\n        value_df[\"bias\"] = 0\n        value_df[\"variance\"] = 0\n        sample_mean = pd.DataFrame(value_df.groupby([\"est\"]).mean().value).reset_index()\n        for est_ in sample_mean[\"est\"]:\n            estimates = value_df.loc[value_df[\"est\"] == est_, \"value\"].values\n            mean_estimates = sample_mean.loc[sample_mean[\"est\"] == est_, \"value\"].values\n            mean_estimates = np.ones_like(estimates) * mean_estimates\n            value_df.loc[value_df[\"est\"] == est_, \"bias\"] = np.sqrt((\n                policy_value - mean_estimates\n            ) ** 2)\n            value_df.loc[value_df[\"est\"] == est_, \"variance\"] = np.sqrt((\n                estimates - mean_estimates\n            ) ** 2)\n        value_df.set_index(\"est\", inplace=True)\n        value_df.to_csv(f\"{cfg.s3_path}/value_df.csv\")\n\n        result_df = (\n            pd.DataFrame(pd.DataFrame(squared_error_list).stack())\n            .reset_index(1)\n            .rename(columns={\"level_1\": \"est\", 0: \"se\"})\n        )\n        result_df.reset_index(inplace=True, drop=True)\n        result_df.to_csv(f\"{cfg.s3_path}/result_df.csv\")\n\n        rel_result_df = (\n            pd.DataFrame(pd.DataFrame(relative_squared_error_list).stack())\n            .reset_index(1)\n            .rename(columns={\"level_1\": \"est\", 0: \"se\"})\n        )\n        rel_result_df.reset_index(inplace=True, drop=True)\n        rel_result_df.to_csv(f\"{cfg.s3_path}/rel_result_df.csv\")\n        mse = rel_result_df.groupby('est').apply(lambda x: np.power(np.e, np.log(x).mean()))['se'][ESTIMATOR]\n        lcb = rel_result_df.groupby('est').apply(lambda x: np.power(np.e, np.log(x).mean() - np.log(x).std() / np.sqrt(len(x) - 1)))['se'][ESTIMATOR]\n        ucb = rel_result_df.groupby('est').apply(lambda x: np.power(np.e, np.log(x).mean() + np.log(x).std() / np.sqrt(len(x) - 1)))['se'][ESTIMATOR]\n        logger.info(f'Relative MSE: {mse:.4f}')\n        logger.info(f'Relative MSE LCB: {lcb:.4f}')\n        logger.info(f'Relative MSE UCB: {ucb:.4f}')\n        logger.info(f'Bias: {value_df[\"bias\"][ESTIMATOR].mean():.10f}')\n        logger.info(f'Variance: {value_df[\"variance\"][ESTIMATOR].mean():.10f}')", "        "]}
{"filename": "jobs/utils/learn_embed.py", "chunked_list": ["from dataclasses import asdict\nfrom typing import Optional\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom obp.ope import MarginalizedInverseProbabilityWeighting\nfrom scipy import stats\nfrom sklearn.decomposition import NMF\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis", "from sklearn.decomposition import NMF\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset, random_split\n\nfrom experiments.utils.configs import LearnEmbedConfig, LearnedEmbedParams\n\n\ndef disable_training(f):\n    def wrapped(self, *args, **kwargs):\n        self.action_embed_stack.training = False\n        output = f(self, *args, **kwargs)\n        self.action_embed_stack.training = True\n        return output\n\n    return wrapped", "def disable_training(f):\n    def wrapped(self, *args, **kwargs):\n        self.action_embed_stack.training = False\n        output = f(self, *args, **kwargs)\n        self.action_embed_stack.training = True\n        return output\n\n    return wrapped\n\n\nclass LearnEmbedNetwork(nn.Module):\n    def __init__(self, action_dim=3, n_actions=10, action_cat_dim=10, context_dim=10, config=LearnEmbedConfig()):\n        super(LearnEmbedNetwork, self).__init__()\n\n    def action_embeddings(self, action=None, action_embed=None):\n        \"\"\"Returns learned action embeddings for the given action and/or pre-defined action embedding\"\"\"\n        return NotImplementedError\n\n    def train_loop(self, dataloader, loss_fn, optimizer):\n        for x, a, e, r in dataloader:\n            # Compute prediction and loss\n            pred = self(x, a, e)\n            loss = loss_fn(pred, r)\n\n            # Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    def report_loss(self, dataloader, loss_fn):\n        loss = 0\n        num_batches = len(dataloader)\n        self.eval()\n        for x, a, e, r in dataloader:\n            pred = self(x, a, e)\n            loss += loss_fn(pred, r).item()\n        self.train()\n        return loss / num_batches", "\n\nclass LearnEmbedNetwork(nn.Module):\n    def __init__(self, action_dim=3, n_actions=10, action_cat_dim=10, context_dim=10, config=LearnEmbedConfig()):\n        super(LearnEmbedNetwork, self).__init__()\n\n    def action_embeddings(self, action=None, action_embed=None):\n        \"\"\"Returns learned action embeddings for the given action and/or pre-defined action embedding\"\"\"\n        return NotImplementedError\n\n    def train_loop(self, dataloader, loss_fn, optimizer):\n        for x, a, e, r in dataloader:\n            # Compute prediction and loss\n            pred = self(x, a, e)\n            loss = loss_fn(pred, r)\n\n            # Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    def report_loss(self, dataloader, loss_fn):\n        loss = 0\n        num_batches = len(dataloader)\n        self.eval()\n        for x, a, e, r in dataloader:\n            pred = self(x, a, e)\n            loss += loss_fn(pred, r).item()\n        self.train()\n        return loss / num_batches", "\n\nclass LearnEmbedLinear(LearnEmbedNetwork):\n    \"\"\"Linear learning objective, where the action network only uses the action identity\"\"\"\n\n    def __init__(self, action_dim=3, n_actions=10, action_cat_dim=10, context_dim=10, config=LearnEmbedConfig()):\n        super().__init__()\n        self.action_embed_stack = nn.Linear(n_actions, context_dim)\n        self.output = nn.Linear(context_dim, 1)\n        nn.init.ones_(self.output.weight)\n        self.output.requires_grad_(False)\n\n    def forward(self, context, action, action_embed):\n        x1 = self.action_embed_stack(action)\n        out = self.output(torch.mul(x1, context))\n        return out\n\n    @disable_training\n    def action_embeddings(self, action=None, action_embed=None):\n        return self.action_embed_stack(action)", "\n\n\nclass LearnEmbedLinearB(LearnEmbedNetwork):\n    \"\"\"Linear learning objective, where the action network uses the action embeddings\"\"\"\n\n    def __init__(self, action_dim=3, n_actions=10, action_cat_dim=10, context_dim=10, config=LearnEmbedConfig()):\n        super().__init__()\n        self.action_embed_stack = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(action_cat_dim * action_dim, context_dim)\n        )\n        self.output = nn.Linear(context_dim, 1)\n        nn.init.ones_(self.output.weight)\n        self.output.requires_grad_(False)\n\n    def forward(self, context, action, action_embed):\n        x1 = self.action_embed_stack(action_embed)\n        out = self.output(torch.mul(x1, context))\n        return out\n\n    @disable_training\n    def action_embeddings(self, action=None, action_embed=None):\n        return self.action_embed_stack(action_embed)", "\n\n\nclass LearnEmbedLinearC(LearnEmbedNetwork):\n    \"\"\"Linear learning objective, where the action network only uses both the action identity and a pre-defined embedding\"\"\"\n\n    def __init__(self, action_dim=3, n_actions=10, action_cat_dim=10, context_dim=10, config=LearnEmbedConfig()):\n        super().__init__()\n        self.action_embed_stack = nn.Linear(action_cat_dim * action_dim + n_actions, context_dim)\n        self.output = nn.Linear(context_dim, 1)\n        nn.init.ones_(self.output.weight)\n        self.output.requires_grad_(False)\n\n    def forward(self, context, action, action_embed):\n        x1 = self.action_embed_stack(torch.cat([nn.Flatten()(action_embed), action], dim=1))\n        out = self.output(torch.mul(x1, context))\n        return out\n\n    @disable_training\n    def action_embeddings(self, action=None, action_embed=None):\n        return self.action_embed_stack(torch.cat([nn.Flatten()(action_embed), action], dim=1))", "\n\n\nclass LearnEmbedMF(LearnEmbedNetwork):\n    \"\"\"Linear learning objective with low rank MF, where the action network only uses the action identity\"\"\"\n\n    def __init__(self, action_dim=3, n_actions=10, action_cat_dim=10, context_dim=10, config=LearnEmbedConfig()):\n        super().__init__()\n        self.action_embed_stack = nn.Linear(n_actions, config.learned_embed_dim)\n        self.context_stack = nn.Linear(context_dim, config.learned_embed_dim)\n        self.output = nn.Linear(config.learned_embed_dim, 1)\n        nn.init.ones_(self.output.weight)\n        self.output.requires_grad_(False)\n\n    def forward(self, context, action, action_embed):\n        x1 = self.action_embed_stack(action)\n        x2 = self.context_stack(context)\n        out = self.output(torch.mul(x1, x2))\n        return out\n\n    @disable_training\n    def action_embeddings(self, action=None, action_embed=None):\n        return self.action_embed_stack(action)", "\n\n\nclass TorchBanditDataset(Dataset):\n    def __init__(self, n_actions, n_dim_action, context, action, action_embed, reward):\n        self.context = torch.from_numpy(context).float()\n        self.action = torch.nn.functional.one_hot(torch.from_numpy(action), num_classes=n_actions).float()\n        self.action_embed = torch.nn.functional.one_hot(torch.from_numpy(action_embed), num_classes=n_dim_action).float()\n        self.reward = torch.unsqueeze(torch.from_numpy(reward), 1).float()\n\n    def __len__(self):\n        return len(self.reward)\n\n    def __getitem__(self, idx):\n        return self.context[idx], self.action[idx], self.action_embed[idx], self.reward[idx]", "\n\nclass LearnedEmbedMIPS(MarginalizedInverseProbabilityWeighting):\n    def __init__(\n        self,\n        embed_model: nn.Module = None,\n        learned_embed_params: LearnedEmbedParams = LearnedEmbedParams(),\n        embed_model_config: LearnEmbedConfig = LearnEmbedConfig(),\n        logging_losses_file=None,\n        **kwargs\n    ):\n        self.embed_model = embed_model\n        self.learned_embed_params = learned_embed_params\n        self.embed_model_config = embed_model_config\n        self.logging_losses_file = logging_losses_file\n        super().__init__(**kwargs)\n\n    def _estimate_round_rewards(\n        self,\n        context: np.ndarray,\n        reward: np.ndarray,\n        action: np.ndarray,\n        action_embed: np.ndarray,\n        pi_b: np.ndarray,\n        action_dist: np.ndarray,\n        position: Optional[np.ndarray] = None,\n        p_e_a: Optional[np.ndarray] = None,\n        with_dev: bool = False,\n        **kwargs,\n    ) -> np.ndarray:\n        \"\"\"Estimate round-wise (or sample-wise) rewards.\n\n        Parameters\n        ----------\n        reward: array-like, shape (n_rounds,)\n            Rewards observed for each data in logged bandit data, i.e., :math:`r_i`.\n\n        action: array-like, shape (n_rounds,)\n            Actions sampled by the logging/behavior policy for each data in logged bandit data, i.e., :math:`a_i`.\n\n        action_embed: array-like, shape (n_rounds, dim_action_embed)\n            Context vectors characterizing actions or action embeddings such as item category information.\n            This is used to estimate the marginal importance weights.\n\n        pi_b: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the logging/behavior policy, i.e., :math:`\\\\pi_b(a_i|x_i)`.\n\n        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n            Action choice probabilities of the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_i|x_i)`.\n\n        position: array-like, shape (n_rounds,), default=None\n            Indices to differentiate positions in a recommendation interface where the actions are presented.\n            If None, the effect of position on the reward will be ignored.\n            (If only a single action is chosen for each data, you can just ignore this argument.)\n\n        p_e_a: array-like, shape (n_actions, n_cat_per_dim, n_cat_dim), default=None\n            Conditional distribution of action embeddings given each action.\n            This distribution is available only when we use synthetic bandit data, i.e.,\n            `obp.dataset.SyntheticBanditDatasetWithActionEmbeds`.\n            See the output of the `obtain_batch_bandit_feedback` argument of this class.\n            If `p_e_a` is given, MIPW uses the true marginal importance weights based on this distribution.\n            The performance of MIPW with the true weights is useful in synthetic experiments of research papers.\n\n        with_dev: bool, default=False.\n            Whether to output a deviation bound with the estimated sample-wise rewards.\n\n        Returns\n        ----------\n        estimated_rewards: array-like, shape (n_rounds,)\n            Estimated rewards for each observation.\n\n        \"\"\"\n        n = reward.shape[0]\n        learned_embed = self.learn_action_embed(\n            context=context,\n            action=action,\n            action_embed=action_embed,\n            reward=reward,\n            **self.learned_embed_params.__dict__\n        )\n        w_x_e = self._estimate_w_x_e(\n            action=action,\n            action_embed=learned_embed,\n            pi_e=action_dist[np.arange(n), :, position],\n            pi_b=pi_b[np.arange(n), :, position],\n        )\n        self.max_w_x_e = w_x_e.max()\n\n        if with_dev:\n            r_hat = reward * w_x_e\n            cnf = np.sqrt(np.var(r_hat) / (n - 1))\n            cnf *= stats.t.ppf(1.0 - (self.delta / 2), n - 1)\n\n            return r_hat.mean(), cnf\n\n        return reward * w_x_e\n\n    def _estimate_w_x_e(\n        self,\n        action: np.ndarray,\n        action_embed: np.ndarray,\n        pi_b: np.ndarray,\n        pi_e: np.ndarray,\n    ) -> np.ndarray:\n        \"\"\"Estimate the marginal importance weights.\"\"\"\n        n = action.shape[0]\n        w_x_a = pi_e / pi_b\n        w_x_a = np.where(w_x_a < np.inf, w_x_a, 0)\n        pi_a_e = np.zeros((n, self.n_actions))\n        self.pi_a_x_e_estimator.fit(action_embed, action)\n        pi_a_e[:, np.unique(action)] = self.pi_a_x_e_estimator.predict_proba(action_embed)\n        w_x_e = (w_x_a * pi_a_e).sum(1)\n\n        return w_x_e\n\n    def learn_action_embed(\n        self,\n        context: np.ndarray,\n        action: np.ndarray,\n        action_embed: np.ndarray,\n        reward: np.ndarray,\n        lr=1e-2,\n        epochs=250,\n        batch_size=32,\n        train_test_ratio=0.9,\n    ) -> np.ndarray:\n        \"\"\"Learn action embeddings\"\"\"\n        n_dim_action = len(np.unique(action_embed))\n        dataset = TorchBanditDataset(self.n_actions, n_dim_action, context, action, action_embed, reward)\n        if train_test_ratio >= 1:\n            train_data = dataset\n            train_positives = dataset.reward.sum().item()\n            train_length = len(dataset)\n        else:\n            train_data, test_data = random_split(dataset, [round(train_test_ratio * len(dataset)), round((1 - train_test_ratio) * len(dataset))])\n            test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n            test_positives = dataset.reward[test_data.indices].sum().item()\n            test_length = len(dataset.reward[test_data.indices])\n            train_length = len(dataset.reward[train_data.indices])\n            train_positives = dataset.reward[train_data.indices].sum().item()\n\n        train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n        model = self.embed_model(n_actions=self.n_actions, action_dim=action_embed.shape[1], action_cat_dim=n_dim_action, context_dim=context.shape[1], config=self.embed_model_config)\n        loss_fn = nn.MSELoss()\n        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n        logs = pd.DataFrame()\n\n        for e in range(epochs):\n            model.train_loop(train_dataloader, loss_fn, optimizer)\n            if self.logging_losses_file and train_test_ratio < 1:\n                train_mse = model.report_loss(train_dataloader, loss_fn)\n                test_mse = model.report_loss(test_dataloader, loss_fn)\n                logs = logs.append(pd.Series(\n                    [e, train_mse, test_mse, self.estimator_name, asdict(self.learned_embed_params), asdict(self.embed_model_config), train_positives, train_length, test_positives, test_length],\n                    index=[\"epoch\", \"train_mse\", \"test_mse\", \"model\", \"learned_embed_params\", \"embed_model_config\", \"train_positives\", \"train_length\", \"test_positives\", \"test_length\"]\n                ), ignore_index=True\n                )\n\n        if self.logging_losses_file and train_test_ratio < 1:\n            logs.to_parquet(self.logging_losses_file, index=False)\n\n        return model.action_embeddings(dataset.action, dataset.action_embed).detach().numpy()", ""]}
{"filename": "jobs/utils/policy.py", "chunked_list": ["import numpy as np\n\n\ndef gen_eps_greedy(\n        expected_reward: np.ndarray,\n        is_optimal: bool = True,\n        eps: float = 0.0,\n) -> np.ndarray:\n    \"Generate an evaluation policy via the epsilon-greedy rule.\"\n    base_pol = np.zeros_like(expected_reward)\n    if is_optimal:\n        a = np.argmax(expected_reward, axis=1)\n    else:\n        a = np.argmin(expected_reward, axis=1)\n    base_pol[\n        np.arange(expected_reward.shape[0]),\n        a,\n    ] = 1\n    pol = (1.0 - eps) * base_pol\n    pol += eps / expected_reward.shape[1]\n\n    return pol[:, :, np.newaxis]", ""]}
{"filename": "jobs/utils/dataset.py", "chunked_list": ["from dataclasses import dataclass\nfrom typing import Optional\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom obp.dataset import OpenBanditDataset, SyntheticBanditDatasetWithActionEmbeds\nfrom obp.dataset.reward_type import RewardType\nfrom obp.types import BanditFeedback\nfrom obp.utils import sample_action_fast, softmax", "from obp.types import BanditFeedback\nfrom obp.utils import sample_action_fast, softmax\nfrom scipy.stats import rankdata\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import check_random_state, check_scalar\nfrom torch import nn\n\n\n@dataclass\nclass ModifiedOpenBanditDataset(OpenBanditDataset):\n    \"\"\"Flattening the list structure of OBD according to item-position click model.\n    As OBD has 80 unique actions and 3 different positions in its recommendation interface,\n    the resulting action space has the cardinality of 80 * 3 = 240.\"\"\"\n    @property\n    def n_actions(self) -> int:\n        \"\"\"Number of actions.\"\"\"\n        return int(self.action.max() + 1)\n\n    def __post_init__(self) -> None:\n        \"\"\"Initialize Open Bandit Dataset Class.\"\"\"\n        if self.behavior_policy not in [\n            \"bts\",\n            \"random\",\n        ]:\n            raise ValueError(\n                f\"`behavior_policy` must be either of 'bts' or 'random', but {self.behavior_policy} is given\"\n            )\n\n        if self.campaign not in [\n            \"all\",\n            \"men\",\n            \"women\",\n        ]:\n            raise ValueError(\n                f\"`campaign` must be one of 'all', 'men', or 'women', but {self.campaign} is given\"\n            )\n\n        self.data_path = f\"{self.data_path}/{self.behavior_policy}/{self.campaign}\"\n        self.raw_data_file = f\"{self.campaign}.csv\"\n\n        self.load_raw_data()\n        self.pre_process()\n\n    def load_raw_data(self) -> None:\n        \"\"\"Load raw open bandit dataset.\"\"\"\n        self.data = pd.read_csv(f\"{self.data_path}/{self.raw_data_file}\", index_col=0)\n        self.item_context = pd.read_csv(\n            f\"{self.data_path}/item_context.csv\", index_col=0\n        )\n        self.data.sort_values(\"timestamp\", inplace=True)\n        self.action = self.data[\"item_id\"].values\n        self.position = (rankdata(self.data[\"position\"].values, \"dense\") - 1).astype(\n            int\n        )\n        self.reward = self.data[\"click\"].values\n        self.pscore = self.data[\"propensity_score\"].values\n\n    def pre_process(self) -> None:\n        \"\"\"Preprocess raw open bandit dataset.\"\"\"\n        user_cols = self.data.columns.str.contains(\"user_feature\")\n        self.context = pd.get_dummies(\n            self.data.loc[:, user_cols], drop_first=True\n        ).values\n        pos = pd.DataFrame(self.position)\n        self.action_context = (\n            self.item_context.drop(columns=[\"item_id\", \"item_feature_0\"], axis=1)\n            .apply(LabelEncoder().fit_transform)\n            .values\n        )\n        self.action_context = self.action_context[self.action]\n        self.action_context = np.c_[self.action_context, pos]\n\n        self.action = self.position * self.n_actions + self.action\n        self.position = np.zeros_like(self.position)\n        self.pscore /= 3\n\n    def sample_bootstrap_bandit_feedback(\n        self,\n        sample_size: Optional[int] = None,\n        test_size: float = 0.3,\n        is_timeseries_split: bool = False,\n        random_state: Optional[int] = None,\n    ) -> BanditFeedback:\n\n        if is_timeseries_split:\n            bandit_feedback = self.obtain_batch_bandit_feedback(\n                test_size=test_size, is_timeseries_split=is_timeseries_split\n            )[0]\n        else:\n            bandit_feedback = self.obtain_batch_bandit_feedback(\n                test_size=test_size, is_timeseries_split=is_timeseries_split\n            )\n        n_rounds = bandit_feedback[\"n_rounds\"]\n        if sample_size is None:\n            sample_size = bandit_feedback[\"n_rounds\"]\n        else:\n            check_scalar(\n                sample_size,\n                name=\"sample_size\",\n                target_type=(int),\n                min_val=0,\n                max_val=n_rounds,\n            )\n        random_ = check_random_state(random_state)\n        bootstrap_idx = random_.choice(\n            np.arange(n_rounds), size=sample_size, replace=True\n        )\n        for key_ in [\n            \"action\",\n            \"position\",\n            \"reward\",\n            \"pscore\",\n            \"context\",\n            \"action_context\",\n        ]:\n            bandit_feedback[key_] = bandit_feedback[key_][bootstrap_idx]\n        bandit_feedback[\"n_rounds\"] = sample_size\n        return bandit_feedback", "@dataclass\nclass ModifiedOpenBanditDataset(OpenBanditDataset):\n    \"\"\"Flattening the list structure of OBD according to item-position click model.\n    As OBD has 80 unique actions and 3 different positions in its recommendation interface,\n    the resulting action space has the cardinality of 80 * 3 = 240.\"\"\"\n    @property\n    def n_actions(self) -> int:\n        \"\"\"Number of actions.\"\"\"\n        return int(self.action.max() + 1)\n\n    def __post_init__(self) -> None:\n        \"\"\"Initialize Open Bandit Dataset Class.\"\"\"\n        if self.behavior_policy not in [\n            \"bts\",\n            \"random\",\n        ]:\n            raise ValueError(\n                f\"`behavior_policy` must be either of 'bts' or 'random', but {self.behavior_policy} is given\"\n            )\n\n        if self.campaign not in [\n            \"all\",\n            \"men\",\n            \"women\",\n        ]:\n            raise ValueError(\n                f\"`campaign` must be one of 'all', 'men', or 'women', but {self.campaign} is given\"\n            )\n\n        self.data_path = f\"{self.data_path}/{self.behavior_policy}/{self.campaign}\"\n        self.raw_data_file = f\"{self.campaign}.csv\"\n\n        self.load_raw_data()\n        self.pre_process()\n\n    def load_raw_data(self) -> None:\n        \"\"\"Load raw open bandit dataset.\"\"\"\n        self.data = pd.read_csv(f\"{self.data_path}/{self.raw_data_file}\", index_col=0)\n        self.item_context = pd.read_csv(\n            f\"{self.data_path}/item_context.csv\", index_col=0\n        )\n        self.data.sort_values(\"timestamp\", inplace=True)\n        self.action = self.data[\"item_id\"].values\n        self.position = (rankdata(self.data[\"position\"].values, \"dense\") - 1).astype(\n            int\n        )\n        self.reward = self.data[\"click\"].values\n        self.pscore = self.data[\"propensity_score\"].values\n\n    def pre_process(self) -> None:\n        \"\"\"Preprocess raw open bandit dataset.\"\"\"\n        user_cols = self.data.columns.str.contains(\"user_feature\")\n        self.context = pd.get_dummies(\n            self.data.loc[:, user_cols], drop_first=True\n        ).values\n        pos = pd.DataFrame(self.position)\n        self.action_context = (\n            self.item_context.drop(columns=[\"item_id\", \"item_feature_0\"], axis=1)\n            .apply(LabelEncoder().fit_transform)\n            .values\n        )\n        self.action_context = self.action_context[self.action]\n        self.action_context = np.c_[self.action_context, pos]\n\n        self.action = self.position * self.n_actions + self.action\n        self.position = np.zeros_like(self.position)\n        self.pscore /= 3\n\n    def sample_bootstrap_bandit_feedback(\n        self,\n        sample_size: Optional[int] = None,\n        test_size: float = 0.3,\n        is_timeseries_split: bool = False,\n        random_state: Optional[int] = None,\n    ) -> BanditFeedback:\n\n        if is_timeseries_split:\n            bandit_feedback = self.obtain_batch_bandit_feedback(\n                test_size=test_size, is_timeseries_split=is_timeseries_split\n            )[0]\n        else:\n            bandit_feedback = self.obtain_batch_bandit_feedback(\n                test_size=test_size, is_timeseries_split=is_timeseries_split\n            )\n        n_rounds = bandit_feedback[\"n_rounds\"]\n        if sample_size is None:\n            sample_size = bandit_feedback[\"n_rounds\"]\n        else:\n            check_scalar(\n                sample_size,\n                name=\"sample_size\",\n                target_type=(int),\n                min_val=0,\n                max_val=n_rounds,\n            )\n        random_ = check_random_state(random_state)\n        bootstrap_idx = random_.choice(\n            np.arange(n_rounds), size=sample_size, replace=True\n        )\n        for key_ in [\n            \"action\",\n            \"position\",\n            \"reward\",\n            \"pscore\",\n            \"context\",\n            \"action_context\",\n        ]:\n            bandit_feedback[key_] = bandit_feedback[key_][bootstrap_idx]\n        bandit_feedback[\"n_rounds\"] = sample_size\n        return bandit_feedback", "\n\nclass DataGeneratingNetwork(nn.Module):\n    def __init__(self, n_features=3, feature_dim=5, context_dim=10, hidden_layer_size=50):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_features * feature_dim + context_dim, hidden_layer_size),\n            nn.ReLU(),\n            nn.Linear(hidden_layer_size, hidden_layer_size),\n            nn.ReLU(),\n            nn.Linear(hidden_layer_size, 1)\n        )\n\n    def forward(self, context, action_embed):\n        x = torch.cat([context, nn.Flatten()(action_embed)], dim=1)\n        return self.layers(x)\n\n    def generate_reward(self, context, action_embed, batch_size=32):\n        rewards = np.zeros(context.shape[0])\n        for i in range(0, context.shape[0], batch_size):\n            context_batch = torch.from_numpy(context[i:i + batch_size]).float()\n            action_embed_batch = torch.from_numpy(np.tile(action_embed, (context_batch.shape[0], 1, 1))).float()\n            rewards_batch = self.forward(context_batch, action_embed_batch)\n            rewards[i:i + batch_size] = rewards_batch.detach().numpy().flatten()\n        return rewards", "\n\n@ dataclass\nclass NonLinearSyntheticBanditDatasetWithActionEmbeds(SyntheticBanditDatasetWithActionEmbeds):\n    def __post_init__(self) -> None:\n        self.random_ = check_random_state(self.random_state)\n        self._define_action_embed()\n        self.model = DataGeneratingNetwork(n_features=self.n_cat_dim, feature_dim=self.latent_param_mat_dim, context_dim=self.dim_context)\n\n    def obtain_batch_bandit_feedback(self, n_rounds: int) -> BanditFeedback:\n        \"\"\"Obtain batch logged bandit data.\n\n        Parameters\n        ----------\n        n_rounds: int\n            Data size of the synthetic logged bandit data.\n\n        Returns\n        ---------\n        bandit_feedback: BanditFeedback\n            Synthesized logged bandit data with action category information.\n\n        \"\"\"\n        check_scalar(n_rounds, \"n_rounds\", int, min_val=1)\n        contexts = self.random_.normal(size=(n_rounds, self.dim_context))\n\n        # calc expected rewards given context and action (n_data, n_actions)\n        q_x_e = np.zeros((n_rounds, *([self.n_cat_per_dim] * self.n_cat_dim)))\n        q_x_a = np.zeros((n_rounds, self.n_actions))\n\n        for index in np.ndindex(q_x_e.shape[1:]):\n            embed_param = np.zeros(self.latent_cat_param.shape[::2])\n            p_e_a = np.ones(self.n_actions)\n            for feature, category in list(zip(range(self.n_cat_dim), index)):\n                embed_param[feature] = self.latent_cat_param[feature, category]\n                p_e_a *= self.p_e_a[(Ellipsis, category, feature)]\n\n            q_x_e[(Ellipsis, *index)] = self.model.generate_reward(\n                context=contexts,\n                action_embed=embed_param,\n            )            \n            # q_x_e[(Ellipsis, *index)] = np.random.randn(n_rounds)\n            q_x_a += np.outer(q_x_e[(Ellipsis, *index)], p_e_a)\n\n        # sample actions for each round based on the behavior policy\n        if self.behavior_policy_function is None:\n            pi_b_logits = q_x_a\n        else:\n            pi_b_logits = self.behavior_policy_function(\n                context=contexts,\n                action_context=self.action_context,\n                random_state=self.random_state,\n            )\n        if self.n_deficient_actions > 0:\n            pi_b = np.zeros_like(q_x_a)\n            n_supported_actions = self.n_actions - self.n_deficient_actions\n            supported_actions = np.argsort(\n                self.random_.gumbel(size=(n_rounds, self.n_actions)), axis=1\n            )[:, ::-1][:, :n_supported_actions]\n            supported_actions_idx = (\n                np.tile(np.arange(n_rounds), (n_supported_actions, 1)).T,\n                supported_actions,\n            )\n            pi_b[supported_actions_idx] = softmax(\n                self.beta * pi_b_logits[supported_actions_idx]\n            )\n        else:\n            pi_b = softmax(self.beta * pi_b_logits)\n        actions = sample_action_fast(pi_b, random_state=self.random_state)\n\n        # sample action embeddings based on sampled actions\n        action_embed = np.zeros((n_rounds, self.n_cat_dim), dtype=int)\n        for d in np.arange(self.n_cat_dim):\n            action_embed[:, d] = sample_action_fast(\n                self.p_e_a[actions, :, d],\n                random_state=d,\n            )\n\n        # sample rewards given the context and action embeddings\n        expected_rewards_factual = q_x_e[(np.arange(n_rounds), *action_embed.T)]\n        if RewardType(self.reward_type) == RewardType.BINARY:\n            rewards = self.random_.binomial(n=1, p=expected_rewards_factual)\n        elif RewardType(self.reward_type) == RewardType.CONTINUOUS:\n            rewards = self.random_.normal(\n                loc=expected_rewards_factual, scale=self.reward_std, size=n_rounds\n            )\n\n        return dict(\n            n_rounds=n_rounds,\n            n_actions=self.n_actions,\n            action_context=self.action_context_reg[\n                :, self.n_unobserved_cat_dim :\n            ].copy(),  # action context used for training a reg model\n            action_embed=action_embed[\n                :, self.n_unobserved_cat_dim :\n            ].copy(),  # action embeddings used for OPE with MIPW\n            context=contexts,\n            action=actions,\n            position=None,  # position effect is not considered in synthetic data\n            reward=rewards,\n            expected_reward=q_x_a,\n            q_x_e=q_x_e[:, :, self.n_unobserved_cat_dim :].copy(),\n            p_e_a=self.p_e_a[\n                :, :, self.n_unobserved_cat_dim :\n            ].copy(),  # true probability distribution of the action embeddings\n            pi_b=pi_b[:, :, np.newaxis].copy(),\n            pscore=pi_b[np.arange(n_rounds), actions].copy(),\n        )", ""]}
{"filename": "jobs/utils/__init__.py", "chunked_list": [""]}
{"filename": "jobs/utils/ope.py", "chunked_list": ["from typing import Dict, Optional\n\nimport numpy as np\nfrom obp.ope import DirectMethod as DM\nfrom obp.ope import DoublyRobust as DR\nfrom obp.ope import InverseProbabilityWeighting as IPS\nfrom obp.ope import MarginalizedInverseProbabilityWeighting as MIPS\nfrom obp.ope import OffPolicyEvaluation\nfrom obp.ope import SelfNormalizedInverseProbabilityWeighting as SNIPS\nfrom obp.ope import SwitchDoublyRobustTuning as SwitchDR", "from obp.ope import SelfNormalizedInverseProbabilityWeighting as SNIPS\nfrom obp.ope import SwitchDoublyRobustTuning as SwitchDR\n\nfrom .learn_embed import LearnedEmbedMIPS, LearnEmbedLinear, LearnEmbedLinearB, LearnEmbedLinearC\n\n\ndef run_ope(\n    val_bandit_data: Dict,\n    action_dist_val: np.ndarray,\n    estimated_rewards: Optional[np.ndarray] = None,\n    **kwargs\n) -> Dict[str, float]:\n    n_actions = val_bandit_data[\"n_actions\"]\n\n    ope_estimators = [\n        IPS(estimator_name=\"IPS\"),\n        DR(estimator_name=\"DR\"),\n        DM(estimator_name=\"DM\"),\n        MIPS(n_actions=n_actions, estimator_name=\"MIPS\"),\n        MIPS(n_actions=n_actions, estimator_name=\"MIPS (true)\"),\n        LearnedEmbedMIPS(estimator_name=\"Learned MIPS OneHot\", n_actions=n_actions, embed_model=LearnEmbedLinear, **kwargs),\n        LearnedEmbedMIPS(estimator_name=\"Learned MIPS FineTune\", n_actions=n_actions, embed_model=LearnEmbedLinearB, **kwargs),\n        LearnedEmbedMIPS(estimator_name=\"Learned MIPS Combined\", n_actions=n_actions, embed_model=LearnEmbedLinearC, **kwargs),\n    ]\n\n    ope = OffPolicyEvaluation(\n        bandit_feedback=val_bandit_data,\n        ope_estimators=ope_estimators,\n    )\n    estimated_policy_values = ope.estimate_policy_values(\n        action_dist=action_dist_val,\n        estimated_rewards_by_reg_model=estimated_rewards,\n        action_embed=val_bandit_data[\"action_embed\"],\n        pi_b=val_bandit_data[\"pi_b\"],\n        p_e_a={\"MIPS (true)\": val_bandit_data[\"p_e_a\"]},\n    )\n\n    return estimated_policy_values", "\n\ndef run_real_dataset_ope(\n    val_bandit_data: Dict,\n    action_dist_val: np.ndarray,\n    estimated_rewards: np.ndarray,\n    policy_value: float,\n    **kwargs\n) -> np.ndarray:\n    n_actions = val_bandit_data[\"n_actions\"]\n    lambdas = [10, 50, 100, 500, 1e3, 5e3, 1e4, 5e4, 1e5, 5e5, np.inf]\n\n    ope = OffPolicyEvaluation(\n        bandit_feedback=val_bandit_data,\n        ope_estimators=[\n            IPS(estimator_name=\"IPS\"),\n            DR(estimator_name=\"DR\"),\n            DM(estimator_name=\"DM\"),\n            SNIPS(estimator_name=\"SNIPS\"),\n            SwitchDR(lambdas=lambdas, tuning_method=\"slope\", estimator_name=\"SwitchDR\"),\n            MIPS(estimator_name=\"MIPS\", n_actions=n_actions),\n            MIPS(estimator_name=\"MIPS (w/ SLOPE)\", n_actions=n_actions, embedding_selection_method=\"exact\"),\n            LearnedEmbedMIPS(estimator_name=\"Learned MIPS OneHot\", n_actions=n_actions, embed_model=LearnEmbedLinear, **kwargs),\n            LearnedEmbedMIPS(estimator_name=\"Learned MIPS FineTune\", n_actions=n_actions, embed_model=LearnEmbedLinearB, **kwargs),\n            LearnedEmbedMIPS(estimator_name=\"Learned MIPS Combined\", n_actions=n_actions, embed_model=LearnEmbedLinearC, **kwargs),\n        ],\n    )\n\n    squared_errors = ope.evaluate_performance_of_estimators(\n        ground_truth_policy_value=policy_value,\n        action_dist=action_dist_val,\n        estimated_rewards_by_reg_model=estimated_rewards,\n        action_embed=val_bandit_data[\"action_context\"],\n        pi_b=val_bandit_data[\"pi_b\"],\n        metric=\"se\",\n    )\n\n    relative_squared_errors = {}\n    baseline = squared_errors[\"MIPS (w/ SLOPE)\"]\n    for key, value in squared_errors.items():\n        relative_squared_errors[key] = value / baseline\n\n    return squared_errors, relative_squared_errors", ""]}
{"filename": "jobs/abstracts/abstract_job.py", "chunked_list": ["import abc\nimport argparse\nimport dataclasses\nfrom abc import ABC, ABCMeta\nfrom logging import getLogger\nfrom types import SimpleNamespace\n\nfrom experiments.utils.configs import LearnedEmbedParams, LearnEmbedConfig, SyntheticOpeTrialConfig\n\n", "\n\nlogger = getLogger(__name__)\n\nclass HandleOpeArgs(ABCMeta):\n    def __init__(cls, name, bases, clsdict):\n        if 'main' in clsdict:\n            def ope_args_main(self, cfg: SyntheticOpeTrialConfig):\n                parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS)\n\n                parser.add_argument(\"--batch-size\", type=int)\n                parser.add_argument(\"--epochs\", type=int)\n                parser.add_argument(\"--lr\", type=float)\n\n                # See LearnEmbedConfig for the description of the parameters\n                parser.add_argument(\"--learned-embed-dim\", type=int)\n\n                args, unknown = parser.parse_known_args()\n                logger.info(f\"Uknown args: {unknown}\")\n\n                if 'embed_model_config' not in cfg.__dict__:\n                    cfg.embed_model_config = LearnEmbedConfig()\n                else:\n                    cfg.embed_model_config = LearnEmbedConfig(**{\n                        **cfg.embed_model_config.__dict__,\n                        **dict([(key, value) for key, value in vars(args).items() if key in vars(LearnEmbedConfig()).keys()])\n                    })\n                if 'learned_embed_params' not in cfg.__dict__:\n                    cfg.learned_embed_params = LearnedEmbedParams()\n                else:\n                    cfg.learned_embed_params = LearnedEmbedParams(**{\n                        **cfg.learned_embed_params.__dict__,\n                        **dict([(key, value) for key, value in vars(args).items() if key in vars(LearnedEmbedParams()).keys()])\n                    })\n                cfg = SimpleNamespace(**{**SyntheticOpeTrialConfig(name=None).__dict__, **cfg.__dict__})\n                clsdict['main'](self, cfg)\n            setattr(cls, 'main', ope_args_main)", "\n\n@dataclasses.dataclass\nclass AbstractJob(ABC, metaclass=HandleOpeArgs):\n\n    @abc.abstractmethod\n    def main(self, cfg):\n        \"\"\"\n        This the main compute function for the training job.\n        @param cfg: config object that contains all parameters passed to the job from the experiment\n        \"\"\"\n        pass", ""]}
{"filename": "jobs/abstracts/__init__.py", "chunked_list": [""]}
{"filename": "jobs/abstracts/abstract_ope_job.py", "chunked_list": ["import abc\nimport dataclasses\nfrom logging import getLogger\n\nfrom experiments.utils.configs import OpeTrialConfig\n\nfrom .abstract_job import AbstractJob\n\n\n@dataclasses.dataclass\nclass AbstractOpeJob(AbstractJob):\n    @abc.abstractmethod\n    def main(self, cfg: OpeTrialConfig):\n        \"\"\"\n        This the main compute function for the training job.\n        @param cfg: config object that contains all parameters passed to the job from the experiment\n        \"\"\"\n        pass", "\n@dataclasses.dataclass\nclass AbstractOpeJob(AbstractJob):\n    @abc.abstractmethod\n    def main(self, cfg: OpeTrialConfig):\n        \"\"\"\n        This the main compute function for the training job.\n        @param cfg: config object that contains all parameters passed to the job from the experiment\n        \"\"\"\n        pass", ""]}
{"filename": "jobs/abstracts/abstract_synthetic_job.py", "chunked_list": ["import abc\nimport warnings\nfrom logging import getLogger\nfrom pathlib import Path\nfrom time import time\nfrom types import SimpleNamespace\n\nimport numpy as np\nimport pandas as pd\nimport torch", "import pandas as pd\nimport torch\nfrom obp.dataset import SyntheticBanditDatasetWithActionEmbeds, linear_reward_function\nfrom pandas import DataFrame\nfrom sklearn.exceptions import ConvergenceWarning\nfrom torch.utils.data import DataLoader\n\nfrom experiments.utils.configs import SyntheticOpeTrialConfig\n\nfrom ..utils.ope import run_ope", "\nfrom ..utils.ope import run_ope\nfrom ..utils.learn_embed import LearnEmbedLinear, TorchBanditDataset\nfrom ..utils.policy import gen_eps_greedy\nfrom ..utils.dataset import NonLinearSyntheticBanditDatasetWithActionEmbeds\nfrom .abstract_ope_job import AbstractOpeJob\n\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)", "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nlogger = getLogger(__name__)\n\n\nclass AbstractSyntheticJob(AbstractOpeJob):\n    @abc.abstractmethod\n    def main(self, cfg: SyntheticOpeTrialConfig):\n        pass\n\n    def run(self, cfg, hyperparam_name, hyperparam_list):\n        logger.info(f\"The current working directory is {Path().cwd()}\")\n        if not cfg.s3_path.startswith(\"s3://\"):\n            Path(cfg.s3_path).mkdir(parents=True, exist_ok=True)\n        start_time = time()\n\n        # log path\n        random_state = cfg.random_state\n\n        elapsed_prev = 0.0\n        result_df_list = []\n        for hyperparam in hyperparam_list:\n            setattr(cfg, hyperparam_name, hyperparam)\n\n            estimated_policy_value_list = []\n            # define a dataset class\n            dataset = SyntheticBanditDatasetWithActionEmbeds(\n            # dataset = NonLinearSyntheticBanditDatasetWithActionEmbeds(\n                n_actions=cfg.n_actions,\n                dim_context=cfg.dim_context,\n                beta=cfg.beta,\n                reward_type=\"continuous\",\n                n_cat_per_dim=cfg.n_cat_per_dim,\n                latent_param_mat_dim=cfg.latent_param_mat_dim,\n                n_cat_dim=cfg.n_cat_dim,\n                n_unobserved_cat_dim=cfg.n_unobserved_cat_dim,\n                n_deficient_actions=int(cfg.n_actions * cfg.n_def_actions),\n                reward_function=linear_reward_function,\n                reward_std=cfg.reward_std,\n                random_state=random_state,\n            )\n            # test bandit data is used to approximate the ground-truth policy value\n            test_bandit_data = dataset.obtain_batch_bandit_feedback(\n                n_rounds=cfg.n_test_data\n            )\n            action_dist_test = gen_eps_greedy(\n                expected_reward=test_bandit_data[\"expected_reward\"],\n                is_optimal=cfg.is_optimal,\n                eps=cfg.eps,\n            )\n            policy_value = dataset.calc_ground_truth_policy_value(\n                expected_reward=test_bandit_data[\"expected_reward\"],\n                action_dist=action_dist_test,\n            )\n\n            for t in range(cfg.n_seeds):\n                # generate validation data\n                val_bandit_data = dataset.obtain_batch_bandit_feedback(\n                    n_rounds=cfg.n_val_data,\n                )\n\n                # make decisions on validation data\n                action_dist_val = gen_eps_greedy(\n                    expected_reward=val_bandit_data[\"expected_reward\"],\n                    is_optimal=cfg.is_optimal,\n                    eps=cfg.eps,\n                )\n\n                # Direct Method\n                model = LearnEmbedLinear(\n                    action_dim=val_bandit_data['action_context'].shape[1],\n                    action_cat_dim=len(np.unique(val_bandit_data['action_embed'])),\n                    n_actions=val_bandit_data['n_actions'],\n                    context_dim=val_bandit_data['context'].shape[1],\n                    config=cfg.embed_model_config\n                )\n\n                model_dataset = TorchBanditDataset(\n                    n_actions=val_bandit_data['n_actions'],\n                    n_dim_action=len(np.unique(val_bandit_data['action_embed'])),\n                    context=val_bandit_data['context'],\n                    action=val_bandit_data['action'],\n                    action_embed=val_bandit_data['action_embed'],\n                    reward=val_bandit_data['reward']\n                )\n                train_dataloader = DataLoader(model_dataset, batch_size=32, shuffle=True)\n\n                loss_fn = torch.nn.MSELoss()\n                optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n                for _ in range(cfg.learned_embed_params.epochs):\n                    model.train_loop(train_dataloader, loss_fn, optimizer)\n\n                estimated_rewards = np.zeros((val_bandit_data['n_rounds'], val_bandit_data['n_actions'], 1))\n                for i in range(val_bandit_data['n_actions']):\n                    _, action, action_embed, _ = model_dataset.__getitem__(i)\n                    estimated_rewards[:, i, :] = model(\n                        torch.from_numpy(val_bandit_data['context']).float(),\n                        action.repeat(val_bandit_data['n_rounds'], 1),\n                        action_embed.repeat(val_bandit_data['n_rounds'], 1, 1)\n                    ).detach().numpy() \n\n                estimated_rewards_dict = {\n                    \"DM\": estimated_rewards,\n                    \"DR\": estimated_rewards,\n                }\n\n                estimated_policy_values = run_ope(\n                    val_bandit_data=val_bandit_data,\n                    action_dist_val=action_dist_val,\n                    estimated_rewards=estimated_rewards_dict,\n                    embed_model_config=cfg.embed_model_config,\n                    learned_embed_params=cfg.learned_embed_params,\n                    logging_losses_file=f\"{cfg.s3_path}/model_losses/{time()}.parquet\"\n                )\n                estimated_policy_value_list.append(estimated_policy_values)\n                elapsed = np.round((time() - start_time) / 60, 2)\n                diff = np.round(elapsed - elapsed_prev, 2)\n                logger.info(f\"t={t}: {elapsed}min (diff {diff}min)\")\n                elapsed_prev = elapsed\n\n                # summarize results\n                result_df = (\n                    DataFrame(DataFrame(estimated_policy_value_list).stack())\n                    .reset_index(1)\n                    .rename(columns={\"level_1\": \"est\", 0: \"value\"})\n                )\n                result_df[hyperparam_name] = hyperparam\n                result_df[\"se\"] = (result_df.value - policy_value) ** 2\n                result_df[\"bias\"] = 0\n                result_df[\"variance\"] = 0\n                sample_mean = DataFrame(result_df.groupby([\"est\"]).mean().value).reset_index()\n                for est_ in sample_mean[\"est\"]:\n                    estimates = result_df.loc[result_df[\"est\"] == est_, \"value\"].values\n                    mean_estimates = sample_mean.loc[sample_mean[\"est\"] == est_, \"value\"].values\n                    mean_estimates = np.ones_like(estimates) * mean_estimates\n                    result_df.loc[result_df[\"est\"] == est_, \"bias\"] = (\n                        policy_value - mean_estimates\n                    ) ** 2\n                    result_df.loc[result_df[\"est\"] == est_, \"variance\"] = (\n                        estimates - mean_estimates\n                    ) ** 2\n                result_df_list.append(result_df)\n\n                # aggregate all results\n                result_df = pd.concat(result_df_list).reset_index(level=0)\n                result_df.to_csv(f\"{cfg.s3_path}/result_df.csv\")\n        return result_df", ""]}
{"filename": "experiments/n_val_data_experiment.py", "chunked_list": ["from logging import getLogger\nfrom pathlib import Path\nfrom typing import List\n\nimport pandas as pd\n\nfrom .abstracts.abstract_experiments import AbstractExperiment\nfrom .utils.configs import SyntheticOpeTrialConfig\nfrom .utils.plots import plot_line\n", "from .utils.plots import plot_line\n\n\nlogger = getLogger(__name__)\n\n\nclass NValDataExperiment(AbstractExperiment):\n    @property\n    def job_class_name(self) -> str:\n        return \"NValDataJob\"\n\n    @property\n    def trial_configs(self) -> List[SyntheticOpeTrialConfig]:\n        return [\n            SyntheticOpeTrialConfig(\n                name=\"800\",\n                n_val_data_list=[800]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"1600\",\n                n_val_data_list=[1600]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"3200\",\n                n_val_data_list=[3200]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"6400\",\n                n_val_data_list=[6400]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"12800\",\n                n_val_data_list=[12800]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"25600\",\n                n_val_data_list=[25600]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"51200\",\n                n_val_data_list=[51200]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"102400\",\n                n_val_data_list=[102400]\n            ),\n        ]\n\n    @property\n    def instance_type(self) -> str:\n        return \"ml.c5.18xlarge\"\n\n    def get_output(self, experiment_name: str, local_path: str = None):\n        result = pd.DataFrame()\n        exclude = [\n            # 'MIPS (true)',\n            # \"Learned MIPS OneHot\", \n            # \"Learned MIPS FineTune\",\n            # \"Learned MIPS Combined\",\n        ]\n        if local_path:\n            result = pd.read_csv(f\"{local_path}/result_df.csv\", index_col=0)\n            output_path = Path(local_path)\n        else:\n            output_path = Path(self.get_output_path(experiment_name))\n            for trial in self.trial_configs:\n                s3_path = self.get_s3_path(experiment_name, trial.name)\n                try:\n                    result = result.append(pd.read_csv(f\"{s3_path}/result_df.csv\", index_col=0), ignore_index=True)\n                except:\n                    logger.error(f\"No result found for {trial.name}\")\n        plot_line(\n            result_df=result,\n            fig_path=output_path,\n            x=\"n_val_data\",\n            xlabel=\"Number of training samples\",\n            xticklabels=result.n_val_data.unique(),\n            exclude=exclude\n        )\n        if not local_path:\n            result.to_csv(f\"{self.get_output_path(experiment_name)}/result_df.csv\")", ""]}
{"filename": "experiments/real_dataset_experiment.py", "chunked_list": ["from typing import List\nfrom pathlib import Path\n\nimport pandas as pd\n\n\nfrom .utils.configs import RealOpeTrialConfig\nfrom .abstracts.abstract_experiments import AbstractExperiment\nfrom .utils.plots import plot_cdf\n", "from .utils.plots import plot_cdf\n\n\nclass RealDatasetExperiment(AbstractExperiment):\n    @property\n    def job_class_name(self) -> str:\n        return \"RealDatasetJob\"\n\n    @property\n    def trial_configs(self) -> List[RealOpeTrialConfig]:\n        return [\n            RealOpeTrialConfig(\n                name=\"1000\",\n                sample_size=1000,\n            ),\n            RealOpeTrialConfig(\n                name=\"10000\",\n                sample_size=10000,\n            ),\n            RealOpeTrialConfig(\n                name=\"50000\",\n                sample_size=50000,\n            ),\n            RealOpeTrialConfig(\n                name=\"100000\",\n                sample_size=100000,\n            )\n        ]\n\n    @property\n    def instance_type(self) -> str:\n        return \"ml.c5.18xlarge\"\n\n    def get_output(self, experiment_name: str, local_path: str = None):\n        exclude = [\n            # \"Learned MIPS OneHot\", \n            # \"Learned MIPS FineTune\",\n            # \"Learned MIPS Combined\",\n            # \"SNIPS\",\n            # \"SwitchDR\",\n        ]\n        for trial in self.trial_configs:\n            s3_path = self.get_s3_path(experiment_name, trial.name) if not local_path else f\"{local_path}/{trial.name}\"\n            fig_dir = self.get_output_path(experiment_name, trial.name) if not local_path else f\"{local_path}/{trial.name}\"\n            Path(fig_dir).mkdir(parents=True, exist_ok=True)\n            result_df = pd.read_csv(f\"{s3_path}/result_df.csv\")\n            result_df.to_csv(f\"{fig_dir}/result_df.csv\")\n            plot_cdf(\n                result_df=result_df,\n                fig_path=f\"{fig_dir}/cdf_IPS.png\",\n                relative_to=\"IPS\",\n                exclude=exclude\n            )\n            plot_cdf(\n                result_df=result_df,\n                fig_path=f\"{fig_dir}/cdf_MIPS.png\",\n                exclude=exclude\n            )\n            plot_cdf(\n                result_df=result_df,\n                fig_path=f\"{fig_dir}/cdf_onehot.png\",\n                relative_to=\"Learned MIPS OneHot\",\n                exclude=exclude,\n            )\n            plot_cdf(\n                result_df=result_df,\n                fig_path=f\"{fig_dir}/cdf_IPS.pdf\",\n                relative_to=\"IPS\",\n                exclude=exclude,\n                remove_legend=True\n            )", ""]}
{"filename": "experiments/__init__.py", "chunked_list": [""]}
{"filename": "experiments/n_actions_experiment.py", "chunked_list": ["from logging import getLogger\nfrom pathlib import Path\nfrom typing import List\n\nimport pandas as pd\n\nfrom .abstracts.abstract_experiments import AbstractExperiment\nfrom .utils.configs import SyntheticOpeTrialConfig\nfrom .utils.plots import plot_line\n", "from .utils.plots import plot_line\n\n\nlogger = getLogger(__name__)\n\n\nclass NActionsExperiment(AbstractExperiment):\n    @property\n    def job_class_name(self) -> str:\n        return \"NActionsJob\"\n\n    @property\n    def instance_type(self) -> str:\n        return \"ml.c5.18xlarge\"\n\n    @property\n    def trial_configs(self) -> List[SyntheticOpeTrialConfig]:\n        return [\n            SyntheticOpeTrialConfig(\n                name=\"10\",\n                n_actions_list=[10]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"20\",\n                n_actions_list=[20]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"50\",\n                n_actions_list=[50]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"100\",\n                n_actions_list=[100]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"200\",\n                n_actions_list=[200]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"500\",\n                n_actions_list=[500]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"1000\",\n                n_actions_list=[1000]\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"2000\",\n                n_actions_list=[2000]\n            ),\n        ]\n\n    def get_output(self, experiment_name: str, local_path: str = None):\n        result = pd.DataFrame()\n        exclude = [\n            # 'MIPS (true)',\n            # \"Learned MIPS OneHot\", \n            # \"Learned MIPS FineTune\",\n            # \"Learned MIPS Combined\",\n        ]\n        if local_path:\n            result = pd.read_csv(f\"{local_path}/result_df.csv\", index_col=0)\n            output_path = Path(local_path)\n        else:\n            output_path = Path(self.get_output_path(experiment_name))\n            for trial in self.trial_configs:\n                s3_path = self.get_s3_path(experiment_name, trial.name)\n                try:\n                    result = result.append(pd.read_csv(f\"{s3_path}/result_df.csv\", index_col=0), ignore_index=True)\n                except:\n                    logger.error(f\"No result found for {trial.name}\")\n        plot_line(\n            result_df = result,\n            fig_path = output_path,\n            x = \"n_actions\",\n            xlabel = \"Number of actions\",\n            xticklabels = result.n_actions.unique(),\n            exclude = exclude\n        )\n        if not local_path:\n            result.to_csv(f\"{self.get_output_path(experiment_name)}/result_df.csv\")", ""]}
{"filename": "experiments/n_unobs_cat_dim_experiment.py", "chunked_list": ["from logging import getLogger\nfrom pathlib import Path\nfrom typing import List\n\nimport pandas as pd\n\nfrom .abstracts.abstract_experiments import AbstractExperiment\nfrom .utils.configs import SyntheticOpeTrialConfig\nfrom .utils.plots import plot_line\n", "from .utils.plots import plot_line\n\n\nlogger = getLogger(__name__)\n\n\nclass NUnobsCatDimExperiment(AbstractExperiment):\n    @property\n    def job_class_name(self) -> str:\n        return \"NUnobsCatDimJob\"\n\n    @property\n    def trial_configs(self) -> List[SyntheticOpeTrialConfig]:\n        return [\n            SyntheticOpeTrialConfig(\n                name=\"0\",\n                n_unobserved_cat_dim_list=[0],\n                n_cat_dim=20,\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"2\",\n                n_unobserved_cat_dim_list=[2],\n                n_cat_dim=20,\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"4\",\n                n_unobserved_cat_dim_list=[4],\n                n_cat_dim=20,\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"6\",\n                n_unobserved_cat_dim_list=[6],\n                n_cat_dim=20,\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"8\",\n                n_unobserved_cat_dim_list=[8],\n                n_cat_dim=20,\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"10\",\n                n_unobserved_cat_dim_list=[10],\n                n_cat_dim=20,\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"12\",\n                n_unobserved_cat_dim_list=[12],\n                n_cat_dim=20,\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"14\",\n                n_unobserved_cat_dim_list=[14],\n                n_cat_dim=20,\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"16\",\n                n_unobserved_cat_dim_list=[16],\n                n_cat_dim=20,\n            ),\n            SyntheticOpeTrialConfig(\n                name=\"18\",\n                n_unobserved_cat_dim_list=[18],\n                n_cat_dim=20,\n            ),\n        ]\n\n    @property\n    def instance_type(self) -> str:\n        return \"ml.c5.18xlarge\"\n\n    def get_output(self, experiment_name: str, local_path: str = None):\n        exclude = [\n            # 'MIPS (true)',\n            # \"Learned MIPS OneHot\", \n            # \"Learned MIPS FineTune\",\n            # \"Learned MIPS Combined\",\n        ]\n        result = pd.DataFrame()\n        if local_path:\n            result = pd.read_csv(f\"{local_path}/result_df.csv\", index_col=0)\n            output_path = Path(local_path)\n        else:\n            output_path = Path(self.get_output_path(experiment_name))\n            for trial in self.trial_configs:\n                s3_path = self.get_s3_path(experiment_name, trial.name)\n                try:\n                    result = result.append(pd.read_csv(f\"{s3_path}/result_df.csv\", index_col=0), ignore_index=True)\n                except:\n                    logger.error(f\"No result found for {trial.name}\")\n        plot_line(\n            result_df=result,\n            fig_path=output_path,\n            x=\"n_unobserved_cat_dim\",\n            xlabel=\"Number of unobserved embedding dimensions\",\n            xticklabels=result.n_unobserved_cat_dim.unique(),\n            exclude=exclude\n        )\n        if not local_path:\n            result.to_csv(f\"{self.get_output_path(experiment_name)}/result_df.csv\")", ""]}
{"filename": "experiments/ablation_hpo_experiment.py", "chunked_list": ["from logging import getLogger\nfrom pathlib import Path\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sagemaker.tuner import IntegerParameter\n\nfrom .abstracts.abstract_hpo_experiment import AbstractHpoExperiment\nfrom .utils.configs import HpoTrialConfig", "from .abstracts.abstract_hpo_experiment import AbstractHpoExperiment\nfrom .utils.configs import HpoTrialConfig\n\nplt.style.use(['science', 'no-latex'])\nlogger = getLogger(__name__)\n\n\nclass AblationHpoExperiment(AbstractHpoExperiment):\n    @property\n    def job_class_name(self) -> str:\n        return \"OpeHpoJob\"\n\n    @property\n    def max_parallel_jobs(self) -> int:\n        return 5\n\n    @property\n    def instance_type(self) -> str:\n        return \"ml.c5.18xlarge\"\n\n    @property\n    def trial_configs(self) -> List[HpoTrialConfig]:\n        return [\n            HpoTrialConfig(\n                name=\"learned-embed-dim\",\n                hyperparameter_ranges={\n                    \"learned-embed-dim\": IntegerParameter(1, 100),\n                },\n                max_jobs=100\n            ),\n        ]\n\n    @property\n    def metric_definitions(self) -> List[dict]:\n        \"\"\"Metrics for HyperparameterTuner object to extract from logs\"\"\"\n        return [\n            {\"Name\": \"Relative MSE\", \"Regex\": \"Relative MSE: ([0-9\\\\.]+)\"},\n            {\"Name\": \"Relative MSE LCB\", \"Regex\": \"Relative MSE LCB: ([0-9\\\\.]+)\"},\n            {\"Name\": \"Relative MSE UCB\", \"Regex\": \"Relative MSE UCB: ([0-9\\\\.]+)\"},\n            {\"Name\": \"Bias\", \"Regex\": \"Bias: ([0-9\\\\.]+)\"},\n            {\"Name\": \"Variance\", \"Regex\": \"Variance: ([0-9\\\\.]+)\"},\n        ]\n\n    def get_output(self, experiment_name: str):\n        for trial in self.trial_configs:\n            try:\n                job_name = f\"{experiment_name}-{trial.name}\"\n                tuner = self.get_tuner(job_name)\n            except:\n                logger.error(f\"Tuning job {experiment_name}-{trial.name} not found\")\n                continue\n            df = tuner.analytics().dataframe()\n            df = df.join(self.get_metrics(job_name), on=\"TrainingJobName\")\n            hyperparameter_name = list(trial.hyperparameter_ranges.keys())[0]\n            df[hyperparameter_name] = df[hyperparameter_name].astype(str).str.replace('\"', '').astype(float)\n            df.sort_values(by=hyperparameter_name, inplace=True)\n            ax = sns.lineplot(\n                x=df[hyperparameter_name].rename(\"Size of the learned embeddings\"),\n                y=df['FinalObjectiveValue'].rename(self.objective_metric_name)\n            )\n            ax.fill_between(df[hyperparameter_name], df[\"Relative MSE LCB\"], df[\"Relative MSE UCB\"], alpha=0.2)\n\n            plt.yscale('log')\n            plt.ylim(top=5, bottom=4e-1)\n            output_dir = self.get_output_path(experiment_name, trial.name)\n            Path(output_dir).mkdir(parents=True, exist_ok=True)\n            tuner.analytics().export_csv(f\"{output_dir}/tuner_results.csv\")\n            plt.savefig(f\"{output_dir}/{hyperparameter_name}.pdf\", bbox_inches='tight')\n            plt.savefig(f\"{output_dir}/{hyperparameter_name}.png\", bbox_inches='tight')\n            plt.close()\n\n            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(9, 3))\n            sns.lineplot(\n                x=df[hyperparameter_name].rename(\"Size of the learned embeddings\"),\n                y=df['Variance'],\n                ax=ax[0]\n            )\n            sns.lineplot(\n                x=df[hyperparameter_name].rename(\"Size of the learned embeddings\"),\n                y=df['Bias'],\n                ax=ax[1]\n            )\n            plt.savefig(f\"{output_dir}/{hyperparameter_name}_bias-variance.pdf\", bbox_inches='tight')\n            plt.savefig(f\"{output_dir}/{hyperparameter_name}_bias-variance.png\", bbox_inches='tight')\n            plt.close()", ""]}
{"filename": "experiments/utils/configs.py", "chunked_list": ["from dataclasses import dataclass, field\nfrom typing import List\n\n\n@dataclass\nclass TrialConfig:\n    name: str # name of the trial\n\n\n@dataclass\nclass LearnEmbedConfig:\n    learned_embed_dim: int = 5 # size of the learned action embedding", "\n@dataclass\nclass LearnEmbedConfig:\n    learned_embed_dim: int = 5 # size of the learned action embedding\n\n\n@dataclass\nclass LearnedEmbedParams:\n    epochs: int = 100\n    batch_size: int = 64\n    lr: float = 1e-4\n    train_test_ratio: float = 1", "\n\n@dataclass\nclass OpeTrialConfig(TrialConfig):\n    embed_model_config: LearnEmbedConfig = field(default_factory=lambda: LearnEmbedConfig())\n    learned_embed_params: LearnedEmbedParams = field(default_factory=lambda: LearnedEmbedParams())\n\n\n@dataclass\nclass SyntheticOpeTrialConfig(OpeTrialConfig):\n    n_cat_dim: int = 3 # number of dimensions in the action embedding\n    n_unobserved_cat_dim: int = 0 # number of unobserved dimensions in the action embedding\n    dim_context: int = 10 # number of context dimensions\n    n_seeds: int = 100 # number of runs to average over\n    n_val_data: int = 10000 # default number of training samples\n    n_actions: int = 100 # default number of distinct actions\n    n_cat_per_dim: int = 10 # number of categories per dimension in the action embedding\n    n_test_data: int = 200000 # number of test samples\n    n_def_actions: int = 0 # number of actions in which we do not have any observations\n    latent_param_mat_dim: int = 5 # size of the random parameters matrix to generate the reward\n    beta: int = -1 # entropy of the logging policy, -1 means almost random uniform, 1 means almost deterministic\n    eps: float = 0.05 # the amount of exploration in eps-greedy evaluation policy\n    reward_std: float = 2.5 # amount of gaussian noise in the reward\n    is_optimal: bool = True # whether the policy selects the best or the worst action\n    embed_selection: bool = False # whether to use the SLOPE algorithm for embedding selection\n    random_state: int = 12345 # fixed seed to replicate the same results\n    n_val_data_list: List[int] = field(default_factory=lambda: [800, 1600, 3200, 6400, 12800, 25600]) # values when varying the number of training samples\n    n_unobserved_cat_dim_list: List[int] = field(\n        default_factory=lambda: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) # values when varying the number of unobserved dimensions in the action embedding\n    eps_list: List[float] = field(default_factory=lambda: [0, 0.2, 0.4, 0.6, 0.8, 1]) # values when varying the amount of exploration in eps-greedy evaluation policy\n    beta_list: List[float] = field(default_factory=lambda: [-3, -2, -1, -0.5, 0, 0.5, 1, 2, 3]) # values when varying the entropy of the logging policy\n    noise_list: List[float] = field(default_factory=lambda: [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]) # values when varying the amount of gaussian noise in the reward\n    n_def_actions_list: List[float] = field(default_factory=lambda: [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]) # values when varying the number of actions in which we do not have any observations\n    n_actions_list: List[float] = field(\n        default_factory=lambda: [10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\n    ) # values when varying the number of distinct actions", "@dataclass\nclass SyntheticOpeTrialConfig(OpeTrialConfig):\n    n_cat_dim: int = 3 # number of dimensions in the action embedding\n    n_unobserved_cat_dim: int = 0 # number of unobserved dimensions in the action embedding\n    dim_context: int = 10 # number of context dimensions\n    n_seeds: int = 100 # number of runs to average over\n    n_val_data: int = 10000 # default number of training samples\n    n_actions: int = 100 # default number of distinct actions\n    n_cat_per_dim: int = 10 # number of categories per dimension in the action embedding\n    n_test_data: int = 200000 # number of test samples\n    n_def_actions: int = 0 # number of actions in which we do not have any observations\n    latent_param_mat_dim: int = 5 # size of the random parameters matrix to generate the reward\n    beta: int = -1 # entropy of the logging policy, -1 means almost random uniform, 1 means almost deterministic\n    eps: float = 0.05 # the amount of exploration in eps-greedy evaluation policy\n    reward_std: float = 2.5 # amount of gaussian noise in the reward\n    is_optimal: bool = True # whether the policy selects the best or the worst action\n    embed_selection: bool = False # whether to use the SLOPE algorithm for embedding selection\n    random_state: int = 12345 # fixed seed to replicate the same results\n    n_val_data_list: List[int] = field(default_factory=lambda: [800, 1600, 3200, 6400, 12800, 25600]) # values when varying the number of training samples\n    n_unobserved_cat_dim_list: List[int] = field(\n        default_factory=lambda: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) # values when varying the number of unobserved dimensions in the action embedding\n    eps_list: List[float] = field(default_factory=lambda: [0, 0.2, 0.4, 0.6, 0.8, 1]) # values when varying the amount of exploration in eps-greedy evaluation policy\n    beta_list: List[float] = field(default_factory=lambda: [-3, -2, -1, -0.5, 0, 0.5, 1, 2, 3]) # values when varying the entropy of the logging policy\n    noise_list: List[float] = field(default_factory=lambda: [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]) # values when varying the amount of gaussian noise in the reward\n    n_def_actions_list: List[float] = field(default_factory=lambda: [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]) # values when varying the number of actions in which we do not have any observations\n    n_actions_list: List[float] = field(\n        default_factory=lambda: [10, 20, 50, 100, 200, 500, 1000, 2000, 5000]\n    ) # values when varying the number of distinct actions", "\n\n@dataclass\nclass RealOpeTrialConfig(OpeTrialConfig):\n    n_seeds: int = 150 # number of bootstrap runs\n    sample_size: int = 1000 # number of observations in one sample\n    random_state: str = 12345 # fixed seed to replicate the same results\n\n\n@dataclass\nclass HpoTrialConfig(SyntheticOpeTrialConfig):\n    random_state: str = 12345 # fixed seed to replicate the same results\n    hyperparameter_ranges: dict = field(default_factory=lambda: {}) # hyperparameter space defined by sagemaker hyperparameter ranges https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html\n    fixed_hyperparameters: dict = field(default_factory=lambda: {}) # fixed values for already optimized hyperparameters, ignored if present in hyperparameter_ranges\n    max_jobs: int = 100 # number of maximum trials when searching for the hyperparameters\n    strategy: str = \"Bayesian\" # strategy used to search over the hyperparameter space. Either 'Bayesian' or 'Random'", "\n@dataclass\nclass HpoTrialConfig(SyntheticOpeTrialConfig):\n    random_state: str = 12345 # fixed seed to replicate the same results\n    hyperparameter_ranges: dict = field(default_factory=lambda: {}) # hyperparameter space defined by sagemaker hyperparameter ranges https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html\n    fixed_hyperparameters: dict = field(default_factory=lambda: {}) # fixed values for already optimized hyperparameters, ignored if present in hyperparameter_ranges\n    max_jobs: int = 100 # number of maximum trials when searching for the hyperparameters\n    strategy: str = \"Bayesian\" # strategy used to search over the hyperparameter space. Either 'Bayesian' or 'Random'\n", ""]}
{"filename": "experiments/utils/__init__.py", "chunked_list": [""]}
{"filename": "experiments/utils/plots.py", "chunked_list": ["from pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n\nplt.style.use(['science', 'no-latex'])\n", "plt.style.use(['science', 'no-latex'])\n\nregistered_colors = {\n    \"MIPS\": \"tab:gray\",\n    \"MIPS (true)\": \"tab:orange\",\n    \"MIPS (w/ SLOPE)\": \"tab:orange\",\n    \"IPS\": \"tab:red\",\n    \"DR\": \"tab:blue\",\n    \"DM\": \"tab:purple\",\n    \"SNIPS\": \"lightblue\",", "    \"DM\": \"tab:purple\",\n    \"SNIPS\": \"lightblue\",\n    \"SwitchDR\": \"tab:pink\",\n    \"Learned MIPS OneHot\": \"tab:olive\",\n    \"Learned MIPS FineTune\": \"green\",\n    \"Learned MIPS Combined\": \"tab:brown\",\n}\n\nmarkers_dict = {\n    \"MIPS\": \"X\",", "markers_dict = {\n    \"MIPS\": \"X\",\n    \"MIPS (true)\": \"X\",\n    \"MIPS (w/ SLOPE)\": \"X\",\n    \"IPS\": \"v\",\n    \"DR\": \"v\",\n    \"DM\": \"v\",\n    \"SNIPS\": \"v\",\n    \"SwitchDR\": \"v\",\n    \"Learned MIPS OneHot\": \"v\",", "    \"SwitchDR\": \"v\",\n    \"Learned MIPS OneHot\": \"v\",\n    \"Learned MIPS FineTune\": \"X\",\n    \"Learned MIPS Combined\": \"X\",\n}\n\n\ndef export_legend(ax, filename=\"legend.pdf\"):\n    fig2 = plt.figure()\n    ax2 = fig2.add_subplot()\n    ax2.axis('off')\n    legend = ax2.legend(*ax.get_legend_handles_labels(), frameon=False, loc='lower center', ncol=10, handlelength=1.5)\n    for legobj in legend.legendHandles:\n        legobj.set_linewidth(2.5)\n        legobj._markeredgecolor = 'white'\n        legobj._markeredgewidth = 0.5\n        legobj._markersize = 8\n    fig = legend.figure\n    fig.canvas.draw()\n    bbox = legend.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n    fig.savefig(filename, dpi=\"figure\", bbox_inches=bbox)", "\n\ndef remove_estimators(df: pd.DataFrame, estimators: list):\n    return df[~df.est.isin(estimators)]\n\n\ndef plot_line(\n    result_df: pd.DataFrame,\n    fig_path: Path,\n    x: str,\n    xlabel: str,\n    xticklabels: list,\n    estimators: list = None,\n    exclude=[],\n    markers=True\n):\n    result_df = remove_estimators(result_df, exclude)\n\n    def plot_part(column, ylabel, fig_name, legend=True, log_scale=True):\n        plt.close()\n        fig, ax = plt.subplots(figsize=(11, 7), tight_layout=True)\n        sns.lineplot(\n            linewidth=5,\n            legend=legend,\n            markers=markers_dict if markers else False,\n            markersize=15,\n            markeredgecolor='white',\n            dashes=False,\n            x=x,\n            y=column,\n            hue=\"est\",\n            style=\"est\",\n            ax=ax,\n            palette=palette,\n            data=result_df.query(query),\n        )\n\n        if legend:\n            l = ax.legend(\n                loc=\"upper left\",\n                fontsize=25,\n            )\n            for legobj in l.legendHandles:\n                legobj.set_linewidth(4)\n                legobj.set_markersize(12)\n\n        # yaxis\n        if log_scale:\n            ax.set_yscale(\"log\")\n        ax.set_ylabel(ylabel, fontsize=25)\n        ax.tick_params(axis=\"y\", labelsize=18)\n        ax.yaxis.set_label_coords(-0.08, 0.5)\n        # ax.set_ylim(top=1.05e-1, bottom=4e-3)\n\n        # xaxis\n        if x in [\"n_actions\", \"n_val_data\"]:\n            ax.set_xscale(\"log\")\n\n        ax.set_xlabel(xlabel, fontsize=25)\n        ax.set_xticks(xticklabels)\n        ax.set_xticklabels(xticklabels, fontsize=18)\n        ax.xaxis.set_label_coords(0.5, -0.1)\n        plt.savefig(fig_path / fig_name, bbox_inches=\"tight\")\n        return ax\n\n    if estimators is None:\n        estimators = [est for est in result_df.est.unique() if est in registered_colors]\n    query = \"(\" + \" or \".join([f\"est == '{est}'\" for est in estimators]) + \")\"\n    palette = [registered_colors[est] for est in estimators]\n\n    fig_path.mkdir(exist_ok=True, parents=True)\n    print(estimators)\n\n    plot_part(\"se\", \"MSE\", \"mse.png\")\n    ax = plot_part(\"se\", \"MSE\", \"mse.pdf\")\n    export_legend(ax, fig_path / \"legend.pdf\")\n    plot_part(\"se\", \"MSE\", \"mse_no_legend.pdf\", legend=False)\n    plot_part(\"se\", \"MSE\", \"mse_no_log.pdf\", log_scale=False)\n    plot_part(\"se\", \"MSE\", \"mse_no_log_no_legend.pdf\", legend=False, log_scale=False)\n    plot_part(\"variance\", \"Variance\", \"variance.pdf\")\n    plot_part(\"variance\", \"Variance\", \"variance_no_legend.pdf\", legend=False)\n    plot_part(\"variance\", \"Variance\", \"variance_no_log.pdf\", log_scale=False)\n    plot_part(\"variance\", \"Variance\", \"variance_no_log_no_legend.pdf\", legend=False, log_scale=False)\n    plot_part(\"bias\", \"Squared bias\", \"bias_no_log.pdf\", log_scale=False)\n    plot_part(\"bias\", \"Squared bias\", \"bias_no_log_no_legend.pdf\", legend=False, log_scale=False)", "\n\ndef plot_cdf(\n    result_df: pd.DataFrame,\n    fig_path: str,\n    relative_to: str = 'MIPS (w/ SLOPE)',\n    remove_legend=False,\n    exclude=[]\n):\n    result_df = remove_estimators(result_df, exclude)\n    estimators = [est for est in result_df.est.unique() if est in registered_colors]\n    query = \"(\" + \" or \".join([f\"est == '{est}'\" for est in estimators]) + \")\"\n    result_df = result_df.query(query).reset_index()\n    relative_index = result_df[result_df.est == relative_to].index[0]\n    rel_result_df = result_df.groupby(result_df.index // len(estimators)) \\\n        .apply(lambda df: pd.DataFrame({'est': df['est'], 'se': df['se'] / df.iloc[relative_index]['se']}))\n    palette = [registered_colors[est] for est in estimators[::-1]]\n\n    fig, ax = plt.subplots(figsize=(10, 6.5), tight_layout=True)\n    sns.ecdfplot(\n        linewidth=3.5,\n        palette=palette,\n        data=rel_result_df,\n        x=\"se\",\n        hue=\"est\",\n        hue_order=estimators[::-1],\n        ax=ax\n    )\n\n    ax.legend(estimators, loc=\"upper left\", fontsize=22)\n    if remove_legend:\n        ax.legend(estimators, loc=\"upper left\", fontsize=22).remove()\n\n    for i in range(len(ax.lines)):\n        line = ax.lines[i]\n        if line._x.max() == 1:\n            continue\n        idx = np.abs(line._x - 1).argmin()\n        y = line._y[idx]\n        ax.axhline(y, 0, 0.52, color=line.get_color(), linewidth=1.5, linestyle=(0, (5, 3)))\n\n    ax.set_ylabel(\"Cumulative distribution\", fontsize=22)\n    ax.tick_params(axis=\"y\", labelsize=18)\n    ax.yaxis.set_label_coords(-0.08, 0.5)\n\n    ax.set_xscale(\"log\")\n    ax.set_xlim(0.3, 3)\n    ax.set_xlabel(f\"Relative MSE w.r.t. {relative_to}\", fontsize=22)\n    ax.tick_params(axis=\"x\", labelsize=18)\n    ax.xaxis.set_label_coords(0.5, -0.1)\n    plt.savefig(fig_path, bbox_inches='tight')\n    plt.close()", ""]}
{"filename": "experiments/utils/constants.py", "chunked_list": ["BUCKET = \"ope-learn-action-embeddings\"\nROLE = \"SageMakerUser\"\n"]}
{"filename": "experiments/abstracts/abstract_hpo_experiment.py", "chunked_list": ["import dataclasses\nimport json\nimport logging\nfrom enum import Enum\nfrom typing import List\n\nimport pandas as pd\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\nfrom sagemaker.tuner import HyperparameterTuner", "from sagemaker.pytorch import PyTorch\nfrom sagemaker.tuner import HyperparameterTuner\n\nfrom experiments.utils.configs import HpoTrialConfig\nfrom experiments.utils.constants import ROLE\n\nfrom .abstract_experiments import AbstractExperiment\n\nsm_sess = sagemaker.Session()\nsm = sm_sess.sagemaker_client", "sm_sess = sagemaker.Session()\nsm = sm_sess.sagemaker_client\nlog = logging.getLogger()\n\n\nclass Status(Enum):\n    FAILED = \"Failed\"\n    IN_PROGRESS = \"InProgress\"\n\n", "\n\n@dataclasses.dataclass\nclass AbstractHpoExperiment(AbstractExperiment):\n    def fit(self, trial_config: HpoTrialConfig, experiment_name):\n        job_parameters = self.get_job_config_parameter(trial_config, experiment_name)\n        estimator = PyTorch(\n            py_version=\"py38\",\n            entry_point=self.entry_point,\n            role=ROLE,\n            sagemaker_session=sagemaker.Session(sagemaker_client=sm),\n            framework_version=\"1.9.0\",\n            instance_count=1,\n            instance_type=self.instance_type,\n            hyperparameters=job_parameters,\n            metric_definitions=[],\n            enable_sagemaker_metrics=True,\n            source_dir=\"./\",\n            max_run=5 * 24 * 60 * 60  # set the training limit to 5 days (maximum allowed time by default)\n        )\n\n        tuner = HyperparameterTuner(\n            estimator,\n            self.objective_metric_name,\n            trial_config.hyperparameter_ranges,\n            self.metric_definitions,\n            max_jobs=trial_config.max_jobs,\n            max_parallel_jobs=self.max_parallel_jobs,\n            objective_type=self.objective_type,\n            strategy=trial_config.strategy\n        )\n\n        training_job_name = f\"{experiment_name}-{trial_config.name}\"\n        trial_name = f\"trial-{training_job_name}\"\n        tuner.fit(\n            job_name=training_job_name,\n            experiment_config={\n                \"TrialName\": trial_name,\n                \"TrialComponentDisplayName\": \"Training\",\n            },\n            wait=False,\n        )\n        return tuner\n\n    def get_tuner(self, training_job_name):\n        return HyperparameterTuner.attach(training_job_name, sagemaker_session=sm_sess)\n\n    def get_job_config_parameter(self, trial: HpoTrialConfig, experiment_name: str) -> dict:\n        parameters = {**dataclasses.asdict(trial),\n                      **{\"s3_path\": self.get_s3_path(experiment_name, trial.name),\n                         \"job_class_name\": self.job_class_name}}\n        del parameters['hyperparameter_ranges']\n\n        return {\"config\": f\"'{json.dumps(parameters)}'\"}\n\n    def get_metrics(self, training_job_name):\n        jobs = []\n\n        while True:\n            result = sm.list_training_jobs_for_hyper_parameter_tuning_job(\n                HyperParameterTuningJobName=training_job_name,\n                MaxResults=100,\n                SortBy='FinalObjectiveMetricValue',\n                SortOrder='Descending'\n            )\n            jobs += result['TrainingJobSummaries']\n            if 'NextToken' not in result:\n                break\n\n        result = pd.DataFrame()\n        for job in jobs:\n            job_description = sm.describe_training_job(TrainingJobName=job['TrainingJobName'])\n            metrics = {m['MetricName']: m['Value'] for m in job_description['FinalMetricDataList']}\n            result = result.append(pd.Series(metrics, name=job['TrainingJobName']))\n        \n        return result\n\n    @property\n    def metric_definitions(self) -> List[dict]:\n        \"\"\"Metrics for HyperparameterTuner object to extract from logs\"\"\"\n        return [\n            {\"Name\": \"Relative MSE\", \"Regex\": \"Relative MSE: ([0-9\\\\.]+)\"},\n        ]\n\n    @property\n    def objective_metric_name(self) -> str:\n        \"\"\"Metric that tuner optimizes for. Must be specified in the @property metrics_definitions field\"\"\"\n        return \"Relative MSE\"\n\n    @property\n    def objective_type(self) -> str:\n        \"\"\"Whether the tuner should 'Minimize' or 'Maximize' the objective metric\"\"\"\n        return \"Minimize\"\n\n    @property\n    def max_parallel_jobs(self) -> int:\n        \"\"\"Maximum number of jobs to run simultaneously\"\"\"\n        return 8\n\n    @property\n    def instance_type(self) -> str:\n        \"\"\"AWS SageMaker Training instance type used for the job execution\"\"\"\n        return \"ml.c5.4xlarge\"", ""]}
{"filename": "experiments/abstracts/abstract_experiments.py", "chunked_list": ["import abc\nimport dataclasses\nimport json\nimport logging\nfrom abc import ABC\nimport time\nfrom enum import Enum\nfrom typing import List\n\nimport sagemaker", "\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\nfrom smexperiments import tracker, experiment\nfrom smexperiments.experiment import Experiment\nfrom smexperiments.trial import Trial\nfrom smexperiments.trial_component import TrialComponent\n\nfrom experiments.utils.configs import TrialConfig\nfrom experiments.utils.constants import ROLE, BUCKET", "from experiments.utils.configs import TrialConfig\nfrom experiments.utils.constants import ROLE, BUCKET\n\nsm_sess = sagemaker.Session()\nsm = sm_sess.sagemaker_client\nlog = logging.getLogger()\n\n\nclass Status(Enum):\n    FAILED = \"Failed\"\n    IN_PROGRESS = \"InProgress\"", "class Status(Enum):\n    FAILED = \"Failed\"\n    IN_PROGRESS = \"InProgress\"\n\n\n@dataclasses.dataclass\nclass AbstractExperiment(ABC):\n\n    def run(self, experiment_name: str):\n        \"\"\"\n        All experiments should extend this abstract class and implement\n        all abstract functions\n        @param experiment_name: unique experiment name\n        \"\"\"\n        experiment = Experiment.create(\n            experiment_name=experiment_name,\n            description=self.description,\n            sagemaker_boto_client=sm,\n        )\n        estimators = []\n        for trial_config in self.trial_configs:\n            job_parameters = self.get_job_config_parameter(trial_config, experiment_name)\n            trial_name = f\"trial-{experiment_name}-{trial_config.name}\"\n            log.info(trial_name)\n            sm_trial = Trial.create(\n                trial_name=trial_name,\n                experiment_name=experiment.experiment_name,\n                sagemaker_boto_client=sm,\n            )\n\n            log.info(job_parameters)\n            with tracker.Tracker.create() as trail_tracker:\n                trail_tracker.log_parameters({**job_parameters})\n                sm_trial.add_trial_component(trail_tracker)\n\n            estimator = self.fit(trial_config, experiment_name)\n            estimators.append(estimator)\n        return estimators\n\n    def fit(self, trial_config, experiment_name):\n        job_parameters = self.get_job_config_parameter(trial_config, experiment_name)\n        estimator = PyTorch(\n            py_version=\"py38\",\n            entry_point=self.entry_point,\n            role=ROLE,\n            sagemaker_session=sagemaker.Session(sagemaker_client=sm),\n            framework_version=\"1.9.0\",\n            instance_count=1,\n            instance_type=self.instance_type,\n            hyperparameters=job_parameters,\n            metric_definitions=[],\n            enable_sagemaker_metrics=True,\n            source_dir=\"./\",\n            max_run=5 * 24 * 60 * 60  # set the training limit to 5 days (maximum allowed time by default)\n        )\n        training_job_name = f\"{experiment_name}-{trial_config.name}\"\n        trial_name = f\"trial-{training_job_name}\"\n        estimator.fit(\n            job_name=training_job_name,\n            experiment_config={\n                \"TrialName\": trial_name,\n                \"TrialComponentDisplayName\": \"Training\",\n            },\n            wait=False,\n        )\n        return estimator\n\n    @staticmethod\n    def training_jobs_has_finished(experiment_name) -> bool:\n        sm_experiment = experiment.Experiment.load(experiment_name)\n        jobs = []\n        for sm_trial_summary in sm_experiment.list_trials():\n            for trial_component_summary in Trial.load(sm_trial_summary.trial_name).list_trial_components():\n                trial_component = TrialComponent.load(trial_component_summary.trial_component_name)\n                if \"sagemaker_job_name\" in trial_component.parameters:\n                    job_name = trial_component.parameters[\"sagemaker_job_name\"].replace(\"'\", \"\").replace('\"', \"\")\n                    jobs.append(sm_sess.describe_training_job(job_name))\n        [log.info((job[\"TrainingJobName\"], job[\"TrainingJobStatus\"])) for job in jobs]\n        failed_jobs = [job for job in jobs if job[\"TrainingJobStatus\"] == Status.FAILED.value]\n        in_progress_jobs = [job for job in jobs if job[\"TrainingJobStatus\"] == Status.IN_PROGRESS.value]\n\n        if failed_jobs:\n            log.info(\"Jobs have failed\")\n            return False\n\n        if in_progress_jobs:\n            log.info(\"Jobs are in progress\")\n            return False\n\n        return True\n\n    @abc.abstractmethod\n    def get_output(self, experiment_name: str):\n        \"\"\"\n        Generates output of the experiment from the data published\n        to the s3 bucket, graphs or tables can be the output of an experiment\n        @param experiment_name: The name of the experiment which the output is related to\n        \"\"\"\n        pass\n\n    def get_s3_path(self, experiment_name, trial_name=None) -> str:\n        result = f\"s3://{BUCKET}/experiments/experiment={experiment_name}\"\n        return result if not trial_name else result + f\"/trial={trial_name}\"\n    \n    def get_local_path(self, experiment_name, trial_name=None) -> str:\n        result = f\"./results/{experiment_name}\"\n        return result if not trial_name else result + f\"/{trial_name}\"\n\n    def list_s3_files(self, experiment_name, trial_name) -> List[str]:\n        objects = []\n        request_params = {}\n        while True:\n            response = sm_sess.boto_session.client('s3').list_objects_v2(\n                Bucket=BUCKET, Prefix=f\"experiments/experiment={experiment_name}/trial={trial_name}\", **request_params\n            )\n            objects += response[\"Contents\"]\n            if \"NextContinuationToken\" in response:\n                request_params[\"ContinuationToken\"] = response[\"NextContinuationToken\"]\n            else:\n                break\n\n        return [f\"s3://{BUCKET}/{obj['Key']}\" for obj in objects]\n\n    def get_output_path(self, experiment_name, trial_name=None) -> str:\n        result = f\"./results/{experiment_name}\"\n        return result if not trial_name else result + f\"/{trial_name}\"\n\n    def get_job_config_parameter(self, trial: TrialConfig, experiment_name: str) -> dict:\n        parameters = {**dataclasses.asdict(trial),\n                      **{\"s3_path\": self.get_s3_path(experiment_name, trial.name),\n                         \"job_class_name\": self.job_class_name}}\n\n        return {\"config\": f\"'{json.dumps(parameters)}'\"}\n\n    @property\n    @abc.abstractmethod\n    def trial_configs(self) -> List[TrialConfig]:\n        \"\"\"\n        This function returns the trial configs for each training job,\n        it needs to be overridden by each subclass\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def job_class_name(self) -> str:\n        \"\"\"\n        This function returns the training job class name e.g ActionsJob,\n        it needs to be overridden by each subclass\n        \"\"\"\n        pass\n\n    @property\n    def entry_point(self) -> str:\n        return \"entry_point.py\"\n\n    @property\n    def instance_type(self) -> str:\n        return \"ml.c5.xlarge\"\n\n    @property\n    def description(self) -> str:\n        \"\"\"\n        This function returns the training job class name e.g ActionsJob,\n        it needs to be overridden by each subclass\n        \"\"\"\n        return \" \"", ""]}
{"filename": "experiments/abstracts/__init__.py", "chunked_list": [""]}
