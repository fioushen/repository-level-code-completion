{"filename": "docs/source/conf.py", "chunked_list": ["#!/usr/bin/env python3\n\n\n# kedro_graphql documentation build\n# configuration file, created by sphinx-quickstart.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this", "#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.", "# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport re\n\nfrom kedro.framework.cli.utils import find_stylesheets\n\nfrom kedro_graphql import __version__ as release\n\n# -- Project information -----------------------------------------------------", "\n# -- Project information -----------------------------------------------------\n\nproject = \"kedro_graphql\"\nauthor = \"Kedro\"\n\n# The short X.Y version.\nversion = re.match(r\"^([0-9]+\\.[0-9]+).*\", release).group(1)\n\n# -- General configuration ---------------------------------------------------", "\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.", "# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx_autodoc_typehints\",\n    \"sphinx.ext.doctest\",\n    \"sphinx.ext.todo\",\n    \"sphinx.ext.coverage\",\n    \"sphinx.ext.mathjax\",", "    \"sphinx.ext.coverage\",\n    \"sphinx.ext.mathjax\",\n    \"sphinx.ext.ifconfig\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx.ext.mathjax\",\n    \"nbsphinx\",\n    \"myst_parser\",\n    \"sphinx_copybutton\",\n]\n", "]\n\n# enable autosummary plugin (table of contents for modules/classes/class\n# methods)\nautosummary_generate = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# The suffix(es) of source filenames.", "\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = {\".rst\": \"restructuredtext\", \".md\": \"markdown\"}\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# The language for content autogenerated by Sphinx. Refer to documentation", "\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.", "# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\"_build\", \"**.ipynb_checkpoints\"]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n\n# -- Options for HTML output -------------------------------------------------\n", "# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \"sphinx_rtd_theme\"\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.", "# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\"collapse_navigation\": False, \"style_external_links\": True}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n", "html_static_path = [\"_static\"]\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don't match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',\n# 'searchbox.html']``.\n#", "# 'searchbox.html']``.\n#\n# html_sidebars = {}\n\nhtml_show_sourcelink = False\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"kedro_graphqldoc\"", "# Output file base name for HTML help builder.\nhtmlhelp_basename = \"kedro_graphqldoc\"\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n    #", "    # 'papersize': 'letterpaper',\n    #\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n    #\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n    #", "    # 'preamble': '',\n    #\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).", "# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\n        master_doc,\n        \"kedro_graphql.tex\",\n        \"kedro_graphql Documentation\",\n        \"Kedro\",\n        \"manual\",\n    )", "        \"manual\",\n    )\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (", "man_pages = [\n    (\n        master_doc,\n        \"kedro_graphql\",\n        \"kedro_graphql Documentation\",\n        [author],\n        1,\n    )\n]\n", "]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,", "    (\n        master_doc,\n        \"kedro_graphql\",\n        \"kedro_graphql Documentation\",\n        author,\n        \"kedro_graphql\",\n        \"Project kedro_graphql codebase.\",\n        \"Data-Science\",\n    )\n]", "    )\n]\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# -- Extension configuration -------------------------------------------------\n", "# -- Extension configuration -------------------------------------------------\n\n# nbsphinx_prolog = \"\"\"\n# see here for prolog/epilog details:\n# https://nbsphinx.readthedocs.io/en/0.3.1/prolog-and-epilog.html\n# \"\"\"\n\n# -- NBconvert kernel config -------------------------------------------------\nnbsphinx_kernel_name = \"python3\"\n", "nbsphinx_kernel_name = \"python3\"\n\n\ndef remove_arrows_in_examples(lines):\n    for i, line in enumerate(lines):\n        lines[i] = line.replace(\">>>\", \"\")\n\n\ndef autodoc_process_docstring(app, what, name, obj, options, lines):\n    remove_arrows_in_examples(lines)", "def autodoc_process_docstring(app, what, name, obj, options, lines):\n    remove_arrows_in_examples(lines)\n\n\ndef skip(app, what, name, obj, skip, options):\n    if name == \"__init__\":\n        return False\n    return skip\n\n\ndef setup(app):\n    app.connect(\"autodoc-process-docstring\", autodoc_process_docstring)\n    app.connect(\"autodoc-skip-member\", skip)\n    # add Kedro stylesheets\n    for stylesheet in find_stylesheets():\n        app.add_css_file(stylesheet)", "\n\ndef setup(app):\n    app.connect(\"autodoc-process-docstring\", autodoc_process_docstring)\n    app.connect(\"autodoc-skip-member\", skip)\n    # add Kedro stylesheets\n    for stylesheet in find_stylesheets():\n        app.add_css_file(stylesheet)\n", ""]}
{"filename": "src/setup.py", "chunked_list": ["from setuptools import find_packages, setup\n\n#entry_point = (\n#    \"kedro-graphql = kedro_graphql.__main__:main\"\n#)\n\n\n# get the dependencies and installs\n##with open(\"requirements.txt\", encoding=\"utf-8\") as f:\n##    # Make sure we strip all comments and options (e.g \"--extra-index-url\")", "##with open(\"requirements.txt\", encoding=\"utf-8\") as f:\n##    # Make sure we strip all comments and options (e.g \"--extra-index-url\")\n##    # that arise from a modified pip.conf file that configure global options\n##    # when running kedro build-reqs\n##    requires = []\n##    for line in f:\n##        req = line.split(\"#\", 1)[0].strip()\n##        if req and not req.startswith(\"--\"):\n##            requires.append(req)\n", "##            requires.append(req)\n\nsetup(\n    name=\"kedro_graphql\",\n    packages=find_packages(exclude=[\"tests\"]),\n    ##entry_points={\n    ##    \"console_scripts\": [entry_point],\n    ##    \"kedro.project_commands\": [\"kedro-graphql = kedro_graphql.commands:commands\"]\n    ##    },\n#    install_requires=requires,", "    ##    },\n#    install_requires=requires,\n    extras_require={\n        \"docs\": [\n            \"docutils<0.18.0\",\n            \"sphinx~=3.4.3\",\n            \"sphinx_rtd_theme==0.5.1\",\n            \"nbsphinx==0.8.1\",\n            \"nbstripout~=0.4\",\n            \"sphinx-autodoc-typehints==1.11.1\",", "            \"nbstripout~=0.4\",\n            \"sphinx-autodoc-typehints==1.11.1\",\n            \"sphinx_copybutton==0.3.1\",\n            \"ipykernel>=5.3, <7.0\",\n            \"Jinja2<3.1.0\",\n            \"myst-parser~=0.17.2\",\n        ]\n    },\n)\n", ")\n"]}
{"filename": "src/tests/test_schema_query.py", "chunked_list": ["\"\"\"\n\n\"\"\"\n\n\nimport pytest\nfrom kedro_graphql.schema import build_schema\n\nschema = build_schema()\n", "schema = build_schema()\n\n@pytest.mark.usefixtures('celery_session_app')\n@pytest.mark.usefixtures('celery_session_worker')\nclass TestSchemaQuery:\n\n    @pytest.mark.asyncio\n    async def test_pipeline(self, mock_info_context, mock_pipeline):\n\n        query = \"\"\"\n        query TestQuery($id: String!) {\n          pipeline(id: $id){\n            id\n          }\n        }\n        \"\"\"\n        resp = await schema.execute(query, variable_values = {\"id\": str(mock_pipeline.id)})\n        assert resp.errors is None\n\n    @pytest.mark.asyncio\n    async def test_pipeline_templates(self):\n\n        query = \"\"\"\n        query TestQuery {\n          pipelineTemplates {\n            name\n            describe\n            inputs {\n              name\n              filepath\n              type\n            }\n            nodes {\n              name\n              inputs\n              outputs\n              tags\n            }\n            outputs {\n              filepath\n              name\n              type\n            }\n            parameters {\n              name\n              value\n            }\n          }\n        }\n        \"\"\"\n        resp = await schema.execute(query)\n        \n        assert resp.errors is None", ""]}
{"filename": "src/tests/test_events.py", "chunked_list": ["\"\"\"\nThis module contains an example test.\n\nTests should be placed in ``src/tests``, in modules that mirror your\nproject's structure, and in files named test_*.py. They are simply functions\nnamed ``test_*`` which test a unit of logic.\n\nTo run the tests, run ``kedro test`` from the project root directory.\n\"\"\"\n", "\"\"\"\n\n\nimport pytest\nfrom kedro_graphql.events import PipelineEventMonitor\nfrom kedro_graphql.tasks import run_pipeline\nimport time\nfrom celery.states import ALL_STATES\nfrom celery.result import AsyncResult \n", "from celery.result import AsyncResult \n\n\n\n@pytest.mark.usefixtures('celery_session_app')\n@pytest.mark.usefixtures('celery_session_worker')\n@pytest.mark.usefixtures('celery_includes')\nclass TestPipelineEventMonitor:\n    @pytest.mark.asyncio\n    async def test_consume_default(self, mocker, celery_session_app, mock_pipeline):\n        \"\"\"\n        Requires Redis to run.\n        \"\"\"\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.before_start\")\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_success\")\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_retry\")\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_failure\")\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.after_return\")\n\n        async for e in PipelineEventMonitor(app = celery_session_app, task_id = mock_pipeline.task_id).start():\n            print(e)\n            assert e[\"status\"] in ALL_STATES\n\n    @pytest.mark.asyncio\n    async def test_consume_short_timeout(self, mocker, celery_session_app, mock_pipeline):\n        \"\"\"\n        Requires Redis to run.\n\n        Test with shorter timeout to test Exception handling\n        \"\"\"\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.before_start\")\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_success\")\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_retry\")\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_failure\")\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.after_return\")\n\n        async for e in PipelineEventMonitor(app = celery_session_app, task_id = mock_pipeline.task_id, timeout = 0.01).start():\n            print(e)\n            assert e[\"status\"] in ALL_STATES\n        \n    @pytest.mark.asyncio\n    async def test_consume_exception(self, mocker, celery_session_app, mock_pipeline):\n        \"\"\"\n        Requires Redis to run.\n\n        Let task finish before starting monitor test Exception handling\n        \"\"\"\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.before_start\")\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_success\")\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_retry\")\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.on_failure\")\n        mocker.patch(\"kedro_graphql.tasks.KedroGraphqlTask.after_return\")\n\n        result = AsyncResult(mock_pipeline.task_id).wait()\n        async for e in PipelineEventMonitor(app = celery_session_app, task_id = mock_pipeline.task_id).start():\n            print(e)\n            assert e[\"status\"] in ALL_STATES", ""]}
{"filename": "src/tests/test_models.py", "chunked_list": ["\"\"\"\nThis module contains an example test.\n\nTests should be placed in ``src/tests``, in modules that mirror your\nproject's structure, and in files named test_*.py. They are simply functions\nnamed ``test_*`` which test a unit of logic.\n\nTo run the tests, run ``kedro test`` from the project root directory.\n\"\"\"\n", "\"\"\"\n\n\nimport pytest\nfrom kedro_graphql.models import DataSet, Parameter\n\nclass TestDataSet:\n\n    def test_serialize(self):\n        params = {\"name\": \"text_in\", \n                  \"type\": \"text.TextDataSet\", \n                  \"filepath\": \"/tmp/test_in.csv\",\n                  \"load_args\":[Parameter(**{\n                    \"name\": \"delimiter\",\n                    \"value\": \"\\t\" \n                  })],\n                  \"save_args\":[Parameter(**{\n                    \"name\":\"delimiter\",\n                    \"value\": \"\\t\"  \n                  })]}\n\n        expected = {\"text_in\":{ \n                      \"type\": \"text.TextDataSet\", \n                      \"filepath\": \"/tmp/test_in.csv\",\n                      \"load_args\":{\n                        \"delimiter\" :\"\\t\" \n                      },\n                      \"save_args\":{\n                        \"delimiter\": \"\\t\"  \n                      }\n                    }\n                   }\n\n        d = DataSet(**params)\n        output = d.serialize()\n        assert output == expected", "    \nclass TestParameter:\n\n    def test_serialize_string(self):\n        params = {\n                    \"name\": \"delimiter\",\n                    \"value\": \"\\t\",\n                    \"type\": \"string\"\n                  }\n\n        expected = {\n                    \"delimiter\": \"\\t\" \n                   }\n\n        p = Parameter(**params)\n        output = p.serialize()\n        assert output == expected\n\n    def test_serialize_int(self):\n        params = {\n                    \"name\": \"delimiter\",\n                    \"value\": \"1\",\n                    \"type\": \"integer\"\n                  }\n\n        expected = {\n                    \"delimiter\": 1\n                   }\n\n        p = Parameter(**params)\n        output = p.serialize()\n        assert output == expected\n\n    def test_serialize_int_exception(self):\n        params = {\n                    \"name\": \"delimiter\",\n                    \"value\": \"0.1\",\n                    \"type\": \"integer\"\n                  }\n\n        p = Parameter(**params)\n        try:\n            output = p.serialize()\n        except ValueError as e:\n            assert True\n\n    def test_serialize_float(self):\n        params = {\n                    \"name\": \"delimiter\",\n                    \"value\": \"0.1\",\n                    \"type\": \"float\"\n                  }\n\n        expected = {\n                    \"delimiter\": 0.1\n                   }\n\n        p = Parameter(**params)\n        output = p.serialize()\n        assert output == expected\n\n    def test_serialize_float_exception(self):\n        params = {\n                    \"name\": \"delimiter\",\n                    \"value\": \"hello\",\n                    \"type\": \"float\"\n                  }\n\n        p = Parameter(**params)\n        try:\n            output = p.serialize()\n        except ValueError as e:\n            assert True\n \n    def test_serialize_bool(self):\n\n        params = {\n                    \"name\": \"delimiter\",\n                    \"value\": \"true\",\n                    \"type\": \"boolean\"\n                  }\n\n        expected = {\n                    \"delimiter\": True\n                   }\n\n        p = Parameter(**params)\n        output = p.serialize()\n        assert output == expected\n\n        params = {\n                    \"name\": \"delimiter\",\n                    \"value\": \"True\",\n                    \"type\": \"boolean\"\n                  }\n\n        expected = {\n                    \"delimiter\": True\n                   }\n\n        p = Parameter(**params)\n        output = p.serialize()\n        assert output == expected\n\n        params = {\n                    \"name\": \"delimiter\",\n                    \"value\": \"false\",\n                    \"type\": \"boolean\"\n                  }\n\n        expected = {\n                    \"delimiter\": False\n                   }\n\n        p = Parameter(**params)\n        output = p.serialize()\n        assert output == expected\n\n        params = {\n                    \"name\": \"delimiter\",\n                    \"value\": \"False\",\n                    \"type\": \"boolean\"\n                  }\n\n        expected = {\n                    \"delimiter\": False\n                   }\n\n        p = Parameter(**params)\n        output = p.serialize()\n        assert output == expected\n\n    def test_serialize_bool_exception(self):\n\n\n        params = {\n                    \"name\": \"delimiter\",\n                    \"value\": \"rue\",\n                    \"type\": \"boolean\"\n                  }\n\n        p = Parameter(**params)\n        try:\n            output = p.serialize()\n        except ValueError as e:\n            assert True"]}
{"filename": "src/tests/test_schema_mutation.py", "chunked_list": ["\"\"\"\n\n\"\"\"\n\n\nimport pytest\nfrom kedro_graphql.schema import build_schema\n\nschema = build_schema()\n", "schema = build_schema()\n\n\nclass TestSchemaMutations:\n    mutation = \"\"\"\n        mutation TestMutation($pipeline: PipelineInput!) {\n          pipeline(pipeline: $pipeline) {\n            name\n            describe\n            inputs {\n              name\n              filepath\n              type\n            }\n            nodes {\n              name\n              inputs\n              outputs\n              tags\n            }\n            outputs {\n              filepath\n              name\n              type\n            }\n            parameters {\n              name\n              value\n            }\n            status\n            tags {\n              key\n              value\n            }\n            taskId\n            taskName\n            taskArgs\n            taskKwargs\n            taskRequest\n            taskException\n            taskTraceback\n            taskEinfo\n            taskResult\n          }\n        }\n        \"\"\"\n\n    @pytest.mark.usefixtures('celery_session_app')\n    @pytest.mark.usefixtures('celery_session_worker')\n    @pytest.mark.asyncio\n    async def test_pipeline(self, mock_info_context, mock_text_in, mock_text_out):\n\n        resp = await schema.execute(self.mutation, \n                                    variable_values = {\"pipeline\": {\n                                      \"name\": \"example00\",\n                                      \"inputs\": [{\"name\": \"text_in\", \"type\": \"text.TextDataSet\", \"filepath\": str(mock_text_in)}],\n                                      \"outputs\": [{\"name\": \"text_out\", \"type\": \"text.TextDataSet\", \"filepath\": str(mock_text_out)}],\n                                      \"parameters\": [{\"name\":\"example\", \"value\":\"hello\"},\n                                                     {\"name\": \"duration\", \"value\": \"0.1\", \"type\": \"FLOAT\"}],\n                                      \"tags\": [{\"key\": \"author\", \"value\": \"opensean\"},{\"key\":\"package\", \"value\":\"kedro-graphql\"}]\n                                    }})\n        \n        assert resp.errors is None\n\n    @pytest.mark.usefixtures('celery_session_app')\n    @pytest.mark.usefixtures('celery_session_worker')\n    @pytest.mark.asyncio\n    async def test_pipeline2(self, mock_info_context, mock_text_in_tsv, mock_text_out_tsv):\n\n        resp = await schema.execute(self.mutation, \n                                    variable_values = {\"pipeline\": {\n                                      \"name\": \"example00\",\n                                      \"inputs\": [{\"name\": \"text_in\", \n                                                  \"type\": \"pandas.CSVDataSet\", \n                                                  \"filepath\": str(mock_text_in_tsv),\n                                                  \"loadArgs\":[\n                                                      {\"name\": \"sep\", \"value\": \"\\t\"}\n                                                  ],\n                                                  \"saveArgs\":[\n                                                      {\"name\": \"sep\", \"value\": \"\\t\"}\n                                                  ]\n                                                }],\n                                      \"outputs\": [{\"name\": \"text_out\", \n                                                   \"type\": \"pandas.CSVDataSet\", \n                                                   \"filepath\": str(mock_text_out_tsv),\n                                                   \"loadArgs\":[\n                                                      {\"name\": \"sep\", \"value\": \"\\t\"}\n                                                  ],\n                                                  \"saveArgs\":[\n                                                      {\"name\": \"sep\", \"value\": \"\\t\"}\n                                                  ]}],\n                                      \"parameters\": [{\"name\":\"example\", \"value\":\"hello\"}],\n                                      \"tags\": [{\"key\": \"author\", \"value\": \"opensean\"},{\"key\":\"package\", \"value\":\"kedro-graphql\"}]\n                                    }})\n        \n        assert resp.errors is None", "\n"]}
{"filename": "src/tests/__init__.py", "chunked_list": [""]}
{"filename": "src/tests/test_logs.py", "chunked_list": ["\"\"\"\n\n\"\"\"\nimport pytest\nfrom kedro_graphql.logs.logger import PipelineLogStream\n\n@pytest.mark.usefixtures('celery_session_app')\n@pytest.mark.usefixtures('celery_session_worker')\nclass TestPipelineLogStream:\n    @pytest.mark.asyncio\n    async def test_consume(self, mock_pipeline):\n        \"\"\"Requires Redis to run.\n        \"\"\"\n        task_id = mock_pipeline.task_id\n        subscriber = await PipelineLogStream().create(task_id=task_id)\n        async for e in subscriber.consume():\n            assert set(e.keys()) == set([\"task_id\", \"message_id\", \"message\", \"time\"])", "class TestPipelineLogStream:\n    @pytest.mark.asyncio\n    async def test_consume(self, mock_pipeline):\n        \"\"\"Requires Redis to run.\n        \"\"\"\n        task_id = mock_pipeline.task_id\n        subscriber = await PipelineLogStream().create(task_id=task_id)\n        async for e in subscriber.consume():\n            assert set(e.keys()) == set([\"task_id\", \"message_id\", \"message\", \"time\"])"]}
{"filename": "src/tests/test_run.py", "chunked_list": ["\"\"\"\nThis module contains an example test.\n\nTests should be placed in ``src/tests``, in modules that mirror your\nproject's structure, and in files named test_*.py. They are simply functions\nnamed ``test_*`` which test a unit of logic.\n\nTo run the tests, run ``kedro test`` from the project root directory.\n\"\"\"\n", "\"\"\"\n\nfrom pathlib import Path\n\nimport pytest\n\nfrom kedro.framework.project import settings\nfrom kedro.config import ConfigLoader\nfrom kedro.framework.context import KedroContext\nfrom kedro.framework.hooks import _create_hook_manager", "from kedro.framework.context import KedroContext\nfrom kedro.framework.hooks import _create_hook_manager\n\n\n@pytest.fixture\ndef config_loader():\n    return ConfigLoader(conf_source=str(Path.cwd() / settings.CONF_SOURCE))\n\n\n@pytest.fixture\ndef project_context(config_loader):\n    return KedroContext(\n        package_name=\"kedro_graphql\",\n        project_path=Path.cwd(),\n        config_loader=config_loader,\n        hook_manager=_create_hook_manager(),\n    )", "\n@pytest.fixture\ndef project_context(config_loader):\n    return KedroContext(\n        package_name=\"kedro_graphql\",\n        project_path=Path.cwd(),\n        config_loader=config_loader,\n        hook_manager=_create_hook_manager(),\n    )\n", "\n\n# The tests below are here for the demonstration purpose\n# and should be replaced with the ones testing the project\n# functionality\nclass TestProjectContext:\n    def test_project_path(self, project_context):\n        assert project_context.project_path == Path.cwd()\n", ""]}
{"filename": "src/tests/test_schema_subscription.py", "chunked_list": ["\"\"\"\n\n\"\"\"\nimport pytest\nfrom kedro_graphql.schema import build_schema\n\nschema = build_schema()\n@pytest.mark.usefixtures('celery_session_app')\n@pytest.mark.usefixtures('celery_session_worker')\nclass TestSchemaSubscriptions:\n    @pytest.mark.asyncio\n    async def test_pipeline(self, mock_info_context, mock_pipeline):\n        \"\"\"Requires Redis to run.\n        \"\"\"\n\n        query = \"\"\"\n    \t  subscription {\n          \tpipeline(id:\"\"\"+ '\"' + str(mock_pipeline.id) + '\"' + \"\"\") {\n              id\n              taskId\n              status\n              result\n              timestamp\n              traceback\n            }\n    \t  }\n        \"\"\"\n\n        sub = await schema.subscribe(query)\n\n        async for result in sub:\n            assert not result.errors\n\n    @pytest.mark.asyncio\n    async def test_pipeline_logs(self, mock_info_context, mock_pipeline, mock_pipeline2):\n        \"\"\"Requires Redis to run.\n        This test runs two pipelines simultaneously to ensure logs messages are scoped\n        to the correct pipeline.\n        \n        \"\"\"\n\n        query = \"\"\"\n    \t  subscription {\n          \tpipelineLogs(id:\"\"\"+ '\"' + str(mock_pipeline.id) + '\"' + \"\"\") {\n              id\n              message\n              messageId\n              taskId\n              time\n            }\n    \t  }\n        \"\"\"\n\n        sub = await schema.subscribe(query)\n\n        async for result in sub:\n            #print(result)\n            assert not result.errors\n            assert result.data[\"pipelineLogs\"][\"id\"] == str(mock_pipeline.id)\n            assert result.data[\"pipelineLogs\"][\"taskId\"] == str(mock_pipeline.task_id)\n\n\n        query2 = \"\"\"\n    \t  subscription {\n          \tpipelineLogs(id:\"\"\"+ '\"' + str(mock_pipeline2.id) + '\"' + \"\"\") {\n              id\n              message\n              messageId\n              taskId\n              time\n            }\n    \t  }\n        \"\"\"\n\n        sub2 = await schema.subscribe(query2)\n\n        async for result in sub2:\n            #print(result)\n            assert not result.errors\n            assert result.data[\"pipelineLogs\"][\"id\"] == str(mock_pipeline2.id)\n            assert result.data[\"pipelineLogs\"][\"taskId\"] == str(mock_pipeline2.task_id)", "@pytest.mark.usefixtures('celery_session_worker')\nclass TestSchemaSubscriptions:\n    @pytest.mark.asyncio\n    async def test_pipeline(self, mock_info_context, mock_pipeline):\n        \"\"\"Requires Redis to run.\n        \"\"\"\n\n        query = \"\"\"\n    \t  subscription {\n          \tpipeline(id:\"\"\"+ '\"' + str(mock_pipeline.id) + '\"' + \"\"\") {\n              id\n              taskId\n              status\n              result\n              timestamp\n              traceback\n            }\n    \t  }\n        \"\"\"\n\n        sub = await schema.subscribe(query)\n\n        async for result in sub:\n            assert not result.errors\n\n    @pytest.mark.asyncio\n    async def test_pipeline_logs(self, mock_info_context, mock_pipeline, mock_pipeline2):\n        \"\"\"Requires Redis to run.\n        This test runs two pipelines simultaneously to ensure logs messages are scoped\n        to the correct pipeline.\n        \n        \"\"\"\n\n        query = \"\"\"\n    \t  subscription {\n          \tpipelineLogs(id:\"\"\"+ '\"' + str(mock_pipeline.id) + '\"' + \"\"\") {\n              id\n              message\n              messageId\n              taskId\n              time\n            }\n    \t  }\n        \"\"\"\n\n        sub = await schema.subscribe(query)\n\n        async for result in sub:\n            #print(result)\n            assert not result.errors\n            assert result.data[\"pipelineLogs\"][\"id\"] == str(mock_pipeline.id)\n            assert result.data[\"pipelineLogs\"][\"taskId\"] == str(mock_pipeline.task_id)\n\n\n        query2 = \"\"\"\n    \t  subscription {\n          \tpipelineLogs(id:\"\"\"+ '\"' + str(mock_pipeline2.id) + '\"' + \"\"\") {\n              id\n              message\n              messageId\n              taskId\n              time\n            }\n    \t  }\n        \"\"\"\n\n        sub2 = await schema.subscribe(query2)\n\n        async for result in sub2:\n            #print(result)\n            assert not result.errors\n            assert result.data[\"pipelineLogs\"][\"id\"] == str(mock_pipeline2.id)\n            assert result.data[\"pipelineLogs\"][\"taskId\"] == str(mock_pipeline2.task_id)"]}
{"filename": "src/tests/conftest.py", "chunked_list": ["import pytest\nfrom pathlib import Path\nfrom kedro.framework.project import settings\nfrom kedro.config import ConfigLoader\nfrom kedro.framework.context import KedroContext\nfrom kedro.framework.hooks import _create_hook_manager\nfrom kedro_graphql.backends import init_backend\nfrom kedro_graphql.tasks import run_pipeline\nfrom kedro_graphql.models import Pipeline, DataSet, Parameter, Tag\nfrom unittest.mock import patch", "from kedro_graphql.models import Pipeline, DataSet, Parameter, Tag\nfrom unittest.mock import patch\n\n\n\n@pytest.fixture(scope=\"session\")\ndef config_loader():\n    return ConfigLoader(conf_source=str(Path.cwd() / settings.CONF_SOURCE))\n\n", "\n\n@pytest.fixture(scope='session')\ndef project_context(config_loader):\n    return KedroContext(\n        package_name=\"kedro_graphql\",\n        project_path=Path.cwd(),\n        config_loader=config_loader,\n        hook_manager=_create_hook_manager(),\n    )", "\n@pytest.fixture(scope='session')\ndef celery_config():\n    return {\n        'broker_url': 'redis://',\n        'result_backend': 'redis://',\n        'result_extened': True,\n        'worker_send_task_events': True,\n        'task_send_sent_event': True,\n        'task_store_eager_result': True,\n        'task_always_eager': False,\n        'task_ignore_result': False,\n        'imports': [\"kedro_graphql.config\", \"kedro_graphql.tasks\"]\n\n    }", "\n\n@pytest.fixture(scope=\"session\")\ndef celery_worker_parameters():\n    return {\"without_heartbeat\": False}\n\n@pytest.fixture\ndef mock_backend():\n    return init_backend()\n", "\n@pytest.fixture\ndef mock_info_context(mock_backend):\n    class App():\n        backend = mock_backend\n\n    class Request():\n        app = App()\n\n    with patch(\"strawberry.types.Info.context\", {\"request\": Request()}) as m:\n        yield m", "\n\n## refer to https://docs.pytest.org/en/7.1.x/how-to/tmp_path.html for info on tmp_path fixture\n@pytest.fixture\ndef mock_text_in(tmp_path):\n    #tmp_path.mkdir()\n    text_in = tmp_path / \"text_in.txt\"\n    text_in.write_text(\"hello\")\n    return text_in\n", "\n@pytest.fixture\ndef mock_text_out(tmp_path):\n    #tmp_path.mkdir()\n    text_out = tmp_path / \"text_out.txt\"\n    text_out.write_text(\"good bye\")\n    return text_out\n\n@pytest.fixture\ndef mock_text_in_tsv(tmp_path):\n    #tmp_path.mkdir()\n    text = tmp_path / \"text_in.tsv\"\n    text.write_text(\"Some parameter\\tOther parameter\\tLast parameter\\nCONST\\t123456\\t12.45\")\n    return text", "@pytest.fixture\ndef mock_text_in_tsv(tmp_path):\n    #tmp_path.mkdir()\n    text = tmp_path / \"text_in.tsv\"\n    text.write_text(\"Some parameter\\tOther parameter\\tLast parameter\\nCONST\\t123456\\t12.45\")\n    return text\n\n@pytest.fixture\ndef mock_text_out_tsv(tmp_path):\n    #tmp_path.mkdir()\n    text = tmp_path / \"text_out.tsv\"\n    text.write_text(\"Some parameter\\tOther parameter\\tLast parameter\\nCONST\\t123456\\t12.45\")\n    return text", "def mock_text_out_tsv(tmp_path):\n    #tmp_path.mkdir()\n    text = tmp_path / \"text_out.tsv\"\n    text.write_text(\"Some parameter\\tOther parameter\\tLast parameter\\nCONST\\t123456\\t12.45\")\n    return text\n\n@pytest.fixture\ndef mock_pipeline(mock_backend, tmp_path, mock_text_in, mock_text_out):\n\n    inputs = [{\"name\": \"text_in\", \"type\": \"text.TextDataSet\", \"filepath\": str(mock_text_in)}]\n    outputs = [{\"name\":\"text_out\", \"type\": \"text.TextDataSet\", \"filepath\": str(mock_text_out)}]\n    parameters = [{\"name\":\"example\", \"value\":\"hello\"}]\n    tags = [{\"key\": \"author\", \"value\": \"opensean\"},{\"key\":\"package\", \"value\":\"kedro-graphql\"}]\n\n    p = Pipeline(\n        name = \"example00\",\n        inputs = [DataSet(**i) for i in inputs],\n        outputs = [DataSet(**o) for o in outputs],\n        parameters = [Parameter(**p) for p in parameters],\n        tags = [Tag(**p) for p in tags],\n        task_name = str(run_pipeline),\n    )\n\n    serial = p.serialize()\n\n    result = run_pipeline.apply_async(kwargs = {\"name\": \"example00\", \n                                                 \"inputs\": serial[\"inputs\"], \n                                                 \"outputs\": serial[\"outputs\"],\n                                                 \"parameters\": serial[\"parameters\"]}, countdown=0.1)\n    p.task_id = result.id\n    p.status = result.status\n    p.task_kwargs = str(\n            {\"name\": serial[\"name\"], \n            \"inputs\": serial[\"inputs\"], \n            \"outputs\": serial[\"outputs\"], \n            \"parameters\": serial[\"parameters\"]}\n    )\n\n    print(f'Starting {p.name} pipeline with task_id: ' + str(p.task_id))\n    p = mock_backend.create(p)\n    return p", "\n@pytest.fixture\ndef mock_pipeline2(mock_backend, tmp_path, mock_text_in, mock_text_out):\n\n    inputs = [{\"name\": \"text_in\", \"type\": \"text.TextDataSet\", \"filepath\": str(mock_text_in)}]\n    outputs = [{\"name\":\"text_out\", \"type\": \"text.TextDataSet\", \"filepath\": str(mock_text_out)}]\n    parameters = [{\"name\":\"example\", \"value\":\"hello\"}]\n    tags = [{\"key\": \"author\", \"value\": \"opensean\"},{\"key\":\"package\", \"value\":\"kedro-graphql\"}]\n\n    p = Pipeline(\n        name = \"example00\",\n        inputs = [DataSet(**i) for i in inputs],\n        outputs = [DataSet(**o) for o in outputs],\n        parameters = [Parameter(**p) for p in parameters],\n        tags = [Tag(**p) for p in tags],\n        task_name = str(run_pipeline),\n    )\n\n    serial = p.serialize()\n\n    result = run_pipeline.apply_async(kwargs = {\"name\": \"example00\", \n                                                 \"inputs\": serial[\"inputs\"], \n                                                 \"outputs\": serial[\"outputs\"],\n                                                 \"parameters\": serial[\"parameters\"]}, countdown=0.1)\n    p.task_id = result.id\n    p.status = result.status\n    p.task_kwargs = str(\n            {\"name\": serial[\"name\"], \n            \"inputs\": serial[\"inputs\"], \n            \"outputs\": serial[\"outputs\"], \n            \"parameters\": serial[\"parameters\"]}\n    )\n\n    print(f'Starting {p.name} pipeline with task_id: ' + str(p.task_id))\n    p = mock_backend.create(p)\n    return p"]}
{"filename": "src/tests/pipelines/__init__.py", "chunked_list": [""]}
{"filename": "src/tests/pipelines/example00/__init__.py", "chunked_list": [""]}
{"filename": "src/tests/pipelines/example00/test_pipeline.py", "chunked_list": ["\"\"\"\nThis is a boilerplate test file for pipeline 'example00'\ngenerated using Kedro 0.18.4.\nPlease add your pipeline tests here.\n\nKedro recommends using `pytest` framework, more info about it can be found\nin the official documentation:\nhttps://docs.pytest.org/en/latest/getting-started.html\n\"\"\"\n", "\"\"\"\n"]}
{"filename": "src/kedro_graphql/decorators.py", "chunked_list": ["import os\nfrom .config import RESOLVER_PLUGINS\nfrom .config import TYPE_PLUGINS\n\nclass NameConflictError(BaseException):\n    \"\"\"Raise for errors in adding plugins do to the same name.\"\"\"\n\ndef gql_resolver(name):\n    \"\"\"\n    \"\"\"\n\n    if name in RESOLVER_PLUGINS:\n        raise NameConflictError(\n            f\"Plugin name conflict: '{name}'. Double check\" \\\n            \" that all plugins have unique names.\"\n        )\n    def register_plugin(plugin_class):\n        plugin = plugin_class()\n        RESOLVER_PLUGINS[name] = plugin\n        print(\"registered resolver plugin\",\"'\" + name + \"'\", RESOLVER_PLUGINS[name])\n        return plugin\n\n    return register_plugin", "\ndef gql_query():\n    \"\"\"\n    \"\"\"\n    ## raise warning if same class is registered twice?\n    ##if type not in TYPE_PLUGINS.keys():\n    ##    raise KeyError(\n    ##        f\"Type plugin error: '{type}', must be one of ['query', 'mutation', or 'subscription']\"\n    ##    )\n\n    def register_plugin(plugin_class):\n        TYPE_PLUGINS[\"query\"].append(plugin_class)\n        print(\"registered type plugin 'query':\", plugin_class)\n        return plugin_class\n\n    return register_plugin", "\ndef gql_mutation():\n    \"\"\"\n    \"\"\"\n    ## raise warning if same class is registered twice?\n    ##if type not in TYPE_PLUGINS.keys():\n    ##    raise KeyError(\n    ##        f\"Type plugin error: '{type}', must be one of ['query', 'mutation', or 'subscription']\"\n    ##    )\n\n    def register_plugin(plugin_class):\n        TYPE_PLUGINS[\"mutation\"].append(plugin_class)\n        print(\"registered type plugin 'query':\", plugin_class)\n        return plugin_class\n\n    return register_plugin", "\ndef gql_subscription():\n    \"\"\"\n    \"\"\"\n    ## raise warning if same class is registered twice?\n    ##if type not in TYPE_PLUGINS.keys():\n    ##    raise KeyError(\n    ##        f\"Type plugin error: '{type}', must be one of ['query', 'mutation', or 'subscription']\"\n    ##    )\n\n    def register_plugin(plugin_class):\n        TYPE_PLUGINS[\"subscription\"].append(plugin_class)\n        print(\"registered type plugin 'query':\", plugin_class)\n        return plugin_class\n\n    return register_plugin"]}
{"filename": "src/kedro_graphql/schema.py", "chunked_list": ["from .config import PIPELINES, TYPE_PLUGINS\nfrom .events import PipelineEventMonitor\nimport strawberry\nfrom strawberry.tools import merge_types\nfrom strawberry.types import Info\nfrom typing import AsyncGenerator, List\nfrom .celeryapp import app as APP_CELERY\nfrom .tasks import run_pipeline\nfrom .models import Parameter, DataSet, Pipeline, PipelineInput, PipelineEvent, PipelineLogMessage, PipelineTemplate, Tag\nfrom .logs.logger import logger, PipelineLogStream", "from .models import Parameter, DataSet, Pipeline, PipelineInput, PipelineEvent, PipelineLogMessage, PipelineTemplate, Tag\nfrom .logs.logger import logger, PipelineLogStream\nfrom fastapi.encoders import jsonable_encoder\n\n@strawberry.type\nclass Query:\n    @strawberry.field\n    def pipeline_templates(self) -> List[PipelineTemplate]:\n        pipes = []\n        for k,v in PIPELINES.items():\n            pipes.append(PipelineTemplate(name = k))\n        return pipes\n\n    @strawberry.field\n    def pipeline(self, id: str, info: Info) -> Pipeline:\n        return info.context[\"request\"].app.backend.load(id)", "\n\n@strawberry.type\nclass Mutation:\n    @strawberry.mutation\n    def pipeline(self, pipeline: PipelineInput, info: Info) -> Pipeline:\n        \"\"\"\n        - is validation against template needed, e.g. check DataSet type or at least check dataset names\n        \"\"\"\n        d = jsonable_encoder(pipeline)\n        p = Pipeline.from_dict(d)\n        p.task_name = str(run_pipeline)\n\n        serial = p.serialize()\n\n        result = run_pipeline.delay(\n            name = serial[\"name\"], \n            inputs = serial[\"inputs\"], \n            outputs = serial[\"outputs\"], \n            parameters = serial[\"parameters\"]\n        )  \n\n        p.task_id = result.id\n        p.status = result.status\n        p.task_kwargs = str(\n                {\"name\": serial[\"name\"], \n                \"inputs\": serial[\"inputs\"], \n                \"outputs\": serial[\"outputs\"], \n                \"parameters\": serial[\"parameters\"]}\n        )\n        \n        ## PLACE HOLDER for future reolver plugins\n        ## testing plugin_resolvers, \n        #RESOLVER_PLUGINS[\"text_in\"].__input__(\"called text_in resolver\")\n\n        logger.info(f'Starting {p.name} pipeline with task_id: ' + str(p.task_id))\n        p = info.context[\"request\"].app.backend.create(p)\n        return p", "\n\n@strawberry.type\nclass Subscription:\n    @strawberry.subscription\n    async def pipeline(self, id: str, info: Info, interval: float = 0.5) -> AsyncGenerator[PipelineEvent, None]:\n        \"\"\"Subscribe to pipeline events.\n        \"\"\"\n        p  = info.context[\"request\"].app.backend.load(id=id)\n        if p:\n            async for e in PipelineEventMonitor(app = APP_CELERY, task_id = p.task_id).start(interval=interval):\n                e[\"id\"] = id\n                yield PipelineEvent(**e)\n\n    @strawberry.subscription\n    async def pipeline_logs(self, id: str, info: Info) -> AsyncGenerator[PipelineLogMessage, None]:\n        p  = info.context[\"request\"].app.backend.load(id=id)\n        if p:\n            stream = await PipelineLogStream().create(task_id = p.task_id )\n            async for e in stream.consume():\n                e[\"id\"] = id\n                yield PipelineLogMessage(**e)", "\n\ndef build_schema():\n    ComboQuery = merge_types(\"Query\", tuple([Query] + TYPE_PLUGINS[\"query\"]))\n    ComboMutation = merge_types(\"Mutation\", tuple([Mutation] + TYPE_PLUGINS[\"mutation\"]))\n    ComboSubscription = merge_types(\"Subscription\", tuple([Subscription] + TYPE_PLUGINS[\"subscription\"]))\n    \n    return strawberry.Schema(query=ComboQuery, mutation=ComboMutation, subscription=ComboSubscription)\n", ""]}
{"filename": "src/kedro_graphql/settings.py", "chunked_list": ["\"\"\"Project settings. There is no need to edit this file unless you want to change values\nfrom the Kedro defaults. For further information, including these default values, see\nhttps://kedro.readthedocs.io/en/stable/kedro_project_setup/settings.html.\"\"\"\n\n# Instantiated project hooks.\n# from kedro_graphql.hooks import ProjectHooks\n# HOOKS = (ProjectHooks(),)\n\n# Installed plugins for which to disable hook auto-registration.\n# DISABLE_HOOKS_FOR_PLUGINS = (\"kedro-viz\",)", "# Installed plugins for which to disable hook auto-registration.\n# DISABLE_HOOKS_FOR_PLUGINS = (\"kedro-viz\",)\n\n# Class that manages storing KedroSession data.\n# from kedro.framework.session.shelvestore import ShelveStore\n# SESSION_STORE_CLASS = ShelveStore\n# Keyword arguments to pass to the `SESSION_STORE_CLASS` constructor.\n# SESSION_STORE_ARGS = {\n#     \"path\": \"./sessions\"\n# }", "#     \"path\": \"./sessions\"\n# }\n\n# Class that manages Kedro's library components.\n# from kedro.framework.context import KedroContext\n# CONTEXT_CLASS = KedroContext\n\n# Directory that holds configuration.\n# CONF_SOURCE = \"conf\"\n", "# CONF_SOURCE = \"conf\"\n\n# Class that manages how configuration is loaded.\n# CONFIG_LOADER_CLASS = ConfigLoader\n# Keyword arguments to pass to the `CONFIG_LOADER_CLASS` constructor.\n# CONFIG_LOADER_ARGS = {\n#       \"config_patterns\": {\n#           \"spark\" : [\"spark*/\"],\n#           \"parameters\": [\"parameters*\", \"parameters*/**\", \"**/parameters*\"],\n#       }", "#           \"parameters\": [\"parameters*\", \"parameters*/**\", \"**/parameters*\"],\n#       }\n# }\n\n# Class that manages the Data Catalog.\n# from kedro.io import DataCatalog\n# DATA_CATALOG_CLASS = DataCatalog\n"]}
{"filename": "src/kedro_graphql/events.py", "chunked_list": ["import asyncio\nfrom queue import Queue\nfrom queue import Empty as QueueEmptyException\nfrom threading import Thread\nimport logging\nfrom typing import AsyncGenerator\nimport time\nfrom celery.states import READY_STATES, EXCEPTION_STATES\n\nlogger = logging.getLogger(\"kedro-graphql\")", "\nlogger = logging.getLogger(\"kedro-graphql\")\n\nclass PipelineEventMonitor:\n    def __init__(self, app = None, task_id = None, timeout = 1):\n        \"\"\"\n        Kwargs:\n            app (Celery): celery application instance.\n            uuid (str): a celery task id.\n            timeout (float): See https://docs.python.org/3/library/queue.html#queue.Queue.get\n        \"\"\"\n        self.task_id = task_id\n        self.app = app\n        self.timeout = timeout\n\n    @staticmethod\n    def _task_event_receiver(app, queue, task_id):\n        \"\"\"\n        Recieves task events from backend broker and puts them in a \n        Queue.  Incoming tasks are filtered and only tasks with a \n        root_id or uuid matching the provided id are put in the Queue.\n\n        Example event payloads:\n    \n        {'hostname': 'gen36975@alligator', 'utcoffset': 5, 'pid': 36975, 'clock': 7864, 'uuid': 'd8253d45-ce28-4719-b2ba-8e266dfdaf04', 'root_id': 'd8253d45-ce28-4719-b2ba-8e266dfdaf04', 'parent_id': None, 'name': 'kedro_graphql.tasks.run_pipeline', 'args': '()', 'kwargs': \"{'name': 'example00', 'inputs': {'text_in': {'type': 'text.TextDataSet', 'filepath': './data/01_raw/text_in.txt'}}, 'outputs': {'text_out': {'type': 'text.TextDataSet', 'filepath': './data/02_intermediate/text_out.txt'}}}\", 'retries': 0, 'eta': None, 'expires': None, 'queue': 'celery', 'exchange': '', 'routing_key': 'celery', 'timestamp': 1672860581.1371481, 'type': 'task-sent', 'local_received': 1672860581.138474}\n        {'hostname': 'celery@alligator', 'utcoffset': 5, 'pid': 37029, 'clock': 7867, 'uuid': 'd8253d45-ce28-4719-b2ba-8e266dfdaf04', 'timestamp': 1672860581.1411166, 'type': 'task-started', 'local_received': 1672860581.144976}\n        {'hostname': 'celery@alligator', 'utcoffset': 5, 'pid': 37029, 'clock': 7870, 'uuid': 'd8253d45-ce28-4719-b2ba-8e266dfdaf04', 'result': \"'success'\", 'runtime': 2.013245126003312, 'timestamp': 1672860583.1549191, 'type': 'task-succeeded', 'local_received': 1672860583.158338}\n        \n        Args:\n            app (Celery): celery application instance.\n            queue (Queue):  a python queue.Queue.\n            task_id (str):  celery task id.\n        \n        \"\"\"\n        \n        def process_tasks(event):\n            if event.get(\"root_id\", \"\") == task_id or event.get(\"uuid\") == task_id:    \n                queue.put(event)\n    \n    \n        with app.connection() as connection:\n            recv = app.events.Receiver(connection, handlers={\n                    'task-sent': process_tasks,\n                    'task-recieved': process_tasks,\n                    'task-started': process_tasks,\n                    'task-succeeded': process_tasks,\n                    'task-failed': process_tasks,\n                    'task-rejected': process_tasks,\n                    'task-revoked': process_tasks,\n                    'task-retried': process_tasks\n            })\n    \n            recv.capture(limit=None, timeout=None, wakeup=True)\n    \n    def _start_task_event_receiver_thread(self, queue):\n        \"\"\"\n        Start the task event receiver in a thread.\n\n        Args:\n            queue (Queue): a python queue.Queue.\n\n        Returns:\n            worker (threading.Thread): a python thread object.\n\n        \"\"\"\n        worker = Thread(target=self._task_event_receiver, args=(self.app,queue,self.task_id))\n        worker.daemon = True\n        worker.start()\n        logger.info(\"started event reciever thread\")\n        return worker\n    \n    async def consume(self) -> AsyncGenerator[dict, None]:\n        \"\"\"\n        \"\"\"\n        q = Queue()\n    \n        event_thread = self._start_task_event_receiver_thread(q)\n        ## https://docs.celeryq.dev/en/stable/reference/celery.events.state.html#module-celery.events.state\n        state = self.app.events.State()\n    \n        while True:\n            try:\n                event = q.get(timeout = self.timeout)\n                state.event(event)\n                # task name is sent only with -received event, and state\n                # will keep track of this for us.\n                task = state.tasks.get(event['uuid'])\n                yield {\"task_id\": task.id, \"status\": task.state, \"result\": task.result, \"timestamp\": task.timestamp, \"traceback\": task.traceback}\n                q.task_done()\n                if task.state in READY_STATES:\n                    break\n            except QueueEmptyException:\n                if self.app.AsyncResult(self.task_id).status in READY_STATES:\n                    break\n                else:\n                    continue\n\n        event_thread.join(timeout = 0.1)\n          \n    async def start(self, interval = 0.5) -> AsyncGenerator[dict, None]:\n        \"\"\"\n        A simplified but fully async version of the PipelineEventMonitor().consume() method.\n        \n        The PipelineEventMonitor.consume() method relies on celery's native\n        real time event processing approach which is syncronous and blocking.\n        https://docs.celeryq.dev/en/stable/userguide/monitoring.html#real-time-processing\n        \n        \"\"\"\n\n        while True:\n            task = self.app.AsyncResult(self.task_id)\n            yield {\"task_id\": task.id, \"status\": task.state, \"result\": task.result, \"timestamp\": time.time(), \"traceback\": task.traceback}\n            if self.app.AsyncResult(self.task_id).status in READY_STATES:\n                break\n            await asyncio.sleep(interval)", ""]}
{"filename": "src/kedro_graphql/__main__.py", "chunked_list": ["\"\"\"kedro-graphql file for ensuring the package is executable\nas `kedro-graphql` and `python -m kedro_graphql`\n\"\"\"\nimport importlib\nfrom pathlib import Path\n\nfrom kedro.framework.cli.utils import KedroCliError, load_entry_points\nfrom kedro.framework.project import configure_project\n\n\ndef _find_run_command(package_name):\n    try:\n        project_cli = importlib.import_module(f\"{package_name}.cli\")\n        # fail gracefully if cli.py does not exist\n    except ModuleNotFoundError as exc:\n        if f\"{package_name}.cli\" not in str(exc):\n            raise\n        plugins = load_entry_points(\"project\")\n        run = _find_run_command_in_plugins(plugins) if plugins else None\n        if run:\n            # use run command from installed plugin if it exists\n            return run\n        # use run command from `kedro.framework.cli.project`\n        from kedro.framework.cli.project import run\n\n        return run\n    # fail badly if cli.py exists, but has no `cli` in it\n    if not hasattr(project_cli, \"cli\"):\n        raise KedroCliError(f\"Cannot load commands from {package_name}.cli\")\n    return project_cli.run", "\n\ndef _find_run_command(package_name):\n    try:\n        project_cli = importlib.import_module(f\"{package_name}.cli\")\n        # fail gracefully if cli.py does not exist\n    except ModuleNotFoundError as exc:\n        if f\"{package_name}.cli\" not in str(exc):\n            raise\n        plugins = load_entry_points(\"project\")\n        run = _find_run_command_in_plugins(plugins) if plugins else None\n        if run:\n            # use run command from installed plugin if it exists\n            return run\n        # use run command from `kedro.framework.cli.project`\n        from kedro.framework.cli.project import run\n\n        return run\n    # fail badly if cli.py exists, but has no `cli` in it\n    if not hasattr(project_cli, \"cli\"):\n        raise KedroCliError(f\"Cannot load commands from {package_name}.cli\")\n    return project_cli.run", "\n\ndef _find_run_command_in_plugins(plugins):\n    for group in plugins:\n        if \"run\" in group.commands:\n            return group.commands[\"run\"]\n\n\ndef main(*args, **kwargs):\n    package_name = Path(__file__).parent.name\n    configure_project(package_name)\n    run = _find_run_command(package_name)\n    run(*args, **kwargs)", "def main(*args, **kwargs):\n    package_name = Path(__file__).parent.name\n    configure_project(package_name)\n    run = _find_run_command(package_name)\n    run(*args, **kwargs)\n\n\nif __name__ == \"__main__\":\n    main()\n", ""]}
{"filename": "src/kedro_graphql/pipeline_registry.py", "chunked_list": ["\"\"\"Project pipelines.\"\"\"\nfrom typing import Dict\n\nfrom kedro.framework.project import find_pipelines\nfrom kedro.pipeline import Pipeline\n\n\ndef register_pipelines() -> Dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    pipelines = find_pipelines()\n    pipelines[\"__default__\"] = sum(pipelines.values())\n    return pipelines", ""]}
{"filename": "src/kedro_graphql/tasks.py", "chunked_list": ["from kedro.framework.project import pipelines\nfrom kedro.io import DataCatalog\nfrom kedro.runner import SequentialRunner\nfrom celery import shared_task, Task\nfrom .backends import init_backend\nfrom fastapi.encoders import jsonable_encoder\n\nclass KedroGraphqlTask(Task):\n    _db = None\n\n    @property\n    def db(self):\n        if self._db is None:\n            self._db = init_backend()\n        return self._db\n\n    def before_start(self, task_id, args, kwargs):\n        \"\"\"Handler called before the task starts.\n\n        .. versionadded:: 5.2\n\n        Arguments:\n            task_id (str): Unique id of the task to execute.\n            args (Tuple): Original arguments for the task to execute.\n            kwargs (Dict): Original keyword arguments for the task to execute.\n\n        Returns:\n            None: The return value of this handler is ignored.\n        \"\"\"\n        self.db.update(task_id = task_id, values = {\"status\": \"STARTED\"})\n\n    def on_success(self, retval, task_id, args, kwargs):\n        \"\"\"Success handler.\n\n        Run by the worker if the task executes successfully.\n\n        Arguments:\n            retval (Any): The return value of the task.\n            task_id (str): Unique id of the executed task.\n            args (Tuple): Original arguments for the executed task.\n            kwargs (Dict): Original keyword arguments for the executed task.\n\n        Returns:\n            None: The return value of this handler is ignored.\n        \"\"\"\n        self.db.update(task_id = task_id, values = {\"status\": \"SUCCESS\"})\n\n\n    def on_retry(self, exc, task_id, args, kwargs, einfo):\n        \"\"\"Retry handler.\n\n        This is run by the worker when the task is to be retried.\n\n        Arguments:\n            exc (Exception): The exception sent to :meth:`retry`.\n            task_id (str): Unique id of the retried task.\n            args (Tuple): Original arguments for the retried task.\n            kwargs (Dict): Original keyword arguments for the retried task.\n            einfo (~billiard.einfo.ExceptionInfo): Exception information.\n\n        Returns:\n            None: The return value of this handler is ignored.\n        \"\"\"\n        self.db.update(task_id = task_id, values = {\"status\": \"RETRY\", \"task_exception\": str(exc), \"task_einfo\": str(einfo)})\n   \n    def on_failure(self, exc, task_id, args, kwargs, einfo):\n        \"\"\"Error handler.\n\n        This is run by the worker when the task fails.\n\n        Arguments:\n            exc (Exception): The exception raised by the task.\n            task_id (str): Unique id of the failed task.\n            args (Tuple): Original arguments for the task that failed.\n            kwargs (Dict): Original keyword arguments for the task that failed.\n            einfo (~billiard.einfo.ExceptionInfo): Exception information.\n\n        Returns:\n            None: The return value of this handler is ignored.\n        \"\"\"\n        self.db.update(task_id = task_id, values = {\"status\": \"FAILURE\", \"task_exception\": str(exc), \"task_einfo\": str(einfo)})\n\n    def after_return(self, status, retval, task_id, args, kwargs, einfo):\n        \"\"\"Handler called after the task returns.\n\n        Arguments:\n            status (str): Current task state.\n            retval (Any): Task return value/exception.\n            task_id (str): Unique id of the task.\n            args (Tuple): Original arguments for the task.\n            kwargs (Dict): Original keyword arguments for the task.\n            einfo (~billiard.einfo.ExceptionInfo): Exception information.\n\n        Returns:\n            None: The return value of this handler is ignored.\n        \"\"\"\n        self.db.update(task_id = task_id, values = {\"status\": status, \"task_einfo\": str(einfo)})", "\n\n@shared_task(bind = True, base = KedroGraphqlTask)\ndef run_pipeline(self, name: str, inputs: dict, outputs: dict, parameters: dict):\n    catalog = {**inputs, **outputs}\n    io = DataCatalog().from_config(catalog = catalog)\n\n    ## add parameters to DataCatalog e.g. {\"params:myparam\":\"value\"}\n    params = {\"params:\"+k:v for k,v in parameters.items()}\n    params[\"parameters\"] = parameters\n    io.add_feed_dict(params)\n\n    SequentialRunner().run(pipelines[name], catalog = io)\n    \n    return \"success\""]}
{"filename": "src/kedro_graphql/config.py", "chunked_list": ["from kedro.framework.startup import bootstrap_project\nfrom kedro.framework.session import KedroSession\nfrom kedro.framework.project import pipelines as PIPELINES\nfrom pathlib import Path\nfrom dotenv import dotenv_values\nimport os\nfrom importlib import import_module\n\n## pyproject.toml located in parent directory, required for project config\nproject_path = Path.cwd()", "## pyproject.toml located in parent directory, required for project config\nproject_path = Path.cwd()\nmetadata = bootstrap_project(project_path)\nsession = KedroSession.create(metadata.package_name, project_path=project_path)\ncontext = session.load_context()\n\nconf_catalog = context.config_loader[\"catalog\"]\nconf_parameters = context.config_loader[\"parameters\"]\n\n## define defaults", "\n## define defaults\nconfig = {\n    \"MONGO_URI\": \"mongodb://root:example@localhost:27017/\",\n    \"MONGO_DB_NAME\": \"pipelines\",\n    \"KEDRO_GRAPHQL_IMPORTS\": \"kedro_graphql.plugins.plugins,\",\n    \"KEDRO_GRAPHQL_APP\": \"kedro_graphql.asgi.KedroGraphQL\",\n    \"KEDRO_GRAPHQL_BACKEND\": \"kedro_graphql.backends.mongodb.MongoBackend\",\n    \"KEDRO_GRAPHQL_BROKER\": \"redis://localhost\",\n    \"KEDRO_GRAPHQL_CELERY_RESULT_BACKEND\": \"redis://localhost\",", "    \"KEDRO_GRAPHQL_BROKER\": \"redis://localhost\",\n    \"KEDRO_GRAPHQL_CELERY_RESULT_BACKEND\": \"redis://localhost\",\n    }\n\nload_config = {\n    **dotenv_values(\".env\"),  # load \n    **os.environ,  # override loaded values with environment variables\n}\n\n## override defaults", "\n## override defaults\nconfig.update(load_config)\n\n\nRESOLVER_PLUGINS = {}\nTYPE_PLUGINS = {\"query\":[],\n                \"mutation\":[],\n                \"subscription\":[]}\n\ndef discover_plugins():\n    ## discover plugins e.g. decorated functions e.g @gql_query, etc...\n    imports = [i.strip() for i in config[\"KEDRO_GRAPHQL_IMPORTS\"].split(\",\") if len(i.strip()) > 0]\n    for i in imports:\n        import_module(i)   ", "                \"subscription\":[]}\n\ndef discover_plugins():\n    ## discover plugins e.g. decorated functions e.g @gql_query, etc...\n    imports = [i.strip() for i in config[\"KEDRO_GRAPHQL_IMPORTS\"].split(\",\") if len(i.strip()) > 0]\n    for i in imports:\n        import_module(i)   "]}
{"filename": "src/kedro_graphql/celeryapp.py", "chunked_list": ["from celery import Celery\nfrom .config import config\n\napp = Celery()\n\nclass Config:\n    broker_url = config[\"KEDRO_GRAPHQL_BROKER\"]\n    result_backend = config[\"KEDRO_GRAPHQL_CELERY_RESULT_BACKEND\"]\n    result_extended = True\n    task_serializer = 'json'\n    result_serializer = 'json'\n    accept_content = ['json']\n    enable_utc = True\n    worker_send_task_events = True\n    task_send_sent_event = True\n    imports = \"kedro_graphql.tasks\"", "\napp.config_from_object(Config)"]}
{"filename": "src/kedro_graphql/models.py", "chunked_list": ["import strawberry\nfrom enum import Enum\nfrom typing import List, Optional\nimport uuid\nfrom .config import conf_catalog, conf_parameters, PIPELINES\n\n@strawberry.type\nclass Tag:\n    key: str\n    value: str", "\n@strawberry.input\nclass TagInput:\n    key: str\n    value: str\n\n@strawberry.enum\nclass ParameterType(Enum):\n    STRING = \"string\"\n    BOOLEAN = \"boolean\"\n    INTEGER = \"integer\"\n    FLOAT = \"float\"", "\n\n@strawberry.type\nclass Parameter:\n    name: str\n    value: str\n    type: Optional[ParameterType] = ParameterType.STRING\n\n    def serialize(self) -> dict:\n        \"\"\"\n        Returns serializable dict in format compatible with kedro.\n        \"\"\"\n        value = self.value\n        if self.type == \"boolean\":\n            value = self.value.lower()\n            if value == \"true\":\n                value = True\n            elif value == \"false\":\n                value = False\n            else:\n                raise ValueError(\"Parameter of type BOOL must be one of 'True', 'true', 'False', or 'false'\")\n\n        elif self.type == \"integer\":\n            value = int(self.value)\n\n        elif self.type == \"float\":\n            value = float(self.value)\n\n        return {self.name: value}", "\n@strawberry.input\nclass ParameterInput:\n    name: str\n    value: str\n    type: Optional[ParameterType] = ParameterType.STRING\n\n@strawberry.type\nclass DataSet:\n    name: str\n    type: str\n    filepath: str\n    save_args: Optional[List[Parameter]] = None\n    load_args: Optional[List[Parameter]] = None\n\n    def serialize(self) -> dict:\n        \"\"\"\n        Returns serializable dict in format compatible with kedro.\n        \"\"\"\n        temp = self.__dict__.copy()\n        temp.pop(\"name\")\n        if not temp[\"save_args\"]:\n            temp.pop(\"save_args\")\n        else:\n            temp[\"save_args\"] = {k:v for p in self.__dict__[\"save_args\"] for k,v in p.serialize().items() }\n        if not temp[\"load_args\"]:\n            temp.pop(\"load_args\")\n        else:\n            temp[\"load_args\"] = {k:v for p in self.__dict__[\"save_args\"] for k,v in p.serialize().items() }\n        return {self.name: temp}\n\n    @staticmethod\n    def from_dict(payload):\n        \"\"\"\n        Return a new DataSet from a dictionary.\n\n        Args:\n            payload (dict): dict representing DataSet e.g.\n\n                {\n                  \"name\": \"text_in\",\n                  \"filepath\": \"./data/01_raw/text_in.txt\",\n                  \"type\": \"text.TextDataSet\",\n                  \"save_args\":[{\"name\": \"say\", \"value\": \"hello\"}],\n                  \"load_args\":[{\"name\": \"say\", \"value\": \"hello\"}]\n                }\n\n        \"\"\"\n\n        if payload.get(\"save_args\", False):\n            save_args = [Parameter(**p) for p in payload[\"save_args\"]]\n        else:\n            save_args = None\n            \n        if payload.get(\"load_args\", False):\n            load_args = [Parameter(**p) for p in payload[\"load_args\"]]\n        else:\n            load_args = None\n\n        return DataSet(\n            name = payload[\"name\"],\n            type = payload[\"type\"],\n            filepath = payload[\"filepath\"],\n            save_args = save_args,\n            load_args = load_args\n        )", "class DataSet:\n    name: str\n    type: str\n    filepath: str\n    save_args: Optional[List[Parameter]] = None\n    load_args: Optional[List[Parameter]] = None\n\n    def serialize(self) -> dict:\n        \"\"\"\n        Returns serializable dict in format compatible with kedro.\n        \"\"\"\n        temp = self.__dict__.copy()\n        temp.pop(\"name\")\n        if not temp[\"save_args\"]:\n            temp.pop(\"save_args\")\n        else:\n            temp[\"save_args\"] = {k:v for p in self.__dict__[\"save_args\"] for k,v in p.serialize().items() }\n        if not temp[\"load_args\"]:\n            temp.pop(\"load_args\")\n        else:\n            temp[\"load_args\"] = {k:v for p in self.__dict__[\"save_args\"] for k,v in p.serialize().items() }\n        return {self.name: temp}\n\n    @staticmethod\n    def from_dict(payload):\n        \"\"\"\n        Return a new DataSet from a dictionary.\n\n        Args:\n            payload (dict): dict representing DataSet e.g.\n\n                {\n                  \"name\": \"text_in\",\n                  \"filepath\": \"./data/01_raw/text_in.txt\",\n                  \"type\": \"text.TextDataSet\",\n                  \"save_args\":[{\"name\": \"say\", \"value\": \"hello\"}],\n                  \"load_args\":[{\"name\": \"say\", \"value\": \"hello\"}]\n                }\n\n        \"\"\"\n\n        if payload.get(\"save_args\", False):\n            save_args = [Parameter(**p) for p in payload[\"save_args\"]]\n        else:\n            save_args = None\n            \n        if payload.get(\"load_args\", False):\n            load_args = [Parameter(**p) for p in payload[\"load_args\"]]\n        else:\n            load_args = None\n\n        return DataSet(\n            name = payload[\"name\"],\n            type = payload[\"type\"],\n            filepath = payload[\"filepath\"],\n            save_args = save_args,\n            load_args = load_args\n        )", "\n\n\n@strawberry.input\nclass DataSetInput:\n    name: str\n    type: str\n    filepath: str\n    save_args: Optional[List[ParameterInput]] = None\n    load_args: Optional[List[ParameterInput]] = None", "\n@strawberry.type\nclass Node:\n    name: str\n    inputs: List[str]\n    outputs: List[str]\n    tags: List[str]\n\n@strawberry.type(description = \"PipelineTemplates are definitions of Pipelines.  They represent the supported interface for executing a Pipeline.\")\nclass PipelineTemplate:\n    name: str\n\n    @strawberry.field\n    def describe(self) -> str:\n        return PIPELINES[self.name].describe()\n\n    @strawberry.field\n    def nodes(self) -> List[Node]:\n        nodes = PIPELINES[self.name].nodes\n\n        return [Node(name = n.name, inputs = n.inputs, outputs = n.outputs, tags = n.tags) for n in nodes]\n\n    @strawberry.field\n    def parameters(self) -> List[Parameter]:\n        ## keep track of parameters to avoid returning duplicates    \n        params = {}\n        for n in PIPELINES[self.name].inputs():\n            if n.startswith(\"params:\"):\n                name = n.split(\"params:\")[1]\n                value = conf_parameters[name]\n                if not params.get(name, False):\n                    params[name] = value\n            elif n == \"parameters\":\n                for k,v in conf_parameters.items():\n                    if not params.get(k, False):\n                        params[k] = v\n        return [Parameter(name = k, value = v) for k,v in params.items()]\n\n    @strawberry.field\n    def inputs(self) -> List[DataSet]:\n        inputs_resolved = []\n        for n in PIPELINES[self.name].inputs():\n            if not n.startswith(\"params:\") and n != \"parameters\":\n                config = conf_catalog[n]\n                inputs_resolved.append(DataSet(name = n, filepath = config[\"filepath\"], type = config[\"type\"], save_args = config.get(\"save_args\", None), load_args = config.get(\"load_args\", None)))\n            \n        return inputs_resolved\n \n    @strawberry.field\n    def outputs(self) -> List[DataSet]:\n        outputs_resolved = []\n        for n in PIPELINES[self.name].outputs():    \n            config = conf_catalog[n]\n            outputs_resolved.append(DataSet(name = n, filepath = config[\"filepath\"], type = config[\"type\"], save_args = config.get(\"save_args\", None), load_args = config.get(\"load_args\", None)))\n \n        return outputs_resolved", "@strawberry.type(description = \"PipelineTemplates are definitions of Pipelines.  They represent the supported interface for executing a Pipeline.\")\nclass PipelineTemplate:\n    name: str\n\n    @strawberry.field\n    def describe(self) -> str:\n        return PIPELINES[self.name].describe()\n\n    @strawberry.field\n    def nodes(self) -> List[Node]:\n        nodes = PIPELINES[self.name].nodes\n\n        return [Node(name = n.name, inputs = n.inputs, outputs = n.outputs, tags = n.tags) for n in nodes]\n\n    @strawberry.field\n    def parameters(self) -> List[Parameter]:\n        ## keep track of parameters to avoid returning duplicates    \n        params = {}\n        for n in PIPELINES[self.name].inputs():\n            if n.startswith(\"params:\"):\n                name = n.split(\"params:\")[1]\n                value = conf_parameters[name]\n                if not params.get(name, False):\n                    params[name] = value\n            elif n == \"parameters\":\n                for k,v in conf_parameters.items():\n                    if not params.get(k, False):\n                        params[k] = v\n        return [Parameter(name = k, value = v) for k,v in params.items()]\n\n    @strawberry.field\n    def inputs(self) -> List[DataSet]:\n        inputs_resolved = []\n        for n in PIPELINES[self.name].inputs():\n            if not n.startswith(\"params:\") and n != \"parameters\":\n                config = conf_catalog[n]\n                inputs_resolved.append(DataSet(name = n, filepath = config[\"filepath\"], type = config[\"type\"], save_args = config.get(\"save_args\", None), load_args = config.get(\"load_args\", None)))\n            \n        return inputs_resolved\n \n    @strawberry.field\n    def outputs(self) -> List[DataSet]:\n        outputs_resolved = []\n        for n in PIPELINES[self.name].outputs():    \n            config = conf_catalog[n]\n            outputs_resolved.append(DataSet(name = n, filepath = config[\"filepath\"], type = config[\"type\"], save_args = config.get(\"save_args\", None), load_args = config.get(\"load_args\", None)))\n \n        return outputs_resolved", "\n@strawberry.input(description = \"PipelineInput\")\nclass PipelineInput:\n    name: str\n    parameters: List[ParameterInput]\n    inputs: List[DataSetInput]\n    outputs: List[DataSetInput]\n    tags: Optional[List[TagInput]] = None\n\n@strawberry.type\nclass Pipeline:\n    id: Optional[uuid.UUID] = None\n    inputs: List[DataSet]\n    name: str\n    outputs: List[DataSet]\n    parameters: List[Parameter]\n    status: Optional[str] = None\n    tags: Optional[List[Tag]] = None\n    task_id: Optional[str] = None\n    task_name: Optional[str] = None\n    task_args: Optional[str] = None\n    task_kwargs: Optional[str] = None\n    task_request: Optional[str] = None\n    task_exception: Optional[str] = None\n    task_traceback: Optional[str] = None\n    task_einfo: Optional[str] = None\n    task_result: Optional[str] = None\n\n    @strawberry.field\n    def template(self) -> PipelineTemplate:\n        return PipelineTemplate(name = self.name)\n\n    @strawberry.field\n    def describe(self) -> str:\n        return self.template().describe()\n\n    @strawberry.field\n    def nodes(self) -> List[Node]:\n        return self.template().nodes()\n\n    def serialize(self):\n        inputs = {}\n        outputs = {}\n        parameters = {}\n        for i in self.inputs:\n            s = i.serialize()\n            inputs.update(s)\n\n        for o in self.outputs:\n            s = o.serialize()\n            outputs.update(s)\n\n        for p in self.parameters:\n            s = p.serialize()\n            parameters.update(s)\n\n        return {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"inputs\": inputs,\n            \"outputs\": outputs,\n            \"parameters\": parameters,\n        }\n\n    @staticmethod\n    def from_dict(payload):\n        if payload[\"tags\"]:\n            tags = [Tag(**t) for t in payload[\"tags\"]]\n        else:\n            tags = None\n\n        return Pipeline(\n            id = payload.get(\"id\", None),\n            name = payload[\"name\"],\n            inputs = [DataSet.from_dict(i) for i in payload[\"inputs\"]],\n            outputs = [DataSet.from_dict(o) for o in payload[\"outputs\"]],\n            parameters = [Parameter(**p) for p in payload[\"parameters\"]],\n            status = payload.get(\"status\", None),\n            tags = tags,\n            task_id = payload.get(\"task_id\", None),\n            task_name = payload.get(\"task_name\", None),\n            task_args = payload.get(\"task_args\", None),\n            task_kwargs = payload.get(\"task_kwargs\", None),\n            task_request = payload.get(\"task_request\", None),\n            task_exception = payload.get(\"task_exception\", None),\n            task_traceback = payload.get(\"task_traceback\", None),\n            task_einfo = payload.get(\"task_einfo\", None),\n        )", "\n@strawberry.type\nclass Pipeline:\n    id: Optional[uuid.UUID] = None\n    inputs: List[DataSet]\n    name: str\n    outputs: List[DataSet]\n    parameters: List[Parameter]\n    status: Optional[str] = None\n    tags: Optional[List[Tag]] = None\n    task_id: Optional[str] = None\n    task_name: Optional[str] = None\n    task_args: Optional[str] = None\n    task_kwargs: Optional[str] = None\n    task_request: Optional[str] = None\n    task_exception: Optional[str] = None\n    task_traceback: Optional[str] = None\n    task_einfo: Optional[str] = None\n    task_result: Optional[str] = None\n\n    @strawberry.field\n    def template(self) -> PipelineTemplate:\n        return PipelineTemplate(name = self.name)\n\n    @strawberry.field\n    def describe(self) -> str:\n        return self.template().describe()\n\n    @strawberry.field\n    def nodes(self) -> List[Node]:\n        return self.template().nodes()\n\n    def serialize(self):\n        inputs = {}\n        outputs = {}\n        parameters = {}\n        for i in self.inputs:\n            s = i.serialize()\n            inputs.update(s)\n\n        for o in self.outputs:\n            s = o.serialize()\n            outputs.update(s)\n\n        for p in self.parameters:\n            s = p.serialize()\n            parameters.update(s)\n\n        return {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"inputs\": inputs,\n            \"outputs\": outputs,\n            \"parameters\": parameters,\n        }\n\n    @staticmethod\n    def from_dict(payload):\n        if payload[\"tags\"]:\n            tags = [Tag(**t) for t in payload[\"tags\"]]\n        else:\n            tags = None\n\n        return Pipeline(\n            id = payload.get(\"id\", None),\n            name = payload[\"name\"],\n            inputs = [DataSet.from_dict(i) for i in payload[\"inputs\"]],\n            outputs = [DataSet.from_dict(o) for o in payload[\"outputs\"]],\n            parameters = [Parameter(**p) for p in payload[\"parameters\"]],\n            status = payload.get(\"status\", None),\n            tags = tags,\n            task_id = payload.get(\"task_id\", None),\n            task_name = payload.get(\"task_name\", None),\n            task_args = payload.get(\"task_args\", None),\n            task_kwargs = payload.get(\"task_kwargs\", None),\n            task_request = payload.get(\"task_request\", None),\n            task_exception = payload.get(\"task_exception\", None),\n            task_traceback = payload.get(\"task_traceback\", None),\n            task_einfo = payload.get(\"task_einfo\", None),\n        )", "\n@strawberry.type\nclass PipelineEvent:\n    id: str\n    task_id: str\n    status: str\n    result: Optional[str] = None\n    timestamp: str\n    traceback: Optional[str] = None\n", "\n@strawberry.type\nclass PipelineLogMessage:\n    id: str\n    message: str\n    message_id: str\n    task_id: str\n    time: str"]}
{"filename": "src/kedro_graphql/__init__.py", "chunked_list": ["\"\"\"kedro-graphql\n\"\"\"\n__version__ = \"0.3.5\"\n\nfrom .asgi import KedroGraphQL\n"]}
{"filename": "src/kedro_graphql/asgi.py", "chunked_list": ["from fastapi import FastAPI\nfrom strawberry.fastapi import GraphQLRouter\nfrom .backends import init_backend\nfrom .schema import build_schema       \nclass KedroGraphQL(FastAPI):\n    def __init__(self):\n        super(KedroGraphQL, self).__init__()\n\n        self.backend = init_backend()\n    \n        @self.on_event(\"startup\")\n        def startup_backend():\n            self.backend.startup()\n    \n        @self.on_event(\"shutdown\")\n        def shutdown_backend():\n            self.backend.shutdown()\n    \n        graphql_app = GraphQLRouter(build_schema())\n        self.include_router(graphql_app, prefix = \"/graphql\")\n        self.add_websocket_route(\"/graphql\", graphql_app)", "\n"]}
{"filename": "src/kedro_graphql/commands.py", "chunked_list": ["import click\nimport uvicorn\nfrom importlib import import_module\nfrom .config import config, discover_plugins\n\n@click.group(name=\"kedro-graphql\")\ndef commands():\n    pass\n\n", "\n\n@commands.command()\n@click.pass_obj\n@click.option(\n    \"--app\",\n    \"-a\",\n    default = config[\"KEDRO_GRAPHQL_APP\"],\n    help=\"Application import path\"\n)", "    help=\"Application import path\"\n)\n@click.option(\n    \"--imports\",\n    \"-i\",\n    default = config[\"KEDRO_GRAPHQL_IMPORTS\"],\n    help=\"Additional import paths\"\n)\n@click.option(\n    \"--worker\",", "@click.option(\n    \"--worker\",\n    \"-w\",\n    is_flag=True,\n    default=False,\n    help=\"Start a celery worker.\"\n)\ndef gql(metadata, app, imports, worker):\n    \"\"\"Commands for working with kedro-graphql.\"\"\"\n    if worker:\n        from .celeryapp import app\n        worker = app.Worker()\n        worker.start()\n    else:\n        config[\"KEDRO_GRAPHQL_IMPORTS\"] = imports\n        config[\"KEDRO_GRAPHQL_APP\"] = app\n\n        discover_plugins()\n                \n        module, class_name = config[\"KEDRO_GRAPHQL_APP\"].rsplit(\".\", 1)\n        module = import_module(module)\n        class_inst = getattr(module, class_name)\n        a = class_inst() \n        uvicorn.run(a, host=\"0.0.0.0\", port=5000, log_level=\"info\")", ""]}
{"filename": "src/kedro_graphql/plugins/plugins.py", "chunked_list": ["from abc import ABC, abstractmethod\nimport asyncio\nfrom kedro_graphql.decorators import gql_resolver, gql_query, gql_mutation, gql_subscription\nfrom kedro_graphql.models import ParameterInput, DataSetInput\nimport strawberry\nfrom typing import AsyncGenerator\n\nclass IOResolverPlugin(ABC):\n    \"\"\"\n    Implement this class to define custom behavior for pipeline inputs and\n    outputs.  The methods of the class will called prior to pipeline \n    execution.\n    \"\"\"\n    @abstractmethod\n    def __input__(self, input: ParameterInput | DataSetInput) -> [ParameterInput | DataSetInput]:\n        pass\n\n    @abstractmethod \n    def __submit__(self, input: ParameterInput | DataSetInput) -> ParameterInput | DataSetInput:\n        pass", "\n@gql_resolver(name = \"text_in\")\nclass ExampleTextInPlugin(IOResolverPlugin):\n    \n    def __input__(self, input: ParameterInput | DataSetInput) -> [ParameterInput | DataSetInput]:\n        print(\"plugin example\", input)\n        return [input]\n\n    def __submit__(self, input: ParameterInput | DataSetInput) -> ParameterInput | DataSetInput:\n        return input", "\n@gql_query()\n@strawberry.type\nclass ExampleQueryTypePlugin():\n    @strawberry.field\n    def hello_world(self) -> str:\n        return \"Hello World\"\n\n@gql_mutation()\n@strawberry.type\nclass ExampleMutationTypePlugin():\n    @strawberry.mutation\n    def hello_world(self, message: str = \"World\") -> str:\n        return \"Hello \" + message", "@gql_mutation()\n@strawberry.type\nclass ExampleMutationTypePlugin():\n    @strawberry.mutation\n    def hello_world(self, message: str = \"World\") -> str:\n        return \"Hello \" + message\n\n@gql_subscription()\n@strawberry.type\nclass ExampleSubscriptionTypePlugin():\n    @strawberry.subscription\n    async def hello_world(self, message: str = \"World\", target: int = 11) -> AsyncGenerator[str, None]:\n        for i in range(target):\n            yield str(i) + \" Hello \" + message\n            await asyncio.sleep(0.5)", "@strawberry.type\nclass ExampleSubscriptionTypePlugin():\n    @strawberry.subscription\n    async def hello_world(self, message: str = \"World\", target: int = 11) -> AsyncGenerator[str, None]:\n        for i in range(target):\n            yield str(i) + \" Hello \" + message\n            await asyncio.sleep(0.5)"]}
{"filename": "src/kedro_graphql/plugins/__init__.py", "chunked_list": [""]}
{"filename": "src/kedro_graphql/pipelines/__init__.py", "chunked_list": [""]}
{"filename": "src/kedro_graphql/pipelines/example00/pipeline.py", "chunked_list": ["\"\"\"\nThis is a boilerplate pipeline 'example00'\ngenerated using Kedro 0.18.4\n\"\"\"\n\nfrom kedro.pipeline import Pipeline, node, pipeline\nfrom .nodes import echo\n\ndef create_pipeline(**kwargs) -> Pipeline:\n    return pipeline([\n        node(\n            func=echo,\n            inputs=[\"text_in\", \"params:example\", \"parameters\"],\n            outputs=\"text_out\",\n            name=\"echo_node\"\n        )\n    ])", "def create_pipeline(**kwargs) -> Pipeline:\n    return pipeline([\n        node(\n            func=echo,\n            inputs=[\"text_in\", \"params:example\", \"parameters\"],\n            outputs=\"text_out\",\n            name=\"echo_node\"\n        )\n    ])\n", ""]}
{"filename": "src/kedro_graphql/pipelines/example00/__init__.py", "chunked_list": ["\"\"\"\nThis is a boilerplate pipeline 'example00'\ngenerated using Kedro 0.18.4\n\"\"\"\n\nfrom .pipeline import create_pipeline\n\n__all__ = [\"create_pipeline\"]\n\n__version__ = \"0.1\"", "\n__version__ = \"0.1\"\n"]}
{"filename": "src/kedro_graphql/pipelines/example00/nodes.py", "chunked_list": ["\"\"\"\nThis is a boilerplate pipeline 'example00'\ngenerated using Kedro 0.18.4\n\"\"\"\nimport time\n\ndef echo(text: str, example: str, parameters: dict) -> str:\n    time.sleep(int(parameters.get(\"duration\", 1)))\n    return text"]}
{"filename": "src/kedro_graphql/logs/logger.py", "chunked_list": ["import redis\nimport redis.asyncio as redis_asyncio\nimport logging\nfrom logging import LogRecord\nfrom .json_log_formatter import JSONFormatter ## VerboseJSONFormatter also available\nfrom celery.signals import task_prerun, task_postrun\nfrom kedro_graphql.config import config\nfrom typing import AsyncGenerator\nfrom celery.result import AsyncResult\nfrom celery.states import READY_STATES", "from celery.result import AsyncResult\nfrom celery.states import READY_STATES\nimport json\nimport os\nfrom inspect import currentframe, getframeinfo\n\nlogger = logging.getLogger(\"kedro-graphql\")\n\nclass RedisLogStreamPublisher(object):\n    def __init__(self, topic, broker_url = config[\"KEDRO_GRAPHQL_BROKER\"]):\n        self.connection = redis.Redis.from_url(broker_url)\n        self.topic = topic\n        if not self.connection.exists(self.topic):\n            self.connection.xadd(self.topic, json.loads(JSONFormatter().format(LogRecord(\"kedro-graphql\", 20, os.path.abspath(__file__), getframeinfo(currentframe()).lineno, \"Starting log stream\", None, None))))\n            ## stream will expire in 24 hours (safety mechanism in case task_postrun fails to delete)\n            self.connection.expire(self.topic, 86400)\n\n    def publish(self, data):\n        self.connection.xadd(self.topic, data)", "class RedisLogStreamPublisher(object):\n    def __init__(self, topic, broker_url = config[\"KEDRO_GRAPHQL_BROKER\"]):\n        self.connection = redis.Redis.from_url(broker_url)\n        self.topic = topic\n        if not self.connection.exists(self.topic):\n            self.connection.xadd(self.topic, json.loads(JSONFormatter().format(LogRecord(\"kedro-graphql\", 20, os.path.abspath(__file__), getframeinfo(currentframe()).lineno, \"Starting log stream\", None, None))))\n            ## stream will expire in 24 hours (safety mechanism in case task_postrun fails to delete)\n            self.connection.expire(self.topic, 86400)\n\n    def publish(self, data):\n        self.connection.xadd(self.topic, data)", "\nclass RedisLogStreamSubscriber(object):\n        \n    @classmethod\n    async def create(cls, topic, broker_url = config[\"KEDRO_GRAPHQL_BROKER\"]):\n        \"\"\"Factory method for async instantiation RedisLogStreamSubscriber objects.\n        \"\"\"\n        self = RedisLogStreamSubscriber()\n        self.topic = topic\n        self.broker_url = broker_url\n        self.connection = await redis_asyncio.from_url(broker_url)\n        return self\n\n    async def consume(self, count = 1, start_id = 0):\n        r = await self.connection.xread(count = count, streams={self.topic:start_id})\n        return r", "    \nclass KedroGraphQLLogHandler(logging.StreamHandler):\n    def __init__(self, topic, broker_url = config[\"KEDRO_GRAPHQL_BROKER\"]):\n        logging.StreamHandler.__init__(self)\n        self.broker_url = broker_url\n        self.topic = topic\n        self.broker = RedisLogStreamPublisher(topic, broker_url)\n        self.setFormatter(JSONFormatter())\n\n    def emit(self, record):\n        msg = self.format(record)\n        self.broker.publish(json.loads(msg))", "\n\n@task_prerun.connect\ndef setup_task_logger(task_id, task, args, **kwargs):\n    logger = logging.getLogger(\"kedro\")\n\n    handler = KedroGraphQLLogHandler(task_id)\n    logger.addHandler(handler)\n    \n@task_postrun.connect\ndef cleanup_task_logger(task_id, task, args, **kwargs):\n    logger = logging.getLogger(\"kedro\")\n    logger.info(\"Closing log stream\")\n    for handler in logger.handlers:\n        if isinstance(handler, KedroGraphQLLogHandler) and handler.topic == task_id:\n            handler.flush()\n            handler.close()\n            handler.broker.connection.delete(task_id) ## delete stream\n            handler.broker.connection.close()\n    logger.handlers = []", "    \n@task_postrun.connect\ndef cleanup_task_logger(task_id, task, args, **kwargs):\n    logger = logging.getLogger(\"kedro\")\n    logger.info(\"Closing log stream\")\n    for handler in logger.handlers:\n        if isinstance(handler, KedroGraphQLLogHandler) and handler.topic == task_id:\n            handler.flush()\n            handler.close()\n            handler.broker.connection.delete(task_id) ## delete stream\n            handler.broker.connection.close()\n    logger.handlers = []", "\n\nclass PipelineLogStream():\n\n    @classmethod\n    async def create(cls, task_id, broker_url = config[\"KEDRO_GRAPHQL_BROKER\"]):\n        \"\"\"Factory method for async instantiation PipelineLogStream objects.\n        \"\"\"\n        self = PipelineLogStream()\n        self.task_id = task_id\n        self.broker_url = broker_url\n        self.broker = await RedisLogStreamSubscriber().create(task_id, broker_url)\n        return self\n\n    async def consume(self) -> AsyncGenerator[dict, None]:\n        start_id = 0\n        while True:\n            stream_data = await self.broker.consume(count = 1, start_id = start_id)\n            if len(stream_data) > 0:\n                for id, value in stream_data[0][1]:\n                    yield {\"task_id\": self.task_id, \"message_id\": id.decode(), \"message\": value[b\"message\"].decode(), \"time\": value[b\"time\"].decode()}\n                ## https://redis-py.readthedocs.io/en/stable/examples/redis-stream-example.html#read-more-data-from-the-stream\n                start_id = stream_data[0][1][-1][0]\n            else:\n                ## check task status\n                r = AsyncResult(self.task_id)\n                if r.status in READY_STATES:\n                    await self.broker.connection.close()\n                    break"]}
{"filename": "src/kedro_graphql/logs/__init__.py", "chunked_list": [""]}
{"filename": "src/kedro_graphql/logs/json_log_formatter.py", "chunked_list": ["## All code below this file is attributed to:\n##\n## The MIT License (MIT)\n## \n## Copyright (c) 2015 Marsel Mavletkulov\n## \n## Permission is hereby granted, free of charge, to any person obtaining a copy\n## of this software and associated documentation files (the \"Software\"), to deal\n## in the Software without restriction, including without limitation the rights\n## to use, copy, modify, merge, publish, distribute, sublicense, and/or sell", "## in the Software without restriction, including without limitation the rights\n## to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n## copies of the Software, and to permit persons to whom the Software is\n## furnished to do so, subject to the following conditions:\n## \n## The above copyright notice and this permission notice shall be included in all\n## copies or substantial portions of the Software.\n## \n## THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n## IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,", "## THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n## IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n## FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n## AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n## LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n## OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n## SOFTWARE.\nimport logging\nfrom datetime import datetime\n", "from datetime import datetime\n\nimport json\n\nBUILTIN_ATTRS = {\n    'args',\n    'asctime',\n    'created',\n    'exc_info',\n    'exc_text',", "    'exc_info',\n    'exc_text',\n    'filename',\n    'funcName',\n    'levelname',\n    'levelno',\n    'lineno',\n    'module',\n    'msecs',\n    'message',", "    'msecs',\n    'message',\n    'msg',\n    'name',\n    'pathname',\n    'process',\n    'processName',\n    'relativeCreated',\n    'stack_info',\n    'thread',", "    'stack_info',\n    'thread',\n    'threadName',\n}\n\n\nclass JSONFormatter(logging.Formatter):\n    \"\"\"JSON log formatter.\n\n    Usage example::\n\n        import logging\n\n        import json_log_formatter\n\n        json_handler = logging.FileHandler(filename='/var/log/my-log.json')\n        json_handler.setFormatter(json_log_formatter.JSONFormatter())\n\n        logger = logging.getLogger('my_json')\n        logger.addHandler(json_handler)\n\n        logger.info('Sign up', extra={'referral_code': '52d6ce'})\n\n    The log file will contain the following log record (inline)::\n\n        {\n            \"message\": \"Sign up\",\n            \"time\": \"2015-09-01T06:06:26.524448\",\n            \"referral_code\": \"52d6ce\"\n        }\n\n    \"\"\"\n\n    json_lib = json\n\n    def format(self, record):\n        message = record.getMessage()\n        extra = self.extra_from_record(record)\n        json_record = self.json_record(message, extra, record)\n        mutated_record = self.mutate_json_record(json_record)\n        # Backwards compatibility: Functions that overwrite this but don't\n        # return a new value will return None because they modified the\n        # argument passed in.\n        if mutated_record is None:\n            mutated_record = json_record\n        return self.to_json(mutated_record)\n\n    def to_json(self, record):\n        \"\"\"Converts record dict to a JSON string.\n\n        It makes best effort to serialize a record (represents an object as a string)\n        instead of raising TypeError if json library supports default argument.\n        Note, ujson doesn't support it.\n        ValueError and OverflowError are also caught to avoid crashing an app,\n        e.g., due to circular reference.\n\n        Override this method to change the way dict is converted to JSON.\n\n        \"\"\"\n        try:\n            return self.json_lib.dumps(record, default=_json_serializable)\n        # ujson doesn't support default argument and raises TypeError.\n        # \"ValueError: Circular reference detected\" is raised\n        # when there is a reference to object inside the object itself.\n        except (TypeError, ValueError, OverflowError):\n            try:\n                return self.json_lib.dumps(record)\n            except (TypeError, ValueError, OverflowError):\n                return '{}'\n\n    def extra_from_record(self, record):\n        \"\"\"Returns `extra` dict you passed to logger.\n\n        The `extra` keyword argument is used to populate the `__dict__` of\n        the `LogRecord`.\n\n        \"\"\"\n        return {\n            attr_name: record.__dict__[attr_name]\n            for attr_name in record.__dict__\n            if attr_name not in BUILTIN_ATTRS\n        }\n\n    def json_record(self, message, extra, record):\n        \"\"\"Prepares a JSON payload which will be logged.\n\n        Override this method to change JSON log format.\n\n        :param message: Log message, e.g., `logger.info(msg='Sign up')`.\n        :param extra: Dictionary that was passed as `extra` param\n            `logger.info('Sign up', extra={'referral_code': '52d6ce'})`.\n        :param record: `LogRecord` we got from `JSONFormatter.format()`.\n        :return: Dictionary which will be passed to JSON lib.\n\n        \"\"\"\n        extra['message'] = message\n        if 'time' not in extra:\n            extra['time'] = datetime.utcnow()\n\n        if record.exc_info:\n            extra['exc_info'] = self.formatException(record.exc_info)\n\n        return extra\n\n    def mutate_json_record(self, json_record):\n        \"\"\"Override it to convert fields of `json_record` to needed types.\n\n        Default implementation converts `datetime` to string in ISO8601 format.\n\n        \"\"\"\n        for attr_name in json_record:\n            attr = json_record[attr_name]\n            if isinstance(attr, datetime):\n                json_record[attr_name] = attr.isoformat()\n        return json_record", "\n\ndef _json_serializable(obj):\n    try:\n        return obj.__dict__\n    except AttributeError:\n        return str(obj)\n\n\nclass VerboseJSONFormatter(JSONFormatter):\n    \"\"\"JSON log formatter with built-in log record attributes such as log level.\n\n    Usage example::\n\n        import logging\n\n        import json_log_formatter\n\n        json_handler = logging.FileHandler(filename='/var/log/my-log.json')\n        json_handler.setFormatter(json_log_formatter.VerboseJSONFormatter())\n\n        logger = logging.getLogger('my_verbose_json')\n        logger.addHandler(json_handler)\n\n        logger.error('An error has occured')\n\n    The log file will contain the following log record (inline)::\n\n        {\n            \"filename\": \"tests.py\",\n            \"funcName\": \"test_file_name_is_testspy\",\n            \"levelname\": \"ERROR\",\n            \"lineno\": 276,\n            \"module\": \"tests\",\n            \"name\": \"my_verbose_json\",\n            \"pathname\": \"/Users/bob/json-log-formatter/tests.py\",\n            \"process\": 3081,\n            \"processName\": \"MainProcess\",\n            \"stack_info\": null,\n            \"thread\": 4664270272,\n            \"threadName\": \"MainThread\",\n            \"message\": \"An error has occured\",\n            \"time\": \"2021-07-04T21:05:42.767726\"\n        }\n\n    Read more about the built-in log record attributes\n    https://docs.python.org/3/library/logging.html#logrecord-attributes.\n\n    \"\"\"\n    def json_record(self, message, extra, record):\n        extra['filename'] = record.filename\n        extra['funcName'] = record.funcName\n        extra['levelname'] = record.levelname\n        extra['lineno'] = record.lineno\n        extra['module'] = record.module\n        extra['name'] = record.name\n        extra['pathname'] = record.pathname\n        extra['process'] = record.process\n        extra['processName'] = record.processName\n        if hasattr(record, 'stack_info'):\n            extra['stack_info'] = record.stack_info\n        else:\n            extra['stack_info'] = None\n        extra['thread'] = record.thread\n        extra['threadName'] = record.threadName\n        return super(VerboseJSONFormatter, self).json_record(message, extra, record)", "\nclass VerboseJSONFormatter(JSONFormatter):\n    \"\"\"JSON log formatter with built-in log record attributes such as log level.\n\n    Usage example::\n\n        import logging\n\n        import json_log_formatter\n\n        json_handler = logging.FileHandler(filename='/var/log/my-log.json')\n        json_handler.setFormatter(json_log_formatter.VerboseJSONFormatter())\n\n        logger = logging.getLogger('my_verbose_json')\n        logger.addHandler(json_handler)\n\n        logger.error('An error has occured')\n\n    The log file will contain the following log record (inline)::\n\n        {\n            \"filename\": \"tests.py\",\n            \"funcName\": \"test_file_name_is_testspy\",\n            \"levelname\": \"ERROR\",\n            \"lineno\": 276,\n            \"module\": \"tests\",\n            \"name\": \"my_verbose_json\",\n            \"pathname\": \"/Users/bob/json-log-formatter/tests.py\",\n            \"process\": 3081,\n            \"processName\": \"MainProcess\",\n            \"stack_info\": null,\n            \"thread\": 4664270272,\n            \"threadName\": \"MainThread\",\n            \"message\": \"An error has occured\",\n            \"time\": \"2021-07-04T21:05:42.767726\"\n        }\n\n    Read more about the built-in log record attributes\n    https://docs.python.org/3/library/logging.html#logrecord-attributes.\n\n    \"\"\"\n    def json_record(self, message, extra, record):\n        extra['filename'] = record.filename\n        extra['funcName'] = record.funcName\n        extra['levelname'] = record.levelname\n        extra['lineno'] = record.lineno\n        extra['module'] = record.module\n        extra['name'] = record.name\n        extra['pathname'] = record.pathname\n        extra['process'] = record.process\n        extra['processName'] = record.processName\n        if hasattr(record, 'stack_info'):\n            extra['stack_info'] = record.stack_info\n        else:\n            extra['stack_info'] = None\n        extra['thread'] = record.thread\n        extra['threadName'] = record.threadName\n        return super(VerboseJSONFormatter, self).json_record(message, extra, record)"]}
{"filename": "src/kedro_graphql/backends/mongodb.py", "chunked_list": ["from .base import BaseBackend\nfrom pymongo import MongoClient\nfrom kedro_graphql.models import Pipeline\nimport uuid\nfrom bson.objectid import ObjectId\nfrom fastapi.encoders import jsonable_encoder\n\n\nclass MongoBackend(BaseBackend):\n\n    def __init__(self, uri = None, db = None):\n        #self.client = MongoClient(app.config[\"MONGO_URI\"])\n        self.client = MongoClient(uri)\n        #self.db = self.client[app.config[\"MONGO_DB_NAME\"]]\n        self.db = self.client[db]\n        #self.app = app\n\n\n    def startup(self, **kwargs):\n        \"\"\"Startup hook.\"\"\"\n        print(\"Connected to the MongoDB database!\")\n\n    def shutdown(self, **kwargs):\n        \"\"\"Shutdown hook.\"\"\"\n        self.client.close()\n\n    def load(self, id: uuid.UUID = None, task_id: str = None):\n        \"\"\"Load a pipeline by id or task_id\"\"\"\n        if task_id:\n            r = self.db[\"pipelines\"].find_one({\"task_id\": task_id})\n        else:\n            r = self.db[\"pipelines\"].find_one({\"_id\": ObjectId(id)})\n\n        if r:\n            id = r.pop(\"_id\")\n            p = Pipeline.from_dict(r)\n            p.id = str(id)\n            return p\n        else:\n            return None\n\n    def create(self, pipeline: Pipeline):\n        \"\"\"Save a pipeline\"\"\"\n        j = jsonable_encoder(pipeline)\n        created = self.db[\"pipelines\"].insert_one(j)\n        created = self.db[\"pipelines\"].find_one({\"_id\": created.inserted_id})\n        pipeline.id = created[\"_id\"]\n        return pipeline\n\n    def update(self, id: uuid.UUID = None, task_id: str = None, values = {}):\n        \"\"\"Update a pipeline using id or task id\"\"\"\n        if task_id:\n            filter = {'task_id': task_id }\n        else:\n            filter = {'_id': ObjectId(id)}\n\n        newvalues = { \"$set\": values }\n        self.db[\"pipelines\"].update_one(filter, newvalues)\n\n        if task_id:\n            p = self.load(task_id = task_id)\n        else:\n            p = self.load(id = id)\n\n        return p", "class MongoBackend(BaseBackend):\n\n    def __init__(self, uri = None, db = None):\n        #self.client = MongoClient(app.config[\"MONGO_URI\"])\n        self.client = MongoClient(uri)\n        #self.db = self.client[app.config[\"MONGO_DB_NAME\"]]\n        self.db = self.client[db]\n        #self.app = app\n\n\n    def startup(self, **kwargs):\n        \"\"\"Startup hook.\"\"\"\n        print(\"Connected to the MongoDB database!\")\n\n    def shutdown(self, **kwargs):\n        \"\"\"Shutdown hook.\"\"\"\n        self.client.close()\n\n    def load(self, id: uuid.UUID = None, task_id: str = None):\n        \"\"\"Load a pipeline by id or task_id\"\"\"\n        if task_id:\n            r = self.db[\"pipelines\"].find_one({\"task_id\": task_id})\n        else:\n            r = self.db[\"pipelines\"].find_one({\"_id\": ObjectId(id)})\n\n        if r:\n            id = r.pop(\"_id\")\n            p = Pipeline.from_dict(r)\n            p.id = str(id)\n            return p\n        else:\n            return None\n\n    def create(self, pipeline: Pipeline):\n        \"\"\"Save a pipeline\"\"\"\n        j = jsonable_encoder(pipeline)\n        created = self.db[\"pipelines\"].insert_one(j)\n        created = self.db[\"pipelines\"].find_one({\"_id\": created.inserted_id})\n        pipeline.id = created[\"_id\"]\n        return pipeline\n\n    def update(self, id: uuid.UUID = None, task_id: str = None, values = {}):\n        \"\"\"Update a pipeline using id or task id\"\"\"\n        if task_id:\n            filter = {'task_id': task_id }\n        else:\n            filter = {'_id': ObjectId(id)}\n\n        newvalues = { \"$set\": values }\n        self.db[\"pipelines\"].update_one(filter, newvalues)\n\n        if task_id:\n            p = self.load(task_id = task_id)\n        else:\n            p = self.load(id = id)\n\n        return p"]}
{"filename": "src/kedro_graphql/backends/base.py", "chunked_list": ["import abc\nfrom kedro_graphql.models import Pipeline\nimport uuid\n\nclass BaseBackend(metaclass=abc.ABCMeta):\n\n    @abc.abstractmethod\n    def startup(self, **kwargs):\n        \"\"\"Startup hook.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def shutdown(self, **kwargs):\n        \"\"\"Shutdown hook.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def load(self, id: uuid.UUID = None, task_id: str = None):\n        \"\"\"Load a pipeline by id\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def create(self, pipeline: Pipeline):\n        \"\"\"Save a pipeline\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def update(self, id: uuid.UUID = None, task_id: str = None, values: dict = None):\n        \"\"\"Update a pipeline\"\"\"\n        raise NotImplementedError"]}
{"filename": "src/kedro_graphql/backends/__init__.py", "chunked_list": ["from kedro_graphql.config import config\nfrom importlib import import_module\n\ndef init_backend():\n    backend_module, backend_class = config[\"KEDRO_GRAPHQL_BACKEND\"].rsplit(\".\", 1)\n    backend_kwargs = {\"uri\": config.get(\"MONGO_URI\"), \"db\": config.get(\"MONGO_DB_NAME\")}\n    backend_module = import_module(backend_module)\n    backend = getattr(backend_module, backend_class)\n    return backend(**backend_kwargs)"]}
{"filename": "src/kedro_graphql/example/app.py", "chunked_list": ["from fastapi.middleware.cors import CORSMiddleware\nfrom kedro_graphql import KedroGraphQL\n\n\nclass MyApp(KedroGraphQL):\n\n    def __init__(self): \n        super(MyApp, self).__init__()\n\n        origins = [\n            \"http://localhost\",\n            \"http://localhost:8080\",\n        ]\n        \n        self.add_middleware(\n            CORSMiddleware,\n            allow_origins=origins,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n        print(\"added CORSMiddleware\")"]}
{"filename": "src/kedro_graphql/example/__init__.py", "chunked_list": [""]}
