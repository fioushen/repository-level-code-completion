{"filename": "setup.py", "chunked_list": ["#############################################################################\n#\n#   Setup based on Labelme install script:\n#   https://github.com/wkentaro/labelme\n#\n##############################################################################\nimport re\nfrom setuptools import setup, find_packages\n\n\ndef get_version():\n    filename = \"faceshine/__init__.py\"\n    with open(filename) as f:\n        match = re.search(\n            r\"\"\"^__version__ = ['\"]([^'\"]*)['\"]\"\"\", f.read(), re.M\n        )\n    if not match:\n        raise RuntimeError(\"{} doesn't contain __version__\".format(filename))\n    version = match.groups()[0]\n    return version", "\n\ndef get_version():\n    filename = \"faceshine/__init__.py\"\n    with open(filename) as f:\n        match = re.search(\n            r\"\"\"^__version__ = ['\"]([^'\"]*)['\"]\"\"\", f.read(), re.M\n        )\n    if not match:\n        raise RuntimeError(\"{} doesn't contain __version__\".format(filename))\n    version = match.groups()[0]\n    return version", "\n\ndef get_install_requires():\n    install_requires = [\n        \"numpy>=1.24.3\",\n        \"torch>=2.0.0\",\n        \"opencv-python\",\n        \"flask>=2.3.2\",\n        \"flask_restful>=0.3.10\",\n        \"pillow>=9.5.0\",\n        \"torchvision\",\n        \"colorama\",\n        \"matplotlib\",\n        \"fastprogress\",\n        \"pandas\",\n        \"scipy\",\n        \"deoldify\",\n        \"realesrgan\",\n        \"zeroscratches\",\n        \"huggingface_hub\"\n    ]\n\n    return install_requires", "\n\ndef get_long_description():\n    with open(\"README.md\") as f:\n        long_description = f.read()\n    try:\n        # when this package is being released\n        import github2pypi\n\n        return github2pypi.replace_url(\n            slug=\"leonelhs/face-shine\", content=long_description, branch=\"main\"\n        )\n    except ImportError:\n        # when this package is being installed\n        return long_description", "\n\ndef main():\n    setup(\n        name='faceshine',\n        version=get_version(),\n        long_description=get_long_description(),\n        long_description_content_type=\"text/markdown\",\n        packages=find_packages(),\n        url='https://github.com/leonelhs/faceshine',\n        license='Apache',\n        author='leonel hernandez',\n        author_email='leonelhs@gmail.com',\n        description='Photo image enhancer',\n        install_requires=get_install_requires(),\n        package_data={\"faceshine\": []},\n        entry_points={\"console_scripts\": [\"faceshine=faceshine.__main__:main\"]},\n        classifiers=[\n            \"Development Status :: 5 - Production/Stable\",\n            \"Intended Audience :: Developers\",\n            \"Intended Audience :: Science/Research\",\n            \"Natural Language :: English\",\n            \"Operating System :: OS Independent\",\n            \"Programming Language :: Python\",\n            \"Programming Language :: Python :: 3.5\",\n            \"Programming Language :: Python :: 3.6\",\n            \"Programming Language :: Python :: 3.7\",\n            \"Programming Language :: Python :: 3.8\",\n            \"Programming Language :: Python :: 3.9\",\n            \"Programming Language :: Python :: 3 :: Only\",\n        ],\n    )", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "playground.py", "chunked_list": ["#  python playground.py -i /home/leonel/damage.jpg -o test.jpg -g 0 -dn 0.2 --fp32 -s 2 -n realesr-general-x4v3\nimport argparse\nimport cv2\nimport glob\nimport os\nfrom basicsr.archs.rrdbnet_arch import RRDBNet\nfrom basicsr.utils.download_util import load_file_from_url\n\nfrom realesrgan import RealESRGANer\nfrom realesrgan.archs.srvgg_arch import SRVGGNetCompact", "from realesrgan import RealESRGANer\nfrom realesrgan.archs.srvgg_arch import SRVGGNetCompact\n\n\ndef main():\n    \"\"\"Inference demo for Real-ESRGAN.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-i', '--input', type=str, default='inputs', help='Input image or folder')\n    parser.add_argument(\n        '-n',\n        '--model_name',\n        type=str,\n        default='RealESRGAN_x4plus',\n        help=('Model names: RealESRGAN_x4plus | RealESRNet_x4plus | RealESRGAN_x4plus_anime_6B | RealESRGAN_x2plus | '\n              'realesr-animevideov3 | realesr-general-x4v3'))\n    parser.add_argument('-o', '--output', type=str, default='results', help='Output folder')\n    parser.add_argument(\n        '-dn',\n        '--denoise_strength',\n        type=float,\n        default=0.5,\n        help=('Denoise strength. 0 for weak denoise (keep noise), 1 for strong denoise ability. '\n              'Only used for the realesr-general-x4v3 model'))\n    parser.add_argument('-s', '--outscale', type=float, default=4, help='The final upsampling scale of the image')\n    parser.add_argument(\n        '--model_path', type=str, default=None, help='[Option] Model path. Usually, you do not need to specify it')\n    parser.add_argument('--suffix', type=str, default='out', help='Suffix of the restored image')\n    parser.add_argument('-t', '--tile', type=int, default=0, help='Tile size, 0 for no tile during testing')\n    parser.add_argument('--tile_pad', type=int, default=10, help='Tile padding')\n    parser.add_argument('--pre_pad', type=int, default=0, help='Pre padding size at each border')\n    parser.add_argument('--face_enhance', action='store_true', help='Use GFPGAN to enhance face')\n    parser.add_argument(\n        '--fp32', action='store_true', help='Use fp32 precision during inference. Default: fp16 (half precision).')\n    parser.add_argument(\n        '--alpha_upsampler',\n        type=str,\n        default='realesrgan',\n        help='The upsampler for the alpha channels. Options: realesrgan | bicubic')\n    parser.add_argument(\n        '--ext',\n        type=str,\n        default='auto',\n        help='Image extension. Options: auto | jpg | png, auto means using the same extension as inputs')\n    parser.add_argument(\n        '-g', '--gpu-id', type=int, default=None, help='gpu device to use (default=None) can be 0,1,2 for multi-gpu')\n\n    args = parser.parse_args()\n\n    # determine models according to model names\n    args.model_name = args.model_name.split('.')[0]\n    if args.model_name == 'RealESRGAN_x4plus':  # x4 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth']\n    elif args.model_name == 'RealESRNet_x4plus':  # x4 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.1/RealESRNet_x4plus.pth']\n    elif args.model_name == 'RealESRGAN_x4plus_anime_6B':  # x4 RRDBNet model with 6 blocks\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth']\n    elif args.model_name == 'RealESRGAN_x2plus':  # x2 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n        netscale = 2\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth']\n    elif args.model_name == 'realesr-animevideov3':  # x4 VGG-style model (XS size)\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu')\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth']\n    elif args.model_name == 'realesr-general-x4v3':  # x4 VGG-style model (S size)\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')\n        netscale = 4\n        file_url = [\n            'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-wdn-x4v3.pth',\n            'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth'\n        ]\n\n    # determine model paths\n    if args.model_path is not None:\n        model_path = args.model_path\n    else:\n        model_path = os.path.join('weights-0', args.model_name + '.pth')\n        if not os.path.isfile(model_path):\n            ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n            for url in file_url:\n                # model_path will be updated\n                model_path = load_file_from_url(\n                    url=url, model_dir=os.path.join(ROOT_DIR, 'weights-0'), progress=True, file_name=None)\n\n    # use dni to control the denoise strength\n    dni_weight = None\n    if args.model_name == 'realesr-general-x4v3' and args.denoise_strength != 1:\n        wdn_model_path = model_path.replace('realesr-general-x4v3', 'realesr-general-wdn-x4v3')\n        model_path = [model_path, wdn_model_path]\n        dni_weight = [args.denoise_strength, 1 - args.denoise_strength]\n    print(model_path)\n    print(dni_weight)\n    # restorer\n    upsampler = RealESRGANer(\n        scale=netscale,\n        model_path=model_path,\n        dni_weight=dni_weight,\n        model=model,\n        tile=args.tile,\n        tile_pad=args.tile_pad,\n        pre_pad=args.pre_pad,\n        half=not args.fp32,\n        gpu_id=args.gpu_id)\n\n    if args.face_enhance:  # Use GFPGAN for face enhancement\n        from gfpgan import GFPGANer\n        face_enhancer = GFPGANer(\n            model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',\n            upscale=args.outscale,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=upsampler)\n    os.makedirs(args.output, exist_ok=True)\n\n    if os.path.isfile(args.input):\n        paths = [args.input]\n    else:\n        paths = sorted(glob.glob(os.path.join(args.input, '*')))\n\n    for idx, path in enumerate(paths):\n        imgname, extension = os.path.splitext(os.path.basename(path))\n        print('Testing', idx, imgname)\n\n        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        if len(img.shape) == 3 and img.shape[2] == 4:\n            img_mode = 'RGBA'\n        else:\n            img_mode = None\n\n        try:\n            if args.face_enhance:\n                _, _, output = face_enhancer.enhance(img, has_aligned=False, only_center_face=False, paste_back=True)\n            else:\n                output, _ = upsampler.enhance(img, outscale=args.outscale)\n        except RuntimeError as error:\n            print('Error', error)\n            print('If you encounter CUDA out of memory, try to set --tile with a smaller number.')\n        else:\n            if args.ext == 'auto':\n                extension = extension[1:]\n            else:\n                extension = args.ext\n            if img_mode == 'RGBA':  # RGBA images should be saved in png format\n                extension = 'png'\n            if args.suffix == '':\n                save_path = os.path.join(args.output, f'{imgname}.{extension}')\n            else:\n                save_path = os.path.join(args.output, f'{imgname}_{args.suffix}.{extension}')\n            cv2.imwrite(save_path, output)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "faceshine/__main__.py", "chunked_list": ["from colorama import Fore, Style\nfrom colorama import init as colorama_init\n\nfrom faceshine import appFaceShine\n\ncolorama_init()\n\n\ndef main():\n    print(Fore.BLUE)\n    print(\"*********************************\")\n    print(\"* Face Shine Service is running *\")\n    print(\"*********************************\")\n    print(Style.RESET_ALL)\n\n    appFaceShine.run(debug=False, port='5000')\n\n    print(Fore.GREEN)\n    print(\"******************************\")\n    print(\"*      Service finished      *\")\n    print(\"******************************\")\n    print(Style.RESET_ALL)", "def main():\n    print(Fore.BLUE)\n    print(\"*********************************\")\n    print(\"* Face Shine Service is running *\")\n    print(\"*********************************\")\n    print(Style.RESET_ALL)\n\n    appFaceShine.run(debug=False, port='5000')\n\n    print(Fore.GREEN)\n    print(\"******************************\")\n    print(\"*      Service finished      *\")\n    print(\"******************************\")\n    print(Style.RESET_ALL)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "faceshine/app.py", "chunked_list": ["from flask import Flask, jsonify\nfrom flask_restful import Resource, Api, reqparse\nfrom werkzeug.datastructures import FileStorage\n\nfrom faceshine import data2image\nfrom faceshine.tasks import TaskZeroBackground, \\\n    TaskImageColorizer, TaskLowLight, TaskEraseScratches, TaskSuperFace, TaskFaceSegmentation\n\nappFaceShine = Flask(__name__)\napi = Api(appFaceShine)", "appFaceShine = Flask(__name__)\napi = Api(appFaceShine)\n\ntaskFaceSegmentation = TaskFaceSegmentation()\ntaskSuperFace = TaskSuperFace()\ntaskLowLight = TaskLowLight()\ntaskEraseScratches = TaskEraseScratches()\ntaskImageColorizer = TaskImageColorizer()\ntaskZeroBackground = TaskZeroBackground()\n", "taskZeroBackground = TaskZeroBackground()\n\nparser = reqparse.RequestParser()\n\nparser.add_argument('image',\n                    type=FileStorage,\n                    location='files',\n                    required=False,\n                    help='Provide one image file')\n", "                    help='Provide one image file')\n\n\nclass Index(Resource):\n    def get(self):\n        return {'It works!': 'AI Remote Procedure Machine'}\n\n\nclass FaceSegmentation(Resource):\n    def post(self):\n        args = parser.parse_args()\n        stream = args['image'].read()\n        image = data2image(stream)\n        prediction = taskFaceSegmentation.executeTask(image)\n        return jsonify(prediction.tolist())", "class FaceSegmentation(Resource):\n    def post(self):\n        args = parser.parse_args()\n        stream = args['image'].read()\n        image = data2image(stream)\n        prediction = taskFaceSegmentation.executeTask(image)\n        return jsonify(prediction.tolist())\n\n\nclass SuperFace(Resource):\n    def post(self):\n        args = parser.parse_args()\n        stream_a = args['image'].read()\n        image = data2image(stream_a)\n        prediction = taskSuperFace.executeTask(image)\n        return jsonify(prediction.tolist())", "\nclass SuperFace(Resource):\n    def post(self):\n        args = parser.parse_args()\n        stream_a = args['image'].read()\n        image = data2image(stream_a)\n        prediction = taskSuperFace.executeTask(image)\n        return jsonify(prediction.tolist())\n\n\nclass ImageLowLight(Resource):\n    def post(self):\n        args = parser.parse_args()\n        stream_a = args['image'].read()\n        image = data2image(stream_a)\n        prediction = taskLowLight.executeTask(image)\n        return jsonify(prediction.tolist())", "\n\nclass ImageLowLight(Resource):\n    def post(self):\n        args = parser.parse_args()\n        stream_a = args['image'].read()\n        image = data2image(stream_a)\n        prediction = taskLowLight.executeTask(image)\n        return jsonify(prediction.tolist())\n", "\n\nclass EraseScratches(Resource):\n    def post(self):\n        args = parser.parse_args()\n        stream_a = args['image'].read()\n        image = data2image(stream_a)\n        prediction = taskEraseScratches.executeTask(image)\n        return jsonify(prediction.tolist())\n", "\n\nclass ImageColorizer(Resource):\n    def post(self):\n        args = parser.parse_args()\n        stream_a = args['image'].read()\n        image = data2image(stream_a)\n        prediction = taskImageColorizer.executeTask(image)\n        return jsonify(prediction.tolist())\n", "\n\nclass ZeroBackground(Resource):\n    def post(self):\n        args = parser.parse_args()\n        stream_a = args['image'].read()\n        image = data2image(stream_a)\n        prediction = taskZeroBackground.executeTask(image)\n        return jsonify(prediction.tolist())\n", "\n\napi.add_resource(FaceSegmentation, '/segment_face')\napi.add_resource(SuperFace, '/super_face')\napi.add_resource(EraseScratches, '/erase_scratches')\napi.add_resource(ImageLowLight, '/enhance_light')\napi.add_resource(ImageColorizer, '/colorize')\napi.add_resource(ZeroBackground, '/zero_background')\napi.add_resource(Index, '/')\n", "api.add_resource(Index, '/')\n"]}
{"filename": "faceshine/__init__.py", "chunked_list": ["__appname__ = \"Face Shine\"\n__version__ = \"1.0.4\"\n\nfrom .utils import tensor_to_ndarray\nfrom .utils import image_to_tensor\nfrom .utils import array2image\nfrom .utils import data2image\nfrom .utils import config\nfrom .app import appFaceShine\n", "from .app import appFaceShine\n"]}
{"filename": "faceshine/utils.py", "chunked_list": ["import PIL\nimport cv2\nimport json\nimport torch\nimport numpy as np\nfrom PIL.Image import Image\nimport torchvision.transforms as transforms\nfrom torchvision.utils import make_grid\n\n\ndef config():\n    data = open('faceshine/conf.json')\n    return json.load(data)", "\n\ndef config():\n    data = open('faceshine/conf.json')\n    return json.load(data)\n\n\ndef array2image(ndarray):\n    return PIL.Image.fromarray(np.uint8(ndarray)).convert('RGB')\n", "\n\ndef data2image(stream):\n    np_img = np.fromstring(stream, np.uint8)\n    return cv2.imdecode(np_img, cv2.IMREAD_UNCHANGED)\n\n\ndef image_to_tensor(image):\n    to_tensor = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\n\n    return to_tensor(image)", "\n\n# Method are common to:\n# TaskMaskScratches\n# TaskEraseScratches\n# TaskLowLight\n\ndef tensor_to_ndarray(tensor, nrow=1, padding=0, normalize=True):\n    grid = make_grid(tensor, nrow, padding, normalize)\n    return grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()", "\n\n"]}
{"filename": "faceshine/tasks/__init__.py", "chunked_list": ["from .base import Task\n\nfrom .segmentation import TaskZeroBackground\nfrom .colorizer import TaskImageColorizer\nfrom .lowlight import TaskLowLight\nfrom .zeroscratches import TaskEraseScratches\nfrom .superface import TaskSuperFace\nfrom .faceparser import TaskFaceSegmentation\n\n", "\n\n"]}
{"filename": "faceshine/tasks/superface/__init__.py", "chunked_list": ["from .task_super_face import TaskSuperFace\n"]}
{"filename": "faceshine/tasks/superface/task_super_face.py", "chunked_list": ["import os\n\nimport numpy as np\nfrom basicsr.archs.rrdbnet_arch import RRDBNet\nfrom gfpgan import GFPGANer\nfrom huggingface_hub import snapshot_download\nfrom realesrgan import RealESRGANer\nfrom realesrgan.archs.srvgg_arch import SRVGGNetCompact\n\nfrom faceshine.tasks import Task", "\nfrom faceshine.tasks import Task\n\nROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n\nREALESRGAN_REPO_ID = 'leonelhs/realesrgan'\nGFPGAN_REPO_ID = 'leonelhs/gfpgan'\n\n\ndef select_model(model_name):\n    model = None\n    netscale = 4\n    dni_weight = None\n\n    snapshot_folder = snapshot_download(repo_id=REALESRGAN_REPO_ID)\n    model_path = os.path.join(snapshot_folder, model_name)\n\n    if model_name == 'RealESRGAN_x4plus.pth':  # x4 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n\n    if model_name == 'RealESRNet_x4plus.pth':  # x4 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n\n    if model_name == 'RealESRGAN_x4plus_anime_6B.pth':  # x4 RRDBNet model with 6 blocks\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\n\n    if model_name == 'RealESRGAN_x2plus.pth':  # x2 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n        netscale = 2  # This is\n\n    if model_name == 'realesr-animevideov3.pth':  # x4 VGG-style model (XS size)\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu')\n\n    if model_name == 'realesr-general-x4v3.pth':  # x4 VGG-style model (S size)\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')\n        model_path = [\n            os.path.join(snapshot_folder, \"realesr-general-wdn-x4v3.pth\"),\n            os.path.join(snapshot_folder, \"realesr-general-x4v3.pth'\")\n        ]\n        dni_weight = [0.2, 0.8]\n\n    return model, netscale, model_path, dni_weight", "\ndef select_model(model_name):\n    model = None\n    netscale = 4\n    dni_weight = None\n\n    snapshot_folder = snapshot_download(repo_id=REALESRGAN_REPO_ID)\n    model_path = os.path.join(snapshot_folder, model_name)\n\n    if model_name == 'RealESRGAN_x4plus.pth':  # x4 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n\n    if model_name == 'RealESRNet_x4plus.pth':  # x4 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n\n    if model_name == 'RealESRGAN_x4plus_anime_6B.pth':  # x4 RRDBNet model with 6 blocks\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\n\n    if model_name == 'RealESRGAN_x2plus.pth':  # x2 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n        netscale = 2  # This is\n\n    if model_name == 'realesr-animevideov3.pth':  # x4 VGG-style model (XS size)\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu')\n\n    if model_name == 'realesr-general-x4v3.pth':  # x4 VGG-style model (S size)\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')\n        model_path = [\n            os.path.join(snapshot_folder, \"realesr-general-wdn-x4v3.pth\"),\n            os.path.join(snapshot_folder, \"realesr-general-x4v3.pth'\")\n        ]\n        dni_weight = [0.2, 0.8]\n\n    return model, netscale, model_path, dni_weight", "\n\nclass TaskSuperFace(Task):\n\n    def __init__(self):\n        super().__init__()\n\n        scale = 2\n        model_name = \"RealESRGAN_x4plus.pth\"\n        model, netscale, model_path, dni_weight = select_model(model_name)\n        upsampler = RealESRGANer(\n            scale=netscale,\n            model_path=model_path,\n            dni_weight=dni_weight,\n            model=model,\n            tile=0,\n            tile_pad=10,\n            pre_pad=0,\n            half=False,\n            gpu_id=0)\n\n        # output, _ = upsampler.enhance(img, outscale=outscale)\n\n        snapshot_folder = snapshot_download(repo_id=GFPGAN_REPO_ID)\n        model_path = os.path.join(snapshot_folder, \"GFPGANv1.3.pth\")\n        self.face_enhancer = GFPGANer(\n            model_path=model_path,\n            upscale=scale,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=upsampler)\n\n    def executeTask(self, image):\n        image = np.array(image)\n        _, _, output = self.face_enhancer.enhance(image, has_aligned=False, only_center_face=False, paste_back=True)\n        return output", ""]}
{"filename": "faceshine/tasks/segmentation/task_zero_background.py", "chunked_list": ["#############################################################################\n#\n#   Source from:\n#   https://github.com/leonelhs/face-makeup.PyTorch\n#   Forked from:\n#   https://github.com/zllrunning/face-makeup.PyTorch\n#   Reimplemented by: Leonel Hern\u00e1ndez\n#\n##############################################################################\nimport numpy as np", "##############################################################################\nimport numpy as np\nimport torch\n\nfrom faceshine import array2image, image_to_tensor\nfrom faceshine.tasks import Task\n\n\nclass TaskZeroBackground(Task):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', weights='DEFAULT')\n        self.model.eval()\n\n    def executeTask(self, image):\n        image = array2image(image)\n        input_tensor = image_to_tensor(image)\n        input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n\n        # move the input and model to GPU for speed if available\n        if torch.cuda.is_available():\n            input_batch = input_batch.to('cuda')\n            self.model.to('cuda')\n\n        with torch.no_grad():\n            output = self.model(input_batch)['out'][0]\n        output_predictions = output.argmax(0)\n\n        # create a binary (black and white) mask of the profile foreground\n        mask = output_predictions.byte().cpu().numpy()\n        background = np.zeros(mask.shape)\n        return np.where(mask, 255, background).astype(np.uint8)", "class TaskZeroBackground(Task):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', weights='DEFAULT')\n        self.model.eval()\n\n    def executeTask(self, image):\n        image = array2image(image)\n        input_tensor = image_to_tensor(image)\n        input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n\n        # move the input and model to GPU for speed if available\n        if torch.cuda.is_available():\n            input_batch = input_batch.to('cuda')\n            self.model.to('cuda')\n\n        with torch.no_grad():\n            output = self.model(input_batch)['out'][0]\n        output_predictions = output.argmax(0)\n\n        # create a binary (black and white) mask of the profile foreground\n        mask = output_predictions.byte().cpu().numpy()\n        background = np.zeros(mask.shape)\n        return np.where(mask, 255, background).astype(np.uint8)", ""]}
{"filename": "faceshine/tasks/segmentation/__init__.py", "chunked_list": ["from .task_zero_background import TaskZeroBackground\n"]}
{"filename": "faceshine/tasks/colorizer/__init__.py", "chunked_list": ["from .task_image_colorizer import TaskImageColorizer\n"]}
{"filename": "faceshine/tasks/colorizer/model_image_colorizer.py", "chunked_list": ["import torch\nfrom PIL import Image as PilImage\n\nfrom deoldify.filters import IFilter, BaseFilter\nfrom deoldify.visualize import ModelImageVisualizer\nfrom fastai.basic_train import Learner\nfrom fastai.vision import normalize_funcs\n\nstats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\nresults_dir = \"./faceshine/tasks/colorizer/results\"", "stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\nresults_dir = \"./faceshine/tasks/colorizer/results\"\n\n\nclass ImageFilter(BaseFilter):\n    def __init__(self, learn: Learner):\n        super().__init__(learn)\n        self.render_base = 16\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.norm, self.denorm = normalize_funcs(*stats)\n\n    def filter(self, filtered_image: PilImage, render_factor=35) -> PilImage:\n        orig_image = filtered_image.copy()\n        render_sz = render_factor * self.render_base\n        model_image = self._model_process(orig=filtered_image, sz=render_sz)\n        raw_color = self._unsquare(model_image, orig_image)\n        return raw_color", "\n\nclass ModelImageColorizer(ModelImageVisualizer):\n    def __init__(self, filter: IFilter):\n        super().__init__(filter, results_dir=results_dir)\n\n    def get_colored_image(self, image, render_factor: int = None) -> PilImage:\n        self._clean_mem()\n        return self.filter.filter(image, render_factor=render_factor)\n", "\n"]}
{"filename": "faceshine/tasks/colorizer/task_image_colorizer.py", "chunked_list": ["#############################################################################\n#\n#   Source from:\n#   https://github.com/jantic/DeOldify\n#   Forked from:\n#   https://github.com/leonelhs/DeOldify\n#   Reimplemented by: Leonel Hern\u00e1ndez\n#\n##############################################################################\nfrom pathlib import Path", "##############################################################################\nfrom pathlib import Path\n\nimport numpy as np\nfrom deoldify import device\nfrom deoldify.device_id import DeviceId\nfrom deoldify.generators import gen_inference_deep\nfrom huggingface_hub import snapshot_download\n\nfrom faceshine import array2image", "\nfrom faceshine import array2image\nfrom faceshine.tasks import Task\nfrom .model_image_colorizer import ImageFilter, ModelImageColorizer\n\ndevice.set(device=DeviceId.CPU)\n\nREPO_ID = \"leonelhs/deoldify\"\nMODEL_NAME = \"ColorizeArtistic_gen\"\n", "MODEL_NAME = \"ColorizeArtistic_gen\"\n\n\nclass TaskImageColorizer(Task):\n    def __init__(self):\n        super().__init__()\n        snapshot_folder = snapshot_download(repo_id=REPO_ID)\n        learn = gen_inference_deep(root_folder=Path(snapshot_folder), weights_name=MODEL_NAME)\n        image_filter = ImageFilter(learn=learn)\n        self.colorizer = ModelImageColorizer(image_filter)\n\n    def executeTask(self, image):\n        image = array2image(image)\n        image = self.colorizer.get_colored_image(image, render_factor=35)\n        return np.array(image)", ""]}
{"filename": "faceshine/tasks/zeroscratches/task_erase_scratches.py", "chunked_list": ["#############################################################################\n#\n#   Source from:\n#   https://github.com/leonelhs/zeroscratches\n#   Forked from:\n#   https://github.com/leonelhs/zeroscratches\n#   Reimplemented by: Leonel Hern\u00e1ndez\n#\n##############################################################################\n", "##############################################################################\n\nfrom zeroscratches import EraseScratches\n\nfrom faceshine import array2image\nfrom faceshine.tasks import Task\n\n\nclass TaskEraseScratches(Task):\n\n    def __init__(self):\n        super().__init__()\n        self.scratchEraser = EraseScratches()\n\n    def executeTask(self, image):\n        image = array2image(image)\n        return self.scratchEraser.erase(image)", "class TaskEraseScratches(Task):\n\n    def __init__(self):\n        super().__init__()\n        self.scratchEraser = EraseScratches()\n\n    def executeTask(self, image):\n        image = array2image(image)\n        return self.scratchEraser.erase(image)\n", "\n"]}
{"filename": "faceshine/tasks/zeroscratches/__init__.py", "chunked_list": ["from .task_erase_scratches import TaskEraseScratches\n\n"]}
{"filename": "faceshine/tasks/base/ai_enhancer.py", "chunked_list": ["import abc\n\nimport numpy as np\n\n\nclass Enhancer(metaclass=abc.ABCMeta):\n\n    @classmethod\n    def __subclasshook__(cls, subclass):\n        return (hasattr(subclass, 'enhance') and\n                callable(subclass.enhance) or\n                NotImplemented)\n\n    @abc.abstractmethod\n    def enhance(self, image=None, outscale=None) -> np.array:\n        return NotImplementedError", ""]}
{"filename": "faceshine/tasks/base/task.py", "chunked_list": ["import abc\n\n\nclass Task(metaclass=abc.ABCMeta):\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def __subclasshook__(cls, subclass):\n        return (hasattr(subclass, 'executeTask') and\n                callable(subclass.executeTask) or\n                NotImplemented)\n\n    @abc.abstractmethod\n    def executeTask(self, image):\n        raise NotImplementedError", ""]}
{"filename": "faceshine/tasks/base/__init__.py", "chunked_list": ["from .task import Task\nfrom .ai_preprocces import Preprocess\nfrom .ai_enhancer import Enhancer\nfrom .ai_upsampler import Upsampler\n"]}
{"filename": "faceshine/tasks/base/ai_preprocces.py", "chunked_list": ["import abc\n\nimport numpy as np\n\n\nclass Preprocess(metaclass=abc.ABCMeta):\n\n    @classmethod\n    def __subclasshook__(cls, subclass):\n        return (hasattr(subclass, 'enhance') and\n                callable(subclass.enhance) or\n                NotImplemented)\n\n    @abc.abstractmethod\n    def process(self, image=None) -> np.array:\n        return NotImplementedError", ""]}
{"filename": "faceshine/tasks/base/ai_upsampler.py", "chunked_list": ["import abc\n\nimport numpy as np\n\n\nclass Upsampler(metaclass=abc.ABCMeta):\n\n    @classmethod\n    def __subclasshook__(cls, subclass):\n        return (hasattr(subclass, 'enhance') and\n                callable(subclass.enhance) or\n                NotImplemented)\n\n    @abc.abstractmethod\n    def enhance(self, image=None, outscale=None) -> np.array:\n        return NotImplementedError", ""]}
{"filename": "faceshine/tasks/lowlight/__init__.py", "chunked_list": ["from .model import enhance_net_nopool\nfrom .task_low_light import TaskLowLight\n"]}
{"filename": "faceshine/tasks/lowlight/task_low_light.py", "chunked_list": ["#############################################################################\n#\n#   Source from:\n#   https://github.com/Li-Chongyi/Zero-DCE/\n#   Forked from:\n#   https://github.com/Li-Chongyi/Zero-DCE/\n#   Reimplemented by: Leonel Hern\u00e1ndez\n#\n##############################################################################\nimport logging", "##############################################################################\nimport logging\nimport os.path\n\nimport numpy as np\nimport torch\nimport torch.optim\n\nfrom faceshine import array2image, tensor_to_ndarray\nfrom faceshine.tasks import Task", "from faceshine import array2image, tensor_to_ndarray\nfrom faceshine.tasks import Task\nfrom faceshine.tasks.lowlight.model import enhance_net_nopool\nfrom huggingface_hub import snapshot_download\n\nREPO_ID = \"leonelhs/lowlight\"\nMODEL_NAME = \"Epoch99.pth\"\n\n\nclass TaskLowLight(Task):\n\n    def __init__(self):\n        super().__init__()\n        self.model = enhance_net_nopool().cpu()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        snapshot_folder = snapshot_download(repo_id=REPO_ID)\n        model_path = os.path.join(snapshot_folder, MODEL_NAME)\n        state = torch.load(model_path, map_location=device)\n        self.model.load_state_dict(state)\n\n    def executeTask(self, image):\n        logging.info(\"Running low light enhancement\")\n        image = array2image(image)\n        image = (np.asarray(image) / 255.0)\n        image = torch.from_numpy(image).float()\n        image = image.permute(2, 0, 1)\n        image = image.cpu().unsqueeze(0)\n        _, enhanced_image, _ = self.model(image)\n\n        return tensor_to_ndarray(enhanced_image, nrow=8, padding=2, normalize=False)", "\nclass TaskLowLight(Task):\n\n    def __init__(self):\n        super().__init__()\n        self.model = enhance_net_nopool().cpu()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        snapshot_folder = snapshot_download(repo_id=REPO_ID)\n        model_path = os.path.join(snapshot_folder, MODEL_NAME)\n        state = torch.load(model_path, map_location=device)\n        self.model.load_state_dict(state)\n\n    def executeTask(self, image):\n        logging.info(\"Running low light enhancement\")\n        image = array2image(image)\n        image = (np.asarray(image) / 255.0)\n        image = torch.from_numpy(image).float()\n        image = image.permute(2, 0, 1)\n        image = image.cpu().unsqueeze(0)\n        _, enhanced_image, _ = self.model(image)\n\n        return tensor_to_ndarray(enhanced_image, nrow=8, padding=2, normalize=False)", ""]}
{"filename": "faceshine/tasks/lowlight/model/model.py", "chunked_list": ["# TaskLowLight\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass enhance_net_nopool(nn.Module):\n\n    def __init__(self):\n        super(enhance_net_nopool, self).__init__()\n\n        self.relu = nn.ReLU(inplace=True)\n\n        number_f = 32\n        self.e_conv1 = nn.Conv2d(3, number_f, 3, 1, 1, bias=True)\n        self.e_conv2 = nn.Conv2d(number_f, number_f, 3, 1, 1, bias=True)\n        self.e_conv3 = nn.Conv2d(number_f, number_f, 3, 1, 1, bias=True)\n        self.e_conv4 = nn.Conv2d(number_f, number_f, 3, 1, 1, bias=True)\n        self.e_conv5 = nn.Conv2d(number_f * 2, number_f, 3, 1, 1, bias=True)\n        self.e_conv6 = nn.Conv2d(number_f * 2, number_f, 3, 1, 1, bias=True)\n        self.e_conv7 = nn.Conv2d(number_f * 2, 24, 3, 1, 1, bias=True)\n\n        self.maxpool = nn.MaxPool2d(2, stride=2, return_indices=False, ceil_mode=False)\n        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n\n    def forward(self, x):\n        x1 = self.relu(self.e_conv1(x))\n        # p1 = self.maxpool(x1)\n        x2 = self.relu(self.e_conv2(x1))\n        # p2 = self.maxpool(x2)\n        x3 = self.relu(self.e_conv3(x2))\n        # p3 = self.maxpool(x3)\n        x4 = self.relu(self.e_conv4(x3))\n\n        x5 = self.relu(self.e_conv5(torch.cat([x3, x4], 1)))\n        # x5 = self.upsample(x5)\n        x6 = self.relu(self.e_conv6(torch.cat([x2, x5], 1)))\n\n        x_r = F.tanh(self.e_conv7(torch.cat([x1, x6], 1)))\n        r1, r2, r3, r4, r5, r6, r7, r8 = torch.split(x_r, 3, dim=1)\n\n        x = x + r1 * (torch.pow(x, 2) - x)\n        x = x + r2 * (torch.pow(x, 2) - x)\n        x = x + r3 * (torch.pow(x, 2) - x)\n        enhance_image_1 = x + r4 * (torch.pow(x, 2) - x)\n        x = enhance_image_1 + r5 * (torch.pow(enhance_image_1, 2) - enhance_image_1)\n        x = x + r6 * (torch.pow(x, 2) - x)\n        x = x + r7 * (torch.pow(x, 2) - x)\n        enhance_image = x + r8 * (torch.pow(x, 2) - x)\n        r = torch.cat([r1, r2, r3, r4, r5, r6, r7, r8], 1)\n        return enhance_image_1, enhance_image, r", ""]}
{"filename": "faceshine/tasks/lowlight/model/dataloader.py", "chunked_list": ["# TaskLowLight\nimport glob\nimport random\n\nimport test as np\nimport torch\nimport torch.utils.data as data\nfrom PIL import Image\n\nrandom.seed(1143)", "\nrandom.seed(1143)\n\n\ndef populate_train_list(lowlight_images_path):\n    image_list_lowlight = glob.glob(lowlight_images_path + \"*.jpg\")\n\n    train_list = image_list_lowlight\n\n    random.shuffle(train_list)\n\n    return train_list", "\n\nclass lowlight_loader(data.Dataset):\n\n    def __init__(self, lowlight_images_path):\n        self.train_list = populate_train_list(lowlight_images_path)\n        self.size = 256\n\n        self.data_list = self.train_list\n        print(\"Total training examples:\", len(self.train_list))\n\n    def __getitem__(self, index):\n        data_lowlight_path = self.data_list[index]\n\n        data_lowlight = Image.open(data_lowlight_path)\n\n        data_lowlight = data_lowlight.resize((self.size, self.size), Image.ANTIALIAS)\n\n        data_lowlight = (np.asarray(data_lowlight) / 255.0)\n        data_lowlight = torch.from_numpy(data_lowlight).float()\n\n        return data_lowlight.permute(2, 0, 1)\n\n    def __len__(self):\n        return len(self.data_list)", "\n"]}
{"filename": "faceshine/tasks/lowlight/model/__init__.py", "chunked_list": ["# TaskLowLight\n\nfrom .model import enhance_net_nopool\n"]}
{"filename": "faceshine/tasks/faceparser/__init__.py", "chunked_list": ["from .bisnet.model import BiSeNet\nfrom .task_face_segmentation import TaskFaceSegmentation\n"]}
{"filename": "faceshine/tasks/faceparser/task_face_segmentation.py", "chunked_list": ["#############################################################################\n#\n#   Source from:\n#   https://github.com/leonelhs/face-makeup.PyTorch\n#   Forked from:\n#   https://github.com/zllrunning/face-makeup.PyTorch\n#   Reimplemented by: Leonel Hern\u00e1ndez\n#\n##############################################################################\nimport logging", "##############################################################################\nimport logging\nimport os.path\n\nimport torch\nfrom PIL import Image\n\nfrom faceshine.tasks import Task\nfrom faceshine import image_to_tensor, array2image\nfrom faceshine.tasks.faceparser import BiSeNet", "from faceshine import image_to_tensor, array2image\nfrom faceshine.tasks.faceparser import BiSeNet\nfrom huggingface_hub import snapshot_download\n\nREPO_ID = \"leonelhs/faceparser\"\nMODEL_NAME = \"79999_iter.pth\"\n\n\nclass TaskFaceSegmentation(Task):\n    def __init__(self):\n        super().__init__()\n        self.net = BiSeNet(n_classes=19)\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        snapshot_folder = snapshot_download(repo_id=REPO_ID)\n        model_path = os.path.join(snapshot_folder, MODEL_NAME)\n        self.net.load_state_dict(torch.load(model_path, map_location=device))\n        self.net.eval()\n\n    \"\"\"\n        Predicts image segments needed to create an alpha blending mask. \n        Final mask will be created at client side using this process result\n        :param image: Image file (jpg, png, ...)\n        :returns: A numpy array.\n        \"\"\"\n\n    def executeTask(self, image):\n        logging.info(\"Running face segmentation.\")\n        with torch.no_grad():\n            image = array2image(image)\n            image = image.resize((512, 512), Image.BILINEAR)\n            input_tensor = image_to_tensor(image)\n            input_tensor = torch.unsqueeze(input_tensor, 0)\n            if torch.cuda.is_available():\n                input_tensor = input_tensor.cuda()\n            output = self.net(input_tensor)[0]\n            return output.squeeze(0).cpu().numpy().argmax(0)", "class TaskFaceSegmentation(Task):\n    def __init__(self):\n        super().__init__()\n        self.net = BiSeNet(n_classes=19)\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        snapshot_folder = snapshot_download(repo_id=REPO_ID)\n        model_path = os.path.join(snapshot_folder, MODEL_NAME)\n        self.net.load_state_dict(torch.load(model_path, map_location=device))\n        self.net.eval()\n\n    \"\"\"\n        Predicts image segments needed to create an alpha blending mask. \n        Final mask will be created at client side using this process result\n        :param image: Image file (jpg, png, ...)\n        :returns: A numpy array.\n        \"\"\"\n\n    def executeTask(self, image):\n        logging.info(\"Running face segmentation.\")\n        with torch.no_grad():\n            image = array2image(image)\n            image = image.resize((512, 512), Image.BILINEAR)\n            input_tensor = image_to_tensor(image)\n            input_tensor = torch.unsqueeze(input_tensor, 0)\n            if torch.cuda.is_available():\n                input_tensor = input_tensor.cuda()\n            output = self.net(input_tensor)[0]\n            return output.squeeze(0).cpu().numpy().argmax(0)", ""]}
{"filename": "faceshine/tasks/faceparser/bisnet/model.py", "chunked_list": ["#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .resnet import Resnet18\n# from modules.bn import InPlaceABNSync as BatchNorm2d", "from .resnet import Resnet18\n# from modules.bn import InPlaceABNSync as BatchNorm2d\n\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n        super(ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_chan,\n                out_chan,\n                kernel_size = ks,\n                stride = stride,\n                padding = padding,\n                bias = False)\n        self.bn = nn.BatchNorm2d(out_chan)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = F.relu(self.bn(x))\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)", "\nclass BiSeNetOutput(nn.Module):\n    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n        super(BiSeNetOutput, self).__init__()\n        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.conv_out(x)\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params", "\n\nclass AttentionRefinementModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(AttentionRefinementModule, self).__init__()\n        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size= 1, bias=False)\n        self.bn_atten = nn.BatchNorm2d(out_chan)\n        self.sigmoid_atten = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv(x)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv_atten(atten)\n        atten = self.bn_atten(atten)\n        atten = self.sigmoid_atten(atten)\n        out = torch.mul(feat, atten)\n        return out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)", "\n\nclass ContextPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(ContextPath, self).__init__()\n        self.resnet = Resnet18()\n        self.arm16 = AttentionRefinementModule(256, 128)\n        self.arm32 = AttentionRefinementModule(512, 128)\n        self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_avg = ConvBNReLU(512, 128, ks=1, stride=1, padding=0)\n\n        self.init_weight()\n\n    def forward(self, x):\n        H0, W0 = x.size()[2:]\n        feat8, feat16, feat32 = self.resnet(x)\n        H8, W8 = feat8.size()[2:]\n        H16, W16 = feat16.size()[2:]\n        H32, W32 = feat32.size()[2:]\n\n        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n        avg = self.conv_avg(avg)\n        avg_up = F.interpolate(avg, (H32, W32), mode='nearest')\n\n        feat32_arm = self.arm32(feat32)\n        feat32_sum = feat32_arm + avg_up\n        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode='nearest')\n        feat32_up = self.conv_head32(feat32_up)\n\n        feat16_arm = self.arm16(feat16)\n        feat16_sum = feat16_arm + feat32_up\n        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode='nearest')\n        feat16_up = self.conv_head16(feat16_up)\n\n        return feat8, feat16_up, feat32_up  # x8, x8, x16\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params", "\n\n### This is not used, since I replace this with the resnet feature with the same size\nclass SpatialPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(SpatialPath, self).__init__()\n        self.conv1 = ConvBNReLU(3, 64, ks=7, stride=2, padding=3)\n        self.conv2 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv3 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv_out = ConvBNReLU(64, 128, ks=1, stride=1, padding=0)\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv1(x)\n        feat = self.conv2(feat)\n        feat = self.conv3(feat)\n        feat = self.conv_out(feat)\n        return feat\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params", "\n\nclass FeatureFusionModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(FeatureFusionModule, self).__init__()\n        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n        self.conv1 = nn.Conv2d(out_chan,\n                out_chan//4,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.conv2 = nn.Conv2d(out_chan//4,\n                out_chan,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, fsp, fcp):\n        fcat = torch.cat([fsp, fcp], dim=1)\n        feat = self.convblk(fcat)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv1(atten)\n        atten = self.relu(atten)\n        atten = self.conv2(atten)\n        atten = self.sigmoid(atten)\n        feat_atten = torch.mul(feat, atten)\n        feat_out = feat_atten + feat\n        return feat_out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params", "\n\nclass BiSeNet(nn.Module):\n    def __init__(self, n_classes, *args, **kwargs):\n        super(BiSeNet, self).__init__()\n        self.cp = ContextPath()\n        ## here self.sp is deleted\n        self.ffm = FeatureFusionModule(256, 256)\n        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n        self.conv_out16 = BiSeNetOutput(128, 64, n_classes)\n        self.conv_out32 = BiSeNetOutput(128, 64, n_classes)\n        self.init_weight()\n\n    def forward(self, x):\n        H, W = x.size()[2:]\n        feat_res8, feat_cp8, feat_cp16 = self.cp(x)  # here return res3b1 feature\n        feat_sp = feat_res8  # use res3b1 feature to replace spatial path feature\n        feat_fuse = self.ffm(feat_sp, feat_cp8)\n\n        feat_out = self.conv_out(feat_fuse)\n        feat_out16 = self.conv_out16(feat_cp8)\n        feat_out32 = self.conv_out32(feat_cp16)\n\n        feat_out = F.interpolate(feat_out, (H, W), mode='bilinear', align_corners=True)\n        feat_out16 = F.interpolate(feat_out16, (H, W), mode='bilinear', align_corners=True)\n        feat_out32 = F.interpolate(feat_out32, (H, W), mode='bilinear', align_corners=True)\n        return feat_out, feat_out16, feat_out32\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n        for name, child in self.named_children():\n            child_wd_params, child_nowd_params = child.get_params()\n            if isinstance(child, FeatureFusionModule) or isinstance(child, BiSeNetOutput):\n                lr_mul_wd_params += child_wd_params\n                lr_mul_nowd_params += child_nowd_params\n            else:\n                wd_params += child_wd_params\n                nowd_params += child_nowd_params\n        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params", "\n\nif __name__ == \"__main__\":\n    net = BiSeNet(19)\n    net.cuda()\n    net.eval()\n    in_ten = torch.randn(16, 3, 640, 480).cuda()\n    out, out16, out32 = net(in_ten)\n    print(out.shape)\n\n    net.get_params()", ""]}
{"filename": "faceshine/tasks/faceparser/bisnet/__init__.py", "chunked_list": [""]}
{"filename": "faceshine/tasks/faceparser/bisnet/resnet.py", "chunked_list": ["#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as modelzoo\n\n# from modules.bn import InPlaceABNSync as BatchNorm2d\n", "# from modules.bn import InPlaceABNSync as BatchNorm2d\n\nresnet18_url = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n", "\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_chan, out_chan, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_chan, out_chan, stride)\n        self.bn1 = nn.BatchNorm2d(out_chan)\n        self.conv2 = conv3x3(out_chan, out_chan)\n        self.bn2 = nn.BatchNorm2d(out_chan)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        if in_chan != out_chan or stride != 1:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_chan, out_chan,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_chan),\n            )\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = F.relu(self.bn1(residual))\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n\n        shortcut = x\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = shortcut + residual\n        out = self.relu(out)\n        return out", "\n\ndef create_layer_basic(in_chan, out_chan, bnum, stride=1):\n    layers = [BasicBlock(in_chan, out_chan, stride=stride)]\n    for i in range(bnum - 1):\n        layers.append(BasicBlock(out_chan, out_chan, stride=1))\n    return nn.Sequential(*layers)\n\n\nclass Resnet18(nn.Module):\n    def __init__(self):\n        super(Resnet18, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = create_layer_basic(64, 64, bnum=2, stride=1)\n        self.layer2 = create_layer_basic(64, 128, bnum=2, stride=2)\n        self.layer3 = create_layer_basic(128, 256, bnum=2, stride=2)\n        self.layer4 = create_layer_basic(256, 512, bnum=2, stride=2)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(self.bn1(x))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        feat8 = self.layer2(x)  # 1/8\n        feat16 = self.layer3(feat8)  # 1/16\n        feat32 = self.layer4(feat16)  # 1/32\n        return feat8, feat16, feat32\n\n    def init_weight(self):\n        state_dict = modelzoo.load_url(resnet18_url)\n        self_state_dict = self.state_dict()\n        for k, v in state_dict.items():\n            if 'fc' in k: continue\n            self_state_dict.update({k: v})\n        self.load_state_dict(self_state_dict)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params", "\nclass Resnet18(nn.Module):\n    def __init__(self):\n        super(Resnet18, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = create_layer_basic(64, 64, bnum=2, stride=1)\n        self.layer2 = create_layer_basic(64, 128, bnum=2, stride=2)\n        self.layer3 = create_layer_basic(128, 256, bnum=2, stride=2)\n        self.layer4 = create_layer_basic(256, 512, bnum=2, stride=2)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(self.bn1(x))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        feat8 = self.layer2(x)  # 1/8\n        feat16 = self.layer3(feat8)  # 1/16\n        feat32 = self.layer4(feat16)  # 1/32\n        return feat8, feat16, feat32\n\n    def init_weight(self):\n        state_dict = modelzoo.load_url(resnet18_url)\n        self_state_dict = self.state_dict()\n        for k, v in state_dict.items():\n            if 'fc' in k: continue\n            self_state_dict.update({k: v})\n        self.load_state_dict(self_state_dict)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params", "\n\nif __name__ == \"__main__\":\n    net = Resnet18()\n    x = torch.randn(16, 3, 224, 224)\n    out = net(x)\n    print(out[0].size())\n    print(out[1].size())\n    print(out[2].size())\n    net.get_params()", ""]}
