{"filename": "run_experiment.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"Run a single experiment.\"\"\"\n\nimport dataclasses\nimport json\nimport os\nfrom pathlib import Path\n\nimport scmm as S", "\nimport scmm as S\n\n# pylint:disable=invalid-name\n\n# ssub -t mk2 -n 1 -- python run_experiment.py\nif __name__ == \"__main__\":\n    out, profile, sweep = None, None, False\n    # profile = Path(\"out/profiles/dev\")\n    # out = Path(\"out/training/dev\")\n    # sweep = True\n\n    settings = S.experiments.Settings(\n        # data=S.experiments.DataSettings(Path(\"scmm/tests/data\"), kind=\"test\"),\n        data=S.experiments.DataSettings(\n            Path(\"/home/research-datasets/wikitext103_raw\")\n        ),\n        model=S.models.Settings(\n            hidden_size=128,\n            depth=8,\n            residual=S.models.Residual(norm=\"pre\", alpha=\"mean\"),\n            sequence=S.models.Attention(\n                heads=2, head_size=64, frequencies=128, max_period=1024\n            ),\n            token=S.models.FFN(multiple=4),\n            dtype=\"float32\",\n            vocab_size=None,  # type:ignore[arg-type]\n            seed=None,  # type:ignore[arg-type]\n        ),\n        training=S.training.Settings(\n            batch=S.datasets.BatchSettings(\n                sequences=8, sequence_length=256, overlap_length=32, loop_seed=None\n            ),\n            steps=int(2**20),\n            valid_interval=int(2**14),\n            optimiser=S.training.AdamW(\n                learning_rate=2**-6, learning_rate_decay=2**-16\n            ),\n            loss_scale=1,\n        ),\n        unit_scale=\"0.4\",\n        target=S.pedal.xpu.IpuSettings(iterations_per_loop=int(2**10)),\n        output=S.experiments.OutputSettings(\n            stderr=False,\n            wandb=True,\n            log=out and out / \"log.jsonl\",\n            checkpoint=out and out / \"model.npz\",\n        ),\n        metadata=dict(experiment=\"dev\"),\n        seed=None,  # type:ignore[arg-type]\n    )\n\n    ####################\n    # Common\n\n    if profile:\n        profile.mkdir(parents=True, exist_ok=True)\n        # os.environ[\"POPLIBS_LOG_LEVEL\"] = \"INFO\"\n        os.environ[\"POPLAR_ENGINE_OPTIONS\"] = json.dumps(\n            {\n                \"autoReport.all\": True,\n                # \"autoReport.outputExecutionProfile\": False,\n                \"autoReport.outputArchive\": False,\n                \"autoReport.directory\": str(profile),\n                \"debug.allowOutOfMemory\": True,\n                \"profiler.replicaToProfile\": 0,\n            }\n        )\n        os.environ[\"PVTI_OPTIONS\"] = json.dumps(\n            dict(enable=True, directory=str(profile))\n        )\n        settings = dataclasses.replace(\n            settings,\n            # Switch out the data to avoid a large delay \"loading\"\n            data=S.experiments.DataSettings(Path(\"scmm/tests/data\")),\n            model=dataclasses.replace(settings.model, vocab_size=5008),\n            training=dataclasses.replace(\n                settings.training, steps=2, valid_interval=None\n            ),\n            target=dataclasses.replace(settings.target, iterations_per_loop=int(2)),\n            output=S.experiments.OutputSettings(\n                stderr=True, wandb=False, log=profile / \"log.jsonl\", checkpoint=None\n            ),\n        )\n    else:\n        os.environ[\"TF_POPLAR_FLAGS\"] = (\n            \"--show_progress_bar=false\"\n            f\" --executable_cache_path=/a/scratch/{os.environ['USER']}_research/tmp/cache\"\n        )\n\n    if sweep:\n        sweep_settings = S.experiments.LrSweep(settings, step=4, threshold=0.05, reps=1)\n        S.experiments.find_learning_rate(settings=sweep_settings)\n    else:\n        # Run in subprocess so that the PVTI options \"take\"\n        S.pedal.utility.run_in_subprocess(S.experiments.run, settings=settings)", ""]}
{"filename": "run_sweep.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"Run a multi-axis hyperparameter sweep.\"\"\"\n\nimport copy\nimport dataclasses\nimport itertools as it\nimport multiprocessing\nimport multiprocessing.pool\nimport os", "import multiprocessing.pool\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict, Iterable, List, Optional, Union\n\nimport scmm as S\n\n# pylint:disable=redefined-outer-name\n", "# pylint:disable=redefined-outer-name\n\n\nclass Sweeper:\n    \"\"\"Utility for sweeping multiple settings axes.\"\"\"\n\n    def __init__(\n        self,\n        settings: Union[S.experiments.Settings, S.experiments.LrSweep],\n        n_workers: int,\n        reps: int,\n    ):\n        self.n_workers = n_workers\n        self.reps = reps\n        if isinstance(settings, S.experiments.LrSweep):\n            self.base_settings = settings.base\n            self.lr_settings: Optional[S.experiments.LrSweep] = settings\n        else:\n            self.base_settings = settings\n            self.lr_settings = None\n        self.axes: List[List[Dict[str, Any]]] = []\n\n    def add(self, values: Iterable[Dict[str, Any]]) -> None:\n        \"\"\"Add an independent axis to the sweep.\"\"\"\n        self.axes.append(list(values))\n\n    @staticmethod\n    def _recursive_assign(\n        settings: S.experiments.Settings, path: str, value: Any\n    ) -> None:\n        # Nested lookup\n        *prefix, last = path.split(\".\")\n        node = settings\n        for key in prefix:\n            node = getattr(node, key)\n\n        # Dataclass type checking\n        expected_type: Any = {f.name: f.type for f in dataclasses.fields(node)}.get(\n            last\n        )\n        if expected_type is float:\n            expected_type = (int, float)\n        if getattr(expected_type, \"__origin__\", None) is Union:\n            expected_type = expected_type.__args__\n        if not isinstance(value, expected_type):\n            raise ValueError(\n                f\"Expected {path} to be {expected_type}, actual {value} (type {type(value)})\"\n            )\n\n        setattr(node, last, value)\n\n    @property\n    def configs(self) -> Iterable[Union[S.experiments.Settings, S.experiments.LrSweep]]:\n        \"\"\"Iterate through all settings configurations included in the sweep.\"\"\"\n        for overrides in it.product(*self.axes):\n            settings = copy.deepcopy(self.base_settings)\n            for override in overrides:\n                for path, value in override.items():\n                    self._recursive_assign(settings, path, value)\n            if self.lr_settings is not None:\n                yield dataclasses.replace(self.lr_settings, base=settings)\n            else:\n                yield settings\n\n    def run(self) -> None:\n        \"\"\"Run a parallel sweep.\"\"\"\n        # os.environ[\"TMPDIR\"] = \"/localdata/tmp\"\n        os.environ[\"TF_POPLAR_FLAGS\"] = (\n            \"--show_progress_bar=false\"\n            f\" --executable_cache_path=/a/scratch/{os.environ['USER']}_research/tmp/cache/sweep\"\n        )\n        with multiprocessing.pool.ThreadPool(self.n_workers) as pool:\n            for _ in range(self.reps):\n                for setting in self.configs:\n                    target = (\n                        S.experiments.run\n                        if isinstance(setting, S.experiments.Settings)\n                        else S.experiments.find_learning_rate\n                    )\n                    pool.apply_async(\n                        S.pedal.utility.run_in_subprocess,\n                        kwds=dict(command=target, settings=setting),\n                    )\n            pool.close()\n            pool.join()", "\n\n# Run sweep\n#   ssub -n 16 -p ipu-large -- python run_sweep.py\n\nif __name__ == \"__main__\":\n    settings = S.experiments.Settings(\n        data=S.experiments.DataSettings(\n            Path(\"/home/research-datasets/wikitext103_raw\")\n        ),\n        model=S.models.Settings(\n            hidden_size=128,\n            depth=8,\n            residual=S.models.Residual(norm=\"pre\", alpha=\"mean\"),\n            sequence=S.models.Attention(\n                heads=2, head_size=64, frequencies=128, max_period=1024\n            ),\n            token=S.models.FFN(multiple=4),\n            dtype=\"float32\",\n            vocab_size=None,  # type:ignore[arg-type]\n            seed=None,  # type:ignore[arg-type]\n        ),\n        training=S.training.Settings(\n            batch=S.datasets.BatchSettings(\n                sequences=8,\n                sequence_length=256,\n                overlap_length=32,\n                loop_seed=None,\n            ),\n            steps=int(2**19),\n            valid_interval=int(2**14),\n            optimiser=S.training.AdamW(\n                learning_rate=2**-14,\n                learning_rate_decay=2**-16,\n            ),\n            loss_scale=1,\n        ),\n        unit_scale=None,\n        target=S.pedal.xpu.IpuSettings(\n            iterations_per_loop=int(2**10),\n            stochastic_rounding=True,\n        ),\n        output=S.experiments.OutputSettings(\n            wandb=True, stderr=False, log=None, checkpoint=None\n        ),\n        seed=None,  # type:ignore[arg-type]\n        metadata=dict(experiment=\"20230115_large_p0\"),\n    )\n\n    sweeper = Sweeper(\n        S.experiments.LrSweep(settings, step=2, threshold=0.1, reps=3),\n        n_workers=16,\n        reps=1,\n    )\n\n    def _all_settings() -> Iterable[Dict[str, Any]]:\n        # pylint:disable=too-many-nested-blocks\n        attention = S.models.Attention(\n            heads=2, head_size=64, frequencies=128, max_period=1024\n        )\n        conv = S.models.Conv(kernel_size=7, groups=8)\n        rnn = S.models.RNN(rebias=1)\n        for sequence in [rnn, conv, attention]:\n            for norm in [\"pre\", \"post\"]:\n                for dtype in [\"float16\", \"float32\"]:\n                    for unit_scale in [None, \"0.4\"]:\n                        for loss_scale in [1, 2048]:\n                            sequence_kind = sequence.kind  # type:ignore[attr-defined]\n                            if (\n                                unit_scale or dtype == \"float32\"\n                            ) and loss_scale != 1:  # unnecessary\n                                continue\n\n                            if norm == \"post\" and sequence_kind != \"attention\":\n                                continue  # only run post-norm for attention\n\n                            yield {\n                                \"model.residual.norm\": norm,\n                                \"model.depth\": 2 if sequence_kind == \"rnn\" else 8,\n                                \"model.sequence\": sequence,\n                                \"model.dtype\": dtype,\n                                \"unit_scale\": unit_scale,\n                                \"training.loss_scale\": loss_scale,\n                                \"training.optimiser.learning_rate\": (\n                                    2**-14 if unit_scale is None else 2**-8\n                                ),\n                            }\n\n    sweeper.add(_all_settings())\n\n    # This also runs basic checks on `configs` (e.g. in --dry-run)\n    print(\n        f\"Sweeping {sum(1 for _ in sweeper.configs)} settings, {sweeper.reps} reps,\"\n        f\" as {sweeper.base_settings.metadata['experiment']!r}\",\n        file=sys.stderr,\n    )\n    if not set(sys.argv) & {\"-d\", \"--dry-run\", \"--dryrun\"}:\n        # subprocess.check_call([\"ulimit\", \"-u\", \"16384\"], shell=True)\n        sweeper.run()", ""]}
{"filename": "scmm/layers.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"General purpose layers and functions.\"\"\"\n\nfrom typing import Iterable, List, Optional, Tuple, Type\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n", "from tensorflow import keras\n\n\ndef batched_gather(tables: tf.Tensor, indices: tf.Tensor) -> tf.Tensor:\n    \"\"\"Simulate tf.gather(tables, indices, batch_dims=indices.ndim).\n\n    Better compilation on IPU vs `tf.gather(logp, ids, batch_dims=2)`\n    \"\"\"\n    # Implemented here and in uscale.ops to avoid circular dependency issues\n    # pylint:disable=R0801\n    assert len(tables.shape) == len(indices.shape) + 1\n    # Use a one-hot encoding to save code memory\n    return tf.squeeze(\n        tf.one_hot(indices, tables.shape[-1], dtype=tables.dtype)[..., tf.newaxis, :]\n        @ tables[..., tf.newaxis],\n        [-1, -2],\n    )", "\n\ndef softmax_cross_entropy(\n    scores: tf.Tensor, ids: tf.Tensor, mask: tf.Tensor\n) -> Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Compute masked softmax cross entropy loss.\n\n    returns -- (average_loss, n_items) -- average_loss always in fp32\n    \"\"\"\n    assert mask.shape == ids.shape, \"mask should match target ids\"\n    # Use float32 for local computation - keeping things simple\n    logp = tf.nn.log_softmax(tf.cast(scores, tf.float32))\n    target_logp = batched_gather(logp, ids)\n    total_loss = tf.reduce_sum(tf.cast(mask, target_logp.dtype) * -target_logp)\n    n_ids = tf.reduce_sum(tf.cast(mask, tf.int32))\n    loss = total_loss / tf.cast(n_ids, total_loss.dtype)\n    return loss, n_ids", "\n\nclass LayerNormalization(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"A FP16-safe variant of keras.layers.LayerNormalization.\"\"\"\n\n    def __init__(self, epsilon: float = 0.001, dtype: tf.DType = tf.float32):\n        super().__init__(dtype=dtype)\n        self.epsilon = epsilon\n        self.beta: tf.Variable = None\n        self.beta_initializer = keras.initializers.zeros()\n        self.gamma: tf.Variable = None\n        self.gamma_initializer = keras.initializers.ones()\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        self.beta = self.add_weight(\n            \"beta\", shape=input_shape[-1], initializer=self.beta_initializer\n        )\n        self.gamma = self.add_weight(\n            \"gamma\", shape=input_shape[-1], initializer=self.gamma_initializer\n        )\n\n    def _normalize(self, inputs: tf.Tensor) -> tf.Tensor:\n        inputs_fp32 = tf.cast(inputs, tf.float32)\n        z = inputs_fp32 - tf.reduce_mean(inputs_fp32, axis=-1, keepdims=True)\n        normed = z / tf.sqrt(\n            tf.reduce_mean(z**2, axis=-1, keepdims=True) + self.epsilon\n        )\n        return tf.cast(normed, inputs.dtype)\n\n    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n        return self.gamma * self._normalize(inputs) + self.beta", "\n\nclass ResidualLayer(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"A residual layer, supporting PreNorm, PostNorm, NoNorm & interpolation.\n\n    norm_type -- None | \"pre\" | \"post\"\n\n    alpha -- None | <float>  -- interpolation constant, higher to incorporate\n                                more of the residual branch, lower to preserve\n                                the skip connection.\n\n        y = sqrt(1 - alpha) * x + sqrt(alpha) * f(x)\n    \"\"\"\n\n    def __init__(\n        self,\n        body: keras.layers.Layer,\n        norm_type: Optional[str],\n        alpha: Optional[float],\n        dtype: tf.DType = tf.float32,\n        norm_cls: Type[keras.layers.Layer] = LayerNormalization,\n    ):\n        super().__init__(dtype=dtype)\n        self.body = body\n        self.norm_type = norm_type\n        self.alpha_value = alpha\n        self.alpha: tf.Variable = None\n        assert norm_type in {None, \"pre\", \"post\"}, f\"unexpected norm_type {norm_type}\"\n        self.norm_cls = norm_cls\n        self.norm: keras.layers.Layer = None\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        self.body.build(input_shape)\n        if self.norm_type is not None:\n            self.norm = self.norm_cls(dtype=self.dtype)\n            self.norm.build(input_shape)\n        if self.alpha_value is not None:\n            # Turn alpha into a non-trainable variable, for sake of outlining\n            self.alpha = self.add_weight(\n                name=\"alpha\",\n                shape=(),\n                initializer=keras.initializers.constant(self.alpha_value),\n                trainable=False,\n            )\n\n    def call(self, x: tf.Tensor) -> tf.Tensor:\n        branch = x\n        if self.norm_type == \"pre\":\n            branch = self.norm(branch)\n\n        branch = self.body(branch)\n\n        if self.alpha is not None:\n            y = (1 - self.alpha) ** 0.5 * x + self.alpha**0.5 * branch\n        else:\n            y = x + branch\n\n        if self.norm_type == \"post\":\n            y = self.norm(y)\n        return y", "\n\nclass FFNLayer(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"A pointwise expansion FFN layer (a la Transformer, https://arxiv.org/abs/1706.03762).\"\"\"\n\n    def __init__(\n        self,\n        multiple: float,\n        dtype: tf.DType = tf.float32,\n        seeds: Optional[Tuple[int, int]] = None,\n    ):\n        super().__init__(dtype=dtype)\n        self.multiple = multiple\n        self.seeds = seeds or (None, None)\n        self.up: Optional[keras.layers.Layer] = None  # pylint:disable=invalid-name\n        self.down: Optional[keras.layers.Layer] = None\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        hidden_size = input_shape[-1]\n        intermediate_size = int(self.multiple * hidden_size)\n        self.up = keras.layers.Dense(\n            intermediate_size,\n            dtype=self.dtype,\n            kernel_initializer=keras.initializers.GlorotUniform(seed=self.seeds[0]),\n        )\n        self.up.build(input_shape[:-1] + (hidden_size,))\n        self.down = keras.layers.Dense(\n            hidden_size,\n            dtype=self.dtype,\n            kernel_initializer=keras.initializers.GlorotUniform(seed=self.seeds[1]),\n        )\n        self.down.build(input_shape[:-1] + (intermediate_size,))\n\n    def call(self, x: tf.Tensor) -> tf.Tensor:\n        return self.down(tf.nn.relu(self.up(x)))  # type:ignore[misc]", "\n\nclass PadAndShiftLayer(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"Shifts sequence features one place to the right with a trainable padding vector.\"\"\"\n\n    def __init__(self, dtype: tf.DType = tf.float32) -> None:\n        super().__init__(dtype=dtype)\n        self.padding: tf.Variable = None\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        if len(input_shape) != 3:\n            raise ValueError(\n                f\"Input should be 3D (batch, sequence, feature), actual shape {input_shape}\"\n            )\n        self.padding = self.add_weight(\n            name=\"padding\",\n            shape=input_shape[-1],\n            initializer=keras.initializers.zeros,\n        )\n\n    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n        pad = tf.tile(self.padding[tf.newaxis, tf.newaxis], [inputs.shape[0], 1, 1])\n        return tf.concat([pad, inputs[:, :-1, :]], axis=1)", "\n\nclass Isotropic(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"Like keras.models.Sequential, but isotropic & with friendly names for each layer.\"\"\"\n\n    def __init__(self, dtype: tf.DType = tf.float32, **layers: keras.layers.Layer):\n        super().__init__(dtype=dtype)\n        self._layers = layers\n        for name, layer in layers.items():\n            setattr(self, name, layer)\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        for layer in self._layers.values():\n            layer.build(input_shape)\n\n    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n        outputs = inputs\n        for layer in self._layers.values():\n            outputs = layer(outputs)\n        return outputs", "\n\n####################\n# Attention\n\n\ndef sinusoid_embedding(\n    sequence_length: int, frequencies: int, max_period: int\n) -> np.ndarray:\n    \"\"\"Generate a family of sin/cos embeddings.\n\n    See \"Attention Is All You Need\", Vaswani et al., section 3.5.\n\n    sequence_length -- output dimension (number of indices)\n\n    frequencies -- number of components to generate\n\n    max_period -- the period (in indices) of the lowest frequency component\n\n    returns -- array(sequence_length x frequencies)\n    \"\"\"\n    index = np.arange(frequencies)\n    frequency = np.pi * (2 / max_period) ** ((index // 2) / (frequencies // 2 - 1))\n    phase = np.pi / 2 * (index % 2)\n    time = np.arange(sequence_length)\n    return np.sin(frequency * time[:, np.newaxis] + phase)", "\n\ndef relative_causal_reshape(scores: tf.Tensor) -> tf.Tensor:\n    \"\"\"Transform relative scores to an attention matrix.\n\n    Fills the lower-left quadrant of the result with scores\n\n        result[..., i, j] = scores[..., i, i - j]\n    \"\"\"\n    sequence_length = scores.shape[-1]\n    ndim = len(scores.shape)\n\n    padded = tf.pad(scores[..., ::-1], [(0, 0)] * (ndim - 1) + [(0, sequence_length)])\n\n    # A reshaping and slicing trick to move to relative positions\n    tmp = tf.reshape(padded, padded.shape[:-2] + (2 * sequence_length**2,))\n    tmp = tmp[..., :-sequence_length]\n    tmp = tf.reshape(tmp, tmp.shape[:-1] + (sequence_length, 2 * sequence_length - 1))\n    tmp = tmp[..., sequence_length - 1 :]\n\n    return tmp", "\n\ndef causal_mask(attention: tf.Tensor, mask_value: float) -> tf.Tensor:\n    \"\"\"Apply a causal mask to an attention matrix of shape (*, L, L).\"\"\"\n    sequence_length = attention.shape[-1]\n    return attention + tf.constant(\n        np.triu(np.full((sequence_length, sequence_length), mask_value), k=1),\n        dtype=attention.dtype,\n    )\n", "\n\nclass MultiHeadAttention(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"Multi-head self attention a la Transformer.\n\n    With causal masking.\n\n    With relative-positional embeddings a la Transformer XL.\n    \"\"\"\n\n    # pylint:disable=too-many-instance-attributes\n\n    def __init__(\n        self,\n        heads: int,\n        head_size: int,\n        frequencies: int,\n        max_period: int,\n        dtype: tf.DType = tf.float32,\n        seeds: Optional[Tuple[int, int, int]] = None,\n    ):\n        super().__init__(dtype=dtype)\n        self.heads = heads\n        self.head_size = head_size\n        self.frequencies = frequencies\n        self.max_period = max_period\n        self.seeds = (None, None, None) if seeds is None else seeds\n        self.qkv: tf.Variable = None\n        self.q_bias: tf.Variable = None\n        self.positional: tf.Variable = None\n        self.out: keras.layers.Layer = None\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        input_size = input_shape[-1]\n        qkv_scale = np.sqrt(3) * input_size**-0.5\n        self.qkv = self.add_weight(\n            name=\"qkv\",\n            shape=(input_size, 3, self.heads, self.head_size),\n            initializer=keras.initializers.random_uniform(\n                -qkv_scale, qkv_scale, seed=self.seeds[0]\n            ),\n        )\n        self.q_bias = self.add_weight(\n            name=\"q_bias\",\n            shape=(self.heads, self.head_size),\n            initializer=keras.initializers.zeros(),\n        )\n        positional_scale = np.sqrt(3) * self.frequencies**-0.5\n        self.positional = self.add_weight(\n            name=\"positional\",\n            shape=(self.frequencies, self.heads, self.head_size),\n            initializer=keras.initializers.random_uniform(\n                -positional_scale, positional_scale, seed=self.seeds[1]\n            ),\n        )\n        self.out = keras.layers.Dense(\n            input_size,\n            dtype=self.dtype,\n            kernel_initializer=keras.initializers.GlorotUniform(seed=self.seeds[2]),\n        )\n        self.out.build(input_shape[:-1] + (self.heads * self.head_size,))\n\n    def _positional_weights(self, query: tf.Tensor) -> tf.Tensor:\n        sequence_length = query.shape[-2]\n        sins = tf.constant(\n            sinusoid_embedding(sequence_length, self.frequencies, self.max_period),\n            dtype=query.dtype,\n        )\n        embeddings = tf.einsum(\"sf,fnh->nsh\", sins, self.positional)\n        scores = tf.einsum(\"bnqh,nvh->bnqv\", query, embeddings) * self.head_size**-0.5\n        return relative_causal_reshape(scores)\n\n    def call(self, input: tf.Tensor) -> tf.Tensor:\n        # pylint:disable=invalid-name\n        q, k, v = tf.unstack(tf.einsum(\"bsx,xAnh -> Abnsh\", input, self.qkv))\n        q += self.q_bias[:, tf.newaxis, :]\n        a = tf.einsum(\"bnqh,bnkh->bnqk\", q, k) * self.head_size**-0.5\n        a += self._positional_weights(q)\n        # Note: oddly, -1e3 can be insufficient in FP16 with no LS, causing \"cheating\"\n        a = causal_mask(a, mask_value=-3e4)\n        a = tf.nn.softmax(a, axis=-1)\n        o = tf.einsum(\"bnqk,bnkh->bqnh\", a, v)\n        return self.out(tf.reshape(o, o.shape[:-2] + (self.head_size * self.heads,)))", "\n\n####################\n# RNN\n\n\nclass RecurrentHighwayCell(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"A recurrent highway cell from https://arxiv.org/abs/1607.03474.\"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        rebias: float,\n        dtype: tf.DType = tf.float32,\n        seed: Optional[int] = None,\n    ):\n        super().__init__(name=type(self).__name__, dtype=dtype)\n        self.hidden_size = hidden_size\n        self.carry_rebias = rebias\n        self.update_rebias = -rebias\n        self.seed = seed\n        self.gates: tf.Variable = None\n        self.gates_bias: tf.Variable = None\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        input_size = input_shape[-1]\n        scale = (3 / (input_size + self.hidden_size)) ** 0.5\n        self.gates = self.add_weight(\n            \"gates\",\n            shape=(2, input_size + self.hidden_size, self.hidden_size),\n            initializer=keras.initializers.random_uniform(\n                -scale, scale, seed=self.seed\n            ),\n        )\n        self.gates_bias = self.add_weight(\n            \"gates_bias\",\n            shape=(2, self.hidden_size),\n            initializer=keras.initializers.zeros(),\n        )\n\n    def call(self, input: tf.Tensor, hidden: tf.Tensor) -> tf.Tensor:\n        transform, update = tf.unstack(\n            tf.concat([input, hidden], axis=1) @ self.gates\n            + self.gates_bias[:, tf.newaxis]\n        )\n        update = tf.sigmoid(update + self.update_rebias)\n        return (1 - update) * hidden + update * tf.tanh(transform)", "\n\nclass RNN(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"A basic, unidirectional RNN.\n\n    Expects inputs of shape (batch, sequence, feature), and produces outputs of shape\n    (batch, sequence, hidden).\n\n    Note that this implementation has patholological memory usage on IPU, due to missing\n    recomputation.\n    \"\"\"\n\n    def __init__(self, cell: keras.layers.Layer):\n        super().__init__(name=type(self).__name__, dtype=cell.dtype)\n        self.cell = cell\n        self.initial_hidden: tf.Variable = None\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        self.cell.build(tf.TensorShape([input_shape[0], input_shape[2]]))\n        self.initial_hidden = self.add_weight(\n            \"initial_hidden\",\n            shape=(self.cell.hidden_size,),\n            initializer=keras.initializers.zeros(),\n        )\n\n    def call(self, input: tf.Tensor) -> tf.Tensor:\n        # Note: sbh = (sequence, batch, hidden)\n        input_sbh = tf.transpose(input, (1, 0, 2))\n        output_sbh = tf.scan(\n            lambda hidden, input: self.cell(input, hidden),\n            input_sbh,\n            initializer=tf.tile(self.initial_hidden[tf.newaxis], (input.shape[0], 1)),\n        )\n        return tf.transpose(output_sbh, (1, 0, 2))", "\n\n####################\n# Optimizers\n\n\nclass _Optimizer(keras.optimizers.Optimizer):  # type:ignore[misc]\n    \"\"\"A small extension of the keras base optimizer.\"\"\"\n\n    # pylint:disable=too-few-public-methods\n\n    def __init__(self, name: str):\n        super().__init__(name=name)\n        self._step_variable: tf.Variable = None\n\n    @property\n    def _step(self) -> tf.Variable:\n        if self._step_variable is None:\n            with tf.name_scope(self._name):\n                self._step_variable = tf.Variable(\n                    0, dtype=tf.int32, name=\"step\", trainable=False\n                )\n        return self._step_variable\n\n    @staticmethod\n    def _hyperparameter(value: float) -> tf.Variable:\n        \"\"\"Create a training hyperparmaeter, as a Variable (for sake of executable caching).\"\"\"\n        return tf.Variable(value, dtype=tf.float32, trainable=False)\n\n    def _add_slot_with_dtype(\n        self, variable: tf.Variable, name: str, dtype: tf.DType\n    ) -> tf.Variable:\n        # pylint:disable=protected-access\n        key = variable._shared_name if variable._in_graph_mode else variable._unique_id\n        result = self._slots.setdefault(key, {}).get(name)\n        if result is None:\n            result = tf.Variable(\n                tf.zeros(variable.shape, dtype=dtype),\n                name=f\"{key}/{name}\",\n                trainable=False,\n            )\n            self._slots[key][name] = result\n        return result", "\n\nclass SgdM(_Optimizer):\n    \"\"\"SGD with momentum and loss scaling support.\"\"\"\n\n    # pylint:disable=too-few-public-methods\n\n    def __init__(\n        self,\n        learning_rate: float = 0.01,\n        learning_rate_decay: float = 0.0,\n        scale_vector_learning_rate: bool = False,\n        loss_scale: float = 1,\n        momentum: float = 0,\n        name: str = \"SGD\",\n    ):\n        super().__init__(name=name)\n        self.learning_rate = self._hyperparameter(learning_rate)\n        self.learning_rate_decay = self._hyperparameter(learning_rate_decay)\n        self.scale_vector_learning_rate = scale_vector_learning_rate\n        self.loss_scale = self._hyperparameter(loss_scale)\n        self.momentum = self._hyperparameter(momentum)\n\n    def _update(\n        self, gradient: tf.Tensor, variable: tf.Variable, scale: tf.Tensor\n    ) -> List[tf.Operation]:\n        if isinstance(gradient, tf.IndexedSlices):\n            # Convert to dense gradient, which is probably fine\n            gradient = tf.math.unsorted_segment_sum(\n                gradient.values, gradient.indices, gradient.shape[0]\n            )\n\n        with tf.name_scope(self._name):\n            momentum_prev = self._add_slot_with_dtype(\n                variable, \"momentum\", dtype=variable.dtype\n            )\n\n        # This FP32 dance probably isn't of much importance/help here\n        momentum_next = self.momentum * tf.cast(momentum_prev, tf.float32) + tf.cast(\n            gradient, tf.float32\n        )\n        if self.scale_vector_learning_rate and len(variable.shape) == 1:\n            scale = scale / np.sqrt(variable.shape[0])\n        variable_next = tf.cast(variable, tf.float32) - scale * momentum_next\n        return [\n            variable.assign(tf.cast(variable_next, variable.dtype)),\n            momentum_prev.assign(tf.cast(momentum_next, momentum_prev.dtype)),\n        ]\n\n    def apply_gradients(\n        self,\n        grads_and_vars: Iterable[Tuple[tf.Tensor, tf.Variable]],\n        name: Optional[str] = None,\n    ) -> tf.Operation:\n        step_prev = self._step\n        decay = tf.pow(2.0, -tf.cast(step_prev, tf.float32) * self.learning_rate_decay)\n        step = step_prev.assign(step_prev + 1)\n        return tf.group(\n            step,\n            *(\n                self._update(\n                    grad, variable, scale=self.learning_rate * decay / self.loss_scale\n                )\n                for grad, variable in grads_and_vars\n            ),\n            name=name,\n        )", "\n\nclass AdamW(_Optimizer):\n    \"\"\"AdamW (https://arxiv.org/abs/1711.05101).\"\"\"\n\n    # pylint:disable=too-few-public-methods\n\n    def __init__(  # pylint:disable=too-many-arguments\n        self,\n        learning_rate: float = 0.001,\n        learning_rate_decay: float = 0.0,\n        scale_vector_learning_rate: bool = False,\n        weight_decay: float = 0.004,\n        beta_1: float = 0.9,\n        beta_2: float = 0.999,\n        epsilon: float = 1e-7,\n        name: str = \"AdamW\",\n    ):\n        super().__init__(name=name)\n        self.learning_rate = self._hyperparameter(learning_rate)\n        self.learning_rate_decay = self._hyperparameter(learning_rate_decay)\n        self.scale_vector_learning_rate = scale_vector_learning_rate\n        self.weight_decay = self._hyperparameter(weight_decay)\n        self.beta_1 = self._hyperparameter(beta_1)\n        self.beta_2 = self._hyperparameter(beta_2)\n        self.epsilon = self._hyperparameter(epsilon)\n\n    def _update(\n        self, gradient: tf.Tensor, variable: tf.Variable, scale: tf.Tensor\n    ) -> List[tf.Operation]:\n        if self.scale_vector_learning_rate and len(variable.shape) == 1:\n            scale = scale / np.sqrt(variable.shape[0])\n\n        if isinstance(gradient, tf.IndexedSlices):\n            # Convert to dense gradient, which is probably fine\n            gradient = tf.math.unsorted_segment_sum(\n                gradient.values, gradient.indices, gradient.shape[0]\n            )\n\n        with tf.name_scope(self._name):\n            m_prev = self._add_slot_with_dtype(variable, \"adam_m\", dtype=variable.dtype)\n            v_prev = self._add_slot_with_dtype(variable, \"adam_v\", dtype=tf.float32)\n\n        gradient_fp32 = tf.cast(gradient, tf.float32)\n        m_next = (\n            self.beta_1 * tf.cast(m_prev, tf.float32)\n            + (1 - self.beta_1) * gradient_fp32\n        )\n        v_next = (\n            self.beta_2 * tf.cast(v_prev, tf.float32)\n            + (1 - self.beta_2) * gradient_fp32**2\n        )\n        variable_fp32 = tf.cast(variable, tf.float32)\n        variable_next = (\n            variable_fp32\n            - self.learning_rate * self.weight_decay * variable_fp32\n            - scale * m_next / (tf.sqrt(v_next) + self.epsilon)\n        )\n        return [\n            variable.assign(tf.cast(variable_next, variable.dtype)),\n            m_prev.assign(tf.cast(m_next, m_prev.dtype)),\n            v_prev.assign(tf.cast(v_next, v_prev.dtype)),\n        ]\n\n    def apply_gradients(\n        self,\n        grads_and_vars: Iterable[Tuple[tf.Tensor, tf.Variable]],\n        name: Optional[str] = None,\n    ) -> tf.Operation:\n        step_prev = self._step\n        decay = tf.pow(2.0, -tf.cast(step_prev, tf.float32) * self.learning_rate_decay)\n        step = step_prev.assign(step_prev + 1)\n        scale = (\n            self.learning_rate\n            * decay\n            * tf.sqrt(1 - self.beta_2 ** tf.cast(step, tf.float32))\n            / (1 - self.beta_1 ** tf.cast(step, tf.float32))\n        )\n        return tf.group(\n            step,\n            *(\n                self._update(grad, variable, scale=scale)\n                for grad, variable in grads_and_vars\n            ),\n            name=name,\n        )", ""]}
{"filename": "scmm/models.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"Core model definitions.\"\"\"\n\nimport itertools as it\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Dict, Iterator, List, Optional, Tuple, Union\n\nimport numpy as np\nimport tensorflow as tf", "import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom . import layers, uscale\nfrom .pedal import utility, xpu\n\n\n@dataclass\nclass Residual:\n    \"\"\"Residual settings.\"\"\"\n\n    norm: Optional[str]  # None | \"pre\" | \"post\"\n    alpha: Union[None, str, float]  # None | \"mean\" | <float>", "@dataclass\nclass Residual:\n    \"\"\"Residual settings.\"\"\"\n\n    norm: Optional[str]  # None | \"pre\" | \"post\"\n    alpha: Union[None, str, float]  # None | \"mean\" | <float>\n\n\n@dataclass\nclass Conv:\n    \"\"\"Convolution (sequence mixing) settings.\"\"\"\n\n    kernel_size: int\n    groups: int\n    kind: str = \"conv\"", "@dataclass\nclass Conv:\n    \"\"\"Convolution (sequence mixing) settings.\"\"\"\n\n    kernel_size: int\n    groups: int\n    kind: str = \"conv\"\n\n\n@dataclass\nclass Attention:\n    \"\"\"Attention (sequence mixing) settings.\"\"\"\n\n    heads: int\n    head_size: int\n    frequencies: int\n    max_period: int\n    kind: str = \"attention\"", "\n@dataclass\nclass Attention:\n    \"\"\"Attention (sequence mixing) settings.\"\"\"\n\n    heads: int\n    head_size: int\n    frequencies: int\n    max_period: int\n    kind: str = \"attention\"", "\n\n@dataclass\nclass RNN:\n    \"\"\"Recurrence (sequence mixing) settings.\"\"\"\n\n    rebias: float\n    kind: str = \"rnn\"\n\n", "\n\n@dataclass\nclass FFN:\n    \"\"\"FFN (token mixing) settings.\"\"\"\n\n    multiple: float\n    kind: str = \"ffn\"\n\n", "\n\n@dataclass\nclass Settings:\n    \"\"\"Model configuration.\"\"\"\n\n    vocab_size: int\n    hidden_size: int\n    depth: int\n    residual: Optional[Residual]\n    sequence: Union[Conv, Attention, RNN]\n    token: Optional[FFN]\n    dtype: str\n    seed: int", "\n\nclass _ModelFactory:  # pylint:disable=missing-function-docstring\n    \"\"\"Builds the various kinds of model from settings.\"\"\"\n\n    def __init__(self, settings: Settings, unit_scale: bool, seeds: Iterator[int]):\n        self.settings = settings\n        self.unit_scale = unit_scale\n        self.dtype = tf.as_dtype(settings.dtype)\n        self.seeds = seeds\n\n    def kernel_initializer(self) -> keras.initializers.Initializer:\n        assert not self.unit_scale, \"unit scale shouldn't use Glorot\"\n        return keras.initializers.GlorotUniform(seed=next(self.seeds))\n\n    def embed(self) -> keras.layers.Layer:\n        if self.unit_scale:\n            return uscale.layers.Embedding(\n                self.settings.vocab_size,\n                self.settings.hidden_size,\n                dtype=self.dtype,\n                seed=next(self.seeds),\n            )\n        # Unit variance embeddings make sense in any case\n        return keras.layers.Embedding(\n            self.settings.vocab_size,\n            self.settings.hidden_size,\n            dtype=self.dtype,\n            embeddings_initializer=keras.initializers.RandomUniform(\n                -np.sqrt(3), np.sqrt(3), seed=next(self.seeds)\n            ),\n        )\n\n    def conv(self, settings: Conv) -> keras.layers.Layer:\n        if self.unit_scale:\n            return uscale.layers.CausalConv1D(\n                self.settings.hidden_size,\n                kernel_size=settings.kernel_size,\n                groups=settings.groups,\n                activation=\"relu\",\n                dtype=self.dtype,\n                seed=next(self.seeds),\n            )\n        return keras.layers.Conv1D(\n            self.settings.hidden_size,\n            kernel_size=settings.kernel_size,\n            groups=settings.groups,\n            activation=\"relu\",\n            padding=\"causal\",\n            dtype=self.dtype,\n            kernel_initializer=self.kernel_initializer(),\n        )\n\n    def attention(self, settings: Attention) -> keras.layers.Layer:\n        cls = (\n            uscale.layers.MultiHeadAttention\n            if self.unit_scale\n            else layers.MultiHeadAttention\n        )\n        return cls(\n            heads=settings.heads,\n            head_size=settings.head_size,\n            frequencies=settings.frequencies,\n            max_period=settings.max_period,\n            dtype=self.dtype,\n            seeds=(next(self.seeds), next(self.seeds), next(self.seeds)),\n        )\n\n    def rnn(self, settings: RNN) -> keras.layers.Layer:\n        (cls, cell_cls) = (\n            (uscale.layers.RNN, uscale.layers.RecurrentHighwayCell)\n            if self.unit_scale\n            else (layers.RNN, layers.RecurrentHighwayCell)\n        )\n        return cls(\n            cell_cls(\n                hidden_size=self.settings.hidden_size,\n                rebias=settings.rebias,\n                dtype=self.dtype,\n                seed=next(self.seeds),\n            )\n        )\n\n    def sequence_layer(self) -> keras.layers.Layer:\n        if isinstance(self.settings.sequence, Conv):\n            return self.conv(self.settings.sequence)\n        if isinstance(self.settings.sequence, Attention):\n            return self.attention(self.settings.sequence)\n        if isinstance(self.settings.sequence, RNN):\n            return self.rnn(self.settings.sequence)\n        assert False, f\"unexpected sequence settings {self.settings.sequence}\"\n\n    def token_layer(self) -> keras.layers.Layer:\n        assert self.settings.token is not None\n        cls = uscale.layers.FFNLayer if self.unit_scale else layers.FFNLayer\n        return cls(\n            self.settings.token.multiple,\n            dtype=self.dtype,\n            seeds=(next(self.seeds), next(self.seeds)),\n        )\n\n    def residual(self, body: keras.layers.Layer, index: int) -> keras.layers.Layer:\n        if self.settings.residual is None:\n            return body\n\n        if self.settings.residual.alpha is None:\n            alpha = None\n        elif self.settings.residual.alpha == \"mean\":\n            alpha = 1 / (1 + index)\n        elif isinstance(self.settings.residual.alpha, (float, int)):\n            alpha = self.settings.residual.alpha\n        else:\n            assert False, f\"unexpected residual.alpha {self.settings.residual.alpha}\"\n\n        layer_cls = (\n            uscale.layers.ResidualLayer if self.unit_scale else layers.ResidualLayer\n        )\n        return layer_cls(\n            body, norm_type=self.settings.residual.norm, alpha=alpha, dtype=self.dtype\n        )\n\n    def trunk_layer(self, index: Iterator[int]) -> keras.layers.Layer:\n        # Relying heavily on dict ordering...\n        parts = dict(sequence=self.residual(self.sequence_layer(), next(index)))\n        if self.settings.token:\n            parts[\"token\"] = self.residual(self.token_layer(), next(index))\n        return layers.Isotropic(dtype=self.dtype, **parts)\n\n    def trunk(self) -> List[keras.layers.Layer]:\n        index = iter(it.count())\n        return [self.trunk_layer(index) for _ in range(self.settings.depth)]\n\n    def norm(self) -> keras.layers.Layer:\n        return (\n            uscale.layers.LayerNormalization(dtype=self.dtype)\n            if self.unit_scale\n            else layers.LayerNormalization(dtype=self.dtype)\n        )\n\n    def predict(self) -> keras.layers.Layer:\n        if self.unit_scale:\n            return uscale.layers.Dense(\n                self.settings.vocab_size,\n                scale_for=\"separate\",\n                dtype=self.dtype,\n                seed=next(self.seeds),\n            )\n        return keras.layers.Dense(\n            self.settings.vocab_size,\n            dtype=self.dtype,\n            kernel_initializer=self.kernel_initializer(),\n        )\n\n    def predict_padding(self) -> keras.layers.Layer:\n        if self.unit_scale:\n            return uscale.layers.PadAndShiftLayer(dtype=self.dtype)\n        return layers.PadAndShiftLayer(dtype=self.dtype)\n\n    def loss(\n        self,\n    ) -> Callable[[tf.Tensor, tf.Tensor, tf.Tensor], Tuple[tf.Tensor, tf.Tensor]]:\n        return (\n            uscale.ops.softmax_cross_entropy\n            if self.unit_scale\n            else layers.softmax_cross_entropy\n        )", "\n\nclass Model(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"Base language model.\"\"\"\n\n    # pylint:disable=too-many-instance-attributes\n\n    def __init__(self, settings: Settings, unit_scale: bool):\n        super().__init__()\n        self.settings = settings\n\n        factory = _ModelFactory(\n            settings,\n            unit_scale=unit_scale,\n            seeds=iter(utility.split_seed(settings.seed, 1000)),  # plenty of seeds\n        )\n        self.embed = factory.embed()\n        self.embed_norm = factory.norm()\n        self.trunk = factory.trunk()\n        self.norm = factory.norm()\n        self.predict = factory.predict()\n        self.predict_padding = factory.predict_padding()\n        self.loss = factory.loss()\n\n        # Our base model is always pre-built\n        self.build((None, None))\n        for name, layer in utility.named_layers(self):\n            assert layer.built, f\"layer {name} ({layer}) was not built\"\n\n        for layer in self.trunk:\n            xpu.current_context().outline(layer)\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n\n        self.embed.build(input_shape)\n        hidden_shape = tuple(input_shape) + (self.settings.hidden_size,)\n        self.embed_norm.build(hidden_shape)\n        for layer in self.trunk:\n            layer.build(hidden_shape)\n        self.norm.build(hidden_shape)\n        self.predict.build(hidden_shape)\n        prediction_shape = tuple(input_shape) + (self.settings.vocab_size,)\n        self.predict_padding.build(prediction_shape)\n\n    def run(self, tokens: tf.Tensor, mask: tf.Tensor) -> Dict[str, tf.Tensor]:\n        \"\"\"Run the language model for cross entropy loss.\"\"\"\n\n        hiddens = self.embed_norm(self.embed(tokens))\n        for layer in self.trunk:\n            hiddens = layer(hiddens)\n        scores = self.predict_padding(self.predict(self.norm(hiddens)))\n        loss, n_tokens = self.loss(scores, tokens, mask)\n        return dict(\n            loss=loss,\n            n_tokens=n_tokens,\n            act_hiddens_final=tf.math.reduce_std(hiddens),\n        )\n\n    def weight_stats(self) -> Dict[str, Any]:\n        \"\"\"Stats regarding weights in the model.\"\"\"\n        shapes = {k: tuple(v.shape) for k, v in utility.named_weights(self)}\n        return dict(\n            n_weights=sum(np.prod(v) for v in shapes.values()),\n            n_weights_no_embedding=sum(\n                np.prod(v)\n                for k, v in shapes.items()\n                if k not in {\"embed.embeddings\", \"predict.kernel\"}\n            ),\n            weight_shapes=shapes,\n        )\n\n    def save(self) -> Dict[str, np.ndarray]:\n        \"\"\"Save model weights to a dictionary of numpy arrays.\"\"\"\n        return {k: np.array(v) for k, v in utility.named_weights(self)}\n\n    def load(self, weights: Dict[str, np.ndarray]) -> None:\n        \"\"\"Load model weights from a dictionary of numpy arrays.\"\"\"\n        variables = dict(utility.named_weights(self))\n        if variables.keys() != weights.keys():\n            raise ValueError(\n                \"Load does not set correct weights\"\n                f\", extra: {weights.keys() - variables.keys()}\"\n                f\", missing: {variables.keys() - weights.keys()}\"\n            )\n        for k in weights:\n            variables[k].assign(weights[k])", ""]}
{"filename": "scmm/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"Scaled matmuls experimentation.\"\"\"\n\nfrom . import (  # NOQA: F401\n    datasets,\n    experiments,\n    layers,\n    models,\n    pedal,", "    models,\n    pedal,\n    training,\n    uscale,\n)\n"]}
{"filename": "scmm/training.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"Top-line training logic.\"\"\"\n\nimport collections\nimport dataclasses\nimport datetime\nimport itertools as it\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Iterable, Optional, Union", "from dataclasses import dataclass\nfrom typing import Any, Dict, Iterable, Optional, Union\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom . import datasets, layers, models, uscale\nfrom .pedal import xpu\n\n", "\n\n@dataclass\nclass SgdM:\n    \"\"\"SGD with momentum.\"\"\"\n\n    learning_rate: float\n    learning_rate_decay: float\n    momentum: float\n    kind: str = \"sgdm\"", "\n\n@dataclass\nclass AdamW:\n    \"\"\"Adam with weight decay.\"\"\"\n\n    learning_rate: float\n    learning_rate_decay: float\n    beta_1: float = 0.9\n    beta_2: float = 0.999\n    weight_decay: float = 0\n    kind: str = \"adamw\"", "\n\nOptimiser = Union[AdamW, SgdM]\n\n\n@dataclass\nclass Settings:\n    \"\"\"Training settings.\"\"\"\n\n    batch: datasets.BatchSettings\n    steps: int\n    valid_interval: Optional[int]\n    optimiser: Optimiser\n    loss_scale: float", "\n\ndef eval_summary(results: Iterable[datasets.Batch]) -> Dict[str, float]:\n    \"\"\"Summarise evaluation results, weighted averages according to \"n_tokens\".\"\"\"\n\n    total_tokens = 0\n    stats: Dict[str, float] = collections.defaultdict(float)\n    for result in results:\n        n_tokens = int(result[\"n_tokens\"])\n        total_tokens += n_tokens\n        for key, value in result.items():\n            if key != \"n_tokens\":\n                stats[key] += value * n_tokens\n\n    # Normalisation\n    for key in stats:\n        stats[key] /= total_tokens\n    stats[\"n_tokens\"] = total_tokens\n    return stats", "\n\ndef _get_optimiser(\n    settings: Optimiser,\n    loss_scale: float,\n    unit_scale: bool,\n) -> keras.optimizers.Optimizer:\n    if isinstance(settings, AdamW):\n        # Note that Adam updates are invariant to fixed gradient scale, so\n        # loss_scale is safely ignored\n        return layers.AdamW(\n            learning_rate=settings.learning_rate,\n            learning_rate_decay=settings.learning_rate_decay,\n            scale_vector_learning_rate=unit_scale,\n            weight_decay=settings.weight_decay,\n            beta_1=settings.beta_1,\n            beta_2=settings.beta_2,\n        )\n    if isinstance(settings, SgdM):\n        return layers.SgdM(\n            learning_rate=settings.learning_rate,\n            learning_rate_decay=settings.learning_rate_decay,\n            scale_vector_learning_rate=unit_scale,\n            loss_scale=loss_scale,\n            momentum=settings.momentum,\n        )\n    assert False, f\"unknown optimiser {settings}\"", "\n\ndef train(\n    model: models.Model,\n    data: datasets.Data,\n    context: xpu.Context,\n    settings: Settings,\n    unit_scale: bool,\n) -> Iterable[Dict[str, Any]]:\n    \"\"\"Train a model.\"\"\"\n\n    assert (\n        settings.batch.loop_seed is not None\n    ), \"please specify a seed for training batches\"\n\n    optimiser = _get_optimiser(\n        settings.optimiser, loss_scale=settings.loss_scale, unit_scale=unit_scale\n    )\n\n    def _log(kind: str, step: int, data: Dict[str, Any]) -> Dict[str, Any]:\n        return dict(kind=kind, step=step, time=datetime.datetime.now(), **data)\n\n    def _validate(step: int, test: bool) -> Iterable[Dict[str, Any]]:\n        for part in [\"valid\", \"train\"] + ([\"test\"] if test else []):\n            batch_settings = settings.batch\n            if part in [\"valid\", \"test\"]:\n                batch_settings = dataclasses.replace(batch_settings, loop_seed=None)\n            batches = data.batches(part, batch_settings)\n            if part == \"train\":\n                batches = it.islice(\n                    batches,\n                    1 + data.parts[\"valid\"].size // settings.batch.target_tokens,\n                )\n            results = eval_summary(context.loop(model.run, batches))\n            results = {f\"{part}_{k}\": v for k, v in results.items()}\n            yield _log(f\"eval_{part}\", step, results)\n\n    def _training_step(**batch: tf.Tensor) -> Dict[str, tf.Tensor]:\n        with tf.GradientTape() as tape:\n            result = model.run(**batch)\n            loss = uscale.ops.scaling(backward=settings.loss_scale)(result[\"loss\"])\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimiser.apply_gradients(zip(gradients, model.trainable_variables))\n        return result\n\n    train_steps = iter(\n        context.loop(_training_step, data.batches(\"train\", settings.batch))\n    )\n    for step in it.count():\n        if step >= settings.steps:\n            yield from _validate(step, test=True)\n            break\n        if settings.valid_interval is not None and step % settings.valid_interval == 0:\n            yield from _validate(step, test=False)\n        results = next(train_steps)  # pylint:disable=stop-iteration-return\n        yield _log(\"train_step\", step, {k: v.tolist() for k, v in results.items()})", ""]}
{"filename": "scmm/datasets.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"Data loading and batching.\"\"\"\n\nimport itertools as it\nimport json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, Iterable, Optional, Tuple\n", "from typing import Dict, Iterable, Optional, Tuple\n\nimport numpy as np\n\nBatch = Dict[str, np.ndarray]\nVocab = Tuple[str, ...]\n\n\ndef to_ids(data: str, vocab: Vocab, dtype: np.dtype) -> np.ndarray:\n    \"\"\"Use a (complete) vocabulary to map characters onto term IDs.\"\"\"\n    assert len(vocab) < np.iinfo(dtype).max\n    ch_to_idx = {ch: idx for idx, ch in enumerate(vocab)}\n    return np.array([ch_to_idx[ch] for ch in data], dtype=dtype)", "def to_ids(data: str, vocab: Vocab, dtype: np.dtype) -> np.ndarray:\n    \"\"\"Use a (complete) vocabulary to map characters onto term IDs.\"\"\"\n    assert len(vocab) < np.iinfo(dtype).max\n    ch_to_idx = {ch: idx for idx, ch in enumerate(vocab)}\n    return np.array([ch_to_idx[ch] for ch in data], dtype=dtype)\n\n\ndef to_str(terms: np.ndarray, vocab: Vocab) -> str:\n    \"\"\"Map term IDs back to characters.\"\"\"\n    return \"\".join(vocab[idx] for idx in terms)", "\n\n@dataclass\nclass BatchSettings:\n    \"\"\"Settings for a stream of batches.\"\"\"\n\n    sequences: int\n    sequence_length: int\n    overlap_length: int\n    loop_seed: Optional[int]\n\n    @property\n    def shape(self) -> Tuple[int, int]:\n        \"\"\"The 2D shape of a batch.\"\"\"\n        return (self.sequences, self.sequence_length)\n\n    @property\n    def target_length(self) -> int:\n        \"\"\"The maximum number of target tokens per sequence.\"\"\"\n        return self.sequence_length - self.overlap_length\n\n    @property\n    def target_tokens(self) -> int:\n        \"\"\"The maximum number of target tokens.\"\"\"\n        return self.sequences * self.target_length", "\n\n@dataclass\nclass Data:\n    \"\"\"A dataset that can generate batches.\"\"\"\n\n    vocab: Vocab\n    parts: Dict[str, np.ndarray]\n\n    def batches(self, part: str, settings: BatchSettings) -> Iterable[Batch]:\n        \"\"\"Batch with overlapping sequences.\n\n        Note - if `loop_seed` is non-None, generates an infinite stream of batches, sampled\n        with replacement.\n        \"\"\"\n\n        data = self.parts[part]\n        batch_tokens = []\n        batch_mask = []\n        idxs = np.arange(settings.sequence_length)\n        if settings.loop_seed is None:\n            starts: Iterable[int] = range(0, len(data), settings.target_length)\n        else:\n            random = np.random.Generator(np.random.PCG64(settings.loop_seed))\n            starts = (\n                random.integers(len(data) - settings.target_length) for _ in it.count()\n            )\n\n        for start in starts:\n            begin = max(0, start - settings.overlap_length)\n            sequence = data[begin : start + settings.target_length]\n            # \"token padding\"\n            npad = settings.sequence_length - len(sequence)\n            sequence = np.pad(sequence, ((0, npad),))\n            mask = ((start - begin) <= idxs) & (idxs < (len(sequence) - npad))\n            batch_tokens.append(sequence.astype(np.int32))\n            batch_mask.append(mask.astype(np.int32))\n            if settings.sequences <= len(batch_tokens):\n                yield dict(tokens=np.stack(batch_tokens), mask=np.stack(batch_mask))\n                batch_tokens.clear()\n                batch_mask.clear()\n\n        # Incomplete final batch - needs \"sequence padding\"\n        if batch_tokens:\n            npad = settings.sequences - len(batch_tokens)\n            batch_tokens.extend(npad * [batch_tokens[-1]])\n            batch_mask.extend(npad * [np.zeros_like(batch_mask[-1])])\n            yield dict(tokens=np.stack(batch_tokens), mask=np.stack(batch_mask))", "\n\ndef load_character(root: Path, **parts: str) -> Data:\n    \"\"\"Load a character-based dataset.\n\n    e.g. given parts=dict(train=\"train.txt\", valid=\"valid.txt\")\n\n    root/\n        vocab.json\n        train.txt\n        valid.txt\n    \"\"\"\n    vocab = tuple(json.loads((root / \"vocab.json\").read_text()))\n    return Data(\n        vocab=vocab,\n        parts={\n            name: to_ids((root / path).read_text(\"utf8\"), vocab, np.uint16)\n            for name, path in parts.items()\n        },\n    )", ""]}
{"filename": "scmm/experiments.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"Top-level experiment running.\"\"\"\n\nimport contextlib\nimport copy\nimport dataclasses\nimport itertools as it\nimport json\nimport os", "import json\nimport os\nimport sys\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, Generator, Iterable, Optional, Tuple\n\nimport numpy as np\nimport wandb\n", "import wandb\n\nfrom . import datasets, models, training\nfrom .pedal import utility, xpu\n\n\n@contextlib.contextmanager\ndef log_wandb() -> Generator[utility.Logger, None, None]:\n    \"\"\"Log to weights & biases.\"\"\"\n\n    def _log(item: Dict[str, Any]) -> None:\n        if item[\"kind\"] == \"settings\":\n            Path(\"out\").mkdir(exist_ok=True, parents=True)\n            wandb.init(\n                config=utility.remove_keys(item, \"kind\"),\n                dir=\"out\",\n                project=\"scaled-matmuls\",\n                reinit=True,\n            )\n        elif item[\"kind\"] == \"stats\":\n            wandb.run.summary.update(  # type:ignore[union-attr]\n                utility.remove_keys(item, \"kind\")\n            )\n        elif item[\"kind\"] == \"train_step\":\n            pass  # skip training steps (too large)\n        else:\n            wandb.log(item, step=item[\"step\"])\n\n    try:\n        yield _log\n    except Exception as exc:\n        wandb.run.summary.update(dict(error=repr(exc)))  # type:ignore[union-attr]\n        wandb.finish(1)\n        raise\n    # Always call finish(), otherwise we hang (when started in a subprocess from a sweep)\n    wandb.finish(0)", "\n\n@contextlib.contextmanager\ndef log_jsonl(path: Path) -> Generator[utility.Logger, None, None]:\n    \"\"\"Log to file.\"\"\"\n    path.parent.mkdir(exist_ok=True, parents=True)\n    with path.open(\"w\") as f:\n        yield lambda item: print(json.dumps(item, default=utility.to_jsonable), file=f)\n\n\ndef log_checkpoint(path: Path, model: models.Model) -> utility.Logger:\n    \"\"\"Save model checkpoints whenever validation runs.\"\"\"\n\n    def log(item: Dict[str, Any]) -> None:\n        if item[\"kind\"] == \"eval_valid\":\n            np.savez(path, step=item[\"step\"], **model.save())\n\n    return log", "\n\ndef log_checkpoint(path: Path, model: models.Model) -> utility.Logger:\n    \"\"\"Save model checkpoints whenever validation runs.\"\"\"\n\n    def log(item: Dict[str, Any]) -> None:\n        if item[\"kind\"] == \"eval_valid\":\n            np.savez(path, step=item[\"step\"], **model.save())\n\n    return log", "\n\ndef log_stderr(item: Dict[str, Any]) -> None:\n    \"\"\"Log to terminal.\"\"\"\n    if item[\"kind\"] == \"train_step\":\n        return\n    print(\n        str(item) + \" \" * 20,\n        file=sys.stderr,\n        end=\"\\r\" if item[\"kind\"] == \"train_step\" else \"\\n\",\n    )", "\n\n@dataclass\nclass DataSettings:\n    \"\"\"Dataset settings.\"\"\"\n\n    path: Path\n    kind: str = \"wikitext-103-raw\"\n\n", "\n\n@dataclass\nclass OutputSettings:\n    \"\"\"Output control settings.\"\"\"\n\n    stderr: bool\n    wandb: bool\n    log: Optional[Path]\n    checkpoint: Optional[Path]", "\n\n@dataclass\nclass Settings:\n    \"\"\"Top-level settings.\"\"\"\n\n    data: DataSettings\n    model: models.Settings\n    training: training.Settings\n    unit_scale: Optional[str]\n    target: xpu.Settings\n    output: OutputSettings\n    metadata: Dict[str, Any]\n    seed: int\n\n    def set_defaults(self, data: datasets.Data) -> None:\n        \"\"\"Fill in all optional fields.\"\"\"\n        # Seeds\n        if self.seed is None:\n            self.seed = int(np.random.SeedSequence().generate_state(1)[0])\n        model_seed, batching_seed = utility.split_seed(self.seed, 2)\n        if self.model.seed is None:\n            self.model.seed = model_seed\n        if self.training.batch.loop_seed is None:\n            self.training.batch.loop_seed = batching_seed\n\n        # Model\n        if self.model.vocab_size is None:\n            self.model.vocab_size = len(data.vocab)\n\n        # Metadata\n        if \"SSUB_UID\" in os.environ:\n            self.metadata.setdefault(\"ssub_id\", os.environ[\"SSUB_UID\"])\n        if \"SLURM_JOB_ID\" in os.environ:\n            self.metadata.setdefault(\"slurm_job_id\", os.environ[\"SLURM_JOB_ID\"])", "\n\ndef _loggers(settings: OutputSettings, model: models.Model) -> Iterable[utility.Logger]:\n    if settings.stderr:\n        yield log_stderr\n    if settings.wandb:\n        yield log_wandb()\n    if settings.log:\n        yield log_jsonl(settings.log)\n    if settings.checkpoint:\n        yield log_checkpoint(settings.checkpoint, model)", "\n\ndef _settings_line(settings: Settings) -> Dict[str, Any]:\n    return dict(\n        kind=\"settings\",\n        **utility.remove_keys(dataclasses.asdict(settings), \"output\"),\n    )\n\n\ndef run(settings: Settings) -> Dict[str, Any]:\n    \"\"\"Run an experiment, logging results as requested.\"\"\"\n    data = datasets.load_character(\n        settings.data.path, train=\"train.txt\", valid=\"valid.txt\", test=\"test.txt\"\n    )\n    settings = copy.deepcopy(settings)\n    settings.set_defaults(data)\n    assert settings.unit_scale in {None, \"0.4\"}\n\n    last_eval_valid: Optional[Dict[str, Any]] = None\n    with xpu.context(settings.target) as context:\n        model = models.Model(settings.model, unit_scale=bool(settings.unit_scale))\n        with utility.logging(*_loggers(settings.output, model)) as log:\n            log(_settings_line(settings))\n            log(dict(kind=\"stats\", **model.weight_stats()))\n            for item in training.train(\n                model, data, context, settings.training, bool(settings.unit_scale)\n            ):\n                log(item)\n                if item[\"kind\"] == \"eval_valid\":\n                    last_eval_valid = item\n    assert last_eval_valid is not None\n    return last_eval_valid", "\ndef run(settings: Settings) -> Dict[str, Any]:\n    \"\"\"Run an experiment, logging results as requested.\"\"\"\n    data = datasets.load_character(\n        settings.data.path, train=\"train.txt\", valid=\"valid.txt\", test=\"test.txt\"\n    )\n    settings = copy.deepcopy(settings)\n    settings.set_defaults(data)\n    assert settings.unit_scale in {None, \"0.4\"}\n\n    last_eval_valid: Optional[Dict[str, Any]] = None\n    with xpu.context(settings.target) as context:\n        model = models.Model(settings.model, unit_scale=bool(settings.unit_scale))\n        with utility.logging(*_loggers(settings.output, model)) as log:\n            log(_settings_line(settings))\n            log(dict(kind=\"stats\", **model.weight_stats()))\n            for item in training.train(\n                model, data, context, settings.training, bool(settings.unit_scale)\n            ):\n                log(item)\n                if item[\"kind\"] == \"eval_valid\":\n                    last_eval_valid = item\n    assert last_eval_valid is not None\n    return last_eval_valid", "\n\n####################\n# LR sweep\n\n\n@dataclass\nclass LrSweep:\n    \"\"\"Learning rate sweep settings.\"\"\"\n\n    base: Settings\n    step: float\n    threshold: float\n    reps: int", "\n\ndef find_learning_rate(\n    settings: LrSweep, run_in_subprocess: bool = True\n) -> Tuple[Settings, float]:\n    \"\"\"Perform a LR sweep, starting from `base`.\n\n    Tries: initial, initial * step, initial * step^2, ...\n\n    Until the validation loss is more than `best_loss + threshold`.\n\n    Run in subprocess (by default) for sake of isolation & memory use.\n    \"\"\"\n\n    def run_test(test_settings: Settings) -> float:\n        if run_in_subprocess:\n            # Run in subprocess for sake of isolation & memory use\n            results = utility.run_in_subprocess(\n                run, settings=test_settings\n            )  # pragma: no cover\n        else:\n            results = run(settings=test_settings)\n        return results[\"valid_loss\"]  # type:ignore[no-any-return]\n\n    best_loss, best_settings = None, None\n    for n in it.count():\n        test_settings = copy.deepcopy(settings.base)\n        test_settings.training.optimiser.learning_rate *= settings.step**n\n        loss = np.median([run_test(test_settings) for _ in range(settings.reps)])\n        print(\n            f\"LR {test_settings.training.optimiser.learning_rate} -> {loss}\",\n            file=sys.stderr,\n        )\n        if best_loss is None or loss < best_loss:\n            best_loss = loss\n            best_settings = test_settings\n        if np.isnan(loss) or best_loss + settings.threshold < loss:\n            return best_settings, best_loss\n    assert False, \"unreachable code (infinite loop)\"", ""]}
{"filename": "scmm/pedal/xpu.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"General utilities to unify CPU/IPU programming.\"\"\"\n\nfrom dataclasses import dataclass\nfrom types import TracebackType\nfrom typing import Any, Callable, Dict, Iterable, Iterator, Optional, Type, Union\n\nimport numpy as np\nimport tensorflow as tf", "import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\ntry:\n    from tensorflow.python import ipu\n\n    IPU = True\nexcept ImportError:  # pragma: no cover\n    IPU = False", "\n\nFunction = Callable[..., Any]\nOperation = Callable[..., Dict[str, tf.Tensor]]\nBatch = Dict[str, np.ndarray]\nFunctionCache = Callable[[Any], Callable[[Function], Function]]\n\n\ndef _make_cache(**function_args: Any) -> FunctionCache:\n    \"\"\"Make a decorator that calls tf.function, with a user-keyed cache.\n\n    E.g.\n\n        cache = make_cache(experimental_compile=True)\n\n        body = ...\n\n        @cache(key=(\"model\", body))\n        def model(x: tf.Tensor) -> tf.Tensor:\n            return 2 * body(x)\n    \"\"\"\n    _cache: Dict[Any, Function] = {}\n\n    def wrap(key: Any) -> Callable[[Function], Function]:\n        def wrapper(fn: Operation) -> Operation:\n            if key not in _cache:\n                _cache[key] = tf.function(**function_args)(fn)\n            return _cache[key]\n\n        return wrapper\n\n    return wrap", "def _make_cache(**function_args: Any) -> FunctionCache:\n    \"\"\"Make a decorator that calls tf.function, with a user-keyed cache.\n\n    E.g.\n\n        cache = make_cache(experimental_compile=True)\n\n        body = ...\n\n        @cache(key=(\"model\", body))\n        def model(x: tf.Tensor) -> tf.Tensor:\n            return 2 * body(x)\n    \"\"\"\n    _cache: Dict[Any, Function] = {}\n\n    def wrap(key: Any) -> Callable[[Function], Function]:\n        def wrapper(fn: Operation) -> Operation:\n            if key not in _cache:\n                _cache[key] = tf.function(**function_args)(fn)\n            return _cache[key]\n\n        return wrapper\n\n    return wrap", "\n\n@dataclass\nclass CpuSettings:\n    \"\"\"CPU-specific settings.\"\"\"\n\n    compile: bool = False\n\n    type: str = \"cpu\"\n", "\n\n@dataclass\nclass IpuSettings:\n    \"\"\"IPU-specific settings.\"\"\"\n\n    iterations_per_loop: int\n    available_memory_proportion: Optional[float] = None\n    stochastic_rounding: bool = False\n\n    type: str = \"ipu\"", "\n\nSettings = Union[CpuSettings, IpuSettings]\n\n\nclass Context:\n    \"\"\"Manages target setup and a cache for compiled functions.\"\"\"\n\n    _CURRENT: Optional[\"Context\"] = None\n\n    def __init__(self, strategy: tf.distribute.Strategy, compile: bool):\n        self.strategy = strategy\n        self._scope = self.strategy.scope()\n        self._cache = (\n            _make_cache(experimental_compile=True)\n            if compile\n            else (lambda key: lambda fn: fn)\n        )\n\n    def __enter__(self) -> \"Context\":\n        assert Context._CURRENT is None, \"xpu.context scopes cannot be nested\"\n        Context._CURRENT = self\n        self._scope.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        self._scope.__exit__(exc_type, exc_val, exc_tb)\n        assert Context._CURRENT is self, \"exiting a scope with the wrong context\"\n        Context._CURRENT = None\n\n    def loop(self, operation: Operation, inputs: Iterable[Batch]) -> Iterable[Batch]:\n        \"\"\"Stream inputs into an operation and return all outputs.\n\n        operation -- callable as `result = operation(**input)`,\n                        where `result` is a `dict`\n        \"\"\"\n        return loop_cpu(operation, inputs, strategy=self.strategy, cache=self._cache)\n\n    @staticmethod\n    def outline(layer: keras.layers.Layer) -> None:\n        \"\"\"Mark a layer for outlining on IPU, do nothing on CPU.\"\"\"", "\n\ndef context(settings: Settings) -> Context:\n    \"\"\"Create an execution context with the given settings.\n\n    Should generally be used in an immediate `with` scope, e.g.\n\n        with xpu.context(xpu.CpuSettings(compile=False)) as context:\n            ...\n            # also accessible as xpu.current_context()\n    \"\"\"\n    if isinstance(settings, CpuSettings):\n        return Context(tf.distribute.OneDeviceStrategy(\"\"), compile=settings.compile)\n    if isinstance(settings, IpuSettings):\n        if not IPU:  # pragma: no cover\n            raise ValueError(\n                \"Cannot create IPU context - tensorflow.python.ipu could not be imported\"\n            )\n        return _create_ipu_context(settings)\n    assert False, f\"Unexpected Context settings type {settings}\"", "\n\ndef current_context() -> Context:\n    \"\"\"Get the currently in-scope Context.\"\"\"\n    # pylint:disable=protected-access\n    assert Context._CURRENT is not None, \"there is no context in scope\"\n    return Context._CURRENT\n\n\ndef loop_cpu(\n    operation: Operation,\n    inputs: Iterable[Batch],\n    strategy: tf.distribute.Strategy,\n    cache: FunctionCache,\n) -> Iterable[Batch]:\n    \"\"\"Stream inputs into an operation and return all outputs.\n\n    operation -- callable as `result = operation(**input)`,\n                    where `result` is a `dict`\n    \"\"\"\n    fn = cache(key=operation)(operation)  # type:ignore[call-arg]\n    for input_ in inputs:\n        yield {k: np.array(v) for k, v in strategy.run(fn, kwargs=input_).items()}", "\ndef loop_cpu(\n    operation: Operation,\n    inputs: Iterable[Batch],\n    strategy: tf.distribute.Strategy,\n    cache: FunctionCache,\n) -> Iterable[Batch]:\n    \"\"\"Stream inputs into an operation and return all outputs.\n\n    operation -- callable as `result = operation(**input)`,\n                    where `result` is a `dict`\n    \"\"\"\n    fn = cache(key=operation)(operation)  # type:ignore[call-arg]\n    for input_ in inputs:\n        yield {k: np.array(v) for k, v in strategy.run(fn, kwargs=input_).items()}", "\n\nif IPU:\n\n    class _IpuContext(Context):\n        def __init__(self, settings: IpuSettings):\n            super().__init__(ipu.ipu_strategy.IPUStrategy(), compile=True)\n            self.settings = settings\n\n        def loop(\n            self, operation: Operation, inputs: Iterable[Batch]\n        ) -> Iterable[Batch]:\n            return loop_ipu(\n                operation,\n                inputs,\n                strategy=self.strategy,\n                cache=self._cache,\n                iterations_per_loop=self.settings.iterations_per_loop,\n            )\n\n        @staticmethod\n        def outline(layer: keras.layers.Layer) -> None:\n            inner_call = layer.call\n\n            def outlined_call(*args: Any, **kwargs: Any) -> Any:\n                @ipu.outlined_function  # type:ignore[misc]\n                def call() -> Any:\n                    return inner_call(*args, **kwargs)\n\n                return call()\n\n            layer.call = outlined_call\n\n    def _create_ipu_context(settings: IpuSettings) -> Context:\n        config = ipu.config.IPUConfig()\n        config.auto_select_ipus = 1\n        config.floating_point_behaviour.esr = (\n            ipu.config.StochasticRoundingBehaviour.from_bool(\n                settings.stochastic_rounding\n            )\n        )\n        config.device_connection.type = ipu.config.DeviceConnectionType.ON_DEMAND\n        if settings.available_memory_proportion is not None:\n            config.matmuls.poplar_options[\"availableMemoryProportion\"] = str(\n                settings.available_memory_proportion\n            )\n        ipu.utils.configure_ipu_system(config)\n        return _IpuContext(settings)\n\n    def _padded_dataset(inputs: Iterable[Batch]) -> tf.data.Dataset:\n        iterator = iter(inputs)\n        head = next(iterator)\n\n        def generator() -> Iterable[Dict[str, np.ndarray]]:  # pragma: no cover\n            yield dict(**head, _pad=np.array(False))\n            for item in iterator:\n                yield dict(**item, _pad=np.array(False))\n            while True:  # padding\n                yield dict(**head, _pad=np.array(True))\n\n        signature = {\n            k: tf.TensorSpec(shape=v.shape, dtype=v.dtype) for k, v in head.items()\n        }\n        signature[\"_pad\"] = tf.TensorSpec(shape=(), dtype=np.bool)\n        return tf.data.Dataset.from_generator(generator, output_signature=signature)\n\n    def loop_ipu(\n        operation: Operation,\n        inputs: Iterable[Batch],\n        strategy: tf.distribute.Strategy,\n        cache: FunctionCache,\n        iterations_per_loop: int,\n    ) -> Iterable[Dict[str, np.ndarray]]:\n        \"\"\"Stream inputs into an operation and return all outputs.\n\n        operation -- callable as `result = operation(**input)`,\n                     where `result` is a `dict`\n        \"\"\"\n\n        @cache(  # type:ignore[call-arg]\n            key=(\"loop_ipu\", operation, iterations_per_loop)\n        )\n        def _loop(\n            iterator: Iterator[Dict[str, tf.Tensor]],\n            outfeed: ipu.ipu_outfeed_queue.IPUOutfeedQueue,\n        ) -> None:  # pragma: no cover\n            for _ in tf.range(iterations_per_loop):\n                batch = next(iterator)\n                pad = batch.pop(\"_pad\")\n                results = operation(**batch)\n                results[\"_pad\"] = pad\n                outfeed.enqueue(results)\n\n        iterator = iter(_padded_dataset(inputs))\n        outfeed = ipu.ipu_outfeed_queue.IPUOutfeedQueue()\n        while True:\n            strategy.run(_loop, (iterator, outfeed))\n            for item in outfeed:\n                if item.pop(\"_pad\"):\n                    # Prevent: Error occurred when finalizing GeneratorDataset iterator\n                    del iterator\n                    del outfeed\n                    return\n                yield {k: np.array(v) for k, v in item.items()}", ""]}
{"filename": "scmm/pedal/utility.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"General standalone utilities.\"\"\"\n\nimport contextlib\nimport datetime\nimport multiprocessing\nfrom pathlib import Path\nfrom typing import (\n    Any,", "from typing import (\n    Any,\n    Callable,\n    ContextManager,\n    Dict,\n    Generator,\n    Iterable,\n    Sequence,\n    Tuple,\n    TypeVar,", "    Tuple,\n    TypeVar,\n    Union,\n)\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\ndef split_seed(seed: int, n: int) -> Tuple[int, ...]:\n    \"\"\"Split a random seed into n seeds.\n\n    Note that the original seed should not be used after calling this.\n    \"\"\"\n    return tuple(\n        int(seq.generate_state(1)[0]) for seq in np.random.SeedSequence(seed).spawn(n)\n    )", "\n\ndef split_seed(seed: int, n: int) -> Tuple[int, ...]:\n    \"\"\"Split a random seed into n seeds.\n\n    Note that the original seed should not be used after calling this.\n    \"\"\"\n    return tuple(\n        int(seq.generate_state(1)[0]) for seq in np.random.SeedSequence(seed).spawn(n)\n    )", "\n\nT = TypeVar(\"T\")\n\n\ndef remove_keys(dict_: Dict[str, T], *keys: str) -> Dict[str, T]:\n    \"\"\"Return a new dictionary with specific keys removed.\"\"\"\n    return {k: v for k, v in dict_.items() if k not in keys}\n\n\ndef to_jsonable(obj: Any) -> Any:\n    \"\"\"A decent default=? function for json.dump.\"\"\"\n    if isinstance(obj, Path):\n        return str(obj)\n    if isinstance(obj, (np.ndarray, np.number)):\n        return obj.tolist()\n    if isinstance(obj, datetime.date):  # datetime.datetime is a subclass\n        return obj.isoformat()\n    raise TypeError(f\"Type '{type(obj).__name__}' is not JSON-serialisable\")", "\n\ndef to_jsonable(obj: Any) -> Any:\n    \"\"\"A decent default=? function for json.dump.\"\"\"\n    if isinstance(obj, Path):\n        return str(obj)\n    if isinstance(obj, (np.ndarray, np.number)):\n        return obj.tolist()\n    if isinstance(obj, datetime.date):  # datetime.datetime is a subclass\n        return obj.isoformat()\n    raise TypeError(f\"Type '{type(obj).__name__}' is not JSON-serialisable\")", "\n\nLogger = Callable[..., None]\n\n\n@contextlib.contextmanager\ndef logging(\n    *loggers: Union[ContextManager[Logger], Logger]\n) -> Generator[Logger, None, None]:\n    \"\"\"A context manager that delegates logging calls to multiple \"loggers\".\n\n    Arguments are either:\n     - Callable actions\n     - Context managers that return callable actions\n\n    For example:\n\n        @contextlib.contextmanager\n        def log_to_file(path: Path) -> None:\n            with path.open(\"w\") as f:\n                yield lambda item: print(item, file=f)\n\n        with logging(print, log_to_file(Path(\"log.txt\"))) as log:\n            log(\"item one\")\n            log(\"item two\")\n    \"\"\"\n    with contextlib.ExitStack() as stack:\n        functions = [\n            stack.enter_context(logger)  # type:ignore[arg-type]\n            if hasattr(logger, \"__enter__\")\n            else logger\n            for logger in loggers\n        ]\n\n        def apply(*args: Any, **kwargs: Any) -> None:\n            for fn in functions:\n                fn(*args, **kwargs)\n\n        yield apply", "\n\ndef named_layers(\n    layer: Union[keras.layers.Layer, Sequence[keras.layers.Layer]],\n    prefix: Tuple[str, ...] = (),\n) -> Iterable[Tuple[str, keras.layers.Layer]]:\n    \"\"\"Walk a layer, recursively trying to find sublayers.\"\"\"\n    if isinstance(layer, (list, tuple)):\n        for n, child in enumerate(layer):\n            yield from named_layers(child, prefix + (str(n),))\n    if isinstance(layer, keras.layers.Layer):\n        yield (\".\".join(prefix), layer)\n        for attr, child in vars(layer).items():\n            if attr.startswith(\"_\"):\n                continue\n            if isinstance(child, (list, tuple, keras.layers.Layer)):\n                yield from named_layers(child, prefix + (attr,))", "\n\ndef named_weights(\n    layer: keras.layers.Layer, recursive: bool = True\n) -> Iterable[Tuple[str, tf.Variable]]:\n    \"\"\"Walk a layer to find weight variables with full path names.\n\n    recursive -- bool -- if `False`, only look at direct weights owned by this layer\n    \"\"\"\n    sublayers = named_layers(layer) if recursive else [(\"\", layer)]\n    for name, sublayer in sublayers:\n        for attr, child in vars(sublayer).items():\n            if not attr.startswith(\"_\") and isinstance(child, tf.Variable):\n                yield (f\"{name}.{attr}\" if name else attr, child)", "\n\ndef _runner(\n    queue: multiprocessing.Queue,  # type:ignore[type-arg]\n    command: Callable[..., T],\n    args: Dict[str, Any],\n) -> None:\n    queue.put_nowait(command(**args))\n\n\ndef run_in_subprocess(command: Callable[..., T], **args: Any) -> T:\n    \"\"\"Run a command synchronously in a (non-daemon) subprocess.\"\"\"\n    # We'd prefer to use a simple multiprocessing.Pool here, but I can't find a\n    # way to make the workers non-daemonic\n    queue = multiprocessing.Manager().Queue()\n    process = multiprocessing.get_context(\"spawn\").Process(\n        target=_runner, args=(queue, command, args)\n    )\n    process.start()\n    process.join()\n    if process.exitcode:\n        raise multiprocessing.ProcessError(\n            f\"Process exited with code {process.exitcode}\"\n        )\n    return queue.get()  # type:ignore[no-any-return]", "\n\ndef run_in_subprocess(command: Callable[..., T], **args: Any) -> T:\n    \"\"\"Run a command synchronously in a (non-daemon) subprocess.\"\"\"\n    # We'd prefer to use a simple multiprocessing.Pool here, but I can't find a\n    # way to make the workers non-daemonic\n    queue = multiprocessing.Manager().Queue()\n    process = multiprocessing.get_context(\"spawn\").Process(\n        target=_runner, args=(queue, command, args)\n    )\n    process.start()\n    process.join()\n    if process.exitcode:\n        raise multiprocessing.ProcessError(\n            f\"Process exited with code {process.exitcode}\"\n        )\n    return queue.get()  # type:ignore[no-any-return]", ""]}
{"filename": "scmm/pedal/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"Press the pedal to go faster.\"\"\"\n\nfrom . import utility, xpu  # NOQA: F401\n"]}
{"filename": "scmm/pedal/tests/test_xpu.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\nfrom typing import Dict, List\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom .. import xpu", "\nfrom .. import xpu\n\nSETTINGS: List[xpu.Settings] = [\n    xpu.CpuSettings(compile=False),\n    xpu.CpuSettings(compile=True),\n]\nif xpu.IPU:\n    SETTINGS.extend(\n        [\n            xpu.IpuSettings(iterations_per_loop=1),\n            xpu.IpuSettings(\n                iterations_per_loop=4,\n                available_memory_proportion=0.2,\n                stochastic_rounding=True,\n            ),\n        ]\n    )", "\n\n@pytest.mark.parametrize(\"settings\", SETTINGS)\ndef test_context(settings: xpu.Settings):\n    traces = 0\n\n    def model(x: tf.Tensor) -> Dict[str, tf.Tensor]:\n        nonlocal traces\n        traces += 1\n        return dict(y=2 * x)\n\n    data = (dict(x=np.array(x, dtype=np.float32)) for x in range(5))\n\n    with xpu.context(settings) as context:\n        results = list(context.loop(model, data))\n        np.testing.assert_equal(results, [dict(y=2 * x) for x in range(5)])\n        if not (isinstance(settings, xpu.CpuSettings) and not settings.compile):\n            assert traces == 1", "\n\n@pytest.mark.parametrize(\n    \"settings\",\n    filter(\n        None,\n        [\n            xpu.CpuSettings(compile=False),\n            xpu.IpuSettings(iterations_per_loop=1) if xpu.IPU else None,\n        ],", "            xpu.IpuSettings(iterations_per_loop=1) if xpu.IPU else None,\n        ],\n    ),\n)\ndef test_outline(settings: xpu.Settings):\n    with xpu.context(settings) as context:\n        assert xpu.current_context() is context\n        layer1 = keras.layers.Dense(10)\n        layer2 = keras.layers.Dense(10)\n        context.outline(layer1)\n        context.outline(layer2)\n\n        results = list(\n            context.loop(\n                lambda x: dict(y=layer2(layer1(x))),\n                [dict(x=np.ones((1, 10), dtype=np.float32))],\n            )\n        )\n        assert results[0][\"y\"].shape == (1, 10)", ""]}
{"filename": "scmm/pedal/tests/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n"]}
{"filename": "scmm/pedal/tests/test_utility.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\nimport contextlib\nimport datetime\nimport json\nimport multiprocessing\nfrom pathlib import Path\nfrom typing import Dict\n\nimport numpy as np", "\nimport numpy as np\nimport pytest\nfrom tensorflow import keras\n\nfrom .. import utility\n\n\ndef test_split_seed():\n    seeds = utility.split_seed(123456789, 3)\n    assert len(seeds) == 3\n    assert len(set(seeds)) == 3", "def test_split_seed():\n    seeds = utility.split_seed(123456789, 3)\n    assert len(seeds) == 3\n    assert len(set(seeds)) == 3\n\n\ndef test_remove_keys():\n    assert utility.remove_keys(dict(a=1, b=2, c=3), \"b\", \"d\") == dict(a=1, c=3)\n\n\ndef test_to_jsonable():\n    t0 = datetime.datetime.now()\n    data = json.dumps(\n        dict(time=t0, array=np.ones(2), path=Path(\"fake/path\")),\n        default=utility.to_jsonable,\n    )\n    assert json.loads(data) == dict(time=t0.isoformat(), array=[1, 1], path=\"fake/path\")\n\n    with pytest.raises(TypeError) as exc:\n        json.dumps(dict(obj=object()), default=utility.to_jsonable)\n    assert \"object\" in str(exc)", "\n\ndef test_to_jsonable():\n    t0 = datetime.datetime.now()\n    data = json.dumps(\n        dict(time=t0, array=np.ones(2), path=Path(\"fake/path\")),\n        default=utility.to_jsonable,\n    )\n    assert json.loads(data) == dict(time=t0.isoformat(), array=[1, 1], path=\"fake/path\")\n\n    with pytest.raises(TypeError) as exc:\n        json.dumps(dict(obj=object()), default=utility.to_jsonable)\n    assert \"object\" in str(exc)", "\n\ndef test_logging():\n    history = []\n\n    @contextlib.contextmanager\n    def logger():\n        history.append(\"pre\")\n        yield history.append\n        history.append(\"post\")\n\n    with utility.logging(\n        logger(), lambda line: history.append(f\"lambda {line}\")\n    ) as log:\n        log(123)\n        log(456)\n\n    assert history == [\"pre\", 123, \"lambda 123\", 456, \"lambda 456\", \"post\"]", "\n\ndef test_named_layers_and_weights():\n    class TestModel(keras.layers.Layer):\n        # pylint:disable=too-few-public-methods\n        def __init__(self):\n            super().__init__()\n            self.projection = keras.layers.Dense(20)\n            self.projection.build((10,))\n            self.transforms = [\n                keras.layers.LayerNormalization(),\n                keras.layers.Dense(20, use_bias=False),\n            ]\n            for transform in self.transforms:\n                transform.build((20,))\n            self.final_bias = self.add_weight(\n                name=\"final_bias\", shape=(7,), initializer=\"zeros\"\n            )\n\n    model = TestModel()\n\n    assert {k: type(v).__name__ for k, v in utility.named_layers(model)} == {\n        \"\": \"TestModel\",\n        \"projection\": \"Dense\",\n        \"transforms.0\": \"LayerNormalization\",\n        \"transforms.1\": \"Dense\",\n    }\n\n    assert {k: tuple(v.shape) for k, v in utility.named_weights(model)} == {\n        \"projection.kernel\": (10, 20),\n        \"projection.bias\": (20,),\n        \"transforms.0.beta\": (20,),\n        \"transforms.0.gamma\": (20,),\n        \"transforms.1.kernel\": (20, 20),\n        \"final_bias\": (7,),\n    }\n    assert dict(utility.named_weights(model, recursive=False)).keys() == {\"final_bias\"}", "\n\ndef _stub(x: int) -> Dict[str, int]:\n    if x % 2 == 0:\n        raise ValueError(\"x is even\")\n    return dict(y=5 * x)\n\n\ndef test_run_in_subprocess():\n    assert utility.run_in_subprocess(_stub, x=3) == dict(y=15)\n    with pytest.raises(multiprocessing.ProcessError):\n        utility.run_in_subprocess(_stub, x=4)", "def test_run_in_subprocess():\n    assert utility.run_in_subprocess(_stub, x=3) == dict(y=15)\n    with pytest.raises(multiprocessing.ProcessError):\n        utility.run_in_subprocess(_stub, x=4)\n"]}
{"filename": "scmm/tests/test_models.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\nimport dataclasses\n\nimport numpy as np\nimport pytest\n\nfrom .. import models\nfrom ..pedal import xpu\n", "from ..pedal import xpu\n\nSETTINGS = models.Settings(\n    vocab_size=100,\n    hidden_size=8,\n    depth=2,\n    residual=None,\n    sequence=models.Conv(kernel_size=5, groups=1),\n    token=None,\n    dtype=\"float32\",", "    token=None,\n    dtype=\"float32\",\n    seed=100,\n)\n\n\n@pytest.fixture\ndef cpu_context():\n    with xpu.context(xpu.CpuSettings(compile=False)) as context:\n        yield context", "\n\nMODEL_SETTINGS = [\n    SETTINGS,\n    dataclasses.replace(\n        SETTINGS,\n        residual=models.Residual(norm=None, alpha=None),\n        token=models.FFN(multiple=1.5),\n    ),\n    dataclasses.replace(", "    ),\n    dataclasses.replace(\n        SETTINGS,\n        residual=models.Residual(norm=\"pre\", alpha=\"mean\"),\n        sequence=models.Attention(heads=2, head_size=4, frequencies=16, max_period=16),\n    ),\n    dataclasses.replace(\n        SETTINGS,\n        residual=models.Residual(norm=\"post\", alpha=0.5),\n        sequence=models.RNN(rebias=1),", "        residual=models.Residual(norm=\"post\", alpha=0.5),\n        sequence=models.RNN(rebias=1),\n    ),\n]\n\n\n@pytest.mark.parametrize(\n    \"settings,unit_scale\",\n    [\n        (dataclasses.replace(settings, dtype=dtype), unit_scale)", "    [\n        (dataclasses.replace(settings, dtype=dtype), unit_scale)\n        for settings in MODEL_SETTINGS\n        for dtype in [\"float32\", \"float16\"]\n        for unit_scale in [None, \"0.4\"]\n    ],\n    ids=repr,\n)\ndef test_model(cpu_context: xpu.Context, settings: models.Settings, unit_scale: bool):\n    if unit_scale and settings.residual is not None and settings.residual.alpha is None:\n        pytest.skip(\"unsupported combination\")\n\n    batch_sequences = 3\n    sequence_length = 12\n    random = np.random.Generator(np.random.PCG64(200))\n    tokens = random.integers(\n        settings.vocab_size, size=(batch_sequences, sequence_length)\n    )\n    mask = random.random(size=(batch_sequences, sequence_length)) < 0.9\n\n    model = models.Model(settings, unit_scale=unit_scale)\n    result = model.run(tokens=tokens, mask=mask)\n    assert 0 < float(result[\"loss\"]) < 1.5 * np.log(settings.vocab_size)\n    assert int(result[\"n_tokens\"]) == np.sum(mask)\n\n    # Same seed - expect same weights & results\n    model2 = models.Model(settings, unit_scale=unit_scale)\n    result2 = model2.run(tokens=tokens, mask=mask)\n    np.testing.assert_allclose(float(result2[\"loss\"]), float(result[\"loss\"]))\n    np.testing.assert_equal(model2.save(), model.save())", "def test_model(cpu_context: xpu.Context, settings: models.Settings, unit_scale: bool):\n    if unit_scale and settings.residual is not None and settings.residual.alpha is None:\n        pytest.skip(\"unsupported combination\")\n\n    batch_sequences = 3\n    sequence_length = 12\n    random = np.random.Generator(np.random.PCG64(200))\n    tokens = random.integers(\n        settings.vocab_size, size=(batch_sequences, sequence_length)\n    )\n    mask = random.random(size=(batch_sequences, sequence_length)) < 0.9\n\n    model = models.Model(settings, unit_scale=unit_scale)\n    result = model.run(tokens=tokens, mask=mask)\n    assert 0 < float(result[\"loss\"]) < 1.5 * np.log(settings.vocab_size)\n    assert int(result[\"n_tokens\"]) == np.sum(mask)\n\n    # Same seed - expect same weights & results\n    model2 = models.Model(settings, unit_scale=unit_scale)\n    result2 = model2.run(tokens=tokens, mask=mask)\n    np.testing.assert_allclose(float(result2[\"loss\"]), float(result[\"loss\"]))\n    np.testing.assert_equal(model2.save(), model.save())", "\n\ndef test_model_load_save(cpu_context: xpu.Context):\n    # Change seed, expect different weights\n    base = models.Model(SETTINGS, unit_scale=False)\n    other = models.Model(\n        dataclasses.replace(SETTINGS, seed=SETTINGS.seed + 1), unit_scale=False\n    )\n    base_weights = base.save()\n    other_weights = other.save()\n    assert any(np.any(other_weights[k] != base_weights[k]) for k in base_weights)\n\n    with pytest.raises(ValueError) as error:\n        other.load(dict(**base_weights, non_existent_weight=np.zeros(2)))\n    assert \"non_existent_weight\" in str(error)\n\n    other.load(base_weights)\n    np.testing.assert_equal(other.save(), base_weights)", ""]}
{"filename": "scmm/tests/test_datasets.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\nimport itertools as it\nfrom pathlib import Path\n\nimport numpy as np\n\nfrom .. import datasets\n\n\ndef test_to_ids_to_str():\n    vocab = tuple(\" abcd\")\n    original = \"bad cad\"\n    ids = datasets.to_ids(original, vocab, dtype=np.uint16)\n    np.testing.assert_equal(ids, [2, 1, 4, 0, 3, 1, 4])\n    assert datasets.to_str(ids, vocab) == original", "\n\ndef test_to_ids_to_str():\n    vocab = tuple(\" abcd\")\n    original = \"bad cad\"\n    ids = datasets.to_ids(original, vocab, dtype=np.uint16)\n    np.testing.assert_equal(ids, [2, 1, 4, 0, 3, 1, 4])\n    assert datasets.to_str(ids, vocab) == original\n\n\ndef test_data():\n    vocab = tuple(\" abcd\")\n    parts = dict(\n        train=\"a bad cad abba a bad dad\",\n        valid=\"a bb ccc dddd\",\n    )\n    data = datasets.Data(\n        vocab=vocab,\n        parts={k: datasets.to_ids(v, vocab, np.uint16) for k, v in parts.items()},\n    )\n    S = datasets.BatchSettings\n    for settings in [\n        S(sequences=3, sequence_length=6, overlap_length=2, loop_seed=None),\n        S(sequences=3, sequence_length=6, overlap_length=2, loop_seed=100),\n        S(sequences=1, sequence_length=8, overlap_length=0, loop_seed=None),\n        S(sequences=1, sequence_length=8, overlap_length=0, loop_seed=200),\n    ]:\n        for part, text in parts.items():\n            max_batch = 25\n            batches = list(it.islice(data.batches(part, settings), max_batch))\n            for batch in batches:\n                assert batch[\"tokens\"].shape == settings.shape\n                assert batch[\"tokens\"].dtype == np.int32\n                assert batch[\"mask\"].shape == settings.shape\n                assert batch[\"mask\"].dtype == np.int32\n                assert np.sum(batch[\"mask\"]) >= (\n                    1 if settings.loop_seed is None else settings.target_tokens\n                )\n\n            if settings.loop_seed is None:\n                flat_tokens = np.ravel([b[\"tokens\"] for b in batches])\n                flat_mask = np.ravel([b[\"mask\"] for b in batches]).astype(np.bool)\n                assert datasets.to_str(flat_tokens[flat_mask], vocab) == text\n            else:\n                assert len(batches) == max_batch", "\n\ndef test_data():\n    vocab = tuple(\" abcd\")\n    parts = dict(\n        train=\"a bad cad abba a bad dad\",\n        valid=\"a bb ccc dddd\",\n    )\n    data = datasets.Data(\n        vocab=vocab,\n        parts={k: datasets.to_ids(v, vocab, np.uint16) for k, v in parts.items()},\n    )\n    S = datasets.BatchSettings\n    for settings in [\n        S(sequences=3, sequence_length=6, overlap_length=2, loop_seed=None),\n        S(sequences=3, sequence_length=6, overlap_length=2, loop_seed=100),\n        S(sequences=1, sequence_length=8, overlap_length=0, loop_seed=None),\n        S(sequences=1, sequence_length=8, overlap_length=0, loop_seed=200),\n    ]:\n        for part, text in parts.items():\n            max_batch = 25\n            batches = list(it.islice(data.batches(part, settings), max_batch))\n            for batch in batches:\n                assert batch[\"tokens\"].shape == settings.shape\n                assert batch[\"tokens\"].dtype == np.int32\n                assert batch[\"mask\"].shape == settings.shape\n                assert batch[\"mask\"].dtype == np.int32\n                assert np.sum(batch[\"mask\"]) >= (\n                    1 if settings.loop_seed is None else settings.target_tokens\n                )\n\n            if settings.loop_seed is None:\n                flat_tokens = np.ravel([b[\"tokens\"] for b in batches])\n                flat_mask = np.ravel([b[\"mask\"] for b in batches]).astype(np.bool)\n                assert datasets.to_str(flat_tokens[flat_mask], vocab) == text\n            else:\n                assert len(batches) == max_batch", "\n\ndef test_load():\n    # Path(\"data/vocab.json\").write_text(json.dumps(sorted({\n    #     ch for path in Path(\"data\").glob(\"*.txt\") for ch in path.read_text(\"utf8\")\n    # })))\n    folder = Path(__file__).parent / \"data\"\n    data = datasets.load_character(folder, train=\"train.txt\", valid=\"valid.txt\")\n\n    assert datasets.to_str(data.parts[\"valid\"], data.vocab) == (\n        folder / \"valid.txt\"\n    ).read_text(\"utf8\")\n    assert len(data.parts[\"train\"]) > len(data.parts[\"valid\"])", ""]}
{"filename": "scmm/tests/test_training.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\nimport numpy as np\nimport pytest\n\nfrom .. import datasets, models, training\nfrom ..pedal import xpu\n\n\n@pytest.mark.parametrize(", "\n@pytest.mark.parametrize(\n    \"optimiser\",\n    [\n        training.AdamW(0.1, learning_rate_decay=0.0),\n        training.SgdM(0.1, learning_rate_decay=0.0, momentum=0.9),\n    ],\n    ids=lambda s: s.kind,\n)\ndef test_training(optimiser: training.Optimiser):\n    data_sequence = np.arange(100) % 3\n    data = datasets.Data(\n        (\"a\", \"b\", \"c\"),\n        dict(train=data_sequence, valid=data_sequence, test=data_sequence),\n    )\n    with xpu.context(xpu.CpuSettings()) as context:\n        model = models.Model(\n            models.Settings(\n                vocab_size=len(data.vocab),\n                hidden_size=32,\n                depth=2,\n                residual=None,\n                sequence=models.Conv(2, groups=1),\n                token=None,\n                dtype=\"float32\",\n                seed=100,\n            ),\n            unit_scale=False,\n        )\n        settings = training.Settings(\n            datasets.BatchSettings(2, 8, 2, loop_seed=200),\n            steps=10,\n            valid_interval=None,\n            optimiser=optimiser,\n            loss_scale=1e3,\n        )\n        log = list(training.train(model, data, context, settings, unit_scale=False))\n        train_log = [line for line in log if line[\"kind\"] == \"train_step\"]\n        assert 0.5 * np.log(3) < train_log[0][\"loss\"]\n        assert train_log[-1][\"loss\"] < 0.01 * np.log(3)", ")\ndef test_training(optimiser: training.Optimiser):\n    data_sequence = np.arange(100) % 3\n    data = datasets.Data(\n        (\"a\", \"b\", \"c\"),\n        dict(train=data_sequence, valid=data_sequence, test=data_sequence),\n    )\n    with xpu.context(xpu.CpuSettings()) as context:\n        model = models.Model(\n            models.Settings(\n                vocab_size=len(data.vocab),\n                hidden_size=32,\n                depth=2,\n                residual=None,\n                sequence=models.Conv(2, groups=1),\n                token=None,\n                dtype=\"float32\",\n                seed=100,\n            ),\n            unit_scale=False,\n        )\n        settings = training.Settings(\n            datasets.BatchSettings(2, 8, 2, loop_seed=200),\n            steps=10,\n            valid_interval=None,\n            optimiser=optimiser,\n            loss_scale=1e3,\n        )\n        log = list(training.train(model, data, context, settings, unit_scale=False))\n        train_log = [line for line in log if line[\"kind\"] == \"train_step\"]\n        assert 0.5 * np.log(3) < train_log[0][\"loss\"]\n        assert train_log[-1][\"loss\"] < 0.01 * np.log(3)", ""]}
{"filename": "scmm/tests/testing.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\nfrom typing import Callable, Dict, Tuple\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom ..pedal import utility\n", "from ..pedal import utility\n\n\ndef assert_unit_scale(value: np.ndarray, tol: float, err_msg: str = \"\") -> None:\n    \"\"\"Check that a tensor has unit std.\"\"\"\n    np.testing.assert_allclose(np.std(value), 1, atol=tol, err_msg=err_msg)\n\n\ndef weight_shapes(layer: keras.layers.Layer) -> Dict[str, Tuple[int, ...]]:\n    \"\"\"A map of name to weight shape.\"\"\"\n    return {name: tuple(w.shape) for name, w in utility.named_weights(layer)}", "def weight_shapes(layer: keras.layers.Layer) -> Dict[str, Tuple[int, ...]]:\n    \"\"\"A map of name to weight shape.\"\"\"\n    return {name: tuple(w.shape) for name, w in utility.named_weights(layer)}\n\n\ndef correlated_batch_random(\n    random: np.random.RandomState, shape: Tuple[int, ...]\n) -> np.ndarray:\n    \"\"\"Create a random tensor tiled over all 'batch' dimensions (all except last).\"\"\"\n    # return random.normal(size=shape).astype(np.float32)\n    return np.tile(random.normal(size=shape[-1]).astype(np.float32), shape[:-1] + (1,))", "\n\ndef output_and_gradients(\n    layer: Callable[..., tf.Tensor], input_shape: Tuple[int, ...], seed: int\n) -> Dict[str, np.ndarray]:\n    \"\"\"Randomly generate inputs and grads (unit norm), and return everything.\n\n    Creates identical outputs and gradients across the batch axis, since\n    realistic gradients are better modelled by perfect correlation than no\n    correlation.\n    \"\"\"\n    random = np.random.Generator(np.random.PCG64(seed))\n    inputs = tf.constant(correlated_batch_random(random, input_shape))\n    with tf.GradientTape() as tape:\n        tape.watch(inputs)\n        outputs = layer(inputs)\n    grad_outputs = correlated_batch_random(random, outputs.shape)\n    gradient_tensors = {f\"grad_{k}\": v for k, v in utility.named_weights(layer)}\n    gradient_tensors[\"grad_inputs\"] = inputs\n    gradients = tape.gradient(outputs, gradient_tensors, grad_outputs)\n    return dict(inputs=inputs, outputs=outputs, grad_outputs=grad_outputs, **gradients)", ""]}
{"filename": "scmm/tests/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n"]}
{"filename": "scmm/tests/test_experiments.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\nimport collections\nimport contextlib\nimport json\nimport os\nimport unittest.mock as um\nfrom pathlib import Path\nfrom typing import Any, Dict\n", "from typing import Any, Dict\n\nimport numpy as np\nimport pytest\n\nfrom .. import datasets, experiments, models, training\nfrom ..pedal import xpu\n\n\ndef test_log_wandb_finish_on_error():\n    with pytest.raises(ValueError), um.patch(\"wandb.init\"), um.patch(\n        \"wandb.run\"\n    ) as wandb_run, um.patch(\"wandb.finish\") as wandb_finish:\n        with experiments.log_wandb():\n            raise ValueError(\"Bad things happened\")\n    wandb_finish.assert_called_once_with(1)\n    wandb_run.summary.update.assert_called_once_with(\n        dict(error=\"ValueError('Bad things happened')\")\n    )", "\ndef test_log_wandb_finish_on_error():\n    with pytest.raises(ValueError), um.patch(\"wandb.init\"), um.patch(\n        \"wandb.run\"\n    ) as wandb_run, um.patch(\"wandb.finish\") as wandb_finish:\n        with experiments.log_wandb():\n            raise ValueError(\"Bad things happened\")\n    wandb_finish.assert_called_once_with(1)\n    wandb_run.summary.update.assert_called_once_with(\n        dict(error=\"ValueError('Bad things happened')\")\n    )", "\n\ndef _test_settings(path: Path) -> experiments.Settings:\n    return experiments.Settings(\n        data=experiments.DataSettings(Path(__file__).parent / \"data\"),\n        model=models.Settings(\n            hidden_size=64,\n            depth=1,\n            residual=None,\n            sequence=models.Conv(kernel_size=5, groups=1),\n            token=None,\n            dtype=\"float32\",\n            vocab_size=None,  # type:ignore[arg-type]\n            seed=None,  # type:ignore[arg-type]\n        ),\n        training=training.Settings(\n            batch=datasets.BatchSettings(\n                sequences=10, sequence_length=32, overlap_length=8, loop_seed=None\n            ),\n            steps=100,\n            valid_interval=50,\n            optimiser=training.AdamW(learning_rate=0.05, learning_rate_decay=1e-3),\n            loss_scale=1,\n        ),\n        unit_scale=\"0.4\",\n        target=xpu.CpuSettings(compile=False),\n        output=experiments.OutputSettings(\n            wandb=True,\n            stderr=True,\n            log=path / \"log.jsonl\",\n            checkpoint=path / \"model.npz\",\n        ),\n        metadata=dict(experiment=\"testxp\"),\n        seed=None,  # type:ignore[arg-type]\n    )", "\n\ndef test_run_experiment(tmp_path: Path):  # pylint:disable=too-many-locals\n    with contextlib.ExitStack() as stack:\n        wandb_init = stack.enter_context(um.patch(\"wandb.init\"))\n        wandb_log = stack.enter_context(um.patch(\"wandb.log\"))\n        stack.enter_context(um.patch(\"wandb.run\"))\n        wandb_finish = stack.enter_context(um.patch(\"wandb.finish\"))\n        stack.enter_context(\n            um.patch.dict(\n                os.environ, {\"SSUB_UID\": \"ssub123\", \"SLURM_JOB_ID\": \"slurm123\"}\n            )\n        )\n\n        experiments.run(_test_settings(tmp_path))\n\n    wandb_init.assert_called_once()\n    wandb_init_args = wandb_init.call_args[1]\n    assert wandb_init_args.get(\"project\") == \"scaled-matmuls\"\n    assert wandb_init_args[\"config\"][\"metadata\"][\"ssub_id\"] == \"ssub123\"\n    assert wandb_log.call_count == 2 * 3 + 1\n    wandb_finish.assert_called_once()\n\n    # Checkpoint\n    checkpoint = np.load(tmp_path / \"model.npz\")\n    assert checkpoint[\"step\"] == 100\n    assert len(checkpoint) >= 2\n\n    # Log\n    log_by_kind = collections.defaultdict(list)\n    with (tmp_path / \"log.jsonl\").open() as file_:\n        for line in file_:\n            item = json.loads(line)\n            log_by_kind[item[\"kind\"]].append(item)\n\n    (log_settings,) = log_by_kind[\"settings\"]\n    assert log_settings[\"metadata\"][\"experiment\"] == \"testxp\"\n    assert isinstance(log_settings[\"model\"][\"seed\"], int)\n\n    (log_stats,) = log_by_kind[\"stats\"]\n    assert 5 * 64 * 64 < log_stats[\"n_weights\"]\n    assert len(log_stats[\"weight_shapes\"])\n\n    assert len(log_by_kind[\"train_step\"]) == 100\n    first_step, *_, last_step = log_by_kind[\"train_step\"]\n    assert first_step[\"step\"] == 0\n    assert last_step[\"step\"] == 99\n    assert last_step[\"loss\"] < first_step[\"loss\"]\n\n    assert [x[\"valid_n_tokens\"] for x in log_by_kind[\"eval_valid\"]] == 3 * [7665]\n    assert log_by_kind[\"eval_valid\"][-1][\"valid_loss\"] < 2.5\n\n    assert len([x[\"train_n_tokens\"] for x in log_by_kind[\"eval_train\"]]) == 3\n    assert log_by_kind[\"eval_train\"][-1][\"train_loss\"] < 2.5", "\n\ndef test_find_learning_rate():\n    called_with_lr = []\n\n    def _fake_run(settings: experiments.Settings) -> Dict[str, Any]:\n        learning_rate = settings.training.optimiser.learning_rate\n        called_with_lr.append(learning_rate)\n        return dict(valid_loss=(15 - learning_rate) ** 2)\n\n    base = _test_settings(Path(\"fake\"))\n    base.training.optimiser.learning_rate = 3\n    with um.patch(\"scmm.experiments.run\", new_callable=lambda: _fake_run):\n        # LR = 3, 6, 12, 24, 48, ...\n        # Note - disable run_in_subprocess, otherwise our um.patch() wouldn't work\n        best, best_loss = experiments.find_learning_rate(\n            experiments.LrSweep(base, step=2, threshold=30, reps=3),\n            run_in_subprocess=False,\n        )\n\n    np.testing.assert_allclose(best.training.optimiser.learning_rate, 12)\n    np.testing.assert_allclose(best_loss, 9)\n    np.testing.assert_allclose(called_with_lr, np.repeat([3, 6, 12, 24], 3))", ""]}
{"filename": "scmm/tests/test_layers.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\nfrom typing import List, Optional\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom .. import layers", "\nfrom .. import layers\nfrom . import testing\n\n\ndef test_batched_gather():\n    tables = tf.reshape(tf.range(2 * 3 * 4), (2, 3, 4))\n    indices = tf.constant([[0, 0, 3], [2, 2, 3]])\n    np.testing.assert_equal(\n        np.array(layers.batched_gather(tables, indices)),\n        [[0 + 0, 4 + 0, 8 + 3], [12 + 2, 16 + 2, 20 + 3]],\n    )", "\n\n# Also tests layers.LayerNormalization\n@pytest.mark.parametrize(\n    [\"norm_type\", \"alpha\"], [(None, 0.5), (\"pre\", None), (\"post\", 0.1)]\n)\ndef test_residual_layer(norm_type: Optional[str], alpha: Optional[float]):\n    layer = layers.ResidualLayer(\n        keras.layers.Dense(7), norm_type=norm_type, alpha=alpha\n    )\n    layer.build((None, None, 7))\n    assert layer.body.kernel.shape == (7, 7)\n    assert layer(tf.ones((2, 3, 7))).shape == (2, 3, 7)", "\n\ndef test_ffn_layer():\n    layer = layers.FFNLayer(3, seeds=(100, 200))\n    layer.build((7,))\n    assert layer.up.kernel.shape == (7, 21)  # type:ignore[union-attr]\n    assert layer.down.kernel.shape == (21, 7)  # type:ignore[union-attr]\n    assert layer(tf.ones((2, 3, 7))).shape == (2, 3, 7)\n\n\ndef test_softmax_cross_entropy():\n    loss, n_ids = layers.softmax_cross_entropy(\n        tf.ones((2, 3, 20)),\n        tf.constant([[0, 9, 19], [2, 2, 2]]),\n        tf.constant([[True, True, True], [True, False, False]]),\n    )\n    assert int(n_ids) == 4\n    np.testing.assert_allclose(float(loss), np.log(20))", "\n\ndef test_softmax_cross_entropy():\n    loss, n_ids = layers.softmax_cross_entropy(\n        tf.ones((2, 3, 20)),\n        tf.constant([[0, 9, 19], [2, 2, 2]]),\n        tf.constant([[True, True, True], [True, False, False]]),\n    )\n    assert int(n_ids) == 4\n    np.testing.assert_allclose(float(loss), np.log(20))", "\n\ndef test_layer_normalization():\n    layer = layers.LayerNormalization()\n    layer.build((None, None, 4))\n    np.testing.assert_allclose(\n        layer(tf.constant([0.0, 0, 4, 4])[tf.newaxis, tf.newaxis, :]),\n        tf.constant([-1.0, -1, 1, 1])[tf.newaxis, tf.newaxis, :],\n        rtol=1e-3,\n    )\n    np.testing.assert_allclose(layer(tf.ones((2, 3, 4))), tf.zeros((2, 3, 4)))", "\n\ndef test_pad_and_shift_layer():\n    layer = layers.PadAndShiftLayer()\n    layer.build((None, None, 11))\n    assert layer.padding.shape == (11,)\n    output = layer(tf.ones((3, 5, 11)))\n    assert output.shape == (3, 5, 11)\n    np.testing.assert_allclose(output[:, 0, :], 0)\n    np.testing.assert_allclose(output[:, 1:, :], 1)\n\n    with pytest.raises(ValueError):\n        layers.PadAndShiftLayer().build((None, 11))", "\n\ndef test_isotropic():\n    layer = layers.Isotropic(\n        dense=keras.layers.Dense(15), norm=keras.layers.LayerNormalization()\n    )\n    layer.build((None, 15))\n    assert layer.dense.built\n    assert layer.norm.built\n    random = np.random.Generator(np.random.PCG64(seed=500))\n    assert layer(random.normal(size=(7, 15))).shape == (7, 15)", "\n\n####################\n# Attention\n\n\ndef test_sinusoid_embedding():\n    embedding = layers.sinusoid_embedding(\n        sequence_length=32, frequencies=6, max_period=16\n    )\n    assert embedding.shape == (32, 6)\n    np.testing.assert_allclose(embedding[:, 0], 0, atol=1e-6)\n    np.testing.assert_allclose(\n        embedding[:, 1], np.cos(np.pi * np.arange(32)), atol=1e-6\n    )\n    np.testing.assert_allclose(\n        embedding[:, -2], np.sin(2 * np.pi / 16 * np.arange(32)), atol=1e-6\n    )\n    np.testing.assert_allclose(\n        embedding[:, -1], np.cos(2 * np.pi / 16 * np.arange(32)), atol=1e-6\n    )", "\n\ndef test_relative_causal_reshape():\n    scores = tf.constant(\n        [\n            [1, 2, 3, 4],\n            [11, 12, 13, 14],\n            [21, 22, 23, 24],\n            [31, 32, 33, 34],\n        ]\n    )\n    attention = layers.relative_causal_reshape(scores)\n    np.testing.assert_equal(\n        attention.numpy(),\n        [\n            [1, 0, 0, 0],\n            [12, 11, 0, 0],\n            [23, 22, 21, 0],\n            [34, 33, 32, 31],\n        ],\n    )", "\n\ndef test_multi_head_attention():\n    random = np.random.Generator(np.random.PCG64(387232))\n    layer = layers.MultiHeadAttention(\n        heads=5, head_size=4, frequencies=13, max_period=16, seeds=(287, 918, 734)\n    )\n    inputs = random.normal(size=(11, 7, 8))\n    result = layer(inputs)\n\n    # Hard to check too much here - just make sure it's broadly sensible\n    assert result.shape == inputs.shape\n    np.testing.assert_array_less(np.std(result, axis=-1), 10)\n    assert testing.weight_shapes(layer) == {\n        \"qkv\": (8, 3, 5, 4),\n        \"q_bias\": (5, 4),\n        \"positional\": (13, 5, 4),\n        \"out.kernel\": (5 * 4, 8),\n        \"out.bias\": (8,),\n    }\n\n    # Check out the causal masking, by perturbing inputs[i, i+1]\n    # in which case inputs[i, :i+1] should be unchanged\n    perturbed = layer(inputs + np.eye(11, 7, k=1)[..., np.newaxis])\n    mask = np.tril(np.ones((11, 7)), k=0)[..., np.newaxis]\n    np.testing.assert_allclose(perturbed * mask, result * mask)\n    assert not np.allclose(perturbed, result)", "\n\n####################\n# RNN\n\n\ndef test_recurrent_highway_cell():\n    random = np.random.Generator(np.random.PCG64(387232))\n    layer = layers.RecurrentHighwayCell(\n        hidden_size=16, rebias=1, seed=random.integers(10000)\n    )\n    layer.build((3, 8))\n    assert testing.weight_shapes(layer) == dict(gates=(2, 24, 16), gates_bias=(2, 16))\n\n    output = layer(random.random(size=(3, 8)), random.random(size=(3, 16)))\n    assert output.shape == (3, 16)\n    assert np.std(output) < 10", "\n\ndef test_rnn():\n    layer = layers.RNN(layers.RecurrentHighwayCell(hidden_size=20, rebias=0, seed=439))\n    out = testing.output_and_gradients(layer, (3, 5, 8), seed=1341)\n    assert out[\"outputs\"].shape == (3, 5, 20)\n\n\n####################\n# Optimizers", "####################\n# Optimizers\n\n\ndef _train_sample_model(optimizer: keras.optimizers.Optimizer) -> List[float]:\n    random = np.random.Generator(np.random.PCG64(983532))\n    xs = random.normal(size=(1000, 20))\n    ys = xs @ random.normal(size=(20, 10))\n    model = keras.layers.Dense(\n        10, kernel_initializer=keras.initializers.GlorotUniform(seed=87321)\n    )\n    losses = []\n    for _ in range(10):\n        with tf.GradientTape() as tape:\n            loss = keras.losses.mse(ys.flatten(), tf.reshape(model(xs), -1))\n        optimizer.minimize(loss, model.trainable_variables, tape=tape)\n        losses.append(float(loss))\n    return losses", "\n\ndef test_sgdm():\n    np.testing.assert_allclose(\n        _train_sample_model(layers.SgdM(0.1, momentum=0.9)),\n        _train_sample_model(keras.optimizers.SGD(0.1, momentum=0.9)),\n        atol=1e-4,\n    )\n\n\ndef test_adamw():\n    reference_loss = _train_sample_model(keras.optimizers.Adam(0.1))\n    custom_loss = _train_sample_model(layers.AdamW(0.1, weight_decay=0))\n    decay_loss = _train_sample_model(layers.AdamW(0.1, weight_decay=0.1))\n\n    np.testing.assert_allclose(custom_loss, reference_loss, atol=1e-4)\n    assert (\n        1e-3 + reference_loss[-1] < decay_loss[-1]\n    ), \"decayed should be slightly worse\"", "\n\ndef test_adamw():\n    reference_loss = _train_sample_model(keras.optimizers.Adam(0.1))\n    custom_loss = _train_sample_model(layers.AdamW(0.1, weight_decay=0))\n    decay_loss = _train_sample_model(layers.AdamW(0.1, weight_decay=0.1))\n\n    np.testing.assert_allclose(custom_loss, reference_loss, atol=1e-4)\n    assert (\n        1e-3 + reference_loss[-1] < decay_loss[-1]\n    ), \"decayed should be slightly worse\"", "\n\ndef test_adamw_indexed_slices():\n    optimizer = layers.AdamW(0.1)\n    table = tf.Variable(np.zeros((5, 3), dtype=np.float32))\n    indices = tf.constant([0, 2, 2, 4])\n    optimizer.minimize(\n        lambda: tf.reduce_sum(tf.gather(table, indices)), var_list=[table]\n    )\n    np.testing.assert_equal(table[:, 0].numpy() < 0, [True, False, True, False, True])", "\n\ndef test_optimiser_decay_and_vector_learning_rate():\n    for optimiser, learning_rate in [(layers.SgdM, 1.0), (layers.AdamW, 0.1)]:\n        log = _train_sample_model(\n            optimiser(\n                learning_rate, learning_rate_decay=0.1, scale_vector_learning_rate=True\n            )\n        )\n        assert log[-1] < log[0] / 2, log", ""]}
{"filename": "scmm/uscale/utility.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"Utilities to support unit scaling development.\"\"\"\n\nimport collections\nimport functools\nimport sys\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple\n", "from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom ..pedal import utility\n\n\nclass ActivationTracker:\n    \"\"\"Track activations (and gradients) for layers in a model.\n\n    Note that this only works in eager mode, and is designed for offline\n    use.\n\n        layer = keras.layers.Dense(10)\n        layer.build((None, 20))\n\n        tracker = ActivationTracker(layer)\n        with tf.GradientTape() as tape:\n            loss = tf.reduce_sum(layer(tf.zeros((3, 20))))\n        grads_and_vars = zip(\n            tape.gradient(loss, layer.trainable_variables),\n            layer.trainable_variables)\n        tracker.log_gradients(grads_and_vars)  # for weight gradients only\n\n        print({t.name: np.std(t.gradient) for t in tracker.trace})\n    \"\"\"\n\n    @dataclass\n    class Trace:\n        \"\"\"Forward and backward pass tensors from a single edge in the graph.\"\"\"\n\n        name: str\n        activation: np.ndarray\n        gradient: Optional[np.ndarray]\n\n    @dataclass\n    class LayerTrace(Trace):\n        \"\"\"Forward and backward pass information for a layer (with one output).\"\"\"\n\n        layer: keras.layers.Layer\n        weights: Tuple[\"ActivationTracker.Trace\", ...]\n\n    def __init__(self, *layers_to_track: keras.layers.Layer):\n        self._layers: Dict[str, keras.layers.Layer] = {}\n        self._variable_to_weight_name: Dict[tf.Variable, Tuple[str, str]] = {}\n        self._weights: Dict[str, Dict[str, np.ndarray]] = collections.defaultdict(dict)\n        self._weight_gradients: Dict[\n            str, Dict[str, List[np.ndarray]]\n        ] = collections.defaultdict(lambda: collections.defaultdict(list))\n        self._activations: Dict[str, List[np.ndarray]] = collections.defaultdict(list)\n        self._gradients: Dict[str, List[np.ndarray]] = collections.defaultdict(list)\n        for layer in layers_to_track:\n            self.track(layer)\n\n    def _track_layer(self, name: str, layer: keras.layers.Layer) -> None:\n        self._layers[name] = layer\n\n        for weight_name, weight in utility.named_weights(layer, recursive=False):\n            self._weights[name][weight_name] = weight.numpy()\n            self._variable_to_weight_name[weight.ref()] = (name, weight_name)\n\n        @tf.custom_gradient  # type:ignore[misc]\n        def identity_log(x: tf.Tensor) -> tf.Tensor:\n            self._activations[name].append(x.numpy())\n\n            def grad(upstream: tf.Tensor) -> tf.Tensor:\n                self._gradients[name].append(upstream.numpy())\n                return upstream\n\n            return x, grad\n\n        original_call = layer.call\n\n        @functools.wraps(original_call)\n        def wrapper(*args: Any, **kwargs: Any) -> tf.Tensor:\n            output = original_call(*args, **kwargs)\n            if not isinstance(output, tf.Tensor):\n                raise ValueError(\n                    \"Expected a layer to output a single tensor, actual output\"\n                    f\" {type(output)} from layer {layer}\"\n                )\n            return identity_log(output)\n\n        layer.call = wrapper\n\n    def track(self, layer: keras.layers.Layer) -> None:\n        \"\"\"Start track this layer's output and any (recursive) sublayers.\"\"\"\n        for name, sublayer in utility.named_layers(layer):\n            self._track_layer(name, sublayer)\n\n    def log_gradients(\n        self, grads_and_vars: Iterable[Tuple[tf.Tensor, tf.Variable]]\n    ) -> None:\n        \"\"\"Log weight gradients (optional call).\"\"\"\n        for grad, variable in grads_and_vars:\n            if isinstance(grad, tf.IndexedSlices):\n                grad = tf.math.unsorted_segment_sum(\n                    grad.values, grad.indices, grad.shape[0]\n                )\n            layer_name, weight_name = self._variable_to_weight_name[variable.ref()]\n            self._weight_gradients[layer_name][weight_name].append(grad.numpy())\n\n    @staticmethod\n    def _stack_optional(items: Sequence[np.ndarray]) -> Optional[np.ndarray]:\n        return np.stack(items) if items else None\n\n    @property\n    def trace(self) -> Tuple[LayerTrace, ...]:\n        \"\"\"Get activation and gradient traces for each layer (ordered by forward pass).\"\"\"\n        return tuple(\n            self.LayerTrace(\n                name=layer_name,\n                activation=np.stack(self._activations[layer_name]),\n                gradient=self._stack_optional(self._gradients[layer_name]),\n                layer=self._layers[layer_name],\n                weights=tuple(\n                    self.Trace(\n                        name=weight_name,\n                        activation=weight,\n                        gradient=self._stack_optional(\n                            self._weight_gradients[layer_name][weight_name]\n                        ),\n                    )\n                    for weight_name, weight in self._weights[layer_name].items()\n                ),\n            )\n            for layer_name in self._activations  # forward pass ordering\n        )", "\nclass ActivationTracker:\n    \"\"\"Track activations (and gradients) for layers in a model.\n\n    Note that this only works in eager mode, and is designed for offline\n    use.\n\n        layer = keras.layers.Dense(10)\n        layer.build((None, 20))\n\n        tracker = ActivationTracker(layer)\n        with tf.GradientTape() as tape:\n            loss = tf.reduce_sum(layer(tf.zeros((3, 20))))\n        grads_and_vars = zip(\n            tape.gradient(loss, layer.trainable_variables),\n            layer.trainable_variables)\n        tracker.log_gradients(grads_and_vars)  # for weight gradients only\n\n        print({t.name: np.std(t.gradient) for t in tracker.trace})\n    \"\"\"\n\n    @dataclass\n    class Trace:\n        \"\"\"Forward and backward pass tensors from a single edge in the graph.\"\"\"\n\n        name: str\n        activation: np.ndarray\n        gradient: Optional[np.ndarray]\n\n    @dataclass\n    class LayerTrace(Trace):\n        \"\"\"Forward and backward pass information for a layer (with one output).\"\"\"\n\n        layer: keras.layers.Layer\n        weights: Tuple[\"ActivationTracker.Trace\", ...]\n\n    def __init__(self, *layers_to_track: keras.layers.Layer):\n        self._layers: Dict[str, keras.layers.Layer] = {}\n        self._variable_to_weight_name: Dict[tf.Variable, Tuple[str, str]] = {}\n        self._weights: Dict[str, Dict[str, np.ndarray]] = collections.defaultdict(dict)\n        self._weight_gradients: Dict[\n            str, Dict[str, List[np.ndarray]]\n        ] = collections.defaultdict(lambda: collections.defaultdict(list))\n        self._activations: Dict[str, List[np.ndarray]] = collections.defaultdict(list)\n        self._gradients: Dict[str, List[np.ndarray]] = collections.defaultdict(list)\n        for layer in layers_to_track:\n            self.track(layer)\n\n    def _track_layer(self, name: str, layer: keras.layers.Layer) -> None:\n        self._layers[name] = layer\n\n        for weight_name, weight in utility.named_weights(layer, recursive=False):\n            self._weights[name][weight_name] = weight.numpy()\n            self._variable_to_weight_name[weight.ref()] = (name, weight_name)\n\n        @tf.custom_gradient  # type:ignore[misc]\n        def identity_log(x: tf.Tensor) -> tf.Tensor:\n            self._activations[name].append(x.numpy())\n\n            def grad(upstream: tf.Tensor) -> tf.Tensor:\n                self._gradients[name].append(upstream.numpy())\n                return upstream\n\n            return x, grad\n\n        original_call = layer.call\n\n        @functools.wraps(original_call)\n        def wrapper(*args: Any, **kwargs: Any) -> tf.Tensor:\n            output = original_call(*args, **kwargs)\n            if not isinstance(output, tf.Tensor):\n                raise ValueError(\n                    \"Expected a layer to output a single tensor, actual output\"\n                    f\" {type(output)} from layer {layer}\"\n                )\n            return identity_log(output)\n\n        layer.call = wrapper\n\n    def track(self, layer: keras.layers.Layer) -> None:\n        \"\"\"Start track this layer's output and any (recursive) sublayers.\"\"\"\n        for name, sublayer in utility.named_layers(layer):\n            self._track_layer(name, sublayer)\n\n    def log_gradients(\n        self, grads_and_vars: Iterable[Tuple[tf.Tensor, tf.Variable]]\n    ) -> None:\n        \"\"\"Log weight gradients (optional call).\"\"\"\n        for grad, variable in grads_and_vars:\n            if isinstance(grad, tf.IndexedSlices):\n                grad = tf.math.unsorted_segment_sum(\n                    grad.values, grad.indices, grad.shape[0]\n                )\n            layer_name, weight_name = self._variable_to_weight_name[variable.ref()]\n            self._weight_gradients[layer_name][weight_name].append(grad.numpy())\n\n    @staticmethod\n    def _stack_optional(items: Sequence[np.ndarray]) -> Optional[np.ndarray]:\n        return np.stack(items) if items else None\n\n    @property\n    def trace(self) -> Tuple[LayerTrace, ...]:\n        \"\"\"Get activation and gradient traces for each layer (ordered by forward pass).\"\"\"\n        return tuple(\n            self.LayerTrace(\n                name=layer_name,\n                activation=np.stack(self._activations[layer_name]),\n                gradient=self._stack_optional(self._gradients[layer_name]),\n                layer=self._layers[layer_name],\n                weights=tuple(\n                    self.Trace(\n                        name=weight_name,\n                        activation=weight,\n                        gradient=self._stack_optional(\n                            self._weight_gradients[layer_name][weight_name]\n                        ),\n                    )\n                    for weight_name, weight in self._weights[layer_name].items()\n                ),\n            )\n            for layer_name in self._activations  # forward pass ordering\n        )", "\n\ndef printing(\n    name: str, summary: Callable[[tf.Tensor], Any] = np.std\n) -> Callable[[tf.Tensor], tf.Tensor]:\n    \"\"\"Utility for printing forward/backward pass statistics.\n\n    E.g.\n        x = printing(\"x\")(x)\n    \"\"\"\n\n    @tf.custom_gradient  # type:ignore[misc]\n    def operation(x: tf.Tensor) -> tf.Tensor:\n        print(f\"{name} forward {summary.__name__}\", summary(x), file=sys.stderr)\n\n        def grad(upstream: tf.Tensor) -> tf.Tensor:\n            print(\n                f\"{name} backward {summary.__name__}\",\n                summary(upstream),\n                file=sys.stderr,\n            )\n            return upstream\n\n        return x, grad\n\n    return operation  # type:ignore[no-any-return]", ""]}
{"filename": "scmm/uscale/layers.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"Keras layers replacements with unit scaling.\"\"\"\n\nfrom typing import Optional, Tuple\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n", "from tensorflow import keras\n\nfrom .. import layers\nfrom . import ops\n\n\nclass initializers:  # pylint:disable=invalid-name\n    \"\"\"Unit-variance initializers.\"\"\"\n\n    @staticmethod\n    def uniform(seed: Optional[int]) -> keras.initializers.Initializer:\n        \"\"\"Uniform distribution (symmetric about 0).\"\"\"\n        return keras.initializers.RandomUniform(-np.sqrt(3), np.sqrt(3), seed=seed)\n\n    @staticmethod\n    def normal(seed: Optional[int]) -> keras.initializers.Initializer:\n        \"\"\"Standard normal distribution.\"\"\"\n        return keras.initializers.RandomNormal(stddev=1, seed=seed)", "\n\nclass Dense(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"A scaled (and more restrictive) version of keras.layers.Dense.\"\"\"\n\n    def __init__(\n        self,\n        units: int,\n        activation: Optional[str] = None,\n        scale_for: str = \"both\",\n        dtype: tf.DType = tf.float32,\n        seed: Optional[int] = None,\n    ):\n        super().__init__(dtype=dtype)\n        self.units = units\n        self.scale_for = scale_for\n        self.kernel: tf.Variable = None\n        self.kernel_initializer = initializers.uniform(seed)\n        self.bias: tf.Variable = None\n        self.bias_initializer = keras.initializers.zeros()\n        self.activation = keras.activations.get(activation)\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        self.kernel = self.add_weight(\n            \"kernel\",\n            shape=(input_shape[-1], self.units),\n            initializer=self.kernel_initializer,\n        )\n        self.bias = self.add_weight(\n            \"bias\",\n            shape=self.units,\n            initializer=self.bias_initializer,\n        )\n\n    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n        return self.activation(\n            ops.add_bias(\n                ops.pointwise(inputs, self.kernel, scale_for=self.scale_for), self.bias\n            )\n        )", "\n\nclass CausalConv1D(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"A scaled causal 1D convolution.\"\"\"\n\n    # pylint:disable=too-many-instance-attributes\n\n    def __init__(\n        self,\n        filters: int,\n        kernel_size: int,\n        groups: Optional[int] = None,\n        activation: Optional[str] = None,\n        dtype: tf.DType = tf.float32,\n        seed: Optional[int] = None,\n    ):\n        super().__init__(dtype=dtype)\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.groups = groups or 1\n        if filters % self.groups != 0:\n            raise ValueError(\n                f\"Filters ({filters}) must be evenly divisible by groups ({self.groups})\"\n            )\n        self.kernel: tf.Variable = None\n        self.kernel_initializer = initializers.uniform(seed)\n        self.bias: tf.Variable = None\n        self.bias_initializer = keras.initializers.zeros()\n        self.activation = keras.activations.get(activation)\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        input_features = input_shape[-1]\n        if input_features % self.groups != 0:\n            raise ValueError(\n                f\"Input feature size ({input_features}) must be evenly divisible\"\n                f\" by groups ({self.groups})\"\n            )\n        self.kernel = self.add_weight(\n            \"kernel\",\n            shape=(self.kernel_size, input_shape[-1] // self.groups, self.filters),\n            initializer=self.kernel_initializer,\n        )\n        self.bias = self.add_weight(\n            \"bias\", shape=self.filters, initializer=self.bias_initializer\n        )\n\n    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n        padded = tf.pad(inputs, [(0, 0), (self.kernel_size - 1, 0), (0, 0)])\n        return self.activation(\n            ops.add_bias(ops.conv1d(padded, self.kernel, padding=\"VALID\"), self.bias)\n        )", "\n\nclass Embedding(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"A scaled variant of keras.layers.Embedding.\"\"\"\n\n    def __init__(\n        self,\n        table_size: int,\n        embeddings_size: int,\n        dtype: tf.DType = tf.float32,\n        seed: Optional[int] = None,\n    ):\n        super().__init__(dtype=dtype)\n        self.table_size = table_size\n        self.embeddings_size = embeddings_size\n        self.embeddings: tf.Variable = None\n        self.embeddings_initializer = keras.initializers.RandomUniform(\n            -np.sqrt(3), np.sqrt(3), seed=seed\n        )\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        self.embeddings = self.add_weight(\n            \"embeddings\",\n            shape=(self.table_size, self.embeddings_size),\n            initializer=self.embeddings_initializer,\n        )\n\n    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n        # We don't need to worry about inputs scaling, as it is non-differentiable\n        batch_size = np.prod(inputs.shape)\n        # Scaling is based on \"batch size per row\"\n        return tf.gather(\n            ops.scaling(backward=self.table_size / batch_size)(self.embeddings),\n            inputs,\n        )", "\n\nclass LayerNormalization(layers.LayerNormalization):\n    \"\"\"A scaled variant of keras.layers.LayerNormalization.\"\"\"\n\n    def __init__(self, epsilon: float = 0.001, dtype: tf.DType = tf.float32):\n        super().__init__(epsilon=epsilon, dtype=dtype)\n        # Overwritten from base\n        self.beta_initializer = keras.initializers.zeros()\n        self.gamma_initializer = keras.initializers.ones()\n\n    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n        return ops.add_bias(\n            ops.multiply_scale(self._normalize(inputs), self.gamma), self.beta\n        )", "\n\nclass ResidualLayer(layers.ResidualLayer):\n    \"\"\"A scaled (interpolation) residual layer.\"\"\"\n\n    def __init__(\n        self,\n        body: keras.layers.Layer,\n        norm_type: Optional[str],\n        alpha: float,\n        dtype: tf.DType = tf.float32,\n    ):\n        super().__init__(\n            body,\n            norm_type=norm_type,\n            alpha=alpha,\n            dtype=dtype,\n            norm_cls=LayerNormalization,\n        )\n\n    def call(self, x: tf.Tensor) -> tf.Tensor:\n        assert (\n            self.alpha is not None\n        ), \"cannot preserve variance with plain residual (please set 'alpha')\"\n\n        residual_scale = self.alpha**0.5\n        branch = ops.scaling(backward=residual_scale)(x)\n        if self.norm_type == \"pre\":\n            branch = self.norm(branch)\n\n        branch = self.body(branch)\n\n        y = (1 - self.alpha) ** 0.5 * x + ops.scaling(forward=residual_scale)(branch)\n\n        if self.norm_type == \"post\":\n            y = self.norm(y)\n        return y", "\n\nclass FFNLayer(layers.FFNLayer):\n    \"\"\"A scaled FFN layer.\"\"\"\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        hidden_size = input_shape[-1]\n        intermediate_size = int(self.multiple * hidden_size)\n        self.up = Dense(intermediate_size, dtype=self.dtype, seed=self.seeds[0])\n        self.up.build(input_shape[:-1] + (hidden_size,))\n        self.down = Dense(hidden_size, dtype=self.dtype, seed=self.seeds[1])\n        self.down.build(input_shape[:-1] + (intermediate_size,))\n\n    def call(self, x: tf.Tensor) -> tf.Tensor:\n        return self.down(keras.activations.relu(self.up(x)))  # type:ignore[misc]", "\n\nclass MultiHeadAttention(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"Scaled multi-head self attention a la Transformer.\n\n    With causal masking.\n\n    With relative-positional embeddings a la Transformer XL.\n    \"\"\"\n\n    # pylint:disable=too-many-instance-attributes\n    # pylint:disable=R0801\n\n    def __init__(\n        self,\n        heads: int,\n        head_size: int,\n        frequencies: int,\n        max_period: int,\n        dtype: tf.DType = tf.float32,\n        seeds: Optional[Tuple[int, int, int]] = None,\n    ):\n        super().__init__(dtype=dtype)\n        self.heads = heads\n        self.head_size = head_size\n        self.frequencies = frequencies\n        self.max_period = max_period\n        self.seeds = (None, None, None) if seeds is None else seeds\n        self.qkv: tf.Variable = None\n        self.q_bias: tf.Variable = None\n        self.positional: tf.Variable = None\n        self.out: keras.layers.Layer = None\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        input_size = input_shape[-1]\n        self.qkv = self.add_weight(\n            name=\"qkv\",\n            shape=(input_size, 3, self.heads, self.head_size),\n            initializer=initializers.uniform(self.seeds[0]),\n        )\n        self.q_bias = self.add_weight(\n            name=\"q_bias\",\n            shape=(self.heads, self.head_size),\n            initializer=keras.initializers.zeros(),\n        )\n        self.positional = self.add_weight(\n            name=\"positional\",\n            shape=(self.frequencies, self.heads, self.head_size),\n            initializer=initializers.uniform(self.seeds[1]),\n        )\n        self.out = Dense(input_size, dtype=self.dtype, seed=self.seeds[2])\n        self.out.build(input_shape[:-1] + (self.heads * self.head_size,))\n\n    def _positional_weights(self, query: tf.Tensor) -> tf.Tensor:\n        sequence_length = query.shape[-2]\n        sins = tf.constant(\n            np.sqrt(2)\n            * layers.sinusoid_embedding(\n                sequence_length, self.frequencies, self.max_period\n            ),\n            dtype=query.dtype,\n        )\n        embeddings = tf.einsum(\n            \"sf,fnh->nsh\",\n            sins,\n            ops.scaling(\n                forward=self.frequencies**-0.5, backward=sequence_length**-1.0\n            )(self.positional),\n        )\n        scores = tf.einsum(\"bnqh,nvh->bnqv\", query, embeddings) * self.head_size**-0.5\n        return layers.relative_causal_reshape(scores)\n\n    def call(self, input: tf.Tensor) -> tf.Tensor:\n        # pylint:disable=invalid-name\n        batch_size, sequence_length, input_size = input.shape\n        q, k, v = tf.unstack(\n            tf.einsum(\n                \"bsx,xAnh -> Abnsh\",\n                input,\n                ops.scaling(\n                    forward=(3 * input_size * self.head_size * self.heads) ** -0.25,\n                    backward=(batch_size * sequence_length) ** -1.0,\n                )(self.qkv),\n            )\n        )\n        q += ops.scaling(backward=(batch_size * sequence_length) ** -1.0)(\n            self.q_bias[:, tf.newaxis, :]\n        )\n        a = tf.einsum(\"bnqh,bnkh->bnqk\", q, k) * self.head_size**-0.5\n        a += self._positional_weights(q)\n        # Note: oddly, -1e3 can be insufficient in FP16 with no LS, causing \"cheating\"\n        a = layers.causal_mask(a, mask_value=-3e4)\n        a = tf.nn.softmax(a, axis=-1)\n        o = tf.einsum(\"bnqk,bnkh->bqnh\", a, v)\n        return self.out(tf.reshape(o, o.shape[:-2] + (self.head_size * self.heads,)))", "\n\nclass RecurrentHighwayCell(keras.layers.Layer):  # type:ignore[misc]\n    \"\"\"Scaled recurrent highway cell from https://arxiv.org/abs/1607.03474.\"\"\"\n\n    # pylint:disable=R0801\n\n    def __init__(\n        self,\n        hidden_size: int,\n        rebias: float,\n        dtype: tf.DType = tf.float32,\n        seed: Optional[int] = None,\n    ):\n        super().__init__(name=type(self).__name__, dtype=dtype)\n        self.hidden_size = hidden_size\n        self.carry_rebias = rebias\n        self.update_rebias = -rebias\n        self.seed = seed\n        self.gates: tf.Variable = None\n        self.gates_bias: tf.Variable = None\n\n    def build(self, input_shape: tf.TensorShape) -> None:\n        super().build(input_shape)\n        self.gates = self.add_weight(\n            \"gates\",\n            shape=(2, input_shape[-1] + self.hidden_size, self.hidden_size),\n            initializer=initializers.uniform(seed=self.seed),\n        )\n        self.gates_bias = self.add_weight(\n            \"gates_bias\",\n            shape=(2, self.hidden_size),\n            initializer=keras.initializers.zeros(),\n        )\n\n    def call(\n        self, input: tf.Tensor, hidden: tf.Tensor, sequence_length: int\n    ) -> tf.Tensor:\n        batch_size = input.shape[0] * sequence_length\n        gates_scale = (\n            2 * (input.shape[1] + self.hidden_size) * self.hidden_size\n        ) ** -0.25\n        gate_outputs = tf.concat([input, hidden], axis=1) @ ops.scaling(\n            forward=gates_scale, backward=batch_size**-1.0\n        )(self.gates)\n        gate_outputs += ops.scaling(backward=batch_size**-1.0)(\n            self.gates_bias[:, tf.newaxis]\n        )\n        transform, update = tf.unstack(gate_outputs)\n        update = tf.sigmoid(update + self.update_rebias)\n        return (1 - update) * hidden + update * tf.tanh(transform)", "\n\nclass RNN(layers.RNN):\n    \"\"\"A scaled, basic unidirectional RNN.\"\"\"\n\n    def call(self, input: tf.Tensor) -> tf.Tensor:\n        batch_size, sequence_length, _ = input.shape\n        # Note: sbh = (sequence, batch, hidden)\n        input_sbh = tf.transpose(input, (1, 0, 2))\n        initial_hidden = tf.tile(\n            ops.scaling(backward=batch_size**-1.0)(self.initial_hidden[tf.newaxis]),\n            (batch_size, 1),\n        )\n        output_sbh = tf.scan(\n            lambda hidden, input: self.cell(\n                input, hidden, sequence_length=sequence_length\n            ),\n            input_sbh,\n            initializer=initial_hidden,\n        )\n        return tf.transpose(output_sbh, (1, 0, 2))", "\n\nclass PadAndShiftLayer(layers.PadAndShiftLayer):\n    \"\"\"Shifts sequence features one place to the right with a trainable padding vector.\"\"\"\n\n    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n        npad = inputs.shape[0]\n        pad = tf.tile(\n            ops.scaling(backward=npad**-1.0)(self.padding[tf.newaxis, tf.newaxis]),\n            [npad, 1, 1],\n        )\n        return tf.concat([pad, inputs[:, :-1, :]], axis=1)", ""]}
{"filename": "scmm/uscale/ops.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"Unit scaling - basic, portable operations.\"\"\"\n\nfrom typing import Callable, Optional, Tuple\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef scaling(\n    forward: Optional[float] = None, backward: Optional[float] = None\n) -> Callable[[tf.Tensor], tf.Tensor]:\n    \"\"\"Perform arbitary *seperate* scaling in the forward and backward passes.\n\n    E.g.\n\n        y = scaling(forward=2, backward=3)(x)\n\n    \"\"\"\n\n    @tf.custom_gradient  # type:ignore[misc]\n    def operation(input: tf.Tensor) -> tf.Tensor:\n        def grad(upstream: tf.Tensor) -> tf.Tensor:\n            grad_input = upstream\n            if backward is not None:\n                if isinstance(grad_input, tf.IndexedSlices):\n                    grad_input = tf.IndexedSlices(\n                        values=grad_input.values\n                        * tf.cast(backward, grad_input.values.dtype),\n                        indices=upstream.indices,\n                        dense_shape=upstream.dense_shape,\n                    )\n                else:\n                    grad_input = grad_input * tf.cast(backward, grad_input.dtype)\n            return grad_input\n\n        output = input\n        if forward is not None:\n            output = output * tf.cast(forward, output.dtype)\n\n        return output, grad\n\n    return operation  # type:ignore[no-any-return]", "\n\ndef scaling(\n    forward: Optional[float] = None, backward: Optional[float] = None\n) -> Callable[[tf.Tensor], tf.Tensor]:\n    \"\"\"Perform arbitary *seperate* scaling in the forward and backward passes.\n\n    E.g.\n\n        y = scaling(forward=2, backward=3)(x)\n\n    \"\"\"\n\n    @tf.custom_gradient  # type:ignore[misc]\n    def operation(input: tf.Tensor) -> tf.Tensor:\n        def grad(upstream: tf.Tensor) -> tf.Tensor:\n            grad_input = upstream\n            if backward is not None:\n                if isinstance(grad_input, tf.IndexedSlices):\n                    grad_input = tf.IndexedSlices(\n                        values=grad_input.values\n                        * tf.cast(backward, grad_input.values.dtype),\n                        indices=upstream.indices,\n                        dense_shape=upstream.dense_shape,\n                    )\n                else:\n                    grad_input = grad_input * tf.cast(backward, grad_input.dtype)\n            return grad_input\n\n        output = input\n        if forward is not None:\n            output = output * tf.cast(forward, output.dtype)\n\n        return output, grad\n\n    return operation  # type:ignore[no-any-return]", "\n\ndef pointwise(\n    inputs: tf.Tensor, weights: tf.Tensor, scale_for: str = \"both\"\n) -> tf.Tensor:\n    \"\"\"A scaled pointwise transformation.\n\n    inputs -- activations, will receive consistent gradients between forward & backward passes\n\n    weights -- will receive scaled gradients\n\n    scale_for -- how should the forward/backward-inputs pass scale be chosen?\n\n                \"forward\" -- preserve variance in the forward pass\n                \"backward\" -- preserve variance in the backward pass\n                \"both\" -- trade off forward and backward pass variance\n                \"both_arithmetic\" -- ditto, using arithmetic mean\n                \"both_min\" - ditto, using the minimum scale between forward and backward\n                \"separate\" -- separate scaling factors in the forward and backward-inputs passes\n                              WARNING - when using skip connections, this may cause inconsistent\n                              gradients.\n    \"\"\"\n    assert len(weights.shape) == 2, \"pointwise requires 2D rhs `weights`\"\n\n    input_size, output_size = weights.shape\n    backward_weights_scale = np.prod(inputs.shape[:-1]) ** -1.0\n\n    if scale_for == \"separate\":\n        return scaling(forward=input_size**-0.5)(\n            scaling(backward=output_size**-0.5)(inputs)\n            @ scaling(backward=backward_weights_scale)(weights)\n        )\n\n    # Note \"both\" is different from Glorot's sqrt(2 / (input_size + output_size)), as this\n    # should preserves scale better after boom_down(boom_up(x))\n    forward_scale = dict(\n        forward=input_size**-0.5,\n        backward=output_size**-0.5,\n        both=(input_size * output_size) ** -0.25,\n        both_arithmetic=((input_size + output_size) / 2) ** -0.5,\n        both_min=max(input_size, output_size) ** -0.5,\n    )[scale_for]\n\n    return inputs @ scaling(forward=forward_scale, backward=backward_weights_scale)(\n        weights\n    )", "\n\ndef conv1d(\n    input: tf.Tensor, filters: tf.Tensor, stride: int = 1, padding: str = \"SAME\"\n) -> tf.Tensor:\n    \"\"\"Scaling version of tf.nn.conv1d.\"\"\"\n    # pylint:disable=too-many-locals\n    batch_size, input_length, channels_in = input.shape\n    filter_width, filter_channels_in, channels_out = filters.shape\n\n    # See https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2\n    output_length = dict(\n        SAME=np.ceil(input_length / stride),\n        VALID=np.ceil((input_length + 1 - filter_width) / stride),\n    )[padding]\n    n_groups = channels_in // filter_channels_in\n\n    forward_contraction = filter_width * channels_in // n_groups\n    backward_contraction = filter_width / stride * channels_out // n_groups\n    forward_scale = (forward_contraction * backward_contraction) ** -0.25\n    backward_scale = (output_length * batch_size) ** -1.0\n\n    return tf.nn.conv1d(\n        input,\n        scaling(forward=forward_scale, backward=backward_scale)(filters),\n        stride=stride,\n        padding=padding,\n    )", "\n\ndef conv2d(\n    input: tf.Tensor, filters: tf.Tensor, strides: int = 1, padding: str = \"SAME\"\n) -> tf.Tensor:\n    \"\"\"Scaling version of tf.nn.conv2d.\"\"\"\n    # pylint:disable=too-many-locals\n    batch_size, height, width, channels_in = input.shape\n    kernel_height, kernel_width, filter_channels_in, channels_out = filters.shape\n\n    # See https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2\n    output_area = dict(\n        SAME=np.ceil(height / strides) * np.ceil(width / strides),\n        VALID=np.ceil((height + 1 - kernel_height) / strides)\n        * np.ceil((width + 1 - kernel_width) / strides),\n    )[padding]\n    n_groups = channels_in // filter_channels_in\n\n    forward_contraction = kernel_height * kernel_width * channels_in // n_groups\n    backward_contraction = (\n        (kernel_height / strides) * (kernel_width / strides) * channels_out // n_groups\n    )\n    forward_scale = (forward_contraction * backward_contraction) ** -0.25\n    backward_scale = (output_area * batch_size) ** -1.0\n\n    return tf.nn.conv2d(\n        input,\n        scaling(forward=forward_scale, backward=backward_scale)(filters),\n        strides=strides,\n        padding=padding,\n    )", "\n\ndef add_bias(features: tf.Tensor, bias: tf.Tensor) -> tf.Tensor:\n    \"\"\"Add a bias (which should be zero-initialized), with a scaled backward pass.\"\"\"\n    assert len(bias.shape) == 1, \"bias should be 1D\"\n    batch_size = np.prod(features.shape[:-1])\n    return features + scaling(backward=batch_size**-1.0)(bias)\n\n\ndef multiply_scale(features: tf.Tensor, scale: tf.Tensor) -> tf.Tensor:\n    \"\"\"Multiply by a scale tensor (which should be unit-initialized), with a scaled backward pass.\"\"\"\n    assert len(scale.shape) == 1, \"scale should be 1D\"\n    batch_size = np.prod(features.shape[:-1])\n    return features * scaling(backward=batch_size**-1.0)(scale)", "\ndef multiply_scale(features: tf.Tensor, scale: tf.Tensor) -> tf.Tensor:\n    \"\"\"Multiply by a scale tensor (which should be unit-initialized), with a scaled backward pass.\"\"\"\n    assert len(scale.shape) == 1, \"scale should be 1D\"\n    batch_size = np.prod(features.shape[:-1])\n    return features * scaling(backward=batch_size**-1.0)(scale)\n\n\ndef batched_gather(tables: tf.Tensor, indices: tf.Tensor) -> tf.Tensor:\n    \"\"\"Simulate tf.gather(tables, indices, batch_dims=indices.ndim).\n\n    Better compilation on IPU vs `tf.gather(logp, ids, batch_dims=2)`\n    \"\"\"\n    # Implemented here and in scmm.layers to avoid circular dependency issues\n    assert len(tables.shape) == len(indices.shape) + 1\n    # Use a one-hot encoding to save code memory\n    return tf.squeeze(\n        tf.one_hot(indices, tables.shape[-1], dtype=tables.dtype)[..., tf.newaxis, :]\n        @ tables[..., tf.newaxis],\n        [-1, -2],\n    )", "def batched_gather(tables: tf.Tensor, indices: tf.Tensor) -> tf.Tensor:\n    \"\"\"Simulate tf.gather(tables, indices, batch_dims=indices.ndim).\n\n    Better compilation on IPU vs `tf.gather(logp, ids, batch_dims=2)`\n    \"\"\"\n    # Implemented here and in scmm.layers to avoid circular dependency issues\n    assert len(tables.shape) == len(indices.shape) + 1\n    # Use a one-hot encoding to save code memory\n    return tf.squeeze(\n        tf.one_hot(indices, tables.shape[-1], dtype=tables.dtype)[..., tf.newaxis, :]\n        @ tables[..., tf.newaxis],\n        [-1, -2],\n    )", "\n\ndef softmax_cross_entropy(\n    scores: tf.Tensor, ids: tf.Tensor, mask: tf.Tensor\n) -> Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Compute masked softmax cross entropy loss.\n\n    Note that we abandon unit scaling in the forward pass, since this is\n    designed as a final loss term.\n\n    returns -- (average_loss, n_items) -- average_loss always in fp32\n    \"\"\"\n    assert mask.shape == ids.shape, \"mask should match target ids\"\n    # Use float32 for local computation - keeping things simple\n    logp = tf.nn.log_softmax(tf.cast(scores, tf.float32), axis=-1)\n    target_logp = batched_gather(logp, ids)\n    total_loss = tf.reduce_sum(tf.cast(mask, target_logp.dtype) * -target_logp)\n    n_ids = tf.reduce_sum(tf.cast(mask, tf.int32))\n    n_classes = scores.shape[-1]\n    loss = scaling(backward=np.prod(mask.shape) * n_classes / np.sqrt(n_classes - 1))(\n        total_loss / tf.cast(n_ids, total_loss.dtype)\n    )\n    return loss, n_ids", ""]}
{"filename": "scmm/uscale/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\n\"\"\"Core components for unit-scaling activations and gradients.\"\"\"\n\nfrom . import layers, ops, utility  # NOQA: F401\n"]}
{"filename": "scmm/uscale/tests/__init__.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n"]}
{"filename": "scmm/uscale/tests/test_utility.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\nfrom typing import Tuple\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom .. import utility", "\nfrom .. import utility\n\n\ndef test_activation_tracker_example():\n    # Sync with docstring\n    layer = keras.layers.Dense(10)\n    layer.build((None, 20))\n\n    tracker = utility.ActivationTracker(layer)\n\n    with tf.GradientTape() as tape:\n        loss = tf.reduce_sum(layer(tf.zeros((3, 20))))\n    grads_and_vars = zip(\n        tape.gradient(loss, layer.trainable_variables), layer.trainable_variables\n    )\n    tracker.log_gradients(grads_and_vars)  # only needed for weight gradients\n\n    # Checks\n    (trace,) = tracker.trace\n    assert trace.name == \"\"\n    assert trace.layer is layer\n    assert trace.activation.shape == (1, 3, 10)\n    assert trace.gradient.shape == (1, 3, 10)  # type:ignore[union-attr]\n\n    weights = {t.name: t for t in trace.weights}\n    assert weights[\"kernel\"].activation.shape == (20, 10)\n    assert weights[\"kernel\"].gradient.shape == (1, 20, 10)  # type:ignore[union-attr]\n    assert weights[\"bias\"].activation.shape == (10,)\n    assert weights[\"bias\"].gradient.shape == (1, 10)  # type:ignore[union-attr]", "\n\ndef test_activation_tracker_multiple_outputs():\n    class Duplicate(keras.layers.Layer):\n        # pylint:disable=too-few-public-methods\n        def call(self, x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n            return x, x\n\n    layer = Duplicate()\n    utility.ActivationTracker(layer)\n    with pytest.raises(ValueError) as error:\n        layer(tf.ones(3))\n    assert \"tuple\" in str(error)", "\n\ndef test_activation_tracker_embedding_gradient():\n    layer = keras.layers.Embedding(5, 8)\n    layer.build((None,))\n\n    tracker = utility.ActivationTracker(layer)\n    with tf.GradientTape() as tape:\n        loss = tf.reduce_sum(layer(tf.constant([0, 3, 3])))\n    grads_and_vars = zip(\n        tape.gradient(loss, layer.trainable_variables), layer.trainable_variables\n    )\n    tracker.log_gradients(grads_and_vars)\n\n    embedding_trace = tracker.trace[0].weights[0]\n    assert embedding_trace.name == \"embeddings\"\n    assert embedding_trace.gradient.shape == (1, 5, 8)  # type:ignore[union-attr]\n    np.testing.assert_equal(\n        embedding_trace.gradient[0, :, 0],  # type:ignore[index]\n        [1, 0, 0, 2, 0],\n    )", "\n\ndef test_printing(capsys):\n    with tf.GradientTape() as tape:\n        x = tf.constant([0, 1, 0, 1], dtype=tf.float32)\n        tape.watch(x)\n        y = tf.reduce_sum(utility.printing(\"x\")(x) ** 2)\n    tape.gradient(y, x)\n    assert capsys.readouterr().err == \"x forward std 0.5\\nx backward std 1.0\\n\"\n", ""]}
{"filename": "scmm/uscale/tests/test_ops.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\nimport functools\nfrom typing import Any, Callable, Dict, Optional, Tuple, Union\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom ...tests import testing", "\nfrom ...tests import testing\nfrom .. import ops\n\n\ndef test_scaling():\n    with tf.GradientTape() as tape:\n        x = tf.range(3, dtype=tf.float32)\n        tape.watch(x)\n        y = ops.scaling(forward=2, backward=3)(x)\n    grad_x = tape.gradient(y, x, tf.ones_like(y))\n    np.testing.assert_allclose(y, [0, 2, 4])\n    np.testing.assert_allclose(grad_x, [3, 3, 3])", "\n\ndef test_scaling_indexed_slices():\n    with tf.GradientTape() as tape:\n        table = tf.range(10, dtype=tf.float32)\n        tape.watch(table)\n        y = tf.gather(ops.scaling(backward=5)(table), tf.constant([1, 1, 2]))\n    grad_table = tape.gradient(y, table, tf.ones_like(y))\n    np.testing.assert_allclose(y, [1, 1, 2])\n    np.testing.assert_allclose(grad_table.indices, [1, 1, 2])\n    np.testing.assert_allclose(grad_table.values, [5, 5, 5])", "\n\ndef assert_scaled_allclose(\n    actual: np.ndarray, desired: np.ndarray, atol: float, err_msg: str = \"\"\n) -> None:\n    np.testing.assert_allclose(\n        actual / np.std(actual), desired / np.std(desired), atol=atol, err_msg=err_msg\n    )\n\n\ndef check_op(\n    scaled: Callable[..., tf.Tensor],\n    reference: Callable[..., tf.Tensor],\n    seed: int,\n    args: Dict[str, Union[np.ndarray, Tuple[int, ...]]],\n    extra_args: Optional[Dict[str, Any]] = None,\n    shifted: bool = False,\n) -> Dict[str, tf.Tensor]:\n    # pylint:disable=too-many-locals\n    random = np.random.Generator(np.random.PCG64(seed))\n\n    inputs = {}\n    for key, value in args.items():\n        if isinstance(value, np.ndarray):\n            inputs[key] = tf.constant(value)\n        elif key in {\"input\", \"inputs\"}:\n            # This is pretty hacky & in need of a refactor...\n            inputs[key] = tf.constant(testing.correlated_batch_random(random, value))\n        else:\n            inputs[key] = tf.constant(random.normal(size=value).astype(np.float32))\n\n    with tf.GradientTape() as tape:\n        tape.watch(inputs.values())\n        scaled_out = scaled(**inputs, **(extra_args or {}))\n    # output_grad = random.normal(size=scaled_out.shape).astype(np.float32)\n    output_grad = testing.correlated_batch_random(random, scaled_out.shape)\n    scaled_grad = tape.gradient(scaled_out, inputs, output_grad)\n    with tf.GradientTape() as tape:\n        tape.watch(inputs.values())\n        reference_out = reference(**inputs, **(extra_args or {}))\n    reference_grad = tape.gradient(reference_out, inputs, output_grad)\n\n    assert_scaled_allclose(\n        scaled_out - shifted * np.mean(scaled_out),\n        reference_out - shifted * np.mean(reference_out),\n        atol=1e-3,\n    )\n    for arg in scaled_grad:\n        assert_scaled_allclose(\n            scaled_grad[arg], reference_grad[arg], atol=1e-3, err_msg=f\"for grad {arg}\"\n        )\n\n    return dict(\n        out=scaled_out,\n        grad=scaled_grad,\n        reference_out=reference_out,\n        reference_grad=reference_grad,\n    )", "\n\ndef check_op(\n    scaled: Callable[..., tf.Tensor],\n    reference: Callable[..., tf.Tensor],\n    seed: int,\n    args: Dict[str, Union[np.ndarray, Tuple[int, ...]]],\n    extra_args: Optional[Dict[str, Any]] = None,\n    shifted: bool = False,\n) -> Dict[str, tf.Tensor]:\n    # pylint:disable=too-many-locals\n    random = np.random.Generator(np.random.PCG64(seed))\n\n    inputs = {}\n    for key, value in args.items():\n        if isinstance(value, np.ndarray):\n            inputs[key] = tf.constant(value)\n        elif key in {\"input\", \"inputs\"}:\n            # This is pretty hacky & in need of a refactor...\n            inputs[key] = tf.constant(testing.correlated_batch_random(random, value))\n        else:\n            inputs[key] = tf.constant(random.normal(size=value).astype(np.float32))\n\n    with tf.GradientTape() as tape:\n        tape.watch(inputs.values())\n        scaled_out = scaled(**inputs, **(extra_args or {}))\n    # output_grad = random.normal(size=scaled_out.shape).astype(np.float32)\n    output_grad = testing.correlated_batch_random(random, scaled_out.shape)\n    scaled_grad = tape.gradient(scaled_out, inputs, output_grad)\n    with tf.GradientTape() as tape:\n        tape.watch(inputs.values())\n        reference_out = reference(**inputs, **(extra_args or {}))\n    reference_grad = tape.gradient(reference_out, inputs, output_grad)\n\n    assert_scaled_allclose(\n        scaled_out - shifted * np.mean(scaled_out),\n        reference_out - shifted * np.mean(reference_out),\n        atol=1e-3,\n    )\n    for arg in scaled_grad:\n        assert_scaled_allclose(\n            scaled_grad[arg], reference_grad[arg], atol=1e-3, err_msg=f\"for grad {arg}\"\n        )\n\n    return dict(\n        out=scaled_out,\n        grad=scaled_grad,\n        reference_out=reference_out,\n        reference_grad=reference_grad,\n    )", "\n\ndef test_add_bias():\n    out = check_op(\n        ops.add_bias,\n        lambda features, bias: features + bias,\n        seed=842,\n        args=dict(features=(1000,), bias=np.zeros(1000, dtype=np.float32)),\n    )\n    testing.assert_unit_scale(out[\"grad\"][\"bias\"], tol=0.05)", "\n\ndef test_multiply_scale():\n    out = check_op(\n        ops.multiply_scale,\n        lambda features, scale: features * scale,\n        seed=2345,\n        args=dict(features=(1000,), scale=np.ones(1000, dtype=np.float32)),\n    )\n    testing.assert_unit_scale(out[\"grad\"][\"scale\"], tol=0.05)", "\n\n@pytest.mark.parametrize(\n    \"scale_for\",\n    [\"forward\", \"backward\", \"both\", \"both_arithmetic\", \"both_min\", \"separate\"],\n)\ndef test_pointwise(scale_for):\n    out = check_op(\n        functools.partial(ops.pointwise, scale_for=scale_for),\n        lambda inputs, weights: tf.matmul(  # pylint:disable=unnecessary-lambda\n            inputs, weights\n        ),\n        seed=300,\n        args=dict(inputs=(3, 33, 1000), weights=(1000, 500)),\n    )\n    testing.assert_unit_scale(out[\"grad\"][\"weights\"], tol=0.05)\n    std_out = np.std(out[\"out\"])\n    std_grad_inputs = np.std(out[\"grad\"][\"inputs\"])\n    if scale_for in {\"forward\", \"separate\"}:\n        np.testing.assert_allclose(std_out, 1, atol=0.05)\n    if scale_for in {\"backward\", \"separate\"}:\n        np.testing.assert_allclose(std_grad_inputs, 1, atol=0.05)\n    if scale_for == \"both\":\n        np.testing.assert_allclose(std_out * std_grad_inputs, 1, atol=0.05)\n    if scale_for == \"both_min\":\n        assert std_out < 1.05 and std_grad_inputs < 1.05\n    if scale_for == \"both_arithmetic\":\n        assert (std_out < 1) != (std_grad_inputs < 1), \"signs should be swapped\"", "\n\n@pytest.mark.parametrize(\n    [\"padding\", \"stride\"], [(\"SAME\", 1), (\"SAME\", 2), (\"VALID\", 1), (\"VALID\", 2)]\n)\ndef test_conv1d(padding, stride):\n    # Note: we're a bit sloppy about padding when using \"SAME\"\n    out = check_op(\n        ops.conv1d,\n        tf.nn.conv1d,\n        seed=400,\n        args=dict(input=(7, 27, 300), filters=(5, 300, 450)),\n        extra_args=dict(stride=stride, padding=padding),\n    )\n    np.testing.assert_allclose(\n        np.std(out[\"out\"]) * np.std(out[\"grad\"][\"input\"]), 1, atol=0.1\n    )\n    testing.assert_unit_scale(out[\"grad\"][\"filters\"], tol=0.1)", "\n\n@pytest.mark.parametrize(\n    [\"padding\", \"stride\"], [(\"SAME\", 1), (\"SAME\", 2), (\"VALID\", 1), (\"VALID\", 2)]\n)\ndef test_conv2d(padding, stride):\n    # Note: we're a bit sloppy about padding when using \"SAME\"\n    out = check_op(\n        ops.conv2d,\n        tf.nn.conv2d,\n        seed=500,\n        args=dict(input=(1, 10, 15, 300), filters=(2, 3, 300, 500)),\n        extra_args=dict(strides=stride, padding=padding),\n    )\n    np.testing.assert_allclose(\n        np.std(out[\"out\"]) * np.std(out[\"grad\"][\"input\"]), 1, atol=0.2\n    )\n    testing.assert_unit_scale(out[\"grad\"][\"filters\"], tol=0.2)", "\n\ndef test_batched_gather():\n    tables = tf.reshape(tf.range(2 * 3 * 4), (2, 3, 4))\n    indices = tf.constant([[0, 0, 3], [2, 2, 3]])\n    np.testing.assert_equal(\n        np.array(ops.batched_gather(tables, indices)),\n        [[0 + 0, 4 + 0, 8 + 3], [12 + 2, 16 + 2, 20 + 3]],\n    )\n", "\n\ndef test_softmax_cross_entropy():\n    batch_size, sequence_length, n_classes = (10, 20, 5)\n    random = np.random.Generator(np.random.PCG64(seed=1000))\n    scores = tf.constant(random.normal(size=(batch_size, sequence_length, n_classes)))\n    ids = tf.constant(random.integers(n_classes, size=(batch_size, sequence_length)))\n    with tf.GradientTape() as tape:\n        tape.watch(scores)\n        loss, n_ids = ops.softmax_cross_entropy(scores, ids, tf.ones_like(ids))\n    assert int(n_ids) == batch_size * sequence_length\n    assert 0 < float(loss) < 2 * np.log(n_classes)\n    grad_scores = tape.gradient(loss, scores)\n    testing.assert_unit_scale(grad_scores, 0.1)", ""]}
{"filename": "scmm/uscale/tests/test_layers.py", "chunked_list": ["# Copyright (c) 2023 Graphcore Ltd. All rights reserved.\n\nfrom typing import Optional\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom ...pedal import utility\nfrom ...tests import testing", "from ...pedal import utility\nfrom ...tests import testing\nfrom .. import layers\n\n\ndef test_initializers():\n    testing.assert_unit_scale(layers.initializers.uniform(100)((1000,)), 0.05)\n    testing.assert_unit_scale(layers.initializers.normal(200)((1000,)), 0.05)\n\n\ndef test_layer_dense():\n    layer = layers.Dense(400, seed=123)\n    out = testing.output_and_gradients(layer, (3, 200), seed=456)\n    assert out[\"outputs\"].shape == (3, 400)\n    np.testing.assert_allclose(\n        np.std(out[\"outputs\"]) * np.std(out[\"grad_inputs\"]), 1, atol=0.05\n    )\n    testing.assert_unit_scale(out[\"grad_kernel\"], 0.05)\n    testing.assert_unit_scale(out[\"grad_bias\"], 0.05)", "\n\ndef test_layer_dense():\n    layer = layers.Dense(400, seed=123)\n    out = testing.output_and_gradients(layer, (3, 200), seed=456)\n    assert out[\"outputs\"].shape == (3, 400)\n    np.testing.assert_allclose(\n        np.std(out[\"outputs\"]) * np.std(out[\"grad_inputs\"]), 1, atol=0.05\n    )\n    testing.assert_unit_scale(out[\"grad_kernel\"], 0.05)\n    testing.assert_unit_scale(out[\"grad_bias\"], 0.05)", "\n\ndef test_layer_causalconv1d():\n    # Choose kernel size << sequence length to reduce the effect of padding\n    # (which we don't account for in variance preservation)\n    layer = layers.CausalConv1D(filters=400, kernel_size=3, seed=321)\n    out = testing.output_and_gradients(layer, (4, 19, 200), seed=654)\n    assert out[\"outputs\"].shape == (4, 19, 400)\n    np.testing.assert_allclose(\n        np.std(out[\"outputs\"]) * np.std(out[\"grad_inputs\"]), 1, atol=0.1\n    )\n    testing.assert_unit_scale(out[\"grad_kernel\"], 0.05)\n    testing.assert_unit_scale(out[\"grad_bias\"], 0.05)\n\n    with pytest.raises(ValueError) as error:\n        layers.CausalConv1D(filters=16, kernel_size=5, groups=3)\n    assert \"Filters (16)\" in str(error)\n    assert \"groups (3)\" in str(error)\n\n    layer = layers.CausalConv1D(filters=15, kernel_size=5, groups=3)\n    with pytest.raises(ValueError) as error:\n        layer(tf.zeros((1, 10, 16)))\n    assert \"feature size (16)\" in str(error)\n    assert \"groups (3)\" in str(error)", "\n\ndef test_layer_embedding():\n    random = np.random.Generator(np.random.PCG64(seed=200))\n    layer = layers.Embedding(table_size=40, embeddings_size=300, seed=100)\n    with tf.GradientTape() as tape:\n        outputs = layer(random.integers(layer.table_size, size=(3, 190)))\n    assert outputs.shape == (3, 190, 300)\n    testing.assert_unit_scale(outputs, tol=0.1)\n\n    grad_embeddings = tape.gradient(\n        outputs,\n        layer.embeddings,\n        testing.correlated_batch_random(random, outputs.shape),\n    )\n    grad_embeddings = tf.math.unsorted_segment_sum(\n        grad_embeddings.values, grad_embeddings.indices, grad_embeddings.shape[0]\n    )\n    testing.assert_unit_scale(grad_embeddings, tol=0.1)", "\n\n@pytest.mark.parametrize(\n    [\"norm_type\", \"alpha\"], [(\"pre\", 0.1), (\"post\", 0.1), (None, 0.1), (None, 0.9)]\n)\ndef test_layer_residual(\n    norm_type: Optional[str], alpha: float\n):  # also tests LayerNormalization\n    layer = layers.ResidualLayer(\n        layers.Dense(250, seed=4832), norm_type=norm_type, alpha=alpha\n    )\n    out = testing.output_and_gradients(layer, (29, 19, 250), seed=2393)\n    testing.assert_unit_scale(out[\"outputs\"], tol=0.1)\n    testing.assert_unit_scale(out[\"grad_inputs\"], tol=0.1)\n    for name, variable in utility.named_weights(layer):\n        if variable.trainable:\n            testing.assert_unit_scale(\n                out[f\"grad_{name}\"], tol=0.1, err_msg=f\"for grad_{name}\"\n            )", "\n\ndef test_layer_ffn():\n    layer = layers.FFNLayer(2, seeds=(48723, 7428))\n    out = testing.output_and_gradients(layer, (5, 6, 7), seed=200)\n    assert out[\"outputs\"].shape == (5, 6, 7)\n    assert {k: v.shape for k, v in utility.named_weights(layer)} == {\n        \"up.kernel\": (7, 14),\n        \"up.bias\": (14,),\n        \"down.kernel\": (14, 7),\n        \"down.bias\": (7,),\n    }", "\n\ndef test_layer_multi_head_attention():\n    layer = layers.MultiHeadAttention(\n        heads=5, head_size=16, frequencies=50, max_period=90, seeds=(50, 60, 70)\n    )\n    out = testing.output_and_gradients(layer, (29, 90, 64), seed=300)\n    assert out[\"outputs\"].shape == (29, 90, 64)\n    for key, _ in utility.named_weights(layer):\n        # When inputs are constant over the sequence axis, these gradients are\n        # approximately zero\n        if key not in {\"positional\", \"q_bias\"}:\n            # Nowhere near unit scale, but we'll just check a broad range\n            assert 0.1 < np.std(out[f\"grad_{key}\"]) < 10, f\"for {key}\"", "\n\ndef test_rnn():  # also tests RecurrentHighwayCell\n    layer = layers.RNN(\n        layers.RecurrentHighwayCell(hidden_size=128, rebias=1, seed=3893)\n    )\n    out = testing.output_and_gradients(layer, (29, 90, 64), seed=1398)\n    assert out[\"outputs\"].shape == (29, 90, 128)\n    for key, _ in utility.named_weights(layer):\n        # Nowhere near unit scale, but we'll just check a broad range\n        assert 0.1 < np.std(out[f\"grad_{key}\"]) < 10", "\n\ndef test_pad_and_shift_layer():\n    layer = layers.PadAndShiftLayer()\n    out = testing.output_and_gradients(layer, (21, 5, 128), seed=49824)\n    assert out[\"outputs\"].shape == (21, 5, 128)\n    testing.assert_unit_scale(out[\"grad_inputs\"][:, :-1, :], 0.05)\n    testing.assert_unit_scale(out[\"grad_padding\"], 0.05)\n", ""]}
