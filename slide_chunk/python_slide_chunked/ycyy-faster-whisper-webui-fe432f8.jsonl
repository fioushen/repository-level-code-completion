{"filename": "setup.py", "chunked_list": ["import os\nimport sys\nfrom pathlib import Path\nimport winreg\n\ndef main():\n    if not check_ffmpeg_path():\n        cmd = input(\"\u9700\u8981\u5b89\u88c5ffmpeg,\u662f\u5426\u7ee7\u7eed?(y/n):\")\n        if cmd.lower() != 'y':\n            sys.exit(1)\n        # ffmpeg \u8def\u5f84\u6dfb\u52a0\u5230\u73af\u5883\u53d8\u91cf\u4e2d\n        add_ffmpeg_path()", "\ndef check_ffmpeg_path():\n    path_list = os.environ['Path'].split(';')\n    ffmpeg_found = False\n\n    for path in path_list:\n        if 'ffmpeg' in path.lower() and 'bin' in path.lower():\n            ffmpeg_found = True\n            break\n    \n    return ffmpeg_found", "\ndef add_ffmpeg_path():\n    ffmpeg_bin_path = Path('.\\\\ffmpeg\\\\bin')\n    if ffmpeg_bin_path.is_dir():\n        abs_path = str(ffmpeg_bin_path.resolve())\n        \n        try:\n            key = winreg.OpenKey(\n                winreg.HKEY_CURRENT_USER,\n                r\"Environment\",\n                0,\n                winreg.KEY_READ | winreg.KEY_WRITE\n            )\n            \n            try:\n                current_path, _ = winreg.QueryValueEx(key, \"Path\")\n                if abs_path not in current_path:\n                    new_path = f\"{current_path};{abs_path}\"\n                    winreg.SetValueEx(key, \"Path\", 0, winreg.REG_EXPAND_SZ, new_path)\n                    print(f\"Added FFmpeg path to user variable 'Path': {abs_path}\")\n                else:\n                    print(\"FFmpeg path already exists in the user variable 'Path'.\")\n            finally:\n                winreg.CloseKey(key)\n        except WindowsError:\n            print(\"Error: Unable to modify user variable 'Path'.\")\n            sys.exit(1)\n\n    else:\n        print(\"Error: ffmpeg\\\\bin folder not found in the current path.\")\n        sys.exit(1)", "\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "app-shared.py", "chunked_list": ["# Run the app with no audio file restrictions\nfrom app import create_ui\nfrom src.config import ApplicationConfig\n\ncreate_ui(ApplicationConfig.create_default(input_audio_max_duration=-1, share=True))"]}
{"filename": "app.py", "chunked_list": ["from datetime import datetime\nimport math\nfrom typing import Iterator, Union\nimport argparse\n\nfrom io import StringIO\nimport os\nimport pathlib\nimport tempfile\nimport zipfile", "import tempfile\nimport zipfile\nimport numpy as np\n\nimport torch\n\nfrom src.config import ApplicationConfig, VadInitialPromptMode\nfrom src.hooks.progressListener import ProgressListener\nfrom src.hooks.subTaskProgressListener import SubTaskProgressListener\nfrom src.hooks.whisperProgressHook import create_progress_listener_handle", "from src.hooks.subTaskProgressListener import SubTaskProgressListener\nfrom src.hooks.whisperProgressHook import create_progress_listener_handle\nfrom src.languages import get_language_names\nfrom src.modelCache import ModelCache\nfrom src.source import get_audio_source_collection\nfrom src.vadParallel import ParallelContext, ParallelTranscription\n\n# External programs\nimport ffmpeg\n", "import ffmpeg\n\n# UI\nimport gradio as gr\n\nfrom src.download import ExceededMaximumDuration, download_url\nfrom src.utils import slugify, write_srt, write_vtt\nfrom src.vad import AbstractTranscription, NonSpeechStrategy, PeriodicTranscriptionConfig, TranscriptionConfig, VadPeriodicTranscription, VadSileroTranscription\nfrom src.whisper.abstractWhisperContainer import AbstractWhisperContainer\nfrom src.whisper.whisperFactory import create_whisper_container", "from src.whisper.abstractWhisperContainer import AbstractWhisperContainer\nfrom src.whisper.whisperFactory import create_whisper_container\nfrom src.description import get_description\n# Configure more application defaults in config.json5\n\n# Gradio seems to truncate files without keeping the extension, so we need to truncate the file prefix ourself \nMAX_FILE_PREFIX_LENGTH = 17\n\n# Limit auto_parallel to a certain number of CPUs (specify vad_cpu_cores to get a higher number)\nMAX_AUTO_CPU_CORES = 8", "# Limit auto_parallel to a certain number of CPUs (specify vad_cpu_cores to get a higher number)\nMAX_AUTO_CPU_CORES = 8\n\nWHISPER_MODELS = [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v1\", \"large-v2\"]\n\nclass VadOptions:\n    def __init__(self, vad: str = None, vadMergeWindow: float = 5, vadMaxMergeSize: float = 150, vadPadding: float = 1, vadPromptWindow: float = 1, \n                                        vadInitialPromptMode: Union[VadInitialPromptMode, str] = VadInitialPromptMode.PREPREND_FIRST_SEGMENT):\n        self.vad = vad\n        self.vadMergeWindow = vadMergeWindow\n        self.vadMaxMergeSize = vadMaxMergeSize\n        self.vadPadding = vadPadding\n        self.vadPromptWindow = vadPromptWindow\n        self.vadInitialPromptMode = vadInitialPromptMode if isinstance(vadInitialPromptMode, VadInitialPromptMode) \\\n                                        else VadInitialPromptMode.from_string(vadInitialPromptMode)", "\nclass WhisperTranscriber:\n    def __init__(self, input_audio_max_duration: float = None, vad_process_timeout: float = None, \n                 vad_cpu_cores: int = 1, delete_uploaded_files: bool = False, output_dir: str = None, \n                 app_config: ApplicationConfig = None):\n        self.model_cache = ModelCache()\n        self.parallel_device_list = None\n        self.gpu_parallel_context = None\n        self.cpu_parallel_context = None\n        self.vad_process_timeout = vad_process_timeout\n        self.vad_cpu_cores = vad_cpu_cores\n\n        self.vad_model = None\n        self.inputAudioMaxDuration = input_audio_max_duration\n        self.deleteUploadedFiles = delete_uploaded_files\n        self.output_dir = output_dir\n\n        self.app_config = app_config\n\n    def set_parallel_devices(self, vad_parallel_devices: str):\n        self.parallel_device_list = [ device.strip() for device in vad_parallel_devices.split(\",\") ] if vad_parallel_devices else None\n\n    def set_auto_parallel(self, auto_parallel: bool):\n        if auto_parallel:\n            if torch.cuda.is_available():\n                self.parallel_device_list = [ str(gpu_id) for gpu_id in range(torch.cuda.device_count())]\n\n            self.vad_cpu_cores = min(os.cpu_count(), MAX_AUTO_CPU_CORES)\n            print(\"[Auto parallel] Using GPU devices \" + str(self.parallel_device_list) + \" and \" + str(self.vad_cpu_cores) + \" CPU cores for VAD/transcription.\")\n\n    # Entry function for the simple tab\n    def transcribe_webui_simple(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow):\n        return self.transcribe_webui_simple_progress(modelName, languageName, urlData, multipleFiles, microphoneData, task, vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow)\n    \n    # Entry function for the simple tab progress\n    def transcribe_webui_simple_progress(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, \n                                progress=gr.Progress()):\n        \n        vadOptions = VadOptions(vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, self.app_config.vad_initial_prompt_mode)\n\n        return self.transcribe_webui(modelName, languageName, urlData, multipleFiles, microphoneData, task, vadOptions, progress=progress)\n\n    # Entry function for the full tab\n    def transcribe_webui_full(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, \n                                vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, vadInitialPromptMode, \n                                initial_prompt: str, temperature: float, best_of: int, beam_size: int, patience: float, length_penalty: float, suppress_tokens: str, \n                                condition_on_previous_text: bool, fp16: bool, temperature_increment_on_fallback: float, \n                                compression_ratio_threshold: float, logprob_threshold: float, no_speech_threshold: float):\n        \n        return self.transcribe_webui_full_progress(modelName, languageName, urlData, multipleFiles, microphoneData, task, \n                                vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, vadInitialPromptMode,\n                                initial_prompt, temperature, best_of, beam_size, patience, length_penalty, suppress_tokens,\n                                condition_on_previous_text, fp16, temperature_increment_on_fallback,\n                                compression_ratio_threshold, logprob_threshold, no_speech_threshold)\n\n    # Entry function for the full tab with progress\n    def transcribe_webui_full_progress(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, \n                                    vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, vadInitialPromptMode, \n                                    initial_prompt: str, temperature: float, best_of: int, beam_size: int, patience: float, length_penalty: float, suppress_tokens: str, \n                                    condition_on_previous_text: bool, fp16: bool, temperature_increment_on_fallback: float, \n                                    compression_ratio_threshold: float, logprob_threshold: float, no_speech_threshold: float, \n                                    progress=gr.Progress()):\n\n        # Handle temperature_increment_on_fallback\n        if temperature_increment_on_fallback is not None:\n            temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n        else:\n            temperature = [temperature]\n\n        vadOptions = VadOptions(vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, vadInitialPromptMode)\n\n        return self.transcribe_webui(modelName, languageName, urlData, multipleFiles, microphoneData, task, vadOptions,\n                                     initial_prompt=initial_prompt, temperature=temperature, best_of=best_of, beam_size=beam_size, patience=patience, length_penalty=length_penalty, suppress_tokens=suppress_tokens,\n                                     condition_on_previous_text=condition_on_previous_text, fp16=fp16,\n                                     compression_ratio_threshold=compression_ratio_threshold, logprob_threshold=logprob_threshold, no_speech_threshold=no_speech_threshold, \n                                     progress=progress)\n\n    def transcribe_webui(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, \n                         vadOptions: VadOptions, progress: gr.Progress = None, **decodeOptions: dict):\n        try:\n            sources = self.__get_source(urlData, multipleFiles, microphoneData)\n            \n            try:\n                selectedLanguage = languageName.lower() if len(languageName) > 0 else None\n                selectedModel = modelName if modelName is not None else \"base\"\n\n                model = create_whisper_container(whisper_implementation=self.app_config.whisper_implementation, \n                                                 model_name=selectedModel, compute_type=self.app_config.compute_type, \n                                                 cache=self.model_cache, models=self.app_config.models)\n\n                # Result\n                download = []\n                zip_file_lookup = {}\n                text = \"\"\n                vtt = \"\"\n\n                # Write result\n                downloadDirectory = tempfile.mkdtemp()\n                source_index = 0\n\n                outputDirectory = self.output_dir if self.output_dir is not None else downloadDirectory\n\n                # Progress\n                total_duration = sum([source.get_audio_duration() for source in sources])\n                current_progress = 0\n\n                # A listener that will report progress to Gradio\n                root_progress_listener = self._create_progress_listener(progress)\n\n                # Execute whisper\n                for source in sources:\n                    source_prefix = \"\"\n                    source_audio_duration = source.get_audio_duration()\n\n                    if (len(sources) > 1):\n                        # Prefix (minimum 2 digits)\n                        source_index += 1\n                        source_prefix = str(source_index).zfill(2) + \"_\"\n                        print(\"Transcribing \", source.source_path)\n\n                    scaled_progress_listener = SubTaskProgressListener(root_progress_listener, \n                                                   base_task_total=total_duration,\n                                                   sub_task_start=current_progress,\n                                                   sub_task_total=source_audio_duration)\n\n                    # Transcribe\n                    result = self.transcribe_file(model, source.source_path, selectedLanguage, task, vadOptions, scaled_progress_listener, **decodeOptions)\n                    filePrefix = slugify(source_prefix + source.get_short_name(), allow_unicode=True)\n\n                    # Update progress\n                    current_progress += source_audio_duration\n\n                    source_download, source_text, source_vtt = self.write_result(result, filePrefix, outputDirectory)\n\n                    if len(sources) > 1:\n                        # Add new line separators\n                        if (len(source_text) > 0):\n                            source_text += os.linesep + os.linesep\n                        if (len(source_vtt) > 0):\n                            source_vtt += os.linesep + os.linesep\n\n                        # Append file name to source text too\n                        source_text = source.get_full_name() + \":\" + os.linesep + source_text\n                        source_vtt = source.get_full_name() + \":\" + os.linesep + source_vtt\n\n                    # Add to result\n                    download.extend(source_download)\n                    text += source_text\n                    vtt += source_vtt\n\n                    if (len(sources) > 1):\n                        # Zip files support at least 260 characters, but we'll play it safe and use 200\n                        zipFilePrefix = slugify(source_prefix + source.get_short_name(max_length=200), allow_unicode=True)\n\n                        # File names in ZIP file can be longer\n                        for source_download_file in source_download:\n                            # Get file postfix (after last -)\n                            filePostfix = os.path.basename(source_download_file).split(\"-\")[-1]\n                            zip_file_name = zipFilePrefix + \"-\" + filePostfix\n                            zip_file_lookup[source_download_file] = zip_file_name\n\n                # Create zip file from all sources\n                if len(sources) > 1:\n                    downloadAllPath = os.path.join(downloadDirectory, \"All_Output-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \".zip\")\n\n                    with zipfile.ZipFile(downloadAllPath, 'w', zipfile.ZIP_DEFLATED) as zip:\n                        for download_file in download:\n                            # Get file name from lookup\n                            zip_file_name = zip_file_lookup.get(download_file, os.path.basename(download_file))\n                            zip.write(download_file, arcname=zip_file_name)\n\n                    download.insert(0, downloadAllPath)\n\n                return download, text, vtt\n\n            finally:\n                # Cleanup source\n                if self.deleteUploadedFiles:\n                    for source in sources:\n                        print(\"Deleting source file \" + source.source_path)\n\n                        try:\n                            os.remove(source.source_path)\n                        except Exception as e:\n                            # Ignore error - it's just a cleanup\n                            print(\"Error deleting source file \" + source.source_path + \": \" + str(e))\n        \n        except ExceededMaximumDuration as e:\n            return [], (\"[ERROR]: Maximum remote video length is \" + str(e.maxDuration) + \"s, file was \" + str(e.videoDuration) + \"s\"), \"[ERROR]\"\n\n    def transcribe_file(self, model: AbstractWhisperContainer, audio_path: str, language: str, task: str = None, \n                        vadOptions: VadOptions = VadOptions(), \n                        progressListener: ProgressListener = None, **decodeOptions: dict):\n        \n        initial_prompt = decodeOptions.pop('initial_prompt', None)\n\n        if progressListener is None:\n            # Default progress listener\n            progressListener = ProgressListener()\n\n        if ('task' in decodeOptions):\n            task = decodeOptions.pop('task')\n\n        # Callable for processing an audio file\n        whisperCallable = model.create_callback(language, task, initial_prompt, initial_prompt_mode=vadOptions.vadInitialPromptMode, **decodeOptions)\n\n        # The results\n        if (vadOptions.vad == 'silero-vad'):\n            # Silero VAD where non-speech gaps are transcribed\n            process_gaps = self._create_silero_config(NonSpeechStrategy.CREATE_SEGMENT, vadOptions)\n            result = self.process_vad(audio_path, whisperCallable, self.vad_model, process_gaps, progressListener=progressListener)\n        elif (vadOptions.vad == 'silero-vad-skip-gaps'):\n            # Silero VAD where non-speech gaps are simply ignored\n            skip_gaps = self._create_silero_config(NonSpeechStrategy.SKIP, vadOptions)\n            result = self.process_vad(audio_path, whisperCallable, self.vad_model, skip_gaps, progressListener=progressListener)\n        elif (vadOptions.vad == 'silero-vad-expand-into-gaps'):\n            # Use Silero VAD where speech-segments are expanded into non-speech gaps\n            expand_gaps = self._create_silero_config(NonSpeechStrategy.EXPAND_SEGMENT, vadOptions)\n            result = self.process_vad(audio_path, whisperCallable, self.vad_model, expand_gaps, progressListener=progressListener)\n        elif (vadOptions.vad == 'periodic-vad'):\n            # Very simple VAD - mark every 5 minutes as speech. This makes it less likely that Whisper enters an infinite loop, but\n            # it may create a break in the middle of a sentence, causing some artifacts.\n            periodic_vad = VadPeriodicTranscription()\n            period_config = PeriodicTranscriptionConfig(periodic_duration=vadOptions.vadMaxMergeSize, max_prompt_window=vadOptions.vadPromptWindow)\n            result = self.process_vad(audio_path, whisperCallable, periodic_vad, period_config, progressListener=progressListener)\n\n        else:\n            if (self._has_parallel_devices()):\n                # Use a simple period transcription instead, as we need to use the parallel context\n                periodic_vad = VadPeriodicTranscription()\n                period_config = PeriodicTranscriptionConfig(periodic_duration=math.inf, max_prompt_window=1)\n\n                result = self.process_vad(audio_path, whisperCallable, periodic_vad, period_config, progressListener=progressListener)\n            else:\n                # Default VAD\n                result = whisperCallable.invoke(audio_path, 0, None, None, progress_listener=progressListener)\n\n        return result\n\n    def _create_progress_listener(self, progress: gr.Progress):\n        if (progress is None):\n            # Dummy progress listener\n            return ProgressListener()\n        \n        class ForwardingProgressListener(ProgressListener):\n            def __init__(self, progress: gr.Progress):\n                self.progress = progress\n\n            def on_progress(self, current: Union[int, float], total: Union[int, float]):\n                # From 0 to 1\n                self.progress(current / total)\n\n            def on_finished(self):\n                self.progress(1)\n\n        return ForwardingProgressListener(progress)\n\n    def process_vad(self, audio_path, whisperCallable, vadModel: AbstractTranscription, vadConfig: TranscriptionConfig, \n                    progressListener: ProgressListener = None):\n        if (not self._has_parallel_devices()):\n            # No parallel devices, so just run the VAD and Whisper in sequence\n            return vadModel.transcribe(audio_path, whisperCallable, vadConfig, progressListener=progressListener)\n\n        gpu_devices = self.parallel_device_list\n\n        if (gpu_devices is None or len(gpu_devices) == 0):\n            # No GPU devices specified, pass the current environment variable to the first GPU process. This may be NULL.\n            gpu_devices = [os.environ.get(\"CUDA_VISIBLE_DEVICES\", None)]\n\n        # Create parallel context if needed\n        if (self.gpu_parallel_context is None):\n            # Create a context wih processes and automatically clear the pool after 1 hour of inactivity\n            self.gpu_parallel_context = ParallelContext(num_processes=len(gpu_devices), auto_cleanup_timeout_seconds=self.vad_process_timeout)\n        # We also need a CPU context for the VAD\n        if (self.cpu_parallel_context is None):\n            self.cpu_parallel_context = ParallelContext(num_processes=self.vad_cpu_cores, auto_cleanup_timeout_seconds=self.vad_process_timeout)\n\n        parallel_vad = ParallelTranscription()\n        return parallel_vad.transcribe_parallel(transcription=vadModel, audio=audio_path, whisperCallable=whisperCallable,  \n                                                config=vadConfig, cpu_device_count=self.vad_cpu_cores, gpu_devices=gpu_devices, \n                                                cpu_parallel_context=self.cpu_parallel_context, gpu_parallel_context=self.gpu_parallel_context, \n                                                progress_listener=progressListener) \n\n    def _has_parallel_devices(self):\n        return (self.parallel_device_list is not None and len(self.parallel_device_list) > 0) or self.vad_cpu_cores > 1\n\n    def _concat_prompt(self, prompt1, prompt2):\n        if (prompt1 is None):\n            return prompt2\n        elif (prompt2 is None):\n            return prompt1\n        else:\n            return prompt1 + \" \" + prompt2\n\n    def _create_silero_config(self, non_speech_strategy: NonSpeechStrategy, vadOptions: VadOptions):\n        # Use Silero VAD \n        if (self.vad_model is None):\n            self.vad_model = VadSileroTranscription()\n\n        config = TranscriptionConfig(non_speech_strategy = non_speech_strategy, \n                max_silent_period=vadOptions.vadMergeWindow, max_merge_size=vadOptions.vadMaxMergeSize, \n                segment_padding_left=vadOptions.vadPadding, segment_padding_right=vadOptions.vadPadding, \n                max_prompt_window=vadOptions.vadPromptWindow)\n\n        return config\n\n    def write_result(self, result: dict, source_name: str, output_dir: str):\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        text = result[\"text\"]\n        language = result[\"language\"]\n        languageMaxLineWidth = self.__get_max_line_width(language)\n\n        print(\"Max line width \" + str(languageMaxLineWidth))\n        vtt = self.__get_subs(result[\"segments\"], \"vtt\", languageMaxLineWidth)\n        srt = self.__get_subs(result[\"segments\"], \"srt\", languageMaxLineWidth)\n\n        output_files = []\n        output_files.append(self.__create_file(srt, output_dir, source_name + \"-subs.srt\"));\n        output_files.append(self.__create_file(vtt, output_dir, source_name + \"-subs.vtt\"));\n        output_files.append(self.__create_file(text, output_dir, source_name + \"-transcript.txt\"));\n\n        return output_files, text, vtt\n\n    def clear_cache(self):\n        self.model_cache.clear()\n        self.vad_model = None\n\n    def __get_source(self, urlData, multipleFiles, microphoneData):\n        return get_audio_source_collection(urlData, multipleFiles, microphoneData, self.inputAudioMaxDuration)\n\n    def __get_max_line_width(self, language: str) -> int:\n        if (language and language.lower() in [\"japanese\", \"ja\", \"chinese\", \"zh\"]):\n            # Chinese characters and kana are wider, so limit line length to 40 characters\n            return 40\n        else:\n            # TODO: Add more languages\n            # 80 latin characters should fit on a 1080p/720p screen\n            return 80\n\n    def __get_subs(self, segments: Iterator[dict], format: str, maxLineWidth: int) -> str:\n        segmentStream = StringIO()\n\n        if format == 'vtt':\n            write_vtt(segments, file=segmentStream, maxLineWidth=maxLineWidth)\n        elif format == 'srt':\n            write_srt(segments, file=segmentStream, maxLineWidth=maxLineWidth)\n        else:\n            raise Exception(\"Unknown format \" + format)\n\n        segmentStream.seek(0)\n        return segmentStream.read()\n\n    def __create_file(self, text: str, directory: str, fileName: str) -> str:\n        # Write the text to a file\n        with open(os.path.join(directory, fileName), 'w+', encoding=\"utf-8\") as file:\n            file.write(text)\n\n        return file.name\n\n    def close(self):\n        print(\"Closing parallel contexts\")\n        self.clear_cache()\n\n        if (self.gpu_parallel_context is not None):\n            self.gpu_parallel_context.close()\n        if (self.cpu_parallel_context is not None):\n            self.cpu_parallel_context.close()", "\n\ndef create_ui(app_config: ApplicationConfig):\n    ui = WhisperTranscriber(app_config.input_audio_max_duration, app_config.vad_process_timeout, app_config.vad_cpu_cores, \n                            app_config.delete_uploaded_files, app_config.output_dir, app_config)\n\n    # Specify a list of devices to use for parallel processing\n    ui.set_parallel_devices(app_config.vad_parallel_devices)\n    ui.set_auto_parallel(app_config.auto_parallel)\n\n    is_whisper = False\n\n    if app_config.whisper_implementation == \"whisper\":\n        implementation_name = \"Whisper\"\n        is_whisper = True\n    elif app_config.whisper_implementation in [\"faster-whisper\", \"faster_whisper\"]:\n        implementation_name = \"Faster Whisper\"\n    else:\n        # Try to convert from camel-case to title-case\n        implementation_name = app_config.whisper_implementation.title().replace(\"_\", \" \").replace(\"-\", \" \")\n\n    ui_description = implementation_name + \" is a general-purpose speech recognition model. It is trained on a large dataset of diverse \" \n    ui_description += \" audio and is also a multi-task model that can perform multilingual speech recognition \"\n    ui_description += \" as well as speech translation and language identification. \"\n    ui_description += \"\\n\\n\\n\\n\"+implementation_name+\" \u662f\u4e00\u4e2a\u901a\u7528\u7684\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u3002\u5b83\u662f\u5728\u5927\u91cf\u4e0d\u540c\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u7684\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u53ef\u4ee5\u6267\u884c\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u3001\u8bed\u97f3\u7ffb\u8bd1\u548c\u8bed\u8a00\u8bc6\u522b\u3002\"\n    ui_description += \"\\n\\n\\n\\nFor longer audio files (>10 minutes) not in English, it is recommended that you select Silero VAD (Voice Activity Detector) in the VAD option.\"\n    ui_description += \"\\n\\n\\n\\n\u5bf9\u4e8e\u957f\u5ea6\u8d85\u8fc710\u5206\u949f\u4e14\u975e\u82f1\u8bed\u7684\u97f3\u9891\u6587\u4ef6\uff0c\u5efa\u8bae\u5728\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\u9009\u9879\u4e2d\u9009\u62e9Silero VAD\uff08Voice Activity Detector\uff09\u3002\"\n    # Recommend faster-whisper\n    if is_whisper:\n        ui_description += \"\\n\\n\\n\\nFor faster inference on GPU, try [faster-whisper](https://huggingface.co/spaces/aadnk/faster-whisper-webui).\"\n\n    if app_config.input_audio_max_duration > 0:\n        ui_description += \"\\n\\n\" + \"Max audio file length: \" + str(app_config.input_audio_max_duration) + \" s\"\n        ui_description += \"\\n\\n\" + \"\u97f3\u9891\u6587\u4ef6\u6700\u5927\u957f\u5ea6: \" + str(app_config.input_audio_max_duration) + \" s\"\n\n    ui_article = \"Read the [documentation here](https://gitlab.com/aadnk/whisper-webui/-/blob/main/docs/options.md).\"\n\n    whisper_models = app_config.get_model_names()\n\n    simple_inputs = lambda : [\n        gr.Dropdown(choices=whisper_models, value=app_config.default_model_name, label=\"Model/\u6a21\u578b\"),\n        gr.Dropdown(choices=sorted(get_language_names()), label=\"Language/\u8bed\u8a00(\u4e3a\u7a7a\u65f6\u81ea\u52a8\u8bc6\u522b)\", value=app_config.language),\n        gr.Text(label=\"URL (YouTube, etc.)/\u94fe\u63a5(YouTube,\u5176\u4ed6)\"),\n        gr.File(label=\"Upload Files/\u4e0a\u4f20\u6587\u4ef6\", file_count=\"multiple\"),\n        gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Microphone Input/\u9ea6\u514b\u98ce\u8f93\u5165\"),\n        gr.Dropdown(choices=[\"transcribe\", \"translate\"], label=\"Task/\u4efb\u52a1\", value=app_config.task),\n        gr.Dropdown(choices=[\"none\", \"silero-vad\", \"silero-vad-skip-gaps\", \"silero-vad-expand-into-gaps\", \"periodic-vad\"], value=app_config.default_vad, label=\"VAD/\u8bed\u97f3\u6d3b\u6027\u68c0\u6d4b\"),\n        gr.Number(label=\"VAD - Merge Window (s)/VAD - \u5408\u5e76\u7a97\u53e3\uff08\u79d2\uff09\", precision=0, value=app_config.vad_merge_window),\n        gr.Number(label=\"VAD - Max Merge Size (s)/VAD - \u6700\u5927\u5408\u5e76\u5927\u5c0f\uff08\u79d2\uff09\", precision=0, value=app_config.vad_max_merge_size),\n        gr.Number(label=\"VAD - Padding (s)/VAD - \u586b\u5145\uff08\u79d2\uff09\", precision=None, value=app_config.vad_padding),\n        gr.Number(label=\"VAD - Prompt Window (s)/VAD - \u63d0\u793a\u7a97\u53e3\uff08\u79d2\uff09\", precision=None, value=app_config.vad_prompt_window),\n    ]\n\n    is_queue_mode = app_config.queue_concurrency_count is not None and app_config.queue_concurrency_count > 0    \n\n    simple_transcribe = gr.Interface(fn=ui.transcribe_webui_simple_progress if is_queue_mode else ui.transcribe_webui_simple, \n                                     description=ui_description, article=ui_article, inputs=simple_inputs(), outputs=[\n        gr.File(label=\"Download/\u4e0b\u8f7d\"),\n        gr.Text(label=\"Transcription/\u8f6c\u5f55\"), \n        gr.Text(label=\"Segments/\u5206\u6bb5\")\n    ])\n\n    full_description = ui_description + \"\\n\\n\\n\\n\" + \"Be careful when changing some of the options in the full interface - this can cause the model to crash.\"\n    full_description = full_description + \"\\n\\n\\n\\n\" + \"<font style='color:red'>\u5728\u5b8c\u6574\u754c\u9762\u4e2d\u66f4\u6539\u67d0\u4e9b\u9009\u9879\u65f6\u8981\u5c0f\u5fc3\uff0c\u5426\u5219\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6a21\u578b\u5d29\u6e83\u3002</font>\"\n    full_transcribe = gr.Interface(fn=ui.transcribe_webui_full_progress if is_queue_mode else ui.transcribe_webui_full,\n                                   description=full_description, article=ui_article, inputs=[\n        *simple_inputs(),\n        gr.Dropdown(choices=[\"prepend_first_segment\", \"prepend_all_segments\"], value=app_config.vad_initial_prompt_mode, label=\"VAD - Initial Prompt Mode/VAD - \u521d\u59cb\u63d0\u793a\u6a21\u5f0f\"),\n        gr.TextArea(label=\"Initial Prompt/\u521d\u59cb\u63d0\u793a\"),\n        gr.Number(label=\"Temperature/\u6e29\u5ea6\", value=app_config.temperature),\n        gr.Number(label=\"Best Of - Non-zero temperature/\u6700\u4f73\u7ed3\u679c - \u975e\u96f6\u6e29\u5ea6\", value=app_config.best_of, precision=0),\n        gr.Number(label=\"Beam Size - Zero temperature/\u675f\u641c\u7d22\u5927\u5c0f - \u96f6\u6e29\u5ea6\", value=app_config.beam_size, precision=0),\n        gr.Number(label=\"Patience - Zero temperature/\u8010\u5fc3 - \u96f6\u6e29\u5ea6\", value=app_config.patience),\n        gr.Number(label=\"Length Penalty - Any temperature/\u957f\u5ea6\u60e9\u7f5a - \u4efb\u4f55\u6e29\u5ea6\", value=app_config.length_penalty), \n        gr.Text(label=\"Suppress Tokens - Comma-separated list of token IDs/\u6291\u5236\u6807\u8bb0 - \u9017\u53f7\u5206\u9694\u7684\u6807\u8bb0ID\u5217\u8868\", value=app_config.suppress_tokens),\n        gr.Checkbox(label=\"Condition on previous text/\u4ee5\u524d\u6587\u4e3a\u6761\u4ef6\", value=app_config.condition_on_previous_text),\n        gr.Checkbox(label=\"FP16\", value=app_config.fp16),\n        gr.Number(label=\"Temperature increment on fallback/\u56de\u9000\u65f6\u6e29\u5ea6\u589e\u91cf\", value=app_config.temperature_increment_on_fallback),\n        gr.Number(label=\"Compression ratio threshold/\u538b\u7f29\u6bd4\u9608\u503c\", value=app_config.compression_ratio_threshold),\n        gr.Number(label=\"Logprob threshold/Logprob\u9608\u503c\", value=app_config.logprob_threshold),\n        gr.Number(label=\"No speech threshold/\u65e0\u8bed\u97f3\u9608\u503c\", value=app_config.no_speech_threshold)\n    ], outputs=[\n        gr.File(label=\"Download/\u4e0b\u8f7d\"),\n        gr.Text(label=\"Transcription/\u8f6c\u5f55\"), \n        gr.Text(label=\"Segments/\u5206\u6bb5\")\n    ])\n    document = gr.Markdown(\n        get_description\n    )\n    demo = gr.TabbedInterface([simple_transcribe, full_transcribe,document], tab_names=[\"Simple/\u57fa\u7840\u7248\", \"Full/\u5b8c\u6574\u7248\",\"document(\u8bf4\u660e\u6587\u6863)\"])\n\n    # Queue up the demo\n    if is_queue_mode:\n        demo.queue(concurrency_count=app_config.queue_concurrency_count)\n        print(\"Queue mode enabled (concurrency count: \" + str(app_config.queue_concurrency_count) + \")\")\n    else:\n        print(\"Queue mode disabled - progress bars will not be shown.\")\n   \n    demo.launch(share=app_config.share, server_name=app_config.server_name, server_port=app_config.server_port,quiet=True)\n    \n    # Clean up\n    ui.close()", "\nif __name__ == '__main__':\n    default_app_config = ApplicationConfig.create_default()\n    whisper_models = default_app_config.get_model_names()\n\n    # Environment variable overrides\n    default_whisper_implementation = os.environ.get(\"WHISPER_IMPLEMENTATION\", default_app_config.whisper_implementation)\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"--input_audio_max_duration\", type=int, default=default_app_config.input_audio_max_duration, \\\n                        help=\"Maximum audio file length in seconds, or -1 for no limit.\") # 600\n    parser.add_argument(\"--share\", type=bool, default=default_app_config.share, \\\n                        help=\"True to share the app on HuggingFace.\") # False\n    parser.add_argument(\"--server_name\", type=str, default=default_app_config.server_name, \\\n                        help=\"The host or IP to bind to. If None, bind to localhost.\") # None\n    parser.add_argument(\"--server_port\", type=int, default=default_app_config.server_port, \\\n                        help=\"The port to bind to.\") # 7860\n    parser.add_argument(\"--queue_concurrency_count\", type=int, default=default_app_config.queue_concurrency_count, \\\n                        help=\"The number of concurrent requests to process.\") # 1\n    parser.add_argument(\"--default_model_name\", type=str, choices=whisper_models, default=default_app_config.default_model_name, \\\n                        help=\"The default model name.\") # medium\n    parser.add_argument(\"--default_vad\", type=str, default=default_app_config.default_vad, \\\n                        help=\"The default VAD.\") # silero-vad\n    parser.add_argument(\"--vad_initial_prompt_mode\", type=str, default=default_app_config.vad_initial_prompt_mode, choices=[\"prepend_all_segments\", \"prepend_first_segment\"], \\\n                        help=\"Whether or not to prepend the initial prompt to each VAD segment (prepend_all_segments), or just the first segment (prepend_first_segment)\") # prepend_first_segment\n    parser.add_argument(\"--vad_parallel_devices\", type=str, default=default_app_config.vad_parallel_devices, \\\n                        help=\"A commma delimited list of CUDA devices to use for parallel processing. If None, disable parallel processing.\") # \"\"\n    parser.add_argument(\"--vad_cpu_cores\", type=int, default=default_app_config.vad_cpu_cores, \\\n                        help=\"The number of CPU cores to use for VAD pre-processing.\") # 1\n    parser.add_argument(\"--vad_process_timeout\", type=float, default=default_app_config.vad_process_timeout, \\\n                        help=\"The number of seconds before inactivate processes are terminated. Use 0 to close processes immediately, or None for no timeout.\") # 1800\n    parser.add_argument(\"--auto_parallel\", type=bool, default=default_app_config.auto_parallel, \\\n                        help=\"True to use all available GPUs and CPU cores for processing. Use vad_cpu_cores/vad_parallel_devices to specify the number of CPU cores/GPUs to use.\") # False\n    parser.add_argument(\"--output_dir\", \"-o\", type=str, default=default_app_config.output_dir, \\\n                        help=\"directory to save the outputs\")\n    parser.add_argument(\"--whisper_implementation\", type=str, default=default_whisper_implementation, choices=[\"whisper\", \"faster-whisper\"],\\\n                        help=\"the Whisper implementation to use\")\n    parser.add_argument(\"--compute_type\", type=str, default=default_app_config.compute_type, choices=[\"default\", \"auto\", \"int8\", \"int8_float16\", \"int16\", \"float16\", \"float32\"], \\\n                        help=\"the compute type to use for inference\")\n\n    args = parser.parse_args().__dict__\n\n    updated_config = default_app_config.update(**args)\n\n    create_ui(app_config=updated_config)"]}
{"filename": "app-network.py", "chunked_list": ["# Run the app with no audio file restrictions, and make it available on the network\nfrom app import create_ui\nfrom src.config import ApplicationConfig\n\ncreate_ui(ApplicationConfig.create_default(input_audio_max_duration=-1, server_name=\"0.0.0.0\"))"]}
{"filename": "app-local.py", "chunked_list": ["# Run the app with no audio file restrictions\nfrom app import create_ui\nfrom src.config import ApplicationConfig\n\ncreate_ui(ApplicationConfig.create_default(input_audio_max_duration=-1))"]}
{"filename": "cli.py", "chunked_list": ["import argparse\nimport os\nimport pathlib\nfrom urllib.parse import urlparse\nimport warnings\nimport numpy as np\n\nimport torch\nfrom app import VadOptions, WhisperTranscriber\nfrom src.config import ApplicationConfig, VadInitialPromptMode", "from app import VadOptions, WhisperTranscriber\nfrom src.config import ApplicationConfig, VadInitialPromptMode\nfrom src.download import download_url\nfrom src.languages import get_language_names\n\nfrom src.utils import optional_float, optional_int, str2bool\nfrom src.whisper.whisperFactory import create_whisper_container\n\ndef cli():\n    app_config = ApplicationConfig.create_default()\n    whisper_models = app_config.get_model_names()\n\n    # For the CLI, we fallback to saving the output to the current directory\n    output_dir = app_config.output_dir if app_config.output_dir is not None else \".\"\n\n    # Environment variable overrides\n    default_whisper_implementation = os.environ.get(\"WHISPER_IMPLEMENTATION\", app_config.whisper_implementation)\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"audio\", nargs=\"+\", type=str, \\\n                        help=\"audio file(s) to transcribe\")\n    parser.add_argument(\"--model\", default=app_config.default_model_name, choices=whisper_models, \\\n                        help=\"name of the Whisper model to use\") # medium\n    parser.add_argument(\"--model_dir\", type=str, default=app_config.model_dir, \\\n                        help=\"the path to save model files; uses ~/.cache/whisper by default\")\n    parser.add_argument(\"--device\", default=app_config.device, \\\n                        help=\"device to use for PyTorch inference\")\n    parser.add_argument(\"--output_dir\", \"-o\", type=str, default=output_dir, \\\n                        help=\"directory to save the outputs\")\n    parser.add_argument(\"--verbose\", type=str2bool, default=app_config.verbose, \\\n                        help=\"whether to print out the progress and debug messages\")\n    parser.add_argument(\"--whisper_implementation\", type=str, default=default_whisper_implementation, choices=[\"whisper\", \"faster-whisper\"],\\\n                        help=\"the Whisper implementation to use\")\n                        \n    parser.add_argument(\"--task\", type=str, default=app_config.task, choices=[\"transcribe\", \"translate\"], \\\n                        help=\"whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')\")\n    parser.add_argument(\"--language\", type=str, default=app_config.language, choices=sorted(get_language_names()), \\\n                        help=\"language spoken in the audio, specify None to perform language detection\")\n\n    parser.add_argument(\"--vad\", type=str, default=app_config.default_vad, choices=[\"none\", \"silero-vad\", \"silero-vad-skip-gaps\", \"silero-vad-expand-into-gaps\", \"periodic-vad\"], \\\n                        help=\"The voice activity detection algorithm to use\") # silero-vad\n    parser.add_argument(\"--vad_initial_prompt_mode\", type=str, default=app_config.vad_initial_prompt_mode, choices=[\"prepend_all_segments\", \"prepend_first_segment\"], \\\n                        help=\"Whether or not to prepend the initial prompt to each VAD segment (prepend_all_segments), or just the first segment (prepend_first_segment)\") # prepend_first_segment\n    parser.add_argument(\"--vad_merge_window\", type=optional_float, default=app_config.vad_merge_window, \\\n                        help=\"The window size (in seconds) to merge voice segments\")\n    parser.add_argument(\"--vad_max_merge_size\", type=optional_float, default=app_config.vad_max_merge_size,\\\n                         help=\"The maximum size (in seconds) of a voice segment\")\n    parser.add_argument(\"--vad_padding\", type=optional_float, default=app_config.vad_padding, \\\n                        help=\"The padding (in seconds) to add to each voice segment\")\n    parser.add_argument(\"--vad_prompt_window\", type=optional_float, default=app_config.vad_prompt_window, \\\n                        help=\"The window size of the prompt to pass to Whisper\")\n    parser.add_argument(\"--vad_cpu_cores\", type=int, default=app_config.vad_cpu_cores, \\\n                        help=\"The number of CPU cores to use for VAD pre-processing.\") # 1\n    parser.add_argument(\"--vad_parallel_devices\", type=str, default=app_config.vad_parallel_devices, \\\n                        help=\"A commma delimited list of CUDA devices to use for parallel processing. If None, disable parallel processing.\") # \"\"\n    parser.add_argument(\"--auto_parallel\", type=bool, default=app_config.auto_parallel, \\\n                        help=\"True to use all available GPUs and CPU cores for processing. Use vad_cpu_cores/vad_parallel_devices to specify the number of CPU cores/GPUs to use.\") # False\n\n    parser.add_argument(\"--temperature\", type=float, default=app_config.temperature, \\\n                        help=\"temperature to use for sampling\")\n    parser.add_argument(\"--best_of\", type=optional_int, default=app_config.best_of, \\\n                        help=\"number of candidates when sampling with non-zero temperature\")\n    parser.add_argument(\"--beam_size\", type=optional_int, default=app_config.beam_size, \\\n                        help=\"number of beams in beam search, only applicable when temperature is zero\")\n    parser.add_argument(\"--patience\", type=float, default=app_config.patience, \\\n                        help=\"optional patience value to use in beam decoding, as in https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to conventional beam search\")\n    parser.add_argument(\"--length_penalty\", type=float, default=app_config.length_penalty, \\\n                        help=\"optional token length penalty coefficient (alpha) as in https://arxiv.org/abs/1609.08144, uses simple lengt normalization by default\")\n\n    parser.add_argument(\"--suppress_tokens\", type=str, default=app_config.suppress_tokens, \\\n                        help=\"comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations\")\n    parser.add_argument(\"--initial_prompt\", type=str, default=app_config.initial_prompt, \\\n                        help=\"optional text to provide as a prompt for the first window.\")\n    parser.add_argument(\"--condition_on_previous_text\", type=str2bool, default=app_config.condition_on_previous_text, \\\n                        help=\"if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop\")\n    parser.add_argument(\"--fp16\", type=str2bool, default=app_config.fp16, \\\n                        help=\"whether to perform inference in fp16; True by default\")\n    parser.add_argument(\"--compute_type\", type=str, default=app_config.compute_type, choices=[\"default\", \"auto\", \"int8\", \"int8_float16\", \"int16\", \"float16\", \"float32\"], \\\n                        help=\"the compute type to use for inference\")\n\n    parser.add_argument(\"--temperature_increment_on_fallback\", type=optional_float, default=app_config.temperature_increment_on_fallback, \\\n                        help=\"temperature to increase when falling back when the decoding fails to meet either of the thresholds below\")\n    parser.add_argument(\"--compression_ratio_threshold\", type=optional_float, default=app_config.compression_ratio_threshold, \\\n                        help=\"if the gzip compression ratio is higher than this value, treat the decoding as failed\")\n    parser.add_argument(\"--logprob_threshold\", type=optional_float, default=app_config.logprob_threshold, \\\n                        help=\"if the average log probability is lower than this value, treat the decoding as failed\")\n    parser.add_argument(\"--no_speech_threshold\", type=optional_float, default=app_config.no_speech_threshold, \\\n                        help=\"if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence\")\n\n    args = parser.parse_args().__dict__\n    model_name: str = args.pop(\"model\")\n    model_dir: str = args.pop(\"model_dir\")\n    output_dir: str = args.pop(\"output_dir\")\n    device: str = args.pop(\"device\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    whisper_implementation = args.pop(\"whisper_implementation\")\n    print(f\"Using {whisper_implementation} for Whisper\")\n\n    if model_name.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n        warnings.warn(f\"{model_name} is an English-only model but receipted '{args['language']}'; using English instead.\")\n        args[\"language\"] = \"en\"\n\n    temperature = args.pop(\"temperature\")\n    temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n    if temperature_increment_on_fallback is not None:\n        temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n    else:\n        temperature = [temperature]\n\n    vad = args.pop(\"vad\")\n    vad_initial_prompt_mode = args.pop(\"vad_initial_prompt_mode\")\n    vad_merge_window = args.pop(\"vad_merge_window\")\n    vad_max_merge_size = args.pop(\"vad_max_merge_size\")\n    vad_padding = args.pop(\"vad_padding\")\n    vad_prompt_window = args.pop(\"vad_prompt_window\")\n    vad_cpu_cores = args.pop(\"vad_cpu_cores\")\n    auto_parallel = args.pop(\"auto_parallel\")\n    \n    compute_type = args.pop(\"compute_type\")\n\n    transcriber = WhisperTranscriber(delete_uploaded_files=False, vad_cpu_cores=vad_cpu_cores, app_config=app_config)\n    transcriber.set_parallel_devices(args.pop(\"vad_parallel_devices\"))\n    transcriber.set_auto_parallel(auto_parallel)\n\n    model = create_whisper_container(whisper_implementation=whisper_implementation, model_name=model_name, \n                                     device=device, compute_type=compute_type, download_root=model_dir, models=app_config.models)\n\n    if (transcriber._has_parallel_devices()):\n        print(\"Using parallel devices:\", transcriber.parallel_device_list)\n\n    for audio_path in args.pop(\"audio\"):\n        sources = []\n\n        # Detect URL and download the audio\n        if (uri_validator(audio_path)):\n            # Download from YouTube/URL directly\n            for source_path in  download_url(audio_path, maxDuration=-1, destinationDirectory=output_dir, playlistItems=None):\n                source_name = os.path.basename(source_path)\n                sources.append({ \"path\": source_path, \"name\": source_name })\n        else:\n            sources.append({ \"path\": audio_path, \"name\": os.path.basename(audio_path) })\n\n        for source in sources:\n            source_path = source[\"path\"]\n            source_name = source[\"name\"]\n\n            vadOptions = VadOptions(vad, vad_merge_window, vad_max_merge_size, vad_padding, vad_prompt_window, \n                                    VadInitialPromptMode.from_string(vad_initial_prompt_mode))\n\n            result = transcriber.transcribe_file(model, source_path, temperature=temperature, vadOptions=vadOptions, **args)\n            \n            transcriber.write_result(result, source_name, output_dir)\n\n    transcriber.close()", "def cli():\n    app_config = ApplicationConfig.create_default()\n    whisper_models = app_config.get_model_names()\n\n    # For the CLI, we fallback to saving the output to the current directory\n    output_dir = app_config.output_dir if app_config.output_dir is not None else \".\"\n\n    # Environment variable overrides\n    default_whisper_implementation = os.environ.get(\"WHISPER_IMPLEMENTATION\", app_config.whisper_implementation)\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"audio\", nargs=\"+\", type=str, \\\n                        help=\"audio file(s) to transcribe\")\n    parser.add_argument(\"--model\", default=app_config.default_model_name, choices=whisper_models, \\\n                        help=\"name of the Whisper model to use\") # medium\n    parser.add_argument(\"--model_dir\", type=str, default=app_config.model_dir, \\\n                        help=\"the path to save model files; uses ~/.cache/whisper by default\")\n    parser.add_argument(\"--device\", default=app_config.device, \\\n                        help=\"device to use for PyTorch inference\")\n    parser.add_argument(\"--output_dir\", \"-o\", type=str, default=output_dir, \\\n                        help=\"directory to save the outputs\")\n    parser.add_argument(\"--verbose\", type=str2bool, default=app_config.verbose, \\\n                        help=\"whether to print out the progress and debug messages\")\n    parser.add_argument(\"--whisper_implementation\", type=str, default=default_whisper_implementation, choices=[\"whisper\", \"faster-whisper\"],\\\n                        help=\"the Whisper implementation to use\")\n                        \n    parser.add_argument(\"--task\", type=str, default=app_config.task, choices=[\"transcribe\", \"translate\"], \\\n                        help=\"whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')\")\n    parser.add_argument(\"--language\", type=str, default=app_config.language, choices=sorted(get_language_names()), \\\n                        help=\"language spoken in the audio, specify None to perform language detection\")\n\n    parser.add_argument(\"--vad\", type=str, default=app_config.default_vad, choices=[\"none\", \"silero-vad\", \"silero-vad-skip-gaps\", \"silero-vad-expand-into-gaps\", \"periodic-vad\"], \\\n                        help=\"The voice activity detection algorithm to use\") # silero-vad\n    parser.add_argument(\"--vad_initial_prompt_mode\", type=str, default=app_config.vad_initial_prompt_mode, choices=[\"prepend_all_segments\", \"prepend_first_segment\"], \\\n                        help=\"Whether or not to prepend the initial prompt to each VAD segment (prepend_all_segments), or just the first segment (prepend_first_segment)\") # prepend_first_segment\n    parser.add_argument(\"--vad_merge_window\", type=optional_float, default=app_config.vad_merge_window, \\\n                        help=\"The window size (in seconds) to merge voice segments\")\n    parser.add_argument(\"--vad_max_merge_size\", type=optional_float, default=app_config.vad_max_merge_size,\\\n                         help=\"The maximum size (in seconds) of a voice segment\")\n    parser.add_argument(\"--vad_padding\", type=optional_float, default=app_config.vad_padding, \\\n                        help=\"The padding (in seconds) to add to each voice segment\")\n    parser.add_argument(\"--vad_prompt_window\", type=optional_float, default=app_config.vad_prompt_window, \\\n                        help=\"The window size of the prompt to pass to Whisper\")\n    parser.add_argument(\"--vad_cpu_cores\", type=int, default=app_config.vad_cpu_cores, \\\n                        help=\"The number of CPU cores to use for VAD pre-processing.\") # 1\n    parser.add_argument(\"--vad_parallel_devices\", type=str, default=app_config.vad_parallel_devices, \\\n                        help=\"A commma delimited list of CUDA devices to use for parallel processing. If None, disable parallel processing.\") # \"\"\n    parser.add_argument(\"--auto_parallel\", type=bool, default=app_config.auto_parallel, \\\n                        help=\"True to use all available GPUs and CPU cores for processing. Use vad_cpu_cores/vad_parallel_devices to specify the number of CPU cores/GPUs to use.\") # False\n\n    parser.add_argument(\"--temperature\", type=float, default=app_config.temperature, \\\n                        help=\"temperature to use for sampling\")\n    parser.add_argument(\"--best_of\", type=optional_int, default=app_config.best_of, \\\n                        help=\"number of candidates when sampling with non-zero temperature\")\n    parser.add_argument(\"--beam_size\", type=optional_int, default=app_config.beam_size, \\\n                        help=\"number of beams in beam search, only applicable when temperature is zero\")\n    parser.add_argument(\"--patience\", type=float, default=app_config.patience, \\\n                        help=\"optional patience value to use in beam decoding, as in https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to conventional beam search\")\n    parser.add_argument(\"--length_penalty\", type=float, default=app_config.length_penalty, \\\n                        help=\"optional token length penalty coefficient (alpha) as in https://arxiv.org/abs/1609.08144, uses simple lengt normalization by default\")\n\n    parser.add_argument(\"--suppress_tokens\", type=str, default=app_config.suppress_tokens, \\\n                        help=\"comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations\")\n    parser.add_argument(\"--initial_prompt\", type=str, default=app_config.initial_prompt, \\\n                        help=\"optional text to provide as a prompt for the first window.\")\n    parser.add_argument(\"--condition_on_previous_text\", type=str2bool, default=app_config.condition_on_previous_text, \\\n                        help=\"if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop\")\n    parser.add_argument(\"--fp16\", type=str2bool, default=app_config.fp16, \\\n                        help=\"whether to perform inference in fp16; True by default\")\n    parser.add_argument(\"--compute_type\", type=str, default=app_config.compute_type, choices=[\"default\", \"auto\", \"int8\", \"int8_float16\", \"int16\", \"float16\", \"float32\"], \\\n                        help=\"the compute type to use for inference\")\n\n    parser.add_argument(\"--temperature_increment_on_fallback\", type=optional_float, default=app_config.temperature_increment_on_fallback, \\\n                        help=\"temperature to increase when falling back when the decoding fails to meet either of the thresholds below\")\n    parser.add_argument(\"--compression_ratio_threshold\", type=optional_float, default=app_config.compression_ratio_threshold, \\\n                        help=\"if the gzip compression ratio is higher than this value, treat the decoding as failed\")\n    parser.add_argument(\"--logprob_threshold\", type=optional_float, default=app_config.logprob_threshold, \\\n                        help=\"if the average log probability is lower than this value, treat the decoding as failed\")\n    parser.add_argument(\"--no_speech_threshold\", type=optional_float, default=app_config.no_speech_threshold, \\\n                        help=\"if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence\")\n\n    args = parser.parse_args().__dict__\n    model_name: str = args.pop(\"model\")\n    model_dir: str = args.pop(\"model_dir\")\n    output_dir: str = args.pop(\"output_dir\")\n    device: str = args.pop(\"device\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    whisper_implementation = args.pop(\"whisper_implementation\")\n    print(f\"Using {whisper_implementation} for Whisper\")\n\n    if model_name.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n        warnings.warn(f\"{model_name} is an English-only model but receipted '{args['language']}'; using English instead.\")\n        args[\"language\"] = \"en\"\n\n    temperature = args.pop(\"temperature\")\n    temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n    if temperature_increment_on_fallback is not None:\n        temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n    else:\n        temperature = [temperature]\n\n    vad = args.pop(\"vad\")\n    vad_initial_prompt_mode = args.pop(\"vad_initial_prompt_mode\")\n    vad_merge_window = args.pop(\"vad_merge_window\")\n    vad_max_merge_size = args.pop(\"vad_max_merge_size\")\n    vad_padding = args.pop(\"vad_padding\")\n    vad_prompt_window = args.pop(\"vad_prompt_window\")\n    vad_cpu_cores = args.pop(\"vad_cpu_cores\")\n    auto_parallel = args.pop(\"auto_parallel\")\n    \n    compute_type = args.pop(\"compute_type\")\n\n    transcriber = WhisperTranscriber(delete_uploaded_files=False, vad_cpu_cores=vad_cpu_cores, app_config=app_config)\n    transcriber.set_parallel_devices(args.pop(\"vad_parallel_devices\"))\n    transcriber.set_auto_parallel(auto_parallel)\n\n    model = create_whisper_container(whisper_implementation=whisper_implementation, model_name=model_name, \n                                     device=device, compute_type=compute_type, download_root=model_dir, models=app_config.models)\n\n    if (transcriber._has_parallel_devices()):\n        print(\"Using parallel devices:\", transcriber.parallel_device_list)\n\n    for audio_path in args.pop(\"audio\"):\n        sources = []\n\n        # Detect URL and download the audio\n        if (uri_validator(audio_path)):\n            # Download from YouTube/URL directly\n            for source_path in  download_url(audio_path, maxDuration=-1, destinationDirectory=output_dir, playlistItems=None):\n                source_name = os.path.basename(source_path)\n                sources.append({ \"path\": source_path, \"name\": source_name })\n        else:\n            sources.append({ \"path\": audio_path, \"name\": os.path.basename(audio_path) })\n\n        for source in sources:\n            source_path = source[\"path\"]\n            source_name = source[\"name\"]\n\n            vadOptions = VadOptions(vad, vad_merge_window, vad_max_merge_size, vad_padding, vad_prompt_window, \n                                    VadInitialPromptMode.from_string(vad_initial_prompt_mode))\n\n            result = transcriber.transcribe_file(model, source_path, temperature=temperature, vadOptions=vadOptions, **args)\n            \n            transcriber.write_result(result, source_name, output_dir)\n\n    transcriber.close()", "\ndef uri_validator(x):\n    try:\n        result = urlparse(x)\n        return all([result.scheme, result.netloc])\n    except:\n        return False\n\nif __name__ == '__main__':\n    cli()", "if __name__ == '__main__':\n    cli()"]}
{"filename": "tests/vad_test.py", "chunked_list": ["import pprint\nimport unittest\nimport numpy as np\nimport sys\n\nsys.path.append('../whisper-webui')\n\nfrom src.vad import AbstractTranscription, TranscriptionConfig, VadSileroTranscription\n\nclass TestVad(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super(TestVad, self).__init__(*args, **kwargs)\n        self.transcribe_calls = []\n\n    def test_transcript(self):\n        mock = MockVadTranscription()\n\n        self.transcribe_calls.clear()\n        result = mock.transcribe(\"mock\", lambda segment : self.transcribe_segments(segment))\n\n        self.assertListEqual(self.transcribe_calls, [ \n            [30, 30],\n            [100, 100]\n        ])\n\n        self.assertListEqual(result['segments'],\n            [{'end': 50.0, 'start': 40.0, 'text': 'Hello world '},\n            {'end': 120.0, 'start': 110.0, 'text': 'Hello world '}]\n        )\n\n    def transcribe_segments(self, segment):\n        self.transcribe_calls.append(segment.tolist())\n\n        # Dummy text\n        return {\n            'text': \"Hello world \",\n            'segments': [\n                {\n                    \"start\": 10.0,\n                    \"end\": 20.0,\n                    \"text\": \"Hello world \"\n                }   \n            ],\n            'language': \"\"\n        }", "\nclass TestVad(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super(TestVad, self).__init__(*args, **kwargs)\n        self.transcribe_calls = []\n\n    def test_transcript(self):\n        mock = MockVadTranscription()\n\n        self.transcribe_calls.clear()\n        result = mock.transcribe(\"mock\", lambda segment : self.transcribe_segments(segment))\n\n        self.assertListEqual(self.transcribe_calls, [ \n            [30, 30],\n            [100, 100]\n        ])\n\n        self.assertListEqual(result['segments'],\n            [{'end': 50.0, 'start': 40.0, 'text': 'Hello world '},\n            {'end': 120.0, 'start': 110.0, 'text': 'Hello world '}]\n        )\n\n    def transcribe_segments(self, segment):\n        self.transcribe_calls.append(segment.tolist())\n\n        # Dummy text\n        return {\n            'text': \"Hello world \",\n            'segments': [\n                {\n                    \"start\": 10.0,\n                    \"end\": 20.0,\n                    \"text\": \"Hello world \"\n                }   \n            ],\n            'language': \"\"\n        }", "\nclass MockVadTranscription(AbstractTranscription):\n    def __init__(self):\n        super().__init__()\n\n    def get_audio_segment(self, str, start_time: str = None, duration: str = None):\n        start_time_seconds = float(start_time.removesuffix(\"s\"))\n        duration_seconds = float(duration.removesuffix(\"s\"))\n\n        # For mocking, this just returns a simple numppy array\n        return np.array([start_time_seconds, duration_seconds], dtype=np.float64)\n\n    def get_transcribe_timestamps(self, audio: str, config: TranscriptionConfig, start_time: float, duration: float):\n        result = []\n\n        result.append( {  'start': 30, 'end': 60 } )\n        result.append( {  'start': 100, 'end': 200 } )\n        return result", "\nif __name__ == '__main__':\n    unittest.main()"]}
{"filename": "tests/segments_test.py", "chunked_list": ["import sys\nimport unittest\n\nsys.path.append('../whisper-webui')\n\nfrom src.segments import merge_timestamps\n\nclass TestSegments(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super(TestSegments, self).__init__(*args, **kwargs)\n        \n    def test_merge_segments(self):\n        segments = [\n            {'start': 10.0, 'end': 20.0},\n            {'start': 22.0, 'end': 27.0},\n            {'start': 31.0, 'end': 35.0},\n            {'start': 45.0, 'end': 60.0},\n            {'start': 61.0, 'end': 65.0},\n            {'start': 68.0, 'end': 98.0},\n            {'start': 100.0, 'end': 102.0},\n            {'start': 110.0, 'end': 112.0}\n        ]\n\n        result = merge_timestamps(segments, merge_window=5, max_merge_size=30, padding_left=1, padding_right=1)\n\n        self.assertListEqual(result, [\n            {'start': 9.0, 'end': 36.0},\n            {'start': 44.0, 'end': 66.0},\n            {'start': 67.0, 'end': 99.0},\n            {'start': 99.0, 'end': 103.0},\n            {'start': 109.0, 'end': 113.0}\n        ])\n\n    def test_overlap_next(self):\n        segments = [\n            {'start': 5.0, 'end': 39.182},\n            {'start': 39.986, 'end': 40.814}\n        ]\n\n        result = merge_timestamps(segments, merge_window=5, max_merge_size=30, padding_left=1, padding_right=1)\n\n        self.assertListEqual(result, [\n            {'start': 4.0, 'end': 39.584},\n            {'start': 39.584, 'end': 41.814}\n        ])", "\nif __name__ == '__main__':\n    unittest.main()"]}
{"filename": "src/vad.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom collections import Counter, deque\nimport time\n\nfrom typing import Any, Deque, Iterator, List, Dict\n\nfrom pprint import pprint\nfrom src.hooks.progressListener import ProgressListener\nfrom src.hooks.subTaskProgressListener import SubTaskProgressListener\nfrom src.hooks.whisperProgressHook import create_progress_listener_handle", "from src.hooks.subTaskProgressListener import SubTaskProgressListener\nfrom src.hooks.whisperProgressHook import create_progress_listener_handle\nfrom src.modelCache import GLOBAL_MODEL_CACHE, ModelCache\n\nfrom src.segments import merge_timestamps\nfrom src.whisper.abstractWhisperContainer import AbstractWhisperCallback\n\n# Workaround for https://github.com/tensorflow/tensorflow/issues/48797\ntry:\n    import tensorflow as tf\nexcept ModuleNotFoundError:\n    # Error handling\n    pass", "try:\n    import tensorflow as tf\nexcept ModuleNotFoundError:\n    # Error handling\n    pass\n\nimport torch\n\nimport ffmpeg\nimport numpy as np", "import ffmpeg\nimport numpy as np\n\nfrom src.utils import format_timestamp\nfrom enum import Enum\n\nclass NonSpeechStrategy(Enum):\n    \"\"\"\n    Ignore non-speech frames segments.\n    \"\"\"\n    SKIP = 1\n    \"\"\"\n    Just treat non-speech segments as speech.\n    \"\"\"\n    CREATE_SEGMENT = 2\n    \"\"\"\n    Expand speech segments into subsequent non-speech segments.\n    \"\"\"\n    EXPAND_SEGMENT = 3", "\n# Defaults for Silero\nSPEECH_TRESHOLD = 0.3\n\n# Minimum size of segments to process\nMIN_SEGMENT_DURATION = 1\n\n# The maximum time for texts from old segments to be used in the next segment \nMAX_PROMPT_WINDOW = 0 # seconds (0 = disabled)\nPROMPT_NO_SPEECH_PROB = 0.1 # Do not pass the text from segments with a no speech probability higher than this", "MAX_PROMPT_WINDOW = 0 # seconds (0 = disabled)\nPROMPT_NO_SPEECH_PROB = 0.1 # Do not pass the text from segments with a no speech probability higher than this\n\nVAD_MAX_PROCESSING_CHUNK = 60 * 60 # 60 minutes of audio\n\nclass TranscriptionConfig(ABC):\n    def __init__(self, non_speech_strategy: NonSpeechStrategy = NonSpeechStrategy.SKIP, \n                       segment_padding_left: float = None, segment_padding_right = None, max_silent_period: float = None, \n                       max_merge_size: float = None, max_prompt_window: float = None, initial_segment_index = -1):\n        self.non_speech_strategy = non_speech_strategy\n        self.segment_padding_left = segment_padding_left\n        self.segment_padding_right = segment_padding_right\n        self.max_silent_period = max_silent_period\n        self.max_merge_size = max_merge_size\n        self.max_prompt_window = max_prompt_window\n        self.initial_segment_index = initial_segment_index", "\nclass PeriodicTranscriptionConfig(TranscriptionConfig):\n    def __init__(self, periodic_duration: float, non_speech_strategy: NonSpeechStrategy = NonSpeechStrategy.SKIP, \n                       segment_padding_left: float = None, segment_padding_right = None, max_silent_period: float = None, \n                       max_merge_size: float = None, max_prompt_window: float = None, initial_segment_index = -1):\n        super().__init__(non_speech_strategy, segment_padding_left, segment_padding_right, max_silent_period, max_merge_size, max_prompt_window, initial_segment_index)\n        self.periodic_duration = periodic_duration\n\nclass AbstractTranscription(ABC):\n    def __init__(self, sampling_rate: int = 16000):\n        self.sampling_rate = sampling_rate\n\n    def get_audio_segment(self, str, start_time: str = None, duration: str = None):\n        return load_audio(str, self.sampling_rate, start_time, duration)\n\n    def is_transcribe_timestamps_fast(self):\n        \"\"\"\n        Determine if get_transcribe_timestamps is fast enough to not need parallelization.\n        \"\"\"\n        return False\n\n    @abstractmethod\n    def get_transcribe_timestamps(self, audio: str, config: TranscriptionConfig, start_time: float, end_time: float):\n        \"\"\"\n        Get the start and end timestamps of the sections that should be transcribed by this VAD method.\n\n        Parameters\n        ----------\n        audio: str\n            The audio file.\n        config: TranscriptionConfig\n            The transcription configuration.\n\n        Returns\n        -------\n        A list of start and end timestamps, in fractional seconds.\n        \"\"\"\n        return \n\n    def get_merged_timestamps(self, timestamps: List[Dict[str, Any]], config: TranscriptionConfig, total_duration: float):\n        \"\"\"\n        Get the start and end timestamps of the sections that should be transcribed by this VAD method,\n        after merging the given segments using the specified configuration.\n\n        Parameters\n        ----------\n        audio: str\n            The audio file. \n        config: TranscriptionConfig\n            The transcription configuration.\n\n        Returns\n        -------\n        A list of start and end timestamps, in fractional seconds.\n        \"\"\"\n        merged = merge_timestamps(timestamps, config.max_silent_period, config.max_merge_size, \n                                  config.segment_padding_left, config.segment_padding_right)\n\n        if config.non_speech_strategy != NonSpeechStrategy.SKIP:\n            # Expand segments to include the gaps between them\n            if (config.non_speech_strategy == NonSpeechStrategy.CREATE_SEGMENT):\n                # When we have a prompt window, we create speech segments betwen each segment if we exceed the merge size\n                merged = self.fill_gaps(merged, total_duration=total_duration, max_expand_size=config.max_merge_size)\n            elif config.non_speech_strategy == NonSpeechStrategy.EXPAND_SEGMENT: \n                # With no prompt window, it is better to just expand the segments (this effectively passes the prompt to the next segment)\n                merged = self.expand_gaps(merged, total_duration=total_duration)\n            else:\n                raise Exception(\"Unknown non-speech strategy: \" + str(config.non_speech_strategy))\n\n            print(\"Transcribing non-speech:\")\n            pprint(merged)\n        return merged\n\n    def transcribe(self, audio: str, whisperCallable: AbstractWhisperCallback, config: TranscriptionConfig, \n                   progressListener: ProgressListener = None):\n        \"\"\"\n        Transcribe the given audo file.\n\n        Parameters\n        ----------\n        audio: str\n            The audio file.\n        whisperCallable: WhisperCallback\n            A callback object to call to transcribe each segment.\n\n        Returns\n        -------\n        A list of start and end timestamps, in fractional seconds.\n        \"\"\"\n\n        try:\n            max_audio_duration = self.get_audio_duration(audio, config)\n            timestamp_segments = self.get_transcribe_timestamps(audio, config, 0, max_audio_duration)\n\n            # Get speech timestamps from full audio file\n            merged = self.get_merged_timestamps(timestamp_segments, config, max_audio_duration)\n\n            # A deque of transcribed segments that is passed to the next segment as a prompt\n            prompt_window = deque()\n\n            print(\"Processing timestamps:\")\n            pprint(merged)\n\n            result = {\n                'text': \"\",\n                'segments': [],\n                'language': \"\"\n            }\n            languageCounter = Counter()\n            detected_language = None\n\n            segment_index = config.initial_segment_index\n\n            # Calculate progress \n            progress_start_offset = merged[0]['start'] if len(merged) > 0 else 0\n            progress_total_duration = sum([segment['end'] - segment['start'] for segment in merged])\n\n            # For each time segment, run whisper\n            for segment in merged:\n                segment_index += 1\n                segment_start = segment['start']\n                segment_end = segment['end']\n                segment_expand_amount = segment.get('expand_amount', 0)\n                segment_gap = segment.get('gap', False)\n\n                segment_duration = segment_end - segment_start\n\n                if segment_duration < MIN_SEGMENT_DURATION:\n                    continue\n\n                # Audio to run on Whisper\n                segment_audio = self.get_audio_segment(audio, start_time = str(segment_start), duration = str(segment_duration))\n                # Previous segments to use as a prompt\n                segment_prompt = ' '.join([segment['text'] for segment in prompt_window]) if len(prompt_window) > 0 else None\n        \n                # Detected language\n                detected_language = languageCounter.most_common(1)[0][0] if len(languageCounter) > 0 else None\n\n                print(\"Running whisper from \", format_timestamp(segment_start), \" to \", format_timestamp(segment_end), \", duration: \", \n                    segment_duration, \"expanded: \", segment_expand_amount, \"prompt: \", segment_prompt, \"language: \", detected_language)\n\n                perf_start_time = time.perf_counter()\n\n                scaled_progress_listener = SubTaskProgressListener(progressListener, base_task_total=progress_total_duration, \n                                                                   sub_task_start=segment_start - progress_start_offset, sub_task_total=segment_duration) \n                segment_result = whisperCallable.invoke(segment_audio, segment_index, segment_prompt, detected_language, progress_listener=scaled_progress_listener)\n\n                perf_end_time = time.perf_counter()\n                print(\"Whisper took {} seconds\".format(perf_end_time - perf_start_time))\n\n                adjusted_segments = self.adjust_timestamp(segment_result[\"segments\"], adjust_seconds=segment_start, max_source_time=segment_duration)\n\n                # Propagate expand amount to the segments\n                if (segment_expand_amount > 0):\n                    segment_without_expansion = segment_duration - segment_expand_amount\n\n                    for adjusted_segment in adjusted_segments:\n                        adjusted_segment_end = adjusted_segment['end']\n\n                        # Add expand amount if the segment got expanded\n                        if (adjusted_segment_end > segment_without_expansion):\n                            adjusted_segment[\"expand_amount\"] = adjusted_segment_end - segment_without_expansion\n\n                # Append to output\n                result['text'] += segment_result['text']\n                result['segments'].extend(adjusted_segments)\n\n                # Increment detected language\n                if not segment_gap:\n                    languageCounter[segment_result['language']] += 1\n\n                # Update prompt window\n                self.__update_prompt_window(prompt_window, adjusted_segments, segment_end, segment_gap, config)\n                \n            if detected_language is not None:\n                result['language'] = detected_language\n        finally:\n            # Notify progress listener that we are done\n            if progressListener is not None:\n                progressListener.on_finished()\n        return result\n    \n    def get_audio_duration(self, audio: str, config: TranscriptionConfig):\n        return get_audio_duration(audio)\n\n    def __update_prompt_window(self, prompt_window: Deque, adjusted_segments: List, segment_end: float, segment_gap: bool, config: TranscriptionConfig):\n        if (config.max_prompt_window is not None and config.max_prompt_window > 0):\n            # Add segments to the current prompt window (unless it is a speech gap)\n            if not segment_gap:\n                for segment in adjusted_segments:\n                    if segment.get('no_speech_prob', 0) <= PROMPT_NO_SPEECH_PROB:\n                        prompt_window.append(segment)\n\n            while (len(prompt_window) > 0):\n                first_end_time = prompt_window[0].get('end', 0)\n                # Time expanded in the segments should be discounted from the prompt window\n                first_expand_time = prompt_window[0].get('expand_amount', 0)\n\n                if (first_end_time - first_expand_time < segment_end - config.max_prompt_window):\n                    prompt_window.popleft()\n                else:\n                    break\n\n    def include_gaps(self, segments: Iterator[dict], min_gap_length: float, total_duration: float):\n        result = []\n        last_end_time = 0\n\n        for segment in segments:\n            segment_start = float(segment['start'])\n            segment_end = float(segment['end'])\n\n            if (last_end_time != segment_start):\n                delta = segment_start - last_end_time\n\n                if (min_gap_length is None or delta >= min_gap_length):\n                    result.append( { 'start': last_end_time, 'end': segment_start, 'gap': True } )\n            \n            last_end_time = segment_end\n            result.append(segment)\n\n        # Also include total duration if specified\n        if (total_duration is not None and last_end_time < total_duration):\n            delta = total_duration - segment_start\n\n            if (min_gap_length is None or delta >= min_gap_length):\n                result.append( { 'start': last_end_time, 'end': total_duration, 'gap': True } )\n\n        return result\n\n    # Expand the end time of each segment to the start of the next segment\n    def expand_gaps(self, segments: List[Dict[str, Any]], total_duration: float):\n        result = []\n\n        if len(segments) == 0:\n            return result\n\n        # Add gap at the beginning if needed\n        if (segments[0]['start'] > 0):\n            result.append({ 'start': 0, 'end': segments[0]['start'], 'gap': True } )\n\n        for i in range(len(segments) - 1):\n            current_segment = segments[i]\n            next_segment = segments[i + 1]\n\n            delta = next_segment['start'] - current_segment['end']\n\n            # Expand if the gap actually exists\n            if (delta >= 0):\n                current_segment = current_segment.copy()\n                current_segment['expand_amount'] = delta\n                current_segment['end'] = next_segment['start']\n            \n            result.append(current_segment)\n\n        # Add last segment\n        last_segment = segments[-1]\n        result.append(last_segment)\n\n        # Also include total duration if specified\n        if (total_duration is not None):\n            last_segment = result[-1]\n\n            if (last_segment['end'] < total_duration):\n                last_segment = last_segment.copy()\n                last_segment['end'] = total_duration\n                result[-1] = last_segment\n\n        return result\n\n    def fill_gaps(self, segments: List[Dict[str, Any]], total_duration: float, max_expand_size: float = None):\n        result = []\n\n        if len(segments) == 0:\n            return result\n\n        # Add gap at the beginning if needed\n        if (segments[0]['start'] > 0):\n            result.append({ 'start': 0, 'end': segments[0]['start'], 'gap': True } )\n\n        for i in range(len(segments) - 1):\n            expanded = False\n            current_segment = segments[i]\n            next_segment = segments[i + 1]\n\n            delta = next_segment['start'] - current_segment['end']\n\n            if (max_expand_size is not None and delta <= max_expand_size):\n                # Just expand the current segment\n                current_segment = current_segment.copy()\n                current_segment['expand_amount'] = delta\n                current_segment['end'] = next_segment['start']\n                expanded = True\n\n            result.append(current_segment)\n\n            # Add a gap to the next segment if needed\n            if (delta >= 0 and not expanded):\n                result.append({ 'start': current_segment['end'], 'end': next_segment['start'], 'gap': True } )\n            \n        # Add last segment\n        last_segment = segments[-1]\n        result.append(last_segment)\n\n        # Also include total duration if specified\n        if (total_duration is not None):\n            last_segment = result[-1]\n\n            delta = total_duration - last_segment['end']\n\n            if (delta > 0):\n                if (max_expand_size is not None and delta <= max_expand_size):\n                    # Expand the last segment\n                    last_segment = last_segment.copy()\n                    last_segment['expand_amount'] = delta\n                    last_segment['end'] = total_duration\n                    result[-1] = last_segment\n                else:\n                    result.append({ 'start': last_segment['end'], 'end': total_duration, 'gap': True } )\n\n        return result\n\n    def adjust_timestamp(self, segments: Iterator[dict], adjust_seconds: float, max_source_time: float = None):\n        result = []\n\n        for segment in segments:\n            segment_start = float(segment['start'])\n            segment_end = float(segment['end'])\n\n            # Filter segments?\n            if (max_source_time is not None):\n                if (segment_start > max_source_time):\n                    continue\n                segment_end = min(max_source_time, segment_end)\n\n                new_segment = segment.copy()\n\n            # Add to start and end\n            new_segment['start'] = segment_start + adjust_seconds\n            new_segment['end'] = segment_end + adjust_seconds\n            result.append(new_segment)\n        return result\n\n    def multiply_timestamps(self, timestamps: List[Dict[str, Any]], factor: float):\n        result = []\n\n        for entry in timestamps:\n            start = entry['start']\n            end = entry['end']\n\n            result.append({\n                'start': start * factor,\n                'end': end * factor\n            })\n        return result", "class AbstractTranscription(ABC):\n    def __init__(self, sampling_rate: int = 16000):\n        self.sampling_rate = sampling_rate\n\n    def get_audio_segment(self, str, start_time: str = None, duration: str = None):\n        return load_audio(str, self.sampling_rate, start_time, duration)\n\n    def is_transcribe_timestamps_fast(self):\n        \"\"\"\n        Determine if get_transcribe_timestamps is fast enough to not need parallelization.\n        \"\"\"\n        return False\n\n    @abstractmethod\n    def get_transcribe_timestamps(self, audio: str, config: TranscriptionConfig, start_time: float, end_time: float):\n        \"\"\"\n        Get the start and end timestamps of the sections that should be transcribed by this VAD method.\n\n        Parameters\n        ----------\n        audio: str\n            The audio file.\n        config: TranscriptionConfig\n            The transcription configuration.\n\n        Returns\n        -------\n        A list of start and end timestamps, in fractional seconds.\n        \"\"\"\n        return \n\n    def get_merged_timestamps(self, timestamps: List[Dict[str, Any]], config: TranscriptionConfig, total_duration: float):\n        \"\"\"\n        Get the start and end timestamps of the sections that should be transcribed by this VAD method,\n        after merging the given segments using the specified configuration.\n\n        Parameters\n        ----------\n        audio: str\n            The audio file. \n        config: TranscriptionConfig\n            The transcription configuration.\n\n        Returns\n        -------\n        A list of start and end timestamps, in fractional seconds.\n        \"\"\"\n        merged = merge_timestamps(timestamps, config.max_silent_period, config.max_merge_size, \n                                  config.segment_padding_left, config.segment_padding_right)\n\n        if config.non_speech_strategy != NonSpeechStrategy.SKIP:\n            # Expand segments to include the gaps between them\n            if (config.non_speech_strategy == NonSpeechStrategy.CREATE_SEGMENT):\n                # When we have a prompt window, we create speech segments betwen each segment if we exceed the merge size\n                merged = self.fill_gaps(merged, total_duration=total_duration, max_expand_size=config.max_merge_size)\n            elif config.non_speech_strategy == NonSpeechStrategy.EXPAND_SEGMENT: \n                # With no prompt window, it is better to just expand the segments (this effectively passes the prompt to the next segment)\n                merged = self.expand_gaps(merged, total_duration=total_duration)\n            else:\n                raise Exception(\"Unknown non-speech strategy: \" + str(config.non_speech_strategy))\n\n            print(\"Transcribing non-speech:\")\n            pprint(merged)\n        return merged\n\n    def transcribe(self, audio: str, whisperCallable: AbstractWhisperCallback, config: TranscriptionConfig, \n                   progressListener: ProgressListener = None):\n        \"\"\"\n        Transcribe the given audo file.\n\n        Parameters\n        ----------\n        audio: str\n            The audio file.\n        whisperCallable: WhisperCallback\n            A callback object to call to transcribe each segment.\n\n        Returns\n        -------\n        A list of start and end timestamps, in fractional seconds.\n        \"\"\"\n\n        try:\n            max_audio_duration = self.get_audio_duration(audio, config)\n            timestamp_segments = self.get_transcribe_timestamps(audio, config, 0, max_audio_duration)\n\n            # Get speech timestamps from full audio file\n            merged = self.get_merged_timestamps(timestamp_segments, config, max_audio_duration)\n\n            # A deque of transcribed segments that is passed to the next segment as a prompt\n            prompt_window = deque()\n\n            print(\"Processing timestamps:\")\n            pprint(merged)\n\n            result = {\n                'text': \"\",\n                'segments': [],\n                'language': \"\"\n            }\n            languageCounter = Counter()\n            detected_language = None\n\n            segment_index = config.initial_segment_index\n\n            # Calculate progress \n            progress_start_offset = merged[0]['start'] if len(merged) > 0 else 0\n            progress_total_duration = sum([segment['end'] - segment['start'] for segment in merged])\n\n            # For each time segment, run whisper\n            for segment in merged:\n                segment_index += 1\n                segment_start = segment['start']\n                segment_end = segment['end']\n                segment_expand_amount = segment.get('expand_amount', 0)\n                segment_gap = segment.get('gap', False)\n\n                segment_duration = segment_end - segment_start\n\n                if segment_duration < MIN_SEGMENT_DURATION:\n                    continue\n\n                # Audio to run on Whisper\n                segment_audio = self.get_audio_segment(audio, start_time = str(segment_start), duration = str(segment_duration))\n                # Previous segments to use as a prompt\n                segment_prompt = ' '.join([segment['text'] for segment in prompt_window]) if len(prompt_window) > 0 else None\n        \n                # Detected language\n                detected_language = languageCounter.most_common(1)[0][0] if len(languageCounter) > 0 else None\n\n                print(\"Running whisper from \", format_timestamp(segment_start), \" to \", format_timestamp(segment_end), \", duration: \", \n                    segment_duration, \"expanded: \", segment_expand_amount, \"prompt: \", segment_prompt, \"language: \", detected_language)\n\n                perf_start_time = time.perf_counter()\n\n                scaled_progress_listener = SubTaskProgressListener(progressListener, base_task_total=progress_total_duration, \n                                                                   sub_task_start=segment_start - progress_start_offset, sub_task_total=segment_duration) \n                segment_result = whisperCallable.invoke(segment_audio, segment_index, segment_prompt, detected_language, progress_listener=scaled_progress_listener)\n\n                perf_end_time = time.perf_counter()\n                print(\"Whisper took {} seconds\".format(perf_end_time - perf_start_time))\n\n                adjusted_segments = self.adjust_timestamp(segment_result[\"segments\"], adjust_seconds=segment_start, max_source_time=segment_duration)\n\n                # Propagate expand amount to the segments\n                if (segment_expand_amount > 0):\n                    segment_without_expansion = segment_duration - segment_expand_amount\n\n                    for adjusted_segment in adjusted_segments:\n                        adjusted_segment_end = adjusted_segment['end']\n\n                        # Add expand amount if the segment got expanded\n                        if (adjusted_segment_end > segment_without_expansion):\n                            adjusted_segment[\"expand_amount\"] = adjusted_segment_end - segment_without_expansion\n\n                # Append to output\n                result['text'] += segment_result['text']\n                result['segments'].extend(adjusted_segments)\n\n                # Increment detected language\n                if not segment_gap:\n                    languageCounter[segment_result['language']] += 1\n\n                # Update prompt window\n                self.__update_prompt_window(prompt_window, adjusted_segments, segment_end, segment_gap, config)\n                \n            if detected_language is not None:\n                result['language'] = detected_language\n        finally:\n            # Notify progress listener that we are done\n            if progressListener is not None:\n                progressListener.on_finished()\n        return result\n    \n    def get_audio_duration(self, audio: str, config: TranscriptionConfig):\n        return get_audio_duration(audio)\n\n    def __update_prompt_window(self, prompt_window: Deque, adjusted_segments: List, segment_end: float, segment_gap: bool, config: TranscriptionConfig):\n        if (config.max_prompt_window is not None and config.max_prompt_window > 0):\n            # Add segments to the current prompt window (unless it is a speech gap)\n            if not segment_gap:\n                for segment in adjusted_segments:\n                    if segment.get('no_speech_prob', 0) <= PROMPT_NO_SPEECH_PROB:\n                        prompt_window.append(segment)\n\n            while (len(prompt_window) > 0):\n                first_end_time = prompt_window[0].get('end', 0)\n                # Time expanded in the segments should be discounted from the prompt window\n                first_expand_time = prompt_window[0].get('expand_amount', 0)\n\n                if (first_end_time - first_expand_time < segment_end - config.max_prompt_window):\n                    prompt_window.popleft()\n                else:\n                    break\n\n    def include_gaps(self, segments: Iterator[dict], min_gap_length: float, total_duration: float):\n        result = []\n        last_end_time = 0\n\n        for segment in segments:\n            segment_start = float(segment['start'])\n            segment_end = float(segment['end'])\n\n            if (last_end_time != segment_start):\n                delta = segment_start - last_end_time\n\n                if (min_gap_length is None or delta >= min_gap_length):\n                    result.append( { 'start': last_end_time, 'end': segment_start, 'gap': True } )\n            \n            last_end_time = segment_end\n            result.append(segment)\n\n        # Also include total duration if specified\n        if (total_duration is not None and last_end_time < total_duration):\n            delta = total_duration - segment_start\n\n            if (min_gap_length is None or delta >= min_gap_length):\n                result.append( { 'start': last_end_time, 'end': total_duration, 'gap': True } )\n\n        return result\n\n    # Expand the end time of each segment to the start of the next segment\n    def expand_gaps(self, segments: List[Dict[str, Any]], total_duration: float):\n        result = []\n\n        if len(segments) == 0:\n            return result\n\n        # Add gap at the beginning if needed\n        if (segments[0]['start'] > 0):\n            result.append({ 'start': 0, 'end': segments[0]['start'], 'gap': True } )\n\n        for i in range(len(segments) - 1):\n            current_segment = segments[i]\n            next_segment = segments[i + 1]\n\n            delta = next_segment['start'] - current_segment['end']\n\n            # Expand if the gap actually exists\n            if (delta >= 0):\n                current_segment = current_segment.copy()\n                current_segment['expand_amount'] = delta\n                current_segment['end'] = next_segment['start']\n            \n            result.append(current_segment)\n\n        # Add last segment\n        last_segment = segments[-1]\n        result.append(last_segment)\n\n        # Also include total duration if specified\n        if (total_duration is not None):\n            last_segment = result[-1]\n\n            if (last_segment['end'] < total_duration):\n                last_segment = last_segment.copy()\n                last_segment['end'] = total_duration\n                result[-1] = last_segment\n\n        return result\n\n    def fill_gaps(self, segments: List[Dict[str, Any]], total_duration: float, max_expand_size: float = None):\n        result = []\n\n        if len(segments) == 0:\n            return result\n\n        # Add gap at the beginning if needed\n        if (segments[0]['start'] > 0):\n            result.append({ 'start': 0, 'end': segments[0]['start'], 'gap': True } )\n\n        for i in range(len(segments) - 1):\n            expanded = False\n            current_segment = segments[i]\n            next_segment = segments[i + 1]\n\n            delta = next_segment['start'] - current_segment['end']\n\n            if (max_expand_size is not None and delta <= max_expand_size):\n                # Just expand the current segment\n                current_segment = current_segment.copy()\n                current_segment['expand_amount'] = delta\n                current_segment['end'] = next_segment['start']\n                expanded = True\n\n            result.append(current_segment)\n\n            # Add a gap to the next segment if needed\n            if (delta >= 0 and not expanded):\n                result.append({ 'start': current_segment['end'], 'end': next_segment['start'], 'gap': True } )\n            \n        # Add last segment\n        last_segment = segments[-1]\n        result.append(last_segment)\n\n        # Also include total duration if specified\n        if (total_duration is not None):\n            last_segment = result[-1]\n\n            delta = total_duration - last_segment['end']\n\n            if (delta > 0):\n                if (max_expand_size is not None and delta <= max_expand_size):\n                    # Expand the last segment\n                    last_segment = last_segment.copy()\n                    last_segment['expand_amount'] = delta\n                    last_segment['end'] = total_duration\n                    result[-1] = last_segment\n                else:\n                    result.append({ 'start': last_segment['end'], 'end': total_duration, 'gap': True } )\n\n        return result\n\n    def adjust_timestamp(self, segments: Iterator[dict], adjust_seconds: float, max_source_time: float = None):\n        result = []\n\n        for segment in segments:\n            segment_start = float(segment['start'])\n            segment_end = float(segment['end'])\n\n            # Filter segments?\n            if (max_source_time is not None):\n                if (segment_start > max_source_time):\n                    continue\n                segment_end = min(max_source_time, segment_end)\n\n                new_segment = segment.copy()\n\n            # Add to start and end\n            new_segment['start'] = segment_start + adjust_seconds\n            new_segment['end'] = segment_end + adjust_seconds\n            result.append(new_segment)\n        return result\n\n    def multiply_timestamps(self, timestamps: List[Dict[str, Any]], factor: float):\n        result = []\n\n        for entry in timestamps:\n            start = entry['start']\n            end = entry['end']\n\n            result.append({\n                'start': start * factor,\n                'end': end * factor\n            })\n        return result", "\n\nclass VadSileroTranscription(AbstractTranscription):\n    def __init__(self, sampling_rate: int = 16000, cache: ModelCache = None):\n        super().__init__(sampling_rate=sampling_rate)\n        self.model = None\n        self.cache = cache\n        self._initialize_model()\n\n    def _initialize_model(self):\n        if (self.cache is not None):\n            model_key = \"VadSileroTranscription\"\n            self.model, self.get_speech_timestamps = self.cache.get(model_key, self._create_model)\n            print(\"Loaded Silerio model from cache.\")\n        else:\n            self.model, self.get_speech_timestamps = self._create_model()\n            print(\"Created Silerio model\")\n\n    def _create_model(self):\n        \n        # model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n        model, utils = torch.hub.load(repo_or_dir='models/silero-vad', model='silero_vad',source=\"local\")\n        \n        # Silero does not benefit from multi-threading\n        torch.set_num_threads(1) # JIT\n        (get_speech_timestamps, _, _, _, _) = utils\n\n        return model, get_speech_timestamps\n\n    def get_transcribe_timestamps(self, audio: str, config: TranscriptionConfig, start_time: float, end_time: float):\n        result = []\n\n        print(\"Getting timestamps from audio file: {}, start: {}, duration: {}\".format(audio, start_time, end_time))\n        perf_start_time = time.perf_counter()\n\n        # Divide procesisng of audio into chunks\n        chunk_start = start_time\n\n        while (chunk_start < end_time):\n            chunk_duration = min(end_time - chunk_start, VAD_MAX_PROCESSING_CHUNK)\n\n            print(\"Processing VAD in chunk from {} to {}\".format(format_timestamp(chunk_start), format_timestamp(chunk_start + chunk_duration)))\n            wav = self.get_audio_segment(audio, str(chunk_start), str(chunk_duration))\n\n            sample_timestamps = self.get_speech_timestamps(wav, self.model, sampling_rate=self.sampling_rate, threshold=SPEECH_TRESHOLD)\n            seconds_timestamps = self.multiply_timestamps(sample_timestamps, factor=1 / self.sampling_rate) \n            adjusted = self.adjust_timestamp(seconds_timestamps, adjust_seconds=chunk_start, max_source_time=chunk_start + chunk_duration)\n\n            #pprint(adjusted)\n\n            result.extend(adjusted)\n            chunk_start += chunk_duration\n\n        perf_end_time = time.perf_counter()\n        print(\"VAD processing took {} seconds\".format(perf_end_time - perf_start_time))\n\n        return result\n\n    def __getstate__(self):\n        # We only need the sampling rate\n        return { 'sampling_rate': self.sampling_rate }\n\n    def __setstate__(self, state):\n        self.sampling_rate = state['sampling_rate']\n        self.model = None\n        # Use the global cache\n        self.cache = GLOBAL_MODEL_CACHE\n        self._initialize_model()", "\n# A very simple VAD that just marks every N seconds as speech\nclass VadPeriodicTranscription(AbstractTranscription):\n    def __init__(self, sampling_rate: int = 16000):\n        super().__init__(sampling_rate=sampling_rate)\n\n    def is_transcribe_timestamps_fast(self):\n        # This is a very fast VAD - no need to parallelize it\n        return True\n\n    def get_transcribe_timestamps(self, audio: str, config: PeriodicTranscriptionConfig, start_time: float, end_time: float):\n        result = []\n\n        # Generate a timestamp every N seconds\n        start_timestamp = start_time\n\n        while (start_timestamp < end_time):\n            end_timestamp = min(start_timestamp + config.periodic_duration, end_time)\n            segment_duration = end_timestamp - start_timestamp\n\n            # Minimum duration is 1 second\n            if (segment_duration >= 1):\n                result.append( {  'start': start_timestamp, 'end': end_timestamp } )\n\n            start_timestamp = end_timestamp\n\n        return result", "\ndef get_audio_duration(file: str):\n    return float(ffmpeg.probe(file)[\"format\"][\"duration\"])\n\ndef load_audio(file: str, sample_rate: int = 16000, \n               start_time: str = None, duration: str = None):\n    \"\"\"\n    Open an audio file and read as mono waveform, resampling as necessary\n\n    Parameters\n    ----------\n    file: str\n        The audio file to open\n\n    sr: int\n        The sample rate to resample the audio if necessary\n\n    start_time: str\n        The start time, using the standard FFMPEG time duration syntax, or None to disable.\n    \n    duration: str\n        The duration, using the standard FFMPEG time duration syntax, or None to disable.\n\n    Returns\n    -------\n    A NumPy array containing the audio waveform, in float32 dtype.\n    \"\"\"\n    try:\n        inputArgs = {'threads': 0}\n\n        if (start_time is not None):\n            inputArgs['ss'] = start_time\n        if (duration is not None):\n            inputArgs['t'] = duration\n\n        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n        out, _ = (\n            ffmpeg.input(file, **inputArgs)\n            .output(\"-\", format=\"s16le\", acodec=\"pcm_s16le\", ac=1, ar=sample_rate)\n            .run(cmd=\"ffmpeg\", capture_stdout=True, capture_stderr=True)\n        )\n    except ffmpeg.Error as e:\n        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\")\n\n    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0"]}
{"filename": "src/segments.py", "chunked_list": ["from typing import Any, Dict, List\n\nimport copy\n\ndef merge_timestamps(timestamps: List[Dict[str, Any]], merge_window: float = 5, max_merge_size: float = 30, padding_left: float = 1, padding_right: float = 1):\n    result = []\n\n    if len(timestamps) == 0:\n        return result\n    if max_merge_size is None:\n        return timestamps\n\n    if padding_left is None:\n        padding_left = 0\n    if padding_right is None:\n        padding_right = 0\n\n    processed_time = 0\n    current_segment = None\n\n    for i in range(len(timestamps)):\n        next_segment = timestamps[i]\n\n        delta = next_segment['start'] - processed_time\n\n        # Note that segments can still be longer than the max merge size, they just won't be merged in that case\n        if current_segment is None or (merge_window is not None and delta > merge_window) \\\n                 or next_segment['end'] - current_segment['start'] > max_merge_size:\n            # Finish the current segment\n            if current_segment is not None:\n                # Add right padding\n                finish_padding = min(padding_right, delta / 2) if delta < padding_left + padding_right else padding_right\n                current_segment['end'] += finish_padding\n                delta -= finish_padding\n\n                result.append(current_segment)\n\n            # Start a new segment\n            current_segment = copy.deepcopy(next_segment)\n\n            # Pad the segment\n            current_segment['start'] = current_segment['start'] - min(padding_left, delta)\n            processed_time = current_segment['end']\n\n        else:\n            # Merge the segment\n            current_segment['end'] = next_segment['end']\n            processed_time = current_segment['end']\n        \n    # Add the last segment\n    if current_segment is not None:\n        current_segment['end'] += padding_right\n        result.append(current_segment)\n    \n    return result"]}
{"filename": "src/vadParallel.py", "chunked_list": ["import multiprocessing\nfrom queue import Empty\nimport threading\nimport time\nfrom src.hooks.progressListener import ProgressListener\nfrom src.vad import AbstractTranscription, TranscriptionConfig, get_audio_duration\n\nfrom multiprocessing import Pool, Queue\n\nfrom typing import Any, Dict, List, Union", "\nfrom typing import Any, Dict, List, Union\nimport os\n\nfrom src.whisper.abstractWhisperContainer import AbstractWhisperCallback\n\nclass _ProgressListenerToQueue(ProgressListener):\n    def __init__(self, progress_queue: Queue):\n        self.progress_queue = progress_queue\n        self.progress_total = 0\n        self.prev_progress = 0\n\n    def on_progress(self, current: Union[int, float], total: Union[int, float]):\n        delta = current - self.prev_progress\n        self.prev_progress = current\n        self.progress_total = total\n        self.progress_queue.put(delta)\n\n    def on_finished(self):\n        if self.progress_total > self.prev_progress:\n            delta = self.progress_total - self.prev_progress\n            self.progress_queue.put(delta)\n            self.prev_progress = self.progress_total", "\nclass ParallelContext:\n    def __init__(self, num_processes: int = None, auto_cleanup_timeout_seconds: float = None):\n        self.num_processes = num_processes\n        self.auto_cleanup_timeout_seconds = auto_cleanup_timeout_seconds\n        self.lock = threading.Lock()\n\n        self.ref_count = 0\n        self.pool = None\n        self.cleanup_timer = None\n\n    def get_pool(self):\n        # Initialize pool lazily\n        if (self.pool is None):\n            context = multiprocessing.get_context('spawn')\n            self.pool = context.Pool(self.num_processes)\n\n        self.ref_count = self.ref_count + 1\n\n        if (self.auto_cleanup_timeout_seconds is not None):\n            self._stop_auto_cleanup()\n\n        return self.pool\n\n    def return_pool(self, pool):\n        if (self.pool == pool and self.ref_count > 0):\n            self.ref_count = self.ref_count - 1\n\n            if (self.ref_count == 0):\n                if (self.auto_cleanup_timeout_seconds is not None):\n                    self._start_auto_cleanup()\n\n    def _start_auto_cleanup(self):\n        if (self.cleanup_timer is not None):\n            self.cleanup_timer.cancel()\n        self.cleanup_timer = threading.Timer(self.auto_cleanup_timeout_seconds, self._execute_cleanup)\n        self.cleanup_timer.start()\n\n        print(\"Started auto cleanup of pool in \" + str(self.auto_cleanup_timeout_seconds) + \" seconds\")\n\n    def _stop_auto_cleanup(self):\n        if (self.cleanup_timer is not None):\n            self.cleanup_timer.cancel()\n            self.cleanup_timer = None\n\n            print(\"Stopped auto cleanup of pool\")\n\n    def _execute_cleanup(self):\n        print(\"Executing cleanup of pool\")\n\n        if (self.ref_count == 0):\n            self.close()\n\n    def close(self):\n        self._stop_auto_cleanup()\n\n        if (self.pool is not None):\n            print(\"Closing pool of \" + str(self.num_processes) + \" processes\")\n            self.pool.close()\n            self.pool.join()\n        self.pool = None", "\nclass ParallelTranscriptionConfig(TranscriptionConfig):\n    def __init__(self, device_id: str, override_timestamps, initial_segment_index, copy: TranscriptionConfig = None):\n        super().__init__(copy.non_speech_strategy, copy.segment_padding_left, copy.segment_padding_right, copy.max_silent_period, copy.max_merge_size, copy.max_prompt_window, initial_segment_index)\n        self.device_id = device_id\n        self.override_timestamps = override_timestamps\n\nclass ParallelTranscription(AbstractTranscription):\n    # Silero VAD typically takes about 3 seconds per minute, so there's no need to split the chunks \n    # into smaller segments than 2 minute (min 6 seconds per CPU core)\n    MIN_CPU_CHUNK_SIZE_SECONDS = 2 * 60\n\n    def __init__(self, sampling_rate: int = 16000):\n        super().__init__(sampling_rate=sampling_rate)\n\n    def transcribe_parallel(self, transcription: AbstractTranscription, audio: str, whisperCallable: AbstractWhisperCallback, config: TranscriptionConfig, \n                            cpu_device_count: int, gpu_devices: List[str], cpu_parallel_context: ParallelContext = None, gpu_parallel_context: ParallelContext = None, \n                            progress_listener: ProgressListener = None):\n        total_duration = get_audio_duration(audio)\n\n        # First, get the timestamps for the original audio\n        if (cpu_device_count > 1 and not transcription.is_transcribe_timestamps_fast()):\n            merged = self._get_merged_timestamps_parallel(transcription, audio, config, total_duration, cpu_device_count, cpu_parallel_context)\n        else:\n            timestamp_segments = transcription.get_transcribe_timestamps(audio, config, 0, total_duration)\n            merged = transcription.get_merged_timestamps(timestamp_segments, config, total_duration)\n\n        # We must make sure the whisper model is downloaded\n        if (len(gpu_devices) > 1):\n            whisperCallable.model_container.ensure_downloaded()\n\n        # Split into a list for each device\n        # TODO: Split by time instead of by number of chunks\n        merged_split = list(self._split(merged, len(gpu_devices)))\n\n        # Parameters that will be passed to the transcribe function\n        parameters = []\n        segment_index = config.initial_segment_index\n\n        processing_manager = multiprocessing.Manager()\n        progress_queue = processing_manager.Queue()\n\n        for i in range(len(gpu_devices)):\n            # Note that device_segment_list can be empty. But we will still create a process for it,\n            # as otherwise we run the risk of assigning the same device to multiple processes.\n            device_segment_list = list(merged_split[i]) if i < len(merged_split) else []\n            device_id = gpu_devices[i]\n\n            print(\"Device \" + str(device_id) + \" (index \" + str(i) + \") has \" + str(len(device_segment_list)) + \" segments\")\n\n            # Create a new config with the given device ID\n            device_config = ParallelTranscriptionConfig(device_id, device_segment_list, segment_index, config)\n            segment_index += len(device_segment_list)\n\n            progress_listener_to_queue = _ProgressListenerToQueue(progress_queue)\n            parameters.append([audio, whisperCallable, device_config, progress_listener_to_queue]);\n\n        merged = {\n            'text': '',\n            'segments': [],\n            'language': None\n        }\n\n        created_context = False\n\n        perf_start_gpu = time.perf_counter()\n\n        # Spawn a separate process for each device\n        try:\n            if (gpu_parallel_context is None):\n                gpu_parallel_context = ParallelContext(len(gpu_devices))\n                created_context = True\n\n            # Get a pool of processes\n            pool = gpu_parallel_context.get_pool()\n\n            # Run the transcription in parallel\n            results_async = pool.starmap_async(self.transcribe, parameters)\n            total_progress = 0\n\n            while not results_async.ready():\n                try:\n                    delta = progress_queue.get(timeout=5)  # Set a timeout of 5 seconds\n                except Empty:\n                    continue\n                \n                total_progress += delta\n                if progress_listener is not None:\n                    progress_listener.on_progress(total_progress, total_duration)\n\n            results = results_async.get()\n\n            # Call the finished callback\n            if progress_listener is not None:\n                progress_listener.on_finished()\n\n            for result in results:\n                # Merge the results\n                if (result['text'] is not None):\n                    merged['text'] += result['text']\n                if (result['segments'] is not None):\n                    merged['segments'].extend(result['segments'])\n                if (result['language'] is not None):\n                    merged['language'] = result['language']\n\n        finally:\n            # Return the pool to the context\n            if (gpu_parallel_context is not None):\n                gpu_parallel_context.return_pool(pool)\n            # Always close the context if we created it\n            if (created_context):\n                gpu_parallel_context.close()\n\n        perf_end_gpu = time.perf_counter()\n        print(\"Parallel transcription took \" + str(perf_end_gpu - perf_start_gpu) + \" seconds\")\n\n        return merged\n\n    def _get_merged_timestamps_parallel(self, transcription: AbstractTranscription, audio: str, config: TranscriptionConfig, total_duration: float, \n                                       cpu_device_count: int, cpu_parallel_context: ParallelContext = None):\n        parameters = []\n\n        chunk_size = max(total_duration / cpu_device_count, self.MIN_CPU_CHUNK_SIZE_SECONDS)\n        chunk_start = 0\n        cpu_device_id = 0\n\n        perf_start_time = time.perf_counter()\n\n        # Create chunks that will be processed on the CPU\n        while (chunk_start < total_duration):\n            chunk_end = min(chunk_start + chunk_size, total_duration)\n\n            if (chunk_end - chunk_start < 1):\n                # No need to process chunks that are less than 1 second\n                break\n\n            print(\"Parallel VAD: Executing chunk from \" + str(chunk_start) + \" to \" + \n                    str(chunk_end) + \" on CPU device \" + str(cpu_device_id))\n            parameters.append([audio, config, chunk_start, chunk_end]);\n\n            cpu_device_id += 1\n            chunk_start = chunk_end\n\n        created_context = False\n\n        # Spawn a separate process for each device\n        try:\n            if (cpu_parallel_context is None):\n                cpu_parallel_context = ParallelContext(cpu_device_count)\n                created_context = True\n\n            # Get a pool of processes\n            pool = cpu_parallel_context.get_pool()\n\n            # Run the transcription in parallel. Note that transcription must be picklable.\n            results = pool.starmap(transcription.get_transcribe_timestamps, parameters)\n\n            timestamps = []\n\n            # Flatten the results\n            for result in results:\n                timestamps.extend(result)\n\n            merged = transcription.get_merged_timestamps(timestamps, config, total_duration)\n\n            perf_end_time = time.perf_counter()\n            print(\"Parallel VAD processing took {} seconds\".format(perf_end_time - perf_start_time))\n            return merged\n\n        finally:\n            # Return the pool to the context\n            if (cpu_parallel_context is not None):\n                cpu_parallel_context.return_pool(pool)\n            # Always close the context if we created it\n            if (created_context):\n                cpu_parallel_context.close()\n\n    def get_transcribe_timestamps(self, audio: str, config: ParallelTranscriptionConfig, start_time: float, duration: float):\n        return []\n\n    def get_merged_timestamps(self,  timestamps: List[Dict[str, Any]], config: ParallelTranscriptionConfig, total_duration: float):\n        # Override timestamps that will be processed\n        if (config.override_timestamps is not None):\n            print(\"(get_merged_timestamps) Using override timestamps of size \" + str(len(config.override_timestamps)))\n            return config.override_timestamps\n        return super().get_merged_timestamps(timestamps, config, total_duration)\n\n    def transcribe(self, audio: str, whisperCallable: AbstractWhisperCallback, config: ParallelTranscriptionConfig, \n                   progressListener: ProgressListener = None):\n        # Override device ID the first time\n        if (os.environ.get(\"INITIALIZED\", None) is None):\n            os.environ[\"INITIALIZED\"] = \"1\"\n\n            # Note that this may be None if the user didn't specify a device. In that case, Whisper will\n            # just use the default GPU device.\n            if (config.device_id is not None):\n                print(\"Using device \" + config.device_id)\n                os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.device_id\n        \n        return super().transcribe(audio, whisperCallable, config, progressListener)\n\n    def _split(self, a, n):\n        \"\"\"Split a list into n approximately equal parts.\"\"\"\n        k, m = divmod(len(a), n)\n        return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))", "\n"]}
{"filename": "src/description.py", "chunked_list": ["class Description():\n    def __init__(self):\n        pass\n \ndef get_description():\n    description =\"\"\"\n        # \u6807\u51c6\u9009\u9879\n\n        \u8981\u5c06\u97f3\u9891\u6587\u4ef6\u8f6c\u5f55\u6216\u7ffb\u8bd1\uff0c\u53ef\u4ee5\u590d\u5236\u6765\u81ea\u7f51\u7ad9\u7684URL\uff08YT-DLP\u652f\u6301\u7684\u6240\u6709[\u7f51\u7ad9](https://github.com/yt-dlp/yt-dlp/blob/master/supportedsites.md)\u90fd\u53ef\u4ee5\uff0c\u5305\u62ecYouTube\uff09\u3002\n        \u5426\u5219\uff0c\u4e0a\u4f20\u97f3\u9891\u6587\u4ef6\uff08\u5728\u6587\u4ef6\u9009\u62e9\u5668\u4e2d\u9009\u62e9\u201c\u6240\u6709\u6587\u4ef6\uff08\u3002\uff09\u201d\u4ee5\u9009\u62e9\u4efb\u4f55\u6587\u4ef6\u7c7b\u578b\uff0c\u5305\u62ec\u89c6\u9891\u6587\u4ef6\uff09\u6216\u4f7f\u7528\u9ea6\u514b\u98ce\u3002\n\n        \u5bf9\u4e8e\u8f83\u957f\u7684\u97f3\u9891\u6587\u4ef6\uff08> 10\u5206\u949f\uff09\uff0c\u5efa\u8bae\u5728VAD\u9009\u9879\u4e2d\u9009\u62e9Silero VAD\uff08Voice Activity Detector\uff09\uff0c\u7279\u522b\u662f\u5982\u679c\u60a8\u6b63\u5728\u4f7f\u7528`large-v1`\u6a21\u578b\u3002\u8bf7\u6ce8\u610f\uff0c`large-v2`\u6a21\u578b\u66f4\u52a0\u5bbd\u5bb9\uff0c\u4f46\u60a8\u53ef\u80fd\u4ecd\u7136\u60f3\u4f7f\u7528\u7a0d\u9ad8\u7684\u201cVAD - Max Merge Size\uff08s\uff09\u201d\uff0860\u79d2\u6216\u66f4\u957f\u65f6\u95f4\uff09\u7684VAD\u3002\n\n\n        ## \u6a21\u578b\n        \u9009\u62e9Whisper\u5c06\u7528\u4e8e\u8f6c\u5f55\u97f3\u9891\u7684\u6a21\u578b\uff1a\n\n        | \u5c3a\u5bf8      | \u53c2\u6570 | \u4ec5\u652f\u6301\u82f1\u8bed\u7684\u6a21\u578b | \u591a\u8bed\u8a00\u6a21\u578b | \u6240\u9700\u7684\u663e\u5b58 (VRAM) | \u76f8\u5bf9\u901f\u5ea6 |\n        |-----------|------------|--------------------|--------------------|---------------|----------------|\n        | tiny      | 39 M       | tiny.en            | tiny               | ~1 GB         | ~32x           |\n        | base      | 74 M       | base.en            | base               | ~1 GB         | ~16x           |\n        | small     | 244 M      | small.en           | small              | ~2 GB         | ~6x            |\n        | medium    | 769 M      | medium.en          | medium             | ~5 GB         | ~2x            |\n        | large     | 1550 M     | N/A                | large              | ~10 GB        | 1x             |\n        | large-v2  | 1550 M     | N/A                | large              | ~10 GB        | 1x             |\n\n        ## \u8bed\u8a00\n\n        \u9009\u62e9\u8bed\u8a00\uff0c\u6216\u5c06\u5176\u7559\u7a7a\u4ee5\u8ba9Whisper\u81ea\u52a8\u68c0\u6d4b\u8bed\u8a00\u3002\n        \u8bf7\u6ce8\u610f\uff0c\u5982\u679c\u6240\u9009\u8bed\u8a00\u548c\u97f3\u9891\u4e2d\u7684\u8bed\u8a00\u4e0d\u540c\uff0cWhisper\u53ef\u80fd\u4f1a\u5f00\u59cb\u5c06\u97f3\u9891\u7ffb\u8bd1\u4e3a\u6240\u9009\u8bed\u8a00\u3002\n        \u4f8b\u5982\uff0c\u5982\u679c\u97f3\u9891\u662f\u82f1\u8bed\uff0c\u4f46\u60a8\u9009\u62e9\u4e86\u65e5\u8bed\uff0c\u5219\u6a21\u578b\u53ef\u80fd\u4f1a\u5c06\u97f3\u9891\u7ffb\u8bd1\u4e3a\u65e5\u8bed\u3002\n\n        ## \u8f93\u5165\n\n        \u201cURL\uff08YouTube\u7b49\uff09\u201d\uff0c\u201c\u4e0a\u4f20\u6587\u4ef6\u201d\u6216\u201c\u9ea6\u514b\u98ce\u8f93\u5165\u201d\u9009\u9879\u5141\u8bb8\u60a8\u5c06\u97f3\u9891\u8f93\u5165\u53d1\u9001\u5230\u6a21\u578b\u3002\n\n        ### \u591a\u4e2a\u6587\u4ef6\n\n        \u8bf7\u6ce8\u610f\uff0c\u754c\u9762\u53ea\u4f1a\u5904\u7406\u7ed9\u5b9a\u7684URL\u6216\u4e0a\u4f20\u7684\u6587\u4ef6\uff08\u5305\u62ec\u9ea6\u514b\u98ce\uff09\u4e2d\u7684\u4e00\u4e2a\uff0c\u800c\u4e0d\u662f\u4e24\u8005\u90fd\u5904\u7406\u3002\n\n        \u4f46\u662f\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u201c\u4e0a\u4f20\u6587\u4ef6\u201d\u9009\u9879\u6216YouTube\u7684\u64ad\u653e\u5217\u8868\u4e0a\u4f20\u591a\u4e2a\u6587\u4ef6\u3002\u7136\u540e\u5c06\u4f9d\u6b21\u5904\u7406\u6bcf\u4e2a\u97f3\u9891\u6587\u4ef6\uff0c\u5e76\u5c06\u751f\u6210\u7684SRT / VTT /\u8f6c\u5f55\u6587\u672c\u5728\u201c\u4e0b\u8f7d\u201d\u90e8\u5206\u4e2d\u63d0\u4f9b\u3002\n        \u5f53\u5904\u7406\u591a\u4e2a\u6587\u4ef6\u65f6\uff0c\u754c\u9762\u8fd8\u4f1a\u751f\u6210\u4e00\u4e2a\u201cAll_Output\u201d zip\u6587\u4ef6\uff0c\u5176\u4e2d\u5305\u542b\u6240\u6709\u6587\u672c\u8f93\u51fa\u6587\u4ef6\u3002\n\n        ## \u4efb\u52a1\n\n        \u9009\u62e9\u4efb\u52a1 - \u9009\u62e9\u201ctranscribe\u201d\u5c06\u97f3\u9891\u8f6c\u5f55\u4e3a\u6587\u672c\uff0c\u9009\u62e9\u201ctranslate\u201d\u5c06\u97f3\u9891\u7ffb\u8bd1\u4e3a\u82f1\u6587\u3002\n\n        ## \u8bed\u97f3\u6d3b\u6027\u68c0\u6d4bVAD\n\n        \u4f7f\u7528VAD\u5c06\u63d0\u9ad8\u6bcf\u4e2a\u8f6c\u5f55\u884c\u7684\u5b9a\u65f6\u51c6\u786e\u6027\uff0c\u5e76\u9632\u6b62Whisper\u9677\u5165\u65e0\u9650\u5faa\u73af\uff0c\u53cd\u590d\u68c0\u6d4b\u76f8\u540c\u7684\u53e5\u5b50\u3002\u7f3a\u70b9\u662f\u8fd9\u53ef\u80fd\u4f1a\u5bf9\u6587\u672c\u51c6\u786e\u6027\u4ea7\u751f\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u51fa\u73b0\u5728\u97f3\u9891\u4e2d\u7684\u552f\u4e00\u5355\u8bcd\u6216\u540d\u79f0\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u63d0\u793a\u7a97\u53e3\u6765\u5f25\u8865\u8fd9\u4e00\u70b9\u3002\n\n        \u8bf7\u6ce8\u610f\uff0cWhisper\u975e\u5e38\u9002\u5408\u5904\u7406\u82f1\u8bed\uff0c\u4e0d\u592a\u5bb9\u6613\u51fa\u73b0\u4e0e\u4e0d\u826f\u5b9a\u65f6\u548c\u65e0\u9650\u5faa\u73af\u76f8\u5173\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u60a8\u53ef\u80fd\u53ea\u9700\u8981\u4e3a\u5176\u4ed6\u8bed\u8a00\uff08\u4f8b\u5982\u65e5\u8bed\uff09\u6216\u97f3\u9891\u975e\u5e38\u957f\u65f6\u4f7f\u7528VAD\u3002\n\n        * none\n            * \u5728\u6574\u4e2a\u97f3\u9891\u8f93\u5165\u4e0a\u8fd0\u884cWhisper\n        * silero-vad\n            * \u4f7f\u7528Silero VAD\u68c0\u6d4b\u5305\u542b\u8bed\u97f3\u7684\u90e8\u5206\uff0c\u5e76\u5728\u6bcf\u4e2a\u90e8\u5206\u4e0a\u72ec\u7acb\u8fd0\u884cWhisper\u3002\u540c\u65f6\uff0cWhisper\u8fd8\u5c06\u5728\u6bcf\u4e2a\u8bed\u97f3\u90e8\u5206\u4e4b\u95f4\u7684\u95f4\u9699\u4e0a\u8fd0\u884c\uff0c\u65b9\u6cd5\u662f\u5c06\u8be5\u90e8\u5206\u6269\u5c55\u5230\u6700\u5927\u5408\u5e76\u5927\u5c0f\u6216\u5728\u975e\u8bed\u97f3\u90e8\u5206\u4e0a\u72ec\u7acb\u8fd0\u884cWhisper\u3002\n        * silero-vad-expand-into-gaps\n        * \u4f7f\u7528Silero VAD\u68c0\u6d4b\u5305\u542b\u8bed\u97f3\u7684\u90e8\u5206\uff0c\u5e76\u5728\u6bcf\u4e2a\u90e8\u5206\u4e0a\u72ec\u7acb\u8fd0\u884cWhisper\u3002\u6bcf\u4e2a\u8bed\u97f3\u90e8\u5206\u90fd\u5c06\u88ab\u6269\u5c55\u4ee5\u8986\u76d6\u76f8\u90bb\u7684\u975e\u8bed\u97f3\u90e8\u5206\u3002\u4f8b\u5982\uff0c\u5982\u679c\u4e00\u4e2a\u6301\u7eed\u4e00\u5206\u949f\u7684\u97f3\u9891\u6587\u4ef6\u5305\u542b\u8bed\u97f3\u90e8\u520600:00-00:10\uff08A\uff09\u548c00:30-00:40\uff08B\uff09\uff0c\u5219\u7b2c\u4e00\u90e8\u5206\uff08A\uff09\u5c06\u6269\u5c55\u523000:00-00:30\uff0c\uff08B\uff09\u5c06\u6269\u5c55\u523000:30-00:60\u3002\n        * silero-vad-skip-gaps\n        * \u4e0e\u4e0a\u8ff0\u76f8\u540c\uff0c\u4f46\u662f\u6839\u636eSilero\u7684\u7ed3\u679c\u8df3\u8fc7\u4e0d\u5305\u542b\u8bed\u97f3\u7684\u90e8\u5206\u3002\u8fd9\u6837\u4f1a\u7a0d\u5fae\u5feb\u4e00\u4e9b\uff0c\u4f46\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5bf9\u8bdd\u88ab\u8df3\u8fc7\u3002\n        * periodic-vad\n        * \u6bcf\u9694\u201cVAD - Max Merge Size\u201d\u79d2\u521b\u5efa\u8bed\u97f3\u90e8\u5206\u3002\u8fd9\u975e\u5e38\u5feb\u901f\u7b80\u5355\uff0c\u4f46\u53ef\u80fd\u4f1a\u5c06\u4e00\u4e2a\u53e5\u5b50\u6216\u5355\u8bcd\u5206\u6210\u4e24\u4e2a\u90e8\u5206\u3002\n\n        ## VAD - \u5408\u5e76\u7a97\u53e3\n\n        \u5982\u679c\u8bbe\u7f6e\u4e86\u8be5\u53c2\u6570\uff0c\u5219\u4efb\u4f55\u76f8\u90bb\u7684\u8bed\u97f3\u90e8\u5206\uff0c\u5176\u65f6\u95f4\u95f4\u9694\u4e0d\u8d85\u8fc7\u8be5\u53c2\u6570\u6240\u8bbe\u7f6e\u7684\u79d2\u6570\uff0c\u5c06\u81ea\u52a8\u5408\u5e76\u3002\n\n        ## VAD - \u6700\u5927\u5408\u5e76\u5927\u5c0f\uff08\u79d2\uff09\n\n        \u5982\u679c\u76f8\u90bb\u7684\u8bed\u97f3\u90e8\u5206\u7684\u957f\u5ea6\u8fbe\u5230\u8be5\u53c2\u6570\u6240\u8bbe\u7f6e\u7684\u79d2\u6570\uff0c\u5219\u7981\u7528\u5b83\u4eec\u7684\u5408\u5e76\u3002\n\n        ## VAD - \u586b\u5145\u65f6\u95f4\uff08\u79d2\uff09\n\n        \u6bcf\u4e2a\u8bed\u97f3\u90e8\u5206\u7684\u5f00\u5934\u548c\u7ed3\u5c3e\u589e\u52a0\u7684\u79d2\u6570\uff08\u6d6e\u70b9\u6570\uff09\u3002\u5c06\u5176\u8bbe\u7f6e\u4e3a\u5927\u4e8e\u96f6\u7684\u6570\u5b57\u53ef\u786e\u4fddWhisper\u66f4\u6709\u53ef\u80fd\u5728\u8bed\u97f3\u90e8\u5206\u5f00\u5934\u6b63\u786e\u8f6c\u5f55\u53e5\u5b50\u3002\u7136\u800c\uff0c\u8fd9\u4e5f\u589e\u52a0\u4e86Whisper\u7ed9\u6bcf\u4e2a\u8f6c\u5f55\u884c\u5206\u914d\u9519\u8bef\u65f6\u95f4\u6233\u7684\u6982\u7387\u3002\u9ed8\u8ba4\u503c\u4e3a1\u79d2\u3002\n\n\n        ## VAD - \u63d0\u793a\u7a97\u53e3\uff08\u79d2\uff09\n\n        \u5982\u679c\u8bed\u97f3\u90e8\u5206\u5f00\u59cb\u7684\u65f6\u95f4\u548c\u4e0a\u4e00\u884c\u7ed3\u675f\u7684\u65f6\u95f4\u95f4\u9694\u4e0d\u8d85\u8fc7\u8be5\u53c2\u6570\u6240\u8bbe\u7f6e\u7684\u79d2\u6570\uff0c\u5219\u68c0\u6d4b\u5230\u7684\u884c\u7684\u6587\u672c\u5c06\u4f5c\u4e3a\u63d0\u793a\u5305\u542b\u5728\u4e0b\u4e00\u4e2a\u8bed\u97f3\u90e8\u5206\u4e2d\u3002\u4f8b\u5982\uff0c\u5982\u679c\u4e00\u884c\u7ed3\u675f\u4e8e10:00\uff0c\u4e0b\u4e00\u4e2a\u8bed\u97f3\u90e8\u5206\u4ece10:04\u5f00\u59cb\uff0c\u5219\u5982\u679c\u63d0\u793a\u7a97\u53e3\u4e3a4\u79d2\u6216\u66f4\u957f\u65f6\u95f4\uff0810:04-10:00 = 4\u79d2\uff09\uff0c\u5c06\u5305\u542b\u8be5\u884c\u7684\u6587\u672c\u3002\n\n        \u8bf7\u6ce8\u610f\uff0c\u5982\u679c\u4f7f\u7528silero-vad\u6216silero-vad-expand-into-gaps\uff0c\u5219\u4e0d\u4f1a\u5c06\u68c0\u6d4b\u5230\u7684\u884c\u5305\u542b\u5728\u8bed\u97f3\u90e8\u5206\u4e4b\u95f4\u7684\u95f4\u9699\u4e2d\u7684\u63d0\u793a\u4e2d\u3002\n\n        # \u547d\u4ee4\u884c\u9009\u9879\n\n        app.py\u548ccli.py\u90fd\u63a5\u53d7\u547d\u4ee4\u884c\u9009\u9879\uff0c\u4f8b\u5982\u542f\u7528\u591a\u4e2aCPU / GPU\u6838\u5fc3\u4e0a\u7684\u5e76\u884c\u6267\u884c\uff0c\u8bbe\u7f6e\u9ed8\u8ba4\u6a21\u578b\u540d\u79f0/VAD\u7b49\u3002\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u67e5\u9605\u6839\u6587\u4ef6\u5939\u4e2d\u7684README\u6587\u4ef6\u3002\n\n\n        # \u5176\u4ed6\u9009\u9879\n\n        \u9664\u4e0a\u8ff0\u9009\u9879\u5916\uff0c\u8fd8\u6709\u4e00\u4e2a\u201c\u5b8c\u6574\u201d\u7684\u9009\u9879\u754c\u9762\uff0c\u5141\u8bb8\u60a8\u8bbe\u7f6eWhisper\u6a21\u578b\u4e2d\u6240\u6709\u53ef\u7528\u7684\u9009\u9879\u3002\u8fd9\u4e9b\u9009\u9879\u5982\u4e0b\uff1a\n\n        ## \u521d\u59cb\u63d0\u793a\n\n        \u63d0\u4f9b\u53ef\u9009\u6587\u672c\u4f5c\u4e3a\u524d30\u79d2\u7a97\u53e3\u7684\u63d0\u793a\u3002Whisper\u5c06\u5c1d\u8bd5\u4f7f\u7528\u6b64\u6587\u672c\u4f5c\u4e3a\u8f6c\u5f55\u7684\u8d77\u70b9\uff0c\u4f46\u60a8\u4e5f\u53ef\u4ee5\u53d1\u6325\u521b\u610f\uff0c\u4e3a\u8f6c\u5f55\u8f93\u51fa\u6307\u5b9a\u6837\u5f0f\u6216\u683c\u5f0f\u3002\n\n        \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u4f7f\u7528\u63d0\u793a\u201chello how is it going always use lowercase no punctuation goodbye one two three start stop i you me they\u201d\uff0c\u5219Whisper\u4f1a\u503e\u5411\u4e8e\u8f93\u51fa\u5c0f\u5199\u5b57\u6bcd\u548c\u65e0\u6807\u70b9\u7b26\u53f7\uff0c\u5e76\u4e14\u53ef\u80fd\u4f1a\u503e\u5411\u4e8e\u66f4\u9891\u7e41\u5730\u8f93\u51fa\u63d0\u793a\u4e2d\u7684\u5355\u8bcd\u3002\n\n        ## \u6e29\u5ea6\n\n        \u8fdb\u884c\u91c7\u6837\u65f6\u4f7f\u7528\u7684\u6e29\u5ea6\u3002\u9ed8\u8ba4\u4e3a0\uff08\u96f6\uff09\u3002\u8f83\u9ad8\u7684\u6e29\u5ea6\u4f1a\u4ea7\u751f\u66f4\u591a\u968f\u673a\u8f93\u51fa\uff0c\u800c\u8f83\u4f4e\u7684\u6e29\u5ea6\u5219\u66f4\u52a0\u786e\u5b9a\u6027\u3002\n\n\n        ## \u6700\u4f73\u7ed3\u679c - \u975e\u96f6\u6e29\u5ea6\n\n        \u4f7f\u7528\u975e\u96f6\u6e29\u5ea6\u8fdb\u884c\u91c7\u6837\u65f6\u8981\u4ece\u4e2d\u91c7\u6837\u7684\u5019\u9009\u9879\u6570\u91cf\u3002\u9ed8\u8ba4\u503c\u4e3a5\u3002\n\n        ## \u675f\u641c\u7d22\u5927\u5c0f - \u96f6\u6e29\u5ea6\n\n       \u4f7f\u7528\u96f6\u6e29\u5ea6\u8fdb\u884c\u91c7\u6837\u65f6\u5728\u675f\u641c\u7d22\u4e2d\u4f7f\u7528\u7684\u675f\u6570\u3002\u9ed8\u8ba4\u503c\u4e3a5\u3002\n\n        ## \u8010\u5fc3 - \u96f6\u6e29\u5ea6\n\n        \u5728\u96f6\u6e29\u5ea6\u4e0b\u8fdb\u884c\u91c7\u6837\u65f6\uff0c\u5728\u675f\u641c\u7d22\u4e2d\u4f7f\u7528\u7684\u8010\u5fc3\u503c\u3002\u5982https://arxiv.org/abs/2204.05424 \u6240\u8ff0\uff0c \u9ed8\u8ba4\u503c\uff081.0\uff09\u7b49\u6548\u4e8e\u4f20\u7edf\u7684\u675f\u641c\u7d22\u3002\n\n        ## \u957f\u5ea6\u60e9\u7f5a - \u4efb\u4f55\u6e29\u5ea6\n\n        \u5728\u4efb\u4f55\u6e29\u5ea6\u4e0b\u8fdb\u884c\u91c7\u6837\u65f6\u4f7f\u7528\u7684\u6807\u8bb0\u957f\u5ea6\u60e9\u7f5a\u7cfb\u6570\uff08alpha\uff09\u3002\u5982https://arxiv.org/abs/1609.08144 \u6240\u8ff0\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4f7f\u7528\u7b80\u5355\u7684\u957f\u5ea6\u5f52\u4e00\u5316\u3002\n\n        ## \u6291\u5236\u6807\u8bb0 - \u9017\u53f7\u5206\u9694\u7684\u6807\u8bb0ID\u5217\u8868\n\n        \u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u6291\u5236\u7684\u6807\u8bb0ID\u7684\u9017\u53f7\u5206\u9694\u5217\u8868\u3002\u9ed8\u8ba4\u503c\u201c-1\u201d\u5c06\u6291\u5236\u9664\u5e38\u89c1\u6807\u70b9\u7b26\u53f7\u4ee5\u5916\u7684\u5927\u591a\u6570\u7279\u6b8a\u5b57\u7b26\u3002\n\n        ## \u4ee5\u524d\u6587\u4e3a\u6761\u4ef6\n\n        \u5982\u679c\u4e3aTrue\uff0c\u5219\u5c06\u6a21\u578b\u7684\u5148\u524d\u8f93\u51fa\u63d0\u4f9b\u4e3a\u4e0b\u4e00\u4e2a\u7a97\u53e3\u7684\u63d0\u793a\u3002\u7981\u7528\u6b64\u9009\u9879\u53ef\u80fd\u4f1a\u5bfc\u81f4\u7a97\u53e3\u4e4b\u95f4\u7684\u6587\u672c\u4e0d\u4e00\u81f4\uff0c\u4f46\u6a21\u578b\u4e0d\u592a\u5bb9\u6613\u9677\u5165\u5931\u8d25\u5faa\u73af\u4e2d\u3002\n\n        ## FP16\n\n        \u662f\u5426\u5728fp16\u4e0b\u6267\u884c\u63a8\u7406\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4e3aTrue\u3002\n\n        ## \u56de\u9000\u65f6\u6e29\u5ea6\u589e\u91cf\n\n        \u5f53\u89e3\u7801\u672a\u8fbe\u5230\u4ee5\u4e0b\u4efb\u4e00\u9608\u503c\u65f6\uff0c\u964d\u7ea7\u65f6\u8981\u589e\u52a0\u7684\u6e29\u5ea6\u3002\u9ed8\u8ba4\u4e3a0.2\u3002\n\n\n        ## \u538b\u7f29\u6bd4\u9608\u503c\n\n        \u5982\u679cgzip\u538b\u7f29\u6bd4\u9ad8\u4e8e\u6b64\u503c\uff0c\u5219\u5c06\u89e3\u7801\u89c6\u4e3a\u5931\u8d25\u3002\u9ed8\u8ba4\u4e3a2.4\u3002\n\n        ## Logprob\u9608\u503c\n\n        \u5982\u679c\u5e73\u5747\u5bf9\u6570\u6982\u7387\u4f4e\u4e8e\u6b64\u503c\uff0c\u5219\u5c06\u89e3\u7801\u89c6\u4e3a\u5931\u8d25\u3002\u9ed8\u8ba4\u4e3a-1.0\u3002\n\n\n        ## \u65e0\u8bed\u97f3\u9608\u503c\n\n        \u5982\u679c\u201c<|nospeech|>\u201d\u6807\u8bb0\u7684\u6982\u7387\u9ad8\u4e8e\u6b64\u503c\u4e14\u7531\u4e8e\u201clogprob_threshold\u201d\u800c\u89e3\u7801\u5931\u8d25\uff0c\u5219\u5c06\u8be5\u6bb5\u89c6\u4e3a\u9759\u97f3\u3002\u9ed8\u8ba4\u4e3a0.6\u3002\n    \"\"\"\n    return description ", ""]}
{"filename": "src/config.py", "chunked_list": ["from enum import Enum\nimport urllib\n\nimport os\nfrom typing import List\nfrom urllib.parse import urlparse\nimport json5\nimport torch\n\nfrom tqdm import tqdm", "\nfrom tqdm import tqdm\n\nclass ModelConfig:\n    def __init__(self, name: str, url: str, path: str = None, type: str = \"whisper\"):\n        \"\"\"\n        Initialize a model configuration.\n\n        name: Name of the model\n        url: URL to download the model from\n        path: Path to the model file. If not set, the model will be downloaded from the URL.\n        type: Type of model. Can be whisper or huggingface.\n        \"\"\"\n        self.name = name\n        self.url = url\n        self.path = path\n        self.type = type", "\nclass VadInitialPromptMode(Enum):\n    PREPEND_ALL_SEGMENTS = 1\n    PREPREND_FIRST_SEGMENT = 2\n\n    @staticmethod\n    def from_string(s: str):\n        normalized = s.lower() if s is not None else None\n\n        if normalized == \"prepend_all_segments\":\n            return VadInitialPromptMode.PREPEND_ALL_SEGMENTS\n        elif normalized == \"prepend_first_segment\":\n            return VadInitialPromptMode.PREPREND_FIRST_SEGMENT\n        else:\n            raise ValueError(f\"Invalid value for VadInitialPromptMode: {s}\")", "\nclass ApplicationConfig:\n    def __init__(self, models: List[ModelConfig] = [], input_audio_max_duration: int = 600, \n                 share: bool = False, server_name: str = None, server_port: int = 7860, \n                 queue_concurrency_count: int = 1, delete_uploaded_files: bool = True,\n                 whisper_implementation: str = \"whisper\",\n                 default_model_name: str = \"medium\", default_vad: str = \"silero-vad\", \n                 vad_parallel_devices: str = \"\", vad_cpu_cores: int = 1, vad_process_timeout: int = 1800, \n                 auto_parallel: bool = False, output_dir: str = None,\n                 model_dir: str = None, device: str = None, \n                 verbose: bool = True, task: str = \"transcribe\", language: str = None,\n                 vad_initial_prompt_mode: str = \"prepend_first_segment \", \n                 vad_merge_window: float = 5, vad_max_merge_size: float = 30,\n                 vad_padding: float = 1, vad_prompt_window: float = 3,\n                 temperature: float = 0, best_of: int = 5, beam_size: int = 5,\n                 patience: float = None, length_penalty: float = None,\n                 suppress_tokens: str = \"-1\", initial_prompt: str = None,\n                 condition_on_previous_text: bool = True, fp16: bool = True,\n                 compute_type: str = \"float16\", \n                 temperature_increment_on_fallback: float = 0.2, compression_ratio_threshold: float = 2.4,\n                 logprob_threshold: float = -1.0, no_speech_threshold: float = 0.6):\n        \n        self.models = models\n        \n        # WebUI settings\n        self.input_audio_max_duration = input_audio_max_duration\n        self.share = share\n        self.server_name = server_name\n        self.server_port = server_port\n        self.queue_concurrency_count = queue_concurrency_count\n        self.delete_uploaded_files = delete_uploaded_files\n\n        self.whisper_implementation = whisper_implementation\n        self.default_model_name = default_model_name\n        self.default_vad = default_vad\n        self.vad_parallel_devices = vad_parallel_devices\n        self.vad_cpu_cores = vad_cpu_cores\n        self.vad_process_timeout = vad_process_timeout\n        self.auto_parallel = auto_parallel\n        self.output_dir = output_dir\n\n        self.model_dir = model_dir\n        self.device = device\n        self.verbose = verbose\n        self.task = task\n        self.language = language\n        self.vad_initial_prompt_mode = vad_initial_prompt_mode\n        self.vad_merge_window = vad_merge_window\n        self.vad_max_merge_size = vad_max_merge_size\n        self.vad_padding = vad_padding\n        self.vad_prompt_window = vad_prompt_window\n        self.temperature = temperature\n        self.best_of = best_of\n        self.beam_size = beam_size\n        self.patience = patience\n        self.length_penalty = length_penalty\n        self.suppress_tokens = suppress_tokens\n        self.initial_prompt = initial_prompt\n        self.condition_on_previous_text = condition_on_previous_text\n        self.fp16 = fp16\n        self.compute_type = compute_type\n        self.temperature_increment_on_fallback = temperature_increment_on_fallback\n        self.compression_ratio_threshold = compression_ratio_threshold\n        self.logprob_threshold = logprob_threshold\n        self.no_speech_threshold = no_speech_threshold\n        \n    def get_model_names(self):\n        return [ x.name for x in self.models ]\n\n    def update(self, **new_values):\n        result = ApplicationConfig(**self.__dict__)\n\n        for key, value in new_values.items():\n            setattr(result, key, value)\n        return result\n\n    @staticmethod\n    def create_default(**kwargs):\n        app_config = ApplicationConfig.parse_file(os.environ.get(\"WHISPER_WEBUI_CONFIG\", \"config.json5\"))\n\n        # Update with kwargs\n        if len(kwargs) > 0:\n            app_config = app_config.update(**kwargs)\n        return app_config\n\n    @staticmethod\n    def parse_file(config_path: str):\n        import json5\n\n        with open(config_path, \"r\") as f:\n            # Load using json5\n            data = json5.load(f)\n            data_models = data.pop(\"models\", [])\n\n            models = [ ModelConfig(**x) for x in data_models ]\n\n            return ApplicationConfig(models, **data)", ""]}
{"filename": "src/__init__.py", "chunked_list": [""]}
{"filename": "src/utils.py", "chunked_list": ["import textwrap\nimport unicodedata\nimport re\n\nimport zlib\nfrom typing import Iterator, TextIO\nimport tqdm\n\nimport urllib3\n", "import urllib3\n\n\ndef exact_div(x, y):\n    assert x % y == 0\n    return x // y\n\n\ndef str2bool(string):\n    str2val = {\"True\": True, \"False\": False}\n    if string in str2val:\n        return str2val[string]\n    else:\n        raise ValueError(f\"Expected one of {set(str2val.keys())}, got {string}\")", "def str2bool(string):\n    str2val = {\"True\": True, \"False\": False}\n    if string in str2val:\n        return str2val[string]\n    else:\n        raise ValueError(f\"Expected one of {set(str2val.keys())}, got {string}\")\n\n\ndef optional_int(string):\n    return None if string == \"None\" else int(string)", "def optional_int(string):\n    return None if string == \"None\" else int(string)\n\n\ndef optional_float(string):\n    return None if string == \"None\" else float(string)\n\n\ndef compression_ratio(text) -> float:\n    return len(text) / len(zlib.compress(text.encode(\"utf-8\")))", "def compression_ratio(text) -> float:\n    return len(text) / len(zlib.compress(text.encode(\"utf-8\")))\n\n\ndef format_timestamp(seconds: float, always_include_hours: bool = False, fractionalSeperator: str = '.'):\n    assert seconds >= 0, \"non-negative timestamp expected\"\n    milliseconds = round(seconds * 1000.0)\n\n    hours = milliseconds // 3_600_000\n    milliseconds -= hours * 3_600_000\n\n    minutes = milliseconds // 60_000\n    milliseconds -= minutes * 60_000\n\n    seconds = milliseconds // 1_000\n    milliseconds -= seconds * 1_000\n\n    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n    return f\"{hours_marker}{minutes:02d}:{seconds:02d}{fractionalSeperator}{milliseconds:03d}\"", "\n\ndef write_txt(transcript: Iterator[dict], file: TextIO):\n    for segment in transcript:\n        print(segment['text'].strip(), file=file, flush=True)\n\n\ndef write_vtt(transcript: Iterator[dict], file: TextIO, maxLineWidth=None):\n    print(\"WEBVTT\\n\", file=file)\n    for segment in transcript:\n        text = process_text(segment['text'], maxLineWidth).replace('-->', '->')\n\n        print(\n            f\"{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}\\n\"\n            f\"{text}\\n\",\n            file=file,\n            flush=True,\n        )", "\n\ndef write_srt(transcript: Iterator[dict], file: TextIO, maxLineWidth=None):\n    \"\"\"\n    Write a transcript to a file in SRT format.\n    Example usage:\n        from pathlib import Path\n        from whisper.utils import write_srt\n        result = transcribe(model, audio_path, temperature=temperature, **args)\n        # save SRT\n        audio_basename = Path(audio_path).stem\n        with open(Path(output_dir) / (audio_basename + \".srt\"), \"w\", encoding=\"utf-8\") as srt:\n            write_srt(result[\"segments\"], file=srt)\n    \"\"\"\n    for i, segment in enumerate(transcript, start=1):\n        text = process_text(segment['text'].strip(), maxLineWidth).replace('-->', '->')\n\n        # write srt lines\n        print(\n            f\"{i}\\n\"\n            f\"{format_timestamp(segment['start'], always_include_hours=True, fractionalSeperator=',')} --> \"\n            f\"{format_timestamp(segment['end'], always_include_hours=True, fractionalSeperator=',')}\\n\"\n            f\"{text}\\n\",\n            file=file,\n            flush=True,\n        )", "\ndef process_text(text: str, maxLineWidth=None):\n    if (maxLineWidth is None or maxLineWidth < 0):\n        return text\n\n    lines = textwrap.wrap(text, width=maxLineWidth, tabsize=4)\n    return '\\n'.join(lines)\n\ndef slugify(value, allow_unicode=False):\n    \"\"\"\n    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n    dashes to single dashes. Remove characters that aren't alphanumerics,\n    underscores, or hyphens. Convert to lowercase. Also strip leading and\n    trailing whitespace, dashes, and underscores.\n    \"\"\"\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize('NFKC', value)\n    else:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n    return re.sub(r'[-\\s]+', '-', value).strip('-_')", "def slugify(value, allow_unicode=False):\n    \"\"\"\n    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n    dashes to single dashes. Remove characters that aren't alphanumerics,\n    underscores, or hyphens. Convert to lowercase. Also strip leading and\n    trailing whitespace, dashes, and underscores.\n    \"\"\"\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize('NFKC', value)\n    else:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n    return re.sub(r'[-\\s]+', '-', value).strip('-_')", "\ndef download_file(url: str, destination: str):\n        with urllib3.request.urlopen(url) as source, open(destination, \"wb\") as output:\n            with tqdm(\n                total=int(source.info().get(\"Content-Length\")),\n                ncols=80,\n                unit=\"iB\",\n                unit_scale=True,\n                unit_divisor=1024,\n            ) as loop:\n                while True:\n                    buffer = source.read(8192)\n                    if not buffer:\n                        break\n\n                    output.write(buffer)\n                    loop.update(len(buffer))"]}
{"filename": "src/download.py", "chunked_list": ["from tempfile import mkdtemp\nfrom typing import List\nfrom yt_dlp import YoutubeDL\n\nimport yt_dlp\nfrom yt_dlp.postprocessor import PostProcessor\n\nclass FilenameCollectorPP(PostProcessor):\n    def __init__(self):\n        super(FilenameCollectorPP, self).__init__(None)\n        self.filenames = []\n\n    def run(self, information):\n        self.filenames.append(information[\"filepath\"])\n        return [], information", "\ndef download_url(url: str, maxDuration: int = None, destinationDirectory: str = None, playlistItems: str = \"1\") -> List[str]: \n    try:\n        return _perform_download(url, maxDuration=maxDuration, outputTemplate=None, destinationDirectory=destinationDirectory, playlistItems=playlistItems)\n    except yt_dlp.utils.DownloadError as e:\n        # In case of an OS error, try again with a different output template\n        if e.msg and e.msg.find(\"[Errno 36] File name too long\") >= 0:\n            return _perform_download(url, maxDuration=maxDuration, outputTemplate=\"%(title).10s %(id)s.%(ext)s\")\n        pass\n\ndef _perform_download(url: str, maxDuration: int = None, outputTemplate: str = None, destinationDirectory: str = None, playlistItems: str = \"1\"):\n    # Create a temporary directory to store the downloaded files\n    if destinationDirectory is None:\n        destinationDirectory = mkdtemp()\n\n    ydl_opts = {\n        \"format\": \"bestaudio/best\",\n        'paths': {\n            'home': destinationDirectory\n        }\n    }\n    if (playlistItems):\n        ydl_opts['playlist_items'] = playlistItems\n\n    # Add output template if specified\n    if outputTemplate:\n        ydl_opts['outtmpl'] = outputTemplate\n\n    filename_collector = FilenameCollectorPP()\n\n    with YoutubeDL(ydl_opts) as ydl:\n        if maxDuration and maxDuration > 0:\n            info = ydl.extract_info(url, download=False)\n            entries = \"entries\" in info and info[\"entries\"] or [info]\n\n            total_duration = 0\n\n            # Compute total duration\n            for entry in entries:\n                total_duration += float(entry[\"duration\"])\n\n            if total_duration >= maxDuration:\n                raise ExceededMaximumDuration(videoDuration=total_duration, maxDuration=maxDuration, message=\"Video is too long\")\n\n        ydl.add_post_processor(filename_collector)\n        ydl.download([url])\n\n    if len(filename_collector.filenames) <= 0:\n        raise Exception(\"Cannot download \" + url)\n\n    result = []\n\n    for filename in filename_collector.filenames:\n        result.append(filename)\n        print(\"Downloaded \" + filename)\n\n    return result ", "\ndef _perform_download(url: str, maxDuration: int = None, outputTemplate: str = None, destinationDirectory: str = None, playlistItems: str = \"1\"):\n    # Create a temporary directory to store the downloaded files\n    if destinationDirectory is None:\n        destinationDirectory = mkdtemp()\n\n    ydl_opts = {\n        \"format\": \"bestaudio/best\",\n        'paths': {\n            'home': destinationDirectory\n        }\n    }\n    if (playlistItems):\n        ydl_opts['playlist_items'] = playlistItems\n\n    # Add output template if specified\n    if outputTemplate:\n        ydl_opts['outtmpl'] = outputTemplate\n\n    filename_collector = FilenameCollectorPP()\n\n    with YoutubeDL(ydl_opts) as ydl:\n        if maxDuration and maxDuration > 0:\n            info = ydl.extract_info(url, download=False)\n            entries = \"entries\" in info and info[\"entries\"] or [info]\n\n            total_duration = 0\n\n            # Compute total duration\n            for entry in entries:\n                total_duration += float(entry[\"duration\"])\n\n            if total_duration >= maxDuration:\n                raise ExceededMaximumDuration(videoDuration=total_duration, maxDuration=maxDuration, message=\"Video is too long\")\n\n        ydl.add_post_processor(filename_collector)\n        ydl.download([url])\n\n    if len(filename_collector.filenames) <= 0:\n        raise Exception(\"Cannot download \" + url)\n\n    result = []\n\n    for filename in filename_collector.filenames:\n        result.append(filename)\n        print(\"Downloaded \" + filename)\n\n    return result ", "\nclass ExceededMaximumDuration(Exception):\n    def __init__(self, videoDuration, maxDuration, message):\n        self.videoDuration = videoDuration\n        self.maxDuration = maxDuration\n        super().__init__(message)"]}
{"filename": "src/languages.py", "chunked_list": ["class Language():\n    def __init__(self, code, name):\n        self.code = code\n        self.name = name\n\n    def __str__(self):\n        return \"Language(code={}, name={})\".format(self.code, self.name)\n\nLANGUAGES = [\n    Language('en', 'English'),", "LANGUAGES = [\n    Language('en', 'English'),\n    Language('zh', 'Chinese'),   \n    Language('de', 'German'),    \n    Language('es', 'Spanish'),   \n    Language('ru', 'Russian'),   \n    Language('ko', 'Korean'),    \n    Language('fr', 'French'),    \n    Language('ja', 'Japanese'),  \n    Language('pt', 'Portuguese'),", "    Language('ja', 'Japanese'),  \n    Language('pt', 'Portuguese'),\n    Language('tr', 'Turkish'),   \n    Language('pl', 'Polish'),    \n    Language('ca', 'Catalan'),   \n    Language('nl', 'Dutch'),     \n    Language('ar', 'Arabic'),\n    Language('sv', 'Swedish'),\n    Language('it', 'Italian'),\n    Language('id', 'Indonesian'),", "    Language('it', 'Italian'),\n    Language('id', 'Indonesian'),\n    Language('hi', 'Hindi'),\n    Language('fi', 'Finnish'),\n    Language('vi', 'Vietnamese'),\n    Language('he', 'Hebrew'),\n    Language('uk', 'Ukrainian'),\n    Language('el', 'Greek'),\n    Language('ms', 'Malay'),\n    Language('cs', 'Czech'),", "    Language('ms', 'Malay'),\n    Language('cs', 'Czech'),\n    Language('ro', 'Romanian'),\n    Language('da', 'Danish'),\n    Language('hu', 'Hungarian'),\n    Language('ta', 'Tamil'),\n    Language('no', 'Norwegian'),\n    Language('th', 'Thai'),\n    Language('ur', 'Urdu'),\n    Language('hr', 'Croatian'),", "    Language('ur', 'Urdu'),\n    Language('hr', 'Croatian'),\n    Language('bg', 'Bulgarian'),\n    Language('lt', 'Lithuanian'),\n    Language('la', 'Latin'),\n    Language('mi', 'Maori'),\n    Language('ml', 'Malayalam'),\n    Language('cy', 'Welsh'),\n    Language('sk', 'Slovak'),\n    Language('te', 'Telugu'),", "    Language('sk', 'Slovak'),\n    Language('te', 'Telugu'),\n    Language('fa', 'Persian'),\n    Language('lv', 'Latvian'),\n    Language('bn', 'Bengali'),\n    Language('sr', 'Serbian'),\n    Language('az', 'Azerbaijani'),\n    Language('sl', 'Slovenian'),\n    Language('kn', 'Kannada'),\n    Language('et', 'Estonian'),", "    Language('kn', 'Kannada'),\n    Language('et', 'Estonian'),\n    Language('mk', 'Macedonian'),\n    Language('br', 'Breton'),\n    Language('eu', 'Basque'),\n    Language('is', 'Icelandic'),\n    Language('hy', 'Armenian'),\n    Language('ne', 'Nepali'),\n    Language('mn', 'Mongolian'),\n    Language('bs', 'Bosnian'),", "    Language('mn', 'Mongolian'),\n    Language('bs', 'Bosnian'),\n    Language('kk', 'Kazakh'),\n    Language('sq', 'Albanian'),\n    Language('sw', 'Swahili'),\n    Language('gl', 'Galician'),\n    Language('mr', 'Marathi'),\n    Language('pa', 'Punjabi'),\n    Language('si', 'Sinhala'),\n    Language('km', 'Khmer'),", "    Language('si', 'Sinhala'),\n    Language('km', 'Khmer'),\n    Language('sn', 'Shona'),\n    Language('yo', 'Yoruba'),\n    Language('so', 'Somali'),\n    Language('af', 'Afrikaans'),\n    Language('oc', 'Occitan'),\n    Language('ka', 'Georgian'),\n    Language('be', 'Belarusian'),\n    Language('tg', 'Tajik'),", "    Language('be', 'Belarusian'),\n    Language('tg', 'Tajik'),\n    Language('sd', 'Sindhi'),\n    Language('gu', 'Gujarati'),\n    Language('am', 'Amharic'),\n    Language('yi', 'Yiddish'),\n    Language('lo', 'Lao'),\n    Language('uz', 'Uzbek'),\n    Language('fo', 'Faroese'),\n    Language('ht', 'Haitian creole'),", "    Language('fo', 'Faroese'),\n    Language('ht', 'Haitian creole'),\n    Language('ps', 'Pashto'),\n    Language('tk', 'Turkmen'),\n    Language('nn', 'Nynorsk'),\n    Language('mt', 'Maltese'),\n    Language('sa', 'Sanskrit'),\n    Language('lb', 'Luxembourgish'),\n    Language('my', 'Myanmar'),\n    Language('bo', 'Tibetan'),", "    Language('my', 'Myanmar'),\n    Language('bo', 'Tibetan'),\n    Language('tl', 'Tagalog'),\n    Language('mg', 'Malagasy'),\n    Language('as', 'Assamese'),\n    Language('tt', 'Tatar'),\n    Language('haw', 'Hawaiian'),\n    Language('ln', 'Lingala'),\n    Language('ha', 'Hausa'),\n    Language('ba', 'Bashkir'),", "    Language('ha', 'Hausa'),\n    Language('ba', 'Bashkir'),\n    Language('jw', 'Javanese'),\n    Language('su', 'Sundanese')\n]\n\n_TO_LANGUAGE_CODE = {\n    **{language.code: language for language in LANGUAGES},\n    \"burmese\": \"my\",\n    \"valencian\": \"ca\",", "    \"burmese\": \"my\",\n    \"valencian\": \"ca\",\n    \"flemish\": \"nl\",\n    \"haitian\": \"ht\",\n    \"letzeburgesch\": \"lb\",\n    \"pushto\": \"ps\",\n    \"panjabi\": \"pa\",\n    \"moldavian\": \"ro\",\n    \"moldovan\": \"ro\",\n    \"sinhalese\": \"si\",", "    \"moldovan\": \"ro\",\n    \"sinhalese\": \"si\",\n    \"castilian\": \"es\",\n}\n    \n_FROM_LANGUAGE_NAME = {\n    **{language.name.lower(): language for language in LANGUAGES}\n}\n\ndef get_language_from_code(language_code, default=None) -> Language:\n    \"\"\"Return the language name from the language code.\"\"\"\n    return _TO_LANGUAGE_CODE.get(language_code, default)", "\ndef get_language_from_code(language_code, default=None) -> Language:\n    \"\"\"Return the language name from the language code.\"\"\"\n    return _TO_LANGUAGE_CODE.get(language_code, default)\n\ndef get_language_from_name(language, default=None) -> Language:\n    \"\"\"Return the language code from the language name.\"\"\"\n    return _FROM_LANGUAGE_NAME.get(language.lower() if language else None, default)\n\ndef get_language_names():\n    \"\"\"Return a list of language names.\"\"\"\n    return [language.name for language in LANGUAGES]", "\ndef get_language_names():\n    \"\"\"Return a list of language names.\"\"\"\n    return [language.name for language in LANGUAGES]\n\nif __name__ == \"__main__\":\n    # Test lookup\n    print(get_language_from_code('en'))\n    print(get_language_from_name('English'))\n    \n    print(get_language_names())"]}
{"filename": "src/modelCache.py", "chunked_list": ["class ModelCache:\n    def __init__(self):\n        self._cache = dict()\n\n    def get(self, model_key: str, model_factory):\n        result = self._cache.get(model_key)\n\n        if result is None:\n            result = model_factory()\n            self._cache[model_key] = result\n        return result\n\n    def clear(self):\n        self._cache.clear()", "\n# A global cache of models. This is mainly used by the daemon processes to avoid loading the same model multiple times.\nGLOBAL_MODEL_CACHE = ModelCache()"]}
{"filename": "src/source.py", "chunked_list": ["# Gradio seems to truncate files without keeping the extension, so we need to truncate the file prefix ourself \nimport os\nimport pathlib\nfrom typing import List\nimport zipfile\n\nimport ffmpeg\nfrom more_itertools import unzip\n\nfrom src.download import ExceededMaximumDuration, download_url", "\nfrom src.download import ExceededMaximumDuration, download_url\n\nMAX_FILE_PREFIX_LENGTH = 17\n\nclass AudioSource:\n    def __init__(self, source_path, source_name = None, audio_duration = None):\n        self.source_path = source_path\n        self.source_name = source_name\n        self._audio_duration = audio_duration\n\n        # Load source name if not provided\n        if (self.source_name is None):\n            file_path = pathlib.Path(self.source_path)\n            self.source_name = file_path.name\n\n    def get_audio_duration(self):\n        if self._audio_duration is None:\n            self._audio_duration = float(ffmpeg.probe(self.source_path)[\"format\"][\"duration\"])\n\n        return self._audio_duration\n\n    def get_full_name(self):\n        return self.source_name\n\n    def get_short_name(self, max_length: int = MAX_FILE_PREFIX_LENGTH):\n        file_path = pathlib.Path(self.source_name)\n        short_name = file_path.stem[:max_length] + file_path.suffix\n\n        return short_name\n\n    def __str__(self) -> str:\n        return self.source_path", "\nclass AudioSourceCollection:\n    def __init__(self, sources: List[AudioSource]):\n        self.sources = sources\n\n    def __iter__(self):\n        return iter(self.sources)\n\ndef get_audio_source_collection(urlData: str, multipleFiles: List, microphoneData: str, input_audio_max_duration: float = -1) -> List[AudioSource]:\n    output: List[AudioSource] = []\n\n    if urlData:\n        # Download from YouTube. This could also be a playlist or a channel.\n        output.extend([ AudioSource(x) for x in download_url(urlData, input_audio_max_duration, playlistItems=None) ])\n    else:\n        # Add input files\n        if (multipleFiles is not None):\n            output.extend([ AudioSource(x.name) for x in multipleFiles ])\n        if (microphoneData is not None):\n            output.append(AudioSource(microphoneData))\n\n    total_duration = 0\n\n    # Calculate total audio length. We do this even if input_audio_max_duration\n    # is disabled to ensure that all the audio files are valid.\n    for source in output:\n        audioDuration = ffmpeg.probe(source.source_path)[\"format\"][\"duration\"]\n        total_duration += float(audioDuration)\n        \n        # Save audio duration\n        source._audio_duration = float(audioDuration)\n\n    # Ensure the total duration of the audio is not too long\n    if input_audio_max_duration > 0:\n        if float(total_duration) > input_audio_max_duration:\n            raise ExceededMaximumDuration(videoDuration=total_duration, maxDuration=input_audio_max_duration, message=\"Video(s) is too long\")\n\n    # Return a list of audio sources\n    return output", "def get_audio_source_collection(urlData: str, multipleFiles: List, microphoneData: str, input_audio_max_duration: float = -1) -> List[AudioSource]:\n    output: List[AudioSource] = []\n\n    if urlData:\n        # Download from YouTube. This could also be a playlist or a channel.\n        output.extend([ AudioSource(x) for x in download_url(urlData, input_audio_max_duration, playlistItems=None) ])\n    else:\n        # Add input files\n        if (multipleFiles is not None):\n            output.extend([ AudioSource(x.name) for x in multipleFiles ])\n        if (microphoneData is not None):\n            output.append(AudioSource(microphoneData))\n\n    total_duration = 0\n\n    # Calculate total audio length. We do this even if input_audio_max_duration\n    # is disabled to ensure that all the audio files are valid.\n    for source in output:\n        audioDuration = ffmpeg.probe(source.source_path)[\"format\"][\"duration\"]\n        total_duration += float(audioDuration)\n        \n        # Save audio duration\n        source._audio_duration = float(audioDuration)\n\n    # Ensure the total duration of the audio is not too long\n    if input_audio_max_duration > 0:\n        if float(total_duration) > input_audio_max_duration:\n            raise ExceededMaximumDuration(videoDuration=total_duration, maxDuration=input_audio_max_duration, message=\"Video(s) is too long\")\n\n    # Return a list of audio sources\n    return output"]}
{"filename": "src/hooks/progressListener.py", "chunked_list": ["from typing import Union\n\nclass ProgressListener:\n    def on_progress(self, current: Union[int, float], total: Union[int, float]):\n        self.total = total\n\n    def on_finished(self):\n        pass"]}
{"filename": "src/hooks/whisperProgressHook.py", "chunked_list": ["import sys\nimport threading\nfrom typing import List, Union\nimport tqdm\n\nfrom src.hooks.progressListener import ProgressListener\n\nclass ProgressListenerHandle:\n    def __init__(self, listener: ProgressListener):\n        self.listener = listener\n    \n    def __enter__(self):\n        register_thread_local_progress_listener(self.listener)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        unregister_thread_local_progress_listener(self.listener)\n        \n        if exc_type is None:\n            self.listener.on_finished()", "\nclass _CustomProgressBar(tqdm.tqdm):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._current = self.n  # Set the initial value\n\n    def update(self, n):\n        super().update(n)\n        # Because the progress bar might be disabled, we need to manually update the progress\n        self._current += n\n\n        # Inform listeners\n        listeners = _get_thread_local_listeners()\n\n        for listener in listeners:\n            listener.on_progress(self._current, self.total)", "\n_thread_local = threading.local()\n\ndef _get_thread_local_listeners():\n    if not hasattr(_thread_local, 'listeners'):\n        _thread_local.listeners = []\n    return _thread_local.listeners\n\n_hooked = False\n\ndef init_progress_hook():\n    global _hooked\n\n    if _hooked:\n        return\n\n    # Inject into tqdm.tqdm of Whisper, so we can see progress\n    import whisper.transcribe \n    transcribe_module = sys.modules['whisper.transcribe']\n    transcribe_module.tqdm.tqdm = _CustomProgressBar\n    _hooked = True", "_hooked = False\n\ndef init_progress_hook():\n    global _hooked\n\n    if _hooked:\n        return\n\n    # Inject into tqdm.tqdm of Whisper, so we can see progress\n    import whisper.transcribe \n    transcribe_module = sys.modules['whisper.transcribe']\n    transcribe_module.tqdm.tqdm = _CustomProgressBar\n    _hooked = True", "\ndef register_thread_local_progress_listener(progress_listener: ProgressListener):\n    # This is a workaround for the fact that the progress bar is not exposed in the API\n    init_progress_hook()\n\n    listeners = _get_thread_local_listeners()\n    listeners.append(progress_listener)\n\ndef unregister_thread_local_progress_listener(progress_listener: ProgressListener):\n    listeners = _get_thread_local_listeners()\n    \n    if progress_listener in listeners:\n        listeners.remove(progress_listener)", "def unregister_thread_local_progress_listener(progress_listener: ProgressListener):\n    listeners = _get_thread_local_listeners()\n    \n    if progress_listener in listeners:\n        listeners.remove(progress_listener)\n\ndef create_progress_listener_handle(progress_listener: ProgressListener):\n    return ProgressListenerHandle(progress_listener)\n\n# Example usage\nif __name__ == '__main__':\n    class PrintingProgressListener:\n        def on_progress(self, current: Union[int, float], total: Union[int, float]):\n            print(f\"Progress: {current}/{total}\")\n\n        def on_finished(self):\n            print(\"Finished\")\n\n    import whisper\n    model = whisper.load_model(\"medium\")\n\n    with create_progress_listener_handle(PrintingProgressListener()) as listener:\n        # Set verbose to None to disable the progress bar, as we are using our own\n        result = model.transcribe(\"J:\\\\Dev\\\\OpenAI\\\\whisper\\\\tests\\\\Noriko\\\\out.mka\", language=\"Japanese\", fp16=False, verbose=None)\n        print(result)\n\n    print(\"Done\")", "\n# Example usage\nif __name__ == '__main__':\n    class PrintingProgressListener:\n        def on_progress(self, current: Union[int, float], total: Union[int, float]):\n            print(f\"Progress: {current}/{total}\")\n\n        def on_finished(self):\n            print(\"Finished\")\n\n    import whisper\n    model = whisper.load_model(\"medium\")\n\n    with create_progress_listener_handle(PrintingProgressListener()) as listener:\n        # Set verbose to None to disable the progress bar, as we are using our own\n        result = model.transcribe(\"J:\\\\Dev\\\\OpenAI\\\\whisper\\\\tests\\\\Noriko\\\\out.mka\", language=\"Japanese\", fp16=False, verbose=None)\n        print(result)\n\n    print(\"Done\")"]}
{"filename": "src/hooks/subTaskProgressListener.py", "chunked_list": ["from src.hooks.progressListener import ProgressListener\n\nfrom typing import Union\n\nclass SubTaskProgressListener(ProgressListener):\n    \"\"\"\n    A sub task listener that reports the progress of a sub task to a base task listener\n    Parameters\n    ----------\n    base_task_listener : ProgressListener\n        The base progress listener to accumulate overall progress in.\n    base_task_total : float\n        The maximum total progress that will be reported to the base progress listener.\n    sub_task_start : float\n        The starting progress of a sub task, in respect to the base progress listener.\n    sub_task_total : float\n        The total amount of progress a sub task will report to the base progress listener.\n    \"\"\"\n    def __init__(\n        self,\n        base_task_listener: ProgressListener,\n        base_task_total: float,\n        sub_task_start: float,\n        sub_task_total: float,\n    ):\n        self.base_task_listener = base_task_listener\n        self.base_task_total = base_task_total\n        self.sub_task_start = sub_task_start\n        self.sub_task_total = sub_task_total\n\n    def on_progress(self, current: Union[int, float], total: Union[int, float]):\n        sub_task_progress_frac = current / total\n        sub_task_progress = self.sub_task_start + self.sub_task_total * sub_task_progress_frac\n        self.base_task_listener.on_progress(sub_task_progress, self.base_task_total)\n\n    def on_finished(self):\n        self.base_task_listener.on_progress(self.sub_task_start + self.sub_task_total, self.base_task_total)"]}
{"filename": "src/whisper/whisperFactory.py", "chunked_list": ["from typing import List\nfrom src import modelCache\nfrom src.config import ModelConfig\nfrom src.whisper.abstractWhisperContainer import AbstractWhisperContainer\n\ndef create_whisper_container(whisper_implementation: str, \n                             model_name: str, device: str = None, compute_type: str = \"float16\",\n                             download_root: str = None,\n                             cache: modelCache = None, models: List[ModelConfig] = []) -> AbstractWhisperContainer:\n    print(\"Creating whisper container for \" + whisper_implementation)\n\n    if (whisper_implementation == \"whisper\"):\n        from src.whisper.whisperContainer import WhisperContainer\n        return WhisperContainer(model_name=model_name, device=device, compute_type=compute_type, download_root=download_root, cache=cache, models=models)\n    elif (whisper_implementation == \"faster-whisper\" or whisper_implementation == \"faster_whisper\"):\n        from src.whisper.fasterWhisperContainer import FasterWhisperContainer\n        return FasterWhisperContainer(model_name=model_name, device=device, compute_type=compute_type, download_root=download_root, cache=cache, models=models)\n    else:\n        raise ValueError(\"Unknown Whisper implementation: \" + whisper_implementation)"]}
{"filename": "src/whisper/whisperContainer.py", "chunked_list": ["# External programs\nimport abc\nimport os\nimport sys\nfrom typing import List\nfrom urllib.parse import urlparse\nimport torch\nimport urllib3\nfrom src.hooks.progressListener import ProgressListener\n", "from src.hooks.progressListener import ProgressListener\n\nimport whisper\nfrom whisper import Whisper\n\nfrom src.config import ModelConfig, VadInitialPromptMode\nfrom src.hooks.whisperProgressHook import create_progress_listener_handle\n\nfrom src.modelCache import GLOBAL_MODEL_CACHE, ModelCache\nfrom src.utils import download_file", "from src.modelCache import GLOBAL_MODEL_CACHE, ModelCache\nfrom src.utils import download_file\nfrom src.whisper.abstractWhisperContainer import AbstractWhisperCallback, AbstractWhisperContainer\n\nclass WhisperContainer(AbstractWhisperContainer):\n    def __init__(self, model_name: str, device: str = None, compute_type: str = \"float16\",\n                 download_root: str = None,\n                 cache: ModelCache = None, models: List[ModelConfig] = []):\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        super().__init__(model_name, device, compute_type, download_root, cache, models)\n    \n    def ensure_downloaded(self):\n        \"\"\"\n        Ensure that the model is downloaded. This is useful if you want to ensure that the model is downloaded before\n        passing the container to a subprocess.\n        \"\"\"\n        # Warning: Using private API here\n        try:\n            root_dir = self.download_root\n            model_config = self._get_model_config()\n\n            if root_dir is None:\n                root_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"whisper\")\n\n            if self.model_name in whisper._MODELS:\n                whisper._download(whisper._MODELS[self.model_name], root_dir, False)\n            else:\n                # If the model is not in the official list, see if it needs to be downloaded\n                model_config.download_url(root_dir)\n            return True\n        \n        except Exception as e:\n            # Given that the API is private, it could change at any time. We don't want to crash the program\n            print(\"Error pre-downloading model: \" + str(e))\n            return False\n\n    def _get_model_config(self) -> ModelConfig:\n        \"\"\"\n        Get the model configuration for the model.\n        \"\"\"\n        for model in self.models:\n            if model.name == self.model_name:\n                return model\n        return None\n\n    def _create_model(self):\n        print(\"Loading whisper model \" + self.model_name)\n        model_config = self._get_model_config()\n\n        # Note that the model will not be downloaded in the case of an official Whisper model\n        model_path = self._get_model_path(model_config, self.download_root)\n\n        return whisper.load_model(model_path, device=self.device, download_root=self.download_root)\n\n    def create_callback(self, language: str = None, task: str = None, initial_prompt: str = None, \n                        initial_prompt_mode: VadInitialPromptMode = VadInitialPromptMode.PREPREND_FIRST_SEGMENT, \n                        **decodeOptions: dict) -> AbstractWhisperCallback:\n        \"\"\"\n        Create a WhisperCallback object that can be used to transcript audio files.\n\n        Parameters\n        ----------\n        language: str\n            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n        task: str\n            The task - either translate or transcribe.\n        initial_prompt: str\n            The initial prompt to use for the transcription.\n        initial_prompt_mode: VadInitialPromptMode\n            The mode to use for the initial prompt. If set to PREPEND_FIRST_SEGMENT, the initial prompt will be prepended to the first segment of audio.\n            If set to PREPEND_ALL_SEGMENTS, the initial prompt will be prepended to all segments of audio.\n        decodeOptions: dict\n            Additional options to pass to the decoder. Must be pickleable.\n\n        Returns\n        -------\n        A WhisperCallback object.\n        \"\"\"\n        return WhisperCallback(self, language=language, task=task, initial_prompt=initial_prompt, initial_prompt_mode=initial_prompt_mode, **decodeOptions)\n\n    def _get_model_path(self, model_config: ModelConfig, root_dir: str = None):\n        from src.conversion.hf_converter import convert_hf_whisper\n        \"\"\"\n        Download the model.\n\n        Parameters\n        ----------\n        model_config: ModelConfig\n            The model configuration.\n        \"\"\"\n        # See if path is already set\n        if model_config.path is not None:\n            return model_config.path\n        \n        if root_dir is None:\n            root_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"whisper\")\n\n        model_type = model_config.type.lower() if model_config.type is not None else \"whisper\"\n\n        if model_type in [\"huggingface\", \"hf\"]:\n            model_config.path = model_config.url\n            destination_target = os.path.join(root_dir, model_config.name + \".pt\")\n\n            # Convert from HuggingFace format to Whisper format\n            if os.path.exists(destination_target):\n                print(f\"File {destination_target} already exists, skipping conversion\")\n            else:\n                print(\"Saving HuggingFace model in Whisper format to \" + destination_target)\n                convert_hf_whisper(model_config.url, destination_target)\n\n            model_config.path = destination_target\n\n        elif model_type in [\"whisper\", \"w\"]:\n            model_config.path = model_config.url\n\n            # See if URL is just a file\n            if model_config.url in whisper._MODELS:\n                # No need to download anything - Whisper will handle it\n                model_config.path = model_config.url\n            elif model_config.url.startswith(\"file://\"):\n                # Get file path\n                model_config.path = urlparse(model_config.url).path\n            # See if it is an URL\n            elif model_config.url.startswith(\"http://\") or model_config.url.startswith(\"https://\"):\n                # Extension (or file name)\n                extension = os.path.splitext(model_config.url)[-1]\n                download_target = os.path.join(root_dir, model_config.name + extension)\n\n                if os.path.exists(download_target) and not os.path.isfile(download_target):\n                    raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n                if not os.path.isfile(download_target):\n                    download_file(model_config.url, download_target)\n                else:\n                    print(f\"File {download_target} already exists, skipping download\")\n\n                model_config.path = download_target\n            # Must be a local file\n            else:\n                model_config.path = model_config.url\n\n        else:\n            raise ValueError(f\"Unknown model type {model_type}\")\n\n        return model_config.path", "\nclass WhisperCallback(AbstractWhisperCallback):\n    def __init__(self, model_container: WhisperContainer, language: str = None, task: str = None, initial_prompt: str = None, \n                 initial_prompt_mode: VadInitialPromptMode=VadInitialPromptMode.PREPREND_FIRST_SEGMENT, **decodeOptions: dict):\n        self.model_container = model_container\n        self.language = language\n        self.task = task\n        self.initial_prompt = initial_prompt\n        self.initial_prompt_mode = initial_prompt_mode\n        self.decodeOptions = decodeOptions\n        \n    def invoke(self, audio, segment_index: int, prompt: str, detected_language: str, progress_listener: ProgressListener = None):\n        \"\"\"\n        Peform the transcription of the given audio file or data.\n\n        Parameters\n        ----------\n        audio: Union[str, np.ndarray, torch.Tensor]\n            The audio file to transcribe, or the audio data as a numpy array or torch tensor.\n        segment_index: int\n            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n        task: str\n            The task - either translate or transcribe.\n        progress_listener: ProgressListener\n            A callback to receive progress updates.\n        \"\"\"\n        model = self.model_container.get_model()\n\n        if progress_listener is not None:\n            with create_progress_listener_handle(progress_listener):\n                return self._transcribe(model, audio, segment_index, prompt, detected_language)\n        else:\n            return self._transcribe(model, audio, segment_index, prompt, detected_language)\n    \n    def _transcribe(self, model: Whisper, audio, segment_index: int, prompt: str, detected_language: str):\n        decodeOptions = self.decodeOptions.copy()\n\n        # Add fp16\n        if self.model_container.compute_type in [\"fp16\", \"float16\"]:\n            decodeOptions[\"fp16\"] = True\n\n        initial_prompt = self._get_initial_prompt(self.initial_prompt, self.initial_prompt_mode, prompt, segment_index)\n\n        return model.transcribe(audio, \\\n            language=self.language if self.language else detected_language, task=self.task, \\\n            initial_prompt=initial_prompt, \\\n            **decodeOptions\n        )"]}
{"filename": "src/whisper/abstractWhisperContainer.py", "chunked_list": ["import abc\nfrom typing import List\nfrom src.config import ModelConfig, VadInitialPromptMode\n\nfrom src.hooks.progressListener import ProgressListener\nfrom src.modelCache import GLOBAL_MODEL_CACHE, ModelCache\n\nclass AbstractWhisperCallback:\n    @abc.abstractmethod\n    def invoke(self, audio, segment_index: int, prompt: str, detected_language: str, progress_listener: ProgressListener = None):\n        \"\"\"\n        Peform the transcription of the given audio file or data.\n\n        Parameters\n        ----------\n        audio: Union[str, np.ndarray, torch.Tensor]\n            The audio file to transcribe, or the audio data as a numpy array or torch tensor.\n        segment_index: int\n            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n        task: str\n            The task - either translate or transcribe.\n        progress_listener: ProgressListener\n            A callback to receive progress updates.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _get_initial_prompt(self, initial_prompt: str, initial_prompt_mode: VadInitialPromptMode, \n                               prompt: str, segment_index: int):\n        if (initial_prompt_mode == VadInitialPromptMode.PREPEND_ALL_SEGMENTS):\n            return self._concat_prompt(initial_prompt, prompt)\n        elif (initial_prompt_mode == VadInitialPromptMode.PREPREND_FIRST_SEGMENT):\n            return self._concat_prompt(initial_prompt, prompt) if segment_index == 0 else prompt\n        else:\n            raise ValueError(f\"Unknown initial prompt mode {initial_prompt_mode}\")\n\n    def _concat_prompt(self, prompt1, prompt2):\n        if (prompt1 is None):\n            return prompt2\n        elif (prompt2 is None):\n            return prompt1\n        else:\n            return prompt1 + \" \" + prompt2", "\nclass AbstractWhisperContainer:\n    def __init__(self, model_name: str, device: str = None, compute_type: str = \"float16\",\n                 download_root: str = None,\n                 cache: ModelCache = None, models: List[ModelConfig] = []):\n        self.model_name = model_name\n        self.device = device\n        self.compute_type = compute_type\n        self.download_root = download_root\n        self.cache = cache\n\n        # Will be created on demand\n        self.model = None\n\n        # List of known models\n        self.models = models\n    \n    def get_model(self):\n        if self.model is None:\n\n            if (self.cache is None):\n                self.model = self._create_model()\n            else:\n                model_key = \"WhisperContainer.\" + self.model_name + \":\" + (self.device if self.device else '')\n                self.model = self.cache.get(model_key, self._create_model)\n        return self.model\n    \n    @abc.abstractmethod\n    def _create_model(self):\n        raise NotImplementedError()\n\n    def ensure_downloaded(self):\n        pass\n\n    @abc.abstractmethod\n    def create_callback(self, language: str = None, task: str = None, initial_prompt: str = None, \n                        initial_prompt_mode: VadInitialPromptMode = VadInitialPromptMode.PREPREND_FIRST_SEGMENT, \n                        **decodeOptions: dict) -> AbstractWhisperCallback:\n        \"\"\"\n        Create a WhisperCallback object that can be used to transcript audio files.\n\n        Parameters\n        ----------\n        language: str\n            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n        task: str\n            The task - either translate or transcribe.\n        initial_prompt: str\n            The initial prompt to use for the transcription.\n        initial_prompt_mode: VadInitialPromptMode\n            The mode to use for the initial prompt. If set to PREPEND_FIRST_SEGMENT, the initial prompt will be prepended to the first segment of audio.\n            If set to PREPEND_ALL_SEGMENTS, the initial prompt will be prepended to all segments of audio.\n        decodeOptions: dict\n            Additional options to pass to the decoder. Must be pickleable.\n\n        Returns\n        -------\n        A WhisperCallback object.\n        \"\"\"\n        raise NotImplementedError()\n\n    # This is required for multiprocessing\n    def __getstate__(self):\n        return { \n            \"model_name\": self.model_name, \n            \"device\": self.device, \n            \"download_root\": self.download_root, \n            \"models\": self.models, \n            \"compute_type\": self.compute_type \n        }\n\n    def __setstate__(self, state):\n        self.model_name = state[\"model_name\"]\n        self.device = state[\"device\"]\n        self.download_root = state[\"download_root\"]\n        self.models = state[\"models\"]\n        self.compute_type = state[\"compute_type\"]\n        self.model = None\n        # Depickled objects must use the global cache\n        self.cache = GLOBAL_MODEL_CACHE"]}
{"filename": "src/whisper/fasterWhisperContainer.py", "chunked_list": ["import os\nfrom typing import List, Union\n\nfrom faster_whisper import WhisperModel, download_model\nfrom src.config import ModelConfig, VadInitialPromptMode\nfrom src.hooks.progressListener import ProgressListener\nfrom src.languages import get_language_from_name\nfrom src.modelCache import ModelCache\nfrom src.whisper.abstractWhisperContainer import AbstractWhisperCallback, AbstractWhisperContainer\nfrom src.utils import format_timestamp", "from src.whisper.abstractWhisperContainer import AbstractWhisperCallback, AbstractWhisperContainer\nfrom src.utils import format_timestamp\n\nclass FasterWhisperContainer(AbstractWhisperContainer):\n    def __init__(self, model_name: str, device: str = None, compute_type: str = \"float16\",\n                       download_root: str = None,\n                       cache: ModelCache = None, models: List[ModelConfig] = []):\n        super().__init__(model_name, device, compute_type, download_root, cache, models)\n    \n    def ensure_downloaded(self):\n        \"\"\"\n        Ensure that the model is downloaded. This is useful if you want to ensure that the model is downloaded before\n        passing the container to a subprocess.\n        \"\"\"\n        model_config = self._get_model_config()\n        \n        if os.path.isdir(model_config.url):\n            model_config.path = model_config.url\n        else:\n            model_config.path = download_model(model_config.url, output_dir=self.download_root)\n\n    def _get_model_config(self) -> ModelConfig:\n        \"\"\"\n        Get the model configuration for the model.\n        \"\"\"\n        for model in self.models:\n            if model.name == self.model_name:\n                return model\n        return None\n\n    def _create_model(self):\n        print(\"Loading faster whisper model \" + self.model_name + \" for device \" + str(self.device))\n        model_config = self._get_model_config()\n        \n        if model_config.type == \"whisper\" and model_config.url not in [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v2\"]:\n            raise Exception(\"FasterWhisperContainer does not yet support Whisper models. Use ct2-transformers-converter to convert the model to a faster-whisper model.\")\n\n        device = self.device\n\n        if (device is None):\n            device = \"auto\"\n        # model_path = FASTER_WHISPER_MODELS_PATH[model_config.url]\n        model_path = os.path.join(os.path.join(\"models\", \"faster-whisper\"),model_config.url)\n        # model = WhisperModel(model_config.url, device=device, compute_type=self.compute_type)\n        model = WhisperModel(model_size_or_path=model_path, device=device, compute_type=self.compute_type)\n        return model\n\n    def create_callback(self, language: str = None, task: str = None, initial_prompt: str = None, \n                        initial_prompt_mode: VadInitialPromptMode = VadInitialPromptMode.PREPREND_FIRST_SEGMENT, \n                        **decodeOptions: dict) -> AbstractWhisperCallback:\n        \"\"\"\n        Create a WhisperCallback object that can be used to transcript audio files.\n\n        Parameters\n        ----------\n        language: str\n            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n        task: str\n            The task - either translate or transcribe.\n        initial_prompt: str\n            The initial prompt to use for the transcription.\n        initial_prompt_mode: VadInitialPromptMode\n            The mode to use for the initial prompt. If set to PREPEND_FIRST_SEGMENT, the initial prompt will be prepended to the first segment of audio.\n            If set to PREPEND_ALL_SEGMENTS, the initial prompt will be prepended to all segments of audio.\n        decodeOptions: dict\n            Additional options to pass to the decoder. Must be pickleable.\n\n        Returns\n        -------\n        A WhisperCallback object.\n        \"\"\"\n        return FasterWhisperCallback(self, language=language, task=task, initial_prompt=initial_prompt, initial_prompt_mode=initial_prompt_mode, **decodeOptions)", "\nclass FasterWhisperCallback(AbstractWhisperCallback):\n    def __init__(self, model_container: FasterWhisperContainer, language: str = None, task: str = None, \n                 initial_prompt: str = None, initial_prompt_mode: VadInitialPromptMode=VadInitialPromptMode.PREPREND_FIRST_SEGMENT, \n                 **decodeOptions: dict):\n        self.model_container = model_container\n        self.language = language\n        self.task = task\n        self.initial_prompt = initial_prompt\n        self.initial_prompt_mode = initial_prompt_mode\n        self.decodeOptions = decodeOptions\n\n        self._printed_warning = False\n        \n    def invoke(self, audio, segment_index: int, prompt: str, detected_language: str, progress_listener: ProgressListener = None):\n        \"\"\"\n        Peform the transcription of the given audio file or data.\n\n        Parameters\n        ----------\n        audio: Union[str, np.ndarray, torch.Tensor]\n            The audio file to transcribe, or the audio data as a numpy array or torch tensor.\n        segment_index: int\n            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n        task: str\n            The task - either translate or transcribe.\n        progress_listener: ProgressListener\n            A callback to receive progress updates.\n        \"\"\"\n        model: WhisperModel = self.model_container.get_model()\n        language_code = self._lookup_language_code(self.language) if self.language else None\n\n        # Copy decode options and remove options that are not supported by faster-whisper\n        decodeOptions = self.decodeOptions.copy()\n        verbose = decodeOptions.pop(\"verbose\", None)\n\n        logprob_threshold = decodeOptions.pop(\"logprob_threshold\", None)\n\n        patience = decodeOptions.pop(\"patience\", None)\n        length_penalty = decodeOptions.pop(\"length_penalty\", None)\n        suppress_tokens = decodeOptions.pop(\"suppress_tokens\", None)\n\n        if (decodeOptions.pop(\"fp16\", None) is not None):\n            if not self._printed_warning:\n                print(\"WARNING: fp16 option is ignored by faster-whisper - use compute_type instead.\")\n            self._printed_warning = True\n\n        # Fix up decode options\n        if (logprob_threshold is not None):\n            decodeOptions[\"log_prob_threshold\"] = logprob_threshold\n\n        decodeOptions[\"patience\"] = float(patience) if patience is not None else 1.0\n        decodeOptions[\"length_penalty\"] = float(length_penalty) if length_penalty is not None else 1.0\n\n        # See if supress_tokens is a string - if so, convert it to a list of ints\n        decodeOptions[\"suppress_tokens\"] = self._split_suppress_tokens(suppress_tokens)\n\n        initial_prompt = self._get_initial_prompt(self.initial_prompt, self.initial_prompt_mode, prompt, segment_index)\n\n        segments_generator, info = model.transcribe(audio, \\\n            language=language_code if language_code else detected_language, task=self.task, \\\n            initial_prompt=initial_prompt, \\\n            **decodeOptions\n        )\n\n        segments = []\n\n        for segment in segments_generator:\n            segments.append(segment)\n\n            if progress_listener is not None:\n                progress_listener.on_progress(segment.end, info.duration)\n            if verbose:\n                print(\"[{}->{}] {}\".format(format_timestamp(segment.start, True), format_timestamp(segment.end, True),\n                                          segment.text))\n\n        text = \" \".join([segment.text for segment in segments])\n\n        # Convert the segments to a format that is easier to serialize\n        whisper_segments = [{\n            \"text\": segment.text,\n            \"start\": segment.start,\n            \"end\": segment.end,\n\n            # Extra fields added by faster-whisper\n            \"words\": [{\n                \"start\": word.start,\n                \"end\": word.end,\n                \"word\": word.word,\n                \"probability\": word.probability\n            } for word in (segment.words if segment.words is not None else []) ]\n        } for segment in segments]\n\n        result = {\n            \"segments\": whisper_segments,\n            \"text\": text,\n            \"language\": info.language if info else None,\n\n            # Extra fields added by faster-whisper\n            \"language_probability\": info.language_probability if info else None,\n            \"duration\": info.duration if info else None\n        }\n\n        if progress_listener is not None:\n            progress_listener.on_finished()\n        return result\n\n    def _split_suppress_tokens(self, suppress_tokens: Union[str, List[int]]):\n        if (suppress_tokens is None):\n            return None\n        if (isinstance(suppress_tokens, list)):\n            return suppress_tokens\n\n        return [int(token) for token in suppress_tokens.split(\",\")]\n\n    def _lookup_language_code(self, language: str):\n        language = get_language_from_name(language)\n\n        if language is None:\n            raise ValueError(\"Invalid language: \" + language)\n        \n        return language.code", ""]}
{"filename": "src/conversion/hf_converter.py", "chunked_list": ["# https://github.com/bayartsogt-ya/whisper-multiple-hf-datasets\n\nfrom copy import deepcopy\nimport torch\n\nWHISPER_MAPPING = {\n    \"layers\": \"blocks\",\n    \"fc1\": \"mlp.0\",\n    \"fc2\": \"mlp.2\",\n    \"final_layer_norm\": \"mlp_ln\",", "    \"fc2\": \"mlp.2\",\n    \"final_layer_norm\": \"mlp_ln\",\n    \"layers\": \"blocks\",\n    \".self_attn.q_proj\": \".attn.query\",\n    \".self_attn.k_proj\": \".attn.key\",\n    \".self_attn.v_proj\": \".attn.value\",\n    \".self_attn_layer_norm\": \".attn_ln\",\n    \".self_attn.out_proj\": \".attn.out\",\n    \".encoder_attn.q_proj\": \".cross_attn.query\",\n    \".encoder_attn.k_proj\": \".cross_attn.key\",", "    \".encoder_attn.q_proj\": \".cross_attn.query\",\n    \".encoder_attn.k_proj\": \".cross_attn.key\",\n    \".encoder_attn.v_proj\": \".cross_attn.value\",\n    \".encoder_attn_layer_norm\": \".cross_attn_ln\",\n    \".encoder_attn.out_proj\": \".cross_attn.out\",\n    \"decoder.layer_norm.\": \"decoder.ln.\",\n    \"encoder.layer_norm.\": \"encoder.ln_post.\",\n    \"embed_tokens\": \"token_embedding\",\n    \"encoder.embed_positions.weight\": \"encoder.positional_embedding\",\n    \"decoder.embed_positions.weight\": \"decoder.positional_embedding\",", "    \"encoder.embed_positions.weight\": \"encoder.positional_embedding\",\n    \"decoder.embed_positions.weight\": \"decoder.positional_embedding\",\n    \"layer_norm\": \"ln_post\",\n}\n\n\ndef rename_keys(s_dict):\n    keys = list(s_dict.keys())\n    for key in keys:\n        new_key = key\n        for k, v in WHISPER_MAPPING.items():\n            if k in key:\n                new_key = new_key.replace(k, v)\n\n        print(f\"{key} -> {new_key}\")\n\n        s_dict[new_key] = s_dict.pop(key)\n    return s_dict", "\n\ndef convert_hf_whisper(hf_model_name_or_path: str, whisper_state_path: str):\n    from transformers import WhisperForConditionalGeneration\n    transformer_model = WhisperForConditionalGeneration.from_pretrained(hf_model_name_or_path)\n    config = transformer_model.config\n\n    # first build dims\n    dims = {\n        'n_mels': config.num_mel_bins,\n        'n_vocab': config.vocab_size,\n        'n_audio_ctx': config.max_source_positions,\n        'n_audio_state': config.d_model,\n        'n_audio_head': config.encoder_attention_heads,\n        'n_audio_layer': config.encoder_layers,\n        'n_text_ctx': config.max_target_positions,\n        'n_text_state': config.d_model,\n        'n_text_head': config.decoder_attention_heads,\n        'n_text_layer': config.decoder_layers\n    }\n\n    state_dict = deepcopy(transformer_model.model.state_dict())\n    state_dict = rename_keys(state_dict)\n\n    torch.save({\"dims\": dims, \"model_state_dict\": state_dict}, whisper_state_path)"]}
