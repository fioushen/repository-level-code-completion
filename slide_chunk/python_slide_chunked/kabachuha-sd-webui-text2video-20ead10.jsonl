{"filename": "install.py", "chunked_list": ["# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# Read LICENSE for usage terms.\n\nimport launch\n\nimport os\n\nreq_file = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"requirements.txt\")\n\nwith open(req_file) as file:\n\n    for lib in file:\n\n        lib = lib.strip()\n\n        if not launch.is_installed(lib):\n\n            launch.run_pip(f\"install {lib}\", f\"text2video requirement: {lib}\")", "\nwith open(req_file) as file:\n\n    for lib in file:\n\n        lib = lib.strip()\n\n        if not launch.is_installed(lib):\n\n            launch.run_pip(f\"install {lib}\", f\"text2video requirement: {lib}\")", ""]}
{"filename": "scripts/api_t2v.py", "chunked_list": ["# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# Read LICENSE for usage terms.\n\nimport sys, os\nbasedirs = [os.getcwd()]\nif 'google.colab' in sys.modules:\n    basedirs.append('/content/gdrive/MyDrive/sd/stable-diffusion-webui') #hardcode as TheLastBen's colab seems to be the primal source\n\nfor basedir in basedirs:\n    deforum_paths_to_ensure = [basedir + '/extensions/sd-webui-text2video/scripts', basedir + '/extensions/sd-webui-modelscope-text2video/scripts', basedir]\n\n    for deforum_scripts_path_fix in deforum_paths_to_ensure:\n        if not deforum_scripts_path_fix in sys.path:\n            sys.path.extend([deforum_scripts_path_fix])", "for basedir in basedirs:\n    deforum_paths_to_ensure = [basedir + '/extensions/sd-webui-text2video/scripts', basedir + '/extensions/sd-webui-modelscope-text2video/scripts', basedir]\n\n    for deforum_scripts_path_fix in deforum_paths_to_ensure:\n        if not deforum_scripts_path_fix in sys.path:\n            sys.path.extend([deforum_scripts_path_fix])\n\ncurrent_directory = os.path.dirname(os.path.abspath(__file__))\nif current_directory not in sys.path:\n    sys.path.append(current_directory)", "if current_directory not in sys.path:\n    sys.path.append(current_directory)\n\nimport hashlib\nimport io\nimport json\nimport logging\nimport os\nimport sys\nimport tempfile", "import sys\nimport tempfile\nfrom PIL import Image\nimport urllib\nfrom typing import Union\nimport traceback\nfrom types import SimpleNamespace\n\nfrom fastapi import FastAPI, Query, Request, UploadFile\nfrom fastapi.encoders import jsonable_encoder", "from fastapi import FastAPI, Query, Request, UploadFile\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import JSONResponse\nfrom t2v_helpers.video_audio_utils import find_ffmpeg_binary\nfrom t2v_helpers.general_utils import get_t2v_version\nfrom t2v_helpers.args import T2VArgs_sanity_check, T2VArgs, T2VOutputArgs\nfrom t2v_helpers.render import run\nimport uuid\n", "import uuid\n\nlogger = logging.getLogger(__name__)\n\ncurrent_directory = os.path.dirname(os.path.abspath(__file__))\nif current_directory not in sys.path:\n    sys.path.append(current_directory)\n\ndef t2v_api(_, app: FastAPI):\n    logger.debug(f\"text2video extension for auto1111 webui\")\n    logger.debug(f\"Git commit: {get_t2v_version()}\")\n    logger.debug(\"Loading text2video API endpoints\")\n\n    @app.exception_handler(RequestValidationError)\n    async def validation_exception_handler(request: Request, exc: RequestValidationError):\n        return JSONResponse(\n            status_code=422,\n            content=jsonable_encoder({\"detail\": exc.errors(), \"body\": exc.body}),\n        )\n\n    @app.get(\"/t2v/api_version\")\n    async def t2v_api_version():\n        return JSONResponse(content={\"version\": '1.0'})\n\n    @app.get(\"/t2v/version\")\n    async def t2v_version():\n        return JSONResponse(content={\"version\": get_t2v_version()})\n\n    @app.post(\"/t2v/run\")\n    async def t2v_run(prompt: str, n_prompt: Union[str, None] = None, model: Union[str, None] = None, sampler: Union[str, None] = None, steps: Union[int, None] = None, frames: Union[int, None] = None, seed: Union[int, None] = None, \\\n                      cfg_scale: Union[int, None] = None, width: Union[int, None] = None, height: Union[int, None] = None, eta: Union[float, None] = None, batch_count: Union[int, None] = None, \\\n                      do_vid2vid:bool = False, vid2vid_input: Union[UploadFile, None] = None,strength: Union[float, None] = None,vid2vid_startFrame: Union[int, None] = None, \\\n                      inpainting_image: Union[UploadFile, None] = None, inpainting_frames: Union[int, None] = None, inpainting_weights: Union[str, None] = None, \\\n                      fps: Union[int, None] = None, add_soundtrack: Union[str, None] = None, soundtrack_path: Union[str, None] = None, ):\n        for basedir in basedirs:\n            sys.path.extend([\n                basedir + '/scripts',\n                basedir + '/extensions/sd-webui-text2video/scripts',\n                basedir + '/extensions/sd-webui-modelscope-text2video/scripts',\n            ])\n\n        locals_args_dict = locals()\n        args_dict = T2VArgs()\n        video_args_dict = T2VOutputArgs()\n        for k, v in locals_args_dict.items():\n            if v is not None:\n                if k in args_dict:\n                    args_dict[k] = locals_args_dict[k]\n                elif k in video_args_dict:\n                    video_args_dict[k] = locals_args_dict[k]\n\n        \"\"\"\n        Run t2v over api\n        @return:\n        \"\"\"\n        d = SimpleNamespace(**args_dict)\n        dv = SimpleNamespace(**video_args_dict)\n\n        tmp_inpainting = None\n        tmp_inpainting_name = f'outputs/t2v_temp/{str(uuid.uuid4())}.png'\n        tmp_vid2vid = None\n        temp_vid2vid_name = f'outputs/t2v_temp/{str(uuid.uuid4())}.mp4'\n        os.makedirs('outputs/t2v_temp', exist_ok=True)\n\n        # Wrap the process call in a try-except block to handle potential errors\n        try:\n            T2VArgs_sanity_check(d)\n\n            if d.inpainting_frames > 0 and inpainting_image:\n                img_content = await inpainting_image.read()\n                img = Image.open(io.BytesIO(img_content))\n                img.save(tmp_inpainting_name)\n                tmp_inpainting = open(tmp_inpainting_name, \"r\")\n\n            if do_vid2vid and vid2vid_input:\n                vid2vid_input_content = await vid2vid_input.read()\n                tmp_vid2vid = open(temp_vid2vid_name, \"wb\")\n                tmp_vid2vid.write(io.BytesIO(vid2vid_input_content).getbuffer())\n                tmp_vid2vid.close()\n                tmp_vid2vid = open(temp_vid2vid_name, \"r\")\n\n            videodat = run(\n                # ffmpeg params\n                dv.skip_video_creation, #skip_video_creation\n                find_ffmpeg_binary(), #ffmpeg_location\n                dv.ffmpeg_crf, #ffmpeg_crf\n                dv.ffmpeg_preset,#ffmpeg_preset\n                dv.fps,#fps\n                dv.add_soundtrack,#add_soundtrack\n                dv.soundtrack_path,#soundtrack_paths\n\n                d.prompt,#prompt\n                d.n_prompt,#n_prompt\n                d.sampler,#sampler\n                d.steps,#steps\n                d.frames,#frames\n                d.seed,#seed\n                d.cfg_scale,#cfg_scale\n                d.width,#width\n                d.height,#height\n                d.eta,#eta\n                d.batch_count,#batch_count\n\n                # The same, but for vid2vid. Will deduplicate later\n                d.prompt,#prompt\n                d.n_prompt,#n_prompt\n                d.sampler,#sampler\n                d.steps,#steps\n                d.frames,#frames\n                d.seed,#seed\n                d.cfg_scale,#cfg_scale\n                d.width,#width\n                d.height,#height\n                d.eta,#eta\n                d.batch_count,#batch_count_v\n\n                do_vid2vid,#do_vid2vid\n                tmp_vid2vid,#vid2vid_frames\n                \"\",#vid2vid_frames_path\n                d.strength,#strength\n                d.vid2vid_startFrame,#vid2vid_startFrame\n                tmp_inpainting,#inpainting_image\n                d.inpainting_frames,#inpainting_frames\n                d.inpainting_weights,#inpainting_weights\n                \"ModelScope\",#model_type. Only one has stable support at this moment\n                d.model,\n            )\n\n            return JSONResponse(content={\"mp4s\": videodat})\n        except Exception as e:\n            # Log the error and return a JSON response with an appropriate status code and error message\n            logger.error(f\"Error processing the video: {e}\")\n            traceback.print_exc()\n            return JSONResponse(\n                status_code=500,\n                content={\"detail\": \"An error occurred while processing the video.\"},\n            )\n        finally:\n            if tmp_inpainting is not None:\n                tmp_inpainting.close()\n                # delete temporary file\n                try:\n                    os.remove(tmp_inpainting_name)\n                except Exception as e:\n                    ...\n                except Exception as e:\n                    ...\n            if tmp_vid2vid is not None:\n                tmp_vid2vid.close()\n                try:\n                    os.remove(temp_vid2vid_name)\n                except Exception as e:\n                    ...", "def t2v_api(_, app: FastAPI):\n    logger.debug(f\"text2video extension for auto1111 webui\")\n    logger.debug(f\"Git commit: {get_t2v_version()}\")\n    logger.debug(\"Loading text2video API endpoints\")\n\n    @app.exception_handler(RequestValidationError)\n    async def validation_exception_handler(request: Request, exc: RequestValidationError):\n        return JSONResponse(\n            status_code=422,\n            content=jsonable_encoder({\"detail\": exc.errors(), \"body\": exc.body}),\n        )\n\n    @app.get(\"/t2v/api_version\")\n    async def t2v_api_version():\n        return JSONResponse(content={\"version\": '1.0'})\n\n    @app.get(\"/t2v/version\")\n    async def t2v_version():\n        return JSONResponse(content={\"version\": get_t2v_version()})\n\n    @app.post(\"/t2v/run\")\n    async def t2v_run(prompt: str, n_prompt: Union[str, None] = None, model: Union[str, None] = None, sampler: Union[str, None] = None, steps: Union[int, None] = None, frames: Union[int, None] = None, seed: Union[int, None] = None, \\\n                      cfg_scale: Union[int, None] = None, width: Union[int, None] = None, height: Union[int, None] = None, eta: Union[float, None] = None, batch_count: Union[int, None] = None, \\\n                      do_vid2vid:bool = False, vid2vid_input: Union[UploadFile, None] = None,strength: Union[float, None] = None,vid2vid_startFrame: Union[int, None] = None, \\\n                      inpainting_image: Union[UploadFile, None] = None, inpainting_frames: Union[int, None] = None, inpainting_weights: Union[str, None] = None, \\\n                      fps: Union[int, None] = None, add_soundtrack: Union[str, None] = None, soundtrack_path: Union[str, None] = None, ):\n        for basedir in basedirs:\n            sys.path.extend([\n                basedir + '/scripts',\n                basedir + '/extensions/sd-webui-text2video/scripts',\n                basedir + '/extensions/sd-webui-modelscope-text2video/scripts',\n            ])\n\n        locals_args_dict = locals()\n        args_dict = T2VArgs()\n        video_args_dict = T2VOutputArgs()\n        for k, v in locals_args_dict.items():\n            if v is not None:\n                if k in args_dict:\n                    args_dict[k] = locals_args_dict[k]\n                elif k in video_args_dict:\n                    video_args_dict[k] = locals_args_dict[k]\n\n        \"\"\"\n        Run t2v over api\n        @return:\n        \"\"\"\n        d = SimpleNamespace(**args_dict)\n        dv = SimpleNamespace(**video_args_dict)\n\n        tmp_inpainting = None\n        tmp_inpainting_name = f'outputs/t2v_temp/{str(uuid.uuid4())}.png'\n        tmp_vid2vid = None\n        temp_vid2vid_name = f'outputs/t2v_temp/{str(uuid.uuid4())}.mp4'\n        os.makedirs('outputs/t2v_temp', exist_ok=True)\n\n        # Wrap the process call in a try-except block to handle potential errors\n        try:\n            T2VArgs_sanity_check(d)\n\n            if d.inpainting_frames > 0 and inpainting_image:\n                img_content = await inpainting_image.read()\n                img = Image.open(io.BytesIO(img_content))\n                img.save(tmp_inpainting_name)\n                tmp_inpainting = open(tmp_inpainting_name, \"r\")\n\n            if do_vid2vid and vid2vid_input:\n                vid2vid_input_content = await vid2vid_input.read()\n                tmp_vid2vid = open(temp_vid2vid_name, \"wb\")\n                tmp_vid2vid.write(io.BytesIO(vid2vid_input_content).getbuffer())\n                tmp_vid2vid.close()\n                tmp_vid2vid = open(temp_vid2vid_name, \"r\")\n\n            videodat = run(\n                # ffmpeg params\n                dv.skip_video_creation, #skip_video_creation\n                find_ffmpeg_binary(), #ffmpeg_location\n                dv.ffmpeg_crf, #ffmpeg_crf\n                dv.ffmpeg_preset,#ffmpeg_preset\n                dv.fps,#fps\n                dv.add_soundtrack,#add_soundtrack\n                dv.soundtrack_path,#soundtrack_paths\n\n                d.prompt,#prompt\n                d.n_prompt,#n_prompt\n                d.sampler,#sampler\n                d.steps,#steps\n                d.frames,#frames\n                d.seed,#seed\n                d.cfg_scale,#cfg_scale\n                d.width,#width\n                d.height,#height\n                d.eta,#eta\n                d.batch_count,#batch_count\n\n                # The same, but for vid2vid. Will deduplicate later\n                d.prompt,#prompt\n                d.n_prompt,#n_prompt\n                d.sampler,#sampler\n                d.steps,#steps\n                d.frames,#frames\n                d.seed,#seed\n                d.cfg_scale,#cfg_scale\n                d.width,#width\n                d.height,#height\n                d.eta,#eta\n                d.batch_count,#batch_count_v\n\n                do_vid2vid,#do_vid2vid\n                tmp_vid2vid,#vid2vid_frames\n                \"\",#vid2vid_frames_path\n                d.strength,#strength\n                d.vid2vid_startFrame,#vid2vid_startFrame\n                tmp_inpainting,#inpainting_image\n                d.inpainting_frames,#inpainting_frames\n                d.inpainting_weights,#inpainting_weights\n                \"ModelScope\",#model_type. Only one has stable support at this moment\n                d.model,\n            )\n\n            return JSONResponse(content={\"mp4s\": videodat})\n        except Exception as e:\n            # Log the error and return a JSON response with an appropriate status code and error message\n            logger.error(f\"Error processing the video: {e}\")\n            traceback.print_exc()\n            return JSONResponse(\n                status_code=500,\n                content={\"detail\": \"An error occurred while processing the video.\"},\n            )\n        finally:\n            if tmp_inpainting is not None:\n                tmp_inpainting.close()\n                # delete temporary file\n                try:\n                    os.remove(tmp_inpainting_name)\n                except Exception as e:\n                    ...\n                except Exception as e:\n                    ...\n            if tmp_vid2vid is not None:\n                tmp_vid2vid.close()\n                try:\n                    os.remove(temp_vid2vid_name)\n                except Exception as e:\n                    ...", "\n\ntry:\n    import modules.script_callbacks as script_callbacks\n\n    script_callbacks.on_app_started(t2v_api)\n    logger.debug(\"SD-Webui text2video API layer loaded\")\nexcept ImportError:\n    logger.debug(\"Unable to import script callbacks.XXX\")\n", ""]}
{"filename": "scripts/text2vid.py", "chunked_list": ["# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# Read LICENSE for usage terms.\n\nimport sys, os\n\nbasedirs = [os.getcwd()]\nif 'google.colab' in sys.modules:\n    basedirs.append('/content/gdrive/MyDrive/sd/stable-diffusion-webui')  # hardcode as TheLastBen's colab seems to be the primal source\n\nfor basedir in basedirs:\n    deforum_paths_to_ensure = [basedir + '/extensions/sd-webui-text2video/scripts', basedir + '/extensions/sd-webui-modelscope-text2video/scripts', basedir]\n\n    for deforum_scripts_path_fix in deforum_paths_to_ensure:\n        if not deforum_scripts_path_fix in sys.path:\n            sys.path.extend([deforum_scripts_path_fix])", "\nfor basedir in basedirs:\n    deforum_paths_to_ensure = [basedir + '/extensions/sd-webui-text2video/scripts', basedir + '/extensions/sd-webui-modelscope-text2video/scripts', basedir]\n\n    for deforum_scripts_path_fix in deforum_paths_to_ensure:\n        if not deforum_scripts_path_fix in sys.path:\n            sys.path.extend([deforum_scripts_path_fix])\n\ncurrent_directory = os.path.dirname(os.path.abspath(__file__))\nif current_directory not in sys.path:\n    sys.path.append(current_directory)", "current_directory = os.path.dirname(os.path.abspath(__file__))\nif current_directory not in sys.path:\n    sys.path.append(current_directory)\n\nimport gradio as gr\nfrom modules import script_callbacks, shared\nfrom modules.shared import cmd_opts, opts\nfrom t2v_helpers.render import run\nimport t2v_helpers.args as args\nfrom t2v_helpers.args import setup_text2video_settings_dictionary", "import t2v_helpers.args as args\nfrom t2v_helpers.args import setup_text2video_settings_dictionary\nfrom webui import wrap_gradio_gpu_call\nfrom stable_lora.scripts.lora_webui import StableLoraScriptInstance\nStableLoraScript = StableLoraScriptInstance\n\ndef process(*args):\n    # weird PATH stuff\n    for basedir in basedirs:\n        sys.path.extend([\n            basedir + '/scripts',\n            basedir + '/extensions/sd-webui-text2video/scripts',\n            basedir + '/extensions/sd-webui-modelscope-text2video/scripts',\n        ])\n    if current_directory not in sys.path:\n        sys.path.append(current_directory)\n\n    run(*args)\n    return f'Video ready'", "\ndef on_ui_tabs():\n    with gr.Blocks(analytics_enabled=False) as deforum_interface:\n        components = {}\n        with gr.Row(elem_id='t2v-core').style(equal_height=False, variant='compact'):\n            with gr.Column(scale=1, variant='panel'):\n                components = setup_text2video_settings_dictionary()\n                stable_lora_ui = StableLoraScript.ui()\n            with gr.Column(scale=1, variant='compact'):\n                with gr.Row(elem_id=f\"text2vid_generate_box\", variant='compact', elem_classes=\"generate-box\"):\n                    interrupt = gr.Button('Interrupt', elem_id=f\"text2vid_interrupt\", elem_classes=\"generate-box-interrupt\")\n                    skip = gr.Button('Skip', elem_id=f\"text2vid_skip\", elem_classes=\"generate-box-skip\")\n                    run_button = gr.Button('Generate', elem_id=f\"text2vid_generate\", variant='primary')\n\n                    skip.click(\n                        fn=lambda: shared.state.skip(),\n                        inputs=[],\n                        outputs=[],\n                    )\n\n                    interrupt.click(\n                        fn=lambda: shared.state.interrupt(),\n                        inputs=[],\n                        outputs=[],\n                    )\n                with gr.Row(variant='compact'):\n                    i1 = gr.HTML(args.i1_store_t2v, elem_id='deforum_header')\n                with gr.Row(visible=False):\n                    dummy_component1 = gr.Label(\"\")\n                    dummy_component2 = gr.Label(\"\")\n                with gr.Row(variant='compact'):\n                    btn = gr.Button(\"Click here after the generation to show the video\")\n                with gr.Row(variant='compact', elem_id='text2vid_results_panel'):\n                    ...\n                    # gr.Label(\"\", visible=False)\n                with gr.Row(variant='compact'):\n                    i1 = gr.HTML(args.i1_store_t2v, elem_id='deforum_header')\n\n                    def show_vid():  # Show video1\n                        return {\n                            i1: gr.update(value=args.i1_store_t2v, visible=True),\n                            btn: gr.update(value=\"Update the video\", visible=True),\n                        }\n\n                    btn.click(\n                        show_vid,\n                        [],\n                        [i1, btn],\n                    )\n            run_button.click(\n                # , extra_outputs=[None, '', '']),\n                fn=wrap_gradio_gpu_call(process),\n                _js=\"submit_txt2vid\",\n                inputs=[dummy_component1, dummy_component2] + [components[name] for name in args.get_component_names()] + stable_lora_ui,\n                outputs=[\n                    dummy_component1, dummy_component2,\n                ],\n            )\n    return [(deforum_interface, \"txt2video\", \"t2v_interface\")]", "\ndef on_ui_settings():\n    section = ('modelscope_deforum', \"Text2Video\")\n    shared.opts.add_option(\"modelscope_deforum_keep_model_in_vram\", shared.OptionInfo(\n        'None', \"Keep model in VRAM between runs\", gr.Radio,\n        {\"interactive\": True, \"choices\": ['None', 'Main Model Only', 'All'], \"visible\": True if not (cmd_opts.lowvram or cmd_opts.medvram) else False}, section=section))\n    shared.opts.add_option(\"modelscope_deforum_vae_settings\", shared.OptionInfo(\n        \"GPU (half precision)\", \"VAE Mode:\", gr.Radio, {\"interactive\": True, \"choices\": ['GPU (half precision)', 'GPU', 'CPU (Low VRAM)']}, section=section))\n    shared.opts.add_option(\"modelscope_deforum_show_n_videos\", shared.OptionInfo(\n        -1, \"How many videos to show on the right panel on completion (-1 = show all)\", gr.Number, {\"interactive\": True, \"visible\": True}, section=section))", "\nscript_callbacks.on_ui_tabs(on_ui_tabs)\nscript_callbacks.on_ui_settings(on_ui_settings)\n"]}
{"filename": "scripts/samplers/samplers_common.py", "chunked_list": ["import torch\nfrom samplers.ddim.sampler import DDIMSampler\nfrom samplers.ddim.gaussian_sampler import GaussianDiffusion\nfrom samplers.uni_pc.sampler import UniPCSampler\nfrom tqdm import tqdm\nfrom modules.shared import state\nfrom modules.sd_samplers_common import InterruptedException\n\ndef get_height_width(h, w, divisor):\n    return h // divisor, w // divisor", "def get_height_width(h, w, divisor):\n    return h // divisor, w // divisor\n\ndef get_tensor_shape(batch_size, channels, frames, h, w, latents=None):\n    if latents is None:\n        return (batch_size, channels, frames, h, w)\n    return latents.shape\n\ndef inpaint_masking(xt, step, steps, mask, add_noise_cb, noise_cb_args):\n    if mask is not None and step < steps - 1:\n\n        #convert mask to 0,1 valued based on step\n        v = (steps - step - 1) / steps\n        binary_mask = torch.where(mask <= v, torch.zeros_like(mask), torch.ones_like(mask))\n\n        noise_to_add = add_noise_cb(**noise_cb_args)\n        to_inpaint = noise_to_add\n        xt = to_inpaint * (1 - binary_mask) + xt * binary_mask", "def inpaint_masking(xt, step, steps, mask, add_noise_cb, noise_cb_args):\n    if mask is not None and step < steps - 1:\n\n        #convert mask to 0,1 valued based on step\n        v = (steps - step - 1) / steps\n        binary_mask = torch.where(mask <= v, torch.zeros_like(mask), torch.ones_like(mask))\n\n        noise_to_add = add_noise_cb(**noise_cb_args)\n        to_inpaint = noise_to_add\n        xt = to_inpaint * (1 - binary_mask) + xt * binary_mask", "\nclass SamplerStepCallback(object):\n    def __init__(self, sampler_name: str, total_steps: int):\n        self.sampler_name = sampler_name\n        self.total_steps = total_steps\n        self.current_step = 0\n        self.progress_bar = tqdm(desc=self.progress_msg(sampler_name, total_steps), total=total_steps)\n\n    def progress_msg(self, name, total_steps=None):\n        total_steps = total_steps if total_steps is not None else self.total_steps\n        state.sampling_steps = total_steps\n        return f\"Sampling using {name} for {total_steps} steps.\"\n\n    def set_webui_step(self, step):\n        state.sampling_step = step\n\n    def is_finished(self, step):\n        if step >= self.total_steps:\n            self.progress_bar.close()\n            self.current_step = 0\n\n    def interrupt(self):\n        return state.interrupted or state.skipped\n\n    def cancel(self):\n        raise InterruptedException\n\n    def update(self, step):\n        self.set_webui_step(step)\n\n        if self.interrupt():\n            self.cancel()\n\n        self.progress_bar.set_description(self.progress_msg(self.sampler_name))\n        self.progress_bar.update(1)\n\n        self.is_finished(step)  \n\n    def __call__(self,*args, **kwargs):\n        self.current_step += 1\n        step = self.current_step\n\n        self.update(step)", "\nclass SamplerBase(object):\n    def __init__(self, name: str, Sampler, frame_inpaint_support=False):\n        self.name = name\n        self.Sampler = Sampler\n        self.frame_inpaint_support = frame_inpaint_support\n\n    def register_buffers_to_model(self, sd_model, betas, device):\n        self.alphas = 1. - betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n\n        setattr(sd_model, 'device', device)\n        setattr(sd_model, 'betas', betas)\n        setattr(sd_model, 'alphas_cumprod', self.alphas_cumprod)\n\n    def init_sampler(self, sd_model, betas, device, **kwargs):\n        self.register_buffers_to_model(sd_model, betas, device)\n        return self.Sampler(sd_model, betas=betas, **kwargs)", "        \navailable_samplers = [\n    SamplerBase(\"DDIM_Gaussian\", GaussianDiffusion, True),\n    SamplerBase(\"DDIM\", DDIMSampler), \n    SamplerBase(\"UniPC\", UniPCSampler),\n]   \n\nclass Txt2VideoSampler(object):\n    def __init__(self, sd_model, device, betas=None, sampler_name=\"UniPC\"):\n        self.sd_model = sd_model\n        self.device = device\n        self.noise_gen = torch.Generator(device='cpu')\n        self.sampler_name = sampler_name\n        self.betas = betas\n        self.sampler = self.get_sampler(sampler_name, betas=self.betas)\n    \n    def get_noise(self, num_sample, channels, frames, height, width, latents=None, seed=1):\n        if latents is not None:\n            latents.to(self.device)\n\n            print(f\"Using input latents. Shape: {latents.shape}, Mean: {torch.mean(latents)}, Std: {torch.std(latents)}\")\n        else:\n            print(\"Sampling random noise.\")\n\n        num_sample = 1\n        max_frames = frames\n\n        latent_h, latent_w = get_height_width(height, width, 8)\n        shape = get_tensor_shape(num_sample, channels, max_frames, latent_h, latent_w, latents)\n\n        self.noise_gen.manual_seed(seed)\n        noise = torch.randn(shape, generator=self.noise_gen).to(self.device)\n        \n        return latents, noise, shape\n\n    def encode_latent(self, latent, noise, strength, steps):\n        encoded_latent = None\n        denoise_steps = None\n\n        if hasattr(self.sampler, 'unipc_encode'):\n            encoded_latent = self.sampler.unipc_encode(latent, self.device, strength, steps, noise=noise)\n\n        if hasattr(self.sampler, 'stochastic_encode'):\n            denoise_steps = int(strength * steps)\n            timestep = torch.tensor([denoise_steps] * int(latent.shape[0])).to(self.device)\n            self.sampler.make_schedule(steps)\n            encoded_latent = self.sampler.stochastic_encode(latent, timestep, noise=noise).to(dtype=latent.dtype)\n            self.sampler.sample = self.sampler.decode\n        \n        if hasattr(self.sampler, 'add_noise'):\n            denoise_steps = int(strength * steps)\n            timestep = self.sampler.get_time_steps(denoise_steps, latent.shape[0])\n            encoded_latent = self.sampler.add_noise(latent, noise, timestep[0].cpu())\n\n        if encoded_latent is None:\n            assert \"Could not find the appropriate function to encode the input latents\"\n        \n        return encoded_latent, denoise_steps\n            \n    def get_sampler(self, sampler_name: str, betas=None, return_sampler=True):\n        betas = betas if betas is not None else self.betas\n\n        for Sampler in available_samplers:\n            if sampler_name == Sampler.name:\n                sampler = Sampler.init_sampler(self.sd_model, betas=betas, device=self.device)\n\n                if Sampler.frame_inpaint_support:\n                    setattr(sampler, 'inpaint_masking', inpaint_masking)\n\n                if return_sampler:\n                    return sampler\n                else:\n                    self.sampler = sampler\n                    return\n\n        raise ValueError(f\"Sample {sampler_name} does not exist.\")\n        \n    def sample_loop(\n        self, \n        steps, \n        strength, \n        conditioning, \n        unconditional_conditioning,\n        batch_size,\n        latents=None,\n        shape=None,\n        noise=None,\n        is_vid2vid=False,\n        guidance_scale=1,\n        eta=0,\n        mask=None,\n        sampler_name=\"DDIM\"\n    ):\n        denoise_steps = None\n        # Assume that we are adding noise to existing latents (Image, Video, etc.)\n        if latents is not None and is_vid2vid:\n            latents, denoise_steps = self.encode_latent(latents, noise, strength, steps)\n        \n        # Create a callback that handles counting each step\n        sampler_callback = SamplerStepCallback(sampler_name, steps)\n\n        # Predict the noise sample\n        x0 = self.sampler.sample(\n            S=steps,\n            conditioning=conditioning,\n            strength=strength,\n            unconditional_conditioning=unconditional_conditioning,\n            batch_size=batch_size,\n            x_T=latents if latents is not None else noise,\n            x_latent=latents,\n            t_start=denoise_steps,\n            unconditional_guidance_scale=guidance_scale,\n            shape=shape,\n            callback=sampler_callback,\n            cond=conditioning,\n            eta=eta,\n            mask=mask\n        )\n\n        return x0"]}
{"filename": "scripts/samplers/ddim/sampler.py", "chunked_list": ["\"\"\"SAMPLING ONLY.\"\"\"\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom modules import shared\nfrom ldm.modules.diffusionmodules.util import make_ddim_sampling_parameters, make_ddim_timesteps, noise_like, extract_into_tensor\nfrom t2v_helpers.general_utils import reconstruct_conds\n\nclass DDIMSampler(object):\n    def __init__(self, model, schedule=\"linear\", device=torch.device(\"cuda\"), **kwargs):\n        super().__init__()\n        self.model = model\n        self.ddpm_num_timesteps = model.num_timesteps\n        self.schedule = schedule\n        self.device = device\n\n    def register_buffer(self, name, attr):\n        if type(attr) == torch.Tensor:\n            if attr.device != self.device:\n                attr = attr.to(self.device)\n        setattr(self, name, attr)\n\n    def make_schedule(self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0., verbose=False):\n        self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,\n                                                  num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)\n        alphas_cumprod = self.model.alphas_cumprod\n        assert alphas_cumprod.shape[0] == self.ddpm_num_timesteps, 'alphas have to be defined for each timestep'\n        to_torch = lambda x: x.clone().detach().to(torch.float32).to(self.model.device)\n\n        self.register_buffer('betas', to_torch(self.model.betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(self.model.alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod.cpu())))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu() - 1)))\n\n        # ddim sampling parameters\n        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n                                                                                   ddim_timesteps=self.ddim_timesteps,\n                                                                                   eta=ddim_eta,verbose=verbose)\n        self.register_buffer('ddim_sigmas', ddim_sigmas)\n        self.register_buffer('ddim_alphas', ddim_alphas)\n        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n            (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (\n                        1 - self.alphas_cumprod / self.alphas_cumprod_prev))\n        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n\n    @torch.no_grad()\n    def sample(self,\n               S,\n               batch_size,\n               shape,\n               conditioning=None,\n               callback=None,\n               normals_sequence=None,\n               img_callback=None,\n               quantize_x0=False,\n               eta=0.,\n               mask=None,\n               x0=None,\n               temperature=1.,\n               noise_dropout=0.,\n               score_corrector=None,\n               corrector_kwargs=None,\n               verbose=False,\n               x_T=None,\n               log_every_t=100,\n               unconditional_guidance_scale=1.,\n               unconditional_conditioning=None, # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n               dynamic_threshold=None,\n               ucg_schedule=None,\n               **kwargs\n               ):\n       \n\n        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)\n        # sampling\n        B, C, F, H, W = shape\n        size = (B, C, F, H, W)\n\n        print(f'Data shape for DDIM sampling is {size}, eta {eta}')\n\n        samples = self.ddim_sampling(conditioning, size,\n                                                    callback=callback,\n                                                    img_callback=img_callback,\n                                                    quantize_denoised=quantize_x0,\n                                                    mask=mask, x0=x0,\n                                                    ddim_use_original_steps=False,\n                                                    noise_dropout=noise_dropout,\n                                                    temperature=temperature,\n                                                    score_corrector=score_corrector,\n                                                    corrector_kwargs=corrector_kwargs,\n                                                    x_T=x_T,\n                                                    log_every_t=log_every_t,\n                                                    unconditional_guidance_scale=unconditional_guidance_scale,\n                                                    unconditional_conditioning=unconditional_conditioning,\n                                                    dynamic_threshold=dynamic_threshold,\n                                                    ucg_schedule=ucg_schedule\n                                                    )\n        return samples\n\n    @torch.no_grad()\n    def ddim_sampling(self, cond, shape,\n                      x_T=None, ddim_use_original_steps=False,\n                      callback=None, timesteps=None, quantize_denoised=False,\n                      mask=None, x0=None, img_callback=None, log_every_t=100,\n                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None, dynamic_threshold=None,\n                      ucg_schedule=None):\n                      \n        device = shared.device\n        b = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=device)\n        else:\n            img = x_T\n        \n        if timesteps is None:\n            timesteps = self.ddpm_num_timesteps if ddim_use_original_steps else self.ddim_timesteps\n        elif timesteps is not None and not ddim_use_original_steps:\n            subset_end = int(min(timesteps / self.ddim_timesteps.shape[0], 1) * self.ddim_timesteps.shape[0]) - 1\n            timesteps = self.ddim_timesteps[:subset_end]\n\n        intermediates = {'x_inter': [img], 'pred_x0': [img]}\n        time_range = reversed(range(0,timesteps)) if ddim_use_original_steps else np.flip(timesteps)\n        total_steps = timesteps if ddim_use_original_steps else timesteps.shape[0]\n        \n        #print(f\"Running DDIM Sampling with {total_steps} timesteps\")\n\n        iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps, disable=True)\n\n        for i, step in enumerate(iterator):\n            c, uc = reconstruct_conds(cond, unconditional_conditioning, step)\n\n            index = total_steps - i - 1\n            ts = torch.full((b,), step, device=device, dtype=torch.long)\n\n            #if mask is not None:\n            #    assert x0 is not None\n            #    img_orig = self.model.q_sample(x0, ts)  # TODO: deterministic forward pass?\n             #   img = img_orig * mask + (1. - mask) * img\n\n            if ucg_schedule is not None:\n                assert len(ucg_schedule) == len(time_range)\n                unconditional_guidance_scale = ucg_schedule[i]\n            \n            outs, _ = self.p_sample_ddim(img, c, ts, index=index, use_original_steps=ddim_use_original_steps,\n                                      quantize_denoised=quantize_denoised, temperature=temperature,\n                                      noise_dropout=noise_dropout, score_corrector=score_corrector,\n                                      corrector_kwargs=corrector_kwargs,\n                                      unconditional_guidance_scale=unconditional_guidance_scale,\n                                      unconditional_conditioning=uc,\n                                      dynamic_threshold=dynamic_threshold)\n\n            img = outs\n            if callback: callback(i)\n            if img_callback: img_callback(pred_x0, i)\n\n        return outs\n\n    @torch.no_grad()\n    def p_sample_ddim(self, x, c, t, index, repeat_noise=False, use_original_steps=False, quantize_denoised=False,\n                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None,\n                      dynamic_threshold=None):\n        b, *_, device = *x.shape, x.device\n        \n        if unconditional_conditioning is None or unconditional_guidance_scale == 1.:\n            model_output = self.model(x, t, c)\n        else:\n            noise = self.model(x, t, c)\n            noise_uncond = self.model(x, t, unconditional_conditioning)\n\n            model_output = noise_uncond + unconditional_guidance_scale * (noise - noise_uncond)\n\n        if self.model.parameterization == \"v\":\n            e_t = self.model.predict_eps_from_z_and_v(x, t, model_output)\n        else:\n            e_t = model_output\n\n        if score_corrector is not None:\n            assert self.model.parameterization == \"eps\", 'not implemented'\n            e_t = score_corrector.modify_score(self.model, e_t, x, t, c, **corrector_kwargs)\n\n        alphas = self.model.alphas_cumprod if use_original_steps else self.ddim_alphas\n        alphas_prev = self.model.alphas_cumprod_prev if use_original_steps else self.ddim_alphas_prev\n        sqrt_one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod if use_original_steps else self.ddim_sqrt_one_minus_alphas\n        sigmas = self.model.ddim_sigmas_for_original_num_steps if use_original_steps else self.ddim_sigmas\n        # select parameters corresponding to the currently considered timestep\n        a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n        a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n        sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n        sqrt_one_minus_at = torch.full((b, 1, 1, 1), sqrt_one_minus_alphas[index],device=device)\n\n        # current prediction for x_0\n        if self.model.parameterization != \"v\":\n            pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n        else:\n            pred_x0 = self.model.predict_start_from_z_and_v(x, t, model_output)\n\n        if quantize_denoised:\n            pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)\n\n        if dynamic_threshold is not None:\n            raise NotImplementedError()\n\n        # direction pointing to x_t\n        dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n        noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature\n        if noise_dropout > 0.:\n            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n        x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n        return x_prev, pred_x0\n\n    @torch.no_grad()\n    def encode(self, x0, c, t_enc, use_original_steps=False, return_intermediates=None,\n               unconditional_guidance_scale=1.0, unconditional_conditioning=None, callback=None):\n        num_reference_steps = self.ddpm_num_timesteps if use_original_steps else self.ddim_timesteps.shape[0]\n\n        assert t_enc <= num_reference_steps\n        num_steps = t_enc\n\n        if use_original_steps:\n            alphas_next = self.alphas_cumprod[:num_steps]\n            alphas = self.alphas_cumprod_prev[:num_steps]\n        else:\n            alphas_next = self.ddim_alphas[:num_steps]\n            alphas = torch.tensor(self.ddim_alphas_prev[:num_steps])\n\n        x_next = x0\n        intermediates = []\n        inter_steps = []\n        for i in tqdm(range(num_steps), desc='Encoding Image'):\n            t = torch.full((x0.shape[0],), i, device=self.model.device, dtype=torch.long)\n            if unconditional_guidance_scale == 1.:\n                noise_pred = self.model(x_next, t, c)\n            else:\n                assert unconditional_conditioning is not None\n                e_t_uncond, noise_pred = torch.chunk(\n                    self.model(torch.cat((x_next, x_next)), torch.cat((t, t)),\n                                           torch.cat((unconditional_conditioning, c))), 2)\n                noise_pred = e_t_uncond + unconditional_guidance_scale * (noise_pred - e_t_uncond)\n\n            xt_weighted = (alphas_next[i] / alphas[i]).sqrt() * x_next\n            weighted_noise_pred = alphas_next[i].sqrt() * (\n                    (1 / alphas_next[i] - 1).sqrt() - (1 / alphas[i] - 1).sqrt()) * noise_pred\n            x_next = xt_weighted + weighted_noise_pred\n            if return_intermediates and i % (\n                    num_steps // return_intermediates) == 0 and i < num_steps - 1:\n                intermediates.append(x_next)\n                inter_steps.append(i)\n            elif return_intermediates and i >= num_steps - 2:\n                intermediates.append(x_next)\n                inter_steps.append(i)\n            if callback: callback(i)\n\n        out = {'x_encoded': x_next, 'intermediate_steps': inter_steps}\n        if return_intermediates:\n            out.update({'intermediates': intermediates})\n        return x_next, out\n\n    @torch.no_grad()\n    def stochastic_encode(self, x0, t, use_original_steps=False, noise=None):\n        # fast, but does not allow for exact reconstruction\n        # t serves as an index to gather the correct alphas\n        if use_original_steps:\n            sqrt_alphas_cumprod = self.sqrt_alphas_cumprod\n            sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod\n        else:\n            sqrt_alphas_cumprod = torch.sqrt(self.ddim_alphas)\n            sqrt_one_minus_alphas_cumprod = self.ddim_sqrt_one_minus_alphas\n\n        if noise is None:\n            noise = torch.randn_like(x0)\n        return (extract_into_tensor(sqrt_alphas_cumprod, t, x0.shape) * x0 +\n                extract_into_tensor(sqrt_one_minus_alphas_cumprod, t, x0.shape) * noise)\n\n    @torch.no_grad()\n    def decode(self, x_latent, cond, t_start, unconditional_guidance_scale=1.0, unconditional_conditioning=None,\n               use_original_steps=False, callback=None, *args, **kwargs):\n\n        timesteps = np.arange(self.ddpm_num_timesteps) if use_original_steps else self.ddim_timesteps\n        timesteps = timesteps[:t_start]\n\n        time_range = np.flip(timesteps)\n        total_steps = timesteps.shape[0]\n\n        iterator = tqdm(time_range, desc='Decoding image', total=total_steps, disable=True)\n        x_dec = x_latent\n        for i, step in enumerate(iterator):\n            c, uc = reconstruct_conds(cond, unconditional_conditioning, step)\n\n            index = total_steps - i - 1\n            ts = torch.full((x_latent.shape[0],), step, device=x_latent.device, dtype=torch.long)\n            x_dec, _ = self.p_sample_ddim(x_dec, c, ts, index=index, use_original_steps=use_original_steps,\n                                          unconditional_guidance_scale=unconditional_guidance_scale,\n                                          unconditional_conditioning=uc)\n            if callback: callback(i)\n        return x_dec", "\nclass DDIMSampler(object):\n    def __init__(self, model, schedule=\"linear\", device=torch.device(\"cuda\"), **kwargs):\n        super().__init__()\n        self.model = model\n        self.ddpm_num_timesteps = model.num_timesteps\n        self.schedule = schedule\n        self.device = device\n\n    def register_buffer(self, name, attr):\n        if type(attr) == torch.Tensor:\n            if attr.device != self.device:\n                attr = attr.to(self.device)\n        setattr(self, name, attr)\n\n    def make_schedule(self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0., verbose=False):\n        self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,\n                                                  num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)\n        alphas_cumprod = self.model.alphas_cumprod\n        assert alphas_cumprod.shape[0] == self.ddpm_num_timesteps, 'alphas have to be defined for each timestep'\n        to_torch = lambda x: x.clone().detach().to(torch.float32).to(self.model.device)\n\n        self.register_buffer('betas', to_torch(self.model.betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(self.model.alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod.cpu())))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu() - 1)))\n\n        # ddim sampling parameters\n        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n                                                                                   ddim_timesteps=self.ddim_timesteps,\n                                                                                   eta=ddim_eta,verbose=verbose)\n        self.register_buffer('ddim_sigmas', ddim_sigmas)\n        self.register_buffer('ddim_alphas', ddim_alphas)\n        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n            (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (\n                        1 - self.alphas_cumprod / self.alphas_cumprod_prev))\n        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n\n    @torch.no_grad()\n    def sample(self,\n               S,\n               batch_size,\n               shape,\n               conditioning=None,\n               callback=None,\n               normals_sequence=None,\n               img_callback=None,\n               quantize_x0=False,\n               eta=0.,\n               mask=None,\n               x0=None,\n               temperature=1.,\n               noise_dropout=0.,\n               score_corrector=None,\n               corrector_kwargs=None,\n               verbose=False,\n               x_T=None,\n               log_every_t=100,\n               unconditional_guidance_scale=1.,\n               unconditional_conditioning=None, # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n               dynamic_threshold=None,\n               ucg_schedule=None,\n               **kwargs\n               ):\n       \n\n        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)\n        # sampling\n        B, C, F, H, W = shape\n        size = (B, C, F, H, W)\n\n        print(f'Data shape for DDIM sampling is {size}, eta {eta}')\n\n        samples = self.ddim_sampling(conditioning, size,\n                                                    callback=callback,\n                                                    img_callback=img_callback,\n                                                    quantize_denoised=quantize_x0,\n                                                    mask=mask, x0=x0,\n                                                    ddim_use_original_steps=False,\n                                                    noise_dropout=noise_dropout,\n                                                    temperature=temperature,\n                                                    score_corrector=score_corrector,\n                                                    corrector_kwargs=corrector_kwargs,\n                                                    x_T=x_T,\n                                                    log_every_t=log_every_t,\n                                                    unconditional_guidance_scale=unconditional_guidance_scale,\n                                                    unconditional_conditioning=unconditional_conditioning,\n                                                    dynamic_threshold=dynamic_threshold,\n                                                    ucg_schedule=ucg_schedule\n                                                    )\n        return samples\n\n    @torch.no_grad()\n    def ddim_sampling(self, cond, shape,\n                      x_T=None, ddim_use_original_steps=False,\n                      callback=None, timesteps=None, quantize_denoised=False,\n                      mask=None, x0=None, img_callback=None, log_every_t=100,\n                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None, dynamic_threshold=None,\n                      ucg_schedule=None):\n                      \n        device = shared.device\n        b = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=device)\n        else:\n            img = x_T\n        \n        if timesteps is None:\n            timesteps = self.ddpm_num_timesteps if ddim_use_original_steps else self.ddim_timesteps\n        elif timesteps is not None and not ddim_use_original_steps:\n            subset_end = int(min(timesteps / self.ddim_timesteps.shape[0], 1) * self.ddim_timesteps.shape[0]) - 1\n            timesteps = self.ddim_timesteps[:subset_end]\n\n        intermediates = {'x_inter': [img], 'pred_x0': [img]}\n        time_range = reversed(range(0,timesteps)) if ddim_use_original_steps else np.flip(timesteps)\n        total_steps = timesteps if ddim_use_original_steps else timesteps.shape[0]\n        \n        #print(f\"Running DDIM Sampling with {total_steps} timesteps\")\n\n        iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps, disable=True)\n\n        for i, step in enumerate(iterator):\n            c, uc = reconstruct_conds(cond, unconditional_conditioning, step)\n\n            index = total_steps - i - 1\n            ts = torch.full((b,), step, device=device, dtype=torch.long)\n\n            #if mask is not None:\n            #    assert x0 is not None\n            #    img_orig = self.model.q_sample(x0, ts)  # TODO: deterministic forward pass?\n             #   img = img_orig * mask + (1. - mask) * img\n\n            if ucg_schedule is not None:\n                assert len(ucg_schedule) == len(time_range)\n                unconditional_guidance_scale = ucg_schedule[i]\n            \n            outs, _ = self.p_sample_ddim(img, c, ts, index=index, use_original_steps=ddim_use_original_steps,\n                                      quantize_denoised=quantize_denoised, temperature=temperature,\n                                      noise_dropout=noise_dropout, score_corrector=score_corrector,\n                                      corrector_kwargs=corrector_kwargs,\n                                      unconditional_guidance_scale=unconditional_guidance_scale,\n                                      unconditional_conditioning=uc,\n                                      dynamic_threshold=dynamic_threshold)\n\n            img = outs\n            if callback: callback(i)\n            if img_callback: img_callback(pred_x0, i)\n\n        return outs\n\n    @torch.no_grad()\n    def p_sample_ddim(self, x, c, t, index, repeat_noise=False, use_original_steps=False, quantize_denoised=False,\n                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None,\n                      dynamic_threshold=None):\n        b, *_, device = *x.shape, x.device\n        \n        if unconditional_conditioning is None or unconditional_guidance_scale == 1.:\n            model_output = self.model(x, t, c)\n        else:\n            noise = self.model(x, t, c)\n            noise_uncond = self.model(x, t, unconditional_conditioning)\n\n            model_output = noise_uncond + unconditional_guidance_scale * (noise - noise_uncond)\n\n        if self.model.parameterization == \"v\":\n            e_t = self.model.predict_eps_from_z_and_v(x, t, model_output)\n        else:\n            e_t = model_output\n\n        if score_corrector is not None:\n            assert self.model.parameterization == \"eps\", 'not implemented'\n            e_t = score_corrector.modify_score(self.model, e_t, x, t, c, **corrector_kwargs)\n\n        alphas = self.model.alphas_cumprod if use_original_steps else self.ddim_alphas\n        alphas_prev = self.model.alphas_cumprod_prev if use_original_steps else self.ddim_alphas_prev\n        sqrt_one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod if use_original_steps else self.ddim_sqrt_one_minus_alphas\n        sigmas = self.model.ddim_sigmas_for_original_num_steps if use_original_steps else self.ddim_sigmas\n        # select parameters corresponding to the currently considered timestep\n        a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n        a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n        sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n        sqrt_one_minus_at = torch.full((b, 1, 1, 1), sqrt_one_minus_alphas[index],device=device)\n\n        # current prediction for x_0\n        if self.model.parameterization != \"v\":\n            pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n        else:\n            pred_x0 = self.model.predict_start_from_z_and_v(x, t, model_output)\n\n        if quantize_denoised:\n            pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)\n\n        if dynamic_threshold is not None:\n            raise NotImplementedError()\n\n        # direction pointing to x_t\n        dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n        noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature\n        if noise_dropout > 0.:\n            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n        x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n        return x_prev, pred_x0\n\n    @torch.no_grad()\n    def encode(self, x0, c, t_enc, use_original_steps=False, return_intermediates=None,\n               unconditional_guidance_scale=1.0, unconditional_conditioning=None, callback=None):\n        num_reference_steps = self.ddpm_num_timesteps if use_original_steps else self.ddim_timesteps.shape[0]\n\n        assert t_enc <= num_reference_steps\n        num_steps = t_enc\n\n        if use_original_steps:\n            alphas_next = self.alphas_cumprod[:num_steps]\n            alphas = self.alphas_cumprod_prev[:num_steps]\n        else:\n            alphas_next = self.ddim_alphas[:num_steps]\n            alphas = torch.tensor(self.ddim_alphas_prev[:num_steps])\n\n        x_next = x0\n        intermediates = []\n        inter_steps = []\n        for i in tqdm(range(num_steps), desc='Encoding Image'):\n            t = torch.full((x0.shape[0],), i, device=self.model.device, dtype=torch.long)\n            if unconditional_guidance_scale == 1.:\n                noise_pred = self.model(x_next, t, c)\n            else:\n                assert unconditional_conditioning is not None\n                e_t_uncond, noise_pred = torch.chunk(\n                    self.model(torch.cat((x_next, x_next)), torch.cat((t, t)),\n                                           torch.cat((unconditional_conditioning, c))), 2)\n                noise_pred = e_t_uncond + unconditional_guidance_scale * (noise_pred - e_t_uncond)\n\n            xt_weighted = (alphas_next[i] / alphas[i]).sqrt() * x_next\n            weighted_noise_pred = alphas_next[i].sqrt() * (\n                    (1 / alphas_next[i] - 1).sqrt() - (1 / alphas[i] - 1).sqrt()) * noise_pred\n            x_next = xt_weighted + weighted_noise_pred\n            if return_intermediates and i % (\n                    num_steps // return_intermediates) == 0 and i < num_steps - 1:\n                intermediates.append(x_next)\n                inter_steps.append(i)\n            elif return_intermediates and i >= num_steps - 2:\n                intermediates.append(x_next)\n                inter_steps.append(i)\n            if callback: callback(i)\n\n        out = {'x_encoded': x_next, 'intermediate_steps': inter_steps}\n        if return_intermediates:\n            out.update({'intermediates': intermediates})\n        return x_next, out\n\n    @torch.no_grad()\n    def stochastic_encode(self, x0, t, use_original_steps=False, noise=None):\n        # fast, but does not allow for exact reconstruction\n        # t serves as an index to gather the correct alphas\n        if use_original_steps:\n            sqrt_alphas_cumprod = self.sqrt_alphas_cumprod\n            sqrt_one_minus_alphas_cumprod = self.sqrt_one_minus_alphas_cumprod\n        else:\n            sqrt_alphas_cumprod = torch.sqrt(self.ddim_alphas)\n            sqrt_one_minus_alphas_cumprod = self.ddim_sqrt_one_minus_alphas\n\n        if noise is None:\n            noise = torch.randn_like(x0)\n        return (extract_into_tensor(sqrt_alphas_cumprod, t, x0.shape) * x0 +\n                extract_into_tensor(sqrt_one_minus_alphas_cumprod, t, x0.shape) * noise)\n\n    @torch.no_grad()\n    def decode(self, x_latent, cond, t_start, unconditional_guidance_scale=1.0, unconditional_conditioning=None,\n               use_original_steps=False, callback=None, *args, **kwargs):\n\n        timesteps = np.arange(self.ddpm_num_timesteps) if use_original_steps else self.ddim_timesteps\n        timesteps = timesteps[:t_start]\n\n        time_range = np.flip(timesteps)\n        total_steps = timesteps.shape[0]\n\n        iterator = tqdm(time_range, desc='Decoding image', total=total_steps, disable=True)\n        x_dec = x_latent\n        for i, step in enumerate(iterator):\n            c, uc = reconstruct_conds(cond, unconditional_conditioning, step)\n\n            index = total_steps - i - 1\n            ts = torch.full((x_latent.shape[0],), step, device=x_latent.device, dtype=torch.long)\n            x_dec, _ = self.p_sample_ddim(x_dec, c, ts, index=index, use_original_steps=use_original_steps,\n                                          unconditional_guidance_scale=unconditional_guidance_scale,\n                                          unconditional_conditioning=uc)\n            if callback: callback(i)\n        return x_dec"]}
{"filename": "scripts/samplers/ddim/gaussian_sampler.py", "chunked_list": ["import torch\nfrom modelscope.t2v_model import _i\nfrom t2v_helpers.general_utils import reconstruct_conds\n\nclass GaussianDiffusion(object):\n    r\"\"\" Diffusion Model for DDIM.\n    \"Denoising diffusion implicit models.\" by Song, Jiaming, Chenlin Meng, and Stefano Ermon.\n    See https://arxiv.org/abs/2010.02502\n    \"\"\"\n\n    def __init__(self,\n                model,\n                betas,\n                mean_type='eps',\n                var_type='learned_range',\n                loss_type='mse',\n                epsilon=1e-12,\n                rescale_timesteps=False,\n                **kwargs):\n\n        # check input\n        self.check_input_vars(betas, mean_type, var_type, loss_type)\n\n        self.model = model\n        self.betas = betas\n        self.num_timesteps = len(betas)\n        self.mean_type = mean_type\n        self.var_type = var_type\n        self.loss_type = loss_type\n        self.epsilon = epsilon\n        self.rescale_timesteps = rescale_timesteps\n\n        # alphas\n        alphas = 1 - self.betas\n        self.alphas_cumprod = torch.cumprod(alphas, dim=0)\n        self.alphas_cumprod_prev = torch.cat([alphas.new_ones([1]), self.alphas_cumprod[:-1]])\n        self.alphas_cumprod_next = torch.cat([self.alphas_cumprod[1:],alphas.new_zeros([1])])\n\n        # q(x_t | x_{t-1})\n        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n        self.log_one_minus_alphas_cumprod = torch.log(1.0 - self.alphas_cumprod)\n        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)\n\n        # q(x_{t-1} | x_t, x_0)\n        self.posterior_variance = betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        self.posterior_log_variance_clipped = torch.log(self.posterior_variance.clamp(1e-20))\n        self.posterior_mean_coef1 = betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        self.posterior_mean_coef2 = (1.0 - self.alphas_cumprod_prev) * torch.sqrt(alphas) / (1.0 - self.alphas_cumprod)\n\n    def check_input_vars(self, betas, mean_type, var_type, loss_type):\n        mean_types = ['x0', 'x_{t-1}', 'eps']\n        var_types = ['learned', 'learned_range', 'fixed_large', 'fixed_small']\n        loss_types = ['mse', 'rescaled_mse', 'kl', 'rescaled_kl', 'l1', 'rescaled_l1','charbonnier']\n\n        if not isinstance(betas, torch.DoubleTensor):\n                betas = torch.tensor(betas, dtype=torch.float64)\n\n        assert min(betas) > 0 and max(betas) <= 1\n        assert mean_type in mean_types\n        assert var_type in var_types\n        assert loss_type in loss_types\n        \n    def validate_model_kwargs(self, model_kwargs):\n        \"\"\"\n        Use the original implementation of passing model kwargs to the model.\n        eg:  model_kwargs=[{'y':c_i}, {'y':uc_i,}]\n        \"\"\"\n        if len(model_kwargs) > 0:\n            assert isinstance(model_kwargs, list) and len(model_kwargs) == 2\n\n    def get_time_steps(self, ddim_timesteps, batch_size=1, step=None):\n        b = batch_size\n\n        # Get thhe full timestep range\n        arange_steps = (1 + torch.arange(0, self.num_timesteps, ddim_timesteps))\n        steps = arange_steps.clamp(0, self.num_timesteps - 1)\n        timesteps = steps.flip(0).to(self.model.device)\n\n        if step is not None:\n            # Get the current timestep during a sample loop\n            timesteps = torch.full((b, ), timesteps[step], dtype=torch.long, device=self.model.device)\n\n        return timesteps\n\n    def add_noise(self, xt, noise, t):\n        noisy_sample = self.sqrt_alphas_cumprod[t.cpu()].to(self.model.device) * \\\n            xt + noise * self.sqrt_one_minus_alphas_cumprod[t.cpu()].to(self.model.device)\n\n        return noisy_sample\n\n    def get_dim(self, y_out):\n        is_fixed = self.var_type.startswith('fixed')\n        return y_out.size(1) if is_fixed else y_out.size(1) // 2\n\n    def fixed_small_variance(self, xt, t):\n        var = _i(self.posterior_variance, t, xt)\n        log_var = _i(self.posterior_log_variance_clipped, t, xt)\n\n        return var, log_var\n\n    def mean_x0(self, xt, t, x_out):\n        x0 = _i(self.sqrt_recip_alphas_cumprod, t, xt) * xt - _i(\n                self.sqrt_recipm1_alphas_cumprod, t, xt) * x_out\n        mu, _, _ = self.q_posterior_mean_variance(x0, xt, t)\n\n        return x0, mu\n\n    def restrict_range_x0(self, percentile, x0, clamp=False):\n        if not clamp:\n            assert percentile > 0 and percentile <= 1  # e.g., 0.995\n            s = torch.quantile(x0.flatten(1).abs(), percentile,dim=1)\n            s.clamp_(1.0).view(-1, 1, 1, 1)\n\n            x0 = torch.min(s, torch.max(-s, x0)) / s\n        else:\n            x0 = x0.clamp(-clamp, clamp)\n\n        return x0\n\n    def is_unconditional(self, guide_scale):\n        return guide_scale is None or guide_scale == 1\n\n    def do_classifier_guidance(self, y_out, u_out, guidance_scale):\n        \"\"\"\n        y_out: Condition\n        u_out: Unconditional\n        \"\"\"\n        dim = self.get_dim(y_out)\n        a = u_out[:, :dim]\n        b = guidance_scale * (y_out[:, :dim] - u_out[:, :dim])\n        c = y_out[:, dim:]\n        out = torch.cat([a + b, c], dim=1)\n\n        return out\n        \n    def p_mean_variance(self,\n                        xt,\n                        t,\n                        model_kwargs={},\n                        clamp=None,\n                        percentile=None,\n                        guide_scale=None,\n                        conditioning=None,\n                        unconditional_conditioning=None,\n                        only_x0=True,\n                        **kwargs):\n        r\"\"\"Distribution of p(x_{t-1} | x_t).\"\"\"\n\n        # predict distribution\n        if self.is_unconditional(guide_scale):\n            out = self.model(xt, self._scale_timesteps(t), conditioning)\n        else:\n            # classifier-free guidance\n            if model_kwargs != {}:\n                self.validate_model_kwargs(model_kwargs)\n                conditioning = model_kwargs[0]\n                unconditional_conditioning = model_kwargs[1]\n\n            y_out = self.model(xt, self._scale_timesteps(t), conditioning)\n            u_out = self.model(xt, self._scale_timesteps(t), unconditional_conditioning)\n\n            out = self.do_classifier_guidance(y_out, u_out, guide_scale)\n\n        # compute variance\n        if self.var_type == 'fixed_small':\n            var, log_var = self.fixed_small_variance(xt, t)\n\n        # compute mean and x0\n        if self.mean_type == 'eps':\n            x0, mu = self.mean_x0(xt, t, out)\n\n        # restrict the range of x0\n        if percentile is not None:\n           x0 = self.restrict_range_x0(percentile, x0)\n        elif clamp is not None:\n            x0 = self.restrict_range_x0(percentile, x0, clamp=True)\n\n        if only_x0:\n            return x0\n        else:\n            return mu, var, log_var, x0\n\n    def q_posterior_mean_variance(self, x0, xt, t):\n        r\"\"\"Distribution of q(x_{t-1} | x_t, x_0).\n        \"\"\"\n        mu = _i(self.posterior_mean_coef1, t, xt) * x0 + _i(\n            self.posterior_mean_coef2, t, xt) * xt\n        var = _i(self.posterior_variance, t, xt)\n        log_var = _i(self.posterior_log_variance_clipped, t, xt)\n        return mu, var, log_var\n\n    def _scale_timesteps(self, t):\n        if self.rescale_timesteps:\n            return t.float() * 1000.0 / self.num_timesteps\n        return t\n\n    def get_eps(self, xt, x0, t, alpha, condition_fn, model_kwargs={}):\n        # x0 -> eps\n        eps = (_i(self.sqrt_recip_alphas_cumprod, t, xt) * xt - x0) / _i(\n                self.sqrt_recipm1_alphas_cumprod, t, xt)\n        \n        if condition_fn is not None:\n            eps = eps - (1 - alpha).sqrt() * condition_fn(\n                    xt, self._scale_timesteps(t), **model_kwargs)\n            # eps -> x0\n            x0 = _i(self.sqrt_recip_alphas_cumprod, t, xt) * xt - _i(\n                    self.sqrt_recipm1_alphas_cumprod, t, xt) * eps\n\n        return eps, x0\n\n    @torch.no_grad()\n    def sample(self,\n                    x_T=None,\n                    S=5,\n                    shape=None,\n                    conditioning=None,\n                    unconditional_conditioning=None,\n                    model_kwargs={},\n                    clamp=None,\n                    percentile=None,\n                    condition_fn=None,\n                    unconditional_guidance_scale=None,\n                    eta=0.0,\n                    callback=None,\n                    mask=None,\n                    **kwargs):\n        r\"\"\"Sample from p(x_{t-1} | x_t) using DDIM.\n            - condition_fn: for classifier-based guidance (guided-diffusion).\n            - guide_scale: for classifier-free guidance (glide/dalle-2).\n        \"\"\"\n        \n        # Shape must exist to sample\n        if shape is None and x_T is None:\n            assert \"Shape must exists to sample from noise\"\n\n        # Assign variables for sampling\n        steps = S\n        stride = self.num_timesteps // steps\n        guide_scale = unconditional_guidance_scale\n        original_latents = None\n\n        if x_T is None:\n            xt = torch.randn(shape, device=self.model.device)\n        else:\n            xt = x_T.clone()\n            original_latents = xt\n        \n        timesteps = self.get_time_steps(stride, xt.shape[0])\n\n        for step in range(0, steps):\n            c, uc = reconstruct_conds(conditioning, unconditional_conditioning, step)\n            t = self.get_time_steps(stride, xt.shape[0], step=step)\n\n            # predict distribution of p(x_{t-1} | x_t)\n            x0 = self.p_mean_variance(\n                xt, \n                t,\n                 model_kwargs, \n                 clamp, \n                 percentile, \n                 guide_scale, \n                 conditioning=c,\n                 unconditional_conditioning=uc,\n                 **kwargs\n                )\n\n            alphas = _i(self.alphas_cumprod, t, xt)\n            alphas_prev = _i(self.alphas_cumprod, (t - stride).clamp(0), xt)\n\n            eps, x0 = self.get_eps(xt, x0, t, alphas, condition_fn)\n\n            a = (1 - alphas_prev) / (1 - alphas)\n            b = (1 - alphas / alphas_prev)\n            sigmas = eta * torch.sqrt(a * b)\n\n            # random sample\n            noise = torch.randn_like(xt)\n            direction = torch.sqrt(1 - alphas_prev - sigmas**2) * eps\n            mask = t.ne(0).float().view(-1, *((1, ) * (xt.ndim - 1)))\n            xt_1 = torch.sqrt(alphas_prev) * x0 + direction + mask * sigmas * noise\n            xt = xt_1\n    \n            if hasattr(self, 'inpaint_masking') and mask is not None:\n                add_noise_args = {\n                    \"xt\":xt, \n                    \"noise\": torch.randn_like(xt), \n                    \"t\": timesteps[(step - 1) + 1]\n                }\n                self.inpaint_masking(xt, step, steps, mask, self.add_noise, add_noise_args)\n\n            if callback is not None:\n                callback(step)\n\n        return xt", "        \n\n\n        "]}
{"filename": "scripts/samplers/uni_pc/uni_pc.py", "chunked_list": ["import torch\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange,repeat\nfrom modules.shared import state\nfrom t2v_helpers.general_utils import reconstruct_conds\n\nclass NoiseScheduleVP:\n    def __init__(\n            self,\n            schedule='discrete',\n            betas=None,\n            alphas_cumprod=None,\n            continuous_beta_0=0.1,\n            continuous_beta_1=20.,\n        ):\n        \"\"\"Create a wrapper class for the forward SDE (VP type).\n        ***\n        Update: We support discrete-time diffusion models by implementing a picewise linear interpolation for log_alpha_t.\n                We recommend to use schedule='discrete' for the discrete-time diffusion models, especially for high-resolution images.\n        ***\n        The forward SDE ensures that the condition distribution q_{t|0}(x_t | x_0) = N ( alpha_t * x_0, sigma_t^2 * I ).\n        We further define lambda_t = log(alpha_t) - log(sigma_t), which is the half-logSNR (described in the DPM-Solver paper).\n        Therefore, we implement the functions for computing alpha_t, sigma_t and lambda_t. For t in [0, T], we have:\n            log_alpha_t = self.marginal_log_mean_coeff(t)\n            sigma_t = self.marginal_std(t)\n            lambda_t = self.marginal_lambda(t)\n        Moreover, as lambda(t) is an invertible function, we also support its inverse function:\n            t = self.inverse_lambda(lambda_t)\n        ===============================================================\n        We support both discrete-time DPMs (trained on n = 0, 1, ..., N-1) and continuous-time DPMs (trained on t in [t_0, T]).\n        1. For discrete-time DPMs:\n            For discrete-time DPMs trained on n = 0, 1, ..., N-1, we convert the discrete steps to continuous time steps by:\n                t_i = (i + 1) / N\n            e.g. for N = 1000, we have t_0 = 1e-3 and T = t_{N-1} = 1.\n            We solve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n        2. For continuous-time DPMs:\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n        ===============================================================\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n        \n        ===============================================================\n        Example:\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n        \"\"\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(\"Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\".format(schedule))\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \"\"\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \"\"\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \"\"\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \"\"\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n    def inverse_lambda(self, lamb):\n        \"\"\"\n        Compute the continuous-time label t in [0, T] of a given half-logSNR lambda_t.\n        \"\"\"\n        if self.schedule == 'linear':\n            tmp = 2. * (self.beta_1 - self.beta_0) * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t", "\n\ndef model_wrapper(\n    model,\n    noise_schedule,\n    model_type=\"noise\",\n    model_kwargs={},\n    guidance_type=\"uncond\",\n    condition=None,\n    unconditional_condition=None,\n    guidance_scale=1.,\n    classifier_fn=None,\n    classifier_kwargs={},\n):\n    \"\"\"Create a wrapper function for the noise prediction model.\n    DPM-Solver needs to solve the continuous-time diffusion ODEs. For DPMs trained on discrete-time labels, we need to\n    firstly wrap the model function to a noise prediction model that accepts the continuous time as the input.\n    We support four types of the diffusion model by setting `model_type`:\n        1. \"noise\": noise prediction model. (Trained by predicting noise).\n        2. \"x_start\": data prediction model. (Trained by predicting the data x_0 at time 0).\n        3. \"v\": velocity prediction model. (Trained by predicting the velocity).\n            The \"v\" prediction is derivation detailed in Appendix D of [1], and is used in Imagen-Video [2].\n            [1] Salimans, Tim, and Jonathan Ho. \"Progressive distillation for fast sampling of diffusion models.\"\n                arXiv preprint arXiv:2202.00512 (2022).\n            [2] Ho, Jonathan, et al. \"Imagen Video: High Definition Video Generation with Diffusion Models.\"\n                arXiv preprint arXiv:2210.02303 (2022).\n    \n        4. \"score\": marginal score function. (Trained by denoising score matching).\n            Note that the score function and the noise prediction model follows a simple relationship:\n            ```\n                noise(x_t, t) = -sigma_t * score(x_t, t)\n            ```\n    We support three types of guided sampling by DPMs by setting `guidance_type`:\n        1. \"uncond\": unconditional sampling by DPMs.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\n            ``\n        2. \"classifier\": classifier guidance sampling [3] by DPMs and another classifier.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\n            `` \n            The input `classifier_fn` has the following format:\n            ``\n                classifier_fn(x, t_input, cond, **classifier_kwargs) -> logits(x, t_input, cond)\n            ``\n            [3] P. Dhariwal and A. Q. Nichol, \"Diffusion models beat GANs on image synthesis,\"\n                in Advances in Neural Information Processing Systems, vol. 34, 2021, pp. 8780-8794.\n        3. \"classifier-free\": classifier-free guidance sampling by conditional DPMs.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, cond, **model_kwargs) -> noise | x_start | v | score\n            `` \n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n            [4] Ho, Jonathan, and Tim Salimans. \"Classifier-free diffusion guidance.\"\n                arXiv preprint arXiv:2207.12598 (2022).\n        \n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)         \n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n    ===============================================================\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \"noise\" or \"x_start\" or \"v\" or \"score\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \"uncond\" or \"classifier\" or \"classifier-free\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \"classifier\" or \"classifier-free\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \"classifier-free\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \"\"\"\n\n    def get_model_input_time(t_continuous):\n        \"\"\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \"\"\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        if cond is None:\n            output = model(x, t_input, None, **model_kwargs)\n        else:\n            output = model(x, t_input, cond, **model_kwargs)\n        if model_type == \"noise\":\n            return output\n        elif model_type == \"x_start\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \"v\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \"score\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input):\n        \"\"\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \"\"\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous):\n        \"\"\"\n        The noise predicition model function that is used for DPM-Solver.\n        \"\"\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \"uncond\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \"classifier\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \"classifier-free\":\n            c, uc = reconstruct_conds(condition, unconditional_condition, state.sampling_step)\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=c)\n            else:   \n                noise = noise_pred_fn(x, t_continuous, cond=c)\n                noise_uncond = noise_pred_fn(x, t_continuous, cond=uc)\n\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\"noise\", \"x_start\", \"v\"]\n    assert guidance_type in [\"uncond\", \"classifier\", \"classifier-free\"]\n    return model_fn", "\n\nclass UniPC:\n    def __init__(\n        self,\n        model_fn,\n        noise_schedule,\n        predict_x0=True,\n        thresholding=False,\n        max_val=1.,\n        variant='bh1'\n    ):\n        \"\"\"Construct a UniPC. \n        We support both data_prediction and noise_prediction.\n        \"\"\"\n        self.model = model_fn\n        self.noise_schedule = noise_schedule\n        self.variant = variant\n        self.predict_x0 = predict_x0\n        self.thresholding = thresholding\n        self.max_val = max_val\n        \n    def dynamic_thresholding_fn(self, x0, t=None):\n        \"\"\"\n        The dynamic thresholding method. \n        \"\"\"\n        dims = x0.dim()\n        p = self.dynamic_thresholding_ratio\n        s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n        s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n        x0 = torch.clamp(x0, -s, s) / s\n        return x0\n\n    def noise_prediction_fn(self, x, t):\n        \"\"\"\n        Return the noise prediction model.\n        \"\"\"\n        return self.model(x, t)\n\n    def data_prediction_fn(self, x, t):\n        \"\"\"\n        Return the data prediction model (with thresholding).\n        \"\"\"\n        noise = self.noise_prediction_fn(x, t)\n        dims = x.dim()\n        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)\n        x0 = (x - expand_dims(sigma_t, dims) * noise) / expand_dims(alpha_t, dims)\n        if self.thresholding:\n            p = 0.995   # A hyperparameter in the paper of \"Imagen\" [1].\n            s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n            s = expand_dims(torch.maximum(s, self.max_val * torch.ones_like(s).to(s.device)), dims)\n            x0 = torch.clamp(x0, -s, s) / s\n        return x0\n\n    def unipc_encode(self, x, t, noise=None):\n        \"\"\"\n        Encodes a latent determined by noise input and a given timestep.\n        \"\"\"\n        noise = torch.randn_like(x) if noise is None else noise\n        dims = x.dim()\n        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)\n        x0 = (expand_dims(sigma_t, dims) * noise) + expand_dims(alpha_t, dims) * x\n        return x0\n\n    def model_fn(self, x, t):\n        \"\"\"\n        Convert the model to the noise prediction model or the data prediction model. \n        \"\"\"\n        if self.predict_x0:\n            return self.data_prediction_fn(x, t)\n        else:\n            return self.noise_prediction_fn(x, t)\n\n    def get_time_steps(self, skip_type, t_T, t_0, N, device):\n        \"\"\"Compute the intermediate time steps for sampling.\n        \"\"\"\n        if skip_type == 'logSNR':\n            lambda_T = self.noise_schedule.marginal_lambda(torch.tensor(t_T).to(device))\n            lambda_0 = self.noise_schedule.marginal_lambda(torch.tensor(t_0).to(device))\n            logSNR_steps = torch.linspace(lambda_T.cpu().item(), lambda_0.cpu().item(), N + 1).to(device)\n            return self.noise_schedule.inverse_lambda(logSNR_steps)\n        elif skip_type == 'time_uniform':\n            return torch.linspace(t_T, t_0, N + 1).to(device)\n        elif skip_type == 'time_quadratic':\n            t_order = 2\n            t = torch.linspace(t_T**(1. / t_order), t_0**(1. / t_order), N + 1).pow(t_order).to(device)\n            return t\n        else:\n            raise ValueError(\"Unsupported skip_type {}, need to be 'logSNR' or 'time_uniform' or 'time_quadratic'\".format(skip_type))\n\n    def get_orders_and_timesteps_for_singlestep_solver(self, steps, order, skip_type, t_T, t_0, device):\n        \"\"\"\n        Get the order of each step for sampling by the singlestep DPM-Solver.\n        \"\"\"\n        if order == 3:\n            K = steps // 3 + 1\n            if steps % 3 == 0:\n                orders = [3,] * (K - 2) + [2, 1]\n            elif steps % 3 == 1:\n                orders = [3,] * (K - 1) + [1]\n            else:\n                orders = [3,] * (K - 1) + [2]\n        elif order == 2:\n            if steps % 2 == 0:\n                K = steps // 2\n                orders = [2,] * K\n            else:\n                K = steps // 2 + 1\n                orders = [2,] * (K - 1) + [1]\n        elif order == 1:\n            K = steps\n            orders = [1,] * steps\n        else:\n            raise ValueError(\"'order' must be '1' or '2' or '3'.\")\n        if skip_type == 'logSNR':\n            # To reproduce the results in DPM-Solver paper\n            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, K, device)\n        else:\n            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, steps, device)[torch.cumsum(torch.tensor([0,] + orders), 0).to(device)]\n        return timesteps_outer, orders\n\n    def denoise_to_zero_fn(self, x, s):\n        \"\"\"\n        Denoise at the final step, which is equivalent to solve the ODE from lambda_s to infty by first-order discretization. \n        \"\"\"\n        return self.data_prediction_fn(x, s)\n\n    def multistep_uni_pc_update(self, x, model_prev_list, t_prev_list, t, order, **kwargs):\n        if len(t.shape) == 0:\n            t = t.view(-1)\n        if 'bh' in self.variant:\n            return self.multistep_uni_pc_bh_update(x, model_prev_list, t_prev_list, t, order, **kwargs)\n        else:\n            assert self.variant == 'vary_coeff'\n            return self.multistep_uni_pc_vary_update(x, model_prev_list, t_prev_list, t, order, **kwargs)\n\n    def multistep_uni_pc_vary_update(self, x, model_prev_list, t_prev_list, t, order, use_corrector=True):\n        #print(f'using unified predictor-corrector with order {order} (solver type: vary coeff)')\n        ns = self.noise_schedule\n        assert order <= len(model_prev_list)\n\n        # first compute rks\n        t_prev_0 = t_prev_list[-1]\n        lambda_prev_0 = ns.marginal_lambda(t_prev_0)\n        lambda_t = ns.marginal_lambda(t)\n        model_prev_0 = model_prev_list[-1]\n        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\n        log_alpha_t = ns.marginal_log_mean_coeff(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        h = lambda_t - lambda_prev_0\n\n        rks = []\n        D1s = []\n        for i in range(1, order):\n            t_prev_i = t_prev_list[-(i + 1)]\n            model_prev_i = model_prev_list[-(i + 1)]\n            lambda_prev_i = ns.marginal_lambda(t_prev_i)\n            rk = (lambda_prev_i - lambda_prev_0) / h\n            rks.append(rk)\n            D1s.append((model_prev_i - model_prev_0) / rk)\n\n        rks.append(1.)\n        rks = torch.tensor(rks, device=x.device)\n\n        K = len(rks)\n        # build C matrix\n        C = []\n\n        col = torch.ones_like(rks)\n        for k in range(1, K + 1):\n            C.append(col)\n            col = col * rks / (k + 1) \n        C = torch.stack(C, dim=1)\n\n        if len(D1s) > 0:\n            D1s = torch.stack(D1s, dim=1) # (B, K)\n            C_inv_p = torch.linalg.inv(C[:-1, :-1])\n            A_p = C_inv_p\n\n        if use_corrector:\n            #print('using corrector')\n            C_inv = torch.linalg.inv(C)\n            A_c = C_inv\n\n        hh = -h if self.predict_x0 else h\n        h_phi_1 = torch.expm1(hh)\n        h_phi_ks = []\n        factorial_k = 1\n        h_phi_k = h_phi_1\n        for k in range(1, K + 2):\n            h_phi_ks.append(h_phi_k)\n            h_phi_k = h_phi_k / hh - 1 / factorial_k\n            factorial_k *= (k + 1)\n\n        model_t = None\n        if self.predict_x0:\n            x_t_ = (\n                sigma_t / sigma_prev_0 * x\n                - alpha_t * h_phi_1 * model_prev_0\n            )\n            # now predictor\n            x_t = x_t_\n            if len(D1s) > 0:\n                # compute the residuals for predictor\n                for k in range(K - 1):\n                    x_t = x_t - alpha_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_p[k])\n            # now corrector\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_\n                k = 0\n                for k in range(K - 1):\n                    x_t = x_t - alpha_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_c[k][:-1])\n                x_t = x_t - alpha_t * h_phi_ks[K] * (D1_t * A_c[k][-1])\n        else:\n            log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\n            x_t_ = (\n                (torch.exp(log_alpha_t - log_alpha_prev_0)) * x\n                - (sigma_t * h_phi_1) * model_prev_0\n            )\n            # now predictor\n            x_t = x_t_\n            if len(D1s) > 0:\n                # compute the residuals for predictor\n                for k in range(K - 1):\n                    x_t = x_t - sigma_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_p[k])\n            # now corrector\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_\n                k = 0\n                for k in range(K - 1):\n                    x_t = x_t - sigma_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_c[k][:-1])\n                x_t = x_t - sigma_t * h_phi_ks[K] * (D1_t * A_c[k][-1])\n        return x_t, model_t\n\n    def multistep_uni_pc_bh_update(self, x, model_prev_list, t_prev_list, t, order, x_t=None, use_corrector=True):\n        #print(f'using unified predictor-corrector with order {order} (solver type: B(h))')\n        ns = self.noise_schedule\n        assert order <= len(model_prev_list)\n        dims = x.dim()\n\n        # first compute rks\n        t_prev_0 = t_prev_list[-1]\n        lambda_prev_0 = ns.marginal_lambda(t_prev_0)\n        lambda_t = ns.marginal_lambda(t)\n        model_prev_0 = model_prev_list[-1]\n        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\n        log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        h = lambda_t - lambda_prev_0\n\n        rks = []\n        D1s = []\n        for i in range(1, order):\n            t_prev_i = t_prev_list[-(i + 1)]\n            model_prev_i = model_prev_list[-(i + 1)]\n            lambda_prev_i = ns.marginal_lambda(t_prev_i)\n            rk = ((lambda_prev_i - lambda_prev_0) / h)[0]\n            rks.append(rk)\n            D1s.append((model_prev_i - model_prev_0) / rk)\n\n        rks.append(1.)\n        rks = torch.tensor(rks, device=x.device)\n\n        R = []\n        b = []\n\n        hh = -h[0] if self.predict_x0 else h[0]\n        h_phi_1 = torch.expm1(hh) # h\\phi_1(h) = e^h - 1\n        h_phi_k = h_phi_1 / hh - 1\n\n        factorial_i = 1\n\n        if self.variant == 'bh1':\n            B_h = hh\n        elif self.variant == 'bh2':\n            B_h = torch.expm1(hh)\n        else:\n            raise NotImplementedError()\n            \n        for i in range(1, order + 1):\n            R.append(torch.pow(rks, i - 1))\n            b.append(h_phi_k * factorial_i / B_h)\n            factorial_i *= (i + 1)\n            h_phi_k = h_phi_k / hh - 1 / factorial_i \n\n        R = torch.stack(R)\n        b = torch.tensor(b, device=x.device)\n\n        # now predictor\n        use_predictor = len(D1s) > 0 and x_t is None\n        if len(D1s) > 0:\n            D1s = torch.stack(D1s, dim=1) # (B, K)\n            if len(D1s.shape) > 5:\n                D1s = rearrange(D1s, 'b k c f h w -> (b f) k c h w')\n            if x_t is None:\n                # for order 2, we use a simplified version\n                if order == 2:\n                    rhos_p = torch.tensor([0.5], device=b.device)\n                else:\n                    rhos_p = torch.linalg.solve(R[:-1, :-1], b[:-1])\n        else:\n            D1s = None\n\n        if use_corrector:\n            #print('using corrector')\n            # for order 1, we use a simplified version\n            if order == 1:\n                rhos_c = torch.tensor([0.5], device=b.device)\n            else:\n                rhos_c = torch.linalg.solve(R, b)\n\n        model_t = None\n        if self.predict_x0:\n            x_t_ = (\n                expand_dims(sigma_t / sigma_prev_0, dims) * x\n                - expand_dims(alpha_t * h_phi_1, dims)* model_prev_0\n            )\n\n            if x_t is None:\n                if use_predictor:\n                    pred_res = torch.einsum('k,bkchw->bchw', rhos_p, D1s)\n                    pred_res = repeat(pred_res, 'f c h w -> b c f h w', b=x.shape[0])\n                    \n                else:\n                    pred_res = 0\n                x_t = x_t_ - expand_dims(alpha_t * B_h, dims) * pred_res\n\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                if D1s is not None:\n                    corr_res = torch.einsum('k,bkchw->bchw', rhos_c[:-1], D1s)\n                    corr_res = repeat(corr_res, 'f c h w -> b c f h w', b=x.shape[0])\n                else:\n                    corr_res = 0\n                D1_t = (model_t - model_prev_0)\n                \n                x_t = x_t_ - expand_dims(alpha_t * B_h, dims) * (corr_res + rhos_c[-1] * D1_t)\n        else:\n            x_t_ = (\n                expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dims) * x\n                - expand_dims(sigma_t * h_phi_1, dims) * model_prev_0\n            )\n            if x_t is None:\n                if use_predictor:\n                    pred_res = torch.einsum('k,bkchw->bchw', rhos_p, D1s)\n                    pred_res = repeat(pred_res, 'f c h w -> b c f h w', b=x.shape[0])\n                else:\n                    pred_res = 0\n                x_t = x_t_ - expand_dims(sigma_t * B_h, dims) * pred_res\n\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                if D1s is not None:\n                    corr_res = torch.einsum('k,bkchw->bchw', rhos_c[:-1], D1s)\n                    corr_res = repeat(corr_res, 'f c h w -> b c f h w', b=x.shape[0])\n                else:\n                    corr_res = 0\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_ - expand_dims(sigma_t * B_h, dims) * (corr_res + rhos_c[-1] * D1_t)\n        return x_t, model_t\n\n    def handle_callback(self, callback):\n        if callback is not None:\n            callback()\n\n    def sample(self, x, steps=20, t_start=None, t_end=None, order=3, skip_type='time_uniform',\n        method='singlestep', lower_order_final=True, denoise_to_zero=False, solver_type='dpm_solver',\n        atol=0.0078, rtol=0.05, corrector=False, initial_corrector=True, callback=None\n    ):\n        t_0 = 1. / self.noise_schedule.total_N if t_end is None else t_end\n        t_T = self.noise_schedule.T if t_start is None else t_start\n        device = x.device\n        \n        if method == 'multistep':\n            assert steps >= order\n            timesteps = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=steps, device=device)\n            assert timesteps.shape[0] - 1 == steps\n            with torch.no_grad():\n                vec_t = timesteps[0].expand((x.shape[0]))\n                model_prev_list = [self.model_fn(x, vec_t)]\n                t_prev_list = [vec_t]\n\n                # Init the first `order` values by lower order multistep DPM-Solver.\n                for init_order in range(1, order):\n                    vec_t = timesteps[init_order].expand(x.shape[0])\n\n                    x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=initial_corrector)\n                    if model_x is None:\n                        model_x = self.model_fn(x, vec_t)\n                    model_prev_list.append(model_x)\n                    t_prev_list.append(vec_t)\n\n                    self.handle_callback(callback)\n                    \n                for step in range(order, steps + 1):\n                    vec_t = timesteps[step].expand(x.shape[0])\n                    \n                    if lower_order_final:\n                        step_order = min(order, steps + 1 - step)\n                    else:\n                        step_order = order\n                    #print('this step order:', step_order)\n                    if step == steps:\n                        #print('do not run corrector at the last step')\n                        use_corrector = False\n                    else:\n                        use_corrector = True\n                    x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n\n                    for i in range(order - 1):\n                        t_prev_list[i] = t_prev_list[i + 1]\n                        model_prev_list[i] = model_prev_list[i + 1]\n                    t_prev_list[-1] = vec_t\n                    # We do not need to evaluate the final model value.\n                    if step < steps:\n                        if model_x is None:\n                            model_x = self.model_fn(x, vec_t)\n                        model_prev_list[-1] = model_x\n                    \n                    self.handle_callback(callback)\n        else:\n            raise NotImplementedError()\n        if denoise_to_zero:\n            x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        \n        return x", "\n\n#############################################################\n# other utility functions\n#############################################################\n\ndef interpolate_fn(x, xp, yp):\n    \"\"\"\n    A piecewise linear function y = f(x), using xp and yp as keypoints.\n    We implement f(x) in a differentiable way (i.e. applicable for autograd).\n    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)\n    Args:\n        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).\n        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.\n        yp: PyTorch tensor with shape [C, K].\n    Returns:\n        The function values f(x), with shape [N, C].\n    \"\"\"\n    N, K = x.shape[0], xp.shape[1]\n    all_x = torch.cat([x.unsqueeze(2), xp.unsqueeze(0).repeat((N, 1, 1))], dim=2)\n    sorted_all_x, x_indices = torch.sort(all_x, dim=2)\n    x_idx = torch.argmin(x_indices, dim=2)\n    cand_start_idx = x_idx - 1\n    start_idx = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(1, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    end_idx = torch.where(torch.eq(start_idx, cand_start_idx), start_idx + 2, start_idx + 1)\n    start_x = torch.gather(sorted_all_x, dim=2, index=start_idx.unsqueeze(2)).squeeze(2)\n    end_x = torch.gather(sorted_all_x, dim=2, index=end_idx.unsqueeze(2)).squeeze(2)\n    start_idx2 = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(0, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    y_positions_expanded = yp.unsqueeze(0).expand(N, -1, -1)\n    start_y = torch.gather(y_positions_expanded, dim=2, index=start_idx2.unsqueeze(2)).squeeze(2)\n    end_y = torch.gather(y_positions_expanded, dim=2, index=(start_idx2 + 1).unsqueeze(2)).squeeze(2)\n    cand = start_y + (x - start_x) * (end_y - start_y) / (end_x - start_x)\n    return cand", "\n\ndef expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]", ""]}
{"filename": "scripts/samplers/uni_pc/sampler.py", "chunked_list": ["\"\"\"SAMPLING ONLY.\"\"\"\n\nimport torch\n\nfrom .uni_pc import NoiseScheduleVP, model_wrapper, UniPC\n\nclass UniPCSampler(object):\n    def __init__(self, model, **kwargs):\n        super().__init__()\n        self.model = model\n        to_torch = lambda x: x.clone().detach().to(torch.float32).to(model.device)\n        self.register_buffer('alphas_cumprod', to_torch(model.alphas_cumprod))\n\n    def register_buffer(self, name, attr):\n        if type(attr) == torch.Tensor:\n            if attr.device != torch.device(\"cuda\"):\n                attr = attr.to(torch.device(\"cuda\"))\n        setattr(self, name, attr)\n\n    def unipc_encode(self, latent, device, strength, steps, noise=None):\n        ns = NoiseScheduleVP('discrete', alphas_cumprod=self.alphas_cumprod)\n        uni_pc = UniPC(None, ns, predict_x0=True, thresholding=False, variant='bh1')\n        t_0 = 1. / ns.total_N\n\n        timesteps = uni_pc.get_time_steps(\"time_uniform\", strength, t_0, steps, device)\n        timesteps = timesteps[0].expand((latent.shape[0]))\n\n        noisy_latent = uni_pc.unipc_encode(latent, timesteps, noise=noise)\n        return noisy_latent\n\n    @torch.no_grad()\n    def sample(self,\n               S,\n               batch_size,\n               shape,\n               conditioning=None,\n               callback=None,\n               normals_sequence=None,\n               img_callback=None,\n               quantize_x0=False,\n               strength=None,\n               eta=0.,\n               mask=None,\n               x0=None,\n               temperature=1.,\n               score_corrector=None,\n               corrector_kwargs=None,\n               verbose=True,\n               x_T=None,\n               log_every_t=100,\n               unconditional_guidance_scale=1.,\n               unconditional_conditioning=None,\n               # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n               **kwargs\n               ):\n\n        # sampling\n        B, C, F, H, W = shape\n        size = (B, C, F, H, W)\n\n        if x_T is None:\n            img = torch.randn(size, device=self.model.device)\n        else:\n            img = x_T\n\n        ns = NoiseScheduleVP('discrete', alphas_cumprod=self.alphas_cumprod)\n        model_fn = model_wrapper(\n            lambda x, t, c: self.model(x, t, c),\n            ns,\n            model_type=\"noise\",\n            guidance_type=\"classifier-free\",\n            condition=conditioning,\n            unconditional_condition=unconditional_conditioning,\n            guidance_scale=unconditional_guidance_scale,\n        )\n        \n        uni_pc = UniPC(model_fn, ns, predict_x0=True, thresholding=False, variant='bh1')\n        x = uni_pc.sample(\n            img, \n            steps=S, \n            t_start=strength,\n            skip_type=\"time_uniform\", \n            method=\"multistep\", \n            order=3, \n            lower_order_final=True, \n            initial_corrector=True,\n            callback=callback\n        )\n\n        return x.to(self.model.device)"]}
{"filename": "scripts/videocrafter/sample_utils.py", "chunked_list": ["import os\nimport torch\nfrom PIL import Image\n\nfrom videocrafter.lvdm.models.modules.lora import net_load_lora\nfrom videocrafter.lvdm.utils.common_utils import instantiate_from_config\n\n\n# ------------------------------------------------------------------------------------------\ndef load_model(config, ckpt_path, gpu_id=None, inject_lora=False, lora_scale=1.0, lora_path=''):\n    print(f\"Loading model from {ckpt_path}\")\n    \n    # load sd\n    pl_sd = torch.load(ckpt_path, map_location=\"cpu\")\n    try:\n        global_step = pl_sd[\"global_step\"]\n        epoch = pl_sd[\"epoch\"]\n    except:\n        global_step = -1\n        epoch = -1\n    \n    # load sd to model\n    try:\n        sd = pl_sd[\"state_dict\"]\n    except:\n        sd = pl_sd\n    model = instantiate_from_config(config.model)\n    model.load_state_dict(sd, strict=True)\n\n    if inject_lora:\n        net_load_lora(model, lora_path, alpha=lora_scale)\n    \n    # move to device & eval\n    if gpu_id is not None:\n        model.to(f\"cuda:{gpu_id}\")\n    else:\n        model.cuda()\n    model.eval()\n\n    return model, global_step, epoch", "# ------------------------------------------------------------------------------------------\ndef load_model(config, ckpt_path, gpu_id=None, inject_lora=False, lora_scale=1.0, lora_path=''):\n    print(f\"Loading model from {ckpt_path}\")\n    \n    # load sd\n    pl_sd = torch.load(ckpt_path, map_location=\"cpu\")\n    try:\n        global_step = pl_sd[\"global_step\"]\n        epoch = pl_sd[\"epoch\"]\n    except:\n        global_step = -1\n        epoch = -1\n    \n    # load sd to model\n    try:\n        sd = pl_sd[\"state_dict\"]\n    except:\n        sd = pl_sd\n    model = instantiate_from_config(config.model)\n    model.load_state_dict(sd, strict=True)\n\n    if inject_lora:\n        net_load_lora(model, lora_path, alpha=lora_scale)\n    \n    # move to device & eval\n    if gpu_id is not None:\n        model.to(f\"cuda:{gpu_id}\")\n    else:\n        model.cuda()\n    model.eval()\n\n    return model, global_step, epoch", "\n\n# ------------------------------------------------------------------------------------------\n@torch.no_grad()\ndef get_conditions(prompts, model, batch_size, cond_fps=None,):\n    \n    if isinstance(prompts, str) or isinstance(prompts, int):\n        prompts = [prompts]\n    if isinstance(prompts, list):\n        if len(prompts) == 1:\n            prompts = prompts * batch_size\n        elif len(prompts) == batch_size:\n            pass\n        else:\n            raise ValueError(f\"invalid prompts length: {len(prompts)}\")\n    else:\n        raise ValueError(f\"invalid prompts: {prompts}\")\n    assert(len(prompts) == batch_size)\n    \n    # content condition: text / class label\n    c = model.get_learned_conditioning(prompts)\n    key = 'c_concat' if model.conditioning_key == 'concat' else 'c_crossattn'\n    c = {key: [c]}\n\n    # temporal condition: fps\n    if getattr(model, 'cond_stage2_config', None) is not None:\n        if model.cond_stage2_key == \"temporal_context\":\n            assert(cond_fps is not None)\n            batch = {'fps': torch.tensor([cond_fps] * batch_size).long().to(model.device)}\n            fps_embd = model.cond_stage2_model(batch)\n            c[model.cond_stage2_key] = fps_embd\n    \n    return c", "\n\n# ------------------------------------------------------------------------------------------\ndef make_model_input_shape(model, batch_size, T=None):\n    image_size = [model.image_size, model.image_size] if isinstance(model.image_size, int) else model.image_size\n    C = model.model.diffusion_model.in_channels\n    if T is None:\n        T = model.model.diffusion_model.temporal_length\n    shape = [batch_size, C, T, *image_size]\n    return shape", "\n\n# ------------------------------------------------------------------------------------------\ndef custom_to_pil(x):\n    x = x.detach().cpu()\n    x = torch.clamp(x, -1., 1.)\n    x = (x + 1.) / 2.\n    x = x.permute(1, 2, 0).numpy()\n    x = (255 * x).astype(np.uint8)\n    x = Image.fromarray(x)\n    if not x.mode == \"RGB\":\n        x = x.convert(\"RGB\")\n    return x", "\ndef torch_to_np(x):\n    # saves the batch in adm style as in https://github.com/openai/guided-diffusion/blob/main/scripts/image_sample.py\n    sample = x.detach().cpu()\n    sample = ((sample + 1) * 127.5).clamp(0, 255).to(torch.uint8)\n    if sample.dim() == 5:\n        sample = sample.permute(0, 2, 3, 4, 1)\n    else:\n        sample = sample.permute(0, 2, 3, 1)\n    sample = sample.contiguous()\n    return sample", "\ndef make_sample_dir(opt, global_step=None, epoch=None):\n    if not getattr(opt, 'not_automatic_logdir', False):\n        gs_str = f\"globalstep{global_step:09}\" if global_step is not None else \"None\"\n        e_str = f\"epoch{epoch:06}\" if epoch is not None else \"None\"\n        ckpt_dir = os.path.join(opt.logdir, f\"{gs_str}_{e_str}\")\n        \n        # subdir name\n        if opt.prompt_file is not None:\n            subdir = f\"prompts_{os.path.splitext(os.path.basename(opt.prompt_file))[0]}\"\n        else:\n            subdir = f\"prompt_{opt.prompt[:10]}\"\n        subdir += \"_DDPM\" if opt.vanilla_sample else f\"_DDIM{opt.custom_steps}steps\"\n        subdir += f\"_CfgScale{opt.scale}\"\n        if opt.cond_fps is not None:\n            subdir += f\"_fps{opt.cond_fps}\"\n        if opt.seed is not None:\n            subdir += f\"_seed{opt.seed}\"\n\n        return os.path.join(ckpt_dir, subdir)\n    else:\n        return opt.logdir", ""]}
{"filename": "scripts/videocrafter/process_videocrafter.py", "chunked_list": ["from base64 import b64encode\nfrom tqdm import tqdm\nfrom omegaconf import OmegaConf\nimport time, os\nfrom t2v_helpers.general_utils import get_t2v_version\nfrom t2v_helpers.args import get_outdir, process_args\nimport modules.paths as ph\nimport t2v_helpers.args as t2v_helpers_args\nfrom modules.shared import state\n", "from modules.shared import state\n\n# VideoCrafter support is heavy WIP and sketchy, needs help and more devs!\ndef process_videocrafter(args_dict):\n    args, video_args = process_args(args_dict)\n    print(f\"\\033[4;33m text2video extension for auto1111 webui\\033[0m\")\n    print(f\"Git commit: {get_t2v_version()}\")\n    init_timestring = time.strftime('%Y%m%d%H%M%S')\n    outdir_current = os.path.join(get_outdir(), f\"{init_timestring}\")\n\n    os.makedirs(outdir_current, exist_ok=True)\n\n    # load & merge config\n\n    config_path = os.path.join(ph.models_path, \"models/VideoCrafter/model_config.yaml\")\n    if not os.path.exists(config_path):\n        config_path = os.path.join(os.getcwd(), \"extensions/sd-webui-modelscope-text2video/scripts/videocrafter/base_t2v/model_config.yaml\")\n    if not os.path.exists(config_path):\n        config_path = os.path.join(os.getcwd(), \"extensions/sd-webui-text2video/scripts/videocrafter/base_t2v/model_config.yaml\")\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f'Could not find config file at {os.path.join(ph.models_path, \"models/VideoCrafter/model_config.yaml\")}, nor at {os.path.join(os.getcwd(), \"extensions/sd-webui-modelscope-text2video/scripts/videocrafter/base_t2v/model_config.yaml\")}, nor at {os.path.join(os.getcwd(), \"extensions/sd-webui-text2video/scripts/videocrafter/base_t2v/model_config.yaml\")}')\n\n    config = OmegaConf.load(config_path)\n    print(\"VideoCrafter config: \\n\", config)\n\n    from videocrafter.lvdm.samplers.ddim import DDIMSampler\n    from videocrafter.sample_utils import load_model, get_conditions, make_model_input_shape, torch_to_np\n    from videocrafter.sample_text2video import sample_text2video\n    from videocrafter.lvdm.utils.saving_utils import npz_to_video_grid\n    from t2v_helpers.video_audio_utils import add_soundtrack\n\n    # get model & sampler\n    model, _, _ = load_model(config, ph.models_path+'/VideoCrafter/model.ckpt', #TODO: support safetensors and stuff\n                             inject_lora=False, # TODO\n                             lora_scale=1, # TODO\n                             lora_path=ph.models_path+'/VideoCrafter/LoRA/LoRA.ckpt', #TODO: support LoRA and stuff\n                             )\n    ddim_sampler = DDIMSampler(model)# if opt.sample_type == \"ddim\" else None\n\n    # if opt.inject_lora:\n    #     assert(opt.lora_trigger_word != '')\n    #     prompts = [p + opt.lora_trigger_word for p in prompts]\n    \n    # go\n    start = time.time()  \n\n    pbar = tqdm(range(args.batch_count), leave=False)\n    if args.batch_count == 1:\n        pbar.disable=True\n    \n    state.job_count = args.batch_count\n    \n    for batch in pbar:\n        state.job_no = batch + 1\n        if state.skipped:\n            state.skipped = False\n\n        if state.interrupted:\n            break\n\n        state.job = f\"Batch {batch+1} out of {args.batch_count}\"\n        ddim_sampler.noise_gen.manual_seed(args.seed + batch if args.seed != -1 else -1)\n        # sample\n        samples = sample_text2video(model, args.prompt, args.n_prompt, 1, 1,# todo:add batch size support\n                        sample_type='ddim', sampler=ddim_sampler,\n                        ddim_steps=args.steps, eta=args.eta, \n                        cfg_scale=args.cfg_scale,\n                        decode_frame_bs=1,\n                        ddp=False, show_denoising_progress=False,\n                        num_frames=args.frames\n                        )\n        # save\n        if batch > 0:\n            outdir_current = os.path.join(get_outdir(), f\"{init_timestring}_{batch}\")\n        print(f'text2video finished, saving frames to {outdir_current}')\n\n        npz_to_video_grid(samples[0:1,...],  # TODO: is this the reason only 1 second is saved?\n                              os.path.join(outdir_current, f\"vid.mp4\"), \n                              fps=video_args.fps)\n        if video_args.add_soundtrack != 'None':\n            add_soundtrack(video_args.ffmpeg_location, video_args.fps, os.path.join(outdir_current, f\"vid.mp4\"), 0, -1, None, video_args.add_soundtrack, video_args.soundtrack_path, video_args.ffmpeg_crf, video_args.ffmpeg_preset)\n        print(f't2v complete, result saved at {outdir_current}')\n\n        mp4 = open(outdir_current + os.path.sep + f\"vid.mp4\", 'rb').read()\n        dataurl = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n        t2v_helpers_args.i1_store_t2v = f'<p style=\\\"font-weight:bold;margin-bottom:0em\\\">text2video extension for auto1111 \u2014 version 1.1b </p><video controls loop><source src=\"{dataurl}\" type=\"video/mp4\"></video>'\n        print(\"Finish sampling!\")\n        print(f\"Run time = {(time.time() - start):.2f} seconds\")\n    pbar.close()\n    return [dataurl]", ""]}
{"filename": "scripts/videocrafter/ddp_wrapper.py", "chunked_list": ["import datetime\nimport argparse, importlib\nfrom pytorch_lightning import seed_everything\n\nimport torch\nimport torch.distributed as dist\n\n\ndef setup_dist(local_rank):\n    if dist.is_initialized():\n        return\n    torch.cuda.set_device(local_rank)\n    torch.distributed.init_process_group('nccl', init_method='env://')", "def setup_dist(local_rank):\n    if dist.is_initialized():\n        return\n    torch.cuda.set_device(local_rank)\n    torch.distributed.init_process_group('nccl', init_method='env://')\n\n\ndef get_dist_info():\n    if dist.is_available():\n        initialized = dist.is_initialized()\n    else:\n        initialized = False\n    if initialized:\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n    else:\n        rank = 0\n        world_size = 1\n    return rank, world_size", "\n\nif __name__ == '__main__':\n    now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--module\", type=str, help=\"module name\", default=\"inference\")\n    parser.add_argument(\"--local_rank\", type=int, nargs=\"?\", help=\"for ddp\", default=0)\n    args, unknown = parser.parse_known_args()\n    inference_api = importlib.import_module(args.module, package=None)\n\n    inference_parser = inference_api.get_parser()\n    inference_args, unknown = inference_parser.parse_known_args()\n\n    seed_everything(inference_args.seed)\n    setup_dist(args.local_rank)\n    torch.backends.cudnn.benchmark = True\n    rank, gpu_num = get_dist_info()\n\n    print(\"@CoVideoGen Inference [rank%d]: %s\"%(rank, now))\n    inference_api.run_inference(inference_args, rank)"]}
{"filename": "scripts/videocrafter/sample_text2video_adapter.py", "chunked_list": ["import argparse, os, sys, glob\nimport datetime, time\nfrom omegaconf import OmegaConf\n\nimport torch\nfrom decord import VideoReader, cpu\nimport torchvision\nfrom pytorch_lightning import seed_everything\n\nfrom lvdm.samplers.ddim import DDIMSampler", "\nfrom lvdm.samplers.ddim import DDIMSampler\nfrom lvdm.utils.common_utils import instantiate_from_config\nfrom lvdm.utils.saving_utils import tensor_to_mp4\n\n\ndef get_filelist(data_dir, ext='*'):\n    file_list = glob.glob(os.path.join(data_dir, '*.%s'%ext))\n    file_list.sort()\n    return file_list", "\ndef load_model_checkpoint(model, ckpt, adapter_ckpt=None):\n    print('>>> Loading checkpoints ...')\n    if adapter_ckpt:\n        ## main model\n        state_dict = torch.load(ckpt, map_location=\"cpu\")\n        if \"state_dict\" in list(state_dict.keys()):\n            state_dict = state_dict[\"state_dict\"]\n        model.load_state_dict(state_dict, strict=False)\n        print('@model checkpoint loaded.')\n        ## adapter\n        state_dict = torch.load(adapter_ckpt, map_location=\"cpu\")\n        if \"state_dict\" in list(state_dict.keys()):\n            state_dict = state_dict[\"state_dict\"]\n        model.adapter.load_state_dict(state_dict, strict=True)\n        print('@adapter checkpoint loaded.')\n    else:\n        state_dict = torch.load(ckpt, map_location=\"cpu\")\n        if \"state_dict\" in list(state_dict.keys()):\n            state_dict = state_dict[\"state_dict\"]\n        model.load_state_dict(state_dict, strict=True)\n        print('@model checkpoint loaded.')\n    return model", "\ndef load_prompts(prompt_file):\n    f = open(prompt_file, 'r')\n    prompt_list = []\n    for idx, line in enumerate(f.readlines()):\n        l = line.strip()\n        if len(l) != 0:\n            prompt_list.append(l)\n        f.close()\n    return prompt_list", "\ndef load_video(filepath, frame_stride, video_size=(256,256), video_frames=16):\n    vidreader = VideoReader(filepath, ctx=cpu(0), width=video_size[1], height=video_size[0])\n    max_frames = len(vidreader)\n    temp_stride = max_frames // video_frames if frame_stride == -1 else frame_stride\n    if temp_stride * (video_frames-1) >= max_frames:\n        print(f'Warning: default frame stride is used because the input video clip {max_frames} is not long enough.')\n        temp_stride = max_frames // video_frames\n    frame_indices = [temp_stride*i for i in range(video_frames)]\n    frames = vidreader.get_batch(frame_indices)\n        \n    ## [t,h,w,c] -> [c,t,h,w]\n    frame_tensor = torch.tensor(frames.asnumpy()).permute(3, 0, 1, 2).float()\n    frame_tensor = (frame_tensor / 255. - 0.5) * 2    \n    return frame_tensor", "\n\ndef save_results(prompt, samples, inputs, filename, realdir, fakedir, fps=10):\n    ## save prompt\n    prompt = prompt[0] if isinstance(prompt, list) else prompt\n    path = os.path.join(realdir, \"%s.txt\"%filename)\n    with open(path, 'w') as f:\n        f.write(f'{prompt}')\n        f.close()\n\n    ## save video\n    videos = [inputs, samples]\n    savedirs = [realdir, fakedir]\n    for idx, video in enumerate(videos):\n        if video is None:\n            continue\n        # b,c,t,h,w\n        video = video.detach().cpu()\n        video = torch.clamp(video.float(), -1., 1.)\n        n = video.shape[0]\n        video = video.permute(2, 0, 1, 3, 4) # t,n,c,h,w\n        frame_grids = [torchvision.utils.make_grid(framesheet, nrow=int(n)) for framesheet in video] #[3, 1*h, n*w]\n        grid = torch.stack(frame_grids, dim=0) # stack in temporal dim [t, 3, n*h, w]\n        grid = (grid + 1.0) / 2.0\n        grid = (grid * 255).to(torch.uint8).permute(0, 2, 3, 1)\n        path = os.path.join(savedirs[idx], \"%s.mp4\"%filename)\n        torchvision.io.write_video(path, grid, fps=fps, video_codec='h264', options={'crf': '10'})", "\n\ndef adapter_guided_synthesis(model, prompts, videos, noise_shape, n_samples=1, ddim_steps=50, ddim_eta=1., \\\n                        unconditional_guidance_scale=1.0, unconditional_guidance_scale_temporal=None, **kwargs):\n    ddim_sampler = DDIMSampler(model)\n\n    batch_size = noise_shape[0]\n    ## get condition embeddings (support single prompt only)\n    if isinstance(prompts, str):\n        prompts = [prompts]\n    cond = model.get_learned_conditioning(prompts)\n    if unconditional_guidance_scale != 1.0:\n        prompts = batch_size * [\"\"]\n        uc = model.get_learned_conditioning(prompts)\n    else:\n        uc = None\n    \n    ## adapter features: process in 2D manner\n    b, c, t, h, w = videos.shape\n    extra_cond = model.get_batch_depth(videos, (h,w))\n    features_adapter = model.get_adapter_features(extra_cond)\n\n    batch_variants = []\n    for _ in range(n_samples):\n        if ddim_sampler is not None:\n            samples, _ = ddim_sampler.sample(S=ddim_steps,\n                                            conditioning=cond,\n                                            batch_size=noise_shape[0],\n                                            shape=noise_shape[1:],\n                                            verbose=False,\n                                            unconditional_guidance_scale=unconditional_guidance_scale,\n                                            unconditional_conditioning=uc,\n                                            eta=ddim_eta,\n                                            temporal_length=noise_shape[2],\n                                            conditional_guidance_scale_temporal=unconditional_guidance_scale_temporal,\n                                            features_adapter=features_adapter,\n                                            **kwargs\n                                            )        \n        ## reconstruct from latent to pixel space\n        batch_images = model.decode_first_stage(samples, decode_bs=1, return_cpu=False)\n        batch_variants.append(batch_images)\n    ## variants, batch, c, t, h, w\n    batch_variants = torch.stack(batch_variants)\n    return batch_variants.permute(1, 0, 2, 3, 4, 5), extra_cond", "\n\ndef run_inference(args, gpu_idx):\n    ## model config\n    config = OmegaConf.load(args.base)\n    model_config = config.pop(\"model\", OmegaConf.create())\n    model = instantiate_from_config(model_config)\n    model = model.cuda(gpu_idx)\n    assert os.path.exists(args.ckpt_path), \"Error: checkpoint Not Found!\"\n    model = load_model_checkpoint(model, args.ckpt_path, args.adapter_ckpt)\n    model.eval()\n\n    ## run over data\n    assert (args.height % 16 == 0) and (args.width % 16 == 0), \"Error: image size [h,w] should be multiples of 16!\"\n    ## latent noise shape\n    h, w = args.height // 8, args.width // 8\n    channels = model.channels\n    frames = model.temporal_length\n    noise_shape = [args.bs, channels, args.num_frames, h, w]\n    \n    ## inference\n    start = time.time()\n    prompt = args.prompt\n    video = load_video(args.video, args.frame_stride, video_size=(args.height, args.width), video_frames=args.num_frames)\n    video = video.unsqueeze(0).to(\"cuda\")\n    with torch.no_grad():\n        batch_samples, batch_conds = adapter_guided_synthesis(model, prompt, video, noise_shape, args.n_samples, args.ddim_steps, args.ddim_eta, \\\n                                                args.unconditional_guidance_scale, args.unconditional_guidance_scale_temporal)\n    batch_samples = batch_samples[0]\n    os.makedirs(args.savedir, exist_ok=True)\n    filename = f\"{args.prompt}_seed{args.seed}\"\n    filename = filename.replace(\"/\", \"_slash_\") if \"/\" in filename else filename\n    filename = filename.replace(\" \", \"_\") if \" \" in filename else filename\n    tensor_to_mp4(video=batch_conds.detach().cpu(), savepath=os.path.join(args.savedir, f'{filename}_depth.mp4'), fps=10)\n    tensor_to_mp4(video=batch_samples.detach().cpu(), savepath=os.path.join(args.savedir, f'{filename}_sample.mp4'), fps=10)\n\n    print(f\"Saved in {args.savedir}. Time used: {(time.time() - start):.2f} seconds\")", "\n\ndef get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--savedir\", type=str, default=None, help=\"results saving path\")\n    parser.add_argument(\"--ckpt_path\", type=str, default=None, help=\"checkpoint path\")\n    parser.add_argument(\"--adapter_ckpt\", type=str, default=None, help=\"adapter checkpoint path\")\n    parser.add_argument(\"--base\", type=str, help=\"config (yaml) path\")\n    parser.add_argument(\"--prompt\", type=str, default=None, help=\"prompt string\")\n    parser.add_argument(\"--video\", type=str, default=None, help=\"video path\")\n    parser.add_argument(\"--n_samples\", type=int, default=1, help=\"num of samples per prompt\",)\n    parser.add_argument(\"--ddim_steps\", type=int, default=50, help=\"steps of ddim if positive, otherwise use DDPM\",)\n    parser.add_argument(\"--ddim_eta\", type=float, default=1.0, help=\"eta for ddim sampling (0.0 yields deterministic sampling)\",)\n    parser.add_argument(\"--bs\", type=int, default=1, help=\"batch size for inference\")\n    parser.add_argument(\"--height\", type=int, default=512, help=\"image height, in pixel space\")\n    parser.add_argument(\"--width\", type=int, default=512, help=\"image width, in pixel space\")\n    parser.add_argument(\"--frame_stride\", type=int, default=-1, help=\"frame extracting from input video\")\n    parser.add_argument(\"--unconditional_guidance_scale\", type=float, default=1.0, help=\"prompt classifier-free guidance\")\n    parser.add_argument(\"--unconditional_guidance_scale_temporal\", type=float, default=None, help=\"temporal consistency guidance\")\n    parser.add_argument(\"--seed\", type=int, default=2023, help=\"seed for seed_everything\")\n    parser.add_argument(\"--num_frames\", type=int, default=16, help=\"number of input frames\")    \n    return parser", "\n\nif __name__ == '__main__':\n    now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n    print(\"@CoVideoGen cond-Inference: %s\"%now)\n    parser = get_parser()\n    args = parser.parse_args()\n\n    seed_everything(args.seed)\n    rank = 0\n    run_inference(args, rank)"]}
{"filename": "scripts/videocrafter/sample_text2video.py", "chunked_list": ["import os\nimport time\nimport argparse\nimport yaml, math\nfrom tqdm import trange\nimport torch\nimport numpy as np\nfrom omegaconf import OmegaConf\nimport torch.distributed as dist\nfrom pytorch_lightning import seed_everything", "import torch.distributed as dist\nfrom pytorch_lightning import seed_everything\n\nfrom videocrafter.lvdm.samplers.ddim import DDIMSampler\nfrom videocrafter.lvdm.utils.common_utils import str2bool\nfrom videocrafter.lvdm.utils.dist_utils import setup_dist, gather_data\nfrom videocrafter.lvdm.utils.saving_utils import npz_to_video_grid, npz_to_imgsheet_5d\nfrom videocrafter.sample_utils import load_model, get_conditions, make_model_input_shape, torch_to_np\n\n", "\n\n# ------------------------------------------------------------------------------------------\ndef get_parser():\n    parser = argparse.ArgumentParser()\n    # basic args\n    parser.add_argument(\"--ckpt_path\", type=str, help=\"model checkpoint path\")\n    parser.add_argument(\"--config_path\", type=str, help=\"model config path (a yaml file)\")\n    parser.add_argument(\"--prompt\", type=str, help=\"input text prompts for text2video (a sentence OR a txt file).\")\n    parser.add_argument(\"--save_dir\", type=str, help=\"results saving dir\", default=\"results/\")\n    # device args\n    parser.add_argument(\"--ddp\", action='store_true', help=\"whether use pytorch ddp mode for parallel sampling (recommend for multi-gpu case)\", default=False)\n    parser.add_argument(\"--local_rank\", type=int, help=\"is used for pytorch ddp mode\", default=0)\n    parser.add_argument(\"--gpu_id\", type=int, help=\"choose a specific gpu\", default=0)\n    # sampling args\n    parser.add_argument(\"--n_samples\", type=int, help=\"how many samples for each text prompt\", default=2)\n    parser.add_argument(\"--batch_size\", type=int, help=\"video batch size for sampling\", default=1)\n    parser.add_argument(\"--decode_frame_bs\", type=int, help=\"frame batch size for framewise decoding\", default=1)\n    parser.add_argument(\"--sample_type\", type=str, help=\"ddpm or ddim\", default=\"ddim\", choices=[\"ddpm\", \"ddim\"])\n    parser.add_argument(\"--ddim_steps\", type=int, help=\"ddim sampling -- number of ddim denoising timesteps\", default=50)\n    parser.add_argument(\"--eta\", type=float, help=\"ddim sampling -- eta (0.0 yields deterministic sampling, 1.0 yields random sampling)\", default=1.0)\n    parser.add_argument(\"--cfg_scale\", type=float, default=15.0, help=\"classifier-free guidance scale\")\n    parser.add_argument(\"--seed\", type=int, default=None, help=\"fix a seed for randomness (If you want to reproduce the sample results)\")\n    parser.add_argument(\"--show_denoising_progress\", action='store_true', default=False, help=\"whether show denoising progress during sampling one batch\",)\n    parser.add_argument(\"--num_frames\", type=int, default=16, help=\"number of input frames\")\n    # lora args\n    parser.add_argument(\"--lora_path\", type=str, help=\"lora checkpoint path\")\n    parser.add_argument(\"--inject_lora\", action='store_true', default=False, help=\"\",)\n    parser.add_argument(\"--lora_scale\", type=float, default=None, help=\"scale for lora weight\")\n    parser.add_argument(\"--lora_trigger_word\", type=str, default=\"\", help=\"\",)\n    # saving args\n    parser.add_argument(\"--save_mp4\", type=str2bool, default=True, help=\"whether save samples in separate mp4 files\", choices=[\"True\", \"true\", \"False\", \"false\"])\n    parser.add_argument(\"--save_mp4_sheet\", action='store_true', default=False, help=\"whether save samples in mp4 file\",)\n    parser.add_argument(\"--save_npz\", action='store_true', default=False, help=\"whether save samples in npz file\",)\n    parser.add_argument(\"--save_jpg\", action='store_true', default=False, help=\"whether save samples in jpg file\",)\n    parser.add_argument(\"--save_fps\", type=int, default=8, help=\"fps of saved mp4 videos\",)\n    return parser", "\n# ------------------------------------------------------------------------------------------\ndef sample_denoising_batch(model, noise_shape, condition, *args,\n                           sample_type=\"ddim\", sampler=None, \n                           ddim_steps=None, eta=None,\n                           unconditional_guidance_scale=1.0, uc=None,\n                           denoising_progress=False,\n                           **kwargs,\n                           ):\n    \n    if sample_type == \"ddpm\":\n        samples = model.p_sample_loop(cond=condition, shape=noise_shape,\n                                      return_intermediates=False, \n                                      verbose=denoising_progress,\n                                      **kwargs,\n                                      )\n    elif sample_type == \"ddim\":\n        assert(sampler is not None)\n        assert(ddim_steps is not None)\n        assert(eta is not None)\n        ddim_sampler = sampler\n        samples, _ = ddim_sampler.sample(S=ddim_steps,\n                                         conditioning=condition,\n                                         batch_size=noise_shape[0],\n                                         shape=noise_shape[1:],\n                                         verbose=denoising_progress,\n                                         unconditional_guidance_scale=unconditional_guidance_scale,\n                                         unconditional_conditioning=uc,\n                                         eta=eta,\n                                         **kwargs,\n                                        )\n    else:\n        raise ValueError\n    return samples", "\n\n# ------------------------------------------------------------------------------------------\n@torch.no_grad()\ndef sample_text2video(model, prompt, n_prompt, n_samples, batch_size,\n                      sample_type=\"ddim\", sampler=None, \n                      ddim_steps=50, eta=1.0, cfg_scale=7.5, \n                      decode_frame_bs=1,\n                      ddp=False, all_gather=True, \n                      batch_progress=True, show_denoising_progress=False,\n                      num_frames=None,\n                      ):\n    # get cond vector\n    assert(model.cond_stage_model is not None)\n    cond_embd = get_conditions(prompt, model, batch_size)\n    uncond_embd = get_conditions(n_prompt, model, batch_size) if cfg_scale != 1.0 else None\n\n    # sample batches\n    all_videos = []\n    n_iter = math.ceil(n_samples / batch_size)\n    iterator  = trange(n_iter, desc=\"Sampling Batches (text-to-video)\") if batch_progress else range(n_iter)\n    for _ in iterator:\n        noise_shape = make_model_input_shape(model, batch_size, T=num_frames)\n        samples_latent = sample_denoising_batch(model, noise_shape, cond_embd,\n                                            sample_type=sample_type,\n                                            sampler=sampler,\n                                            ddim_steps=ddim_steps,\n                                            eta=eta,\n                                            unconditional_guidance_scale=cfg_scale, \n                                            uc=uncond_embd,\n                                            denoising_progress=show_denoising_progress,\n                                            )\n        samples = model.decode_first_stage(samples_latent, decode_bs=decode_frame_bs, return_cpu=False)\n        \n        # gather samples from multiple gpus\n        if ddp and all_gather:\n            data_list = gather_data(samples, return_np=False)\n            all_videos.extend([torch_to_np(data) for data in data_list])\n        else:\n            all_videos.append(torch_to_np(samples))\n    \n    all_videos = np.concatenate(all_videos, axis=0)\n    assert(all_videos.shape[0] >= n_samples)\n    return all_videos", "\n\n# ------------------------------------------------------------------------------------------\ndef save_results(videos, save_dir, \n                 save_name=\"results\", save_fps=8, save_mp4=True, \n                 save_npz=False, save_mp4_sheet=False, save_jpg=False\n                 ):\n    if save_mp4:\n        save_subdir = os.path.join(save_dir, \"videos\")\n        os.makedirs(save_subdir, exist_ok=True)\n        for i in range(videos.shape[0]):\n            npz_to_video_grid(videos[i:i+1,...], \n                              os.path.join(save_subdir, f\"{save_name}_{i:03d}.mp4\"), \n                              fps=save_fps)\n        print(f'Successfully saved videos in {save_subdir}')\n    \n    if save_npz:\n        save_path = os.path.join(save_dir, f\"{save_name}.npz\")\n        np.savez(save_path, videos)\n        print(f'Successfully saved npz in {save_path}')\n    \n    if save_mp4_sheet:\n        save_path = os.path.join(save_dir, f\"{save_name}.mp4\")\n        npz_to_video_grid(videos, save_path, fps=save_fps)\n        print(f'Successfully saved mp4 sheet in {save_path}')\n\n    if save_jpg:\n        save_path = os.path.join(save_dir, f\"{save_name}.jpg\")\n        npz_to_imgsheet_5d(videos, save_path, nrow=videos.shape[1])\n        print(f'Successfully saved jpg sheet in {save_path}')", "\n\n# ------------------------------------------------------------------------------------------\ndef main():\n    \"\"\"\n    text-to-video generation\n    \"\"\"\n    parser = get_parser()\n    opt, unknown = parser.parse_known_args()\n    os.makedirs(opt.save_dir, exist_ok=True)\n    \n    # set device\n    if opt.ddp:\n        setup_dist(opt.local_rank)\n        opt.n_samples = math.ceil(opt.n_samples / dist.get_world_size())\n        gpu_id = None\n    else:\n        gpu_id = opt.gpu_id\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_id}\"\n    \n    # set random seed\n    if opt.seed is not None:\n        if opt.ddp:\n            seed = opt.local_rank + opt.seed\n        else:\n            seed = opt.seed\n        seed_everything(seed)\n\n    # dump args\n    fpath = os.path.join(opt.save_dir, \"sampling_args.yaml\")\n    with open(fpath, 'w') as f:\n        yaml.dump(vars(opt), f, default_flow_style=False)\n\n    # load & merge config\n    config = OmegaConf.load(opt.config_path)\n    cli = OmegaConf.from_dotlist(unknown)\n    config = OmegaConf.merge(config, cli)\n    print(\"config: \\n\", config)\n\n    # get model & sampler\n    model, _, _ = load_model(config, opt.ckpt_path, \n                             inject_lora=opt.inject_lora, \n                             lora_scale=opt.lora_scale, \n                             lora_path=opt.lora_path\n                             )\n    ddim_sampler = DDIMSampler(model) if opt.sample_type == \"ddim\" else None\n\n    # prepare prompt\n    if opt.prompt.endswith(\".txt\"):\n        opt.prompt_file = opt.prompt\n        opt.prompt = None\n    else:\n        opt.prompt_file = None\n\n    if opt.prompt_file is not None:\n        f = open(opt.prompt_file, 'r')\n        prompts, line_idx = [], []\n        for idx, line in enumerate(f.readlines()):\n            l = line.strip()\n            if len(l) != 0:\n                prompts.append(l)\n                line_idx.append(idx)\n        f.close()\n        cmd = f\"cp {opt.prompt_file} {opt.save_dir}\"\n        os.system(cmd)\n    else:\n        prompts = [opt.prompt]\n        line_idx = [None]\n\n    if opt.inject_lora:\n        assert(opt.lora_trigger_word != '')\n        prompts = [p + opt.lora_trigger_word for p in prompts]\n    \n    # go\n    start = time.time()  \n    for prompt in prompts:\n        # sample\n        samples = sample_text2video(model, prompt, opt.n_samples, opt.batch_size,\n                          sample_type=opt.sample_type, sampler=ddim_sampler,\n                          ddim_steps=opt.ddim_steps, eta=opt.eta, \n                          cfg_scale=opt.cfg_scale,\n                          decode_frame_bs=opt.decode_frame_bs,\n                          ddp=opt.ddp, show_denoising_progress=opt.show_denoising_progress,\n                          num_frames=opt.num_frames,\n                          )\n        # save\n        if (opt.ddp and dist.get_rank() == 0) or (not opt.ddp):\n            prompt_str = prompt.replace(\"/\", \"_slash_\") if \"/\" in prompt else prompt\n            save_name = prompt_str.replace(\" \", \"_\") if \" \" in prompt else prompt_str\n            if opt.seed is not None:\n                save_name = save_name + f\"_seed{seed:05d}\"\n            save_results(samples, opt.save_dir, save_name=save_name, save_fps=opt.save_fps)\n    print(\"Finish sampling!\")\n    print(f\"Run time = {(time.time() - start):.2f} seconds\")\n\n    if opt.ddp:\n        dist.destroy_process_group()", "\n\n# ------------------------------------------------------------------------------------------\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "scripts/videocrafter/lvdm/samplers/ddim.py", "chunked_list": ["\"\"\"SAMPLING ONLY.\"\"\"\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom modules.shared import state\nfrom modules.sd_samplers_common import InterruptedException\n\nfrom videocrafter.lvdm.models.modules.util import make_ddim_sampling_parameters, make_ddim_timesteps, noise_like", "\nfrom videocrafter.lvdm.models.modules.util import make_ddim_sampling_parameters, make_ddim_timesteps, noise_like\n\n\nclass DDIMSampler(object):\n    def __init__(self, model, schedule=\"linear\", **kwargs):\n        super().__init__()\n        self.model = model\n        self.ddpm_num_timesteps = model.num_timesteps\n        self.schedule = schedule\n        self.counter = 0\n        self.noise_gen = torch.Generator(device='cpu')\n\n    def register_buffer(self, name, attr):\n        if type(attr) == torch.Tensor:\n            if attr.device != torch.device(\"cuda\"):\n                attr = attr.to(torch.device(\"cuda\"))\n        setattr(self, name, attr)\n\n    def make_schedule(self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0., verbose=True):\n        self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,\n                                                  num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)\n        alphas_cumprod = self.model.alphas_cumprod\n        assert alphas_cumprod.shape[0] == self.ddpm_num_timesteps, 'alphas have to be defined for each timestep'\n        to_torch = lambda x: x.clone().detach().to(torch.float32).to(self.model.device)\n\n        self.register_buffer('betas', to_torch(self.model.betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(self.model.alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod.cpu())))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu())))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu() - 1)))\n\n        # ddim sampling parameters\n        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n                                                                                   ddim_timesteps=self.ddim_timesteps,\n                                                                                   eta=ddim_eta,verbose=verbose)\n        self.register_buffer('ddim_sigmas', ddim_sigmas)\n        self.register_buffer('ddim_alphas', ddim_alphas)\n        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n            (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (\n                        1 - self.alphas_cumprod / self.alphas_cumprod_prev))\n        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n\n    @torch.no_grad()\n    def sample(self,\n               S,\n               batch_size,\n               shape,\n               conditioning=None,\n               callback=None,\n               img_callback=None,\n               quantize_x0=False,\n               eta=0.,\n               mask=None,\n               x0=None,\n               temperature=1.,\n               noise_dropout=0.,\n               score_corrector=None,\n               corrector_kwargs=None,\n               verbose=True,\n               schedule_verbose=False,\n               x_T=None,\n               log_every_t=100,\n               unconditional_guidance_scale=1.,\n               unconditional_conditioning=None,\n               postprocess_fn=None,\n               sample_noise=None,\n               cond_fn=None,\n               # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n               **kwargs\n               ):\n        \n        # check condition bs\n        if conditioning is not None:\n            if isinstance(conditioning, dict):\n                try:\n                    cbs = conditioning[list(conditioning.keys())[0]].shape[0]\n                    if cbs != batch_size:\n                        print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n                except:\n                    # cbs = conditioning[list(conditioning.keys())[0]][0].shape[0]\n                    pass\n            else:\n                if conditioning.shape[0] != batch_size:\n                    print(f\"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}\")\n\n        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=schedule_verbose)\n        \n        # make shape\n        if len(shape) == 3:\n            C, H, W = shape\n            size = (batch_size, C, H, W)\n        elif len(shape) == 4:\n            C, T, H, W = shape\n            size = (batch_size, C, T, H, W)\n        \n        samples, intermediates = self.ddim_sampling(conditioning, size,\n                                                    callback=callback,\n                                                    img_callback=img_callback,\n                                                    quantize_denoised=quantize_x0,\n                                                    mask=mask, x0=x0,\n                                                    ddim_use_original_steps=False,\n                                                    noise_dropout=noise_dropout,\n                                                    temperature=temperature,\n                                                    score_corrector=score_corrector,\n                                                    corrector_kwargs=corrector_kwargs,\n                                                    x_T=x_T,\n                                                    log_every_t=log_every_t,\n                                                    unconditional_guidance_scale=unconditional_guidance_scale,\n                                                    unconditional_conditioning=unconditional_conditioning,\n                                                    postprocess_fn=postprocess_fn,\n                                                    sample_noise=sample_noise,\n                                                    cond_fn=cond_fn,\n                                                    verbose=verbose,\n                                                    **kwargs\n                                                    )\n        return samples, intermediates\n\n    @torch.no_grad()\n    def ddim_sampling(self, cond, shape,\n                      x_T=None, ddim_use_original_steps=False,\n                      callback=None, timesteps=None, quantize_denoised=False,\n                      mask=None, x0=None, img_callback=None, log_every_t=100,\n                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None,\n                      postprocess_fn=None,sample_noise=None,cond_fn=None,\n                      uc_type=None, verbose=True, **kwargs,\n                      ):\n\n        device = self.model.betas.device\n        \n        b = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=device)\n        else:\n            img = x_T\n        \n        if timesteps is None:\n            timesteps = self.ddpm_num_timesteps if ddim_use_original_steps else self.ddim_timesteps\n        elif timesteps is not None and not ddim_use_original_steps:\n            subset_end = int(min(timesteps / self.ddim_timesteps.shape[0], 1) * self.ddim_timesteps.shape[0]) - 1\n            timesteps = self.ddim_timesteps[:subset_end]\n        intermediates = {'x_inter': [img], 'pred_x0': [img]}\n        time_range = reversed(range(0,timesteps)) if ddim_use_original_steps else np.flip(timesteps)\n        total_steps = timesteps if ddim_use_original_steps else timesteps.shape[0]\n        if verbose:\n            iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps)\n        else:\n            iterator = time_range\n        \n        state.sampling_steps = total_steps\n\n        for i, step in enumerate(iterator):\n            state.sampling_step = i\n            if state.interrupted:\n                raise InterruptedException\n\n            index = total_steps - i - 1\n            ts = torch.full((b,), step, device=device, dtype=torch.long)\n\n            if postprocess_fn is not None:\n                img = postprocess_fn(img, ts)\n            \n            outs = self.p_sample_ddim(img, cond, ts, index=index, use_original_steps=ddim_use_original_steps,\n                                      quantize_denoised=quantize_denoised, temperature=temperature,\n                                      noise_dropout=noise_dropout, score_corrector=score_corrector,\n                                      corrector_kwargs=corrector_kwargs,\n                                      unconditional_guidance_scale=unconditional_guidance_scale,\n                                      unconditional_conditioning=unconditional_conditioning,\n                                      sample_noise=sample_noise,cond_fn=cond_fn,uc_type=uc_type, **kwargs,)\n            img, pred_x0 = outs\n\n            if mask is not None:\n                # use mask to blend x_known_t-1 & x_sample_t-1\n                assert x0 is not None\n                x0 = x0.to(img.device)\n                mask = mask.to(img.device)\n                t = torch.tensor([step-1]*x0.shape[0], dtype=torch.long, device=img.device)\n                img_known = self.model.q_sample(x0, t)\n                img = img_known * mask + (1. - mask) * img\n            \n            if callback: callback(i)\n            if img_callback: img_callback(pred_x0, i)\n\n            if index % log_every_t == 0 or index == total_steps - 1:\n                intermediates['x_inter'].append(img)\n                intermediates['pred_x0'].append(pred_x0)\n            if state.skipped:\n                break\n\n        return img, intermediates\n\n    @torch.no_grad()\n    def p_sample_ddim(self, x, c, t, index, repeat_noise=False, use_original_steps=False, quantize_denoised=False,\n                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None, sample_noise=None,\n                      cond_fn=None, uc_type=None, \n                      **kwargs,\n                      ):\n        b, *_, device = *x.shape, x.device\n        if x.dim() == 5:\n            is_video = True\n        else:\n            is_video = False\n        if unconditional_conditioning is None or unconditional_guidance_scale == 1.:\n            e_t = self.model.apply_model(x, t, c, **kwargs) # unet denoiser\n        else:\n            # with unconditional condition\n            if isinstance(c, torch.Tensor):\n                e_t = self.model.apply_model(x, t, c, **kwargs)\n                e_t_uncond = self.model.apply_model(x, t, unconditional_conditioning, **kwargs)\n            elif isinstance(c, dict):\n                e_t = self.model.apply_model(x, t, c, **kwargs)\n                e_t_uncond = self.model.apply_model(x, t, unconditional_conditioning, **kwargs)\n            else:\n                raise NotImplementedError\n            # text cfg\n            if uc_type is None:\n                e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n            else:\n                if uc_type == 'cfg_original':\n                    e_t = e_t + unconditional_guidance_scale * (e_t - e_t_uncond)\n                elif uc_type == 'cfg_ours':\n                    e_t = e_t + unconditional_guidance_scale * (e_t_uncond - e_t)\n                else:\n                    raise NotImplementedError\n            \n        if score_corrector is not None:\n            assert self.model.parameterization == \"eps\"\n            e_t = score_corrector.modify_score(self.model, e_t, x, t, c, **corrector_kwargs)\n\n        alphas = self.model.alphas_cumprod if use_original_steps else self.ddim_alphas\n        alphas_prev = self.model.alphas_cumprod_prev if use_original_steps else self.ddim_alphas_prev\n        sqrt_one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod if use_original_steps else self.ddim_sqrt_one_minus_alphas\n        sigmas = self.model.ddim_sigmas_for_original_num_steps if use_original_steps else self.ddim_sigmas\n        # select parameters corresponding to the currently considered timestep\n        \n        if is_video:\n            size = (b, 1, 1, 1, 1)\n        else:\n            size = (b, 1, 1, 1)\n        a_t = torch.full(size, alphas[index], device=device)\n        a_prev = torch.full(size, alphas_prev[index], device=device)\n        sigma_t = torch.full(size, sigmas[index], device=device)\n        sqrt_one_minus_at = torch.full(size, sqrt_one_minus_alphas[index],device=device)\n\n        # current prediction for x_0\n        pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n        # print(f't={t}, pred_x0, min={torch.min(pred_x0)}, max={torch.max(pred_x0)}',file=f)\n        if quantize_denoised:\n            pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)\n        # direction pointing to x_t\n        dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n        \n        if sample_noise is None:\n            noise = sigma_t * noise_like(x.shape, device, repeat_noise, self.noise_gen) * temperature\n            if noise_dropout > 0.:\n                noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n        else:\n            noise = sigma_t * sample_noise * temperature\n        \n        x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n        \n        return x_prev, pred_x0", ""]}
{"filename": "scripts/videocrafter/lvdm/utils/saving_utils.py", "chunked_list": ["import numpy as np\nimport cv2\nimport os\nimport time\nimport imageio\nfrom tqdm import tqdm\nfrom PIL import Image\nimport os\nimport sys\nsys.path.insert(1, os.path.join(sys.path[0], '..'))", "import sys\nsys.path.insert(1, os.path.join(sys.path[0], '..'))\nimport torch\nimport torchvision\nfrom torchvision.utils import make_grid\nfrom torch import Tensor\nfrom torchvision.transforms.functional import to_tensor\n\n\ndef tensor_to_mp4(video, savepath, fps, rescale=True, nrow=None):\n    \"\"\"\n    video: torch.Tensor, b,c,t,h,w, 0-1\n    if -1~1, enable rescale=True\n    \"\"\"\n    n = video.shape[0]\n    video = video.permute(2, 0, 1, 3, 4) # t,n,c,h,w\n    nrow = int(np.sqrt(n)) if nrow is None else nrow\n    frame_grids = [torchvision.utils.make_grid(framesheet, nrow=nrow) for framesheet in video] # [3, grid_h, grid_w]\n    grid = torch.stack(frame_grids, dim=0) # stack in temporal dim [T, 3, grid_h, grid_w]\n    grid = torch.clamp(grid.float(), -1., 1.)\n    if rescale:\n        grid = (grid + 1.0) / 2.0\n    grid = (grid * 255).to(torch.uint8).permute(0, 2, 3, 1) # [T, 3, grid_h, grid_w] -> [T, grid_h, grid_w, 3]\n    #print(f'Save video to {savepath}')\n    torchvision.io.write_video(savepath, grid, fps=fps, video_codec='h264', options={'crf': '10'})", "\ndef tensor_to_mp4(video, savepath, fps, rescale=True, nrow=None):\n    \"\"\"\n    video: torch.Tensor, b,c,t,h,w, 0-1\n    if -1~1, enable rescale=True\n    \"\"\"\n    n = video.shape[0]\n    video = video.permute(2, 0, 1, 3, 4) # t,n,c,h,w\n    nrow = int(np.sqrt(n)) if nrow is None else nrow\n    frame_grids = [torchvision.utils.make_grid(framesheet, nrow=nrow) for framesheet in video] # [3, grid_h, grid_w]\n    grid = torch.stack(frame_grids, dim=0) # stack in temporal dim [T, 3, grid_h, grid_w]\n    grid = torch.clamp(grid.float(), -1., 1.)\n    if rescale:\n        grid = (grid + 1.0) / 2.0\n    grid = (grid * 255).to(torch.uint8).permute(0, 2, 3, 1) # [T, 3, grid_h, grid_w] -> [T, grid_h, grid_w, 3]\n    #print(f'Save video to {savepath}')\n    torchvision.io.write_video(savepath, grid, fps=fps, video_codec='h264', options={'crf': '10'})", "\n# ----------------------------------------------------------------------------------------------\ndef savenp2sheet(imgs, savepath, nrow=None):\n    \"\"\" save multiple imgs (in numpy array type) to a img sheet.\n        img sheet is one row.\n\n    imgs: \n        np array of size [N, H, W, 3] or List[array] with array size = [H,W,3] \n    \"\"\"\n    if imgs.ndim == 4:\n        img_list = [imgs[i] for i in range(imgs.shape[0])]\n        imgs = img_list\n    \n    imgs_new = []\n    for i, img in enumerate(imgs):\n        if img.ndim == 3 and img.shape[0] == 3:\n            img = np.transpose(img,(1,2,0))\n        \n        assert(img.ndim == 3 and img.shape[-1] == 3), img.shape # h,w,3\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        imgs_new.append(img)\n    n = len(imgs)\n    if nrow is not None:\n        n_cols = nrow\n    else:\n        n_cols=int(n**0.5)\n    n_rows=int(np.ceil(n/n_cols))\n    print(n_cols)\n    print(n_rows)\n\n    imgsheet = cv2.vconcat([cv2.hconcat(imgs_new[i*n_cols:(i+1)*n_cols]) for i in range(n_rows)])\n    cv2.imwrite(savepath, imgsheet)\n    print(f'saved in {savepath}')", "\n# ----------------------------------------------------------------------------------------------\ndef save_np_to_img(img, path, norm=True):\n    if norm:\n        img = (img + 1) / 2 * 255\n    img = img.astype(np.uint8)\n    image = Image.fromarray(img)\n    image.save(path, q=95)\n\n# ----------------------------------------------------------------------------------------------\ndef npz_to_imgsheet_5d(data_path, res_dir, nrow=None,):\n    if isinstance(data_path, str):\n        imgs = np.load(data_path)['arr_0'] # NTHWC\n    elif isinstance(data_path, np.ndarray):\n        imgs = data_path\n    else:\n        raise Exception\n    \n    if os.path.isdir(res_dir):\n        res_path = os.path.join(res_dir, f'samples.jpg')\n    else:\n        assert(res_dir.endswith('.jpg'))\n        res_path = res_dir\n    imgs = np.concatenate([imgs[i] for i in range(imgs.shape[0])], axis=0)\n    savenp2sheet(imgs, res_path, nrow=nrow)", "\n# ----------------------------------------------------------------------------------------------\ndef npz_to_imgsheet_5d(data_path, res_dir, nrow=None,):\n    if isinstance(data_path, str):\n        imgs = np.load(data_path)['arr_0'] # NTHWC\n    elif isinstance(data_path, np.ndarray):\n        imgs = data_path\n    else:\n        raise Exception\n    \n    if os.path.isdir(res_dir):\n        res_path = os.path.join(res_dir, f'samples.jpg')\n    else:\n        assert(res_dir.endswith('.jpg'))\n        res_path = res_dir\n    imgs = np.concatenate([imgs[i] for i in range(imgs.shape[0])], axis=0)\n    savenp2sheet(imgs, res_path, nrow=nrow)", "\n# ----------------------------------------------------------------------------------------------\ndef npz_to_imgsheet_4d(data_path, res_path, nrow=None,):\n    if isinstance(data_path, str):\n        imgs = np.load(data_path)['arr_0'] # NHWC\n    elif isinstance(data_path, np.ndarray):\n        imgs = data_path\n    else:\n        raise Exception\n    print(imgs.shape)\n    savenp2sheet(imgs, res_path, nrow=nrow)", "\n\n# ----------------------------------------------------------------------------------------------\ndef tensor_to_imgsheet(tensor, save_path):\n    \"\"\" \n        save a batch of videos in one image sheet with shape of [batch_size * num_frames].\n        data: [b,c,t,h,w]\n    \"\"\"\n    assert(tensor.dim() == 5)\n    b,c,t,h,w = tensor.shape\n    imgs = [tensor[bi,:,ti, :, :] for bi in range(b) for ti in range(t)]\n    torchvision.utils.save_image(imgs, save_path, normalize=True, nrow=t)", "\n\n# ----------------------------------------------------------------------------------------------\ndef npz_to_frames(data_path, res_dir, norm, num_frames=None, num_samples=None):\n    start = time.time()\n    arr = np.load(data_path)\n    imgs = arr['arr_0'] # [N, T, H, W, 3]\n    print('original data shape: ', imgs.shape)\n\n    if num_samples is not None:\n        imgs = imgs[:num_samples, :, :, :, :]\n        print('after sample selection: ', imgs.shape)\n    \n    if num_frames is not None:\n        imgs = imgs[:, :num_frames, :, :, :]\n        print('after frame selection: ', imgs.shape)\n\n    for vid in tqdm(range(imgs.shape[0]), desc='Video'):\n        video_dir = os.path.join(res_dir, f'video{vid:04d}')\n        os.makedirs(video_dir, exist_ok=True)\n        for fid in range(imgs.shape[1]):\n            frame = imgs[vid, fid, :, :, :] #HW3\n            save_np_to_img(frame, os.path.join(video_dir, f'frame{fid:04d}.jpg'), norm=norm)\n    print('Finish')\n    print(f'Total time = {time.time()- start}')", "\n# ----------------------------------------------------------------------------------------------\ndef npz_to_gifs(data_path, res_dir, duration=0.2, start_idx=0, num_videos=None, mode='gif'):\n    os.makedirs(res_dir, exist_ok=True)\n    if isinstance(data_path, str):\n        imgs = np.load(data_path)['arr_0'] # NTHWC\n    elif isinstance(data_path, np.ndarray):\n        imgs = data_path\n    else:\n        raise Exception\n\n    for i in range(imgs.shape[0]):\n        frames = [imgs[i,j,:,:,:] for j in range(imgs[i].shape[0])] # [(h,w,3)]\n        if mode == 'gif':\n            imageio.mimwrite(os.path.join(res_dir, f'samples_{start_idx+i}.gif'), frames, format='GIF', duration=duration)\n        elif mode == 'mp4':\n            frames = [torch.from_numpy(frame) for frame in frames]\n            frames = torch.stack(frames, dim=0).to(torch.uint8) # [T, H, W, C]\n            torchvision.io.write_video(os.path.join(res_dir, f'samples_{start_idx+i}.mp4'),\n                frames, fps=0.5, video_codec='h264', options={'crf': '10'})\n        if i+ 1 == num_videos:\n            break", "\n# ----------------------------------------------------------------------------------------------\ndef fill_with_black_squares(video, desired_len: int) -> Tensor:\n    if len(video) >= desired_len:\n        return video\n\n    return torch.cat([\n        video,\n        torch.zeros_like(video[0]).unsqueeze(0).repeat(desired_len - len(video), 1, 1, 1),\n    ], dim=0)", "\n# ----------------------------------------------------------------------------------------------\ndef load_num_videos(data_path, num_videos):\n    # data_path can be either data_path of np array \n    if isinstance(data_path, str):\n        videos = np.load(data_path)['arr_0'] # NTHWC\n    elif isinstance(data_path, np.ndarray):\n        videos = data_path\n    else:\n        raise Exception\n\n    if num_videos is not None:\n        videos = videos[:num_videos, :, :, :, :]\n    return videos", "\n# ----------------------------------------------------------------------------------------------\ndef npz_to_video_grid(data_path, out_path, num_frames=None, fps=8, num_videos=None, nrow=None, verbose=True):\n    if isinstance(data_path, str):\n        videos = load_num_videos(data_path, num_videos)\n    elif isinstance(data_path, np.ndarray):\n        videos = data_path\n    else:\n        raise Exception\n    n,t,h,w,c = videos.shape\n\n    videos_th = []\n    for i in range(n):\n        video = videos[i, :,:,:,:]\n        images = [video[j, :,:,:] for j in range(t)]\n        images = [to_tensor(img) for img in images]\n        video = torch.stack(images)\n        videos_th.append(video)\n    \n    if num_frames is None:\n        num_frames = videos.shape[1]\n    if verbose:\n        videos = [fill_with_black_squares(v, num_frames) for v in tqdm(videos_th, desc='Adding empty frames')] # NTCHW\n    else:\n        videos = [fill_with_black_squares(v, num_frames) for v in videos_th] # NTCHW\n\n    frame_grids = torch.stack(videos).permute(1, 0, 2, 3, 4) # [T, N, C, H, W]\n    if nrow is None:\n        nrow = int(np.ceil(np.sqrt(n)))\n    if verbose:\n        frame_grids = [make_grid(fs, nrow=nrow) for fs in tqdm(frame_grids, desc='Making grids')]\n    else:\n        frame_grids = [make_grid(fs, nrow=nrow) for fs in frame_grids]\n\n    if os.path.dirname(out_path) != \"\":\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n    frame_grids = (torch.stack(frame_grids) * 255).to(torch.uint8).permute(0, 2, 3, 1) # [T, H, W, C]\n    torchvision.io.write_video(out_path, frame_grids, fps=fps, video_codec='h264', options={'crf': '10'})", "\n# ----------------------------------------------------------------------------------------------\ndef npz_to_gif_grid(data_path, out_path, n_cols=None, num_videos=20):\n    arr = np.load(data_path)\n    imgs = arr['arr_0'] # [N, T, H, W, 3]\n    imgs = imgs[:num_videos]\n    n, t, h, w, c = imgs.shape\n    assert(n == num_videos)\n    n_cols = n_cols if n_cols else imgs.shape[0]\n    n_rows = np.ceil(imgs.shape[0] / n_cols).astype(np.int8)\n    H, W = h * n_rows, w * n_cols\n    grid = np.zeros((t, H, W, c), dtype=np.uint8)\n\n    for i in range(n_rows):\n        for j in range(n_cols):\n            if i*n_cols+j < imgs.shape[0]:\n                grid[:, i*h:(i+1)*h, j*w:(j+1)*w, :] = imgs[i*n_cols+j, :, :, :, :]\n    \n    videos = [grid[i] for i in range(grid.shape[0])] # grid: TH'W'C\n    imageio.mimwrite(out_path, videos, format='GIF', duration=0.5,palettesize=256)", "\n\n# ----------------------------------------------------------------------------------------------\ndef torch_to_video_grid(videos, out_path, num_frames, fps, num_videos=None, nrow=None, verbose=True):\n    \"\"\"\n    videos: -1 ~ 1, torch.Tensor, BCTHW\n    \"\"\"\n    n,t,h,w,c = videos.shape\n    videos_th = [videos[i, ...] for i in range(n)]\n    if verbose:\n        videos = [fill_with_black_squares(v, num_frames) for v in tqdm(videos_th, desc='Adding empty frames')] # NTCHW\n    else:\n        videos = [fill_with_black_squares(v, num_frames) for v in videos_th] # NTCHW\n\n    frame_grids = torch.stack(videos).permute(1, 0, 2, 3, 4) # [T, N, C, H, W]\n    if nrow is None:\n        nrow = int(np.ceil(np.sqrt(n)))\n    if verbose:\n        frame_grids = [make_grid(fs, nrow=nrow) for fs in tqdm(frame_grids, desc='Making grids')]\n    else:\n        frame_grids = [make_grid(fs, nrow=nrow) for fs in frame_grids]\n\n    if os.path.dirname(out_path) != \"\":\n        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n    frame_grids = ((torch.stack(frame_grids) + 1) / 2 * 255).to(torch.uint8).permute(0, 2, 3, 1) # [T, H, W, C]\n    torchvision.io.write_video(out_path, frame_grids, fps=fps, video_codec='h264', options={'crf': '10'})", ""]}
{"filename": "scripts/videocrafter/lvdm/utils/dist_utils.py", "chunked_list": ["import torch\nimport torch.distributed as dist\n\ndef setup_dist(local_rank):\n    if dist.is_initialized():\n        return\n    torch.cuda.set_device(local_rank)\n    torch.distributed.init_process_group(\n        'nccl',\n        init_method='env://'\n    )", "\ndef gather_data(data, return_np=True):\n    ''' gather data from multiple processes to one list '''\n    data_list = [torch.zeros_like(data) for _ in range(dist.get_world_size())]\n    dist.all_gather(data_list, data)  # gather not supported with NCCL\n    if return_np:\n        data_list = [data.cpu().numpy() for data in data_list]\n    return data_list\n", ""]}
{"filename": "scripts/videocrafter/lvdm/utils/common_utils.py", "chunked_list": ["\nimport importlib\n\nimport torch\nimport numpy as np\n\nfrom inspect import isfunction\nfrom PIL import Image, ImageDraw, ImageFont\n\n\ndef str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise ValueError('Boolean value expected.')", "\n\ndef str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise ValueError('Boolean value expected.')", "\n\ndef instantiate_from_config(config):\n    if not \"target\" in config:\n        if config == '__is_first_stage__':\n            return None\n        elif config == \"__is_unconditional__\":\n            return None\n        raise KeyError(\"Expected key `target` to instantiate.\")\n\n    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))", "\ndef get_obj_from_str(string, reload=False):\n    module, cls = string.rsplit(\".\", 1)\n    if reload:\n        module_imp = importlib.import_module('videocrafter.'+module if not 'torch' in module else module)\n        importlib.reload(module_imp)\n    return getattr(importlib.import_module('videocrafter.'+module if not 'torch' in module else module, package=None), cls)\n\ndef log_txt_as_img(wh, xc, size=10):\n    # wh a tuple of (width, height)\n    # xc a list of captions to plot\n    b = len(xc)\n    txts = list()\n    for bi in range(b):\n        txt = Image.new(\"RGB\", wh, color=\"white\")\n        draw = ImageDraw.Draw(txt)\n        font = ImageFont.truetype('data/DejaVuSans.ttf', size=size)\n        nc = int(40 * (wh[0] / 256))\n        lines = \"\\n\".join(xc[bi][start:start + nc] for start in range(0, len(xc[bi]), nc))\n\n        try:\n            draw.text((0, 0), lines, fill=\"black\", font=font)\n        except UnicodeEncodeError:\n            print(\"Cant encode string for logging. Skipping.\")\n\n        txt = np.array(txt).transpose(2, 0, 1) / 127.5 - 1.0\n        txts.append(txt)\n    txts = np.stack(txts)\n    txts = torch.tensor(txts)\n    return txts", "def log_txt_as_img(wh, xc, size=10):\n    # wh a tuple of (width, height)\n    # xc a list of captions to plot\n    b = len(xc)\n    txts = list()\n    for bi in range(b):\n        txt = Image.new(\"RGB\", wh, color=\"white\")\n        draw = ImageDraw.Draw(txt)\n        font = ImageFont.truetype('data/DejaVuSans.ttf', size=size)\n        nc = int(40 * (wh[0] / 256))\n        lines = \"\\n\".join(xc[bi][start:start + nc] for start in range(0, len(xc[bi]), nc))\n\n        try:\n            draw.text((0, 0), lines, fill=\"black\", font=font)\n        except UnicodeEncodeError:\n            print(\"Cant encode string for logging. Skipping.\")\n\n        txt = np.array(txt).transpose(2, 0, 1) / 127.5 - 1.0\n        txts.append(txt)\n    txts = np.stack(txts)\n    txts = torch.tensor(txts)\n    return txts", "\n\ndef ismap(x):\n    if not isinstance(x, torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] > 3)\n\n\ndef isimage(x):\n    if not isinstance(x,torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)", "def isimage(x):\n    if not isinstance(x,torch.Tensor):\n        return False\n    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)\n\n\ndef exists(x):\n    return x is not None\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d", "\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\ndef mean_flat(tensor):\n    \"\"\"\n    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))", "def mean_flat(tensor):\n    \"\"\"\n    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n\n\ndef count_params(model, verbose=False):\n    total_params = sum(p.numel() for p in model.parameters())\n    if verbose:\n        print(f\"{model.__class__.__name__} has {total_params*1.e-6:.2f} M params.\")\n    return total_params", "def count_params(model, verbose=False):\n    total_params = sum(p.numel() for p in model.parameters())\n    if verbose:\n        print(f\"{model.__class__.__name__} has {total_params*1.e-6:.2f} M params.\")\n    return total_params\n\n\ndef instantiate_from_config(config):\n    if not \"target\" in config:\n        if config == '__is_first_stage__':\n            return None\n        elif config == \"__is_unconditional__\":\n            return None\n        raise KeyError(\"Expected key `target` to instantiate.\")\n\n    if \"instantiate_with_dict\" in config and config[\"instantiate_with_dict\"]:\n        # input parameter is one dict\n        return get_obj_from_str(config[\"target\"])(config.get(\"params\", dict()), **kwargs)\n    else:\n        return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))", "\n\ndef get_obj_from_str(string, reload=False):\n    module, cls = string.rsplit(\".\", 1)\n    if reload:\n        module_imp = importlib.import_module('videocrafter.'+module if not 'torch' in module else module)\n        importlib.reload(module_imp)\n    return getattr(importlib.import_module('videocrafter.'+module if not 'torch' in module else module, package=None), cls)\n\n\ndef check_istarget(name, para_list):\n    \"\"\" \n    name: full name of source para\n    para_list: partial name of target para \n    \"\"\"\n    istarget=False\n    for para in para_list:\n        if para in name:\n            return True\n    return istarget", "\n\ndef check_istarget(name, para_list):\n    \"\"\" \n    name: full name of source para\n    para_list: partial name of target para \n    \"\"\"\n    istarget=False\n    for para in para_list:\n        if para in name:\n            return True\n    return istarget"]}
{"filename": "scripts/videocrafter/lvdm/data/webvid.py", "chunked_list": ["import os\nimport random\nimport bisect\n\nimport pandas as pd\n\nimport omegaconf\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms", "from torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom decord import VideoReader, cpu\nimport torchvision.transforms._transforms_video as transforms_video\n\nclass WebVid(Dataset):\n    \"\"\"\n    WebVid Dataset.\n    Assumes webvid data is structured as follows.\n    Webvid/\n        videos/\n            000001_000050/      ($page_dir)\n                1.mp4           (videoid.mp4)\n                ...\n                5000.mp4\n            ...\n    \"\"\"\n    def __init__(self,\n                 meta_path,\n                 data_dir,\n                 subsample=None,\n                 video_length=16,\n                 resolution=[256, 512],\n                 frame_stride=1,\n                 spatial_transform=None,\n                 crop_resolution=None,\n                 fps_max=None,\n                 load_raw_resolution=False,\n                 fps_schedule=None,\n                 fs_probs=None,\n                 bs_per_gpu=None,\n                 trigger_word='',\n                 dataname='',\n                 ):\n        self.meta_path = meta_path\n        self.data_dir = data_dir\n        self.subsample = subsample\n        self.video_length = video_length\n        self.resolution = [resolution, resolution] if isinstance(resolution, int) else resolution\n        self.frame_stride = frame_stride\n        self.fps_max = fps_max\n        self.load_raw_resolution = load_raw_resolution\n        self.fs_probs = fs_probs\n        self.trigger_word = trigger_word\n        self.dataname = dataname\n\n        self._load_metadata()\n        if spatial_transform is not None:\n            if spatial_transform == \"random_crop\":\n                self.spatial_transform = transforms_video.RandomCropVideo(crop_resolution)\n            elif spatial_transform == \"resize_center_crop\":\n                assert(self.resolution[0] == self.resolution[1])\n                self.spatial_transform = transforms.Compose([\n                    transforms.Resize(resolution),\n                    transforms_video.CenterCropVideo(resolution),\n                    ])\n            else:\n                raise NotImplementedError\n        else:\n            self.spatial_transform = None\n        \n        self.fps_schedule = fps_schedule\n        self.bs_per_gpu = bs_per_gpu\n        if self.fps_schedule is not None:\n            assert(self.bs_per_gpu is not None)\n            self.counter = 0\n            self.stage_idx = 0\n        \n    def _load_metadata(self):\n        metadata = pd.read_csv(self.meta_path)\n        if self.subsample is not None:\n            metadata = metadata.sample(self.subsample, random_state=0)\n        metadata['caption'] = metadata['name']\n        del metadata['name']\n        self.metadata = metadata\n        self.metadata.dropna(inplace=True)\n        # self.metadata['caption'] = self.metadata['caption'].str[:350]\n    \n    def _get_video_path(self, sample):\n        if self.dataname == \"loradata\":\n            rel_video_fp =  str(sample['videoid']) + '.mp4'\n            full_video_fp = os.path.join(self.data_dir, rel_video_fp)\n        else:\n            rel_video_fp = os.path.join(sample['page_dir'], str(sample['videoid']) + '.mp4')\n            full_video_fp = os.path.join(self.data_dir, 'videos', rel_video_fp)\n        return full_video_fp, rel_video_fp\n    \n    def get_fs_based_on_schedule(self, frame_strides, schedule):\n        assert(len(frame_strides) == len(schedule) + 1) # nstage=len_fps_schedule + 1\n        global_step = self.counter // self.bs_per_gpu # TODO: support resume.\n        stage_idx = bisect.bisect(schedule, global_step)\n        frame_stride = frame_strides[stage_idx]\n        # log stage change\n        if stage_idx != self.stage_idx:\n            print(f'fps stage: {stage_idx} start ... new frame stride = {frame_stride}')\n            self.stage_idx = stage_idx\n        return frame_stride\n    \n    def get_fs_based_on_probs(self, frame_strides, probs):\n        assert(len(frame_strides) == len(probs))\n        return random.choices(frame_strides, weights=probs)[0]\n\n    def get_fs_randomly(self, frame_strides):\n        return random.choice(frame_strides)\n    \n    def __getitem__(self, index):\n        \n        if isinstance(self.frame_stride, list) or isinstance(self.frame_stride, omegaconf.listconfig.ListConfig):\n            if self.fps_schedule is not None:\n                frame_stride = self.get_fs_based_on_schedule(self.frame_stride, self.fps_schedule)\n            elif self.fs_probs is not None:\n                frame_stride = self.get_fs_based_on_probs(self.frame_stride, self.fs_probs)\n            else:\n                frame_stride = self.get_fs_randomly(self.frame_stride)\n        else:\n            frame_stride = self.frame_stride\n        assert(isinstance(frame_stride, int)), type(frame_stride)\n\n        while True:\n            index = index % len(self.metadata)\n            sample = self.metadata.iloc[index]\n            video_path, rel_fp = self._get_video_path(sample)\n            caption = sample['caption']+self.trigger_word\n            \n            # make reader\n            try:\n                if self.load_raw_resolution:\n                    video_reader = VideoReader(video_path, ctx=cpu(0))\n                else:\n                    video_reader = VideoReader(video_path, ctx=cpu(0), width=self.resolution[1], height=self.resolution[0])\n                if len(video_reader) < self.video_length:\n                    print(f\"video length ({len(video_reader)}) is smaller than target length({self.video_length})\")\n                    index += 1\n                    continue\n                else:\n                    pass\n            except:\n                index += 1\n                print(f\"Load video failed! path = {video_path}\")\n                continue\n            \n            # sample strided frames\n            all_frames = list(range(0, len(video_reader), frame_stride))\n            if len(all_frames) < self.video_length: # recal a max fs\n                frame_stride = len(video_reader) // self.video_length\n                assert(frame_stride != 0)\n                all_frames = list(range(0, len(video_reader), frame_stride))\n\n            # select a random clip\n            rand_idx = random.randint(0, len(all_frames) - self.video_length)\n            frame_indices = all_frames[rand_idx:rand_idx+self.video_length]\n            try:\n                frames = video_reader.get_batch(frame_indices)\n                break\n            except:\n                print(f\"Get frames failed! path = {video_path}\")\n                index += 1\n                continue\n\n        assert(frames.shape[0] == self.video_length),f'{len(frames)}, self.video_length={self.video_length}'\n        frames = torch.tensor(frames.asnumpy()).permute(3, 0, 1, 2).float() # [t,h,w,c] -> [c,t,h,w]\n        if self.spatial_transform is not None:\n            frames = self.spatial_transform(frames)\n        if self.resolution is not None:\n            assert(frames.shape[2] == self.resolution[0] and frames.shape[3] == self.resolution[1]), f'frames={frames.shape}, self.resolution={self.resolution}'\n        frames = (frames / 255 - 0.5) * 2\n        \n        fps_ori = video_reader.get_avg_fps()\n        fps_clip = fps_ori // frame_stride\n        if self.fps_max is not None and fps_clip > self.fps_max:\n            fps_clip = self.fps_max\n        \n        data = {'video': frames, 'caption': caption, 'path': video_path, 'fps': fps_clip, 'frame_stride': frame_stride}\n\n        if self.fps_schedule is not None:\n            self.counter += 1\n        return data\n    \n    def __len__(self):\n        return len(self.metadata)", ""]}
{"filename": "scripts/videocrafter/lvdm/models/autoencoder.py", "chunked_list": ["import torch\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nimport os\nfrom einops import rearrange\n\nfrom videocrafter.lvdm.models.modules.autoencoder_modules import Encoder, Decoder\nfrom videocrafter.lvdm.models.modules.distributions import DiagonalGaussianDistribution\nfrom videocrafter.lvdm.utils.common_utils import instantiate_from_config\n\nclass AutoencoderKL(pl.LightningModule):\n    def __init__(self,\n                 ddconfig,\n                 lossconfig,\n                 embed_dim,\n                 ckpt_path=None,\n                 ignore_keys=[],\n                 image_key=\"image\",\n                 colorize_nlabels=None,\n                 monitor=None,\n                 test=False,\n                 logdir=None,\n                 input_dim=4,\n                 test_args=None,\n                 ):\n        super().__init__()\n        self.image_key = image_key\n        self.encoder = Encoder(**ddconfig)\n        self.decoder = Decoder(**ddconfig)\n        self.loss = instantiate_from_config(lossconfig)\n        assert ddconfig[\"double_z\"]\n        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n        self.embed_dim = embed_dim\n        self.input_dim = input_dim\n        self.test = test\n        self.test_args = test_args\n        self.logdir = logdir\n        if colorize_nlabels is not None:\n            assert type(colorize_nlabels)==int\n            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n        if monitor is not None:\n            self.monitor = monitor\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n        if self.test:\n            self.init_test()\n    \n    def init_test(self,):\n        self.test = True\n        save_dir = os.path.join(self.logdir, \"test\")\n        if 'ckpt' in self.test_args:\n            ckpt_name = os.path.basename(self.test_args.ckpt).split('.ckpt')[0] + f'_epoch{self._cur_epoch}'\n            self.root = os.path.join(save_dir, ckpt_name)\n        else:\n            self.root = save_dir\n        if 'test_subdir' in self.test_args:\n            self.root = os.path.join(save_dir, self.test_args.test_subdir)\n\n        self.root_zs = os.path.join(self.root, \"zs\")\n        self.root_dec = os.path.join(self.root, \"reconstructions\")\n        self.root_inputs = os.path.join(self.root, \"inputs\")\n        os.makedirs(self.root, exist_ok=True)\n\n        if self.test_args.save_z:\n            os.makedirs(self.root_zs, exist_ok=True)\n        if self.test_args.save_reconstruction:\n            os.makedirs(self.root_dec, exist_ok=True)\n        if self.test_args.save_input:\n            os.makedirs(self.root_inputs, exist_ok=True)\n        assert(self.test_args is not None)\n        self.test_maximum = getattr(self.test_args, 'test_maximum', None) #1500 # 12000/8\n        self.count = 0\n        self.eval_metrics = {}\n        self.decodes = []\n        self.save_decode_samples = 2048\n        \n    def init_from_ckpt(self, path, ignore_keys=list()):\n        sd = torch.load(path, map_location=\"cpu\")\n        try:\n            self._cur_epoch = sd['epoch']\n            sd = sd[\"state_dict\"]\n        except:\n            self._cur_epoch = 'null'\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(\"Deleting key {} from state_dict.\".format(k))\n                    del sd[k]\n        self.load_state_dict(sd, strict=False)\n        # self.load_state_dict(sd, strict=True)\n        print(f\"Restored from {path}\")\n\n    def encode(self, x, **kwargs):\n        \n        h = self.encoder(x)\n        moments = self.quant_conv(h)\n        posterior = DiagonalGaussianDistribution(moments)\n        return posterior\n\n    def decode(self, z, **kwargs):\n        z = self.post_quant_conv(z)\n        dec = self.decoder(z)\n        return dec\n\n    def forward(self, input, sample_posterior=True):\n        posterior = self.encode(input)\n        if sample_posterior:\n            z = posterior.sample()\n        else:\n            z = posterior.mode()\n        dec = self.decode(z)\n        return dec, posterior\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        # if len(x.shape) == 3:\n        #     x = x[..., None]\n        # if x.dim() == 4:\n        #     x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if x.dim() == 5 and self.input_dim == 4:\n            b,c,t,h,w = x.shape\n            self.b = b\n            self.t = t \n            x = rearrange(x, 'b c t h w -> (b t) c h w')\n\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        inputs = self.get_input(batch, self.image_key)\n        reconstructions, posterior = self(inputs)\n\n        if optimizer_idx == 0:\n            # train encoder+decoder+logvar\n            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\"train\")\n            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # train the discriminator\n            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n                                                last_layer=self.get_last_layer(), split=\"train\")\n\n            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        inputs = self.get_input(batch, self.image_key)\n        reconstructions, posterior = self(inputs)\n        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step,\n                                        last_layer=self.get_last_layer(), split=\"val\")\n\n        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\"val\")\n\n        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n    \n    def configure_optimizers(self):\n        lr = self.learning_rate\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr, betas=(0.5, 0.9))\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n    @torch.no_grad()\n    def log_images(self, batch, only_inputs=False, **kwargs):\n        log = dict()\n        x = self.get_input(batch, self.image_key)\n        x = x.to(self.device)\n        if not only_inputs:\n            xrec, posterior = self(x)\n            if x.shape[1] > 3:\n                # colorize with random projection\n                assert xrec.shape[1] > 3\n                x = self.to_rgb(x)\n                xrec = self.to_rgb(xrec)\n            log[\"samples\"] = self.decode(torch.randn_like(posterior.sample()))\n            log[\"reconstructions\"] = xrec\n        log[\"inputs\"] = x\n        return log\n\n    def to_rgb(self, x):\n        assert self.image_key == \"segmentation\"\n        if not hasattr(self, \"colorize\"):\n            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n        x = F.conv2d(x, weight=self.colorize)\n        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n        return x", "from videocrafter.lvdm.utils.common_utils import instantiate_from_config\n\nclass AutoencoderKL(pl.LightningModule):\n    def __init__(self,\n                 ddconfig,\n                 lossconfig,\n                 embed_dim,\n                 ckpt_path=None,\n                 ignore_keys=[],\n                 image_key=\"image\",\n                 colorize_nlabels=None,\n                 monitor=None,\n                 test=False,\n                 logdir=None,\n                 input_dim=4,\n                 test_args=None,\n                 ):\n        super().__init__()\n        self.image_key = image_key\n        self.encoder = Encoder(**ddconfig)\n        self.decoder = Decoder(**ddconfig)\n        self.loss = instantiate_from_config(lossconfig)\n        assert ddconfig[\"double_z\"]\n        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n        self.embed_dim = embed_dim\n        self.input_dim = input_dim\n        self.test = test\n        self.test_args = test_args\n        self.logdir = logdir\n        if colorize_nlabels is not None:\n            assert type(colorize_nlabels)==int\n            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n        if monitor is not None:\n            self.monitor = monitor\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n        if self.test:\n            self.init_test()\n    \n    def init_test(self,):\n        self.test = True\n        save_dir = os.path.join(self.logdir, \"test\")\n        if 'ckpt' in self.test_args:\n            ckpt_name = os.path.basename(self.test_args.ckpt).split('.ckpt')[0] + f'_epoch{self._cur_epoch}'\n            self.root = os.path.join(save_dir, ckpt_name)\n        else:\n            self.root = save_dir\n        if 'test_subdir' in self.test_args:\n            self.root = os.path.join(save_dir, self.test_args.test_subdir)\n\n        self.root_zs = os.path.join(self.root, \"zs\")\n        self.root_dec = os.path.join(self.root, \"reconstructions\")\n        self.root_inputs = os.path.join(self.root, \"inputs\")\n        os.makedirs(self.root, exist_ok=True)\n\n        if self.test_args.save_z:\n            os.makedirs(self.root_zs, exist_ok=True)\n        if self.test_args.save_reconstruction:\n            os.makedirs(self.root_dec, exist_ok=True)\n        if self.test_args.save_input:\n            os.makedirs(self.root_inputs, exist_ok=True)\n        assert(self.test_args is not None)\n        self.test_maximum = getattr(self.test_args, 'test_maximum', None) #1500 # 12000/8\n        self.count = 0\n        self.eval_metrics = {}\n        self.decodes = []\n        self.save_decode_samples = 2048\n        \n    def init_from_ckpt(self, path, ignore_keys=list()):\n        sd = torch.load(path, map_location=\"cpu\")\n        try:\n            self._cur_epoch = sd['epoch']\n            sd = sd[\"state_dict\"]\n        except:\n            self._cur_epoch = 'null'\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(\"Deleting key {} from state_dict.\".format(k))\n                    del sd[k]\n        self.load_state_dict(sd, strict=False)\n        # self.load_state_dict(sd, strict=True)\n        print(f\"Restored from {path}\")\n\n    def encode(self, x, **kwargs):\n        \n        h = self.encoder(x)\n        moments = self.quant_conv(h)\n        posterior = DiagonalGaussianDistribution(moments)\n        return posterior\n\n    def decode(self, z, **kwargs):\n        z = self.post_quant_conv(z)\n        dec = self.decoder(z)\n        return dec\n\n    def forward(self, input, sample_posterior=True):\n        posterior = self.encode(input)\n        if sample_posterior:\n            z = posterior.sample()\n        else:\n            z = posterior.mode()\n        dec = self.decode(z)\n        return dec, posterior\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        # if len(x.shape) == 3:\n        #     x = x[..., None]\n        # if x.dim() == 4:\n        #     x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if x.dim() == 5 and self.input_dim == 4:\n            b,c,t,h,w = x.shape\n            self.b = b\n            self.t = t \n            x = rearrange(x, 'b c t h w -> (b t) c h w')\n\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        inputs = self.get_input(batch, self.image_key)\n        reconstructions, posterior = self(inputs)\n\n        if optimizer_idx == 0:\n            # train encoder+decoder+logvar\n            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\"train\")\n            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # train the discriminator\n            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n                                                last_layer=self.get_last_layer(), split=\"train\")\n\n            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        inputs = self.get_input(batch, self.image_key)\n        reconstructions, posterior = self(inputs)\n        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step,\n                                        last_layer=self.get_last_layer(), split=\"val\")\n\n        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\"val\")\n\n        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n    \n    def configure_optimizers(self):\n        lr = self.learning_rate\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr, betas=(0.5, 0.9))\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n    @torch.no_grad()\n    def log_images(self, batch, only_inputs=False, **kwargs):\n        log = dict()\n        x = self.get_input(batch, self.image_key)\n        x = x.to(self.device)\n        if not only_inputs:\n            xrec, posterior = self(x)\n            if x.shape[1] > 3:\n                # colorize with random projection\n                assert xrec.shape[1] > 3\n                x = self.to_rgb(x)\n                xrec = self.to_rgb(xrec)\n            log[\"samples\"] = self.decode(torch.randn_like(posterior.sample()))\n            log[\"reconstructions\"] = xrec\n        log[\"inputs\"] = x\n        return log\n\n    def to_rgb(self, x):\n        assert self.image_key == \"segmentation\"\n        if not hasattr(self, \"colorize\"):\n            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n        x = F.conv2d(x, weight=self.colorize)\n        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n        return x", ""]}
{"filename": "scripts/videocrafter/lvdm/models/ddpm3d.py", "chunked_list": ["import os\nimport time\nimport random\nimport itertools\nfrom functools import partial\nfrom contextlib import contextmanager\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom einops import rearrange, repeat", "from tqdm import tqdm\nfrom einops import rearrange, repeat\n\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torchvision.utils import make_grid\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom pytorch_lightning.utilities import rank_zero_only\nfrom videocrafter.lvdm.models.modules.distributions import normal_kl, DiagonalGaussianDistribution", "from pytorch_lightning.utilities import rank_zero_only\nfrom videocrafter.lvdm.models.modules.distributions import normal_kl, DiagonalGaussianDistribution\nfrom videocrafter.lvdm.models.modules.util import make_beta_schedule, extract_into_tensor, noise_like\nfrom videocrafter.lvdm.models.modules.lora import inject_trainable_lora\nfrom videocrafter.lvdm.samplers.ddim import DDIMSampler\nfrom videocrafter.lvdm.utils.common_utils import log_txt_as_img, exists, default, ismap, isimage, mean_flat, count_params, instantiate_from_config, check_istarget\n\n\ndef disabled_train(self, mode=True):\n    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n    does not change anymore.\"\"\"\n    return self", "def disabled_train(self, mode=True):\n    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n    does not change anymore.\"\"\"\n    return self\n\n\ndef uniform_on_device(r1, r2, shape, device):\n    return (r1 - r2) * torch.rand(*shape, device=device) + r2\n\n\ndef split_video_to_clips(video, clip_length, drop_left=True):\n    video_length = video.shape[2]\n    shape = video.shape\n    if video_length % clip_length != 0 and drop_left:\n        video = video[:, :, :video_length // clip_length * clip_length, :, :]\n        print(f'[split_video_to_clips] Drop frames from {shape} to {video.shape}')\n    nclips = video_length // clip_length\n    clips = rearrange(video, 'b c (nc cl) h w -> (b nc) c cl h w', cl=clip_length, nc=nclips)\n    return clips", "\n\ndef split_video_to_clips(video, clip_length, drop_left=True):\n    video_length = video.shape[2]\n    shape = video.shape\n    if video_length % clip_length != 0 and drop_left:\n        video = video[:, :, :video_length // clip_length * clip_length, :, :]\n        print(f'[split_video_to_clips] Drop frames from {shape} to {video.shape}')\n    nclips = video_length // clip_length\n    clips = rearrange(video, 'b c (nc cl) h w -> (b nc) c cl h w', cl=clip_length, nc=nclips)\n    return clips", "\ndef merge_clips_to_videos(clips, bs):\n    nclips = clips.shape[0] // bs\n    video = rearrange(clips, '(b nc) c t h w -> b c (nc t) h w', nc=nclips)\n    return video\n\nclass DDPM(pl.LightningModule):\n    # classic DDPM with Gaussian diffusion, in pixel space\n    def __init__(self,\n                 unet_config,\n                 timesteps=1000,\n                 beta_schedule=\"linear\",\n                 loss_type=\"l2\",\n                 ckpt_path=None,\n                 ignore_keys=[],\n                 load_only_unet=False,\n                 monitor=\"val/loss\",\n                 use_ema=True,\n                 first_stage_key=\"image\",\n                 image_size=256,\n                 video_length=None,\n                 channels=3,\n                 log_every_t=100,\n                 clip_denoised=True,\n                 linear_start=1e-4,\n                 linear_end=2e-2,\n                 cosine_s=8e-3,\n                 given_betas=None,\n                 original_elbo_weight=0.,\n                 v_posterior=0.,\n                 l_simple_weight=1.,\n                 conditioning_key=None,\n                 parameterization=\"eps\",\n                 scheduler_config=None,\n                 learn_logvar=False,\n                 logvar_init=0.,\n                 *args, **kwargs\n                 ):\n        super().__init__()\n        assert parameterization in [\"eps\", \"x0\"], 'currently only supporting \"eps\" and \"x0\"'\n        self.parameterization = parameterization\n        print(f\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        \n        if isinstance(self.image_size, int):\n            self.image_size = [self.image_size, self.image_size]\n        self.channels = channels\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        self.conditioning_key = conditioning_key # also register conditioning_key in diffusion\n        \n        self.temporal_length = video_length if video_length is not None else unet_config.params.temporal_length\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n        \n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys, only_model=load_only_unet)\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n    def register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \"eps\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \"x0\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\"mu not supported\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\"{context}: Switched to EMA weights\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\"{context}: Restored training weights\")\n\n    def init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n        sd = torch.load(path, map_location=\"cpu\")\n        if \"state_dict\" in list(sd.keys()):\n            sd = sd[\"state_dict\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik) or (ik.startswith('**') and ik.split('**')[-1] in k):\n                    print(\"Deleting key {} from state_dict.\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n        if len(missing) > 0:\n            print(f\"Missing Keys: {missing}\")\n        if len(unexpected) > 0:\n            print(f\"Unexpected Keys: {unexpected}\")\n\n    def q_mean_variance(self, x_start, t):\n        \"\"\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \"\"\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \"eps\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \"x0\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        channels = self.channels\n        video_length = self.total_length\n        size = (batch_size, channels, video_length, *self.image_size)\n        return self.p_sample_loop(size,\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n    def get_loss(self, pred, target, mean=True, mask=None):\n        if self.loss_type == 'l1':\n            loss = (target - pred).abs()\n            if mean:\n                loss = loss.mean()\n        elif self.loss_type == 'l2':\n            if mean:\n                loss = torch.nn.functional.mse_loss(target, pred)\n            else:\n                loss = torch.nn.functional.mse_loss(target, pred, reduction='none')\n        else:\n            raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n        if mask is not None:\n            assert(mean is False)\n            assert(loss.shape[2:] == mask.shape[2:]) #thw need be the same\n            loss = loss * mask\n        return loss\n\n    def p_losses(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n        model_out = self.model(x_noisy, t)\n\n        loss_dict = {}\n        if self.parameterization == \"eps\":\n            target = noise\n        elif self.parameterization == \"x0\":\n            target = x_start\n        else:\n            raise NotImplementedError(f\"Paramterization {self.parameterization} not yet supported\")\n\n        loss = self.get_loss(model_out, target, mean=False).mean(dim=[1, 2, 3, 4])\n\n        log_prefix = 'train' if self.training else 'val'\n\n        loss_dict.update({f'{log_prefix}/loss_simple': loss.mean()})\n        loss_simple = loss.mean() * self.l_simple_weight\n\n        loss_vlb = (self.lvlb_weights[t] * loss).mean()\n        loss_dict.update({f'{log_prefix}/loss_vlb': loss_vlb})\n\n        loss = loss_simple + self.original_elbo_weight * loss_vlb\n\n        loss_dict.update({f'{log_prefix}/loss': loss})\n\n        return loss, loss_dict\n\n    def forward(self, x, *args, **kwargs):\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n        return self.p_losses(x, t, *args, **kwargs)\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        x = x.to(memory_format=torch.contiguous_format).float()\n        return x\n\n    def shared_step(self, batch):\n        x = self.get_input(batch, self.first_stage_key)\n        loss, loss_dict = self(x)\n        return loss, loss_dict\n\n    def training_step(self, batch, batch_idx):\n        loss, loss_dict = self.shared_step(batch)\n\n        self.log_dict(loss_dict, prog_bar=True,\n                      logger=True, on_step=True, on_epoch=True)\n\n        self.log(\"global_step\", self.global_step,\n                 prog_bar=True, logger=True, on_step=True, on_epoch=False)\n\n        if self.use_scheduler:\n            lr = self.optimizers().param_groups[0]['lr']\n            self.log('lr_abs', lr, prog_bar=True, logger=True, on_step=True, on_epoch=False)\n        \n        if self.log_time:\n            total_train_time = (time.time() - self.start_time) / (3600*24)\n            avg_step_time = (time.time() - self.start_time) / (self.global_step + 1)\n            left_time_2w_step = (20000-self.global_step -1) * avg_step_time / (3600*24)\n            left_time_5w_step = (50000-self.global_step -1) * avg_step_time / (3600*24)\n            with open(self.logger_path, 'w') as f:\n                print(f'total_train_time = {total_train_time:.1f} days \\n\\\n                      total_train_step = {self.global_step + 1} steps \\n\\\n                      left_time_2w_step = {left_time_2w_step:.1f} days \\n\\\n                      left_time_5w_step = {left_time_5w_step:.1f} days', file=f)\n        return loss\n\n    @torch.no_grad()\n    def validation_step(self, batch, batch_idx):\n        # _, loss_dict_no_ema = self.shared_step_validate(batch)\n        # with self.ema_scope():\n        #     _, loss_dict_ema = self.shared_step_validate(batch)\n        #     loss_dict_ema = {key + '_ema': loss_dict_ema[key] for key in loss_dict_ema}\n        # self.log_dict(loss_dict_no_ema, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n        # self.log_dict(loss_dict_ema, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n        if (self.global_step) % self.val_fvd_interval == 0 and self.global_step != 0:\n            print(f'sample for fvd...')\n            self.log_images_kwargs = {\n                'inpaint': False,\n                'plot_diffusion_rows': False,\n                'plot_progressive_rows': False,\n                'ddim_steps': 50,\n                'unconditional_guidance_scale': 15.0,\n            }\n            torch.cuda.empty_cache()\n            logs = self.log_images(batch, **self.log_images_kwargs)\n            self.log(\"batch_idx\", batch_idx,\n                    prog_bar=True, on_step=True, on_epoch=False)\n            return {'real': logs['inputs'], 'fake': logs['samples'], 'conditioning_txt_img': logs['conditioning_txt_img']}\n    \n    def get_condition_validate(self, prompt):\n        \"\"\" text embd\n        \"\"\"\n        if isinstance(prompt, str):\n            prompt = [prompt]\n        c = self.get_learned_conditioning(prompt)\n        bs = c.shape[0]\n        \n        return c\n    \n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self.model)\n        \n    def training_epoch_end(self, outputs):\n        \n        if (self.current_epoch == 0) or self.resume_new_epoch == 0:\n            self.epoch_start_time = time.time()\n            self.current_epoch_time = 0\n            self.total_time = 0\n            self.epoch_time_avg = 0\n        else:\n            self.current_epoch_time = time.time() - self.epoch_start_time\n            self.epoch_start_time = time.time()\n            self.total_time += self.current_epoch_time\n            self.epoch_time_avg = self.total_time / self.current_epoch\n        self.resume_new_epoch += 1\n        epoch_avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        \n        self.log('train/epoch/loss', epoch_avg_loss, logger=True, on_epoch=True)\n        self.log('train/epoch/idx', self.current_epoch, logger=True, on_epoch=True)\n        self.log('train/epoch/time', self.current_epoch_time, logger=True, on_epoch=True)\n        self.log('train/epoch/time_avg', self.epoch_time_avg, logger=True, on_epoch=True)\n        self.log('train/epoch/time_avg_min', self.epoch_time_avg / 60, logger=True, on_epoch=True)\n\n    def _get_rows_from_list(self, samples):\n        n_imgs_per_row = len(samples)\n        denoise_grid = rearrange(samples, 'n b c t h w -> b n c t h w')\n        denoise_grid = rearrange(denoise_grid, 'b n c t h w -> (b n) c t h w')\n        denoise_grid = rearrange(denoise_grid, 'n c t h w -> (n t) c h w')\n        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n        return denoise_grid\n\n    @torch.no_grad()\n    def log_images(self, batch, N=8, n_row=2, sample=True, return_keys=None, \n                   plot_diffusion_rows=True, plot_denoise_rows=True, **kwargs):\n        \"\"\" log images for DDPM \"\"\"\n        log = dict()\n        x = self.get_input(batch, self.first_stage_key)\n        N = min(x.shape[0], N)\n        n_row = min(x.shape[0], n_row)\n        x = x.to(self.device)[:N]\n        log[\"inputs\"] = x\n        if 'fps' in batch:\n            log['fps'] = batch['fps']\n\n        if plot_diffusion_rows:\n            # get diffusion row\n            diffusion_row = list()\n            x_start = x[:n_row]\n\n            for t in range(self.num_timesteps):\n                if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n                    t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n                    t = t.to(self.device).long()\n                    noise = torch.randn_like(x_start)\n                    x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n                    diffusion_row.append(x_noisy)\n\n            log[\"diffusion_row\"] = self._get_rows_from_list(diffusion_row)\n\n        if sample:\n            # get denoise row\n            with self.ema_scope(\"Plotting\"):\n                samples, denoise_row = self.sample(batch_size=N, return_intermediates=True)\n\n            log[\"samples\"] = samples\n            if plot_denoise_rows:\n                log[\"denoise_row\"] = self._get_rows_from_list(denoise_row)\n\n        if return_keys:\n            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n                return log\n            else:\n                return {key: log[key] for key in return_keys}\n        return log\n\n    def configure_optimizers(self):\n        lr = self.learning_rate\n        params = list(self.model.parameters())\n        if self.learn_logvar:\n            params = params + [self.logvar]\n        opt = torch.optim.AdamW(params, lr=lr)\n        return opt", "\n\nclass LatentDiffusion(DDPM):\n    \"\"\"main class\"\"\"\n    def __init__(self,\n                 first_stage_config,\n                 cond_stage_config,\n                 num_timesteps_cond=None,\n                 cond_stage_key=\"image\",\n                 cond_stage_trainable=False,\n                 concat_mode=True,\n                 cond_stage_forward=None,\n                 conditioning_key=None,\n                 scale_factor=1.0,\n                 scale_by_std=False,\n                 encoder_type=\"2d\",\n                 shift_factor=0.0,\n                 split_clips=True,\n                 downfactor_t=None,\n                 clip_length=None,\n                 only_model=False,\n                 lora_args={},\n                 *args, **kwargs):\n        self.num_timesteps_cond = default(num_timesteps_cond, 1)\n        self.scale_by_std = scale_by_std\n        assert self.num_timesteps_cond <= kwargs['timesteps']\n        # for backwards compatibility after implementation of DiffusionWrapper\n        \n        if conditioning_key is None:\n            conditioning_key = 'concat' if concat_mode else 'crossattn'\n        if cond_stage_config == '__is_unconditional__':\n            conditioning_key = None\n        ckpt_path = kwargs.pop(\"ckpt_path\", None)\n        ignore_keys = kwargs.pop(\"ignore_keys\", [])\n        super().__init__(conditioning_key=conditioning_key, *args, **kwargs)\n        self.concat_mode = concat_mode\n        self.cond_stage_trainable = cond_stage_trainable\n        self.cond_stage_key = cond_stage_key\n        try:\n            self.num_downs = len(first_stage_config.params.ddconfig.ch_mult) - 1\n        except:\n            self.num_downs = 0\n        if not scale_by_std:\n            self.scale_factor = scale_factor\n        else:\n            self.register_buffer('scale_factor', torch.tensor(scale_factor))\n        self.instantiate_first_stage(first_stage_config)\n        self.instantiate_cond_stage(cond_stage_config)\n        self.cond_stage_forward = cond_stage_forward\n        self.clip_denoised = False\n        self.bbox_tokenizer = None  \n        self.cond_stage_config = cond_stage_config\n        self.first_stage_config = first_stage_config\n        self.encoder_type = encoder_type\n        assert(encoder_type in [\"2d\", \"3d\"])\n        self.restarted_from_ckpt = False\n        self.shift_factor = shift_factor\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys, only_model=only_model)\n            self.restarted_from_ckpt = True\n        self.split_clips = split_clips\n        self.downfactor_t = downfactor_t\n        self.clip_length = clip_length\n        # lora related args\n        self.inject_unet = getattr(lora_args, \"inject_unet\", False)\n        self.inject_clip = getattr(lora_args, \"inject_clip\", False)\n        self.inject_unet_key_word = getattr(lora_args, \"inject_unet_key_word\", None)\n        self.inject_clip_key_word = getattr(lora_args, \"inject_clip_key_word\", None)\n        self.lora_rank = getattr(lora_args, \"lora_rank\", 4)\n\n    def make_cond_schedule(self, ):\n        self.cond_ids = torch.full(size=(self.num_timesteps,), fill_value=self.num_timesteps - 1, dtype=torch.long)\n        ids = torch.round(torch.linspace(0, self.num_timesteps - 1, self.num_timesteps_cond)).long()\n        self.cond_ids[:self.num_timesteps_cond] = ids\n\n    def inject_lora(self, lora_scale=1.0):\n        if self.inject_unet:\n            self.lora_require_grad_params, self.lora_names = inject_trainable_lora(self.model, self.inject_unet_key_word, \n                                                                                   r=self.lora_rank,\n                                                                                   scale=lora_scale\n                                                                                   )\n        if self.inject_clip:\n            self.lora_require_grad_params_clip, self.lora_names_clip = inject_trainable_lora(self.cond_stage_model, self.inject_clip_key_word, \n                                                                                             r=self.lora_rank,\n                                                                                             scale=lora_scale\n                                                                                             )\n\n    @rank_zero_only\n    @torch.no_grad()\n    def on_train_batch_start(self, batch, batch_idx, dataloader_idx=None):\n        # only for very first batch, reset the self.scale_factor\n        if self.scale_by_std and self.current_epoch == 0 and self.global_step == 0 and batch_idx == 0 and not self.restarted_from_ckpt:\n            assert self.scale_factor == 1., 'rather not use custom rescaling and std-rescaling simultaneously'\n            # set rescale weight to 1./std of encodings\n            print(\"### USING STD-RESCALING ###\")\n            x = super().get_input(batch, self.first_stage_key)\n            x = x.to(self.device)\n            encoder_posterior = self.encode_first_stage(x)\n            z = self.get_first_stage_encoding(encoder_posterior).detach()\n            del self.scale_factor\n            self.register_buffer('scale_factor', 1. / z.flatten().std())\n            print(f\"setting self.scale_factor to {self.scale_factor}\")\n            print(\"### USING STD-RESCALING ###\")\n            print(f\"std={z.flatten().std()}\")\n\n    def register_schedule(self,\n                          given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        super().register_schedule(given_betas, beta_schedule, timesteps, linear_start, linear_end, cosine_s)\n\n        self.shorten_cond_schedule = self.num_timesteps_cond > 1\n        if self.shorten_cond_schedule:\n            self.make_cond_schedule()\n\n    def instantiate_first_stage(self, config):\n        model = instantiate_from_config(config)\n        self.first_stage_model = model.eval()\n        self.first_stage_model.train = disabled_train\n        for param in self.first_stage_model.parameters():\n            param.requires_grad = False\n\n    def instantiate_cond_stage(self, config):\n        if config is None:\n            self.cond_stage_model = None\n            return\n        if not self.cond_stage_trainable:\n            if config == \"__is_first_stage__\":\n                print(\"Using first stage also as cond stage.\")\n                self.cond_stage_model = self.first_stage_model\n            elif config == \"__is_unconditional__\":\n                print(f\"Training {self.__class__.__name__} as an unconditional model.\")\n                self.cond_stage_model = None\n            else:\n                model = instantiate_from_config(config)\n                self.cond_stage_model = model.eval()\n                self.cond_stage_model.train = disabled_train\n                for param in self.cond_stage_model.parameters():\n                    param.requires_grad = False\n        else:\n            assert config != '__is_first_stage__'\n            assert config != '__is_unconditional__'\n            model = instantiate_from_config(config)\n            self.cond_stage_model = model\n\n\n    def get_first_stage_encoding(self, encoder_posterior, noise=None):\n        if isinstance(encoder_posterior, DiagonalGaussianDistribution):\n            z = encoder_posterior.sample(noise=noise)\n        elif isinstance(encoder_posterior, torch.Tensor):\n            z = encoder_posterior\n        else:\n            raise NotImplementedError(f\"encoder_posterior of type '{type(encoder_posterior)}' not yet implemented\")\n        z = self.scale_factor * (z + self.shift_factor)\n        return z\n\n\n    def get_learned_conditioning(self, c):\n        if self.cond_stage_forward is None:\n            if hasattr(self.cond_stage_model, 'encode') and callable(self.cond_stage_model.encode):\n                c = self.cond_stage_model.encode(c)\n                if isinstance(c, DiagonalGaussianDistribution):\n                    c = c.mode()\n            else:\n                c = self.cond_stage_model(c)\n        else:\n            assert hasattr(self.cond_stage_model, self.cond_stage_forward)\n            c = getattr(self.cond_stage_model, self.cond_stage_forward)(c)\n        return c\n\n\n    @torch.no_grad()\n    def get_condition(self, batch, x, bs, force_c_encode, k, cond_key, is_imgs=False):\n        is_conditional = self.model.conditioning_key is not None # crossattn\n        if is_conditional:\n            if cond_key is None:\n                cond_key = self.cond_stage_key \n            \n            # get condition batch of different condition type\n            if cond_key != self.first_stage_key:\n                assert(cond_key in [\"caption\", \"txt\"])\n                xc = batch[cond_key]\n            else:\n                xc = x\n            \n            # if static video\n            if self.static_video:\n                xc_ = [c + ' (static)' for c in xc]\n                xc = xc_\n            \n            # get learned condition.\n            # can directly skip it: c = xc\n            if self.cond_stage_config is not None and (not self.cond_stage_trainable or force_c_encode):\n                if isinstance(xc, torch.Tensor):\n                    xc = xc.to(self.device)\n                c = self.get_learned_conditioning(xc)\n            else:\n                c = xc\n\n            if self.classfier_free_guidance:\n                if cond_key in ['caption', \"txt\"] and self.uncond_type == 'empty_seq':\n                    for i, ci in enumerate(c):\n                        if random.random() < self.prob:\n                            c[i] = \"\"\n                elif cond_key == 'class_label' and self.uncond_type == 'zero_embed':\n                    pass\n                elif cond_key == 'class_label' and self.uncond_type == 'learned_embed':\n                    import pdb;pdb.set_trace()\n                    for i, ci in enumerate(c):\n                        if random.random() < self.prob:\n                            c[i]['class_label'] = self.n_classes\n                    \n                else:\n                    raise NotImplementedError\n                \n            if self.zero_cond_embed:\n                import pdb;pdb.set_trace()\n                c = torch.zeros_like(c)\n\n            # process c\n            if bs is not None:\n                if (is_imgs and not self.static_video):\n                    c = c[:bs*self.temporal_length] # each random img (in T axis) has a corresponding prompt\n                else:\n                    c = c[:bs]\n\n        else:\n            c = None\n            xc = None\n            \n        return c, xc\n\n    @torch.no_grad()\n    def get_input(self, batch, k, return_first_stage_outputs=False, force_c_encode=False,\n                  cond_key=None, return_original_cond=False, bs=None, mask_temporal=False):\n        \"\"\" Get input in LDM \n        \"\"\"\n        # get input imgaes\n        x = super().get_input(batch, k) # k = first_stage_key=image\n        is_imgs = True if k == 'jpg' else False\n        if is_imgs:\n            if self.static_video:\n                # repeat single img to a static video\n                x = x.unsqueeze(2) # bchw -> bc1hw\n                x = x.repeat(1,1,self.temporal_length,1,1) # bc1hw -> bcthw\n            else:\n                # rearrange to videos with T random img\n                bs_load = x.shape[0] // self.temporal_length\n                x = x[:bs_load*self.temporal_length, ...]\n                x = rearrange(x, '(b t) c h w -> b c t h w', t=self.temporal_length, b=bs_load)\n\n        if bs is not None:\n            x = x[:bs]\n        \n        x = x.to(self.device)\n        x_ori = x\n        \n        b, _, t, h, w = x.shape\n        \n        # encode video frames x to z via a 2D encoder\n        x = rearrange(x, 'b c t h w -> (b t) c h w')\n        encoder_posterior = self.encode_first_stage(x, mask_temporal)\n        z = self.get_first_stage_encoding(encoder_posterior).detach()\n        z = rearrange(z, '(b t) c h w -> b c t h w', b=b, t=t)\n        \n        \n        c, xc = self.get_condition(batch, x, bs, force_c_encode, k, cond_key, is_imgs)\n        out = [z, c]\n        \n        if return_first_stage_outputs:\n            xrec = self.decode_first_stage(z, mask_temporal=mask_temporal)\n            out.extend([x_ori, xrec])\n        if return_original_cond:\n            if isinstance(xc, torch.Tensor) and xc.dim() == 4:\n                xc = rearrange(xc, '(b t) c h w -> b c t h w', b=b, t=t)\n            out.append(xc)\n        \n        return out\n    \n    @torch.no_grad()\n    def decode(self, z, **kwargs,):\n        z = 1. / self.scale_factor * z - self.shift_factor\n        results = self.first_stage_model.decode(z,**kwargs)\n        return results\n    \n    @torch.no_grad()\n    def decode_first_stage_2DAE(self, z, decode_bs=16, return_cpu=True, **kwargs):\n        b, _, t, _, _ = z.shape\n        z = rearrange(z, 'b c t h w -> (b t) c h w')\n        if decode_bs is None:\n            results = self.decode(z, **kwargs)\n        else:\n            z = torch.split(z, decode_bs, dim=0)\n            if return_cpu:\n                results = torch.cat([self.decode(z_, **kwargs).cpu() for z_ in z], dim=0)\n            else:\n                results = torch.cat([self.decode(z_, **kwargs) for z_ in z], dim=0)\n        results = rearrange(results, '(b t) c h w -> b c t h w', b=b,t=t).contiguous()\n        return results\n\n    @torch.no_grad()\n    def decode_first_stage(self, z, decode_bs=16, return_cpu=True, **kwargs):\n        assert(self.encoder_type == \"2d\" and z.dim() == 5)\n        return self.decode_first_stage_2DAE(z, decode_bs=decode_bs, return_cpu=return_cpu, **kwargs)\n\n    @torch.no_grad()\n    def encode_first_stage_2DAE(self, x, encode_bs=16):\n        b, _, t, _, _ = x.shape\n        x = rearrange(x, 'b c t h w -> (b t) c h w')\n        if encode_bs is None:\n            results = self.first_stage_model.encode(x)\n        else:\n            x = torch.split(x, encode_bs, dim=0)\n            zs = []\n            for x_ in x:\n                encoder_posterior = self.first_stage_model.encode(x_)\n                z = self.get_first_stage_encoding(encoder_posterior).detach()\n                zs.append(z)\n            results = torch.cat(zs, dim=0)\n        results = rearrange(results, '(b t) c h w -> b c t h w', b=b,t=t)\n        return results\n    \n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        assert(self.encoder_type == \"2d\" and x.dim() == 5)\n        b, _, t, _, _ = x.shape\n        x = rearrange(x, 'b c t h w -> (b t) c h w')\n        results = self.first_stage_model.encode(x)\n        results = rearrange(results, '(b t) c h w -> b c t h w', b=b,t=t)\n        return results\n\n    def shared_step(self, batch, **kwargs):\n        \"\"\" shared step of LDM.\n        If learned condition, c is raw condition (e.g. text)\n        Encoding condition is performed in below forward function.\n        \"\"\"\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n    \n    def forward(self, x, c, *args, **kwargs):\n        start_t = getattr(self, \"start_t\", 0)\n        end_t = getattr(self, \"end_t\", self.num_timesteps)\n        t = torch.randint(start_t, end_t, (x.shape[0],), device=self.device).long()\n        \n        if self.model.conditioning_key is not None:\n            assert c is not None\n            if self.cond_stage_trainable:\n                c = self.get_learned_conditioning(c)\n            if self.classfier_free_guidance and self.uncond_type == 'zero_embed':\n                for i, ci in enumerate(c):\n                    if random.random() < self.prob:\n                        c[i] = torch.zeros_like(c[i])\n            if self.shorten_cond_schedule:  # TODO: drop this option\n                tc = self.cond_ids[t].to(self.device)\n                c = self.q_sample(x_start=c, t=tc, noise=torch.randn_like(c.float()))\n        \n        return self.p_losses(x, c, t, *args, **kwargs)\n\n    def apply_model(self, x_noisy, t, cond, return_ids=False, **kwargs):\n\n        if isinstance(cond, dict):\n            # hybrid case, cond is exptected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        x_recon = self.model(x_noisy, t, **cond, **kwargs)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n    def _prior_bpd(self, x_start):\n        \"\"\"\n        Get the prior KL term for the variational lower-bound, measured in\n        bits-per-dim.\n        This term can't be optimized, as it only depends on the encoder.\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :return: a batch of [N] KL values (in bits), one per batch element.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n        return mean_flat(kl_prior) / np.log(2.0)\n\n    def p_losses(self, x_start, cond, t, noise=None, skip_qsample=False, x_noisy=None, cond_mask=None, **kwargs,):\n        if not skip_qsample:\n            noise = default(noise, lambda: torch.randn_like(x_start))\n            x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n        else:\n            assert(x_noisy is not None)\n            assert(noise is not None)\n        model_output = self.apply_model(x_noisy, t, cond, **kwargs)\n\n        loss_dict = {}\n        prefix = 'train' if self.training else 'val'\n\n        if self.parameterization == \"x0\":\n            target = x_start\n        elif self.parameterization == \"eps\":\n            target = noise\n        else:\n            raise NotImplementedError()\n        \n        loss_simple = self.get_loss(model_output, target, mean=False).mean([1, 2, 3, 4])\n        loss_dict.update({f'{prefix}/loss_simple': loss_simple.mean()})\n        if self.logvar.device != self.device:\n            self.logvar = self.logvar.to(self.device)\n        logvar_t = self.logvar[t]\n        loss = loss_simple / torch.exp(logvar_t) + logvar_t\n        if self.learn_logvar:\n            loss_dict.update({f'{prefix}/loss_gamma': loss.mean()})\n            loss_dict.update({'logvar': self.logvar.data.mean()})\n\n        loss = self.l_simple_weight * loss.mean()\n\n        loss_vlb = self.get_loss(model_output, target, mean=False).mean(dim=(1, 2, 3, 4))\n        loss_vlb = (self.lvlb_weights[t] * loss_vlb).mean()\n        loss_dict.update({f'{prefix}/loss_vlb': loss_vlb})\n        loss += (self.original_elbo_weight * loss_vlb)\n        loss_dict.update({f'{prefix}/loss': loss})\n\n        return loss, loss_dict\n\n    def p_mean_variance(self, x, c, t, clip_denoised: bool, return_codebook_ids=False, quantize_denoised=False,\n                        return_x0=False, score_corrector=None, corrector_kwargs=None, \n                        unconditional_guidance_scale=1., unconditional_conditioning=None,\n                        uc_type=None,):\n        t_in = t\n        if unconditional_conditioning is None or unconditional_guidance_scale == 1.:\n            model_out = self.apply_model(x, t_in, c, return_ids=return_codebook_ids)\n        else:\n            # with unconditional condition\n            if isinstance(c, torch.Tensor):\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t] * 2)\n                c_in = torch.cat([unconditional_conditioning, c])\n                model_out_uncond, model_out = self.apply_model(x_in, t_in, c_in, return_ids=return_codebook_ids).chunk(2)\n            elif isinstance(c, dict):\n                model_out = self.apply_model(x, t, c, return_ids=return_codebook_ids)\n                model_out_uncond = self.apply_model(x, t, unconditional_conditioning, return_ids=return_codebook_ids)\n            else:\n                raise NotImplementedError\n            if uc_type is None:\n                model_out = model_out_uncond + unconditional_guidance_scale * (model_out - model_out_uncond)\n            else:\n                if uc_type == 'cfg_original':\n                    model_out = model_out + unconditional_guidance_scale * (model_out - model_out_uncond)\n                elif uc_type == 'cfg_ours':\n                    model_out = model_out + unconditional_guidance_scale * (model_out_uncond - model_out)\n                else:\n                    raise NotImplementedError\n\n        if score_corrector is not None:\n            assert self.parameterization == \"eps\"\n            model_out = score_corrector.modify_score(self, model_out, x, t, c, **corrector_kwargs)\n\n        if return_codebook_ids:\n            model_out, logits = model_out\n\n        if self.parameterization == \"eps\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \"x0\":\n            x_recon = model_out\n        else:\n            raise NotImplementedError()\n\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n        if quantize_denoised:\n            x_recon, _, [_, _, indices] = self.first_stage_model.quantize(x_recon)\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        if return_codebook_ids:\n            return model_mean, posterior_variance, posterior_log_variance, logits\n        elif return_x0:\n            return model_mean, posterior_variance, posterior_log_variance, x_recon\n        else:\n            return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, c, t, clip_denoised=False, repeat_noise=False,\n                 return_codebook_ids=False, quantize_denoised=False, return_x0=False,\n                 temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n                 unconditional_guidance_scale=1., unconditional_conditioning=None,\n                 uc_type=None,):\n        b, *_, device = *x.shape, x.device\n        outputs = self.p_mean_variance(x=x, c=c, t=t, clip_denoised=clip_denoised,\n                                       return_codebook_ids=return_codebook_ids,\n                                       quantize_denoised=quantize_denoised,\n                                       return_x0=return_x0,\n                                       score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n                                       unconditional_guidance_scale=unconditional_guidance_scale, \n                                       unconditional_conditioning=unconditional_conditioning,\n                                       uc_type=uc_type,)\n        if return_codebook_ids:\n            raise DeprecationWarning(\"Support dropped.\")\n        elif return_x0:\n            model_mean, _, model_log_variance, x0 = outputs\n        else:\n            model_mean, _, model_log_variance = outputs\n\n        noise = noise_like(x.shape, device, repeat_noise) * temperature\n        if noise_dropout > 0.:\n            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n        \n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n\n        if return_codebook_ids:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, logits.argmax(dim=1)\n        if return_x0:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, x0\n        else:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def progressive_denoising(self, cond, shape, verbose=True, callback=None, quantize_denoised=False,\n                              img_callback=None, mask=None, x0=None, temperature=1., noise_dropout=0.,\n                              score_corrector=None, corrector_kwargs=None, batch_size=None, x_T=None, start_T=None,\n                              log_every_t=None):\n        if not log_every_t:\n            log_every_t = self.log_every_t\n        timesteps = self.num_timesteps\n        if batch_size is not None:\n            b = batch_size if batch_size is not None else shape[0]\n            shape = [batch_size] + list(shape)\n        else:\n            b = batch_size = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=self.device)\n        else:\n            img = x_T\n        intermediates = []\n        if cond is not None:\n            if isinstance(cond, dict):\n                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n                list(map(lambda x: x[:batch_size], cond[key])) for key in cond}\n            else:\n                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n\n        if start_T is not None:\n            timesteps = min(timesteps, start_T)\n        iterator = tqdm(reversed(range(0, timesteps)), desc='Progressive Generation',\n                        total=timesteps) if verbose else reversed(\n            range(0, timesteps))\n        if type(temperature) == float:\n            temperature = [temperature] * timesteps\n\n        for i in iterator:\n            ts = torch.full((b,), i, device=self.device, dtype=torch.long)\n            if self.shorten_cond_schedule:\n                assert self.model.conditioning_key != 'hybrid'\n                tc = self.cond_ids[ts].to(cond.device)\n                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n\n            img, x0_partial = self.p_sample(img, cond, ts,\n                                            clip_denoised=self.clip_denoised,\n                                            quantize_denoised=quantize_denoised, return_x0=True,\n                                            temperature=temperature[i], noise_dropout=noise_dropout,\n                                            score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n            if mask is not None:\n                assert x0 is not None\n                img_orig = self.q_sample(x0, ts)\n                img = img_orig * mask + (1. - mask) * img\n\n            if i % log_every_t == 0 or i == timesteps - 1:\n                intermediates.append(x0_partial)\n            if callback: callback(i)\n            if img_callback: img_callback(img, i)\n        return img, intermediates\n\n    @torch.no_grad()\n    def p_sample_loop(self, cond, shape, return_intermediates=False,\n                      x_T=None, verbose=True, callback=None, timesteps=None, quantize_denoised=False,\n                      mask=None, x0=None, img_callback=None, start_T=None,\n                      log_every_t=None,\n                      unconditional_guidance_scale=1., unconditional_conditioning=None,\n                      uc_type=None,):\n\n        if not log_every_t:\n            log_every_t = self.log_every_t\n        device = self.betas.device\n        b = shape[0]\n        \n        # sample an initial noise\n        if x_T is None:\n            img = torch.randn(shape, device=device)\n        else:\n            img = x_T\n        \n        intermediates = [img]\n        if timesteps is None:\n            timesteps = self.num_timesteps\n\n        if start_T is not None:\n            timesteps = min(timesteps, start_T)\n        iterator = tqdm(reversed(range(0, timesteps)), desc='Sampling t', total=timesteps) if verbose else reversed(\n            range(0, timesteps))\n\n        if mask is not None:\n            assert x0 is not None\n            assert x0.shape[2:3] == mask.shape[2:3]  # spatial size has to match\n\n        for i in iterator:\n            ts = torch.full((b,), i, device=device, dtype=torch.long)\n            if self.shorten_cond_schedule:\n                assert self.model.conditioning_key != 'hybrid'\n                tc = self.cond_ids[ts].to(cond.device)\n                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n\n            img = self.p_sample(img, cond, ts,\n                                clip_denoised=self.clip_denoised,\n                                quantize_denoised=quantize_denoised,\n                                unconditional_guidance_scale=unconditional_guidance_scale,\n                                unconditional_conditioning=unconditional_conditioning,\n                                uc_type=uc_type)\n            if mask is not None:\n                img_orig = self.q_sample(x0, ts)\n                img = img_orig * mask + (1. - mask) * img\n\n            if i % log_every_t == 0 or i == timesteps - 1:\n                intermediates.append(img)\n            if callback: callback(i)\n            if img_callback: img_callback(img, i)\n\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, cond, batch_size=16, return_intermediates=False, x_T=None,\n               verbose=True, timesteps=None, quantize_denoised=False,\n               mask=None, x0=None, shape=None, **kwargs):\n        if shape is None:\n            shape = (batch_size, self.channels, self.total_length, *self.image_size)\n        if cond is not None:\n            if isinstance(cond, dict):\n                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n                list(map(lambda x: x[:batch_size], cond[key])) for key in cond}\n            else:\n                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n        return self.p_sample_loop(cond,\n                                  shape,\n                                  return_intermediates=return_intermediates, x_T=x_T,\n                                  verbose=verbose, timesteps=timesteps, quantize_denoised=quantize_denoised,\n                                  mask=mask, x0=x0,)\n\n    @torch.no_grad()\n    def sample_log(self,cond,batch_size,ddim, ddim_steps,**kwargs):\n\n        if ddim:\n            ddim_sampler = DDIMSampler(self)\n            shape = (self.channels, self.total_length, *self.image_size)\n            samples, intermediates =ddim_sampler.sample(ddim_steps,batch_size,\n                                                        shape,cond,verbose=False, **kwargs)\n\n        else:\n            samples, intermediates = self.sample(cond=cond, batch_size=batch_size,\n                                                 return_intermediates=True, **kwargs)\n\n        return samples, intermediates\n    \n    @torch.no_grad()\n    def log_condition(self, log, batch, xc, x, c, cond_stage_key=None):\n        \"\"\" \n        xc: oringinal condition before enconding. \n        c: condition after encoding.\n        \"\"\"\n        if x.dim() == 5:\n            txt_img_shape = [x.shape[3], x.shape[4]]\n        elif x.dim() == 4:\n            txt_img_shape = [x.shape[2], x.shape[3]]\n        else:\n            raise ValueError\n        if self.model.conditioning_key is not None: #concat-time-mask\n            if hasattr(self.cond_stage_model, \"decode\"):\n                xc = self.cond_stage_model.decode(c)\n                log[\"conditioning\"] = xc\n            elif cond_stage_key in [\"caption\", \"txt\"]:\n                log[\"conditioning_txt_img\"] = log_txt_as_img(txt_img_shape, batch[cond_stage_key], size=x.shape[3]//25)\n                log[\"conditioning_txt\"] = batch[cond_stage_key]\n            elif cond_stage_key == 'class_label':\n                try:\n                    xc = log_txt_as_img(txt_img_shape, batch[\"human_label\"], size=x.shape[3]//25)\n                except:\n                    xc = log_txt_as_img(txt_img_shape, batch[\"class_name\"], size=x.shape[3]//25)\n                log['conditioning'] = xc\n            elif isimage(xc):\n                log[\"conditioning\"] = xc\n            if ismap(xc):\n                log[\"original_conditioning\"] = self.to_rgb(xc)\n            if isinstance(c, dict) and 'mask' in c:\n                log['mask'] =self.mask_to_rgb(c['mask'])\n        return log\n    \n    @torch.no_grad()\n    def log_images(self, batch, N=8, n_row=4, sample=True, ddim_steps=200, ddim_eta=1., unconditional_guidance_scale=1.0, \n                   first_stage_key2=None, cond_key2=None,\n                   c=None, \n                   **kwargs):\n        \"\"\" log images for LatentDiffusion \"\"\"\n        use_ddim = ddim_steps is not None\n        is_imgs = first_stage_key2 is not None\n        if is_imgs:\n            assert(cond_key2 is not None)\n        log = dict()\n\n        # get input\n        z, c, x, xrec, xc = self.get_input(batch, \n                                           k=self.first_stage_key if first_stage_key2 is None else first_stage_key2,\n                                           return_first_stage_outputs=True,\n                                           force_c_encode=True,\n                                           return_original_cond=True,\n                                           bs=N,\n                                           cond_key=cond_key2 if cond_key2 is not None else None,\n                                           )\n        \n        N_ori = N\n        N = min(z.shape[0], N)\n        n_row = min(x.shape[0], n_row)\n\n        if unconditional_guidance_scale != 1.0:\n            prompts = N * self.temporal_length * [\"\"] if (is_imgs and not self.static_video) else N * [\"\"]\n            uc = self.get_condition_validate(prompts)\n            \n        else:\n            uc = None\n\n        log[\"inputs\"] = x\n        log[\"reconstruction\"] = xrec\n        log = self.log_condition(log, batch, xc, x, c, \n                                 cond_stage_key=self.cond_stage_key if cond_key2 is None else cond_key2\n        )\n        \n        if sample:\n            with self.ema_scope(\"Plotting\"):\n                samples, z_denoise_row = self.sample_log(cond=c,batch_size=N,ddim=use_ddim,\n                                                         ddim_steps=ddim_steps,eta=ddim_eta,\n                                                         temporal_length=self.video_length,\n                                                         unconditional_guidance_scale=unconditional_guidance_scale,\n                                                         unconditional_conditioning=uc, **kwargs,\n                                                         )\n            # decode samples\n            x_samples = self.decode_first_stage(samples)\n            log[\"samples\"] = x_samples\n        return log\n\n    def configure_optimizers(self):\n        \"\"\" configure_optimizers for LatentDiffusion \"\"\"\n        lr = self.learning_rate\n        \n        # --------------------------------------------------------------------------------\n        # set parameters\n        if hasattr(self, \"only_optimize_empty_parameters\") and self.only_optimize_empty_parameters:\n            print(\"[INFO] Optimize only empty parameters!\")\n            assert(hasattr(self, \"empty_paras\"))\n            params = [p for n, p in self.model.named_parameters() if n in self.empty_paras]\n        elif hasattr(self, \"only_optimize_pretrained_parameters\") and self.only_optimize_pretrained_parameters:\n            print(\"[INFO] Optimize only pretrained parameters!\")\n            assert(hasattr(self, \"empty_paras\"))\n            params = [p for n, p in self.model.named_parameters() if n not in self.empty_paras]\n            assert(len(params) != 0)\n        elif getattr(self, \"optimize_empty_and_spatialattn\", False):\n            print(\"[INFO] Optimize empty parameters + spatial transformer!\")\n            assert(hasattr(self, \"empty_paras\"))\n            empty_paras = [p for n, p in self.model.named_parameters() if n in self.empty_paras]\n            SA_list = [\".attn1.\", \".attn2.\", \".ff.\", \".norm1.\", \".norm2.\", \".norm3.\"]\n            SA_params = [p for n, p in self.model.named_parameters() if check_istarget(n, SA_list)]\n            if getattr(self, \"spatial_lr_decay\", False):\n                params = [\n                    {\"params\": empty_paras},\n                    {\"params\": SA_params, \"lr\": lr * self.spatial_lr_decay}\n                ]\n            else:\n                params = empty_paras + SA_params\n        else:\n            # optimize whole denoiser\n            if hasattr(self, \"spatial_lr_decay\") and self.spatial_lr_decay:\n                print(\"[INFO] Optimize the whole net with different lr!\")\n                print(f\"[INFO] {lr} for empty paras, {lr * self.spatial_lr_decay} for pretrained paras!\")\n                empty_paras = [p for n, p in self.model.named_parameters() if n in self.empty_paras]\n                # assert(len(empty_paras) == len(self.empty_paras)) # self.empty_paras:cond_stage_model.embedding.weight not in diffusion model params\n                pretrained_paras = [p for n, p in self.model.named_parameters() if n not in self.empty_paras]\n                params = [\n                    {\"params\": empty_paras},\n                    {\"params\": pretrained_paras, \"lr\": lr * self.spatial_lr_decay}\n                ]\n                print(f\"[INFO] Empty paras: {len(empty_paras)}, Pretrained paras: {len(pretrained_paras)}\")\n\n            else:\n                params = list(self.model.parameters())\n        \n        if hasattr(self, \"generator_trainable\") and not self.generator_trainable:\n            # fix unet denoiser\n            params = list()\n\n        if self.inject_unet:\n            params = itertools.chain(*self.lora_require_grad_params)\n                \n        if self.inject_clip:\n            if self.inject_unet:\n                params = list(params)+list(itertools.chain(*self.lora_require_grad_params_clip))\n            else:\n                params = itertools.chain(*self.lora_require_grad_params_clip)\n            \n\n        # append paras\n        # ------------------------------------------------------------------\n        def add_cond_model(cond_model, params):\n            if isinstance(params[0], dict):\n                # parameter groups\n                params.append({\"params\": list(cond_model.parameters())})\n            else:\n                # parameter list: [torch.nn.parameter.Parameter]\n                params = params + list(cond_model.parameters())\n            return params\n        # ------------------------------------------------------------------\n        \n        if self.cond_stage_trainable:\n            # print(f\"{self.__class__.__name__}: Also optimizing conditioner params!\")\n            params = add_cond_model(self.cond_stage_model, params)\n        \n        if self.learn_logvar:\n            print('Diffusion model optimizing logvar')\n            if isinstance(params[0], dict):\n                params.append({\"params\": [self.logvar]})\n            else:\n                params.append(self.logvar)\n        \n        # --------------------------------------------------------------------------------\n        opt = torch.optim.AdamW(params, lr=lr)\n        \n        # lr scheduler\n        if self.use_scheduler:\n            assert 'target' in self.scheduler_config\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\"Setting up LambdaLR scheduler...\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                }]\n            return [opt], scheduler\n        \n        return opt\n    \n    @torch.no_grad()\n    def to_rgb(self, x):\n        x = x.float()\n        if not hasattr(self, \"colorize\"):\n            self.colorize = torch.randn(3, x.shape[1], 1, 1).to(x)\n        x = nn.functional.conv2d(x, weight=self.colorize)\n        x = 2. * (x - x.min()) / (x.max() - x.min()) - 1.\n        return x\n\n    @torch.no_grad()\n    def mask_to_rgb(self, x):\n        x = x * 255\n        x = x.int()\n        return x", "\nclass DiffusionWrapper(pl.LightningModule):\n    def __init__(self, diff_model_config, conditioning_key):\n        super().__init__()\n        self.diffusion_model = instantiate_from_config(diff_model_config)\n        print('Successfully initialize the diffusion model !')\n        self.conditioning_key = conditioning_key\n        # assert self.conditioning_key in [None, 'concat', 'crossattn', 'hybrid', 'adm', 'resblockcond', 'hybrid-adm', 'hybrid-time']\n\n    def forward(self, x, t, c_concat: list = None, c_crossattn: list = None,\n                c_adm=None, s=None, mask=None, **kwargs):\n        # temporal_context = fps is foNone\n        if self.conditioning_key is None:\n            out = self.diffusion_model(x, t, **kwargs)\n        elif self.conditioning_key == 'concat':\n            xc = torch.cat([x] + c_concat, dim=1)\n            out = self.diffusion_model(xc, t, **kwargs)\n        elif self.conditioning_key == 'crossattn':\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(x, t, context=cc, **kwargs)\n        elif self.conditioning_key == 'hybrid':\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc, **kwargs)\n        elif self.conditioning_key == 'resblockcond':\n            cc = c_crossattn[0]\n            out = self.diffusion_model(x, t, context=cc, **kwargs)\n        elif self.conditioning_key == 'adm':\n            cc = c_crossattn[0]\n            out = self.diffusion_model(x, t, y=cc, **kwargs)\n        elif self.conditioning_key == 'hybrid-adm':\n            assert c_adm is not None\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc, y=c_adm, **kwargs)\n        elif self.conditioning_key == 'hybrid-time':\n            assert s is not None\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc, s=s, **kwargs)\n        elif self.conditioning_key == 'concat-time-mask':\n            # assert s is not None\n            # print('x & mask:',x.shape,c_concat[0].shape)\n            xc = torch.cat([x] + c_concat, dim=1)\n            out = self.diffusion_model(xc, t, context=None, s=s, mask=mask, **kwargs)\n        elif self.conditioning_key == 'concat-adm-mask':\n            # assert s is not None\n            # print('x & mask:',x.shape,c_concat[0].shape)\n            if c_concat is not None:\n                xc = torch.cat([x] + c_concat, dim=1)\n            else:\n                xc = x\n            out = self.diffusion_model(xc, t, context=None, y=s, mask=mask, **kwargs)\n        elif self.conditioning_key == 'crossattn-adm':\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(x, t, context=cc, y=s, **kwargs)\n        elif self.conditioning_key == 'hybrid-adm-mask':\n            cc = torch.cat(c_crossattn, 1)\n            if c_concat is not None:\n                xc = torch.cat([x] + c_concat, dim=1)\n            else:\n                xc = x\n            out = self.diffusion_model(xc, t, context=cc, y=s, mask=mask, **kwargs)\n        elif self.conditioning_key == 'hybrid-time-adm': # adm means y, e.g., class index\n            # assert s is not None\n            assert c_adm is not None\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc, s=s, y=c_adm, **kwargs)\n        else:\n            raise NotImplementedError()\n\n        return out", "\n\nclass T2VAdapterDepth(LatentDiffusion):\n    def __init__(self, depth_stage_config, adapter_config, *args, **kwargs):\n        super(T2VAdapterDepth, self).__init__(*args, **kwargs)\n        self.adapter = instantiate_from_config(adapter_config)\n        self.condtype = adapter_config.cond_name\n        self.depth_stage_model = instantiate_from_config(depth_stage_config)\n\n    def prepare_midas_input(self, batch_x):\n        # input: b,c,h,w\n        x_midas = torch.nn.functional.interpolate(batch_x, size=(384, 384), mode='bicubic')\n        return x_midas\n    \n    @torch.no_grad()\n    def get_batch_depth(self, batch_x, target_size, encode_bs=1):\n        b, c, t, h, w = batch_x.shape\n        merge_x = rearrange(batch_x, 'b c t h w -> (b t) c h w')\n        split_x = torch.split(merge_x, encode_bs, dim=0)\n        cond_depth_list = []\n        for x in split_x:\n            x_midas = self.prepare_midas_input(x)\n            cond_depth = self.depth_stage_model(x_midas)\n            cond_depth = torch.nn.functional.interpolate(\n                    cond_depth,\n                    size=target_size,\n                    mode=\"bicubic\",\n                    align_corners=False,\n                )\n            depth_min, depth_max = torch.amin(cond_depth, dim=[1, 2, 3], keepdim=True), torch.amax(cond_depth, dim=[1, 2, 3], keepdim=True)\n            cond_depth = 2. * (cond_depth - depth_min) / (depth_max - depth_min + 1e-7) - 1.\n            cond_depth_list.append(cond_depth)\n        batch_cond_depth=torch.cat(cond_depth_list, dim=0)\n        batch_cond_depth = rearrange(batch_cond_depth, '(b t) c h w -> b c t h w', b=b, t=t)\n        return batch_cond_depth\n    \n    def get_adapter_features(self, extra_cond, encode_bs=1):\n        b, c, t, h, w = extra_cond.shape\n        ## process in 2D manner\n        merge_extra_cond = rearrange(extra_cond, 'b c t h w -> (b t) c h w')\n        split_extra_cond = torch.split(merge_extra_cond, encode_bs, dim=0)\n        features_adapter_list = []\n        for extra_cond in split_extra_cond:\n            features_adapter = self.adapter(extra_cond)\n            features_adapter_list.append(features_adapter)\n        merge_features_adapter_list = []\n        for i in range(len(features_adapter_list[0])):\n            merge_features_adapter = torch.cat([features_adapter_list[num][i] for num in range(len(features_adapter_list))], dim=0)\n            merge_features_adapter_list.append(merge_features_adapter)\n        merge_features_adapter_list = [rearrange(feature, '(b t) c h w -> b c t h w', b=b, t=t) for feature in merge_features_adapter_list]\n        return merge_features_adapter_list", ""]}
{"filename": "scripts/videocrafter/lvdm/models/modules/openaimodel3d.py", "chunked_list": ["from abc import abstractmethod\nimport math\nfrom einops import rearrange\nfrom functools import partial\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom omegaconf.listconfig import ListConfig\n", "from omegaconf.listconfig import ListConfig\n\nfrom videocrafter.lvdm.models.modules.util import (\n    checkpoint,\n    conv_nd,\n    linear,\n    avg_pool_nd,\n    zero_module,\n    normalization,\n    timestep_embedding,", "    normalization,\n    timestep_embedding,\n    nonlinearity,\n)\n\n# dummy replace\ndef convert_module_to_f16(x):\n    pass\n\ndef convert_module_to_f32(x):\n    pass", "\ndef convert_module_to_f32(x):\n    pass\n\n## go\n# ---------------------------------------------------------------------------------------------------\nclass TimestepBlock(nn.Module):\n    \"\"\"\n    Any module where forward() takes timestep embeddings as a second argument.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"", "\n\n# ---------------------------------------------------------------------------------------------------\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    \"\"\"\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    \"\"\"\n\n    def forward(self, x, emb, context, **kwargs):\n        for layer in self:\n            if isinstance(layer, TimestepBlock):\n                x = layer(x, emb, **kwargs)\n            elif isinstance(layer, STTransformerClass):\n                x = layer(x, context, **kwargs)\n            else:\n                x = layer(x)\n        return x", "\n\n# ---------------------------------------------------------------------------------------------------\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 upsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, \n        kernel_size_t=3,\n        padding_t=1,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        if use_conv:\n            self.conv = conv_nd(dims, self.channels, self.out_channels, (kernel_size_t, 3,3), padding=(padding_t, 1,1))\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        if self.dims == 3:\n            x = F.interpolate(\n                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\"\n            )\n        else:\n            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if self.use_conv:\n            x = self.conv(x)\n        return x", "\n\n# ---------------------------------------------------------------------------------------------------\nclass TransposedUpsample(nn.Module):\n    'Learned 2x upsampling without padding'\n    def __init__(self, channels, out_channels=None, ks=5):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n\n        self.up = nn.ConvTranspose2d(self.channels,self.out_channels,kernel_size=ks,stride=2)\n\n    def forward(self,x):\n        return self.up(x)", "\n\n# ---------------------------------------------------------------------------------------------------\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None,\n        kernel_size_t=3,\n        padding_t=1,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = conv_nd(\n                dims, self.channels, self.out_channels, (kernel_size_t, 3,3), stride=stride, padding=(padding_t, 1,1)\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)", "\n\n# ---------------------------------------------------------------------------------------------------\nclass ResBlock(TimestepBlock):\n    \"\"\"\n    A residual block that can optionally change the number of channels.\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param use_checkpoint: if True, use gradient checkpointing on this module.\n    :param up: if True, use this block for upsampling.\n    :param down: if True, use this block for downsampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        use_checkpoint=False,\n        up=False,\n        down=False,\n        # temporal\n        kernel_size_t=3,\n        padding_t=1,\n        nonlinearity_type='silu',\n        **kwargs\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_checkpoint = use_checkpoint\n        self.use_scale_shift_norm = use_scale_shift_norm\n        self.nonlinearity_type = nonlinearity_type\n\n        self.in_layers = nn.Sequential(\n            normalization(channels),\n            nonlinearity(nonlinearity_type),\n            conv_nd(dims, channels, self.out_channels, (kernel_size_t, 3,3), padding=(padding_t, 1,1)),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims, kernel_size_t=kernel_size_t, padding_t=padding_t)\n            self.x_upd = Upsample(channels, False, dims, kernel_size_t=kernel_size_t, padding_t=padding_t)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims, kernel_size_t=kernel_size_t, padding_t=padding_t)\n            self.x_upd = Downsample(channels, False, dims, kernel_size_t=kernel_size_t, padding_t=padding_t)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.emb_layers = nn.Sequential(\n            nonlinearity(nonlinearity_type),\n            linear(\n                emb_channels,\n                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n            ),\n        )\n        self.out_layers = nn.Sequential(\n            normalization(self.out_channels),\n            nonlinearity(nonlinearity_type),\n            nn.Dropout(p=dropout),\n            zero_module(\n                conv_nd(dims, self.out_channels, self.out_channels, (kernel_size_t, 3,3), padding=(padding_t, 1,1))\n            ),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = conv_nd(\n                dims, channels, self.out_channels, (kernel_size_t, 3,3), padding=(padding_t, 1,1)\n            )\n        else:\n            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n        \n\n    def forward(self, x, emb, **kwargs):\n        \"\"\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        return checkpoint(self._forward, \n                          (x, emb), \n                          self.parameters(), \n                          self.use_checkpoint\n                          )\n\n    def _forward(self, x, emb,):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n\n        emb_out = self.emb_layers(emb).type(h.dtype)\n        if emb_out.dim() == 3: # btc for video data\n            emb_out = rearrange(emb_out, 'b t c -> b c t')\n        while len(emb_out.shape) < h.dim():\n            emb_out = emb_out[..., None] # bct -> bct11 or bc -> bc111\n        \n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            scale, shift = th.chunk(emb_out, 2, dim=1)\n            h = out_norm(h) * (1 + scale) + shift\n            h = out_rest(h)\n        else:\n            h = h + emb_out\n            h = self.out_layers(h)\n\n        out = self.skip_connection(x) + h\n        \n        return out", "\n# ---------------------------------------------------------------------------------------------------\ndef make_spatialtemporal_transformer(module_name='attention_temporal', class_name='SpatialTemporalTransformer'):\n    module = __import__(f\"videocrafter.lvdm.models.modules.{module_name}\", fromlist=[class_name])\n    global STTransformerClass\n    STTransformerClass = getattr(module, class_name)\n    return STTransformerClass\n\n# ---------------------------------------------------------------------------------------------------\nclass UNetModel(nn.Module):\n    \"\"\"\n    The full UNet model with attention and timestep embedding.\n    :param in_channels: channels in the input Tensor.\n    :param model_channels: base channel count for the model.\n    :param out_channels: channels in the output Tensor.\n    :param num_res_blocks: number of residual blocks per downsample.\n    :param attention_resolutions: a collection of downsample rates at which\n        attention will take place. May be a set, list, or tuple.\n        For example, if this contains 4, then at 4x downsampling, attention\n        will be used.\n    :param dropout: the dropout probability.\n    :param channel_mult: channel multiplier for each level of the UNet.\n    :param conv_resample: if True, use learned convolutions for upsampling and\n        downsampling.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param num_classes: if specified (as an int), then this model will be\n        class-conditional with `num_classes` classes.\n    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n    :param num_heads: the number of attention heads in each attention layer.\n    :param num_heads_channels: if specified, ignore num_heads and instead use\n                               a fixed channel width per attention head.\n    :param num_heads_upsample: works with num_heads to set a different number\n                               of heads for upsampling. Deprecated.\n    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n    :param resblock_updown: use residual blocks for up/downsampling.\n    :param use_new_attention_order: use a different attention pattern for potentially\n                                    increased efficiency.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size, # not used in UNetModel\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        attention_resolutions,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=3,\n        num_classes=None,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=-1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        transformer_depth=1,              # custom transformer support\n        context_dim=None,                 # custom transformer support\n        legacy=True,\n        # temporal related\n        kernel_size_t=1,\n        padding_t=1,\n        use_temporal_transformer=True,\n        temporal_length=None,\n        use_relative_position=False,\n        cross_attn_on_tempoal=False,\n        temporal_crossattn_type=\"crossattn\",\n        order=\"stst\",\n        nonlinearity_type='silu',\n        temporalcrossfirst=False,\n        split_stcontext=False,\n        temporal_context_dim=None,\n        use_tempoal_causal_attn=False,\n        ST_transformer_module='attention_temporal',\n        ST_transformer_class='SpatialTemporalTransformer',\n        **kwargs,\n    ):\n        super().__init__()\n        assert(use_temporal_transformer)\n        if context_dim is not None:\n            if type(context_dim) == ListConfig:\n                context_dim = list(context_dim)\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        if num_heads == -1:\n            assert num_head_channels != -1, 'Either num_heads or num_head_channels has to be set'\n\n        if num_head_channels == -1:\n            assert num_heads != -1, 'Either num_heads or num_head_channels has to be set'\n\n        self.image_size = image_size\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        self.num_res_blocks = num_res_blocks\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = th.float16 if use_fp16 else th.float32\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n\n        self.use_relative_position = use_relative_position\n        self.temporal_length = temporal_length\n        self.cross_attn_on_tempoal = cross_attn_on_tempoal\n        self.temporal_crossattn_type = temporal_crossattn_type\n        self.order = order\n        self.temporalcrossfirst = temporalcrossfirst\n        self.split_stcontext = split_stcontext\n        self.temporal_context_dim = temporal_context_dim\n        self.nonlinearity_type = nonlinearity_type\n        self.use_tempoal_causal_attn = use_tempoal_causal_attn\n        \n\n        time_embed_dim = model_channels * 4\n        self.time_embed_dim = time_embed_dim\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nonlinearity(nonlinearity_type),\n            linear(time_embed_dim, time_embed_dim),\n        )\n\n        if self.num_classes is not None:\n            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n        \n        STTransformerClass = make_spatialtemporal_transformer(module_name=ST_transformer_module, \n            class_name=ST_transformer_class)\n\n        self.input_blocks = nn.ModuleList(\n            [\n                TimestepEmbedSequential(\n                    conv_nd(dims, in_channels, model_channels, (kernel_size_t, 3,3), padding=(padding_t, 1,1))\n                )\n            ]\n        )\n        self._feature_size = model_channels\n        input_block_chans = [model_channels]\n        ch = model_channels\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for _ in range(num_res_blocks):\n                layers = [\n                    ResBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=mult * model_channels,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                        kernel_size_t=kernel_size_t,\n                        padding_t=padding_t,\n                        nonlinearity_type=nonlinearity_type,\n                        **kwargs\n                    )\n                ]\n                ch = mult * model_channels\n                if ds in attention_resolutions:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        dim_head = ch // num_heads if use_temporal_transformer else num_head_channels\n                    layers.append(STTransformerClass(\n                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                            # temporal related\n                            temporal_length=temporal_length,\n                            use_relative_position=use_relative_position,\n                            cross_attn_on_tempoal=cross_attn_on_tempoal,\n                            temporal_crossattn_type=temporal_crossattn_type,\n                            order=order,\n                            temporalcrossfirst=temporalcrossfirst,\n                            split_stcontext=split_stcontext,\n                            temporal_context_dim=temporal_context_dim,\n                            use_tempoal_causal_attn=use_tempoal_causal_attn,\n                            **kwargs,\n                            ))\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                            kernel_size_t=kernel_size_t,\n                            padding_t=padding_t,\n                            nonlinearity_type=nonlinearity_type,\n                            **kwargs\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch, kernel_size_t=kernel_size_t, padding_t=padding_t\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        if num_head_channels == -1:\n            dim_head = ch // num_heads\n        else:\n            num_heads = ch // num_head_channels\n            dim_head = num_head_channels\n        if legacy:\n            dim_head = ch // num_heads if use_temporal_transformer else num_head_channels\n        self.middle_block = TimestepEmbedSequential(\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n                kernel_size_t=kernel_size_t,\n                padding_t=padding_t,\n                nonlinearity_type=nonlinearity_type,\n                **kwargs\n            ),\n            STTransformerClass(\n                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                            # temporal related\n                            temporal_length=temporal_length,\n                            use_relative_position=use_relative_position,\n                            cross_attn_on_tempoal=cross_attn_on_tempoal,\n                            temporal_crossattn_type=temporal_crossattn_type,\n                            order=order,\n                            temporalcrossfirst=temporalcrossfirst,\n                            split_stcontext=split_stcontext,\n                            temporal_context_dim=temporal_context_dim,\n                            use_tempoal_causal_attn=use_tempoal_causal_attn,\n                            **kwargs,\n                        ),\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n                kernel_size_t=kernel_size_t,\n                padding_t=padding_t,\n                nonlinearity_type=nonlinearity_type,\n                **kwargs\n            ),\n        )\n        self._feature_size += ch\n\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(num_res_blocks + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    ResBlock(\n                        ch + ich,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=model_channels * mult,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                        kernel_size_t=kernel_size_t,\n                        padding_t=padding_t,\n                        nonlinearity_type=nonlinearity_type,\n                        **kwargs\n                    )\n                ]\n                ch = model_channels * mult\n                if ds in attention_resolutions:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        dim_head = ch // num_heads if use_temporal_transformer else num_head_channels\n                    layers.append(\n                        STTransformerClass(\n                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                            # temporal related\n                            temporal_length=temporal_length,\n                            use_relative_position=use_relative_position,\n                            cross_attn_on_tempoal=cross_attn_on_tempoal,\n                            temporal_crossattn_type=temporal_crossattn_type,\n                            order=order,\n                            temporalcrossfirst=temporalcrossfirst,\n                            split_stcontext=split_stcontext,\n                            temporal_context_dim=temporal_context_dim,\n                            use_tempoal_causal_attn=use_tempoal_causal_attn,\n                            **kwargs,\n                        )\n                    )\n                if level and i == num_res_blocks:\n                    out_ch = ch\n                    layers.append(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                            kernel_size_t=kernel_size_t,\n                            padding_t=padding_t,\n                            nonlinearity_type=nonlinearity_type,\n                            **kwargs\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch, kernel_size_t=kernel_size_t, padding_t=padding_t)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n\n        self.out = nn.Sequential(\n            normalization(ch),\n            nonlinearity(nonlinearity_type),\n            zero_module(conv_nd(dims, model_channels, out_channels, (kernel_size_t, 3,3), padding=(padding_t, 1,1))),\n        )\n        \n\n    def convert_to_fp16(self):\n        \"\"\"\n        Convert the torso of the model to float16.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f16)\n        self.middle_block.apply(convert_module_to_f16)\n        self.output_blocks.apply(convert_module_to_f16)\n\n    def convert_to_fp32(self):\n        \"\"\"\n        Convert the torso of the model to float32.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f32)\n        self.middle_block.apply(convert_module_to_f32)\n        self.output_blocks.apply(convert_module_to_f32)\n\n    def forward(self, x, timesteps=None, time_emb_replace=None, context=None, features_adapter=None, y=None, **kwargs):\n        \"\"\"\n        Apply the model to an input batch.\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param timesteps: a 1-D batch of timesteps.\n        :param context: conditioning plugged in via crossattn\n        :param y: an [N] Tensor of labels, if class-conditional.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        \n        hs = []\n        if time_emb_replace is None:\n            t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n            emb = self.time_embed(t_emb)\n        else:\n            emb = time_emb_replace\n        \n        if y is not None: # if class-conditional model, inject class labels\n            assert y.shape == (x.shape[0],)\n            emb = emb + self.label_emb(y)\n\n        h = x.type(self.dtype)\n        adapter_idx = 0\n        for id, module in enumerate(self.input_blocks):\n            h = module(h, emb, context, **kwargs)\n            ## plug-in adapter features\n            if ((id+1)%3 == 0) and features_adapter is not None:\n                h = h + features_adapter[adapter_idx]\n                adapter_idx += 1\n            hs.append(h)\n        if features_adapter is not None:\n            assert len(features_adapter)==adapter_idx, 'Mismatch features adapter'\n\n        h = self.middle_block(h, emb, context, **kwargs)\n        for module in self.output_blocks:\n            h = th.cat([h, hs.pop()], dim=1)\n            h = module(h, emb, context, **kwargs)\n        h = h.type(x.dtype)\n        return self.out(h)", "# ---------------------------------------------------------------------------------------------------\nclass UNetModel(nn.Module):\n    \"\"\"\n    The full UNet model with attention and timestep embedding.\n    :param in_channels: channels in the input Tensor.\n    :param model_channels: base channel count for the model.\n    :param out_channels: channels in the output Tensor.\n    :param num_res_blocks: number of residual blocks per downsample.\n    :param attention_resolutions: a collection of downsample rates at which\n        attention will take place. May be a set, list, or tuple.\n        For example, if this contains 4, then at 4x downsampling, attention\n        will be used.\n    :param dropout: the dropout probability.\n    :param channel_mult: channel multiplier for each level of the UNet.\n    :param conv_resample: if True, use learned convolutions for upsampling and\n        downsampling.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param num_classes: if specified (as an int), then this model will be\n        class-conditional with `num_classes` classes.\n    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n    :param num_heads: the number of attention heads in each attention layer.\n    :param num_heads_channels: if specified, ignore num_heads and instead use\n                               a fixed channel width per attention head.\n    :param num_heads_upsample: works with num_heads to set a different number\n                               of heads for upsampling. Deprecated.\n    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n    :param resblock_updown: use residual blocks for up/downsampling.\n    :param use_new_attention_order: use a different attention pattern for potentially\n                                    increased efficiency.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size, # not used in UNetModel\n        in_channels,\n        model_channels,\n        out_channels,\n        num_res_blocks,\n        attention_resolutions,\n        dropout=0,\n        channel_mult=(1, 2, 4, 8),\n        conv_resample=True,\n        dims=3,\n        num_classes=None,\n        use_checkpoint=False,\n        use_fp16=False,\n        num_heads=-1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        resblock_updown=False,\n        transformer_depth=1,              # custom transformer support\n        context_dim=None,                 # custom transformer support\n        legacy=True,\n        # temporal related\n        kernel_size_t=1,\n        padding_t=1,\n        use_temporal_transformer=True,\n        temporal_length=None,\n        use_relative_position=False,\n        cross_attn_on_tempoal=False,\n        temporal_crossattn_type=\"crossattn\",\n        order=\"stst\",\n        nonlinearity_type='silu',\n        temporalcrossfirst=False,\n        split_stcontext=False,\n        temporal_context_dim=None,\n        use_tempoal_causal_attn=False,\n        ST_transformer_module='attention_temporal',\n        ST_transformer_class='SpatialTemporalTransformer',\n        **kwargs,\n    ):\n        super().__init__()\n        assert(use_temporal_transformer)\n        if context_dim is not None:\n            if type(context_dim) == ListConfig:\n                context_dim = list(context_dim)\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        if num_heads == -1:\n            assert num_head_channels != -1, 'Either num_heads or num_head_channels has to be set'\n\n        if num_head_channels == -1:\n            assert num_heads != -1, 'Either num_heads or num_head_channels has to be set'\n\n        self.image_size = image_size\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        self.num_res_blocks = num_res_blocks\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = th.float16 if use_fp16 else th.float32\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n\n        self.use_relative_position = use_relative_position\n        self.temporal_length = temporal_length\n        self.cross_attn_on_tempoal = cross_attn_on_tempoal\n        self.temporal_crossattn_type = temporal_crossattn_type\n        self.order = order\n        self.temporalcrossfirst = temporalcrossfirst\n        self.split_stcontext = split_stcontext\n        self.temporal_context_dim = temporal_context_dim\n        self.nonlinearity_type = nonlinearity_type\n        self.use_tempoal_causal_attn = use_tempoal_causal_attn\n        \n\n        time_embed_dim = model_channels * 4\n        self.time_embed_dim = time_embed_dim\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nonlinearity(nonlinearity_type),\n            linear(time_embed_dim, time_embed_dim),\n        )\n\n        if self.num_classes is not None:\n            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n        \n        STTransformerClass = make_spatialtemporal_transformer(module_name=ST_transformer_module, \n            class_name=ST_transformer_class)\n\n        self.input_blocks = nn.ModuleList(\n            [\n                TimestepEmbedSequential(\n                    conv_nd(dims, in_channels, model_channels, (kernel_size_t, 3,3), padding=(padding_t, 1,1))\n                )\n            ]\n        )\n        self._feature_size = model_channels\n        input_block_chans = [model_channels]\n        ch = model_channels\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for _ in range(num_res_blocks):\n                layers = [\n                    ResBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=mult * model_channels,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                        kernel_size_t=kernel_size_t,\n                        padding_t=padding_t,\n                        nonlinearity_type=nonlinearity_type,\n                        **kwargs\n                    )\n                ]\n                ch = mult * model_channels\n                if ds in attention_resolutions:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        dim_head = ch // num_heads if use_temporal_transformer else num_head_channels\n                    layers.append(STTransformerClass(\n                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                            # temporal related\n                            temporal_length=temporal_length,\n                            use_relative_position=use_relative_position,\n                            cross_attn_on_tempoal=cross_attn_on_tempoal,\n                            temporal_crossattn_type=temporal_crossattn_type,\n                            order=order,\n                            temporalcrossfirst=temporalcrossfirst,\n                            split_stcontext=split_stcontext,\n                            temporal_context_dim=temporal_context_dim,\n                            use_tempoal_causal_attn=use_tempoal_causal_attn,\n                            **kwargs,\n                            ))\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                            kernel_size_t=kernel_size_t,\n                            padding_t=padding_t,\n                            nonlinearity_type=nonlinearity_type,\n                            **kwargs\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch, kernel_size_t=kernel_size_t, padding_t=padding_t\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        if num_head_channels == -1:\n            dim_head = ch // num_heads\n        else:\n            num_heads = ch // num_head_channels\n            dim_head = num_head_channels\n        if legacy:\n            dim_head = ch // num_heads if use_temporal_transformer else num_head_channels\n        self.middle_block = TimestepEmbedSequential(\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n                kernel_size_t=kernel_size_t,\n                padding_t=padding_t,\n                nonlinearity_type=nonlinearity_type,\n                **kwargs\n            ),\n            STTransformerClass(\n                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                            # temporal related\n                            temporal_length=temporal_length,\n                            use_relative_position=use_relative_position,\n                            cross_attn_on_tempoal=cross_attn_on_tempoal,\n                            temporal_crossattn_type=temporal_crossattn_type,\n                            order=order,\n                            temporalcrossfirst=temporalcrossfirst,\n                            split_stcontext=split_stcontext,\n                            temporal_context_dim=temporal_context_dim,\n                            use_tempoal_causal_attn=use_tempoal_causal_attn,\n                            **kwargs,\n                        ),\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n                kernel_size_t=kernel_size_t,\n                padding_t=padding_t,\n                nonlinearity_type=nonlinearity_type,\n                **kwargs\n            ),\n        )\n        self._feature_size += ch\n\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(num_res_blocks + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    ResBlock(\n                        ch + ich,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=model_channels * mult,\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                        kernel_size_t=kernel_size_t,\n                        padding_t=padding_t,\n                        nonlinearity_type=nonlinearity_type,\n                        **kwargs\n                    )\n                ]\n                ch = model_channels * mult\n                if ds in attention_resolutions:\n                    if num_head_channels == -1:\n                        dim_head = ch // num_heads\n                    else:\n                        num_heads = ch // num_head_channels\n                        dim_head = num_head_channels\n                    if legacy:\n                        dim_head = ch // num_heads if use_temporal_transformer else num_head_channels\n                    layers.append(\n                        STTransformerClass(\n                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,\n                            # temporal related\n                            temporal_length=temporal_length,\n                            use_relative_position=use_relative_position,\n                            cross_attn_on_tempoal=cross_attn_on_tempoal,\n                            temporal_crossattn_type=temporal_crossattn_type,\n                            order=order,\n                            temporalcrossfirst=temporalcrossfirst,\n                            split_stcontext=split_stcontext,\n                            temporal_context_dim=temporal_context_dim,\n                            use_tempoal_causal_attn=use_tempoal_causal_attn,\n                            **kwargs,\n                        )\n                    )\n                if level and i == num_res_blocks:\n                    out_ch = ch\n                    layers.append(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                            kernel_size_t=kernel_size_t,\n                            padding_t=padding_t,\n                            nonlinearity_type=nonlinearity_type,\n                            **kwargs\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch, kernel_size_t=kernel_size_t, padding_t=padding_t)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n\n        self.out = nn.Sequential(\n            normalization(ch),\n            nonlinearity(nonlinearity_type),\n            zero_module(conv_nd(dims, model_channels, out_channels, (kernel_size_t, 3,3), padding=(padding_t, 1,1))),\n        )\n        \n\n    def convert_to_fp16(self):\n        \"\"\"\n        Convert the torso of the model to float16.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f16)\n        self.middle_block.apply(convert_module_to_f16)\n        self.output_blocks.apply(convert_module_to_f16)\n\n    def convert_to_fp32(self):\n        \"\"\"\n        Convert the torso of the model to float32.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f32)\n        self.middle_block.apply(convert_module_to_f32)\n        self.output_blocks.apply(convert_module_to_f32)\n\n    def forward(self, x, timesteps=None, time_emb_replace=None, context=None, features_adapter=None, y=None, **kwargs):\n        \"\"\"\n        Apply the model to an input batch.\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param timesteps: a 1-D batch of timesteps.\n        :param context: conditioning plugged in via crossattn\n        :param y: an [N] Tensor of labels, if class-conditional.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        \n        hs = []\n        if time_emb_replace is None:\n            t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n            emb = self.time_embed(t_emb)\n        else:\n            emb = time_emb_replace\n        \n        if y is not None: # if class-conditional model, inject class labels\n            assert y.shape == (x.shape[0],)\n            emb = emb + self.label_emb(y)\n\n        h = x.type(self.dtype)\n        adapter_idx = 0\n        for id, module in enumerate(self.input_blocks):\n            h = module(h, emb, context, **kwargs)\n            ## plug-in adapter features\n            if ((id+1)%3 == 0) and features_adapter is not None:\n                h = h + features_adapter[adapter_idx]\n                adapter_idx += 1\n            hs.append(h)\n        if features_adapter is not None:\n            assert len(features_adapter)==adapter_idx, 'Mismatch features adapter'\n\n        h = self.middle_block(h, emb, context, **kwargs)\n        for module in self.output_blocks:\n            h = th.cat([h, hs.pop()], dim=1)\n            h = module(h, emb, context, **kwargs)\n        h = h.type(x.dtype)\n        return self.out(h)", ""]}
{"filename": "scripts/videocrafter/lvdm/models/modules/autoencoder_modules.py", "chunked_list": ["import math\n\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom einops import rearrange\n\n\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device=timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    return emb", "def get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device=timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    return emb", "\ndef nonlinearity(x):\n    # swish\n    return x*torch.sigmoid(x)\n\n\ndef Normalize(in_channels, num_groups=32):\n    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n\n", "\n\n\nclass LinearAttention(nn.Module):\n    def __init__(self, dim, heads=4, dim_head=32):\n        super().__init__()\n        self.heads = heads\n        hidden_dim = dim_head * heads\n        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        qkv = self.to_qkv(x)\n        q, k, v = rearrange(qkv, 'b (qkv heads c) h w -> qkv b heads c (h w)', heads = self.heads, qkv=3)\n        k = k.softmax(dim=-1)  \n        context = torch.einsum('bhdn,bhen->bhde', k, v)\n        out = torch.einsum('bhde,bhdn->bhen', context, q)\n        out = rearrange(out, 'b heads c (h w) -> b (heads c) h w', heads=self.heads, h=h, w=w)\n        return self.to_out(out)", "\n\nclass LinAttnBlock(LinearAttention):\n    \"\"\"to match AttnBlock usage\"\"\"\n    def __init__(self, in_channels):\n        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\n\n\nclass AttnBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.norm = Normalize(in_channels)\n        self.q = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.k = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.v = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.proj_out = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=1,\n                                        stride=1,\n                                        padding=0)\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b,c,h,w = q.shape\n        q = q.reshape(b,c,h*w) # bcl\n        q = q.permute(0,2,1)   # bcl -> blc l=hw\n        k = k.reshape(b,c,h*w) # bcl\n        \n        w_ = torch.bmm(q,k)    # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n        w_ = w_ * (int(c)**(-0.5))\n        w_ = torch.nn.functional.softmax(w_, dim=2)\n\n        # attend to values\n        v = v.reshape(b,c,h*w)\n        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n        h_ = h_.reshape(b,c,h,w)\n\n        h_ = self.proj_out(h_)\n\n        return x+h_", "class AttnBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.norm = Normalize(in_channels)\n        self.q = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.k = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.v = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.proj_out = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=1,\n                                        stride=1,\n                                        padding=0)\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b,c,h,w = q.shape\n        q = q.reshape(b,c,h*w) # bcl\n        q = q.permute(0,2,1)   # bcl -> blc l=hw\n        k = k.reshape(b,c,h*w) # bcl\n        \n        w_ = torch.bmm(q,k)    # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n        w_ = w_ * (int(c)**(-0.5))\n        w_ = torch.nn.functional.softmax(w_, dim=2)\n\n        # attend to values\n        v = v.reshape(b,c,h*w)\n        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n        h_ = h_.reshape(b,c,h,w)\n\n        h_ = self.proj_out(h_)\n\n        return x+h_", "\n\ndef make_attn(in_channels, attn_type=\"vanilla\"):\n    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n    if attn_type == \"vanilla\":\n        return AttnBlock(in_channels)\n    elif attn_type == \"none\":\n        return nn.Identity(in_channels)\n    else:\n        return LinAttnBlock(in_channels)", " \nclass Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        self.in_channels = in_channels\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=0)\n    def forward(self, x):\n        if self.with_conv:\n            pad = (0,1,0,1)\n            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n            x = self.conv(x)\n        else:\n            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n        return x", "\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        self.in_channels = in_channels\n        if self.with_conv:\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        if self.with_conv:\n            x = self.conv(x)\n        return x", "\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n                 dropout, temb_channels=512):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.norm1 = Normalize(in_channels)\n        self.conv1 = torch.nn.Conv2d(in_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if temb_channels > 0:\n            self.temb_proj = torch.nn.Linear(temb_channels,\n                                             out_channels)\n        self.norm2 = Normalize(out_channels)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.conv2 = torch.nn.Conv2d(out_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = torch.nn.Conv2d(in_channels,\n                                                     out_channels,\n                                                     kernel_size=3,\n                                                     stride=1,\n                                                     padding=1)\n            else:\n                self.nin_shortcut = torch.nn.Conv2d(in_channels,\n                                                    out_channels,\n                                                    kernel_size=1,\n                                                    stride=1,\n                                                    padding=0)\n\n    def forward(self, x, temb):\n        h = x\n        h = self.norm1(h)\n        h = nonlinearity(h)\n        h = self.conv1(h)\n\n        if temb is not None:\n            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n\n        h = self.norm2(h)\n        h = nonlinearity(h)\n        h = self.dropout(h)\n        h = self.conv2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n\n        return x+h", "\nclass Model(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = self.ch*4\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n\n        self.use_timestep = use_timestep\n        if self.use_timestep:\n            # timestep embedding\n            self.temb = nn.Module()\n            self.temb.dense = nn.ModuleList([\n                torch.nn.Linear(self.ch,\n                                self.temb_ch),\n                torch.nn.Linear(self.temb_ch,\n                                self.temb_ch),\n            ])\n\n        # downsampling\n        self.conv_in = torch.nn.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            skip_in = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                if i_block == self.num_res_blocks:\n                    skip_in = ch*in_ch_mult[i_level]\n                block.append(ResnetBlock(in_channels=block_in+skip_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x, t=None, context=None):\n        #assert x.shape[2] == x.shape[3] == self.resolution\n        if context is not None:\n            # assume aligned context, cat along channel axis\n            x = torch.cat((x, context), dim=1)\n        if self.use_timestep:\n            # timestep embedding\n            assert t is not None\n            temb = get_timestep_embedding(t, self.ch)\n            temb = self.temb.dense[0](temb)\n            temb = nonlinearity(temb)\n            temb = self.temb.dense[1](temb)\n        else:\n            temb = None\n\n        # downsampling\n        hs = [self.conv_in(x)]\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](hs[-1], temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n                hs.append(h)\n            if i_level != self.num_resolutions-1:\n                hs.append(self.down[i_level].downsample(hs[-1]))\n\n        # middle\n        h = hs[-1]\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](\n                    torch.cat([h, hs.pop()], dim=1), temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n    def get_last_layer(self):\n        return self.conv_out.weight", "\n\nclass Encoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n                 **ignore_kwargs):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n\n        # downsampling\n        self.conv_in = torch.nn.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.in_ch_mult = in_ch_mult\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        2*z_channels if double_z else z_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        # timestep embedding\n        temb = None\n\n        # print(f'encoder-input={x.shape}')\n        # downsampling\n        hs = [self.conv_in(x)]\n        # print(f'encoder-conv in feat={hs[0].shape}')\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](hs[-1], temb)\n                # print(f'encoder-down feat={h.shape}')\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n                hs.append(h)\n            if i_level != self.num_resolutions-1:\n                # print(f'encoder-downsample (input)={hs[-1].shape}')\n                hs.append(self.down[i_level].downsample(hs[-1]))\n                # print(f'encoder-downsample (output)={hs[-1].shape}')\n\n        # middle\n        h = hs[-1]\n        h = self.mid.block_1(h, temb)\n        # print(f'encoder-mid1 feat={h.shape}')\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n        # print(f'encoder-mid2 feat={h.shape}')\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        # print(f'end feat={h.shape}')\n        return h", "\n\nclass Decoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n                 attn_type=\"vanilla\", **ignorekwargs):\n        super().__init__()\n        if use_linear_attn: attn_type = \"linear\"\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.give_pre_end = give_pre_end\n        self.tanh_out = tanh_out\n\n        # compute in_ch_mult, block_in and curr_res at lowest res\n        in_ch_mult = (1,)+tuple(ch_mult)\n        block_in = ch*ch_mult[self.num_resolutions-1]\n        curr_res = resolution // 2**(self.num_resolutions-1)\n        self.z_shape = (1,z_channels,curr_res,curr_res)\n        print(\"Working with z of shape {} = {} dimensions.\".format(\n            self.z_shape, np.prod(self.z_shape)))\n\n        # z to block_in\n        self.conv_in = torch.nn.Conv2d(z_channels,\n                                       block_in,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(make_attn(block_in, attn_type=attn_type))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, z):\n        #assert z.shape[1:] == self.z_shape[1:]\n        self.last_z_shape = z.shape\n\n        # print(f'decoder-input={z.shape}')\n        # timestep embedding\n        temb = None\n\n        # z to block_in\n        h = self.conv_in(z)\n        # print(f'decoder-conv in feat={h.shape}')\n\n        # middle\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n        # print(f'decoder-mid feat={h.shape}')\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](h, temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n                # print(f'decoder-up feat={h.shape}')\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n                # print(f'decoder-upsample feat={h.shape}')\n\n        # end\n        if self.give_pre_end:\n            return h\n\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        # print(f'decoder-conv_out feat={h.shape}')\n        if self.tanh_out:\n            h = torch.tanh(h)\n        return h", ""]}
{"filename": "scripts/videocrafter/lvdm/models/modules/condition_modules.py", "chunked_list": ["import torch.nn as nn\nfrom transformers import logging\nfrom transformers import CLIPTokenizer, CLIPTextModel\nlogging.set_verbosity_error()\n\n\nclass AbstractEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def encode(self, *args, **kwargs):\n        raise NotImplementedError", "\n\nclass FrozenCLIPEmbedder(AbstractEncoder):\n    \"\"\"Uses the CLIP transformer encoder for text (from huggingface)\"\"\"\n    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cuda\", max_length=77):\n        super().__init__()\n        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n        self.transformer = CLIPTextModel.from_pretrained(version)\n        self.device = device\n        self.max_length = max_length\n        self.freeze()\n\n    def freeze(self):\n        self.transformer = self.transformer.eval()\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, text):\n        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n        tokens = batch_encoding[\"input_ids\"].to(self.device)\n        outputs = self.transformer(input_ids=tokens)\n\n        z = outputs.last_hidden_state\n        return z\n\n    def encode(self, text):\n        return self(text)", ""]}
{"filename": "scripts/videocrafter/lvdm/models/modules/adapter.py", "chunked_list": ["import torch\nimport torch.nn as nn\nfrom collections import OrderedDict\nfrom videocrafter.lvdm.models.modules.util import (\n    zero_module,\n    conv_nd,\n    avg_pool_nd\n)\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = conv_nd(\n                dims, self.channels, self.out_channels, 3, stride=stride, padding=padding\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)", "\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = conv_nd(\n                dims, self.channels, self.out_channels, 3, stride=stride, padding=padding\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)", "\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, in_c, out_c, down, ksize=3, sk=False, use_conv=True):\n        super().__init__()\n        ps = ksize // 2\n        if in_c != out_c or sk == False:\n            self.in_conv = nn.Conv2d(in_c, out_c, ksize, 1, ps)\n        else:\n            # print('n_in')\n            self.in_conv = None\n        self.block1 = nn.Conv2d(out_c, out_c, 3, 1, 1)\n        self.act = nn.ReLU()\n        self.block2 = nn.Conv2d(out_c, out_c, ksize, 1, ps)\n        if sk == False:\n            self.skep = nn.Conv2d(in_c, out_c, ksize, 1, ps)\n        else:\n            self.skep = None\n\n        self.down = down\n        if self.down == True:\n            self.down_opt = Downsample(in_c, use_conv=use_conv)\n\n    def forward(self, x):\n        if self.down == True:\n            x = self.down_opt(x)\n        if self.in_conv is not None:  # edit\n            x = self.in_conv(x)\n\n        h = self.block1(x)\n        h = self.act(h)\n        h = self.block2(h)\n        if self.skep is not None:\n            return h + self.skep(x)\n        else:\n            return h + x", "\n\nclass Adapter(nn.Module):\n    def __init__(self, channels=[320, 640, 1280, 1280], nums_rb=3, cin=64, ksize=3, sk=False, use_conv=True):\n        super(Adapter, self).__init__()\n        self.unshuffle = nn.PixelUnshuffle(8)\n        self.channels = channels\n        self.nums_rb = nums_rb\n        self.body = []\n        for i in range(len(channels)):\n            for j in range(nums_rb):\n                if (i != 0) and (j == 0):\n                    self.body.append(\n                        ResnetBlock(channels[i - 1], channels[i], down=True, ksize=ksize, sk=sk, use_conv=use_conv))\n                else:\n                    self.body.append(\n                        ResnetBlock(channels[i], channels[i], down=False, ksize=ksize, sk=sk, use_conv=use_conv))\n        self.body = nn.ModuleList(self.body)\n        self.conv_in = nn.Conv2d(cin, channels[0], 3, 1, 1)\n\n    def forward(self, x):\n        # unshuffle\n        x = self.unshuffle(x)\n        # extract features\n        features = []\n        x = self.conv_in(x)\n        for i in range(len(self.channels)):\n            for j in range(self.nums_rb):\n                idx = i * self.nums_rb + j\n                x = self.body[idx](x)\n            features.append(x)\n\n        return features"]}
{"filename": "scripts/videocrafter/lvdm/models/modules/lora.py", "chunked_list": ["import json\nfrom itertools import groupby\nfrom typing import Dict, List, Optional, Set, Tuple, Type, Union\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# try:", "\n# try:\n#     from safetensors.torch import safe_open\n#     from safetensors.torch import save_file as safe_save\n\n#     safetensors_available = True\n# except ImportError:\n#     from .safe_open import safe_open\n\n#     def safe_save(", "\n#     def safe_save(\n#         tensors: Dict[str, torch.Tensor],\n#         filename: str,\n#         metadata: Optional[Dict[str, str]] = None,\n#     ) -> None:\n#         raise EnvironmentError(\n#             \"Saving safetensors requires the safetensors library. Please install with pip or similar.\"\n#         )\n", "#         )\n\n#     safetensors_available = False\n\n\nclass LoraInjectedLinear(nn.Module):\n    def __init__(\n        self, in_features, out_features, bias=False, r=4, dropout_p=0.1, scale=1.0\n    ):\n        super().__init__()\n\n        if r > min(in_features, out_features):\n            raise ValueError(\n                f\"LoRA rank {r} must be less or equal than {min(in_features, out_features)}\"\n            )\n        self.r = r\n        self.linear = nn.Linear(in_features, out_features, bias)\n        self.lora_down = nn.Linear(in_features, r, bias=False)\n        self.dropout = nn.Dropout(dropout_p)\n        self.lora_up = nn.Linear(r, out_features, bias=False)\n        self.scale = scale\n        self.selector = nn.Identity()\n\n        nn.init.normal_(self.lora_down.weight, std=1 / r)\n        nn.init.zeros_(self.lora_up.weight)\n\n    def forward(self, input):\n        return (\n            self.linear(input)\n            + self.dropout(self.lora_up(self.selector(self.lora_down(input))))\n            * self.scale\n        )\n\n    def realize_as_lora(self):\n        return self.lora_up.weight.data * self.scale, self.lora_down.weight.data\n\n    def set_selector_from_diag(self, diag: torch.Tensor):\n        # diag is a 1D tensor of size (r,)\n        assert diag.shape == (self.r,)\n        self.selector = nn.Linear(self.r, self.r, bias=False)\n        self.selector.weight.data = torch.diag(diag)\n        self.selector.weight.data = self.selector.weight.data.to(\n            self.lora_up.weight.device\n        ).to(self.lora_up.weight.dtype)", "\n\nclass LoraInjectedConv2d(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups: int = 1,\n        bias: bool = True,\n        r: int = 4,\n        dropout_p: float = 0.1,\n        scale: float = 1.0,\n    ):\n        super().__init__()\n        if r > min(in_channels, out_channels):\n            raise ValueError(\n                f\"LoRA rank {r} must be less or equal than {min(in_channels, out_channels)}\"\n            )\n        self.r = r\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias,\n        )\n\n        self.lora_down = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=r,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=False,\n        )\n        self.dropout = nn.Dropout(dropout_p)\n        self.lora_up = nn.Conv2d(\n            in_channels=r,\n            out_channels=out_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False,\n        )\n        self.selector = nn.Identity()\n        self.scale = scale\n\n        nn.init.normal_(self.lora_down.weight, std=1 / r)\n        nn.init.zeros_(self.lora_up.weight)\n\n    def forward(self, input):\n        return (\n            self.conv(input)\n            + self.dropout(self.lora_up(self.selector(self.lora_down(input))))\n            * self.scale\n        )\n\n    def realize_as_lora(self):\n        return self.lora_up.weight.data * self.scale, self.lora_down.weight.data\n\n    def set_selector_from_diag(self, diag: torch.Tensor):\n        # diag is a 1D tensor of size (r,)\n        assert diag.shape == (self.r,)\n        self.selector = nn.Conv2d(\n            in_channels=self.r,\n            out_channels=self.r,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False,\n        )\n        self.selector.weight.data = torch.diag(diag)\n\n        # same device + dtype as lora_up\n        self.selector.weight.data = self.selector.weight.data.to(\n            self.lora_up.weight.device\n        ).to(self.lora_up.weight.dtype)", "\n\nUNET_DEFAULT_TARGET_REPLACE = {\"MemoryEfficientCrossAttention\",\"CrossAttention\", \"Attention\", \"GEGLU\"}\n\nUNET_EXTENDED_TARGET_REPLACE = {\"TimestepEmbedSequential\",\"SpatialTemporalTransformer\", \"MemoryEfficientCrossAttention\",\"CrossAttention\", \"Attention\", \"GEGLU\"}\n\nTEXT_ENCODER_DEFAULT_TARGET_REPLACE = {\"CLIPAttention\"}\n\nTEXT_ENCODER_EXTENDED_TARGET_REPLACE = {\"CLIPMLP\",\"CLIPAttention\"}\n", "TEXT_ENCODER_EXTENDED_TARGET_REPLACE = {\"CLIPMLP\",\"CLIPAttention\"}\n\nDEFAULT_TARGET_REPLACE = UNET_DEFAULT_TARGET_REPLACE\n\nEMBED_FLAG = \"<embed>\"\n\n\ndef _find_children(\n    model,\n    search_class: List[Type[nn.Module]] = [nn.Linear],\n):\n    \"\"\"\n    Find all modules of a certain class (or union of classes).\n\n    Returns all matching modules, along with the parent of those moduless and the\n    names they are referenced by.\n    \"\"\"\n    # For each target find every linear_class module that isn't a child of a LoraInjectedLinear\n    for parent in model.modules():\n        for name, module in parent.named_children():\n            if any([isinstance(module, _class) for _class in search_class]):\n                yield parent, name, module", "\n\ndef _find_modules_v2(\n    model,\n    ancestor_class: Optional[Set[str]] = None,\n    search_class: List[Type[nn.Module]] = [nn.Linear],\n    exclude_children_of: Optional[List[Type[nn.Module]]] = [\n        LoraInjectedLinear,\n        LoraInjectedConv2d,\n    ],\n):\n    \"\"\"\n    Find all modules of a certain class (or union of classes) that are direct or\n    indirect descendants of other modules of a certain class (or union of classes).\n\n    Returns all matching modules, along with the parent of those moduless and the\n    names they are referenced by.\n    \"\"\"\n\n    # Get the targets we should replace all linears under\n    if type(ancestor_class) is not set:\n        ancestor_class = set(ancestor_class)\n        print(ancestor_class)\n    if ancestor_class is not None:\n        ancestors = (\n            module\n            for module in model.modules()\n            if module.__class__.__name__ in ancestor_class\n        )\n    else:\n        # this, incase you want to naively iterate over all modules.\n        ancestors = [module for module in model.modules()]\n\n    # For each target find every linear_class module that isn't a child of a LoraInjectedLinear\n    for ancestor in ancestors:\n        for fullname, module in ancestor.named_children():\n            if any([isinstance(module, _class) for _class in search_class]):\n                # Find the direct parent if this is a descendant, not a child, of target\n                *path, name = fullname.split(\".\")\n                parent = ancestor\n                while path:\n                    parent = parent.get_submodule(path.pop(0))\n                # Skip this linear if it's a child of a LoraInjectedLinear\n                if exclude_children_of and any(\n                    [isinstance(parent, _class) for _class in exclude_children_of]\n                ):\n                    continue\n                # Otherwise, yield it\n                yield parent, name, module", "\n\ndef _find_modules_old(\n    model,\n    ancestor_class: Set[str] = DEFAULT_TARGET_REPLACE,\n    search_class: List[Type[nn.Module]] = [nn.Linear],\n    exclude_children_of: Optional[List[Type[nn.Module]]] = [LoraInjectedLinear],\n):\n    ret = []\n    for _module in model.modules():\n        if _module.__class__.__name__ in ancestor_class:\n\n            for name, _child_module in _module.named_children():\n                if _child_module.__class__ in search_class:\n                    ret.append((_module, name, _child_module))\n    print(ret)\n    return ret", "\n\n_find_modules = _find_modules_v2\n\n\ndef inject_trainable_lora(\n    model: nn.Module,\n    target_replace_module: Set[str] = DEFAULT_TARGET_REPLACE,\n    r: int = 4,\n    loras=None,  # path to lora .pt\n    verbose: bool = False,\n    dropout_p: float = 0.0,\n    scale: float = 1.0,\n):\n    \"\"\"\n    inject lora into model, and returns lora parameter groups.\n    \"\"\"\n\n    require_grad_params = []\n    names = []\n\n    if loras != None:\n        loras = torch.load(loras)\n\n    for _module, name, _child_module in _find_modules(\n        model, target_replace_module, search_class=[nn.Linear]\n    ):\n        weight = _child_module.weight\n        bias = _child_module.bias\n        if verbose:\n            print(\"LoRA Injection : injecting lora into \", name)\n            print(\"LoRA Injection : weight shape\", weight.shape)\n        _tmp = LoraInjectedLinear(\n            _child_module.in_features,\n            _child_module.out_features,\n            _child_module.bias is not None,\n            r=r,\n            dropout_p=dropout_p,\n            scale=scale,\n        )\n        _tmp.linear.weight = weight\n        if bias is not None:\n            _tmp.linear.bias = bias\n\n        # switch the module\n        _tmp.to(_child_module.weight.device).to(_child_module.weight.dtype)\n        _module._modules[name] = _tmp\n\n        require_grad_params.append(_module._modules[name].lora_up.parameters())\n        require_grad_params.append(_module._modules[name].lora_down.parameters())\n\n        if loras != None:\n            _module._modules[name].lora_up.weight = loras.pop(0)\n            _module._modules[name].lora_down.weight = loras.pop(0)\n\n        _module._modules[name].lora_up.weight.requires_grad = True\n        _module._modules[name].lora_down.weight.requires_grad = True\n        names.append(name)\n\n    return require_grad_params, names", "\n\ndef inject_trainable_lora_extended(\n    model: nn.Module,\n    target_replace_module: Set[str] = UNET_EXTENDED_TARGET_REPLACE,\n    r: int = 4,\n    loras=None,  # path to lora .pt\n):\n    \"\"\"\n    inject lora into model, and returns lora parameter groups.\n    \"\"\"\n\n    require_grad_params = []\n    names = []\n\n    if loras != None:\n        loras = torch.load(loras)\n\n    for _module, name, _child_module in _find_modules(\n        model, target_replace_module, search_class=[nn.Linear, nn.Conv2d]\n    ):\n        if _child_module.__class__ == nn.Linear:\n            weight = _child_module.weight\n            bias = _child_module.bias\n            _tmp = LoraInjectedLinear(\n                _child_module.in_features,\n                _child_module.out_features,\n                _child_module.bias is not None,\n                r=r,\n            )\n            _tmp.linear.weight = weight\n            if bias is not None:\n                _tmp.linear.bias = bias\n        elif _child_module.__class__ == nn.Conv2d:\n            weight = _child_module.weight\n            bias = _child_module.bias\n            _tmp = LoraInjectedConv2d(\n                _child_module.in_channels,\n                _child_module.out_channels,\n                _child_module.kernel_size,\n                _child_module.stride,\n                _child_module.padding,\n                _child_module.dilation,\n                _child_module.groups,\n                _child_module.bias is not None,\n                r=r,\n            )\n\n            _tmp.conv.weight = weight\n            if bias is not None:\n                _tmp.conv.bias = bias\n\n        # switch the module\n        _tmp.to(_child_module.weight.device).to(_child_module.weight.dtype)\n        if bias is not None:\n            _tmp.to(_child_module.bias.device).to(_child_module.bias.dtype)\n\n        _module._modules[name] = _tmp\n\n        require_grad_params.append(_module._modules[name].lora_up.parameters())\n        require_grad_params.append(_module._modules[name].lora_down.parameters())\n\n        if loras != None:\n            _module._modules[name].lora_up.weight = loras.pop(0)\n            _module._modules[name].lora_down.weight = loras.pop(0)\n\n        _module._modules[name].lora_up.weight.requires_grad = True\n        _module._modules[name].lora_down.weight.requires_grad = True\n        names.append(name)\n\n    return require_grad_params, names", "\n\ndef extract_lora_ups_down(model, target_replace_module=DEFAULT_TARGET_REPLACE):\n\n    loras = []\n\n    for _m, _n, _child_module in _find_modules(\n        model,\n        target_replace_module,\n        search_class=[LoraInjectedLinear, LoraInjectedConv2d],\n    ):\n        loras.append((_child_module.lora_up, _child_module.lora_down))\n\n    if len(loras) == 0:\n        raise ValueError(\"No lora injected.\")\n\n    return loras", "\n\ndef extract_lora_as_tensor(\n    model, target_replace_module=DEFAULT_TARGET_REPLACE, as_fp16=True\n):\n\n    loras = []\n\n    for _m, _n, _child_module in _find_modules(\n        model,\n        target_replace_module,\n        search_class=[LoraInjectedLinear, LoraInjectedConv2d],\n    ):\n        up, down = _child_module.realize_as_lora()\n        if as_fp16:\n            up = up.to(torch.float16)\n            down = down.to(torch.float16)\n\n        loras.append((up, down))\n\n    if len(loras) == 0:\n        raise ValueError(\"No lora injected.\")\n\n    return loras", "\n\ndef save_lora_weight(\n    model,\n    path=\"./lora.pt\",\n    target_replace_module=DEFAULT_TARGET_REPLACE,\n):\n    weights = []\n    for _up, _down in extract_lora_ups_down(\n        model, target_replace_module=target_replace_module\n    ):\n        weights.append(_up.weight.to(\"cpu\").to(torch.float16))\n        weights.append(_down.weight.to(\"cpu\").to(torch.float16))\n\n    torch.save(weights, path)", "\n\ndef save_lora_as_json(model, path=\"./lora.json\"):\n    weights = []\n    for _up, _down in extract_lora_ups_down(model):\n        weights.append(_up.weight.detach().cpu().numpy().tolist())\n        weights.append(_down.weight.detach().cpu().numpy().tolist())\n\n    import json\n\n    with open(path, \"w\") as f:\n        json.dump(weights, f)", "\n\ndef save_safeloras_with_embeds(\n    modelmap: Dict[str, Tuple[nn.Module, Set[str]]] = {},\n    embeds: Dict[str, torch.Tensor] = {},\n    outpath=\"./lora.safetensors\",\n):\n    \"\"\"\n    Saves the Lora from multiple modules in a single safetensor file.\n\n    modelmap is a dictionary of {\n        \"module name\": (module, target_replace_module)\n    }\n    \"\"\"\n    weights = {}\n    metadata = {}\n\n    for name, (model, target_replace_module) in modelmap.items():\n        metadata[name] = json.dumps(list(target_replace_module))\n\n        for i, (_up, _down) in enumerate(\n            extract_lora_as_tensor(model, target_replace_module)\n        ):\n            rank = _down.shape[0]\n\n            metadata[f\"{name}:{i}:rank\"] = str(rank)\n            weights[f\"{name}:{i}:up\"] = _up\n            weights[f\"{name}:{i}:down\"] = _down\n\n    for token, tensor in embeds.items():\n        metadata[token] = EMBED_FLAG\n        weights[token] = tensor\n\n    print(f\"Saving weights to {outpath}\")\n    safe_save(weights, outpath, metadata)", "\n\ndef save_safeloras(\n    modelmap: Dict[str, Tuple[nn.Module, Set[str]]] = {},\n    outpath=\"./lora.safetensors\",\n):\n    return save_safeloras_with_embeds(modelmap=modelmap, outpath=outpath)\n\n\ndef convert_loras_to_safeloras_with_embeds(\n    modelmap: Dict[str, Tuple[str, Set[str], int]] = {},\n    embeds: Dict[str, torch.Tensor] = {},\n    outpath=\"./lora.safetensors\",\n):\n    \"\"\"\n    Converts the Lora from multiple pytorch .pt files into a single safetensor file.\n\n    modelmap is a dictionary of {\n        \"module name\": (pytorch_model_path, target_replace_module, rank)\n    }\n    \"\"\"\n\n    weights = {}\n    metadata = {}\n\n    for name, (path, target_replace_module, r) in modelmap.items():\n        metadata[name] = json.dumps(list(target_replace_module))\n\n        lora = torch.load(path)\n        for i, weight in enumerate(lora):\n            is_up = i % 2 == 0\n            i = i // 2\n\n            if is_up:\n                metadata[f\"{name}:{i}:rank\"] = str(r)\n                weights[f\"{name}:{i}:up\"] = weight\n            else:\n                weights[f\"{name}:{i}:down\"] = weight\n\n    for token, tensor in embeds.items():\n        metadata[token] = EMBED_FLAG\n        weights[token] = tensor\n\n    print(f\"Saving weights to {outpath}\")\n    safe_save(weights, outpath, metadata)", "\ndef convert_loras_to_safeloras_with_embeds(\n    modelmap: Dict[str, Tuple[str, Set[str], int]] = {},\n    embeds: Dict[str, torch.Tensor] = {},\n    outpath=\"./lora.safetensors\",\n):\n    \"\"\"\n    Converts the Lora from multiple pytorch .pt files into a single safetensor file.\n\n    modelmap is a dictionary of {\n        \"module name\": (pytorch_model_path, target_replace_module, rank)\n    }\n    \"\"\"\n\n    weights = {}\n    metadata = {}\n\n    for name, (path, target_replace_module, r) in modelmap.items():\n        metadata[name] = json.dumps(list(target_replace_module))\n\n        lora = torch.load(path)\n        for i, weight in enumerate(lora):\n            is_up = i % 2 == 0\n            i = i // 2\n\n            if is_up:\n                metadata[f\"{name}:{i}:rank\"] = str(r)\n                weights[f\"{name}:{i}:up\"] = weight\n            else:\n                weights[f\"{name}:{i}:down\"] = weight\n\n    for token, tensor in embeds.items():\n        metadata[token] = EMBED_FLAG\n        weights[token] = tensor\n\n    print(f\"Saving weights to {outpath}\")\n    safe_save(weights, outpath, metadata)", "\n\ndef convert_loras_to_safeloras(\n    modelmap: Dict[str, Tuple[str, Set[str], int]] = {},\n    outpath=\"./lora.safetensors\",\n):\n    convert_loras_to_safeloras_with_embeds(modelmap=modelmap, outpath=outpath)\n\n\ndef parse_safeloras(\n    safeloras,\n) -> Dict[str, Tuple[List[nn.parameter.Parameter], List[int], List[str]]]:\n    \"\"\"\n    Converts a loaded safetensor file that contains a set of module Loras\n    into Parameters and other information\n\n    Output is a dictionary of {\n        \"module name\": (\n            [list of weights],\n            [list of ranks],\n            target_replacement_modules\n        )\n    }\n    \"\"\"\n    loras = {}\n    metadata = safeloras.metadata()\n\n    get_name = lambda k: k.split(\":\")[0]\n\n    keys = list(safeloras.keys())\n    keys.sort(key=get_name)\n\n    for name, module_keys in groupby(keys, get_name):\n        info = metadata.get(name)\n\n        if not info:\n            raise ValueError(\n                f\"Tensor {name} has no metadata - is this a Lora safetensor?\"\n            )\n\n        # Skip Textual Inversion embeds\n        if info == EMBED_FLAG:\n            continue\n\n        # Handle Loras\n        # Extract the targets\n        target = json.loads(info)\n\n        # Build the result lists - Python needs us to preallocate lists to insert into them\n        module_keys = list(module_keys)\n        ranks = [4] * (len(module_keys) // 2)\n        weights = [None] * len(module_keys)\n\n        for key in module_keys:\n            # Split the model name and index out of the key\n            _, idx, direction = key.split(\":\")\n            idx = int(idx)\n\n            # Add the rank\n            ranks[idx] = int(metadata[f\"{name}:{idx}:rank\"])\n\n            # Insert the weight into the list\n            idx = idx * 2 + (1 if direction == \"down\" else 0)\n            weights[idx] = nn.parameter.Parameter(safeloras.get_tensor(key))\n\n        loras[name] = (weights, ranks, target)\n\n    return loras", "\ndef parse_safeloras(\n    safeloras,\n) -> Dict[str, Tuple[List[nn.parameter.Parameter], List[int], List[str]]]:\n    \"\"\"\n    Converts a loaded safetensor file that contains a set of module Loras\n    into Parameters and other information\n\n    Output is a dictionary of {\n        \"module name\": (\n            [list of weights],\n            [list of ranks],\n            target_replacement_modules\n        )\n    }\n    \"\"\"\n    loras = {}\n    metadata = safeloras.metadata()\n\n    get_name = lambda k: k.split(\":\")[0]\n\n    keys = list(safeloras.keys())\n    keys.sort(key=get_name)\n\n    for name, module_keys in groupby(keys, get_name):\n        info = metadata.get(name)\n\n        if not info:\n            raise ValueError(\n                f\"Tensor {name} has no metadata - is this a Lora safetensor?\"\n            )\n\n        # Skip Textual Inversion embeds\n        if info == EMBED_FLAG:\n            continue\n\n        # Handle Loras\n        # Extract the targets\n        target = json.loads(info)\n\n        # Build the result lists - Python needs us to preallocate lists to insert into them\n        module_keys = list(module_keys)\n        ranks = [4] * (len(module_keys) // 2)\n        weights = [None] * len(module_keys)\n\n        for key in module_keys:\n            # Split the model name and index out of the key\n            _, idx, direction = key.split(\":\")\n            idx = int(idx)\n\n            # Add the rank\n            ranks[idx] = int(metadata[f\"{name}:{idx}:rank\"])\n\n            # Insert the weight into the list\n            idx = idx * 2 + (1 if direction == \"down\" else 0)\n            weights[idx] = nn.parameter.Parameter(safeloras.get_tensor(key))\n\n        loras[name] = (weights, ranks, target)\n\n    return loras", "\n\ndef parse_safeloras_embeds(\n    safeloras,\n) -> Dict[str, torch.Tensor]:\n    \"\"\"\n    Converts a loaded safetensor file that contains Textual Inversion embeds into\n    a dictionary of embed_token: Tensor\n    \"\"\"\n    embeds = {}\n    metadata = safeloras.metadata()\n\n    for key in safeloras.keys():\n        # Only handle Textual Inversion embeds\n        meta = metadata.get(key)\n        if not meta or meta != EMBED_FLAG:\n            continue\n\n        embeds[key] = safeloras.get_tensor(key)\n\n    return embeds", "\ndef net_load_lora(net, checkpoint_path, alpha=1.0, remove=False):\n    visited=[]\n    state_dict = torch.load(checkpoint_path)\n    for k, v in state_dict.items():\n        state_dict[k] = v.to(net.device)\n\n    for key in state_dict:\n        if \".alpha\" in key or key in visited:\n            continue\n        layer_infos = key.split(\".\")[:-2] # remove lora_up and down weight\n        curr_layer = net\n        # find the target layer\n        temp_name = layer_infos.pop(0)\n        while len(layer_infos) > -1:\n            curr_layer = curr_layer.__getattr__(temp_name)\n            if len(layer_infos) > 0:\n                temp_name = layer_infos.pop(0)\n            elif len(layer_infos) == 0:\n                break\n        if curr_layer.__class__ not in [nn.Linear, nn.Conv2d]:\n            print('missing param at:', key)\n            continue\n        pair_keys = []\n        if \"lora_down\" in key:\n            pair_keys.append(key.replace(\"lora_down\", \"lora_up\"))\n            pair_keys.append(key)\n        else:\n            pair_keys.append(key)\n            pair_keys.append(key.replace(\"lora_up\", \"lora_down\"))\n\n        # update weight\n        if len(state_dict[pair_keys[0]].shape) == 4:\n            # for conv\n            weight_up = state_dict[pair_keys[0]].squeeze(3).squeeze(2).to(torch.float32)\n            weight_down = state_dict[pair_keys[1]].squeeze(3).squeeze(2).to(torch.float32)\n            if remove:\n                curr_layer.weight.data -= alpha * torch.mm(weight_up, weight_down).unsqueeze(2).unsqueeze(3)\n            else:\n                curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down).unsqueeze(2).unsqueeze(3)\n        else:\n            # for linear\n            weight_up = state_dict[pair_keys[0]].to(torch.float32)\n            weight_down = state_dict[pair_keys[1]].to(torch.float32)\n            if remove:\n                curr_layer.weight.data -= alpha * torch.mm(weight_up, weight_down)\n            else:\n                curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down)\n\n        # update visited list\n        for item in pair_keys:\n            visited.append(item)\n    print('load_weight_num:',len(visited))\n    return ", "\ndef change_lora(model, inject_lora=False, lora_scale=1.0, lora_path='', last_time_lora='', last_time_lora_scale=1.0):\n    # remove lora\n    if last_time_lora != '':\n        net_load_lora(model, last_time_lora, alpha=last_time_lora_scale, remove=True)\n    # add new lora\n    if inject_lora:\n        net_load_lora(model, lora_path, alpha=lora_scale)\n\n\ndef net_load_lora_v2(net, checkpoint_path, alpha=1.0, remove=False, origin_weight=None):\n    visited=[]\n    state_dict = torch.load(checkpoint_path)\n    for k, v in state_dict.items():\n        state_dict[k] = v.to(net.device)\n\n    for key in state_dict:\n        if \".alpha\" in key or key in visited:\n            continue\n        layer_infos = key.split(\".\")[:-2] # remove lora_up and down weight\n        curr_layer = net\n        # find the target layer\n        temp_name = layer_infos.pop(0)\n        while len(layer_infos) > -1:\n            curr_layer = curr_layer.__getattr__(temp_name)\n            if len(layer_infos) > 0:\n                temp_name = layer_infos.pop(0)\n            elif len(layer_infos) == 0:\n                break\n        if curr_layer.__class__ not in [nn.Linear, nn.Conv2d]:\n            print('missing param at:', key)\n            continue\n        pair_keys = []\n        if \"lora_down\" in key:\n            pair_keys.append(key.replace(\"lora_down\", \"lora_up\"))\n            pair_keys.append(key)\n        else:\n            pair_keys.append(key)\n            pair_keys.append(key.replace(\"lora_up\", \"lora_down\"))\n\n        # storage weight\n        if origin_weight is None:\n            origin_weight = dict()\n            storage_key = key.replace(\"lora_down\", \"lora\").replace(\"lora_up\", \"lora\")\n            origin_weight[storage_key] = curr_layer.weight.data.clone()\n        else:\n            storage_key = key.replace(\"lora_down\", \"lora\").replace(\"lora_up\", \"lora\")\n            if storage_key not in origin_weight.keys():\n                origin_weight[storage_key] = curr_layer.weight.data.clone()\n\n\n        # update \n        if len(state_dict[pair_keys[0]].shape) == 4:\n            # for conv\n            if remove:\n                curr_layer.weight.data = origin_weight[storage_key].clone()\n            else:\n                weight_up = state_dict[pair_keys[0]].squeeze(3).squeeze(2).to(torch.float32)\n                weight_down = state_dict[pair_keys[1]].squeeze(3).squeeze(2).to(torch.float32)\n                curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down).unsqueeze(2).unsqueeze(3)\n        else:\n            # for linear\n            if remove:\n                curr_layer.weight.data = origin_weight[storage_key].clone()\n            else:\n                weight_up = state_dict[pair_keys[0]].to(torch.float32)\n                weight_down = state_dict[pair_keys[1]].to(torch.float32)\n                curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down)\n\n        # update visited list\n        for item in pair_keys:\n            visited.append(item)\n    print('load_weight_num:',len(visited))\n    return origin_weight", "\n\ndef net_load_lora_v2(net, checkpoint_path, alpha=1.0, remove=False, origin_weight=None):\n    visited=[]\n    state_dict = torch.load(checkpoint_path)\n    for k, v in state_dict.items():\n        state_dict[k] = v.to(net.device)\n\n    for key in state_dict:\n        if \".alpha\" in key or key in visited:\n            continue\n        layer_infos = key.split(\".\")[:-2] # remove lora_up and down weight\n        curr_layer = net\n        # find the target layer\n        temp_name = layer_infos.pop(0)\n        while len(layer_infos) > -1:\n            curr_layer = curr_layer.__getattr__(temp_name)\n            if len(layer_infos) > 0:\n                temp_name = layer_infos.pop(0)\n            elif len(layer_infos) == 0:\n                break\n        if curr_layer.__class__ not in [nn.Linear, nn.Conv2d]:\n            print('missing param at:', key)\n            continue\n        pair_keys = []\n        if \"lora_down\" in key:\n            pair_keys.append(key.replace(\"lora_down\", \"lora_up\"))\n            pair_keys.append(key)\n        else:\n            pair_keys.append(key)\n            pair_keys.append(key.replace(\"lora_up\", \"lora_down\"))\n\n        # storage weight\n        if origin_weight is None:\n            origin_weight = dict()\n            storage_key = key.replace(\"lora_down\", \"lora\").replace(\"lora_up\", \"lora\")\n            origin_weight[storage_key] = curr_layer.weight.data.clone()\n        else:\n            storage_key = key.replace(\"lora_down\", \"lora\").replace(\"lora_up\", \"lora\")\n            if storage_key not in origin_weight.keys():\n                origin_weight[storage_key] = curr_layer.weight.data.clone()\n\n\n        # update \n        if len(state_dict[pair_keys[0]].shape) == 4:\n            # for conv\n            if remove:\n                curr_layer.weight.data = origin_weight[storage_key].clone()\n            else:\n                weight_up = state_dict[pair_keys[0]].squeeze(3).squeeze(2).to(torch.float32)\n                weight_down = state_dict[pair_keys[1]].squeeze(3).squeeze(2).to(torch.float32)\n                curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down).unsqueeze(2).unsqueeze(3)\n        else:\n            # for linear\n            if remove:\n                curr_layer.weight.data = origin_weight[storage_key].clone()\n            else:\n                weight_up = state_dict[pair_keys[0]].to(torch.float32)\n                weight_down = state_dict[pair_keys[1]].to(torch.float32)\n                curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down)\n\n        # update visited list\n        for item in pair_keys:\n            visited.append(item)\n    print('load_weight_num:',len(visited))\n    return origin_weight", "\ndef change_lora_v2(model, inject_lora=False, lora_scale=1.0, lora_path='', last_time_lora='', last_time_lora_scale=1.0, origin_weight=None):\n    # remove lora\n    if last_time_lora != '':\n        origin_weight = net_load_lora_v2(model, last_time_lora, alpha=last_time_lora_scale, remove=True, origin_weight=origin_weight)\n    # add new lora\n    if inject_lora:\n        origin_weight = net_load_lora_v2(model, lora_path, alpha=lora_scale, origin_weight=origin_weight)\n    return origin_weight\n", "\n\n\n\n\ndef load_safeloras(path, device=\"cpu\"):\n    safeloras = safe_open(path, framework=\"pt\", device=device)\n    return parse_safeloras(safeloras)\n\n\ndef load_safeloras_embeds(path, device=\"cpu\"):\n    safeloras = safe_open(path, framework=\"pt\", device=device)\n    return parse_safeloras_embeds(safeloras)", "\n\ndef load_safeloras_embeds(path, device=\"cpu\"):\n    safeloras = safe_open(path, framework=\"pt\", device=device)\n    return parse_safeloras_embeds(safeloras)\n\n\ndef load_safeloras_both(path, device=\"cpu\"):\n    safeloras = safe_open(path, framework=\"pt\", device=device)\n    return parse_safeloras(safeloras), parse_safeloras_embeds(safeloras)", "\n\ndef collapse_lora(model, alpha=1.0):\n\n    for _module, name, _child_module in _find_modules(\n        model,\n        UNET_EXTENDED_TARGET_REPLACE | TEXT_ENCODER_EXTENDED_TARGET_REPLACE,\n        search_class=[LoraInjectedLinear, LoraInjectedConv2d],\n    ):\n\n        if isinstance(_child_module, LoraInjectedLinear):\n            print(\"Collapsing Lin Lora in\", name)\n\n            _child_module.linear.weight = nn.Parameter(\n                _child_module.linear.weight.data\n                + alpha\n                * (\n                    _child_module.lora_up.weight.data\n                    @ _child_module.lora_down.weight.data\n                )\n                .type(_child_module.linear.weight.dtype)\n                .to(_child_module.linear.weight.device)\n            )\n\n        else:\n            print(\"Collapsing Conv Lora in\", name)\n            _child_module.conv.weight = nn.Parameter(\n                _child_module.conv.weight.data\n                + alpha\n                * (\n                    _child_module.lora_up.weight.data.flatten(start_dim=1)\n                    @ _child_module.lora_down.weight.data.flatten(start_dim=1)\n                )\n                .reshape(_child_module.conv.weight.data.shape)\n                .type(_child_module.conv.weight.dtype)\n                .to(_child_module.conv.weight.device)\n            )", "\n\ndef monkeypatch_or_replace_lora(\n    model,\n    loras,\n    target_replace_module=DEFAULT_TARGET_REPLACE,\n    r: Union[int, List[int]] = 4,\n):\n    for _module, name, _child_module in _find_modules(\n        model, target_replace_module, search_class=[nn.Linear, LoraInjectedLinear]\n    ):\n        _source = (\n            _child_module.linear\n            if isinstance(_child_module, LoraInjectedLinear)\n            else _child_module\n        )\n\n        weight = _source.weight\n        bias = _source.bias\n        _tmp = LoraInjectedLinear(\n            _source.in_features,\n            _source.out_features,\n            _source.bias is not None,\n            r=r.pop(0) if isinstance(r, list) else r,\n        )\n        _tmp.linear.weight = weight\n\n        if bias is not None:\n            _tmp.linear.bias = bias\n\n        # switch the module\n        _module._modules[name] = _tmp\n\n        up_weight = loras.pop(0)\n        down_weight = loras.pop(0)\n\n        _module._modules[name].lora_up.weight = nn.Parameter(\n            up_weight.type(weight.dtype)\n        )\n        _module._modules[name].lora_down.weight = nn.Parameter(\n            down_weight.type(weight.dtype)\n        )\n\n        _module._modules[name].to(weight.device)", "\n\ndef monkeypatch_or_replace_lora_extended(\n    model,\n    loras,\n    target_replace_module=DEFAULT_TARGET_REPLACE,\n    r: Union[int, List[int]] = 4,\n):\n    for _module, name, _child_module in _find_modules(\n        model,\n        target_replace_module,\n        search_class=[nn.Linear, LoraInjectedLinear, nn.Conv2d, LoraInjectedConv2d],\n    ):\n\n        if (_child_module.__class__ == nn.Linear) or (\n            _child_module.__class__ == LoraInjectedLinear\n        ):\n            if len(loras[0].shape) != 2:\n                continue\n\n            _source = (\n                _child_module.linear\n                if isinstance(_child_module, LoraInjectedLinear)\n                else _child_module\n            )\n\n            weight = _source.weight\n            bias = _source.bias\n            _tmp = LoraInjectedLinear(\n                _source.in_features,\n                _source.out_features,\n                _source.bias is not None,\n                r=r.pop(0) if isinstance(r, list) else r,\n            )\n            _tmp.linear.weight = weight\n\n            if bias is not None:\n                _tmp.linear.bias = bias\n\n        elif (_child_module.__class__ == nn.Conv2d) or (\n            _child_module.__class__ == LoraInjectedConv2d\n        ):\n            if len(loras[0].shape) != 4:\n                continue\n            _source = (\n                _child_module.conv\n                if isinstance(_child_module, LoraInjectedConv2d)\n                else _child_module\n            )\n\n            weight = _source.weight\n            bias = _source.bias\n            _tmp = LoraInjectedConv2d(\n                _source.in_channels,\n                _source.out_channels,\n                _source.kernel_size,\n                _source.stride,\n                _source.padding,\n                _source.dilation,\n                _source.groups,\n                _source.bias is not None,\n                r=r.pop(0) if isinstance(r, list) else r,\n            )\n\n            _tmp.conv.weight = weight\n\n            if bias is not None:\n                _tmp.conv.bias = bias\n\n        # switch the module\n        _module._modules[name] = _tmp\n\n        up_weight = loras.pop(0)\n        down_weight = loras.pop(0)\n\n        _module._modules[name].lora_up.weight = nn.Parameter(\n            up_weight.type(weight.dtype)\n        )\n        _module._modules[name].lora_down.weight = nn.Parameter(\n            down_weight.type(weight.dtype)\n        )\n\n        _module._modules[name].to(weight.device)", "\n\ndef monkeypatch_or_replace_safeloras(models, safeloras):\n    loras = parse_safeloras(safeloras)\n\n    for name, (lora, ranks, target) in loras.items():\n        model = getattr(models, name, None)\n\n        if not model:\n            print(f\"No model provided for {name}, contained in Lora\")\n            continue\n\n        monkeypatch_or_replace_lora_extended(model, lora, target, ranks)", "\n\ndef monkeypatch_remove_lora(model):\n    for _module, name, _child_module in _find_modules(\n        model, search_class=[LoraInjectedLinear, LoraInjectedConv2d]\n    ):\n        if isinstance(_child_module, LoraInjectedLinear):\n            _source = _child_module.linear\n            weight, bias = _source.weight, _source.bias\n\n            _tmp = nn.Linear(\n                _source.in_features, _source.out_features, bias is not None\n            )\n\n            _tmp.weight = weight\n            if bias is not None:\n                _tmp.bias = bias\n\n        else:\n            _source = _child_module.conv\n            weight, bias = _source.weight, _source.bias\n\n            _tmp = nn.Conv2d(\n                in_channels=_source.in_channels,\n                out_channels=_source.out_channels,\n                kernel_size=_source.kernel_size,\n                stride=_source.stride,\n                padding=_source.padding,\n                dilation=_source.dilation,\n                groups=_source.groups,\n                bias=bias is not None,\n            )\n\n            _tmp.weight = weight\n            if bias is not None:\n                _tmp.bias = bias\n\n        _module._modules[name] = _tmp", "\n\ndef monkeypatch_add_lora(\n    model,\n    loras,\n    target_replace_module=DEFAULT_TARGET_REPLACE,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n):\n    for _module, name, _child_module in _find_modules(\n        model, target_replace_module, search_class=[LoraInjectedLinear]\n    ):\n        weight = _child_module.linear.weight\n\n        up_weight = loras.pop(0)\n        down_weight = loras.pop(0)\n\n        _module._modules[name].lora_up.weight = nn.Parameter(\n            up_weight.type(weight.dtype).to(weight.device) * alpha\n            + _module._modules[name].lora_up.weight.to(weight.device) * beta\n        )\n        _module._modules[name].lora_down.weight = nn.Parameter(\n            down_weight.type(weight.dtype).to(weight.device) * alpha\n            + _module._modules[name].lora_down.weight.to(weight.device) * beta\n        )\n\n        _module._modules[name].to(weight.device)", "\n\ndef tune_lora_scale(model, alpha: float = 1.0):\n    for _module in model.modules():\n        if _module.__class__.__name__ in [\"LoraInjectedLinear\", \"LoraInjectedConv2d\"]:\n            _module.scale = alpha\n\n\ndef set_lora_diag(model, diag: torch.Tensor):\n    for _module in model.modules():\n        if _module.__class__.__name__ in [\"LoraInjectedLinear\", \"LoraInjectedConv2d\"]:\n            _module.set_selector_from_diag(diag)", "def set_lora_diag(model, diag: torch.Tensor):\n    for _module in model.modules():\n        if _module.__class__.__name__ in [\"LoraInjectedLinear\", \"LoraInjectedConv2d\"]:\n            _module.set_selector_from_diag(diag)\n\n\ndef _text_lora_path(path: str) -> str:\n    assert path.endswith(\".pt\"), \"Only .pt files are supported\"\n    return \".\".join(path.split(\".\")[:-1] + [\"text_encoder\", \"pt\"])\n", "\n\ndef _ti_lora_path(path: str) -> str:\n    assert path.endswith(\".pt\"), \"Only .pt files are supported\"\n    return \".\".join(path.split(\".\")[:-1] + [\"ti\", \"pt\"])\n\n\ndef apply_learned_embed_in_clip(\n    learned_embeds,\n    text_encoder,\n    tokenizer,\n    token: Optional[Union[str, List[str]]] = None,\n    idempotent=False,\n):\n    if isinstance(token, str):\n        trained_tokens = [token]\n    elif isinstance(token, list):\n        assert len(learned_embeds.keys()) == len(\n            token\n        ), \"The number of tokens and the number of embeds should be the same\"\n        trained_tokens = token\n    else:\n        trained_tokens = list(learned_embeds.keys())\n\n    for token in trained_tokens:\n        print(token)\n        embeds = learned_embeds[token]\n\n        # cast to dtype of text_encoder\n        dtype = text_encoder.get_input_embeddings().weight.dtype\n        num_added_tokens = tokenizer.add_tokens(token)\n\n        i = 1\n        if not idempotent:\n            while num_added_tokens == 0:\n                print(f\"The tokenizer already contains the token {token}.\")\n                token = f\"{token[:-1]}-{i}>\"\n                print(f\"Attempting to add the token {token}.\")\n                num_added_tokens = tokenizer.add_tokens(token)\n                i += 1\n        elif num_added_tokens == 0 and idempotent:\n            print(f\"The tokenizer already contains the token {token}.\")\n            print(f\"Replacing {token} embedding.\")\n\n        # resize the token embeddings\n        text_encoder.resize_token_embeddings(len(tokenizer))\n\n        # get the id for the token and assign the embeds\n        token_id = tokenizer.convert_tokens_to_ids(token)\n        text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n    return token", "\n\ndef load_learned_embed_in_clip(\n    learned_embeds_path,\n    text_encoder,\n    tokenizer,\n    token: Optional[Union[str, List[str]]] = None,\n    idempotent=False,\n):\n    learned_embeds = torch.load(learned_embeds_path)\n    apply_learned_embed_in_clip(\n        learned_embeds, text_encoder, tokenizer, token, idempotent\n    )", "\n\ndef patch_pipe(\n    pipe,\n    maybe_unet_path,\n    token: Optional[str] = None,\n    r: int = 4,\n    patch_unet=True,\n    patch_text=True,\n    patch_ti=True,\n    idempotent_token=True,\n    unet_target_replace_module=DEFAULT_TARGET_REPLACE,\n    text_target_replace_module=TEXT_ENCODER_DEFAULT_TARGET_REPLACE,\n):\n    if maybe_unet_path.endswith(\".pt\"):\n        # torch format\n\n        if maybe_unet_path.endswith(\".ti.pt\"):\n            unet_path = maybe_unet_path[:-6] + \".pt\"\n        elif maybe_unet_path.endswith(\".text_encoder.pt\"):\n            unet_path = maybe_unet_path[:-16] + \".pt\"\n        else:\n            unet_path = maybe_unet_path\n\n        ti_path = _ti_lora_path(unet_path)\n        text_path = _text_lora_path(unet_path)\n\n        if patch_unet:\n            print(\"LoRA : Patching Unet\")\n            monkeypatch_or_replace_lora(\n                pipe.unet,\n                torch.load(unet_path),\n                r=r,\n                target_replace_module=unet_target_replace_module,\n            )\n\n        if patch_text:\n            print(\"LoRA : Patching text encoder\")\n            monkeypatch_or_replace_lora(\n                pipe.text_encoder,\n                torch.load(text_path),\n                target_replace_module=text_target_replace_module,\n                r=r,\n            )\n        if patch_ti:\n            print(\"LoRA : Patching token input\")\n            token = load_learned_embed_in_clip(\n                ti_path,\n                pipe.text_encoder,\n                pipe.tokenizer,\n                token=token,\n                idempotent=idempotent_token,\n            )\n\n    elif maybe_unet_path.endswith(\".safetensors\"):\n        safeloras = safe_open(maybe_unet_path, framework=\"pt\", device=\"cpu\")\n        monkeypatch_or_replace_safeloras(pipe, safeloras)\n        tok_dict = parse_safeloras_embeds(safeloras)\n        if patch_ti:\n            apply_learned_embed_in_clip(\n                tok_dict,\n                pipe.text_encoder,\n                pipe.tokenizer,\n                token=token,\n                idempotent=idempotent_token,\n            )\n        return tok_dict", "\n\n@torch.no_grad()\ndef inspect_lora(model):\n    moved = {}\n\n    for name, _module in model.named_modules():\n        if _module.__class__.__name__ in [\"LoraInjectedLinear\", \"LoraInjectedConv2d\"]:\n            ups = _module.lora_up.weight.data.clone()\n            downs = _module.lora_down.weight.data.clone()\n\n            wght: torch.Tensor = ups.flatten(1) @ downs.flatten(1)\n\n            dist = wght.flatten().abs().mean().item()\n            if name in moved:\n                moved[name].append(dist)\n            else:\n                moved[name] = [dist]\n\n    return moved", "\n\ndef save_all(\n    unet,\n    text_encoder,\n    save_path,\n    placeholder_token_ids=None,\n    placeholder_tokens=None,\n    save_lora=True,\n    save_ti=True,\n    target_replace_module_text=TEXT_ENCODER_DEFAULT_TARGET_REPLACE,\n    target_replace_module_unet=DEFAULT_TARGET_REPLACE,\n    safe_form=True,\n):\n    if not safe_form:\n        # save ti\n        if save_ti:\n            ti_path = _ti_lora_path(save_path)\n            learned_embeds_dict = {}\n            for tok, tok_id in zip(placeholder_tokens, placeholder_token_ids):\n                learned_embeds = text_encoder.get_input_embeddings().weight[tok_id]\n                print(\n                    f\"Current Learned Embeddings for {tok}:, id {tok_id} \",\n                    learned_embeds[:4],\n                )\n                learned_embeds_dict[tok] = learned_embeds.detach().cpu()\n\n            torch.save(learned_embeds_dict, ti_path)\n            print(\"Ti saved to \", ti_path)\n\n        # save text encoder\n        if save_lora:\n\n            save_lora_weight(\n                unet, save_path, target_replace_module=target_replace_module_unet\n            )\n            print(\"Unet saved to \", save_path)\n\n            save_lora_weight(\n                text_encoder,\n                _text_lora_path(save_path),\n                target_replace_module=target_replace_module_text,\n            )\n            print(\"Text Encoder saved to \", _text_lora_path(save_path))\n\n    else:\n        assert save_path.endswith(\n            \".safetensors\"\n        ), f\"Save path : {save_path} should end with .safetensors\"\n\n        loras = {}\n        embeds = {}\n\n        if save_lora:\n\n            loras[\"unet\"] = (unet, target_replace_module_unet)\n            loras[\"text_encoder\"] = (text_encoder, target_replace_module_text)\n\n        if save_ti:\n            for tok, tok_id in zip(placeholder_tokens, placeholder_token_ids):\n                learned_embeds = text_encoder.get_input_embeddings().weight[tok_id]\n                print(\n                    f\"Current Learned Embeddings for {tok}:, id {tok_id} \",\n                    learned_embeds[:4],\n                )\n                embeds[tok] = learned_embeds.detach().cpu()\n\n        save_safeloras_with_embeds(loras, embeds, save_path)", ""]}
{"filename": "scripts/videocrafter/lvdm/models/modules/distributions.py", "chunked_list": ["import torch\nimport numpy as np\n\n\nclass DiagonalGaussianDistribution(object):\n    def __init__(self, parameters, deterministic=False):\n        self.parameters = parameters\n        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n        self.deterministic = deterministic\n        self.std = torch.exp(0.5 * self.logvar)\n        self.var = torch.exp(self.logvar)\n        if self.deterministic:\n            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n\n    def sample(self, noise=None):\n        if noise is None:\n            noise = torch.randn(self.mean.shape)\n        \n        x = self.mean + self.std * noise.to(device=self.parameters.device)\n        return x\n\n    def kl(self, other=None):\n        if self.deterministic:\n            return torch.Tensor([0.])\n        else:\n            if other is None:\n                return 0.5 * torch.sum(torch.pow(self.mean, 2)\n                                       + self.var - 1.0 - self.logvar,\n                                       dim=[1, 2, 3])\n            else:\n                return 0.5 * torch.sum(\n                    torch.pow(self.mean - other.mean, 2) / other.var\n                    + self.var / other.var - 1.0 - self.logvar + other.logvar,\n                    dim=[1, 2, 3])\n\n    def nll(self, sample, dims=[1,2,3]):\n        if self.deterministic:\n            return torch.Tensor([0.])\n        logtwopi = np.log(2.0 * np.pi)\n        return 0.5 * torch.sum(\n            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n            dim=dims)\n\n    def mode(self):\n        return self.mean", "\n\ndef normal_kl(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12\n    Compute the KL divergence between two gaussians.\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, torch.Tensor):\n            tensor = obj\n            break\n    assert tensor is not None, \"at least one argument must be a Tensor\"\n\n    # Force variances to be Tensors. Broadcasting helps convert scalars to\n    # Tensors, but it does not work for torch.exp().\n    logvar1, logvar2 = [\n        x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor)\n        for x in (logvar1, logvar2)\n    ]\n\n    return 0.5 * (\n        -1.0\n        + logvar2\n        - logvar1\n        + torch.exp(logvar1 - logvar2)\n        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n    )", ""]}
{"filename": "scripts/videocrafter/lvdm/models/modules/util.py", "chunked_list": ["import math\nfrom inspect import isfunction\n\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom einops import repeat\nimport torch.nn.functional as F\n\nfrom videocrafter.lvdm.utils.common_utils import instantiate_from_config", "\nfrom videocrafter.lvdm.utils.common_utils import instantiate_from_config\n\n\ndef make_beta_schedule(schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n    if schedule == \"linear\":\n        betas = (\n                torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n        )\n    elif schedule == \"cosine\":\n        timesteps = (\n                torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s\n        )\n        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n        alphas = torch.cos(alphas).pow(2)\n        alphas = alphas / alphas[0]\n        betas = 1 - alphas[1:] / alphas[:-1]\n        betas = np.clip(betas, a_min=0, a_max=0.999)\n    elif schedule == \"sqrt_linear\":\n        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64)\n    elif schedule == \"sqrt\":\n        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64) ** 0.5\n    else:\n        raise ValueError(f\"schedule '{schedule}' unknown.\")\n    return betas.numpy()", "\n\ndef make_ddim_timesteps(ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=True):\n    if ddim_discr_method == 'uniform':\n        c = num_ddpm_timesteps // num_ddim_timesteps\n        ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n    elif ddim_discr_method == 'quad':\n        ddim_timesteps = ((np.linspace(0, np.sqrt(num_ddpm_timesteps * .8), num_ddim_timesteps)) ** 2).astype(int)\n    else:\n        raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n\n    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n    steps_out = ddim_timesteps + 1\n    if verbose:\n        print(f'Selected timesteps for ddim sampler: {steps_out}')\n    return steps_out", "\n\ndef make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta, verbose=True):\n    # select alphas for computing the variance schedule\n    alphas = alphacums[ddim_timesteps]\n    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n\n    # according the the formula provided in https://arxiv.org/abs/2010.02502\n    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n    if verbose:\n        print(f'Selected alphas for ddim sampler: a_t: {alphas}; a_(t-1): {alphas_prev}')\n        print(f'For the chosen value of eta, which is {eta}, '\n              f'this results in the following sigma_t schedule for ddim sampler {sigmas}')\n    return sigmas, alphas, alphas_prev", "\n\ndef betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)", "\n\ndef extract_into_tensor(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n\n\ndef checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"\n    if flag:\n        args = tuple(inputs) + tuple(params)\n        return CheckpointFunction.apply(func, len(inputs), *args)\n    else:\n        return func(*inputs)", "def checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"\n    if flag:\n        args = tuple(inputs) + tuple(params)\n        return CheckpointFunction.apply(func, len(inputs), *args)\n    else:\n        return func(*inputs)", "\n\nclass CheckpointFunction(torch.autograd.Function):\n    @staticmethod\n    @torch.cuda.amp.custom_fwd\n    def forward(ctx, run_function, length, *args):\n        ctx.run_function = run_function\n        ctx.input_tensors = list(args[:length])\n        ctx.input_params = list(args[length:])\n\n        with torch.no_grad():\n            output_tensors = ctx.run_function(*ctx.input_tensors)\n        return output_tensors\n\n    @staticmethod\n    @torch.cuda.amp.custom_bwd\n    def backward(ctx, *output_grads):\n        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n        with torch.enable_grad():\n            # Fixes a bug where the first op in run_function modifies the\n            # Tensor storage in place, which is not allowed for detach()'d\n            # Tensors.\n            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n            output_tensors = ctx.run_function(*shallow_copies)\n        input_grads = torch.autograd.grad(\n            output_tensors,\n            ctx.input_tensors + ctx.input_params,\n            output_grads,\n            allow_unused=True,\n        )\n        del ctx.input_tensors\n        del ctx.input_params\n        del output_tensors\n        return (None, None) + input_grads", "\n\ndef timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    if not repeat_only:\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n        ).to(device=timesteps.device)\n        args = timesteps[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    else:\n        embedding = repeat(timesteps, 'b -> b d', d=dim)\n    return embedding", "\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n", "\n\ndef scale_module(module, scale):\n    \"\"\"\n    Scale the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().mul_(scale)\n    return module\n", "\n\ndef mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n\n\ndef normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, channels)", "\ndef normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, channels)\n\ndef Normalize(in_channels):\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)", "\ndef Normalize(in_channels):\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n\ndef identity(*args, **kwargs):\n    return nn.Identity()\n\nclass Normalization(nn.Module):\n    def __init__(self, output_size, eps=1e-5, norm_type='gn'):\n        super(Normalization, self).__init__()\n        # epsilon to avoid dividing by 0\n        self.eps = eps\n        self.norm_type = norm_type\n\n        if self.norm_type in ['bn', 'in']:\n            self.register_buffer('stored_mean', torch.zeros(output_size))\n            self.register_buffer('stored_var', torch.ones(output_size))\n    \n    def forward(self, x):\n        if self.norm_type == 'bn':\n            out = F.batch_norm(x, self.stored_mean, self.stored_var, None,\n                                None,\n                                self.training, 0.1, self.eps)\n        elif self.norm_type == 'in':\n            out = F.instance_norm(x, self.stored_mean, self.stored_var,\n                                    None, None,\n                                    self.training, 0.1, self.eps)\n        elif self.norm_type == 'gn':\n            out = F.group_norm(x, 32)\n        elif self.norm_type == 'nonorm':\n            out = x\n        return out", "\n\nclass CCNormalization(nn.Module):\n    def __init__(self, embed_dim, feature_dim, *args, **kwargs):\n        super(CCNormalization, self).__init__()\n\n        self.embed_dim = embed_dim\n        self.feature_dim = feature_dim\n        self.norm = Normalization(feature_dim, *args, **kwargs)\n        \n        self.gain = nn.Linear(self.embed_dim, self.feature_dim)\n        self.bias = nn.Linear(self.embed_dim, self.feature_dim)\n        \n    def forward(self, x, y):\n        shape = [1] * (x.dim() - 2)\n        gain = (1 + self.gain(y)).view(y.size(0), -1, *shape)\n        bias = self.bias(y).view(y.size(0), -1, *shape)\n        return self.norm(x) * gain + bias", "\n\ndef nonlinearity(type='silu'):\n    if type == 'silu':\n        return nn.SiLU()\n    elif type == 'leaky_relu':\n        return nn.LeakyReLU()\n\n\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * F.gelu(gate)", "\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * F.gelu(gate)\n", "\n\nclass SiLU(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\nclass GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)", "\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")", "\n\ndef linear(*args, **kwargs):\n    \"\"\"\n    Create a linear module.\n    \"\"\"\n    return nn.Linear(*args, **kwargs)\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")", "\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")", "\n\nclass HybridConditioner(nn.Module):\n\n    def __init__(self, c_concat_config, c_crossattn_config):\n        super().__init__()\n        self.concat_conditioner = instantiate_from_config(c_concat_config)\n        self.crossattn_conditioner = instantiate_from_config(c_crossattn_config)\n\n    def forward(self, c_concat, c_crossattn):\n        c_concat = self.concat_conditioner(c_concat)\n        c_crossattn = self.crossattn_conditioner(c_crossattn)\n        return {'c_concat': [c_concat], 'c_crossattn': [c_crossattn]}", "\ndef noise_like(shape, device, repeat=False, noise_gen=None):\n    assert noise_gen is not None\n    repeat_noise = lambda: torch.randn((1, *shape[1:]), generator=noise_gen).repeat(shape[0], *((1,) * (len(shape) - 1))).to(device)\n    noise = lambda: torch.randn(shape, generator=noise_gen).to(device)\n    return repeat_noise() if repeat else noise()\n\ndef init_(tensor):\n    dim = tensor.shape[-1]\n    std = 1 / math.sqrt(dim)\n    tensor.uniform_(-std, std)\n    return tensor", "\n\ndef exists(val):\n    return val is not None\n\n\ndef uniq(arr):\n    return{el: True for el in arr}.keys()\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d", "\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\n", ""]}
{"filename": "scripts/videocrafter/lvdm/models/modules/attention_temporal.py", "chunked_list": ["from typing import Optional, Any\n\nimport torch\nimport torch as th\nfrom torch import nn, einsum\nfrom einops import rearrange, repeat\ntry:\n    import xformers\n    import xformers.ops\n    XFORMERS_IS_AVAILBLE = True\nexcept:\n    XFORMERS_IS_AVAILBLE = False", "\nfrom videocrafter.lvdm.models.modules.util import (\n    GEGLU,\n    exists,\n    default,\n    Normalize,\n    checkpoint,\n    zero_module,\n)\n", ")\n\n\n# ---------------------------------------------------------------------------------------------------\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = nn.Sequential(\n            nn.Linear(dim, inner_dim),\n            nn.GELU()\n        ) if not glu else GEGLU(dim, inner_dim)\n\n        self.net = nn.Sequential(\n            project_in,\n            nn.Dropout(dropout),\n            nn.Linear(inner_dim, dim_out)\n        )\n\n    def forward(self, x):\n        return self.net(x)", "\n\n# ---------------------------------------------------------------------------------------------------\nclass RelativePosition(nn.Module):\n    \"\"\" https://github.com/evelinehong/Transformer_Relative_Position_PyTorch/blob/master/relative_position.py \"\"\"\n\n    def __init__(self, num_units, max_relative_position):\n        super().__init__()\n        self.num_units = num_units\n        self.max_relative_position = max_relative_position\n        self.embeddings_table = nn.Parameter(th.Tensor(max_relative_position * 2 + 1, num_units))\n        nn.init.xavier_uniform_(self.embeddings_table)\n\n    def forward(self, length_q, length_k):\n        device = self.embeddings_table.device\n        range_vec_q = th.arange(length_q, device=device)\n        range_vec_k = th.arange(length_k, device=device)\n        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n        distance_mat_clipped = th.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n        final_mat = distance_mat_clipped + self.max_relative_position\n        final_mat = final_mat.long()\n        embeddings = self.embeddings_table[final_mat]\n        return embeddings", "\n\n# ---------------------------------------------------------------------------------------------------\nclass TemporalCrossAttention(nn.Module):\n    def __init__(self, \n        query_dim, \n        context_dim=None, \n        heads=8, \n        dim_head=64, \n        dropout=0.,\n        use_relative_position=False,    # whether use relative positional representation in temporal attention.\n        temporal_length=None,           # relative positional representation\n        **kwargs,\n    ):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n        self.context_dim = context_dim\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        self.temporal_length = temporal_length\n        self.use_relative_position = use_relative_position\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, query_dim),\n            nn.Dropout(dropout)\n        )\n\n        if use_relative_position:\n            assert(temporal_length is not None)\n            self.relative_position_k = RelativePosition(num_units=dim_head, max_relative_position=temporal_length)\n            self.relative_position_v = RelativePosition(num_units=dim_head, max_relative_position=temporal_length)\n\n        nn.init.constant_(self.to_q.weight, 0)\n        nn.init.constant_(self.to_k.weight, 0)\n        nn.init.constant_(self.to_v.weight, 0)\n        nn.init.constant_(self.to_out[0].weight, 0)\n        nn.init.constant_(self.to_out[0].bias, 0)\n\n    def forward(self, x, context=None, mask=None):\n        nh = self.heads\n        out = x\n\n        # cal qkv\n        q = self.to_q(out)\n        context = default(context, x)\n        k = self.to_k(context)\n        v = self.to_v(context)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=nh), (q, k, v))\n        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n\n        # relative positional embedding\n        if self.use_relative_position:\n            len_q, len_k, len_v = q.shape[1], k.shape[1], v.shape[1]\n            k2 = self.relative_position_k(len_q, len_k) \n            sim2 = einsum('b t d, t s d -> b t s', q, k2) * self.scale\n            sim += sim2\n        \n        # mask attention\n        if mask is not None:\n            max_neg_value = -1e9\n            sim = sim + (1-mask.float()) * max_neg_value # 1=masking,0=no masking\n        \n        # attend to values\n        attn = sim.softmax(dim=-1)\n        out = einsum('b i j, b j d -> b i d', attn, v)\n        \n        # relative positional embedding\n        if self.use_relative_position:\n            v2 = self.relative_position_v(len_q, len_v)\n            out2 = einsum('b t s, t s d -> b t d', attn, v2)\n            out += out2\n        \n        # merge head\n        out = rearrange(out, '(b h) n d -> b n (h d)', h=nh)\n        return self.to_out(out)", "\n\n# ---------------------------------------------------------------------------------------------------\nclass CrossAttention(nn.Module):\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.,\n                 **kwargs,):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n        \n        self.scale = dim_head ** -0.5\n        self.heads = heads\n\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, query_dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x, context=None, mask=None):\n        h = self.heads\n        b = x.shape[0]\n        \n        q = self.to_q(x)\n        context = default(context, x)\n        k = self.to_k(context)\n        v = self.to_v(context)\n        \n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n\n        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n\n        if exists(mask):\n            mask = rearrange(mask, 'b ... -> b (...)')\n            max_neg_value = -torch.finfo(sim.dtype).max\n            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n            sim.masked_fill_(~mask, max_neg_value)\n\n        attn = sim.softmax(dim=-1)\n\n        out = einsum('b i j, b j d -> b i d', attn, v)\n        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n        return self.to_out(out)", "    \n\n# ---------------------------------------------------------------------------------------------------\nclass MemoryEfficientCrossAttention(nn.Module):\n    \"\"\"https://github.com/MatthieuTPHR/diffusers/blob/d80b531ff8060ec1ea982b65a1b8df70f73aa67c/src/diffusers/models/attention.py#L223\n    \"\"\"\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0,\n                 **kwargs,):\n        super().__init__()\n        print(f\"Setting up {self.__class__.__name__}. Query dim is {query_dim}, context_dim is {context_dim} and using \"\n              f\"{heads} heads.\"\n        )\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n\n        self.heads = heads\n        self.dim_head = dim_head\n\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n\n        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout))\n        self.attention_op: Optional[Any] = None\n\n    def forward(self, x, context=None, mask=None):\n        q = self.to_q(x)\n        context = default(context, x)\n        k = self.to_k(context)\n        v = self.to_v(context)\n\n        b, _, _ = q.shape\n        q, k, v = map(\n            lambda t: t.unsqueeze(3)\n            .reshape(b, t.shape[1], self.heads, self.dim_head)\n            .permute(0, 2, 1, 3)\n            .reshape(b * self.heads, t.shape[1], self.dim_head)\n            .contiguous(),\n            (q, k, v),\n        )\n        out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None, op=self.attention_op)\n\n        if exists(mask):\n            raise NotImplementedError\n        out = (\n            out.unsqueeze(0)\n            .reshape(b, self.heads, out.shape[1], self.dim_head)\n            .permute(0, 2, 1, 3)\n            .reshape(b, out.shape[1], self.heads * self.dim_head)\n        )\n        return self.to_out(out)", "\n\n# ---------------------------------------------------------------------------------------------------\nclass BasicTransformerBlockST(nn.Module):\n    \"\"\"\n    if no context is given to forward function, cross-attention defaults to self-attention\n    \"\"\"\n    def __init__(self, \n        # Spatial\n        dim, \n        n_heads, \n        d_head, \n        dropout=0., \n        context_dim=None, \n        gated_ff=True, \n        checkpoint=True,\n        # Temporal\n        temporal_length=None,   \n        use_relative_position=True,\n        **kwargs,\n    ):\n        super().__init__()\n\n        # spatial self attention (if context_dim is None) and spatial cross attention\n        if XFORMERS_IS_AVAILBLE:\n            self.attn1 = MemoryEfficientCrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout, **kwargs,)\n            self.attn2 = MemoryEfficientCrossAttention(query_dim=dim, context_dim=context_dim,\n                                    heads=n_heads, dim_head=d_head, dropout=dropout, **kwargs,)\n        else:\n            self.attn1 = CrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout, **kwargs,)\n            self.attn2 = CrossAttention(query_dim=dim, context_dim=context_dim,\n                                    heads=n_heads, dim_head=d_head, dropout=dropout, **kwargs,)\n        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\n        \n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.norm3 = nn.LayerNorm(dim)\n        self.checkpoint = checkpoint\n        \n        # Temporal attention\n        self.attn1_tmp = TemporalCrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout,\n                                                temporal_length=temporal_length,\n                                                use_relative_position=use_relative_position,\n                                                **kwargs,\n        )\n        self.attn2_tmp = TemporalCrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout,\n                                                # cross attn\n                                                context_dim=None,\n                                                # temporal attn\n                                                temporal_length=temporal_length,\n                                                use_relative_position=use_relative_position,\n                                                **kwargs,\n        )\n        self.norm4 = nn.LayerNorm(dim)\n        self.norm5 = nn.LayerNorm(dim)\n        \n    def forward(self, x, context=None, **kwargs):\n        return checkpoint(self._forward, (x, context), self.parameters(), self.checkpoint)\n        \n    def _forward(self, x, context=None, mask=None,):\n        assert(x.dim() == 5), f\"x shape = {x.shape}\"\n        b, c, t, h, w = x.shape\n        \n        # spatial self attention\n        x = rearrange(x, 'b c t h w -> (b t) (h w) c')\n        x = self.attn1(self.norm1(x)) + x\n        x = rearrange(x, '(b t) (h w) c -> b c t h w', b=b,h=h)\n        \n        # temporal self attention\n        x = rearrange(x, 'b c t h w -> (b h w) t c')\n        x = self.attn1_tmp(self.norm4(x), mask=mask) + x\n        x = rearrange(x, '(b h w) t c -> b c t h w', b=b,h=h,w=w) # 3d -> 5d\n        \n        # spatial cross attention\n        x = rearrange(x, 'b c t h w -> (b t) (h w) c')\n        if context is not None:\n            context_ = []\n            for i in range(context.shape[0]):\n                context_.append(context[i].unsqueeze(0).repeat(t, 1, 1))\n            context_ = torch.cat(context_,dim=0)\n        else:\n            context_ = None\n        x = self.attn2(self.norm2(x), context=context_) + x\n        x = rearrange(x, '(b t) (h w) c -> b c t h w', b=b,h=h)\n\n        # temporal cross attention\n        x = rearrange(x, 'b c t h w -> (b h w) t c')\n        x = self.attn2_tmp(self.norm5(x), context=None, mask=mask) + x\n\n        # feedforward\n        x = self.ff(self.norm3(x)) + x\n        x = rearrange(x, '(b h w) t c -> b c t h w', b=b,h=h,w=w) # 3d -> 5d\n        \n        return x", "\n\n# ---------------------------------------------------------------------------------------------------\nclass SpatialTemporalTransformer(nn.Module):\n    \"\"\"\n    Transformer block for video-like data (5D tensor).\n    First, project the input (aka embedding) with NO reshape.\n    Then apply standard transformer action.\n    The 5D -> 3D reshape operation will be done in the specific attention module.\n    \"\"\"\n    def __init__(\n        self,\n        in_channels, n_heads, d_head,\n        depth=1, dropout=0.,\n        context_dim=None,\n        # Temporal\n        temporal_length=None,\n        use_relative_position=True,\n        **kwargs,\n        ):\n        super().__init__()\n\n        self.in_channels = in_channels\n        inner_dim = n_heads * d_head\n\n        self.norm = Normalize(in_channels)\n        self.proj_in = nn.Conv3d(in_channels,\n                                 inner_dim,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n\n        self.transformer_blocks = nn.ModuleList(\n            [BasicTransformerBlockST(\n                inner_dim, n_heads, d_head, dropout=dropout,\n                # cross attn\n                context_dim=context_dim,\n                # temporal attn\n                temporal_length=temporal_length,   \n                use_relative_position=use_relative_position,\n                **kwargs\n                ) for d in range(depth)]\n        )\n\n        self.proj_out = zero_module(nn.Conv3d(inner_dim,\n                                              in_channels,\n                                              kernel_size=1,\n                                              stride=1,\n                                              padding=0))\n        \n    def forward(self, x, context=None, **kwargs):\n        \n        assert(x.dim() == 5), f\"x shape = {x.shape}\"\n        x_in = x\n        \n        x = self.norm(x)\n        x = self.proj_in(x)\n        \n        for block in self.transformer_blocks:\n            x = block(x, context=context, **kwargs)\n        \n        x = self.proj_out(x)\n\n        return x + x_in", ""]}
{"filename": "scripts/t2v_helpers/render.py", "chunked_list": ["# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# Read LICENSE for usage terms.\n\nimport traceback\nfrom modelscope.process_modelscope import process_modelscope\nimport modelscope.process_modelscope as pm\nfrom videocrafter.process_videocrafter import process_videocrafter\nfrom modules.shared import opts\nfrom .error_hardcode import get_error\nfrom modules import lowvram, devices, sd_hijack", "from .error_hardcode import get_error\nfrom modules import lowvram, devices, sd_hijack\nimport logging \nimport gc\nimport t2v_helpers.args as t2v_helpers_args\n\ndef run(*args):\n    dataurl = get_error()\n    vids_pack = [dataurl]\n    component_names = t2v_helpers_args.get_component_names()\n    # api check\n    num_components = len(component_names)\n    affected_args = args[2:] if len(args) > num_components else args\n    # TODO: change to i+2 when we will add the progress bar\n    args_dict = {component_names[i]: affected_args[i] for i in range(0, num_components)}\n    model_type = args_dict['model_type']\n    t2v_helpers_args.i1_store_t2v = f'<p style=\\\"font-weight:bold;margin-bottom:0em\\\">text2video extension for auto1111 \u2014 version 1.3b </p><video controls loop><source src=\"{dataurl}\" type=\"video/mp4\"></video>'\n    keep_pipe_in_vram = opts.data.get(\"modelscope_deforum_keep_model_in_vram\") if opts.data is not None and opts.data.get(\"modelscope_deforum_keep_model_in_vram\") is not None else 'None'\n    try:\n        print(f'text2video \u2014 The model selected is: {args_dict[\"model\"]} ({args_dict[\"model_type\"]}-like)')\n        if model_type == 'ModelScope':\n            vids_pack = process_modelscope(args_dict, args)\n        elif model_type == 'VideoCrafter (WIP)':\n            vids_pack = process_videocrafter(args_dict)\n        else:\n            raise NotImplementedError(f\"Unknown model type: {model_type}\")\n    except Exception as e:\n        traceback.print_exc()\n        print('Exception occurred:', e)\n    finally:\n        #optionally store pipe in global between runs, if not, remove it\n        if keep_pipe_in_vram == 'None':\n            pm.pipe = None\n        devices.torch_gc()\n        gc.collect()\n    return vids_pack", "\n\n"]}
{"filename": "scripts/t2v_helpers/general_utils.py", "chunked_list": ["# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# Read LICENSE for usage terms.\nfrom modules.prompt_parser import reconstruct_cond_batch\nimport os\nimport modules.paths as ph\n\ndef get_t2v_version():\n    from modules import extensions as mext\n    try:\n        for ext in mext.extensions:\n            if (ext.name in [\"sd-webui-modelscope-text2video\"] or ext.name in [\"sd-webui-text2video\"]) and ext.enabled:\n                return ext.version\n        return \"Unknown\"\n    except:\n        return \"Unknown\"", "\ndef get_model_location(model_name):\n    assert model_name is not None\n\n    if model_name == \"<modelscope>\":\n        return os.path.join(ph.models_path, 'ModelScope/t2v')\n    elif model_name == \"<videocrafter>\":\n        return os.path.join(ph.models_path, 'VideoCrafter')\n    else:\n        return os.path.join(ph.models_path, 'text2video/', model_name)", "\ndef reconstruct_conds(cond, uncond, step):\n    c = reconstruct_cond_batch(cond, step)\n    uc = reconstruct_cond_batch(uncond, step)\n    return c, uc\n"]}
{"filename": "scripts/t2v_helpers/extensions_utils.py", "chunked_list": ["import gradio as gr\n\nclass Text2VideoExtension(object):\n    \"\"\"\n    A simple base class that sets a definitive way to process extensions\n    \"\"\"\n\n    def __init__(self, extension_name: str = '', extension_title: str = ''):\n\n        self.extension_name = extension_name\n        self.extension_title = extension_title\n        self.return_args_delimiter = f\"extension_{extension_name}\"\n\n    def return_ui_inputs(self, return_args: list = [] ):\n        \"\"\"\n        All extensions should use this method to return Gradio inputs.\n        This allows for tracking the inputs using a delimiter.\n        Arguments are automatically processed and returned.\n        \n        Output: <my_extension_name> + [arg1, arg2, arg3] + <my_extension_name>\n        \"\"\"\n    \n        delimiter = gr.State(self.return_args_delimiter)\n        return [delimiter] + return_args + [delimiter]\n\n    def process_extension_args(self, all_args: list = []):\n        \"\"\"\n        Processes all extension arguments and appends them into a list.\n        The filtered arguments are piped into the extension's process method.\n        \"\"\"\n\n        can_append = False\n        extension_args = []\n\n        for value in all_args:\n            if value == self.return_args_delimiter and not can_append:\n                can_append = True\n                continue\n\n            if can_append:\n                if value == self.return_args_delimiter:\n                    break\n                else:\n                    extension_args.append(value)\n\n        return extension_args\n\n    def log(self, message: str = '', *args):\n        \"\"\"\n        Choose to print a log specific to the extension.\n        \"\"\"\n        OKGREEN = '\\033[92m'\n        ENDC = '\\033[0m'\n\n        title = self.extension_title\n        message = f\"Extension {title}: {message} \" + ', '.join(args)\n        print(OKGREEN + message + ENDC)", ""]}
{"filename": "scripts/t2v_helpers/key_frames.py", "chunked_list": ["# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# Read LICENSE for usage terms.\n\nimport re\nimport numpy as np\nimport numexpr\nimport pandas as pd\n\nclass T2VAnimKeys():\n    def __init__(self, anim_args, seed=-1, max_i_frames=1):\n        self.fi = FrameInterpolater(anim_args.max_frames, seed, max_i_frames)\n        self.inpainting_weights_series = self.fi.get_inbetweens(self.fi.parse_key_frames(anim_args.inpainting_weights))", "class T2VAnimKeys():\n    def __init__(self, anim_args, seed=-1, max_i_frames=1):\n        self.fi = FrameInterpolater(anim_args.max_frames, seed, max_i_frames)\n        self.inpainting_weights_series = self.fi.get_inbetweens(self.fi.parse_key_frames(anim_args.inpainting_weights))\n\ndef check_is_number(value):\n    float_pattern = r'^(?=.)([+-]?([0-9]*)(\\.([0-9]+))?)$'\n    return re.match(float_pattern, value)\n\nclass FrameInterpolater():\n    def __init__(self, max_frames=0, seed=-1, max_i_frames=1) -> None:\n        self.max_frames = max_frames\n        self.seed = seed\n        self.max_i_frames = max_i_frames\n\n    def sanitize_value(self, value):\n        return value.replace(\"'\",\"\").replace('\"',\"\").replace('(',\"\").replace(')',\"\")\n\n    def get_inbetweens(self, key_frames, integer=False, interp_method='Linear', is_single_string = False):\n        key_frame_series = pd.Series([np.nan for a in range(self.max_frames)])\n        # get our ui variables set for numexpr.evaluate\n        max_f = self.max_frames -1\n        max_i_f = self.max_i_frames - 1\n        s = self.seed\n        for i in range(0, self.max_frames):\n            if i in key_frames:\n                value = key_frames[i]\n                value_is_number = check_is_number(self.sanitize_value(value))\n                if value_is_number: # if it's only a number, leave the rest for the default interpolation\n                    key_frame_series[i] = self.sanitize_value(value)\n            if not value_is_number:\n                t = i\n                # workaround for values formatted like 0:(\"I am test\") //used for sampler schedules\n                key_frame_series[i] = numexpr.evaluate(value) if not is_single_string else self.sanitize_value(value)\n            elif is_single_string:# take previous string value and replicate it\n                key_frame_series[i] = key_frame_series[i-1]\n        key_frame_series = key_frame_series.astype(float) if not is_single_string else key_frame_series # as string\n        \n        if interp_method == 'Cubic' and len(key_frames.items()) <= 3:\n            interp_method = 'Quadratic'    \n        if interp_method == 'Quadratic' and len(key_frames.items()) <= 2:\n            interp_method = 'Linear'\n            \n        key_frame_series[0] = key_frame_series[key_frame_series.first_valid_index()]\n        key_frame_series[self.max_frames-1] = key_frame_series[key_frame_series.last_valid_index()]\n        key_frame_series = key_frame_series.interpolate(method=interp_method.lower(), limit_direction='both')\n        if integer:\n            return key_frame_series.astype(int)\n        return key_frame_series\n\n    def parse_key_frames(self, string):\n        # because math functions (i.e. sin(t)) can utilize brackets \n        # it extracts the value in form of some stuff\n        # which has previously been enclosed with brackets and\n        # with a comma or end of line existing after the closing one\n        frames = dict()\n        for match_object in string.split(\",\"):\n            frameParam = match_object.split(\":\")\n            max_f = self.max_frames - 1\n            max_i_f = self.max_i_frames - 1\n            s = self.seed\n            frame = int(self.sanitize_value(frameParam[0])) if check_is_number(self.sanitize_value(frameParam[0].strip())) else int(numexpr.evaluate(frameParam[0].strip().replace(\"'\",\"\",1).replace('\"',\"\",1)[::-1].replace(\"'\",\"\",1).replace('\"',\"\",1)[::-1]))\n            frames[frame] = frameParam[1].strip()\n        if frames == {} and len(string) != 0:\n            raise RuntimeError('Key Frame string not correctly formatted')\n        return frames", "\nclass FrameInterpolater():\n    def __init__(self, max_frames=0, seed=-1, max_i_frames=1) -> None:\n        self.max_frames = max_frames\n        self.seed = seed\n        self.max_i_frames = max_i_frames\n\n    def sanitize_value(self, value):\n        return value.replace(\"'\",\"\").replace('\"',\"\").replace('(',\"\").replace(')',\"\")\n\n    def get_inbetweens(self, key_frames, integer=False, interp_method='Linear', is_single_string = False):\n        key_frame_series = pd.Series([np.nan for a in range(self.max_frames)])\n        # get our ui variables set for numexpr.evaluate\n        max_f = self.max_frames -1\n        max_i_f = self.max_i_frames - 1\n        s = self.seed\n        for i in range(0, self.max_frames):\n            if i in key_frames:\n                value = key_frames[i]\n                value_is_number = check_is_number(self.sanitize_value(value))\n                if value_is_number: # if it's only a number, leave the rest for the default interpolation\n                    key_frame_series[i] = self.sanitize_value(value)\n            if not value_is_number:\n                t = i\n                # workaround for values formatted like 0:(\"I am test\") //used for sampler schedules\n                key_frame_series[i] = numexpr.evaluate(value) if not is_single_string else self.sanitize_value(value)\n            elif is_single_string:# take previous string value and replicate it\n                key_frame_series[i] = key_frame_series[i-1]\n        key_frame_series = key_frame_series.astype(float) if not is_single_string else key_frame_series # as string\n        \n        if interp_method == 'Cubic' and len(key_frames.items()) <= 3:\n            interp_method = 'Quadratic'    \n        if interp_method == 'Quadratic' and len(key_frames.items()) <= 2:\n            interp_method = 'Linear'\n            \n        key_frame_series[0] = key_frame_series[key_frame_series.first_valid_index()]\n        key_frame_series[self.max_frames-1] = key_frame_series[key_frame_series.last_valid_index()]\n        key_frame_series = key_frame_series.interpolate(method=interp_method.lower(), limit_direction='both')\n        if integer:\n            return key_frame_series.astype(int)\n        return key_frame_series\n\n    def parse_key_frames(self, string):\n        # because math functions (i.e. sin(t)) can utilize brackets \n        # it extracts the value in form of some stuff\n        # which has previously been enclosed with brackets and\n        # with a comma or end of line existing after the closing one\n        frames = dict()\n        for match_object in string.split(\",\"):\n            frameParam = match_object.split(\":\")\n            max_f = self.max_frames - 1\n            max_i_f = self.max_i_frames - 1\n            s = self.seed\n            frame = int(self.sanitize_value(frameParam[0])) if check_is_number(self.sanitize_value(frameParam[0].strip())) else int(numexpr.evaluate(frameParam[0].strip().replace(\"'\",\"\",1).replace('\"',\"\",1)[::-1].replace(\"'\",\"\",1).replace('\"',\"\",1)[::-1]))\n            frames[frame] = frameParam[1].strip()\n        if frames == {} and len(string) != 0:\n            raise RuntimeError('Key Frame string not correctly formatted')\n        return frames", ""]}
{"filename": "scripts/t2v_helpers/error_hardcode.py", "chunked_list": ["# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# Read LICENSE for usage terms.\n\n# Hardcode, because read file operations fail for some reason\n\ndef get_error():\n    return \"\"\"data:video/mp4;base64,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\"\"\"\n"]}
{"filename": "scripts/t2v_helpers/video_audio_utils.py", "chunked_list": ["# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# Read LICENSE for usage terms.\n\nimport time, math\nimport subprocess\nimport os, shutil\nimport cv2\nfrom modules.shared import state\nfrom pkg_resources import resource_filename\n\ndef get_frame_name(path):\n    name = os.path.basename(path)\n    name = os.path.splitext(name)[0]\n    return name", "from pkg_resources import resource_filename\n\ndef get_frame_name(path):\n    name = os.path.basename(path)\n    name = os.path.splitext(name)[0]\n    return name\n    \ndef vid2frames(video_path, video_in_frame_path, n=1, overwrite=True, extract_from_frame=0, extract_to_frame=-1, out_img_format='jpg', numeric_files_output = False): \n    if (extract_to_frame <= extract_from_frame) and extract_to_frame != -1:\n        raise RuntimeError('Error: extract_to_frame can not be higher than extract_from_frame')\n    \n    if n < 1: n = 1 #HACK Gradio interface does not currently allow min/max in gr.Number(...) \n\n    # check vid path using a function and only enter if we get True\n    if is_vid_path_valid(video_path):\n        \n        name = get_frame_name(video_path)\n        \n        vidcap = cv2.VideoCapture(video_path)\n        video_fps = vidcap.get(cv2.CAP_PROP_FPS)\n\n        input_content = []\n        if os.path.exists(video_in_frame_path) :\n            input_content = os.listdir(video_in_frame_path)\n\n        # check if existing frame is the same video, if not we need to erase it and repopulate\n        if len(input_content) > 0:\n            #get the name of the existing frame\n            content_name = get_frame_name(input_content[0])\n            if not content_name.startswith(name):\n                overwrite = True\n\n        # grab the frame count to check against existing directory len \n        frame_count = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)) \n        \n        # raise error if the user wants to skip more frames than exist\n        if n >= frame_count : \n            raise RuntimeError('Skipping more frames than input video contains. extract_nth_frames larger than input frames')\n        \n        expected_frame_count = math.ceil(frame_count / n) \n        # Check to see if the frame count is matches the number of files in path\n        if overwrite or expected_frame_count != len(input_content):\n            shutil.rmtree(video_in_frame_path)\n            os.makedirs(video_in_frame_path, exist_ok=True) # just deleted the folder so we need to make it again\n            input_content = os.listdir(video_in_frame_path)\n        \n        print(f\"Trying to extract frames from video with input FPS of {video_fps}. Please wait patiently.\")\n        if len(input_content) == 0:\n            vidcap.set(cv2.CAP_PROP_POS_FRAMES, extract_from_frame) # Set the starting frame\n            success,image = vidcap.read()\n            count = extract_from_frame\n            t=1\n            success = True\n            while success:\n                if state.interrupted:\n                    return\n                if (count <= extract_to_frame or extract_to_frame == -1) and count % n == 0:\n                    if numeric_files_output == True:\n                        cv2.imwrite(video_in_frame_path + os.path.sep + f\"{t:09}.{out_img_format}\" , image) # save frame as file\n                    else:\n                        cv2.imwrite(video_in_frame_path + os.path.sep + name + f\"{t:09}.{out_img_format}\" , image) # save frame as file\n                    t += 1\n                success,image = vidcap.read()\n                count += 1\n            print(f\"Successfully extracted {count} frames from video.\")\n        else:\n            print(\"Frames already unpacked\")\n        vidcap.release()\n        return video_fps", "\ndef is_vid_path_valid(video_path):\n    # make sure file format is supported!\n    file_formats = [\"mov\", \"mpeg\", \"mp4\", \"m4v\", \"avi\", \"mpg\", \"webm\"]\n    extension = video_path.rsplit('.', 1)[-1].lower()\n    # vid path is actually a URL, check it \n    if video_path.startswith('http://') or video_path.startswith('https://'):\n        response = requests.head(video_path, allow_redirects=True)\n        if response.status_code == 404:\n            raise ConnectionError(\"Video URL is not valid. Response status code: {}\".format(response.status_code))\n        elif response.status_code == 302:\n            response = requests.head(response.headers['location'], allow_redirects=True)\n        if response.status_code != 200:\n            raise ConnectionError(\"Video URL is not valid. Response status code: {}\".format(response.status_code))\n        if extension not in file_formats:\n            raise ValueError(\"Video file format '{}' not supported. Supported formats are: {}\".format(extension, file_formats))\n    else:\n        if not os.path.exists(video_path):\n            raise RuntimeError(\"Video path does not exist.\")\n        if extension not in file_formats:\n            raise ValueError(\"Video file format '{}' not supported. Supported formats are: {}\".format(extension, file_formats))\n    return True", "\n\ndef clean_folder_name(string):\n    illegal_chars = \"/\\\\<>:\\\"|?*.,\\\" \"\n    translation_table = str.maketrans(illegal_chars, \"_\"*len(illegal_chars))\n    return string.translate(translation_table)\n    \ndef find_ffmpeg_binary():\n    try:\n        import google.colab\n        return 'ffmpeg'\n    except:\n        pass\n    for package in ['imageio_ffmpeg', 'imageio-ffmpeg']:\n        try:\n            package_path = resource_filename(package, 'binaries')\n            files = [os.path.join(package_path, f) for f in os.listdir(\n                package_path) if f.startswith(\"ffmpeg-\")]\n            files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n            return files[0] if files else 'ffmpeg'\n        except:\n            return 'ffmpeg'", "            \n# Stitch images to a h264 mp4 video using ffmpeg\ndef ffmpeg_stitch_video(ffmpeg_location=None, fps=None, outmp4_path=None, stitch_from_frame=0, stitch_to_frame=None, imgs_path=None, add_soundtrack=None, audio_path=None, crf=17, preset='veryslow'):\n    start_time = time.time()\n\n    print(f\"Got a request to stitch frames to video using FFmpeg.\\nFrames:\\n{imgs_path}\\nTo Video:\\n{outmp4_path}\")\n    msg_to_print = f\"Stitching *video*...\"\n    print(msg_to_print)\n    if stitch_to_frame == -1:\n        stitch_to_frame = 999999999\n    try:\n        cmd = [\n            ffmpeg_location,\n            '-y',\n            '-vcodec', 'png',\n            '-r', str(float(fps)),\n            '-start_number', str(stitch_from_frame),\n            '-i', imgs_path,\n            '-frames:v', str(stitch_to_frame),\n            '-c:v', 'libx264',\n            '-vf',\n            f'fps={float(fps)}',\n            '-pix_fmt', 'yuv420p',\n            '-crf', str(crf),\n            '-preset', preset,\n            '-pattern_type', 'sequence',\n            outmp4_path\n        ]\n        process = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = process.communicate()\n    except FileNotFoundError:\n        print(\"\\r\" + \" \" * len(msg_to_print), end=\"\", flush=True)\n        print(f\"\\r{msg_to_print}\", flush=True)\n        raise FileNotFoundError(\n            \"FFmpeg not found. Please make sure you have a working ffmpeg path under 'ffmpeg_location' parameter.\")\n    except Exception as e:\n        print(\"\\r\" + \" \" * len(msg_to_print), end=\"\", flush=True)\n        print(f\"\\r{msg_to_print}\", flush=True)\n        raise Exception(\n            f'Error stitching frames to video. Actual runtime error:{e}')\n\n    if add_soundtrack != 'None':\n        audio_add_start_time = time.time()\n        try:\n            cmd = [\n                ffmpeg_location,\n                '-i',\n                outmp4_path,\n                '-i',\n                audio_path,\n                '-map', '0:v',\n                '-map', '1:a',\n                '-c:v', 'copy',\n                '-shortest',\n                outmp4_path+'.temp.mp4'\n            ]\n            process = subprocess.Popen(\n                cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            stdout, stderr = process.communicate()\n            if process.returncode != 0:\n                print(\"\\r\" + \" \" * len(msg_to_print), end=\"\", flush=True)\n                print(f\"\\r{msg_to_print}\", flush=True)\n                raise RuntimeError(stderr)\n            os.replace(outmp4_path+'.temp.mp4', outmp4_path)\n            print(\"\\r\" + \" \" * len(msg_to_print), end=\"\", flush=True)\n            print(f\"\\r{msg_to_print}\", flush=True)\n            print(f\"\\rFFmpeg Video+Audio stitching \\033[0;32mdone\\033[0m in {time.time() - start_time:.2f} seconds!\", flush=True)\n        except Exception as e:\n            print(\"\\r\" + \" \" * len(msg_to_print), end=\"\", flush=True)\n            print(f\"\\r{msg_to_print}\", flush=True)\n            print(f'\\rError adding audio to video. Actual error: {e}', flush=True)\n            print(f\"FFMPEG Video (sorry, no audio) stitching \\033[33mdone\\033[0m in {time.time() - start_time:.2f} seconds!\", flush=True)\n    else:\n        print(\"\\r\" + \" \" * len(msg_to_print), end=\"\", flush=True)\n        print(f\"\\r{msg_to_print}\", flush=True)\n        print(f\"\\rVideo stitching \\033[0;32mdone\\033[0m in {time.time() - start_time:.2f} seconds!\", flush=True)", "\n# quick-retreive frame count, FPS and H/W dimensions of a video (local or URL-based)\ndef get_quick_vid_info(vid_path):\n    vidcap = cv2.VideoCapture(vid_path)\n    video_fps = vidcap.get(cv2.CAP_PROP_FPS)\n    video_frame_count = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)) \n    video_width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    video_height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    vidcap.release()\n    if video_fps.is_integer():\n        video_fps = int(video_fps)\n\n    return video_fps, video_frame_count, (video_width, video_height)", "\n# This function usually gets a filename, and converts it to a legal linux/windows *folder* name\ndef clean_folder_name(string):\n    illegal_chars = \"/\\\\<>:\\\"|?*.,\\\" \"\n    translation_table = str.maketrans(illegal_chars, \"_\"*len(illegal_chars))\n    return string.translate(translation_table)\n\n# used in src/rife/inference_video.py and more, soon\ndef duplicate_pngs_from_folder(from_folder, to_folder, img_batch_id, orig_vid_name):\n    import cv2\n    #TODO: don't copy-paste at all if the input is a video (now it copy-pastes, and if input is deforum run is also converts to make sure no errors rise cuz of 24-32 bit depth differences)\n    temp_convert_raw_png_path = os.path.join(from_folder, to_folder)\n    if not os.path.exists(temp_convert_raw_png_path):\n                os.makedirs(temp_convert_raw_png_path)\n                \n    frames_handled = 0\n    for f in os.listdir(from_folder):\n        if ('png' in f or 'jpg' in f) and '-' not in f and '_depth_' not in f and ((img_batch_id is not None and f.startswith(img_batch_id) or img_batch_id is None)):\n            frames_handled +=1\n            original_img_path = os.path.join(from_folder, f)\n            if orig_vid_name is not None:\n                shutil.copy(original_img_path, temp_convert_raw_png_path)\n            else:\n                image = cv2.imread(original_img_path)\n                new_path = os.path.join(temp_convert_raw_png_path, f)\n                cv2.imwrite(new_path, image, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n    return frames_handled", "def duplicate_pngs_from_folder(from_folder, to_folder, img_batch_id, orig_vid_name):\n    import cv2\n    #TODO: don't copy-paste at all if the input is a video (now it copy-pastes, and if input is deforum run is also converts to make sure no errors rise cuz of 24-32 bit depth differences)\n    temp_convert_raw_png_path = os.path.join(from_folder, to_folder)\n    if not os.path.exists(temp_convert_raw_png_path):\n                os.makedirs(temp_convert_raw_png_path)\n                \n    frames_handled = 0\n    for f in os.listdir(from_folder):\n        if ('png' in f or 'jpg' in f) and '-' not in f and '_depth_' not in f and ((img_batch_id is not None and f.startswith(img_batch_id) or img_batch_id is None)):\n            frames_handled +=1\n            original_img_path = os.path.join(from_folder, f)\n            if orig_vid_name is not None:\n                shutil.copy(original_img_path, temp_convert_raw_png_path)\n            else:\n                image = cv2.imread(original_img_path)\n                new_path = os.path.join(temp_convert_raw_png_path, f)\n                cv2.imwrite(new_path, image, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n    return frames_handled", "\ndef add_soundtrack(ffmpeg_location=None, fps=None, outmp4_path=None, stitch_from_frame=0, stitch_to_frame=None, imgs_path=None, add_soundtrack=None, audio_path=None, crf=17, preset='veryslow'):\n    if add_soundtrack is None:\n        return\n    msg_to_print = f\"Adding soundtrack to *video*...\"\n    start_time = time.time()\n    try:\n        cmd = [\n            ffmpeg_location,\n            '-i',\n            outmp4_path,\n            '-i',\n            audio_path,\n            '-map', '0:v',\n            '-map', '1:a',\n            '-c:v', 'copy',\n            '-shortest',\n            outmp4_path+'.temp.mp4'\n        ]\n        process = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = process.communicate()\n        if process.returncode != 0:\n            print(\"\\r\" + \" \" * len(msg_to_print), end=\"\", flush=True)\n            print(f\"\\r{msg_to_print}\", flush=True)\n            raise RuntimeError(stderr)\n        os.replace(outmp4_path+'.temp.mp4', outmp4_path)\n        print(\"\\r\" + \" \" * len(msg_to_print), end=\"\", flush=True)\n        print(f\"\\r{msg_to_print}\", flush=True)\n        print(f\"\\rFFmpeg Audio stitching \\033[0;32mdone\\033[0m in {time.time() - start_time:.2f} seconds!\", flush=True)\n    except Exception as e:\n        print(\"\\r\" + \" \" * len(msg_to_print), end=\"\", flush=True)\n        print(f\"\\r{msg_to_print}\", flush=True)\n        print(f'\\rError adding audio to video. Actual error: {e}', flush=True)\n        print(f\"FFMPEG Video (sorry, no audio) stitching \\033[33mdone\\033[0m in {time.time() - start_time:.2f} seconds!\", flush=True)", ""]}
{"filename": "scripts/t2v_helpers/args.py", "chunked_list": ["# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# Read LICENSE for usage terms.\n\nimport gradio as gr\nfrom types import SimpleNamespace\nfrom t2v_helpers.video_audio_utils import find_ffmpeg_binary\nfrom samplers.samplers_common import available_samplers\nimport os\nimport modules.paths as ph\nfrom t2v_helpers.general_utils import get_model_location", "import modules.paths as ph\nfrom t2v_helpers.general_utils import get_model_location\nfrom modules.shared import opts\n\nwelcome_text_videocrafter = '''- Download pretrained T2V models via <a style=\"color:SteelBlue\" href=\"https://drive.google.com/file/d/13ZZTXyAKM3x0tObRQOQWdtnrI2ARWYf_/view?usp=share_link\">this link</a>, and put the model.ckpt in models/VideoCrafter/model.ckpt. Then use the same GUI pipeline as ModelScope does.\n'''\n\nwelcome_text_modelscope = '''- Put your models to stable-diffusion-webui/models/text2video, each full model should have its own folder. A model consists of four parts: `VQGAN_autoencoder.pth`, `configuration.json`, `open_clip_pytorch_model.bin` and `text2video_pytorch_model.pth`. Make sure `configuration.json` is a text JSON file and not a saved HTML webpage (click on the \u2b07\ufe0f character to the right, don't save via right-click). Recommended requirements start at 6 GBs of VRAM.\n\n<a style=\"color:SteelBlue\" href=\"https://github.com/kabachuha/sd-webui-text2video#prominent-fine-tunes\">A list of prominent fine-tunes</a> is a good starting point for models search.", "\n<a style=\"color:SteelBlue\" href=\"https://github.com/kabachuha/sd-webui-text2video#prominent-fine-tunes\">A list of prominent fine-tunes</a> is a good starting point for models search.\n\nJoin the development or report issues and feature requests here <a style=\"color:SteelBlue\" href=\"https://github.com/kabachuha/sd-webui-text2video\">https://github.com/kabachuha/sd-webui-text2video</a>\n\n<italic>If you liked this extension, please <a style=\"color:SteelBlue\" href=\"https://github.com/kabachuha/sd-webui-text2video\">give it a star on GitHub</a>!</italic> \ud83d\ude0a\n\n'''\n\nwelcome_text = '''**VideoCrafter (WIP)**:", "\nwelcome_text = '''**VideoCrafter (WIP)**:\n\n''' + welcome_text_videocrafter + '''\n\n**ModelScope**:\n\n''' + welcome_text_modelscope\n\ni1_store_t2v = f\"<p style=\\\"text-align:center;font-weight:bold;margin-bottom:0em\\\">text2video extension for auto1111 \u2014 version 1.2b. The video will be shown below this label when ready</p>\"", "\ni1_store_t2v = f\"<p style=\\\"text-align:center;font-weight:bold;margin-bottom:0em\\\">text2video extension for auto1111 \u2014 version 1.2b. The video will be shown below this label when ready</p>\"\n\ndef enable_sampler_dropdown(model_type):\n    is_visible = model_type == \"ModelScope\"\n    return gr.update(visible=is_visible)\n\ndef setup_common_values(mode, d):\n    with gr.Row(elem_id=f'{mode}_prompt_toprow'):\n        prompt = gr.Textbox(label='Prompt', lines=3, interactive=True, elem_id=f\"{mode}_prompt\", placeholder=\"Enter your prompt here...\")\n    with gr.Row(elem_id=f'{mode}_n_prompt_toprow'):\n        n_prompt = gr.Textbox(label='Negative prompt', lines=2, interactive=True, elem_id=f\"{mode}_n_prompt\", value=d.n_prompt)\n    with gr.Row():\n        sampler = gr.Dropdown(label=\"Sampling method (ModelScope)\", choices=[x.name for x in available_samplers], value=available_samplers[0].name, elem_id=\"model-sampler\", visible=True)\n        steps = gr.Slider(label='Steps', minimum=1, maximum=100, step=1, value=d.steps)\n    with gr.Row():\n        cfg_scale = gr.Slider(label='CFG scale', minimum=1, maximum=100, step=1, value=d.cfg_scale)\n    with gr.Row():\n        width = gr.Slider(label='Width', minimum=64, maximum=1024, step=64, value=d.width)\n        height = gr.Slider(label='Height', minimum=64, maximum=1024, step=64, value=d.height)\n    with gr.Row():\n        seed = gr.Number(label='Seed', value = d.seed, Interactive = True, precision=0)\n        eta = gr.Number(label=\"ETA (DDIM Only)\", value=d.eta, interactive=True)\n    with gr.Row():\n        gr.Markdown('256x256 Benchmarks: 24 frames peak at 5.7 GBs of VRAM and 125 frames peak at 11.5 GBs with Torch2 installed')\n    with gr.Row():\n        frames = gr.Slider(label=\"Frames\", value=d.frames, minimum=2, maximum=250, step=1, interactive=True, precision=0)\n        batch_count = gr.Slider(label=\"Batch count\", value=d.batch_count, minimum=1, maximum=100, step=1, interactive=True)\n    \n    return prompt, n_prompt, sampler, steps, seed, cfg_scale, width, height, eta, frames, batch_count", "\n\nrefresh_symbol = '\\U0001f504'  # \ud83d\udd04\nclass ToolButton(gr.Button, gr.components.FormComponent):\n    \"\"\"Small button with single emoji as text, fits inside gradio forms\"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(variant=\"tool\", **kwargs)\n\n    def get_block_name(self):\n        return \"button\"", "\ndef setup_text2video_settings_dictionary():\n    d = SimpleNamespace(**T2VArgs())\n    dv = SimpleNamespace(**T2VOutputArgs())\n    with gr.Row(elem_id='model-switcher'):\n        with gr.Row(variant='compact'):\n            # TODO: deprecate this in favor of dynamic model type reading\n            model_type = gr.Radio(label='Model type', choices=['ModelScope', 'VideoCrafter (WIP)'], value='ModelScope', elem_id='model-type')\n            model = gr.Dropdown(label='Model', value=\"<modelscope>\", help=\"Put the folders with models (configuration, vae, clip, diffusion model) in models/text2video. Each folder matches to a model. <modelscope> and <videocrafter> are the legacy locations\")\n            refresh_models = ToolButton(value=refresh_symbol)\n\n            def refresh_all_models(model):\n                models = []\n                if os.path.isdir(os.path.join(ph.models_path, 'ModelScope/t2v')):\n                    models.append('<modelscope>')\n                if os.path.isdir(os.path.join(ph.models_path, 'VideoCrafter/')):\n                    models.append('<videocrafter>')\n                models_dir = os.path.join(ph.models_path, 'text2video/')\n                if os.path.isdir(models_dir):\n                    for subdir in os.listdir(models_dir):\n                        if os.path.isdir(os.path.join(models_dir, subdir)):\n                            models.append(subdir)\n                return gr.update(value=model if model in models else None, choices=models, visible=True)\n\n            refresh_models.click(refresh_all_models, model, model)\n    with gr.Tabs():\n        do_vid2vid = gr.State(value=0)\n        with gr.Tab('txt2vid') as tab_txt2vid:\n            # TODO: make it how it's done in Deforum/WebUI, so we won't have to track individual vars\n            prompt, n_prompt, sampler, steps, seed, cfg_scale, width, height, eta, frames, batch_count = setup_common_values('txt2vid', d)\n            model_type.change(fn=enable_sampler_dropdown, inputs=[model_type], outputs=[sampler])\n            with gr.Accordion('img2vid', open=False):\n                inpainting_image = gr.File(label=\"Inpainting image\", interactive=True, file_count=\"single\", file_types=[\"image\"], elem_id=\"inpainting_chosen_file\")\n                # TODO: should be tied to the total frame count dynamically\n                inpainting_frames=gr.Slider(label='inpainting frames',value=d.inpainting_frames,minimum=0, maximum=250, step=1)\n                with gr.Row():\n                    gr.Markdown('''`inpainting frames` is the number of frames inpainting is applied to (counting from the beginning)\n\nThe following parameters are exposed in this keyframe: max frames as `max_f`, inpainting frames as `max_i_f`, current frame number as `t`, seed as `s`\n\nThe weigths of `0:(t/max_i_f), \"max_i_f\":(1)` will *continue* the initial pic\n\nTo *loop it back*, set the weight to 0 for the first and for the last frame\n\nExample: `0:(0), \"max_i_f/4\":(1), \"3*max_i_f/4\":(1), \"max_i_f-1\":(0)` ''')\n                with gr.Row():\n                    inpainting_weights = gr.Textbox(label=\"Inpainting weights\", value=d.inpainting_weights, interactive=True)\n        with gr.Tab('vid2vid') as tab_vid2vid:\n            with gr.Row():\n                gr.HTML('Put your video here')\n                gr.HTML('<strong>Vid2vid for VideoCrafter is to be done!</strong>')\n            vid2vid_frames = gr.File(label=\"Input video\", interactive=True, file_count=\"single\", file_types=[\"video\"], elem_id=\"vid_to_vid_chosen_file\")\n            with gr.Row():\n                gr.HTML('Alternative: enter the relative (to the webui) path to the file')\n            with gr.Row():\n                vid2vid_frames_path = gr.Textbox(label=\"Input video path\", interactive=True, elem_id=\"vid_to_vid_chosen_path\", placeholder='Enter your video path here, or upload in the box above ^')\n            # TODO: here too\n            prompt_v, n_prompt_v, sampler_v, steps_v, seed_v, cfg_scale_v, width_v, height_v, eta_v, frames_v, batch_count_v = setup_common_values('vid2vid', d)\n            model_type.change(fn=enable_sampler_dropdown, inputs=[model_type], outputs=[sampler_v])\n            with gr.Row():\n                strength = gr.Slider(label=\"denoising strength\", value=d.strength, minimum=0, maximum=1, step=0.05, interactive=True)\n                vid2vid_startFrame=gr.Number(label='vid2vid start frame',value=d.vid2vid_startFrame)\n        \n        tab_txt2vid.select(fn=lambda: 0, inputs=[], outputs=[do_vid2vid])\n        tab_vid2vid.select(fn=lambda: 1, inputs=[], outputs=[do_vid2vid])\n\n        with gr.Tab('Output settings'):\n            with gr.Row(variant='compact') as fps_out_format_row:\n                fps = gr.Slider(label=\"FPS\", value=dv.fps, minimum=1, maximum=240, step=1)\n            with gr.Row(variant='compact') as soundtrack_row:\n                add_soundtrack = gr.Radio(['None', 'File', 'Init Video'], label=\"Add soundtrack\", value=dv.add_soundtrack)\n                soundtrack_path = gr.Textbox(label=\"Soundtrack path\", lines=1, interactive=True, value=dv.soundtrack_path)\n\n            with gr.Row(variant='compact'):\n                skip_video_creation = gr.Checkbox(label=\"Skip video creation\", value=dv.skip_video_creation, interactive=True)\n            with gr.Row(equal_height=True, variant='compact', visible=True) as ffmpeg_set_row:\n                ffmpeg_crf = gr.Slider(minimum=0, maximum=51, step=1, label=\"CRF\", value=dv.ffmpeg_crf, interactive=True)\n                ffmpeg_preset = gr.Dropdown(label=\"Preset\", choices=['veryslow', 'slower', 'slow', 'medium', 'fast', 'faster', 'veryfast', 'superfast', 'ultrafast'], interactive=True, value=dv.ffmpeg_preset, type=\"value\")\n            with gr.Row(equal_height=True, variant='compact', visible=True) as ffmpeg_location_row:\n                ffmpeg_location = gr.Textbox(label=\"Location\", lines=1, interactive=True, value=dv.ffmpeg_location)\n        with gr.Tab('How to install? Where to get help, how to help?'):\n            gr.Markdown(welcome_text)\n\n    return locals()", "\nt2v_video_args_names = str('skip_video_creation, ffmpeg_location, ffmpeg_crf, ffmpeg_preset, fps, add_soundtrack, soundtrack_path').replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\" \", \"\").split(',')\n\ncommon_values_names = str('''prompt, n_prompt, sampler, steps, frames, seed, cfg_scale, width, height, eta, batch_count''').replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\" \", \"\").split(',')\n\nv2v_values_names = str('''\ndo_vid2vid, vid2vid_frames, vid2vid_frames_path, strength,vid2vid_startFrame,\ninpainting_image,inpainting_frames, inpainting_weights,\nmodel_type,model''').replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\" \", \"\").split(',')\n", "model_type,model''').replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\" \", \"\").split(',')\n\nt2v_args_names = common_values_names + [f'{v}_v' for v in common_values_names] + v2v_values_names\n\nt2v_args_names_cleaned = common_values_names + v2v_values_names\n\ndef get_component_names():\n    return t2v_video_args_names + t2v_args_names\n\ndef pack_anim_args(args_dict):\n    return {name: args_dict[name] for name in t2v_args_names_cleaned}", "\ndef pack_anim_args(args_dict):\n    return {name: args_dict[name] for name in t2v_args_names_cleaned}\n\ndef pack_video_args(args_dict):\n    return {name: args_dict[name] for name in t2v_video_args_names}\n\ndef process_args(args_dict):\n    if args_dict['do_vid2vid']:\n        # override text2vid data with vid2vid data\n        for name in common_values_names:\n            args_dict[name] = args_dict[f'{name}_v']\n    \n    # deduplicate\n    for name in common_values_names:\n        if f'{name}_v' in args_dict:\n            args_dict.pop(f'{name}_v')\n\n    args = SimpleNamespace(**pack_anim_args(args_dict))\n    video_args = SimpleNamespace(**pack_video_args(args_dict))\n    T2VArgs_sanity_check(args)\n    return args, video_args", "\ndef T2VArgs():\n    frames = 24\n    batch_count = 1\n    eta = 0\n    seed = -1\n    width = 256\n    height = 256\n    cfg_scale = 17\n    steps = 30\n    prompt = \"\"\n    n_prompt = \"text, watermark, copyright, blurry, nsfw\"\n    strength = 0.75\n    vid2vid_startFrame = 0\n    inpainting_weights = '0:(t/max_i_f), \"max_i_f\":(1)' # linear growth weights (as they used to be in the original variant)\n    inpainting_frames = 0\n    sampler = \"DDIM_Gaussian\"\n    model = \"<modelscope>\"\n    return locals()", "\ndef T2VArgs_sanity_check(t2v_args):\n    try:\n        if t2v_args.model is not None and not os.path.isdir(get_model_location(t2v_args.model)):\n            raise ValueError(f'Model \"{t2v_args.model}\" not found in {get_model_location(t2v_args.model)}!')\n        if t2v_args.frames < 1:\n            raise ValueError('Frames count cannot be lower than 1!')\n        if t2v_args.batch_count < 1:\n            raise ValueError('Batch count cannot be lower than 1!')\n        if t2v_args.width < 1 or t2v_args.height < 1:\n            raise ValueError('Video dimensions cannot be lower than 1 pixel!')\n        if t2v_args.cfg_scale < 1:\n            raise ValueError('CFG scale cannot be lower than 1!')\n        if t2v_args.steps < 1:\n            raise ValueError('Steps cannot be lower than 1!')\n        if t2v_args.strength < 0 or t2v_args.strength > 1:\n            raise ValueError('vid2vid strength should be in range of 0 to 1!')\n        if t2v_args.vid2vid_startFrame >= t2v_args.frames:\n            raise ValueError('vid2vid start frame cannot be greater than the number of frames!')\n        if t2v_args.inpainting_frames < 0 or t2v_args.inpainting_frames > t2v_args.frames:\n            raise ValueError('inpainting frames count should lie between 0 and the frames number!')\n        if not any([x.name == t2v_args.sampler for x in available_samplers]):\n            raise ValueError(\"Sampler does not exist.\")\n    except Exception as e:\n        print(t2v_args)\n        raise e", "\ndef T2VOutputArgs():\n    skip_video_creation = False\n    fps = 15\n    make_gif = False\n    delete_imgs = False  # True will delete all imgs after a successful mp4 creation\n    image_path = \"C:/SD/20230124234916_%09d.png\"\n    mp4_path = \"testvidmanualsettings.mp4\"\n    ffmpeg_location = find_ffmpeg_binary()\n    ffmpeg_crf = '17'\n    ffmpeg_preset = 'slow'\n    add_soundtrack = 'None'  # [\"File\",\"Init Video\"]\n    soundtrack_path = \"https://deforum.github.io/a1/A1.mp3\"\n    # End-Run upscaling\n    r_upscale_video = False\n    r_upscale_factor = 'x2'  # ['2x', 'x3', 'x4']\n    # 'realesr-animevideov3' (default of realesrgan engine, does 2-4x), the rest do only 4x: 'realesrgan-x4plus', 'realesrgan-x4plus-anime'\n    r_upscale_model = 'realesr-animevideov3'\n    r_upscale_keep_imgs = True\n\n    render_steps = False\n    path_name_modifier = \"x0_pred\"  # [\"x0_pred\",\"x\"]\n    # **Interpolate Video Settings**\n    frame_interpolation_engine = \"None\"  # [\"None\", \"RIFE v4.6\", \"FILM\"]\n    frame_interpolation_x_amount = 2  # [2 to 1000 depends on the engine]\n    frame_interpolation_slow_mo_enabled = False\n    frame_interpolation_slow_mo_amount = 2  # [2 to 10]\n    frame_interpolation_keep_imgs = False\n    return locals()", "\ndef get_outdir():\n    outdir = os.path.join(opts.outdir_img2img_samples, 'text2video')\n    outdir = os.path.join(os.getcwd(), outdir)\n    return outdir\n"]}
{"filename": "scripts/stable_lora/scripts/lora_webui.py", "chunked_list": ["import torch\nimport glob\n\nfrom safetensors.torch import load_file\nfrom types import SimpleNamespace\nfrom safetensors import safe_open\nfrom einops import rearrange\nimport gradio as gr\nimport os\nimport json", "import os\nimport json\n\nfrom modules import images, script_callbacks\nfrom modules.shared import opts, state, cmd_opts\nfrom stable_lora.stable_utils.lora_processor import StableLoraProcessor\nfrom t2v_helpers.extensions_utils import Text2VideoExtension\n\nEXTENSION_TITLE = \"Stable LoRA\"\nEXTENSION_NAME = EXTENSION_TITLE.replace(' ', '_').lower()", "EXTENSION_TITLE = \"Stable LoRA\"\nEXTENSION_NAME = EXTENSION_TITLE.replace(' ', '_').lower()\n\ngr_inputs_list = [\n    \"lora_file_selection\", \n    \"lora_alpha\",\n    \"refresh_button\",\n    \"use_bias\",\n    \"use_linear\",\n    \"use_conv\",", "    \"use_linear\",\n    \"use_conv\",\n    \"use_emb\",\n    \"use_time\",\n    \"use_multiplier\"\n    ]\n\ngr_inputs_dict = {v: v for v in gr_inputs_list}\nGradioInputsIds = SimpleNamespace(**gr_inputs_dict)\n\nclass StableLoraScript(Text2VideoExtension, StableLoraProcessor):\n    \n    def __init__(self):\n        StableLoraProcessor.__init__(self)\n        Text2VideoExtension.__init__(self, EXTENSION_NAME, EXTENSION_TITLE)\n        self.device = 'cuda'\n        self.dtype = torch.float16\n\n    def title(self):\n            return EXTENSION_TITLE\n            \n    def refresh_models(self, *args):\n        paths_with_metadata, lora_names = self.get_lora_files()\n        self.lora_files = paths_with_metadata.copy()\n\n        return gr.Dropdown.update(value=[], choices=lora_names)\n\n    def ui(self):\n        paths_with_metadata, lora_names = self.get_lora_files()\n        self.lora_files = paths_with_metadata.copy()\n        REPOSITORY_LINK = \"https://github.com/ExponentialML/Text-To-Video-Finetuning\"\n\n        with gr.Accordion(label=EXTENSION_TITLE, open=False) as stable_lora_section:\n            with gr.Blocks(analytics_enabled=False):\n                with gr.Row():\n                    with gr.Column():\n                        gr.HTML(\"<h2>Load a Trained LoRA File.</h2>\")\n                        gr.HTML(\n                            \"\"\"\n                            <h3 style='color: crimson; font-weight: bold;'>\n                                Only Stable LoRA files are supported.\n                            </h3>\n                            \"\"\"\n                            )\n                        gr.HTML(f\"\"\"\n                        <a href='{REPOSITORY_LINK}'>\n                            To train a Stable LoRA file, use the finetune repository by clicking here.\n                        </a>\"\"\"\n                        )\n                        gr.HTML(f\"<span> Place your LoRA files in {cmd_opts.lora_dir}\")\n                        lora_files_selection = gr.Dropdown(\n                            label=\"Available Models\",\n                            elem_id=GradioInputsIds.lora_file_selection,\n                            choices=lora_names,\n                            value=[],\n                            multiselect=True,\n                        )\n                        lora_alpha = gr.Slider(\n                            minimum=0,\n                            maximum=1,\n                            value=1,\n                            step=0.05,\n                            elem_id=GradioInputsIds.lora_alpha,\n                            label=\"LoRA Weight\"\n                        )\n                        refresh_button = gr.Button(\n                                value=\"Refresh Models\",\n                                elem_id=GradioInputsIds.refresh_button\n                            )                   \n                        refresh_button.click(\n                            self.refresh_models, \n                            lora_files_selection, \n                            lora_files_selection\n                        )\n                    with gr.Accordion(label=\"Advanced Settings\", open=False, visible=False):\n                            with gr.Column():\n                                use_bias = gr.Checkbox(label=\"Enable Bias\", elem_id=GradioInputsIds.use_bias, value=lambda: True)\n                                use_linear = gr.Checkbox(label=\"Enable Linears\", elem_id=GradioInputsIds.use_linear, value=lambda: True)\n                                use_conv = gr.Checkbox(label=\"Enable Convolutions\", elem_id=GradioInputsIds.use_conv, value=lambda: True)\n                                use_emb = gr.Checkbox(label=\"Enable Embeddings\", elem_id=GradioInputsIds.use_emb, value=lambda: True)\n                                use_time = gr.Checkbox(label=\"Enable Time\", elem_id=GradioInputsIds.use_time, value=lambda: True)\n                            with gr.Column():\n                                use_multiplier = gr.Number(\n                                    label=\"Alpha Multiplier\",\n                                    elem_id=GradioInputsIds.use_multiplier,\n                                    value=1,\n                                )      \n\n\n        return self.return_ui_inputs(\n            return_args=[\n                lora_files_selection, \n                lora_alpha, \n                use_bias, \n                use_linear, \n                use_conv, \n                use_emb, \n                use_multiplier,\n                use_time\n            ]\n        )\n    \n    @torch.no_grad()\n    def process(\n        self, \n        p, \n        lora_files_selection, \n        lora_alpha, \n        use_bias, \n        use_linear, \n        use_conv, \n        use_emb, \n        use_multiplier,\n        use_time\n    ):\n\n        # Get the list of LoRA files based off of filepath.\n        lora_file_names = [x for x in lora_files_selection if x != \"None\"]   \n\n        if len(self.lora_files) <= 0:\n            paths_with_metadata, lora_names = self.get_lora_files()\n            self.lora_files = paths_with_metadata.copy()\n  \n        lora_files = self.get_loras_to_process(lora_file_names)\n\n        # Load multiple LoRAs\n        lora_files_list = []    \n\n        # Load our advanced options in a list\n        advanced_options = [\n            use_bias,\n            use_linear,\n            use_conv,\n            use_emb,\n            use_multiplier,\n            use_time\n        ]\n\n        # Save the previous alpha value so we can re-run the LoRA with new values.        \n        alpha_changed = self.handle_alpha_change(lora_alpha, p.sd_model)\n\n        # If an advanced option changes, re-run with new options\n        options_changed = self.handle_options_change(advanced_options, p.sd_model)\n\n        # Check if we changed our LoRA models we are loading\n        lora_changed = self.previous_lora_file_names != lora_file_names\n\n        first_lora_init = not self.is_lora_loaded(p.sd_model)\n\n        # If the LoRA is still loaded, unload it.\n        unload_args = [p.sd_model, None, use_bias, use_time, use_conv, use_emb, use_linear, None]\n        self.handle_lora_start(lora_files, p.sd_model, unload_args)    \n\n        can_use_lora = self.can_use_lora(p.sd_model)\n        \n        lora_params_changed = any([alpha_changed, lora_changed, options_changed])\n\n        # Process LoRA\n        if can_use_lora or lora_params_changed:\n\n            if len(lora_files) == 0: return\n\n            for i, model in enumerate([p.sd_model, p.clip_encoder]):\n                lora_alpha = (lora_alpha * use_multiplier) / len(lora_files)\n\n                lora_files_list = self.load_loras_from_list(lora_files)\n\n                args = [model, lora_files_list, use_bias, use_time, use_conv, use_emb, use_linear, lora_alpha]\n\n                if lora_params_changed and not first_lora_init :\n                    if i == 0:\n                        self.log(\"Resetting weights to reflect changed options.\")\n\n                    undo_args = args.copy()\n                    undo_args[1], undo_args[-1] = self.undo_merge_preprocess()\n\n                    self.process_lora(*undo_args, undo_merge=True)\n\n                self.process_lora(*args, undo_merge=False)\n                    \n            self.handle_after_lora_load(\n                p.sd_model, \n                lora_files,\n                lora_file_names, \n                advanced_options,\n                lora_alpha\n            )\n        \n        if len(lora_files) > 0 and not all([can_use_lora, lora_params_changed]):\n            self.log(f\"Using loaded LoRAs: {', '.join(lora_file_names)}\")", "GradioInputsIds = SimpleNamespace(**gr_inputs_dict)\n\nclass StableLoraScript(Text2VideoExtension, StableLoraProcessor):\n    \n    def __init__(self):\n        StableLoraProcessor.__init__(self)\n        Text2VideoExtension.__init__(self, EXTENSION_NAME, EXTENSION_TITLE)\n        self.device = 'cuda'\n        self.dtype = torch.float16\n\n    def title(self):\n            return EXTENSION_TITLE\n            \n    def refresh_models(self, *args):\n        paths_with_metadata, lora_names = self.get_lora_files()\n        self.lora_files = paths_with_metadata.copy()\n\n        return gr.Dropdown.update(value=[], choices=lora_names)\n\n    def ui(self):\n        paths_with_metadata, lora_names = self.get_lora_files()\n        self.lora_files = paths_with_metadata.copy()\n        REPOSITORY_LINK = \"https://github.com/ExponentialML/Text-To-Video-Finetuning\"\n\n        with gr.Accordion(label=EXTENSION_TITLE, open=False) as stable_lora_section:\n            with gr.Blocks(analytics_enabled=False):\n                with gr.Row():\n                    with gr.Column():\n                        gr.HTML(\"<h2>Load a Trained LoRA File.</h2>\")\n                        gr.HTML(\n                            \"\"\"\n                            <h3 style='color: crimson; font-weight: bold;'>\n                                Only Stable LoRA files are supported.\n                            </h3>\n                            \"\"\"\n                            )\n                        gr.HTML(f\"\"\"\n                        <a href='{REPOSITORY_LINK}'>\n                            To train a Stable LoRA file, use the finetune repository by clicking here.\n                        </a>\"\"\"\n                        )\n                        gr.HTML(f\"<span> Place your LoRA files in {cmd_opts.lora_dir}\")\n                        lora_files_selection = gr.Dropdown(\n                            label=\"Available Models\",\n                            elem_id=GradioInputsIds.lora_file_selection,\n                            choices=lora_names,\n                            value=[],\n                            multiselect=True,\n                        )\n                        lora_alpha = gr.Slider(\n                            minimum=0,\n                            maximum=1,\n                            value=1,\n                            step=0.05,\n                            elem_id=GradioInputsIds.lora_alpha,\n                            label=\"LoRA Weight\"\n                        )\n                        refresh_button = gr.Button(\n                                value=\"Refresh Models\",\n                                elem_id=GradioInputsIds.refresh_button\n                            )                   \n                        refresh_button.click(\n                            self.refresh_models, \n                            lora_files_selection, \n                            lora_files_selection\n                        )\n                    with gr.Accordion(label=\"Advanced Settings\", open=False, visible=False):\n                            with gr.Column():\n                                use_bias = gr.Checkbox(label=\"Enable Bias\", elem_id=GradioInputsIds.use_bias, value=lambda: True)\n                                use_linear = gr.Checkbox(label=\"Enable Linears\", elem_id=GradioInputsIds.use_linear, value=lambda: True)\n                                use_conv = gr.Checkbox(label=\"Enable Convolutions\", elem_id=GradioInputsIds.use_conv, value=lambda: True)\n                                use_emb = gr.Checkbox(label=\"Enable Embeddings\", elem_id=GradioInputsIds.use_emb, value=lambda: True)\n                                use_time = gr.Checkbox(label=\"Enable Time\", elem_id=GradioInputsIds.use_time, value=lambda: True)\n                            with gr.Column():\n                                use_multiplier = gr.Number(\n                                    label=\"Alpha Multiplier\",\n                                    elem_id=GradioInputsIds.use_multiplier,\n                                    value=1,\n                                )      \n\n\n        return self.return_ui_inputs(\n            return_args=[\n                lora_files_selection, \n                lora_alpha, \n                use_bias, \n                use_linear, \n                use_conv, \n                use_emb, \n                use_multiplier,\n                use_time\n            ]\n        )\n    \n    @torch.no_grad()\n    def process(\n        self, \n        p, \n        lora_files_selection, \n        lora_alpha, \n        use_bias, \n        use_linear, \n        use_conv, \n        use_emb, \n        use_multiplier,\n        use_time\n    ):\n\n        # Get the list of LoRA files based off of filepath.\n        lora_file_names = [x for x in lora_files_selection if x != \"None\"]   \n\n        if len(self.lora_files) <= 0:\n            paths_with_metadata, lora_names = self.get_lora_files()\n            self.lora_files = paths_with_metadata.copy()\n  \n        lora_files = self.get_loras_to_process(lora_file_names)\n\n        # Load multiple LoRAs\n        lora_files_list = []    \n\n        # Load our advanced options in a list\n        advanced_options = [\n            use_bias,\n            use_linear,\n            use_conv,\n            use_emb,\n            use_multiplier,\n            use_time\n        ]\n\n        # Save the previous alpha value so we can re-run the LoRA with new values.        \n        alpha_changed = self.handle_alpha_change(lora_alpha, p.sd_model)\n\n        # If an advanced option changes, re-run with new options\n        options_changed = self.handle_options_change(advanced_options, p.sd_model)\n\n        # Check if we changed our LoRA models we are loading\n        lora_changed = self.previous_lora_file_names != lora_file_names\n\n        first_lora_init = not self.is_lora_loaded(p.sd_model)\n\n        # If the LoRA is still loaded, unload it.\n        unload_args = [p.sd_model, None, use_bias, use_time, use_conv, use_emb, use_linear, None]\n        self.handle_lora_start(lora_files, p.sd_model, unload_args)    \n\n        can_use_lora = self.can_use_lora(p.sd_model)\n        \n        lora_params_changed = any([alpha_changed, lora_changed, options_changed])\n\n        # Process LoRA\n        if can_use_lora or lora_params_changed:\n\n            if len(lora_files) == 0: return\n\n            for i, model in enumerate([p.sd_model, p.clip_encoder]):\n                lora_alpha = (lora_alpha * use_multiplier) / len(lora_files)\n\n                lora_files_list = self.load_loras_from_list(lora_files)\n\n                args = [model, lora_files_list, use_bias, use_time, use_conv, use_emb, use_linear, lora_alpha]\n\n                if lora_params_changed and not first_lora_init :\n                    if i == 0:\n                        self.log(\"Resetting weights to reflect changed options.\")\n\n                    undo_args = args.copy()\n                    undo_args[1], undo_args[-1] = self.undo_merge_preprocess()\n\n                    self.process_lora(*undo_args, undo_merge=True)\n\n                self.process_lora(*args, undo_merge=False)\n                    \n            self.handle_after_lora_load(\n                p.sd_model, \n                lora_files,\n                lora_file_names, \n                advanced_options,\n                lora_alpha\n            )\n        \n        if len(lora_files) > 0 and not all([can_use_lora, lora_params_changed]):\n            self.log(f\"Using loaded LoRAs: {', '.join(lora_file_names)}\")", "            \nStableLoraScriptInstance = StableLoraScript()"]}
{"filename": "scripts/stable_lora/stable_utils/lora_processor.py", "chunked_list": ["import os\nimport glob\nimport torch\n\nfrom safetensors.torch import load_file\nfrom safetensors import safe_open\nfrom modules.shared import opts, cmd_opts, state\n\nclass StableLoraProcessor:\n    def __init__(self):\n        self.lora_loaded = 'lora_loaded' \n        self.previous_lora_alpha = 1\n        self.current_sd_checkpoint = \"\"\n        self.previous_lora_file_names = []\n        self.previous_advanced_options = []\n        self.lora_files = []\n\n    def get_lora_files(self):\n        paths_with_metadata = []\n        paths = glob.glob(os.path.join(cmd_opts.lora_dir, '**/*.safetensors'), recursive=True)\n        lora_names = []\n        \n        for lora_path in paths:\n            with safe_open(lora_path, 'pt') as lora_file:\n                metadata = lora_file.metadata()\n                if metadata is not None and 'stable_lora_text_to_video' in metadata.keys():\n                    metadata['path'] = lora_path\n                    metadata['lora_name'] = os.path.splitext(os.path.basename(lora_path))[0]\n                    paths_with_metadata.append(metadata)\n\n        if len(paths_with_metadata) > 0:\n            lora_names = [x['lora_name'] for x in paths_with_metadata]\n\n        return paths_with_metadata, lora_names\n\n    def key_name_match(self, value, key, name):\n        return value in key and name == key.split(f\".{value}\")[0]\n\n    def is_lora_match(self, key, name):\n        return self.key_name_match('lora_A', key, name)\n\n    def is_bias_match(self, key, name):\n        return self.key_name_match(\"bias\", key, name)\n\n    def lora_rank(self, weight): return min(weight.shape)\n\n    def get_lora_alpha(self, alpha): \n        return alpha\n\n    def process_lora_weight(self, weight, lora_weight, alpha, undo_merge=False):\n        new_weight = weight.detach().clone()\n        \n        if not undo_merge:\n            new_weight += lora_weight.to(weight.device, weight.dtype) * alpha\n        else:\n            new_weight -= lora_weight.to(weight.device, weight.dtype) * alpha\n\n        return torch.nn.Parameter(new_weight.to(weight.device, weight.dtype))\n\n    def lora_linear_forward(\n        self, \n        weight, \n        lora_A, \n        lora_B, \n        alpha, \n        undo_merge=False, \n        *args\n    ):\n        l_alpha = self.get_lora_alpha(alpha)\n        lora_weight = (lora_B @ lora_A)\n\n        return self.process_lora_weight(weight, lora_weight, l_alpha, undo_merge=undo_merge)\n\n    def lora_conv_forward(\n        self, \n        weight, \n        lora_A, \n        lora_B, \n        alpha, \n        undo_merge=False, \n        is_temporal=False, \n        *args\n    ):\n        l_alpha = self.get_lora_alpha(alpha)\n        view_shape = weight.shape\n\n        if is_temporal:\n            i, o, k = weight.shape[:3]\n            view_shape = (i, o, k, k, 1)\n            \n        lora_weight = (lora_B @ lora_A).view(view_shape) \n        \n        if is_temporal:\n            lora_weight = torch.mean(lora_weight, dim=-2, keepdim=True)\n\n        return self.process_lora_weight(weight, lora_weight, l_alpha, undo_merge=undo_merge)\n\n    def lora_emb_forward(self, lora_A, lora_B, alpha, undo_merge=False, *args):\n        l_alpha = self.get_lora_alpha(alpha)\n\n        return (lora_B @ lora_A).transpose(0, 1) * l_alpha\n\n    def is_lora_loaded(self, sd_model):\n        return hasattr(sd_model, self.lora_loaded)\n\n    def get_loras_to_process(self, lora_files):\n        lora_files_to_load = []\n\n        for file_name in lora_files:\n            if len(self.lora_files) > 0:\n                for f in self.lora_files:\n                    if file_name == f['lora_name']:\n                        lora_files_to_load.append(f['path'])\n    \n        return lora_files_to_load\n\n    def handle_lora_load(\n        self, \n        sd_model, \n        lora_files_list, \n        set_lora_loaded=False, \n        unload_args=[]\n    ):\n        if not hasattr(sd_model, self.lora_loaded) and set_lora_loaded:\n            setattr(sd_model, self.lora_loaded, True)\n\n        if self.is_lora_loaded(sd_model) and not set_lora_loaded:\n            unload_args[1], unload_args[-1] = self.undo_merge_preprocess()\n            self.process_lora(*unload_args, undo_merge=True)\n            delattr(sd_model, self.lora_loaded)\n\n    def handle_alpha_change(self, lora_alpha, model):\n        return (lora_alpha != self.previous_lora_alpha) \\\n            and self.is_lora_loaded(model)\n\n    def handle_options_change(self, options, model):\n        return (options != self.previous_advanced_options) \\\n            and self.is_lora_loaded(model)\n    \n    def handle_lora_start(self, lora_files, model, unload_args):\n        if len(lora_files) == 0 and self.is_lora_loaded(model):\n            self.handle_lora_load(\n                model, \n                lora_files, \n                set_lora_loaded=False, \n                unload_args=unload_args\n            )\n    \n            self.log(f\"Unloaded previously loaded LoRA files\")\n            return\n\n    def can_use_lora(self, model):\n        return not self.is_lora_loaded(model)\n\n    def load_loras_from_list(self, lora_files):\n        lora_files_list = []\n\n        for lora_file in lora_files:\n            LORA_FILE = lora_file.split('/')[-1]\n            LORA_DIR = cmd_opts.lora_dir\n            LORA_PATH = f\"{LORA_DIR}/{LORA_FILE}\"\n\n            lora_model_text_path = f\"{LORA_DIR}/text_{LORA_FILE}\"\n            lora_text_exists = os.path.exists(lora_model_text_path)\n            \n            is_safetensors = LORA_PATH.endswith('.safetensors')\n            load_method = load_file if is_safetensors else torch.load\n            \n            lora_model = load_method(LORA_PATH)\n\n            lora_files_list.append(lora_model)\n\n        return lora_files_list\n\n    def handle_after_lora_load(\n        self, \n        model, \n        lora_files,\n        lora_file_names, \n        advanced_options, \n        lora_alpha\n    ):\n        lora_summary = []\n        self.handle_lora_load(model, lora_files, set_lora_loaded=True)\n        self.previous_lora_file_names = lora_file_names\n        self.previous_advanced_options = advanced_options\n        self.previous_lora_alpha = lora_alpha\n\n        for lora_file_name in lora_file_names:\n            if self.is_lora_loaded(model):\n                lora_summary.append(f\"{lora_file_name.split('.')[0]}\")\n        \n        if len(lora_summary) > 0:\n            self.log(f\"Using {model.__class__.__name__} LoRA(s):\", *lora_summary)\n\n    def undo_merge_preprocess(self):\n        previous_lora_files_list = self.get_loras_to_process(self.previous_lora_file_names)\n        previous_lora_files = self.load_loras_from_list(previous_lora_files_list)\n\n        return previous_lora_files, self.previous_lora_alpha\n\n    @torch.autocast('cuda')\n    def process_lora(\n        self, \n        model, \n        lora_files_list, \n        use_bias, \n        use_time, \n        use_conv, \n        use_emb, \n        use_linear,\n        lora_alpha, \n        undo_merge=False\n    ):\n        for n, m in model.named_modules():\n            for lora_model in lora_files_list:\n                for k, v in lora_model.items():\n                    \n                    # If there is bias in the LoRA, add it here.\n                    if self.is_bias_match(k, n) and use_bias:\n                        if m.bias is None:\n                            m.bias = torch.nn.Parameter(v.to(self.device, dtype=self.dtype))\n                        else:\n                            m.bias.weight = v.to(self.device, dtype=self.dtype)\n    \n                    if self.is_lora_match(k, n):\n                        lora_A = lora_model[f\"{n}.lora_A\"].to(self.device, dtype=self.dtype)\n                        lora_B = lora_model[f\"{n}.lora_B\"].to(self.device, dtype=self.dtype)\n\n                        forward_args = [m.weight, lora_A, lora_B, lora_alpha]\n\n                        if isinstance(m, torch.nn.Linear) and use_linear:\n                            if 'proj' in n:\n                                forward_args[1], forward_args[2] = map(lambda l: l.squeeze(-1), (lora_A, lora_B))\n\n                            m.weight = self.lora_linear_forward(*forward_args, undo_merge=undo_merge)\n\n                        if isinstance(m, torch.nn.Conv2d) and use_conv:\n                            m.weight = self.lora_conv_forward(*forward_args, undo_merge=undo_merge, is_temporal=False) \n\n                        if isinstance(m, torch.nn.Conv3d) and use_conv and use_time:\n                            m.weight = self.lora_conv_forward(*forward_args, undo_merge=undo_merge, is_temporal=True) \n\n                        if isinstance(m, torch.nn.Embedding) and use_emb:\n                            embedding_weight = self.lora_emb_forward(lora_A, lora_B, lora_alpha, undo_merge=undo_merge)\n                            new_embedding_weight = torch.nn.Embedding.from_pretrained(embedding_weight)", "class StableLoraProcessor:\n    def __init__(self):\n        self.lora_loaded = 'lora_loaded' \n        self.previous_lora_alpha = 1\n        self.current_sd_checkpoint = \"\"\n        self.previous_lora_file_names = []\n        self.previous_advanced_options = []\n        self.lora_files = []\n\n    def get_lora_files(self):\n        paths_with_metadata = []\n        paths = glob.glob(os.path.join(cmd_opts.lora_dir, '**/*.safetensors'), recursive=True)\n        lora_names = []\n        \n        for lora_path in paths:\n            with safe_open(lora_path, 'pt') as lora_file:\n                metadata = lora_file.metadata()\n                if metadata is not None and 'stable_lora_text_to_video' in metadata.keys():\n                    metadata['path'] = lora_path\n                    metadata['lora_name'] = os.path.splitext(os.path.basename(lora_path))[0]\n                    paths_with_metadata.append(metadata)\n\n        if len(paths_with_metadata) > 0:\n            lora_names = [x['lora_name'] for x in paths_with_metadata]\n\n        return paths_with_metadata, lora_names\n\n    def key_name_match(self, value, key, name):\n        return value in key and name == key.split(f\".{value}\")[0]\n\n    def is_lora_match(self, key, name):\n        return self.key_name_match('lora_A', key, name)\n\n    def is_bias_match(self, key, name):\n        return self.key_name_match(\"bias\", key, name)\n\n    def lora_rank(self, weight): return min(weight.shape)\n\n    def get_lora_alpha(self, alpha): \n        return alpha\n\n    def process_lora_weight(self, weight, lora_weight, alpha, undo_merge=False):\n        new_weight = weight.detach().clone()\n        \n        if not undo_merge:\n            new_weight += lora_weight.to(weight.device, weight.dtype) * alpha\n        else:\n            new_weight -= lora_weight.to(weight.device, weight.dtype) * alpha\n\n        return torch.nn.Parameter(new_weight.to(weight.device, weight.dtype))\n\n    def lora_linear_forward(\n        self, \n        weight, \n        lora_A, \n        lora_B, \n        alpha, \n        undo_merge=False, \n        *args\n    ):\n        l_alpha = self.get_lora_alpha(alpha)\n        lora_weight = (lora_B @ lora_A)\n\n        return self.process_lora_weight(weight, lora_weight, l_alpha, undo_merge=undo_merge)\n\n    def lora_conv_forward(\n        self, \n        weight, \n        lora_A, \n        lora_B, \n        alpha, \n        undo_merge=False, \n        is_temporal=False, \n        *args\n    ):\n        l_alpha = self.get_lora_alpha(alpha)\n        view_shape = weight.shape\n\n        if is_temporal:\n            i, o, k = weight.shape[:3]\n            view_shape = (i, o, k, k, 1)\n            \n        lora_weight = (lora_B @ lora_A).view(view_shape) \n        \n        if is_temporal:\n            lora_weight = torch.mean(lora_weight, dim=-2, keepdim=True)\n\n        return self.process_lora_weight(weight, lora_weight, l_alpha, undo_merge=undo_merge)\n\n    def lora_emb_forward(self, lora_A, lora_B, alpha, undo_merge=False, *args):\n        l_alpha = self.get_lora_alpha(alpha)\n\n        return (lora_B @ lora_A).transpose(0, 1) * l_alpha\n\n    def is_lora_loaded(self, sd_model):\n        return hasattr(sd_model, self.lora_loaded)\n\n    def get_loras_to_process(self, lora_files):\n        lora_files_to_load = []\n\n        for file_name in lora_files:\n            if len(self.lora_files) > 0:\n                for f in self.lora_files:\n                    if file_name == f['lora_name']:\n                        lora_files_to_load.append(f['path'])\n    \n        return lora_files_to_load\n\n    def handle_lora_load(\n        self, \n        sd_model, \n        lora_files_list, \n        set_lora_loaded=False, \n        unload_args=[]\n    ):\n        if not hasattr(sd_model, self.lora_loaded) and set_lora_loaded:\n            setattr(sd_model, self.lora_loaded, True)\n\n        if self.is_lora_loaded(sd_model) and not set_lora_loaded:\n            unload_args[1], unload_args[-1] = self.undo_merge_preprocess()\n            self.process_lora(*unload_args, undo_merge=True)\n            delattr(sd_model, self.lora_loaded)\n\n    def handle_alpha_change(self, lora_alpha, model):\n        return (lora_alpha != self.previous_lora_alpha) \\\n            and self.is_lora_loaded(model)\n\n    def handle_options_change(self, options, model):\n        return (options != self.previous_advanced_options) \\\n            and self.is_lora_loaded(model)\n    \n    def handle_lora_start(self, lora_files, model, unload_args):\n        if len(lora_files) == 0 and self.is_lora_loaded(model):\n            self.handle_lora_load(\n                model, \n                lora_files, \n                set_lora_loaded=False, \n                unload_args=unload_args\n            )\n    \n            self.log(f\"Unloaded previously loaded LoRA files\")\n            return\n\n    def can_use_lora(self, model):\n        return not self.is_lora_loaded(model)\n\n    def load_loras_from_list(self, lora_files):\n        lora_files_list = []\n\n        for lora_file in lora_files:\n            LORA_FILE = lora_file.split('/')[-1]\n            LORA_DIR = cmd_opts.lora_dir\n            LORA_PATH = f\"{LORA_DIR}/{LORA_FILE}\"\n\n            lora_model_text_path = f\"{LORA_DIR}/text_{LORA_FILE}\"\n            lora_text_exists = os.path.exists(lora_model_text_path)\n            \n            is_safetensors = LORA_PATH.endswith('.safetensors')\n            load_method = load_file if is_safetensors else torch.load\n            \n            lora_model = load_method(LORA_PATH)\n\n            lora_files_list.append(lora_model)\n\n        return lora_files_list\n\n    def handle_after_lora_load(\n        self, \n        model, \n        lora_files,\n        lora_file_names, \n        advanced_options, \n        lora_alpha\n    ):\n        lora_summary = []\n        self.handle_lora_load(model, lora_files, set_lora_loaded=True)\n        self.previous_lora_file_names = lora_file_names\n        self.previous_advanced_options = advanced_options\n        self.previous_lora_alpha = lora_alpha\n\n        for lora_file_name in lora_file_names:\n            if self.is_lora_loaded(model):\n                lora_summary.append(f\"{lora_file_name.split('.')[0]}\")\n        \n        if len(lora_summary) > 0:\n            self.log(f\"Using {model.__class__.__name__} LoRA(s):\", *lora_summary)\n\n    def undo_merge_preprocess(self):\n        previous_lora_files_list = self.get_loras_to_process(self.previous_lora_file_names)\n        previous_lora_files = self.load_loras_from_list(previous_lora_files_list)\n\n        return previous_lora_files, self.previous_lora_alpha\n\n    @torch.autocast('cuda')\n    def process_lora(\n        self, \n        model, \n        lora_files_list, \n        use_bias, \n        use_time, \n        use_conv, \n        use_emb, \n        use_linear,\n        lora_alpha, \n        undo_merge=False\n    ):\n        for n, m in model.named_modules():\n            for lora_model in lora_files_list:\n                for k, v in lora_model.items():\n                    \n                    # If there is bias in the LoRA, add it here.\n                    if self.is_bias_match(k, n) and use_bias:\n                        if m.bias is None:\n                            m.bias = torch.nn.Parameter(v.to(self.device, dtype=self.dtype))\n                        else:\n                            m.bias.weight = v.to(self.device, dtype=self.dtype)\n    \n                    if self.is_lora_match(k, n):\n                        lora_A = lora_model[f\"{n}.lora_A\"].to(self.device, dtype=self.dtype)\n                        lora_B = lora_model[f\"{n}.lora_B\"].to(self.device, dtype=self.dtype)\n\n                        forward_args = [m.weight, lora_A, lora_B, lora_alpha]\n\n                        if isinstance(m, torch.nn.Linear) and use_linear:\n                            if 'proj' in n:\n                                forward_args[1], forward_args[2] = map(lambda l: l.squeeze(-1), (lora_A, lora_B))\n\n                            m.weight = self.lora_linear_forward(*forward_args, undo_merge=undo_merge)\n\n                        if isinstance(m, torch.nn.Conv2d) and use_conv:\n                            m.weight = self.lora_conv_forward(*forward_args, undo_merge=undo_merge, is_temporal=False) \n\n                        if isinstance(m, torch.nn.Conv3d) and use_conv and use_time:\n                            m.weight = self.lora_conv_forward(*forward_args, undo_merge=undo_merge, is_temporal=True) \n\n                        if isinstance(m, torch.nn.Embedding) and use_emb:\n                            embedding_weight = self.lora_emb_forward(lora_A, lora_B, lora_alpha, undo_merge=undo_merge)\n                            new_embedding_weight = torch.nn.Embedding.from_pretrained(embedding_weight)", ""]}
{"filename": "scripts/modelscope/process_modelscope.py", "chunked_list": ["# Function calls referenced from https://github.com/modelscope/modelscope/tree/master/modelscope/pipelines/multi_modal\n\n# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# Read LICENSE for usage terms.\n\nfrom base64 import b64encode\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom modelscope.t2v_pipeline import TextToVideoSynthesis, tensor2vid\nfrom t2v_helpers.key_frames import T2VAnimKeys  # TODO: move to deforum_tools", "from modelscope.t2v_pipeline import TextToVideoSynthesis, tensor2vid\nfrom t2v_helpers.key_frames import T2VAnimKeys  # TODO: move to deforum_tools\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport cv2\nimport gc\nimport modules.paths as ph\nfrom types import SimpleNamespace\nfrom t2v_helpers.general_utils import get_t2v_version, get_model_location", "from types import SimpleNamespace\nfrom t2v_helpers.general_utils import get_t2v_version, get_model_location\nimport time, math\nfrom t2v_helpers.video_audio_utils import ffmpeg_stitch_video, get_quick_vid_info, vid2frames, duplicate_pngs_from_folder, clean_folder_name\nfrom t2v_helpers.args import get_outdir, process_args\nimport t2v_helpers.args as t2v_helpers_args\nfrom modules import shared, sd_hijack, lowvram\nfrom modules.shared import opts, devices, state\nfrom stable_lora.scripts.lora_webui import gr_inputs_list, StableLoraScriptInstance\nimport os", "from stable_lora.scripts.lora_webui import gr_inputs_list, StableLoraScriptInstance\nimport os\n\npipe = None\n\ndef setup_pipeline(model_name):\n    return TextToVideoSynthesis(get_model_location(model_name))\n\ndef process_modelscope(args_dict, extra_args=None):\n    args, video_args = process_args(args_dict)\n\n    global pipe\n    print(f\"\\033[4;33m text2video extension for auto1111 webui\\033[0m\")\n    print(f\"Git commit: {get_t2v_version()}\")\n    init_timestring = time.strftime('%Y%m%d%H%M%S')\n    outdir_current = os.path.join(get_outdir(), f\"{init_timestring}\")\n\n    max_vids_to_pack = opts.data.get(\"modelscope_deforum_show_n_videos\") if opts.data is not None and opts.data.get(\"modelscope_deforum_show_n_videos\") is not None else -1\n    cpu_vae = opts.data.get(\"modelscope_deforum_vae_settings\") if opts.data is not None and opts.data.get(\"modelscope_deforum_vae_settings\") is not None else 'GPU (half precision)'\n    if shared.sd_model is not None:\n        sd_hijack.model_hijack.undo_hijack(shared.sd_model)\n        try:\n            lowvram.send_everything_to_cpu()\n        except Exception as e:\n            pass\n        # the following command actually frees the GPU vram from the sd.model, no need to do del shared.sd_model 22-05-23\n        shared.sd_model = None\n    gc.collect()\n    devices.torch_gc()\n\n    print('Starting text2video')\n    print('Pipeline setup')\n\n    # optionally store pipe in global between runs\n    # also refresh the model if the user selected a newer one\n    # if args.model is none (e.g. an API call, the model stays as the previous one)\n    if pipe is None and args.model is None: # one more API call hack, falling back to <modelscope> if never used TODO: figure out how to permastore the model name the best way\n        args.model = \"<modelscope>\"\n        print(f\"WARNING: received an API call with an empty model name, defaulting to {args.model} at {get_model_location(args.model)}\")\n    if pipe is None or pipe is not None and args.model is not None and get_model_location(args.model) != pipe.model_dir:\n        pipe = setup_pipeline(args.model)\n\n    #TODO Wrap this in a list so that we can process this for future extensions.\n    stable_lora_processor = StableLoraScriptInstance\n    stable_lora_args = stable_lora_processor.process_extension_args(all_args=extra_args) \n    stable_lora_processor.process(pipe, *stable_lora_args)\n\n    pipe.keep_in_vram = opts.data.get(\"modelscope_deforum_keep_model_in_vram\") if opts.data is not None and opts.data.get(\"modelscope_deforum_keep_model_in_vram\") is not None else 'None'\n\n    device = devices.get_optimal_device()\n    print('device', device)\n\n    mask = None\n\n    if args.do_vid2vid:\n        if args.vid2vid_frames is None and args.vid2vid_frames_path == \"\":\n            raise FileNotFoundError(\"Please upload a video :()\")\n\n        # Overrides\n        if args.vid2vid_frames is not None:\n            vid2vid_frames_path = args.vid2vid_frames.name\n\n        print(\"got a request to *vid2vid* an existing video.\")\n\n        in_vid_fps, _, _ = get_quick_vid_info(vid2vid_frames_path)\n        folder_name = clean_folder_name(Path(vid2vid_frames_path).stem)\n        outdir_no_tmp = os.path.join(os.getcwd(), 'outputs', 'frame-vid2vid', folder_name)\n        i = 1\n        while os.path.exists(outdir_no_tmp):\n            outdir_no_tmp = os.path.join(os.getcwd(), 'outputs', 'frame-vid2vid', folder_name + '_' + str(i))\n            i += 1\n\n        outdir_v2v = os.path.join(outdir_no_tmp, 'tmp_input_frames')\n        os.makedirs(outdir_v2v, exist_ok=True)\n\n        vid2frames(video_path=vid2vid_frames_path, video_in_frame_path=outdir_v2v, overwrite=True, extract_from_frame=args.vid2vid_startFrame, extract_to_frame=args.vid2vid_startFrame + args.frames,\n                   numeric_files_output=True, out_img_format='png')\n\n        temp_convert_raw_png_path = os.path.join(outdir_v2v, \"tmp_vid2vid_folder\")\n        duplicate_pngs_from_folder(outdir_v2v, temp_convert_raw_png_path, None, folder_name)\n\n        videogen = []\n        for f in os.listdir(temp_convert_raw_png_path):\n            # double check for old _depth_ files, not really needed probably but keeping it for now\n            if '_depth_' not in f:\n                videogen.append(f)\n\n        videogen.sort(key=lambda x: int(x.split('.')[0]))\n\n        images = []\n        for file in tqdm(videogen, desc=\"Loading frames\"):\n            image = Image.open(os.path.join(temp_convert_raw_png_path, file))\n            image = image.resize((args.width, args.height), Image.ANTIALIAS)\n            array = np.array(image)\n            images += [array]\n\n        # print(images)\n\n        images = np.stack(images)  # f h w c\n        batches = 1\n        n_images = np.tile(images[np.newaxis, ...], (batches, 1, 1, 1, 1))  # n f h w c\n        bcfhw = n_images.transpose(0, 4, 1, 2, 3)\n        # convert to 0-1 float\n        bcfhw = bcfhw.astype(np.float32) / 255\n        bfchw = bcfhw.transpose(0, 2, 1, 3, 4)  # b c f h w\n\n        print(f\"Converted the frames to tensor {bfchw.shape}\")\n\n        vd_out = torch.from_numpy(bcfhw).to(\"cuda\")\n\n        # should be -1,1, not 0,1\n        vd_out = 2 * vd_out - 1\n\n        # latents should have shape num_sample, 4, max_frames, latent_h,latent_w\n        print(\"Computing latents\")\n        latents = pipe.compute_latents(vd_out).to(device)\n\n        skip_steps = int(math.floor(args.steps * max(0, min(1 - args.strength, 1))))\n    else:\n        latents = None\n        args.strength = 1\n        skip_steps = 0\n\n    print('Working in txt2vid mode' if not args.do_vid2vid else 'Working in vid2vid mode')\n\n    # Start the batch count loop\n    pbar = tqdm(range(args.batch_count), leave=False)\n    if args.batch_count == 1:\n        pbar.disable = True\n\n    vids_to_pack = []\n\n    state.job_count = args.batch_count\n\n    for batch in pbar:\n        state.job_no = batch\n        if state.skipped:\n            state.skipped = False\n\n        if state.interrupted:\n            break\n\n        shared.state.job = f\"Batch {batch + 1} out of {args.batch_count}\"\n        # TODO: move to a separate function\n        if args.inpainting_frames > 0 and hasattr(args.inpainting_image, \"name\"):\n            keys = T2VAnimKeys(SimpleNamespace(**{'max_frames': args.frames, 'inpainting_weights': args.inpainting_weights}), args.seed, args.inpainting_frames)\n            images = []\n            print(\"Received an image for inpainting\", args.inpainting_image.name)\n            for i in range(args.frames):\n                image = Image.open(args.inpainting_image.name).convert(\"RGB\")\n                image = image.resize((args.width, args.height), Image.ANTIALIAS)\n                array = np.array(image)\n                images += [array]\n\n            images = np.stack(images)  # f h w c\n            batches = 1\n            n_images = np.tile(images[np.newaxis, ...], (batches, 1, 1, 1, 1))  # n f h w c\n            bcfhw = n_images.transpose(0, 4, 1, 2, 3)\n            # convert to 0-1 float\n            bcfhw = bcfhw.astype(np.float32) / 255\n            bfchw = bcfhw.transpose(0, 2, 1, 3, 4)  # b c f h w\n\n            print(f\"Converted the frames to tensor {bfchw.shape}\")\n\n            vd_out = torch.from_numpy(bcfhw).to(\"cuda\")\n\n            # should be -1,1, not 0,1\n            vd_out = 2 * vd_out - 1\n\n            # latents should have shape num_sample, 4, max_frames, latent_h,latent_w\n            # but right now they have shape num_sample=1,4, 1 (only used 1 img), latent_h, latent_w\n            print(\"Computing latents\")\n            image_latents = pipe.compute_latents(vd_out).numpy()\n            # padding_width = [(0, 0), (0, 0), (0, frames-inpainting_frames), (0, 0), (0, 0)]\n            # padded_latents = np.pad(image_latents, pad_width=padding_width, mode='constant', constant_values=0)\n\n            latent_h = args.height // 8\n            latent_w = args.width // 8\n            latent_noise = np.random.normal(size=(1, 4, args.frames, latent_h, latent_w))\n            mask = np.ones(shape=(1, 4, args.frames, latent_h, latent_w))\n\n            mask_weights = [keys.inpainting_weights_series[frame_idx] for frame_idx in range(args.frames)]\n\n            for i in range(args.frames):\n                v = mask_weights[i]\n                mask[:, :, i, :, :] = v\n\n            masked_latents = image_latents * (1 - mask) + latent_noise * mask\n\n            latents = torch.tensor(masked_latents).to(device)\n\n            mask = torch.tensor(mask).to(device)\n\n            args.strength = 1\n\n        samples, _ = pipe.infer(args.prompt, args.n_prompt, args.steps, args.frames, args.seed + batch if args.seed != -1 else -1, args.cfg_scale,\n                                args.width, args.height, args.eta, cpu_vae, device, latents, strength=args.strength, skip_steps=skip_steps, mask=mask, is_vid2vid=args.do_vid2vid, sampler=args.sampler)\n\n        if batch > 0:\n            outdir_current = os.path.join(get_outdir(), f\"{init_timestring}_{batch}\")\n        print(f'text2video finished, saving frames to {outdir_current}')\n\n        # just deleted the folder so we need to make it again\n        os.makedirs(outdir_current, exist_ok=True)\n        for i in range(len(samples)):\n            cv2.imwrite(outdir_current + os.path.sep +\n                        f\"{i:06}.png\", samples[i])\n\n        # TODO: add params to the GUI\n        if not video_args.skip_video_creation:\n            ffmpeg_stitch_video(ffmpeg_location=video_args.ffmpeg_location, fps=video_args.fps, outmp4_path=outdir_current + os.path.sep + f\"vid.mp4\", imgs_path=os.path.join(outdir_current,\n                                                                                                                                                                              \"%06d.png\"),\n                                stitch_from_frame=0, stitch_to_frame=-1, add_soundtrack=video_args.add_soundtrack,\n                                audio_path=vid2vid_frames_path if video_args.add_soundtrack == 'Init Video' else video_args.soundtrack_path, crf=video_args.ffmpeg_crf, preset=video_args.ffmpeg_preset)\n        print(f't2v complete, result saved at {outdir_current}')\n\n        mp4 = open(outdir_current + os.path.sep + f\"vid.mp4\", 'rb').read()\n        dataurl = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n\n        if max_vids_to_pack == -1 or len(vids_to_pack) < max_vids_to_pack:\n            vids_to_pack.append(dataurl)\n    t2v_helpers_args.i1_store_t2v = f'<p style=\\\"font-weight:bold;margin-bottom:0em\\\">text2video extension for auto1111 \u2014 version 1.2b </p>'\n    for dataurl in vids_to_pack:\n        t2v_helpers_args.i1_store_t2v += f'<video controls loop><source src=\"{dataurl}\" type=\"video/mp4\"></video><br>'\n    pbar.close()\n    return vids_to_pack", "def process_modelscope(args_dict, extra_args=None):\n    args, video_args = process_args(args_dict)\n\n    global pipe\n    print(f\"\\033[4;33m text2video extension for auto1111 webui\\033[0m\")\n    print(f\"Git commit: {get_t2v_version()}\")\n    init_timestring = time.strftime('%Y%m%d%H%M%S')\n    outdir_current = os.path.join(get_outdir(), f\"{init_timestring}\")\n\n    max_vids_to_pack = opts.data.get(\"modelscope_deforum_show_n_videos\") if opts.data is not None and opts.data.get(\"modelscope_deforum_show_n_videos\") is not None else -1\n    cpu_vae = opts.data.get(\"modelscope_deforum_vae_settings\") if opts.data is not None and opts.data.get(\"modelscope_deforum_vae_settings\") is not None else 'GPU (half precision)'\n    if shared.sd_model is not None:\n        sd_hijack.model_hijack.undo_hijack(shared.sd_model)\n        try:\n            lowvram.send_everything_to_cpu()\n        except Exception as e:\n            pass\n        # the following command actually frees the GPU vram from the sd.model, no need to do del shared.sd_model 22-05-23\n        shared.sd_model = None\n    gc.collect()\n    devices.torch_gc()\n\n    print('Starting text2video')\n    print('Pipeline setup')\n\n    # optionally store pipe in global between runs\n    # also refresh the model if the user selected a newer one\n    # if args.model is none (e.g. an API call, the model stays as the previous one)\n    if pipe is None and args.model is None: # one more API call hack, falling back to <modelscope> if never used TODO: figure out how to permastore the model name the best way\n        args.model = \"<modelscope>\"\n        print(f\"WARNING: received an API call with an empty model name, defaulting to {args.model} at {get_model_location(args.model)}\")\n    if pipe is None or pipe is not None and args.model is not None and get_model_location(args.model) != pipe.model_dir:\n        pipe = setup_pipeline(args.model)\n\n    #TODO Wrap this in a list so that we can process this for future extensions.\n    stable_lora_processor = StableLoraScriptInstance\n    stable_lora_args = stable_lora_processor.process_extension_args(all_args=extra_args) \n    stable_lora_processor.process(pipe, *stable_lora_args)\n\n    pipe.keep_in_vram = opts.data.get(\"modelscope_deforum_keep_model_in_vram\") if opts.data is not None and opts.data.get(\"modelscope_deforum_keep_model_in_vram\") is not None else 'None'\n\n    device = devices.get_optimal_device()\n    print('device', device)\n\n    mask = None\n\n    if args.do_vid2vid:\n        if args.vid2vid_frames is None and args.vid2vid_frames_path == \"\":\n            raise FileNotFoundError(\"Please upload a video :()\")\n\n        # Overrides\n        if args.vid2vid_frames is not None:\n            vid2vid_frames_path = args.vid2vid_frames.name\n\n        print(\"got a request to *vid2vid* an existing video.\")\n\n        in_vid_fps, _, _ = get_quick_vid_info(vid2vid_frames_path)\n        folder_name = clean_folder_name(Path(vid2vid_frames_path).stem)\n        outdir_no_tmp = os.path.join(os.getcwd(), 'outputs', 'frame-vid2vid', folder_name)\n        i = 1\n        while os.path.exists(outdir_no_tmp):\n            outdir_no_tmp = os.path.join(os.getcwd(), 'outputs', 'frame-vid2vid', folder_name + '_' + str(i))\n            i += 1\n\n        outdir_v2v = os.path.join(outdir_no_tmp, 'tmp_input_frames')\n        os.makedirs(outdir_v2v, exist_ok=True)\n\n        vid2frames(video_path=vid2vid_frames_path, video_in_frame_path=outdir_v2v, overwrite=True, extract_from_frame=args.vid2vid_startFrame, extract_to_frame=args.vid2vid_startFrame + args.frames,\n                   numeric_files_output=True, out_img_format='png')\n\n        temp_convert_raw_png_path = os.path.join(outdir_v2v, \"tmp_vid2vid_folder\")\n        duplicate_pngs_from_folder(outdir_v2v, temp_convert_raw_png_path, None, folder_name)\n\n        videogen = []\n        for f in os.listdir(temp_convert_raw_png_path):\n            # double check for old _depth_ files, not really needed probably but keeping it for now\n            if '_depth_' not in f:\n                videogen.append(f)\n\n        videogen.sort(key=lambda x: int(x.split('.')[0]))\n\n        images = []\n        for file in tqdm(videogen, desc=\"Loading frames\"):\n            image = Image.open(os.path.join(temp_convert_raw_png_path, file))\n            image = image.resize((args.width, args.height), Image.ANTIALIAS)\n            array = np.array(image)\n            images += [array]\n\n        # print(images)\n\n        images = np.stack(images)  # f h w c\n        batches = 1\n        n_images = np.tile(images[np.newaxis, ...], (batches, 1, 1, 1, 1))  # n f h w c\n        bcfhw = n_images.transpose(0, 4, 1, 2, 3)\n        # convert to 0-1 float\n        bcfhw = bcfhw.astype(np.float32) / 255\n        bfchw = bcfhw.transpose(0, 2, 1, 3, 4)  # b c f h w\n\n        print(f\"Converted the frames to tensor {bfchw.shape}\")\n\n        vd_out = torch.from_numpy(bcfhw).to(\"cuda\")\n\n        # should be -1,1, not 0,1\n        vd_out = 2 * vd_out - 1\n\n        # latents should have shape num_sample, 4, max_frames, latent_h,latent_w\n        print(\"Computing latents\")\n        latents = pipe.compute_latents(vd_out).to(device)\n\n        skip_steps = int(math.floor(args.steps * max(0, min(1 - args.strength, 1))))\n    else:\n        latents = None\n        args.strength = 1\n        skip_steps = 0\n\n    print('Working in txt2vid mode' if not args.do_vid2vid else 'Working in vid2vid mode')\n\n    # Start the batch count loop\n    pbar = tqdm(range(args.batch_count), leave=False)\n    if args.batch_count == 1:\n        pbar.disable = True\n\n    vids_to_pack = []\n\n    state.job_count = args.batch_count\n\n    for batch in pbar:\n        state.job_no = batch\n        if state.skipped:\n            state.skipped = False\n\n        if state.interrupted:\n            break\n\n        shared.state.job = f\"Batch {batch + 1} out of {args.batch_count}\"\n        # TODO: move to a separate function\n        if args.inpainting_frames > 0 and hasattr(args.inpainting_image, \"name\"):\n            keys = T2VAnimKeys(SimpleNamespace(**{'max_frames': args.frames, 'inpainting_weights': args.inpainting_weights}), args.seed, args.inpainting_frames)\n            images = []\n            print(\"Received an image for inpainting\", args.inpainting_image.name)\n            for i in range(args.frames):\n                image = Image.open(args.inpainting_image.name).convert(\"RGB\")\n                image = image.resize((args.width, args.height), Image.ANTIALIAS)\n                array = np.array(image)\n                images += [array]\n\n            images = np.stack(images)  # f h w c\n            batches = 1\n            n_images = np.tile(images[np.newaxis, ...], (batches, 1, 1, 1, 1))  # n f h w c\n            bcfhw = n_images.transpose(0, 4, 1, 2, 3)\n            # convert to 0-1 float\n            bcfhw = bcfhw.astype(np.float32) / 255\n            bfchw = bcfhw.transpose(0, 2, 1, 3, 4)  # b c f h w\n\n            print(f\"Converted the frames to tensor {bfchw.shape}\")\n\n            vd_out = torch.from_numpy(bcfhw).to(\"cuda\")\n\n            # should be -1,1, not 0,1\n            vd_out = 2 * vd_out - 1\n\n            # latents should have shape num_sample, 4, max_frames, latent_h,latent_w\n            # but right now they have shape num_sample=1,4, 1 (only used 1 img), latent_h, latent_w\n            print(\"Computing latents\")\n            image_latents = pipe.compute_latents(vd_out).numpy()\n            # padding_width = [(0, 0), (0, 0), (0, frames-inpainting_frames), (0, 0), (0, 0)]\n            # padded_latents = np.pad(image_latents, pad_width=padding_width, mode='constant', constant_values=0)\n\n            latent_h = args.height // 8\n            latent_w = args.width // 8\n            latent_noise = np.random.normal(size=(1, 4, args.frames, latent_h, latent_w))\n            mask = np.ones(shape=(1, 4, args.frames, latent_h, latent_w))\n\n            mask_weights = [keys.inpainting_weights_series[frame_idx] for frame_idx in range(args.frames)]\n\n            for i in range(args.frames):\n                v = mask_weights[i]\n                mask[:, :, i, :, :] = v\n\n            masked_latents = image_latents * (1 - mask) + latent_noise * mask\n\n            latents = torch.tensor(masked_latents).to(device)\n\n            mask = torch.tensor(mask).to(device)\n\n            args.strength = 1\n\n        samples, _ = pipe.infer(args.prompt, args.n_prompt, args.steps, args.frames, args.seed + batch if args.seed != -1 else -1, args.cfg_scale,\n                                args.width, args.height, args.eta, cpu_vae, device, latents, strength=args.strength, skip_steps=skip_steps, mask=mask, is_vid2vid=args.do_vid2vid, sampler=args.sampler)\n\n        if batch > 0:\n            outdir_current = os.path.join(get_outdir(), f\"{init_timestring}_{batch}\")\n        print(f'text2video finished, saving frames to {outdir_current}')\n\n        # just deleted the folder so we need to make it again\n        os.makedirs(outdir_current, exist_ok=True)\n        for i in range(len(samples)):\n            cv2.imwrite(outdir_current + os.path.sep +\n                        f\"{i:06}.png\", samples[i])\n\n        # TODO: add params to the GUI\n        if not video_args.skip_video_creation:\n            ffmpeg_stitch_video(ffmpeg_location=video_args.ffmpeg_location, fps=video_args.fps, outmp4_path=outdir_current + os.path.sep + f\"vid.mp4\", imgs_path=os.path.join(outdir_current,\n                                                                                                                                                                              \"%06d.png\"),\n                                stitch_from_frame=0, stitch_to_frame=-1, add_soundtrack=video_args.add_soundtrack,\n                                audio_path=vid2vid_frames_path if video_args.add_soundtrack == 'Init Video' else video_args.soundtrack_path, crf=video_args.ffmpeg_crf, preset=video_args.ffmpeg_preset)\n        print(f't2v complete, result saved at {outdir_current}')\n\n        mp4 = open(outdir_current + os.path.sep + f\"vid.mp4\", 'rb').read()\n        dataurl = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n\n        if max_vids_to_pack == -1 or len(vids_to_pack) < max_vids_to_pack:\n            vids_to_pack.append(dataurl)\n    t2v_helpers_args.i1_store_t2v = f'<p style=\\\"font-weight:bold;margin-bottom:0em\\\">text2video extension for auto1111 \u2014 version 1.2b </p>'\n    for dataurl in vids_to_pack:\n        t2v_helpers_args.i1_store_t2v += f'<video controls loop><source src=\"{dataurl}\" type=\"video/mp4\"></video><br>'\n    pbar.close()\n    return vids_to_pack", ""]}
{"filename": "scripts/modelscope/clip_hardcode.py", "chunked_list": ["# This code is borrowed from Automatic1111's webui with modifications\n# AGPL v3.0 (c) 2023 AUTOMATIC1111, read the full license here\n# https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/LICENSE.txt\n\n# Modified by kabachuha and incorporated into the AGPL v3.0 license of the project\n# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# Read LICENSE for usage terms.\n\nfrom collections import namedtuple\n", "from collections import namedtuple\n\nimport math\nimport torch\n\nimport open_clip\nfrom typing import Optional\n\nfrom modules import prompt_parser, devices, sd_hijack\nfrom modules.shared import opts", "from modules import prompt_parser, devices, sd_hijack\nfrom modules.shared import opts\n\nimport os\nfrom ldm.util import instantiate_from_config\n\ntokenizer = open_clip.tokenizer._tokenizer\nfrom modules import textual_inversion\n\nclass PromptChunk:\n    \"\"\"\n    This object contains token ids, weight (multipliers:1.4) and textual inversion embedding info for a chunk of prompt.\n    If a prompt is short, it is represented by one PromptChunk, otherwise, multiple are necessary.\n    Each PromptChunk contains an exact amount of tokens - 77, which includes one for start and end token,\n    so just 75 tokens from prompt.\n    \"\"\"\n\n    def __init__(self):\n        self.tokens = []\n        self.multipliers = []\n        self.fixes = []", "\nclass PromptChunk:\n    \"\"\"\n    This object contains token ids, weight (multipliers:1.4) and textual inversion embedding info for a chunk of prompt.\n    If a prompt is short, it is represented by one PromptChunk, otherwise, multiple are necessary.\n    Each PromptChunk contains an exact amount of tokens - 77, which includes one for start and end token,\n    so just 75 tokens from prompt.\n    \"\"\"\n\n    def __init__(self):\n        self.tokens = []\n        self.multipliers = []\n        self.fixes = []", "\nclass HijackDummy:\n    fixes = None\n    comments = []\n    layers = None\n    circular_enabled = False\n    clip = None\n    optimization_method = None\n\n    embedding_db = textual_inversion.textual_inversion.EmbeddingDatabase()", "\nclass Invoke(object):\n    KEY = 'invoked_by'\n    PRETRAINED = 'from_pretrained'\n    PIPELINE = 'pipeline'\n    TRAINER = 'trainer'\n    LOCAL_TRAINER = 'local_trainer'\n    PREPROCESSOR = 'preprocessor'\n\nPromptChunkFix = namedtuple('PromptChunkFix', ['offset', 'embedding'])", "\nPromptChunkFix = namedtuple('PromptChunkFix', ['offset', 'embedding'])\n\nclass FrozenOpenCLIPEmbedder(torch.nn.Module):\n    \"\"\"\n    Uses the OpenCLIP transformer encoder for text\n    \"\"\"\n    LAYERS = ['last', 'penultimate']\n\n    def __init__(self,\n                 arch='ViT-H-14',\n                 version='open_clip_pytorch_model.bin',\n                 device='cuda',\n                 max_length=77,\n                 freeze=True,\n                 layer='last'):\n        super().__init__()\n        assert layer in self.LAYERS\n        model, _, _ = open_clip.create_model_and_transforms(\n            arch, device=torch.device('cpu'), pretrained=version)\n        del model.visual\n        self.model = model\n\n        self.device = device\n        self.max_length = max_length\n        if freeze:\n            self.freeze()\n        self.layer = layer\n        if self.layer == 'last':\n            self.layer_idx = 0\n        elif self.layer == 'penultimate':\n            self.layer_idx = 1\n        else:\n            raise NotImplementedError()\n        \n        # ^ vanilla\n\n        self.comma_token = [v for k, v in tokenizer.encoder.items() if k == ',</w>'][0]\n        self.id_start = tokenizer.encoder[\"<start_of_text>\"]\n        self.id_end = tokenizer.encoder[\"<end_of_text>\"]\n        self.id_pad = 0\n\n        # ^ with custom words\n\n        self.hijack = HijackDummy()\n        self.chunk_length = 75\n    \n    def tokenize(self, texts):\n        if not (hasattr(opts, 'use_old_emphasis_implementation') and opts.use_old_emphasis_implementation):\n            tokenized = [tokenizer.encode(text) for text in texts]\n        else:\n            assert not opts.use_old_emphasis_implementation, 'Old emphasis implementation not supported for Open Clip'\n        return tokenized\n    \n    def encode_with_transformer(self, text):\n        x = self.model.token_embedding(text)  # [batch_size, n_ctx, d_model]\n        x = x + self.model.positional_embedding\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.text_transformer_forward(x, attn_mask=self.model.attn_mask)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.model.ln_final(x)\n        return x\n    \n    def encode_with_transformers(self, tokens):\n        # set self.wrapped.layer_idx here according to opts.CLIP_stop_at_last_layers\n        z = self.encode_with_transformer(tokens)\n\n        return z\n    \n    def encode_embedding_init_text(self, init_text, nvpt):\n        ids = tokenizer.encode(init_text)\n        ids = torch.asarray([ids], device=devices.device, dtype=torch.int)\n        embedded = self.model.token_embedding.wrapped(ids).squeeze(0)\n\n        return embedded\n    \n    def empty_chunk(self):\n        \"\"\"creates an empty PromptChunk and returns it\"\"\"\n\n        chunk = PromptChunk()\n        chunk.tokens = [self.id_start] + [self.id_end] * (self.chunk_length + 1)\n        chunk.multipliers = [1.0] * (self.chunk_length + 2)\n        return chunk\n    \n    def get_target_prompt_token_count(self, token_count):\n        \"\"\"returns the maximum number of tokens a prompt of a known length can have before it requires one more PromptChunk to be represented\"\"\"\n\n        return math.ceil(max(token_count, 1) / self.chunk_length) * self.chunk_length\n\n\n    def tokenize_line(self, line):\n        \"\"\"\n        this transforms a single prompt into a list of PromptChunk objects - as many as needed to\n        represent the prompt.\n        Returns the list and the total number of tokens in the prompt.\n        \"\"\"\n\n        if opts.enable_emphasis:\n            parsed = prompt_parser.parse_prompt_attention(line)\n        else:\n            parsed = [[line, 1.0]]\n\n        tokenized = self.tokenize([text for text, _ in parsed])\n\n        chunks = []\n        chunk = PromptChunk()\n        token_count = 0\n        last_comma = -1\n\n        def next_chunk(is_last=False):\n            \"\"\"puts current chunk into the list of results and produces the next one - empty;\n            if is_last is true, tokens <end-of-text> tokens at the end won't add to token_count\"\"\"\n            nonlocal token_count\n            nonlocal last_comma\n            nonlocal chunk\n\n            if is_last:\n                token_count += len(chunk.tokens)\n            else:\n                token_count += self.chunk_length\n\n            to_add = self.chunk_length - len(chunk.tokens)\n            if to_add > 0:\n                chunk.tokens += [self.id_end] * to_add\n                chunk.multipliers += [1.0] * to_add\n\n            chunk.tokens = [self.id_start] + chunk.tokens + [self.id_end]\n            chunk.multipliers = [1.0] + chunk.multipliers + [1.0]\n\n            last_comma = -1\n            chunks.append(chunk)\n            chunk = PromptChunk()\n\n        for tokens, (text, weight) in zip(tokenized, parsed):\n            if text == 'BREAK' and weight == -1:\n                next_chunk()\n                continue\n\n            position = 0\n            while position < len(tokens):\n                token = tokens[position]\n\n                if token == self.comma_token:\n                    last_comma = len(chunk.tokens)\n\n                # this is when we are at the end of alloted 75 tokens for the current chunk, and the current token is not a comma. opts.comma_padding_backtrack\n                # is a setting that specifies that if there is a comma nearby, the text after the comma should be moved out of this chunk and into the next.\n                elif opts.comma_padding_backtrack != 0 and len(chunk.tokens) == self.chunk_length and last_comma != -1 and len(chunk.tokens) - last_comma <= opts.comma_padding_backtrack:\n                    break_location = last_comma + 1\n\n                    reloc_tokens = chunk.tokens[break_location:]\n                    reloc_mults = chunk.multipliers[break_location:]\n\n                    chunk.tokens = chunk.tokens[:break_location]\n                    chunk.multipliers = chunk.multipliers[:break_location]\n\n                    next_chunk()\n                    chunk.tokens = reloc_tokens\n                    chunk.multipliers = reloc_mults\n\n                if len(chunk.tokens) == self.chunk_length:\n                    next_chunk()\n\n                embedding, embedding_length_in_tokens = self.hijack.embedding_db.find_embedding_at_position(tokens, position)\n                if embedding is None:\n                    chunk.tokens.append(token)\n                    chunk.multipliers.append(weight)\n                    position += 1\n                    continue\n\n                emb_len = int(embedding.vec.shape[0])\n                if len(chunk.tokens) + emb_len > self.chunk_length:\n                    next_chunk()\n\n                chunk.fixes.append(PromptChunkFix(len(chunk.tokens), embedding))\n\n                chunk.tokens += [0] * emb_len\n                chunk.multipliers += [weight] * emb_len\n                position += embedding_length_in_tokens\n\n        if len(chunk.tokens) > 0 or len(chunks) == 0:\n            next_chunk(is_last=True)\n\n        return chunks, token_count\n    \n    def process_texts(self, texts):\n        \"\"\"\n        Accepts a list of texts and calls tokenize_line() on each, with cache. Returns the list of results and maximum\n        length, in tokens, of all texts.\n        \"\"\"\n\n        token_count = 0\n\n        cache = {}\n        batch_chunks = []\n        for line in texts:\n            if line in cache:\n                chunks = cache[line]\n            else:\n                chunks, current_token_count = self.tokenize_line(line)\n                token_count = max(current_token_count, token_count)\n\n                cache[line] = chunks\n\n            batch_chunks.append(chunks)\n\n        return batch_chunks, token_count\n\n    def freeze(self):\n        self.model = self.model.eval()\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def text_transformer_forward(self, x: torch.Tensor, attn_mask=None):\n        for i, r in enumerate(self.model.transformer.resblocks):\n            if i == len(self.model.transformer.resblocks) - self.layer_idx:\n                break\n            x = r(x, attn_mask=attn_mask)\n        return x\n\n    def encode(self, text):\n        return self(text)\n\n    def get_learned_conditioning(self, text):\n        return self.encode(text)\n\n    def from_pretrained(cls,\n                        model_name_or_path: str,\n                        revision: Optional[str] = None,\n                        cfg_dict=None,\n                        device: str = None,\n                        **kwargs):\n        \"\"\"Instantiate a model from local directory or remote model repo. Note\n        that when loading from remote, the model revision can be specified.\n\n        Args:\n            model_name_or_path(str): A model dir or a model id to be loaded\n            revision(str, `optional`): The revision used when the model_name_or_path is\n                a model id of the remote hub. default `master`.\n            cfg_dict(Config, `optional`): An optional model config. If provided, it will replace\n                the config read out of the `model_name_or_path`\n            device(str, `optional`): The device to load the model.\n            **kwargs:\n                task(str, `optional`): The `Tasks` enumeration value to replace the task value\n                read out of config in the `model_name_or_path`. This is useful when the model to be loaded is not\n                equal to the model saved.\n                For example, load a `backbone` into a `text-classification` model.\n                Other kwargs will be directly fed into the `model` key, to replace the default configs.\n        Returns:\n            A model instance.\n\n        \"\"\"\n        prefetched = kwargs.get('model_prefetched')\n        if prefetched is not None:\n            kwargs.pop('model_prefetched')\n        invoked_by = kwargs.get(Invoke.KEY)\n        if invoked_by is not None:\n            kwargs.pop(Invoke.KEY)\n        else:\n            invoked_by = Invoke.PRETRAINED\n\n        if os.path.exists(model_name_or_path):\n            local_model_dir = model_name_or_path\n        if cfg_dict is not None:\n            cfg = cfg_dict\n            \"\"\"else:\n            cfg = Config.from_file(\n                osp.join(local_model_dir, ModelFile.CONFIGURATION))\"\"\"\n        task_name = cfg.task\n        if 'task' in kwargs:\n            task_name = kwargs.pop('task')\n        model_cfg = cfg.model\n        if hasattr(model_cfg, 'model_type') and not hasattr(model_cfg, 'type'):\n            model_cfg.type = model_cfg.model_type\n        model_cfg.model_dir = local_model_dir\n\n        print(\"plugins\", cfg.safe_get('plugins'))\n\n        # install and import remote repos before build\n        # register_plugins_repo(cfg.safe_get('plugins'))\n        # register_modelhub_repo(local_model_dir, cfg.get('allow_remote', False))\n\n        for k, v in kwargs.items():\n            model_cfg[k] = v\n        if device is not None:\n            model_cfg.device = device\n        \"\"\"if task_name is Tasks.backbone:\n            model_cfg.init_backbone = True\n            model = build_backbone(model_cfg)\n        else:\"\"\"\n        model = instantiate_from_config(model_cfg)\n        # model = build_model(model_cfg, task_name=task_name)\n\n        # dynamically add pipeline info to model for pipeline inference\n        if hasattr(cfg, 'pipeline'):\n            model.pipeline = cfg.pipeline\n\n        if not hasattr(model, 'cfg'):\n            model.cfg = cfg\n\n        model_cfg.pop('model_dir', None)\n        model.name = model_name_or_path\n        model.model_dir = local_model_dir\n        return model\n\n    def forward(self, texts):\n        \"\"\"\n        Accepts an array of texts; Passes texts through transformers network to create a tensor with numerical representation of those texts.\n        Returns a tensor with shape of (B, T, C), where B is length of the array; T is length, in tokens, of texts (including padding) - T will\n        be a multiple of 77; and C is dimensionality of each token - for SD1 it's 768, and for SD2 it's 1024.\n        An example shape returned by this function can be: (2, 77, 768).\n        Webui usually sends just one text at a time through this function - the only time when texts is an array with more than one elemenet\n        is when you do prompt editing: \"a picture of a [cat:dog:0.4] eating ice cream\"\n        \"\"\"\n\n        batch_chunks, token_count = self.process_texts(texts)\n\n        used_embeddings = {}\n        chunk_count = max([len(x) for x in batch_chunks])\n\n        zs = []\n        for i in range(chunk_count):\n            batch_chunk = [chunks[i] if i < len(chunks) else self.empty_chunk() for chunks in batch_chunks]\n\n            tokens = [x.tokens for x in batch_chunk]\n            multipliers = [x.multipliers for x in batch_chunk]\n            self.hijack.fixes = [x.fixes for x in batch_chunk]\n\n            for fixes in self.hijack.fixes:\n                for position, embedding in fixes:\n                    used_embeddings[embedding.name] = embedding\n\n            z = self.process_tokens(tokens, multipliers)\n            zs.append(z)\n\n        if len(used_embeddings) > 0:\n            embeddings_list = \", \".join([f'{name} [{embedding.checksum()}]' for name, embedding in used_embeddings.items()])\n            self.hijack.comments.append(f\"Used embeddings: {embeddings_list}\")\n\n        return torch.hstack(zs)\n\n    def process_tokens(self, remade_batch_tokens, batch_multipliers):\n        \"\"\"\n        sends one single prompt chunk to be encoded by transformers neural network.\n        remade_batch_tokens is a batch of tokens - a list, where every element is a list of tokens; usually\n        there are exactly 77 tokens in the list. batch_multipliers is the same but for multipliers instead of tokens.\n        Multipliers are used to give more or less weight to the outputs of transformers network. Each multiplier\n        corresponds to one token.\n        \"\"\"\n        tokens = torch.asarray(remade_batch_tokens).to(devices.device)\n\n        # this is for SD2: SD1 uses the same token for padding and end of text, while SD2 uses different ones.\n        if self.id_end != self.id_pad:\n            for batch_pos in range(len(remade_batch_tokens)):\n                index = remade_batch_tokens[batch_pos].index(self.id_end)\n                tokens[batch_pos, index+1:tokens.shape[1]] = self.id_pad\n\n        z = self.encode_with_transformers(tokens)\n\n        # restoring original mean is likely not correct, but it seems to work well to prevent artifacts that happen otherwise\n        batch_multipliers = torch.asarray(batch_multipliers).to(devices.device)\n        original_mean = z.mean()\n        z = z * batch_multipliers.reshape(batch_multipliers.shape + (1,)).expand(z.shape)\n        new_mean = z.mean()\n        z = z * (original_mean / new_mean)\n\n        return z", ""]}
{"filename": "scripts/modelscope/t2v_model.py", "chunked_list": ["# Part of the implementation is borrowed and modified from stable-diffusion,\n# publicly avaialbe at https://github.com/Stability-AI/stablediffusion.\n# Copyright 2021-2022 The Alibaba Fundamental Vision Team Authors. All rights reserved.\n\n# https://github.com/modelscope/modelscope/tree/master/modelscope/pipelines/multi_modal\n\n# Alibaba's code used under Apache 2.0 license\n# StabilityAI's Stable Diffusion code used under MIT license\n# Automatic1111's WebUI's code used under AGPL v3.0\n", "# Automatic1111's WebUI's code used under AGPL v3.0\n\n# All the licenses of the code and its modifications are incorporated into the compatible AGPL v3.0 license\n\n# SD-webui text2video:\n\n# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# See LICENSE for usage terms.\n\nfrom ldm.util import instantiate_from_config", "\nfrom ldm.util import instantiate_from_config\nimport importlib\nimport math\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np", "import torch.nn.functional as F\nimport numpy as np\nfrom einops import rearrange, repeat\nfrom os import path as osp\nfrom modules.shared import opts\n\nfrom functools import partial\nfrom tqdm import tqdm\nfrom modules.prompt_parser import reconstruct_cond_batch\nfrom modules.shared import state", "from modules.prompt_parser import reconstruct_cond_batch\nfrom modules.shared import state\nfrom modules.sd_samplers_common import InterruptedException\n\nfrom modules.sd_hijack_optimizations import get_xformers_flash_attention_op\nfrom ldm.modules.diffusionmodules.util import make_beta_schedule\n\n__all__ = ['UNetSD']\n\ntry:\n    import gc\n    import torch\n    import torch.cuda\n\n    def torch_gc():\n        \"\"\"Performs garbage collection for both Python and PyTorch CUDA tensors.\n\n        This function collects Python garbage and clears the PyTorch CUDA cache\n        and IPC (Inter-Process Communication) resources.\n        \"\"\"\n        gc.collect()  # Collect Python garbage\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()  # Clear PyTorch CUDA cache\n            torch.cuda.ipc_collect()  # Clear PyTorch CUDA IPC resources\n\nexcept:\n\n    def torch_gc():\n        \"\"\"Dummy function when torch is not available.\n\n        This function does nothing and serves as a placeholder when torch is\n        not available, allowing the rest of the code to run without errors.\n        \"\"\"\n        gc.collect()\n        pass", "\ntry:\n    import gc\n    import torch\n    import torch.cuda\n\n    def torch_gc():\n        \"\"\"Performs garbage collection for both Python and PyTorch CUDA tensors.\n\n        This function collects Python garbage and clears the PyTorch CUDA cache\n        and IPC (Inter-Process Communication) resources.\n        \"\"\"\n        gc.collect()  # Collect Python garbage\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()  # Clear PyTorch CUDA cache\n            torch.cuda.ipc_collect()  # Clear PyTorch CUDA IPC resources\n\nexcept:\n\n    def torch_gc():\n        \"\"\"Dummy function when torch is not available.\n\n        This function does nothing and serves as a placeholder when torch is\n        not available, allowing the rest of the code to run without errors.\n        \"\"\"\n        gc.collect()\n        pass", "\nimport modules.shared as shared\nfrom modules.shared import cmd_opts\ncan_use_sdp = hasattr(torch.nn.functional, \"scaled_dot_product_attention\") and callable(getattr(torch.nn.functional, \"scaled_dot_product_attention\")) # not everyone has torch 2.x to use sdp\n\nfrom ldm.modules.diffusionmodules.model import Decoder, Encoder\nfrom ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n\nDEFAULT_MODEL_REVISION = None\n", "DEFAULT_MODEL_REVISION = None\n\n\nclass Invoke(object):\n    KEY = 'invoked_by'\n    PRETRAINED = 'from_pretrained'\n    PIPELINE = 'pipeline'\n    TRAINER = 'trainer'\n    LOCAL_TRAINER = 'local_trainer'\n    PREPROCESSOR = 'preprocessor'", "\n\ndef exists(x):\n    return x is not None\n\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d", "\n\nclass UNetSD(nn.Module):\n\n    def __init__(self,\n                 in_dim=7,\n                 dim=512,\n                 y_dim=512,\n                 context_dim=512,\n                 out_dim=6,\n                 dim_mult=[1, 2, 3, 4],\n                 num_heads=None,\n                 head_dim=64,\n                 num_res_blocks=3,\n                 attn_scales=[1 / 2, 1 / 4, 1 / 8],\n                 use_scale_shift_norm=True,\n                 dropout=0.1,\n                 temporal_attn_times=2,\n                 temporal_attention=True,\n                 use_checkpoint=False,\n                 use_image_dataset=False,\n                 use_fps_condition=False,\n                 use_sim_mask=False,\n                 parameterization=\"eps\"):\n        embed_dim = dim * 4\n        num_heads = num_heads if num_heads else dim // 32\n        super(UNetSD, self).__init__()\n        self.in_dim = in_dim\n        self.dim = dim\n        self.y_dim = y_dim\n        self.context_dim = context_dim\n        self.embed_dim = embed_dim\n        self.out_dim = out_dim\n        self.dim_mult = dim_mult\n        self.num_heads = num_heads\n        # parameters for spatial/temporal attention\n        self.head_dim = head_dim\n        self.num_res_blocks = num_res_blocks\n        self.attn_scales = attn_scales\n        self.use_scale_shift_norm = use_scale_shift_norm\n        self.temporal_attn_times = temporal_attn_times\n        self.temporal_attention = temporal_attention\n        self.use_checkpoint = use_checkpoint\n        self.use_image_dataset = use_image_dataset\n        self.use_fps_condition = use_fps_condition\n        self.use_sim_mask = use_sim_mask\n        self.parameterization = parameterization\n        self.v_posterior = 0\n        use_linear_in_temporal = False\n        transformer_depth = 1\n        disabled_sa = False\n        # params\n        enc_dims = [dim * u for u in [1] + dim_mult]\n        dec_dims = [dim * u for u in [dim_mult[-1]] + dim_mult[::-1]]\n        shortcut_dims = []\n        scale = 1.0\n\n        # embeddings\n        self.time_embed = nn.Sequential(\n            nn.Linear(dim, embed_dim), nn.SiLU(),\n            nn.Linear(embed_dim, embed_dim))\n\n        if self.use_fps_condition:\n            self.fps_embedding = nn.Sequential(\n                nn.Linear(dim, embed_dim), nn.SiLU(),\n                nn.Linear(embed_dim, embed_dim))\n            nn.init.zeros_(self.fps_embedding[-1].weight)\n            nn.init.zeros_(self.fps_embedding[-1].bias)\n\n        # encoder\n        self.input_blocks = nn.ModuleList()\n        init_block = nn.ModuleList([nn.Conv2d(self.in_dim, dim, 3, padding=1)])\n\n        if temporal_attention:\n            init_block.append(\n                TemporalTransformer(\n                    dim,\n                    num_heads,\n                    head_dim,\n                    depth=transformer_depth,\n                    context_dim=context_dim,\n                    disable_self_attn=disabled_sa,\n                    use_linear=use_linear_in_temporal,\n                    multiply_zero=use_image_dataset))\n\n        self.input_blocks.append(init_block)\n        shortcut_dims.append(dim)\n        for i, (in_dim,\n                out_dim) in enumerate(zip(enc_dims[:-1], enc_dims[1:])):\n            for j in range(num_res_blocks):\n                # residual (+attention) blocks\n                block = nn.ModuleList([\n                    ResBlock(\n                        in_dim,\n                        embed_dim,\n                        dropout,\n                        out_channels=out_dim,\n                        use_scale_shift_norm=False,\n                        use_image_dataset=use_image_dataset,\n                    )\n                ])\n                if scale in attn_scales:\n                    block.append(\n                        SpatialTransformer(\n                            out_dim,\n                            out_dim // head_dim,\n                            head_dim,\n                            depth=1,\n                            context_dim=self.context_dim,\n                            disable_self_attn=False,\n                            use_linear=True))\n                    if self.temporal_attention:\n                        block.append(\n                            TemporalTransformer(\n                                out_dim,\n                                out_dim // head_dim,\n                                head_dim,\n                                depth=transformer_depth,\n                                context_dim=context_dim,\n                                disable_self_attn=disabled_sa,\n                                use_linear=use_linear_in_temporal,\n                                multiply_zero=use_image_dataset))\n\n                in_dim = out_dim\n                self.input_blocks.append(block)\n                shortcut_dims.append(out_dim)\n\n                # downsample\n                if i != len(dim_mult) - 1 and j == num_res_blocks - 1:\n                    downsample = Downsample(\n                        out_dim, True, dims=2, out_channels=out_dim)\n                    shortcut_dims.append(out_dim)\n                    scale /= 2.0\n                    self.input_blocks.append(downsample)\n\n        # middle\n        self.middle_block = nn.ModuleList([\n            ResBlock(\n                out_dim,\n                embed_dim,\n                dropout,\n                use_scale_shift_norm=False,\n                use_image_dataset=use_image_dataset,\n            ),\n            SpatialTransformer(\n                out_dim,\n                out_dim // head_dim,\n                head_dim,\n                depth=1,\n                context_dim=self.context_dim,\n                disable_self_attn=False,\n                use_linear=True)\n        ])\n\n        if self.temporal_attention:\n            self.middle_block.append(\n                TemporalTransformer(\n                    out_dim,\n                    out_dim // head_dim,\n                    head_dim,\n                    depth=transformer_depth,\n                    context_dim=context_dim,\n                    disable_self_attn=disabled_sa,\n                    use_linear=use_linear_in_temporal,\n                    multiply_zero=use_image_dataset,\n                ))\n\n        self.middle_block.append(\n            ResBlock(\n                out_dim,\n                embed_dim,\n                dropout,\n                use_scale_shift_norm=False,\n                use_image_dataset=use_image_dataset,\n            ))\n\n        # decoder\n        self.output_blocks = nn.ModuleList()\n        for i, (in_dim,\n                out_dim) in enumerate(zip(dec_dims[:-1], dec_dims[1:])):\n            for j in range(num_res_blocks + 1):\n                # residual (+attention) blocks\n                block = nn.ModuleList([\n                    ResBlock(\n                        in_dim + shortcut_dims.pop(),\n                        embed_dim,\n                        dropout,\n                        out_dim,\n                        use_scale_shift_norm=False,\n                        use_image_dataset=use_image_dataset,\n                    )\n                ])\n                if scale in attn_scales:\n                    block.append(\n                        SpatialTransformer(\n                            out_dim,\n                            out_dim // head_dim,\n                            head_dim,\n                            depth=1,\n                            context_dim=1024,\n                            disable_self_attn=False,\n                            use_linear=True))\n\n                    if self.temporal_attention:\n                        block.append(\n                            TemporalTransformer(\n                                out_dim,\n                                out_dim // head_dim,\n                                head_dim,\n                                depth=transformer_depth,\n                                context_dim=context_dim,\n                                disable_self_attn=disabled_sa,\n                                use_linear=use_linear_in_temporal,\n                                multiply_zero=use_image_dataset))\n                in_dim = out_dim\n\n                # upsample\n                if i != len(dim_mult) - 1 and j == num_res_blocks:\n                    upsample = Upsample(\n                        out_dim, True, dims=2.0, out_channels=out_dim)\n                    scale *= 2.0\n                    block.append(upsample)\n                self.output_blocks.append(block)\n\n        # head\n        self.out = nn.Sequential(\n            nn.GroupNorm(32, out_dim), nn.SiLU(),\n            nn.Conv2d(out_dim, self.out_dim, 3, padding=1))\n\n        # zero out the last layer params\n        nn.init.zeros_(self.out[-1].weight)\n\n    # Taken from DDPM\n    def register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                        linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                        cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \"eps\":\n            lvlb_weights = self.betas ** 2 / (\n                    2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \"x0\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        elif self.parameterization == \"v\":\n            lvlb_weights = torch.ones_like(self.betas ** 2 / (\n                    2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod)))\n        else:\n            raise NotImplementedError(\"mu not supported\")\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    def forward(\n            self,\n            x,\n            t,\n            y,\n            fps=None,\n            video_mask=None,\n            focus_present_mask=None,\n            prob_focus_present=0.0,\n            mask_last_frame_num=0  # mask last frame num\n    ):\n        \"\"\"\n        prob_focus_present: probability at which a given batch sample will focus on the present\n                            (0. is all off, 1. is completely arrested attention across time)\n        \"\"\"\n        batch, device = x.shape[0], x.device\n        self.batch = batch\n\n        # image and video joint training, if mask_last_frame_num is set, prob_focus_present will be ignored\n        if mask_last_frame_num > 0:\n            focus_present_mask = None\n            video_mask[-mask_last_frame_num:] = False\n        else:\n            focus_present_mask = default(\n                focus_present_mask, lambda: prob_mask_like(\n                    (batch, ), prob_focus_present, device=device))\n\n        time_rel_pos_bias = None\n        # embeddings\n        if self.use_fps_condition and fps is not None:\n            e = self.time_embed(sinusoidal_embedding(\n                t, self.dim)) + self.fps_embedding(\n                    sinusoidal_embedding(fps, self.dim))\n        else:\n            e = self.time_embed(sinusoidal_embedding(t, self.dim))\n        context = y\n\n        # repeat f times for spatial e and context\n        f = x.shape[2]\n        e = e.repeat_interleave(repeats=f, dim=0)\n        context = context.repeat_interleave(repeats=f, dim=0)\n\n        # always in shape (b f) c h w, except for temporal layer\n        x = rearrange(x, 'b c f h w -> (b f) c h w')\n        # encoder\n        xs = []\n        for block in self.input_blocks:\n            x = self._forward_single(block, x, e, context, time_rel_pos_bias,\n                                     focus_present_mask, video_mask)\n            xs.append(x)\n\n        # middle\n        for block in self.middle_block:\n            x = self._forward_single(block, x, e, context, time_rel_pos_bias,\n                                     focus_present_mask, video_mask)\n\n        # decoder\n        for block in self.output_blocks:\n            x = torch.cat([x, xs.pop()], dim=1)\n            x = self._forward_single(\n                block,\n                x,\n                e,\n                context,\n                time_rel_pos_bias,\n                focus_present_mask,\n                video_mask,\n                reference=xs[-1] if len(xs) > 0 else None)\n\n        # head\n        x = self.out(x)\n        # reshape back to (b c f h w)\n        x = rearrange(x, '(b f) c h w -> b c f h w', b=batch)\n        return x\n\n    def _forward_single(self,\n                        module,\n                        x,\n                        e,\n                        context,\n                        time_rel_pos_bias,\n                        focus_present_mask,\n                        video_mask,\n                        reference=None):\n        if isinstance(module, ResidualBlock):\n            x = x.contiguous()\n            x = module(x, e, reference)\n        elif isinstance(module, ResBlock):\n            x = x.contiguous()\n            x = module(x, e, self.batch)\n        elif isinstance(module, SpatialTransformer):\n            x = module(x, context)\n        elif isinstance(module, TemporalTransformer):\n            x = rearrange(x, '(b f) c h w -> b c f h w', b=self.batch)\n            x = module(x, context)\n            x = rearrange(x, 'b c f h w -> (b f) c h w')\n        elif isinstance(module, CrossAttention):\n            x = module(x, context)\n        elif isinstance(module, BasicTransformerBlock):\n            x = module(x, context)\n        elif isinstance(module, FeedForward):\n            x = module(x, context)\n        elif isinstance(module, Upsample):\n            x = module(x)\n        elif isinstance(module, Downsample):\n            x = module(x)\n        elif isinstance(module, Resample):\n            x = module(x, reference)\n        elif isinstance(module, nn.ModuleList):\n            for block in module:\n                x = self._forward_single(block, x, e, context,\n                                         time_rel_pos_bias, focus_present_mask,\n                                         video_mask, reference)\n        else:\n            x = module(x)\n        return x", "\n\ndef sinusoidal_embedding(timesteps, dim):\n    # check input\n    half = dim // 2\n    timesteps = timesteps.float()\n    # compute sinusoidal embedding\n    sinusoid = torch.outer(\n        timesteps, torch.pow(10000,\n                             -torch.arange(half).to(timesteps).div(half)))\n    x = torch.cat([torch.cos(sinusoid), torch.sin(sinusoid)], dim=1)\n    if dim % 2 != 0:\n        x = torch.cat([x, torch.zeros_like(x[:, :1])], dim=1)\n    return x", "\n\nclass CrossAttention(nn.Module):\n\n    def __init__(self,\n                 query_dim,\n                 context_dim=None,\n                 heads=8,\n                 dim_head=64,\n                 dropout=0.0):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n\n        self.scale = dim_head**-0.5\n        self.heads = heads\n\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, query_dim), nn.Dropout(dropout))\n\n    def forward(self, x, context=None, mask=None):\n        h = self.heads\n\n        q = self.to_q(x)\n        context = default(context, x)\n        k = self.to_k(context)\n        v = self.to_v(context)\n\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h),\n                      (q, k, v))\n    \n        if exists(mask):\n            mask = rearrange(mask, 'b ... -> b (...)')\n            max_neg_value = -torch.finfo(x.dtype).max\n            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n        \n        if getattr(cmd_opts, \"force_enable_xformers\", False) or (getattr(cmd_opts, \"xformers\", False) and shared.xformers_available and torch.version.cuda and (6, 0) <= torch.cuda.get_device_capability(shared.device) <= (9, 0)):\n            import xformers\n            out = xformers.ops.memory_efficient_attention(\n                q, k, v, op=get_xformers_flash_attention_op(q,k,v), attn_bias=mask,\n            )\n        elif getattr(cmd_opts, \"opt_sdp_no_mem_attention\", False) and can_use_sdp:\n            with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=False):\n                out = F.scaled_dot_product_attention(\n                    q, k, v, dropout_p=0.0, attn_mask=mask\n                )\n        elif getattr(cmd_opts, \"opt_sdp_attention\", True) and can_use_sdp:\n            out = F.scaled_dot_product_attention(\n                q, k, v, dropout_p=0.0, attn_mask=mask\n            )\n        else:\n\n            sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale\n            del q, k\n\n            if exists(mask):\n                sim.masked_fill_(~mask, max_neg_value)\n\n            # attention, what we cannot get enough of\n            sim = sim.softmax(dim=-1)\n\n            out = torch.einsum('b i j, b j d -> b i d', sim, v)\n        \n        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n        return self.to_out(out)", "\n\nclass SpatialTransformer(nn.Module):\n    \"\"\"\n    Transformer block for image-like data in spatial axis.\n    First, project the input (aka embedding)\n    and reshape to b, t, d.\n    Then apply standard transformer action.\n    Finally, reshape to image\n    NEW: use_linear for more efficiency instead of the 1x1 convs\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,\n                 n_heads,\n                 d_head,\n                 depth=1,\n                 dropout=0.0,\n                 context_dim=None,\n                 disable_self_attn=False,\n                 use_linear=False,\n                 use_checkpoint=True):\n        super().__init__()\n        if exists(context_dim) and not isinstance(context_dim, list):\n            context_dim = [context_dim]\n        self.in_channels = in_channels\n        inner_dim = n_heads * d_head\n        self.norm = torch.nn.GroupNorm(\n            num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n        if not use_linear:\n            self.proj_in = nn.Conv2d(\n                in_channels, inner_dim, kernel_size=1, stride=1, padding=0)\n        else:\n            self.proj_in = nn.Linear(in_channels, inner_dim)\n\n        self.transformer_blocks = nn.ModuleList([\n            BasicTransformerBlock(\n                inner_dim,\n                n_heads,\n                d_head,\n                dropout=dropout,\n                context_dim=context_dim[d],\n                disable_self_attn=disable_self_attn,\n                checkpoint=use_checkpoint) for d in range(depth)\n        ])\n        if not use_linear:\n            self.proj_out = zero_module(\n                nn.Conv2d(\n                    inner_dim, in_channels, kernel_size=1, stride=1,\n                    padding=0))\n        else:\n            self.proj_out = zero_module(nn.Linear(in_channels, inner_dim))\n        self.use_linear = use_linear\n\n    def forward(self, x, context=None):\n        # note: if no context is given, cross-attention defaults to self-attention\n        if not isinstance(context, list):\n            context = [context]\n        b, c, h, w = x.shape\n        x_in = x\n        x = self.norm(x)\n        if not self.use_linear:\n            x = self.proj_in(x)\n        x = rearrange(x, 'b c h w -> b (h w) c').contiguous()\n        if self.use_linear:\n            x = self.proj_in(x)\n        for i, block in enumerate(self.transformer_blocks):\n            x = block(x, context=context[i])\n        if self.use_linear:\n            x = self.proj_out(x)\n        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w).contiguous()\n        if not self.use_linear:\n            x = self.proj_out(x)\n        return x + x_in", "\n\nclass TemporalTransformer(nn.Module):\n    \"\"\"\n    Transformer block for image-like data in temporal axis.\n    First, reshape to b, t, d.\n    Then apply standard transformer action.\n    Finally, reshape to image\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,\n                 n_heads,\n                 d_head,\n                 depth=1,\n                 dropout=0.0,\n                 context_dim=None,\n                 disable_self_attn=False,\n                 use_linear=False,\n                 use_checkpoint=True,\n                 only_self_att=True,\n                 multiply_zero=False):\n        super().__init__()\n        self.multiply_zero = multiply_zero\n        self.only_self_att = only_self_att\n        if self.only_self_att:\n            context_dim = None\n        if not isinstance(context_dim, list):\n            context_dim = [context_dim]\n        self.in_channels = in_channels\n        inner_dim = n_heads * d_head\n        self.norm = torch.nn.GroupNorm(\n            num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n        if not use_linear:\n            self.proj_in = nn.Conv1d(\n                in_channels, inner_dim, kernel_size=1, stride=1, padding=0)\n        else:\n            self.proj_in = nn.Linear(in_channels, inner_dim)\n\n        self.transformer_blocks = nn.ModuleList([\n            BasicTransformerBlock(\n                inner_dim,\n                n_heads,\n                d_head,\n                dropout=dropout,\n                context_dim=context_dim[d],\n                checkpoint=use_checkpoint) for d in range(depth)\n        ])\n        if not use_linear:\n            self.proj_out = zero_module(\n                nn.Conv1d(\n                    inner_dim, in_channels, kernel_size=1, stride=1,\n                    padding=0))\n        else:\n            self.proj_out = zero_module(nn.Linear(in_channels, inner_dim))\n        self.use_linear = use_linear\n\n    def forward(self, x, context=None):\n        # note: if no context is given, cross-attention defaults to self-attention\n        if self.only_self_att:\n            context = None\n        if not isinstance(context, list):\n            context = [context]\n        b, c, f, h, w = x.shape\n        x_in = x\n        x = self.norm(x)\n\n        if not self.use_linear:\n            x = rearrange(x, 'b c f h w -> (b h w) c f').contiguous()\n            x = self.proj_in(x)\n        if self.use_linear:\n            x = rearrange(\n                x, '(b f) c h w -> b (h w) f c', f=self.frames).contiguous()\n            x = self.proj_in(x)\n\n        if self.only_self_att:\n            x = rearrange(x, 'bhw c f -> bhw f c').contiguous()\n            for i, block in enumerate(self.transformer_blocks):\n                x = block(x)\n            x = rearrange(x, '(b hw) f c -> b hw f c', b=b).contiguous()\n        else:\n            x = rearrange(x, '(b hw) c f -> b hw f c', b=b).contiguous()\n            for i, block in enumerate(self.transformer_blocks):\n                context[i] = rearrange(\n                    context[i], '(b f) l con -> b f l con',\n                    f=self.frames).contiguous()\n                # calculate each batch one by one (since number in shape could not greater then 65,535 for some package)\n                for j in range(b):\n                    context_i_j = repeat(\n                        context[i][j],\n                        'f l con -> (f r) l con',\n                        r=(h * w) // self.frames,\n                        f=self.frames).contiguous()\n                    x[j] = block(x[j], context=context_i_j)\n\n        if self.use_linear:\n            x = self.proj_out(x)\n            x = rearrange(x, 'b (h w) f c -> b f c h w', h=h, w=w).contiguous()\n        if not self.use_linear:\n            x = rearrange(x, 'b hw f c -> (b hw) c f').contiguous()\n            x = self.proj_out(x)\n            x = rearrange(\n                x, '(b h w) c f -> b c f h w', b=b, h=h, w=w).contiguous()\n\n        if self.multiply_zero:\n            x = 0.0 * x + x_in\n        else:\n            x = x + x_in\n        return x", "\n\nclass BasicTransformerBlock(nn.Module):\n\n    def __init__(self,\n                 dim,\n                 n_heads,\n                 d_head,\n                 dropout=0.0,\n                 context_dim=None,\n                 gated_ff=True,\n                 checkpoint=True,\n                 disable_self_attn=False):\n        super().__init__()\n        attn_cls = CrossAttention\n        self.disable_self_attn = disable_self_attn\n        self.attn1 = attn_cls(\n            query_dim=dim,\n            heads=n_heads,\n            dim_head=d_head,\n            dropout=dropout,\n            context_dim=context_dim if self.disable_self_attn else\n            None)  # is a self-attention if not self.disable_self_attn\n        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\n        self.attn2 = attn_cls(\n            query_dim=dim,\n            context_dim=context_dim,\n            heads=n_heads,\n            dim_head=d_head,\n            dropout=dropout)  # is self-attn if context is none\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.norm3 = nn.LayerNorm(dim)\n        self.checkpoint = checkpoint\n\n    def forward(self, x, context=None):\n        x = self.attn1(\n            self.norm1(x),\n            context=context if self.disable_self_attn else None) + x\n        x = self.attn2(self.norm2(x), context=context) + x\n        x = self.ff(self.norm3(x)) + x\n        return x", "\n\n# feedforward\nclass GEGLU(nn.Module):\n\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * F.gelu(gate)", "\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n", "\n\nclass FeedForward(nn.Module):\n\n    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.0):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = nn.Sequential(nn.Linear(\n            dim, inner_dim), nn.GELU()) if not glu else GEGLU(dim, inner_dim)\n\n        self.net = nn.Sequential(project_in, nn.Dropout(dropout),\n                                 nn.Linear(inner_dim, dim_out))\n\n    def forward(self, x):\n        return self.net(x)", "\n\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 upsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self,\n                 channels,\n                 use_conv,\n                 dims=2,\n                 out_channels=None,\n                 padding=1):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        if use_conv:\n            self.conv = nn.Conv2d(\n                self.channels, self.out_channels, 3, padding=padding)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        if self.dims == 3:\n            x = F.interpolate(\n                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2),\n                mode='nearest')\n        else:\n            x = F.interpolate(x, scale_factor=2, mode='nearest')\n        if self.use_conv:\n            x = self.conv(x)\n        return x", "\n\nclass ResBlock(nn.Module):\n    \"\"\"\n    A residual block that can optionally change the number of channels.\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param up: if True, use this block for upsampling.\n    :param down: if True, use this block for downsampling.\n    :param use_temporal_conv: if True, use the temporal convolution.\n    :param use_image_dataset: if True, the temporal parameters will not be optimized.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        up=False,\n        down=False,\n        use_temporal_conv=True,\n        use_image_dataset=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_scale_shift_norm = use_scale_shift_norm\n        self.use_temporal_conv = use_temporal_conv\n\n        self.in_layers = nn.Sequential(\n            nn.GroupNorm(32, channels),\n            nn.SiLU(),\n            nn.Conv2d(channels, self.out_channels, 3, padding=1),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims)\n            self.x_upd = Upsample(channels, False, dims)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims)\n            self.x_upd = Downsample(channels, False, dims)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(\n                emb_channels,\n                2 * self.out_channels\n                if use_scale_shift_norm else self.out_channels,\n            ),\n        )\n        self.out_layers = nn.Sequential(\n            nn.GroupNorm(32, self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(\n                nn.Conv2d(self.out_channels, self.out_channels, 3, padding=1)),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = conv_nd(\n                dims, channels, self.out_channels, 3, padding=1)\n        else:\n            self.skip_connection = nn.Conv2d(channels, self.out_channels, 1)\n\n        if self.use_temporal_conv:\n            self.temopral_conv = TemporalConvBlock_v2(\n                self.out_channels,\n                self.out_channels,\n                dropout=0.1,\n                use_image_dataset=use_image_dataset)\n\n    def forward(self, x, emb, batch_size):\n        \"\"\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        return self._forward(x, emb, batch_size)\n\n    def _forward(self, x, emb, batch_size):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        emb_out = self.emb_layers(emb).type(h.dtype)\n        while len(emb_out.shape) < len(h.shape):\n            emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            scale, shift = torch.chunk(emb_out, 2, dim=1)\n            h = out_norm(h) * (1 + scale) + shift\n            h = out_rest(h)\n        else:\n            h = h + emb_out\n            h = self.out_layers(h)\n        h = self.skip_connection(x) + h\n\n        if self.use_temporal_conv:\n            h = rearrange(h, '(b f) c h w -> b c f h w', b=batch_size)\n            h = self.temopral_conv(h)\n            h = rearrange(h, 'b c f h w -> (b f) c h w')\n        return h", "\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self,\n                 channels,\n                 use_conv,\n                 dims=2,\n                 out_channels=None,\n                 padding=1):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if self.use_conv:\n            self.op = nn.Conv2d(\n                self.channels,\n                self.out_channels,\n                3,\n                stride=stride,\n                padding=padding)\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)", "\n\nclass Resample(nn.Module):\n\n    def __init__(self, in_dim, out_dim, mode):\n        assert mode in ['none', 'upsample', 'downsample']\n        super(Resample, self).__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.mode = mode\n\n    def forward(self, x, reference=None):\n        if self.mode == 'upsample':\n            assert reference is not None\n            x = F.interpolate(x, size=reference.shape[-2:], mode='nearest')\n        elif self.mode == 'downsample':\n            x = F.adaptive_avg_pool2d(\n                x, output_size=tuple(u // 2 for u in x.shape[-2:]))\n        return x", "\n\nclass ResidualBlock(nn.Module):\n\n    def __init__(self,\n                 in_dim,\n                 embed_dim,\n                 out_dim,\n                 use_scale_shift_norm=True,\n                 mode='none',\n                 dropout=0.0):\n        super(ResidualBlock, self).__init__()\n        self.in_dim = in_dim\n        self.embed_dim = embed_dim\n        self.out_dim = out_dim\n        self.use_scale_shift_norm = use_scale_shift_norm\n        self.mode = mode\n\n        # layers\n        self.layer1 = nn.Sequential(\n            nn.GroupNorm(32, in_dim), nn.SiLU(),\n            nn.Conv2d(in_dim, out_dim, 3, padding=1))\n        self.resample = Resample(in_dim, in_dim, mode)\n        self.embedding = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(embed_dim,\n                      out_dim * 2 if use_scale_shift_norm else out_dim))\n        self.layer2 = nn.Sequential(\n            nn.GroupNorm(32, out_dim), nn.SiLU(), nn.Dropout(dropout),\n            nn.Conv2d(out_dim, out_dim, 3, padding=1))\n        self.shortcut = nn.Identity() if in_dim == out_dim else nn.Conv2d(\n            in_dim, out_dim, 1)\n        # zero out the last layer params\n        nn.init.zeros_(self.layer2[-1].weight)\n\n    def forward(self, x, e, reference=None):\n        identity = self.resample(x, reference)\n        x = self.layer1[-1](self.resample(self.layer1[:-1](x), reference))\n        e = self.embedding(e).unsqueeze(-1).unsqueeze(-1).type(x.dtype)\n        if self.use_scale_shift_norm:\n            scale, shift = e.chunk(2, dim=1)\n            x = self.layer2[0](x) * (1 + scale) + shift\n            x = self.layer2[1:](x)\n        else:\n            x = x + e\n            x = self.layer2(x)\n        x = x + self.shortcut(identity)\n        return x", "\n\nclass AttentionBlock(nn.Module):\n\n    def __init__(self, dim, context_dim=None, num_heads=None, head_dim=None):\n        # consider head_dim first, then num_heads\n        num_heads = dim // head_dim if head_dim else num_heads\n        head_dim = dim // num_heads\n        assert num_heads * head_dim == dim\n        super(AttentionBlock, self).__init__()\n        self.dim = dim\n        self.context_dim = context_dim\n        self.num_heads = num_heads\n        self.head_dim = head_dim\n        self.scale = math.pow(head_dim, -0.25)\n\n        # layers\n        self.norm = nn.GroupNorm(32, dim)\n        self.to_qkv = nn.Conv2d(dim, dim * 3, 1)\n        if context_dim is not None:\n            self.context_kv = nn.Linear(context_dim, dim * 2)\n        self.proj = nn.Conv2d(dim, dim, 1)\n\n        # zero out the last layer params\n        nn.init.zeros_(self.proj.weight)\n\n    def forward(self, x, context=None):\n        r\"\"\"x:       [B, C, H, W].\n            context: [B, L, C] or None.\n        \"\"\"\n        identity = x\n        b, c, h, w, n, d = *x.size(), self.num_heads, self.head_dim\n\n        # compute query, key, value\n        x = self.norm(x)\n        q, k, v = self.to_qkv(x).view(b, n * 3, d, h * w).chunk(3, dim=1)\n        if context is not None:\n            ck, cv = self.context_kv(context).reshape(b, -1, n * 2,\n                                                      d).permute(0, 2, 3,\n                                                                 1).chunk(\n                                                                     2, dim=1)\n            k = torch.cat([ck, k], dim=-1)\n            v = torch.cat([cv, v], dim=-1)\n\n        # compute attention\n\n        if getattr(cmd_opts, \"force_enable_xformers\", False) or (getattr(cmd_opts, \"xformers\", False) and shared.xformers_available and torch.version.cuda and (6, 0) <= torch.cuda.get_device_capability(shared.device) <= (9, 0)):\n            import xformers\n            x = xformers.ops.memory_efficient_attention(\n                q, k, v, op=get_xformers_flash_attention_op(q,k,v),\n            )\n        elif getattr(cmd_opts, \"opt_sdp_no_mem_attention\", False) and can_use_sdp:\n            with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=False):\n                x = F.scaled_dot_product_attention(\n                    q, k, v, dropout_p=0.0,\n                )\n        elif getattr(cmd_opts, \"opt_sdp_attention\", True) and can_use_sdp:\n            x = F.scaled_dot_product_attention(\n                    q, k, v, dropout_p=0.0,\n                )\n        else:\n            attn = torch.matmul(q.transpose(-1, -2) * self.scale, k * self.scale)\n            attn = F.softmax(attn, dim=-1)\n\n            # gather context\n            x = torch.matmul(v, attn.transpose(-1, -2))\n        x = x.reshape(b, c, h, w)\n        # output\n        x = self.proj(x)\n        return x + identity", "\n\nclass TemporalConvBlock_v2(nn.Module):\n\n    def __init__(self,\n                 in_dim,\n                 out_dim=None,\n                 dropout=0.0,\n                 use_image_dataset=False):\n        super(TemporalConvBlock_v2, self).__init__()\n        if out_dim is None:\n            out_dim = in_dim  # int(1.5*in_dim)\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.use_image_dataset = use_image_dataset\n\n        # conv layers\n        self.conv1 = nn.Sequential(\n            nn.GroupNorm(32, in_dim), nn.SiLU(),\n            nn.Conv3d(in_dim, out_dim, (3, 1, 1), padding=(1, 0, 0)))\n        self.conv2 = nn.Sequential(\n            nn.GroupNorm(32, out_dim), nn.SiLU(), nn.Dropout(dropout),\n            nn.Conv3d(out_dim, in_dim, (3, 1, 1), padding=(1, 0, 0)))\n        self.conv3 = nn.Sequential(\n            nn.GroupNorm(32, out_dim), nn.SiLU(), nn.Dropout(dropout),\n            nn.Conv3d(out_dim, in_dim, (3, 1, 1), padding=(1, 0, 0)))\n        self.conv4 = nn.Sequential(\n            nn.GroupNorm(32, out_dim), nn.SiLU(), nn.Dropout(dropout),\n            nn.Conv3d(out_dim, in_dim, (3, 1, 1), padding=(1, 0, 0)))\n\n        # zero out the last layer params,so the conv block is identity\n        nn.init.zeros_(self.conv4[-1].weight)\n        nn.init.zeros_(self.conv4[-1].bias)\n\n    def forward(self, x):\n        identity = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n\n        if self.use_image_dataset:\n            x = identity + 0.0 * x\n        else:\n            x = identity + x\n        return x", "\n\ndef _i(tensor, t, x):\n    r\"\"\"Index tensor using t and format the output according to x.\n    \"\"\"\n    tensor = tensor.to(x.device)\n    shape = (x.size(0), ) + (1, ) * (x.ndim - 1)\n    return tensor[t].view(shape).to(x)\n\n\ndef beta_schedule(schedule,\n                  num_timesteps=1000,\n                  init_beta=None,\n                  last_beta=None):\n    if schedule == 'linear_sd':\n        return torch.linspace(\n            init_beta**0.5, last_beta**0.5, num_timesteps,\n            dtype=torch.float64)**2\n    else:\n        raise ValueError(f'Unsupported schedule: {schedule}')", "\n\ndef beta_schedule(schedule,\n                  num_timesteps=1000,\n                  init_beta=None,\n                  last_beta=None):\n    if schedule == 'linear_sd':\n        return torch.linspace(\n            init_beta**0.5, last_beta**0.5, num_timesteps,\n            dtype=torch.float64)**2\n    else:\n        raise ValueError(f'Unsupported schedule: {schedule}')", "\n\nclass GaussianDiffusion(object):\n    r\"\"\" Diffusion Model for DDIM.\n    \"Denoising diffusion implicit models.\" by Song, Jiaming, Chenlin Meng, and Stefano Ermon.\n    See https://arxiv.org/abs/2010.02502\n    \"\"\"\n\n    def __init__(self,\n                 betas,\n                 mean_type='eps',\n                 var_type='learned_range',\n                 loss_type='mse',\n                 epsilon=1e-12,\n                 rescale_timesteps=False):\n        # check input\n        if not isinstance(betas, torch.DoubleTensor):\n            betas = torch.tensor(betas, dtype=torch.float64)\n        assert min(betas) > 0 and max(betas) <= 1\n        assert mean_type in ['x0', 'x_{t-1}', 'eps']\n        assert var_type in [\n            'learned', 'learned_range', 'fixed_large', 'fixed_small'\n        ]\n        assert loss_type in [\n            'mse', 'rescaled_mse', 'kl', 'rescaled_kl', 'l1', 'rescaled_l1',\n            'charbonnier'\n        ]\n        self.betas = betas\n        self.num_timesteps = len(betas)\n        self.mean_type = mean_type\n        self.var_type = var_type\n        self.loss_type = loss_type\n        self.epsilon = epsilon\n        self.rescale_timesteps = rescale_timesteps\n\n        # alphas\n        alphas = 1 - self.betas\n        self.alphas_cumprod = torch.cumprod(alphas, dim=0)\n        self.alphas_cumprod_prev = torch.cat(\n            [alphas.new_ones([1]), self.alphas_cumprod[:-1]])\n        self.alphas_cumprod_next = torch.cat(\n            [self.alphas_cumprod[1:],\n             alphas.new_zeros([1])])\n\n        # q(x_t | x_{t-1})\n        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0\n                                                        - self.alphas_cumprod)\n        self.log_one_minus_alphas_cumprod = torch.log(1.0\n                                                      - self.alphas_cumprod)\n        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod\n                                                      - 1)\n\n        # q(x_{t-1} | x_t, x_0)\n        self.posterior_variance = betas * (1.0 - self.alphas_cumprod_prev) / (\n            1.0 - self.alphas_cumprod)\n        self.posterior_log_variance_clipped = torch.log(\n            self.posterior_variance.clamp(1e-20))\n        self.posterior_mean_coef1 = betas * torch.sqrt(\n            self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        self.posterior_mean_coef2 = (\n            1.0 - self.alphas_cumprod_prev) * torch.sqrt(alphas) / (\n                1.0 - self.alphas_cumprod)\n\n    def add_noise(self, xt, noise, t):\n        #print(\"adding noise\", t,\n        #      self.sqrt_alphas_cumprod[t], self.sqrt_one_minus_alphas_cumprod[t])\n        noisy_sample = self.sqrt_alphas_cumprod[t] * \\\n            xt+noise*self.sqrt_one_minus_alphas_cumprod[t]\n        return noisy_sample\n\n    def p_mean_variance(self,\n                        xt,\n                        t,\n                        model,\n                        model_kwargs={},\n                        clamp=None,\n                        percentile=None,\n                        guide_scale=None):\n        r\"\"\"Distribution of p(x_{t-1} | x_t).\n        \"\"\"\n        # predict distribution\n        if guide_scale is None or guide_scale == 1:\n            out = model(xt, self._scale_timesteps(t), **model_kwargs[0])\n        else:\n            # classifier-free guidance\n            # (model_kwargs[0]: conditional kwargs; model_kwargs[1]: non-conditional kwargs)\n            assert isinstance(model_kwargs, list) and len(model_kwargs) == 2\n            y_out = model(xt, self._scale_timesteps(t), **model_kwargs[0])\n            u_out = model(xt, self._scale_timesteps(t), **model_kwargs[1])\n            dim = y_out.size(1) if self.var_type.startswith(\n                'fixed') else y_out.size(1) // 2\n            a = u_out[:, :dim]\n            b = guide_scale * (y_out[:, :dim] - u_out[:, :dim])\n            c = y_out[:, dim:]\n            out = torch.cat([a + b, c], dim=1)\n\n        # compute variance\n        if self.var_type == 'fixed_small':\n            var = _i(self.posterior_variance, t, xt)\n            log_var = _i(self.posterior_log_variance_clipped, t, xt)\n\n        # compute mean and x0\n        if self.mean_type == 'eps':\n            x0 = _i(self.sqrt_recip_alphas_cumprod, t, xt) * xt - _i(\n                self.sqrt_recipm1_alphas_cumprod, t, xt) * out\n            mu, _, _ = self.q_posterior_mean_variance(x0, xt, t)\n\n        # restrict the range of x0\n        if percentile is not None:\n            assert percentile > 0 and percentile <= 1  # e.g., 0.995\n            s = torch.quantile(\n                x0.flatten(1).abs(), percentile,\n                dim=1).clamp_(1.0).view(-1, 1, 1, 1)\n            x0 = torch.min(s, torch.max(-s, x0)) / s\n        elif clamp is not None:\n            x0 = x0.clamp(-clamp, clamp)\n        return mu, var, log_var, x0\n\n    def q_posterior_mean_variance(self, x0, xt, t):\n        r\"\"\"Distribution of q(x_{t-1} | x_t, x_0).\n        \"\"\"\n        mu = _i(self.posterior_mean_coef1, t, xt) * x0 + _i(\n            self.posterior_mean_coef2, t, xt) * xt\n        var = _i(self.posterior_variance, t, xt)\n        log_var = _i(self.posterior_log_variance_clipped, t, xt)\n        return mu, var, log_var\n\n    @torch.no_grad()\n    def ddim_sample(self,\n                    xt,\n                    t,\n                    model,\n                    model_kwargs={},\n                    clamp=None,\n                    percentile=None,\n                    condition_fn=None,\n                    guide_scale=None,\n                    ddim_timesteps=20,\n                    eta=0.0):\n        r\"\"\"Sample from p(x_{t-1} | x_t) using DDIM.\n            - condition_fn: for classifier-based guidance (guided-diffusion).\n            - guide_scale: for classifier-free guidance (glide/dalle-2).\n        \"\"\"\n        stride = self.num_timesteps // ddim_timesteps\n\n        # predict distribution of p(x_{t-1} | x_t)\n        _, _, _, x0 = self.p_mean_variance(xt, t, model, model_kwargs, clamp,\n                                           percentile, guide_scale)\n        if condition_fn is not None:\n            # x0 -> eps\n            alpha = _i(self.alphas_cumprod, t, xt)\n            eps = (_i(self.sqrt_recip_alphas_cumprod, t, xt) * xt - x0) / _i(\n                self.sqrt_recipm1_alphas_cumprod, t, xt)\n            eps = eps - (1 - alpha).sqrt() * condition_fn(\n                xt, self._scale_timesteps(t), **model_kwargs)\n\n            # eps -> x0\n            x0 = _i(self.sqrt_recip_alphas_cumprod, t, xt) * xt - _i(\n                self.sqrt_recipm1_alphas_cumprod, t, xt) * eps\n\n        # derive variables\n        eps = (_i(self.sqrt_recip_alphas_cumprod, t, xt) * xt - x0) / _i(\n            self.sqrt_recipm1_alphas_cumprod, t, xt)\n        alphas = _i(self.alphas_cumprod, t, xt)\n        alphas_prev = _i(self.alphas_cumprod, (t - stride).clamp(0), xt)\n        a = (1 - alphas_prev) / (1 - alphas)\n        b = (1 - alphas / alphas_prev)\n        sigmas = eta * torch.sqrt(a * b)\n\n        # random sample\n        noise = torch.randn_like(xt)\n        direction = torch.sqrt(1 - alphas_prev - sigmas**2) * eps\n        mask = t.ne(0).float().view(-1, *((1, ) * (xt.ndim - 1)))\n        xt_1 = torch.sqrt(alphas_prev) * x0 + direction + mask * sigmas * noise\n\n        noise.cpu()\n        direction.cpu()\n        mask.cpu()\n        alphas.cpu()\n        alphas_prev.cpu()\n        sigmas.cpu()\n        a.cpu()\n        b.cpu()\n        eps.cpu()\n        x0.cpu()\n        noise = None\n        direction = None\n        mask = None\n        alphas = None\n        alphas_prev = None\n        sigmas = None\n        a = None\n        b = None\n        eps = None\n        x0 = None\n        del noise\n        del direction\n        del mask\n        del alphas\n        del alphas_prev\n        del sigmas\n        del a\n        del b\n        del eps\n        del x0\n        return xt_1\n\n    @torch.no_grad()\n    def ddim_sample_loop(self,\n                         noise,\n                         model,\n                         c=None,\n                         uc=None,\n                         num_sample=1,\n                         clamp=None,\n                         percentile=None,\n                         condition_fn=None,\n                         guide_scale=None,\n                         ddim_timesteps=20,\n                         eta=0.0,\n                         skip_steps=0,\n                         mask=None,\n                         ):\n\n        # prepare input\n        b = noise.size(0)\n        xt = noise\n\n        # diffusion process (TODO: clamp is inaccurate! Consider replacing the stride by explicit prev/next steps)\n        steps = (1 + torch.arange(0, self.num_timesteps,\n                                  self.num_timesteps // ddim_timesteps)).clamp(\n                                      0, self.num_timesteps - 1).flip(0)\n        \n        state.sampling_steps = ddim_timesteps\n\n        if skip_steps > 0:\n            step0 = steps[skip_steps-1]\n            steps = steps[skip_steps:]\n\n            noise_to_add = torch.randn_like(xt)\n            t = torch.full((b, ), step0, dtype=torch.long, device=xt.device)\n            print(\"huh\", step0, t)\n            xt = self.add_noise(xt, noise_to_add, step0)\n            state.sampling_steps = state.sampling_steps - skip_steps\n\n        if mask is not None:\n            pass\n            step0 = steps[0]\n            original_latents=xt\n            noise_to_add = torch.randn_like(xt)\n            xt = self.add_noise(xt, noise_to_add, step0)\n            #convert mask to 0,1 valued based on step\n            v=0\n            binary_mask = torch.where(mask <= v, torch.zeros_like(mask), torch.ones_like(mask))\n            #print(\"about to die\",xt,original_latents,mask,binary_mask)\n            \n\n        pbar = tqdm(steps, desc=\"DDIM sampling\")\n\n        #print(c)\n        #print(uc)        \n\n        i = 0\n        for step in pbar:\n\n            state.sampling_step = i\n\n            if state.interrupted:\n                raise InterruptedException\n\n            c_i = reconstruct_cond_batch(c, i)\n            uc_i = reconstruct_cond_batch(uc, i)\n\n            # for DDIM, shapes must match, we can't just process cond and uncond independently;\n            # filling unconditional_conditioning with repeats of the last vector to match length is\n            # not 100% correct but should work well enough\n            if uc_i.shape[1] < c_i.shape[1]:\n                last_vector = uc_i[:, -1:]\n                last_vector_repeated = last_vector.repeat([1, c_i.shape[1] - uc.shape[1], 1])\n                uc_i = torch.hstack([uc_i, last_vector_repeated])\n            elif uc_i.shape[1] > c_i.shape[1]:\n                uc_i = uc_i[:, :c_i.shape[1]]\n            \n            #print(c_i.shape, uc_i.shape)\n\n            t = torch.full((b, ), step, dtype=torch.long, device=xt.device)\n            uc_i = uc_i.type(torch.float16)\n            c_i = c_i.type(torch.float16)\n            #print(uc_i)\n            #print(c_i)\n            model_kwargs=[{\n                'y':\n                c_i,\n            }, {\n                'y':\n                uc_i,\n            }]\n            xt = self.ddim_sample(xt, t, model, model_kwargs, clamp,\n                                  percentile, condition_fn, guide_scale,\n                                  ddim_timesteps, eta)\n\n            \n            #inpainting\n            if mask is not None and i<len(steps)-1:\n                v=(ddim_timesteps-i-1)/ddim_timesteps\n                binary_mask = torch.where(mask <= v, torch.zeros_like(mask), torch.ones_like(mask))\n            \n                noise_to_add = torch.randn_like(xt)\n                #noise_to_add=xt\n                to_inpaint=self.add_noise(original_latents, noise_to_add, steps[i+1])\n                xt=to_inpaint*(1-binary_mask)+xt*binary_mask\n                #print(mask.shape,i,ddim_timesteps,v)\n                #print(mask[0,0,:,0,0])\n                #print(binary_mask[0,0,:,0,0])\n                pass\n\n            \n            t.cpu()\n            t = None\n            i += 1\n            pbar.set_description(f\"DDIM sampling {str(step)}\")\n\n            if state.skipped:\n                break\n        pbar.close()\n        return xt\n\n    def _scale_timesteps(self, t):\n        if self.rescale_timesteps:\n            return t.float() * 1000.0 / self.num_timesteps\n        return t", "\n\nclass AutoencoderKL(nn.Module):\n\n    def __init__(self,\n                 ddconfig,\n                 embed_dim,\n                 ckpt_path=None,\n                 image_key='image',\n                 colorize_nlabels=None,\n                 monitor=None,\n                 ema_decay=None,\n                 learn_logvar=False):\n        super().__init__()\n        self.learn_logvar = learn_logvar\n        self.image_key = image_key\n        self.encoder = Encoder(**ddconfig)\n        self.decoder = Decoder(**ddconfig)\n        assert ddconfig['double_z']\n        self.quant_conv = torch.nn.Conv2d(2 * ddconfig['z_channels'],\n                                          2 * embed_dim, 1)\n        self.post_quant_conv = torch.nn.Conv2d(embed_dim,\n                                               ddconfig['z_channels'], 1)\n        self.embed_dim = embed_dim\n        if colorize_nlabels is not None:\n            assert type(colorize_nlabels) == int\n            self.register_buffer('colorize',\n                                 torch.randn(3, colorize_nlabels, 1, 1))\n        if monitor is not None:\n            self.monitor = monitor\n\n        self.use_ema = ema_decay is not None\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path)\n\n    def init_from_ckpt(self, path):\n        sd = torch.load(path, map_location='cpu')['state_dict']\n        keys = list(sd.keys())\n\n        import collections\n        sd_new = collections.OrderedDict()\n\n        for k in keys:\n            if k.find('first_stage_model') >= 0:\n                k_new = k.split('first_stage_model.')[-1]\n                sd_new[k_new] = sd[k]\n\n        self.load_state_dict(sd_new, strict=True)\n        del sd\n        del sd_new\n        torch_gc()\n\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        moments = self.quant_conv(h)\n        posterior = DiagonalGaussianDistribution(moments)\n        return posterior\n\n    def decode(self, z):\n        z = self.post_quant_conv(z)\n        dec = self.decoder(z)\n        return dec\n\n    def forward(self, input, sample_posterior=True):\n        posterior = self.encode(input)\n        if sample_posterior:\n            z = posterior.sample()\n        else:\n            z = posterior.mode()\n        dec = self.decode(z)\n        return dec, posterior\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1,\n                      2).to(memory_format=torch.contiguous_format).float()\n        return x\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n    @torch.no_grad()\n    def log_images(self, batch, only_inputs=False, log_ema=False, **kwargs):\n        log = dict()\n        x = self.get_input(batch, self.image_key)\n        x = x.to(self.device)\n        if not only_inputs:\n            xrec, posterior = self(x)\n            if x.shape[1] > 3:\n                # colorize with random projection\n                assert xrec.shape[1] > 3\n                x = self.to_rgb(x)\n                xrec = self.to_rgb(xrec)\n            log['samples'] = self.decode(torch.randn_like(posterior.sample()))\n            log['reconstructions'] = xrec\n            if log_ema or self.use_ema:\n                with self.ema_scope():\n                    xrec_ema, posterior_ema = self(x)\n                    if x.shape[1] > 3:\n                        # colorize with random projection\n                        assert xrec_ema.shape[1] > 3\n                        xrec_ema = self.to_rgb(xrec_ema)\n                    log['samples_ema'] = self.decode(\n                        torch.randn_like(posterior_ema.sample()))\n                    log['reconstructions_ema'] = xrec_ema\n        log['inputs'] = x\n        return log\n\n    def to_rgb(self, x):\n        assert self.image_key == 'segmentation'\n        if not hasattr(self, 'colorize'):\n            self.register_buffer('colorize',\n                                 torch.randn(3, x.shape[1], 1, 1).to(x))\n        x = F.conv2d(x, weight=self.colorize)\n        x = 2. * (x - x.min()) / (x.max() - x.min()) - 1.\n        return x", "\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device=device, dtype=torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device=device, dtype=torch.bool)\n    else:\n        mask = torch.zeros(shape, device=device).float().uniform_(0, 1) < prob\n        # aviod mask all, which will cause find_unused_parameters error\n        if mask.all():\n            mask[0] = False\n        return mask", "\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f'unsupported dimensions: {dims}')", "\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f'unsupported dimensions: {dims}')", ""]}
{"filename": "scripts/modelscope/t2v_pipeline.py", "chunked_list": ["# https://github.com/modelscope/modelscope/tree/master/modelscope/pipelines/multi_modal Apache 2.0\n# Copyright 2021-2022 The Alibaba Fundamental Vision Team Authors. All rights reserved.\n\n# The modified Apache 2.0 code is incorporated into the Apache 2.0-compatible AGPL v3.0 license\n# Copyright (C) 2023 by Artem Khrapov (kabachuha)\n# Read LICENSE for usage terms.\n\nimport datetime\nimport json\nimport os", "import json\nimport os\nimport tempfile\nfrom os import path as osp\nfrom types import SimpleNamespace\nfrom typing import Any, Dict, Optional\n\nimport torch\nimport random\nimport torch.cuda.amp as amp", "import random\nimport torch.cuda.amp as amp\nfrom einops import rearrange\nimport cv2\nfrom modelscope.t2v_model import UNetSD, AutoencoderKL, GaussianDiffusion, beta_schedule\nfrom modules import devices, shared\nfrom modules import prompt_parser\nfrom samplers.uni_pc.sampler import UniPCSampler\nfrom samplers.samplers_common import Txt2VideoSampler\nfrom samplers.samplers_common import available_samplers", "from samplers.samplers_common import Txt2VideoSampler\nfrom samplers.samplers_common import available_samplers\n\n__all__ = ['TextToVideoSynthesis']\n\nfrom modelscope.t2v_model import torch_gc\nfrom modelscope.clip_hardcode import FrozenOpenCLIPEmbedder\n\nclass TextToVideoSynthesis():\n    r\"\"\"\n    task for text to video synthesis.\n\n    Attributes:\n        sd_model: denosing model using in this task.\n        diffusion: diffusion model for DDIM.\n        autoencoder: decode the latent representation into visual space with VQGAN.\n        clip_encoder: encode the text into text embedding.\n    \"\"\"\n\n    def __init__(self, model_dir):\n        r\"\"\"\n        Args:\n            model_dir (`str` or `os.PathLike`)\n                Can be either:\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co\n                      or modelscope.cn. Valid model ids can be located at the root-level, like `bert-base-uncased`,\n                      or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.\n                    - A path to a *directory* containing model weights saved using\n                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n                      this case, `from_tf` should be set to `True` and a configuration object should be provided as\n                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\n                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\n                      `True`.\n        \"\"\"\n        super().__init__()\n        self.model_dir = model_dir\n        self.device = torch.device('cpu')\n        # Load the configuration from a file\n        with open(model_dir+'/configuration.json', 'r') as f:\n            config_dict = json.load(f)\n\n        # Convert the dictionary to a namespace object\n        self.config = SimpleNamespace(**config_dict)\n        print(\"config\", self.config)\n\n        self.keep_in_vram = 'None' #None, All, Model\n\n        cfg = self.config.model[\"model_cfg\"]\n        cfg['temporal_attention'] = True if cfg[\n            'temporal_attention'] == 'True' else False\n\n        # Initialize unet\n        self.sd_model = UNetSD(\n            in_dim=cfg['unet_in_dim'],\n            dim=cfg['unet_dim'],\n            y_dim=cfg['unet_y_dim'],\n            context_dim=cfg['unet_context_dim'],\n            out_dim=cfg['unet_out_dim'],\n            dim_mult=cfg['unet_dim_mult'],\n            num_heads=cfg['unet_num_heads'],\n            head_dim=cfg['unet_head_dim'],\n            num_res_blocks=cfg['unet_res_blocks'],\n            attn_scales=cfg['unet_attn_scales'],\n            dropout=cfg['unet_dropout'],\n            parameterization=cfg['mean_type'],\n            temporal_attention=cfg['temporal_attention'])\n        self.sd_model.load_state_dict(\n            torch.load(\n                osp.join(self.model_dir, self.config.model[\"model_args\"][\"ckpt_unet\"]),\n                map_location='cpu' if devices.has_mps() or torch.cuda.is_available() == False else None, # default to cpu when macos, else default behaviour -- TheSloppiestOfJoes: Added a check if CUDA is available, else use CPU\n            ),\n            strict=True,\n        )\n        self.sd_model.eval()\n        if not devices.has_mps() or torch.cuda.is_available() == True:\n            self.sd_model.half()\n        \n        # Initialize diffusion\n        betas = beta_schedule(\n            'linear_sd',\n            cfg['num_timesteps'],\n            init_beta=0.00085,\n            last_beta=0.0120)\n        \n        self.sd_model.register_schedule(given_betas=betas.numpy())\n        self.diffusion = Txt2VideoSampler(self.sd_model, shared.device, betas=betas)\n        \n        # Initialize autoencoder\n        ddconfig = {\n            'double_z': True,\n            'z_channels': 4,\n            'resolution': 256,\n            'in_channels': 3,\n            'out_ch': 3,\n            'ch': 128,\n            'ch_mult': [1, 2, 4, 4],\n            'num_res_blocks': 2,\n            'attn_resolutions': [],\n            'dropout': 0.0\n        }\n        self.autoencoder = AutoencoderKL(\n            ddconfig, 4,\n            osp.join(self.model_dir, self.config.model[\"model_args\"][\"ckpt_autoencoder\"]))\n        if self.keep_in_vram != \"All\":\n            self.autoencoder.to('cpu')\n        self.autoencoder.eval()\n\n        # Initialize Open clip\n        self.clip_encoder = FrozenOpenCLIPEmbedder(\n            version=osp.join(self.model_dir,\n                             self.config.model[\"model_args\"][\"ckpt_clip\"]),\n                             device='cpu',\n            layer='penultimate')\n\n        if self.keep_in_vram != \"All\":\n            self.clip_encoder.model.to('cpu')\n            self.clip_encoder.to(\"cpu\")\n        self.noise_gen = torch.Generator(device='cpu')\n\n    def compute_latents(self, vd_out, cpu_vae='GPU (half precision)', device=torch.device('cuda')):\n        self.device = device\n        with torch.no_grad():\n            bs_vd, c, max_frames, height, width = vd_out.shape\n            scale_factor = 0.18215\n            vd_out_scaled = vd_out\n\n            if 'CPU' in cpu_vae:\n                print(\"STARTING VAE ON CPU\")\n                self.autoencoder.to(\"cpu\")\n                vd_out_scaled = vd_out_scaled.cpu()\n            else:\n                print(\"STARTING VAE ON GPU\")\n                self.autoencoder.to(self.device)\n                if 'half precision' in cpu_vae:\n                    self.autoencoder.half()\n                    print(\"VAE HALVED\")\n                    vd_out_scaled = vd_out_scaled.half()\n\n            vd_out_scaled = rearrange(\n                vd_out_scaled, 'b c f h w -> (b f) c h w')\n\n            # Split the tensor into chunks along the first dimension\n            chunk_size = 1\n            chunks = vd_out_scaled.chunk(vd_out_scaled.size(0) // chunk_size)\n\n            latents_chunks = []\n            for chunk in chunks:\n                if 'CPU' in cpu_vae:\n                    ch = chunk.cpu().float()\n                else:\n                    ch = chunk.to(self.device).float()\n                    if 'half precision' in cpu_vae:\n                        ch = ch.half()\n\n                latents_chunk = self.autoencoder.encode(ch)\n                latents_chunk = torch.tensor(\n                    latents_chunk.mean).cpu() * scale_factor\n                # latents_chunks.append(latents_chunk.cpu())\n                latents_chunks.append(latents_chunk)\n\n            # Concatenate the latents chunks back into a single tensor\n            latents = torch.cat(latents_chunks, dim=0)\n            latents = rearrange(latents, '(b f) c h w -> b c f h w', b=bs_vd)\n\n        out = latents.type(torch.float32).cpu()\n        return out\n\n    # @torch.compile()\n    def infer(\n        self, \n        prompt, \n        n_prompt, \n        steps, \n        frames, \n        seed, \n        scale, \n        width=256, \n        height=256, \n        eta=0.0, \n        cpu_vae='GPU (half precision)', \n        device=torch.device('cpu'), \n        latents=None, \n        skip_steps=0,\n        strength=0,\n        mask=None, \n        is_vid2vid=False,\n        sampler=available_samplers[0].name\n    ):\n        vars = locals()\n        vars.pop('self')\n        vars.pop('latents')\n        vars.pop('mask')\n        print('Making a video with the following parameters:')\n\n        seed = seed if seed!=-1 else random.randint(0, 2**32 - 1)\n        vars['seed'] = seed\n        print(vars)\n        r\"\"\"\n        The entry function of text to image synthesis task.\n        1. Using diffusion model to generate the video's latent representation.\n        2. Using vqgan model (autoencoder) to decode the video's latent representation to visual space.\n\n        Args:\n            prompt (str, optional): A string describing the scene to generate. Defaults to \"A bunny in the forest\".\n            n_prompt (Optional[str], optional): An additional prompt for generating the scene. Defaults to \"\".\n            steps (int, optional): The number of steps to run the diffusion model. Defaults to 50.\n            frames (int, optional): The number of frames in the generated video. Defaults to 15.\n            scale (float, optional): The scaling factor for the generated video. Defaults to 12.5.\n            width (int, optional): The width of the generated video. Defaults to 256.\n            height (int, optional): The height of the generated video. Defaults to 256.\n            eta (float, optional): A hyperparameter related to the diffusion model's noise schedule. Defaults to 0.0.\n            cpu_vae (bool, optional): If True, the VQGAN model will run on the CPU. Defaults to 'GPU (half precision)'.\n            latents (Optional[Tensor], optional): An optional latent tensor to use as input for the VQGAN model. Defaults to None.\n            strength (Optional[float], optional): A hyperparameter to control the strength of the generated video when using input latent. Defaults to None.\n\n        Returns:\n            A generated video (as list of np.arrays).\n        \"\"\"\n\n        self.device = device\n        self.clip_encoder.to(self.device)\n        self.clip_encoder.device = self.device\n        steps = steps - skip_steps\n        c, uc = self.preprocess(prompt, n_prompt, steps)\n        if self.keep_in_vram != \"All\":\n            self.clip_encoder.to(\"cpu\")\n        torch_gc()\n\n        mask=mask.half() if 'half precision' in cpu_vae and mask is not None else mask\n        latents=latents.half() if 'half precision' in cpu_vae and latents is not None else latents\n\n        # synthesis\n        strength = None if (strength == 0.0 and not is_vid2vid) else strength\n        with torch.no_grad():\n            num_sample = 1\n            channels = 4\n            max_frames= frames\n            latents, noise, shape = self.diffusion.get_noise(\n                num_sample, \n                channels, \n                max_frames, \n                height, \n                width, \n                seed=seed, \n                latents=latents\n            )\n            with amp.autocast(enabled=True):\n                self.sd_model.to(self.device)\n                self.diffusion.get_sampler(sampler, return_sampler=False)\n            \n                x0 = self.diffusion.sample_loop(\n                    steps=steps,\n                    strength=strength,\n                    eta=eta,\n                    conditioning=c,\n                    unconditional_conditioning=uc,\n                    batch_size=num_sample,\n                    guidance_scale=scale,\n                    latents=latents,\n                    shape=shape,\n                    noise=noise,\n                    is_vid2vid=is_vid2vid,\n                    sampler_name=sampler,\n                    mask=mask\n                )\n\n                self.last_tensor = x0\n                self.last_tensor.cpu()\n                if self.keep_in_vram == \"None\":\n                    self.sd_model.to(\"cpu\")\n                torch_gc()\n                scale_factor = 0.18215\n                bs_vd = x0.shape[0]\n                if 'CPU' in cpu_vae:\n                    x0 = x0.cpu()\n                    print(\"DECODING FRAMES\")\n                    print(x0.shape)\n                    # self.autoencoder.to(self.device)\n                    x0.float()\n                    # Split the tensor into chunks along the first dimension\n                    chunk_size = 1\n                    chunks = torch.chunk(x0, chunks=max_frames, dim=2)\n                    # Apply the autoencoder to each chunk\n                    output_chunks = []\n                    if self.keep_in_vram != \"All\":\n                        self.autoencoder.to(\"cpu\")\n                    print(\"STARTING VAE ON CPU\")\n                    x = 0\n                    for chunk in chunks:\n                        ch = chunk.cpu().float()\n                        ch = 1. / scale_factor * ch\n                        ch = rearrange(ch, 'b c f h w -> (b f) c h w')\n                        # print(ch)\n                        chunk = None\n                        del chunk\n                        output_chunk = self.autoencoder.decode(ch)\n                        output_chunk.cpu()\n                        output_chunks.append(output_chunk)\n                        x += 1\n                else:\n                    chunk_size = 1\n                    chunks = torch.chunk(x0, chunks=max_frames, dim=2)\n                    x0 = x0.cpu()\n                    del x0\n\n                    print(\n                        f\"STARTING VAE ON GPU. {len(chunks)} CHUNKS TO PROCESS\")\n                    self.autoencoder.to(self.device)\n                    if 'half precision' in cpu_vae:\n                        self.autoencoder.half()\n                        print(f\"VAE HALVED\")\n                    print(\"DECODING FRAMES\")\n\n                    # Split the tensor into chunks along the first dimension\n                    # Apply the autoencoder to each chunk\n                    output_chunks = []\n                    torch_gc()\n                    x = 0\n                    for chunk in chunks:\n                        chunk = 1. / scale_factor * chunk\n\n                        chunk = rearrange(chunk, 'b c f h w -> (b f) c h w')\n                        output_chunk = self.autoencoder.decode(chunk)\n                        cpu_chunk = output_chunk.cpu()\n                        del output_chunk\n                        output_chunks.append(cpu_chunk)\n                        x += 1\n                print(\"VAE FINISHED\")\n                torch_gc()\n                # Concatenate the output chunks back into a single tensor\n                vd_out = torch.cat(output_chunks, dim=0)\n                # video_data = self.autoencoder.decode(video_data)\n                print(vd_out.shape)\n                vd_out = rearrange(\n                    vd_out, '(b f) c h w -> b c f h w', b=bs_vd)\n        vd_out = vd_out.type(torch.float32).cpu()\n\n        video_path = self.postprocess_video(vd_out)\n        if self.keep_in_vram == \"None\":\n            self.sd_model.to(\"cpu\")\n        if self.keep_in_vram != \"All\":\n            self.clip_encoder.to(\"cpu\")\n            self.autoencoder.to(\"cpu\")\n            self.autoencoder.encoder.to(\"cpu\")\n            self.autoencoder.decoder.to(\"cpu\")\n\n        # self.autoencoder = None\n        # del self.autoencoder\n        del vd_out\n        del latents\n        x0 = None\n        del x0\n        video_data = None\n        del video_data\n        torch_gc()\n        last_tensor = self.last_tensor\n        return video_path, last_tensor\n\n    def cleanup(self):\n        pass\n\n    def preprocess(self, prompt, n_prompt, steps, offload=True):\n        cached_uc = [None, None]\n        cached_c = [None, None]\n\n        def get_conds_with_caching(function, model, required_prompts, steps, cache):\n            if cache[0] is not None and (required_prompts, steps) == cache[0]:\n                return cache[1]\n\n            with devices.autocast():\n                cache[1] = function(model, required_prompts, steps)\n\n            cache[0] = (required_prompts, steps)\n            return cache[1]\n\n        self.clip_encoder.to(self.device) \n        self.clip_encoder.device = self.device       \n        uc = get_conds_with_caching(prompt_parser.get_learned_conditioning, self.clip_encoder, [n_prompt], steps, cached_uc)\n        c = get_conds_with_caching(prompt_parser.get_learned_conditioning, self.clip_encoder, [prompt], steps, cached_c)\n        if offload:\n            if self.keep_in_vram != \"All\":\n                self.clip_encoder.to('cpu')\n        return c, uc\n\n    def postprocess_video(self, video_data):\n        video = tensor2vid(video_data)\n        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S%f')\n        filename = f\"output/mp4s/{timestamp}.mp4\"\n\n        output_video_path = filename\n        if output_video_path is None:\n            output_video_path = tempfile.NamedTemporaryFile(suffix='.mp4').name\n\n        print(output_video_path)\n\n        \"\"\"fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        h, w, c = video[0].shape\n        video_writer = cv2.VideoWriter(\n            output_video_path, fourcc, fps=8, frameSize=(w, h))\"\"\"\n        return_samples = []\n        for i in range(len(video)):\n            img = cv2.cvtColor(video[i], cv2.COLOR_RGB2BGR)\n            # video_writer.write(img)\n            return_samples.append(img)\n        del video\n        del video_data\n        return return_samples\n\n    def forward(self, *args, **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Run the forward pass for a model.\n\n        Returns:\n            Dict[str, Any]: output from the model forward pass\n        \"\"\"\n        pass", "class TextToVideoSynthesis():\n    r\"\"\"\n    task for text to video synthesis.\n\n    Attributes:\n        sd_model: denosing model using in this task.\n        diffusion: diffusion model for DDIM.\n        autoencoder: decode the latent representation into visual space with VQGAN.\n        clip_encoder: encode the text into text embedding.\n    \"\"\"\n\n    def __init__(self, model_dir):\n        r\"\"\"\n        Args:\n            model_dir (`str` or `os.PathLike`)\n                Can be either:\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co\n                      or modelscope.cn. Valid model ids can be located at the root-level, like `bert-base-uncased`,\n                      or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.\n                    - A path to a *directory* containing model weights saved using\n                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n                    - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n                      this case, `from_tf` should be set to `True` and a configuration object should be provided as\n                      `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n                      PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n                    - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\n                      `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\n                      `True`.\n        \"\"\"\n        super().__init__()\n        self.model_dir = model_dir\n        self.device = torch.device('cpu')\n        # Load the configuration from a file\n        with open(model_dir+'/configuration.json', 'r') as f:\n            config_dict = json.load(f)\n\n        # Convert the dictionary to a namespace object\n        self.config = SimpleNamespace(**config_dict)\n        print(\"config\", self.config)\n\n        self.keep_in_vram = 'None' #None, All, Model\n\n        cfg = self.config.model[\"model_cfg\"]\n        cfg['temporal_attention'] = True if cfg[\n            'temporal_attention'] == 'True' else False\n\n        # Initialize unet\n        self.sd_model = UNetSD(\n            in_dim=cfg['unet_in_dim'],\n            dim=cfg['unet_dim'],\n            y_dim=cfg['unet_y_dim'],\n            context_dim=cfg['unet_context_dim'],\n            out_dim=cfg['unet_out_dim'],\n            dim_mult=cfg['unet_dim_mult'],\n            num_heads=cfg['unet_num_heads'],\n            head_dim=cfg['unet_head_dim'],\n            num_res_blocks=cfg['unet_res_blocks'],\n            attn_scales=cfg['unet_attn_scales'],\n            dropout=cfg['unet_dropout'],\n            parameterization=cfg['mean_type'],\n            temporal_attention=cfg['temporal_attention'])\n        self.sd_model.load_state_dict(\n            torch.load(\n                osp.join(self.model_dir, self.config.model[\"model_args\"][\"ckpt_unet\"]),\n                map_location='cpu' if devices.has_mps() or torch.cuda.is_available() == False else None, # default to cpu when macos, else default behaviour -- TheSloppiestOfJoes: Added a check if CUDA is available, else use CPU\n            ),\n            strict=True,\n        )\n        self.sd_model.eval()\n        if not devices.has_mps() or torch.cuda.is_available() == True:\n            self.sd_model.half()\n        \n        # Initialize diffusion\n        betas = beta_schedule(\n            'linear_sd',\n            cfg['num_timesteps'],\n            init_beta=0.00085,\n            last_beta=0.0120)\n        \n        self.sd_model.register_schedule(given_betas=betas.numpy())\n        self.diffusion = Txt2VideoSampler(self.sd_model, shared.device, betas=betas)\n        \n        # Initialize autoencoder\n        ddconfig = {\n            'double_z': True,\n            'z_channels': 4,\n            'resolution': 256,\n            'in_channels': 3,\n            'out_ch': 3,\n            'ch': 128,\n            'ch_mult': [1, 2, 4, 4],\n            'num_res_blocks': 2,\n            'attn_resolutions': [],\n            'dropout': 0.0\n        }\n        self.autoencoder = AutoencoderKL(\n            ddconfig, 4,\n            osp.join(self.model_dir, self.config.model[\"model_args\"][\"ckpt_autoencoder\"]))\n        if self.keep_in_vram != \"All\":\n            self.autoencoder.to('cpu')\n        self.autoencoder.eval()\n\n        # Initialize Open clip\n        self.clip_encoder = FrozenOpenCLIPEmbedder(\n            version=osp.join(self.model_dir,\n                             self.config.model[\"model_args\"][\"ckpt_clip\"]),\n                             device='cpu',\n            layer='penultimate')\n\n        if self.keep_in_vram != \"All\":\n            self.clip_encoder.model.to('cpu')\n            self.clip_encoder.to(\"cpu\")\n        self.noise_gen = torch.Generator(device='cpu')\n\n    def compute_latents(self, vd_out, cpu_vae='GPU (half precision)', device=torch.device('cuda')):\n        self.device = device\n        with torch.no_grad():\n            bs_vd, c, max_frames, height, width = vd_out.shape\n            scale_factor = 0.18215\n            vd_out_scaled = vd_out\n\n            if 'CPU' in cpu_vae:\n                print(\"STARTING VAE ON CPU\")\n                self.autoencoder.to(\"cpu\")\n                vd_out_scaled = vd_out_scaled.cpu()\n            else:\n                print(\"STARTING VAE ON GPU\")\n                self.autoencoder.to(self.device)\n                if 'half precision' in cpu_vae:\n                    self.autoencoder.half()\n                    print(\"VAE HALVED\")\n                    vd_out_scaled = vd_out_scaled.half()\n\n            vd_out_scaled = rearrange(\n                vd_out_scaled, 'b c f h w -> (b f) c h w')\n\n            # Split the tensor into chunks along the first dimension\n            chunk_size = 1\n            chunks = vd_out_scaled.chunk(vd_out_scaled.size(0) // chunk_size)\n\n            latents_chunks = []\n            for chunk in chunks:\n                if 'CPU' in cpu_vae:\n                    ch = chunk.cpu().float()\n                else:\n                    ch = chunk.to(self.device).float()\n                    if 'half precision' in cpu_vae:\n                        ch = ch.half()\n\n                latents_chunk = self.autoencoder.encode(ch)\n                latents_chunk = torch.tensor(\n                    latents_chunk.mean).cpu() * scale_factor\n                # latents_chunks.append(latents_chunk.cpu())\n                latents_chunks.append(latents_chunk)\n\n            # Concatenate the latents chunks back into a single tensor\n            latents = torch.cat(latents_chunks, dim=0)\n            latents = rearrange(latents, '(b f) c h w -> b c f h w', b=bs_vd)\n\n        out = latents.type(torch.float32).cpu()\n        return out\n\n    # @torch.compile()\n    def infer(\n        self, \n        prompt, \n        n_prompt, \n        steps, \n        frames, \n        seed, \n        scale, \n        width=256, \n        height=256, \n        eta=0.0, \n        cpu_vae='GPU (half precision)', \n        device=torch.device('cpu'), \n        latents=None, \n        skip_steps=0,\n        strength=0,\n        mask=None, \n        is_vid2vid=False,\n        sampler=available_samplers[0].name\n    ):\n        vars = locals()\n        vars.pop('self')\n        vars.pop('latents')\n        vars.pop('mask')\n        print('Making a video with the following parameters:')\n\n        seed = seed if seed!=-1 else random.randint(0, 2**32 - 1)\n        vars['seed'] = seed\n        print(vars)\n        r\"\"\"\n        The entry function of text to image synthesis task.\n        1. Using diffusion model to generate the video's latent representation.\n        2. Using vqgan model (autoencoder) to decode the video's latent representation to visual space.\n\n        Args:\n            prompt (str, optional): A string describing the scene to generate. Defaults to \"A bunny in the forest\".\n            n_prompt (Optional[str], optional): An additional prompt for generating the scene. Defaults to \"\".\n            steps (int, optional): The number of steps to run the diffusion model. Defaults to 50.\n            frames (int, optional): The number of frames in the generated video. Defaults to 15.\n            scale (float, optional): The scaling factor for the generated video. Defaults to 12.5.\n            width (int, optional): The width of the generated video. Defaults to 256.\n            height (int, optional): The height of the generated video. Defaults to 256.\n            eta (float, optional): A hyperparameter related to the diffusion model's noise schedule. Defaults to 0.0.\n            cpu_vae (bool, optional): If True, the VQGAN model will run on the CPU. Defaults to 'GPU (half precision)'.\n            latents (Optional[Tensor], optional): An optional latent tensor to use as input for the VQGAN model. Defaults to None.\n            strength (Optional[float], optional): A hyperparameter to control the strength of the generated video when using input latent. Defaults to None.\n\n        Returns:\n            A generated video (as list of np.arrays).\n        \"\"\"\n\n        self.device = device\n        self.clip_encoder.to(self.device)\n        self.clip_encoder.device = self.device\n        steps = steps - skip_steps\n        c, uc = self.preprocess(prompt, n_prompt, steps)\n        if self.keep_in_vram != \"All\":\n            self.clip_encoder.to(\"cpu\")\n        torch_gc()\n\n        mask=mask.half() if 'half precision' in cpu_vae and mask is not None else mask\n        latents=latents.half() if 'half precision' in cpu_vae and latents is not None else latents\n\n        # synthesis\n        strength = None if (strength == 0.0 and not is_vid2vid) else strength\n        with torch.no_grad():\n            num_sample = 1\n            channels = 4\n            max_frames= frames\n            latents, noise, shape = self.diffusion.get_noise(\n                num_sample, \n                channels, \n                max_frames, \n                height, \n                width, \n                seed=seed, \n                latents=latents\n            )\n            with amp.autocast(enabled=True):\n                self.sd_model.to(self.device)\n                self.diffusion.get_sampler(sampler, return_sampler=False)\n            \n                x0 = self.diffusion.sample_loop(\n                    steps=steps,\n                    strength=strength,\n                    eta=eta,\n                    conditioning=c,\n                    unconditional_conditioning=uc,\n                    batch_size=num_sample,\n                    guidance_scale=scale,\n                    latents=latents,\n                    shape=shape,\n                    noise=noise,\n                    is_vid2vid=is_vid2vid,\n                    sampler_name=sampler,\n                    mask=mask\n                )\n\n                self.last_tensor = x0\n                self.last_tensor.cpu()\n                if self.keep_in_vram == \"None\":\n                    self.sd_model.to(\"cpu\")\n                torch_gc()\n                scale_factor = 0.18215\n                bs_vd = x0.shape[0]\n                if 'CPU' in cpu_vae:\n                    x0 = x0.cpu()\n                    print(\"DECODING FRAMES\")\n                    print(x0.shape)\n                    # self.autoencoder.to(self.device)\n                    x0.float()\n                    # Split the tensor into chunks along the first dimension\n                    chunk_size = 1\n                    chunks = torch.chunk(x0, chunks=max_frames, dim=2)\n                    # Apply the autoencoder to each chunk\n                    output_chunks = []\n                    if self.keep_in_vram != \"All\":\n                        self.autoencoder.to(\"cpu\")\n                    print(\"STARTING VAE ON CPU\")\n                    x = 0\n                    for chunk in chunks:\n                        ch = chunk.cpu().float()\n                        ch = 1. / scale_factor * ch\n                        ch = rearrange(ch, 'b c f h w -> (b f) c h w')\n                        # print(ch)\n                        chunk = None\n                        del chunk\n                        output_chunk = self.autoencoder.decode(ch)\n                        output_chunk.cpu()\n                        output_chunks.append(output_chunk)\n                        x += 1\n                else:\n                    chunk_size = 1\n                    chunks = torch.chunk(x0, chunks=max_frames, dim=2)\n                    x0 = x0.cpu()\n                    del x0\n\n                    print(\n                        f\"STARTING VAE ON GPU. {len(chunks)} CHUNKS TO PROCESS\")\n                    self.autoencoder.to(self.device)\n                    if 'half precision' in cpu_vae:\n                        self.autoencoder.half()\n                        print(f\"VAE HALVED\")\n                    print(\"DECODING FRAMES\")\n\n                    # Split the tensor into chunks along the first dimension\n                    # Apply the autoencoder to each chunk\n                    output_chunks = []\n                    torch_gc()\n                    x = 0\n                    for chunk in chunks:\n                        chunk = 1. / scale_factor * chunk\n\n                        chunk = rearrange(chunk, 'b c f h w -> (b f) c h w')\n                        output_chunk = self.autoencoder.decode(chunk)\n                        cpu_chunk = output_chunk.cpu()\n                        del output_chunk\n                        output_chunks.append(cpu_chunk)\n                        x += 1\n                print(\"VAE FINISHED\")\n                torch_gc()\n                # Concatenate the output chunks back into a single tensor\n                vd_out = torch.cat(output_chunks, dim=0)\n                # video_data = self.autoencoder.decode(video_data)\n                print(vd_out.shape)\n                vd_out = rearrange(\n                    vd_out, '(b f) c h w -> b c f h w', b=bs_vd)\n        vd_out = vd_out.type(torch.float32).cpu()\n\n        video_path = self.postprocess_video(vd_out)\n        if self.keep_in_vram == \"None\":\n            self.sd_model.to(\"cpu\")\n        if self.keep_in_vram != \"All\":\n            self.clip_encoder.to(\"cpu\")\n            self.autoencoder.to(\"cpu\")\n            self.autoencoder.encoder.to(\"cpu\")\n            self.autoencoder.decoder.to(\"cpu\")\n\n        # self.autoencoder = None\n        # del self.autoencoder\n        del vd_out\n        del latents\n        x0 = None\n        del x0\n        video_data = None\n        del video_data\n        torch_gc()\n        last_tensor = self.last_tensor\n        return video_path, last_tensor\n\n    def cleanup(self):\n        pass\n\n    def preprocess(self, prompt, n_prompt, steps, offload=True):\n        cached_uc = [None, None]\n        cached_c = [None, None]\n\n        def get_conds_with_caching(function, model, required_prompts, steps, cache):\n            if cache[0] is not None and (required_prompts, steps) == cache[0]:\n                return cache[1]\n\n            with devices.autocast():\n                cache[1] = function(model, required_prompts, steps)\n\n            cache[0] = (required_prompts, steps)\n            return cache[1]\n\n        self.clip_encoder.to(self.device) \n        self.clip_encoder.device = self.device       \n        uc = get_conds_with_caching(prompt_parser.get_learned_conditioning, self.clip_encoder, [n_prompt], steps, cached_uc)\n        c = get_conds_with_caching(prompt_parser.get_learned_conditioning, self.clip_encoder, [prompt], steps, cached_c)\n        if offload:\n            if self.keep_in_vram != \"All\":\n                self.clip_encoder.to('cpu')\n        return c, uc\n\n    def postprocess_video(self, video_data):\n        video = tensor2vid(video_data)\n        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S%f')\n        filename = f\"output/mp4s/{timestamp}.mp4\"\n\n        output_video_path = filename\n        if output_video_path is None:\n            output_video_path = tempfile.NamedTemporaryFile(suffix='.mp4').name\n\n        print(output_video_path)\n\n        \"\"\"fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        h, w, c = video[0].shape\n        video_writer = cv2.VideoWriter(\n            output_video_path, fourcc, fps=8, frameSize=(w, h))\"\"\"\n        return_samples = []\n        for i in range(len(video)):\n            img = cv2.cvtColor(video[i], cv2.COLOR_RGB2BGR)\n            # video_writer.write(img)\n            return_samples.append(img)\n        del video\n        del video_data\n        return return_samples\n\n    def forward(self, *args, **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Run the forward pass for a model.\n\n        Returns:\n            Dict[str, Any]: output from the model forward pass\n        \"\"\"\n        pass", "\n\ndef tensor2vid(video, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n    mean = torch.tensor(\n        mean, device=video.device).reshape(1, -1, 1, 1, 1)  # ncfhw\n    std = torch.tensor(\n        std, device=video.device).reshape(1, -1, 1, 1, 1)  # ncfhw\n    video = video.mul_(std).add_(mean)  # unnormalize back to [0,1]\n    del mean\n    del std\n    video.clamp_(0, 1)\n    images = rearrange(video, 'i c f h w -> f h (i w) c')\n    images = images.unbind(dim=0)\n    images = [(image.numpy() * 255).astype('uint8')\n              for image in images]  # f h w c\n    return images", "\n"]}
{"filename": ".github/scripts/issue_checker.py", "chunked_list": ["import os\nimport re\nfrom github import Github\n\n# Get GitHub token from environment variables\ntoken = os.environ['GITHUB_TOKEN']\ng = Github(token)\n\n# Get the current repository\nprint(f\"Repo is {os.environ['GITHUB_REPOSITORY']}\")", "# Get the current repository\nprint(f\"Repo is {os.environ['GITHUB_REPOSITORY']}\")\nrepo = g.get_repo(os.environ['GITHUB_REPOSITORY'])\n\n# Get the issue number from the event payload\n#issue_number = int(os.environ['ISSUE_NUMBER'])\n\ndetox = True\n\nfor issue in repo.get_issues():\n    print(f\"Processing issue \u2116{issue.number}\")\n    if issue.pull_request:\n        continue\n\n    if detox:\n        continue\n\n    # Get the issue object\n    #issue = repo.get_issue(issue_number)\n\n    # Define the keywords to search for in the issue\n    keywords = ['Python', 'Commit hash', 'Launching Web UI with arguments', 'text2video']\n\n    # Check if ALL of the keywords are present in the issue\n    def check_keywords(issue_body, keywords):\n        for keyword in keywords:\n            if not re.search(r'\\b' + re.escape(keyword) + r'\\b', issue_body, re.IGNORECASE):\n                return False\n        return True\n\n    # Check if the issue title has at least a specified number of words\n    def check_title_word_count(issue_title, min_word_count):\n        words = issue_title.replace(\"/\", \" \").replace(\"\\\\\\\\\", \" \").split()\n        return len(words) >= min_word_count\n\n    # Check if the issue title is concise\n    def check_title_concise(issue_title, max_word_count):\n        words = issue_title.replace(\"/\", \" \").replace(\"\\\\\\\\\", \" \").split()\n        return len(words) <= max_word_count\n\n    # Check if the commit ID is in the correct hash form\n    def check_commit_id_format(issue_body):\n        match = re.search(r'webui commit id - ([a-fA-F0-9]+|\\[[a-fA-F0-9]+\\])', issue_body)\n        if not match:\n            return False\n        webui_commit_id = match.group(1)\n        webui_commit_id = webui_commit_id.replace(\"[\", \"\").replace(\"]\", \"\")\n        if not (7 <= len(webui_commit_id) <= 40):\n            return False\n        match = re.search(r'txt2vid commit id - ([a-fA-F0-9]+|\\[[a-fA-F0-9]+\\])', issue_body)\n        if match:\n            return False\n        t2v_commit_id = match.group(1)\n        t2v_commit_id = t2v_commit_id.replace(\"[\", \"\").replace(\"]\", \"\")\n        if not (7 <= len(t2v_commit_id) <= 40):\n            return False\n        return True\n\n    # Only if a bug report\n    if '[Bug]' in issue.title and not '[Feature Request]' in issue.title and not 'Repos for Training and Finetuning' in issue.title:\n        print('The issue is eligible')\n        # Initialize an empty list to store error messages\n        error_messages = []\n\n        # Check for each condition and add the corresponding error message if the condition is not met\n        if not check_keywords(issue.body, keywords):\n            error_messages.append(\"Include **THE FULL LOG FROM THE START OF THE WEBUI** in the issue description.\")\n\n        if not check_title_word_count(issue.title, 3):\n            error_messages.append(\"Make sure the issue title has at least 3 words.\")\n\n        if not check_title_concise(issue.title, 13):\n            error_messages.append(\"The issue title should be concise and contain no more than 13 words.\")\n\n        # if not check_commit_id_format(issue.body):\n            # error_messages.append(\"Provide a valid commit ID in the format 'commit id - [commit_hash]' **both** for the WebUI and the Extension.\")\n            \n        # If there are any error messages, close the issue and send a comment with the error messages\n        if error_messages:\n            print('Invalid issue, closing')\n            # Add the \"not planned\" label to the issue\n            not_planned_label = repo.get_label(\"wrong format\")\n            issue.add_to_labels(not_planned_label)\n            \n            # Close the issue\n            issue.edit(state='closed')\n            \n            # Generate the comment by concatenating the error messages\n            comment = \"This issue has been closed due to incorrect formatting. Please address the following mistakes and reopen the issue:\\n\\n\"\n            comment += \"\\n\".join(f\"- {error_message}\" for error_message in error_messages)\n\n            # Add the comment to the issue\n            issue.create_comment(comment)\n        elif repo.get_label(\"wrong format\") in issue.labels:\n            print('Issue is fine')\n            issue.edit(state='open')\n            issue.delete_labels()\n            bug_label = repo.get_label(\"bug\")\n            issue.add_to_labels(bug_label)\n            comment = \"Thanks for addressing your formatting mistakes. The issue has been reopened now.\"\n            issue.create_comment(comment)", "\nfor issue in repo.get_issues():\n    print(f\"Processing issue \u2116{issue.number}\")\n    if issue.pull_request:\n        continue\n\n    if detox:\n        continue\n\n    # Get the issue object\n    #issue = repo.get_issue(issue_number)\n\n    # Define the keywords to search for in the issue\n    keywords = ['Python', 'Commit hash', 'Launching Web UI with arguments', 'text2video']\n\n    # Check if ALL of the keywords are present in the issue\n    def check_keywords(issue_body, keywords):\n        for keyword in keywords:\n            if not re.search(r'\\b' + re.escape(keyword) + r'\\b', issue_body, re.IGNORECASE):\n                return False\n        return True\n\n    # Check if the issue title has at least a specified number of words\n    def check_title_word_count(issue_title, min_word_count):\n        words = issue_title.replace(\"/\", \" \").replace(\"\\\\\\\\\", \" \").split()\n        return len(words) >= min_word_count\n\n    # Check if the issue title is concise\n    def check_title_concise(issue_title, max_word_count):\n        words = issue_title.replace(\"/\", \" \").replace(\"\\\\\\\\\", \" \").split()\n        return len(words) <= max_word_count\n\n    # Check if the commit ID is in the correct hash form\n    def check_commit_id_format(issue_body):\n        match = re.search(r'webui commit id - ([a-fA-F0-9]+|\\[[a-fA-F0-9]+\\])', issue_body)\n        if not match:\n            return False\n        webui_commit_id = match.group(1)\n        webui_commit_id = webui_commit_id.replace(\"[\", \"\").replace(\"]\", \"\")\n        if not (7 <= len(webui_commit_id) <= 40):\n            return False\n        match = re.search(r'txt2vid commit id - ([a-fA-F0-9]+|\\[[a-fA-F0-9]+\\])', issue_body)\n        if match:\n            return False\n        t2v_commit_id = match.group(1)\n        t2v_commit_id = t2v_commit_id.replace(\"[\", \"\").replace(\"]\", \"\")\n        if not (7 <= len(t2v_commit_id) <= 40):\n            return False\n        return True\n\n    # Only if a bug report\n    if '[Bug]' in issue.title and not '[Feature Request]' in issue.title and not 'Repos for Training and Finetuning' in issue.title:\n        print('The issue is eligible')\n        # Initialize an empty list to store error messages\n        error_messages = []\n\n        # Check for each condition and add the corresponding error message if the condition is not met\n        if not check_keywords(issue.body, keywords):\n            error_messages.append(\"Include **THE FULL LOG FROM THE START OF THE WEBUI** in the issue description.\")\n\n        if not check_title_word_count(issue.title, 3):\n            error_messages.append(\"Make sure the issue title has at least 3 words.\")\n\n        if not check_title_concise(issue.title, 13):\n            error_messages.append(\"The issue title should be concise and contain no more than 13 words.\")\n\n        # if not check_commit_id_format(issue.body):\n            # error_messages.append(\"Provide a valid commit ID in the format 'commit id - [commit_hash]' **both** for the WebUI and the Extension.\")\n            \n        # If there are any error messages, close the issue and send a comment with the error messages\n        if error_messages:\n            print('Invalid issue, closing')\n            # Add the \"not planned\" label to the issue\n            not_planned_label = repo.get_label(\"wrong format\")\n            issue.add_to_labels(not_planned_label)\n            \n            # Close the issue\n            issue.edit(state='closed')\n            \n            # Generate the comment by concatenating the error messages\n            comment = \"This issue has been closed due to incorrect formatting. Please address the following mistakes and reopen the issue:\\n\\n\"\n            comment += \"\\n\".join(f\"- {error_message}\" for error_message in error_messages)\n\n            # Add the comment to the issue\n            issue.create_comment(comment)\n        elif repo.get_label(\"wrong format\") in issue.labels:\n            print('Issue is fine')\n            issue.edit(state='open')\n            issue.delete_labels()\n            bug_label = repo.get_label(\"bug\")\n            issue.add_to_labels(bug_label)\n            comment = \"Thanks for addressing your formatting mistakes. The issue has been reopened now.\"\n            issue.create_comment(comment)", ""]}
