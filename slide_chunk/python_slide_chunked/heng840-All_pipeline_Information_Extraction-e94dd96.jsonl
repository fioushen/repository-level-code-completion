{"filename": "demo.py", "chunked_list": ["import os\n\n# Disable Tokenizers parallelism\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport pandas\nimport gradio as gr\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    AutoModel,", "    AutoModelForTokenClassification,\n    AutoModel,\n    AutoModelForSequenceClassification,\n    pipeline\n)\n\nimport spacy\n\nnlp_trigger = spacy.load('en_core_web_sm')\n# Define a set of auxiliary verbs", "nlp_trigger = spacy.load('en_core_web_sm')\n# Define a set of auxiliary verbs\naux_verbs = {\"be\", \"am\", \"is\", \"are\", \"was\", \"were\", \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"can\",\n             \"could\", \"will\", \"would\", \"shall\", \"should\", \"may\", \"might\", \"must\"}\n\n# load models and tokenizers\n# tokenizer_ie = AutoTokenizer.from_pretrained(\"IDEA-CCNL/Erlangshen-BERT-120M-IE-Chinese\")\n# model_ie = AutoModel.from_pretrained(\"IDEA-CCNL/Erlangshen-BERT-120M-IE-Chinese\")\n# nlp_ie = pipeline(\"ner\", model=model_ie, tokenizer=tokenizer_ie)\n", "# nlp_ie = pipeline(\"ner\", model=model_ie, tokenizer=tokenizer_ie)\n\ntokenizer_ner = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\nmodel_ner = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n\ntokenizer_trigger = AutoTokenizer.from_pretrained(\"lfcc/bert-portuguese-event-trigger\")\nmodel_trigger = AutoModelForTokenClassification.from_pretrained(\"lfcc/bert-portuguese-event-trigger\")\n\ntokenizer_causal = AutoTokenizer.from_pretrained(\"noahjadallah/cause-effect-detection\")\nmodel_causal = AutoModelForTokenClassification.from_pretrained(\"noahjadallah/cause-effect-detection\")", "tokenizer_causal = AutoTokenizer.from_pretrained(\"noahjadallah/cause-effect-detection\")\nmodel_causal = AutoModelForTokenClassification.from_pretrained(\"noahjadallah/cause-effect-detection\")\n\ntokenizer_factual = AutoTokenizer.from_pretrained(\"amandakonet/climatebert-fact-checking\")\nmodel_factual = AutoModelForSequenceClassification.from_pretrained(\"amandakonet/climatebert-fact-checking\")\n\ntokenizer_event = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\nmodel_event = AutoModel.from_pretrained(\"facebook/bart-large\")\n\n", "\n\n# define predict function to perform information extraction\ndef predict(sentence):\n    # extract triggers\n    doc = nlp_trigger(sentence)\n    triggers = {}\n    for token in doc:\n        if token.dep_ == \"ROOT\" or (token.pos_ == \"VERB\" and token.lemma_ not in aux_verbs):\n            triggers = token.text\n            break\n    # extract triggers\n    # trigger_nlp = pipeline(\"ner\", model=model_trigger, tokenizer=tokenizer_trigger)\n    # triggers = trigger_nlp(sentence)\n\n    argument_nlp = pipeline('ner', model=model_ner, tokenizer=tokenizer_ner)\n    arguments = argument_nlp(sentence)\n    arguments_new = []\n    for arg in arguments:\n        arguments_new.append(\n            {'entity': arg['entity'], 'word': arg['word']}\n        )\n    # extract causal relations\n    causal_nlp = pipeline(\"ner\", model=model_causal, tokenizer=tokenizer_causal)\n    causal_relations = causal_nlp(sentence)\n\n    # extract factual information\n    factual_nlp = pipeline(\"text-classification\", model=model_factual, tokenizer=tokenizer_factual)\n    factual_info = factual_nlp(sentence)\n\n    # # extract events\n    # event_nlp = pipeline(\"text2text-generation\", model=model_event, tokenizer=tokenizer_event)\n    # event_input = \"\"\n    # for trigger in triggers:\n    #     event_input += trigger['word'] + \": \"\n    #     event_input += sentence[trigger['start']:trigger['end']] + \" \"\n    #     for entity in named_entities:\n    #         if entity['start'] >= trigger['start'] and entity['end'] <= trigger['end']:\n    #             event_input += entity['word'] + \": \" + entity['entity_group'] + \" \"\n    #     event_input += \"\\n\"\n    # events = event_nlp(event_input, max_length=1024, do_sample=False)\n    # events = [e['generated_text'].strip() for e in events]\n\n    # combine results\n    causal_relations_new = []\n    for cau in causal_relations:\n        if cau['entity'] != 'OTHER':\n            causal_relations_new.append({'entity': cau['entity'], 'word': cau['word']})\n    fact_label_mapping = ['entailment', 'contradiction', 'neutral']\n    factual_new = []\n    for fact in factual_info:\n        if fact['label'] == 'LABEL_0':\n            factual_new.append(fact_label_mapping[0])\n        elif fact['label'] == 'LABEL_1':\n            factual_new.append(fact_label_mapping[1])\n        else:\n            factual_new.append(fact_label_mapping[2])\n\n    results = {\n        # 'entities': named_entities,\n        'triggers': triggers,\n        'arguments': arguments_new,\n        'causal': causal_relations_new,\n        'factual': factual_new,\n        # 'events': events\n    }\n    results = str('triggers:' + str(triggers) + '\\n\\n' + 'arguments:' + str(arguments_new) + '\\n\\n' + 'causal:' + str(\n        causal_relations_new) + '\\n\\n' + 'factual:' + str(factual_new))\n    # print(results)\n    # results_pd = pandas.json_normalize(results)\n    # return results_pd\n    return results", "\n\nsentence = 'Bob, I think that the reason everybody in the south -- you know, first of all, we were -- when Franklin ' \\\n           'Roosevelt was elected president, ' \\\n           'we had been living what we thought was still a conquered nation after the Civil War'\nx = predict(sentence)\nprint(x)\nwith gr.Blocks() as demo:\n    name = gr.Textbox(label=\"Enter a sentence or a document\")\n    output = gr.Textbox(label=\"Output Box\")\n    greet_btn = gr.Button(\"Extract\")\n    greet_btn.click(fn=predict, inputs=name, outputs=output)", "\ndemo.launch(share=True)\n\n# fixme done \u603b\u662f\u51fa\u73b0numpy.float\u7684\u539f\u56e0\u5728score\u4e0a\u3002\n"]}
{"filename": "demostrations/demo_ee.py", "chunked_list": ["import nltk\nimport numpy as np\nimport torch\nimport gradio as gr\n# from consts import test_event1, test_event2,\nfrom torch.utils import data\n\nNONE = 'O'\nPAD = \"[PAD]\"\nUNK = \"[UNK]\"", "PAD = \"[PAD]\"\nUNK = \"[UNK]\"\n\n# for BERT\nCLS = '[CLS]'\nSEP = '[SEP]'\nmax_length = 400\n\nall_triggers, trigger2idx, idx2trigger = build_vocab(TRIGGERS)\nall_arguments, argument2idx, idx2argument = build_vocab(ARGUMENTS, BIO_tagging=False)", "all_triggers, trigger2idx, idx2trigger = build_vocab(TRIGGERS)\nall_arguments, argument2idx, idx2argument = build_vocab(ARGUMENTS, BIO_tagging=False)\nhparams = get_hparams()\ntokenizer = BertTokenizer.from_pretrained(hparams.model_name, do_lower_case=False, never_split=(PAD, CLS, SEP, UNK))\nclass Data_input(data.Dataset):\n    def __init__(self, document):\n        self.sent_li = []\n        self.entities_li = []\n        self.postags_li = []\n        self.triggers_li = []\n        self.arguments_li = []\n        self.entities_li = []\n\n        words = nltk.word_tokenize(document)\n        split_num = len(words) // max_length\n        for i in range(split_num + 1):\n            if i < split_num:\n                w = words[i * max_length: (i + 1) * max_length]\n            else:\n                w = words[i * max_length:]\n            triggers = [NONE] * len(w)\n            arguments = {\n                'candidates': [],\n                'events': {},\n            }\n            self.sent_li.append([CLS] + w + [SEP])\n            self.triggers_li.append(triggers)\n            self.arguments_li.append(arguments)\n\n    def __len__(self):\n        return len(self.sent_li)\n\n    def __getitem__(self, idx):\n        words = self.sent_li[idx]\n        triggers = self.triggers_li[idx]\n        arguments = self.arguments_li[idx]\n\n        # We give credits only to the first piece.\n        tokens_x = []\n        is_heads = []\n        for w in words:\n            tokens = tokenizer.tokenize(w) if w not in [CLS, SEP] else [w]\n            tokens_xx = tokenizer.convert_tokens_to_ids(tokens)\n\n            if w in [CLS, SEP]:\n                is_head = [0]\n            else:\n                is_head = [1] + [0] * (len(tokens) - 1)\n            tokens_x.extend(tokens_xx)\n            is_heads.extend(is_head)\n\n        triggers_y = [trigger2idx[t] for t in triggers]\n        head_indexes = []\n        for i in range(len(is_heads)):\n            if is_heads[i]:\n                head_indexes.append(i)\n\n        seqlen = len(tokens_x)\n\n        return tokens_x, triggers_y, arguments, seqlen, head_indexes, words, triggers\n\n    def get_samples_weight(self):\n        samples_weight = []\n        for triggers in self.triggers_li:\n            not_none = False\n            for trigger in triggers:\n                if trigger != NONE:\n                    not_none = True\n                    break\n            if not_none:\n                samples_weight.append(5.0)\n            else:\n                samples_weight.append(1.0)\n        return np.array(samples_weight)", "\n\ndef extract_events(user_input):\n    hp = get_hparams()\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = torch.load(f'{hp.model_save_path}/best_model_11.pt')\n\n    if device == 'cuda':\n        model = model.cuda()\n    if hasattr(model, 'module'):\n        model = model.module\n    model.eval()\n\n    data_input = Data_input(user_input)\n    data_iter = data.DataLoader(dataset=data_input,\n                                batch_size=len(data_input.triggers_li),\n                                shuffle=False,\n                                num_workers=4,\n                                collate_fn=pad)\n    words_all = []\n    triggers_all = []\n    triggers_hat_all = []\n    arguments_all = []\n    arguments_hat_all = []\n    # if direct:\n    #     return test_event2\n    with torch.no_grad():\n        for i, batch in enumerate(data_iter):\n            tokens_x_2d, triggers_y_2d, arguments_2d, seqlens_1d, head_indexes_2d, words_2d, triggers_2d = batch\n\n            trigger_logits, triggers_y_2d, trigger_hat_2d, argument_hidden, argument_keys \\\n                = model.predict_triggers(tokens_x_2d=tokens_x_2d, head_indexes_2d=head_indexes_2d,\n                                         triggers_y_2d=triggers_y_2d, arguments_2d=arguments_2d)\n\n        words_all.extend(words_2d)\n        triggers_all.extend(triggers_2d)\n        triggers_hat_all.extend(trigger_hat_2d.cpu().numpy().tolist())\n        arguments_all.extend(arguments_2d)\n\n        if len(argument_keys) > 0:\n            argument_logits, arguments_y_1d, argument_hat_1d, argument_hat_2d = \\\n                model.predict_arguments(argument_hidden, argument_keys, arguments_2d)\n            arguments_hat_all.extend(argument_hat_2d)\n        else:\n            batch_size = len(arguments_2d)\n            argument_hat_2d = [{'events': {}} for _ in range(batch_size)]\n            arguments_hat_all.extend(argument_hat_2d)\n    triggers_pred = []\n    arguments_pred = []\n    events = []\n    for i, (words, triggers, triggers_hat, arguments, arguments_hat) \\\n            in enumerate(zip(words_all, triggers_all, triggers_hat_all, arguments_all, arguments_hat_all)):\n        triggers_hat = triggers_hat[:len(words)]\n        triggers_hat = [idx2trigger[hat] for hat in triggers_hat]\n\n        # [(ith sentence, t_start, t_end, t_type_str)]\n        triggers_pred.extend([(i, *item) for item in find_triggers(triggers_hat)])\n\n        for trigger in arguments_hat['events']:\n            t_start, t_end, t_type_str = trigger\n            for argument in arguments_hat['events'][trigger]:\n                a_start, a_end, a_type_idx = argument\n                arguments_pred.append((i, t_start, t_end, t_type_str, a_start, a_end, a_type_idx))\n        event = {\n            'trigger': triggers_pred,\n            'argument': arguments_pred,\n        }\n        events.append(event)\n\n    return events", "\n\nif __name__ == \"__main__\":\n    input_text = gr.inputs.Textbox(lines=10, label=\"Input Text\")\n    output_text = gr.outputs.Textbox(label=\"Output\")\n\n    gr.Interface(fn=extract_events, inputs=input_text, outputs=output_text, title=\"Event Extraction\",\n                 description=\"Enter some text and the model will extract events.\").launch(share=True)\n", ""]}
{"filename": "demostrations/demo_casaul.py", "chunked_list": ["import torch\nimport transformers\nimport pandas\nimport gradio as gr\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    AutoModel,\n    AutoModelForSequenceClassification,\n    pipeline", "    AutoModelForSequenceClassification,\n    pipeline\n)\n\n# load models and tokenizers\ntokenizer_ie = AutoTokenizer.from_pretrained(\"IDEA-CCNL/Erlangshen-BERT-120M-IE-Chinese\")\nmodel_ie = AutoModel.from_pretrained(\"IDEA-CCNL/Erlangshen-BERT-120M-IE-Chinese\")\nnlp_ie = pipeline(\"ner\", model=model_ie, tokenizer=tokenizer_ie)\n\ntokenizer_ner = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")", "\ntokenizer_ner = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\nmodel_ner = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n\ntokenizer_trigger = AutoTokenizer.from_pretrained(\"lfcc/bert-portuguese-event-trigger\")\nmodel_trigger = AutoModelForTokenClassification.from_pretrained(\"lfcc/bert-portuguese-event-trigger\")\n\ntokenizer_causal = AutoTokenizer.from_pretrained(\"noahjadallah/cause-effect-detection\")\nmodel_causal = AutoModelForTokenClassification.from_pretrained(\"noahjadallah/cause-effect-detection\")\n", "model_causal = AutoModelForTokenClassification.from_pretrained(\"noahjadallah/cause-effect-detection\")\n\ntokenizer_factual = AutoTokenizer.from_pretrained(\"amandakonet/climatebert-fact-checking\")\nmodel_factual = AutoModelForSequenceClassification.from_pretrained(\"amandakonet/climatebert-fact-checking\")\n\ntokenizer_event = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\nmodel_event = AutoModel.from_pretrained(\"facebook/bart-large\")\n\n\n# define predict function to perform information extraction\ndef predict(sentence):\n\n    # extract triggers\n    trigger_nlp = pipeline(\"ner\", model=model_trigger, tokenizer=tokenizer_trigger)\n    triggers = trigger_nlp(sentence)\n\n    argument_nlp = pipeline('ner', model=model_ner, tokenizer=tokenizer_ner)\n    arguments = argument_nlp(sentence)\n    # extract causal relations\n    causal_nlp = pipeline(\"ner\", model=model_causal, tokenizer=tokenizer_causal)\n    causal_relations = causal_nlp(sentence)\n\n    # extract factual information\n    factual_nlp = pipeline(\"text-classification\", model=model_factual, tokenizer=tokenizer_factual)\n    factual_info = factual_nlp(sentence)\n\n    # # extract events\n    # event_nlp = pipeline(\"text2text-generation\", model=model_event, tokenizer=tokenizer_event)\n    # event_input = \"\"\n    # for trigger in triggers:\n    #     event_input += trigger['word'] + \": \"\n    #     event_input += sentence[trigger['start']:trigger['end']] + \" \"\n    #     for entity in named_entities:\n    #         if entity['start'] >= trigger['start'] and entity['end'] <= trigger['end']:\n    #             event_input += entity['word'] + \": \" + entity['entity_group'] + \" \"\n    #     event_input += \"\\n\"\n    # events = event_nlp(event_input, max_length=1024, do_sample=False)\n    # events = [e['generated_text'].strip() for e in events]\n\n    # combine results\n    results = {\n        # 'entities': named_entities,\n        'triggers': triggers,\n        'arguments': arguments,\n        'causal': causal_relations,\n        'factual': factual_info,\n        # 'events': events\n    }\n    print(results)\n    results_pd = pandas.DataFrame.from_dict(results)\n    return results_pd", "\n# define predict function to perform information extraction\ndef predict(sentence):\n\n    # extract triggers\n    trigger_nlp = pipeline(\"ner\", model=model_trigger, tokenizer=tokenizer_trigger)\n    triggers = trigger_nlp(sentence)\n\n    argument_nlp = pipeline('ner', model=model_ner, tokenizer=tokenizer_ner)\n    arguments = argument_nlp(sentence)\n    # extract causal relations\n    causal_nlp = pipeline(\"ner\", model=model_causal, tokenizer=tokenizer_causal)\n    causal_relations = causal_nlp(sentence)\n\n    # extract factual information\n    factual_nlp = pipeline(\"text-classification\", model=model_factual, tokenizer=tokenizer_factual)\n    factual_info = factual_nlp(sentence)\n\n    # # extract events\n    # event_nlp = pipeline(\"text2text-generation\", model=model_event, tokenizer=tokenizer_event)\n    # event_input = \"\"\n    # for trigger in triggers:\n    #     event_input += trigger['word'] + \": \"\n    #     event_input += sentence[trigger['start']:trigger['end']] + \" \"\n    #     for entity in named_entities:\n    #         if entity['start'] >= trigger['start'] and entity['end'] <= trigger['end']:\n    #             event_input += entity['word'] + \": \" + entity['entity_group'] + \" \"\n    #     event_input += \"\\n\"\n    # events = event_nlp(event_input, max_length=1024, do_sample=False)\n    # events = [e['generated_text'].strip() for e in events]\n\n    # combine results\n    results = {\n        # 'entities': named_entities,\n        'triggers': triggers,\n        'arguments': arguments,\n        'causal': causal_relations,\n        'factual': factual_info,\n        # 'events': events\n    }\n    print(results)\n    results_pd = pandas.DataFrame.from_dict(results)\n    return results_pd", "\n\nsentence = 'Bob, I think that the reason everybody in the south -- you know, first of all, we were -- when Franklin Roosevelt was elected president, we had been living what we thought was still a conquered nation after the Civil War'\nx = predict(sentence)\nprint(x)"]}
{"filename": "demostrations/demo_factual.py", "chunked_list": [""]}
{"filename": "demostrations/__init__.py", "chunked_list": [""]}
{"filename": "demostrations/demo_ner.py", "chunked_list": ["import torch\nimport transformers\nimport gradio as gr\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\n# tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n", "# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"My name is Wolfgang and I live in Berlin\"\n#\n# ner_results = nlp(example)\n# print(ner_results)", "# ner_results = nlp(example)\n# print(ner_results)\n\n# \u521b\u5efapredict\u51fd\u6570\u6765\u6267\u884c\u4fe1\u606f\u62bd\u53d6\ndef predict(sentence):\n    ner_results = nlp(sentence)\n    # print(ner_results)\n    return ner_results\n\n# predict(example)", "\n# predict(example)\n# \u521b\u5efa\u8f93\u5165\u7ec4\u4ef6\u548c\u8f93\u51fa\u7ec4\u4ef6\ninput_text = gr.inputs.Textbox(label=\"\u8f93\u5165\u53e5\u5b50\")\noutput_df = gr.outputs.Dataframe(type='array', label=\"\u8f93\u51fa\u7ed3\u679c\")\n\n# \u5c06\u8f93\u5165\u7ec4\u4ef6\u548c\u8f93\u51fa\u7ec4\u4ef6\u4f20\u9012\u7ed9Gradio\u63a5\u53e3\niface = gr.Interface(fn=predict, inputs=input_text, outputs=output_df)\n\n# \u542f\u52a8Gradio\u754c\u9762", "\n# \u542f\u52a8Gradio\u754c\u9762\niface.launch(share=True)\n"]}
{"filename": "src/Event_causality/model.py", "chunked_list": ["import torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\n\nclass BertCausalModel(nn.Module):\n    def __init__(self, y_num):\n        super(BertCausalModel, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.drop = nn.Dropout(0.4)\n        self.fc = nn.Linear(768*3, y_num)\n\n    def forward(self, sentences_s, mask_s, sentences_t, mask_t, event1, event1_mask, event2, event2_mask):\n        \"\"\"\n        :param sentences_s: source\n        :param mask_s:\n        :param sentences_t: target\n        :param mask_t:\n        :param event1:\n        :param event1_mask:\n        :param event2:\n        :param event2_mask:\n        :return:\n        \"\"\"\n        enc_s = self.bert(sentences_s, attention_mask = mask_s)\n        enc_t = self.bert(sentences_t, attention_mask=mask_t)\n\n        hidden_enc_s = enc_s[0]\n        hidden_enc_t = enc_t[0]\n\n        event1 = torch.cat([torch.index_select(a, 0, i).unsqueeze(0) for a, i in zip(hidden_enc_s, event1)], dim=0)\n        event2 = torch.cat([torch.index_select(a, 0, i).unsqueeze(0) for a, i in zip(hidden_enc_t, event2)], dim=0)\n\n        m1 = event1_mask.unsqueeze(-1).expand_as(event1).float()\n        m2 = event2_mask.unsqueeze(-1).expand_as(event2).float()\n\n        event1 = event1 * m1\n        event2 = event2 * m2\n\n        opt1 = torch.sum(event1, dim=1)\n        opt2 = torch.sum(event2, dim=1)\n\n        opt = torch.cat((enc_s[1], opt1, opt2), 1)\n        opt = self.fc(opt)\n        return opt", "\n"]}
{"filename": "src/Event_causality/train.py", "chunked_list": ["import os\nimport pickle\nimport random\nfrom os.path import exists\n\nimport torch\nfrom transformers import AdamW, BertTokenizer, get_linear_schedule_with_warmup\n\nfrom dataset import Dataset\nfrom model import BertCausalModel", "from dataset import Dataset\nfrom model import BertCausalModel\nfrom preprocess import make_data_pickle\nfrom utils import split_train_test, compute_f1, get_hparams\nimport logging\n\n\ndef train(processed_files, use_scheduler=True, batch_size_train=25, batch_size_test=20, epoch_nums=100,\n          saved_models=None, learning_rate=2e-5, output_dir=None):\n    logging.basicConfig(level=logging.INFO, filename=output_dir, filemode='w')\n    data_pickle = f'{processed_files}/data.pickle'\n    debug = False\n    if not exists(data_pickle) or debug:\n        raw_pickle = f'{processed_files}/document_raw.pickle'\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        make_data_pickle(raw_pickle=raw_pickle, data_pickle=data_pickle, tokenizer=tokenizer, debug=True)\n    with open(data_pickle, 'rb') as f:\n        data = pickle.load(f)\n\n    train_set, test_set = split_train_test(data)\n    filter = []\n    for d in train_set:\n        if d[-1] == 'NULL':\n            if random.random() < 0.7:\n                continue\n        filter.append(d)\n    train_set = filter\n\n    train_dataset = Dataset(batch_size=batch_size_train, dataset=train_set)\n    test_dataset = Dataset(batch_size=batch_size_test, dataset=test_set)\n\n    device = torch.device(\"cuda\")\n\n    model = BertCausalModel(y_num=2).to(device)  # binary\n    if not exists(saved_models):\n        os.makedirs(saved_models)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(\n            nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n    scheduler = None\n    if use_scheduler:\n        logging.info('with_scheduler')\n        t_total = int(len(train_set) / 25 / 1 * 60)\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(t_total * 0.1),\n                                                    num_training_steps=t_total)\n    else:\n        logging.info('wo_scheduler')\n    loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n\n    max_f1 = 0\n    saved_model_path = None\n    for epoch in range(epoch_nums):\n        model.train()\n        for batch in train_dataset.get_tqdm(device, True):\n            sentences_s, mask_s, sentences_t, mask_t, event1, event1_mask, event2, event2_mask, data_y = batch\n            opt = model(sentences_s, mask_s, sentences_t, mask_t, event1, event1_mask, event2, event2_mask)\n            loss = loss_fn(opt, data_y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if use_scheduler:\n                scheduler.step()\n\n        model.eval()\n        with torch.no_grad():\n            predicted_all = []\n            gold_all = []\n            for batch in test_dataset.reader(device, False):\n                sentences_s, mask_s, sentences_t, mask_t, event1, event1_mask, event2, event2_mask, data_y = batch\n                opt = model(sentences_s, mask_s, sentences_t, mask_t, event1, event1_mask, event2, event2_mask)\n                predicted = torch.argmax(opt, -1)\n                predicted = list(predicted.cpu().numpy())\n                predicted_all += predicted\n\n                gold = list(data_y.cpu().numpy())\n                gold_all += gold\n            p, r, f = compute_f1(gold_all, predicted_all)\n            if f > max_f1:\n                max_f1 = f\n                if saved_model_path:\n                    os.remove(saved_model_path)\n                saved_model_path = f'{saved_models}/best_model_{epoch}.pt'\n                torch.save(model, saved_model_path)\n                logging.info(f'epoch={epoch}: p={p}, r={r}, f1={f}')", "\n\nif __name__ == '__main__':\n    hparams = get_hparams()\n    train(processed_files=hparams.processed_files, use_scheduler=hparams.use_scheduler,\n          batch_size_train=hparams.batch_size_train, batch_size_test=hparams.batch_size_test,\n          epoch_nums=hparams.epoch_nums, saved_models=hparams.saved_models, learning_rate=hparams.learning_rate,\n          output_dir=hparams.output_dir)\n\n", "\n\n"]}
{"filename": "src/Event_causality/read_document.py", "chunked_list": ["import collections\nimport os\nimport os.path\nimport pickle\n\nfrom lxml import etree\n\n\ndef read_evaluation_file(fn):\n    res = []\n    if not os.path.exists(fn):\n        return res\n    for line in open(fn):\n        fields = line.strip().split('\\t')\n        res.append(fields)\n    return res", "def read_evaluation_file(fn):\n    res = []\n    if not os.path.exists(fn):\n        return res\n    for line in open(fn):\n        fields = line.strip().split('\\t')\n        res.append(fields)\n    return res\n\n\ndef all_tokens(filename):\n    ecb_plus = etree.parse(filename, etree.XMLParser(remove_blank_text=True))\n    root_ecb_plus = ecb_plus.getroot()\n    root_ecb_plus.getchildren()\n\n    all_token = []\n\n    for elem in root_ecb_plus.findall('token'):\n        temp = (elem.get('t_id'), elem.get('sentence'),\n                elem.get('number'), elem.text)\n        all_token.append(temp)\n    return all_token", "\n\ndef all_tokens(filename):\n    ecb_plus = etree.parse(filename, etree.XMLParser(remove_blank_text=True))\n    root_ecb_plus = ecb_plus.getroot()\n    root_ecb_plus.getchildren()\n\n    all_token = []\n\n    for elem in root_ecb_plus.findall('token'):\n        temp = (elem.get('t_id'), elem.get('sentence'),\n                elem.get('number'), elem.text)\n        all_token.append(temp)\n    return all_token", "\n\ndef extract_event_CAT(etreeRoot):\n    \"\"\"\n    :param etreeRoot: ECB+/ESC XML root\n    :return: dictionary with annotated events in ECB+\n    \"\"\"\n\n    event_dict = collections.defaultdict(list)\n\n    for elem in etreeRoot.findall('Markables/'):\n        if elem.tag.startswith(\"ACTION\") or elem.tag.startswith(\"NEG_ACTION\"):\n            for token_id in elem.findall('token_anchor'):  # the event should have at least one token\n                event_mention_id = elem.get('m_id', 'nothing')\n                token_mention_id = token_id.get('t_id', 'nothing')\n                event_dict[event_mention_id].append(token_mention_id)\n    return event_dict", "\n\ndef extract_plotLink(etreeRoot, d):\n    \"\"\"\n    :param etreeRoot: ESC XML root\n    :param d: dictionary with annotated events in ESC (event_dict)\n    :return:\n    \"\"\"\n    plot_dict = collections.defaultdict(list)\n    for elem in etreeRoot.findall('Relations/'):\n        if elem.tag == \"PLOT_LINK\":\n            source_pl = elem.find('source').get('m_id', 'null')\n            target_pl = elem.find('target').get('m_id', 'null')\n            rel_valu = elem.get('relType', 'null')\n\n            if source_pl in d:\n                val1 = \"_\".join(d[source_pl])\n                if target_pl in d:\n                    val2 = \"_\".join(d[target_pl])\n                    plot_dict[(val1, val2)] = rel_valu\n    return plot_dict", "\n\ndef read_file(ecb_start_new, evaluate_file):\n    ecb_star = etree.parse(ecb_start_new, etree.XMLParser(remove_blank_text=True))\n    ecb_star_root = ecb_star.getroot()\n    ecb_star_root.getchildren()\n\n    ecb_star_events = extract_event_CAT(ecb_star_root)\n    ecb_star_events_plotLink = extract_plotLink(ecb_star_root, ecb_star_events)\n    evaluation_data = read_evaluation_file(evaluate_file)\n    return ecb_star_events, ecb_star_events_plotLink, evaluation_data", "\n\ndef make_corpus(ecb_start_topic, evaluation_topic, datadict):\n    if os.path.isdir(ecb_start_topic):\n        if ecb_start_topic[-1] != '/':\n            ecb_start_topic += '/'\n        if evaluation_topic[-1] != '/':\n            evaluation_topic += '/'\n\n        for f in os.listdir(evaluation_topic):\n            if f.endswith('plus.xml'):\n                ecb_file = f\n                star_file = ecb_start_topic + f + \".xml\"\n                evaluate_file = evaluation_topic + f\n                ecb_star_events, ecb_star_events_plotLink, evaluation_data = read_file(star_file, evaluate_file)\n                for key in ecb_star_events:\n                    ecb_star_events[key] = '_'.join(ecb_star_events[key])\n\n                all_token = all_tokens(star_file)\n                datadict[star_file] = [all_token, ecb_star_events, ecb_star_events_plotLink, evaluation_data]", "\n\ndef make_raw_pickle():\n    version = 'v1.0'\n    ECB_star_Topic = '../../DataSets/annotated_data/' + version + '/'\n    EvaluationTopic = '../../DataSets/evaluation_format/full_corpus/' + version + '/event_mentions_extended/'\n\n    data_dict = {}\n    for topic in os.listdir(f'{ECB_star_Topic}'):\n        if os.path.isdir(f'{ECB_star_Topic}' + topic):\n            dir1 = ECB_star_Topic + topic\n            dir2 = EvaluationTopic + topic\n            make_corpus(dir1, dir2, data_dict)\n\n    processed_files = 'processed_files'\n    if not os.path.exists(processed_files):\n        os.makedirs(processed_files)\n    with open(f'{processed_files}/document_raw.pickle', 'wb') as f:\n        pickle.dump(data_dict, f, pickle.HIGHEST_PROTOCOL)", "\n\n"]}
{"filename": "src/Event_causality/dataset.py", "chunked_list": ["import pickle\nimport random\nimport sys\n\nimport torch\nfrom allennlp.common.util import pad_sequence_to_length\nfrom allennlp.nn.util import get_mask_from_sequence_lengths\nfrom tqdm import tqdm\n\n\nclass Dataset(object):\n    def __init__(self, batch_size, dataset):\n        super(Dataset, self).__init__()\n\n        self.dataset = None\n        self.index_length = None\n        self.shuffle_list = None\n        self.batch_size = batch_size\n        self.y_label = {\n            'NULL': 0,\n            'null': 0,\n            'FALLING_ACTION': 1,\n            'PRECONDITION': 1,\n            'Coref': 1\n        }\n\n        self.construct_index(dataset)\n\n    def construct_index(self, dataset):\n        self.dataset = dataset\n        self.index_length = len(dataset)\n        self.shuffle_list = list(range(0, self.index_length))\n\n    def shuffle(self):\n        random.shuffle(self.shuffle_list)\n\n    def get_tqdm(self, device, shuffle=True):\n        return tqdm(self.reader(device, shuffle), mininterval=2, total=self.index_length // self.batch_size,\n                    leave=False, file=sys.stdout, ncols=80)\n\n    def reader(self, device, shuffle):\n        cur_idx = 0\n        while cur_idx < self.index_length:\n            end_index = min(cur_idx + self.batch_size, self.index_length)\n            batch = [self.dataset[self.shuffle_list[index]] for index in range(cur_idx, end_index)]\n            cur_idx = end_index\n            yield self.batchify(batch, device)\n        if shuffle:\n            self.shuffle()\n\n    def batchify(self, batch, device):\n        sentence_len_s = [len(tup[1]) for tup in batch]\n        sentence_len_t = [len(tup[2]) for tup in batch]\n\n        max_sentence_len_s = max(sentence_len_s)\n        max_sentence_len_t = max(sentence_len_t)\n\n        event1_lens = [len(tup[2]) for tup in batch]\n        event2_lens = [len(tup[3]) for tup in batch]\n\n        sentences_s = list()\n        sentences_t = list()\n        event1 = list()\n        event2 = list()\n        data_y = list()\n        for data in batch:\n            sentences_s.append(data[1])\n            sentences_t.append(data[2])\n            event1.append(data[3])\n            event2.append(data[4])\n            y = self.y_label[data[5]] if data[5] in self.y_label else 0\n            data_y.append(y)\n\n        sentences_s = list(map(lambda x: pad_sequence_to_length(x, max_sentence_len_s), sentences_s))\n        sentences_t = list(map(lambda x: pad_sequence_to_length(x, max_sentence_len_t), sentences_t))\n\n        event1 = list(map(lambda x: pad_sequence_to_length(x, 5), event1))\n        event2 = list(map(lambda x: pad_sequence_to_length(x, 5), event2))\n\n        mask_sentences_s = get_mask_from_sequence_lengths(torch.LongTensor(sentence_len_s), max_sentence_len_s)\n        mask_sentences_t = get_mask_from_sequence_lengths(torch.LongTensor(sentence_len_t), max_sentence_len_t)\n\n        mask_even1 = get_mask_from_sequence_lengths(torch.LongTensor(event1_lens), 5)\n        mask_even2 = get_mask_from_sequence_lengths(torch.LongTensor(event2_lens), 5)\n\n        return [torch.LongTensor(sentences_s).to(device), mask_sentences_s.to(device),\n                torch.LongTensor(sentences_t).to(device), mask_sentences_t.to(device),\n                torch.LongTensor(event1).to(device), mask_even1.to(device),\n                torch.LongTensor(event2).to(device), mask_even2.to(device),\n                torch.LongTensor(data_y).to(device)]", "\n\nclass Dataset(object):\n    def __init__(self, batch_size, dataset):\n        super(Dataset, self).__init__()\n\n        self.dataset = None\n        self.index_length = None\n        self.shuffle_list = None\n        self.batch_size = batch_size\n        self.y_label = {\n            'NULL': 0,\n            'null': 0,\n            'FALLING_ACTION': 1,\n            'PRECONDITION': 1,\n            'Coref': 1\n        }\n\n        self.construct_index(dataset)\n\n    def construct_index(self, dataset):\n        self.dataset = dataset\n        self.index_length = len(dataset)\n        self.shuffle_list = list(range(0, self.index_length))\n\n    def shuffle(self):\n        random.shuffle(self.shuffle_list)\n\n    def get_tqdm(self, device, shuffle=True):\n        return tqdm(self.reader(device, shuffle), mininterval=2, total=self.index_length // self.batch_size,\n                    leave=False, file=sys.stdout, ncols=80)\n\n    def reader(self, device, shuffle):\n        cur_idx = 0\n        while cur_idx < self.index_length:\n            end_index = min(cur_idx + self.batch_size, self.index_length)\n            batch = [self.dataset[self.shuffle_list[index]] for index in range(cur_idx, end_index)]\n            cur_idx = end_index\n            yield self.batchify(batch, device)\n        if shuffle:\n            self.shuffle()\n\n    def batchify(self, batch, device):\n        sentence_len_s = [len(tup[1]) for tup in batch]\n        sentence_len_t = [len(tup[2]) for tup in batch]\n\n        max_sentence_len_s = max(sentence_len_s)\n        max_sentence_len_t = max(sentence_len_t)\n\n        event1_lens = [len(tup[2]) for tup in batch]\n        event2_lens = [len(tup[3]) for tup in batch]\n\n        sentences_s = list()\n        sentences_t = list()\n        event1 = list()\n        event2 = list()\n        data_y = list()\n        for data in batch:\n            sentences_s.append(data[1])\n            sentences_t.append(data[2])\n            event1.append(data[3])\n            event2.append(data[4])\n            y = self.y_label[data[5]] if data[5] in self.y_label else 0\n            data_y.append(y)\n\n        sentences_s = list(map(lambda x: pad_sequence_to_length(x, max_sentence_len_s), sentences_s))\n        sentences_t = list(map(lambda x: pad_sequence_to_length(x, max_sentence_len_t), sentences_t))\n\n        event1 = list(map(lambda x: pad_sequence_to_length(x, 5), event1))\n        event2 = list(map(lambda x: pad_sequence_to_length(x, 5), event2))\n\n        mask_sentences_s = get_mask_from_sequence_lengths(torch.LongTensor(sentence_len_s), max_sentence_len_s)\n        mask_sentences_t = get_mask_from_sequence_lengths(torch.LongTensor(sentence_len_t), max_sentence_len_t)\n\n        mask_even1 = get_mask_from_sequence_lengths(torch.LongTensor(event1_lens), 5)\n        mask_even2 = get_mask_from_sequence_lengths(torch.LongTensor(event2_lens), 5)\n\n        return [torch.LongTensor(sentences_s).to(device), mask_sentences_s.to(device),\n                torch.LongTensor(sentences_t).to(device), mask_sentences_t.to(device),\n                torch.LongTensor(event1).to(device), mask_even1.to(device),\n                torch.LongTensor(event2).to(device), mask_even2.to(device),\n                torch.LongTensor(data_y).to(device)]", "\n\nif __name__ == '__main__':\n    with open('processed_files/data.pickle', 'rb') as f:\n        # The protocol version used is detected automatically, so we do not\n        # have to specify it.\n        data = pickle.load(f)\n    dataset = Dataset(10, data[:20])\n    for batch in dataset.reader('cpu', True):\n        sentences_s, \\\n        mask_s, \\\n        sentences_t,\\\n        mask_t, \\\n        event1, \\\n        event1_mask,\\\n        event2, \\\n        event2_mask,\\\n        y = \\\n            batch\n        print(sentences_s[0])\n        print(mask_s[0])\n        print(event1[0])\n        print(event2[0])\n        break", ""]}
{"filename": "src/Event_causality/utils.py", "chunked_list": ["import argparse\n\n\ndef get_hparams():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--processed_files', type=str, default='processed_files')\n    parser.add_argument('--use_scheduler', default=True, action='store_false')\n    parser.add_argument('--batch_size_train', type=int, default=25)\n    parser.add_argument('--batch_size_test', type=int, default=20)\n    parser.add_argument('--epoch_nums', type=int, default=100)\n    parser.add_argument('--saved_models', type=str, default='saved_models/wo_scheduler')\n    parser.add_argument('--learning_rate', type=float, default=2e-5)\n    parser.add_argument('--output_dir', type=str, default='output_with_scheduler.txt')\n    hparams = parser.parse_args()\n    return hparams", "\n\ndef split_train_test(dataset):\n    train_set = []\n    test_set = []\n\n    test_topic = ['1', '3', '4', '5']\n    for data in dataset:\n        t = data[0]\n        if t.split('/')[-2] in test_topic:\n            test_set.append(data)\n        else:\n            train_set.append(data)\n    return train_set, test_set", "\n\ndef compute_f1(gold, predicted):\n    c_predict = 0\n    c_correct = 0\n    c_gold = 0\n\n    for g, p in zip(gold, predicted):\n        if g != 0:\n            c_gold += 1\n        if p != 0:\n            c_predict += 1\n        if g != 0 and p != 0:\n            c_correct += 1\n\n    p = c_correct / (c_predict + 1e-100)\n    r = c_correct / c_gold\n    f = 2 * p * r / (p + r + 1e-100)\n\n    # print('correct', c_correct)\n    # print('predicted', c_predict)\n    # print('golden', c_gold)\n\n    return p, r, f", ""]}
{"filename": "src/Event_causality/eval.py", "chunked_list": ["import pickle\nfrom os.path import exists\n\nimport torch\nfrom transformers import BertTokenizer\n\nfrom dataset import Dataset\nfrom preprocess import make_data_pickle\nfrom utils import split_train_test, compute_f1, get_hparams\n", "from utils import split_train_test, compute_f1, get_hparams\n\n\ndef evaluate(processed_files, batch_size_test=20, saved_models=None):\n    data_pickle = f'{processed_files}/data.pickle'\n    if not exists(data_pickle):\n        raw_pickle = f'{processed_files}/document_raw.pickle'\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        make_data_pickle(raw_pickle=raw_pickle, data_pickle=data_pickle, tokenizer=tokenizer)\n    with open(data_pickle, 'rb') as f:\n        data = pickle.load(f)\n\n    _, test_set = split_train_test(data)\n    test_dataset = Dataset(batch_size=batch_size_test, dataset=test_set)\n    device = torch.device(\"cuda\")\n    model = torch.load(f'{saved_models}/best_model_12.pt')\n    model.eval()\n    with torch.no_grad():\n        predicted_all = []\n        gold_all = []\n        for batch in test_dataset.reader(device, False):\n            sentences_s, mask_s, sentences_t, mask_t, event1, event1_mask, event2, event2_mask, data_y = batch\n            opt = model(sentences_s, mask_s, sentences_t, mask_t, event1, event1_mask, event2, event2_mask)\n            predicted = torch.argmax(opt, -1)\n            predicted = list(predicted.cpu().numpy())\n            predicted_all += predicted\n\n            gold = list(data_y.cpu().numpy())\n            gold_all += gold\n        p, r, f = compute_f1(gold_all, predicted_all)\n        print(p, r, f)", "\n\nif __name__ == '__main__':\n    hparams = get_hparams()\n    evaluate(processed_files=hparams.processed_files, batch_size_test=hparams.batch_size_test,\n             saved_models=hparams.saved_models)\n"]}
{"filename": "src/Event_causality/preprocess.py", "chunked_list": ["import pickle\nfrom os.path import exists\n\nfrom transformers import BertTokenizer\n\nfrom read_document import make_raw_pickle\n\n\ndef get_sentence_number(s, all_token):\n    tid = s.split('_')[0]\n    for token in all_token:\n        if token[0] == tid:\n            return token[1]", "def get_sentence_number(s, all_token):\n    tid = s.split('_')[0]\n    for token in all_token:\n        if token[0] == tid:\n            return token[1]\n\n\ndef nth_sentence(sen_no, all_token):\n    res = []\n    for token in all_token:\n        if token[1] == sen_no:\n            res.append(token[-1])\n    return res", "\n\ndef get_sentence_offset(s, all_token):\n    positions = []\n    for c in s.split('_'):\n        token = all_token[int(c) - 1]\n        positions.append(token[2])\n    return '_'.join(positions)\n\n\ndef make_data_pickle(raw_pickle, data_pickle, tokenizer, debug=True):\n    # make_raw_pickle()\n    if not exists(raw_pickle) or debug:\n        make_raw_pickle()\n    with open(raw_pickle, 'rb') as f:\n        documents = pickle.load(f)\n\n    data_set = []\n    count = 0\n    for doc in documents:\n        [all_token,\n         ecb_star_events,\n         ecb_star_events_plotLink,\n         evaluation_data] \\\n            = documents[doc]\n\n        for event1 in ecb_star_events:\n            for event2 in ecb_star_events:\n                if event1 == event2:  # event ID\n                    continue\n                offset1 = ecb_star_events[event1]\n                offset2 = ecb_star_events[event2]\n\n                rel = 'NULL'\n                for elem in evaluation_data:\n                    e1, e2, value = elem\n                    if e1 == offset1 and e2 == offset2:\n                        rel = value\n                sen_s = get_sentence_number(offset1, all_token)\n                sen_t = get_sentence_number(offset2, all_token)\n\n                if abs(int(sen_s) - int(sen_t)) == 0:  # #\n                    if rel != 'NULL':\n                        count += 1\n                    sentence_s = nth_sentence(sen_s, all_token)\n                    sentence_t = nth_sentence(sen_t, all_token)\n                    sen_offset1 = get_sentence_offset(offset1, all_token)\n                    sen_offset2 = get_sentence_offset(offset2, all_token)\n\n                    span1 = [int(x) for x in sen_offset1.split('_')]\n                    span2 = [int(x) for x in sen_offset2.split('_')]\n\n                    sentence_s = ['[CLS]'] + sentence_s + ['[SEP]']\n                    sentence_t = ['[CLS]'] + sentence_t + ['[SEP]']\n\n                    span1 = list(map(lambda x: x + 1, span1))\n                    span2 = list(map(lambda x: x + 1, span2))\n\n                    sentence_vec_s = []\n                    sentence_vec_t = []\n\n                    span1_vec = []\n                    span2_vec = []\n                    for i, w in enumerate(sentence_s):\n                        tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n                        xx = tokenizer.convert_tokens_to_ids(tokens)\n\n                        if i in span1:\n                            span1_vec.extend(list(range(len(sentence_vec_s), len(sentence_vec_s) + len(xx))))\n\n                        sentence_vec_s.extend(xx)\n\n                    for i, w in enumerate(sentence_t):\n                        tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n                        xx = tokenizer.convert_tokens_to_ids(tokens)\n\n                        if i in span2:\n                            span2_vec.extend(list(range(len(sentence_vec_t), len(sentence_vec_t) + len(xx))))\n\n                        sentence_vec_t.extend(xx)\n\n                    data_set.append([doc, sentence_vec_s, sentence_vec_t, span1_vec, span2_vec, rel])\n\n    # print(len(data_set))\n    # print(data_set[0])\n    # print(count)\n    with open(data_pickle, 'wb') as f:\n        pickle.dump(data_set, f, pickle.HIGHEST_PROTOCOL)", "\n\ndef make_data_pickle(raw_pickle, data_pickle, tokenizer, debug=True):\n    # make_raw_pickle()\n    if not exists(raw_pickle) or debug:\n        make_raw_pickle()\n    with open(raw_pickle, 'rb') as f:\n        documents = pickle.load(f)\n\n    data_set = []\n    count = 0\n    for doc in documents:\n        [all_token,\n         ecb_star_events,\n         ecb_star_events_plotLink,\n         evaluation_data] \\\n            = documents[doc]\n\n        for event1 in ecb_star_events:\n            for event2 in ecb_star_events:\n                if event1 == event2:  # event ID\n                    continue\n                offset1 = ecb_star_events[event1]\n                offset2 = ecb_star_events[event2]\n\n                rel = 'NULL'\n                for elem in evaluation_data:\n                    e1, e2, value = elem\n                    if e1 == offset1 and e2 == offset2:\n                        rel = value\n                sen_s = get_sentence_number(offset1, all_token)\n                sen_t = get_sentence_number(offset2, all_token)\n\n                if abs(int(sen_s) - int(sen_t)) == 0:  # #\n                    if rel != 'NULL':\n                        count += 1\n                    sentence_s = nth_sentence(sen_s, all_token)\n                    sentence_t = nth_sentence(sen_t, all_token)\n                    sen_offset1 = get_sentence_offset(offset1, all_token)\n                    sen_offset2 = get_sentence_offset(offset2, all_token)\n\n                    span1 = [int(x) for x in sen_offset1.split('_')]\n                    span2 = [int(x) for x in sen_offset2.split('_')]\n\n                    sentence_s = ['[CLS]'] + sentence_s + ['[SEP]']\n                    sentence_t = ['[CLS]'] + sentence_t + ['[SEP]']\n\n                    span1 = list(map(lambda x: x + 1, span1))\n                    span2 = list(map(lambda x: x + 1, span2))\n\n                    sentence_vec_s = []\n                    sentence_vec_t = []\n\n                    span1_vec = []\n                    span2_vec = []\n                    for i, w in enumerate(sentence_s):\n                        tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n                        xx = tokenizer.convert_tokens_to_ids(tokens)\n\n                        if i in span1:\n                            span1_vec.extend(list(range(len(sentence_vec_s), len(sentence_vec_s) + len(xx))))\n\n                        sentence_vec_s.extend(xx)\n\n                    for i, w in enumerate(sentence_t):\n                        tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n                        xx = tokenizer.convert_tokens_to_ids(tokens)\n\n                        if i in span2:\n                            span2_vec.extend(list(range(len(sentence_vec_t), len(sentence_vec_t) + len(xx))))\n\n                        sentence_vec_t.extend(xx)\n\n                    data_set.append([doc, sentence_vec_s, sentence_vec_t, span1_vec, span2_vec, rel])\n\n    # print(len(data_set))\n    # print(data_set[0])\n    # print(count)\n    with open(data_pickle, 'wb') as f:\n        pickle.dump(data_set, f, pickle.HIGHEST_PROTOCOL)", "\n\nif __name__ == '__main__':\n    # todo \u53ef\u4ee5\u628aread_document\u6539\u8fc7\u6765\n    processed_files = 'processed_files'\n    raw_pickle0 = f'{processed_files}/document_raw.pickle'\n    data_pickle0 = f'{processed_files}/data.pickle'\n    tokenizer0 = BertTokenizer.from_pretrained('bert-base-uncased')\n    make_data_pickle(raw_pickle=raw_pickle0, data_pickle=data_pickle0, tokenizer=tokenizer0)\n", ""]}
{"filename": "src/Event_factuality/model.py", "chunked_list": ["import torch.nn as nn\nfrom transformers import BertModel, BertConfig\nimport torch\nimport torch.nn.functional as F\n\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_dim, out_dim, acti=True):\n        super(GCNLayer, self).__init__()\n        self.linear = nn.Linear(in_dim, out_dim)\n        if acti:\n            self.acti = nn.ReLU(inplace=True)\n        else:\n            self.acti = None\n\n    def forward(self, F):\n        output = self.linear(F)\n        if not self.acti:\n            return output\n        return self.acti(output)", "\n\nclass UGCN(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim, dropout):\n        super(UGCN, self).__init__()\n        self.gcn_layer1 = GCNLayer(in_dim, hid_dim)\n        self.gcn_layer2 = GCNLayer(hid_dim, out_dim)\n        self.gcn_layer1_var = GCNLayer(in_dim, hid_dim)\n        self.gcn_layer2_var = GCNLayer(hid_dim, out_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.trans_mean = nn.Linear(in_dim, in_dim)\n        self.trans_var = nn.Linear(in_dim, in_dim)\n\n    def forward(self, adj, gcn_inputs):\n        # \u4f20\u5165\u56fe\u7ed3\u6784 adj \u548c\u8f93\u5165\u7279\u5f81 gcn_inputs\n        output_features = []\n        out_vars = []\n        output_features.append(gcn_inputs)\n        out_vars.append(gcn_inputs)\n        adj_list = []\n        adj_var_list = []\n        for i in range(adj.size()[0]):\n            adj_list.append(self.normalize(adj[i].view(adj.size()[1], adj.size()[2])))\n            adj_var_list.append(self.normalize1(adj[i].view(adj.size()[1], adj.size()[2])))\n        adj = torch.cat(adj_list, dim=0)\n        adj_var = torch.cat(adj_var_list, dim=0)\n        adj = adj.type_as(gcn_inputs)\n        adj_var = adj_var.type_as(gcn_inputs)\n        mean_vectors = F.relu(self.trans_mean(gcn_inputs))\n        var_vectors = F.relu(self.trans_var(gcn_inputs))\n        output_features.append(mean_vectors)\n        out_vars.append(var_vectors)\n\n        node_weight = torch.exp(-0.001 * var_vectors)\n        x_mean = mean_vectors.mul(node_weight)\n        x_var = var_vectors.mul(node_weight).mul(node_weight)\n        Ax_mean = adj.bmm(x_mean)\n        Ax_var = adj_var.bmm(x_var)\n        hid_output_mean = self.gcn_layer1(Ax_mean)\n        hid_output_var = self.gcn_layer1_var(Ax_var)\n        output_features.append(hid_output_mean)\n        out_vars.append(hid_output_var)\n\n        node_weight = torch.exp(-0.001 * hid_output_var)\n        x_mean = hid_output_mean.mul(node_weight)\n        x_var = hid_output_var.mul(node_weight).mul(node_weight)\n        Ax_mean = adj.bmm(x_mean)\n        Ax_var = adj_var.bmm(x_var)\n        output_mean = self.gcn_layer2(Ax_mean)\n        output_var = self.gcn_layer2_var(Ax_var)\n        sample_v = torch.randn(1, 1)[0][0]\n        output_mean = output_mean + (torch.sqrt(output_var + 1e-8) * sample_v)\n        output_features.append(output_mean)\n        out_vars.append(output_var)\n        return output_features, out_vars\n\n    def normalize(self, A, symmetric=True):\n        # A = A+I\n        A = A + torch.eye(A.size(0)).cuda()\n        # degree of nodes\n        d = A.sum(1)\n        D = torch.diag(torch.pow(d, -0.5))\n        return D.mm(A).mm(D).unsqueeze(0)\n\n    def normalize1(self, A, symmetric=True):\n        # A = A+I\n        A = A + torch.eye(A.size(0)).cuda()\n        # degree of nodes\n        d = A.sum(1)\n        D = torch.diag(torch.pow(d, -1))\n        return D.mm(A).mm(D).unsqueeze(0)", "\n\nclass GCN_Joint_EFP(nn.Module):\n    def __init__(self, config, y_num):\n        super(GCN_Joint_EFP, self).__init__()\n        self.config = config\n        self.y_num = y_num\n        if config.activation == 'tanh':\n            self.activation = nn.Tanh()\n        elif config.activation == 'relu':\n            self.activation = nn.ReLU()\n        else:\n            assert 1 == 2, \"you should provide activation function.\"\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        # bert-base-uncased for english data, bert-base-chinese for chinese data\n        print('The number of parameters of bert: ',\n              sum(p.numel() for p in self.bert.parameters() if p.requires_grad))\n\n        self.gcn_in_dim = config.bert_hid_size\n        self.gcn_hid_dim = config.gcn_hid_dim\n        self.gcn_out_dim = config.gcn_out_dim\n        self.dropout = config.dropout\n\n        self.ugcn = UGCN(self.gcn_in_dim, self.gcn_hid_dim, self.gcn_out_dim, self.dropout)\n\n        self.bank_size = self.gcn_in_dim + self.gcn_hid_dim + self.gcn_out_dim\n        self.linear_dim = config.linear_dim\n        self.predict = nn.Linear(self.bank_size, self.y_num)\n        # self.trigger_predict = nn.Linear(self.bank_size, self.y_num)\n\n    def forward(self, **params):\n        triggers = params['triggers']\n        trigger_masks = params['trigger_masks']\n        bsz = triggers.size()[0]\n        doc_outputs = self.bert(triggers, attention_mask=trigger_masks)\n        document_cls = doc_outputs[1]\n\n        words = params['words']  # [bsz, seq_len]\n        masks = params['masks']  # [bsz, seq_len]\n        sent_outputs = self.bert(words, attention_mask=masks)  # sentence_cls: [bsz, bert_dim]\n        sentence_embed = sent_outputs[0]\n        sentence_cls = sent_outputs[1]\n\n        sent_idx = params['sent_idx']  # bsz * [trigger_num]\n        trigger_word_idx = params['trigger_word_idx']  # bsz * [trigger_num, seq_len]\n        graphs = params['graphs']\n        assert graphs.size()[0] == bsz, \"batch size inconsistent\"\n\n        split_sizes = params['sent_nums'].tolist()\n        # for i in range(bsz):\n        #     sentence_num = graphs[i].number_of_nodes('node') - 1 - sent_idx[i].shape[0]\n        #     split_sizes.append(sentence_num)\n        feature_list = list(torch.split(sentence_cls, split_sizes, dim=0))  # bsz * [num, bert_dim]\n        sentence_embed_list = list(torch.split(sentence_embed, split_sizes, dim=0))\n\n        sentence_trigger = []\n        trigger_nums = []\n        for i in range(bsz):\n            # extract sentences containing triggers\n            t = sentence_embed_list[i].index_select(0, sent_idx[i])  # [trigger_num, seq_len, bert_dim]\n            # extract trigger embeds\n            trigger_embed = torch.sum(trigger_word_idx[i].unsqueeze(-1) * t, dim=1)  # [trigger_num, bert_dim]\n            # assert trigger_embed.size()[0]==sent_idx[i].size()[-1]\n            trigger_nums.append(trigger_embed.size()[0])\n            fea = torch.cat((feature_list[i], trigger_embed), dim=0)\n            pad = torch.zeros(graphs.size()[1] - 1 - fea.size()[0], fea.size()[-1]).cuda()\n            fea = torch.cat((fea, pad), dim=0).unsqueeze(0)\n            assert fea.size()[1] == graphs.size()[1] - 1\n            sentence_trigger.append(fea)\n        sentence_trigger = torch.cat(sentence_trigger, dim=0)\n        features = torch.cat((document_cls.unsqueeze(1), sentence_trigger), dim=1)\n        assert features.size()[0] == bsz\n        assert features.size()[1] == graphs.size()[1]\n\n        output_features, output_means = self.ugcn(graphs, features)  # [bsz, num_node, dim]\n        output_feature_list = [output_features[0], output_features[2], output_features[3]]\n        output_feature = torch.cat(output_feature_list, dim=-1)\n        document_features = []\n        trigger_features = []\n        for i in range(bsz):\n            document_features.append(output_feature[i:i + 1, 0, :])\n            trigger_start = 1 + split_sizes[i]\n            trigger_end = trigger_start + trigger_nums[i]\n            trigger_features.append(\n                output_feature[i:i + 1, trigger_start:trigger_end, :].view(-1, output_feature.size()[-1]))\n\n        document_feature = torch.cat(document_features, dim=0).view(-1, output_feature.size()[-1])\n        trigger_feature = torch.cat(trigger_features, dim=0).view(-1, output_feature.size()[-1])\n\n        # classification\n        predictions = self.predict(document_feature)\n        # trigger_predictions = self.trigger_predict(trigger_feature)\n        trigger_predictions = trigger_feature\n\n        mean = output_features[1]  # (bsz, node_num, gcn_out_dim)\n        var = output_means[1]\n        KL_divergence = 0.5 * torch.mean(torch.square(mean) + var - torch.log(1e-8 + var) - 1, dim=-1)\n        KL_divergence = torch.mean(KL_divergence)\n        KL_loss = 5e-4 * KL_divergence\n\n        return predictions, trigger_predictions, KL_loss", ""]}
{"filename": "src/Event_factuality/main.py", "chunked_list": ["import torch\nimport torch.utils.data as D\nimport torch.nn.functional as F\nfrom dataset import Data\nfrom config import opt\nfrom model import GCN_Joint_EFP\nfrom transformers import AdamW\nimport torch.nn as nn\nimport os\nimport numpy as np", "import os\nimport numpy as np\nimport xml.etree.ElementTree as ET\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\n\ndef k_fold_split(data_path):\n    train_idx, test_idx = [], []\n    labels = []\n    label2idx = {}\n\n    tree = ET.parse(data_path)\n    root = tree.getroot()\n    for document_set in root:\n        for document in document_set:\n            id = document.attrib['id']\n            if id != 'ED1397':\n                label = document.attrib['document_level_value']\n                if label not in label2idx:\n                    label2idx[label] = len(label2idx)\n                labels.append(label2idx[label])\n    skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n    for train, test in skf.split(np.zeros(len(labels)), labels):\n        train_idx.append(train)\n        test_idx.append(test)\n    return train_idx, test_idx, label2idx", "\n\ndef collate(samples):\n    id, label, trigger, trigger_mask, data, attention, \\\n    sent_idx, trigger_word_idx, trigger_label, sent_num, graph = map(list, zip(*samples))\n\n    batched_ids = tuple(id)\n    batched_labels = torch.tensor(label)\n    batched_triggers = torch.cat(trigger, dim=0)\n    batched_trigger_mask = torch.cat(trigger_mask, dim=0)\n    batched_data = torch.cat(data, dim=0)\n    batched_attention = torch.cat(attention, dim=0)\n    batched_sent_idx = sent_idx\n    batched_trigger_word_idx = trigger_word_idx\n    batched_trigger_labels = torch.cat(trigger_label, dim=0)\n    batched_sent_num = torch.tensor(sent_num)\n    batched_graph = torch.cat(graph, dim=0)\n    return batched_ids, batched_labels, batched_triggers, batched_trigger_mask, \\\n           batched_data, batched_attention, \\\n           batched_sent_idx, batched_trigger_word_idx, batched_trigger_labels, batched_sent_num, \\\n           batched_graph", "\n\ndef get_data(train_idx, test_idx, label2idx):\n    trainset = Data(opt.data_path, opt.saved_path, train_idx, label2idx, is_training=True)\n    train_loader = D.DataLoader(trainset, batch_size=opt.batch_size, shuffle=True, num_workers=10, collate_fn=collate)\n    testset = Data(opt.data_path, opt.saved_path, test_idx, label2idx, is_training=False)\n    test_loader = D.DataLoader(testset, batch_size=opt.batch_size, shuffle=False, num_workers=10, collate_fn=collate)\n    return train_loader, test_loader\n\n\ndef train(model, trainloader, optimizer, opt):\n    model.train()\n    # start_time = time.time()\n\n    loss_list = []\n    for batch_idx, (ids, labels, triggers, trigger_masks, words, masks, sent_idx, trigger_word_idx, trigger_labels,\n                    sent_nums, graphs) in enumerate(trainloader):\n        if opt.gpu:\n            triggers = triggers.cuda()\n            trigger_masks = trigger_masks.cuda()\n            words = words.cuda()\n            masks = masks.cuda()\n            # trigger_word_idx = trigger_word_idx.cuda()\n            graphs = graphs.cuda()\n            labels = labels.cuda()\n            sent_nums = sent_nums.cuda()\n            trigger_labels = trigger_labels.cuda()\n        sent_idx_list = []\n        trigger_word_idx_list = []\n        for i in range(len(sent_idx)):\n            sent_idx_list.append(sent_idx[i].cuda())\n            trigger_word_idx_list.append(trigger_word_idx[i].cuda())\n        optimizer.zero_grad()\n        logit, trigger_logit, kl_loss = model(ids=ids, triggers=triggers, trigger_masks=trigger_masks, words=words,\n                                              masks=masks,\n                                              sent_idx=sent_idx_list, trigger_word_idx=trigger_word_idx_list,\n                                              sent_nums=sent_nums, graphs=graphs)\n        main_loss = nn.functional.cross_entropy(logit, labels)\n        aux_loss = nn.functional.cross_entropy(trigger_logit, trigger_labels)\n        loss = main_loss + kl_loss\n\n        loss.backward()\n        optimizer.step()\n        loss_list.append(loss.item())\n    # print(\"time:%.3f\" % (time.time() - start_time))\n    return np.mean(loss_list)", "\n\ndef train(model, trainloader, optimizer, opt):\n    model.train()\n    # start_time = time.time()\n\n    loss_list = []\n    for batch_idx, (ids, labels, triggers, trigger_masks, words, masks, sent_idx, trigger_word_idx, trigger_labels,\n                    sent_nums, graphs) in enumerate(trainloader):\n        if opt.gpu:\n            triggers = triggers.cuda()\n            trigger_masks = trigger_masks.cuda()\n            words = words.cuda()\n            masks = masks.cuda()\n            # trigger_word_idx = trigger_word_idx.cuda()\n            graphs = graphs.cuda()\n            labels = labels.cuda()\n            sent_nums = sent_nums.cuda()\n            trigger_labels = trigger_labels.cuda()\n        sent_idx_list = []\n        trigger_word_idx_list = []\n        for i in range(len(sent_idx)):\n            sent_idx_list.append(sent_idx[i].cuda())\n            trigger_word_idx_list.append(trigger_word_idx[i].cuda())\n        optimizer.zero_grad()\n        logit, trigger_logit, kl_loss = model(ids=ids, triggers=triggers, trigger_masks=trigger_masks, words=words,\n                                              masks=masks,\n                                              sent_idx=sent_idx_list, trigger_word_idx=trigger_word_idx_list,\n                                              sent_nums=sent_nums, graphs=graphs)\n        main_loss = nn.functional.cross_entropy(logit, labels)\n        aux_loss = nn.functional.cross_entropy(trigger_logit, trigger_labels)\n        loss = main_loss + kl_loss\n\n        loss.backward()\n        optimizer.step()\n        loss_list.append(loss.item())\n    # print(\"time:%.3f\" % (time.time() - start_time))\n    return np.mean(loss_list)", "\n\ndef evaluate(model, test_loader, filepath=None):\n    if filepath is not None:\n        f = open(filepath, 'w')\n    model.eval()\n    total = 0\n    correct = 0\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for batch_idx, (ids, labels, triggers, trigger_masks, words, masks, sent_idx, trigger_word_idx, trigger_labels,\n                        sent_nums, graphs) in enumerate(test_loader):\n            if opt.gpu:\n                triggers = triggers.cuda()\n                trigger_masks = trigger_masks.cuda()\n                words = words.cuda()\n                masks = masks.cuda()\n                # sent_idx = sent_idx.cuda()\n                # trigger_word_idx = trigger_word_idx.cuda()\n                graphs = graphs.cuda()\n                labels = labels.cuda()\n                sent_nums = sent_nums.cuda()\n                trigger_labels = trigger_labels.cuda()\n            sent_idx_list = []\n            trigger_word_idx_list = []\n            for i in range(len(sent_idx)):\n                sent_idx_list.append(sent_idx[i].cuda())\n                trigger_word_idx_list.append(trigger_word_idx[i].cuda())\n            logit, _, _ = model(ids=ids, triggers=triggers, trigger_masks=trigger_masks, words=words, masks=masks,\n                                sent_idx=sent_idx_list, trigger_word_idx=trigger_word_idx_list, sent_nums=sent_nums,\n                                graphs=graphs)\n            _, predicted = torch.max(logit.data, 1)\n            correct += predicted.data.eq(labels.data).cpu().sum()\n            y_true += labels.cpu().data.numpy().tolist()\n            y_pred += predicted.cpu().data.numpy().tolist()\n\n            if filepath is not None:\n                batch = labels.shape[0]\n                for i in range(batch):\n                    f.write(ids[i] + \"\\t\" + str(labels[i].item()) + \"\\t\" + str(predicted[i].item()) + \"\\n\")\n\n    f1_micro = f1_score(y_true, y_pred, labels=[0, 1, 2], average='micro')\n    f1_macro = f1_score(y_true, y_pred, labels=[0, 1, 2], average='macro')\n    if filepath is not None:\n        f.write(\"f1_micro: \" + str(f1_micro) + \"\\n\")\n        f.write(\"f1_macro: \" + str(f1_macro) + \"\\n\")\n        f.close()\n    return f1_micro, f1_macro", "\n\nif __name__ == '__main__':\n    train_idx, test_idx, label2idx = k_fold_split(opt.data_path)\n    f1_micro_list = []\n    f1_macro_list = []\n    if not os.path.exists('checkpoint'):\n        os.mkdir('checkpoint')\n    if not os.path.exists('result'):\n        os.mkdir('result')\n\n    for i in range(10):\n        model_path = opt.model_path + \"_\" + str(i) + \".pt\"\n        output_path = opt.output_path + \"_\" + str(i) + \".txt\"\n        train_loader, test_loader = get_data(train_idx[i], test_idx[i], label2idx)\n        model = GCN_Joint_EFP(opt, len(label2idx))\n        if opt.gpu:\n            model = model.cuda()\n        optimizer = AdamW(model.parameters(), lr=opt.lr, no_deprecation_warning=True)\n        max_f1 = 0\n        max_f1_micro = 0\n        max_f1_macro = 0\n        for epoch in range(opt.n_epochs):\n            train_loss = train(model, train_loader, optimizer, opt)\n            test_f1_micro, test_f1_macro = evaluate(model, test_loader)\n            print(\"Epoch:%d-%d loss:%f F1_micro:%.2f F1_macro:%.2f\" % (\n                i, epoch, train_loss, test_f1_micro * 100, test_f1_macro * 100))\n            if test_f1_micro + test_f1_macro > max_f1:\n                max_f1 = test_f1_micro + test_f1_macro\n                torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n                            'optimizer_state_dict': optimizer.state_dict()}, model_path)\n\n        checkpoint = torch.load(model_path)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        test_f1_micro, test_f1_macro = evaluate(model, test_loader, filepath=output_path)\n        print(\"Epoch:%d-%d F1_micro:%.2f F1_macro:%.2f\" % (\n            i, checkpoint['epoch'], test_f1_micro * 100, test_f1_macro * 100))\n        f1_micro_list.append(test_f1_micro)\n        f1_macro_list.append(test_f1_macro)\n\n    output = open(opt.output_path + \".txt\", \"w\")\n    # \u6700\u7ec8\u4fdd\u5b58\u5728\u8fd9\u513f\n    f1_micro_a = np.mean(f1_micro_list)\n    f1_macro_a = np.mean(f1_macro_list)\n    output.write(\"batch_size=\" + str(opt.batch_size) + \"\\n\")\n    output.write(\"lr=\" + str(opt.lr) + \"\\n\")\n    output.write(\"f1_micro_a: \" + str(f1_micro_a) + \"\\n\")\n    output.write(\"f1_macro_a: \" + str(f1_macro_a) + \"\\n\")\n    print(\"F1_micro_a: %.2f F1_macro_a: %.2f\" % (f1_micro_a * 100, f1_macro_a * 100))\n\n    ct_p = []\n    ct_m = []\n    ps_p = []\n    for i in range(10):\n        filename = opt.output_path + \"_\" + str(i) + \".txt\"\n        y_true = []\n        y_pred = []\n        with open(filename, \"r\") as f:\n            for l in f.readlines():\n                line = l.split()\n                if len(line) < 3:\n                    break\n                y_true.append(line[1])\n                y_pred.append(line[2])\n\n        with open(filename, \"a\") as f:\n            # 'a'\u662f\u6253\u5f00\u6587\u4ef6\u7684\u6a21\u5f0f\u4e4b\u4e00\uff0c\u8868\u793a\u4ee5\u8ffd\u52a0\u6a21\u5f0f\u6253\u5f00\u6587\u4ef6(append)\n            t_ct_p = f1_score(y_true, y_pred, labels=[0], average=\"macro\")\n            f.write(\"CT+: \" + str(t_ct_p) + \"\\n\")\n            ct_p.append(t_ct_p)\n\n            t_ct_m = f1_score(y_true, y_pred, labels=[1], average=\"macro\")\n            f.write(\"CT-: \" + str(t_ct_m) + \"\\n\")\n            ct_m.append(t_ct_m)\n\n            t_ps_p = f1_score(y_true, y_pred, labels=[2], average=\"macro\")\n            f.write(\"PS+: \" + str(t_ps_p) + \"\\n\")\n            ps_p.append(t_ps_p)\n\n    output.write(\"CT+: \" + str(np.mean(ct_p)) + \"\\n\")\n    output.write(\"CT-: \" + str(np.mean(ct_m)) + \"\\n\")\n    output.write(\"PS+: \" + str(np.mean(ps_p)) + \"\\n\")\n    output.close()", ""]}
{"filename": "src/Event_factuality/config.py", "chunked_list": ["import argparse\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='train a neural network for document-level event factuality prediction')\n    parser.add_argument('--data_path', type=str, default='data/english.xml', help='path to the data file')\n    parser.add_argument('--saved_path', type=str, default='data/english_uncertain_plain_gcn_joint_doc.pkl',\n                        help='path to the saved_data file')\n    parser.add_argument('--n_epochs', type=int, default=30, help='number of epochs to train')\n    parser.add_argument('--lr', type=float, default=2e-5, help='learning rate')\n    parser.add_argument('--batch_size', type=int, default=2, help='size of the training batches')\n    parser.add_argument('--labmda', type=float, default=0.2)\n\n    parser.add_argument('--gpu', dest=\"gpu\", action=\"store_const\", const=True, default=True, required=False,\n                        help='optional flag to use GPU if available')\n    # \u5728\u4f7f\u7528 argparse \u5e93\u65f6\uff0cdest \u53c2\u6570\u6307\u5b9a\u4e86\u5c06\u53c2\u6570\u503c\u5b58\u50a8\u5230\u89e3\u6790\u7ed3\u679c\u5bf9\u8c61\u7684\u54ea\u4e2a\u5c5e\u6027\u4e2d\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cdest \u53c2\u6570\u7684\u503c\u4e0e\u53c2\u6570\u540d\u76f8\u540c\uff0c\n    # \u4f46\u53ef\u4ee5\u4f7f\u7528\u8be5\u53c2\u6570\u6765\u81ea\u5b9a\u4e49\u540d\u79f0\u3002\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n    parser.add_argument('--warmup_proportion', default=0.1, type=float,\n                        help=\"Proportion of training to perform linear learning rate warmup for. \")\n    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n    parser.add_argument('--bert_hid_size', type=int, default=768)\n    parser.add_argument('--gcn_layers', type=int, default=2, help=\"the number of gcn layers\")\n    parser.add_argument('--gcn_hid_dim', type=int, default=768, help='the hidden size of gcn')\n    parser.add_argument('--gcn_out_dim', type=int, default=768, help='the output size of gcn')\n    parser.add_argument('--activation', type=str, default=\"relu\")\n    parser.add_argument('--dropout', type=float, default=0)\n    parser.add_argument('--linear_dim', type=int, default=300)\n    # parser.add_argument('--model_path', type=str, default=\"./checkpoint/chinese_model\")\n    # parser.add_argument('--output_path', type=str, default=\"./result/chinese_output\")\n    # parser.add_argument('--output_path1', type=str, default=\"./result/chinese_output_n1\")\n    # parser.add_argument('--output_path2', type=str, default=\"./result/chinese_output_n2\")\n    parser.add_argument('--model_path', type=str, default=\"./checkpoint/english_model\")\n    parser.add_argument('--output_path', type=str, default=\"./result/english_output\")\n    parser.add_argument('--output_path1', type=str, default=\"./result/english_output_n1\")\n    parser.add_argument('--output_path2', type=str, default=\"./result/english_output_n2\")\n    args = parser.parse_args()\n    for arg in vars(args):\n        print('{}={}'.format(arg.upper(), getattr(args, arg)))\n    print('')\n    return args", "\n\nopt = parse_args()\n"]}
{"filename": "src/Event_factuality/dataset.py", "chunked_list": ["import torch\nfrom transformers import BertTokenizer\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport os\nimport pickle\nimport re\nfrom torch.utils.data import Dataset\n\n\nclass Data(Dataset):\n    def __init__(self, data_path, saved_data_path, data_idx, label2idx, is_training=True):\n        self.is_training = is_training\n        self.document_data = None\n        self.data = []\n        self.document_max_length = 512\n        self.sentence_max_length = 150\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.debug_data = True\n        print('Reading data from {}.'.format(data_path))\n        if os.path.exists(saved_data_path) and not self.debug_data:\n            with open(file=saved_data_path, mode='rb') as fr:\n                info = pickle.load(fr)\n                self.document_data = info['data']\n            print('load preprocessed data from {}.'.format(saved_data_path))\n        else:\n            self.document_data = []\n            count = 0\n            tree = ET.parse(data_path)\n            root = tree.getroot()\n            for doc in root[0]:\n                id = doc.attrib['id']\n                label = label2idx[doc.attrib['document_level_value']]\n                sentence_list = []\n                trigger_word_list = []\n                flag = False\n                for sent in doc:\n                    if sent.text == '-EOP-.' or sent.text == '\u3002':\n                        continue\n\n                    s = ''\n                    for t in sent.itertext():\n                        s += t\n                    s = s.replace('-EOP-.', '\u3002').lower()\n                    if re.match(r'\\d{4}\\D\\d{2}\\D\\d{2}\\D\\d{2}:\\d{2}\\D$', s) is not None:\n                        flag = True\n                        continue\n                    elif flag:\n                        flag = False\n                        if len(sent) == 0:\n                            continue\n                    if len(s) <= 4:\n                        continue\n                    data = self.tokenizer(s, return_tensors='pt', padding='max_length', truncation=True, max_length=150)\n                    sent_info = {'trigger_num': 0,\n                                 'data': data['input_ids'],\n                                 'attention': data['attention_mask']}\n                    # x = len(sent)\n                    # if len(sent) == 0:\n                    #     raise AttributeError\n                    if len(sent) > 0:\n                        # has triggers \u4e3a\u4ec0\u4e48\uff1f\n                        # done \u7c7b\u4f3c\u4e8e '2017_07_26 06:37 -eop- .' ,len(sent)==0\n                        tmp = sent.text.lower() if sent.text is not None else ''\n                        for event in sent:\n                            tmp_subwords = self.tokenizer.tokenize(tmp)\n                            trigger_subwords = self.tokenizer.tokenize(event.text.lower())\n                            pos0 = len(tmp_subwords) + 1\n                            pos1 = pos0 + len(trigger_subwords)\n                            if pos0 >= self.sentence_max_length - 1:\n                                break\n                            if pos1 >= self.sentence_max_length - 1:\n                                pos1 = self.sentence_max_length - 1\n                            else:\n                                assert self.tokenizer.convert_ids_to_tokens(\n                                    data['input_ids'][0, pos0:pos1]) == trigger_subwords\n\n                            trigger_word_idx = torch.zeros(self.sentence_max_length)\n                            trigger_word_idx[pos0:pos1] = 1.0 / (pos1 - pos0)\n\n                            trigger_word_list.append({'sent_id': len(sentence_list),\n                                                      'idx': trigger_word_idx,\n                                                      'value': label2idx[event.attrib['sentence_level_value']]})\n                            tmp += event.text.lower()\n                            if event.tail is not None:\n                                tmp += event.tail.lower()\n                            sent_info['trigger_num'] += 1\n                    sentence_list.append(sent_info)\n                    if len(sentence_list) >= 35:\n                        break\n\n                trigger = ''\n                for sent in doc:\n                    if len(sent) > 0:\n                        s = ''\n                        for t in sent.itertext():\n                            s += t\n                        s = s.replace('-EOP-.', '\u3002').lower()\n                        trigger += s\n                trigger_data = self.tokenizer(trigger, return_tensors='pt',\n                                              padding='max_length', truncation=True, max_length=260)\n\n                # construct graph\n                # 1 document node, 35 sentence nodes, 20 trigger nodes for english data,\n                # 1 document node, 35 sentence nodes, 45 trigger nodes for chinese data\n                graph = self.create_graph(sentence_list, trigger_word_list)\n                assert len(graph) == 81\n                # \u4e3a\u4ec0\u4e48\u662f81\n\n                if len(trigger_word_list) > 0:\n                    x = {\n                        'ids': id,\n                        'labels': label,\n                        'triggers': trigger_data['input_ids'],\n                        'trigger_masks': trigger_data['attention_mask'],\n                        'sentences': sentence_list,\n                        'trigger_words': trigger_word_list,\n                        'graphs': graph\n                    }\n                    self.document_data.append({\n                        'ids': id,\n                        'labels': label,\n                        'triggers': trigger_data['input_ids'],\n                        'trigger_masks': trigger_data['attention_mask'],\n                        'sentences': sentence_list,\n                        'trigger_words': trigger_word_list,\n                        'graphs': graph\n                    })\n                else:\n                    print(id)\n                    count += 1\n\n            print('count: ', count)\n            # save data\n            with open(file=saved_data_path, mode='wb') as fw:\n                pickle.dump({'data': self.document_data}, fw)\n            print('finish reading {} and save preprocessed data to {}.'.format(data_path, saved_data_path))\n\n        for i in data_idx:\n            self.data.append(self.document_data[i])\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sentence_list = self.data[idx]['sentences']\n        data_list = []\n        attention_list = []\n        for s in sentence_list:\n            data_list.append(s['data'])\n            attention_list.append(s['attention'])\n        data = torch.cat(data_list, dim=0)\n        attention = torch.cat(attention_list, dim=0)\n        sentence_num = len(data)\n\n        trigger_word_list = self.data[idx]['trigger_words']\n        sent_idx_list = []\n        trigger_word_idx_list = []\n        trigger_label_list = []\n        for t in trigger_word_list:\n            sent_idx_list.append(t['sent_id'])\n            trigger_word_idx_list.append(t['idx'])\n            trigger_label_list.append(t['value'])\n        sent_idx = torch.tensor(sent_idx_list)\n        trigger_word_idx = torch.stack(trigger_word_idx_list, dim=0)\n        trigger_label = torch.tensor(trigger_label_list)\n        graph = torch.tensor(self.data[idx]['graphs']).unsqueeze(0)\n\n        return self.data[idx]['ids'], \\\n               torch.tensor(self.data[idx]['labels'], dtype=torch.long), \\\n               self.data[idx]['triggers'], \\\n               self.data[idx]['trigger_masks'], \\\n               data, \\\n               attention, \\\n               sent_idx, \\\n               trigger_word_idx, \\\n               trigger_label, \\\n               sentence_num, graph\n\n    def create_graph(self, sentence_list, trigger_word_list):\n        graph = np.zeros((81, 81))\n        sent_num = len(sentence_list)\n        trigger_num = len(trigger_word_list)\n\n        # add neighbor edges\n        for i in range(1, sent_num):\n            graph[i][i + 1] = 1.0\n            graph[i + 1][i] = 1.0\n\n        # add global edges\n        for i in range(1, sent_num + 1):\n            graph[0][i] = 1.0\n            graph[i][0] = 1.0\n\n        for i in range(trigger_num):\n            j = i + sent_num + 1\n            graph[0][j] = 1.0\n            graph[j][0] = 1.0\n            graph[j][trigger_word_list[i]['sent_id'] + 1] = 1.0\n            graph[trigger_word_list[i]['sent_id'] + 1][j] = 1.0\n        return graph", "\n\nclass Data(Dataset):\n    def __init__(self, data_path, saved_data_path, data_idx, label2idx, is_training=True):\n        self.is_training = is_training\n        self.document_data = None\n        self.data = []\n        self.document_max_length = 512\n        self.sentence_max_length = 150\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.debug_data = True\n        print('Reading data from {}.'.format(data_path))\n        if os.path.exists(saved_data_path) and not self.debug_data:\n            with open(file=saved_data_path, mode='rb') as fr:\n                info = pickle.load(fr)\n                self.document_data = info['data']\n            print('load preprocessed data from {}.'.format(saved_data_path))\n        else:\n            self.document_data = []\n            count = 0\n            tree = ET.parse(data_path)\n            root = tree.getroot()\n            for doc in root[0]:\n                id = doc.attrib['id']\n                label = label2idx[doc.attrib['document_level_value']]\n                sentence_list = []\n                trigger_word_list = []\n                flag = False\n                for sent in doc:\n                    if sent.text == '-EOP-.' or sent.text == '\u3002':\n                        continue\n\n                    s = ''\n                    for t in sent.itertext():\n                        s += t\n                    s = s.replace('-EOP-.', '\u3002').lower()\n                    if re.match(r'\\d{4}\\D\\d{2}\\D\\d{2}\\D\\d{2}:\\d{2}\\D$', s) is not None:\n                        flag = True\n                        continue\n                    elif flag:\n                        flag = False\n                        if len(sent) == 0:\n                            continue\n                    if len(s) <= 4:\n                        continue\n                    data = self.tokenizer(s, return_tensors='pt', padding='max_length', truncation=True, max_length=150)\n                    sent_info = {'trigger_num': 0,\n                                 'data': data['input_ids'],\n                                 'attention': data['attention_mask']}\n                    # x = len(sent)\n                    # if len(sent) == 0:\n                    #     raise AttributeError\n                    if len(sent) > 0:\n                        # has triggers \u4e3a\u4ec0\u4e48\uff1f\n                        # done \u7c7b\u4f3c\u4e8e '2017_07_26 06:37 -eop- .' ,len(sent)==0\n                        tmp = sent.text.lower() if sent.text is not None else ''\n                        for event in sent:\n                            tmp_subwords = self.tokenizer.tokenize(tmp)\n                            trigger_subwords = self.tokenizer.tokenize(event.text.lower())\n                            pos0 = len(tmp_subwords) + 1\n                            pos1 = pos0 + len(trigger_subwords)\n                            if pos0 >= self.sentence_max_length - 1:\n                                break\n                            if pos1 >= self.sentence_max_length - 1:\n                                pos1 = self.sentence_max_length - 1\n                            else:\n                                assert self.tokenizer.convert_ids_to_tokens(\n                                    data['input_ids'][0, pos0:pos1]) == trigger_subwords\n\n                            trigger_word_idx = torch.zeros(self.sentence_max_length)\n                            trigger_word_idx[pos0:pos1] = 1.0 / (pos1 - pos0)\n\n                            trigger_word_list.append({'sent_id': len(sentence_list),\n                                                      'idx': trigger_word_idx,\n                                                      'value': label2idx[event.attrib['sentence_level_value']]})\n                            tmp += event.text.lower()\n                            if event.tail is not None:\n                                tmp += event.tail.lower()\n                            sent_info['trigger_num'] += 1\n                    sentence_list.append(sent_info)\n                    if len(sentence_list) >= 35:\n                        break\n\n                trigger = ''\n                for sent in doc:\n                    if len(sent) > 0:\n                        s = ''\n                        for t in sent.itertext():\n                            s += t\n                        s = s.replace('-EOP-.', '\u3002').lower()\n                        trigger += s\n                trigger_data = self.tokenizer(trigger, return_tensors='pt',\n                                              padding='max_length', truncation=True, max_length=260)\n\n                # construct graph\n                # 1 document node, 35 sentence nodes, 20 trigger nodes for english data,\n                # 1 document node, 35 sentence nodes, 45 trigger nodes for chinese data\n                graph = self.create_graph(sentence_list, trigger_word_list)\n                assert len(graph) == 81\n                # \u4e3a\u4ec0\u4e48\u662f81\n\n                if len(trigger_word_list) > 0:\n                    x = {\n                        'ids': id,\n                        'labels': label,\n                        'triggers': trigger_data['input_ids'],\n                        'trigger_masks': trigger_data['attention_mask'],\n                        'sentences': sentence_list,\n                        'trigger_words': trigger_word_list,\n                        'graphs': graph\n                    }\n                    self.document_data.append({\n                        'ids': id,\n                        'labels': label,\n                        'triggers': trigger_data['input_ids'],\n                        'trigger_masks': trigger_data['attention_mask'],\n                        'sentences': sentence_list,\n                        'trigger_words': trigger_word_list,\n                        'graphs': graph\n                    })\n                else:\n                    print(id)\n                    count += 1\n\n            print('count: ', count)\n            # save data\n            with open(file=saved_data_path, mode='wb') as fw:\n                pickle.dump({'data': self.document_data}, fw)\n            print('finish reading {} and save preprocessed data to {}.'.format(data_path, saved_data_path))\n\n        for i in data_idx:\n            self.data.append(self.document_data[i])\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sentence_list = self.data[idx]['sentences']\n        data_list = []\n        attention_list = []\n        for s in sentence_list:\n            data_list.append(s['data'])\n            attention_list.append(s['attention'])\n        data = torch.cat(data_list, dim=0)\n        attention = torch.cat(attention_list, dim=0)\n        sentence_num = len(data)\n\n        trigger_word_list = self.data[idx]['trigger_words']\n        sent_idx_list = []\n        trigger_word_idx_list = []\n        trigger_label_list = []\n        for t in trigger_word_list:\n            sent_idx_list.append(t['sent_id'])\n            trigger_word_idx_list.append(t['idx'])\n            trigger_label_list.append(t['value'])\n        sent_idx = torch.tensor(sent_idx_list)\n        trigger_word_idx = torch.stack(trigger_word_idx_list, dim=0)\n        trigger_label = torch.tensor(trigger_label_list)\n        graph = torch.tensor(self.data[idx]['graphs']).unsqueeze(0)\n\n        return self.data[idx]['ids'], \\\n               torch.tensor(self.data[idx]['labels'], dtype=torch.long), \\\n               self.data[idx]['triggers'], \\\n               self.data[idx]['trigger_masks'], \\\n               data, \\\n               attention, \\\n               sent_idx, \\\n               trigger_word_idx, \\\n               trigger_label, \\\n               sentence_num, graph\n\n    def create_graph(self, sentence_list, trigger_word_list):\n        graph = np.zeros((81, 81))\n        sent_num = len(sentence_list)\n        trigger_num = len(trigger_word_list)\n\n        # add neighbor edges\n        for i in range(1, sent_num):\n            graph[i][i + 1] = 1.0\n            graph[i + 1][i] = 1.0\n\n        # add global edges\n        for i in range(1, sent_num + 1):\n            graph[0][i] = 1.0\n            graph[i][0] = 1.0\n\n        for i in range(trigger_num):\n            j = i + sent_num + 1\n            graph[0][j] = 1.0\n            graph[j][0] = 1.0\n            graph[j][trigger_word_list[i]['sent_id'] + 1] = 1.0\n            graph[trigger_word_list[i]['sent_id'] + 1][j] = 1.0\n        return graph", ""]}
{"filename": "src/Event/model.py", "chunked_list": ["import torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertConfig\n\nfrom consts import NONE\nfrom data_load import idx2trigger, argument2idx\nfrom utils import find_triggers\n\n\nclass Net(nn.Module):\n    def __init__(self, trigger_size=None, argument_size=None, device=torch.device(\"cpu\"), hparams=None):\n        super().__init__()\n        self.bert_model = BertModel.from_pretrained(hparams.model_name).to(device)\n        self.bert_tokenizer = BertConfig.from_pretrained(hparams.model_name)\n        hidden_size = self.bert_tokenizer.hidden_size\n        self.fc1 = nn.Sequential(\n            # nn.Dropout(0.5),\n            nn.Linear(hidden_size, hidden_size, bias=True),\n            nn.ReLU(),\n        ).to(device)\n        self.fc_trigger = nn.Sequential(\n            nn.Linear(hidden_size, trigger_size),\n        ).to(device)\n        self.fc_argument = nn.Sequential(\n            nn.Linear(hidden_size * 2, argument_size),\n        ).to(device)\n        self.device = device\n\n    def predict_triggers(self, tokens_x_2d, head_indexes_2d, triggers_y_2d, arguments_2d):\n        tokens_x_2d = torch.LongTensor(tokens_x_2d).to(self.device)\n        triggers_y_2d = torch.LongTensor(triggers_y_2d).to(self.device)\n        head_indexes_2d = torch.LongTensor(head_indexes_2d).to(self.device)\n\n        if self.training:\n            self.bert_model.train()\n            output = self.bert_model(tokens_x_2d)\n            encoded_layers = output['last_hidden_state']\n\n        else:\n            self.bert_model.eval()\n            with torch.no_grad():\n                output = self.bert_model(tokens_x_2d)\n                encoded_layers = output['last_hidden_state']\n\n        batch_size = tokens_x_2d.shape[0]\n        for i in range(batch_size):\n            encoded_layers[i] = torch.index_select(encoded_layers[i], 0, head_indexes_2d[i])\n\n        trigger_logits = self.fc_trigger(encoded_layers)\n        trigger_hat_2d = trigger_logits.argmax(-1)\n\n        argument_hidden, argument_keys = [], []\n        for i in range(batch_size):\n            candidates = arguments_2d[i]['candidates']\n            golden_entity_tensors = {}\n\n            for j in range(len(candidates)):\n                e_start, e_end = candidates[j]\n                golden_entity_tensors[candidates[j]] = encoded_layers[i, e_start:e_end, ].mean(dim=0)\n\n            predicted_triggers = find_triggers([idx2trigger[trigger] for trigger in trigger_hat_2d[i].tolist()])\n            for predicted_trigger in predicted_triggers:\n                t_start, t_end, t_type_str = predicted_trigger\n                event_tensor = encoded_layers[i, t_start:t_end, ].mean(dim=0)\n                for j in range(len(candidates)):\n                    e_start, e_end = candidates[j]\n                    entity_tensor = golden_entity_tensors[candidates[j]]\n\n                    argument_hidden.append(torch.cat([event_tensor, entity_tensor]))\n                    argument_keys.append((i, t_start, t_end, t_type_str, e_start, e_end))\n\n        return trigger_logits, triggers_y_2d, trigger_hat_2d, argument_hidden, argument_keys\n\n    def predict_arguments(self,\n                          argument_hidden,\n                          argument_keys,\n                          arguments_2d):\n        argument_hidden = torch.stack(argument_hidden)\n        argument_logits = self.fc_argument(argument_hidden)\n        argument_hat_1d = argument_logits.argmax(-1)\n\n        arguments_y_1d = []\n        for i, t_start, t_end, t_type_str, \\\n            e_start, e_end in argument_keys:\n            arg_label = argument2idx[NONE]\n            if (t_start, t_end, t_type_str) in arguments_2d[i]['events']:\n                for (a_start, a_end, a_type_idx) in arguments_2d[i]['events'][(t_start, t_end, t_type_str)]:\n                    if e_start == a_start and e_end == a_end:\n                        # raise Exception\n                        arg_label = a_type_idx\n                        break\n            arguments_y_1d.append(arg_label)\n\n        arguments_y_1d = torch.LongTensor(arguments_y_1d).to(self.device)\n\n        batch_size = len(arguments_2d)\n        argument_hat_2d = [{'events': {}} for _ in range(batch_size)]\n        for (i, st, ed, event_type_str, e_st, e_ed), a_label in zip(argument_keys, argument_hat_1d.cpu().numpy()):\n            if a_label == argument2idx[NONE]:\n                continue\n            if (st, ed, event_type_str) not in argument_hat_2d[i]['events']:\n                argument_hat_2d[i]['events'][(st, ed, event_type_str)] = []\n            argument_hat_2d[i]['events'][(st, ed, event_type_str)].append((e_st, e_ed, a_label))\n\n        return argument_logits, arguments_y_1d, argument_hat_1d, argument_hat_2d", "\nclass Net(nn.Module):\n    def __init__(self, trigger_size=None, argument_size=None, device=torch.device(\"cpu\"), hparams=None):\n        super().__init__()\n        self.bert_model = BertModel.from_pretrained(hparams.model_name).to(device)\n        self.bert_tokenizer = BertConfig.from_pretrained(hparams.model_name)\n        hidden_size = self.bert_tokenizer.hidden_size\n        self.fc1 = nn.Sequential(\n            # nn.Dropout(0.5),\n            nn.Linear(hidden_size, hidden_size, bias=True),\n            nn.ReLU(),\n        ).to(device)\n        self.fc_trigger = nn.Sequential(\n            nn.Linear(hidden_size, trigger_size),\n        ).to(device)\n        self.fc_argument = nn.Sequential(\n            nn.Linear(hidden_size * 2, argument_size),\n        ).to(device)\n        self.device = device\n\n    def predict_triggers(self, tokens_x_2d, head_indexes_2d, triggers_y_2d, arguments_2d):\n        tokens_x_2d = torch.LongTensor(tokens_x_2d).to(self.device)\n        triggers_y_2d = torch.LongTensor(triggers_y_2d).to(self.device)\n        head_indexes_2d = torch.LongTensor(head_indexes_2d).to(self.device)\n\n        if self.training:\n            self.bert_model.train()\n            output = self.bert_model(tokens_x_2d)\n            encoded_layers = output['last_hidden_state']\n\n        else:\n            self.bert_model.eval()\n            with torch.no_grad():\n                output = self.bert_model(tokens_x_2d)\n                encoded_layers = output['last_hidden_state']\n\n        batch_size = tokens_x_2d.shape[0]\n        for i in range(batch_size):\n            encoded_layers[i] = torch.index_select(encoded_layers[i], 0, head_indexes_2d[i])\n\n        trigger_logits = self.fc_trigger(encoded_layers)\n        trigger_hat_2d = trigger_logits.argmax(-1)\n\n        argument_hidden, argument_keys = [], []\n        for i in range(batch_size):\n            candidates = arguments_2d[i]['candidates']\n            golden_entity_tensors = {}\n\n            for j in range(len(candidates)):\n                e_start, e_end = candidates[j]\n                golden_entity_tensors[candidates[j]] = encoded_layers[i, e_start:e_end, ].mean(dim=0)\n\n            predicted_triggers = find_triggers([idx2trigger[trigger] for trigger in trigger_hat_2d[i].tolist()])\n            for predicted_trigger in predicted_triggers:\n                t_start, t_end, t_type_str = predicted_trigger\n                event_tensor = encoded_layers[i, t_start:t_end, ].mean(dim=0)\n                for j in range(len(candidates)):\n                    e_start, e_end = candidates[j]\n                    entity_tensor = golden_entity_tensors[candidates[j]]\n\n                    argument_hidden.append(torch.cat([event_tensor, entity_tensor]))\n                    argument_keys.append((i, t_start, t_end, t_type_str, e_start, e_end))\n\n        return trigger_logits, triggers_y_2d, trigger_hat_2d, argument_hidden, argument_keys\n\n    def predict_arguments(self,\n                          argument_hidden,\n                          argument_keys,\n                          arguments_2d):\n        argument_hidden = torch.stack(argument_hidden)\n        argument_logits = self.fc_argument(argument_hidden)\n        argument_hat_1d = argument_logits.argmax(-1)\n\n        arguments_y_1d = []\n        for i, t_start, t_end, t_type_str, \\\n            e_start, e_end in argument_keys:\n            arg_label = argument2idx[NONE]\n            if (t_start, t_end, t_type_str) in arguments_2d[i]['events']:\n                for (a_start, a_end, a_type_idx) in arguments_2d[i]['events'][(t_start, t_end, t_type_str)]:\n                    if e_start == a_start and e_end == a_end:\n                        # raise Exception\n                        arg_label = a_type_idx\n                        break\n            arguments_y_1d.append(arg_label)\n\n        arguments_y_1d = torch.LongTensor(arguments_y_1d).to(self.device)\n\n        batch_size = len(arguments_2d)\n        argument_hat_2d = [{'events': {}} for _ in range(batch_size)]\n        for (i, st, ed, event_type_str, e_st, e_ed), a_label in zip(argument_keys, argument_hat_1d.cpu().numpy()):\n            if a_label == argument2idx[NONE]:\n                continue\n            if (st, ed, event_type_str) not in argument_hat_2d[i]['events']:\n                argument_hat_2d[i]['events'][(st, ed, event_type_str)] = []\n            argument_hat_2d[i]['events'][(st, ed, event_type_str)].append((e_st, e_ed, a_label))\n\n        return argument_logits, arguments_y_1d, argument_hat_1d, argument_hat_2d", "\n"]}
{"filename": "src/Event/params.py", "chunked_list": ["import argparse\n# from transformers import set_seed\n\n\ndef get_hparams():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--batch_size\", type=int, default=1)\n    parser.add_argument(\"--eval_batch_size\", type=int, default=24)\n    parser.add_argument(\"--lr\", type=float, default=0.00002)\n    parser.add_argument(\"--n_epochs\", type=int, default=1)\n    parser.add_argument(\"--logdir\", type=str, default=\"logdir\")\n    # fixme_done path problem\n    parser.add_argument(\"--data_class\", type=str, choices=['ace', 'wiki_src', 'wiki_info'], default='wiki_src')\n    parser.add_argument(\"--model_name\", type=str, default=\"bert-large-cased\")\n    parser.add_argument(\"--seed\", type=int, default=42)\n    parser.add_argument(\"--model_save_path\", type=str, default='model_saved')\n    hparams = parser.parse_args()\n\n    # set_seed(hparams.seed)\n    return hparams", ""]}
{"filename": "src/Event/train_eval.py", "chunked_list": ["import torch.nn as nn\nimport os\nimport torch\nfrom data_load import idx2trigger\nfrom utils import calc_metric, find_triggers\nimport logging\nlogging.basicConfig(level=logging.INFO, filename='output.txt', filemode='w')\n\n\ndef train(model, iterator, optimizer, criterion):\n    if hasattr(model, 'module'):\n        model = model.module\n    model.train()\n    for i, batch in enumerate(iterator):\n        tokens_x_2d, triggers_y_2d, arguments_2d, seqlens_1d, head_indexes_2d, words_2d, triggers_2d = batch\n        optimizer.zero_grad()\n        trigger_logits, \\\n        triggers_y_2d, \\\n        trigger_hat_2d,\\\n        argument_hidden, \\\n        argument_keys = \\\n            model.predict_triggers(tokens_x_2d=tokens_x_2d, head_indexes_2d=head_indexes_2d,\n                                   triggers_y_2d=triggers_y_2d, arguments_2d=arguments_2d)\n\n        trigger_logits = trigger_logits.view(-1, trigger_logits.shape[-1])\n        trigger_loss = criterion(trigger_logits, triggers_y_2d.view(-1))\n\n        if len(argument_keys) > 0:\n            argument_logits,\\\n            arguments_y_1d,\\\n            argument_hat_1d,\\\n            argument_hat_2d \\\n                = model.predict_arguments(argument_hidden, argument_keys, arguments_2d)\n            argument_loss = criterion(argument_logits, arguments_y_1d)\n            loss = trigger_loss + 2 * argument_loss\n        else:\n            loss = trigger_loss\n\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        loss.backward()\n        optimizer.step()", "\ndef train(model, iterator, optimizer, criterion):\n    if hasattr(model, 'module'):\n        model = model.module\n    model.train()\n    for i, batch in enumerate(iterator):\n        tokens_x_2d, triggers_y_2d, arguments_2d, seqlens_1d, head_indexes_2d, words_2d, triggers_2d = batch\n        optimizer.zero_grad()\n        trigger_logits, \\\n        triggers_y_2d, \\\n        trigger_hat_2d,\\\n        argument_hidden, \\\n        argument_keys = \\\n            model.predict_triggers(tokens_x_2d=tokens_x_2d, head_indexes_2d=head_indexes_2d,\n                                   triggers_y_2d=triggers_y_2d, arguments_2d=arguments_2d)\n\n        trigger_logits = trigger_logits.view(-1, trigger_logits.shape[-1])\n        trigger_loss = criterion(trigger_logits, triggers_y_2d.view(-1))\n\n        if len(argument_keys) > 0:\n            argument_logits,\\\n            arguments_y_1d,\\\n            argument_hat_1d,\\\n            argument_hat_2d \\\n                = model.predict_arguments(argument_hidden, argument_keys, arguments_2d)\n            argument_loss = criterion(argument_logits, arguments_y_1d)\n            loss = trigger_loss + 2 * argument_loss\n        else:\n            loss = trigger_loss\n\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        loss.backward()\n        optimizer.step()", "        #\n        # if i % 10 == 0:  # monitoring\n        #     print(\"step: {}, loss: {}\".format(i, loss.item()))\n\n\ndef evaluate(model, iterator, f_name, f1_max=0):\n    if hasattr(model, 'module'):\n        model = model.module\n    model.eval()\n    words_all = []\n    triggers_all = []\n    triggers_hat_all = []\n    arguments_all = []\n    arguments_hat_all = []\n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n            tokens_x_2d, triggers_y_2d, arguments_2d, seqlens_1d, head_indexes_2d, words_2d, triggers_2d = batch\n\n            trigger_logits, triggers_y_2d, trigger_hat_2d, argument_hidden, argument_keys \\\n                = model.predict_triggers(tokens_x_2d=tokens_x_2d, head_indexes_2d=head_indexes_2d,\n                                         triggers_y_2d=triggers_y_2d, arguments_2d=arguments_2d)\n\n            words_all.extend(words_2d)\n            triggers_all.extend(triggers_2d)\n            triggers_hat_all.extend(trigger_hat_2d.cpu().numpy().tolist())\n            arguments_all.extend(arguments_2d)\n\n            if len(argument_keys) > 0:\n                argument_logits, arguments_y_1d, argument_hat_1d, argument_hat_2d = \\\n                    model.predict_arguments(argument_hidden, argument_keys, arguments_2d)\n                arguments_hat_all.extend(argument_hat_2d)\n            else:\n                batch_size = len(arguments_2d)\n                argument_hat_2d = [{'events': {}} for _ in range(batch_size)]\n                arguments_hat_all.extend(argument_hat_2d)\n\n    triggers_true = []\n    triggers_pred = []\n    arguments_true = []\n    arguments_pred = []\n    with open('temp', 'w') as f_out:\n        for i, (words, triggers, triggers_hat, arguments, arguments_hat) \\\n                in enumerate(zip(words_all, triggers_all, triggers_hat_all, arguments_all, arguments_hat_all)):\n            triggers_hat = triggers_hat[:len(words)]\n            triggers_hat = [idx2trigger[hat] for hat in triggers_hat]\n\n            # [(ith sentence, t_start, t_end, t_type_str)]\n            triggers_true.extend([(i, *item) for item in find_triggers(triggers)])\n            triggers_pred.extend([(i, *item) for item in find_triggers(triggers_hat)])\n\n            # [(ith sentence, t_start, t_end, t_type_str, a_start, a_end, a_type_idx)]\n            for trigger in arguments['events']:\n                t_start, t_end, t_type_str = trigger\n                for argument in arguments['events'][trigger]:\n                    a_start, a_end, a_type_idx = argument\n                    arguments_true.append((i, t_start, t_end, t_type_str, a_start, a_end, a_type_idx))\n\n            for trigger in arguments_hat['events']:\n                t_start, t_end, t_type_str = trigger\n                for argument in arguments_hat['events'][trigger]:\n                    a_start, a_end, a_type_idx = argument\n                    arguments_pred.append((i, t_start, t_end, t_type_str, a_start, a_end, a_type_idx))\n\n            for w, t, t_h in zip(words[1:-1], triggers, triggers_hat):\n                f_out.write('{}\\t{}\\t{}\\n'.format(w, t, t_h))\n            f_out.write('#arguments#{}\\n'.format(arguments['events']))\n            f_out.write('#arguments_hat#{}\\n'.format(arguments_hat['events']))\n            f_out.write(\"\\n\")\n\n    # print(classification_report([idx2trigger[idx] for idx in y_true], [idx2trigger[idx] for idx in y_pred]))\n\n    trigger_p, trigger_r, trigger_f1 = calc_metric(triggers_true, triggers_pred)\n    argument_p, argument_r, argument_f1 = calc_metric(arguments_true, arguments_pred)\n    triggers_true = [(item[0], item[1], item[2]) for item in triggers_true]\n    triggers_pred = [(item[0], item[1], item[2]) for item in triggers_pred]\n    arguments_true = [(item[0], item[1], item[2], item[3], item[4], item[5]) for item in arguments_true]\n    arguments_pred = [(item[0], item[1], item[2], item[3], item[4], item[5]) for item in arguments_pred]\n    argument_p_, argument_r_, argument_f1_ = calc_metric(arguments_true, arguments_pred)\n    trigger_p_, trigger_r_, trigger_f1_ = calc_metric(triggers_true, triggers_pred)\n    f1 = argument_f1 + argument_f1_ + trigger_f1 + trigger_f1_\n    if f1 > f1_max:\n        print('[trigger classification]')\n        print('P={:.3f}\\tR={:.3f}\\tF1={:.3f}'.format(trigger_p, trigger_r, trigger_f1))\n\n        print('[argument classification]')\n        print('P={:.3f}\\tR={:.3f}\\tF1={:.3f}'.format(argument_p, argument_r, argument_f1))\n        print('[trigger identification]')\n        print('P={:.3f}\\tR={:.3f}\\tF1={:.3f}'.format(trigger_p_, trigger_r_, trigger_f1_))\n\n        print('[argument identification]')\n        print('P={:.3f}\\tR={:.3f}\\tF1={:.3f}'.format(argument_p_, argument_r_, argument_f1_))\n\n        logging.info('[trigger classification]')\n        logging.info('P={:.3f}\\tR={:.3f}\\tF1={:.3f}'.format(trigger_p, trigger_r, trigger_f1))\n\n        logging.info('[argument classification]')\n        logging.info('P={:.3f}\\tR={:.3f}\\tF1={:.3f}'.format(argument_p, argument_r, argument_f1))\n        logging.info('[trigger identification]')\n        logging.info('P={:.3f}\\tR={:.3f}\\tF1={:.3f}'.format(trigger_p_, trigger_r_, trigger_f1_))\n\n        logging.info('[argument identification]')\n        logging.info('P={:.3f}\\tR={:.3f}\\tF1={:.3f}'.format(argument_p_, argument_r_, argument_f1_))\n\n        metric = '[trigger classification]\\tP={:.3f}\\tR={:.3f}\\tF1={:.3f}\\n'.format(trigger_p, trigger_r,\n                                                                                    trigger_f1)\n        metric += '[argument classification]\\tP={:.3f}\\tR={:.3f}\\tF1={:.3f}\\n'.format(argument_p, argument_r,\n                                                                                      argument_f1)\n        metric += '[trigger identification]\\tP={:.3f}\\tR={:.3f}\\tF1={:.3f}\\n'.format(trigger_p_, trigger_r_,\n                                                                                     trigger_f1_)\n        metric += '[argument identification]\\tP={:.3f}\\tR={:.3f}\\tF1={:.3f}\\n'.format(argument_p_, argument_r_,\n                                                                                      argument_f1_)\n        final = f_name + \".P%.2f_R%.2f_F%.2f\" % (trigger_p, trigger_r, trigger_f1)\n        with open(final, 'w') as f_out:\n            result = open(\"temp\", \"r\").read()\n            f_out.write(\"{}\\n\".format(result))\n            f_out.write(metric)\n        os.remove(\"temp\")\n    return f1", ""]}
{"filename": "src/Event/consts.py", "chunked_list": ["import json\nimport os\nfrom os.path import exists\n\nfrom utils import get_event_type, write_json, process, get_argument_role, get_entity_type, \\\n    get_event_type_ace, get_argument_role_ace, get_entity_type_ace\nfrom params import get_hparams\nmax_length = 400\n\nNONE = 'O'", "\nNONE = 'O'\nPAD = \"[PAD]\"\nUNK = \"[UNK]\"\n\n# for BERT\nCLS = '[CLS]'\nSEP = '[SEP]'\n\nhp = get_hparams()", "\nhp = get_hparams()\ndata_dir = None\nif hp.data_class == 'wiki_src':\n    data_dir = 'Datasets/wiki_processed_data/source/'\nelif hp.data_class == 'wiki_info':\n    data_dir = 'Datasets/wiki_processed_data/info/'\nelif hp.data_class == 'ace':\n    data_dir = 'Datasets/ace2005/'\ntrain_set = data_dir + 'train.json'", "train_set = data_dir + 'train.json'\ntest_set = data_dir + 'test.json'\ndev_set = data_dir + 'dev.json'\n# trigger\nif not os.path.exists('{}/event_type_list.json'.format(data_dir)):\n    triggers0 = set()\n    if hp.data_class == 'ace':\n        triggers1 = get_event_type_ace(train_set, triggers0)\n        triggers2 = get_event_type_ace(dev_set, triggers1)\n        triggers3 = get_event_type_ace(test_set, triggers2)\n        TRIGGERS = list(triggers3)\n    else:\n        triggers1 = get_event_type(train_set, triggers0)\n        triggers2 = get_event_type(dev_set, triggers1)\n        triggers3 = get_event_type(test_set, triggers2)\n        TRIGGERS = list(triggers3)\n    write_json(data=TRIGGERS, fn='{}/event_type_list.json'.format(data_dir))\nelse:\n    with open('{}/event_type_list.json'.format(data_dir), mode='r') as f:\n        TRIGGERS = json.load(f)", "\n# argument\nif not os.path.exists('{}/argument_role_list.json'.format(data_dir)):\n    if hp.data_class == 'ace':\n        argument = get_argument_role_ace(train_set)\n        argument1 = get_argument_role_ace(dev_set, argument)\n        argument2 = get_argument_role_ace(test_set, argument1)\n        ARGUMENTS = list(argument2)\n    else:\n        argument = get_argument_role(train_set)\n        argument1 = get_argument_role(dev_set, argument)\n        argument2 = get_argument_role(test_set, argument1)\n        ARGUMENTS = list(argument2)\n    write_json(data=ARGUMENTS, fn='{}/argument_role_list.json'.format(data_dir))\nelse:\n    with open('{}/argument_role_list.json'.format(data_dir), mode='r') as f:\n        ARGUMENTS = json.load(f)", "\n# entity\nif not os.path.exists('{}/entity_type_list.json'.format(data_dir)):\n    if hp.data_class == 'ace':\n        entities = get_entity_type_ace(train_set)\n        entities = get_entity_type_ace(dev_set, entities)\n        entities = get_entity_type_ace(test_set, entities)\n        ENTITIES = list(entities)\n    else:\n        entities, mention_type = get_entity_type(train_set)\n        entities, mention_type = get_entity_type(dev_set, entities, mention_type)\n        entities, mention_type = get_entity_type(test_set, entities, mention_type)\n        ENTITIES = list(entities)\n    write_json(data=ENTITIES, fn='{}/entity_type_list.json'.format(data_dir))\nelse:\n    with open('{}/entity_type_list.json'.format(data_dir), mode='r') as f:\n        ENTITIES = json.load(f)", "\n\n# if __name__ == \"__main__\":\n#     hp = get_hparams()\n#     data_dir = None\n#     if hp.data_class == 'wiki_src':\n#         data_dir = 'Datasets/wiki_processed_data/source/'\n#     elif hp.data_class == 'wiki_info':\n#         data_dir = 'Datasets/wiki_processed_data/info/'\n#     elif hp.data_class == 'ace':", "#         data_dir = 'Datasets/wiki_processed_data/info/'\n#     elif hp.data_class == 'ace':\n#         data_dir = 'Datasets/ace2005/'\n#     train_set = data_dir + 'train.json'\n#     test_set = data_dir + 'test.json'\n#     dev_set = data_dir + 'dev.json'\n    # # \u5728\u6ca1\u6709\u9884\u5904\u7406\u6587\u4ef6\u65f6\u53ef\u4ee5\u4f7f\u7528\u3002\u5426\u5219\uff0c\u76f4\u63a5\u521b\u5efaTRIGGER AND ARGUMENTS\u5373\u53ef\n    # test_event1 = [{\"id\": \"scenario_en_kairos_14-E3\",\n    #                 \"event_type\": \"Cognitive.IdentifyCategorize.Unspecified\",\n    #                 \"trigger\": (30, 31, 'Cognitive.IdentifyCategorize.Unspecified'),", "    #                 \"event_type\": \"Cognitive.IdentifyCategorize.Unspecified\",\n    #                 \"trigger\": (30, 31, 'Cognitive.IdentifyCategorize.Unspecified'),\n    #                 \"arguments\": []},\n    #                {\"id\": \"scenario_en_kairos_14-E2\",\n    #                 \"event_type\": \"Cognitive.Inspection.SensoryObserve\",\n    #                 \"trigger\": (88, 89, 'Cognitive.Inspection.SensoryObserve'),\n    #                 \"arguments\": []},\n    #                {\"id\": \"scenario_en_kairos_14-E1\",\n    #                 \"event_type\": \"Cognitive.IdentifyCategorize.Unspecified\",\n    #                 \"trigger\": (166, 167, 'Cognitive.IdentifyCategorize.Unspecified'),", "    #                 \"event_type\": \"Cognitive.IdentifyCategorize.Unspecified\",\n    #                 \"trigger\": (166, 167, 'Cognitive.IdentifyCategorize.Unspecified'),\n    #                 \"arguments\": []}]\n    #\n    # test_event2 = [{\"id\": \"scenario_en_kairos_65-E1\", \"event_type\": \"Conflict.Attack.Unspecified\",\n    #                 \"trigger\": (50, 51, 'Conflict.Attack.Unspecified'),\n    #                 \"arguments\": []},\n    #                {\"id\": \"scenario_en_kairos_65-E2\", \"event_type\": \"Life.Injure.Unspecified\",\n    #                 \"trigger\": (62, 63, 'Life.Injure.Unspecified'),\n    #                 \"arguments\": [(59, 61, 'Victim')]},", "    #                 \"trigger\": (62, 63, 'Life.Injure.Unspecified'),\n    #                 \"arguments\": [(59, 61, 'Victim')]},\n    #                {\"id\": \"scenario_en_kairos_65-E3\", \"event_type\": \"Conflict.Attack.DetonateExplode\",\n    #                 \"trigger\": (65, 66, 'Conflict.Attack.DetonateExplode'),\n    #                 \"arguments\": []},\n    #                {\"id\": \"scenario_en_kairos_65-E4\", \"event_type\": \"Conflict.Attack.DetonateExplode\",\n    #                 \"trigger\": (417, 419, 'Conflict.Attack.DetonateExplode'),\n    #                 \"arguments\": []},\n    #                {\"id\": \"scenario_en_kairos_65-E5\", \"event_type\": \"Conflict.Attack.DetonateExplode\",\n    #                 \"trigger\": (433, 434, 'Conflict.Attack.DetonateExplode'),", "    #                {\"id\": \"scenario_en_kairos_65-E5\", \"event_type\": \"Conflict.Attack.DetonateExplode\",\n    #                 \"trigger\": (433, 434, 'Conflict.Attack.DetonateExplode'),\n    #                 \"arguments\": []}]\n\n#     process_data_fn = 'wiki_processed_data/source'\n    # train_set = \"../../wiki_events_dataset/info_data/train_info.jsonl\"\n    # dev_set = \"../../wiki_events_dataset/info_data/dev_info.jsonl\"\n    # test_set = \"../../wiki_events_dataset/info_data/test_info.jsonl\"\n    # process_data_fn = 'processed_data/info'\n    # if not exists(process_data_fn):", "    # process_data_fn = 'processed_data/info'\n    # if not exists(process_data_fn):\n    #     os.makedirs(process_data_fn)\n\n    # trigger\n    # if not os.path.exists('{}/event_type_list.json'.format(process_data_fn)):\n    #     triggers0 = set()\n    #     triggers1 = get_event_type(train_set, triggers0)\n    #     triggers2 = get_event_type(dev_set, triggers1)\n    #     triggers3 = get_event_type(test_set, triggers2)", "    #     triggers2 = get_event_type(dev_set, triggers1)\n    #     triggers3 = get_event_type(test_set, triggers2)\n    #     TRIGGERS = list(triggers3)\n    #     write_json(data=TRIGGERS, fn='{}/event_type_list.json'.format(process_data_fn))\n    # else:\n    #     with open('{}/event_type_list.json'.format(process_data_fn), mode='r') as f:\n    #         TRIGGERS = json.load(f)\n    #\n    # # argument\n    # if not os.path.exists('{}/argument_role_list.json'.format(process_data_fn)):", "    # # argument\n    # if not os.path.exists('{}/argument_role_list.json'.format(process_data_fn)):\n    #     argument = get_argument_role(train_set)\n    #     argument1 = get_argument_role(dev_set, argument)\n    #     argument2 = get_argument_role(test_set, argument1)\n    #     ARGUMENTS = list(argument2)\n    #     write_json(data=ARGUMENTS, fn='{}/argument_role_list.json'.format(process_data_fn))\n    # else:\n    #     with open('{}/argument_role_list.json'.format(process_data_fn), mode='r') as f:\n    #         ARGUMENTS = json.load(f)", "    #     with open('{}/argument_role_list.json'.format(process_data_fn), mode='r') as f:\n    #         ARGUMENTS = json.load(f)\n    #\n    # # entity\n    # if not os.path.exists('{}/entity_type_list.json'.format(process_data_fn)):\n    #     entities, mention_type = get_entity_type(train_set)\n    #     entities, mention_type = get_entity_type(dev_set, entities, mention_type)\n    #     entities, mention_type = get_entity_type(test_set, entities, mention_type)\n    #     ENTITIES = list(entities)\n    #     write_json(data=ENTITIES, fn='{}/entity_type_list.json'.format(process_data_fn))", "    #     ENTITIES = list(entities)\n    #     write_json(data=ENTITIES, fn='{}/entity_type_list.json'.format(process_data_fn))\n    # else:\n    #     with open('{}/entity_type_list.json'.format(process_data_fn), mode='r') as f:\n    #         ENTITIES = json.load(f)\n    #\n    # processed_train = '{}/train.json'.format(process_data_fn)\n    # processed_dev = '{}/dev.json'.format(process_data_fn)\n    # processed_test = '{}/test.json'.format(process_data_fn)\n    # chunk_test = '{}/test_chunk.json'.format(process_data_fn)", "    # processed_test = '{}/test.json'.format(process_data_fn)\n    # chunk_test = '{}/test_chunk.json'.format(process_data_fn)\n    #\n    # over_write = False\n    # if not os.path.exists(processed_train) or over_write:\n    #     process(train_set, processed_train)\n    # if not os.path.exists(processed_dev) or over_write:\n    #     process(dev_set, processed_dev)\n    # if not os.path.exists(processed_test) or over_write:\n    #     process(test_set, processed_test)", "    # if not os.path.exists(processed_test) or over_write:\n    #     process(test_set, processed_test)\n\n"]}
{"filename": "src/Event/train.py", "chunked_list": ["import os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils import data\n\nfrom data_load import Dataset, pad, all_triggers, all_arguments\nfrom model import Net\nfrom params import get_hparams", "from model import Net\nfrom params import get_hparams\nfrom train_eval import train, evaluate\n\nimport logging\nlogging.basicConfig(level=logging.INFO, filename='output.txt', filemode='w')\n\n\nif __name__ == \"__main__\":\n    hp = get_hparams()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model = Net(\n        device=device,\n        trigger_size=len(all_triggers),\n        argument_size=len(all_arguments),\n        hparams=hp\n    )\n    if device == 'cuda':\n        model = model.cuda()\n\n    model = nn.DataParallel(model)\n    data_dir = None\n    if hp.data_class == 'wiki_src':\n        data_dir = 'Datasets/wiki_processed_data/source/'\n    elif hp.data_class == 'wiki_info':\n        data_dir = 'Datasets/wiki_processed_data/info/'\n    elif hp.data_class == 'ace':\n        data_dir = 'Datasets/ace2005/'\n    train_set = data_dir + 'train.json'\n    test_set = data_dir + 'test.json'\n    dev_set = data_dir + 'dev.json'\n    train_dataset = Dataset(train_set)\n    dev_dataset = Dataset(dev_set)\n    test_dataset = Dataset(test_set)\n\n    samples_weight = train_dataset.get_samples_weight()\n    sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight))\n\n    train_iter = data.DataLoader(dataset=train_dataset,\n                                 batch_size=hp.batch_size,\n                                 shuffle=False,\n                                 sampler=sampler,\n                                 num_workers=4,\n                                 collate_fn=pad)\n    dev_iter = data.DataLoader(dataset=dev_dataset,\n                               batch_size=hp.batch_size,\n                               shuffle=False,\n                               num_workers=4,\n                               collate_fn=pad)\n    test_iter = data.DataLoader(dataset=test_dataset,\n                                batch_size=hp.batch_size,\n                                shuffle=False,\n                                num_workers=4,\n                                collate_fn=pad)\n\n    optimizer = optim.Adam(model.parameters(), lr=hp.lr)\n    # optimizer = optim.Adadelta(model.parameters(), lr=1.0, weight_decay=1e-2)\n\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    if not os.path.exists(hp.logdir):\n        os.makedirs(hp.logdir)\n    f1_max_dev = 0\n    f1_max_test = 0\n    for epoch in range(1, hp.n_epochs + 1):\n        train(model, train_iter, optimizer, criterion)\n\n        f_name = os.path.join(hp.logdir, str(epoch))\n        print(f\"=========eval dev at epoch={epoch}=========\")\n        logging.info(f\"=========eval dev at epoch={epoch}=========\")\n        f1_dev = evaluate(model, dev_iter, f_name + '_dev', f1_max_dev)\n        if f1_dev > f1_max_dev:\n            f1_max_dev = f1_dev\n            if not os.path.exists(hp.model_save_path):\n                os.makedirs(hp.model_save_path)\n            torch.save(model, f\"{hp.model_save_path}/best_model_{epoch}.pt\")\n            print('best_model has been saved')\n            print(f\"weights were saved to {f_name}.pt\")\n            logging.info('best_model has been saved')\n            logging.info(f\"weights were saved to {f_name}.pt\")\n        print(f\"=========eval test at epoch={epoch}=========\")\n        logging.info(f\"=========eval test at epoch={epoch}=========\")\n        f1_test = evaluate(model, test_iter, f_name + '_test', f1_max_test)\n        if f1_test > f1_max_test:\n            f1_max_test = f1_test", "if __name__ == \"__main__\":\n    hp = get_hparams()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model = Net(\n        device=device,\n        trigger_size=len(all_triggers),\n        argument_size=len(all_arguments),\n        hparams=hp\n    )\n    if device == 'cuda':\n        model = model.cuda()\n\n    model = nn.DataParallel(model)\n    data_dir = None\n    if hp.data_class == 'wiki_src':\n        data_dir = 'Datasets/wiki_processed_data/source/'\n    elif hp.data_class == 'wiki_info':\n        data_dir = 'Datasets/wiki_processed_data/info/'\n    elif hp.data_class == 'ace':\n        data_dir = 'Datasets/ace2005/'\n    train_set = data_dir + 'train.json'\n    test_set = data_dir + 'test.json'\n    dev_set = data_dir + 'dev.json'\n    train_dataset = Dataset(train_set)\n    dev_dataset = Dataset(dev_set)\n    test_dataset = Dataset(test_set)\n\n    samples_weight = train_dataset.get_samples_weight()\n    sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight))\n\n    train_iter = data.DataLoader(dataset=train_dataset,\n                                 batch_size=hp.batch_size,\n                                 shuffle=False,\n                                 sampler=sampler,\n                                 num_workers=4,\n                                 collate_fn=pad)\n    dev_iter = data.DataLoader(dataset=dev_dataset,\n                               batch_size=hp.batch_size,\n                               shuffle=False,\n                               num_workers=4,\n                               collate_fn=pad)\n    test_iter = data.DataLoader(dataset=test_dataset,\n                                batch_size=hp.batch_size,\n                                shuffle=False,\n                                num_workers=4,\n                                collate_fn=pad)\n\n    optimizer = optim.Adam(model.parameters(), lr=hp.lr)\n    # optimizer = optim.Adadelta(model.parameters(), lr=1.0, weight_decay=1e-2)\n\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    if not os.path.exists(hp.logdir):\n        os.makedirs(hp.logdir)\n    f1_max_dev = 0\n    f1_max_test = 0\n    for epoch in range(1, hp.n_epochs + 1):\n        train(model, train_iter, optimizer, criterion)\n\n        f_name = os.path.join(hp.logdir, str(epoch))\n        print(f\"=========eval dev at epoch={epoch}=========\")\n        logging.info(f\"=========eval dev at epoch={epoch}=========\")\n        f1_dev = evaluate(model, dev_iter, f_name + '_dev', f1_max_dev)\n        if f1_dev > f1_max_dev:\n            f1_max_dev = f1_dev\n            if not os.path.exists(hp.model_save_path):\n                os.makedirs(hp.model_save_path)\n            torch.save(model, f\"{hp.model_save_path}/best_model_{epoch}.pt\")\n            print('best_model has been saved')\n            print(f\"weights were saved to {f_name}.pt\")\n            logging.info('best_model has been saved')\n            logging.info(f\"weights were saved to {f_name}.pt\")\n        print(f\"=========eval test at epoch={epoch}=========\")\n        logging.info(f\"=========eval test at epoch={epoch}=========\")\n        f1_test = evaluate(model, test_iter, f_name + '_test', f1_max_test)\n        if f1_test > f1_max_test:\n            f1_max_test = f1_test", "\n"]}
{"filename": "src/Event/model_demo.py", "chunked_list": ["import torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertConfig\n\nfrom consts import NONE\nfrom data_load import idx2trigger, argument2idx\nfrom utils import find_triggers\n\n\nclass Net_demo(nn.Module):\n    def __init__(self, trigger_size=None, argument_size=None, device=torch.device(\"cpu\"), hparams=None):\n        super().__init__()\n        self.bert_model = BertModel.from_pretrained(hparams.model_name).to(device)\n        self.bert_tokenizer = BertConfig.from_pretrained(hparams.model_name)\n        hidden_size = self.bert_tokenizer.hidden_size\n        self.fc1 = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size, bias=True),\n            nn.ReLU(),\n        ).to(device)\n        self.fc_trigger = nn.Sequential(\n            nn.Linear(hidden_size, trigger_size),\n        ).to(device)\n        self.fc_argument = nn.Sequential(\n            nn.Linear(hidden_size, argument_size),\n        ).to(device)\n        self.device = device\n\n    def predict_triggers(self, tokens_x_2d, head_indexes_2d, triggers_y_2d, arguments_2d):\n        tokens_x_2d = torch.LongTensor(tokens_x_2d).to(self.device)\n        triggers_y_2d = torch.LongTensor(triggers_y_2d).to(self.device)\n        head_indexes_2d = torch.LongTensor(head_indexes_2d).to(self.device)\n\n        if self.training:\n            self.bert_model.train()\n            output = self.bert_model(tokens_x_2d)\n            encoded_layers = output['last_hidden_state']\n\n        else:\n            self.bert_model.eval()\n            with torch.no_grad():\n                output = self.bert_model(tokens_x_2d)\n                encoded_layers = output['last_hidden_state']\n\n        batch_size = tokens_x_2d.shape[0]\n        for i in range(batch_size):\n            encoded_layers[i] = torch.index_select(encoded_layers[i], 0, head_indexes_2d[i])\n\n        trigger_logits = self.fc_trigger(encoded_layers)\n        trigger_hat_2d = trigger_logits.argmax(-1)\n\n        argument_hidden = []\n        argument_keys = []\n        for i in range(batch_size):\n            candidates = arguments_2d[i]['candidates']\n            golden_entity_tensors = {}\n\n            for j in range(len(candidates)):\n                e_start, e_end = candidates[j]\n                golden_entity_tensors[candidates[j]] = encoded_layers[i, e_start:e_end, ].mean(dim=0)\n\n            predicted_triggers = find_triggers([idx2trigger[trigger] for trigger in trigger_hat_2d[i].tolist()])\n            for predicted_trigger in predicted_triggers:\n                t_start, t_end, t_type_str = predicted_trigger\n                event_tensor = encoded_layers[i, t_start:t_end, ].mean(dim=0)\n                for j in range(len(candidates)):\n                    e_start, e_end = candidates[j]\n                    argument_hidden.append(torch.cat([event_tensor]))\n                    argument_keys.append((i, t_start, t_end, t_type_str, e_start, e_end))\n\n        return trigger_logits, triggers_y_2d, trigger_hat_2d, argument_hidden, argument_keys\n\n    def predict_arguments(self,\n                          argument_hidden,\n                          argument_keys,\n                          arguments_2d):\n        argument_hidden = torch.stack(argument_hidden)\n        argument_logits = self.fc_argument(argument_hidden)\n        argument_hat_1d = argument_logits.argmax(-1)\n\n        arguments_y_1d = []\n        for i, t_start, t_end, t_type_str, \\\n            e_start, e_end in argument_keys:\n            arg_label = argument2idx[NONE]\n            if (t_start, t_end, t_type_str) in arguments_2d[i]['events']:\n                for (a_start, a_end, a_type_idx) in arguments_2d[i]['events'][(t_start, t_end, t_type_str)]:\n                    if e_start == a_start and e_end == a_end:\n                        # raise Exception\n                        arg_label = a_type_idx\n                        break\n            arguments_y_1d.append(arg_label)\n\n        arguments_y_1d = torch.LongTensor(arguments_y_1d).to(self.device)\n\n        batch_size = len(arguments_2d)\n        argument_hat_2d = [{'events': {}} for _ in range(batch_size)]\n        for (i, st, ed, event_type_str, e_st, e_ed), a_label in zip(argument_keys, argument_hat_1d.cpu().numpy()):\n            if a_label == argument2idx[NONE]:\n                continue\n            if (st, ed, event_type_str) not in argument_hat_2d[i]['events']:\n                argument_hat_2d[i]['events'][(st, ed, event_type_str)] = []\n            argument_hat_2d[i]['events'][(st, ed, event_type_str)].append((e_st, e_ed, a_label))\n\n        return argument_logits, arguments_y_1d, argument_hat_1d, argument_hat_2d\n\n    def predict_triggers_demo(self, tokens_x_2d, head_indexes_2d, arguments_2d):\n        tokens_x_2d = torch.LongTensor(tokens_x_2d).to(self.device)\n        head_indexes_2d = torch.LongTensor(head_indexes_2d).to(self.device)\n\n        self.bert_model.eval()\n        with torch.no_grad():\n            output = self.bert_model(tokens_x_2d)\n            encoded_layers = output['last_hidden_state']\n\n        batch_size = tokens_x_2d.shape[0]\n        for i in range(batch_size):\n            encoded_layers[i] = torch.index_select(encoded_layers[i], 0, head_indexes_2d[i])\n\n        trigger_logits = self.fc_trigger(encoded_layers)\n        trigger_hat_2d = trigger_logits.argmax(-1)\n\n        argument_hidden = []\n        for i in range(batch_size):\n            predicted_triggers = find_triggers([idx2trigger[trigger] for trigger in trigger_hat_2d[i].tolist()])\n            for predicted_trigger in predicted_triggers:\n                t_start, t_end, t_type_str = predicted_trigger\n                event_tensor = encoded_layers[i, t_start:t_end, ].mean(dim=0)\n                for j in range(len(candidates)):\n                    e_start, e_end = candidates[j]\n                    argument_hidden.append(torch.cat([event_tensor]))\n                    argument_keys.append((i, t_start, t_end, t_type_str, e_start, e_end))\n\n        return trigger_logits, triggers_y_2d, trigger_hat_2d, argument_hidden, argument_keys\n\n    def predict_arguments_demo(self,\n                          argument_hidden,\n                          argument_keys,\n                          arguments_2d):\n        argument_hidden = torch.stack(argument_hidden)\n        argument_logits = self.fc_argument(argument_hidden)\n        argument_hat_1d = argument_logits.argmax(-1)\n\n        arguments_y_1d = []\n        for i, t_start, t_end, t_type_str, \\\n            e_start, e_end in argument_keys:\n            arg_label = argument2idx[NONE]\n            if (t_start, t_end, t_type_str) in arguments_2d[i]['events']:\n                for (a_start, a_end, a_type_idx) in arguments_2d[i]['events'][(t_start, t_end, t_type_str)]:\n                    if e_start == a_start and e_end == a_end:\n                        # raise Exception\n                        arg_label = a_type_idx\n                        break\n            arguments_y_1d.append(arg_label)\n\n        arguments_y_1d = torch.LongTensor(arguments_y_1d).to(self.device)\n\n        batch_size = len(arguments_2d)\n        argument_hat_2d = [{'events': {}} for _ in range(batch_size)]\n        for (i, st, ed, event_type_str, e_st, e_ed), a_label in zip(argument_keys, argument_hat_1d.cpu().numpy()):\n            if a_label == argument2idx[NONE]:\n                continue\n            if (st, ed, event_type_str) not in argument_hat_2d[i]['events']:\n                argument_hat_2d[i]['events'][(st, ed, event_type_str)] = []\n            argument_hat_2d[i]['events'][(st, ed, event_type_str)].append((e_st, e_ed, a_label))\n\n        return argument_logits, arguments_y_1d, argument_hat_1d, argument_hat_2d", "\nclass Net_demo(nn.Module):\n    def __init__(self, trigger_size=None, argument_size=None, device=torch.device(\"cpu\"), hparams=None):\n        super().__init__()\n        self.bert_model = BertModel.from_pretrained(hparams.model_name).to(device)\n        self.bert_tokenizer = BertConfig.from_pretrained(hparams.model_name)\n        hidden_size = self.bert_tokenizer.hidden_size\n        self.fc1 = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size, bias=True),\n            nn.ReLU(),\n        ).to(device)\n        self.fc_trigger = nn.Sequential(\n            nn.Linear(hidden_size, trigger_size),\n        ).to(device)\n        self.fc_argument = nn.Sequential(\n            nn.Linear(hidden_size, argument_size),\n        ).to(device)\n        self.device = device\n\n    def predict_triggers(self, tokens_x_2d, head_indexes_2d, triggers_y_2d, arguments_2d):\n        tokens_x_2d = torch.LongTensor(tokens_x_2d).to(self.device)\n        triggers_y_2d = torch.LongTensor(triggers_y_2d).to(self.device)\n        head_indexes_2d = torch.LongTensor(head_indexes_2d).to(self.device)\n\n        if self.training:\n            self.bert_model.train()\n            output = self.bert_model(tokens_x_2d)\n            encoded_layers = output['last_hidden_state']\n\n        else:\n            self.bert_model.eval()\n            with torch.no_grad():\n                output = self.bert_model(tokens_x_2d)\n                encoded_layers = output['last_hidden_state']\n\n        batch_size = tokens_x_2d.shape[0]\n        for i in range(batch_size):\n            encoded_layers[i] = torch.index_select(encoded_layers[i], 0, head_indexes_2d[i])\n\n        trigger_logits = self.fc_trigger(encoded_layers)\n        trigger_hat_2d = trigger_logits.argmax(-1)\n\n        argument_hidden = []\n        argument_keys = []\n        for i in range(batch_size):\n            candidates = arguments_2d[i]['candidates']\n            golden_entity_tensors = {}\n\n            for j in range(len(candidates)):\n                e_start, e_end = candidates[j]\n                golden_entity_tensors[candidates[j]] = encoded_layers[i, e_start:e_end, ].mean(dim=0)\n\n            predicted_triggers = find_triggers([idx2trigger[trigger] for trigger in trigger_hat_2d[i].tolist()])\n            for predicted_trigger in predicted_triggers:\n                t_start, t_end, t_type_str = predicted_trigger\n                event_tensor = encoded_layers[i, t_start:t_end, ].mean(dim=0)\n                for j in range(len(candidates)):\n                    e_start, e_end = candidates[j]\n                    argument_hidden.append(torch.cat([event_tensor]))\n                    argument_keys.append((i, t_start, t_end, t_type_str, e_start, e_end))\n\n        return trigger_logits, triggers_y_2d, trigger_hat_2d, argument_hidden, argument_keys\n\n    def predict_arguments(self,\n                          argument_hidden,\n                          argument_keys,\n                          arguments_2d):\n        argument_hidden = torch.stack(argument_hidden)\n        argument_logits = self.fc_argument(argument_hidden)\n        argument_hat_1d = argument_logits.argmax(-1)\n\n        arguments_y_1d = []\n        for i, t_start, t_end, t_type_str, \\\n            e_start, e_end in argument_keys:\n            arg_label = argument2idx[NONE]\n            if (t_start, t_end, t_type_str) in arguments_2d[i]['events']:\n                for (a_start, a_end, a_type_idx) in arguments_2d[i]['events'][(t_start, t_end, t_type_str)]:\n                    if e_start == a_start and e_end == a_end:\n                        # raise Exception\n                        arg_label = a_type_idx\n                        break\n            arguments_y_1d.append(arg_label)\n\n        arguments_y_1d = torch.LongTensor(arguments_y_1d).to(self.device)\n\n        batch_size = len(arguments_2d)\n        argument_hat_2d = [{'events': {}} for _ in range(batch_size)]\n        for (i, st, ed, event_type_str, e_st, e_ed), a_label in zip(argument_keys, argument_hat_1d.cpu().numpy()):\n            if a_label == argument2idx[NONE]:\n                continue\n            if (st, ed, event_type_str) not in argument_hat_2d[i]['events']:\n                argument_hat_2d[i]['events'][(st, ed, event_type_str)] = []\n            argument_hat_2d[i]['events'][(st, ed, event_type_str)].append((e_st, e_ed, a_label))\n\n        return argument_logits, arguments_y_1d, argument_hat_1d, argument_hat_2d\n\n    def predict_triggers_demo(self, tokens_x_2d, head_indexes_2d, arguments_2d):\n        tokens_x_2d = torch.LongTensor(tokens_x_2d).to(self.device)\n        head_indexes_2d = torch.LongTensor(head_indexes_2d).to(self.device)\n\n        self.bert_model.eval()\n        with torch.no_grad():\n            output = self.bert_model(tokens_x_2d)\n            encoded_layers = output['last_hidden_state']\n\n        batch_size = tokens_x_2d.shape[0]\n        for i in range(batch_size):\n            encoded_layers[i] = torch.index_select(encoded_layers[i], 0, head_indexes_2d[i])\n\n        trigger_logits = self.fc_trigger(encoded_layers)\n        trigger_hat_2d = trigger_logits.argmax(-1)\n\n        argument_hidden = []\n        for i in range(batch_size):\n            predicted_triggers = find_triggers([idx2trigger[trigger] for trigger in trigger_hat_2d[i].tolist()])\n            for predicted_trigger in predicted_triggers:\n                t_start, t_end, t_type_str = predicted_trigger\n                event_tensor = encoded_layers[i, t_start:t_end, ].mean(dim=0)\n                for j in range(len(candidates)):\n                    e_start, e_end = candidates[j]\n                    argument_hidden.append(torch.cat([event_tensor]))\n                    argument_keys.append((i, t_start, t_end, t_type_str, e_start, e_end))\n\n        return trigger_logits, triggers_y_2d, trigger_hat_2d, argument_hidden, argument_keys\n\n    def predict_arguments_demo(self,\n                          argument_hidden,\n                          argument_keys,\n                          arguments_2d):\n        argument_hidden = torch.stack(argument_hidden)\n        argument_logits = self.fc_argument(argument_hidden)\n        argument_hat_1d = argument_logits.argmax(-1)\n\n        arguments_y_1d = []\n        for i, t_start, t_end, t_type_str, \\\n            e_start, e_end in argument_keys:\n            arg_label = argument2idx[NONE]\n            if (t_start, t_end, t_type_str) in arguments_2d[i]['events']:\n                for (a_start, a_end, a_type_idx) in arguments_2d[i]['events'][(t_start, t_end, t_type_str)]:\n                    if e_start == a_start and e_end == a_end:\n                        # raise Exception\n                        arg_label = a_type_idx\n                        break\n            arguments_y_1d.append(arg_label)\n\n        arguments_y_1d = torch.LongTensor(arguments_y_1d).to(self.device)\n\n        batch_size = len(arguments_2d)\n        argument_hat_2d = [{'events': {}} for _ in range(batch_size)]\n        for (i, st, ed, event_type_str, e_st, e_ed), a_label in zip(argument_keys, argument_hat_1d.cpu().numpy()):\n            if a_label == argument2idx[NONE]:\n                continue\n            if (st, ed, event_type_str) not in argument_hat_2d[i]['events']:\n                argument_hat_2d[i]['events'][(st, ed, event_type_str)] = []\n            argument_hat_2d[i]['events'][(st, ed, event_type_str)].append((e_st, e_ed, a_label))\n\n        return argument_logits, arguments_y_1d, argument_hat_1d, argument_hat_2d"]}
{"filename": "src/Event/utils.py", "chunked_list": ["import json\n\nmax_length = 400\n\n\ndef build_vocab(labels, BIO_tagging=True):\n    all_labels = [\"[PAD]\", 'O']\n    for label in labels:\n        if BIO_tagging:\n            all_labels.append('B-{}'.format(label))\n            all_labels.append('I-{}'.format(label))\n        else:\n            all_labels.append(label)\n    label2idx = {tag: idx for idx, tag in enumerate(all_labels)}\n    idx2label = {idx: tag for idx, tag in enumerate(all_labels)}\n\n    return all_labels, label2idx, idx2label", "\n\ndef calc_metric(y_true, y_pred):\n    \"\"\"\n    :param y_true: [(tuple), ...]\n    :param y_pred: [(tuple), ...]\n    :return:\n    \"\"\"\n    num_proposed = len(y_pred)\n    num_gold = len(y_true)\n\n    y_true_set = set(y_true)\n    num_correct = 0\n    for item in y_pred:\n        if item in y_true_set:\n            num_correct += 1\n\n    print('proposed: {}\\tcorrect: {}\\tgold: {}'.format(num_proposed, num_correct, num_gold))\n\n    if num_proposed != 0:\n        precision = num_correct / num_proposed\n    else:\n        precision = 1.0\n\n    if num_gold != 0:\n        recall = num_correct / num_gold\n    else:\n        recall = 1.0\n\n    if precision + recall != 0:\n        f1 = 2 * precision * recall / (precision + recall)\n    else:\n        f1 = 0\n\n    return precision, recall, f1", "\n\ndef find_triggers(labels):\n    \"\"\"\n    :param labels: ['B-Conflict:Attack', 'I-Conflict:Attack', 'O', 'B-Life:Marry']\n    :return: [(0, 2, 'Conflict:Attack'), (3, 4, 'Life:Marry')]\n    \"\"\"\n    result = []\n    labels = [label.split('-') for label in labels]\n\n    for i in range(len(labels)):\n        if labels[i][0] == 'B':\n            result.append([i, i + 1, labels[i][1]])\n\n    for item in result:\n        j = item[1]\n        while j < len(labels):\n            if labels[j][0] == 'I':\n                j = j + 1\n                item[1] = j\n            else:\n                break\n\n    return [tuple(item) for item in result]", "\n\n\ndef get_pre_dict(sentence_e_mentions,  # \u4e00\u4e2a\u53e5\u5b50\uff0c\u5305\u542b\u4e86\u591a\u4e2aevent\n                 sentence_before_len,\n                 context,\n                 words,\n                 data_new,\n                 sent_entity_mentions,\n                 entity_id_num_none=0, entity_id_num=0):\n    \"\"\"\n    sent_entity_mentions:  todo entity\u662f\u53ef\u4ee5\u8de8\u53e5\u7684\uff1f\n    \"\"\"\n    # label triggers and arguments\n    golden_event_mentions = []\n    for event in sentence_e_mentions:\n        trigger = {\"start\": event['trigger']['start'] - sentence_before_len,\n                   \"end\": event['trigger']['end'] - sentence_before_len,\n                   \"text\": event['trigger']['text']}\n        if trigger['start'] > max_length or trigger['end'] < 0:\n            continue\n        arguments = event['arguments']\n        args_list = []\n        for arg in arguments:\n            entity_id = arg['entity_id']\n            span = []\n            for entity in sent_entity_mentions:\n                if entity['id'] == entity_id:\n                    span = [entity['start'] - sentence_before_len, entity['end'] - sentence_before_len]\n                    break\n            if span:\n                args_dict = {\n                    \"role\": arg['role'],\n                    \"text\": arg['text'],\n                    \"start\": span[0],\n                    \"end\": span[1],\n                    # 'span': span\n                }\n                args_list.append(args_dict)\n        golden_event_mentions.append({\n            'trigger': trigger,\n            'arguments': args_list,\n            'event_type': event['event_type']\n        })\n\n    entity_mentions = []\n    for entity in sent_entity_mentions:\n        enti = {'entity_type': entity['entity_type'],\n                'start': entity['start'] - sentence_before_len,\n                'end': entity['end'] - sentence_before_len}\n        if enti['start'] > max_length or enti['end'] < 0:\n            continue\n        entity_mentions.append(enti)\n\n    data_dict = {\n        'sentence': context,\n        'words': words,\n        'golden_event_mentions': golden_event_mentions,\n        'golden_entity_mentions': entity_mentions\n    }\n    data_new.append(data_dict)\n    return entity_id_num_none, entity_id_num", "\n\ndef write_json(data, fn):\n    with open(fn, 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False)\n\n\ndef process(wiki_json, processed_json):\n    \"\"\"\n    return:\n        {'id': data_id, 'content': content, 'occur': type_occur, 'type': TYPE, 'triggers': triggers, 'index': index,\n        'args': trigger_args[trigger_str]}\n        todo \u5982\u4f55\u52a0\u4e0aargs\uff0c\u5173\u952e\u662f\u5728arg_span\u4e0a\uff0c\u6839\u636eentity_id\u8fdb\u884c\u5bf9\u5e94\n    \"\"\"\n    data_new = []\n    entity_id_num_none, entity_id_num = 0, 0\n    # source: train:22/4542, dev:2/428 , test:3/566\n    # info: train:1151/4413, dev:99/411 , test:152/556\n    with open(wiki_json, 'r') as wiki:\n        lines = wiki.readlines()\n        for line in lines:\n            record = json.loads(line)\n            idx2sentence = {idx: tag for idx, tag in enumerate(record['sentences'])}\n            event_mentions = record['event_mentions']\n            entity_mentions = record['entity_mentions']\n            # todo sentence_entity_mentions\n            for sent_idx in range(len(idx2sentence)):\n                sentence_before_len = 0\n                for k in range(sent_idx):\n                    sentence_before_len += len(idx2sentence[k][0])\n\n                sentence_e_mentions = []\n                sent_entity_mentions = []\n                # \u4e00\u4e2a\u53e5\u5b50\u591a\u4e2aevent_mention,\u52a0\u5165\u5230\u4e00\u4e2alist\u91cc\n                for e_mention in event_mentions:\n                    if e_mention['trigger']['sent_idx'] == sent_idx:\n                        sentence_e_mentions.append(e_mention)\n                for entity_men in entity_mentions:\n                    if entity_men['sent_idx'] == sent_idx:\n                        sent_entity_mentions.append(entity_men)\n                sentence_words_with_span = idx2sentence[sent_idx][0]\n                words = [w[0] for w in sentence_words_with_span]\n                context = idx2sentence[sent_idx][1]\n                if len(idx2sentence[sent_idx][0]) >= max_length:\n                    # continue\n                    split_nums = len(idx2sentence[sent_idx][0]) // max_length + 1\n                    for s_num in range(split_nums):\n                        if s_num < split_nums - 1:\n                            split_words = words[s_num * max_length: (s_num + 1) * max_length]\n                            split_context = ' '.join(w for w in split_words)\n                        else:\n                            split_words = words[s_num * max_length:]\n                            split_context = ' '.join(w for w in split_words)\n                            # entity_id_num_none, entity_id_num = \\\n                        get_pre_dict(sentence_e_mentions=sentence_e_mentions,\n                                     sentence_before_len=sentence_before_len,\n                                     context=split_context, words=split_words, data_new=data_new,\n                                     sent_entity_mentions=sent_entity_mentions,\n                                     # entity_id_num_none=entity_id_num_none, entity_id_num=entity_id_num\n                                     )\n                        sentence_before_len += max_length\n                else:  # \u4e0d\u957f\u7684\u53e5\u5b50\n                    # label all occurrence types\n                    # entity_id_num_none, entity_id_num = \\\n                    get_pre_dict(sentence_e_mentions=sentence_e_mentions,\n                                 sentence_before_len=sentence_before_len, context=context,\n                                 words=words,\n                                 data_new=data_new, sent_entity_mentions=sent_entity_mentions,\n                                 # entity_id_num_none=entity_id_num_none, entity_id_num=entity_id_num\n                                 )\n    x = 0\n\n    with open(processed_json, 'w', encoding='utf-8') as f:\n        for line in data_new:\n            line = json.dumps(line, ensure_ascii=False)\n            f.write(line + '\\n')", "\n\ndef process_test(wiki_json, processed_json):\n    \"\"\"\n    return:\n        {'id': data_id,\n        'content': content,\n        events->dict_list:\n        [{'type': event_type, 'triggers': {\"span\":[start, end], \"word\": text},\n         {'type':,'trigger':,}...}, {}...]}\n        'args': trigger_args[trigger_str]}  \u8fd9\u4e2aargument\u4e5f\u53ef\u4ee5\u52a0\u4e0a\u8bd5\u8bd5\u6548\u679c\u3002\n    \"\"\"\n    data_new = []\n    with open(wiki_json, 'r') as wiki:\n        lines = wiki.readlines()\n        for line in lines:\n            suf_id = 0  # \u52a0\u5728doc_id\u7684\u540e\u7f00\n            record = json.loads(line)\n            data_id = record['doc_id']\n            idx2sentence = {idx: tag for idx, tag in enumerate(record['sentences'])}\n            event_mentions = record['event_mentions']\n            # \u540c\u4e00\u4e2asent_idx\u805a\u96c6\u5230\u4e00\u8d77\n            for sent_idx in range(len(idx2sentence)):\n                sentence_before_len = 0\n                for k in range(sent_idx):\n                    sentence_before_len += len(idx2sentence[k][0])\n\n                sentence_e_mentions = []\n                # \u4e00\u4e2a\u53e5\u5b50\u591a\u4e2aevent_mention,\u52a0\u5165\u5230\u4e00\u4e2alist\u91cc\n                for e_mention in event_mentions:\n                    if e_mention['trigger']['sent_idx'] == sent_idx:\n                        sentence_e_mentions.append(e_mention)\n                context = idx2sentence[sent_idx][1]\n                sentence_words_with_span = idx2sentence[sent_idx][0]\n                words = [w[0] for w in sentence_words_with_span]\n\n                # # label triggers and arguments\n                event_dict_list = []\n                for sent_event in sentence_e_mentions:\n                    # get event_dict_list\n                    tri_span = [sent_event['trigger']['start'] - sentence_before_len,\n                                sent_event['trigger']['end'] - sentence_before_len]\n                    event_dict = {'type': sent_event['event_type'],\n                                  'triggers': {\"span\": tri_span}, \"word\": sent_event['trigger']['text']}\n                    event_dict_list.append(event_dict)\n                data_new_dict = {'id': data_id + '_' + str(suf_id), 'content': context,\n                                 'words': words, 'events': event_dict_list}\n                data_new.append(data_new_dict)\n                suf_id += 1\n    x = data_new\n    with open(processed_json, 'w', encoding='utf-8') as f:\n        for line in data_new:\n            line = json.dumps(line, ensure_ascii=False)\n            f.write(line + '\\n')", "\n\ndef get_event_type(wiki_json, e_type):\n    \"\"\"\n    read wikiEvents,\n    return:\n        event_type_list\n        event_type2arg_role\n        shared_args\n    \"\"\"\n    with open(wiki_json, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            item = json.loads(line)\n            event_mentions = item['event_mentions']\n            for e_mention in event_mentions:\n                e_type.add(e_mention['event_type'])\n    return e_type", "\n\ndef get_argument_role(wiki_json, arguments=None):\n    if arguments is None:\n        arguments = set()\n    with open(wiki_json, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            item = json.loads(line)\n            event_mentions = item['event_mentions']\n            for e_mention in event_mentions:\n                for i in range(len(e_mention['arguments'])):\n                    arguments.add(e_mention['arguments'][i]['role'])\n    return arguments", "\n\ndef get_entity_type(wiki_json, entities=None, mention_type=None):\n    if mention_type is None:\n        mention_type = set()\n    if entities is None:\n        entities = set()\n    with open(wiki_json, 'r') as f:\n        lines = f.readlines()\n        for line in lines:\n            item = json.loads(line)\n            entity_mentions = item['entity_mentions']\n            for e_mention in entity_mentions:\n                entities.add(e_mention['entity_type'])\n                mention_type.add(e_mention['mention_type'])\n    return entities, mention_type", "\n\ndef get_entity_type_ace(ace_json, entities=None):\n    if entities is None:\n        entities = set()\n    with open(ace_json, 'r') as f:\n        lines = json.load(f)\n        for line in lines:\n            entity_mentions = line['golden-entity-mentions']\n            for e_mention in entity_mentions:\n                entities.add(e_mention['entity_type'])\n    return entities", "\n\ndef get_event_type_ace(ace_json, e_type):\n    with open(ace_json, 'r') as f:\n        for line in f:\n            item = json.loads(line)\n        lines = json.load(f)\n        for line in lines:\n            event_mentions = line['golden-event-mentions']\n            for e_mention in event_mentions:\n                e_type.add(e_mention['event_type'])\n    return e_type", "\n\ndef get_argument_role_ace(ace_json, arguments=None):\n    if arguments is None:\n        arguments = set()\n    with open(ace_json, 'r') as f:\n        lines = json.load(f)\n        for line in lines:\n            event_mentions = line['golden-event-mentions']\n            for e_mention in event_mentions:\n                for i in range(len(e_mention['arguments'])):\n                    arguments.add(e_mention['arguments'][i]['role'])\n    return arguments", "\n\n\n"]}
{"filename": "src/Event/eval.py", "chunked_list": ["import os\n\nimport torch\nfrom torch.utils import data\n\nfrom data_load import Dataset, pad\nfrom params import get_hparams\nfrom train_eval import evaluate\n\nif __name__ == \"__main__\":\n    hp = get_hparams()\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = torch.load(f'{hp.model_save_path}/best_model_12.pt')\n\n    if device == 'cuda':\n        model = model.cuda()\n\n    test_dataset = Dataset(hp.test_set)\n\n    test_iter = data.DataLoader(dataset=test_dataset,\n                                batch_size=hp.batch_size,\n                                shuffle=False,\n                                num_workers=4,\n                                collate_fn=pad)\n    dev_dataset = Dataset(hp.dev_set)\n\n    dev_iter = data.DataLoader(dataset=dev_dataset,\n                               batch_size=hp.batch_size,\n                               shuffle=False,\n                               num_workers=4,\n                               collate_fn=pad)\n    if not os.path.exists(hp.logdir):\n        os.makedirs(hp.logdir)\n\n    print(f\"=========eval test=========\")\n    evaluate(model, test_iter, 'eval_test')\n\n    print(f\"=========eval dev=========\")\n    evaluate(model, dev_iter, 'eval_dev')", "\nif __name__ == \"__main__\":\n    hp = get_hparams()\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = torch.load(f'{hp.model_save_path}/best_model_12.pt')\n\n    if device == 'cuda':\n        model = model.cuda()\n\n    test_dataset = Dataset(hp.test_set)\n\n    test_iter = data.DataLoader(dataset=test_dataset,\n                                batch_size=hp.batch_size,\n                                shuffle=False,\n                                num_workers=4,\n                                collate_fn=pad)\n    dev_dataset = Dataset(hp.dev_set)\n\n    dev_iter = data.DataLoader(dataset=dev_dataset,\n                               batch_size=hp.batch_size,\n                               shuffle=False,\n                               num_workers=4,\n                               collate_fn=pad)\n    if not os.path.exists(hp.logdir):\n        os.makedirs(hp.logdir)\n\n    print(f\"=========eval test=========\")\n    evaluate(model, test_iter, 'eval_test')\n\n    print(f\"=========eval dev=========\")\n    evaluate(model, dev_iter, 'eval_dev')", ""]}
{"filename": "src/Event/demo_event.py", "chunked_list": ["import nltk\nimport numpy as np\nimport torch\nimport gradio as gr\nfrom consts import CLS, SEP, NONE, max_length\n# from consts import test_event1, test_event2,\nfrom data_load import tokenizer, trigger2idx, pad, idx2trigger\nfrom params import get_hparams\nfrom torch.utils import data\n", "from torch.utils import data\n\nfrom utils import find_triggers\n\n\nclass Data_input(data.Dataset):\n    def __init__(self, document):\n        self.sent_li = []\n        self.entities_li = []\n        self.postags_li = []\n        self.triggers_li = []\n        self.arguments_li = []\n        self.entities_li = []\n\n        words = nltk.word_tokenize(document)\n        split_num = len(words) // max_length\n        for i in range(split_num + 1):\n            if i < split_num:\n                w = words[i * max_length: (i + 1) * max_length]\n            else:\n                w = words[i * max_length:]\n            triggers = [NONE] * len(w)\n            arguments = {\n                'candidates': [],\n                'events': {},\n            }\n            self.sent_li.append([CLS] + w + [SEP])\n            self.triggers_li.append(triggers)\n            self.arguments_li.append(arguments)\n\n    def __len__(self):\n        return len(self.sent_li)\n\n    def __getitem__(self, idx):\n        words = self.sent_li[idx]\n        triggers = self.triggers_li[idx]\n        arguments = self.arguments_li[idx]\n\n        # We give credits only to the first piece.\n        tokens_x = []\n        is_heads = []\n        for w in words:\n            tokens = tokenizer.tokenize(w) if w not in [CLS, SEP] else [w]\n            tokens_xx = tokenizer.convert_tokens_to_ids(tokens)\n\n            if w in [CLS, SEP]:\n                is_head = [0]\n            else:\n                is_head = [1] + [0] * (len(tokens) - 1)\n            tokens_x.extend(tokens_xx)\n            is_heads.extend(is_head)\n\n        triggers_y = [trigger2idx[t] for t in triggers]\n        head_indexes = []\n        for i in range(len(is_heads)):\n            if is_heads[i]:\n                head_indexes.append(i)\n\n        seqlen = len(tokens_x)\n\n        return tokens_x, triggers_y, arguments, seqlen, head_indexes, words, triggers\n\n    def get_samples_weight(self):\n        samples_weight = []\n        for triggers in self.triggers_li:\n            not_none = False\n            for trigger in triggers:\n                if trigger != NONE:\n                    not_none = True\n                    break\n            if not_none:\n                samples_weight.append(5.0)\n            else:\n                samples_weight.append(1.0)\n        return np.array(samples_weight)", "\n\ndef extract_events(user_input):\n    hp = get_hparams()\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = torch.load(f'{hp.model_save_path}/best_model_11.pt')\n\n    if device == 'cuda':\n        model = model.cuda()\n    if hasattr(model, 'module'):\n        model = model.module\n    model.eval()\n\n    data_input = Data_input(user_input)\n    data_iter = data.DataLoader(dataset=data_input,\n                                batch_size=len(data_input.triggers_li),\n                                shuffle=False,\n                                num_workers=4,\n                                collate_fn=pad)\n    words_all = []\n    triggers_all = []\n    triggers_hat_all = []\n    arguments_all = []\n    arguments_hat_all = []\n    # if direct:\n    #     return test_event2\n    with torch.no_grad():\n        for i, batch in enumerate(data_iter):\n            tokens_x_2d, triggers_y_2d, arguments_2d, seqlens_1d, head_indexes_2d, words_2d, triggers_2d = batch\n\n            trigger_logits, triggers_y_2d, trigger_hat_2d, argument_hidden, argument_keys \\\n                = model.predict_triggers(tokens_x_2d=tokens_x_2d, head_indexes_2d=head_indexes_2d,\n                                         triggers_y_2d=triggers_y_2d, arguments_2d=arguments_2d)\n\n        words_all.extend(words_2d)\n        triggers_all.extend(triggers_2d)\n        triggers_hat_all.extend(trigger_hat_2d.cpu().numpy().tolist())\n        arguments_all.extend(arguments_2d)\n\n        if len(argument_keys) > 0:\n            argument_logits, arguments_y_1d, argument_hat_1d, argument_hat_2d = \\\n                model.predict_arguments(argument_hidden, argument_keys, arguments_2d)\n            arguments_hat_all.extend(argument_hat_2d)\n        else:\n            batch_size = len(arguments_2d)\n            argument_hat_2d = [{'events': {}} for _ in range(batch_size)]\n            arguments_hat_all.extend(argument_hat_2d)\n    triggers_pred = []\n    arguments_pred = []\n    events = []\n    for i, (words, triggers, triggers_hat, arguments, arguments_hat) \\\n            in enumerate(zip(words_all, triggers_all, triggers_hat_all, arguments_all, arguments_hat_all)):\n        triggers_hat = triggers_hat[:len(words)]\n        triggers_hat = [idx2trigger[hat] for hat in triggers_hat]\n\n        # [(ith sentence, t_start, t_end, t_type_str)]\n        triggers_pred.extend([(i, *item) for item in find_triggers(triggers_hat)])\n\n        for trigger in arguments_hat['events']:\n            t_start, t_end, t_type_str = trigger\n            for argument in arguments_hat['events'][trigger]:\n                a_start, a_end, a_type_idx = argument\n                arguments_pred.append((i, t_start, t_end, t_type_str, a_start, a_end, a_type_idx))\n        event = {\n            'trigger': triggers_pred,\n            'argument': arguments_pred,\n        }\n        events.append(event)\n\n    return events", "\n\nif __name__ == \"__main__\":\n    document1 = 'As of early Tuesday there was no claim of responsibility. Prayuth Chan-ocha, the head of Thailand\\u2019s military government, said that the authorities were searching for a person seen on closed-circuit footage but that it was not clear who the person was, news agencies reported.A spokesman for the police, Lt. Gen. Prawut Thavornsiri, told a Thai television interviewer that \\u201cwe haven\\u2019t concluded anything.\\u201d The authorities said they were reviewing footage from 15 security cameras in the area but that the rush-hour crowds made deciphering the video difficult.\\u201cThe shrine was very crowded,\\u201d General Prawut said. \\u201cIt\\u2019s not clear even looking at the CCTV footage.\\u201dThe bomb, General Prawut said, was placed under a bench on the outer rim of the shrine\\u2019s grounds. Initially, the police said they had discovered at least two additional devices that they suspected were unexploded bombs inside the shrine and said other bombs may have been placed in the area, yelling at bystanders: \\u201cGet out! Get out!\\u201d'\n    document2 = \"A settlement has been reached in a $1-million lawsuit filed by a taxi driver accusing police of negligence after he got caught up in the August 2016 take-down of ISIS-sympathizer Aaron Driver.READ MORE: FBI agent whose tip thwarted 2016 ISIS attack in Ontario says he was glad to helpTerry Duffield was injured when Driver detonated a homemade explosive in the back of his cab in August 2016.\\u201cI have to be very careful because there is an agreement to not disclose any of the terms of the settlement,\\u201d Duffield\\u2019s lawyer Kevin Egan told 980 CFPL.\\u201cThe statement of claim, I guess, speaks for itself in regard to what we alleged.\\u201dWATCH: Ontario taxi driver files $1M lawsuit against police2:18 Ontario taxi driver files $1M lawsuit against police Ontario taxi driver files $1M lawsuit against policeThat statement of claim, which Global News obtained a copy of in late March 2018, said police had more than enough time to intervene before Driver got into Duffield\\u2019s taxi. The Attorney General of Canada, the Ontario government, Strathroy-Caradoc Police Service and London Police Service were named as defendants.Story continues below advertisementOn the morning of Aug. 10, 2016, U.S. authorities notified the RCMP they had detected a so-called martyrdom video in which a Canadian man said he was about to conduct an attack.The RCMP identified the man in the video as Driver and a tactical team surrounded his house in Strathroy.At 3:45 p.m., Driver called for a cab to take him to Citi Plaza in London. The claim alleged that despite the police presence, Duffield was not stopped from pulling into Driver\\u2019s driveway. Driver then came out of the house and got into the back seat of the cab.\\u201cWhen the SWAT team approached the vehicle, [Duffield] turned to Mr. Driver and said, \\u2018I think they\\u2019re here to talk to you\\u2019 and he leaned over to get his cigarettes, it\\u2019s a bench seat in the front of the taxicab, as he put his head down below the bench seat, the bomb went off.\\u201dStory continues below advertisementThe inside of the cab where Aaron Driver detonated an explosive device on Aug. 10, 2016. Handout / RCMPEgan said Duffield had a preexisting back injury and the bomb blast triggered recurring pain. He also noted that his client was psychologically impacted by the event and is no longer able to work as a taxi driver.\\u201cHe did try it. Got in a vehicle, turned the key on and started to shake and sweat and got out of the vehicle and vomited,\\u201d said Egan. Tweet This\\u201cHe was so traumatized by the event. He realized that any time any potential passenger was approaching the vehicle with a package he would be hyper-vigilant about that and just couldn\\u2019t handle it emotionally.\\u201dDetails of the settlement will not be made public, but Egan noted that no amount of money can properly compensate someone for physical or psychological injuries, but \\u201cis the best we can do in the circumstance.\\u201d He also noted that, while he was unwilling to disclose too much of Duffield\\u2019s personal health, he has received some counselling and is \\u201ccoping better now than he was then.\\u201dStory continues below advertisement\\u2013 with files from Stewart Bell and Andrew Russell.\"\n    input_text = gr.inputs.Textbox(lines=10, label=\"Input Text\")\n    output_text = gr.outputs.Textbox(label=\"Output\")\n\n    gr.Interface(fn=extract_events, inputs=input_text, outputs=output_text, title=\"Event Extraction\",\n                 description=\"Enter some text and the model will extract events.\").launch(share=True)", "    # # todo for debug\n    # events = extract_events(document2)\n    # print(\"Triggers and Arguments:\")\n    # for event in events:\n    #     print(f\"Trigger: {event['trigger']}\")\n    #     print(\"Arguments:\")\n    #     if event['argument']:\n    #         for argument in event['argument']:\n    #             print(f\"\\t{argument}\")\n    #     else:", "    #             print(f\"\\t{argument}\")\n    #     else:\n    #         print('argument is None')\n    # while True:\n    #     user_input = input(\"Enter a sentence: \")\n    #     if user_input == \"quit\":\n    #         break\n    #     elif user_input == 'None':\n    #         events = extract_events(document2)\n    #     else:", "    #         events = extract_events(document2)\n    #     else:\n    #         events = extract_events(user_input)\n    #     print(\"Triggers and Arguments:\")\n    #     for event in events:\n    #         print(f\"Trigger: {event['trigger']}\")\n    #         print(\"Arguments:\")\n    #         for argument in event['arguments']:\n    #             print(f\"\\t{argument}\")\n", "    #             print(f\"\\t{argument}\")\n"]}
{"filename": "src/Event/data_load.py", "chunked_list": ["import numpy as np\nfrom torch.utils import data\nimport json\n\nfrom transformers import BertTokenizer\n\n# init vocab\nfrom consts import TRIGGERS, ARGUMENTS, NONE, SEP, CLS, PAD, UNK\nfrom params import get_hparams\nfrom utils import build_vocab", "from params import get_hparams\nfrom utils import build_vocab\n\nall_triggers, trigger2idx, idx2trigger = build_vocab(TRIGGERS)\nall_arguments, argument2idx, idx2argument = build_vocab(ARGUMENTS, BIO_tagging=False)\nhparams = get_hparams()\ntokenizer = BertTokenizer.from_pretrained(hparams.model_name, do_lower_case=False, never_split=(PAD, CLS, SEP, UNK))\n\n\nclass Dataset(data.Dataset):\n    def __init__(self, fpath):\n        self.sent_li = []\n        self.entities_li = []\n        self.postags_li = []\n        self.triggers_li = []\n        self.arguments_li = []\n        self.entities_li = []\n\n        with open(fpath, 'r') as f:\n            lines = f.readlines()\n            for line in lines:\n                item = json.loads(line)\n                words = item['words']\n                triggers = [NONE] * len(words)\n                arguments = {\n                    'candidates': [\n                        # ex. (5, 6, \"entity_type_str\"), ...\n                    ],\n                    'events': {\n                        # ex. (1, 3, \"trigger_type_str\"): [(5, 6, \"argument_role_idx\"), ...]\n                    },\n                }\n                for entity_mention in item['golden_entity_mentions']:\n                    arguments['candidates'].append(\n                        (entity_mention['start'], entity_mention['end']))\n                for event_mention in item['golden_event_mentions']:\n                    for i in range(event_mention['trigger']['start'], event_mention['trigger']['end']):\n                        trigger_type = event_mention['event_type']\n                        if i == event_mention['trigger']['start']:\n                            triggers[i] = 'B-{}'.format(trigger_type)\n                        else:\n                            triggers[i] = 'I-{}'.format(trigger_type)\n\n                    event_key = (event_mention['trigger']['start'], event_mention['trigger']['end'], event_mention['event_type'])\n                    arguments['events'][event_key] = []\n                    for argument in event_mention['arguments']:\n                        role = argument['role']\n                        arguments['events'][event_key].append((argument['start'], argument['end'], argument2idx[role]))\n\n                self.sent_li.append([CLS] + words + [SEP])\n                self.triggers_li.append(triggers)\n                self.arguments_li.append(arguments)\n\n    def __len__(self):\n        return len(self.sent_li)\n\n    def __getitem__(self, idx):\n        words = self.sent_li[idx]\n        triggers = self.triggers_li[idx]\n        arguments = self.arguments_li[idx]\n\n        # We give credits only to the first piece.\n        tokens_x = []\n        is_heads = []\n        for w in words:\n            tokens = tokenizer.tokenize(w) if w not in [CLS, SEP] else [w]\n            tokens_xx = tokenizer.convert_tokens_to_ids(tokens)\n\n            if w in [CLS, SEP]:\n                is_head = [0]\n            else:\n                is_head = [1] + [0] * (len(tokens) - 1)\n            tokens_x.extend(tokens_xx)\n            is_heads.extend(is_head)\n\n        triggers_y = [trigger2idx[t] for t in triggers]\n        head_indexes = []\n        for i in range(len(is_heads)):\n            if is_heads[i]:\n                head_indexes.append(i)\n\n        seqlen = len(tokens_x)\n\n        return tokens_x, triggers_y, arguments, seqlen, head_indexes, words, triggers\n\n    def get_samples_weight(self):\n        samples_weight = []\n        for triggers in self.triggers_li:\n            not_none = False\n            for trigger in triggers:\n                if trigger != NONE:\n                    not_none = True\n                    break\n            if not_none:\n                samples_weight.append(5.0)\n            else:\n                samples_weight.append(1.0)\n        return np.array(samples_weight)", "\nclass Dataset(data.Dataset):\n    def __init__(self, fpath):\n        self.sent_li = []\n        self.entities_li = []\n        self.postags_li = []\n        self.triggers_li = []\n        self.arguments_li = []\n        self.entities_li = []\n\n        with open(fpath, 'r') as f:\n            lines = f.readlines()\n            for line in lines:\n                item = json.loads(line)\n                words = item['words']\n                triggers = [NONE] * len(words)\n                arguments = {\n                    'candidates': [\n                        # ex. (5, 6, \"entity_type_str\"), ...\n                    ],\n                    'events': {\n                        # ex. (1, 3, \"trigger_type_str\"): [(5, 6, \"argument_role_idx\"), ...]\n                    },\n                }\n                for entity_mention in item['golden_entity_mentions']:\n                    arguments['candidates'].append(\n                        (entity_mention['start'], entity_mention['end']))\n                for event_mention in item['golden_event_mentions']:\n                    for i in range(event_mention['trigger']['start'], event_mention['trigger']['end']):\n                        trigger_type = event_mention['event_type']\n                        if i == event_mention['trigger']['start']:\n                            triggers[i] = 'B-{}'.format(trigger_type)\n                        else:\n                            triggers[i] = 'I-{}'.format(trigger_type)\n\n                    event_key = (event_mention['trigger']['start'], event_mention['trigger']['end'], event_mention['event_type'])\n                    arguments['events'][event_key] = []\n                    for argument in event_mention['arguments']:\n                        role = argument['role']\n                        arguments['events'][event_key].append((argument['start'], argument['end'], argument2idx[role]))\n\n                self.sent_li.append([CLS] + words + [SEP])\n                self.triggers_li.append(triggers)\n                self.arguments_li.append(arguments)\n\n    def __len__(self):\n        return len(self.sent_li)\n\n    def __getitem__(self, idx):\n        words = self.sent_li[idx]\n        triggers = self.triggers_li[idx]\n        arguments = self.arguments_li[idx]\n\n        # We give credits only to the first piece.\n        tokens_x = []\n        is_heads = []\n        for w in words:\n            tokens = tokenizer.tokenize(w) if w not in [CLS, SEP] else [w]\n            tokens_xx = tokenizer.convert_tokens_to_ids(tokens)\n\n            if w in [CLS, SEP]:\n                is_head = [0]\n            else:\n                is_head = [1] + [0] * (len(tokens) - 1)\n            tokens_x.extend(tokens_xx)\n            is_heads.extend(is_head)\n\n        triggers_y = [trigger2idx[t] for t in triggers]\n        head_indexes = []\n        for i in range(len(is_heads)):\n            if is_heads[i]:\n                head_indexes.append(i)\n\n        seqlen = len(tokens_x)\n\n        return tokens_x, triggers_y, arguments, seqlen, head_indexes, words, triggers\n\n    def get_samples_weight(self):\n        samples_weight = []\n        for triggers in self.triggers_li:\n            not_none = False\n            for trigger in triggers:\n                if trigger != NONE:\n                    not_none = True\n                    break\n            if not_none:\n                samples_weight.append(5.0)\n            else:\n                samples_weight.append(1.0)\n        return np.array(samples_weight)", "\n\ndef pad(batch):\n    tokens_x_2d, triggers_y_2d, arguments_2d, seqlens_1d, head_indexes_2d, words_2d, triggers_2d = list(map(list, zip(*batch)))\n    maxlen = np.array(seqlens_1d).max()\n\n    for i in range(len(tokens_x_2d)):\n        tokens_x_2d[i] = tokens_x_2d[i] + [0] * (maxlen - len(tokens_x_2d[i]))\n        head_indexes_2d[i] = head_indexes_2d[i] + [0] * (maxlen - len(head_indexes_2d[i]))\n        triggers_y_2d[i] = triggers_y_2d[i] + [trigger2idx[PAD]] * (maxlen - len(triggers_y_2d[i]))\n\n    return tokens_x_2d, \\\n           triggers_y_2d, arguments_2d, \\\n           seqlens_1d, head_indexes_2d, \\\n           words_2d, triggers_2d", ""]}
{"filename": "src/Entity/run_eval.py", "chunked_list": ["import argparse\n\nfrom shared.data_structures import Dataset, evaluate_predictions\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--prediction_file', type=str, default=None, required=True)\n    args = parser.parse_args()\n\n    data = Dataset(args.prediction_file)\n    eval_result = evaluate_predictions(data)\n    print('Evaluation result %s' % args.prediction_file)\n    print('NER - P: %f, R: %f, F1: %f' % (\n        eval_result['ner']['precision'], eval_result['ner']['recall'], eval_result['ner']['f1']))\n    print('REL - P: %f, R: %f, F1: %f' % (\n        eval_result['relation']['precision'], eval_result['relation']['recall'], eval_result['relation']['f1']))\n    print('REL (strict) - P: %f, R: %f, F1: %f' % (\n        eval_result['strict_relation']['precision'], eval_result['strict_relation']['recall'],\n        eval_result['strict_relation']['f1']))", ""]}
{"filename": "src/Entity/run_entity.py", "chunked_list": ["import json\nimport logging\nimport os\nimport random\nimport sys\nimport time\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AdamW, get_linear_schedule_with_warmup", "from tqdm import tqdm\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom entity.models import EntityModel\nfrom entity.utils import convert_dataset_to_samples, batchify, NpEncoder\nfrom shared.const import task_ner_labels, get_labelmap\nfrom shared.data_structures import Dataset\nfrom shared.get_hparams import get_hparams_entity\n\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',", "\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n                    datefmt='%m/%d/%Y %H:%M:%S',\n                    level=logging.INFO)\nlogger = logging.getLogger('root')\n\n\ndef save_model(model, args):\n    \"\"\"\n    Save the model to the output directory\n    \"\"\"\n    logger.info('Saving model to %s...' % (args.output_dir))\n    model_to_save = model.bert_model.module if hasattr(model.bert_model, 'module') else model.bert_model\n    model_to_save.save_pretrained(args.output_dir)\n    model.tokenizer.save_pretrained(args.output_dir)", "\n\ndef output_ner_predictions(model, batches, dataset, output_file):\n    \"\"\"\n    Save the prediction as a json file\n    \"\"\"\n    ner_result = {}\n    span_hidden_table = {}\n    tot_pred_ett = 0\n    for i in range(len(batches)):\n        output_dict = model.run_batch(batches[i], training=False)\n        pred_ner = output_dict['pred_ner']\n        for sample, preds in zip(batches[i], pred_ner):\n            off = sample['sent_start_in_doc'] - sample['sent_start']\n            k = sample['doc_key'] + '-' + str(sample['sentence_ix'])\n            ner_result[k] = []\n            for span, pred in zip(sample['spans'], preds):\n                span_id = '%s::%d::(%d,%d)' % (sample['doc_key'], sample['sentence_ix'], span[0] + off, span[1] + off)\n                if pred == 0:\n                    continue\n                ner_result[k].append([span[0] + off, span[1] + off, ner_id2label[pred]])\n            tot_pred_ett += len(ner_result[k])\n\n    logger.info('Total pred entities: %d' % tot_pred_ett)\n\n    js = dataset.js\n    for i, doc in enumerate(js):\n        doc[\"predicted_ner\"] = []\n        doc[\"predicted_relations\"] = []\n        for j in range(len(doc[\"sentences\"])):\n            k = doc['doc_key'] + '-' + str(j)\n            if k in ner_result:\n                doc[\"predicted_ner\"].append(ner_result[k])\n            else:\n                logger.info('%s not in NER results!' % k)\n                doc[\"predicted_ner\"].append([])\n\n            doc[\"predicted_relations\"].append([])\n\n        js[i] = doc\n\n    logger.info('Output predictions to %s..' % (output_file))\n    with open(output_file, 'w') as f:\n        f.write('\\n'.join(json.dumps(doc, cls=NpEncoder) for doc in js))", "\n\ndef evaluate(model, batches, tot_gold):\n    \"\"\"\n    Evaluate the entity model\n    \"\"\"\n    logger.info('Evaluating...')\n    c_time = time.time()\n    cor = 0\n    tot_pred = 0\n    l_cor = 0\n    l_tot = 0\n\n    for i in range(len(batches)):\n        output_dict = model.run_batch(batches[i], training=False)\n        pred_ner = output_dict['pred_ner']\n        for sample, \\\n            preds in \\\n                zip(batches[i], pred_ner):\n            for gold, \\\n                pred in zip(sample['spans_label'], preds):\n                l_tot += 1\n                if pred == gold:\n                    l_cor += 1\n                if pred != 0 and gold != 0 and pred == gold:\n                    cor += 1\n                if pred != 0:\n                    tot_pred += 1\n\n    acc = l_cor / l_tot\n    logger.info('Accuracy: %5f' % acc)\n    logger.info('Cor: %d, Pred TOT: %d, Gold TOT: %d' % (cor, tot_pred, tot_gold))\n    p = cor / tot_pred if cor > 0 else 0.0\n    r = cor / tot_gold if cor > 0 else 0.0\n    f1 = 2 * (p * r) / (p + r) if cor > 0 else 0.0\n    logger.info('P: %.5f, R: %.5f, F1: %.5f' % (p, r, f1))\n    logger.info('Used time: %f' % (time.time() - c_time))\n    return f1", "\n\ndef setseed(seed):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\nif __name__ == '__main__':\n    hparams = get_hparams_entity()\n    hparams.train_data_path = os.path.join(hparams.data_dir, 'train.json')\n    hparams.dev_data_path = os.path.join(hparams.data_dir, 'dev.json')\n    hparams.test_data_path = os.path.join(hparams.data_dir, 'test.json')\n\n    if 'albert' in hparams.model:\n        logger.info('Use Albert: %s' % hparams.model)\n        hparams.use_albert = True\n\n    setseed(hparams.seed)\n\n    if not os.path.exists(hparams.output_dir):\n        os.makedirs(hparams.output_dir)\n\n    if hparams.do_train:\n        logger.addHandler(logging.FileHandler(os.path.join(hparams.output_dir, \"train.log\"), 'w'))\n    else:\n        logger.addHandler(logging.FileHandler(os.path.join(hparams.output_dir, \"eval.log\"), 'w'))\n\n    logger.info(sys.argv)\n    logger.info(hparams)\n\n    ner_label2id, ner_id2label = get_labelmap(task_ner_labels[hparams.task])\n\n    num_ner_labels = len(task_ner_labels[hparams.task]) + 1\n    model = EntityModel(hparams, num_ner_labels=num_ner_labels)\n\n    dev_data = Dataset(hparams.dev_data_path)\n    dev_samples, dev_ner = convert_dataset_to_samples(dev_data, hparams.max_span_length, ner_label2id=ner_label2id,\n                                                      context_window=hparams.context_window)\n    dev_batches = batchify(dev_samples, hparams.eval_batch_size)\n\n    if hparams.do_train:\n        train_data = Dataset(hparams.train_data_path)\n        train_samples, train_ner = convert_dataset_to_samples(train_data, hparams.max_span_length,\n                                                              ner_label2id=ner_label2id,\n                                                              context_window=hparams.context_window)\n        train_batches = batchify(train_samples, hparams.train_batch_size)\n        best_result = 0.0\n\n        param_optimizer = list(model.bert_model.named_parameters())\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer\n                        if 'bert' in n]},\n            {'params': [p for n, p in param_optimizer\n                        if 'bert' not in n], 'lr': hparams.task_learning_rate}]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=hparams.learning_rate, correct_bias=not hparams.bertadam)\n        t_total = len(train_batches) * hparams.num_epoch\n        scheduler = get_linear_schedule_with_warmup(optimizer, int(t_total * hparams.warmup_proportion), t_total)\n\n        tr_loss = 0\n        tr_examples = 0\n        global_step = 0\n        eval_step = len(train_batches) // hparams.eval_per_epoch\n        for epoch in tqdm(range(hparams.num_epoch)):\n            if hparams.train_shuffle:\n                random.shuffle(train_batches)\n            for i in tqdm(range(len(train_batches))):\n                output_dict = model.run_batch(train_batches[i], training=True)\n                loss = output_dict['ner_loss']\n                loss.backward()\n\n                tr_loss += loss.item()\n                tr_examples += len(train_batches[i])\n                global_step += 1\n\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n\n                if global_step % hparams.print_loss_step == 0:\n                    logger.info('Epoch=%d, iter=%d, loss=%.5f' % (epoch, i, tr_loss / tr_examples))\n                    tr_loss = 0\n                    tr_examples = 0\n\n                if global_step % eval_step == 0:\n                    f1 = evaluate(model, dev_batches, dev_ner)\n                    if f1 > best_result:\n                        best_result = f1\n                        logger.info('!!! Best valid (epoch=%d): %.2f' % (epoch, f1 * 100))\n                        save_model(model, hparams)\n\n    if hparams.do_eval:\n        hparams.bert_model_dir = hparams.output_dir\n        model = EntityModel(hparams, num_ner_labels=num_ner_labels)\n        eval_dev_data = Dataset(hparams.dev_data_path)\n        prediction_file = os.path.join(hparams.output_dir, hparams.dev_pred_filename)\n        eval_dev_samples, eval_dev_ner = convert_dataset_to_samples(eval_dev_data, hparams.max_span_length,\n                                                                    ner_label2id=ner_label2id,\n                                                                    context_window=hparams.context_window)\n        eval_dev_batches = batchify(eval_dev_samples, hparams.eval_batch_size)\n        evaluate(model, eval_dev_batches, eval_dev_ner)\n        output_ner_predictions(model, eval_dev_batches, eval_dev_data, output_file=prediction_file)\n        if hparams.eval_test:\n            eval_test_data = Dataset(hparams.test_data)\n            prediction_file = os.path.join(hparams.output_dir, hparams.test_pred_filename)\n            eval_test_samples, eval_test_ner = convert_dataset_to_samples(eval_test_data, hparams.max_span_length,\n                                                                          ner_label2id=ner_label2id,\n                                                                          context_window=hparams.context_window)\n            eval_test_batches = batchify(eval_test_samples, hparams.eval_batch_size)\n            evaluate(model, eval_test_batches, eval_test_ner)\n            output_ner_predictions(model, eval_test_batches, eval_test_data, output_file=prediction_file)", "\nif __name__ == '__main__':\n    hparams = get_hparams_entity()\n    hparams.train_data_path = os.path.join(hparams.data_dir, 'train.json')\n    hparams.dev_data_path = os.path.join(hparams.data_dir, 'dev.json')\n    hparams.test_data_path = os.path.join(hparams.data_dir, 'test.json')\n\n    if 'albert' in hparams.model:\n        logger.info('Use Albert: %s' % hparams.model)\n        hparams.use_albert = True\n\n    setseed(hparams.seed)\n\n    if not os.path.exists(hparams.output_dir):\n        os.makedirs(hparams.output_dir)\n\n    if hparams.do_train:\n        logger.addHandler(logging.FileHandler(os.path.join(hparams.output_dir, \"train.log\"), 'w'))\n    else:\n        logger.addHandler(logging.FileHandler(os.path.join(hparams.output_dir, \"eval.log\"), 'w'))\n\n    logger.info(sys.argv)\n    logger.info(hparams)\n\n    ner_label2id, ner_id2label = get_labelmap(task_ner_labels[hparams.task])\n\n    num_ner_labels = len(task_ner_labels[hparams.task]) + 1\n    model = EntityModel(hparams, num_ner_labels=num_ner_labels)\n\n    dev_data = Dataset(hparams.dev_data_path)\n    dev_samples, dev_ner = convert_dataset_to_samples(dev_data, hparams.max_span_length, ner_label2id=ner_label2id,\n                                                      context_window=hparams.context_window)\n    dev_batches = batchify(dev_samples, hparams.eval_batch_size)\n\n    if hparams.do_train:\n        train_data = Dataset(hparams.train_data_path)\n        train_samples, train_ner = convert_dataset_to_samples(train_data, hparams.max_span_length,\n                                                              ner_label2id=ner_label2id,\n                                                              context_window=hparams.context_window)\n        train_batches = batchify(train_samples, hparams.train_batch_size)\n        best_result = 0.0\n\n        param_optimizer = list(model.bert_model.named_parameters())\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer\n                        if 'bert' in n]},\n            {'params': [p for n, p in param_optimizer\n                        if 'bert' not in n], 'lr': hparams.task_learning_rate}]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=hparams.learning_rate, correct_bias=not hparams.bertadam)\n        t_total = len(train_batches) * hparams.num_epoch\n        scheduler = get_linear_schedule_with_warmup(optimizer, int(t_total * hparams.warmup_proportion), t_total)\n\n        tr_loss = 0\n        tr_examples = 0\n        global_step = 0\n        eval_step = len(train_batches) // hparams.eval_per_epoch\n        for epoch in tqdm(range(hparams.num_epoch)):\n            if hparams.train_shuffle:\n                random.shuffle(train_batches)\n            for i in tqdm(range(len(train_batches))):\n                output_dict = model.run_batch(train_batches[i], training=True)\n                loss = output_dict['ner_loss']\n                loss.backward()\n\n                tr_loss += loss.item()\n                tr_examples += len(train_batches[i])\n                global_step += 1\n\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n\n                if global_step % hparams.print_loss_step == 0:\n                    logger.info('Epoch=%d, iter=%d, loss=%.5f' % (epoch, i, tr_loss / tr_examples))\n                    tr_loss = 0\n                    tr_examples = 0\n\n                if global_step % eval_step == 0:\n                    f1 = evaluate(model, dev_batches, dev_ner)\n                    if f1 > best_result:\n                        best_result = f1\n                        logger.info('!!! Best valid (epoch=%d): %.2f' % (epoch, f1 * 100))\n                        save_model(model, hparams)\n\n    if hparams.do_eval:\n        hparams.bert_model_dir = hparams.output_dir\n        model = EntityModel(hparams, num_ner_labels=num_ner_labels)\n        eval_dev_data = Dataset(hparams.dev_data_path)\n        prediction_file = os.path.join(hparams.output_dir, hparams.dev_pred_filename)\n        eval_dev_samples, eval_dev_ner = convert_dataset_to_samples(eval_dev_data, hparams.max_span_length,\n                                                                    ner_label2id=ner_label2id,\n                                                                    context_window=hparams.context_window)\n        eval_dev_batches = batchify(eval_dev_samples, hparams.eval_batch_size)\n        evaluate(model, eval_dev_batches, eval_dev_ner)\n        output_ner_predictions(model, eval_dev_batches, eval_dev_data, output_file=prediction_file)\n        if hparams.eval_test:\n            eval_test_data = Dataset(hparams.test_data)\n            prediction_file = os.path.join(hparams.output_dir, hparams.test_pred_filename)\n            eval_test_samples, eval_test_ner = convert_dataset_to_samples(eval_test_data, hparams.max_span_length,\n                                                                          ner_label2id=ner_label2id,\n                                                                          context_window=hparams.context_window)\n            eval_test_batches = batchify(eval_test_samples, hparams.eval_batch_size)\n            evaluate(model, eval_test_batches, eval_test_ner)\n            output_ner_predictions(model, eval_test_batches, eval_test_data, output_file=prediction_file)", ""]}
{"filename": "src/Entity/demo_entity.py", "chunked_list": ["import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ntext = \"Apple is looking at buying U.K. startup for $1 billion\"\n\ndoc = nlp(text)\n\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)", "for ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)"]}
{"filename": "src/Entity/entity/models.py", "chunked_list": ["import logging\n\nimport torch\nimport torch.nn.functional as F\nfrom allennlp.modules import FeedForward\nfrom allennlp.nn.util import batched_index_select\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import AlbertTokenizer, AlbertPreTrainedModel, AlbertModel\nfrom transformers import BertTokenizer, BertPreTrainedModel, BertModel", "from transformers import AlbertTokenizer, AlbertPreTrainedModel, AlbertModel\nfrom transformers import BertTokenizer, BertPreTrainedModel, BertModel\n\nlogger = logging.getLogger('root')\n\n\nclass BertForEntity(BertPreTrainedModel):\n    def __init__(self, config, num_ner_labels, head_hidden_dim=150, width_embedding_dim=150, max_span_length=8):\n        super().__init__(config)\n        self.config = config\n        self.bert = BertModel(config)\n        self.hidden_dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.width_embedding = nn.Embedding(max_span_length + 1, width_embedding_dim)\n\n        self.ner_classifier = nn.Sequential(\n            FeedForward(input_dim=config.hidden_size * 2 + width_embedding_dim,\n                        num_layers=2,\n                        hidden_dims=head_hidden_dim,\n                        activations=F.relu,\n                        dropout=0.2),\n            nn.Linear(head_hidden_dim, num_ner_labels)\n        )\n\n        self.init_weights()  # \u8fd9\u91cc\u5e94\u8be5\u662f\u7f3a\u5931\u5b9a\u4e49\u4e86\uff0c\u6ca1\u6709\u8fd9\u6837\u7684\u51fd\u6570\n\n    def _get_span_embeddings(self, input_ids, spans, token_type_ids=None, attention_mask=None):\n        z = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n        # sequence_output0 = z.last_hidden_state\n        last_hidden_state = z[0]\n        sequence_output = self.hidden_dropout(last_hidden_state)\n\n        \"\"\"\n        spans: [batch_size, num_spans, 3]; 0: left_ned, 1: right_end, 2: width\n        spans_mask: (batch_size, num_spans, )\n        \"\"\"\n        spans_start = spans[:, :, 0].view(spans.size(0), -1)\n        spans_start_embedding = batched_index_select(sequence_output, spans_start)\n        spans_end = spans[:, :, 1].view(spans.size(0), -1)\n        spans_end_embedding = batched_index_select(sequence_output, spans_end)\n\n        spans_width = spans[:, :, 2].view(spans.size(0), -1)\n        spans_width_embedding = self.width_embedding(spans_width)\n\n        # Concatenate embeddings of left/right points and the width embedding\n        spans_embedding = torch.cat((spans_start_embedding, spans_end_embedding, spans_width_embedding), dim=-1)\n        \"\"\"\n        spans_embedding: (batch_size, num_spans, hidden_size*2+embedding_dim)\n        \"\"\"\n        return spans_embedding\n\n    def forward(self, input_ids, spans, spans_mask, spans_ner_label=None, token_type_ids=None, attention_mask=None):\n        spans_embedding = self._get_span_embeddings(input_ids, spans, token_type_ids=token_type_ids,\n                                                    attention_mask=attention_mask)\n        ffnn_hidden = []\n        hidden = spans_embedding\n        for layer in self.ner_classifier:\n            hidden = layer(hidden)\n            ffnn_hidden.append(hidden)\n        logits = ffnn_hidden[-1]\n\n        if spans_ner_label is not None:\n            loss_fct = CrossEntropyLoss(reduction='sum')\n            if attention_mask is not None:\n                active_loss = spans_mask.view(-1) == 1\n                active_logits = logits.view(-1, logits.shape[-1])\n                active_labels = torch.where(\n                    active_loss, spans_ner_label.view(-1), torch.tensor(loss_fct.ignore_index).type_as(spans_ner_label)\n                )\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, logits.shape[-1]), spans_ner_label.view(-1))\n            return loss, logits, spans_embedding\n        else:\n            return logits, spans_embedding, spans_embedding", "\n\nclass AlbertForEntity(AlbertPreTrainedModel):\n    def __init__(self, config, num_ner_labels, head_hidden_dim=150, width_embedding_dim=150, max_span_length=8):\n        super().__init__(config)\n\n        self.albert = AlbertModel(config)\n        self.hidden_dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.width_embedding = nn.Embedding(max_span_length + 1, width_embedding_dim)\n\n        self.ner_classifier = nn.Sequential(\n            FeedForward(input_dim=config.hidden_size * 2 + width_embedding_dim,\n                        num_layers=2,\n                        hidden_dims=head_hidden_dim,\n                        activations=F.relu,\n                        dropout=0.2),\n            nn.Linear(head_hidden_dim, num_ner_labels)\n        )\n\n        self.init_weights()\n\n    def _get_span_embeddings(self, input_ids, spans, token_type_ids=None, attention_mask=None):\n        sequence_output, pooled_output = self.albert(input_ids=input_ids, token_type_ids=token_type_ids,\n                                                     attention_mask=attention_mask)\n\n        sequence_output = self.hidden_dropout(sequence_output)\n\n        \"\"\"\n        spans: [batch_size, num_spans, 3]; 0: left_ned, 1: right_end, 2: width\n        spans_mask: (batch_size, num_spans, )\n        \"\"\"\n        spans_start = spans[:, :, 0].view(spans.size(0), -1)\n        spans_start_embedding = batched_index_select(sequence_output, spans_start)\n        spans_end = spans[:, :, 1].view(spans.size(0), -1)\n        spans_end_embedding = batched_index_select(sequence_output, spans_end)\n\n        spans_width = spans[:, :, 2].view(spans.size(0), -1)\n        spans_width_embedding = self.width_embedding(spans_width)\n\n        spans_embedding = torch.cat((spans_start_embedding, spans_end_embedding, spans_width_embedding), dim=-1)\n        \"\"\"\n        spans_embedding: (batch_size, num_spans, hidden_size*2+embedding_dim)\n        \"\"\"\n        return spans_embedding\n\n    def forward(self, input_ids, spans, spans_mask, spans_ner_label=None, token_type_ids=None, attention_mask=None):\n        spans_embedding = self._get_span_embeddings(input_ids, spans, token_type_ids=token_type_ids,\n                                                    attention_mask=attention_mask)\n        ffnn_hidden = []\n        hidden = spans_embedding\n        for layer in self.ner_classifier:\n            hidden = layer(hidden)\n            ffnn_hidden.append(hidden)\n        logits = ffnn_hidden[-1]\n\n        if spans_ner_label is not None:\n            loss_fct = CrossEntropyLoss(reduction='sum')\n            if attention_mask is not None:\n                active_loss = spans_mask.view(-1) == 1\n                active_logits = logits.view(-1, logits.shape[-1])\n                active_labels = torch.where(\n                    active_loss, spans_ner_label.view(-1), torch.tensor(loss_fct.ignore_index).type_as(spans_ner_label)\n                )\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, logits.shape[-1]), spans_ner_label.view(-1))\n            return loss, logits, spans_embedding\n        else:\n            return logits, spans_embedding, spans_embedding", "\n\nclass EntityModel:\n    def __init__(self, args, num_ner_labels):\n        super().__init__()\n\n        bert_model_name = args.model\n        vocab_name = bert_model_name\n\n        if args.bert_model_dir is not None:\n            bert_model_name = str(args.bert_model_dir) + '/'\n            # vocab_name = bert_model_name + 'vocab.txt'\n            vocab_name = bert_model_name\n            logger.info('Loading BERT model from {}'.format(bert_model_name))\n\n        if args.use_albert:\n            self.tokenizer = AlbertTokenizer.from_pretrained(vocab_name)\n            self.bert_model = AlbertForEntity.from_pretrained(bert_model_name, num_ner_labels=num_ner_labels,\n                                                              max_span_length=args.max_span_length)\n        else:  # use_bert\n            self.tokenizer = BertTokenizer.from_pretrained(vocab_name)\n            self.bert_model = BertForEntity.from_pretrained(bert_model_name, num_ner_labels=num_ner_labels,\n                                                            max_span_length=args.max_span_length)\n\n        self._model_device = 'cpu'\n        self.move_model_to_cuda()\n\n    def move_model_to_cuda(self):\n        if not torch.cuda.is_available():\n            logger.error('No CUDA found!')\n            exit(-1)\n        logger.info('Moving to CUDA...')\n        self._model_device = 'cuda'\n        self.bert_model.cuda()\n        logger.info('# GPUs = %d' % (torch.cuda.device_count()))\n        if torch.cuda.device_count() > 1:\n            self.bert_model = torch.nn.DataParallel(self.bert_model)\n\n    def _get_input_tensors(self, tokens, spans, spans_ner_label):\n        start2idx = []\n        end2idx = []\n\n        bert_tokens = [self.tokenizer.cls_token]\n        for token in tokens:\n            start2idx.append(len(bert_tokens))\n            sub_tokens = self.tokenizer.tokenize(token)\n            bert_tokens += sub_tokens\n            end2idx.append(len(bert_tokens) - 1)\n        bert_tokens.append(self.tokenizer.sep_token)\n\n        indexed_tokens = self.tokenizer.convert_tokens_to_ids(bert_tokens)\n        tokens_tensor = torch.tensor([indexed_tokens])\n\n        bert_spans = [[start2idx[span[0]], end2idx[span[1]], span[2]] for span in spans]\n        bert_spans_tensor = torch.tensor([bert_spans])\n\n        spans_ner_label_tensor = torch.tensor([spans_ner_label])\n\n        return tokens_tensor, bert_spans_tensor, spans_ner_label_tensor\n\n    def _get_input_tensors_batch(self, samples_list, training=True):\n        tokens_tensor_list = []\n        bert_spans_tensor_list = []\n        spans_ner_label_tensor_list = []\n        sentence_length = []\n\n        max_tokens = 0\n        max_spans = 0\n        for sample in samples_list:\n            tokens = sample['tokens']\n            spans = sample['spans']\n            spans_ner_label = sample['spans_label']\n\n            tokens_tensor, bert_spans_tensor, spans_ner_label_tensor = self._get_input_tensors(tokens, spans,\n                                                                                               spans_ner_label)\n            tokens_tensor_list.append(tokens_tensor)\n            bert_spans_tensor_list.append(bert_spans_tensor)\n            spans_ner_label_tensor_list.append(spans_ner_label_tensor)\n            assert (bert_spans_tensor.shape[1] == spans_ner_label_tensor.shape[1])\n            if tokens_tensor.shape[1] > max_tokens:\n                max_tokens = tokens_tensor.shape[1]\n            if bert_spans_tensor.shape[1] > max_spans:\n                max_spans = bert_spans_tensor.shape[1]\n            sentence_length.append(sample['sent_length'])\n        sentence_length = torch.Tensor(sentence_length)\n\n        # apply padding and concatenate tensors\n        final_tokens_tensor = None\n        final_attention_mask = None\n        final_bert_spans_tensor = None\n        final_spans_ner_label_tensor = None\n        final_spans_mask_tensor = None\n        for tokens_tensor, bert_spans_tensor, spans_ner_label_tensor in zip(tokens_tensor_list, bert_spans_tensor_list,\n                                                                            spans_ner_label_tensor_list):\n            # padding for tokens\n            num_tokens = tokens_tensor.shape[1]\n            tokens_pad_length = max_tokens - num_tokens\n            attention_tensor = torch.full([1, num_tokens], 1, dtype=torch.long)\n            if tokens_pad_length > 0:\n                pad = torch.full([1, tokens_pad_length], self.tokenizer.pad_token_id, dtype=torch.long)\n                tokens_tensor = torch.cat((tokens_tensor, pad), dim=1)\n                attention_pad = torch.full([1, tokens_pad_length], 0, dtype=torch.long)\n                attention_tensor = torch.cat((attention_tensor, attention_pad), dim=1)\n\n            # padding for spans\n            num_spans = bert_spans_tensor.shape[1]\n            spans_pad_length = max_spans - num_spans\n            spans_mask_tensor = torch.full([1, num_spans], 1, dtype=torch.long)\n            if spans_pad_length > 0:\n                pad = torch.full([1, spans_pad_length, bert_spans_tensor.shape[2]], 0, dtype=torch.long)\n                bert_spans_tensor = torch.cat((bert_spans_tensor, pad), dim=1)\n                mask_pad = torch.full([1, spans_pad_length], 0, dtype=torch.long)\n                spans_mask_tensor = torch.cat((spans_mask_tensor, mask_pad), dim=1)\n                spans_ner_label_tensor = torch.cat((spans_ner_label_tensor, mask_pad), dim=1)\n\n            # update final outputs\n            if final_tokens_tensor is None:\n                final_tokens_tensor = tokens_tensor\n                final_attention_mask = attention_tensor\n                final_bert_spans_tensor = bert_spans_tensor\n                final_spans_ner_label_tensor = spans_ner_label_tensor\n                final_spans_mask_tensor = spans_mask_tensor\n            else:\n                final_tokens_tensor = torch.cat((final_tokens_tensor, tokens_tensor), dim=0)\n                final_attention_mask = torch.cat((final_attention_mask, attention_tensor), dim=0)\n                final_bert_spans_tensor = torch.cat((final_bert_spans_tensor, bert_spans_tensor), dim=0)\n                final_spans_ner_label_tensor = torch.cat((final_spans_ner_label_tensor, spans_ner_label_tensor), dim=0)\n                final_spans_mask_tensor = torch.cat((final_spans_mask_tensor, spans_mask_tensor), dim=0)\n        # logger.info(final_tokens_tensor)\n        # logger.info(final_attention_mask)\n        # logger.info(final_bert_spans_tensor)\n        # logger.info(final_bert_spans_tensor.shape)\n        # logger.info(final_spans_mask_tensor.shape)\n        # logger.info(final_spans_ner_label_tensor.shape)\n        return final_tokens_tensor, final_attention_mask, final_bert_spans_tensor, final_spans_mask_tensor, \\\n               final_spans_ner_label_tensor, sentence_length\n\n    def run_batch(self, samples_list, try_cuda=True, training=True):\n        # convert samples to input tensors\n        tokens_tensor, attention_mask_tensor, \\\n        bert_spans_tensor, spans_mask_tensor, \\\n        spans_ner_label_tensor, \\\n        sentence_length = self._get_input_tensors_batch(samples_list, training)\n\n        output_dict = {\n            'ner_loss': 0,\n        }\n\n        if training:\n            self.bert_model.train()\n            ner_loss, \\\n            ner_logits, \\\n            spans_embedding \\\n                = self.bert_model(\n                input_ids=tokens_tensor.to(self._model_device),\n                spans=bert_spans_tensor.to(self._model_device),\n                spans_mask=spans_mask_tensor.to(self._model_device),\n                spans_ner_label=spans_ner_label_tensor.to(self._model_device),\n                attention_mask=attention_mask_tensor.to(self._model_device),\n            )\n            output_dict['ner_loss'] = ner_loss.sum()\n            output_dict['ner_llh'] = F.log_softmax(ner_logits, dim=-1)\n        else:\n            self.bert_model.eval()\n            with torch.no_grad():\n                ner_logits, spans_embedding, last_hidden = self.bert_model(\n                    input_ids=tokens_tensor.to(self._model_device),\n                    spans=bert_spans_tensor.to(self._model_device),\n                    spans_mask=spans_mask_tensor.to(self._model_device),\n                    spans_ner_label=None,\n                    attention_mask=attention_mask_tensor.to(self._model_device),\n                )\n            _, predicted_label = ner_logits.max(2)\n            predicted_label = predicted_label.cpu().numpy()\n            last_hidden = last_hidden.cpu().numpy()\n\n            predicted = []\n            pred_prob = []\n            hidden = []\n            for i, sample in enumerate(samples_list):\n                ner = []\n                prob = []\n                lh = []\n                for j in range(len(sample['spans'])):\n                    ner.append(predicted_label[i][j])\n                    # prob.append(F.softmax(ner_logits[i][j], dim=-1).cpu().numpy())\n                    prob.append(ner_logits[i][j].cpu().numpy())\n                    lh.append(last_hidden[i][j])\n                predicted.append(ner)\n                pred_prob.append(prob)\n                hidden.append(lh)\n            output_dict['pred_ner'] = predicted\n            output_dict['ner_probs'] = pred_prob\n            output_dict['ner_last_hidden'] = hidden\n\n        return output_dict", ""]}
{"filename": "src/Entity/entity/utils.py", "chunked_list": ["import numpy as np\nimport json\nimport logging\n\nlogger = logging.getLogger('root')\n\n\ndef batchify(samples, batch_size):\n    \"\"\"\n    Batchfy samples with a batch size\n    \"\"\"\n    num_samples = len(samples)\n\n    list_samples_batches = []\n\n    # if a sentence is too long, make itself a batch to avoid GPU OOM\n    to_single_batch = []\n    for i in range(0, len(samples)):\n        if len(samples[i]['tokens']) > 350:\n            to_single_batch.append(i)\n\n    for i in to_single_batch:\n        logger.info('Single batch sample.json: %s-%d', samples[i]['doc_key'], samples[i]['sentence_ix'])\n        list_samples_batches.append([samples[i]])\n    samples = [sample for i, sample in enumerate(samples) if i not in to_single_batch]\n\n    for i in range(0, len(samples), batch_size):\n        list_samples_batches.append(samples[i:i + batch_size])\n\n    assert (sum([len(batch) for batch in list_samples_batches]) == num_samples)\n\n    return list_samples_batches", "\n\ndef overlap(s1, s2):\n    if s2.start_sent >= s1.start_sent and s2.start_sent <= s1.end_sent:\n        return True\n    if s2.end_sent >= s1.start_sent and s2.end_sent <= s1.end_sent:\n        return True\n    return False\n\n\ndef convert_dataset_to_samples(dataset, max_span_length, ner_label2id=None, context_window=0, split=0):\n    \"\"\"\n    Extract sentences and gold entities from a dataset\n    \"\"\"\n    # split: split the data into train and dev (for ACE04)\n    # split == 0: don't split\n    # split == 1: return first 90% (train)\n    # split == 2: return last 10% (dev)\n    samples = []\n    num_ner = 0\n    max_len = 0\n    max_ner = 0\n    num_overlap = 0\n\n    if split == 0:\n        data_range = (0, len(dataset))\n    elif split == 1:\n        data_range = (0, int(len(dataset) * 0.9))\n    elif split == 2:\n        data_range = (int(len(dataset) * 0.9), len(dataset))\n    else:\n        data_range = None\n\n    for c, doc in enumerate(dataset):\n        if c < data_range[0] or c >= data_range[1]:\n            continue\n        for i, sent in enumerate(doc):\n            num_ner += len(sent.ner)\n            sample = {'doc_key': doc._doc_key, 'sentence_ix': sent.sentence_ix, 'tokens': sent.text,\n                      'sent_length': len(sent.text)}\n            # if context_window != 0 and len(sent.text) > context_window:\n            #     logger.info('Long sentence: {} {}'.format(sample.json, len(sent.text)))\n                # print('Exclude:', sample.json)\n                # continue\n            sent_start = 0\n            sent_end = len(sample['tokens'])\n\n            max_len = max(max_len, len(sent.text))\n            max_ner = max(max_ner, len(sent.ner))\n\n            if context_window > 0:\n                add_left = (context_window - len(sent.text)) // 2\n                add_right = (context_window - len(sent.text)) - add_left\n\n                # add left context\n                j = i - 1\n                while j >= 0 and add_left > 0:\n                    context_to_add = doc[j].text[-add_left:]\n                    sample['tokens'] = context_to_add + sample['tokens']\n                    add_left -= len(context_to_add)\n                    sent_start += len(context_to_add)\n                    sent_end += len(context_to_add)\n                    j -= 1\n\n                # add right context\n                j = i + 1\n                while j < len(doc) and add_right > 0:\n                    context_to_add = doc[j].text[:add_right]\n                    sample['tokens'] = sample['tokens'] + context_to_add\n                    add_right -= len(context_to_add)\n                    j += 1\n\n            sample['sent_start'] = sent_start\n            sample['sent_end'] = sent_end\n            sample['sent_start_in_doc'] = sent.sentence_start\n\n            sent_ner = {}\n            for ner in sent.ner:\n                sent_ner[ner.span.span_sent] = ner.label\n\n            span2id = {}\n            sample['spans'] = []\n            sample['spans_label'] = []\n            for i in range(len(sent.text)):\n                for j in range(i, min(len(sent.text), i + max_span_length)):\n                    # \u4e24\u6b21\u5faa\u73af\uff0c\u56e0\u4e3a\u8981n(n-1)/2\u4e2aentity\n                    sample['spans'].append((i + sent_start, j + sent_start, j - i + 1))\n                    # start\uff0cend\uff0clen\n                    span2id[(i, j)] = len(sample['spans']) - 1\n                    if (i, j) not in sent_ner:\n                        sample['spans_label'].append(0)\n                    else:\n                        sample['spans_label'].append(ner_label2id[sent_ner[(i, j)]])\n            samples.append(sample)\n    avg_length = sum([len(sample['tokens']) for sample in samples]) / len(samples)\n    max_length = max([len(sample['tokens']) for sample in samples])\n    logger.info('# Overlap: %d' % num_overlap)\n    logger.info('Extracted %d samples from %d documents, with %d NER labels, %.3f avg input length, %d max length' % (\n        len(samples), data_range[1] - data_range[0], num_ner, avg_length, max_length))\n    logger.info('Max Length: %d, max NER: %d' % (max_len, max_ner))\n    return samples, num_ner", "\n\ndef convert_dataset_to_samples(dataset, max_span_length, ner_label2id=None, context_window=0, split=0):\n    \"\"\"\n    Extract sentences and gold entities from a dataset\n    \"\"\"\n    # split: split the data into train and dev (for ACE04)\n    # split == 0: don't split\n    # split == 1: return first 90% (train)\n    # split == 2: return last 10% (dev)\n    samples = []\n    num_ner = 0\n    max_len = 0\n    max_ner = 0\n    num_overlap = 0\n\n    if split == 0:\n        data_range = (0, len(dataset))\n    elif split == 1:\n        data_range = (0, int(len(dataset) * 0.9))\n    elif split == 2:\n        data_range = (int(len(dataset) * 0.9), len(dataset))\n    else:\n        data_range = None\n\n    for c, doc in enumerate(dataset):\n        if c < data_range[0] or c >= data_range[1]:\n            continue\n        for i, sent in enumerate(doc):\n            num_ner += len(sent.ner)\n            sample = {'doc_key': doc._doc_key, 'sentence_ix': sent.sentence_ix, 'tokens': sent.text,\n                      'sent_length': len(sent.text)}\n            # if context_window != 0 and len(sent.text) > context_window:\n            #     logger.info('Long sentence: {} {}'.format(sample.json, len(sent.text)))\n                # print('Exclude:', sample.json)\n                # continue\n            sent_start = 0\n            sent_end = len(sample['tokens'])\n\n            max_len = max(max_len, len(sent.text))\n            max_ner = max(max_ner, len(sent.ner))\n\n            if context_window > 0:\n                add_left = (context_window - len(sent.text)) // 2\n                add_right = (context_window - len(sent.text)) - add_left\n\n                # add left context\n                j = i - 1\n                while j >= 0 and add_left > 0:\n                    context_to_add = doc[j].text[-add_left:]\n                    sample['tokens'] = context_to_add + sample['tokens']\n                    add_left -= len(context_to_add)\n                    sent_start += len(context_to_add)\n                    sent_end += len(context_to_add)\n                    j -= 1\n\n                # add right context\n                j = i + 1\n                while j < len(doc) and add_right > 0:\n                    context_to_add = doc[j].text[:add_right]\n                    sample['tokens'] = sample['tokens'] + context_to_add\n                    add_right -= len(context_to_add)\n                    j += 1\n\n            sample['sent_start'] = sent_start\n            sample['sent_end'] = sent_end\n            sample['sent_start_in_doc'] = sent.sentence_start\n\n            sent_ner = {}\n            for ner in sent.ner:\n                sent_ner[ner.span.span_sent] = ner.label\n\n            span2id = {}\n            sample['spans'] = []\n            sample['spans_label'] = []\n            for i in range(len(sent.text)):\n                for j in range(i, min(len(sent.text), i + max_span_length)):\n                    # \u4e24\u6b21\u5faa\u73af\uff0c\u56e0\u4e3a\u8981n(n-1)/2\u4e2aentity\n                    sample['spans'].append((i + sent_start, j + sent_start, j - i + 1))\n                    # start\uff0cend\uff0clen\n                    span2id[(i, j)] = len(sample['spans']) - 1\n                    if (i, j) not in sent_ner:\n                        sample['spans_label'].append(0)\n                    else:\n                        sample['spans_label'].append(ner_label2id[sent_ner[(i, j)]])\n            samples.append(sample)\n    avg_length = sum([len(sample['tokens']) for sample in samples]) / len(samples)\n    max_length = max([len(sample['tokens']) for sample in samples])\n    logger.info('# Overlap: %d' % num_overlap)\n    logger.info('Extracted %d samples from %d documents, with %d NER labels, %.3f avg input length, %d max length' % (\n        len(samples), data_range[1] - data_range[0], num_ner, avg_length, max_length))\n    logger.info('Max Length: %d, max NER: %d' % (max_len, max_ner))\n    return samples, num_ner", "\n\nclass NpEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return super(NpEncoder, self).default(obj)", "\n\ndef get_train_fold(data, fold):\n    print('Getting train fold %d...' % fold)\n    l = int(len(data) * 0.1 * fold)\n    r = int(len(data) * 0.1 * (fold + 1))\n    new_js = []\n    new_docs = []\n    for i in range(len(data)):\n        if i < l or i >= r:\n            new_js.append(data.js[i])\n            new_docs.append(data.documents[i])\n    print('# documents: %d --> %d' % (len(data), len(new_docs)))\n    data.js = new_js\n    data.documents = new_docs\n    return data", "\n\ndef get_test_fold(data, fold):\n    print('Getting test fold %d...' % fold)\n    l = int(len(data) * 0.1 * fold)\n    r = int(len(data) * 0.1 * (fold + 1))\n    new_js = []\n    new_docs = []\n    for i in range(len(data)):\n        if i >= l and i < r:\n            new_js.append(data.js[i])\n            new_docs.append(data.documents[i])\n    print('# documents: %d --> %d' % (len(data), len(new_docs)))\n    data.js = new_js\n    data.documents = new_docs\n    return data", ""]}
{"filename": "src/Entity/shared/data_structures.py", "chunked_list": ["\"\"\"\nThis code is based on DYGIE++'s codebase\n\"\"\"\nimport json\nimport copy\nimport os\nfrom collections import Counter\nimport numpy as np\n\nimport torch", "\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\ndef fields_to_batches(d, keys_to_ignore=None):\n    if keys_to_ignore is None:\n        keys_to_ignore = []\n    keys = [key for key in d.keys() if key not in keys_to_ignore]\n    lengths = [len(d[k]) for k in keys]\n    assert len(set(lengths)) == 1\n    length = lengths[0]\n    res = [{k: d[k][i] for k in keys} for i in range(length)]\n    return res", "\n\ndef get_sentence_of_span(span, sentence_starts, doc_tokens):\n    \"\"\"\n    Return the index of the sentence that the span is part of.\n    \"\"\"\n    # Inclusive sentence ends\n    sentence_ends = [x - 1 for x in sentence_starts[1:]] + [doc_tokens - 1]\n    in_between = [span[0] >= start and span[1] <= end\n                  for start, end in zip(sentence_starts, sentence_ends)]\n    assert sum(in_between) == 1\n    the_sentence = in_between.index(True)\n    return the_sentence", "\n\nclass Dataset:\n    def __init__(self, json_file,\n                 pred_file=None, doc_range=None):\n        # default\u8bbe\u7f6epred_file=None\n        self.js = self._read(json_file, pred_file)\n        if doc_range is not None:\n            self.js = self.js[doc_range[0]:doc_range[1]]\n        self.documents = [Document(js) for js in self.js]\n\n    def update_from_js(self, js):\n        self.js = js\n        self.documents = [Document(js) for js in self.js]\n\n    def _read(self, json_file, pred_file=None):\n        with open(json_file) as f:\n            gold_docs = json.load(f)\n        # gold_docs = [json.loads(line) for line in open(json_file)]\n        if pred_file is None:\n            return gold_docs\n\n        pred_docs = [json.loads(line) for line in open(pred_file)]\n        merged_docs = []\n        for gold, pred in zip(gold_docs, pred_docs):\n            assert gold[\"doc_key\"] == pred[\"doc_key\"]\n            assert gold[\"sentences\"] == pred[\"sentences\"]\n            merged = copy.deepcopy(gold)\n            for k, v in pred.items():\n                if \"predicted\" in k:\n                    merged[k] = v\n            merged_docs.append(merged)\n\n        return merged_docs\n\n    def __getitem__(self, ix):\n        return self.documents[ix]\n\n    def __len__(self):\n        return len(self.documents)", "\n\nclass Document:\n    def __init__(self, js):\n        if 'doc_key' in js:\n            self._doc_key = js[\"doc_key\"]\n            entries = fields_to_batches(js, [\"doc_key\", \"clusters\", \"predicted_clusters\", \"section_starts\"])\n        else:\n            entries = fields_to_batches(js, [\"clusters\", \"predicted_clusters\", \"section_starts\"])\n        sentence_lengths = [len(entry[\"sentences\"]) for entry in entries]\n        sentence_starts = np.cumsum(sentence_lengths)\n        # \u7d2f\u79ef\u6c42\u548c\uff0ce.g if the input array is [a, b, c], the output of numpy.cumsum will be [a, a + b, a + b + c].\n        sentence_starts = np.roll(sentence_starts, 1)\n        # \u6eda\u52a8\uff0c\u5168\u90e8\u6570\u5b57\u5411\u53f3\u6eda\u52a8\u4e00\u4f4d\n        sentence_starts[0] = 0\n        self.sentence_starts = sentence_starts\n        self.sentences = [Sentence(entry, sentence_start, sentence_ix)\n                          for sentence_ix, (entry, sentence_start)\n                          in enumerate(zip(entries, sentence_starts))]\n        # \u53ef\u4ee5\u6ca1\u6709clusters\n        if \"clusters\" in js:\n            self.clusters = [Cluster(entry, i, self)\n                             for i, entry in enumerate(js[\"clusters\"])]\n        if \"predicted_clusters\" in js:\n            self.predicted_clusters = [Cluster(entry, i, self)\n                                       for i, entry in enumerate(js[\"predicted_clusters\"])]\n\n    def __repr__(self):\n        return \"\\n\".join([str(i) + \": \" + \" \".join(sent.text) for i, sent in enumerate(self.sentences)])\n\n    def __getitem__(self, ix):\n        return self.sentences[ix]\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def print_plaintext(self):\n        for sent in self:\n            print(\" \".join(sent.text))\n\n    def find_cluster(self, entity, predicted=True):\n        \"\"\"\n        Search through erence clusters and return the one containing the query entity, if it's\n        part of a cluster. If we don't find a match, return None.\n        \"\"\"\n        clusters = self.predicted_clusters if predicted else self.clusters\n        for clust in clusters:\n            for entry in clust:\n                if entry.span == entity.span:\n                    return clust\n\n        return None\n\n    @property\n    def n_tokens(self):\n        return sum([len(sent) for sent in self.sentences])", "\n\nclass Sentence:\n    def __init__(self, entry,\n                 sentence_start,\n                 sentence_ix):\n        self.sentence_start = sentence_start\n        self.text = entry[\"sentences\"]\n        self.sentence_ix = sentence_ix\n        # Gold\n        if \"ner_flavor\" in entry:\n            self.ner = [NER(this_ner, self.text, sentence_start, flavor=this_flavor)\n                        for this_ner, this_flavor in zip(entry[\"ner\"], entry[\"ner_flavor\"])]\n        elif \"ner\" in entry:  # default\n            self.ner = [NER(this_ner, self.text, sentence_start)\n                        for this_ner in entry[\"ner\"]]\n        if \"relations\" in entry:\n            self.relations = [Relation(this_relation, self.text, sentence_start) for\n                              this_relation in entry[\"relations\"]]\n        if \"events\" in entry:\n            self.events = Events(entry[\"events\"], self.text, sentence_start)\n\n        # Predicted\n        if \"predicted_ner\" in entry:\n            self.predicted_ner = [NER(this_ner, self.text, sentence_start, flavor=None) for\n                                  this_ner in entry[\"predicted_ner\"]]\n        if \"predicted_relations\" in entry:\n            self.predicted_relations = [Relation(this_relation, self.text, sentence_start) for\n                                        this_relation in entry[\"predicted_relations\"]]\n        if \"predicted_events\" in entry:\n            self.predicted_events = Events(entry[\"predicted_events\"], self.text, sentence_start)\n\n        # Top spans\n        if \"top_spans\" in entry:\n            self.top_spans = [NER(this_ner, self.text, sentence_start, flavor=None) for\n                              this_ner in entry[\"top_spans\"]]\n\n    def __repr__(self):\n        the_text = \" \".join(self.text)\n        the_lengths = np.array([len(x) for x in self.text])\n        tok_ixs = \"\"\n        for i, offset in enumerate(the_lengths):\n            true_offset = offset if i < 10 else offset - 1\n            tok_ixs += str(i)\n            tok_ixs += \" \" * true_offset\n\n        return the_text + \"\\n\" + tok_ixs\n\n    def __len__(self):\n        return len(self.text)\n\n    def get_flavor(self, argument):\n        the_ner = [x for x in self.ner if x.span == argument.span]\n        if len(the_ner) > 1:\n            print(\"Weird\")\n        if the_ner:\n            the_flavor = the_ner[0].flavor\n        else:\n            the_flavor = None\n        return the_flavor", "\n\nclass Span:\n    def __init__(self, start, end, text, sentence_start):\n        self.start_doc = start\n        self.end_doc = end\n        self.span_doc = (self.start_doc, self.end_doc)\n        # done \u662f\u6709\u51cf\u53bbstart\u7684\n        self.start_sent = start - sentence_start\n        self.end_sent = end - sentence_start\n        self.span_sent = (self.start_sent, self.end_sent)\n        self.text = text[self.start_sent:self.end_sent + 1]\n\n    def __repr__(self):\n        return str((self.start_sent, self.end_sent, self.text))\n\n    def __eq__(self, other):\n        return (self.span_doc == other.span_doc and\n                self.span_sent == other.span_sent and\n                self.text == other.text)\n\n    def __hash__(self):\n        tup = self.span_doc + self.span_sent + (\" \".join(self.text),)\n        return hash(tup)", "\n\nclass Token:\n    def __init__(self, ix, text, sentence_start):\n        self.ix_doc = ix\n        self.ix_sent = ix - sentence_start\n        self.text = text[self.ix_sent]\n\n    def __repr__(self):\n        return str((self.ix_sent, self.text))", "\n\nclass Trigger:\n    def __init__(self, token, label):\n        self.token = token\n        self.label = label\n\n    def __repr__(self):\n        return self.token.__repr__()[:-1] + \", \" + self.label + \")\"\n", "\n\nclass Argument:\n    def __init__(self, span, role, event_type):\n        self.span = span\n        self.role = role\n        self.event_type = event_type\n\n    def __repr__(self):\n        return self.span.__repr__()[:-1] + \", \" + self.event_type + \", \" + self.role + \")\"\n\n    def __eq__(self, other):\n        return (self.span == other.span and\n                self.role == other.role and\n                self.event_type == other.event_type)\n\n    def __hash__(self):\n        return self.span.__hash__() + hash((self.role, self.event_type))", "\n\nclass NER:\n    def __init__(self, ner, text, sentence_start, flavor=None):\n        self.span = Span(ner[0], ner[1], text, sentence_start)\n        self.label = ner[2]\n        self.flavor = flavor\n\n    def __repr__(self):\n        return self.span.__repr__() + \": \" + self.label\n\n    def __eq__(self, other):\n        return (self.span == other.span and\n                self.label == other.label and\n                self.flavor == other.flavor)", "\n\nclass Relation:\n    def __init__(self, relation, text, sentence_start):\n        start1, end1 = relation[0], relation[1]\n        start2, end2 = relation[2], relation[3]\n        label = relation[4]\n        span1 = Span(start1, end1, text, sentence_start)\n        span2 = Span(start2, end2, text, sentence_start)\n        self.pair = (span1, span2)\n        self.label = label\n\n    def __repr__(self):\n        return self.pair[0].__repr__() + \", \" + self.pair[1].__repr__() + \": \" + self.label\n\n    def __eq__(self, other):\n        return (self.pair == other.pair) and (self.label == other.label)", "\n\nclass AtomicRelation:\n    def __init__(self, ent0, ent1, label):\n        self.ent0 = ent0\n        self.ent1 = ent1\n        self.label = label\n\n    @classmethod\n    def from_relation(cls, relation):\n        ent0 = \" \".join(relation.pair[0].text)\n        ent1 = \" \".join(relation.pair[1].text)\n        label = relation.label\n        return cls(ent0, ent1, label)\n\n    def __repr__(self):\n        return f\"({self.ent0} | {self.ent1} | {self.label})\"", "\n\nclass Event:\n    def __init__(self, event, text, sentence_start):\n        trig = event[0]\n        args = event[1:]\n        trigger_token = Token(trig[0], text, sentence_start)\n        self.trigger = Trigger(trigger_token, trig[1])\n\n        self.arguments = []\n        for arg in args:\n            span = Span(arg[0], arg[1], text, sentence_start)\n            self.arguments.append(Argument(span, arg[2], self.trigger.label))\n\n    def __repr__(self):\n        res = \"<\"\n        res += self.trigger.__repr__() + \":\\n\"\n        for arg in self.arguments:\n            res += 6 * \" \" + arg.__repr__() + \";\\n\"\n        res = res[:-2] + \">\"\n        return res", "\n\nclass Events:\n    def __init__(self, events_json, text, sentence_start):\n        self.event_list = [Event(this_event, text, sentence_start) for this_event in events_json]\n        self.triggers = set([event.trigger for event in self.event_list])\n        self.arguments = set([arg for event in self.event_list for arg in event.arguments])\n\n    def __len__(self):\n        return len(self.event_list)\n\n    def __getitem__(self, i):\n        return self.event_list[i]\n\n    def __repr__(self):\n        return \"\\n\\n\".join([event.__repr__() for event in self.event_list])\n\n    def span_matches(self, argument):\n        return set([candidate for candidate in self.arguments\n                    if candidate.span.span_sent == argument.span.span_sent])\n\n    def event_type_matches(self, argument):\n        return set([candidate for candidate in self.span_matches(argument)\n                    if candidate.event_type == argument.event_type])\n\n    def matches_except_event_type(self, argument):\n        matched = [candidate for candidate in self.span_matches(argument)\n                   if candidate.event_type != argument.event_type\n                   and candidate.role == argument.role]\n        return set(matched)\n\n    def exact_match(self, argument):\n        for candidate in self.arguments:\n            if candidate == argument:\n                return True\n        return False", "\n\nclass Cluster:\n    def __init__(self, cluster, cluster_id, document):\n        members = []\n        for entry in cluster:\n            sentence_ix = get_sentence_of_span(entry, document.sentence_starts, document.n_tokens)\n            sentence = document[sentence_ix]\n            span = Span(entry[0], entry[1], sentence.text, sentence.sentence_start)\n            ners = [x for x in sentence.ner if x.span == span]\n            assert len(ners) <= 1\n            ner = ners[0] if len(ners) == 1 else None\n            to_append = ClusterMember(span, ner, sentence, cluster_id)\n            members.append(to_append)\n\n        self.members = members\n        self.cluster_id = cluster_id\n\n    def __repr__(self):\n        return f\"{self.cluster_id}: \" + self.members.__repr__()\n\n    def __getitem__(self, ix):\n        return self.members[ix]", "\n\nclass ClusterMember:\n    def __init__(self, span, ner, sentence, cluster_id):\n        self.span = span\n        self.ner = ner\n        self.sentence = sentence\n        self.cluster_id = cluster_id\n\n    def __repr__(self):\n        return f\"<{self.sentence.sentence_ix}> \" + self.span.__repr__()", "\n\n####################\n\n# Code to do evaluation of predictions for a loaded dataset.\n\ndef safe_div(num, denom):\n    if denom > 0:\n        return num / denom\n    else:\n        return 0", "\n\ndef compute_f1(predicted, gold, matched):\n    # F1 score.\n    precision = safe_div(matched, predicted)\n    recall = safe_div(matched, gold)\n    f1 = safe_div(2 * precision * recall, precision + recall)\n    return dict(precision=precision, recall=recall, f1=f1)\n\n\ndef evaluate_sent(sent, counts):\n    correct_ner = set()\n    # Entities.\n    counts[\"ner_gold\"] += len(sent.ner)\n    counts[\"ner_predicted\"] += len(sent.predicted_ner)\n    for prediction in sent.predicted_ner:\n        if any([prediction == actual for actual in sent.ner]):\n            counts[\"ner_matched\"] += 1\n            correct_ner.add(prediction.span)\n\n    # Relations.\n    counts[\"relations_gold\"] += len(sent.relations)\n    counts[\"relations_predicted\"] += len(sent.predicted_relations)\n    for prediction in sent.predicted_relations:\n        if any([prediction == actual for actual in sent.relations]):\n            counts[\"relations_matched\"] += 1\n            if (prediction.pair[0] in correct_ner) and (prediction.pair[1] in correct_ner):\n                counts[\"strict_relations_matched\"] += 1\n\n    # Return the updated counts.\n    return counts", "\n\ndef evaluate_sent(sent, counts):\n    correct_ner = set()\n    # Entities.\n    counts[\"ner_gold\"] += len(sent.ner)\n    counts[\"ner_predicted\"] += len(sent.predicted_ner)\n    for prediction in sent.predicted_ner:\n        if any([prediction == actual for actual in sent.ner]):\n            counts[\"ner_matched\"] += 1\n            correct_ner.add(prediction.span)\n\n    # Relations.\n    counts[\"relations_gold\"] += len(sent.relations)\n    counts[\"relations_predicted\"] += len(sent.predicted_relations)\n    for prediction in sent.predicted_relations:\n        if any([prediction == actual for actual in sent.relations]):\n            counts[\"relations_matched\"] += 1\n            if (prediction.pair[0] in correct_ner) and (prediction.pair[1] in correct_ner):\n                counts[\"strict_relations_matched\"] += 1\n\n    # Return the updated counts.\n    return counts", "\n\ndef evaluate_predictions(dataset):\n    counts = Counter()\n\n    for doc in dataset:\n        for sent in doc:\n            counts = evaluate_sent(sent, counts)\n\n    scores_ner = compute_f1(\n        counts[\"ner_predicted\"], counts[\"ner_gold\"], counts[\"ner_matched\"])\n    scores_relations = compute_f1(\n        counts[\"relations_predicted\"], counts[\"relations_gold\"], counts[\"relations_matched\"])\n    scores_strict_relations = compute_f1(\n        counts[\"relations_predicted\"], counts[\"relations_gold\"], counts[\"strict_relations_matched\"])\n\n    return dict(ner=scores_ner, relation=scores_relations, strict_relation=scores_strict_relations)", "\n\ndef analyze_relation_coverage(dataset):\n    def overlap(s1, s2):\n        if s2.start_sent >= s1.start_sent and s2.start_sent <= s1.end_sent:\n            return True\n        if s2.end_sent >= s1.start_sent and s2.end_sent <= s1.end_sent:\n            return True\n        return False\n\n    nrel_gold = 0\n    nrel_pred_cover = 0\n    nrel_top_cover = 0\n\n    npair_pred = 0\n    npair_top = 0\n\n    nrel_overlap = 0\n\n    for d in dataset:\n        for s in d:\n            pred = set([ner.span for ner in s.predicted_ner])\n            top = set([ner.span for ner in s.top_spans])\n            npair_pred += len(s.predicted_ner) * (len(s.predicted_ner) - 1)\n            npair_top += len(s.top_spans) * (len(s.top_spans) - 1)\n            for r in s.relations:\n                nrel_gold += 1\n                if (r.pair[0] in pred) and (r.pair[1] in pred):\n                    nrel_pred_cover += 1\n                if (r.pair[0] in top) and (r.pair[1] in top):\n                    nrel_top_cover += 1\n\n                if overlap(r.pair[0], r.pair[1]):\n                    nrel_overlap += 1\n\n    print('Coverage by predicted entities: %.3f (%d / %d), #candidates: %d' % (\n        nrel_pred_cover / nrel_gold * 100.0, nrel_pred_cover, nrel_gold, npair_pred))\n    print('Coverage by top 0.4 spans: %.3f (%d / %d), #candidates: %d' % (\n        nrel_top_cover / nrel_gold * 100.0, nrel_top_cover, nrel_gold, npair_top))\n    print('Overlap: %.3f (%d / %d)' % (nrel_overlap / nrel_gold * 100.0, nrel_overlap, nrel_gold))", ""]}
{"filename": "src/Entity/shared/get_hparams.py", "chunked_list": ["import argparse\n\n\ndef get_hparams_entity():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--task', type=str, default='ACE')\n\n    parser.add_argument('--data_dir', type=str,\n                        default='../ace2005/',\n                        help=\"path to the preprocessed dataset\")\n    parser.add_argument('--output_dir', type=str, default='entity_output_ace',\n                        help=\"output directory of the entity model\")\n\n    parser.add_argument('--max_span_length', type=int, default=8,\n                        help=\"spans w/ length up to max_span_length are considered as candidates\")\n    parser.add_argument('--train_batch_size', type=int, default=8,\n                        help=\"batch size during training\")\n    parser.add_argument('--eval_batch_size', type=int, default=8,\n                        help=\"batch size during inference\")\n    parser.add_argument('--learning_rate', type=float, default=1e-5,\n                        help=\"learning rate for the BERT encoder\")\n    parser.add_argument('--task_learning_rate', type=float, default=5e-4,\n                        help=\"learning rate for task-specific parameters, i.e., classification head\")\n    parser.add_argument('--warmup_proportion', type=float, default=0.1,\n                        help=\"the ratio of the warmup steps to the total steps\")\n    parser.add_argument('--num_epoch', type=int, default=100,\n                        help=\"number of the training epochs\")\n    parser.add_argument('--print_loss_step', type=int, default=100,\n                        help=\"how often logging the loss value during training\")\n    parser.add_argument('--eval_per_epoch', type=int, default=1,\n                        help=\"how often evaluating the trained model on dev set during training\")\n    parser.add_argument(\"--bertadam\", action=\"store_true\", help=\"If bertadam, then set correct_bias = False\")\n\n    parser.add_argument('--do_train', action='store_false', default=True,\n                        help=\"whether to run training\")\n    parser.add_argument('--train_shuffle', action='store_false', default=True,\n                        help=\"whether to train with randomly shuffled data\")\n    parser.add_argument('--do_eval', action='store_false', default=True,\n                        help=\"whether to run evaluation\")\n    parser.add_argument('--eval_test', action='store_true', default=False,\n                        help=\"whether to evaluate on test set\")\n    parser.add_argument('--dev_pred_filename', type=str, default=\"ent_pred_dev.json\",\n                        help=\"the prediction filename for the dev set\")\n    parser.add_argument('--test_pred_filename', type=str, default=\"ent_pred_test.json\",\n                        help=\"the prediction filename for the test set\")\n\n    parser.add_argument('--use_albert', action='store_true',\n                        help=\"whether to use ALBERT model\")\n    parser.add_argument('--model', type=str, default='bert-base-uncased',\n                        help=\"the base model name (a huggingface model)\")\n    parser.add_argument('--bert_model_dir', type=str, default=None,\n                        help=\"the base model directory\")\n\n    parser.add_argument('--seed', type=int, default=42)\n\n    parser.add_argument('--context_window', type=int, default=300,\n                        help=\"the context window size W for the entity model\")\n\n    args = parser.parse_args()\n    return args", ""]}
{"filename": "src/Entity/shared/utils.py", "chunked_list": ["max_length = 400\n\n\ndef get_pre_dict(sentence_e_mentions,  # \u4e00\u4e2a\u53e5\u5b50\uff0c\u5305\u542b\u4e86\u591a\u4e2aevent\n                 sentence_before_len,\n                 context,\n                 words,\n                 data_new,\n                 sent_entity_mentions,\n                 entity_id_num_none=0, entity_id_num=0):\n    \"\"\"\n    sent_entity_mentions:  todo entity\u662f\u53ef\u4ee5\u8de8\u53e5\u7684\uff1f\n    \"\"\"\n    # label triggers and arguments\n    golden_event_mentions = []\n    for event in sentence_e_mentions:\n        trigger = {\"start\": event['trigger']['start'] - sentence_before_len,\n                   \"end\": event['trigger']['end'] - sentence_before_len,\n                   \"text\": event['trigger']['text']}\n        if trigger['start'] > max_length or trigger['end'] < 0:\n            continue\n        arguments = event['arguments']\n        args_list = []\n        for arg in arguments:\n            entity_id = arg['entity_id']\n            span = []\n            for entity in sent_entity_mentions:\n                if entity['id'] == entity_id:\n                    span = [entity['start'] - sentence_before_len, entity['end'] - sentence_before_len]\n                    break\n            if span:\n                args_dict = {\n                    \"role\": arg['role'],\n                    \"text\": arg['text'],\n                    \"start\": span[0],\n                    \"end\": span[1],\n                    # 'span': span\n                }\n                args_list.append(args_dict)\n        golden_event_mentions.append({\n            'trigger': trigger,\n            'arguments': args_list,\n            'event_type': event['event_type']\n        })\n\n    entity_mentions = []\n    for entity in sent_entity_mentions:\n        enti = {'entity_type': entity['entity_type'],\n                'start': entity['start'] - sentence_before_len,\n                'end': entity['end'] - sentence_before_len}\n        if enti['start'] > max_length or enti['end'] < 0:\n            continue\n        entity_mentions.append(enti)\n\n    data_dict = {\n        'sentence': context,\n        'words': words,\n        'golden_event_mentions': golden_event_mentions,\n        'golden_entity_mentions': entity_mentions\n    }\n    data_new.append(data_dict)\n    return entity_id_num_none, entity_id_num", ""]}
{"filename": "src/Entity/shared/const.py", "chunked_list": ["task_ner_labels = {\n    'ace04': ['FAC', 'WEA', 'LOC', 'VEH', 'GPE', 'ORG', 'PER'],\n    'ace05': ['FAC', 'WEA', 'LOC', 'VEH', 'GPE', 'ORG', 'PER'],\n    'scierc': ['Method', 'OtherScientificTerm', 'Task', 'Generic', 'Material', 'Metric'],\n    'WikiEvents': [\"TTL\", \"INF\", \"VAL\", \"LOC\", \"COM\", \"CRM\", \"ORG\", \"FAC\", \"MHI\", \"WEA\", \"VEH\", \"SID\", \"ABS\", \"MON\",\n                   \"GPE\", \"BOD\", \"PER\"],\n    'ACE': ['VEH:Water',\n            'GPE:Nation',\n            'ORG:Commercial',\n            'GPE:State-or-Province',", "            'ORG:Commercial',\n            'GPE:State-or-Province',\n            'Contact-Info:E-Mail',\n            'Crime',\n            'ORG:Non-Governmental',\n            'Contact-Info:URL',\n            'Sentence',\n            'ORG:Religious',\n            'VEH:Underspecified',\n            'WEA:Projectile',", "            'VEH:Underspecified',\n            'WEA:Projectile',\n            'FAC:Building-Grounds',\n            'PER:Group',\n            'WEA:Exploding',\n            'WEA:Biological',\n            'Contact-Info:Phone-Number',\n            'WEA:Chemical',\n            'LOC:Land-Region-Natural',\n            'WEA:Nuclear',", "            'LOC:Land-Region-Natural',\n            'WEA:Nuclear',\n            'LOC:Region-General',\n            'PER:Individual',\n            'WEA:Sharp',\n            'ORG:Sports',\n            'ORG:Government',\n            'ORG:Media',\n            'LOC:Address',\n            'WEA:Shooting',", "            'LOC:Address',\n            'WEA:Shooting',\n            'LOC:Water-Body',\n            'LOC:Boundary',\n            'GPE:Population-Center',\n            'GPE:Special',\n            'LOC:Celestial',\n            'FAC:Subarea-Facility',\n            'PER:Indeterminate',\n            'VEH:Subarea-Vehicle',", "            'PER:Indeterminate',\n            'VEH:Subarea-Vehicle',\n            'WEA:Blunt',\n            'VEH:Land',\n            'TIM:time',\n            'Numeric:Money',\n            'FAC:Airport',\n            'GPE:GPE-Cluster',\n            'ORG:Educational',\n            'Job-Title',", "            'ORG:Educational',\n            'Job-Title',\n            'GPE:County-or-District',\n            'ORG:Entertainment',\n            'Numeric:Percent',\n            'LOC:Region-International',\n            'WEA:Underspecified',\n            'VEH:Air',\n            'FAC:Path',\n            'ORG:Medical-Science',", "            'FAC:Path',\n            'ORG:Medical-Science',\n            'FAC:Plant',\n            'GPE:Continent']\n}\n\n\n\ndef get_labelmap(label_list):\n    label2id = {}\n    id2label = {}\n    for i, label in enumerate(label_list):\n        label2id[label] = i + 1\n        id2label[i + 1] = label\n    return label2id, id2label", "def get_labelmap(label_list):\n    label2id = {}\n    id2label = {}\n    for i, label in enumerate(label_list):\n        label2id[label] = i + 1\n        id2label[i + 1] = label\n    return label2id, id2label\n"]}
