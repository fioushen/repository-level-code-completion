{"filename": "config_reader.py", "chunked_list": ["import copy\nimport multiprocessing as mp\nimport os\nimport re\nimport time\nimport random\nimport json\nimport pynvml\nimport numpy as np\n", "import numpy as np\n\npynvml.nvmlInit()\n\ndef process_configs(target, arg_parser):\n    args, _ = arg_parser.parse_known_args()\n    ctx = mp.get_context('fork')\n\n    subprocess=[]\n    if \"ALL_GPU\" in os.environ:\n        all_gpu_queue = list(map(int, os.environ[\"ALL_GPU\"].split(\",\")))\n    else:\n        all_gpu_queue = [0, 1, 2, 3, 4, 5, 6, 7]\n    gpu_queue = []\n    waittime = 300\n    gpu_just_used = []\n    for run_args, _run_config, _run_repeat in _yield_configs(arg_parser, args):\n        if \"eval\" in run_args.label:\n            waittime = 90\n            if \"genia\" in run_args.dataset_path:\n                waittime = 180\n            if \"fewnerd\" in run_args.dataset_path:\n                waittime = 240\n            if \"ontonotes\" in run_args.dataset_path:\n                waittime = 240\n            \n        if run_args.seed==-1:\n            run_args.seed=random.randint(0,1000)\n        # debug\n        if run_args.debug:\n            target(run_args)\n        while not run_args.cpu and (len(gpu_queue)==0 or len(gpu_queue)<run_args.world_size):\n            gpu_queue = []\n            candidate_gpu = list(set(all_gpu_queue) - set(gpu_just_used))\n            for index in candidate_gpu:\n                try:\n                    handle = pynvml.nvmlDeviceGetHandleByIndex(index)\n                    meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n                    free = meminfo.free//(1024*1024)\n                    if run_args.world_size>0:\n                        gpu_queue.extend([(index, free)])\n                    elif ((\"eval\" in run_args.label) or (\"base\" in  run_args.model_path)):\n                        gpu_queue.extend([(index, free)]*(free//24000))\n                    else:\n                        gpu_queue.extend([(index, free)]*(free//40000))\n                    \n                except Exception as e:\n                    pass\n            gpu_queue = sorted(gpu_queue, key=lambda x:x[1], reverse=True)\n            print(dict(set(gpu_queue)))\n            if len(gpu_queue)<run_args.world_size:\n                print(f\"Need {run_args.world_size} GPUs for DDP Training, but only {len(gpu_queue)} free devices: {gpu_queue}. Waiting for Free GPU ......\")\n                time.sleep(waittime)\n                gpu_just_used = []\n            elif len(gpu_queue)==0:\n                print(\"Need 1 GPU for Normal Training, All are busy. Waiting for Free GPU ......\")\n                time.sleep(waittime)\n                gpu_just_used = []\n            else:\n                print(\"Avaliable devices: \",list(map(lambda x:x[0],gpu_queue)))\n        # CPU Training:\n        if run_args.cpu:\n                print(\"########### Using CPU Training ###########\")\n                print(\"Using Random Seed\", run_args.seed)\n                p = ctx.Process(target=target, args=(run_args,))\n                subprocess.append(p)\n                p.start()\n                time.sleep(1)\n        # GPU Training\n        else:\n            # GPU DDP Training\n            if run_args.world_size != -1:\n                print(\"########### Using GPU DDP Training ###########\")\n                # print(\"Using devices: \", gpu_queue)\n                os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str,list(set(map(lambda x:x[0],gpu_queue)))))\n                os.environ['MASTER_ADDR'] = 'localhost'\n                os.environ['MASTER_PORT'] = str(random.randint(10000, 20000))\n                \n                for local_rank in range(run_args.world_size):\n                    gpu_just_used.append(gpu_queue[0][0])\n                    gpu_queue = gpu_queue[1:]\n                    run_args.local_rank = local_rank\n                    print(\"Using Random Seed\", run_args.seed)\n                    p = ctx.Process(target=target, args=(run_args,))\n                    subprocess.append(p)\n                    p.start()\n                time.sleep(1)\n            # GPU Normal Training\n            if run_args.world_size == -1:\n                print(\"########### Using GPU Normal Training ###########\")\n                device_id = gpu_queue[0][0]\n                run_args.device_id = device_id\n                gpu_just_used.append(run_args.device_id)\n                gpu_queue.remove(gpu_queue[0])\n                print(\"Using devices: \", run_args.device_id)\n                print(\"Using Random Seed\", run_args.seed)\n                p = ctx.Process(target=target, args=(run_args,))\n                subprocess.append(p)\n                p.start()\n                time.sleep(1)\n\n    list(map(lambda x:x.join(),subprocess))", "\ndef _read_config(path):\n    lines = open(path).readlines()\n\n    runs = []\n    run = [1, dict()]\n    for line in lines:\n        stripped_line = line.strip()\n\n        if stripped_line.startswith('#'):\n            continue\n\n        if not stripped_line:\n            if run[1]:\n                runs.append(run)\n\n            run = [1, dict()]\n            continue\n\n        if stripped_line.startswith('[') and stripped_line.endswith(']'):\n            repeat = int(stripped_line[1:-1])\n            run[0] = repeat\n        else:\n            key, value = stripped_line.split('=')\n            key, value = (key.strip(), value.strip())\n            run[1][key] = value\n\n    if run[1]:\n        runs.append(run)\n\n    return runs", "\n\ndef _convert_config(config):\n    config_list = []\n    for k, v in config.items():\n        if k == \"config\":\n            continue\n        if v == \"None\":\n            continue\n        if v.startswith(\"[\"):\n            v = v[1:-1].replace(\",\", \"\")\n        if v.lower() == 'true':\n            config_list.append('--' + k)\n        elif v.lower() != 'false':\n            # config_list.extend(['--' + k] + v.split(' '))\n            config_list.extend(['--' + k] + [v])\n    return config_list", "\n\ndef _yield_configs(arg_parser, args, verbose=True):\n    _print = (lambda x: print(x)) if verbose else lambda x: x\n\n    if args.config:\n        config = _read_config(args.config)\n\n        for run_repeat, run_config in config:\n            print(\"-\" * 50)\n            print(\"Config:\")\n            # print(run_config)\n\n            args_copy = copy.deepcopy(args)\n            run_config=copy.deepcopy(run_config)\n            config_list = _convert_config(run_config)\n            run_args = arg_parser.parse_args(config_list, namespace=args_copy)\n            \n\n            run_args_list = []\n            # batch eval\n            if run_args.label==\"batch_eval_flag\":\n                save_path=run_args.model_path\n                # save_model_type = run_args.save_model_type\n                for dirpath,dirnames,filenames in sorted(os.walk(save_path),key=lambda x:x[0]):\n                    if \"final_model\" in dirpath:\n                        print(dirpath)\n                        dataset_name=re.match(\".*/(.*?)_train\",dirpath).group(1)\n                        args_path=\"/\".join(dirpath.split(\"/\")[:-1])+\"/args.json\"\n                        args_dict=json.load(open(args_path))\n\n                        run_args.label= dataset_name+\"_eval\"\n                        if \"train_dev\" in args_dict[\"train_path\"]:\n                            run_args.dataset_path =  args_dict[\"train_path\"].replace(\"train_dev\",\"test\")\n                        else:\n                            run_args.dataset_path =  args_dict[\"train_path\"].replace(\"train\",\"test\")\n                        run_args.dataset_path =  args_dict[\"valid_path\"]\n                        run_args.model_path=dirpath\n                        run_args.tokenizer_path=dirpath\n                        run_args.types_path = args_dict[\"types_path\"]\n                        # run_args.log_path = args_dict[\"log_path\"]\n                        run_args.log_path = \"/\".join(dirpath.split(\"/\")[:-3])\n\n                        run_args.seed=args_dict[\"seed\"]\n                        run_args.model_type=args_dict[\"model_type\"]\n                        run_args.weight_decay =args_dict[\"weight_decay\"]\n                        # run_args.no_overlapping =args_dict[\"no_overlapping\"]\n                        # run_args.no_partial_overlapping =args_dict[\"no_partial_overlapping\"]\n                        # run_args.no_duplicate =args_dict[\"no_duplicate\"]\n\n                        run_args.eval_batch_size =args_dict[\"eval_batch_size\"]\n                        run_args.prop_drop =args_dict[\"prop_drop\"]\n                        run_args.pool_type =args_dict[\"pool_type\"]\n                        run_args.use_masked_lm = args_dict[\"use_masked_lm\"]\n                        run_args.repeat_gt_entities = args_dict[\"repeat_gt_entities\"]\n                        \n                        run_args.nil_weight = args_dict[\"nil_weight\"]\n                        run_args.match_boundary_weight = args_dict[\"match_boundary_weight\"]\n                        run_args.match_class_weight = args_dict[\"match_class_weight\"]\n                        run_args.loss_boundary_weight = args_dict[\"loss_boundary_weight\"]\n                        run_args.loss_class_weight = args_dict[\"loss_class_weight\"]\n                        run_args.match_solver = args_dict[\"match_solver\"]\n                        run_args.prompt_number = args_dict[\"prompt_number\"]\n                        run_args.prompt_length = args_dict[\"prompt_length\"]\n                        run_args.prompt_type = args_dict[\"prompt_type\"]\n                        run_args.last_layer_for_loss = args_dict[\"last_layer_for_loss\"]\n                        \n                        run_args.prompt_individual_attention = args_dict[\"prompt_individual_attention\"]\n                        run_args.sentence_individual_attention = args_dict[\"sentence_individual_attention\"]\n\n                        run_args.lstm_layers = args_dict[\"lstm_layers\"]\n                        run_args.decoder_layers = args_dict[\"decoder_layers\"]\n                        run_args.split_epoch = args_dict[\"split_epoch\"]\n                        run_args.epochs = args_dict[\"epochs\"]\n                        run_args.withimage = args_dict[\"withimage\"]\n\n                        if run_args.cls_threshold == -1 and run_args.boundary_threshold != -1:\n                            for cls_threshold in np.arange(0, 1, 0.1):\n                                run_args_instance = copy.deepcopy(run_args)\n                                run_args_instance.cls_threshold = cls_threshold\n                                run_args_list.append(run_args_instance)\n\n                        if run_args.boundary_threshold == -1 and run_args.cls_threshold != -1:\n                            # for boundary_threshold in np.arange(0.9, 1, 0.01):\n                            for boundary_threshold in np.arange(0, 1, 0.1):\n                                run_args_instance = copy.deepcopy(run_args)\n                                run_args_instance.boundary_threshold = boundary_threshold\n                                run_args_list.append(run_args_instance)\n                                \n                        if run_args.cls_threshold == -1 and run_args.boundary_threshold == -1:\n                            for cls_threshold in np.arange(0, 1, 0.1):\n                                for boundary_threshold in np.arange(0, 1, 0.1):\n                                    run_args_instance = copy.deepcopy(run_args)\n                                    run_args_instance.cls_threshold = cls_threshold\n                                    run_args_instance.boundary_threshold = boundary_threshold\n                                    run_args_list.append(run_args_instance)\n\n                        if run_args.cls_threshold != -1 and run_args.boundary_threshold != -1:\n                            run_args_list.append(copy.deepcopy(run_args))\n            else:\n                run_args_list.append(run_args)\n\n            for run_args in run_args_list:\n                print(run_args)\n                print(\"Repeat %s times\" % run_repeat)\n                print(\"-\" * 50)\n                for iteration in range(run_repeat):\n                    _print(\"Iteration %s\" % iteration)\n                    _print(\"-\" * 50)\n\n                    yield copy.deepcopy(run_args), run_config, run_repeat\n            \n            # time.sleep(3)\n\n    else:\n        yield args, None, None"]}
{"filename": "prompt4ner.py", "chunked_list": ["import argparse\n\nfrom args import train_argparser, eval_argparser\nfrom config_reader import process_configs\nfrom prompt4ner import input_reader\nfrom prompt4ner.prompt4ner_trainer import Prompt4NERTrainer\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n", "warnings.filterwarnings(\"ignore\")\n\n\ndef __train(run_args):\n    trainer = Prompt4NERTrainer(run_args)\n    trainer.train(train_path=run_args.train_path, valid_path=run_args.valid_path,\n                  types_path=run_args.types_path, input_reader_cls=input_reader.JsonInputReader)\n\n\ndef _train():\n    arg_parser = train_argparser()\n    process_configs(target=__train, arg_parser=arg_parser)", "\ndef _train():\n    arg_parser = train_argparser()\n    process_configs(target=__train, arg_parser=arg_parser)\n\n\ndef __eval(run_args):\n    trainer = Prompt4NERTrainer(run_args)\n    trainer.eval(dataset_path=run_args.dataset_path, types_path=run_args.types_path,\n                 input_reader_cls=input_reader.JsonInputReader)", "\n\ndef _eval():\n    arg_parser = eval_argparser()\n    process_configs(target=__eval, arg_parser=arg_parser)\n\n\nif __name__ == '__main__':\n    arg_parser = argparse.ArgumentParser(add_help=False)\n    arg_parser.add_argument('mode', type=str, help=\"Mode: 'train' or 'eval'\")\n    args, _ = arg_parser.parse_known_args()\n\n    if args.mode == 'train':\n        _train()\n    elif args.mode == 'eval':\n        _eval()\n    else:\n        raise Exception(\"Mode not in ['train', 'eval'], e.g. 'python prompt4ner.py train ...'\")", ""]}
{"filename": "args.py", "chunked_list": ["import argparse\n\n\ndef _add_common_args(arg_parser):\n    arg_parser.add_argument('--config', type=str)\n\n    arg_parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"\")\n    arg_parser.add_argument(\"--world_size\", type=int, default=-1, help=\"\")\n\n    # Input\n    arg_parser.add_argument('--types_path', type=str, help=\"Path to type specifications\")\n\n    # Preprocessing\n    arg_parser.add_argument('--tokenizer_path', type=str, help=\"Path to tokenizer\")\n    arg_parser.add_argument('--lowercase', action='store_true', default=False,\n                            help=\"If true, input is lowercased during preprocessing\")\n    arg_parser.add_argument('--sampling_processes', type=int, default=4,\n                            help=\"Number of sampling processes. 0 = no multiprocessing for sampling\")\n\n    # Logging\n    arg_parser.add_argument('--label', type=str, help=\"Label of run. Used as the directory name of logs/models\")\n    arg_parser.add_argument('--log_path', type=str, help=\"Path do directory where training/evaluation logs are stored\")\n    arg_parser.add_argument('--store_predictions', action='store_true', default=False,\n                            help=\"If true, store predictions on disc (in log directory)\")\n    arg_parser.add_argument('--store_examples', action='store_true', default=False,\n                            help=\"If true, store evaluation examples on disc (in log directory)\")\n    arg_parser.add_argument('--example_count', type=int, default=None,\n                            help=\"Count of evaluation example to store (if store_examples == True)\")\n    arg_parser.add_argument('--debug', action='store_true', default=False, help=\"Debugging mode on/off\")\n\n    # Model / Training / Evaluation\n    arg_parser.add_argument('--device_id', type=int, default=-1, help=\"gpu device id\")\n    arg_parser.add_argument('--model_path', type=str, help=\"Path to directory that contains model checkpoints\")\n    arg_parser.add_argument('--model_type', type=str, default=\"prompt4ner\", help=\"Type of model\")\n    arg_parser.add_argument('--cpu', action='store_true', default=False,\n                            help=\"If true, train/evaluate on CPU even if a CUDA device is available\")\n    arg_parser.add_argument('--eval_batch_size', type=int, default=1, help=\"Evaluation batch size\")\n    arg_parser.add_argument('--prop_drop', type=float, default=0.1, help=\"Probability of dropout used in Prompt4NER\")\n    arg_parser.add_argument('--freeze_transformer', action='store_true', default=False, help=\"Freeze BERT weights\")\n\n    arg_parser.add_argument('--no_overlapping', action='store_true', default=False)\n    arg_parser.add_argument('--no_partial_overlapping', action='store_true', default=False)\n    arg_parser.add_argument('--no_duplicate', action='store_true', default=False)\n    arg_parser.add_argument('--cls_threshold', type=float, default=0.5)\n    arg_parser.add_argument('--boundary_threshold', type=float, default=0.5)\n    arg_parser.add_argument('--lstm_layers', type=int, default=3)\n    arg_parser.add_argument('--decoder_layers', type=int, default=3)\n    arg_parser.add_argument('--pool_type', type=str, default = \"max\")\n    arg_parser.add_argument('--prompt_number', type=int, default=60)\n    arg_parser.add_argument('--prompt_type', type=str, default=\"soft\")\n    arg_parser.add_argument('--prompt_length', type=int, default=3) # 2 + n\n    arg_parser.add_argument('--prompt_individual_attention', action='store_true', default=False)\n    arg_parser.add_argument('--sentence_individual_attention', action='store_true', default=False)\n    arg_parser.add_argument('--last_layer_for_loss', type=int, default=1)\n    arg_parser.add_argument('--withimage', action='store_true', default=False)\n    \n    # Misc\n    arg_parser.add_argument('--seed', type=int, default=-1, help=\"Seed\")\n    arg_parser.add_argument('--cache_path', type=str, default=None,\n                            help=\"Path to cache transformer models (for HuggingFace transformers library)\")", "\n\ndef train_argparser():\n    arg_parser = argparse.ArgumentParser()\n\n    # Input\n    arg_parser.add_argument('--train_path', type=str, help=\"Path to train dataset\")\n    arg_parser.add_argument('--valid_path', type=str, help=\"Path to validation dataset\")\n\n    # Logging\n    arg_parser.add_argument('--save_path', type=str, help=\"Path to directory where model checkpoints are stored\")\n    arg_parser.add_argument('--save_path_include_iteration', action='store_true', default=False)\n\n    \n    arg_parser.add_argument('--init_eval', action='store_true', default=False,\n                            help=\"If true, evaluate validation set before training\")\n    arg_parser.add_argument('--save_optimizer', action='store_true', default=False,\n                            help=\"Save optimizer alongside model\")\n    arg_parser.add_argument('--train_log_iter', type=int, default=1, help=\"Log training process every x iterations\")\n    arg_parser.add_argument('--final_eval', action='store_true', default=False,\n                            help=\"Evaluate the model only after training, not at every epoch\")\n\n    # Model / Training\n    arg_parser.add_argument('--train_batch_size', type=int, default=2, help=\"Training batch size\")\n    arg_parser.add_argument('--epochs', type=int, default=20, help=\"Number of epochs\")\n    arg_parser.add_argument('--lr', type=float, default=5e-5, help=\"Learning rate\")\n    arg_parser.add_argument('--stage_one_lr_scale', type=float, default=10)\n    arg_parser.add_argument('--lr_warmup', type=float, default=0.1,\n                            help=\"Proportion of total train iterations to warmup in linear increase/decrease schedule\")\n    arg_parser.add_argument('--weight_decay', type=float, default=0.01, help=\"Weight decay to apply\")\n    arg_parser.add_argument('--max_grad_norm', type=float, default=1.0, help=\"Maximum gradient norm\")\n\n\n    \n    arg_parser.add_argument('--match_solver', type=str, help=\"\", default=\"hungarian\")\n    arg_parser.add_argument('--type_loss', type=str, help=\"\", default=\"celoss\")\n    arg_parser.add_argument('--match_warmup_epoch', type=int, help=\"\", default=0)\n\n    \n    \n    arg_parser.add_argument('--nil_weight', type=float, default=-1)\n    arg_parser.add_argument('--match_boundary_weight', type=float, default=10.0)\n    arg_parser.add_argument('--match_class_weight', type=float, default=2.0)\n    arg_parser.add_argument('--loss_boundary_weight', type=float, default=2.0)\n    arg_parser.add_argument('--loss_class_weight', type=float, default=2.0)\n\n    \n    arg_parser.add_argument('--deeply_weight', type=str, help=\"\", default=\"same\")\n    arg_parser.add_argument('--use_masked_lm', action='store_true', default=False)\n    arg_parser.add_argument('--repeat_gt_entities', type=int, default=-1, help=\"\")\n    arg_parser.add_argument('--eval_every_epochs', type=int, default=1, help=\"\")\n    \n\n    arg_parser.add_argument('--split_epoch', type=int, default=0, help=\"\")\n\n    _add_common_args(arg_parser)\n\n    return arg_parser", "\n\ndef eval_argparser():\n    arg_parser = argparse.ArgumentParser()\n\n    # Input\n    arg_parser.add_argument('--dataset_path', type=str, help=\"Path to dataset\")\n\n    _add_common_args(arg_parser)\n\n    return arg_parser", ""]}
{"filename": "prompt4ner/modeling_bert.py", "chunked_list": ["# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch BERT model. \"\"\"\n\n", "\n\nimport math\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.utils.checkpoint", "import torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.file_utils import (\n    ModelOutput,\n    add_code_sample_docstrings,\n    add_start_docstrings,", "    add_code_sample_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    replace_return_docstrings,\n)\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    BaseModelOutputWithPoolingAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,", "    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    NextSentencePredictorOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import (\n    PreTrainedModel,", "from transformers.modeling_utils import (\n    PreTrainedModel,\n    apply_chunking_to_forward,\n    find_pruneable_heads_and_indices,\n    prune_linear_layer,\n)\nfrom transformers.utils import logging\nfrom transformers import BertConfig\n\n", "\n\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"bert-base-uncased\"\n_CONFIG_FOR_DOC = \"BertConfig\"\n_TOKENIZER_FOR_DOC = \"BertTokenizer\"\n\nBERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"bert-base-uncased\",", "BERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"bert-base-uncased\",\n    \"bert-large-uncased\",\n    \"bert-base-cased\",\n    \"bert-large-cased\",\n    \"bert-base-multilingual-uncased\",\n    \"bert-base-multilingual-cased\",\n    \"bert-base-chinese\",\n    \"bert-base-german-cased\",\n    \"bert-large-uncased-whole-word-masking\",", "    \"bert-base-german-cased\",\n    \"bert-large-uncased-whole-word-masking\",\n    \"bert-large-cased-whole-word-masking\",\n    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n    \"bert-large-cased-whole-word-masking-finetuned-squad\",\n    \"bert-base-cased-finetuned-mrpc\",\n    \"bert-base-german-dbmdz-cased\",\n    \"bert-base-german-dbmdz-uncased\",\n    \"cl-tohoku/bert-base-japanese\",\n    \"cl-tohoku/bert-base-japanese-whole-word-masking\",", "    \"cl-tohoku/bert-base-japanese\",\n    \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n    \"cl-tohoku/bert-base-japanese-char\",\n    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\",\n    \"TurkuNLP/bert-base-finnish-cased-v1\",\n    \"TurkuNLP/bert-base-finnish-uncased-v1\",\n    \"wietsedv/bert-base-dutch-cased\",\n    # See all BERT models at https://huggingface.co/models?filter=bert\n]\n", "]\n\n\ndef load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n    try:\n        import re\n\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\"\n        )\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split(\"/\")\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(\n            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n            for n in name\n        ):\n            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n                scope_names = re.split(r\"_(\\d+)\", m_name)\n            else:\n                scope_names = [m_name]\n            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n                pointer = getattr(pointer, \"bias\")\n            elif scope_names[0] == \"output_weights\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"squad\":\n                pointer = getattr(pointer, \"classifier\")\n            else:\n                try:\n                    pointer = getattr(pointer, scope_names[0])\n                except AttributeError:\n                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n                    continue\n            if len(scope_names) >= 2:\n                num = int(scope_names[1])\n                pointer = pointer[num]\n        if m_name[-11:] == \"_embeddings\":\n            pointer = getattr(pointer, \"weight\")\n        elif m_name == \"kernel\":\n            array = np.transpose(array)\n        try:\n            assert (\n                pointer.shape == array.shape\n            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(\"Initialize PyTorch weight {}\".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model", "\n\nclass BertEmbeddings(nn.Module):\n    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n\n    def __init__(self, config, prompt_ids):\n        super().__init__()\n        self.word_embeddings = PromptEmbedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id, prompt_ids = prompt_ids)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n\n    def forward(\n        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n    ):\n        if input_ids is not None:\n            input_shape = input_ids.size()\n        else:\n            input_shape = inputs_embeds.size()[:-1]\n\n        seq_length = input_shape[1]\n\n        if position_ids is None:\n            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n\n        if inputs_embeds is None:\n            inputs_embeds = self.word_embeddings(input_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = inputs_embeds + token_type_embeddings\n        if self.position_embedding_type == \"absolute\":\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings += position_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings", "\n\n\nclass PromptEmbedding(nn.Module):\n    __constants__ = ['num_embeddings', 'embedding_dim', 'padding_idx', 'max_norm',\n                     'norm_type', 'scale_grad_by_freq', 'sparse']\n\n    num_embeddings: int\n    embedding_dim: int\n    padding_idx: Optional[int]\n    max_norm: Optional[float]\n    norm_type: float\n    scale_grad_by_freq: bool\n    weight: torch.Tensor\n    sparse: bool\n\n    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None,\n                 max_norm: Optional[float] = None, norm_type: float = 2., scale_grad_by_freq: bool = False,\n                 sparse: bool = False, _weight: Optional[torch.Tensor] = None, prompt_ids = None) -> None:\n        super(PromptEmbedding, self).__init__()\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        if padding_idx is not None:\n            if padding_idx > 0:\n                assert padding_idx < self.num_embeddings, 'Padding_idx must be within num_embeddings'\n            elif padding_idx < 0:\n                assert padding_idx >= -self.num_embeddings, 'Padding_idx must be within num_embeddings'\n                padding_idx = self.num_embeddings + padding_idx\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        if _weight is None:\n            self.weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))\n            # self.prompt_weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))\n            self.reset_parameters()\n        else:\n            assert list(_weight.shape) == [num_embeddings, embedding_dim], \\\n                'Shape of weight does not match num_embeddings and embedding_dim'\n            self.weight = nn.Parameter(_weight)\n\n        self.sparse = sparse\n        \n        prompt_mask = torch.zeros_like(self.weight.data)\n        prompt_mask[prompt_ids] = 1\n\n        self.register_buffer(\"prompt_mask\", prompt_mask)\n\n    def reset_parameters(self) -> None:\n        torch.nn.init.normal_(self.weight)\n        self._fill_padding_idx_with_zero()\n\n    def _fill_padding_idx_with_zero(self) -> None:\n        if self.padding_idx is not None:\n            with torch.no_grad():\n                self.weight[self.padding_idx].fill_(0)\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        # fix_weight = self.weight.detach()\n        # weight_new = self.weight*self.prompt_mask + fix_weight*(1-self.prompt_mask)\n        return torch.nn.functional.embedding(\n            input, self.weight, self.padding_idx, self.max_norm,\n            self.norm_type, self.scale_grad_by_freq, self.sparse)\n\n    def extra_repr(self) -> str:\n        s = '{num_embeddings}, {embedding_dim}'\n        if self.padding_idx is not None:\n            s += ', padding_idx={padding_idx}'\n        if self.max_norm is not None:\n            s += ', max_norm={max_norm}'\n        if self.norm_type != 2:\n            s += ', norm_type={norm_type}'\n        if self.scale_grad_by_freq is not False:\n            s += ', scale_grad_by_freq={scale_grad_by_freq}'\n        if self.sparse is not False:\n            s += ', sparse=True'\n        return s.format(**self.__dict__)\n\n    @torch.no_grad()\n    def init_prompt_embeddings(self):\n        # new = torch.zeros(self.weight.data.size(0),self.weight.data.size(1)).normal_(mean=0.0, std=0.02)\n        # self.weight.data = self.weight.data*(1-self.prompt_mask) + new*self.prompt_mask\n        \n        # self.weight.data = self.weight.data*(1-self.prompt_mask)\n\n        self.weight.data[self.prompt_mask].normal_(mean=0.0, std=0.02)\n    \n    @classmethod\n    def from_pretrained(cls, embeddings, freeze=True, padding_idx=None,\n                        max_norm=None, norm_type=2., scale_grad_by_freq=False,\n                        sparse=False):\n        r\"\"\"Creates Embedding instance from given 2-dimensional FloatTensor.\n\n        Args:\n            embeddings (Tensor): FloatTensor containing weights for the Embedding.\n                First dimension is being passed to Embedding as ``num_embeddings``, second as ``embedding_dim``.\n            freeze (boolean, optional): If ``True``, the tensor does not get updated in the learning process.\n                Equivalent to ``embedding.weight.requires_grad = False``. Default: ``True``\n            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n                                         i.e. it remains as a fixed \"pad\".\n            max_norm (float, optional): See module initialization documentation.\n            norm_type (float, optional): See module initialization documentation. Default ``2``.\n            scale_grad_by_freq (boolean, optional): See module initialization documentation. Default ``False``.\n            sparse (bool, optional): See module initialization documentation.\n\n        Examples::\n\n            >>> # FloatTensor containing pretrained weights\n            >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n            >>> embedding = nn.Embedding.from_pretrained(weight)\n            >>> # Get embeddings for index 1\n            >>> input = torch.LongTensor([1])\n            >>> embedding(input)\n            tensor([[ 4.0000,  5.1000,  6.3000]])\n        \"\"\"\n        assert embeddings.dim() == 2, \\\n            'Embeddings parameter is expected to be 2-dimensional'\n        rows, cols = embeddings.shape\n        embedding = cls(\n            num_embeddings=rows,\n            embedding_dim=cols,\n            _weight=embeddings,\n            padding_idx=padding_idx,\n            max_norm=max_norm,\n            norm_type=norm_type,\n            scale_grad_by_freq=scale_grad_by_freq,\n            sparse=sparse)\n        embedding.weight.requires_grad = not freeze\n        return embedding", "\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            self.max_position_embeddings = config.max_position_embeddings\n            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n\n        self.is_decoder = config.is_decoder\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        mixed_query_layer = self.query(hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            seq_length = hidden_states.size()[1]\n            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n            distance = position_ids_l - position_ids_r\n            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n\n            if self.position_embedding_type == \"relative_key\":\n                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n\n        if self.is_decoder:\n            outputs = outputs + (past_key_value,)\n        return outputs", "\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states", "\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n        )\n\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions,\n        )\n        attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs", "\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states", "\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states", "\n\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.attention = BertAttention(config)\n        self.is_decoder = config.is_decoder\n        self.add_cross_attention = config.add_cross_attention\n        if self.add_cross_attention:\n            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n            self.crossattention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=self_attn_past_key_value,\n        )\n        attention_output = self_attention_outputs[0]\n\n        # if decoder, the last output is tuple of self-attn cache\n        if self.is_decoder:\n            outputs = self_attention_outputs[1:-1]\n            present_key_value = self_attention_outputs[-1]\n        else:\n            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n        cross_attn_present_key_value = None\n        if self.is_decoder and encoder_hidden_states is not None:\n            assert hasattr(\n                self, \"crossattention\"\n            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n\n            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                cross_attn_past_key_value,\n                output_attentions,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n\n            # add cross-attn cache to positions 3,4 of present_key_value tuple\n            cross_attn_present_key_value = cross_attention_outputs[-1]\n            present_key_value = present_key_value + cross_attn_present_key_value\n\n        layer_output = apply_chunking_to_forward(\n            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n        )\n        outputs = (layer_output,) + outputs\n\n        # if decoder, return the attn key/values as the last output\n        if self.is_decoder:\n            outputs = outputs + (present_key_value,)\n\n        return outputs\n\n    def feed_forward_chunk(self, attention_output):\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output", "\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict=True,\n    ):\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n\n        next_decoder_cache = () if use_cache else None\n        for i, layer_module in enumerate(self.layer):\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n            past_key_value = past_key_values[i] if past_key_values is not None else None\n\n            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n\n                if use_cache:\n                    logger.warn(\n                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n                        \"`use_cache=False`...\"\n                    )\n                    use_cache = False\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, past_key_value, output_attentions)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    past_key_value,\n                    output_attentions,\n                )\n\n            hidden_states = layer_outputs[0]\n            if use_cache:\n                next_decoder_cache += (layer_outputs[-1],)\n            if output_attentions:\n                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                if self.config.add_cross_attention:\n                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    next_decoder_cache,\n                    all_hidden_states,\n                    all_self_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_decoder_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n            cross_attentions=all_cross_attentions,\n        )", "\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output", "\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states", "\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n        self.decoder.bias = self.bias\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states)\n        return hidden_states", "\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores", "\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score", "\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score", "\n\nclass BertPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = BertConfig\n    load_tf_weights = load_tf_weights_in_bert\n    base_model_prefix = \"bert\"\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)", "\n\n@dataclass\nclass BertForPreTrainingOutput(ModelOutput):\n    \"\"\"\n    Output type of :class:`~transformers.BertForPreTraining`.\n\n    Args:\n        loss (`optional`, returned when ``labels`` is provided, ``torch.FloatTensor`` of shape :obj:`(1,)`):\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n            (classification) loss.\n        prediction_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        seq_relationship_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2)`):\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n            before SoftMax).\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n            sequence_length, sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n\n    loss: Optional[torch.FloatTensor] = None\n    prediction_logits: torch.FloatTensor = None\n    seq_relationship_logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None", "\n\nBERT_START_DOCSTRING = r\"\"\"\n\n    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n    pruning heads etc.)\n\n    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to", "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n    general usage and behavior.\n\n    Parameters:\n        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n            weights.\n\"\"\"", "            weights.\n\"\"\"\n\nBERT_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n            Indices of input sequence tokens in the vocabulary.\n\n            Indices can be obtained using :class:`~transformers.BertTokenizer`. See\n            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for", "            Indices can be obtained using :class:`~transformers.BertTokenizer`. See\n            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n            details.\n\n            `What are input IDs? <../glossary.html#input-ids>`__\n        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.", "            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            `What are attention masks? <../glossary.html#attention-mask>`__\n        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n            1]``:\n\n            - 0 corresponds to a `sentence A` token,\n            - 1 corresponds to a `sentence B` token.", "            - 0 corresponds to a `sentence A` token,\n            - 1 corresponds to a `sentence B` token.\n\n            `What are token type IDs? <../glossary.html#token-type-ids>`_\n        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n            config.max_position_embeddings - 1]``.\n\n            `What are position IDs? <../glossary.html#position-ids>`_\n        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):", "            `What are position IDs? <../glossary.html#position-ids>`_\n        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated", "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n            vectors than the model's internal embedding lookup matrix.\n        output_attentions (:obj:`bool`, `optional`):\n            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n            tensors for more detail.\n        output_hidden_states (:obj:`bool`, `optional`):\n            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n            more detail.\n        return_dict (:obj:`bool`, `optional`):", "            more detail.\n        return_dict (:obj:`bool`, `optional`):\n            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n    BERT_START_DOCSTRING,\n)\nclass BertModel(BertPreTrainedModel):\n    \"\"\"\n\n    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n\n    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n    input to the forward pass.\n    \"\"\"\n\n    def __init__(self, config, add_pooling_layer=True, prompt_ids = None):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = BertEmbeddings(config, prompt_ids = prompt_ids)\n        self.encoder = BertEncoder(config)\n\n        self.pooler = BertPooler(config) if add_pooling_layer else None\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n            batch_size, seq_length = input_shape\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n            batch_size, seq_length = input_shape\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )", "    BERT_START_DOCSTRING,\n)\nclass BertModel(BertPreTrainedModel):\n    \"\"\"\n\n    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n\n    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n    input to the forward pass.\n    \"\"\"\n\n    def __init__(self, config, add_pooling_layer=True, prompt_ids = None):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = BertEmbeddings(config, prompt_ids = prompt_ids)\n        self.encoder = BertEncoder(config)\n\n        self.pooler = BertPooler(config) if add_pooling_layer else None\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n            batch_size, seq_length = input_shape\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n            batch_size, seq_length = input_shape\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a `next\n    sentence prediction (classification)` head.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForPreTraining(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        next_sentence_label=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n\n            - 0 indicates sequence B is a continuation of sequence A,\n            - 1 indicates sequence B is a random sequence.\n        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n            Used to hide legacy arguments that have been deprecated.\n\n        Returns:\n\n        Example::\n\n            >>> from transformers import BertTokenizer, BertForPreTraining\n            >>> import torch\n\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            >>> model = BertForPreTraining.from_pretrained('bert-base-uncased')\n\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n\n            >>> prediction_logits = outputs.prediction_logits\n            >>> seq_relationship_logits = outputs.seq_relationship_logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output, pooled_output = outputs[:2]\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        total_loss = None\n        if labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n\n        if not return_dict:\n            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return BertForPreTrainingOutput(\n            loss=total_loss,\n            prediction_logits=prediction_scores,\n            seq_relationship_logits=seq_relationship_score,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForPreTraining(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        next_sentence_label=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n\n            - 0 indicates sequence B is a continuation of sequence A,\n            - 1 indicates sequence B is a random sequence.\n        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n            Used to hide legacy arguments that have been deprecated.\n\n        Returns:\n\n        Example::\n\n            >>> from transformers import BertTokenizer, BertForPreTraining\n            >>> import torch\n\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            >>> model = BertForPreTraining.from_pretrained('bert-base-uncased')\n\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n\n            >>> prediction_logits = outputs.prediction_logits\n            >>> seq_relationship_logits = outputs.seq_relationship_logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output, pooled_output = outputs[:2]\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        total_loss = None\n        if labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n\n        if not return_dict:\n            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return BertForPreTrainingOutput(\n            loss=total_loss,\n            prediction_logits=prediction_scores,\n            seq_relationship_logits=seq_relationship_score,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"Bert Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", BERT_START_DOCSTRING\n)\nclass BertLMHeadModel(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        if not config.is_decoder:\n            logger.warning(\"If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\")\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n\n        Returns:\n\n        Example::\n\n            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n            >>> import torch\n\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n            >>> config.is_decoder = True\n            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n\n            >>> prediction_logits = outputs.logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if labels is not None:\n            use_cache = False\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        lm_loss = None\n        if labels is not None:\n            # we are doing next-token prediction; shift prediction scores and input ids by one\n            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n            labels = labels[:, 1:].contiguous()\n            loss_fct = CrossEntropyLoss()\n            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((lm_loss,) + output) if lm_loss is not None else output\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=lm_loss,\n            logits=prediction_scores,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n        input_shape = input_ids.shape\n        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n        if attention_mask is None:\n            attention_mask = input_ids.new_ones(input_shape)\n\n        # cut decoder_input_ids if past is used\n        if past is not None:\n            input_ids = input_ids[:, -1:]\n\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n\n    def _reorder_cache(self, past, beam_idx):\n        reordered_past = ()\n        for layer_past in past:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n        return reordered_past", "\n\n@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top. \"\"\", BERT_START_DOCSTRING)\nclass BertForMaskedLM(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        if config.is_decoder:\n            logger.warning(\n                \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n                \"bi-directional self-attention.\"\n            )\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MaskedLMOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return MaskedLMOutput(\n            loss=masked_lm_loss,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n        input_shape = input_ids.shape\n        effective_batch_size = input_shape[0]\n\n        #  add a dummy token\n        assert self.config.pad_token_id is not None, \"The PAD token should be defined for generation\"\n        attention_mask = torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n        dummy_token = torch.full(\n            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n        )\n        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}", "\n\n@add_start_docstrings(\n    \"\"\"Bert Model with a `next sentence prediction (classification)` head on top. \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see ``input_ids`` docstring). Indices should be in ``[0, 1]``:\n\n            - 0 indicates sequence B is a continuation of sequence A,\n            - 1 indicates sequence B is a random sequence.\n\n        Returns:\n\n        Example::\n\n            >>> from transformers import BertTokenizer, BertForNextSentencePrediction\n            >>> import torch\n\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            >>> model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n\n            >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n            >>> next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n            >>> encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n\n            >>> outputs = model(**encoding, labels=torch.LongTensor([1]))\n            >>> logits = outputs.logits\n            >>> assert logits[0, 0] < logits[0, 1] # next sentence was random\n        \"\"\"\n\n        if \"next_sentence_label\" in kwargs:\n            warnings.warn(\n                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n                FutureWarning,\n            )\n            labels = kwargs.pop(\"next_sentence_label\")\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        seq_relationship_scores = self.cls(pooled_output)\n\n        next_sentence_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n\n        if not return_dict:\n            output = (seq_relationship_scores,) + outputs[2:]\n            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n\n        return NextSentencePredictorOutput(\n            loss=next_sentence_loss,\n            logits=seq_relationship_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n    output) e.g. for GLUE tasks.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n    softmax) e.g. for RocStories/SWAG tasks.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForMultipleChoice(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n            :obj:`input_ids` above)\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n        inputs_embeds = (\n            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n            if inputs_embeds is not None\n            else None\n        )\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n\n        if not return_dict:\n            output = (reshaped_logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MultipleChoiceModelOutput(\n            loss=loss,\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForMultipleChoice(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n            :obj:`input_ids` above)\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n        inputs_embeds = (\n            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n            if inputs_embeds is not None\n            else None\n        )\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n\n        if not return_dict:\n            output = (reshaped_logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MultipleChoiceModelOutput(\n            loss=loss,\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n    Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForTokenClassification(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n            1]``.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)\n                active_labels = torch.where(\n                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n                )\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForTokenClassification(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n            1]``.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)\n                active_labels = torch.where(\n                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n                )\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForQuestionAnswering(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        start_positions=None,\n        end_positions=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass BertForQuestionAnswering(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        start_positions=None,\n        end_positions=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ""]}
{"filename": "prompt4ner/prompt4ner_trainer.py", "chunked_list": ["import argparse\nfrom collections import defaultdict\nimport json\nimport math\nimport os\nimport torch\nfrom torch.optim import Optimizer\nimport transformers\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW, AutoTokenizer, AutoConfig", "from torch.utils.data import DataLoader\nfrom transformers import AdamW, AutoTokenizer, AutoConfig\n\nfrom prompt4ner import models\nfrom prompt4ner import sampling\nfrom prompt4ner import util\nfrom prompt4ner.entities import Dataset\nfrom prompt4ner.evaluator import Evaluator\nfrom prompt4ner.input_reader import JsonInputReader, BaseInputReader\nfrom prompt4ner.loss import Prompt4NERLoss, Loss", "from prompt4ner.input_reader import JsonInputReader, BaseInputReader\nfrom prompt4ner.loss import Prompt4NERLoss, Loss\nfrom tqdm import tqdm\nfrom prompt4ner.trainer import BaseTrainer\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.optim.lr_scheduler import LambdaLR\n\nSCRIPT_PATH = os.path.dirname(os.path.realpath(__file__))\n", "SCRIPT_PATH = os.path.dirname(os.path.realpath(__file__))\n\n\ndef get_linear_schedule_with_warmup_two_stage(optimizer, num_warmup_steps_stage_one, num_training_steps_stage_one, num_warmup_steps_stage_two, num_training_steps_stage_two, stage_one_lr_scale, last_epoch=-1):\n    def lr_lambda(current_step: int):\n        if current_step < num_training_steps_stage_one:\n            if current_step < num_warmup_steps_stage_one:\n                return float(current_step) / float(max(1, num_warmup_steps_stage_one))\n            return max(\n                0.0, float(num_training_steps_stage_one - current_step) / float(max(1, num_training_steps_stage_one - num_warmup_steps_stage_one)) * stage_one_lr_scale\n            )\n        else:\n            current_step = current_step - num_training_steps_stage_one\n            if current_step < num_warmup_steps_stage_two:\n                return float(current_step) / float(max(1, num_warmup_steps_stage_two))\n            return max(\n                0.0, float(num_training_steps_stage_two - current_step) / float(max(1, num_training_steps_stage_two - num_warmup_steps_stage_two))\n            )\n    return LambdaLR(optimizer, lr_lambda, last_epoch)", "\nclass Prompt4NERTrainer(BaseTrainer):\n    \"\"\" Joint entity and relation extraction training and evaluation \"\"\"\n\n    def __init__(self, args: argparse.Namespace):\n        super().__init__(args)\n\n        # byte-pair encoding\n\n        self._tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path,\n                                                        local_files_only = True,\n                                                        do_lower_case=args.lowercase,\n                                                        cache_dir=args.cache_path,\n                                                        use_fast = False)\n\n        self._processor = None\n        if args.withimage:\n            self._processor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\",\n                                                        local_files_only = True,\n                                                        cache_dir=args.cache_path)\n        \n        # path to export predictions to\n        self._predictions_path = os.path.join(self._log_path, 'predictions_%s_epoch_%s.json')\n\n        # path to export relation extraction examples to\n        self._examples_path = os.path.join(self._log_path, 'examples_%s_%s_epoch_%s.html')\n\n        self._logger.info(json.dumps(vars(args), indent=4, sort_keys=True))\n\n    def load_model(self, input_reader, is_eval = False):\n        args = self.args\n        # create model\n        model_class = models.get_model(args.model_type)\n        \n        # load model\n        clip_v = None\n        if args.withimage:\n            clip_v = transformers.CLIPVisionModel.from_pretrained('openai/clip-vit-base-patch32')\n        config = AutoConfig.from_pretrained(args.model_path, cache_dir=args.cache_path)\n        model = model_class.from_pretrained(args.model_path,\n                                            ignore_mismatched_sizes=True,\n                                            # proxies = {'http': '10.15.82.42:7890'},\n                                            local_files_only = True,\n                                            config = config,\n                                            # Prompt4NER model parameters\n                                            entity_type_count=input_reader.entity_type_count,\n                                            prop_drop=args.prop_drop,\n                                            freeze_transformer=args.freeze_transformer,\n                                            lstm_layers = args.lstm_layers,\n                                            decoder_layers = args.decoder_layers,\n                                            pool_type = args.pool_type,\n                                            prompt_individual_attention = args.prompt_individual_attention,\n                                            sentence_individual_attention = args.sentence_individual_attention,\n                                            use_masked_lm = args.use_masked_lm,\n                                            last_layer_for_loss = args.last_layer_for_loss,\n                                            split_epoch = args.split_epoch,\n                                            clip_v = clip_v,\n                                            prompt_length = args.prompt_length,\n                                            prompt_number = args.prompt_number,\n                                            prompt_token_ids = input_reader.prompt_token_ids)\n        return model\n\n    def train(self, train_path: str, valid_path: str, types_path: str, input_reader_cls: BaseInputReader):\n        args = self.args\n        train_label, valid_label = 'train', 'valid'\n\n        if self.record:\n            self._logger.info(\"Datasets: %s, %s\" % (train_path, valid_path))\n            self._logger.info(\"Model type: %s\" % args.model_type)\n\n            # create log csv files\n            self._init_train_logging(train_label)\n            self._init_eval_logging(valid_label)\n\n        # read datasets\n        input_reader = input_reader_cls(\n            types_path, \n            self._tokenizer, \n            self._processor,\n            self._logger, \n            random_mask_word = args.use_masked_lm, \n            repeat_gt_entities = args.repeat_gt_entities,\n            prompt_length = args.prompt_length,\n            prompt_type = args.prompt_type,\n            prompt_number = args.prompt_number)\n            \n        input_reader.read({train_label: train_path, valid_label: valid_path})\n\n        if self.local_rank < 1:\n            self._log_datasets(input_reader)\n\n        world_size = 1\n        if args.local_rank != -1:\n            world_size = dist.get_world_size()\n\n        train_dataset = input_reader.get_dataset(train_label)\n        train_sample_count = train_dataset.document_count\n        updates_epoch = math.ceil(train_sample_count / (args.train_batch_size * world_size))\n        updates_total_stage_one = updates_epoch * args.split_epoch\n        updates_total_stage_two = updates_epoch * (args.epochs - args.split_epoch)\n\n        validation_dataset = input_reader.get_dataset(valid_label)\n\n        if self.record:\n            self._logger.info(\"Updates per epoch: %s\" % updates_epoch)\n            self._logger.info(\"Updates total: %s\" % (updates_total_stage_one + updates_total_stage_two))\n\n        model = self.load_model(input_reader, is_eval = False)\n        self._logger.info(model)\n\n        model.to(self._device)\n        if args.local_rank != -1:\n            model = DDP(model, device_ids=[args.local_rank], find_unused_parameters=False)\n\n        # create optimizer\n        optimizer_params = self._get_optimizer_params(model)\n        optimizer = AdamW(optimizer_params, lr=args.lr, weight_decay=args.weight_decay, correct_bias=False)\n        scheduler = get_linear_schedule_with_warmup_two_stage(optimizer,\n                                                            num_warmup_steps_stage_one = args.lr_warmup * updates_total_stage_one,\n                                                            num_training_steps_stage_one = updates_total_stage_one,\n                                                            num_warmup_steps_stage_two = args.lr_warmup * updates_total_stage_two,\n                                                            num_training_steps_stage_two = updates_total_stage_two,\n                                                            stage_one_lr_scale = args.stage_one_lr_scale)\n\n\n        compute_loss = Prompt4NERLoss(input_reader.entity_type_count, self._device, model, optimizer, scheduler, args.max_grad_norm, args.nil_weight, args.match_class_weight, args.match_boundary_weight, args.loss_class_weight, args.loss_boundary_weight, args.type_loss, solver = args.match_solver, match_warmup_epoch = args.match_warmup_epoch)\n\n        # eval validation set\n        if args.init_eval and self.record:\n            self._eval(model, validation_dataset, input_reader, 0, updates_epoch)\n\n        # train\n        best_f1 = 0\n        best_epoch = 0\n        for epoch in range(args.epochs):\n            if epoch == args.split_epoch:\n                optimizer.__setstate__({'state': defaultdict(dict)})\n            # train epoch\n            self._train_epoch(model, compute_loss, optimizer, train_dataset, updates_epoch, epoch)\n\n            # eval validation sets\n            if (not args.final_eval or (epoch == args.epochs - 1)) and self.record and ((epoch%args.eval_every_epochs)==0 or (epoch == args.epochs - 1)):\n                f1 = self._eval(model, validation_dataset, input_reader, epoch + 1, updates_epoch)\n                if best_f1 < f1[2]:\n                    self._logger.info(f\"Best F1 score update, from {best_f1} to {f1[2]}\")\n                    best_f1 = f1[2]\n                    best_epoch = epoch + 1\n                    extra = dict(epoch=epoch, updates_epoch=updates_epoch, epoch_iteration=0)\n                    self._save_model(self._save_path, model, self._tokenizer, epoch * updates_epoch,\n                        optimizer=optimizer if args.save_optimizer else None, extra=extra,\n                        include_iteration=False, name='best_model')\n            if self.record and ((epoch%args.eval_every_epochs)==0 or (epoch == args.epochs - 1)):\n                if args.save_path_include_iteration:\n                    self._save_model(self._save_path, model, self._tokenizer, epoch,\n                            optimizer=optimizer if args.save_optimizer else None, extra=extra,\n                            include_iteration=args.save_path_include_iteration, name='model')\n                self._logger.info(f\"Best F1 score: {best_f1}, achieved at Epoch: {best_epoch}\")\n\n        # save final model\n        extra = dict(epoch=args.epochs, updates_epoch=updates_epoch, epoch_iteration=0)\n        global_iteration = args.epochs * updates_epoch\n        if self.record:\n            self._save_model(self._save_path, model, self._tokenizer, global_iteration,\n                            optimizer=optimizer if args.save_optimizer else None, extra=extra,\n                            include_iteration=False, name='final_model')\n            self._logger.info(\"Logged in: %s\" % self._log_path)\n            self._logger.info(\"Saved in: %s\" % self._save_path)\n            self._close_summary_writer()\n\n    def eval(self, dataset_path: str, types_path: str, input_reader_cls: BaseInputReader):\n        args = self.args\n        dataset_label = 'test'\n\n        self._logger.info(\"Dataset: %s\" % dataset_path)\n        self._logger.info(\"Model: %s\" % args.model_type)\n\n        # create log csv files\n        self._init_eval_logging(dataset_label)\n\n        # read datasets\n        input_reader = input_reader_cls(\n            types_path, \n            self._tokenizer, \n            self._processor,\n            self._logger, \n            random_mask_word = args.use_masked_lm, \n            repeat_gt_entities = args.repeat_gt_entities,\n            prompt_length = args.prompt_length,\n            prompt_type = args.prompt_type,\n            prompt_number = args.prompt_number)\n        \n        input_reader.read({dataset_label: dataset_path})\n        self._log_datasets(input_reader)\n\n        model = self.load_model(input_reader, is_eval = True)\n\n        model.to(self._device)\n        \n        # evaluate\n        self._eval(model, input_reader.get_dataset(dataset_label), input_reader)\n\n        self._logger.info(\"Logged in: %s\" % self._log_path)\n        self._close_summary_writer()\n\n    def _train_epoch(self, model: torch.nn.Module, compute_loss: Loss, optimizer: Optimizer, dataset,\n                     updates_epoch: int, epoch: int):\n        args = self.args\n        self._logger.info(\"Train epoch: %s\" % epoch)\n\n        # create data loader\n        dataset.switch_mode(Dataset.TRAIN_MODE)\n\n        world_size = 1\n        if args.local_rank != -1:\n            world_size = dist.get_world_size()\n\n        train_sampler = None\n        shuffle = False\n        if isinstance(dataset, Dataset):\n            if len(dataset) < 100000:\n                shuffle = True\n            if args.local_rank != -1:\n                train_sampler = torch.utils.data.distributed.DistributedSampler(dataset, num_replicas = world_size,rank = args.local_rank, shuffle = shuffle)\n                shuffle = False\n\n        data_loader = DataLoader(dataset, batch_size=args.train_batch_size, shuffle=shuffle, drop_last=False,\n                                    num_workers=args.sampling_processes, collate_fn=sampling.collate_fn_padding,  sampler=train_sampler)\n                                    \n\n        model.zero_grad()\n\n        iteration = 0\n        total = math.ceil(dataset.document_count / (args.train_batch_size * world_size))\n        for batch in tqdm(data_loader, total=total, desc='Train epoch %s' % epoch):\n            if epoch == 0 and iteration == 0:\n                for k, v in batch.items():\n                    torch.set_printoptions(profile='full')\n                    if v is None:\n                        continue\n                    if isinstance(v, dict):\n                        for sub_k, sub_v in v.items():\n                            extended_k = k + ' -> ' + sub_k\n                            self._logger.info(extended_k)\n                            self._logger.info(sub_v[:2].size())\n                    else:\n                        if isinstance(v, torch.Tensor) and v[:2].numel()> 5120:\n                            torch.set_printoptions(profile='default')\n                        self._logger.info(k)\n                        # if sum(v.size()[1:]) > \n                        self._logger.info(v[:2])\n                # torch.set_printoptions(profile='default')\n            model.train()\n            batch = util.to_device(batch, self._device)\n\n            # forward step\n            entity_logits, p_left, p_right, masked_seq_logits, output = model(\n                encodings=batch['encodings'], \n                context_masks=batch['context_masks'], \n                raw_context_masks=batch['raw_context_masks'], \n                inx4locator = batch[\"inx4locator\"],\n                pos_encoding = batch[\"pos_encoding\"],\n                seg_encoding = batch['seg_encoding'], \n                context2token_masks=batch['context2token_masks'], \n                token_masks=batch['token_masks'],\n                image_inputs = batch['image_inputs'], \n                meta_doc = batch['meta_doc'], \n                epoch = epoch)\n\n            # compute loss and optimize parameters\n            batch_loss = compute_loss.compute(entity_logits, p_left, p_right, output, gt_types=batch['gt_types'], gt_spans = batch['gt_spans'], entity_masks=batch['entity_masks'], epoch = epoch,  deeply_weight = args.deeply_weight, seq_logits = masked_seq_logits, gt_seq_labels=batch['gt_seq_labels'], batch = batch)\n\n            # logging\n            iteration += 1\n            global_iteration = epoch * updates_epoch + iteration\n\n            if global_iteration % args.train_log_iter == 0 and self.local_rank < 1:\n                self._log_train(optimizer, batch_loss, epoch, iteration, global_iteration, dataset.label)\n\n        return iteration\n\n    def _eval(self, model: torch.nn.Module, dataset, input_reader: JsonInputReader,\n              epoch: int = 0, updates_epoch: int = 0, iteration: int = 0):\n        args = self.args\n        self._logger.info(\"Evaluate: %s\" % dataset.label)\n\n        # create evaluator\n        evaluator = Evaluator(dataset, input_reader, self._tokenizer, self._logger, args.no_overlapping, args.no_partial_overlapping, args.no_duplicate, self._predictions_path,\n                              self._examples_path, args.example_count, epoch, dataset.label, cls_threshold = args.cls_threshold, boundary_threshold = args.boundary_threshold, save_prediction = args.store_predictions)\n\n        # create data loader\n        dataset.switch_mode(Dataset.EVAL_MODE)\n\n        world_size = 1\n        eval_sampler = None\n\n        if isinstance(dataset, Dataset):\n            data_loader = DataLoader(dataset, batch_size=args.eval_batch_size, shuffle=False, drop_last=False,\n                                 num_workers=args.sampling_processes, collate_fn=sampling.collate_fn_padding, sampler=eval_sampler)\n        else:\n            data_loader = DataLoader(dataset, batch_size=args.eval_batch_size, drop_last=False, collate_fn=sampling.collate_fn_padding, sampler=eval_sampler)\n\n        with torch.no_grad():\n            model.eval()\n\n            # iterate batches\n            total = math.ceil(dataset.document_count / (args.eval_batch_size * world_size))\n            for batch in tqdm(data_loader, total=total, desc='Evaluate epoch %s' % epoch):\n                # move batch to selected device\n                batch = util.to_device(batch, self._device)\n\n                # run model (forward pass)\n                entity_logits, p_left, p_right, _, outputs = model(\n                    encodings=batch['encodings'], \n                    context_masks=batch['context_masks'], \n                    raw_context_masks=batch['raw_context_masks'], \n                    inx4locator = batch[\"inx4locator\"],\n                    pos_encoding = batch[\"pos_encoding\"],\n                    seg_encoding = batch['seg_encoding'], \n                    context2token_masks=batch['context2token_masks'], \n                    token_masks=batch['token_masks'],\n                    image_inputs = batch['image_inputs'], \n                    meta_doc = batch['meta_doc'], \n                    evaluate = True)\n\n                # evaluate batch\n                evaluator.eval_batch(entity_logits, p_left, p_right, outputs, batch)\n        global_iteration = epoch * updates_epoch + iteration\n        ner_eval, ner_loc_eval, ner_cls_eval = evaluator.compute_scores()\n        self._log_eval(*ner_eval, *ner_loc_eval, *ner_cls_eval, epoch, iteration, global_iteration, dataset.label)\n\n        if args.store_predictions:\n            evaluator.store_predictions()\n\n        if args.store_examples:\n            evaluator.store_examples()\n        \n        return ner_eval\n\n    def _get_optimizer_params(self, model):\n        param_optimizer = list(model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        # regressier\n        optimizer_params = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n             'weight_decay': self.args.weight_decay},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n        return optimizer_params\n\n    def _log_train(self, optimizer: Optimizer, loss: float, epoch: int,\n                   iteration: int, global_iteration: int, label: str):\n        # average loss\n        avg_loss = loss / self.args.train_batch_size\n        # get current learning rate\n        lr = self._get_lr(optimizer)[0]\n\n        # log to tensorboard\n        self._log_tensorboard(label, 'loss', loss, global_iteration)\n        self._log_tensorboard(label, 'loss_avg', avg_loss, global_iteration)\n        self._log_tensorboard(label, 'lr', lr, global_iteration)\n\n        # log to csv\n        self._log_csv(label, 'loss', loss, epoch, iteration, global_iteration)\n        self._log_csv(label, 'loss_avg', avg_loss, epoch, iteration, global_iteration)\n        self._log_csv(label, 'lr', lr, epoch, iteration, global_iteration)\n\n    def _log_eval(self, ner_prec_micro: float, ner_rec_micro: float, ner_f1_micro: float,\n                  ner_prec_macro: float, ner_rec_macro: float, ner_f1_macro: float,\n                  loc_prec_micro: float, loc_rec_micro: float, loc_f1_micro: float,\n                  loc_prec_macro: float, loc_rec_macro: float, loc_f1_macro: float,\n                  cls_prec_micro: float, cls_rec_micro: float, cls_f1_micro: float,\n                  cls_prec_macro: float, cls_rec_macro: float, cls_f1_macro: float,\n                  epoch: int, iteration: int, global_iteration: int, label: str):\n\n        # log to tensorboard\n        self._log_tensorboard(label, 'eval/ner_prec_micro', ner_prec_micro, global_iteration)\n        self._log_tensorboard(label, 'eval/ner_recall_micro', ner_rec_micro, global_iteration)\n        self._log_tensorboard(label, 'eval/ner_f1_micro', ner_f1_micro, global_iteration)\n        self._log_tensorboard(label, 'eval/ner_prec_macro', ner_prec_macro, global_iteration)\n        self._log_tensorboard(label, 'eval/ner_recall_macro', ner_rec_macro, global_iteration)\n        self._log_tensorboard(label, 'eval/ner_f1_macro', ner_f1_macro, global_iteration)\n\n\n        self._log_tensorboard(label, 'eval/loc_prec_micro', loc_prec_micro, global_iteration)\n        self._log_tensorboard(label, 'eval/loc_recall_micro', loc_rec_micro, global_iteration)\n        self._log_tensorboard(label, 'eval/loc_f1_micro', loc_f1_micro, global_iteration)\n        self._log_tensorboard(label, 'eval/loc_prec_macro', loc_prec_macro, global_iteration)\n        self._log_tensorboard(label, 'eval/loc_recall_macro', loc_rec_macro, global_iteration)\n        self._log_tensorboard(label, 'eval/loc_f1_macro', loc_f1_macro, global_iteration)\n\n        self._log_tensorboard(label, 'eval/cls_prec_micro', cls_prec_micro, global_iteration)\n        self._log_tensorboard(label, 'eval/cls_recall_micro', cls_rec_micro, global_iteration)\n        self._log_tensorboard(label, 'eval/cls_f1_micro', cls_f1_micro, global_iteration)\n        self._log_tensorboard(label, 'eval/cls_prec_macro', cls_prec_macro, global_iteration)\n        self._log_tensorboard(label, 'eval/cls_recall_macro', cls_rec_macro, global_iteration)\n        self._log_tensorboard(label, 'eval/cls_f1_macro', cls_f1_macro, global_iteration)\n\n\n        # log to csv\n        self._log_csv(label, 'eval', ner_prec_micro, ner_rec_micro, ner_f1_micro,\n                      ner_prec_macro, ner_rec_macro, ner_f1_macro,\n                      loc_prec_micro, loc_rec_micro, loc_f1_micro,\n                      loc_prec_macro, loc_rec_macro, loc_f1_macro,\n                      cls_prec_micro, cls_rec_micro, cls_f1_micro,\n                      cls_prec_macro, cls_rec_macro, cls_f1_macro,\n                      epoch, iteration, global_iteration)\n\n    def _log_datasets(self, input_reader):\n        self._logger.info(\"Entity type count: %s\" % input_reader.entity_type_count)\n\n        self._logger.info(\"Entities:\")\n        for e in input_reader.entity_types.values():\n            self._logger.info(e.verbose_name + '=' + str(e.index))\n\n        for k, d in input_reader.datasets.items():\n            self._logger.info('Dataset: %s' % k)\n            self._logger.info(\"Document count: %s\" % d.document_count)\n            self._logger.info(\"Entity count: %s\" % d.entity_count)\n\n        self._logger.info(\"Context size: %s\" % input_reader.context_size)\n\n    def _init_train_logging(self, label):\n        self._add_dataset_logging(label,\n                                  data={'lr': ['lr', 'epoch', 'iteration', 'global_iteration'],\n                                        'loss': ['loss', 'epoch', 'iteration', 'global_iteration'],\n                                        'loss_avg': ['loss_avg', 'epoch', 'iteration', 'global_iteration']})\n\n    def _init_eval_logging(self, label):\n        self._add_dataset_logging(label,\n                                  data={'eval': ['ner_prec_micro', 'ner_rec_micro', 'ner_f1_micro',\n                                                 'ner_prec_macro', 'ner_rec_macro', 'ner_f1_macro',\n                                                 'loc_prec_micro', 'loc_rec_micro', 'loc_f1_micro',\n                                                 'loc_prec_macro', 'loc_rec_macro', 'loc_f1_macro',\n                                                 'cls_prec_micro', 'cls_rec_micro', 'cls_f1_micro',\n                                                 'cls_prec_macro', 'cls_rec_macro', 'cls_f1_macro',\n                                                 'epoch', 'iteration', 'global_iteration']})", "\n "]}
{"filename": "prompt4ner/sampling.py", "chunked_list": ["import random\nimport torch\nfrom prompt4ner import util\n\n\ndef create_train_sample(doc, random_mask = False, tokenizer = None, processor = None, repeat_gt_entities = -1):\n    encodings = doc.encoding\n    raw_encoding = doc.raw_encoding\n    inx4locator = doc.inx4locator\n    pos_encoding = doc.pos_encoding\n    images = doc.images\n\n    if images is not None and len(images)>0:\n        images = images[0]\n        image_inputs = images.apply(processor)\n    else:\n        images = None\n        image_inputs = None\n\n    seg_encoding = doc.seg_encoding\n    # if len(doc.encoding) > 512:\n    #     return None\n    token_count = len(doc.tokens)\n    context_size = len(encodings)\n    raw_context_size = len(raw_encoding)\n\n    gt_seq_labels = [0] * len(encodings)\n    special_tokens_map = tokenizer.special_tokens_map\n    if random_mask:\n        if random.random() < 0.5:\n            for i in range(len(raw_encoding) -1):\n                replace_rnd = random.random()\n                if replace_rnd < 0.15 and i != 0:\n                    gt_seq_labels[i] = encodings[i]\n                    strategy_rnd = random.random()\n                    if strategy_rnd < 0.8:\n                        encodings[i] = tokenizer.convert_tokens_to_ids(special_tokens_map['mask_token'])\n                    elif strategy_rnd < 0.9:\n                        encodings[i] = random.randint(0, tokenizer.vocab_size - 1)\n    context2token_masks = []\n    for t in doc.tokens:\n        context2token_masks.append(create_entity_mask(*t.span, context_size))\n\n    gt_entities_spans_token = []\n    gt_entity_types = []\n    gt_entity_masks = []\n    \n    for e in doc.entities:\n        gt_entities_spans_token.append(e.span_token)\n        gt_entity_types.append(e.entity_type.index)\n        gt_entity_masks.append(1)\n\n    if repeat_gt_entities != -1:\n        if len(doc.entities)!=0:\n            k = repeat_gt_entities//len(doc.entities)\n            m = repeat_gt_entities%len(doc.entities)\n            gt_entities_spans_token = gt_entities_spans_token*k + gt_entities_spans_token[:m]\n            gt_entity_types = gt_entity_types*k + gt_entity_types[:m]\n            gt_entity_masks = gt_entity_masks*k + gt_entity_masks[:m]\n            assert len(gt_entities_spans_token) == len(gt_entity_types) == len(gt_entity_masks) == repeat_gt_entities\n\n    encodings = torch.tensor(encodings, dtype=torch.long)\n    seg_encoding = torch.tensor(seg_encoding, dtype=torch.long)\n    gt_seq_labels = torch.tensor(gt_seq_labels, dtype=torch.long)\n    if inx4locator is not None:\n        inx4locator = torch.tensor(inx4locator, dtype=torch.long)\n    pos_encoding = torch.tensor(pos_encoding, dtype=torch.long)\n\n    context_masks = torch.ones(context_size, dtype=torch.bool)\n    raw_context_masks = torch.ones(raw_context_size, dtype=torch.bool)\n\n    token_masks = torch.ones(token_count, dtype=torch.bool)\n\n    context2token_masks = torch.stack(context2token_masks)\n\n    if len(gt_entity_types) > 0:\n        gt_entity_types = torch.tensor(gt_entity_types, dtype=torch.long)\n        gt_entity_spans_token = torch.tensor(gt_entities_spans_token, dtype=torch.long)\n        gt_entity_masks = torch.tensor(gt_entity_masks, dtype=torch.bool)\n    else:\n        gt_entity_types = torch.zeros([1], dtype=torch.long)\n        gt_entity_spans_token = torch.zeros([1, 2], dtype=torch.long)\n        gt_entity_masks = torch.zeros([1], dtype=torch.bool)\n\n    return dict(encodings=encodings, context_masks=context_masks, raw_context_masks = raw_context_masks, inx4locator= inx4locator, pos_encoding = pos_encoding, seg_encoding = seg_encoding, context2token_masks=context2token_masks, token_masks=token_masks, \n                gt_types=gt_entity_types, gt_spans=gt_entity_spans_token, entity_masks=gt_entity_masks, gt_seq_labels = gt_seq_labels, image_inputs =image_inputs, meta_doc = doc)", "\n\ndef create_eval_sample(doc, processor = None):\n    # if len(doc.encoding) > 512:\n    #     return None\n    encodings = doc.encoding\n    raw_encoding = doc.raw_encoding\n    inx4locator = doc.inx4locator\n    pos_encoding = doc.pos_encoding\n    images = doc.images\n\n    if len(images)>0:\n        images = images[0]\n        image_inputs = images.apply(processor)\n    else:\n        images = None\n        image_inputs = None\n\n    seg_encoding = doc.seg_encoding\n    token_count = len(doc.tokens)\n    context_size = len(encodings)\n    raw_context_size = len(raw_encoding)\n    \n    context2token_masks = []\n    for t in doc.tokens:\n        context2token_masks.append(create_entity_mask(*t.span, context_size))\n\n    # create tensors\n    # token indices\n    encodings = torch.tensor(encodings, dtype=torch.long)\n    seg_encoding = torch.tensor(seg_encoding, dtype=torch.long)\n    if inx4locator is not None:\n        inx4locator = torch.tensor(inx4locator, dtype=torch.long)\n    pos_encoding = torch.tensor(pos_encoding, dtype=torch.long)\n\n    \n    # masking of tokens\n    context_masks = torch.ones(context_size, dtype=torch.bool)\n    raw_context_masks = torch.ones(raw_context_size, dtype=torch.bool)\n\n    token_masks = torch.ones(token_count, dtype=torch.bool)\n\n    context2token_masks = torch.stack(context2token_masks)\n\n    return dict(encodings=encodings, context_masks=context_masks, raw_context_masks =raw_context_masks, inx4locator = inx4locator, pos_encoding= pos_encoding, seg_encoding = seg_encoding, context2token_masks=context2token_masks, token_masks=token_masks, \n                image_inputs =image_inputs, meta_doc = doc)", "\ndef create_entity_mask(start, end, context_size):\n    mask = torch.zeros(context_size, dtype=torch.bool)\n    mask[start:end+1] = 1\n    return mask\n\ndef collate_fn_padding(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    padded_batch = dict()\n    keys = batch[0].keys()\n    \n    for key in keys:\n        samples = [s[key] for s in batch]\n        if key.startswith(\"meta\"):\n            padded_batch[key] = samples\n            continue\n\n        if key.startswith(\"image_inputs\"):\n            if batch[0][\"image_inputs\"] == None:\n                padded_batch[\"image_inputs\"] = None\n            else:\n                padded_batch[\"image_inputs\"] = dict((k , torch.cat([s[\"image_inputs\"][k] for s in batch], dim=0) ) for k in batch[0][\"image_inputs\"].keys())\n            continue\n        \n        if batch[0][key] is None:\n            padded_batch[key] = None\n            continue\n\n        if not batch[0][key].shape:\n            padded_batch[key] = torch.stack(samples)\n        else:\n            padded_batch[key] = util.padded_stack([s[key] for s in batch])\n\n    return padded_batch", ""]}
{"filename": "prompt4ner/entities.py", "chunked_list": ["from collections import OrderedDict\nimport io\nimport json\nfrom typing import List\nimport requests\nfrom torch.utils import data\nfrom torch.utils.data import Dataset as TorchDataset\nfrom torch.utils.data import IterableDataset as IterableTorchDataset\nfrom prompt4ner import sampling\nimport itertools", "from prompt4ner import sampling\nimport itertools\nimport torch.distributed as dist\nimport os\nfrom PIL import Image\n\nclass RelationType:\n    def __init__(self, identifier, index, short_name, verbose_name, symmetric=False):\n        self._identifier = identifier\n        self._index = index\n        self._short_name = short_name\n        self._verbose_name = verbose_name\n        self._symmetric = symmetric\n\n    @property\n    def identifier(self):\n        return self._identifier\n\n    @property\n    def index(self):\n        return self._index\n\n    @property\n    def short_name(self):\n        return self._short_name\n\n    @property\n    def verbose_name(self):\n        return self._verbose_name\n\n    @property\n    def symmetric(self):\n        return self._symmetric\n\n    def __int__(self):\n        return self._index\n\n    def __eq__(self, other):\n        if isinstance(other, RelationType):\n            return self._identifier == other._identifier\n        return False\n\n    def __hash__(self):\n        return hash(self._identifier)", "\n\nclass EntityType:\n    def __init__(self, identifier, index, short_name, verbose_name):\n        self._identifier = identifier\n        self._index = index\n        self._short_name = short_name\n        self._verbose_name = verbose_name\n\n    @property\n    def identifier(self):\n        return self._identifier\n\n    @property\n    def index(self):\n        return self._index\n\n    @property\n    def short_name(self):\n        return self._short_name\n\n    @property\n    def verbose_name(self):\n        return self._verbose_name\n\n    def __int__(self):\n        return self._index\n\n    def __eq__(self, other):\n        if isinstance(other, EntityType):\n            return self._identifier == other._identifier\n        return False\n\n    def __hash__(self):\n        return hash(self._identifier)\n\n    def __str__(self) -> str:\n        return self._identifier + \"=\" + self._verbose_name", "\n\nclass Token:\n    def __init__(self, tid: int, index: int, span_start: int, span_end: int, phrase: str):\n        self._tid = tid  # ID within the corresponding dataset\n        self._index = index  # original token index in document\n\n        self._span_start = span_start  # start of token span in document (inclusive)\n        self._span_end = span_end  # end of token span in document (inclusive)\n        self._phrase = phrase\n\n    @property\n    def index(self):\n        return self._index\n    @property\n    def span_start(self):\n        return self._span_start\n\n    @property\n    def span_end(self):\n        return self._span_end\n\n    @property\n    def span(self):\n        return self._span_start, self._span_end\n\n    @property\n    def phrase(self):\n        return self._phrase\n\n\n    def __eq__(self, other):\n        if isinstance(other, Token):\n            return self._tid == other._tid\n        return False\n\n    def __hash__(self):\n        return hash(self._tid)\n\n    def __str__(self):\n        return self._phrase\n\n    def __repr__(self):\n        return self._phrase", "\n\nclass TokenSpan:\n    def __init__(self, tokens):\n        self._tokens = tokens\n\n    @property\n    def span_start(self):\n        return self._tokens[0].span_start\n\n    @property\n    def span_end(self):\n        return self._tokens[-1].span_end\n\n    @property\n    def span(self):\n        return self.span_start, self.span_end\n\n    # @property\n    # def c(self):\n    #     return self._tokens[0].index,self._tokens[-1].index + 1\n\n    def __getitem__(self, s):\n        if isinstance(s, slice):\n            return TokenSpan(self._tokens[s.start:s.stop:s.step])\n        else:\n            return self._tokens[s]\n\n    def __iter__(self):\n        return iter(self._tokens)\n\n    def __len__(self):\n        return len(self._tokens)\n\n    def __str__(self) -> str:\n        return \" \".join([str(t) for t in self._tokens])\n\n    def __repr__(self) -> str:\n        return str(self)", "\n\nclass Entity:\n    def __init__(self, eid: int, entity_type: EntityType, tokens: List[Token], phrase: str):\n        self._eid = eid  # ID within the corresponding dataset\n\n        self._entity_type = entity_type\n\n        self._tokens = tokens\n        self._phrase = phrase\n\n    def as_tuple(self):\n        return self.span_start, self.span_end, self._entity_type\n\n    def as_tuple_token(self):\n        return self._tokens[0].index,self._tokens[-1].index, self._entity_type\n\n    @property\n    def entity_type(self):\n        return self._entity_type\n\n    @property\n    def tokens(self):\n        return TokenSpan(self._tokens)\n\n    @property\n    def span_start(self):\n        return self._tokens[0].span_start\n\n    @property\n    def span_end(self):\n        return self._tokens[-1].span_end\n\n    @property\n    def span(self):\n        return self.span_start, self.span_end\n\n    @property\n    def span_token(self):\n        return self._tokens[0].index,self._tokens[-1].index\n\n    @property\n    def phrase(self):\n        return self._phrase\n\n    def __eq__(self, other):\n        if isinstance(other, Entity):\n            return self._eid == other._eid\n        return False\n\n    def __hash__(self):\n        return hash(self._eid)\n\n    def __str__(self):\n        return self._phrase + f\" -> {self.span_token}-> {self.entity_type.identifier}\"\n\n    def __repr__(self) -> str:\n        return str(self)", "\n\nclass Relation:\n    def __init__(self, rid: int, relation_type: RelationType, head_entity: Entity,\n                 tail_entity: Entity, reverse: bool = False):\n        self._rid = rid  # ID within the corresponding dataset\n        self._relation_type = relation_type\n\n        self._head_entity = head_entity\n        self._tail_entity = tail_entity\n\n        self._reverse = reverse\n\n        self._first_entity = head_entity if not reverse else tail_entity\n        self._second_entity = tail_entity if not reverse else head_entity\n\n    def as_tuple(self):\n        head = self._head_entity\n        tail = self._tail_entity\n        head_start, head_end = (head.span_start, head.span_end)\n        tail_start, tail_end = (tail.span_start, tail.span_end)\n\n        t = ((head_start, head_end, head.entity_type),\n             (tail_start, tail_end, tail.entity_type), self._relation_type)\n        return t\n\n    @property\n    def relation_type(self):\n        return self._relation_type\n\n    @property\n    def head_entity(self):\n        return self._head_entity\n\n    @property\n    def tail_entity(self):\n        return self._tail_entity\n\n    @property\n    def first_entity(self):\n        return self._first_entity\n\n    @property\n    def second_entity(self):\n        return self._second_entity\n\n    @property\n    def reverse(self):\n        return self._reverse\n\n    def __eq__(self, other):\n        if isinstance(other, Relation):\n            return self._rid == other._rid\n        return False\n\n    def __hash__(self):\n        return hash(self._rid)", "\n\nclass Document:\n    def __init__(self, doc_id: int, tokens: List[Token], entities: List[Entity], relations: List[Relation],\n                 encoding: List[int], seg_encoding: List[int], raw_encoding: List[int], inx4locator, pos_encoding, images = None):\n        self._doc_id = doc_id  # ID within the corresponding dataset\n\n        self._tokens = tokens\n        self._entities = entities\n        self._relations = relations\n\n        # byte-pair document encoding including special tokens ([CLS] and [SEP])\n        self._encoding = encoding\n        self._raw_encoding = raw_encoding\n        self._seg_encoding = seg_encoding\n        self._inx4locator = inx4locator\n        self._pos_encoding = pos_encoding\n        self._images = images\n\n    @property\n    def doc_id(self):\n        return self._doc_id\n\n    @property\n    def entities(self):\n        return self._entities\n\n    @property\n    def relations(self):\n        return self._relations\n\n    @property\n    def tokens(self):\n        return TokenSpan(self._tokens)\n\n    @property\n    def encoding(self):\n        return self._encoding\n\n\n    @property\n    def raw_encoding(self):\n        return self._raw_encoding\n\n    @property\n    def pos_encoding(self):\n        return self._pos_encoding\n\n    @property\n    def inx4locator(self):\n        return self._inx4locator\n\n    @property\n    def char_encoding(self):\n        return self._char_encoding\n\n    @property\n    def seg_encoding(self):\n        return self._seg_encoding\n\n    @property\n    def images(self):\n        return self._images\n\n    @encoding.setter\n    def encoding(self, value):\n        self._encoding = value\n\n    @char_encoding.setter\n    def char_encoding(self, value):\n        self._char_encoding = value\n\n    @seg_encoding.setter\n    def seg_encoding(self, value):\n        self._seg_encoding = value\n\n    @images.setter\n    def images(self, value):\n        self._images = value\n\n    def __str__(self) -> str:\n        raw_document = str(self.tokens)\n        raw_entities = str(self.entities)\n        \n        return raw_document + \" => \" + raw_entities\n    \n    def __repr__(self) -> str:\n        return str(self)\n\n    def __eq__(self, other):\n        if isinstance(other, Document):\n            return self._doc_id == other._doc_id\n        return False\n\n    def __hash__(self):\n        return hash(self._doc_id)", "\n\nclass BatchIterator:\n    def __init__(self, entities, batch_size, order=None, truncate=False):\n        self._entities = entities\n        self._batch_size = batch_size\n        self._truncate = truncate\n        self._length = len(self._entities)\n        self._order = order\n\n        if order is None:\n            self._order = list(range(len(self._entities)))\n\n        self._i = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self._truncate and self._i + self._batch_size > self._length:\n            raise StopIteration\n        elif not self._truncate and self._i >= self._length:\n            raise StopIteration\n        else:\n            entities = [self._entities[n] for n in self._order[self._i:self._i + self._batch_size]]\n            self._i += self._batch_size\n            return entities", "class SimImage:\n    def __init__(self, url: str, caption: str, img_id: int, sim: float, local_dir: str):\n        self._url = url\n        self._caption = caption\n        self._img_id = img_id \n        self._sim = sim\n        self._local_dir = local_dir\n        # self._image_input = None\n        # self._processor = processor\n        # self.apply(processor)\n\n    def apply(self, processor):\n        path = self._local_dir + str(self._img_id) +'.jpg'\n        f = open(path, 'rb')\n        try:\n            im = Image.open(f)\n        except:\n            im = Image.open(open(self._local_dir+'0.jpg', 'rb'))\n        image_input = processor(images=im, return_tensors=\"pt\")\n        return image_input\n\n    @property\n    def url(self):\n        return self._url\n    @property\n    def caption(self):\n        return self._caption\n\n    @property\n    def img_id(self):\n        return self._img_id\n\n    @property\n    def sim(self):\n        return self._sim\n\n    @property\n    def image_input(self):\n        return self._image_input\n\n    def __eq__(self, other):\n        if isinstance(other, SimImage):\n            return self._img_id == other._img_id\n        return False\n\n    def __hash__(self):\n        return hash(self._img_id)\n\n    def __str__(self):\n        return f' {self.id} @ {self.caption} @ {self.url} '\n\n    def __repr__(self):\n        return str(self)", "\nclass Dataset(TorchDataset):\n    TRAIN_MODE = 'train'\n    EVAL_MODE = 'eval'\n\n    def __init__(self, label, dataset_path, rel_types, entity_types, random_mask_word = False, tokenizer = None, processor = None, repeat_gt_entities = None):\n        self._label = label\n        self._rel_types = rel_types\n        self._entity_types = entity_types\n        self._mode = Dataset.TRAIN_MODE\n        self.random_mask_word = random_mask_word\n        self._tokenizer = tokenizer\n        self._processor = processor\n        self._repeat_gt_entities = repeat_gt_entities\n        self._path = dataset_path\n\n        self._documents = OrderedDict()\n        self._entities = OrderedDict()\n        self._relations = OrderedDict()\n\n        # current ids\n        self._doc_id = 0\n        self._rid = 0\n        self._eid = 0\n        self._tid = 0\n        self._iid = 0\n\n    def iterate_documents(self, batch_size, order=None, truncate=False):\n        return BatchIterator(self.documents, batch_size, order=order, truncate=truncate)\n\n    def iterate_relations(self, batch_size, order=None, truncate=False):\n        return BatchIterator(self.relations, batch_size, order=order, truncate=truncate)\n\n    def create_image(self, url, caption, img_id, sim, local_dir) -> SimImage:\n        image = SimImage(url, caption, img_id, sim, local_dir)\n        self._iid += 1\n        return image\n\n    def create_token(self, idx, span_start, span_end, phrase) -> Token:\n        token = Token(self._tid, idx, span_start, span_end, phrase)\n        self._tid += 1\n        return token\n\n    def create_document(self, tokens, entity_mentions, relations, doc_encoding, seg_encoding, raw_doc_encoding, inx4locator, pos_encoding, images = None) -> Document:\n        document = Document(self._doc_id, tokens, entity_mentions, relations, doc_encoding, seg_encoding, raw_doc_encoding, inx4locator, pos_encoding, images = images)\n        self._documents[self._doc_id] = document\n        self._doc_id += 1\n\n        return document\n\n    def create_entity(self, entity_type, tokens, phrase) -> Entity:\n        mention = Entity(self._eid, entity_type, tokens, phrase)\n        self._entities[self._eid] = mention\n        self._eid += 1\n        return mention\n\n    def create_relation(self, relation_type, head_entity, tail_entity, reverse=False) -> Relation:\n        relation = Relation(self._rid, relation_type, head_entity, tail_entity, reverse)\n        self._relations[self._rid] = relation\n        self._rid += 1\n        return relation\n\n    def __len__(self):\n        return len(self._documents)\n\n    def __getitem__(self, index: int):\n        doc = self._documents[index]\n\n        if self._mode == Dataset.TRAIN_MODE:\n            return sampling.create_train_sample(doc, random_mask=self.random_mask_word, tokenizer = self._tokenizer, processor = self._processor,  repeat_gt_entities = self._repeat_gt_entities)\n        else:\n            return sampling.create_eval_sample(doc, processor = self._processor)\n\n    def switch_mode(self, mode):\n        self._mode = mode\n\n    @property\n    def label(self):\n        return self._label\n\n    @property\n    def input_reader(self):\n        return self._input_reader\n\n    @property\n    def documents(self):\n        return list(self._documents.values())\n\n    @property\n    def entities(self):\n        return list(self._entities.values())\n\n    @property\n    def relations(self):\n        return list(self._relations.values())\n\n    @property\n    def document_count(self):\n        return len(self._documents)\n\n    @property\n    def entity_count(self):\n        return len(self._entities)\n\n    @property\n    def relation_count(self):\n        return len(self._relations)", "\nclass DistributedIterableDataset(IterableTorchDataset):\n    TRAIN_MODE = 'train'\n    EVAL_MODE = 'eval'\n\n    def __init__(self, label, path, rel_types, entity_types, input_reader, random_mask_word = False, tokenizer = None, processor = None, repeat_gt_entities = None):\n        self._label = label\n        self._path = path\n        self._rel_types = rel_types\n        self._entity_types = entity_types\n        self._mode = Dataset.TRAIN_MODE\n        self.random_mask_word = random_mask_word\n        self._tokenizer = tokenizer\n        self._processor = processor\n        self._input_reader = input_reader\n        self._repeat_gt_entities = repeat_gt_entities\n        self._local_rank = dist.get_rank()\n        self._world_size = dist.get_world_size()\n        # print(self._local_rank, self._world_size)\n\n        self.statistic = json.load(open(path.split(\".\")[0] + \"_statistic.json\"))\n\n        # current ids\n        self._doc_id = 0\n        self._rid = 0\n        self._eid = 0\n        self._tid = 0\n\n    def create_token(self, idx, span_start, span_end, phrase) -> Token:\n        token = Token(self._tid, idx, span_start, span_end, phrase)\n        self._tid += 1\n        return token\n\n    def create_document(self, tokens, entity_mentions, relations, doc_encoding, seg_encoding, raw_doc_encoding, inx4locator, pos_encoding, images = None) -> Document:\n        document = Document(self._doc_id, tokens, entity_mentions, relations, doc_encoding, seg_encoding, raw_doc_encoding, inx4locator, pos_encoding, images = None)\n        self._doc_id += 1\n        return document\n\n    def create_entity(self, entity_type, tokens, phrase) -> Entity:\n        mention = Entity(self._eid, entity_type, tokens, phrase)\n        self._eid += 1\n        return mention\n\n    def create_relation(self, relation_type, head_entity, tail_entity, reverse=False) -> Relation:\n        relation = Relation(self._rid, relation_type, head_entity, tail_entity, reverse)\n        self._rid += 1\n        return relation\n\n    def parse_doc(self, path):\n        inx = 0\n        worker_info = data.get_worker_info()\n        num_workers = 1\n        worker_id = 0\n        if worker_info is not None:\n            num_workers = worker_info.num_workers\n            worker_id = worker_info.id\n\n        offset = 0\n        mod = 1\n        if self._local_rank != -1:\n            offset = self._local_rank*num_workers + worker_id\n            mod = self._world_size * num_workers\n        with open(self._path, encoding=\"utf8\") as rf:\n            for line in rf:\n                if inx % mod == offset:\n                    doc = json.loads(line)\n                    doc = self._input_reader._parse_document(doc, self)\n                    if doc is not None:\n                        if self._mode == Dataset.TRAIN_MODE:\n                            yield sampling.create_train_sample(doc, random_mask=self.random_mask_word, tokenizer = self._tokenizer, processor = self._processor,  repeat_gt_entities = self._repeat_gt_entities)\n                        else:\n                            yield sampling.create_eval_sample(doc, processor = self._processor)\n                inx += 1 # maybe imblance\n\n\n    def _get_stream(self, path):\n        # return itertools.cycle(self.parse_doc(path))\n        return self.parse_doc(path)\n\n\n    def __iter__(self):\n        return self._get_stream(self._path)\n    \n\n    def switch_mode(self, mode):\n        self._mode = mode\n\n    @property\n    def label(self):\n        return self._label\n\n    @property\n    def input_reader(self):\n        return self._input_reader\n\n    @property\n    def document_count(self):\n        return self.statistic[\"document_count\"]\n\n    @property\n    def entity_count(self):\n        return self.statistic[\"entity_count\"]", ""]}
{"filename": "prompt4ner/loss.py", "chunked_list": ["from abc import ABC\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\nfrom .matcher import HungarianMatcher\n\nclass Loss(ABC):\n    def compute(self, *args, **kwargs):\n        pass", "\nclass Prompt4NERLoss(Loss):\n    def __init__(self, entity_type_count, device, model, optimizer, scheduler, max_grad_norm, nil_weight, match_class_weight, match_boundary_weight, loss_class_weight, loss_boundary_weight, type_loss, solver, match_warmup_epoch = 0):\n        self._model = model\n        self._optimizer = optimizer\n        self._scheduler = scheduler\n        \n        # losses = ['labels', 'boundary', 'cardinality']\n        losses = ['labels', 'boundary']\n        self.weight_dict = {'loss_ce': loss_class_weight, 'loss_boundary': loss_boundary_weight}\n        self.criterion = Criterion(entity_type_count, self.weight_dict, nil_weight, losses, type_loss = type_loss, match_class_weight = match_class_weight, match_boundary_weight = match_boundary_weight, solver = solver, match_warmup_epoch = match_warmup_epoch)\n        self.criterion.to(device)\n        self._max_grad_norm = max_grad_norm\n\n    def del_attrs(self):\n        del self._optimizer \n        del self._scheduler\n\n    def compute(self, entity_logits, pred_left, pred_right, output, gt_types, gt_spans, entity_masks, epoch, deeply_weight = \"same\", seq_logits = None, gt_seq_labels = None, batch = None):\n        # set_trace()\n\n        maskedlm_loss = None\n        if seq_logits is not None:\n            loss_fct = nn.CrossEntropyLoss(ignore_index=0)\n            maskedlm_loss = loss_fct(seq_logits.view(-1, seq_logits.size(-1)), gt_seq_labels.view(-1))\n            \n        \n\n        gt_types_wo_nil = gt_types.masked_select(entity_masks)\n        \n        if len(gt_types_wo_nil) == 0:\n            return 0.1\n\n        sizes = [i.sum() for i in entity_masks]\n        entity_masks = entity_masks.unsqueeze(2).repeat(1, 1, 2)\n        spans_wo_nil = gt_spans.masked_select(entity_masks).view(-1, 2)\n\n        targets = {\"labels\": gt_types_wo_nil, \"gt_left\":spans_wo_nil[:, 0], \"gt_right\":spans_wo_nil[:, 1], \"sizes\":sizes}\n\n        train_loss = []\n        indices = None\n        for out_dict in output[::-1]:\n            entity_logits, pred_left, pred_right = out_dict[\"entity_logits\"], out_dict[\"p_left\"], out_dict[\"p_right\"]\n            outputs = {\"pred_logits\":entity_logits, \"pred_left\":pred_left, \"pred_right\":pred_right, \"token_mask\": batch[\"token_masks\"]}\n            loss_dict, indices = self.criterion(outputs, targets, epoch, indices = indices)\n            \n            train_loss.append(sum(loss_dict[k] * self.weight_dict[k] for k in loss_dict.keys()))\n        if deeply_weight == \"same\":\n            deeply_weight = [1] * len(output)\n        elif deeply_weight == \"linear\":\n            deeply_weight = list(range(len(output), 0, -1))\n            deeply_weight = list(map(lambda x: x/sum(deeply_weight), deeply_weight))\n        train_loss = sum(train_loss[i] * deeply_weight[i] for i in range(len(output)))\n\n        if maskedlm_loss is not None:\n            train_loss += maskedlm_loss\n        train_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self._model.parameters(), self._max_grad_norm)\n        self._optimizer.step()\n        self._scheduler.step()\n        self._model.zero_grad()\n        return train_loss.item()", "\n\nclass Criterion(nn.Module):\n    def __init__(self, entity_type_count, weight_dict, nil_weight, losses, type_loss, match_class_weight, match_boundary_weight, solver, match_warmup_epoch):\n        super().__init__()\n        self.entity_type_count = entity_type_count\n        self.matcher = HungarianMatcher(cost_class = match_class_weight, cost_span = match_boundary_weight, solver = solver)\n        self.match_warmup_epoch = match_warmup_epoch\n        if match_warmup_epoch > 0:\n            self.order_matcher = HungarianMatcher(solver = \"order\")\n        self.weight_dict = weight_dict\n        self.nil_weight = nil_weight\n        self.losses = losses\n        empty_weight = torch.ones(self.entity_type_count)\n        empty_weight[0] = self.nil_weight\n        self.register_buffer('empty_weight', empty_weight)\n        self.type_loss = type_loss\n\n    def loss_labels(self, outputs, targets, indices, num_spans):\n        \"\"\"Classification loss (NLL)\n        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n        \"\"\"\n        assert 'pred_logits' in outputs\n\n        src_logits = outputs['pred_logits']\n        idx = self._get_src_permutation_idx(indices)\n\n        labels = targets[\"labels\"].split(targets[\"sizes\"], dim=-1)\n\n        target_classes_o = torch.cat([t[J] for t, (_, J) in zip(labels, indices)])\n        target_classes = torch.full(src_logits.shape[:2], 0,\n                                    dtype=torch.int64, device=src_logits.device)\n        target_classes[idx] = target_classes_o\n        empty_weight = self.empty_weight.clone()\n\n        if self.nil_weight == -1:\n            empty_weight[0] = num_spans / (src_logits.size(0) * src_logits.size(1) - num_spans)\n        if self.type_loss == \"celoss\":\n            src_logits = src_logits.view(-1, src_logits.size(2))\n            target_classes = target_classes.view(-1)\n            loss_ce = F.cross_entropy(src_logits, target_classes, empty_weight, reduction='none')\n        if self.type_loss == \"bceloss\":\n            src_logits = src_logits.view(-1, src_logits.size(2))\n            target_classes = target_classes.view(-1)\n            target_classes_onehot = torch.zeros([target_classes.size(0), src_logits.size(1)], dtype=torch.float32).to(device=target_classes.device)\n            target_classes_onehot.scatter_(1, target_classes.unsqueeze(1), 1)\n            src_logits_p = F.sigmoid(src_logits)\n            loss_ce = F.binary_cross_entropy(src_logits_p, target_classes_onehot, reduction='none')\n        losses = {'loss_ce': loss_ce.mean()}\n\n        return losses\n\n    @torch.no_grad()\n    def loss_cardinality(self, outputs, targets, indices, num_spans):\n        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n        \"\"\"\n        pred_logits = outputs['pred_logits']\n        device = pred_logits.device\n        tgt_lengths = torch.as_tensor(targets[\"sizes\"], device=device)\n        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n        losses = {'cardinality_error': card_err}\n        return losses\n\n    def loss_boundary(self, outputs, targets, indices, num_spans):\n        idx = self._get_src_permutation_idx(indices)\n        src_spans_left = outputs['pred_left'][idx]\n        src_spans_right = outputs['pred_right'][idx]\n        token_masks = outputs['token_mask'].unsqueeze(1).expand(-1, outputs['pred_right'].size(1), -1)\n        token_masks = token_masks[idx]\n\n        gt_left = targets[\"gt_left\"].split(targets[\"sizes\"], dim=0)\n        target_spans_left = torch.cat([t[i] for t, (_, i) in zip(gt_left , indices)], dim=0)\n        gt_right = targets[\"gt_right\"].split(targets[\"sizes\"], dim=0)\n        target_spans_right = torch.cat([t[i] for t, (_, i) in zip(gt_right , indices)], dim=0)\n\n        left_onehot = torch.zeros([target_spans_left.size(0), src_spans_left.size(1)], dtype=torch.float32).to(device=target_spans_left.device)\n        left_onehot.scatter_(1, target_spans_left.unsqueeze(1), 1)\n    \n        right_onehot = torch.zeros([target_spans_right.size(0), src_spans_right.size(1)], dtype=torch.float32).to(device=target_spans_right.device)\n        right_onehot.scatter_(1, target_spans_right.unsqueeze(1), 1)\n\n        left_nll_loss = F.binary_cross_entropy(src_spans_left, left_onehot, reduction='none')\n        right_nll_loss = F.binary_cross_entropy(src_spans_right, right_onehot, reduction='none')\n\n        loss_boundary = (left_nll_loss + right_nll_loss) * token_masks\n\n        losses = {}\n        losses['loss_boundary'] = loss_boundary.sum() / num_spans        \n\n        return losses\n\n    def _get_src_permutation_idx(self, indices):\n        # permute predictions following indices\n        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n        src_idx = torch.cat([src for (src, _) in indices])\n        return batch_idx, src_idx\n\n    def _get_tgt_permutation_idx(self, indices):\n        # permute targets following indices\n        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n        return batch_idx, tgt_idx\n\n    def get_loss(self, loss, outputs, targets, indices, num_spans, **kwargs):\n        loss_map = {\n            'labels': self.loss_labels,\n            'cardinality': self.loss_cardinality,\n            'boundary': self.loss_boundary,\n        }\n        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n        return loss_map[loss](outputs, targets, indices, num_spans, **kwargs)\n\n    # @torchsnooper.snoop()\n    def forward(self, outputs, targets, epoch, indices = None):\n        # Retrieve the matching between the outputs of the last layer and the targets\n\n        if indices is None:\n            if epoch < self.match_warmup_epoch:\n                indices = self.order_matcher(outputs, targets)\n            else:\n                indices = self.matcher(outputs, targets)\n\n        # Compute the average number of target boxes accross all nodes, for normalization purposes\n        num_spans = sum(targets[\"sizes\"])\n        num_spans = torch.as_tensor([num_spans], dtype=torch.float, device=next(iter(outputs.values())).device)\n\n        # Compute all the requested losses\n        losses = {}\n        for loss in self.losses:\n            losses.update(self.get_loss(loss, outputs, targets, indices, num_spans))\n        return losses, indices"]}
{"filename": "prompt4ner/lap.py", "chunked_list": ["import sys\nimport torch\n\ndef auction_lap(X, eps=None, compute_score=True):\n    device = X.device\n    if X.size(0) == 0 or X.size(1) == 0:\n        return torch.tensor([], device=device),torch.tensor([], device=device),0\n\n    X = -X\n    flag = 0\n    if X.size(0) > X.size(1):\n        flag = 1\n        X = X.transpose(0, 1)\n    eps = 1 / X.shape[0] if eps is None else eps\n    \n    cost = torch.zeros((1, X.shape[1]), device = device)\n    curr_ass = torch.zeros(X.shape[0], device = device).long() - 1\n    bids = torch.zeros(X.shape, device = device)\n\n    counter = 0\n    while (curr_ass == -1).any():\n        counter += 1\n\n        # bidding\n        \n        unassigned = (curr_ass == -1).nonzero().squeeze(-1)\n        value = X[unassigned] - cost\n        top_value, top_idx = value.topk(2, dim=1)\n        \n        first_idx = top_idx[:,0]\n        first_value, second_value = top_value[:,0], top_value[:,1]\n        \n        bid_increments = first_value - second_value + eps\n        \n        bids_ = bids[unassigned]\n        bids_.zero_()\n        bids_.scatter_(\n            dim=1,\n            index=first_idx.contiguous().view(-1, 1),\n            src=bid_increments.view(-1, 1)\n        )\n\n        # assignment\n        \n        have_bidder = (bids_ > 0).int().sum(dim=0).nonzero()\n        \n        high_bids, high_bidders = bids_[:,have_bidder].max(dim=0)\n        high_bidders = unassigned[high_bidders.squeeze()]\n        \n        cost[:,have_bidder] += high_bids\n        \n        curr_ass[(curr_ass.view(-1, 1) == have_bidder.view(1, -1)).sum(dim=1)] = -1\n        curr_ass[high_bidders] = have_bidder.squeeze()\n    \n    score = None\n    if compute_score:\n        score = int(X.gather(dim=1, index=curr_ass.view(-1, 1)).sum())\n    \n    if flag == 1:\n        return  curr_ass, torch.arange(X.size(0), device=device), -score\n    return torch.arange(X.size(0), device=device), curr_ass, -score", "\nclass SinkhornDistance(torch.nn.Module):\n    r\"\"\"\n        Given two empirical measures each with :math:`P_1` locations\n        :math:`x\\in\\mathbb{R}^{D_1}` and :math:`P_2` locations :math:`y\\in\\mathbb{R}^{D_2}`,\n        outputs an approximation of the regularized OT cost for point clouds.\n        Args:\n        eps (float): regularization coefficient\n        max_iter (int): maximum number of Sinkhorn iterations\n        reduction (string, optional): Specifies the reduction to apply to the output:\n        'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n        'mean': the sum of the output will be divided by the number of\n        elements in the output, 'sum': the output will be summed. Default: 'none'\n        Shape:\n            - Input: :math:`(N, P_1, D_1)`, :math:`(N, P_2, D_2)`\n            - Output: :math:`(N)` or :math:`()`, depending on `reduction`\n    \"\"\"\n\n    def __init__(self, eps=1e-3, max_iter=100, reduction='none'):\n        super(SinkhornDistance, self).__init__()\n        self.eps = eps\n        self.max_iter = max_iter\n        self.reduction = reduction\n\n    def forward(self, mu, nu, C):\n        u = torch.ones_like(mu)\n        v = torch.ones_like(nu)\n\n        # Sinkhorn iterations\n        for i in range(self.max_iter):\n            v = self.eps * \\\n                (torch.log(\n                    nu + 1e-8) - torch.logsumexp(self.M(C, u, v).transpose(-2, -1), dim=-1)) + v\n            u = self.eps * \\\n                (torch.log(\n                    mu + 1e-8) - torch.logsumexp(self.M(C, u, v), dim=-1)) + u\n\n        U, V = u, v\n        # Transport plan pi = diag(a)*K*diag(b)\n        pi = torch.exp(\n            self.M(C, U, V)).detach()\n        # Sinkhorn distance\n        cost = torch.sum(\n            pi * C, dim=(-2, -1))\n        return cost, pi\n\n    def M(self, C, u, v):\n        '''\n        \"Modified cost for logarithmic updates\"\n        \"$M_{ij} = (-c_{ij} + u_i + v_j) / epsilon$\"\n        '''\n        return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / self.eps"]}
{"filename": "prompt4ner/models.py", "chunked_list": ["import copy\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\nfrom prompt4ner.modeling_albert import AlbertModel, AlbertMLMHead\nfrom prompt4ner.modeling_bert import BertConfig, BertModel\nfrom prompt4ner.modeling_roberta import RobertaConfig, RobertaModel, RobertaLMHead\nfrom prompt4ner.modeling_xlm_roberta import XLMRobertaConfig\nfrom transformers.modeling_utils import PreTrainedModel\nfrom prompt4ner import util", "from transformers.modeling_utils import PreTrainedModel\nfrom prompt4ner import util\nimport logging\n\nlogger = logging.getLogger()\n\nclass EntityBoundaryPredictor(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.token_embedding_linear = nn.Sequential(\n            nn.Linear(self.hidden_size, self.hidden_size)\n        ) \n        self.entity_embedding_linear = nn.Sequential(\n            nn.Linear(self.hidden_size, self.hidden_size)\n        ) \n        self.boundary_predictor = nn.Linear(self.hidden_size, 1)\n    \n    def forward(self, token_embedding, entity_embedding, token_mask):\n        # B x #ent x #token x hidden_size\n        entity_token_matrix = self.token_embedding_linear(token_embedding).unsqueeze(1) + self.entity_embedding_linear(entity_embedding).unsqueeze(2)\n        entity_token_cls = self.boundary_predictor(torch.tanh(entity_token_matrix)).squeeze(-1)\n        token_mask = token_mask.unsqueeze(1).expand(-1, entity_token_cls.size(1), -1)\n        entity_token_cls[~token_mask] = -1e25\n        # entity_token_p = entity_token_cls.softmax(dim=-1)\n        entity_token_p = F.sigmoid(entity_token_cls)\n        return entity_token_p", "\nclass EntityBoundaryPredictorBak(nn.Module):\n    def __init__(self, config, prop_drop):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.token_embedding_linear = nn.Sequential(\n            nn.Linear(self.hidden_size, self.hidden_size),\n            nn.Dropout(prop_drop)\n        ) \n        self.entity_embedding_linear = nn.Sequential(\n            nn.Linear(self.hidden_size, self.hidden_size),\n            nn.Dropout(prop_drop)\n        ) \n        self.boundary_predictor = nn.Linear(self.hidden_size, 1)\n    \n    def forward(self, token_embedding, entity_embedding, token_mask):\n        entity_token_matrix = self.token_embedding_linear(token_embedding).unsqueeze(1) + self.entity_embedding_linear(entity_embedding).unsqueeze(2)\n        entity_token_cls = self.boundary_predictor(torch.relu(entity_token_matrix)).squeeze(-1)\n        token_mask = token_mask.unsqueeze(1).expand(-1, entity_token_cls.size(1), -1)\n        entity_token_cls[~token_mask] = -1e30\n        entity_token_p = F.sigmoid(entity_token_cls)\n\n        return entity_token_p", "\n\nclass EntityTypePredictor(nn.Module):\n    def __init__(self, config, entity_type_count, mlm_head):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(config.hidden_size, entity_type_count),\n        )\n    \n    def forward(self, h_cls):\n        entity_logits = self.classifier(h_cls)\n        return entity_logits", "\ndef _get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n\ndef _get_activation_fn(activation):\n    \"\"\"Return an activation function given a string\"\"\"\n    if activation == \"relu\":\n        return F.relu\n    if activation == \"gelu\":\n        return F.gelu\n    if activation == \"glu\":\n        return F.glu\n    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")", "\n\nclass DetrTransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model=768, d_ffn=1024, dropout=0.1, activation=\"relu\", n_heads=8, selfattn = True, ffn = True):\n        super().__init__()\n\n        # cross attention\n        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.selfattn = selfattn\n        self.ffn = ffn\n\n        if selfattn:\n            # self attention\n            self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n            self.dropout2 = nn.Dropout(dropout)\n            self.norm2 = nn.LayerNorm(d_model)\n        if ffn:\n            # ffn\n            self.linear1 = nn.Linear(d_model, d_ffn)\n            self.activation = _get_activation_fn(activation)\n            self.dropout3 = nn.Dropout(dropout)\n            self.linear2 = nn.Linear(d_ffn, d_model)\n            self.dropout4 = nn.Dropout(dropout)\n            self.norm3 = nn.LayerNorm(d_model)\n\n    @staticmethod\n    def with_pos_embed(tensor, pos):\n        return tensor if pos is None else tensor + pos\n\n    def forward(self, tgt, pos, src, mask):\n        if self.selfattn:\n            q = k = self.with_pos_embed(tgt, pos)\n            v = tgt\n            tgt2 = self.self_attn(q.transpose(0, 1), k.transpose(0, 1), v.transpose(0, 1))[0].transpose(0, 1)\n            tgt = tgt + self.dropout2(tgt2)\n            tgt = self.norm2(tgt)\n\n        q = self.with_pos_embed(tgt, pos)\n        k = v = src\n        tgt2 = self.cross_attn(q.transpose(0, 1), k.transpose(0, 1), v.transpose(0, 1), key_padding_mask=~mask if mask is not None else None)[0].transpose(0, 1)\n        tgt = tgt + self.dropout1(tgt2)\n        tgt = self.norm1(tgt)\n\n        if self.ffn:\n            tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))\n            tgt = tgt + self.dropout4(tgt2)\n            tgt = self.norm3(tgt)\n\n        return tgt", "\nclass DetrTransformerDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, return_intermediate=False):\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.return_intermediate = return_intermediate\n        self.bbox_embed = None\n        self.class_embed = None\n\n    def forward(self, tgt, pos, src, mask):\n        output = tgt\n\n        intermediate = []\n        intermediate_reference_points = []\n\n        for lid, layer in enumerate(self.layers):\n            output = layer(output, tgt, src, mask)\n\n        if self.return_intermediate:\n            return torch.stack(intermediate), torch.stack(intermediate_reference_points)\n\n        return output", "\nclass Prompt4NER(PreTrainedModel):\n\n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def _compute_extended_attention_mask(self, attention_mask, context_count, prompt_number):\n        \n        if not self.prompt_individual_attention and not self.sentence_individual_attention:\n            # #batch x seq_len\n            extended_attention_mask = attention_mask\n        else:\n            # #batch x seq_len x seq_len\n            extended_attention_mask = attention_mask.unsqueeze(1).expand(-1, attention_mask.size(-1), -1).clone()\n\n            for mask, c_count in zip(extended_attention_mask, context_count):\n                # mask seq_len x seq_len\n                # mask prompt for sentence encoding\n                if self.prompt_individual_attention:\n                    # encode for each prompt\n                    for p in range(prompt_number):\n                        mask[p*self.prompt_length:  p*self.prompt_length + self.prompt_length, :prompt_number*self.prompt_length] = 0\n                        mask[p*self.prompt_length: p*self.prompt_length + self.prompt_length, p*self.prompt_length: p*self.prompt_length + self.prompt_length] = 1\n                if self.sentence_individual_attention:\n                    for c in range(c_count):\n                        mask[c+self.prompt_length*prompt_number, :self.prompt_length*prompt_number] = 0\n\n        return extended_attention_mask\n\n    def __init__(\n        self,\n        model_type, \n        config, \n        entity_type_count: int, \n        prop_drop: float, \n        freeze_transformer: bool, \n        lstm_layers = 3, \n        decoder_layers = 3,\n        pool_type:str = \"max\", \n        prompt_individual_attention = True, \n        sentence_individual_attention = True,\n        use_masked_lm = False, \n        last_layer_for_loss = 3, \n        split_epoch = 0, \n        clip_v = None,\n        prompt_length = 3,\n        prompt_number = 60,\n        prompt_token_ids = None):\n        super().__init__(config)\n        \n        self.freeze_transformer = freeze_transformer\n        self.split_epoch = split_epoch\n        self.has_changed = False\n        self.loss_layers = last_layer_for_loss\n        self.model_type = model_type\n        self.use_masked_lm = use_masked_lm\n        self._entity_type_count = entity_type_count\n        self.prop_drop = prop_drop\n        self.split_epoch = split_epoch\n        self.lstm_layers = lstm_layers\n        self.prompt_individual_attention = prompt_individual_attention\n        self.sentence_individual_attention = sentence_individual_attention\n\n        self.decoder_layers = decoder_layers\n        self.prompt_number = prompt_number\n        self.prompt_length = prompt_length\n        self.pool_type = pool_type\n\n        self.withimage = False\n        if clip_v is not None:\n            self.withimage = True\n        \n        self.query_embed = nn.Embedding(prompt_number, config.hidden_size * 2)\n\n        if self.decoder_layers > 0:\n            decoder_layer = DetrTransformerDecoderLayer(d_model=config.hidden_size, d_ffn=1024, dropout=0.1, selfattn = True, ffn = True)\n            self.decoder = DetrTransformerDecoder(decoder_layer=decoder_layer, num_layers=self.decoder_layers)\n            if self.withimage:\n                self.img_decoder = DetrTransformerDecoder(decoder_layer=decoder_layer, num_layers=self.decoder_layers)\n            if self.prompt_length>1:\n                self.decoder2 = DetrTransformerDecoder(decoder_layer=decoder_layer, num_layers=self.decoder_layers)\n\n        if model_type == \"roberta\":\n            self.roberta = RobertaModel(config)\n            self.model = self.roberta\n            self.lm_head = RobertaLMHead(config)\n            self.entity_classifier = EntityTypePredictor(config, entity_type_count, lambda x: self.lm_head(x))\n\n        if model_type == \"bert\":\n            # self.bert = BertModel(config)\n            self.prompt_ids = prompt_token_ids\n            self.bert = BertModel(config, prompt_ids = prompt_token_ids)\n            self.model = self.bert\n            for name, param in self.bert.named_parameters():\n                if \"pooler\" in name:\n                    param.requires_grad = False\n            # self.cls = BertOnlyMLMHead(config)\n            self.cls = None\n            self.entity_classifier = EntityTypePredictor(config, entity_type_count, lambda x: self.cls(x))\n\n        if model_type == \"albert\":\n            # self.bert = BertModel(config)\n            self.prompt_ids = prompt_token_ids\n            self.bert = AlbertModel(config)\n            self.model = self.bert\n            self.predictions = AlbertMLMHead(config)\n            self.entity_classifier = EntityTypePredictor(config, entity_type_count, lambda x: self.predictions(x))\n\n        if self.withimage:\n            self.vision2text = nn.Linear(clip_v.config.hidden_size, config.hidden_size)\n\n        Prompt4NER._keys_to_ignore_on_save = [\"model.\" + k for k,v in self.model.named_parameters()]\n        # Prompt4NER._keys_to_ignore_on_load_unexpected = [\"model.\" + k for k,v in self.model.named_parameters()]\n        Prompt4NER._keys_to_ignore_on_load_missing = [\"model.\" + k for k,v in self.model.named_parameters()]\n\n        if self.lstm_layers > 0:\n            self.lstm = nn.LSTM(input_size = config.hidden_size, hidden_size = config.hidden_size//2, num_layers = lstm_layers,  bidirectional = True, dropout = 0.1, batch_first = True)\n\n        self.left_boundary_classfier = EntityBoundaryPredictor(config, self.prop_drop)\n        self.right_boundary_classfier = EntityBoundaryPredictor(config, self.prop_drop)\n        # self.entity_classifier = EntityTypePredictor(config, config.hidden_size, entity_type_count)\n        self.init_weights()\n\n        self.clip_v = clip_v\n\n        if freeze_transformer or self.split_epoch > 0:\n            logger.info(\"Freeze transformer weights\")\n            if self.model_type == \"bert\":\n                model = self.bert\n                mlm_head = self.cls\n            if self.model_type == \"roberta\":\n                model = self.roberta\n                mlm_head = self.lm_head\n            if self.model_type == \"albert\":\n                model = self.albert\n                mlm_head = self.predictions\n            for name, param in model.named_parameters():\n                param.requires_grad = False\n\n    def _common_forward(\n        self, \n        encodings: torch.tensor, \n        context_masks: torch.tensor, \n        raw_context_masks: torch.tensor, \n        inx4locator: torch.tensor, \n        pos_encoding: torch.tensor, \n        seg_encoding: torch.tensor, \n        context2token_masks:torch.tensor,\n        token_masks:torch.tensor,\n        image_inputs: dict = None,\n        meta_doc = None):\n        \n        batch_size = encodings.shape[0]\n        context_masks = context_masks.float()\n        token_count = token_masks.long().sum(-1,keepdim=True)\n        context_count = context_masks.long().sum(-1,keepdim=True)\n        raw_context_count = raw_context_masks.long().sum(-1,keepdim=True)\n        pos = None\n        tgt = None\n        tgt2 = None\n\n        # pdb.set_trace()\n        \n        context_masks = self._compute_extended_attention_mask(context_masks, raw_context_count, self.prompt_number)\n        # self = self.eval()\n        if self.model_type == \"bert\":\n            model = self.bert\n        if self.model_type == \"roberta\":\n            model = self.roberta\n        # model.embeddings.position_embeddings\n        outputs = model(\n                    input_ids=encodings,\n                    attention_mask=context_masks,\n                    # token_type_ids=seg_encoding,\n                    # position_ids=pos_encoding,\n                    output_hidden_states=True)\n        # last_hidden_state, pooler_output, hidden_states \n\n        masked_seq_logits = None\n        if self.use_masked_lm and self.training:\n            if self.model_type == \"bert\":\n                masked_seq_logits = self.cls(outputs.last_hidden_state)\n            if self.model_type == \"roberta\":\n                masked_seq_logits = self.lm_head(outputs.last_hidden_state)\n\n        if self.withimage:\n            image_h = self.clip_v(**image_inputs)\n            image_last_hidden_state = image_h.last_hidden_state\n            aligned_image_h = self.vision2text(image_last_hidden_state)\n        \n        query_embed = self.query_embed.weight # [100, 768 * 2]\n        query_embeds = torch.split(query_embed, outputs.last_hidden_state.size(2), dim=-1)\n        \n\n        if tgt is None:\n            tgt = query_embeds[1]\n            tgt = tgt.unsqueeze(0).expand(batch_size, -1, -1) # [2, 100, 768]\n\n        if pos is None:\n            pos = query_embeds[0]\n            pos = pos.unsqueeze(0).expand(batch_size, -1, -1) # [2, 100, 768]\n\n        orig_tgt = tgt\n        intermediate = []\n        for i in range(self.loss_layers, 0, -1):\n\n            h = outputs.hidden_states[-1]\n            h_token = util.combine(h, context2token_masks, self.pool_type)\n\n            if self.lstm_layers > 0:\n                h_token = nn.utils.rnn.pack_padded_sequence(input = h_token, lengths = token_count.squeeze(-1).cpu().tolist(), enforce_sorted = False, batch_first = True)\n                h_token, (_, _) = self.lstm(h_token)\n                h_token, _ = nn.utils.rnn.pad_packed_sequence(h_token, batch_first=True)\n\n            if inx4locator is not None:\n                \n                tgt = util.batch_index(outputs.hidden_states[-i], inx4locator) + orig_tgt\n                if self.prompt_length > 1:\n                    tgt2 = util.batch_index(outputs.hidden_states[-i], inx4locator + self.prompt_length-1) + orig_tgt\n\n            updated_tgt = tgt\n\n            if tgt2 is None:\n                updated_tgt2 = tgt\n            else:\n                updated_tgt2 = tgt2\n\n            if self.decoder_layers > 0:\n                if self.withimage:\n                    tgt = self.img_decoder(tgt, pos, aligned_image_h, mask = None)\n                updated_tgt = self.decoder(tgt, pos, h_token, mask = token_masks)\n\n                if self.prompt_length > 1:\n                    updated_tgt2 = self.decoder2(tgt2, pos, h_token, mask = token_masks)\n                else:\n                    updated_tgt2 = updated_tgt\n            intermediate.append({\"h_token\":h_token, \"left_h_locator\":updated_tgt, \"right_h_locator\":updated_tgt, \"h_cls\":updated_tgt2})\n\n        output = []\n        \n        for h_dict in intermediate:\n            h_token, left_h_locator, right_h_locator, h_cls = h_dict[\"h_token\"], h_dict[\"left_h_locator\"], h_dict[\"right_h_locator\"], h_dict[\"h_cls\"]\n            p_left = self.left_boundary_classfier(h_token, left_h_locator, token_masks)\n            p_right = self.right_boundary_classfier(h_token, right_h_locator, token_masks)\n            entity_logits = self.entity_classifier(h_cls)\n            output.append({\"p_left\": p_left, \"p_right\": p_right, \"entity_logits\": entity_logits})\n\n        return entity_logits, p_left, p_right, masked_seq_logits, output\n    \n    def _forward_train(self, *args, epoch=0, **kwargs):\n        if not self.has_changed and epoch >= self.split_epoch and not self.freeze_transformer:\n            logger.info(f\"Now, update bert weights @ epoch = {self.split_epoch }\")\n            self.has_changed = True\n            for name, param in self.named_parameters():\n                param.requires_grad = True\n\n        return self._common_forward(*args, **kwargs)\n\n    def _forward_eval(self, *args, **kwargs):\n        return self._common_forward(*args, **kwargs)\n\n    def forward(self, *args, evaluate=False, **kwargs):\n        if not evaluate:\n            return self._forward_train(*args, **kwargs)\n        else:\n            return self._forward_eval(*args, **kwargs)", "\nclass BertPrompt4NER(Prompt4NER):\n    \n    config_class = BertConfig\n    base_model_prefix = \"bert\"\n    # base_model_prefix = \"model\"\n    authorized_missing_keys = [r\"position_ids\"]\n\n    def __init__(self, *args, **kwagrs):\n        super().__init__(\"bert\", *args, **kwagrs)", "\nclass RobertaPrompt4NER(Prompt4NER):\n\n    config_class = RobertaConfig\n    base_model_prefix = \"roberta\"\n    # base_model_prefix = \"model\"\n    \n    def __init__(self, *args, **kwagrs):\n        super().__init__(\"roberta\", *args, **kwagrs)\n    \nclass XLMRobertaPrompt4NER(Prompt4NER):\n\n    config_class = XLMRobertaConfig\n    base_model_prefix = \"roberta\"\n    # base_model_prefix = \"model\"\n    \n    def __init__(self, *args, **kwagrs):\n        super().__init__(\"roberta\", *args, **kwagrs)", "    \nclass XLMRobertaPrompt4NER(Prompt4NER):\n\n    config_class = XLMRobertaConfig\n    base_model_prefix = \"roberta\"\n    # base_model_prefix = \"model\"\n    \n    def __init__(self, *args, **kwagrs):\n        super().__init__(\"roberta\", *args, **kwagrs)\n\nclass AlbertPrompt4NER(Prompt4NER):\n\n    config_class = XLMRobertaConfig\n    base_model_prefix = \"albert\"\n    # base_model_prefix = \"model\"\n    \n    def __init__(self, *args, **kwagrs):\n        super().__init__(\"albert\", *args, **kwagrs)", "\nclass AlbertPrompt4NER(Prompt4NER):\n\n    config_class = XLMRobertaConfig\n    base_model_prefix = \"albert\"\n    # base_model_prefix = \"model\"\n    \n    def __init__(self, *args, **kwagrs):\n        super().__init__(\"albert\", *args, **kwagrs)\n", "\n\n_MODELS = {\n    'prompt4ner': BertPrompt4NER,\n    'roberta_prompt4ner': RobertaPrompt4NER,\n    'xlmroberta_prompt4ner': XLMRobertaPrompt4NER,\n    'albert_prompt4ner': AlbertPrompt4NER\n}\n\ndef get_model(name):\n    return _MODELS[name]", "\ndef get_model(name):\n    return _MODELS[name]\n"]}
{"filename": "prompt4ner/prompt_tokens.py", "chunked_list": ["PROMPT_TOKENS = \"\"\"\n[unused1]\n[unused2]\n[unused3]\n[unused4]\n[unused5]\n[unused6]\n[unused7]\n[unused8]\n[unused9]", "[unused8]\n[unused9]\n[unused10]\n[unused11]\n[unused12]\n[unused13]\n[unused14]\n[unused15]\n[unused16]\n[unused17]", "[unused16]\n[unused17]\n[unused18]\n[unused19]\n[unused20]\n[unused21]\n[unused22]\n[unused23]\n[unused24]\n[unused25]", "[unused24]\n[unused25]\n[unused26]\n[unused27]\n[unused28]\n[unused29]\n[unused30]\n[unused31]\n[unused32]\n[unused33]", "[unused32]\n[unused33]\n[unused34]\n[unused35]\n[unused36]\n[unused37]\n[unused38]\n[unused39]\n[unused40]\n[unused41]", "[unused40]\n[unused41]\n[unused42]\n[unused43]\n[unused44]\n[unused45]\n[unused46]\n[unused47]\n[unused48]\n[unused49]", "[unused48]\n[unused49]\n[unused50]\n[unused51]\n[unused52]\n[unused53]\n[unused54]\n[unused55]\n[unused56]\n[unused57]", "[unused56]\n[unused57]\n[unused58]\n[unused59]\n[unused60]\n[unused61]\n[unused62]\n[unused63]\n[unused64]\n[unused65]", "[unused64]\n[unused65]\n[unused66]\n[unused67]\n[unused68]\n[unused69]\n[unused70]\n[unused71]\n[unused72]\n[unused73]", "[unused72]\n[unused73]\n[unused74]\n[unused75]\n[unused76]\n[unused77]\n[unused78]\n[unused79]\n[unused80]\n[unused81]", "[unused80]\n[unused81]\n[unused82]\n[unused83]\n[unused84]\n[unused85]\n[unused86]\n[unused87]\n[unused88]\n[unused89]", "[unused88]\n[unused89]\n[unused90]\n[unused91]\n[unused92]\n[unused93]\n[unused94]\n[unused95]\n[unused96]\n[unused97]", "[unused96]\n[unused97]\n[unused98]\n[unused99]\n[unused100]\n[unused101]\n\u00a1\n\u00a2\n\u00a3\n\u00a5", "\u00a3\n\u00a5\n\u00a7\n\u00a8\n\u00a9\n\u00aa\n\u00ab\n\u00ac\n\u00ae\n\u00b0", "\u00ae\n\u00b0\n\u00b1\n\u00b2\n\u00b3\n\u00b4\n\u00b5\n\u00b6\n\u00b7\n\u00b9", "\u00b7\n\u00b9\n\u00ba\n\u00bb\n\u00bc\n\u00bd\n\u00be\n\u00bf\n\u00c0\n\u00c1", "\u00c0\n\u00c1\n\u00c2\n\u00c4\n\u00c5\n\u00c6\n\u00c7\n\u00c8\n\u00c9\n\u00cd", "\u00c9\n\u00cd\n\u00ce\n\u00d1\n\u00d3\n\u00d6\n\u00d7\n\u00d8\n\u00da\n\u00dc", "\u00da\n\u00dc\n\u00de\n\u00df\n\u00e0\n\u00e1\n\u00e2\n\u00e3\n\u00e4\n\u00e5", "\u00e4\n\u00e5\n\u00e6\n\u00e7\n\u00e8\n\u00e9\n\u00ea\n\u00eb\n\u00ec\n\u00ed", "\u00ec\n\u00ed\n\u00ee\n\u00ef\n\u00f0\n\u00f1\n\u00f2\n\u00f3\n\u00f4\n\u00f5", "\u00f4\n\u00f5\n\u00f6\n\u00f7\n\u00f8\n\u00f9\n\u00fa\n\u00fb\n\u00fc\n\u00fd", "\u00fc\n\u00fd\n\u00fe\n\u00ff\n\u0100\n\u0101\n\u0103\n\u0105\n\u0106\n\u0107", "\u0106\n\u0107\n\u010c\n\u010d\n\u010f\n\u0110\n\u0111\n\u0113\n\u0117\n\u0119", "\u0117\n\u0119\n\u011b\n\u011f\n\u0121\n\u0126\n\u0127\n\u0129\n\u012a\n\u012b", "\u012a\n\u012b\n\u0130\n\u0131\n\u013c\n\u013d\n\u013e\n\u0141\n\u0142\n\u0144", "\u0142\n\u0144\n\u0146\n\u0148\n\u014b\n\u014c\n\u014d\n\u014f\n\u0151\n\u0152", "\u0151\n\u0152\n\u0153\n\u0159\n\u015a\n\u015b\n\u015e\n\u015f\n\u0160\n\u0161", "\u0160\n\u0161\n\u0162\n\u0163\n\u0165\n\u0169\n\u016b\n\u016d\n\u016f\n\u0171", "\u016f\n\u0171\n\u0173\n\u0175\n\u0177\n\u017a\n\u017b\n\u017c\n\u017d\n\u017e", "\u017d\n\u017e\n\u018f\n\u0192\n\u01a1\n\u01b0\n\u01ce\n\u01d0\n\u01d2\n\u01d4", "\u01d2\n\u01d4\n\u01eb\n\u0218\n\u0219\n\u021a\n\u021b\n\u0250\n\u0251\n\u0254", "\u0251\n\u0254\n\u0255\n\u0259\n\u025b\n\u0261\n\u0263\n\u0268\n\u026a\n\u0272", "\u026a\n\u0272\n\u027e\n\u0280\n\u0281\n\u0282\n\u0283\n\u028a\n\u028b\n\u028c", "\u028b\n\u028c\n\u0290\n\u0291\n\u0292\n\u0294\n\u02b0\n\u02b2\n\u02b3\n\u02b7", "\u02b3\n\u02b7\n\u05d0\n\u05d1\n\u05d2\n\u05d3\n\u05d4\n\u05d5\n\u05d6\n\u05d7", "\u05d6\n\u05d7\n\u05d8\n\u05d9\n\u05db\n\u05dc\n\u05dd\n\u05de\n\u05df\n\u05e0", "\u05df\n\u05e0\n\u05e1\n\u05e2\n\u05e4\n\u05e6\n\u05e7\n\u05e8\n\u05e9\n\u05ea", "\u05e9\n\u05ea\n\u060c\n\u0621\n\u0622\n\u0623\n\u0625\n\u0626\n\u0627\n\u0628", "\u0627\n\u0628\n\u0629\n\u062a\n\u062b\n\u062c\n\u062d\n\u062e\n\u062f\n\u0630", "\u062f\n\u0630\n\u0631\n\u0632\n\u0633\n\u0634\n\u0635\n\u0636\n\u0637\n\u0638", "\u0637\n\u0638\n\u0639\n\u063a\n\u0641\n\u0642\n\u0643\n\u0644\n\u0645\n\u0646", "\u0645\n\u0646\n\u0647\n\u0648\n\u0649\n\u064a\n\u064e\n\u0650\n\u0679\n\u067e", "\u0679\n\u067e\n\u0686\n\u06a9\n\u06af\n\u06c1\n\u06cc\n\u06d2\n\u0902\n\u0906", "\u0902\n\u0906\n\u0915\n\u0917\n\u091a\n\u091c\n\u0923\n\u0924\n\u0926\n\u0927", "\u0926\n\u0927\n\u0928\n\u092a\n\u092c\n\u092d\n\u092e\n\u092f\n\u0930\n\u0932", "\u0930\n\u0932\n\u0935\n\u0936\n\u0937\n\u0938\n\u0939\n\u093e\n\u093f\n\u0940", "\u093f\n\u0940\n\u0941\n\u0947\n\u094b\n\u094d\n\u0964\n\u0965\n\u0986\n\u0987", "\u0986\n\u0987\n\u098f\n\u0993\n\u0995\n\u0996\n\u0997\n\u099a\n\u099b\n\u099c", "\u099b\n\u099c\n\u099f\n\u09a4\n\u09a5\n\u09a6\n\u09a7\n\u09a8\n\u09aa\n\u09ac", "\u09aa\n\u09ac\n\u09ae\n\u09af\n\u09b0\n\u09b2\n\u09b6\n\u09b8\n\u09b9\n\u09bc", "\u09b9\n\u09bc\n\u09be\n\u09bf\n\u09c0\n\u09c1\n\u09c7\n\u09cb\n\u09cd\n\u09df", "\u09cd\n\u09df\n\u0b95\n\u0ba4\n\u0baa\n\u0bae\n\u0baf\n\u0bb0\n\u0bb2\n\u0bb5", "\u0bb2\n\u0bb5\n\u0bbe\n\u0bbf\n\u0bc1\n\u0bcd\n\u0e23\n\u0f0b\n\u0f42\n\u0f44", "\u0f42\n\u0f44\n\u0f51\n\u0f53\n\u0f56\n\u0f58\n\u0f62\n\u0f63\n\u0f66\n\u0f72", "\u0f66\n\u0f72\n\u0f74\n\u0f7a\n\u0f7c\n\u10d0\n\u10d4\n\u10d8\n\u10da\n\u10dc", "\u10da\n\u10dc\n\u10dd\n\u10e0\n\u10e1\n\u1d2c\n\u1d35\n\u1d40\n\u1d43\n\u1d47", "\u1d43\n\u1d47\n\u1d48\n\u1d49\n\u1d4d\n\u1d4f\n\u1d50\n\u1d52\n\u1d56\n\u1d57", "\u1d56\n\u1d57\n\u1d58\n\u1d62\n\u1d63\n\u1d64\n\u1d65\n\u1d9c\n\u1da0\n\u1e0d", "\u1da0\n\u1e0d\n\u1e24\n\u1e25\n\u1e28\n\u1e29\n\u1e33\n\u1e43\n\u1e45\n\u1e47", "\u1e45\n\u1e47\n\u1e5b\n\u1e63\n\u1e6d\n\u1ea1\n\u1ea3\n\u1ea5\n\u1ea7\n\u1ea9", "\u1ea7\n\u1ea9\n\u1ead\n\u1eaf\n\u1ebf\n\u1ec1\n\u1ec3\n\u1ec5\n\u1ec7\n\u1ecb", "\u1ec7\n\u1ecb\n\u1ecd\n\u1ed1\n\u1ed3\n\u1ed5\n\u1ed9\n\u1edb\n\u1edd\n\u1ee3", "\u1edd\n\u1ee3\n\u1ee5\n\u1ee7\n\u1ee9\n\u1eeb\n\u1eed\n\u1eef\n\u1ef1\n\u1ef3", "\u1ef1\n\u1ef3\n\u1ef9\n\u1f00\n\u1f10\n\u1f41\n\u1f50\n\u1f70\n\u1f76\n\u1f78", "\u1f76\n\u1f78\n\u1fc6\n\u1fd6\n\u1fe6\n\u1ff6\n\u301c\n\u3044\n\u3046\n\u3048", "\u3046\n\u3048\n\u304a\n\u304b\n\u304d\n\u304f\n\u3051\n\u3053\n\u3055\n\u3057", "\u3055\n\u3057\n\u3059\n\u305b\n\u305d\n\u305f\n\u3061\n\u3064\n\u3066\n\u3068", "\u3066\n\u3068\n\u306a\n\u306b\n\u306e\n\u306f\n\u3072\n\u307e\n\u307f\n\u3080", "\u307f\n\u3080\n\u3081\n\u3082\n\u3084\n\u3086\n\u3088\n\u3089\n\u308a\n\u308b", "\u308a\n\u308b\n\u308c\n\u3093\n\u30a2\n\u30a3\n\u30a4\n\u30a6\n\u30a8\n\u30aa", "\u30a8\n\u30aa\n\u30ab\n\u30ac\n\u30ad\n\u30af\n\u30b0\n\u30b3\n\u30b5\n\u30b7", "\u30b5\n\u30b7\n\u30b8\n\u30b9\n\u30ba\n\u30bf\n\u30c0\n\u30c3\n\u30c6\n\u30c7", "\u30c6\n\u30c7\n\u30c8\n\u30c9\n\u30ca\n\u30cb\n\u30cf\n\u30d0\n\u30d1\n\u30d5", "\u30d1\n\u30d5\n\u30d6\n\u30d7\n\u30de\n\u30df\n\u30e0\n\u30e3\n\u30e5\n\u30e9", "\u30e5\n\u30e9\n\u30ea\n\u30eb\n\u30ec\n\u30ed\n\u30f3\n##\u301c\n##\u3044\n##\u3046", "##\u3044\n##\u3046\n##\u3048\n##\u304a\n##\u304b\n##\u304d\n##\u304f\n##\u3051\n##\u3053\n##\u3055", "##\u3053\n##\u3055\n##\u3057\n##\u3059\n##\u305b\n##\u305d\n##\u305f\n##\u3061\n##\u3064\n##\u3066", "##\u3064\n##\u3066\n##\u3068\n##\u306a\n##\u306b\n##\u306e\n##\u306f\n##\u3072\n##\u307e\n##\u307f", "##\u307e\n##\u307f\n##\u3080\n##\u3081\n##\u3082\n##\u3084\n##\u3086\n##\u3088\n##\u3089\n##\u308a", "##\u3089\n##\u308a\n##\u308b\n##\u308c\n##\u3093\n##\u30a2\n##\u30a3\n##\u30a4\n##\u30a6\n##\u30a8", "##\u30a6\n##\u30a8\n##\u30aa\n##\u30ab\n##\u30ac\n##\u30ad\n##\u30af\n##\u30b0\n##\u30b3\n##\u30b5", "##\u30b3\n##\u30b5\n##\u30b7\n##\u30b8\n##\u30b9\n##\u30ba\n##\u30bf\n##\u30c0\n##\u30c3\n##\u30c6", "##\u30c3\n##\u30c6\n##\u30c7\n##\u30c8\n##\u30c9\n##\u30ca\n##\u30cb\n##\u30cf\n##\u30d0\n##\u30d1", "##\u30d0\n##\u30d1\n##\u30d5\n##\u30d6\n##\u30d7\n##\u30de\n##\u30df\n##\u30e0\n##\u30e3\n##\u30e5", "##\u30e3\n##\u30e5\n##\u30e9\n##\u30ea\n##\u30eb\n##\u30ec\n##\u30ed\n##\u30f3\n##\u05d0\n##\u05d1", "##\u05d0\n##\u05d1\n##\u05d2\n##\u05d3\n##\u05d4\n##\u05d5\n##\u05d6\n##\u05d7\n##\u05d8\n##\u05d9", "##\u05d8\n##\u05d9\n##\u05db\n##\u05dc\n##\u05dd\n##\u05de\n##\u05df\n##\u05e0\n##\u05e1\n##\u05e2", "##\u05e1\n##\u05e2\n##\u05e4\n##\u05e6\n##\u05e7\n##\u05e8\n##\u05e9\n##\u05ea\n##\u060c\n##\u0621", "##\u060c\n##\u0621\n##\u0622\n##\u0623\n##\u0625\n##\u0626\n##\u0627\n##\u0628\n##\u062a\n##\u062b", "##\u062a\n##\u062b\n##\u062c\n##\u062d\n##\u062e\n##\u0630\n##\u0632\n##\u0633\n##\u0634\n##\u0635", "##\u0634\n##\u0635\n##\u0636\n##\u0637\n##\u0638\n##\u0639\n##\u063a\n##\u0641\n##\u0642\n##\u0643", "##\u0642\n##\u0643\n##\u0644\n##\u0648\n##\u0649\n##\u064e\n##\u0650\n##\u0679\n##\u067e\n##\u0686", "##\u067e\n##\u0686\n##\u06a9\n##\u06af\n##\u06c1\n##\u06cc\n##\u06d2\n##\u0902\n##\u0906\n##\u0915", "##\u0906\n##\u0915\n##\u0917\n##\u091a\n##\u091c\n##\u0923\n##\u0924\n##\u0926\n##\u0927\n##\u0928", "##\u0927\n##\u0928\n##\u092a\n##\u092c\n##\u092d\n##\u092e\n##\u092f\n##\u0930\n##\u0932\n##\u0935", "##\u0932\n##\u0935\n##\u0936\n##\u0937\n##\u0938\n##\u0939\n##\u093e\n##\u093f\n##\u0940\n##\u0941", "##\u0940\n##\u0941\n##\u0947\n##\u094b\n##\u094d\n##\u0964\n##\u0965\n##\u0986\n##\u0987\n##\u098f", "##\u0987\n##\u098f\n##\u0993\n##\u0995\n##\u0996\n##\u0997\n##\u099a\n##\u099b\n##\u099c\n##\u099f", "##\u099c\n##\u099f\n##\u09a4\n##\u09a5\n##\u09a6\n##\u09a7\n##\u09a8\n##\u09aa\n##\u09ac\n##\u09ae", "##\u09ac\n##\u09ae\n##\u09af\n##\u09b0\n##\u09b2\n##\u09b6\n##\u09b8\n##\u09b9\n##\u09bc\n##\u09be", "##\u09bc\n##\u09be\n##\u09bf\n##\u09c0\n##\u09c1\n##\u09c7\n##\u09cb\n##\u09cd\n##\u09df\n##\u0b95", "##\u09df\n##\u0b95\n##\u0ba4\n##\u0baa\n##\u0bae\n##\u0baf\n##\u0bb0\n##\u0bb2\n##\u0bb5\n##\u0bbe", "##\u0bb5\n##\u0bbe\n##\u0bbf\n##\u0bc1\n##\u0bcd\n##\u0e23\n##\u0f0b\n##\u0f42\n##\u0f44\n##\u0f51", "##\u0f44\n##\u0f51\n##\u0f53\n##\u0f56\n##\u0f58\n##\u0f62\n##\u0f63\n##\u0f66\n##\u0f72\n##\u0f74", "##\u0f72\n##\u0f74\n##\u0f7a\n##\u0f7c\n##\u10d0\n##\u10d4\n##\u10d8\n##\u10da\n##\u10dc\n##\u10dd", "##\u10dc\n##\u10dd\n##\u10e0\n##\u10e1\n##\u1d2c\n##\u1d35\n##\u1d40\n##\u1d43\n##\u1d47\n##\u1d48", "##\u1d47\n##\u1d48\n##\u1d49\n##\u1d4d\n##\u1d4f\n##\u1d50\n##\u1d52\n##\u1d56\n##\u1d57\n##\u1d58", "##\u1d57\n##\u1d58\n##\u1d63\n##\u1d64\n##\u1d65\n##\u1d9c\n##\u1da0\n##\u1e0d\n##\u1e24\n##\u1e25", "##\u1e24\n##\u1e25\n##\u1e28\n##\u1e29\n##\u1e33\n##\u1e43\n##\u1e45\n##\u1e47\n##\u1e5b\n##\u1e63", "##\u1e5b\n##\u1e63\n##\u1e6d\n##\u1ea1\n##\u1ea3\n##\u1ea5\n##\u1ea7\n##\u1ea9\n##\u1ead\n##\u1eaf", "##\u1ead\n##\u1eaf\n##\u1ebf\n##\u1ec1\n##\u1ec3\n##\u1ec5\n##\u1ec7\n##\u1ecb\n##\u1ecd\n##\u1ed1", "##\u1ecd\n##\u1ed1\n##\u1ed3\n##\u1ed5\n##\u1ed9\n##\u1edb\n##\u1edd\n##\u1ee3\n##\u1ee5\n##\u1ee7", "##\u1ee5\n##\u1ee7\n##\u1ee9\n##\u1eeb\n##\u1eed\n##\u1eef\n##\u1ef1\n##\u1ef3\n##\u1ef9\n##\u1f00", "##\u1ef9\n##\u1f00\n##\u1f10\n##\u1f41\n##\u1f50\n##\u1f70\n##\u1f76\n##\u1f78\n##\u1fc6\n##\u1fd6", "##\u1fc6\n##\u1fd6\n##\u1fe6\n##\u1ff6\n##\u00bc\n##\u00be\n##\u00bf\n##\u00c0\n##\u00c1\n##\u00c2", "##\u00c1\n##\u00c2\n##\u00c4\n##\u00c5\n##\u00c6\n##\u00c7\n##\u00c8\n##\u00c9\n##\u00cd\n##\u00ce", "##\u00cd\n##\u00ce\n##\u00d1\n##\u00d3\n##\u00d6\n##\u00d7\n##\u00d8\n##\u00da\n##\u00dc\n##\u00de", "##\u00dc\n##\u00de\n##\u00e2\n##\u00e3\n##\u00e6\n##\u00e7\n##\u00ee\n##\u00ef\n##\u00f0\n##\u00f1", "##\u00f0\n##\u00f1\n##\u00f4\n##\u00f5\n##\u00f7\n##\u00fb\n##\u00fe\n##\u00ff\n##\u0100\n##\u0105", "##\u0100\n##\u0105\n##\u0106\n##\u010c\n##\u010f\n##\u0110\n##\u0111\n##\u0113\n##\u0117\n##\u0119", "##\u0117\n##\u0119\n##\u011b\n##\u011f\n##\u0121\n##\u0126\n##\u0127\n##\u0129\n##\u012a\n##\u0130", "##\u012a\n##\u0130\n##\u013c\n##\u013d\n##\u013e\n##\u0141\n##\u0146\n##\u0148\n##\u014b\n##\u014c", "##\u014b\n##\u014c\n##\u014f\n##\u0151\n##\u0152\n##\u0153\n##\u0159\n##\u015a\n##\u015b\n##\u015e", "##\u015b\n##\u015e\n##\u0160\n##\u0162\n##\u0163\n##\u0165\n##\u0169\n##\u016d\n##\u016f\n##\u0171", "##\u016f\n##\u0171\n##\u0173\n##\u0175\n##\u0177\n##\u017a\n##\u017b\n##\u017c\n##\u017d\n##\u017e", "##\u017d\n##\u017e\n##\u018f\n##\u0192\n##\u01a1\n##\u01b0\n##\u01ce\n##\u01d0\n##\u01d2\n##\u01d4", "##\u01d2\n##\u01d4\n##\u01eb\n##\u0218\n##\u021a\n##\u021b\n##\u0250\n\u00c0\n\u00c1\n\u00f1", "\u00c1\n\u00f1\n\u00f2\n\u00f5\n\u00f6\n\u00f7\n\u00f8\n\u00f9\n\u00fa\n\u00fb", "\u00fa\n\u00fb\n\u00fc\n\u00fd\n\u00fe\n\u00ff\n\u0100\n\u0101\n\u0102\n\u0103", "\u0102\n\u0103\n\u0104\n\u0105\n\u0106\n\u0107\n\u0108\n\u0109\n\u010a\n\u010b", "\u010a\n\u010b\n\u010c\n\u010d\n\u010e\n\u010f\n\u0110\n\u0111\n\u0112\n\u0113", "\u0112\n\u0113\n\u0114\n\u0115\n\u0116\n\u0117\n\u0118\n\u0119\n\u011a\n\u011b", "\u011a\n\u011b\n\u011c\n\u011d\n\u011e\n\u011f\n\u010a\u010a\n\u00c2\u0142\n\u00c2\u0142\u00c2\u0142\n\u0120\u00c2\u0142", "\u00c2\u0142\u00c2\u0142\n\u0120\u00c2\u0142\n\u00c2\u0142\u00c2\u0142\u00c2\u0142\u00c2\u0142\n\u0120\u00c2\u0142\u0120\u00c2\u0142\nwcsstore\n\u00c2\u0142\u00c2\u0142\u00c2\u0142\u00c2\u0142\u00c2\u0142\u00c2\u0142\u00c2\u0142\u00c2\u0142\n\u0120Dragonbound\n\u0120guiActive\n\u0120\u00c2\u0142\u0120\u00c2\u0142\u0120\u00c2\u0142\u0120\u00c2\u0142\n\u013c\u00e9\u0128\u0134", "\u0120\u00c2\u0142\u0120\u00c2\u0142\u0120\u00c2\u0142\u0120\u00c2\u0142\n\u013c\u00e9\u0128\u0134\n\u0120davidjl\n\u00e8\u00a6\u013c\u00e9\u0128\u0134\n\"\"\".strip().split(\"\\n\")\n\n\ndef build_prompt_tokens(tokenizer):\n    for i in range(1000):\n        PROMPT_TOKENS.append(f\"\u2581{i}\")\n        PROMPT_TOKENS.append(f\"\u2581#{i}\")\n    return list(filter(lambda x:x in tokenizer.get_vocab(), PROMPT_TOKENS))", ""]}
{"filename": "prompt4ner/__init__.py", "chunked_list": [""]}
{"filename": "prompt4ner/util.py", "chunked_list": ["import csv\nimport json\nimport os\nimport random\nimport shutil\n\nimport numpy as np\nimport torch\n\nfrom prompt4ner.entities import TokenSpan", "\nfrom prompt4ner.entities import TokenSpan\n\nCSV_DELIMETER = ';'\n\n\ndef create_directories_file(f):\n    d = os.path.dirname(f)\n\n    if d and not os.path.exists(d):\n        os.makedirs(d)\n\n    return f", "\n\ndef create_directories_dir(d):\n    if d and not os.path.exists(d):\n        os.makedirs(d)\n\n    return d\n\n\ndef create_csv(file_path, *column_names):\n    if not os.path.exists(file_path):\n        with open(file_path, 'w', newline='') as csv_file:\n            writer = csv.writer(csv_file, delimiter=CSV_DELIMETER, quotechar='|', quoting=csv.QUOTE_MINIMAL)\n\n            if column_names:\n                writer.writerow(column_names)", "\ndef create_csv(file_path, *column_names):\n    if not os.path.exists(file_path):\n        with open(file_path, 'w', newline='') as csv_file:\n            writer = csv.writer(csv_file, delimiter=CSV_DELIMETER, quotechar='|', quoting=csv.QUOTE_MINIMAL)\n\n            if column_names:\n                writer.writerow(column_names)\n\n\ndef append_csv(file_path, *row):\n    if not os.path.exists(file_path):\n        raise Exception(\"The given file doesn't exist\")\n\n    with open(file_path, 'a', newline='') as csv_file:\n        writer = csv.writer(csv_file, delimiter=CSV_DELIMETER, quotechar='|', quoting=csv.QUOTE_MINIMAL)\n        writer.writerow(row)", "\n\ndef append_csv(file_path, *row):\n    if not os.path.exists(file_path):\n        raise Exception(\"The given file doesn't exist\")\n\n    with open(file_path, 'a', newline='') as csv_file:\n        writer = csv.writer(csv_file, delimiter=CSV_DELIMETER, quotechar='|', quoting=csv.QUOTE_MINIMAL)\n        writer.writerow(row)\n", "\n\ndef append_csv_multiple(file_path, *rows):\n    if not os.path.exists(file_path):\n        raise Exception(\"The given file doesn't exist\")\n\n    with open(file_path, 'a', newline='') as csv_file:\n        writer = csv.writer(csv_file, delimiter=CSV_DELIMETER, quotechar='|', quoting=csv.QUOTE_MINIMAL)\n        for row in rows:\n            writer.writerow(row)", "\n\ndef read_csv(file_path):\n    lines = []\n    with open(file_path, 'r') as csv_file:\n        reader = csv.reader(csv_file, delimiter=CSV_DELIMETER, quotechar='|', quoting=csv.QUOTE_MINIMAL)\n        for row in reader:\n            lines.append(row)\n\n    return lines[0], lines[1:]", "\n\ndef copy_python_directory(source, dest, ignore_dirs=None):\n    source = source if source.endswith('/') else source + '/'\n    for (dir_path, dir_names, file_names) in os.walk(source):\n        tail = '/'.join(dir_path.split(source)[1:])\n        new_dir = os.path.join(dest, tail)\n\n        if ignore_dirs and True in [(ignore_dir in tail) for ignore_dir in ignore_dirs]:\n            continue\n\n        create_directories_dir(new_dir)\n\n        for file_name in file_names:\n            if file_name.endswith('.py'):\n                file_path = os.path.join(dir_path, file_name)\n                shutil.copy2(file_path, new_dir)", "\n\ndef save_dict(log_path, dic, name):\n    # save arguments\n    # 1. as json\n    path = os.path.join(log_path, '%s.json' % name)\n    f = open(path, 'w')\n    json.dump(vars(dic), f, sort_keys=True, indent=4)\n    f.close()\n\n    # 2. as string\n    path = os.path.join(log_path, '%s.txt' % name)\n    f = open(path, 'w')\n    # args_str = [\"%s = %s\" % (key, value) for key, value in sorted(vars(dic).items())]\n    args_str = [\"%s = %s\" % (key, value) for key, value in vars(dic).items()]\n\n    f.write('\\n'.join(args_str))\n    f.close()", "\n\ndef summarize_dict(summary_writer, dic, name):\n    table = 'Argument|Value\\n-|-'\n\n    for k, v in vars(dic).items():\n        row = '\\n%s|%s' % (k, v)\n        table += row\n    summary_writer.add_text(name, table)\n", "\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef reset_logger(logger):\n    for handler in logger.handlers[:]:\n        logger.removeHandler(handler)\n\n    for f in logger.filters[:]:\n        logger.removeFilters(f)", "\ndef reset_logger(logger):\n    for handler in logger.handlers[:]:\n        logger.removeHandler(handler)\n\n    for f in logger.filters[:]:\n        logger.removeFilters(f)\n\n\ndef flatten(l):\n    return [i for p in l for i in p]", "\ndef flatten(l):\n    return [i for p in l for i in p]\n\n\ndef get_as_list(dic, key):\n    if key in dic:\n        return [dic[key]]\n    else:\n        return []", "\n\ndef extend_tensor(tensor, extended_shape, fill=0):\n    tensor_shape = tensor.shape\n\n    extended_tensor = torch.zeros(extended_shape, dtype=tensor.dtype).to(tensor.device)\n    extended_tensor = extended_tensor.fill_(fill)\n\n    if len(tensor_shape) == 1:\n        extended_tensor[:tensor_shape[0]] = tensor\n    elif len(tensor_shape) == 2:\n        extended_tensor[:tensor_shape[0], :tensor_shape[1]] = tensor\n    elif len(tensor_shape) == 3:\n        extended_tensor[:tensor_shape[0], :tensor_shape[1], :tensor_shape[2]] = tensor\n    elif len(tensor_shape) == 4:\n        extended_tensor[:tensor_shape[0], :tensor_shape[1], :tensor_shape[2], :tensor_shape[3]] = tensor\n\n    return extended_tensor", "\n\ndef padded_stack(tensors, padding=0):\n    dim_count = len(tensors[0].shape)\n\n    max_shape = [max([t.shape[d] for t in tensors]) for d in range(dim_count)]\n    padded_tensors = []\n\n    for t in tensors:\n        e = extend_tensor(t, max_shape, fill=padding)\n        padded_tensors.append(e)\n\n    stacked = torch.stack(padded_tensors)\n    return stacked", "\n\ndef batch_index(tensor, index, pad=False):\n    if tensor.shape[0] != index.shape[0]:\n        raise Exception()\n\n    if not pad:\n        return torch.stack([tensor[i][index[i]] for i in range(index.shape[0])])\n    else:\n        return padded_stack([tensor[i][index[i]] for i in range(index.shape[0])])", "\n\ndef padded_nonzero(tensor, padding=0):\n    indices = padded_stack([tensor[i].nonzero().view(-1) for i in range(tensor.shape[0])], padding)\n    return indices\n\n\ndef swap(v1, v2):\n    return v2, v1\n", "\n\ndef get_span_tokens(tokens, span):\n    inside = False\n    span_tokens = []\n\n    for t in tokens:\n        # print(t.index)\n        if t.index == span[0]:\n            inside = True\n\n        if inside:\n            span_tokens.append(t)\n\n        if inside and t.index == span[1]:\n            return TokenSpan(span_tokens)\n\n    return None", "\n\ndef to_device(batch, device, skip_keys = ['meta_doc'], nested_keys = ['image_inputs']):\n    converted_batch = dict()\n    for key in batch.keys():\n        if key in nested_keys:\n            if batch[key] == None:\n                converted_batch[key] = None\n            else:\n                converted_batch[key] = dict((k, v.to(device)) for k, v in batch[key].items())\n            continue\n        \n        if batch[key] is None:\n            converted_batch[key] = None\n            continue\n        \n        if key in skip_keys:\n            converted_batch[key] = batch[key]\n        else:\n            converted_batch[key] = batch[key].to(device)\n\n    return converted_batch", "\n\ndef round(arr, n_digits):\n    return torch.round(arr * 10**n_digits) / (10**n_digits)\n\ndef combine(sub, sup_mask, pool_type = \"max\" ):\n    sup = None\n    if pool_type == \"first\":\n        sup_mask_shift = torch.roll(sup_mask, 1, -1)\n        sup_mask = sup_mask&(~sup_mask_shift)\n\n        m = (sup_mask.unsqueeze(-1) == 0).float() * (-1e30)\n        sup = m + sub.unsqueeze(1).repeat(1, sup_mask.shape[1], 1, 1)\n        sup = sup.max(dim=2)[0]\n        sup[sup==-1e30]=0\n\n    if pool_type == \"last\":\n        sup_mask_shift = torch.roll(sup_mask, -1, -1)\n        sup_mask = sup_mask&(~sup_mask_shift)\n        \n        m = (sup_mask.unsqueeze(-1) == 0).float() * (-1e30)\n        sup = m + sub.unsqueeze(1).repeat(1, sup_mask.shape[1], 1, 1)\n        sup = sup.max(dim=2)[0]\n        sup[sup==-1e30]=0\n\n    if len(sub.shape) == len(sup_mask.shape):   # sub -> B #ST E ==== sup_mask -> B #T #ST\n        if pool_type == \"mean\":\n            size = (sup_mask == 1).float().sum(-1).unsqueeze(-1) + 1e-30\n            m = (sup_mask.unsqueeze(-1) == 1).float()\n            sup = m * sub.unsqueeze(1).repeat(1, sup_mask.shape[1], 1, 1)\n            sup = sup.sum(dim=2) / size\n        if pool_type == \"sum\":\n            m = (sup_mask.unsqueeze(-1) == 1).float()\n            sup = m * sub.unsqueeze(1).repeat(1, sup_mask.shape[1], 1, 1)\n            sup = sup.sum(dim=2)\n        if pool_type == \"max\":\n            m = (sup_mask.unsqueeze(-1) == 0).float() * (-1e30)\n            sup = m + sub.unsqueeze(1).repeat(1, sup_mask.shape[1], 1, 1)\n            sup = sup.max(dim=2)[0]\n            sup[sup==-1e30]=0\n    else: # sub -> B #T #C E ==== sup_mask -> B #T #C\n        if pool_type == \"mean\":\n            size = (sup_mask == 1).float().sum(-1).unsqueeze(-1) + 1e-30\n            m = (sup_mask.unsqueeze(-1) == 1).float()\n            sup = m * sub\n            sup = sup.sum(dim=2) / size\n        if pool_type == \"sum\":\n            m = (sup_mask.unsqueeze(-1) == 1).float()\n            sup = m * sub\n            sup = sup.sum(dim=2)\n        if pool_type == \"max\":\n            m = (sup_mask.unsqueeze(-1) == 0).float() * (-1e30)\n            sup = m + sub\n            sup = sup.max(dim=2)[0]\n            sup[sup==-1e30]=0\n    return sup"]}
{"filename": "prompt4ner/modeling_roberta.py", "chunked_list": ["# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch RoBERTa model.\"\"\"\n\nimport math", "\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom packaging import version\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n", "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN, gelu\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    BaseModelOutputWithPoolingAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    QuestionAnsweringModelOutput,", "    MultipleChoiceModelOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\nfrom transformers.utils import (\n    add_code_sample_docstrings,\n    add_start_docstrings,", "    add_code_sample_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers import RobertaConfig\n\n\nlogger = logging.get_logger(__name__)", "\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"roberta-base\"\n_CONFIG_FOR_DOC = \"RobertaConfig\"\n_TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\n\nROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"roberta-base\",\n    \"roberta-large\",", "    \"roberta-base\",\n    \"roberta-large\",\n    \"roberta-large-mnli\",\n    \"distilroberta-base\",\n    \"roberta-base-openai-detector\",\n    \"roberta-large-openai-detector\",\n    # See all RoBERTa models at https://huggingface.co/models?filter=roberta\n]\n\n\nclass PromptEmbedding(nn.Module):\n    __constants__ = ['num_embeddings', 'embedding_dim', 'padding_idx', 'max_norm',\n                     'norm_type', 'scale_grad_by_freq', 'sparse']\n\n    num_embeddings: int\n    embedding_dim: int\n    padding_idx: Optional[int]\n    max_norm: Optional[float]\n    norm_type: float\n    scale_grad_by_freq: bool\n    weight: torch.Tensor\n    sparse: bool\n\n    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None,\n                 max_norm: Optional[float] = None, norm_type: float = 2., scale_grad_by_freq: bool = False,\n                 sparse: bool = False, _weight: Optional[torch.Tensor] = None) -> None:\n        super(PromptEmbedding, self).__init__()\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        if padding_idx is not None:\n            if padding_idx > 0:\n                assert padding_idx < self.num_embeddings, 'Padding_idx must be within num_embeddings'\n            elif padding_idx < 0:\n                assert padding_idx >= -self.num_embeddings, 'Padding_idx must be within num_embeddings'\n                padding_idx = self.num_embeddings + padding_idx\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        if _weight is None:\n            self.weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))\n            # self.prompt_weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))\n            self.reset_parameters()\n        else:\n            assert list(_weight.shape) == [num_embeddings, embedding_dim], \\\n                'Shape of weight does not match num_embeddings and embedding_dim'\n            self.weight = nn.Parameter(_weight)\n\n        self.sparse = sparse\n\n    def reset_parameters(self) -> None:\n        torch.nn.init.normal_(self.weight)\n        self._fill_padding_idx_with_zero()\n\n    def _fill_padding_idx_with_zero(self) -> None:\n        if self.padding_idx is not None:\n            with torch.no_grad():\n                self.weight[self.padding_idx].fill_(0)\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        # fix_weight = self.weight.detach()\n        # weight_new = self.weight*self.prompt_mask + fix_weight*(1-self.prompt_mask)\n        return torch.nn.functional.embedding(\n            input, self.weight, self.padding_idx, self.max_norm,\n            self.norm_type, self.scale_grad_by_freq, self.sparse)\n\n    def extra_repr(self) -> str:\n        s = '{num_embeddings}, {embedding_dim}'\n        if self.padding_idx is not None:\n            s += ', padding_idx={padding_idx}'\n        if self.max_norm is not None:\n            s += ', max_norm={max_norm}'\n        if self.norm_type != 2:\n            s += ', norm_type={norm_type}'\n        if self.scale_grad_by_freq is not False:\n            s += ', scale_grad_by_freq={scale_grad_by_freq}'\n        if self.sparse is not False:\n            s += ', sparse=True'\n        return s.format(**self.__dict__)\n\n    @torch.no_grad()\n    def init_prompt_embeddings(self, prompt_ids, init_method = 'zero'):\n        prompt_mask = torch.zeros_like(self.weight.data)\n        prompt_mask[prompt_ids] = 1\n        new = torch.zeros(self.weight.data.size(0),self.weight.data.size(1))\n        if init_method == 'normal':\n            new.normal_(mean=0.0, std=0.02)\n        self.weight.data = self.weight.data*(1-prompt_mask) + new*prompt_mask\n\n        # prompt_mask = torch.zeros_like(self.weight.data)\n        # prompt_mask[prompt_ids] = 1\n        # self.weight.data[prompt_mask].normal_(mean=0.0, std=0.02)\n    \n    @classmethod\n    def from_pretrained(cls, embeddings, freeze=True, padding_idx=None,\n                        max_norm=None, norm_type=2., scale_grad_by_freq=False,\n                        sparse=False):\n        r\"\"\"Creates Embedding instance from given 2-dimensional FloatTensor.\n\n        Args:\n            embeddings (Tensor): FloatTensor containing weights for the Embedding.\n                First dimension is being passed to Embedding as ``num_embeddings``, second as ``embedding_dim``.\n            freeze (boolean, optional): If ``True``, the tensor does not get updated in the learning process.\n                Equivalent to ``embedding.weight.requires_grad = False``. Default: ``True``\n            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n                                         i.e. it remains as a fixed \"pad\".\n            max_norm (float, optional): See module initialization documentation.\n            norm_type (float, optional): See module initialization documentation. Default ``2``.\n            scale_grad_by_freq (boolean, optional): See module initialization documentation. Default ``False``.\n            sparse (bool, optional): See module initialization documentation.\n\n        Examples::\n\n            >>> # FloatTensor containing pretrained weights\n            >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n            >>> embedding = nn.Embedding.from_pretrained(weight)\n            >>> # Get embeddings for index 1\n            >>> input = torch.LongTensor([1])\n            >>> embedding(input)\n            tensor([[ 4.0000,  5.1000,  6.3000]])\n        \"\"\"\n        assert embeddings.dim() == 2, \\\n            'Embeddings parameter is expected to be 2-dimensional'\n        rows, cols = embeddings.shape\n        embedding = cls(\n            num_embeddings=rows,\n            embedding_dim=cols,\n            _weight=embeddings,\n            padding_idx=padding_idx,\n            max_norm=max_norm,\n            norm_type=norm_type,\n            scale_grad_by_freq=scale_grad_by_freq,\n            sparse=sparse)\n        embedding.weight.requires_grad = not freeze\n        return embedding", "\n\nclass PromptEmbedding(nn.Module):\n    __constants__ = ['num_embeddings', 'embedding_dim', 'padding_idx', 'max_norm',\n                     'norm_type', 'scale_grad_by_freq', 'sparse']\n\n    num_embeddings: int\n    embedding_dim: int\n    padding_idx: Optional[int]\n    max_norm: Optional[float]\n    norm_type: float\n    scale_grad_by_freq: bool\n    weight: torch.Tensor\n    sparse: bool\n\n    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None,\n                 max_norm: Optional[float] = None, norm_type: float = 2., scale_grad_by_freq: bool = False,\n                 sparse: bool = False, _weight: Optional[torch.Tensor] = None) -> None:\n        super(PromptEmbedding, self).__init__()\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        if padding_idx is not None:\n            if padding_idx > 0:\n                assert padding_idx < self.num_embeddings, 'Padding_idx must be within num_embeddings'\n            elif padding_idx < 0:\n                assert padding_idx >= -self.num_embeddings, 'Padding_idx must be within num_embeddings'\n                padding_idx = self.num_embeddings + padding_idx\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        if _weight is None:\n            self.weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))\n            # self.prompt_weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))\n            self.reset_parameters()\n        else:\n            assert list(_weight.shape) == [num_embeddings, embedding_dim], \\\n                'Shape of weight does not match num_embeddings and embedding_dim'\n            self.weight = nn.Parameter(_weight)\n\n        self.sparse = sparse\n\n    def reset_parameters(self) -> None:\n        torch.nn.init.normal_(self.weight)\n        self._fill_padding_idx_with_zero()\n\n    def _fill_padding_idx_with_zero(self) -> None:\n        if self.padding_idx is not None:\n            with torch.no_grad():\n                self.weight[self.padding_idx].fill_(0)\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        # fix_weight = self.weight.detach()\n        # weight_new = self.weight*self.prompt_mask + fix_weight*(1-self.prompt_mask)\n        return torch.nn.functional.embedding(\n            input, self.weight, self.padding_idx, self.max_norm,\n            self.norm_type, self.scale_grad_by_freq, self.sparse)\n\n    def extra_repr(self) -> str:\n        s = '{num_embeddings}, {embedding_dim}'\n        if self.padding_idx is not None:\n            s += ', padding_idx={padding_idx}'\n        if self.max_norm is not None:\n            s += ', max_norm={max_norm}'\n        if self.norm_type != 2:\n            s += ', norm_type={norm_type}'\n        if self.scale_grad_by_freq is not False:\n            s += ', scale_grad_by_freq={scale_grad_by_freq}'\n        if self.sparse is not False:\n            s += ', sparse=True'\n        return s.format(**self.__dict__)\n\n    @torch.no_grad()\n    def init_prompt_embeddings(self, prompt_ids, init_method = 'zero'):\n        prompt_mask = torch.zeros_like(self.weight.data)\n        prompt_mask[prompt_ids] = 1\n        new = torch.zeros(self.weight.data.size(0),self.weight.data.size(1))\n        if init_method == 'normal':\n            new.normal_(mean=0.0, std=0.02)\n        self.weight.data = self.weight.data*(1-prompt_mask) + new*prompt_mask\n\n        # prompt_mask = torch.zeros_like(self.weight.data)\n        # prompt_mask[prompt_ids] = 1\n        # self.weight.data[prompt_mask].normal_(mean=0.0, std=0.02)\n    \n    @classmethod\n    def from_pretrained(cls, embeddings, freeze=True, padding_idx=None,\n                        max_norm=None, norm_type=2., scale_grad_by_freq=False,\n                        sparse=False):\n        r\"\"\"Creates Embedding instance from given 2-dimensional FloatTensor.\n\n        Args:\n            embeddings (Tensor): FloatTensor containing weights for the Embedding.\n                First dimension is being passed to Embedding as ``num_embeddings``, second as ``embedding_dim``.\n            freeze (boolean, optional): If ``True``, the tensor does not get updated in the learning process.\n                Equivalent to ``embedding.weight.requires_grad = False``. Default: ``True``\n            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n                                         i.e. it remains as a fixed \"pad\".\n            max_norm (float, optional): See module initialization documentation.\n            norm_type (float, optional): See module initialization documentation. Default ``2``.\n            scale_grad_by_freq (boolean, optional): See module initialization documentation. Default ``False``.\n            sparse (bool, optional): See module initialization documentation.\n\n        Examples::\n\n            >>> # FloatTensor containing pretrained weights\n            >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n            >>> embedding = nn.Embedding.from_pretrained(weight)\n            >>> # Get embeddings for index 1\n            >>> input = torch.LongTensor([1])\n            >>> embedding(input)\n            tensor([[ 4.0000,  5.1000,  6.3000]])\n        \"\"\"\n        assert embeddings.dim() == 2, \\\n            'Embeddings parameter is expected to be 2-dimensional'\n        rows, cols = embeddings.shape\n        embedding = cls(\n            num_embeddings=rows,\n            embedding_dim=cols,\n            _weight=embeddings,\n            padding_idx=padding_idx,\n            max_norm=max_norm,\n            norm_type=norm_type,\n            scale_grad_by_freq=scale_grad_by_freq,\n            sparse=sparse)\n        embedding.weight.requires_grad = not freeze\n        return embedding", "\nclass RobertaEmbeddings(nn.Module):\n    \"\"\"\n    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n    \"\"\"\n\n    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = PromptEmbedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n            self.register_buffer(\n                \"token_type_ids\",\n                torch.zeros(self.position_ids.size(), dtype=torch.long),\n                persistent=False,\n            )\n\n        # End copy\n        self.padding_idx = config.pad_token_id\n        self.position_embeddings = nn.Embedding(\n            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n        )\n\n    def forward(\n        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n    ):\n        if position_ids is None:\n            if input_ids is not None:\n                # Create the position ids from the input token ids. Any padded tokens remain padded.\n                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n            else:\n                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n\n        if input_ids is not None:\n            input_shape = input_ids.size()\n        else:\n            input_shape = inputs_embeds.size()[:-1]\n\n        seq_length = input_shape[1]\n\n        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n        # issue #5664\n        if token_type_ids is None:\n            if hasattr(self, \"token_type_ids\"):\n                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n\n        if inputs_embeds is None:\n            inputs_embeds = self.word_embeddings(input_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = inputs_embeds + token_type_embeddings\n        if self.position_embedding_type == \"absolute\":\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings += position_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n        \"\"\"\n        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n\n        Args:\n            inputs_embeds: torch.Tensor\n\n        Returns: torch.Tensor\n        \"\"\"\n        input_shape = inputs_embeds.size()[:-1]\n        sequence_length = input_shape[1]\n\n        position_ids = torch.arange(\n            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n        )\n        return position_ids.unsqueeze(0).expand(input_shape)", "\n\n# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Roberta\nclass RobertaSelfAttention(nn.Module):\n    def __init__(self, config, position_embedding_type=None):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.position_embedding_type = position_embedding_type or getattr(\n            config, \"position_embedding_type\", \"absolute\"\n        )\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            self.max_position_embeddings = config.max_position_embeddings\n            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n\n        self.is_decoder = config.is_decoder\n\n    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        mixed_query_layer = self.query(hidden_states)\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_layer = past_key_value[0]\n            value_layer = past_key_value[1]\n            attention_mask = encoder_attention_mask\n        elif is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            seq_length = hidden_states.size()[1]\n            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n            distance = position_ids_l - position_ids_r\n            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n\n            if self.position_embedding_type == \"relative_key\":\n                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(new_context_layer_shape)\n\n        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n\n        if self.is_decoder:\n            outputs = outputs + (past_key_value,)\n        return outputs", "\n\n# Copied from transformers.models.bert.modeling_bert.BertSelfOutput\nclass RobertaSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states", "\n\n# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta\nclass RobertaAttention(nn.Module):\n    def __init__(self, config, position_embedding_type=None):\n        super().__init__()\n        self.self = RobertaSelfAttention(config, position_embedding_type=position_embedding_type)\n        self.output = RobertaSelfOutput(config)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n        )\n\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions,\n        )\n        attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs", "\n\n# Copied from transformers.models.bert.modeling_bert.BertIntermediate\nclass RobertaIntermediate(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states", "\n\n# Copied from transformers.models.bert.modeling_bert.BertOutput\nclass RobertaOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states", "\n\n# Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Roberta\nclass RobertaLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.attention = RobertaAttention(config)\n        self.is_decoder = config.is_decoder\n        self.add_cross_attention = config.add_cross_attention\n        if self.add_cross_attention:\n            if not self.is_decoder:\n                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n            self.crossattention = RobertaAttention(config, position_embedding_type=\"absolute\")\n        self.intermediate = RobertaIntermediate(config)\n        self.output = RobertaOutput(config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=self_attn_past_key_value,\n        )\n        attention_output = self_attention_outputs[0]\n\n        # if decoder, the last output is tuple of self-attn cache\n        if self.is_decoder:\n            outputs = self_attention_outputs[1:-1]\n            present_key_value = self_attention_outputs[-1]\n        else:\n            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n        cross_attn_present_key_value = None\n        if self.is_decoder and encoder_hidden_states is not None:\n            if not hasattr(self, \"crossattention\"):\n                raise ValueError(\n                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                    \" by setting `config.add_cross_attention=True`\"\n                )\n\n            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                cross_attn_past_key_value,\n                output_attentions,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n\n            # add cross-attn cache to positions 3,4 of present_key_value tuple\n            cross_attn_present_key_value = cross_attention_outputs[-1]\n            present_key_value = present_key_value + cross_attn_present_key_value\n\n        layer_output = apply_chunking_to_forward(\n            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n        )\n        outputs = (layer_output,) + outputs\n\n        # if decoder, return the attn key/values as the last output\n        if self.is_decoder:\n            outputs = outputs + (present_key_value,)\n\n        return outputs\n\n    def feed_forward_chunk(self, attention_output):\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output", "\n\n# Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Roberta\nclass RobertaEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = False,\n        output_hidden_states: Optional[bool] = False,\n        return_dict: Optional[bool] = True,\n    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n\n        next_decoder_cache = () if use_cache else None\n        for i, layer_module in enumerate(self.layer):\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n            past_key_value = past_key_values[i] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n\n                if use_cache:\n                    logger.warning(\n                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                    )\n                    use_cache = False\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, past_key_value, output_attentions)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    past_key_value,\n                    output_attentions,\n                )\n\n            hidden_states = layer_outputs[0]\n            if use_cache:\n                next_decoder_cache += (layer_outputs[-1],)\n            if output_attentions:\n                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                if self.config.add_cross_attention:\n                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    next_decoder_cache,\n                    all_hidden_states,\n                    all_self_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_decoder_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n            cross_attentions=all_cross_attentions,\n        )", "\n\n# Copied from transformers.models.bert.modeling_bert.BertPooler\nclass RobertaPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output", "\n\nclass RobertaPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = RobertaConfig\n    base_model_prefix = \"roberta\"\n    supports_gradient_checkpointing = True\n\n    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, RobertaEncoder):\n            module.gradient_checkpointing = value\n\n    def update_keys_to_ignore(self, config, del_keys_to_ignore):\n        \"\"\"Remove some keys from ignore list\"\"\"\n        if not config.tie_word_embeddings:\n            # must make a new list, or the class variable gets modified!\n            self._keys_to_ignore_on_save = [k for k in self._keys_to_ignore_on_save if k not in del_keys_to_ignore]\n            self._keys_to_ignore_on_load_missing = [\n                k for k in self._keys_to_ignore_on_load_missing if k not in del_keys_to_ignore\n            ]", "\n\nROBERTA_START_DOCSTRING = r\"\"\"\n\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage", "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`RobertaConfig`]): Model configuration class with all the parameters of the\n            model. Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n", "\"\"\"\n\nROBERTA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `({0})`):\n            Indices of input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`RobertaTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n", "            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)", "\n            [What are attention masks?](../glossary#attention-mask)\n        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n            1]`:\n\n            - 0 corresponds to a *sentence A* token,\n            - 1 corresponds to a *sentence B* token.\n\n            [What are token type IDs?](../glossary#token-type-ids)", "\n            [What are token type IDs?](../glossary#token-type-ids)\n        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.max_position_embeddings - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n", "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        output_attentions (`bool`, *optional*):", "            model's internal embedding lookup matrix.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"", "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\n    ROBERTA_START_DOCSTRING,\n)\nclass RobertaModel(RobertaPreTrainedModel):\n    \"\"\"\n\n    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n    Kaiser and Illia Polosukhin.\n\n    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n\n    .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n\n    \"\"\"\n\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n    def __init__(self, config, add_pooling_layer=True):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = RobertaEmbeddings(config)\n        self.encoder = RobertaEncoder(config)\n\n        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n\n        if token_type_ids is None:\n            if hasattr(self.embeddings, \"token_type_ids\"):\n                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )", "class RobertaModel(RobertaPreTrainedModel):\n    \"\"\"\n\n    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n    Kaiser and Illia Polosukhin.\n\n    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n\n    .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n\n    \"\"\"\n\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n    def __init__(self, config, add_pooling_layer=True):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = RobertaEmbeddings(config)\n        self.encoder = RobertaEncoder(config)\n\n        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n\n        if token_type_ids is None:\n            if hasattr(self.embeddings, \"token_type_ids\"):\n                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"RoBERTa Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", ROBERTA_START_DOCSTRING\n)\nclass RobertaForCausalLM(RobertaPreTrainedModel):\n    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        if not config.is_decoder:\n            logger.warning(\"If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\")\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        self.lm_head = RobertaLMHead(config)\n\n        # The LM head weights require special treatment only when they are tied with the word embeddings\n        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_output_embeddings(self):\n        return self.lm_head.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n            `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n            ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import RobertaTokenizer, RobertaForCausalLM, RobertaConfig\n        >>> import torch\n\n        >>> tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n        >>> config = RobertaConfig.from_pretrained(\"roberta-base\")\n        >>> config.is_decoder = True\n        >>> model = RobertaForCausalLM.from_pretrained(\"roberta-base\", config=config)\n\n        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n        >>> outputs = model(**inputs)\n\n        >>> prediction_logits = outputs.logits\n        ```\"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if labels is not None:\n            use_cache = False\n\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n        prediction_scores = self.lm_head(sequence_output)\n\n        lm_loss = None\n        if labels is not None:\n            # we are doing next-token prediction; shift prediction scores and input ids by one\n            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n            labels = labels[:, 1:].contiguous()\n            loss_fct = CrossEntropyLoss()\n            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((lm_loss,) + output) if lm_loss is not None else output\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=lm_loss,\n            logits=prediction_scores,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n        input_shape = input_ids.shape\n        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n        if attention_mask is None:\n            attention_mask = input_ids.new_ones(input_shape)\n\n        # cut decoder_input_ids if past is used\n        if past is not None:\n            input_ids = input_ids[:, -1:]\n\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n\n    def _reorder_cache(self, past, beam_idx):\n        reordered_past = ()\n        for layer_past in past:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n        return reordered_past", "\n\n@add_start_docstrings(\"\"\"RoBERTa Model with a `language modeling` head on top.\"\"\", ROBERTA_START_DOCSTRING)\nclass RobertaForMaskedLM(RobertaPreTrainedModel):\n    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        if config.is_decoder:\n            logger.warning(\n                \"If you want to use `RobertaForMaskedLM` make sure `config.is_decoder=False` for \"\n                \"bi-directional self-attention.\"\n            )\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        self.lm_head = RobertaLMHead(config)\n\n        # The LM head weights require special treatment only when they are tied with the word embeddings\n        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_output_embeddings(self):\n        return self.lm_head.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head.decoder = new_embeddings\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MaskedLMOutput,\n        config_class=_CONFIG_FOR_DOC,\n        mask=\"<mask>\",\n        expected_output=\"' Paris'\",\n        expected_loss=0.1,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        kwargs (`Dict[str, any]`, optional, defaults to *{}*):\n            Used to hide legacy arguments that have been deprecated.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = outputs[0]\n        prediction_scores = self.lm_head(sequence_output)\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return MaskedLMOutput(\n            loss=masked_lm_loss,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\nclass RobertaLMHead(nn.Module):\n    \"\"\"Roberta Head for masked language modeling.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n        self.decoder.bias = self.bias\n\n    def forward(self, features, **kwargs):\n        x = self.dense(features)\n        x = gelu(x)\n        x = self.layer_norm(x)\n\n        # project back to size of vocabulary with bias\n        x = self.decoder(x)\n\n        return x\n\n    def _tie_weights(self):\n        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n        self.bias = self.decoder.bias", "\n\n@add_start_docstrings(\n    \"\"\"\n    RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n    pooled output) e.g. for GLUE tasks.\n    \"\"\",\n    ROBERTA_START_DOCSTRING,\n)\nclass RobertaForSequenceClassification(RobertaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        self.classifier = RobertaClassificationHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"cardiffnlp/twitter-roberta-base-emotion\",\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=\"'optimism'\",\n        expected_loss=0.08,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = outputs[0]\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass RobertaForSequenceClassification(RobertaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        self.classifier = RobertaClassificationHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"cardiffnlp/twitter-roberta-base-emotion\",\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=\"'optimism'\",\n        expected_loss=0.08,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = outputs[0]\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n    softmax) e.g. for RocStories/SWAG tasks.\n    \"\"\",\n    ROBERTA_START_DOCSTRING,\n)\nclass RobertaForMultipleChoice(RobertaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.roberta = RobertaModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], MultipleChoiceModelOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n            `input_ids` above)\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        flat_inputs_embeds = (\n            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n            if inputs_embeds is not None\n            else None\n        )\n\n        outputs = self.roberta(\n            flat_input_ids,\n            position_ids=flat_position_ids,\n            token_type_ids=flat_token_type_ids,\n            attention_mask=flat_attention_mask,\n            head_mask=head_mask,\n            inputs_embeds=flat_inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n\n        if not return_dict:\n            output = (reshaped_logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MultipleChoiceModelOutput(\n            loss=loss,\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass RobertaForMultipleChoice(RobertaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.roberta = RobertaModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], MultipleChoiceModelOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n            `input_ids` above)\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        flat_inputs_embeds = (\n            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n            if inputs_embeds is not None\n            else None\n        )\n\n        outputs = self.roberta(\n            flat_input_ids,\n            position_ids=flat_position_ids,\n            token_type_ids=flat_token_type_ids,\n            attention_mask=flat_attention_mask,\n            head_mask=head_mask,\n            inputs_embeds=flat_inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n\n        if not return_dict:\n            output = (reshaped_logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MultipleChoiceModelOutput(\n            loss=loss,\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Roberta Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n    Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    ROBERTA_START_DOCSTRING,\n)\nclass RobertaForTokenClassification(RobertaPreTrainedModel):\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"Jean-Baptiste/roberta-large-ner-english\",\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=\"['O', 'ORG', 'ORG', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'LOC']\",\n        expected_loss=0.01,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass RobertaForTokenClassification(RobertaPreTrainedModel):\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"Jean-Baptiste/roberta-large-ner-english\",\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=\"['O', 'ORG', 'ORG', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'LOC']\",\n        expected_loss=0.01,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\nclass RobertaClassificationHead(nn.Module):\n    \"\"\"Head for sentence-level classification tasks.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, features, **kwargs):\n        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x", "\n\n@add_start_docstrings(\n    \"\"\"\n    Roberta Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    ROBERTA_START_DOCSTRING,\n)\nclass RobertaForQuestionAnswering(RobertaPreTrainedModel):\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"deepset/roberta-base-squad2\",\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=\"' puppet'\",\n        expected_loss=0.86,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass RobertaForQuestionAnswering(RobertaPreTrainedModel):\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"deepset/roberta-base-squad2\",\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=\"' puppet'\",\n        expected_loss=0.86,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\ndef create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n    \"\"\"\n    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n    are ignored. This is modified from fairseq's `utils.make_positions`.\n\n    Args:\n        x: torch.Tensor x:\n\n    Returns: torch.Tensor\n    \"\"\"\n    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n    mask = input_ids.ne(padding_idx).int()\n    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n    return incremental_indices.long() + padding_idx", ""]}
{"filename": "prompt4ner/modeling_albert.py", "chunked_list": ["# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch ALBERT model.\"\"\"\n\nimport math\nimport os", "import math\nimport os\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple, Union\n\nimport torch\nfrom packaging import version\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n", "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import (\n    BaseModelOutput,\n    BaseModelOutputWithPooling,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,", "    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\nfrom transformers.utils import (\n    ModelOutput,\n    add_code_sample_docstrings,\n    add_start_docstrings,", "    add_code_sample_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers import AlbertConfig\n\n\nlogger = logging.get_logger(__name__)", "\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"albert-base-v2\"\n_CONFIG_FOR_DOC = \"AlbertConfig\"\n_TOKENIZER_FOR_DOC = \"AlbertTokenizer\"\n\n\nALBERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"albert-base-v1\",", "ALBERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"albert-base-v1\",\n    \"albert-large-v1\",\n    \"albert-xlarge-v1\",\n    \"albert-xxlarge-v1\",\n    \"albert-base-v2\",\n    \"albert-large-v2\",\n    \"albert-xlarge-v2\",\n    \"albert-xxlarge-v2\",\n    # See all ALBERT models at https://huggingface.co/models?filter=albert", "    \"albert-xxlarge-v2\",\n    # See all ALBERT models at https://huggingface.co/models?filter=albert\n]\n\n\ndef load_tf_weights_in_albert(model, config, tf_checkpoint_path):\n    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n    try:\n        import re\n\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\"\n        )\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        print(name)\n\n    for name, array in zip(names, arrays):\n        original_name = name\n\n        # If saved from the TF HUB module\n        name = name.replace(\"module/\", \"\")\n\n        # Renaming and simplifying\n        name = name.replace(\"ffn_1\", \"ffn\")\n        name = name.replace(\"bert/\", \"albert/\")\n        name = name.replace(\"attention_1\", \"attention\")\n        name = name.replace(\"transform/\", \"\")\n        name = name.replace(\"LayerNorm_1\", \"full_layer_layer_norm\")\n        name = name.replace(\"LayerNorm\", \"attention/LayerNorm\")\n        name = name.replace(\"transformer/\", \"\")\n\n        # The feed forward layer had an 'intermediate' step which has been abstracted away\n        name = name.replace(\"intermediate/dense/\", \"\")\n        name = name.replace(\"ffn/intermediate/output/dense/\", \"ffn_output/\")\n\n        # ALBERT attention was split between self and output which have been abstracted away\n        name = name.replace(\"/output/\", \"/\")\n        name = name.replace(\"/self/\", \"/\")\n\n        # The pooler is a linear layer\n        name = name.replace(\"pooler/dense\", \"pooler\")\n\n        # The classifier was simplified to predictions from cls/predictions\n        name = name.replace(\"cls/predictions\", \"predictions\")\n        name = name.replace(\"predictions/attention\", \"predictions\")\n\n        # Naming was changed to be more explicit\n        name = name.replace(\"embeddings/attention\", \"embeddings\")\n        name = name.replace(\"inner_group_\", \"albert_layers/\")\n        name = name.replace(\"group_\", \"albert_layer_groups/\")\n\n        # Classifier\n        if len(name.split(\"/\")) == 1 and (\"output_bias\" in name or \"output_weights\" in name):\n            name = \"classifier/\" + name\n\n        # No ALBERT model currently handles the next sentence prediction task\n        if \"seq_relationship\" in name:\n            name = name.replace(\"seq_relationship/output_\", \"sop_classifier/classifier/\")\n            name = name.replace(\"weights\", \"weight\")\n\n        name = name.split(\"/\")\n\n        # Ignore the gradients applied by the LAMB/ADAM optimizers.\n        if (\n            \"adam_m\" in name\n            or \"adam_v\" in name\n            or \"AdamWeightDecayOptimizer\" in name\n            or \"AdamWeightDecayOptimizer_1\" in name\n            or \"global_step\" in name\n        ):\n            logger.info(f\"Skipping {'/'.join(name)}\")\n            continue\n\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n                scope_names = re.split(r\"_(\\d+)\", m_name)\n            else:\n                scope_names = [m_name]\n\n            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n                pointer = getattr(pointer, \"bias\")\n            elif scope_names[0] == \"output_weights\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"squad\":\n                pointer = getattr(pointer, \"classifier\")\n            else:\n                try:\n                    pointer = getattr(pointer, scope_names[0])\n                except AttributeError:\n                    logger.info(f\"Skipping {'/'.join(name)}\")\n                    continue\n            if len(scope_names) >= 2:\n                num = int(scope_names[1])\n                pointer = pointer[num]\n\n        if m_name[-11:] == \"_embeddings\":\n            pointer = getattr(pointer, \"weight\")\n        elif m_name == \"kernel\":\n            array = np.transpose(array)\n        try:\n            if pointer.shape != array.shape:\n                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(f\"Initialize PyTorch weight {name} from {original_name}\")\n        pointer.data = torch.from_numpy(array)\n\n    return model", "\n\nclass AlbertEmbeddings(nn.Module):\n    \"\"\"\n    Construct the embeddings from word, position and token_type embeddings.\n    \"\"\"\n\n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.embedding_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.embedding_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n            self.register_buffer(\n                \"token_type_ids\",\n                torch.zeros(self.position_ids.size(), dtype=torch.long),\n                persistent=False,\n            )\n\n    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.forward\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        past_key_values_length: int = 0,\n    ) -> torch.Tensor:\n        if input_ids is not None:\n            input_shape = input_ids.size()\n        else:\n            input_shape = inputs_embeds.size()[:-1]\n\n        seq_length = input_shape[1]\n\n        if position_ids is None:\n            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n\n        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n        # issue #5664\n        if token_type_ids is None:\n            if hasattr(self, \"token_type_ids\"):\n                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n\n        if inputs_embeds is None:\n            inputs_embeds = self.word_embeddings(input_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = inputs_embeds + token_type_embeddings\n        if self.position_embedding_type == \"absolute\":\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings += position_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings", "\n\nclass AlbertAttention(nn.Module):\n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads}\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.hidden_size = config.hidden_size\n        self.attention_head_size = config.hidden_size // config.num_attention_heads\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.attention_dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.output_dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.pruned_heads = set()\n\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            self.max_position_embeddings = config.max_position_embeddings\n            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n\n    # Copied from transformers.models.bert.modeling_bert.BertSelfAttention.transpose_for_scores\n    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def prune_heads(self, heads: List[int]) -> None:\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.num_attention_heads, self.attention_head_size, self.pruned_heads\n        )\n\n        # Prune linear layers\n        self.query = prune_linear_layer(self.query, index)\n        self.key = prune_linear_layer(self.key, index)\n        self.value = prune_linear_layer(self.value, index)\n        self.dense = prune_linear_layer(self.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.num_attention_heads = self.num_attention_heads - len(heads)\n        self.all_head_size = self.attention_head_size * self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: bool = False,\n    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            seq_length = hidden_states.size()[1]\n            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n            distance = position_ids_l - position_ids_r\n            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n\n            if self.position_embedding_type == \"relative_key\":\n                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.attention_dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.transpose(2, 1).flatten(2)\n\n        projected_context_layer = self.dense(context_layer)\n        projected_context_layer_dropout = self.output_dropout(projected_context_layer)\n        layernormed_context_layer = self.LayerNorm(hidden_states + projected_context_layer_dropout)\n        return (layernormed_context_layer, attention_probs) if output_attentions else (layernormed_context_layer,)", "\n\nclass AlbertLayer(nn.Module):\n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n\n        self.config = config\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.full_layer_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.attention = AlbertAttention(config)\n        self.ffn = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.ffn_output = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.activation = ACT2FN[config.hidden_act]\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        attention_output = self.attention(hidden_states, attention_mask, head_mask, output_attentions)\n\n        ffn_output = apply_chunking_to_forward(\n            self.ff_chunk,\n            self.chunk_size_feed_forward,\n            self.seq_len_dim,\n            attention_output[0],\n        )\n        hidden_states = self.full_layer_layer_norm(ffn_output + attention_output[0])\n\n        return (hidden_states,) + attention_output[1:]  # add attentions if we output them\n\n    def ff_chunk(self, attention_output: torch.Tensor) -> torch.Tensor:\n        ffn_output = self.ffn(attention_output)\n        ffn_output = self.activation(ffn_output)\n        ffn_output = self.ffn_output(ffn_output)\n        return ffn_output", "\n\nclass AlbertLayerGroup(nn.Module):\n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n\n        self.albert_layers = nn.ModuleList([AlbertLayer(config) for _ in range(config.inner_group_num)])\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n        layer_hidden_states = ()\n        layer_attentions = ()\n\n        for layer_index, albert_layer in enumerate(self.albert_layers):\n            layer_output = albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)\n            hidden_states = layer_output[0]\n\n            if output_attentions:\n                layer_attentions = layer_attentions + (layer_output[1],)\n\n            if output_hidden_states:\n                layer_hidden_states = layer_hidden_states + (hidden_states,)\n\n        outputs = (hidden_states,)\n        if output_hidden_states:\n            outputs = outputs + (layer_hidden_states,)\n        if output_attentions:\n            outputs = outputs + (layer_attentions,)\n        return outputs  # last-layer hidden state, (layer hidden states), (layer attentions)", "\n\nclass AlbertTransformer(nn.Module):\n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n\n        self.config = config\n        self.embedding_hidden_mapping_in = nn.Linear(config.embedding_size, config.hidden_size)\n        self.albert_layer_groups = nn.ModuleList([AlbertLayerGroup(config) for _ in range(config.num_hidden_groups)])\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ) -> Union[BaseModelOutput, Tuple]:\n        hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n\n        all_hidden_states = (hidden_states,) if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n\n        head_mask = [None] * self.config.num_hidden_layers if head_mask is None else head_mask\n\n        for i in range(self.config.num_hidden_layers):\n            # Number of layers in a hidden group\n            layers_per_group = int(self.config.num_hidden_layers / self.config.num_hidden_groups)\n\n            # Index of the hidden group\n            group_idx = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))\n\n            layer_group_output = self.albert_layer_groups[group_idx](\n                hidden_states,\n                attention_mask,\n                head_mask[group_idx * layers_per_group : (group_idx + 1) * layers_per_group],\n                output_attentions,\n                output_hidden_states,\n            )\n            hidden_states = layer_group_output[0]\n\n            if output_attentions:\n                all_attentions = all_attentions + layer_group_output[-1]\n\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n        return BaseModelOutput(\n            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n        )", "\n\nclass AlbertPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = AlbertConfig\n    load_tf_weights = load_tf_weights_in_albert\n    base_model_prefix = \"albert\"\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights.\"\"\"\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)", "\n\n@dataclass\nclass AlbertForPreTrainingOutput(ModelOutput):\n    \"\"\"\n    Output type of [`AlbertForPreTraining`].\n\n    Args:\n        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n            (classification) loss.\n        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        sop_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n            before SoftMax).\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n            shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n\n    loss: Optional[torch.FloatTensor] = None\n    prediction_logits: torch.FloatTensor = None\n    sop_logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None", "\n\nALBERT_START_DOCSTRING = r\"\"\"\n\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage", "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Args:\n        config ([`AlbertConfig`]): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n", "\"\"\"\n\nALBERT_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `({0})`):\n            Indices of input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`AlbertTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n            [`PreTrainedTokenizer.encode`] for details.\n", "            [`PreTrainedTokenizer.encode`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)", "\n            [What are attention masks?](../glossary#attention-mask)\n        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n            1]`:\n\n            - 0 corresponds to a *sentence A* token,\n            - 1 corresponds to a *sentence B* token.\n\n            [What are token type IDs?](../glossary#token-type-ids)", "\n            [What are token type IDs?](../glossary#token-type-ids)\n        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.max_position_embeddings - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n", "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        output_attentions (`bool`, *optional*):", "            model's internal embedding lookup matrix.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"", "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare ALBERT Model transformer outputting raw hidden-states without any specific head on top.\",\n    ALBERT_START_DOCSTRING,\n)\nclass AlbertModel(AlbertPreTrainedModel):\n\n    config_class = AlbertConfig\n    base_model_prefix = \"albert\"\n\n    def __init__(self, config: AlbertConfig, add_pooling_layer: bool = True):\n        super().__init__(config)\n\n        self.config = config\n        self.embeddings = AlbertEmbeddings(config)\n        self.encoder = AlbertTransformer(config)\n        if add_pooling_layer:\n            self.pooler = nn.Linear(config.hidden_size, config.hidden_size)\n            self.pooler_activation = nn.Tanh()\n        else:\n            self.pooler = None\n            self.pooler_activation = None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self) -> nn.Embedding:\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value: nn.Embedding) -> None:\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune: Dict[int, List[int]]) -> None:\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} ALBERT has\n        a different architecture in that its layers are shared across groups, which then has inner groups. If an ALBERT\n        model has 12 hidden layers and 2 hidden groups, with two inner groups, there is a total of 4 different layers.\n\n        These layers are flattened: the indices [0,1] correspond to the two inner groups of the first hidden layer,\n        while [2,3] correspond to the two inner groups of the second hidden layer.\n\n        Any layer with in index other than [0,1,2,3] will result in an error. See base class PreTrainedModel for more\n        information about head pruning\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            group_idx = int(layer / self.config.inner_group_num)\n            inner_group_idx = int(layer - group_idx * self.config.inner_group_num)\n            self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPooling,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[None] = None,\n        output_hidden_states: Optional[None] = None,\n        return_dict: Optional[None] = None,\n    ) -> Union[BaseModelOutputWithPooling, Tuple]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        if attention_mask is None:\n            attention_mask = torch.ones(input_shape, device=device)\n        if token_type_ids is None:\n            if hasattr(self.embeddings, \"token_type_ids\"):\n                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            extended_attention_mask,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = encoder_outputs[0]\n\n        pooled_output = self.pooler_activation(self.pooler(sequence_output[:, 0])) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPooling(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )", "class AlbertModel(AlbertPreTrainedModel):\n\n    config_class = AlbertConfig\n    base_model_prefix = \"albert\"\n\n    def __init__(self, config: AlbertConfig, add_pooling_layer: bool = True):\n        super().__init__(config)\n\n        self.config = config\n        self.embeddings = AlbertEmbeddings(config)\n        self.encoder = AlbertTransformer(config)\n        if add_pooling_layer:\n            self.pooler = nn.Linear(config.hidden_size, config.hidden_size)\n            self.pooler_activation = nn.Tanh()\n        else:\n            self.pooler = None\n            self.pooler_activation = None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self) -> nn.Embedding:\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value: nn.Embedding) -> None:\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune: Dict[int, List[int]]) -> None:\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} ALBERT has\n        a different architecture in that its layers are shared across groups, which then has inner groups. If an ALBERT\n        model has 12 hidden layers and 2 hidden groups, with two inner groups, there is a total of 4 different layers.\n\n        These layers are flattened: the indices [0,1] correspond to the two inner groups of the first hidden layer,\n        while [2,3] correspond to the two inner groups of the second hidden layer.\n\n        Any layer with in index other than [0,1,2,3] will result in an error. See base class PreTrainedModel for more\n        information about head pruning\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            group_idx = int(layer / self.config.inner_group_num)\n            inner_group_idx = int(layer - group_idx * self.config.inner_group_num)\n            self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=BaseModelOutputWithPooling,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[None] = None,\n        output_hidden_states: Optional[None] = None,\n        return_dict: Optional[None] = None,\n    ) -> Union[BaseModelOutputWithPooling, Tuple]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        if attention_mask is None:\n            attention_mask = torch.ones(input_shape, device=device)\n        if token_type_ids is None:\n            if hasattr(self.embeddings, \"token_type_ids\"):\n                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            extended_attention_mask,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = encoder_outputs[0]\n\n        pooled_output = self.pooler_activation(self.pooler(sequence_output[:, 0])) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPooling(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Albert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a\n    `sentence order prediction (classification)` head.\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass AlbertForPreTraining(AlbertPreTrainedModel):\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n\n        self.albert = AlbertModel(config)\n        self.predictions = AlbertMLMHead(config)\n        self.sop_classifier = AlbertSOPHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_output_embeddings(self) -> nn.Linear:\n        return self.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings: nn.Linear) -> None:\n        self.predictions.decoder = new_embeddings\n\n    def get_input_embeddings(self) -> nn.Embedding:\n        return self.albert.embeddings.word_embeddings\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=AlbertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        sentence_order_label: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        sentence_order_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see `input_ids` docstring) Indices should be in `[0, 1]`. `0` indicates original order (sequence A, then\n            sequence B), `1` indicates switched order (sequence B, then sequence A).\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AlbertTokenizer, AlbertForPreTraining\n        >>> import torch\n\n        >>> tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n        >>> model = AlbertForPreTraining.from_pretrained(\"albert-base-v2\")\n\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)\n        >>> # Batch size 1\n        >>> outputs = model(input_ids)\n\n        >>> prediction_logits = outputs.prediction_logits\n        >>> sop_logits = outputs.sop_logits\n        ```\"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.albert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output, pooled_output = outputs[:2]\n\n        prediction_scores = self.predictions(sequence_output)\n        sop_scores = self.sop_classifier(pooled_output)\n\n        total_loss = None\n        if labels is not None and sentence_order_label is not None:\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            sentence_order_loss = loss_fct(sop_scores.view(-1, 2), sentence_order_label.view(-1))\n            total_loss = masked_lm_loss + sentence_order_loss\n\n        if not return_dict:\n            output = (prediction_scores, sop_scores) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return AlbertForPreTrainingOutput(\n            loss=total_loss,\n            prediction_logits=prediction_scores,\n            sop_logits=sop_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass AlbertForPreTraining(AlbertPreTrainedModel):\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n\n        self.albert = AlbertModel(config)\n        self.predictions = AlbertMLMHead(config)\n        self.sop_classifier = AlbertSOPHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_output_embeddings(self) -> nn.Linear:\n        return self.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings: nn.Linear) -> None:\n        self.predictions.decoder = new_embeddings\n\n    def get_input_embeddings(self) -> nn.Embedding:\n        return self.albert.embeddings.word_embeddings\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=AlbertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        sentence_order_label: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        sentence_order_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n            (see `input_ids` docstring) Indices should be in `[0, 1]`. `0` indicates original order (sequence A, then\n            sequence B), `1` indicates switched order (sequence B, then sequence A).\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AlbertTokenizer, AlbertForPreTraining\n        >>> import torch\n\n        >>> tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n        >>> model = AlbertForPreTraining.from_pretrained(\"albert-base-v2\")\n\n        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)\n        >>> # Batch size 1\n        >>> outputs = model(input_ids)\n\n        >>> prediction_logits = outputs.prediction_logits\n        >>> sop_logits = outputs.sop_logits\n        ```\"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.albert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output, pooled_output = outputs[:2]\n\n        prediction_scores = self.predictions(sequence_output)\n        sop_scores = self.sop_classifier(pooled_output)\n\n        total_loss = None\n        if labels is not None and sentence_order_label is not None:\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n            sentence_order_loss = loss_fct(sop_scores.view(-1, 2), sentence_order_label.view(-1))\n            total_loss = masked_lm_loss + sentence_order_loss\n\n        if not return_dict:\n            output = (prediction_scores, sop_scores) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return AlbertForPreTrainingOutput(\n            loss=total_loss,\n            prediction_logits=prediction_scores,\n            sop_logits=sop_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\nclass AlbertMLMHead(nn.Module):\n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n\n        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n        self.dense = nn.Linear(config.hidden_size, config.embedding_size)\n        self.decoder = nn.Linear(config.embedding_size, config.vocab_size)\n        self.activation = ACT2FN[config.hidden_act]\n        self.decoder.bias = self.bias\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.activation(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        hidden_states = self.decoder(hidden_states)\n\n        prediction_scores = hidden_states\n\n        return prediction_scores\n\n    def _tie_weights(self) -> None:\n        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n        self.bias = self.decoder.bias", "\n\nclass AlbertSOPHead(nn.Module):\n    def __init__(self, config: AlbertConfig):\n        super().__init__()\n\n        self.dropout = nn.Dropout(config.classifier_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n        dropout_pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(dropout_pooled_output)\n        return logits", "\n\n@add_start_docstrings(\n    \"Albert Model with a `language modeling` head on top.\",\n    ALBERT_START_DOCSTRING,\n)\nclass AlbertForMaskedLM(AlbertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.albert = AlbertModel(config, add_pooling_layer=False)\n        self.predictions = AlbertMLMHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_output_embeddings(self) -> nn.Linear:\n        return self.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings: nn.Linear) -> None:\n        self.predictions.decoder = new_embeddings\n\n    def get_input_embeddings(self) -> nn.Embedding:\n        return self.albert.embeddings.word_embeddings\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @replace_return_docstrings(output_type=MaskedLMOutput, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[MaskedLMOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> import torch\n        >>> from transformers import AlbertTokenizer, AlbertForMaskedLM\n\n        >>> tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n        >>> model = AlbertForMaskedLM.from_pretrained(\"albert-base-v2\")\n\n        >>> # add mask_token\n        >>> inputs = tokenizer(\"The capital of [MASK] is Paris.\", return_tensors=\"pt\")\n        >>> with torch.no_grad():\n        ...     logits = model(**inputs).logits\n\n        >>> # retrieve index of [MASK]\n        >>> mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n        >>> predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n        >>> tokenizer.decode(predicted_token_id)\n        'france'\n        ```\n\n        ```python\n        >>> labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n        >>> labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n        >>> outputs = model(**inputs, labels=labels)\n        >>> round(outputs.loss.item(), 2)\n        0.81\n        ```\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.albert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_outputs = outputs[0]\n\n        prediction_scores = self.predictions(sequence_outputs)\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return MaskedLMOutput(\n            loss=masked_lm_loss,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Albert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n    output) e.g. for GLUE tasks.\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass AlbertForSequenceClassification(AlbertPreTrainedModel):\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n\n        self.albert = AlbertModel(config)\n        self.dropout = nn.Dropout(config.classifier_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"textattack/albert-base-v2-imdb\",\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=\"'LABEL_1'\",\n        expected_loss=0.12,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[SequenceClassifierOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.albert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass AlbertForSequenceClassification(AlbertPreTrainedModel):\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n\n        self.albert = AlbertModel(config)\n        self.dropout = nn.Dropout(config.classifier_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"textattack/albert-base-v2-imdb\",\n        output_type=SequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=\"'LABEL_1'\",\n        expected_loss=0.12,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[SequenceClassifierOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.albert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Albert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n    Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass AlbertForTokenClassification(AlbertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.albert = AlbertModel(config, add_pooling_layer=False)\n        classifier_dropout_prob = (\n            config.classifier_dropout_prob\n            if config.classifier_dropout_prob is not None\n            else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"vumichien/tiny-albert\",\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=(\n            \"['LABEL_1', 'LABEL_1', 'LABEL_1', 'LABEL_0', 'LABEL_1', 'LABEL_0', 'LABEL_1', 'LABEL_1', \"\n            \"'LABEL_0', 'LABEL_1', 'LABEL_0', 'LABEL_0', 'LABEL_1', 'LABEL_1']\"\n        ),\n        expected_loss=0.66,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[TokenClassifierOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.albert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass AlbertForTokenClassification(AlbertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.albert = AlbertModel(config, add_pooling_layer=False)\n        classifier_dropout_prob = (\n            config.classifier_dropout_prob\n            if config.classifier_dropout_prob is not None\n            else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"vumichien/tiny-albert\",\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=(\n            \"['LABEL_1', 'LABEL_1', 'LABEL_1', 'LABEL_0', 'LABEL_1', 'LABEL_0', 'LABEL_1', 'LABEL_1', \"\n            \"'LABEL_0', 'LABEL_1', 'LABEL_0', 'LABEL_0', 'LABEL_1', 'LABEL_1']\"\n        ),\n        expected_loss=0.66,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[TokenClassifierOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.albert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Albert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass AlbertForQuestionAnswering(AlbertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.albert = AlbertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"twmkn9/albert-base-v2-squad2\",\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        qa_target_start_index=12,\n        qa_target_end_index=13,\n        expected_output=\"'a nice puppet'\",\n        expected_loss=7.36,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.albert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits: torch.Tensor = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass AlbertForQuestionAnswering(AlbertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.albert = AlbertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=\"twmkn9/albert-base-v2-squad2\",\n        output_type=QuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        qa_target_start_index=12,\n        qa_target_end_index=13,\n        expected_output=\"'a nice puppet'\",\n        expected_loss=7.36,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.albert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits: torch.Tensor = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", "\n\n@add_start_docstrings(\n    \"\"\"\n    Albert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n    softmax) e.g. for RocStories/SWAG tasks.\n    \"\"\",\n    ALBERT_START_DOCSTRING,\n)\nclass AlbertForMultipleChoice(AlbertPreTrainedModel):\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n\n        self.albert = AlbertModel(config)\n        self.dropout = nn.Dropout(config.classifier_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n            num_choices-1]` where *num_choices* is the size of the second dimension of the input tensors. (see\n            *input_ids* above)\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n        inputs_embeds = (\n            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n            if inputs_embeds is not None\n            else None\n        )\n        outputs = self.albert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits: torch.Tensor = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n\n        if not return_dict:\n            output = (reshaped_logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MultipleChoiceModelOutput(\n            loss=loss,\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\nclass AlbertForMultipleChoice(AlbertPreTrainedModel):\n    def __init__(self, config: AlbertConfig):\n        super().__init__(config)\n\n        self.albert = AlbertModel(config)\n        self.dropout = nn.Dropout(config.classifier_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=MultipleChoiceModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n            num_choices-1]` where *num_choices* is the size of the second dimension of the input tensors. (see\n            *input_ids* above)\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n        inputs_embeds = (\n            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n            if inputs_embeds is not None\n            else None\n        )\n        outputs = self.albert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits: torch.Tensor = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n\n        if not return_dict:\n            output = (reshaped_logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MultipleChoiceModelOutput(\n            loss=loss,\n            logits=reshaped_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ""]}
{"filename": "prompt4ner/modeling_xlm_roberta.py", "chunked_list": ["# coding=utf-8\n# Copyright 2019 Facebook AI Research and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch XLM-RoBERTa model.\"\"\"\n\nfrom transformers.utils import add_start_docstrings, logging", "\nfrom transformers.utils import add_start_docstrings, logging\nfrom .modeling_roberta import (\n    RobertaForCausalLM,\n    RobertaForMaskedLM,\n    RobertaForMultipleChoice,\n    RobertaForQuestionAnswering,\n    RobertaForSequenceClassification,\n    RobertaForTokenClassification,\n    RobertaModel,", "    RobertaForTokenClassification,\n    RobertaModel,\n)\nfrom transformers import XLMRobertaConfig\n\n\nlogger = logging.get_logger(__name__)\n\nXLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"xlm-roberta-base\",", "XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"xlm-roberta-base\",\n    \"xlm-roberta-large\",\n    \"xlm-roberta-large-finetuned-conll02-dutch\",\n    \"xlm-roberta-large-finetuned-conll02-spanish\",\n    \"xlm-roberta-large-finetuned-conll03-english\",\n    \"xlm-roberta-large-finetuned-conll03-german\",\n    # See all XLM-RoBERTa models at https://huggingface.co/models?filter=xlm-roberta\n]\n", "]\n\n\nXLM_ROBERTA_START_DOCSTRING = r\"\"\"\n\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.", "\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`XLMRobertaConfig`]): Model configuration class with all the parameters of the\n            model. Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"", "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare XLM-RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\n    XLM_ROBERTA_START_DOCSTRING,\n)\nclass XLMRobertaModel(RobertaModel):\n    \"\"\"\n    This class overrides [`RobertaModel`]. Please check the superclass for the appropriate documentation alongside\n    usage examples.\n    \"\"\"\n\n    config_class = XLMRobertaConfig", "class XLMRobertaModel(RobertaModel):\n    \"\"\"\n    This class overrides [`RobertaModel`]. Please check the superclass for the appropriate documentation alongside\n    usage examples.\n    \"\"\"\n\n    config_class = XLMRobertaConfig\n\n\n@add_start_docstrings(", "\n@add_start_docstrings(\n    \"XLM-RoBERTa Model with a `language modeling` head on top for CLM fine-tuning.\",\n    XLM_ROBERTA_START_DOCSTRING,\n)\nclass XLMRobertaForCausalLM(RobertaForCausalLM):\n    \"\"\"\n    This class overrides [`RobertaForCausalLM`]. Please check the superclass for the appropriate documentation\n    alongside usage examples.\n    \"\"\"\n\n    config_class = XLMRobertaConfig", "\n\n@add_start_docstrings(\n    \"\"\"XLM-RoBERTa Model with a `language modeling` head on top.\"\"\",\n    XLM_ROBERTA_START_DOCSTRING,\n)\nclass XLMRobertaForMaskedLM(RobertaForMaskedLM):\n    \"\"\"\n    This class overrides [`RobertaForMaskedLM`]. Please check the superclass for the appropriate documentation\n    alongside usage examples.\n    \"\"\"\n\n    config_class = XLMRobertaConfig", "\n\n@add_start_docstrings(\n    \"\"\"\n    XLM-RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n    pooled output) e.g. for GLUE tasks.\n    \"\"\",\n    XLM_ROBERTA_START_DOCSTRING,\n)\nclass XLMRobertaForSequenceClassification(RobertaForSequenceClassification):\n    \"\"\"\n    This class overrides [`RobertaForSequenceClassification`]. Please check the superclass for the appropriate\n    documentation alongside usage examples.\n    \"\"\"\n\n    config_class = XLMRobertaConfig", ")\nclass XLMRobertaForSequenceClassification(RobertaForSequenceClassification):\n    \"\"\"\n    This class overrides [`RobertaForSequenceClassification`]. Please check the superclass for the appropriate\n    documentation alongside usage examples.\n    \"\"\"\n\n    config_class = XLMRobertaConfig\n\n", "\n\n@add_start_docstrings(\n    \"\"\"\n    XLM-RoBERTa Model with a multiple choice classification head on top (a linear layer on top of the pooled output and\n    a softmax) e.g. for RocStories/SWAG tasks.\n    \"\"\",\n    XLM_ROBERTA_START_DOCSTRING,\n)\nclass XLMRobertaForMultipleChoice(RobertaForMultipleChoice):\n    \"\"\"\n    This class overrides [`RobertaForMultipleChoice`]. Please check the superclass for the appropriate documentation\n    alongside usage examples.\n    \"\"\"\n\n    config_class = XLMRobertaConfig", ")\nclass XLMRobertaForMultipleChoice(RobertaForMultipleChoice):\n    \"\"\"\n    This class overrides [`RobertaForMultipleChoice`]. Please check the superclass for the appropriate documentation\n    alongside usage examples.\n    \"\"\"\n\n    config_class = XLMRobertaConfig\n\n", "\n\n@add_start_docstrings(\n    \"\"\"\n    XLM-RoBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.\n    for Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    XLM_ROBERTA_START_DOCSTRING,\n)\nclass XLMRobertaForTokenClassification(RobertaForTokenClassification):\n    \"\"\"\n    This class overrides [`RobertaForTokenClassification`]. Please check the superclass for the appropriate\n    documentation alongside usage examples.\n    \"\"\"\n\n    config_class = XLMRobertaConfig", ")\nclass XLMRobertaForTokenClassification(RobertaForTokenClassification):\n    \"\"\"\n    This class overrides [`RobertaForTokenClassification`]. Please check the superclass for the appropriate\n    documentation alongside usage examples.\n    \"\"\"\n\n    config_class = XLMRobertaConfig\n\n", "\n\n@add_start_docstrings(\n    \"\"\"\n    XLM-RoBERTa Model with a span classification head on top for extractive question-answering tasks like SQuAD (a\n    linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    XLM_ROBERTA_START_DOCSTRING,\n)\nclass XLMRobertaForQuestionAnswering(RobertaForQuestionAnswering):\n    \"\"\"\n    This class overrides [`RobertaForQuestionAnswering`]. Please check the superclass for the appropriate documentation\n    alongside usage examples.\n    \"\"\"\n\n    config_class = XLMRobertaConfig", ")\nclass XLMRobertaForQuestionAnswering(RobertaForQuestionAnswering):\n    \"\"\"\n    This class overrides [`RobertaForQuestionAnswering`]. Please check the superclass for the appropriate documentation\n    alongside usage examples.\n    \"\"\"\n\n    config_class = XLMRobertaConfig\n", ""]}
{"filename": "prompt4ner/matcher.py", "chunked_list": ["import torch\nfrom scipy.optimize import linear_sum_assignment\n# from lapsolver import solve_dense\nfrom .lap import auction_lap\nfrom torch import nn\n\nclass HungarianMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n    while the others are un-matched (and thus treated as non-objects).\n    \"\"\"\n\n    def __init__(self, cost_class: float = 1, cost_span: float = 1, solver = \"hungarian\"):\n        \"\"\"Creates the matcher\n        Params:\n            cost_class: This is the relative weight of the classification error in the matching cost\n            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n        \"\"\"\n        super().__init__()\n        self.cost_class = cost_class\n        self.cost_span = cost_span\n        self.solver = solver\n\n    @torch.no_grad()\n    def forward(self, outputs, targets):\n        \"\"\" Performs the matching\n        Params:\n            outputs: This is a dict that contains at least these entries:\n                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n                           objects in the target) containing the class labels\n                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n        Returns:\n            A list of size batch_size, containing tuples of (index_i, index_j) where:\n                - index_i is the indices of the selected predictions (in order)\n                - index_j is the indices of the corresponding selected targets (in order)\n            For each batch element, it holds:\n                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n        \"\"\"\n\n        if self.solver == \"order\":\n            sizes = targets[\"sizes\"]\n            indices = [(list(range(size)),list(range(size))) for size in sizes]\n        else:\n            bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n\n            # We flatten to compute the cost matrices in a batch\n            out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(dim=-1) # [batch_size * num_queries, 8]\n\n            entity_left = outputs[\"pred_left\"].flatten(0, 1)\n            entity_right = outputs[\"pred_right\"].flatten(0, 1) # [batch_size * num_queries]\n\n\n            gt_ids = targets[\"labels\"]\n            gt_left = targets[\"gt_left\"]\n            gt_right = targets[\"gt_right\"]\n            \n            # import pdb;pdb.set_trace()\n            cost_class = -out_prob[:, gt_ids]\n            cost_span = -(entity_left[:, gt_left] + entity_right[:, gt_right])\n\n            # Final cost matrix\n            C = self.cost_span * cost_span + self.cost_class * cost_class\n\n            C = C.view(bs, num_queries, -1)\n\n            sizes = targets[\"sizes\"]\n            indices = None\n            \n            if self.solver == \"hungarian\":\n                C = C.cpu()\n                indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n            if self.solver == \"auction\":\n                indices = [auction_lap(c[i])[:2] for i, c in enumerate(C.split(sizes, -1))]\n\n        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"]}
{"filename": "prompt4ner/input_reader.py", "chunked_list": ["import json\nfrom abc import abstractmethod, ABC\nfrom collections import OrderedDict\nfrom logging import Logger\nimport os\nfrom typing import List\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, CLIPProcessor\n", "from transformers import AutoTokenizer, CLIPProcessor\n\nfrom prompt4ner import util\nfrom prompt4ner.entities import Dataset, EntityType, RelationType, Entity, Relation, Document, DistributedIterableDataset\nfrom prompt4ner.prompt_tokens import build_prompt_tokens\nimport copy\n\n\nclass BaseInputReader(ABC):\n    def __init__(self, types_path: str, tokenizer: AutoTokenizer, processor: CLIPProcessor = None, logger: Logger = None, random_mask_word = None, repeat_gt_entities = None):\n        types = json.load(open(types_path), object_pairs_hook=OrderedDict) \n\n        self._entity_types = OrderedDict()\n        self._idx2entity_type = OrderedDict()\n        self._relation_types = OrderedDict()\n        self._idx2relation_type = OrderedDict()\n\n        # entities\n        # add 'None' entity type\n        none_entity_type = EntityType('None', 0, 'None', 'No Entity')\n        self._entity_types['None'] = none_entity_type\n        self._idx2entity_type[0] = none_entity_type\n\n        # specified entity types\n        for i, (key, v) in enumerate(types['entities'].items()):\n            entity_type = EntityType(key, i+1, v['short'], v['verbose'])\n            self._entity_types[key] = entity_type\n            self._idx2entity_type[i+1] = entity_type\n\n        # relations\n        none_relation_type = RelationType('None', 0, 'None', 'No Relation')\n        self._relation_types['None'] = none_relation_type\n        self._idx2relation_type[0] = none_relation_type\n\n        for i, (key, v) in enumerate(types['relations'].items()):\n            relation_type = RelationType(key, i + 1, v['short'], v['verbose'], v['symmetric'])\n            self._relation_types[key] = relation_type\n            self._idx2relation_type[i + 1] = relation_type\n            \n        self._datasets = dict()\n\n        self._tokenizer = tokenizer\n        self._processor = processor\n        self._logger = logger\n        self._random_mask_word = random_mask_word\n        self._repeat_gt_entities = repeat_gt_entities\n\n        self._vocabulary_size = tokenizer.vocab_size\n        self._context_size = -1\n\n    @abstractmethod\n    def read(self, datasets):\n        pass\n\n    def get_dataset(self, label):\n        return self._datasets[label]\n\n    def get_entity_type(self, idx) -> EntityType:\n        entity = self._idx2entity_type[idx]\n        return entity\n\n    def get_relation_type(self, idx) -> RelationType:\n        relation = self._idx2relation_type[idx]\n        return relation\n\n    def _calc_context_size(self, datasets):\n        sizes = [-1]\n\n        for dataset in datasets:\n            if isinstance(dataset, Dataset):\n                for doc in dataset.documents:\n                    sizes.append(len(doc.encoding))\n\n        context_size = max(sizes)\n        return context_size\n\n    def _log(self, text):\n        if self._logger is not None:\n            self._logger.info(text)\n\n    @property\n    def datasets(self):\n        return self._datasets\n\n    @property\n    def entity_types(self):\n        return self._entity_types\n\n    @property\n    def relation_types(self):\n        return self._relation_types\n\n    @property\n    def relation_type_count(self):\n        return len(self._relation_types)\n\n    @property\n    def entity_type_count(self):\n        return len(self._entity_types)\n\n    @property\n    def vocabulary_size(self):\n        return self._vocabulary_size\n\n    @property\n    def context_size(self):\n        return self._context_size\n\n    def __str__(self):\n        string = \"\"\n        for dataset in self._datasets.values():\n            string += \"Dataset: %s\\n\" % dataset\n            string += str(dataset)\n\n        return string\n\n    def __repr__(self):\n        return self.__str__()", "class BaseInputReader(ABC):\n    def __init__(self, types_path: str, tokenizer: AutoTokenizer, processor: CLIPProcessor = None, logger: Logger = None, random_mask_word = None, repeat_gt_entities = None):\n        types = json.load(open(types_path), object_pairs_hook=OrderedDict) \n\n        self._entity_types = OrderedDict()\n        self._idx2entity_type = OrderedDict()\n        self._relation_types = OrderedDict()\n        self._idx2relation_type = OrderedDict()\n\n        # entities\n        # add 'None' entity type\n        none_entity_type = EntityType('None', 0, 'None', 'No Entity')\n        self._entity_types['None'] = none_entity_type\n        self._idx2entity_type[0] = none_entity_type\n\n        # specified entity types\n        for i, (key, v) in enumerate(types['entities'].items()):\n            entity_type = EntityType(key, i+1, v['short'], v['verbose'])\n            self._entity_types[key] = entity_type\n            self._idx2entity_type[i+1] = entity_type\n\n        # relations\n        none_relation_type = RelationType('None', 0, 'None', 'No Relation')\n        self._relation_types['None'] = none_relation_type\n        self._idx2relation_type[0] = none_relation_type\n\n        for i, (key, v) in enumerate(types['relations'].items()):\n            relation_type = RelationType(key, i + 1, v['short'], v['verbose'], v['symmetric'])\n            self._relation_types[key] = relation_type\n            self._idx2relation_type[i + 1] = relation_type\n            \n        self._datasets = dict()\n\n        self._tokenizer = tokenizer\n        self._processor = processor\n        self._logger = logger\n        self._random_mask_word = random_mask_word\n        self._repeat_gt_entities = repeat_gt_entities\n\n        self._vocabulary_size = tokenizer.vocab_size\n        self._context_size = -1\n\n    @abstractmethod\n    def read(self, datasets):\n        pass\n\n    def get_dataset(self, label):\n        return self._datasets[label]\n\n    def get_entity_type(self, idx) -> EntityType:\n        entity = self._idx2entity_type[idx]\n        return entity\n\n    def get_relation_type(self, idx) -> RelationType:\n        relation = self._idx2relation_type[idx]\n        return relation\n\n    def _calc_context_size(self, datasets):\n        sizes = [-1]\n\n        for dataset in datasets:\n            if isinstance(dataset, Dataset):\n                for doc in dataset.documents:\n                    sizes.append(len(doc.encoding))\n\n        context_size = max(sizes)\n        return context_size\n\n    def _log(self, text):\n        if self._logger is not None:\n            self._logger.info(text)\n\n    @property\n    def datasets(self):\n        return self._datasets\n\n    @property\n    def entity_types(self):\n        return self._entity_types\n\n    @property\n    def relation_types(self):\n        return self._relation_types\n\n    @property\n    def relation_type_count(self):\n        return len(self._relation_types)\n\n    @property\n    def entity_type_count(self):\n        return len(self._entity_types)\n\n    @property\n    def vocabulary_size(self):\n        return self._vocabulary_size\n\n    @property\n    def context_size(self):\n        return self._context_size\n\n    def __str__(self):\n        string = \"\"\n        for dataset in self._datasets.values():\n            string += \"Dataset: %s\\n\" % dataset\n            string += str(dataset)\n\n        return string\n\n    def __repr__(self):\n        return self.__str__()", "\n\nclass JsonInputReader(BaseInputReader):\n    def __init__(self, types_path: str, tokenizer: AutoTokenizer, processor: CLIPProcessor = None, logger: Logger = None, random_mask_word = False, repeat_gt_entities = None, prompt_length = 3, prompt_type = \"soft\", prompt_number = 30):\n        super().__init__(types_path, tokenizer, processor, logger, random_mask_word, repeat_gt_entities)\n        self.prompt_length = prompt_length\n        self.prompt_type = prompt_type\n        self.prompt_number = prompt_number\n        if prompt_type == \"hard\":\n            assert prompt_length == 3\n        self.prompt_tokens = build_prompt_tokens(tokenizer)[:prompt_number*prompt_length]\n        self.prompt_token_ids = tokenizer.convert_tokens_to_ids(self.prompt_tokens)\n\n        \n    def read(self, dataset_paths):\n        for dataset_label, dataset_path in dataset_paths.items():\n            if dataset_path.endswith(\".jsonl\"):\n                dataset = DistributedIterableDataset(dataset_label, dataset_path, self._relation_types, self._entity_types, self, random_mask_word = self._random_mask_word, tokenizer = self._tokenizer, processor = self._processor, repeat_gt_entities = self._repeat_gt_entities)\n                self._datasets[dataset_label] = dataset\n            else:\n                dataset = Dataset(dataset_label, dataset_path, self._relation_types, self._entity_types, random_mask_word = self._random_mask_word, tokenizer = self._tokenizer, processor = self._processor, repeat_gt_entities = self._repeat_gt_entities)\n                self._parse_dataset(dataset_path, dataset, dataset_label)\n                self._datasets[dataset_label] = dataset\n\n        self._context_size = self._calc_context_size(self._datasets.values())\n\n    def _parse_dataset(self, dataset_path, dataset, dataset_label):\n        documents = json.load(open(dataset_path))\n        for document in tqdm(documents, desc=\"Parse dataset '%s'\" % dataset.label):\n            self._parse_document(document, dataset)\n\n    def _parse_document(self, doc, dataset: Dataset) -> Document:\n        jimages = None\n        ltokens = None\n        rtokens = None\n        jrelations = None\n\n        jtokens = doc['tokens']\n        # jrelations = doc['relations']\n        jentities = doc['entities']\n        if \"orig_id\" in doc:\n            orig_id = doc['orig_id']\n        else:\n            orig_id = doc['org_id']\n        if \"ltokens\" in doc:\n            ltokens = doc[\"ltokens\"]\n        if \"rtokens\" in doc:\n            rtokens = doc[\"rtokens\"]\n        if \"images\" in doc and self._processor is not None:\n            jimages = doc[\"images\"]\n\n        prompt_number = self.prompt_number\n        prompt_tokens = self.prompt_tokens\n        # context_word = \"is\"\n        if self.prompt_length <= 2:\n            common_token = 0\n        else:\n            common_token = self.prompt_length-2\n\n        if self.prompt_length>0:\n            prompt = [\"{}\"] * self.prompt_length\n            if self.prompt_type == \"hard\":\n                prompt[1] = \"is\"\n                common_token = 0\n            else:\n                for i in range(common_token):\n                    prompt[i+1] = f\"{prompt_tokens[-i]}\"\n            prompt = \" \".join(prompt*prompt_number).format(*[prompt_tokens[i] for i in range(0, prompt_number*self.prompt_length)])\n            prompt = prompt.split(\" \")\n        else:\n            prompt = []\n\n        images = []\n        if jimages is not None:\n            images = self._parse_images(jimages, orig_id, dataset)\n\n        # parse tokens\n        doc_tokens, doc_encoding, seg_encoding, raw_doc_encoding, pos_encoding, inx4locator = self._parse_tokens(jtokens, ltokens, rtokens, prompt, dataset)\n\n        if len(doc_encoding) > 512:\n            self._log(f\"Document {doc['orig_id']} len(doc_encoding) = {len(doc_encoding) } > 512, Ignored!\")\n            return None\n        \n        # parse entity mentions\n        entities = self._parse_entities(jentities, doc_tokens, dataset)\n\n        # parse relations\n        relations = []\n        if jrelations is not None:\n            relations = self._parse_relations(jrelations, entities, dataset)\n\n        # create document\n        document = dataset.create_document(doc_tokens, entities, relations, doc_encoding, seg_encoding, raw_doc_encoding, inx4locator, pos_encoding, images = images)\n\n        return document\n\n    def _parse_images(self, jimages, org_id, dataset: Dataset, num = 1):\n        images = []\n        local_dir = \"/\".join(dataset._path.split(\"/\")[:-1])+\"/images/\"\n\n        # if not is_cached:\n        for jimage in jimages:\n            path = local_dir+str(jimage['id'])+\".jpg\"\n            if os.path.exists(path):\n                image = dataset.create_image(jimage['url'], jimage['caption'], jimage['id'], jimage['similarity'], local_dir)\n                images.append(image)\n\n            if len(images)>=num:\n                break\n            \n        while len(images)<num:\n            image = dataset.create_image(\"\", \"\", 0, 0, local_dir)\n            images.append(image)\n\n            # with open(cache_path, 'wb') as f:\n            #     torch.save(images, f)\n        assert len(images) == num\n        return images\n\n    def _parse_tokens(self, jtokens, ltokens, rtokens, prompt, dataset):\n        doc_tokens = []\n\n        special_tokens_map = self._tokenizer.special_tokens_map\n        doc_encoding = [self._tokenizer.convert_tokens_to_ids(special_tokens_map['cls_token'])]\n        seg_encoding = [1]\n\n        if ltokens is not None and len(ltokens)>0:\n            for token_phrase in ltokens:\n                token_encoding = self._tokenizer.encode(token_phrase, add_special_tokens=False)\n                doc_encoding += token_encoding\n                seg_encoding += [1] * len(token_encoding)\n            doc_encoding += [self._tokenizer.convert_tokens_to_ids(special_tokens_map['sep_token'])]\n            seg_encoding += [1]\n        \n        for i, token_phrase in enumerate(jtokens):\n            token_encoding = self._tokenizer.encode(token_phrase, add_special_tokens=False)\n\n            span_start, span_end = (len(doc_encoding), len(doc_encoding) + len(token_encoding) - 1 )\n            span_start = span_start + len(prompt)\n            span_end = span_end + len(prompt)\n            token = dataset.create_token(i, span_start, span_end, token_phrase)\n            doc_tokens.append(token)\n            doc_encoding += token_encoding\n            seg_encoding += [1] * len(token_encoding)\n        \n        if rtokens is not None and len(rtokens)>0:\n            doc_encoding += [self._tokenizer.convert_tokens_to_ids(special_tokens_map['sep_token'])]\n            seg_encoding += [1]\n            for token_phrase in rtokens:\n                token_encoding = self._tokenizer.encode(token_phrase, add_special_tokens=False)\n                doc_encoding += token_encoding\n                seg_encoding += [1] * len(token_encoding)\n\n        doc_encoding = doc_encoding[:512-self.prompt_number*self.prompt_length]\n        seg_encoding = seg_encoding[:512-self.prompt_number*self.prompt_length]\n\n        raw_doc_encoding = copy.deepcopy(doc_encoding)\n        \n        for token_phrase in prompt:\n            token_encoding = self._tokenizer.convert_tokens_to_ids(token_phrase)\n            doc_encoding.insert(0, token_encoding)\n            seg_encoding.insert(0, 0)\n\n        pos_encoding = list(range(self.prompt_number*self.prompt_length)) + list(range(len(raw_doc_encoding)))\n        \n        inx4locator = None\n        if self.prompt_length>0:\n            inx4locator = np.array(range(0, self.prompt_number*self.prompt_length, self.prompt_length))\n\n        return doc_tokens, doc_encoding, seg_encoding, raw_doc_encoding, pos_encoding, inx4locator\n\n    def _parse_entities(self, jentities, doc_tokens, dataset) -> List[Entity]:\n        entities = []\n\n        for entity_idx, jentity in enumerate(jentities):\n            entity_type = self._entity_types[jentity['type']]\n            start, end = jentity['start'], jentity['end']\n\n            # create entity mention  (exclusive)\n            tokens = doc_tokens[start:end]\n            phrase = \" \".join([t.phrase for t in tokens])\n            entity = dataset.create_entity(entity_type, tokens, phrase)\n            entities.append(entity)\n\n        return entities\n\n    def _parse_relations(self, jrelations, entities, dataset) -> List[Relation]:\n        relations = []\n\n        for jrelation in jrelations:\n            relation_type = self._relation_types[jrelation['type']]\n\n            head_idx = jrelation['head']\n            tail_idx = jrelation['tail']\n\n            # create relation\n            head = entities[head_idx]\n            tail = entities[tail_idx]\n\n            reverse = int(tail.tokens[0].index) < int(head.tokens[0].index)\n\n            # for symmetric relations: head occurs before tail in sentence\n            if relation_type.symmetric and reverse:\n                head, tail = util.swap(head, tail)\n\n            relation = dataset.create_relation(relation_type, head_entity=head, tail_entity=tail, reverse=reverse)\n            relations.append(relation)\n\n        return relations", ""]}
{"filename": "prompt4ner/trainer.py", "chunked_list": ["import argparse\nimport datetime\nimport logging\nimport os\nimport sys\nfrom typing import List, Dict, Tuple\n\nimport torch\nfrom torch.nn import DataParallel\nfrom torch.nn.parallel.distributed import DistributedDataParallel", "from torch.nn import DataParallel\nfrom torch.nn.parallel.distributed import DistributedDataParallel\nfrom torch.optim import Optimizer\nfrom transformers import PreTrainedModel\nfrom transformers import PreTrainedTokenizer\n\nfrom prompt4ner import util\nimport torch.utils.tensorboard as tensorboard\n\nSCRIPT_PATH = os.path.dirname(os.path.realpath(__file__))", "\nSCRIPT_PATH = os.path.dirname(os.path.realpath(__file__))\n\n\nclass BaseTrainer:\n    \"\"\" Trainer base class with common methods \"\"\"\n\n    def __init__(self, args: argparse.Namespace):\n        self.args = args\n        self._debug = self.args.debug\n\n        self.local_rank = args.local_rank\n        self.record = False\n        if self.local_rank < 1:\n            self.record = True\n\n        # logging\n        name = str(datetime.datetime.now()).replace(' ', '_')\n        self._log_path = os.path.join(self.args.log_path, self.args.label, name)\n        if self.record:\n            util.create_directories_dir(self._log_path)\n\n        if hasattr(args, 'save_path') and self.record:\n            self._save_path = os.path.join(self.args.save_path, self.args.label, name)\n            util.create_directories_dir(self._save_path)\n\n        self._log_paths = dict()\n\n        # file + console logging\n        log_formatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n        self._logger = logging.getLogger()\n        util.reset_logger(self._logger)\n\n        if self.record:\n            file_handler = logging.FileHandler(os.path.join(self._log_path, 'all.log'))\n            file_handler.setFormatter(log_formatter)\n            self._logger.addHandler(file_handler)\n\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setFormatter(log_formatter)\n        if \"pretrain\" not in self.args.label:\n            self._logger.addHandler(console_handler)\n\n        if self._debug:\n            self._logger.setLevel(logging.DEBUG)\n        else:\n            self._logger.setLevel(logging.INFO)\n\n        # tensorboard summary\n        if self.record:\n            self._summary_writer = tensorboard.SummaryWriter(self._log_path)\n\n        self._best_results = dict()\n        if self.record:\n            self._log_arguments()\n\n        # CUDA devices\n        # self._device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\")\n        if args.cpu:\n            device=\"cpu\"\n        else:\n            device=\"cuda:\" + str(args.device_id)\n\n\n        # set seed\n        if args.seed is not None:\n            util.set_seed(args.seed)\n\n        if args.local_rank != -1 and \"eval\"  not in args.label:\n            torch.cuda.set_device(args.local_rank)\n            torch.distributed.init_process_group(backend='nccl', init_method='env://', rank = args.local_rank, world_size = args.world_size)\n            self._device = torch.device('cuda', args.local_rank)\n        else:\n            self._device = torch.device(device)\n            self._gpu_count = torch.cuda.device_count()\n\n    def _add_dataset_logging(self, *labels, data: Dict[str, List[str]]):\n        for label in labels:\n            dic = dict()\n\n            for key, columns in data.items():\n                path = os.path.join(self._log_path, '%s_%s.csv' % (key, label))\n                util.create_csv(path, *columns)\n                dic[key] = path\n\n            self._log_paths[label] = dic\n            self._best_results[label] = 0\n\n    def _log_arguments(self):\n        util.save_dict(self._log_path, self.args, 'args')\n        if self._summary_writer is not None:\n            util.summarize_dict(self._summary_writer, self.args, 'args')\n\n    def _log_tensorboard(self, dataset_label: str, data_label: str, data: object, iteration: int):\n        if self._summary_writer is not None:\n            self._summary_writer.add_scalar('data/%s/%s' % (dataset_label, data_label), data, iteration)\n\n    def _log_csv(self, dataset_label: str, data_label: str, *data: Tuple[object]):\n        logs = self._log_paths[dataset_label]\n        util.append_csv(logs[data_label], *data)\n\n    # def _save_best(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, optimizer: Optimizer,\n    #                accuracy: float, iteration: int, label: str, extra=None):\n    #     if accuracy > self._best_results[label]:\n    #         self._logger.info(\"[%s] Best model in iteration %s: %s%% accuracy\" % (label, iteration, accuracy))\n    #         self._save_model(self._save_path, model, tokenizer, iteration,\n    #                          optimizer=optimizer if self.args.save_optimizer else None,\n    #                          save_as_best=True, name='model_%s' % label, extra=extra)\n    #         self._best_results[label] = accuracy\n\n    def _save_model(self, save_path: str, model: PreTrainedModel, tokenizer: PreTrainedTokenizer,\n                    iteration: int, optimizer: Optimizer = None, save_as_best: bool = False,\n                    extra: dict = None, include_iteration: int = True, name: str = 'model'):\n        extra_state = dict(iteration=iteration)\n\n        if optimizer:\n            extra_state['optimizer'] = optimizer.state_dict()\n\n        if extra:\n            extra_state.update(extra)\n\n        if save_as_best:\n            dir_path = os.path.join(save_path, 'best_%s' % name)\n        else:\n            dir_name = '%s_%s' % (name, iteration) if include_iteration else name\n            dir_path = os.path.join(save_path, dir_name)\n\n        util.create_directories_dir(dir_path)\n\n        # save model\n        if isinstance(model, (DataParallel, DistributedDataParallel)):\n            model.module.save_pretrained(dir_path)\n        else:\n            model.save_pretrained(dir_path)\n\n        # save vocabulary\n        tokenizer.save_pretrained(dir_path)\n\n        # save extra\n        state_path = os.path.join(dir_path, 'extra.state')\n        torch.save(extra_state, state_path)\n\n    def _get_lr(self, optimizer):\n        lrs = []\n        for group in optimizer.param_groups:\n            lr_scheduled = group['lr']\n            lrs.append(lr_scheduled)\n        return lrs\n\n    def _close_summary_writer(self):\n        if self._summary_writer is not None:\n            self._summary_writer.close()", ""]}
{"filename": "prompt4ner/evaluator.py", "chunked_list": ["from .entities import Token\nimport json\nimport os\nfrom typing import List, Tuple, Dict\n\nimport torch\nfrom sklearn.metrics import precision_recall_fscore_support as prfs\nfrom transformers import BertTokenizer\n\nfrom prompt4ner import util", "\nfrom prompt4ner import util\nfrom prompt4ner.entities import Document, Dataset, EntityType\nfrom prompt4ner.input_reader import JsonInputReader\nimport jinja2\n\nSCRIPT_PATH = os.path.dirname(os.path.realpath(__file__))\n\n\nclass Evaluator:\n    def __init__(self, dataset: Dataset, input_reader: JsonInputReader, text_encoder: BertTokenizer, logger, no_overlapping: bool, no_partial_overlapping: bool, no_duplicate: bool, predictions_path: str, examples_path: str, example_count: int, epoch: int, dataset_label: str, cls_threshold: float, boundary_threshold: float, save_prediction = False):\n        self._text_encoder = text_encoder\n        self._input_reader = input_reader\n        self._dataset = dataset\n        self._logger = logger\n        self._no_overlapping = no_overlapping\n        self._no_partial_overlapping = no_partial_overlapping\n        self._no_duplicate = no_duplicate\n        self._save_prediction = save_prediction\n\n\n        self._epoch = epoch\n        self._dataset_label = dataset_label\n\n        self._predictions_path = predictions_path\n\n        self._examples_path = examples_path\n        self._example_count = example_count\n\n        # entities\n        self._gt_entities = []  # ground truth\n        self._pred_entities = []  # prediction\n        self._raw_preds = []\n        self._raw_raw_preds = []\n\n        self._pseudo_entity_type = EntityType('Entity', 1, 'Entity', 'Entity')  # for span only evaluation\n        self._cls_threshold = cls_threshold\n        self._boundary_threshold = boundary_threshold\n        self._convert_gt(self._dataset.documents)\n\n    def eval_batch(self, entity_logits: torch.tensor, p_left:torch.tensor, p_right:torch.tensor, outputs, batch = None):\n        batch_size = entity_logits.shape[0] \n        entity_prob = entity_logits.softmax(-1)\n        batch_entity_types = entity_prob.argmax(dim=-1)\n\n        batch_entity_scores = entity_prob.max(dim=-1)[0]\n        batch_entity_mask = (batch_entity_scores > self._cls_threshold) * (batch_entity_types != 0)\n\n        entity_left = p_left.argmax(dim=-1)\n        entity_right = p_right.argmax(dim=-1)\n        batch_entity_left_scores = p_left.max(dim=-1)[0]\n        batch_entity_right_scores = p_right.max(dim=-1)[0]\n\n        batch_entity_spans = torch.stack([entity_left, entity_right], dim=-1)\n\n        batch_entity_mask = batch_entity_mask * (batch_entity_spans[:,:,0] <= batch_entity_spans[:,:,1]) * (batch_entity_left_scores > self._boundary_threshold) * (batch_entity_right_scores > self._boundary_threshold)\n\n        def roundlist(x):\n            return list(map(lambda x:round(x, 2), x))\n\n        for i in range(batch_size):\n            \n            doc = batch[\"meta_doc\"][i]\n            if self._input_reader.entity_type_count < 1000 and self._save_prediction:\n                decode_entity = dict(tokens=[t.phrase for t in doc.tokens], pre_entities=[], gt_entities = [], org_id= doc.doc_id)\n\n                gt_converted_entities = []\n                for entity in doc.entities:\n                    entity = entity.as_tuple_token()\n                    entity_span = entity[:2]\n                    span_tokens = util.get_span_tokens(doc.tokens, entity_span)\n                    entity_type = entity[2].identifier\n                    entity_phrase = str(span_tokens)\n                    converted_entity = dict(type=entity_type, start=span_tokens[0].index, end=span_tokens[-1].index, phrase=entity_phrase)\n                    gt_converted_entities.append(converted_entity)\n                decode_entity[\"gt_entities\"] = sorted(gt_converted_entities, key=lambda e: e['start'])\n\n            \n                for j in range(entity_left.size(1)):\n                    span_tokens = str(util.get_span_tokens(doc.tokens, batch_entity_spans[i][j]))\n                    decode_entity[\"pre_entities\"].append(dict(entity_left=entity_left[i][j].item(), entity_right=entity_right[i][j].item(), p_left=roundlist(p_left[i][j].tolist()), p_right=roundlist(p_right[i][j].tolist()), phrase=span_tokens, entity_type=self._input_reader.get_entity_type(batch_entity_types[i][j].item()).identifier, entity_prob = roundlist(entity_prob[i][j].tolist())))\n                self._raw_raw_preds.append(decode_entity)\n\n            # #query\n            entity_mask = batch_entity_mask[i]\n\n            # #query\n            entity_types = batch_entity_types[i]\n            entity_spans = batch_entity_spans[i]\n            entity_scores = batch_entity_scores[i]\n\n            # #ent\n            valid_entity_types = entity_types[entity_mask]\n            valid_entity_spans = entity_spans[entity_mask]\n            valid_entity_scores = entity_scores[entity_mask]\n\n            valid_left_scores = batch_entity_left_scores[i][entity_mask]\n            valid_right_scores = batch_entity_right_scores[i][entity_mask]\n\n            sample_pred_entities = self._convert_pred_entities(valid_entity_types, valid_entity_spans, valid_entity_scores, valid_left_scores, valid_right_scores, doc)\n            sample_pred_entities = sorted(sample_pred_entities, key=lambda x:x[3], reverse=True)\n\n            if self._no_overlapping:\n                sample_pred_entities = self._remove_overlapping(sample_pred_entities)\n            elif self._no_partial_overlapping:\n                sample_pred_entities = self._remove_partial_overlapping(sample_pred_entities)\n\n            if self._no_duplicate:\n                sample_pred_entities = self._remove_duplicate(sample_pred_entities)\n\n            self._pred_entities.append(sample_pred_entities)\n\n    def _log(self, text):\n        if self._logger is not None:\n            self._logger.info(text)\n\n    def compute_scores(self):\n        self._log(\"Evaluation\")\n\n        self._log(\"\")\n        self._log(\"--- NER ---\")\n        self._log(\"\")\n        gt, pred = self._convert_by_setting(self._gt_entities, self._pred_entities, include_entity_types=True)\n        ner_eval = self._score(gt, pred, print_results=True)\n        \n        self._log(\"\")\n        self._log(\"--- NER on Localization ---\")\n        self._log(\"\")\n        gt_wo_type, pred_wo_type = self._convert_by_setting(self._gt_entities, self._pred_entities, include_entity_types=False)\n        ner_loc_eval = self._score(gt_wo_type, pred_wo_type, print_results=True)\n\n\n        self._log(\"\")\n        self._log(\"--- NER on Classification ---\")\n        self._log(\"\")\n        ner_cls_eval = self._score(gt, pred, print_results=True, cls_metric= True)\n\n        return ner_eval, ner_loc_eval, ner_cls_eval\n\n    def store_predictions(self):\n        predictions = []\n\n        for i, doc in enumerate(self._dataset.documents):\n            tokens = doc.tokens\n            gt_entities = self._gt_entities[i]\n            pred_entities = self._pred_entities[i]\n\n            gt_converted_entities = []\n            for entity in gt_entities:\n                entity_span = entity[:2]\n                span_tokens = util.get_span_tokens(tokens, entity_span)\n                entity_type = entity[2].identifier\n                entity_phrase = str(util.get_span_tokens(doc.tokens, entity_span))\n                converted_entity = dict(type=entity_type, start=span_tokens[0].index, end=span_tokens[-1].index, phrase=entity_phrase)\n                gt_converted_entities.append(converted_entity)\n            gt_converted_entities = sorted(gt_converted_entities, key=lambda e: e['start'])\n\n            # convert entities\n            pre_converted_entities = []\n            for entity in pred_entities:\n                entity_span = entity[:2]\n                span_tokens = util.get_span_tokens(tokens, entity_span)\n                entity_type = entity[2].identifier\n                entity_phrase = str(util.get_span_tokens(doc.tokens, entity_span))\n                converted_entity = dict(type=entity_type, start=span_tokens[0].index, end=span_tokens[-1].index, phrase=entity_phrase)\n                pre_converted_entities.append(converted_entity)\n            pre_converted_entities = sorted(pre_converted_entities, key=lambda e: e['start'])\n\n            doc_predictions = dict(tokens=[t.phrase for t in tokens], pre_entities=pre_converted_entities, gt_entities = gt_converted_entities)\n            predictions.append(doc_predictions)\n\n        # store as json\n        label, epoch = self._dataset_label, self._epoch\n        with open(self._predictions_path % (label, epoch), 'w') as predictions_file:\n            json.dump(predictions, predictions_file)\n        with open(self._predictions_path % (\"raw_all\", epoch), 'w') as predictions_file:\n            json.dump(self._raw_preds, predictions_file)\n        if len(self._raw_raw_preds) != 0:\n            with open(self._predictions_path % (\"raw_raw_all\", epoch), 'w') as predictions_file:\n                json.dump(self._raw_raw_preds, predictions_file)\n        # \n        raw_preds_match_gt = []\n        raw_preds_not_match_gt = []\n        for i, (pre, gt) in enumerate(zip(self._raw_preds, self._gt_entities)):\n            doc = self._dataset.documents[i]\n            \n            def is_match(ent):\n                for gt_ent in gt:\n                    if ent[\"start\"] == gt_ent[0] and  ent[\"end\"] == gt_ent[1] and ent[\"entity_type\"] == gt_ent[2].identifier:\n                        return True\n                else:\n                    return False\n            pre_not_match_gt = dict(tokens=[t.phrase for t in doc.tokens], entities=[], org_id= doc.doc_id)\n            no_dup_pre_match_gt = dict(tokens=[t.phrase for t in doc.tokens], entities=[], org_id= doc.doc_id)\n            pre_match_gt_set = []\n\n            for ent in pre[\"entities\"]:\n                entity_span = (ent[\"start\"], ent[\"end\"])\n                ent[\"phrase\"] = str(util.get_span_tokens(doc.tokens, entity_span))\n                if is_match(ent):\n                    if (ent[\"start\"], ent[\"end\"], ent[\"entity_type\"]) not in pre_match_gt_set:\n                        pre_match_gt_set.append((ent[\"start\"], ent[\"end\"], ent[\"entity_type\"]))\n                        no_dup_pre_match_gt[\"entities\"].append(ent)\n                else:\n                    pre_not_match_gt[\"entities\"].append(ent)\n\n            raw_preds_not_match_gt.append(pre_not_match_gt)\n\n            raw_preds_match_gt.append(no_dup_pre_match_gt)\n        with open(self._predictions_path % (\"match_gt\", epoch), 'w') as predictions_file:\n            json.dump(raw_preds_match_gt, predictions_file)\n        with open(self._predictions_path % (\"not_match_gt\", epoch), 'w') as predictions_file:\n            json.dump(raw_preds_not_match_gt, predictions_file)\n\n    def store_examples(self):\n        entity_examples = []\n\n        for i, doc in enumerate(self._dataset.documents):\n            # entities\n            entity_example = self._convert_example(doc, self._gt_entities[i], self._pred_entities[i],\n                                                   include_entity_types=True, to_html=self._entity_to_html)\n            entity_examples.append(entity_example)\n\n        label, epoch = self._dataset_label, self._epoch\n\n        # entities\n        self._store_examples(entity_examples[:self._example_count],\n                             file_path=self._examples_path % ('entities', label, epoch),\n                             template='entity_examples.html')\n\n        self._store_examples(sorted(entity_examples[:self._example_count],\n                                    key=lambda k: k['length']),\n                             file_path=self._examples_path % ('entities_sorted', label, epoch),\n                             template='entity_examples.html')\n\n    def _convert_gt(self, docs: List[Document]):\n        for doc in docs:\n            gt_entities = doc.entities\n            sample_gt_entities = [entity.as_tuple_token() for entity in gt_entities]\n\n            self._gt_entities.append(sample_gt_entities)\n\n    def _convert_pred_entities(self, pred_types: torch.tensor, pred_spans: torch.tensor, pred_scores: torch.tensor,  left_scores, right_scores, doc):\n        converted_preds = []\n        \n        decode_entity = dict(tokens=[t.phrase for t in doc.tokens], entities=[], org_id= doc.doc_id)\n        for i in range(pred_types.shape[0]):\n            label_idx = pred_types[i].item()\n            entity_type = self._input_reader.get_entity_type(label_idx)\n\n            start, end = pred_spans[i].tolist()\n            cls_score = pred_scores[i].item()\n            left_score = left_scores[i].item()\n            right_score = right_scores[i].item()\n\n            converted_pred = (start, end, entity_type, cls_score)\n            converted_preds.append(converted_pred)\n            decode_entity[\"entities\"].append({\"start\": start, \"end\": end, \"entity_type\":entity_type.identifier, \"cls_score\": round(cls_score, 2), \"left_score\": round(left_score, 2), \"right_score\": round(right_score, 2)})\n        self._raw_preds.append(decode_entity)\n        return converted_preds\n\n    def _remove_duplicate(self, entities):\n        non_duplicate_entities = []\n        for i, can_entity in enumerate(entities):\n            find = False\n            for j, entity in enumerate(non_duplicate_entities):\n                if can_entity[0] == entity[0] and can_entity[1] == entity[1]:\n                    find = True\n            if not find:\n                non_duplicate_entities.append(can_entity)\n        return non_duplicate_entities\n\n    def _remove_overlapping(self, entities):\n        non_overlapping_entities = []\n        for i, entity in enumerate(entities):\n            if not self._is_overlapping(entity, non_overlapping_entities):\n                non_overlapping_entities.append(entity)\n\n        return non_overlapping_entities\n\n    def _remove_partial_overlapping(self, entities):\n        non_overlapping_entities = []\n        for i, entity in enumerate(entities):\n            if not self._is_partial_overlapping(entity, non_overlapping_entities):\n                non_overlapping_entities.append(entity)\n\n        return non_overlapping_entities\n\n    def _is_partial_overlapping(self, e1, entities):\n        for e2 in entities:\n            if self._check_partial_overlap(e1, e2):\n                return True\n\n        return False\n\n    def _is_overlapping(self, e1, entities):\n        for e2 in entities:\n            if self._check_overlap(e1, e2):\n                return True\n\n        return False\n\n    def _check_overlap(self, e1, e2):\n        if e1[1] < e2[0] or e2[1] < e1[0]:\n            return False\n        else:\n            return True\n    \n    def _check_partial_overlap(self, e1, e2):\n        if (e1[0] < e2[0] and e2[0]<=e1[1] and e1[1]<e2[1] ) or  (e2[0]<e1[0] and e1[0] <= e2[1] and e2[1] < e1[1]):\n            return True\n        else:\n            return False\n\n    def _convert_by_setting(self, gt: List[List[Tuple]], pred: List[List[Tuple]],\n                            include_entity_types: bool = True, include_score: bool = False):\n        assert len(gt) == len(pred)\n\n        # either include or remove entity types based on setting\n        def convert(t):\n            if not include_entity_types:\n                # remove entity type and score for evaluation\n                c = [t[0], t[1], self._pseudo_entity_type]\n            else:\n                c = list(t[:3])\n\n            if include_score and len(t) > 3:\n                # include prediction scores\n                c.append(t[3])\n\n            return tuple(c)\n\n        converted_gt, converted_pred = [], []\n\n        for sample_gt, sample_pred in zip(gt, pred):\n            converted_gt.append([convert(t) for t in sample_gt])\n            converted_pred.append([convert(t) for t in sample_pred])\n\n        return converted_gt, converted_pred\n\n    def _score(self, gt: List[List[Tuple]], pred: List[List[Tuple]], print_results: bool = False, cls_metric = False):\n        assert len(gt) == len(pred)\n        # import pdb;pdb.set_trace()\n\n        gt_flat = []\n        pred_flat = []\n        types = set()\n\n        for (sample_gt, sample_pred) in zip(gt, pred):\n            union = set()\n            if cls_metric:\n                union.update(sample_gt)\n                loc_gt = list(map(lambda x:(x[0],x[1]), sample_gt))\n                sample_loc_true_pred =  list(filter(lambda x:(x[0], x[1]) in  loc_gt, sample_pred))\n                union.update(sample_loc_true_pred)\n            else:\n                union.update(sample_gt)\n                union.update(sample_pred)\n\n            for s in union:\n                if s in sample_gt:\n                    t = s[2]\n                    gt_flat.append(t.index)\n                    types.add(t)\n                else:\n                    gt_flat.append(-1)\n\n                if s in sample_pred:\n                    t = s[2]\n                    pred_flat.append(t.index)\n                    types.add(t)\n                else:\n                    pred_flat.append(-1)\n        metrics = self._compute_metrics(gt_flat, pred_flat, types, print_results)\n        return metrics\n\n    def _compute_metrics(self, gt_all, pred_all, types, print_results: bool = False):\n        labels = [t.index for t in types]\n        per_type = prfs(gt_all, pred_all, labels=labels, average=None)\n        micro = prfs(gt_all, pred_all, labels=labels, average='micro')[:-1]\n        macro = prfs(gt_all, pred_all, labels=labels, average='macro')[:-1]\n        total_support = sum(per_type[-1])\n\n        if print_results:\n            self._print_results(per_type, list(micro) + [total_support], list(macro) + [total_support], types)\n\n        return [m * 100 for m in micro + macro]\n\n    def _print_results(self, per_type: List, micro: List, macro: List, types: List):\n        columns = ('type', 'precision', 'recall', 'f1-score', 'support')\n\n        row_fmt = \"%20s\" + (\" %12s\" * (len(columns) - 1))\n        self._log(row_fmt % columns)\n\n        metrics_per_type = []\n        for i, t in enumerate(types):\n            metrics = []\n            for j in range(len(per_type)):\n                metrics.append(per_type[j][i])\n            metrics_per_type.append(metrics)\n\n        for m, t in zip(metrics_per_type, types):\n            self._log(row_fmt % self._get_row(m, t.short_name))\n\n        self._log('')\n\n        # micro\n        self._log(row_fmt % self._get_row(micro, 'micro'))\n\n        # macro\n        self._log(row_fmt % self._get_row(macro, 'macro'))\n\n    def _get_row(self, data, label):\n        row = [label]\n        for i in range(len(data) - 1):\n            row.append(\"%.2f\" % (data[i] * 100))\n        row.append(data[3])\n        return tuple(row)\n\n    def _convert_example(self, doc: Document, gt: List[Tuple], pred: List[Tuple],\n                         include_entity_types: bool, to_html):\n        # encoding = doc.encoding\n        tokens = doc.tokens\n\n        gt, pred = self._convert_by_setting([gt], [pred], include_entity_types=include_entity_types, include_score=True)\n        gt, pred = gt[0], pred[0]\n\n        # get micro precision/recall/f1 scores\n        if gt or pred:\n            pred_s = [p[:3] for p in pred]  # remove score\n            precision, recall, f1 = self._score([gt], [pred_s])[:3]\n        else:\n            # corner case: no ground truth and no predictions\n            precision, recall, f1 = [100] * 3\n\n        cls_scores = [p[3] for p in pred]\n        pred = [p[:3] for p in pred]\n        union = set(gt + pred)\n\n        # true positives\n        tp = []\n        # false negatives\n        fn = []\n        # false positives\n        fp = []\n\n        for s in union:\n            type_verbose = s[2].verbose_name\n\n            if s in gt:\n                if s in pred:\n                    cls_score = cls_scores[pred.index(s)]\n                    tp.append((to_html(s, tokens), type_verbose, cls_score))\n                else:\n                    fn.append((to_html(s, tokens), type_verbose, -1))\n            else:\n                cls_score = cls_scores[pred.index(s)]\n                fp.append((to_html(s, tokens), type_verbose, cls_score))\n\n        tp = sorted(tp, key=lambda p: p[2], reverse=True)\n        fp = sorted(fp, key=lambda p: p[2], reverse=True)\n\n        phrases = []\n        for token in tokens:\n            phrases.append(token.phrase)\n        text = \" \".join(phrases)\n        \n\n        # text = self._prettify(self._text_encoder.decode(encoding))\n        text = self._prettify(text)\n        return dict(text=text, tp=tp, fn=fn, fp=fp, precision=precision, recall=recall, f1=f1, length=len(doc.tokens))\n\n    def _entity_to_html(self, entity: Tuple, tokens: List[Token]):\n        start, end = entity[:2]\n        entity_type = entity[2].verbose_name\n\n        tag_start = ' <span class=\"entity\">'\n        tag_start += '<span class=\"type\">%s</span>' % entity_type\n\n        ctx_before = \"\"\n        ctx_after = \"\"\n        e1 = \"\"\n        for i in range(start):\n            ctx_before += tokens[i].phrase\n            if i!=start-1:\n                ctx_before += \" \"\n        for i in range(end + 1, len(tokens)):\n            ctx_after += tokens[i].phrase\n            if i!=(len(tokens)-1):\n                ctx_after += \" \"\n        for i in range(start, end + 1):\n            e1 += tokens[i].phrase\n            if i!=end:\n                e1 += \" \"\n\n        html = ctx_before + tag_start + e1 + '</span> ' + ctx_after\n        html = self._prettify(html)\n\n        return html\n\n    def _prettify(self, text: str):\n        text = text.replace('_start_', '').replace('_classify_', '').replace('<unk>', '').replace('\u2047', '')\n        text = text.replace('[CLS]', '').replace('[SEP]', '').replace('[PAD]', '')\n        return text\n\n    def _store_examples(self, examples: List[Dict], file_path: str, template: str):\n        template_path = os.path.join(SCRIPT_PATH, 'templates', template)\n\n        # read template\n        with open(os.path.join(SCRIPT_PATH, template_path)) as f:\n            template = jinja2.Template(f.read())\n\n        # write to disc\n        template.stream(examples=examples).dump(file_path)", "\nclass Evaluator:\n    def __init__(self, dataset: Dataset, input_reader: JsonInputReader, text_encoder: BertTokenizer, logger, no_overlapping: bool, no_partial_overlapping: bool, no_duplicate: bool, predictions_path: str, examples_path: str, example_count: int, epoch: int, dataset_label: str, cls_threshold: float, boundary_threshold: float, save_prediction = False):\n        self._text_encoder = text_encoder\n        self._input_reader = input_reader\n        self._dataset = dataset\n        self._logger = logger\n        self._no_overlapping = no_overlapping\n        self._no_partial_overlapping = no_partial_overlapping\n        self._no_duplicate = no_duplicate\n        self._save_prediction = save_prediction\n\n\n        self._epoch = epoch\n        self._dataset_label = dataset_label\n\n        self._predictions_path = predictions_path\n\n        self._examples_path = examples_path\n        self._example_count = example_count\n\n        # entities\n        self._gt_entities = []  # ground truth\n        self._pred_entities = []  # prediction\n        self._raw_preds = []\n        self._raw_raw_preds = []\n\n        self._pseudo_entity_type = EntityType('Entity', 1, 'Entity', 'Entity')  # for span only evaluation\n        self._cls_threshold = cls_threshold\n        self._boundary_threshold = boundary_threshold\n        self._convert_gt(self._dataset.documents)\n\n    def eval_batch(self, entity_logits: torch.tensor, p_left:torch.tensor, p_right:torch.tensor, outputs, batch = None):\n        batch_size = entity_logits.shape[0] \n        entity_prob = entity_logits.softmax(-1)\n        batch_entity_types = entity_prob.argmax(dim=-1)\n\n        batch_entity_scores = entity_prob.max(dim=-1)[0]\n        batch_entity_mask = (batch_entity_scores > self._cls_threshold) * (batch_entity_types != 0)\n\n        entity_left = p_left.argmax(dim=-1)\n        entity_right = p_right.argmax(dim=-1)\n        batch_entity_left_scores = p_left.max(dim=-1)[0]\n        batch_entity_right_scores = p_right.max(dim=-1)[0]\n\n        batch_entity_spans = torch.stack([entity_left, entity_right], dim=-1)\n\n        batch_entity_mask = batch_entity_mask * (batch_entity_spans[:,:,0] <= batch_entity_spans[:,:,1]) * (batch_entity_left_scores > self._boundary_threshold) * (batch_entity_right_scores > self._boundary_threshold)\n\n        def roundlist(x):\n            return list(map(lambda x:round(x, 2), x))\n\n        for i in range(batch_size):\n            \n            doc = batch[\"meta_doc\"][i]\n            if self._input_reader.entity_type_count < 1000 and self._save_prediction:\n                decode_entity = dict(tokens=[t.phrase for t in doc.tokens], pre_entities=[], gt_entities = [], org_id= doc.doc_id)\n\n                gt_converted_entities = []\n                for entity in doc.entities:\n                    entity = entity.as_tuple_token()\n                    entity_span = entity[:2]\n                    span_tokens = util.get_span_tokens(doc.tokens, entity_span)\n                    entity_type = entity[2].identifier\n                    entity_phrase = str(span_tokens)\n                    converted_entity = dict(type=entity_type, start=span_tokens[0].index, end=span_tokens[-1].index, phrase=entity_phrase)\n                    gt_converted_entities.append(converted_entity)\n                decode_entity[\"gt_entities\"] = sorted(gt_converted_entities, key=lambda e: e['start'])\n\n            \n                for j in range(entity_left.size(1)):\n                    span_tokens = str(util.get_span_tokens(doc.tokens, batch_entity_spans[i][j]))\n                    decode_entity[\"pre_entities\"].append(dict(entity_left=entity_left[i][j].item(), entity_right=entity_right[i][j].item(), p_left=roundlist(p_left[i][j].tolist()), p_right=roundlist(p_right[i][j].tolist()), phrase=span_tokens, entity_type=self._input_reader.get_entity_type(batch_entity_types[i][j].item()).identifier, entity_prob = roundlist(entity_prob[i][j].tolist())))\n                self._raw_raw_preds.append(decode_entity)\n\n            # #query\n            entity_mask = batch_entity_mask[i]\n\n            # #query\n            entity_types = batch_entity_types[i]\n            entity_spans = batch_entity_spans[i]\n            entity_scores = batch_entity_scores[i]\n\n            # #ent\n            valid_entity_types = entity_types[entity_mask]\n            valid_entity_spans = entity_spans[entity_mask]\n            valid_entity_scores = entity_scores[entity_mask]\n\n            valid_left_scores = batch_entity_left_scores[i][entity_mask]\n            valid_right_scores = batch_entity_right_scores[i][entity_mask]\n\n            sample_pred_entities = self._convert_pred_entities(valid_entity_types, valid_entity_spans, valid_entity_scores, valid_left_scores, valid_right_scores, doc)\n            sample_pred_entities = sorted(sample_pred_entities, key=lambda x:x[3], reverse=True)\n\n            if self._no_overlapping:\n                sample_pred_entities = self._remove_overlapping(sample_pred_entities)\n            elif self._no_partial_overlapping:\n                sample_pred_entities = self._remove_partial_overlapping(sample_pred_entities)\n\n            if self._no_duplicate:\n                sample_pred_entities = self._remove_duplicate(sample_pred_entities)\n\n            self._pred_entities.append(sample_pred_entities)\n\n    def _log(self, text):\n        if self._logger is not None:\n            self._logger.info(text)\n\n    def compute_scores(self):\n        self._log(\"Evaluation\")\n\n        self._log(\"\")\n        self._log(\"--- NER ---\")\n        self._log(\"\")\n        gt, pred = self._convert_by_setting(self._gt_entities, self._pred_entities, include_entity_types=True)\n        ner_eval = self._score(gt, pred, print_results=True)\n        \n        self._log(\"\")\n        self._log(\"--- NER on Localization ---\")\n        self._log(\"\")\n        gt_wo_type, pred_wo_type = self._convert_by_setting(self._gt_entities, self._pred_entities, include_entity_types=False)\n        ner_loc_eval = self._score(gt_wo_type, pred_wo_type, print_results=True)\n\n\n        self._log(\"\")\n        self._log(\"--- NER on Classification ---\")\n        self._log(\"\")\n        ner_cls_eval = self._score(gt, pred, print_results=True, cls_metric= True)\n\n        return ner_eval, ner_loc_eval, ner_cls_eval\n\n    def store_predictions(self):\n        predictions = []\n\n        for i, doc in enumerate(self._dataset.documents):\n            tokens = doc.tokens\n            gt_entities = self._gt_entities[i]\n            pred_entities = self._pred_entities[i]\n\n            gt_converted_entities = []\n            for entity in gt_entities:\n                entity_span = entity[:2]\n                span_tokens = util.get_span_tokens(tokens, entity_span)\n                entity_type = entity[2].identifier\n                entity_phrase = str(util.get_span_tokens(doc.tokens, entity_span))\n                converted_entity = dict(type=entity_type, start=span_tokens[0].index, end=span_tokens[-1].index, phrase=entity_phrase)\n                gt_converted_entities.append(converted_entity)\n            gt_converted_entities = sorted(gt_converted_entities, key=lambda e: e['start'])\n\n            # convert entities\n            pre_converted_entities = []\n            for entity in pred_entities:\n                entity_span = entity[:2]\n                span_tokens = util.get_span_tokens(tokens, entity_span)\n                entity_type = entity[2].identifier\n                entity_phrase = str(util.get_span_tokens(doc.tokens, entity_span))\n                converted_entity = dict(type=entity_type, start=span_tokens[0].index, end=span_tokens[-1].index, phrase=entity_phrase)\n                pre_converted_entities.append(converted_entity)\n            pre_converted_entities = sorted(pre_converted_entities, key=lambda e: e['start'])\n\n            doc_predictions = dict(tokens=[t.phrase for t in tokens], pre_entities=pre_converted_entities, gt_entities = gt_converted_entities)\n            predictions.append(doc_predictions)\n\n        # store as json\n        label, epoch = self._dataset_label, self._epoch\n        with open(self._predictions_path % (label, epoch), 'w') as predictions_file:\n            json.dump(predictions, predictions_file)\n        with open(self._predictions_path % (\"raw_all\", epoch), 'w') as predictions_file:\n            json.dump(self._raw_preds, predictions_file)\n        if len(self._raw_raw_preds) != 0:\n            with open(self._predictions_path % (\"raw_raw_all\", epoch), 'w') as predictions_file:\n                json.dump(self._raw_raw_preds, predictions_file)\n        # \n        raw_preds_match_gt = []\n        raw_preds_not_match_gt = []\n        for i, (pre, gt) in enumerate(zip(self._raw_preds, self._gt_entities)):\n            doc = self._dataset.documents[i]\n            \n            def is_match(ent):\n                for gt_ent in gt:\n                    if ent[\"start\"] == gt_ent[0] and  ent[\"end\"] == gt_ent[1] and ent[\"entity_type\"] == gt_ent[2].identifier:\n                        return True\n                else:\n                    return False\n            pre_not_match_gt = dict(tokens=[t.phrase for t in doc.tokens], entities=[], org_id= doc.doc_id)\n            no_dup_pre_match_gt = dict(tokens=[t.phrase for t in doc.tokens], entities=[], org_id= doc.doc_id)\n            pre_match_gt_set = []\n\n            for ent in pre[\"entities\"]:\n                entity_span = (ent[\"start\"], ent[\"end\"])\n                ent[\"phrase\"] = str(util.get_span_tokens(doc.tokens, entity_span))\n                if is_match(ent):\n                    if (ent[\"start\"], ent[\"end\"], ent[\"entity_type\"]) not in pre_match_gt_set:\n                        pre_match_gt_set.append((ent[\"start\"], ent[\"end\"], ent[\"entity_type\"]))\n                        no_dup_pre_match_gt[\"entities\"].append(ent)\n                else:\n                    pre_not_match_gt[\"entities\"].append(ent)\n\n            raw_preds_not_match_gt.append(pre_not_match_gt)\n\n            raw_preds_match_gt.append(no_dup_pre_match_gt)\n        with open(self._predictions_path % (\"match_gt\", epoch), 'w') as predictions_file:\n            json.dump(raw_preds_match_gt, predictions_file)\n        with open(self._predictions_path % (\"not_match_gt\", epoch), 'w') as predictions_file:\n            json.dump(raw_preds_not_match_gt, predictions_file)\n\n    def store_examples(self):\n        entity_examples = []\n\n        for i, doc in enumerate(self._dataset.documents):\n            # entities\n            entity_example = self._convert_example(doc, self._gt_entities[i], self._pred_entities[i],\n                                                   include_entity_types=True, to_html=self._entity_to_html)\n            entity_examples.append(entity_example)\n\n        label, epoch = self._dataset_label, self._epoch\n\n        # entities\n        self._store_examples(entity_examples[:self._example_count],\n                             file_path=self._examples_path % ('entities', label, epoch),\n                             template='entity_examples.html')\n\n        self._store_examples(sorted(entity_examples[:self._example_count],\n                                    key=lambda k: k['length']),\n                             file_path=self._examples_path % ('entities_sorted', label, epoch),\n                             template='entity_examples.html')\n\n    def _convert_gt(self, docs: List[Document]):\n        for doc in docs:\n            gt_entities = doc.entities\n            sample_gt_entities = [entity.as_tuple_token() for entity in gt_entities]\n\n            self._gt_entities.append(sample_gt_entities)\n\n    def _convert_pred_entities(self, pred_types: torch.tensor, pred_spans: torch.tensor, pred_scores: torch.tensor,  left_scores, right_scores, doc):\n        converted_preds = []\n        \n        decode_entity = dict(tokens=[t.phrase for t in doc.tokens], entities=[], org_id= doc.doc_id)\n        for i in range(pred_types.shape[0]):\n            label_idx = pred_types[i].item()\n            entity_type = self._input_reader.get_entity_type(label_idx)\n\n            start, end = pred_spans[i].tolist()\n            cls_score = pred_scores[i].item()\n            left_score = left_scores[i].item()\n            right_score = right_scores[i].item()\n\n            converted_pred = (start, end, entity_type, cls_score)\n            converted_preds.append(converted_pred)\n            decode_entity[\"entities\"].append({\"start\": start, \"end\": end, \"entity_type\":entity_type.identifier, \"cls_score\": round(cls_score, 2), \"left_score\": round(left_score, 2), \"right_score\": round(right_score, 2)})\n        self._raw_preds.append(decode_entity)\n        return converted_preds\n\n    def _remove_duplicate(self, entities):\n        non_duplicate_entities = []\n        for i, can_entity in enumerate(entities):\n            find = False\n            for j, entity in enumerate(non_duplicate_entities):\n                if can_entity[0] == entity[0] and can_entity[1] == entity[1]:\n                    find = True\n            if not find:\n                non_duplicate_entities.append(can_entity)\n        return non_duplicate_entities\n\n    def _remove_overlapping(self, entities):\n        non_overlapping_entities = []\n        for i, entity in enumerate(entities):\n            if not self._is_overlapping(entity, non_overlapping_entities):\n                non_overlapping_entities.append(entity)\n\n        return non_overlapping_entities\n\n    def _remove_partial_overlapping(self, entities):\n        non_overlapping_entities = []\n        for i, entity in enumerate(entities):\n            if not self._is_partial_overlapping(entity, non_overlapping_entities):\n                non_overlapping_entities.append(entity)\n\n        return non_overlapping_entities\n\n    def _is_partial_overlapping(self, e1, entities):\n        for e2 in entities:\n            if self._check_partial_overlap(e1, e2):\n                return True\n\n        return False\n\n    def _is_overlapping(self, e1, entities):\n        for e2 in entities:\n            if self._check_overlap(e1, e2):\n                return True\n\n        return False\n\n    def _check_overlap(self, e1, e2):\n        if e1[1] < e2[0] or e2[1] < e1[0]:\n            return False\n        else:\n            return True\n    \n    def _check_partial_overlap(self, e1, e2):\n        if (e1[0] < e2[0] and e2[0]<=e1[1] and e1[1]<e2[1] ) or  (e2[0]<e1[0] and e1[0] <= e2[1] and e2[1] < e1[1]):\n            return True\n        else:\n            return False\n\n    def _convert_by_setting(self, gt: List[List[Tuple]], pred: List[List[Tuple]],\n                            include_entity_types: bool = True, include_score: bool = False):\n        assert len(gt) == len(pred)\n\n        # either include or remove entity types based on setting\n        def convert(t):\n            if not include_entity_types:\n                # remove entity type and score for evaluation\n                c = [t[0], t[1], self._pseudo_entity_type]\n            else:\n                c = list(t[:3])\n\n            if include_score and len(t) > 3:\n                # include prediction scores\n                c.append(t[3])\n\n            return tuple(c)\n\n        converted_gt, converted_pred = [], []\n\n        for sample_gt, sample_pred in zip(gt, pred):\n            converted_gt.append([convert(t) for t in sample_gt])\n            converted_pred.append([convert(t) for t in sample_pred])\n\n        return converted_gt, converted_pred\n\n    def _score(self, gt: List[List[Tuple]], pred: List[List[Tuple]], print_results: bool = False, cls_metric = False):\n        assert len(gt) == len(pred)\n        # import pdb;pdb.set_trace()\n\n        gt_flat = []\n        pred_flat = []\n        types = set()\n\n        for (sample_gt, sample_pred) in zip(gt, pred):\n            union = set()\n            if cls_metric:\n                union.update(sample_gt)\n                loc_gt = list(map(lambda x:(x[0],x[1]), sample_gt))\n                sample_loc_true_pred =  list(filter(lambda x:(x[0], x[1]) in  loc_gt, sample_pred))\n                union.update(sample_loc_true_pred)\n            else:\n                union.update(sample_gt)\n                union.update(sample_pred)\n\n            for s in union:\n                if s in sample_gt:\n                    t = s[2]\n                    gt_flat.append(t.index)\n                    types.add(t)\n                else:\n                    gt_flat.append(-1)\n\n                if s in sample_pred:\n                    t = s[2]\n                    pred_flat.append(t.index)\n                    types.add(t)\n                else:\n                    pred_flat.append(-1)\n        metrics = self._compute_metrics(gt_flat, pred_flat, types, print_results)\n        return metrics\n\n    def _compute_metrics(self, gt_all, pred_all, types, print_results: bool = False):\n        labels = [t.index for t in types]\n        per_type = prfs(gt_all, pred_all, labels=labels, average=None)\n        micro = prfs(gt_all, pred_all, labels=labels, average='micro')[:-1]\n        macro = prfs(gt_all, pred_all, labels=labels, average='macro')[:-1]\n        total_support = sum(per_type[-1])\n\n        if print_results:\n            self._print_results(per_type, list(micro) + [total_support], list(macro) + [total_support], types)\n\n        return [m * 100 for m in micro + macro]\n\n    def _print_results(self, per_type: List, micro: List, macro: List, types: List):\n        columns = ('type', 'precision', 'recall', 'f1-score', 'support')\n\n        row_fmt = \"%20s\" + (\" %12s\" * (len(columns) - 1))\n        self._log(row_fmt % columns)\n\n        metrics_per_type = []\n        for i, t in enumerate(types):\n            metrics = []\n            for j in range(len(per_type)):\n                metrics.append(per_type[j][i])\n            metrics_per_type.append(metrics)\n\n        for m, t in zip(metrics_per_type, types):\n            self._log(row_fmt % self._get_row(m, t.short_name))\n\n        self._log('')\n\n        # micro\n        self._log(row_fmt % self._get_row(micro, 'micro'))\n\n        # macro\n        self._log(row_fmt % self._get_row(macro, 'macro'))\n\n    def _get_row(self, data, label):\n        row = [label]\n        for i in range(len(data) - 1):\n            row.append(\"%.2f\" % (data[i] * 100))\n        row.append(data[3])\n        return tuple(row)\n\n    def _convert_example(self, doc: Document, gt: List[Tuple], pred: List[Tuple],\n                         include_entity_types: bool, to_html):\n        # encoding = doc.encoding\n        tokens = doc.tokens\n\n        gt, pred = self._convert_by_setting([gt], [pred], include_entity_types=include_entity_types, include_score=True)\n        gt, pred = gt[0], pred[0]\n\n        # get micro precision/recall/f1 scores\n        if gt or pred:\n            pred_s = [p[:3] for p in pred]  # remove score\n            precision, recall, f1 = self._score([gt], [pred_s])[:3]\n        else:\n            # corner case: no ground truth and no predictions\n            precision, recall, f1 = [100] * 3\n\n        cls_scores = [p[3] for p in pred]\n        pred = [p[:3] for p in pred]\n        union = set(gt + pred)\n\n        # true positives\n        tp = []\n        # false negatives\n        fn = []\n        # false positives\n        fp = []\n\n        for s in union:\n            type_verbose = s[2].verbose_name\n\n            if s in gt:\n                if s in pred:\n                    cls_score = cls_scores[pred.index(s)]\n                    tp.append((to_html(s, tokens), type_verbose, cls_score))\n                else:\n                    fn.append((to_html(s, tokens), type_verbose, -1))\n            else:\n                cls_score = cls_scores[pred.index(s)]\n                fp.append((to_html(s, tokens), type_verbose, cls_score))\n\n        tp = sorted(tp, key=lambda p: p[2], reverse=True)\n        fp = sorted(fp, key=lambda p: p[2], reverse=True)\n\n        phrases = []\n        for token in tokens:\n            phrases.append(token.phrase)\n        text = \" \".join(phrases)\n        \n\n        # text = self._prettify(self._text_encoder.decode(encoding))\n        text = self._prettify(text)\n        return dict(text=text, tp=tp, fn=fn, fp=fp, precision=precision, recall=recall, f1=f1, length=len(doc.tokens))\n\n    def _entity_to_html(self, entity: Tuple, tokens: List[Token]):\n        start, end = entity[:2]\n        entity_type = entity[2].verbose_name\n\n        tag_start = ' <span class=\"entity\">'\n        tag_start += '<span class=\"type\">%s</span>' % entity_type\n\n        ctx_before = \"\"\n        ctx_after = \"\"\n        e1 = \"\"\n        for i in range(start):\n            ctx_before += tokens[i].phrase\n            if i!=start-1:\n                ctx_before += \" \"\n        for i in range(end + 1, len(tokens)):\n            ctx_after += tokens[i].phrase\n            if i!=(len(tokens)-1):\n                ctx_after += \" \"\n        for i in range(start, end + 1):\n            e1 += tokens[i].phrase\n            if i!=end:\n                e1 += \" \"\n\n        html = ctx_before + tag_start + e1 + '</span> ' + ctx_after\n        html = self._prettify(html)\n\n        return html\n\n    def _prettify(self, text: str):\n        text = text.replace('_start_', '').replace('_classify_', '').replace('<unk>', '').replace('\u2047', '')\n        text = text.replace('[CLS]', '').replace('[SEP]', '').replace('[PAD]', '')\n        return text\n\n    def _store_examples(self, examples: List[Dict], file_path: str, template: str):\n        template_path = os.path.join(SCRIPT_PATH, 'templates', template)\n\n        # read template\n        with open(os.path.join(SCRIPT_PATH, template_path)) as f:\n            template = jinja2.Template(f.read())\n\n        # write to disc\n        template.stream(examples=examples).dump(file_path)", ""]}
