{"filename": "src/einsum_wrapper.py", "chunked_list": ["import torch\nimport torch.nn as nn   \nimport numpy as np\nfrom EinsumNetwork import EinsumNetwork, Graph\n\ndevice = torch.device('cuda:0')\n\n#wrapper class to create an Einsum Network given a specific structure and parameters\nclass EiNet(EinsumNetwork.EinsumNetwork):\n    def __init__(self , \n                 use_em,\n                 structure = 'poon-domingos',\n                 pd_num_pieces = [4],\n                 depth = 8,\n                 num_repetitions = 20,\n                 num_var = 784,\n                 class_count = 3,\n                 K = 10,\n                 num_sums = 10,\n                 pd_height = 28,\n                 pd_width = 28,\n                 learn_prior = True\n                ):\n        \n\n        # Structure\n        self.structure = structure        \n        self.class_count = class_count        \n        classes = np.arange(class_count)  # [0,1,2,..,n-1]\n        \n        # Define the prior, i.e. P(C) and make it learnable.\n        self.learnable_prior = learn_prior\n        # P(C) is needed to apply the Bayes' theorem and to retrive\n        # P(C|X) = P(X|C)*(P(C) / P(X)\n        if self.class_count == 4:\n            self.prior = torch.tensor([(1/3)*(2/3), (1/3)*(2/3), (1/3)*(2/3), (1/3)], dtype=torch.float, requires_grad=True, device=device).log()\n        else:\n            self.prior = torch.ones(class_count, device=device, dtype=torch.float)\n        self.prior.fill_(1 / class_count)\n        self.prior.log_()\n        if self.learnable_prior:\n            print(\"P(C) is learnable.\")\n            self.prior.requires_grad_()\n        \n        self.K = K\n        self.num_sums = num_sums\n        \n        # 'poon-domingos'\n        self.pd_num_pieces = pd_num_pieces  # [10, 28],[4],[7]\n        self.pd_height = pd_height\n        self.pd_width = pd_width\n\n        \n        # 'binary-trees'\n        self.depth = depth\n        self.num_repetitions = num_repetitions\n        self.num_var = num_var\n        \n        # drop-out rate\n        # self.drop_out = drop_out\n        # print(\"The drop-out rate:\", self.drop_out)\n        \n        # EM-settings\n        self.use_em = use_em\n        online_em_frequency = 1\n        online_em_stepsize = 0.05  # 0.05\n        print(\"train SPN with EM:\",self.use_em)\n\n        \n        # exponential_family = EinsumNetwork.BinomialArray\n        # exponential_family = EinsumNetwork.CategoricalArray\n        exponential_family = EinsumNetwork.NormalArray\n        \n        exponential_family_args = None\n        if exponential_family == EinsumNetwork.BinomialArray:\n            exponential_family_args = {'N': 255}\n        if exponential_family == EinsumNetwork.CategoricalArray:\n            exponential_family_args = {'K': 1366120}\n        if exponential_family == EinsumNetwork.NormalArray:\n            exponential_family_args = {'min_var': 1e-6, 'max_var': 0.1}\n\n        # Make EinsumNetwork\n        if self.structure == 'poon-domingos':\n            pd_delta = [[self.pd_height / d, self.pd_width / d] for d in self.pd_num_pieces]\n            graph = Graph.poon_domingos_structure(shape=(self.pd_height, self.pd_width), delta=pd_delta)\n        elif self.structure == 'binary-trees':\n            graph = Graph.random_binary_trees(num_var=self.num_var, depth=self.depth, num_repetitions=self.num_repetitions)\n        else:\n            raise AssertionError(\"Unknown Structure\")\n\n    \n        args = EinsumNetwork.Args(\n                num_var=self.num_var,\n                num_dims=1,\n                num_classes=self.class_count,\n                num_sums=self.num_sums,\n                num_input_distributions=self.K,\n                exponential_family=exponential_family,\n                exponential_family_args=exponential_family_args,\n                use_em=self.use_em,\n                online_em_frequency=online_em_frequency,\n                online_em_stepsize=online_em_stepsize)\n        \n        super().__init__(graph, args)\n        super().initialize()\n    \n    def get_log_likelihoods(self, x):\n        log_likelihood = super().forward(x)\n        return log_likelihood\n\n    def forward(self, x, marg_idx=None, type=1):\n        \n        # PRIOR\n        if type == 4:\n            expanded_prior = self.prior.expand(x.shape[0], self.prior.shape[0])\n            return expanded_prior\n\n        else:\n            # Obtain P(X|C) in log domain\n            if marg_idx:  # If marginalisation mask is passed\n                self.set_marginalization_idx(marg_idx)\n                log_likelihood = super().forward(x)\n                self.set_marginalization_idx(None)\n                likelihood = torch.nn.functional.softmax(log_likelihood, dim=1)\n            else:\n                log_likelihood = super().forward(x)\n\n            #LIKELIHOOD\n            if type == 2:\n                likelihood = torch.nn.functional.softmax(log_likelihood, dim=1)\n                # Sanity check for NaN-values\n                if torch.isnan(log_likelihood).sum() > 0:\n                    print(\"likelihood nan\")\n                \n                return likelihood\n            else:\n                # Apply Bayes' Theorem to obtain P(C|X) instead of P(X|C)\n                # as it is provided by the EiNet\n                # 1. Computation of the prior, i.e. P(C), is already being\n                # dealt with at the initialisation of the EiNet.\n                # 2. Compute the normalization constant P(X)\n                z = torch.logsumexp(log_likelihood + self.prior, dim=1)\n                # 3. Compute the posterior, i.e. P(C|X) = (P(X|C) * P(C)) / P(X)\n                posterior_log = (log_likelihood + self.prior - z[:, None])  # log domain\n                #posterior = posterior_log.exp()  # decimal domain\n            \n\n            #POSTERIOR\n            if type == 1:\n                posterior = torch.nn.functional.softmax(posterior_log, dim=1)\n                \n                # Sanity check for NaN-values\n                if torch.isnan(z).sum() > 0:\n                    print(\"z nan\")\n                if torch.isnan(posterior).sum() > 0:\n                    print(\"posterior nan\")\n                return posterior\n            \n            #JOINT\n            elif type == 3:\n                #compute the joint P(X|C) * P(C) \n                joint = torch.nn.functional.softmax(log_likelihood + self.prior, dim=1)\n\n                return joint", "class EiNet(EinsumNetwork.EinsumNetwork):\n    def __init__(self , \n                 use_em,\n                 structure = 'poon-domingos',\n                 pd_num_pieces = [4],\n                 depth = 8,\n                 num_repetitions = 20,\n                 num_var = 784,\n                 class_count = 3,\n                 K = 10,\n                 num_sums = 10,\n                 pd_height = 28,\n                 pd_width = 28,\n                 learn_prior = True\n                ):\n        \n\n        # Structure\n        self.structure = structure        \n        self.class_count = class_count        \n        classes = np.arange(class_count)  # [0,1,2,..,n-1]\n        \n        # Define the prior, i.e. P(C) and make it learnable.\n        self.learnable_prior = learn_prior\n        # P(C) is needed to apply the Bayes' theorem and to retrive\n        # P(C|X) = P(X|C)*(P(C) / P(X)\n        if self.class_count == 4:\n            self.prior = torch.tensor([(1/3)*(2/3), (1/3)*(2/3), (1/3)*(2/3), (1/3)], dtype=torch.float, requires_grad=True, device=device).log()\n        else:\n            self.prior = torch.ones(class_count, device=device, dtype=torch.float)\n        self.prior.fill_(1 / class_count)\n        self.prior.log_()\n        if self.learnable_prior:\n            print(\"P(C) is learnable.\")\n            self.prior.requires_grad_()\n        \n        self.K = K\n        self.num_sums = num_sums\n        \n        # 'poon-domingos'\n        self.pd_num_pieces = pd_num_pieces  # [10, 28],[4],[7]\n        self.pd_height = pd_height\n        self.pd_width = pd_width\n\n        \n        # 'binary-trees'\n        self.depth = depth\n        self.num_repetitions = num_repetitions\n        self.num_var = num_var\n        \n        # drop-out rate\n        # self.drop_out = drop_out\n        # print(\"The drop-out rate:\", self.drop_out)\n        \n        # EM-settings\n        self.use_em = use_em\n        online_em_frequency = 1\n        online_em_stepsize = 0.05  # 0.05\n        print(\"train SPN with EM:\",self.use_em)\n\n        \n        # exponential_family = EinsumNetwork.BinomialArray\n        # exponential_family = EinsumNetwork.CategoricalArray\n        exponential_family = EinsumNetwork.NormalArray\n        \n        exponential_family_args = None\n        if exponential_family == EinsumNetwork.BinomialArray:\n            exponential_family_args = {'N': 255}\n        if exponential_family == EinsumNetwork.CategoricalArray:\n            exponential_family_args = {'K': 1366120}\n        if exponential_family == EinsumNetwork.NormalArray:\n            exponential_family_args = {'min_var': 1e-6, 'max_var': 0.1}\n\n        # Make EinsumNetwork\n        if self.structure == 'poon-domingos':\n            pd_delta = [[self.pd_height / d, self.pd_width / d] for d in self.pd_num_pieces]\n            graph = Graph.poon_domingos_structure(shape=(self.pd_height, self.pd_width), delta=pd_delta)\n        elif self.structure == 'binary-trees':\n            graph = Graph.random_binary_trees(num_var=self.num_var, depth=self.depth, num_repetitions=self.num_repetitions)\n        else:\n            raise AssertionError(\"Unknown Structure\")\n\n    \n        args = EinsumNetwork.Args(\n                num_var=self.num_var,\n                num_dims=1,\n                num_classes=self.class_count,\n                num_sums=self.num_sums,\n                num_input_distributions=self.K,\n                exponential_family=exponential_family,\n                exponential_family_args=exponential_family_args,\n                use_em=self.use_em,\n                online_em_frequency=online_em_frequency,\n                online_em_stepsize=online_em_stepsize)\n        \n        super().__init__(graph, args)\n        super().initialize()\n    \n    def get_log_likelihoods(self, x):\n        log_likelihood = super().forward(x)\n        return log_likelihood\n\n    def forward(self, x, marg_idx=None, type=1):\n        \n        # PRIOR\n        if type == 4:\n            expanded_prior = self.prior.expand(x.shape[0], self.prior.shape[0])\n            return expanded_prior\n\n        else:\n            # Obtain P(X|C) in log domain\n            if marg_idx:  # If marginalisation mask is passed\n                self.set_marginalization_idx(marg_idx)\n                log_likelihood = super().forward(x)\n                self.set_marginalization_idx(None)\n                likelihood = torch.nn.functional.softmax(log_likelihood, dim=1)\n            else:\n                log_likelihood = super().forward(x)\n\n            #LIKELIHOOD\n            if type == 2:\n                likelihood = torch.nn.functional.softmax(log_likelihood, dim=1)\n                # Sanity check for NaN-values\n                if torch.isnan(log_likelihood).sum() > 0:\n                    print(\"likelihood nan\")\n                \n                return likelihood\n            else:\n                # Apply Bayes' Theorem to obtain P(C|X) instead of P(X|C)\n                # as it is provided by the EiNet\n                # 1. Computation of the prior, i.e. P(C), is already being\n                # dealt with at the initialisation of the EiNet.\n                # 2. Compute the normalization constant P(X)\n                z = torch.logsumexp(log_likelihood + self.prior, dim=1)\n                # 3. Compute the posterior, i.e. P(C|X) = (P(X|C) * P(C)) / P(X)\n                posterior_log = (log_likelihood + self.prior - z[:, None])  # log domain\n                #posterior = posterior_log.exp()  # decimal domain\n            \n\n            #POSTERIOR\n            if type == 1:\n                posterior = torch.nn.functional.softmax(posterior_log, dim=1)\n                \n                # Sanity check for NaN-values\n                if torch.isnan(z).sum() > 0:\n                    print(\"z nan\")\n                if torch.isnan(posterior).sum() > 0:\n                    print(\"posterior nan\")\n                return posterior\n            \n            #JOINT\n            elif type == 3:\n                #compute the joint P(X|C) * P(C) \n                joint = torch.nn.functional.softmax(log_likelihood + self.prior, dim=1)\n\n                return joint", ""]}
{"filename": "src/__init__.py", "chunked_list": [""]}
{"filename": "src/slot_attention_module.py", "chunked_list": ["\"\"\"\nSlot attention model based on code of tkipf and the corresponding paper Locatello et al. 2020\n\"\"\"\nfrom torch import nn\nimport torch\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport numpy as np\nfrom torchsummary import summary\n", "from torchsummary import summary\n\n\ndef build_grid(resolution):\n    ranges = [np.linspace(0., 1., num=res) for res in resolution]\n    grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n    grid = np.stack(grid, axis=-1)\n    grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n    grid = np.expand_dims(grid, axis=0)\n    grid = grid.astype(np.float32)\n    return np.concatenate([grid, 1.0 - grid], axis=-1)", "\n\ndef spatial_broadcast(slots, resolution):\n    \"\"\"Broadcast slot features to a 2D grid and collapse slot dimension.\"\"\"\n    # `slots` has shape: [batch_size, num_slots, slot_size].\n    slots = torch.reshape(slots, [slots.shape[0] * slots.shape[1], 1, 1, slots.shape[2]])\n    \n    grid = slots.repeat(1, resolution[0], resolution[1], 1) #repeat expands the data along differnt dimensions\n    # `grid` has shape: [batch_size*num_slots, width, height, slot_size].\n    return grid", "\n\ndef unstack_and_split(x, batch_size, n_slots, num_channels=3):\n  \"\"\"Unstack batch dimension and split into channels and alpha mask.\"\"\"\n  # unstacked = torch.reshape(x, [batch_size, -1] + list(x.shape[1:]))\n  # channels, masks = torch.split(unstacked, [num_channels, 1], dim=-1)\n  unstacked = torch.reshape(x, [batch_size, n_slots] + list(x.shape[1:]))\n  channels, masks = torch.split(unstacked, [num_channels, 1], dim=2)\n  return channels, masks\n", "\n\nclass SlotAttention(nn.Module):\n    def __init__(self, num_slots, dim, iters=3, eps=1e-8, hidden_dim=128):\n        super().__init__()\n        self.num_slots = num_slots\n        self.iters = iters\n        self.eps = eps\n        self.scale = dim ** -0.5 #named D in the paper\n\n        self.slots_mu = nn.Parameter(torch.randn(1, 1, dim)) #randomly initialize sigma and mu \n        self.slots_log_sigma = nn.Parameter(torch.randn(1, 1, dim)).abs().to(device='cuda')\n        #self.slots_mu = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(1,1,dim), gain=1.0)) #randomly initialize sigma and mu \n        #self.slots_log_sigma = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(1,1,dim), gain=1.0))\n\n        self.project_q = nn.Linear(dim, dim, bias=True) #query  projection\n        self.project_k = nn.Linear(dim, dim, bias=True) #\n        self.project_v = nn.Linear(dim, dim, bias=True) #feature key projection\n\n        self.gru = nn.GRUCell(dim, dim)\n\n        hidden_dim = max(dim, hidden_dim)\n\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, dim)\n        )\n\n        self.norm_inputs = nn.LayerNorm(dim, eps=1e-05)\n        self.norm_slots = nn.LayerNorm(dim, eps=1e-05)\n        self.norm_mlp = nn.LayerNorm(dim, eps=1e-05)\n\n        self.attn = 0\n\n    def forward(self, inputs, num_slots=None):\n        b, n, d = inputs.shape #b is the batchsize, n is the dimensionsize of the features, d is the amount of features([15, 1024, 32])\n        n_s = num_slots if num_slots is not None else self.num_slots\n\n        mu = self.slots_mu.expand(b, n_s, -1) #mu and sigma are shared by all slots\n        sigma = self.slots_log_sigma.expand(b, n_s, -1)\n        slots = torch.normal(mu, sigma) #sample slots from mu and sigma\n        #slots = torch.normal(mu, sigma.exp()) #sample slots from mu and sigma\n\n        \n        inputs = self.norm_inputs(inputs) #layer normalization of inputs \n        k, v = self.project_k(inputs), self.project_v(inputs) #*self.scale\n        \n\n        for _ in range(self.iters):\n            slots_prev = slots #store old slots\n\n            slots = self.norm_slots(slots) #layer norm of slots\n            q = self.project_q(slots) #emit a query for all slots\n\n            dots = torch.einsum('bid,bjd->bij', q, k) * self.scale #is M in the paper, has shape 1024(feature map)| 7(slot amount)\n            attn = dots.softmax(dim=1) + self.eps #calcualte the softmax for each slot which is also 1024 * 7\n            attn = attn / attn.sum(dim=-1, keepdim=True) #weighted mean\n\n            updates = torch.einsum('bjd,bij->bid', v, attn)\n\n            #recurrently update the slots with the slot updates and the previous slots\n            slots = self.gru(\n                updates.reshape(-1, d),\n                slots_prev.reshape(-1, d)\n            )\n            \n            #apply 2 layer relu mlp to GRU output\n            slots = slots.reshape(b, -1, d)\n            slots = slots + self.mlp(self.norm_mlp(slots))\n\n        self.attn = attn\n\n        return slots", "\n\nclass SlotAttention_encoder(nn.Module):\n    def __init__(self, in_channels, hidden_channels, clevr_encoding):\n        super(SlotAttention_encoder, self).__init__()\n\n        if clevr_encoding:\n            self.network = nn.Sequential(\n                nn.Conv2d(in_channels, hidden_channels, (5, 5), stride=(1, 1), padding=2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(hidden_channels, hidden_channels, (5, 5), stride=(2, 2), padding=2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(hidden_channels, hidden_channels, (5, 5), stride=(2, 2), padding=2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(hidden_channels, hidden_channels, (5, 5), stride=(1, 1), padding=2),\n                nn.ReLU(inplace=True))\n        else:\n            self.network = nn.Sequential(\n                nn.Conv2d(in_channels, hidden_channels, (5, 5), stride=(1, 1), padding=2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(hidden_channels, hidden_channels, (5, 5), stride=(1, 1), padding=2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(hidden_channels, hidden_channels, (5, 5), stride=(1, 1), padding=2),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(hidden_channels, hidden_channels, (5, 5), stride=(1, 1), padding=2),\n                nn.ReLU(inplace=True))\n\n\n\n\n    def forward(self, x):\n        return self.network(x)", "\n\nclass MLP(nn.Module):\n    def __init__(self, hidden_channels):\n        super(MLP, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(hidden_channels, hidden_channels),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_channels, hidden_channels),\n        )\n\n    def forward(self, x):\n        return self.network(x)", "\n\nclass SoftPositionEmbed(nn.Module):\n    \"\"\"Adds soft positional embedding with learnable projection.\"\"\"\n\n    def __init__(self, hidden_size, resolution, device=\"cuda:0\"):\n        \"\"\"Builds the soft position embedding layer.\n        Args:\n          hidden_size: Size of input feature dimension.\n          resolution: Tuple of integers specifying width and height of grid.\n        \"\"\"\n        super().__init__()\n        self.dense = nn.Linear(4, hidden_size)\n        # self.grid = torch.FloatTensor(build_grid(resolution))\n        # self.grid = self.grid.to(device)\n        # for nn.DataParallel\n        self.register_buffer(\"grid\", torch.FloatTensor(build_grid(resolution)))\n        self.resolution = resolution[0]\n        self.hidden_size = hidden_size\n\n    def forward(self, inputs):\n        return inputs + self.dense(self.grid).view((-1, self.hidden_size, self.resolution, self.resolution))", "\n\nclass SlotAttention_classifier(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(SlotAttention_classifier, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(in_channels, in_channels),  # nn.Conv1d(in_channels, in_channels, 1, stride=1, groups=in_channels)\n            nn.ReLU(inplace=True),\n            nn.Linear(in_channels, out_channels),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.network(x)", "\n\nclass SlotAttention_model(nn.Module):\n    def __init__(self, n_slots, n_iters, n_attr,\n                 in_channels=3,\n                 encoder_hidden_channels=64,\n                 attention_hidden_channels=128,\n                 mlp_prediction = False,\n                 device=\"cuda\",\n                 clevr_encoding=False):\n        super(SlotAttention_model, self).__init__()\n        self.n_slots = n_slots\n        self.n_iters = n_iters\n        self.n_attr = n_attr\n        self.n_attr = n_attr + 1  # additional slot to indicate if it is a object or empty slot\n        self.device = device\n\n        self.encoder_cnn = SlotAttention_encoder(in_channels=in_channels, hidden_channels=encoder_hidden_channels , clevr_encoding=clevr_encoding)\n        self.encoder_pos = SoftPositionEmbed(encoder_hidden_channels, (32, 32), device=device)# changed from 128* 128\n        self.layer_norm = nn.LayerNorm(encoder_hidden_channels, eps=1e-05)\n        self.mlp = MLP(hidden_channels=encoder_hidden_channels)\n        self.slot_attention = SlotAttention(num_slots=n_slots, dim=encoder_hidden_channels, iters=n_iters, eps=1e-8,\n                                            hidden_dim=attention_hidden_channels)\n\n        #for set prediction baseline\n        self.mlp_prediction = mlp_prediction\n        self.mlp_classifier = SlotAttention_classifier(in_channels=encoder_hidden_channels, out_channels=self.n_attr)\n\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, img):\n        # `x` has shape: [batch_size, width, height, num_channels].\n        \n        # SLOT ATTENTION ENCODER\n        x = self.encoder_cnn(img)\n        x = self.encoder_pos(x)\n        x = torch.flatten(x, start_dim=2)\n\n        # permute channel dimensions\n        x = x.permute(0, 2, 1)\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n                \n        slots = self.slot_attention(x)\n        # slots has shape: [batch_size, num_slots, slot_size].\n        if self.mlp_prediction:        \n            x = self.mlp_classifier(slots)\n            return x\n        else:\n            return slots", "        \n\nif __name__ == \"__main__\":\n    x = torch.rand(15, 3, 32, 32).cuda()\n    net = SlotAttention_model(n_slots=11, n_iters=3, n_attr=18,\n                              encoder_hidden_channels=32, attention_hidden_channels=64,\n                              decoder_hidden_channels=32, decoder_initial_size=(8, 8))\n    net = net.cuda()\n    output = net(x)\n    summary(net, (3, 32, 32))", "\n"]}
{"filename": "src/utils.py", "chunked_list": ["import numpy as np\nimport os\nimport torch\nimport errno\nfrom PIL import Image\nimport torch \nfrom torch.utils.tensorboard import SummaryWriter\nimport time\nimport matplotlib\nimport matplotlib.pyplot as plt", "import matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nimport random\n\n\ndef mkdir_p(path):\n    \"\"\"Linux mkdir -p\"\"\"\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise", "\n\ndef one_hot(x, K, dtype=torch.float):\n    \"\"\"One hot encoding\"\"\"\n    with torch.no_grad():\n        ind = torch.zeros(x.shape + (K,), dtype=dtype, device=x.device)\n        ind.scatter_(-1, x.unsqueeze(-1), 1)\n        return ind\n\n\ndef save_image_stack(samples, num_rows, num_columns, filename, margin=5, margin_gray_val=1., frame=0, frame_gray_val=0.0):\n    \"\"\"Save image stack in a tiled image\"\"\"\n\n    # for gray scale, convert to rgb\n    if len(samples.shape) == 3:\n        samples = np.stack((samples,) * 3, -1)\n\n    height = samples.shape[1]\n    width = samples.shape[2]\n\n    samples -= samples.min()\n    samples /= samples.max()\n\n    img = margin_gray_val * np.ones((height*num_rows + (num_rows-1)*margin, width*num_columns + (num_columns-1)*margin, 3))\n    for h in range(num_rows):\n        for w in range(num_columns):\n            img[h*(height+margin):h*(height+margin)+height, w*(width+margin):w*(width+margin)+width, :] = samples[h*num_columns + w, :]\n\n    framed_img = frame_gray_val * np.ones((img.shape[0] + 2*frame, img.shape[1] + 2*frame, 3))\n    framed_img[frame:(frame+img.shape[0]), frame:(frame+img.shape[1]), :] = img\n\n    img = Image.fromarray(np.round(framed_img * 255.).astype(np.uint8))\n\n    img.save(filename)", "\n\ndef save_image_stack(samples, num_rows, num_columns, filename, margin=5, margin_gray_val=1., frame=0, frame_gray_val=0.0):\n    \"\"\"Save image stack in a tiled image\"\"\"\n\n    # for gray scale, convert to rgb\n    if len(samples.shape) == 3:\n        samples = np.stack((samples,) * 3, -1)\n\n    height = samples.shape[1]\n    width = samples.shape[2]\n\n    samples -= samples.min()\n    samples /= samples.max()\n\n    img = margin_gray_val * np.ones((height*num_rows + (num_rows-1)*margin, width*num_columns + (num_columns-1)*margin, 3))\n    for h in range(num_rows):\n        for w in range(num_columns):\n            img[h*(height+margin):h*(height+margin)+height, w*(width+margin):w*(width+margin)+width, :] = samples[h*num_columns + w, :]\n\n    framed_img = frame_gray_val * np.ones((img.shape[0] + 2*frame, img.shape[1] + 2*frame, 3))\n    framed_img[frame:(frame+img.shape[0]), frame:(frame+img.shape[1]), :] = img\n\n    img = Image.fromarray(np.round(framed_img * 255.).astype(np.uint8))\n\n    img.save(filename)", "\n\ndef sample_matrix_categorical(p):\n    \"\"\"Sample many Categorical distributions represented as rows in a matrix.\"\"\"\n    with torch.no_grad():\n        cp = torch.cumsum(p[:, 0:-1], -1)\n        rand = torch.rand((cp.shape[0], 1), device=cp.device)\n        rand_idx = torch.sum(rand > cp, -1).long()\n        return rand_idx\n", "\n\ndef set_manual_seed(seed:int=1):\n    \"\"\"Set the seed for the PRNGs.\"\"\"\n    os.environ['PYTHONASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.cuda.benchmark = True", "    \n\ndef time_delta_now(t_start: float, simple_format=False, ret_sec=False) -> str:\n    \"\"\"\n                    \n    Convert a timestamp into a human readable timestring.\n    Parameters\n    ----------\n    t_start : float\n        The timestamp describing the begin of any event.\n    Returns\n    -------\n    Human readable timestring.\n    \"\"\"\n    a = t_start\n    b = time.time()  # current epoch time\n    c = b - a  # seconds\n    days = round(c // 86400)\n    hours = round(c // 3600 % 24)\n    minutes = round(c // 60 % 60)\n    seconds = round(c % 60)\n    millisecs = round(c % 1 * 1000)\n    if simple_format:\n        return f\"{hours}h:{minutes}m:{seconds}s\"\n\n    return f\"{days} days, {hours} hours, {minutes} minutes, {seconds} seconds, {millisecs} milliseconds\", c", "\ndef time_delta(c: float, simple_format=False,) -> str:\n    c# seconds\n    days = round(c // 86400)\n    hours = round(c // 3600 % 24)\n    minutes = round(c // 60 % 60)\n    seconds = round(c % 60)\n    millisecs = round(c % 1 * 1000)\n    if simple_format:\n        return f\"{hours}h:{minutes}m:{seconds}s\"\n    return f\"{days} days, {hours} hours, {minutes} minutes, {seconds} seconds, {millisecs} milliseconds\", c", "\n\ndef export_results(test_accuracy_list, train_accuracy_list,\n                   export_path, export_suffix, \n                   confusion_matrix , exp_dict):\n    \n    #set matplotlib styles\n    plt.style.use(['science','grid'])\n    matplotlib.rcParams.update(\n        {\n            \"font.family\": \"serif\",\n            \"text.usetex\": False,\n            \"legend.fontsize\": 22\n        }\n    )\n\n    \n    fig, axs = plt.subplots(1, 1 , figsize=(10,21))\n    # fig.suptitle(exp_dict['exp_name'], fontsize=16)\n    #axs[0]\n    axs.plot(test_accuracy_list[:,1],test_accuracy_list[:,0], label='test accuracy')\n    #axs[0]\n    axs.plot(train_accuracy_list[:,1],train_accuracy_list[:,0], label='train accuracy')\n    #axs[0].\n    axs.legend(loc=\"lower right\")\n    #axs[0].\n    axs.set(xlabel='epochs', ylabel='accuracy')\n    #axs[0].\n    axs.xaxis.set_major_locator(MaxNLocator(integer=True))\n\n\n    #ax.[0, 1].set_xticklabels([0,1,2,3,4,5,6,7,8,9])\n    #ax.[0, 1].set_yticklabels([0,1,2,3,4,5,6,7,8,9])\n\n    #axs[0, 1].set(xlabel='target', ylabel='prediction')\n    #axs[1] = sns.heatmap(confusion_matrix, linewidths=2, cmap=\"viridis\")\n\n\n    \n    if exp_dict['structure'] == 'poon-domingos':\n        text = \"trainable parameters = {}, lr = {}, batchsize= {}, time_per_epoch= {},\\n structure= {}, pd_num_pieces = {}\".format(\n            exp_dict['num_trainable_params'], exp_dict['lr'],\n            exp_dict['bs'], exp_dict['train_time'],            \n            exp_dict['structure'], exp_dict['pd_num_pieces'] )    \n        \n    else:\n        text = \"trainable parameters = {}, lr = {}, batchsize= {}, time_per_epoch= {},\\n structure= {}, num_repetitions = {} , depth = {}\".format(\n            exp_dict['num_trainable_params'], exp_dict['lr'],\n            exp_dict['bs'],exp_dict['train_time'],\n            exp_dict['structure'], exp_dict['num_repetitions'], exp_dict['depth'])\n        \n    #plt.gcf().text(\n    #0.5,\n    #0.02,\n    #text,\n    #ha=\"center\",\n    #fontsize=12,\n    #linespacing=1.5,\n    #bbox=dict(\n    #    facecolor=\"grey\", alpha=0.2, edgecolor=\"black\", boxstyle=\"round,pad=1\"\n    #))\n    \n    fig.savefig(export_path, format=\"svg\")\n    \n    plt.show()\n    \n    \n    #Tensorboard outputs\n    writer = SummaryWriter(\"../../results\", filename_suffix=export_suffix)\n\n    for train_acc_elem, test_acc_elem in zip(train_accuracy_list, test_accuracy_list):\n        writer.add_scalar('Accuracy/train', train_acc_elem[0], train_acc_elem[1])\n        writer.add_scalar('Accuracy/test', test_acc_elem[0], test_acc_elem[1])", "\n        \n"]}
{"filename": "src/datasets.py", "chunked_list": ["import os\nimport tempfile\nimport  urllib.request\nimport shutil\n\nfrom zipfile import ZipFile\nimport gzip\nimport utils\n\ndef maybe_download(directory, url_base, filename, suffix='.zip'):\n    '''\n    Downloads the specified dataset and extracts it\n\n    @param directory:\n    @param url_base: URL where to find the file\n    @param filename: name of the file to be downloaded\n    @param suffix: suffix of the file\n\n    :returns: true if nothing went wrong downloading \n    '''\n\n    filepath = os.path.join(directory, filename)\n    if os.path.isfile(filepath):\n        return False\n\n    if not os.path.isdir(directory):\n        utils.mkdir_p(directory)\n\n    url = url_base  +filename\n    \n    _, zipped_filepath = tempfile.mkstemp(suffix=suffix)\n        \n    print('Downloading {} to {}'.format(url, zipped_filepath))\n    \n    urllib.request.urlretrieve(url, zipped_filepath)\n    print('{} Bytes'.format(os.path.getsize(zipped_filepath)))\n    \n    print('Move to {}'.format(filepath))\n    shutil.move(zipped_filepath, filepath)\n    return True", "\ndef maybe_download(directory, url_base, filename, suffix='.zip'):\n    '''\n    Downloads the specified dataset and extracts it\n\n    @param directory:\n    @param url_base: URL where to find the file\n    @param filename: name of the file to be downloaded\n    @param suffix: suffix of the file\n\n    :returns: true if nothing went wrong downloading \n    '''\n\n    filepath = os.path.join(directory, filename)\n    if os.path.isfile(filepath):\n        return False\n\n    if not os.path.isdir(directory):\n        utils.mkdir_p(directory)\n\n    url = url_base  +filename\n    \n    _, zipped_filepath = tempfile.mkstemp(suffix=suffix)\n        \n    print('Downloading {} to {}'.format(url, zipped_filepath))\n    \n    urllib.request.urlretrieve(url, zipped_filepath)\n    print('{} Bytes'.format(os.path.getsize(zipped_filepath)))\n    \n    print('Move to {}'.format(filepath))\n    shutil.move(zipped_filepath, filepath)\n    return True", "\n\ndef extract_dataset(directory, filepath, filepath_extracted):\n    if not os.path.isdir(filepath_extracted):\n        print('unzip ',filepath, \" to\", filepath_extracted)\n        with ZipFile(filepath, 'r') as zipObj:\n           # Extract all the contents of zip file in current directory\n           zipObj.extractall(directory)\n            \n            \ndef maybe_download_shapeworld4():\n    '''\n    Downloads the shapeworld4 dataset if it is not downloaded yet\n    '''\n        \n    directory = \"../../data/\"\n    file_name= \"shapeworld4.zip\"\n    maybe_download(directory, \"https://hessenbox.tu-darmstadt.de/dl/fiEE3hftM4n1gBGn4HJLKUkU/\", file_name)\n    \n    filepath = os.path.join(directory, file_name)\n    filepath_extracted = os.path.join(directory,\"shapeworld4\")\n    \n    extract_dataset(directory, filepath, filepath_extracted)", "            \n            \ndef maybe_download_shapeworld4():\n    '''\n    Downloads the shapeworld4 dataset if it is not downloaded yet\n    '''\n        \n    directory = \"../../data/\"\n    file_name= \"shapeworld4.zip\"\n    maybe_download(directory, \"https://hessenbox.tu-darmstadt.de/dl/fiEE3hftM4n1gBGn4HJLKUkU/\", file_name)\n    \n    filepath = os.path.join(directory, file_name)\n    filepath_extracted = os.path.join(directory,\"shapeworld4\")\n    \n    extract_dataset(directory, filepath, filepath_extracted)", "\n    \ndef maybe_download_shapeworld_cogent():\n    '''\n    Downloads the shapeworld4 cogent dataset if it is not downloaded yet\n    '''\n\n    directory = \"../../data/\"\n    file_name= \"shapeworld_cogent.zip\"\n    maybe_download(directory, \"https://hessenbox.tu-darmstadt.de/dl/fi3CDjPRsYgAvotHcC8GPaWj/\", file_name)\n    \n    filepath = os.path.join(directory, file_name)\n    filepath_extracted = os.path.join(directory,\"shapeworld_cogent\")\n    \n    extract_dataset(directory, filepath, filepath_extracted)", ""]}
{"filename": "src/SLASH/__init__.py", "chunked_list": [""]}
{"filename": "src/SLASH/slash.py", "chunked_list": ["\"\"\"\nThe source code is based on:\nNeurASP: Embracing Neural Networks into Answer Set Programming\nZhun Yang, Adam Ishay, Joohyung Lee. Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence\nMain track. Pages 1755-1762.\n\nhttps://github.com/azreasoners/NeurASP\n\"\"\"\n\n", "\n\nimport re\nimport sys\nimport time\n\nimport clingo\nimport torch\nimport torch.nn as nn\n", "import torch.nn as nn\n\nimport numpy as np\nimport time\n\nsys.path.append('../')\nsys.path.append('../../')\nsys.path.append('../../SLASH/')\n\nfrom operator import itemgetter", "\nfrom operator import itemgetter\nfrom mvpp import MVPP\nfrom sklearn.metrics import confusion_matrix\n\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\nimport scipy \n\nfrom itertools import count", "\nfrom itertools import count\n\ndef pad_3d_tensor(target, framework, bs, ruleNum, max_classes):\n    '''\n    Turns a 3d list with unequal length into a padded tensor or 3D numpy array \n    \n    @param target: nested list\n    @param framework: type of the framework (either numpy or pytorch) of the output\n    @param bs: len of the first dimension of the list\n    @param ruleNum: max len of the second dimension of the list\n    @param max_classes:  max len of the third dimension of the list\n    :return:returns a tensor or 3D numpy array\n    '''\n\n    if framework == 'numpy':\n        padded = torch.tensor([np.hstack((np.asarray(row, dtype=np.float32),  [0] * (max_classes - len(row))) ) for batch in target for row in batch]).type(torch.FloatTensor).view(bs, ruleNum, max_classes)    \n    \n    if framework == 'torch':\n        padded = torch.stack([torch.hstack((row,  torch.tensor([0] * (max_classes - len(row)), device=\"cuda\" ) ) ) for batch in target for row in batch ]).view(ruleNum, bs, max_classes)\n    return padded", "\n\ndef replace_plus_minus_occurences(pi_prime):\n    \"\"\"\n    For a given query/program replace all occurences of the +- Notation with the ASP readable counterpart eg. digit(0, +i1, -0) -> digit(0, 1, i1, 0)\n\n    @param pi_prime: input query/program as string\n    :return:returns the ASP readable counterpart of the given query/program and a dict of all used operators per npp\n    \"\"\"\n\n\n    pat_pm = r'(\\s*[a-z][a-zA-Z0-9_]*)\\((\\s*[A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\+([A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\-([A-Z]*[a-zA-Z0-9_]*\\s*)\\)' #1 +- p(c|x) posterior\n    pat_mp = r'(\\s*[a-z][a-zA-Z0-9_]*)\\((\\s*[A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\-([A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\+([A-Z]*[a-zA-Z0-9_]*\\s*)\\)' #2 -+ p(x|c) likelihood\n    pat_mm = r'(\\s*[a-z][a-zA-Z0-9_]*)\\((\\s*[A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\-([A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\-([A-Z]*[a-zA-Z0-9_]*\\s*)\\)' #3 -- p(x,c) joint\n    pat_pp = r'(\\s*[a-z][a-zA-Z0-9_]*)\\((\\s*[A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\+([A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\+([A-Z]*[a-zA-Z0-9_]*\\s*)\\)' #3 -- p(c) prior\n\n    #pattern for vqa relations with two vars\n    pat_pm2 = r'([a-z][a-zA-Z0-9_]*)\\((\\s*[A-Z]*[a-zA-Z0-9_]*\\s*),\\s*\\+\\(([a-zA-Z0-9_ ]*,[a-zA-Z0-9_ ]*)\\),\\s*\\-([A-Z]*[a-zA-Z0-9_]*\\s*)\\)'\n\n    #track which operator(+-,-+,--) was used for which npp\n    npp_operators= {}\n    for match in re.findall(pat_pm, pi_prime):\n        if match[0] not in npp_operators:\n            npp_operators[match[0]] = {}\n        npp_operators[match[0]][1] = True\n    for match in re.findall(pat_mp, pi_prime):\n        if match[0] not in npp_operators:\n            npp_operators[match[0]] = {}\n        npp_operators[match[0]][2] = True\n    for match in re.findall(pat_mm, pi_prime):\n        if match[0] not in npp_operators:\n            npp_operators[match[0]] = {}\n        npp_operators[match[0]][3] = True\n    for match in re.findall(pat_pp, pi_prime):\n        if match[0] not in npp_operators:\n            npp_operators[match[0]] = {}\n        npp_operators[match[0]][4] = True\n\n    for match in re.findall(pat_pm2, pi_prime):\n        if match[0] not in npp_operators:\n            npp_operators[match[0]] = {}\n        npp_operators[match[0]][1] = True\n    \n    #replace found matches with asp compatible form for npp occuring in rules\n    #example: digit(0,+A,-N1) -> digit(0,1,A,N1)\n    pi_prime = re.sub( pat_pm, r'\\1(\\2,1,\\3,\\4)', pi_prime)\n    pi_prime = re.sub( pat_mp, r'\\1(\\2,2,\\3,\\4)', pi_prime)\n    pi_prime = re.sub( pat_mm, r'\\1(\\2,3,\\3,\\4)', pi_prime)\n    pi_prime = re.sub( pat_pp, r'\\1(\\2,4,\\3,\\4)', pi_prime)\n\n    pi_prime = re.sub( pat_pm2, r'\\1(\\2,1,\\3,\\4)', pi_prime)\n\n    return pi_prime, npp_operators", "\ndef compute_models_splitwise(big_M_split, query_batch_split, dmvpp, method, k, same_threshold, obj_filter, vqa_params):\n        \"\"\"\n        Computes the potential solutions and P(Q) for a split of a batch.\n        \n        @param big_M_split:\n        @param query_batch_split:\n        @param dmvpp:\n        @param method:\n        @param k: \n        @same_threshold:\n        @obj_filter:\n        @vqa_params: \n        :return: returns the potential solutions, the atom indices for the gradient computation and the probability P(Q)\n        \"\"\"\n\n        query_batch_split = query_batch_split.tolist()\n        \n\n        \n        #create a list to store the stable models into\n        model_batch_list_split = []\n        models_idx_list = []\n\n        #iterate over all queries\n        for bidx, query in enumerate(query_batch_split):\n            \n            #produce gradients for every data entry\n            dmvpp.M = big_M_split[:,bidx,:]\n            dmvpp.normalize_M()\n\n            #if len(dmvpp.parameters[ruleIdx]) == 1:\n                #    dmvpp.parameters[ruleIdx] =  [dmvpp.parameters[ruleIdx][0][0],1-dmvpp.parameters[ruleIdx][0][0]]\n\n            query, _ = replace_plus_minus_occurences(query)\n            \n\n            #exact (default): uses all/k-first stable models for WMC \n            if method == 'exact': \n                models = dmvpp.find_all_SM_under_query(query)\n\n            #uses only one most probable stable model for WMC  \n            elif method == 'most_prob':\n                models = dmvpp.find_k_most_probable_SM_under_query_noWC(query, k=1)\n\n            #uses the k most probable stables models for WMC\n            elif method == 'top_k':\n                models = dmvpp.find_k_most_probable_SM_under_query_noWC(query,k=k)\n                #gradients = dmvpp.mvppLearn(models)\n\n            #reduces the npp grounding to the most probable instantiations\n            elif method == \"same\":\n                if obj_filter is not None: #in the vqa case we want to filter out objects which are not in the image\n                    models = dmvpp.find_SM_vqa_with_same(query, k=k, threshold=same_threshold, obj_filter=obj_filter[bidx], vqa_params= vqa_params)\n                else:\n                    models = dmvpp.find_SM_with_same(query, k=k, threshold=same_threshold)\n                \n\n            #reduces the npp grounding to the most probable instantiations and then uses the top k stable models for WMC\n            elif method == \"same_top_k\":\n                models = dmvpp.find_k_most_probable_SM_with_same(query,k=k)\n            else:\n                print('Error: the method \\'%s\\' should be either \\'exact\\', \\'most_prob\\',\\'top_k\\',\\'same_top_k\\'  or \\'same\\'', method)\n    \n            #old NeurASP code - Can also be reenabled in SLASH\n            # #samples atoms from the npp until k stable models are found\n            # elif method == 'sampling':\n            #     models = dmvpp.sample_query(query, num=k)\n\n            # elif method == 'network_prediction':\n            #     models = dmvpp.find_k_most_probable_SM_under_query_noWC(query, k=1)\n            #     check = SLASH.satisfy(models[0], mvpp['program_asp'] + query)\n            #     if check:\n            #         continue\n            # elif method == 'penalty':\n            #     models = dmvpp.find_all_SM_under_query()\n            #     models_noSM = [model for model in models if not SLASH.satisfy(model, mvpp['program_asp'] + query)]\n                        \n            models_idx = dmvpp.collect_atom_idx(models)\n\n            models_idx_list.append(models_idx)\n            model_batch_list_split.append(models)\n\n        return model_batch_list_split, models_idx_list", "\n\ndef compute_vqa_splitwise(big_M_split, query_batch_split, dmvpp, obj_filter_split, k, pred_vector_size, vqa_params):\n    \"\"\"\n    Computes the gradients, stable models and P(Q) for a split of a batch.\n    \n    @param networkOutput_split:\n    @param query_batch_split:\n    @param mvpp:\n    @param n:\n    @param normalProbs:\n    @param dmvpp:\n    @param method:\n    @param opt:\n    @param k: \n    :return:returns the gradients, the stable models and the probability P(Q)\n    \"\"\"\n\n    query_batch_split = query_batch_split.tolist()\n    \n    \n    #create a list to store the target predictions into\n    pred_batch_list_split = []\n    \n    \n    #iterate over all queries\n    for bidx, query in enumerate(query_batch_split):\n        \n        dmvpp.M = big_M_split[:,bidx,:]\n        dmvpp.normalize_M()\n\n        #if len(dmvpp.parameters[ruleIdx]) == 1:\n            #    dmvpp.parameters[ruleIdx] =  [dmvpp.parameters[ruleIdx][0][0],1-dmvpp.parameters[ruleIdx][0][0]]\n\n\n\n        #query, _ = replace_plus_minus_occurences(query)\n\n        #find all targets with same\n        models = dmvpp.find_SM_vqa_with_same(query, k=k, obj_filter= obj_filter_split[bidx], threshold={\"relation\":1, \"name\":1, \"attr\":1}, train=False, vqa_params=vqa_params)\n\n        #compute probabilites of targets in the model\n        # first retrieve the object ids of targets in the model  \n        pred_vec = np.zeros(pred_vector_size)\n\n        #if we have a model\n        if len(models) > 0:\n            targets = {}\n            models = [model for model in models if model != []] #remove empty models\n\n            #go through all models and check for targets\n            for model in models:    \n                if len(model)> 1:   \n                    model_prob = 0\n                    model_has_npp_atom = False\n                    for atom in model:\n                        \n                        #we found the target atom and save the id of the object\n                        if re.match(r'target', atom):\n                            #add target id\n                            target_id = int(re.match(r'target\\(([0-9]*)\\)',atom)[1])\n                        else:\n                            #if the non target atom is a ground npp atom get its probability and add it \n                            if atom in dmvpp.ga_map:\n                                rule_idx, atom_idx = dmvpp.ga_map[atom]\n                                model_prob += dmvpp.M[rule_idx, atom_idx].log()\n                                model_has_npp_atom = True\n\n                    # only add model prob for real probability values < 1\n                    if model_has_npp_atom:\n                        #if this is the first model with a target of that id\n                        if target_id not in targets:\n                            targets[target_id] = []\n                        targets[target_id].append(model_prob)\n\n            #get the maximum value for each target object\n            for target_id in targets:\n                pred_vec[target_id] = torch.tensor(targets[target_id]).max().exp()\n        \n        #add prediction vector entry for this query to the batch list containing all queries\n        pred_batch_list_split.append(pred_vec)\n    return pred_batch_list_split", "\n\n\nclass SLASH(object):\n    def __init__(self, dprogram, networkMapping, optimizers, gpu=True):\n\n        \"\"\"\n        @param dprogram: a string for a NeurASP program\n        @param networkMapping: a dictionary maps network names to neural networks modules\n        @param optimizers: a dictionary maps network names to their optimizers\n        \n        @param gpu: a Boolean denoting whether the user wants to use GPU for training and testing\n        \"\"\"\n\n        # the neural models should be run on a GPU for better performance. The gradient computation is vectorized but in some cases it makes sense\n        # to run them on the cpu. For example in the case of the MNIST addition we create a lot of small tensors and putting them all on the gpu is significant overhead\n        # while the computation itself is comparable for small tensors. \n        # As a rule of thumb you can say that with more date or more npps it makes more sense to use the gpu for fast gradient computation.\n        self.device = torch.device('cuda' if torch.cuda.is_available() and gpu else 'cpu')\n        self.grad_comp_device = torch.device('cuda' if torch.cuda.is_available() and gpu else 'cpu')\n\n        self.dprogram = dprogram\n        self.const = {} # the mapping from c to v for rule #const c=v.\n        self.n = {} # the mapping from network name to an integer n denoting the domain size; n would be 1 or N (>=3); note that n=2 in theory is implemented as n=1\n        self.max_n = 2 # integer denoting biggest domain of all npps\n        self.e = {} # the mapping from network name to an integer e\n        self.domain = {} # the mapping from network name to the domain of the predicate in that network atom\n        self.normalProbs = None # record the probabilities from normal prob rules\n        self.networkOutputs = {}\n        self.networkGradients = {}\n        self.networkTypes = {}\n        if gpu==True:\n            self.networkMapping = {key : nn.DataParallel(networkMapping[key].to(self.device)) for key in networkMapping}\n        else:\n            self.networkMapping = networkMapping\n        self.optimizers = optimizers\n        # self.mvpp is a dictionary consisting of 4 keys: \n        # 1. 'program': a string denoting an MVPP program where the probabilistic rules generated from network are followed by other rules;\n        # 2. 'networkProb': a list of lists of tuples, each tuple is of the form (model, i ,term, j)\n        # 3. 'atom': a list of list of atoms, where each list of atoms is corresponding to a prob. rule\n        # 4. 'networkPrRuleNum': an integer denoting the number of probabilistic rules generated from network\n        self.mvpp = {'networkProb': [], 'atom': [], 'networkPrRuleNum': 0,'networkPrRuleNumWithoutBinary': 0, 'binary_rule_belongings':{}, 'program': '','networkProbSinglePred':{}}\n        self.mvpp['program'], self.mvpp['program_pr'], self.mvpp['program_asp'], self.npp_operators = self.parse(query='')\n        self.stableModels = [] # a list of stable models, where each stable model is a list\n        self.prob_q = [] # a list of probabilites for each query in the batch\n\n\n    def constReplacement(self, t):\n        \"\"\" Return a string obtained from t by replacing all c with v if '#const c=v.' is present\n\n        @param t: a string, which is a term representing an input to a neural network\n        \"\"\"\n        t = t.split(',')\n        t = [self.const[i.strip()] if i.strip() in self.const else i.strip() for i in t]\n        return ','.join(t)\n\n    def networkAtom2MVPPrules(self, networkAtom, npp_operators):\n        \"\"\"\n        @param networkAtom: a string of a neural atom\n        @param countIdx: a Boolean value denoting whether we count the index for the value of m(t, i)[j]\n        \"\"\"\n\n        # STEP 1: obtain all information\n        regex = '^(npp)\\((.+)\\((.+)\\),\\((.+)\\)\\)$'\n        out = re.search(regex, networkAtom)        \n        \n        network_type = out.group(1)\n        m = out.group(2)\n        e, inf_type, t = out.group(3).split(',', 2) # in case t is of the form t1,...,tk, we only split on the second comma\n        domain = out.group(4).split(',')\n        inf_type =int(inf_type)\n\n        #TODO how do we get the network type?\n        self.networkTypes[m] = network_type\n\n        t = self.constReplacement(t)\n        # check the value of e\n        e = e.strip()\n        e = int(self.constReplacement(e))\n        n = len(domain)\n        if n == 2:\n            n = 1\n        self.n[m] = n\n        if self.max_n <= n:\n            self.max_n = n\n        self.e[m] = e\n\n        self.domain[m] = domain\n        if m not in self.networkOutputs:\n            self.networkOutputs[m] = {}\n\n        for o in npp_operators[m]:\n            if o not in self.networkOutputs[m]:\n                self.networkOutputs[m][o] = {}\n            self.networkOutputs[m][o][t]= None    \n        \n        # STEP 2: generate MVPP rules\n        mvppRules = []\n\n        # we have different translations when n = 2 (i.e., n = 1 in implementation) or when n > 2\n        if n == 1:\n            for i in range(e):\n                rule = '@0.0 {}({}, {}, {}, {}); @0.0 {}({},{}, {}, {}).'.format(m, i, inf_type, t, domain[0], m, i, inf_type, t, domain[1])\n                prob = [tuple((m, i, inf_type, t, 0))]\n                atoms = ['{}({}, {}, {}, {})'.format(m, i,inf_type, t, domain[0]), '{}({},{},{}, {})'.format(m, i, inf_type,  t, domain[1])]\n                mvppRules.append(rule)\n                self.mvpp['networkProb'].append(prob)\n                self.mvpp['atom'].append(atoms)\n                self.mvpp['networkPrRuleNum'] += 1\n                self.mvpp['networkPrRuleNumWithoutBinary'] += 1\n\n            self.mvpp['networkProbSinglePred'][m] = True\n        elif n > 2:\n            if m == \"attr\": #TODO special case hack to map a model to multiple npp's as in the attributes in vqa\n                self.mvpp['binary_rule_belongings'][self.mvpp['networkPrRuleNum']] = (m,t)\n            for i in range(e):\n                rule = ''\n                prob = []\n                atoms = []\n                for j in range(n):\n                    atom = '{}({},{}, {}, {})'.format(m,  i, inf_type, t, domain[j])\n                    rule += '@0.0 {}({},{}, {}, {}); '.format(m, i,inf_type, t, domain[j])\n                    prob.append(tuple((m, i, inf_type, t, j)))\n                    atoms.append(atom)\n\n            mvppRules.append(rule[:-2]+'.')\n            self.mvpp['networkProb'].append(prob)\n            self.mvpp['atom'].append(atoms)\n            self.mvpp['networkPrRuleNum'] += 1\n            self.mvpp['networkPrRuleNumWithoutBinary'] += 1\n\n            \n        else:\n            print('Error: the number of element in the domain %s is less than 2' % domain)\n        return mvppRules\n\n\n\n    def parse(self, query=''):\n        dprogram = self.dprogram + query\n        # 1. Obtain all const definitions c for each rule #const c=v.\n        regex = '#const\\s+(.+)=(.+).'\n        out = re.search(regex, dprogram)\n        if out:\n            self.const[out.group(1).strip()] = out.group(2).strip()\n            \n        # 2. Generate prob. rules for grounded network atoms\n        clingo_control = clingo.Control([\"--warn=none\"])\n        \n        # 2.1 remove weak constraints and comments\n        program = re.sub(r'\\n:~ .+\\.[ \\t]*\\[.+\\]', '\\n', dprogram)\n        program = re.sub(r'\\n%[^\\n]*', '\\n', program)\n        \n        # 2.2 replace [] with ()\n        program = program.replace('[', '(').replace(']', ')')\n        \n\n        # 2.3 use MVPP package to parse prob. rules and obtain ASP counter-part\n        mvpp = MVPP(program)\n        #if mvpp.parameters and not self.normalProbs:\n        #    self.normalProbs = mvpp.parameters\n        pi_prime = mvpp.pi_prime\n\n\n\n        #2.4 parse +-Notation and add a const to flag the operation in the npp call\n        pi_prime = pi_prime.replace(' ','').replace('#const','#const ')\n        \n        #replace all occurences of the +- calls\n        pi_prime, npp_operators = replace_plus_minus_occurences(pi_prime)\n\n\n        #extend npps definitions with the operators found\n        #example: npp(digit(1,X),(0,1,2,3,4,5,6,7,8,9)):-img(X). with a +- and -- call in the program \n        # -> npp(digit(1,3,X),(0,1,2,3,4,5,6,7,8,9)):-img(X). npp(digit(1,1,X),(0,1,2,3,4,5,6,7,8,9)):-img(X).\n        #pat_npp = r'(npp\\()([a-z]*[a-zA-Z0-9_]*)(\\([0-9]*,)([A-Z]*[a-zA-Z0-9_]*\\),\\((?:[a-z0-9_]*,)*[a-z0-9_]*\\)\\))(:-[a-z][a-zA-Z0-9_]*\\([A-Z][a-zA-Z0-9_]*\\))?.'\n\n        #can match fact with arity > 1\n        pat_npp = r'(npp\\()([a-z]*[a-zA-Z0-9_]*)(\\([0-9]*,)([A-Z]*)([A-Z]*[a-zA-Z0-9_]*\\),\\((?:[a-z0-9_]*,)*[a-z0-9_]*\\)\\))(:-[a-z][a-zA-Z0-9_]*\\([A-Z][a-zA-Z0-9_,]*\\))?.'\n\n\n        def npp_sub(match):\n            \"\"\"\n            filters program for npp occurances\n            \"\"\"\n            npp_extended =\"\"\n            for o in npp_operators[match.group(2)]:\n\n                \n                if match.group(6) is None:\n                    body = \"\"\n                    var_replacement = match.group(4)\n                \n                elif re.match(r':-[a-z]*\\([0-9a-zA-Z_]*,[0-9a-zA-Z_]*\\)', match.group(6)):\n                    body = match.group(6)\n                    var_replacement = re.sub(r'(:-)([a-zA-Z0-9]*)\\(([a-z0-9A-Z]*,[a-z0-9A-Z]*)\\)', r\"\\3\" ,body)\n                else: \n                    body = match.group(6)\n                    var_replacement = match.group(4)\n\n                npp_extended = '{}{}{}{}{}{}{}{}.\\n'.format(match.group(1), match.group(2),match.group(3),o,\",\", var_replacement,match.group(5), body)+ npp_extended \n            return npp_extended\n\n        pi_prime = re.sub(pat_npp, npp_sub, pi_prime)\n\n        # 2.5 use clingo to generate all grounded network atoms and turn them into prob. rules\n        clingo_control.add(\"base\", [], pi_prime)\n        clingo_control.ground([(\"base\", [])])\n        #symbolic mappings map the constants to functions in the ASP program\n        symbols = [atom.symbol for atom in clingo_control.symbolic_atoms]\n        \n\n        #iterate over all NPP atoms and extract information for the MVPP program\n        mvppRules = [self.networkAtom2MVPPrules(str(atom),npp_operators) for atom in symbols if (atom.name == 'npp')]\n        mvppRules = [rule for rules in mvppRules for rule in rules]\n        \n        # 3. obtain the ASP part in the original NeurASP program after +- replacements\n        lines = [line.strip() for line in pi_prime.split('\\n') if line and not re.match(\"^\\s*npp\\(\", line)]\n\n        return '\\n'.join(mvppRules + lines), '\\n'.join(mvppRules), '\\n'.join(lines), npp_operators\n\n\n    @staticmethod\n    def satisfy(model, asp):\n        \"\"\"\n        Return True if model satisfies the asp program; False otherwise\n        @param model: a stable model in the form of a list of atoms, where each atom is a string\n        @param asp: an ASP program (constraints) in the form of a string\n        \"\"\"\n        asp_with_facts = asp + '\\n'\n        for atom in model:\n            asp_with_facts += atom + '.\\n'\n        clingo_control = clingo.Control(['--warn=none'])\n        clingo_control.add('base', [], asp_with_facts)\n        clingo_control.ground([('base', [])])\n        result = clingo_control.solve()\n        if str(result) == 'SAT':\n            return True\n        return False\n\n        \n    def infer(self, dataDic, query='', mvpp='',  dataIdx=None):\n        \"\"\"\n        @param dataDic: a dictionary that maps terms to tensors/np-arrays\n        @param query: a string which is a set of constraints denoting a query\n        @param mvpp: an MVPP program used in inference\n        \"\"\"\n\n        mvppRules = ''\n        facts = ''\n\n        # Step 1: get the output of each neural network\n        for m in self.networkOutputs:\n            self.networkMapping[m].eval()\n            \n            for o in self.networkOutputs[m]:\n                for t in self.networkOutputs[m][o]:\n\n                    if dataIdx is not None:\n                        dataTensor = dataDic[t][dataIdx]\n                    else:\n                        dataTensor = dataDic[t]\n                    \n                    self.networkOutputs[m][o][t] = self.networkMapping[m](dataTensor).view(-1).tolist()\n\n        for ruleIdx in range(self.mvpp['networkPrRuleNum']):\n            \n            probs = [self.networkOutputs[m][inf_type][t][i*self.n[m]+j] for (m, i, inf_type, t, j) in self.mvpp['networkProb'][ruleIdx]]\n            #probs = [self.networkOutputs[m][inf_type][t][0][i*self.n[m]+j] for (m, i, inf_type, t, j) in self.mvpp['networkProb'][ruleIdx]]\n\n            if len(probs) == 1:\n                mvppRules += '@{:.15f} {}; @{:.15f} {}.\\n'.format(probs[0], self.mvpp['atom'][ruleIdx][0], 1 - probs[0], self.mvpp['atom'][ruleIdx][1])\n            else:\n                tmp = ''\n                for atomIdx, prob in enumerate(probs):\n                    tmp += '@{:.15f} {}; '.format(prob, self.mvpp['atom'][ruleIdx][atomIdx])\n                mvppRules += tmp[:-2] + '.\\n'\n        \n        # Step 3: find an optimal SM under query\n        dmvpp = MVPP(facts + mvppRules + mvpp)\n        return dmvpp.find_k_most_probable_SM_under_query_noWC(query, k=1)\n\n\n    def split_network_outputs(self, big_M, obj_filter, query_batch, p_num):\n        if len(query_batch)< p_num:\n            if type(query_batch) == tuple:\n                p_num = len(query_batch)\n            else:\n                p_num=query_batch.shape[0]\n\n        #partition dictionary for different processors                \n        \n        splits = np.arange(0, int(p_num))\n        partition = int(len(query_batch) / p_num)\n        partition_mod = len(query_batch) % p_num \n        partition = [partition]*p_num\n        partition[-1]+= partition_mod\n        \n        query_batch_split = np.split(query_batch,np.cumsum(partition))[:-1]\n\n        if obj_filter is not  None:\n            obj_filter = np.array(obj_filter)\n            obj_filter_split = np.split(obj_filter, np.cumsum(partition))[:-1]\n        else:\n            obj_filter_split = None \n\n        big_M_splits = torch.split(big_M, dim=1, split_size_or_sections=partition)\n\n\n        return big_M_splits, query_batch_split, obj_filter_split, p_num\n\n\n \n    def learn(self, dataset_loader, epoch, method='exact', opt=False, k_num=0, p_num=1, slot_net=None, hungarian_matching=False, vqa=False, marginalisation_masks=None,  writer=None, same_threshold=0.99, batched_pass=False, vqa_params=None):\n        \n        \"\"\"\n        @param dataset_loader: a pytorch dataloader object returning a dictionary e.g {im1: [bs,28,28], im2:[bs,28,28]} and the queries\n        @param epoch: an integer denoting the current epoch\n        @param method: a string in {'exact', 'same',''} denoting whether the gradients are computed exactly or by sampling\n        @param opt: stands for optimal -> if true we select optimal stable models\n        @param k_num: select k stable models, default k_num=0 means all stable models\n        @param p_num: a positive integer denoting the number of processor cores to be used during the training\n        @param slot_net: a slot attention network for the set prediction experiment mapping from an image to slots\n        @param vqa: VQA option for the vqa experiment. Enables VQA specific datahandling\n        @param hungarian_matching: boolean denoting wherever the matching is done in the LP or if the results of the hungarian matching should be integrated in the LP\n        @param marginalisation_masks: a list entailing one marginalisation mask for each batch of dataList\n        @param writer: Tensorboard writer for plotting metrics\n        @param same_threshold: Threshold when method = same or same_top_k. Can be either a scalar value or a dict mapping treshold to networks m {\"digit\":0.99}\n        @param batched_pass: boolean to forward all t belonging to the same m in one pass instead of t passes\n        \"\"\"\n        \n        assert p_num >= 1 and isinstance(p_num, int), 'Error: the number of processors used should greater equals one and a natural number'\n\n        # get the mvpp program by self.mvpp\n        #old NeurASP code. Can be reanbled if needed\n        #if method == 'network_prediction':\n        #    dmvpp = MVPP(self.mvpp['program_pr'], max_n= self.max_n)\n        #elif method == 'penalty':\n        #    dmvpp = MVPP(self.mvpp['program_pr'], max_n= self.max_n)\n\n        #add the npps to the program now or later\n        if method == 'same' or method == 'same_top_k':\n            dmvpp = MVPP(self.mvpp['program'], prob_ground=True , binary_rule_belongings= self.mvpp['binary_rule_belongings'], max_n= self.max_n)\n        else:\n            dmvpp = MVPP(self.mvpp['program'], max_n= self.max_n)\n\n    \n        # we train all neural network models\n        for m in self.networkMapping:\n            self.networkMapping[m].train() #torch training mode\n            self.networkMapping[m].module.train() #torch training mode\n\n    \n        total_loss = []\n        sm_per_batch_list = []\n        #store time per epoch\n        forward_time = []\n        asp_time = []\n        gradient_time  = []\n        backward_time = []\n        \n        #iterate over all batches\n        pbar = tqdm(total=len(dataset_loader))\n        for batch_idx, (batch) in enumerate(dataset_loader):      \n            start_time = time.time()\n\n            if hungarian_matching:\n                data_batch, query_batch, obj_enc_batch = batch\n            elif vqa:\n                data_batch, query_batch, obj_filter, _ = batch\n            else:\n                data_batch, query_batch = batch\n\n\n                    \n            # If we have marginalisation masks, than we have to pick one for the batch\n            if marginalisation_masks is not None:\n                marg_mask = marginalisation_masks[i]\n            else:\n                marg_mask = None\n            \n            #STEP 0: APPLY SLOT ATTENTION TO TRANSFORM IMAGE TO SLOTS\n            #we have a map which is : im: im_data\n            #we want a map which is : s1: slot1_data, s2: slot2_data, s3: slot3_data\n            if slot_net is not None:\n                slot_net.train()\n                dataTensor_after_slot = slot_net(data_batch['im'].to(self.device)) #forward the image\n\n                #add the slot outputs to the data batch\n                for slot_num in range(slot_net.n_slots):\n                    key = 's'+str(slot_num+1)\n                    data_batch[key] = dataTensor_after_slot[:,slot_num,:]\n                        \n\n                \n            #data is a dictionary. we need to edit its key if the key contains a defined const c\n            #where c is defined in rule #const c=v.\n            data_batch_keys = list(data_batch.keys())              \n            for key in data_batch_keys:\n                data_batch[self.constReplacement(key)] = data_batch.pop(key)\n                        \n            \n            # Step 1: get the output of each network and initialize the gradients\n            networkOutput = {}\n            networkLLOutput = {}\n\n            #iterate over all networks            \n            for m in self.networkOutputs:\n                if m not in networkOutput:\n                    networkOutput[m] = {}\n\n\n                #iterate over all output types and forwarded the input t trough the network\n                networkLLOutput[m] = {}\n                for o in self.networkOutputs[m]: \n                    if o not in networkOutput[m]:\n                        networkOutput[m][o] = {}\n                                        \n                    \n                    #one forward pass to get the outputs\n                    if self.networkTypes[m] == 'npp':\n                        \n                        #forward all t belonging to the same m in one pass instead of t passes\n                        if batched_pass:\n\n                            #collect all inputs t for every network m \n                            dataTensor = [data_batch.get(key).to(device=self.device) for key in self.networkOutputs[m][o].keys()]\n                            dataTensor = torch.cat(dataTensor)\n\n                            len_keys = len(query_batch)\n\n\n                            output = self.networkMapping[m].forward(\n                                                            dataTensor.to(self.device),\n                                                            marg_idx=marg_mask,\n                                                            type=o)\n\n                            outputs = output.split(len_keys)\n\n                            networkOutput[m][o] = {**networkOutput[m][o], **dict(zip(self.networkOutputs[m][o].keys(), outputs))}\n                        \n                        else:\n                            for t in self.networkOutputs[m][o]:\n\n                                dataTensor = data_batch[t]                                    \n                                \n                                #we have a list of data elements but want a Tensor of the form [batchsize,...]\n                                if isinstance(dataTensor, list):\n                                    dataTensor = torch.stack(dataTensor).squeeze(dim=1) #TODO re-enable\n\n                                #foward the data\n                                networkOutput[m][o][t] = self.networkMapping[m].forward(\n                                                                                dataTensor.to(self.device),\n                                                                                marg_idx=marg_mask,\n                                                                                type=o)\n\n                                #if the network predicts only one probability we add a placeholder for the false class\n                                if m in self.mvpp['networkProbSinglePred']:\n                                    networkOutput[m][o][t] = torch.stack((networkOutput[m][o][t], torch.zeros_like(networkOutput[m][o][t])), dim=-1)\n\n\n\n                    #store the outputs of the neural networks as a class variable\n                    self.networkOutputs[m][o] = networkOutput[m][o] #this is of shape [first batch entry, second batch entry,...]\n\n            \n            #match the outputs with the hungarian matching and create a predicate to add to the logic program\n            #NOTE: this is a hacky solution which leaves open the question wherever we can have neural mappings in our logic program\n            if hungarian_matching is True:\n                \n                obj_batch = {}\n                if \"shade\" in networkOutput:\n                    obj_batch['color'] = obj_enc_batch[:,:,0:9] # [500, 4,20] #[c,c,c,c,c,c,c,c,c , s,s,s,s , h,h,h, z,z,z, confidence]\n                    obj_batch['shape'] = obj_enc_batch[:,:,9:13]\n                    obj_batch['shade'] = obj_enc_batch[:,:,13:16]\n                    obj_batch['size'] = obj_enc_batch[:,:,16:19]\n\n                    concepts = ['color', 'shape', 'shade','size']\n                    slots = ['s1', 's2', 's3','s4']\n                    num_obs = 4\n                \n                else:        \n                    obj_batch['size'] = obj_enc_batch[:,:,0:3] # [500, 4,20] #[c,c,c,c,c,c,c,c,c , s,s,s,s , h,h,h, z,z,z, confidence]\n                    obj_batch['material'] = obj_enc_batch[:,:,3:6]\n                    obj_batch['shape'] = obj_enc_batch[:,:,6:10]\n                    obj_batch['color'] = obj_enc_batch[:,:,10:19]\n\n                    concepts = ['color', 'shape', 'material','size']\n                    slots = ['s1', 's2', 's3','s4','s5','s6','s7','s8','s9','s10']\n                    num_obs = 10\n                    \n                kl_cost_matrix = torch.zeros((num_obs,num_obs,len(query_batch)))\n\n                #build KL cost matrix\n                for obj_id in range(0,num_obs):\n                    for slot_idx, slot in enumerate(slots):\n                        summed_kl = 0\n                        for concept in concepts:\n                            b = obj_batch[concept][:,obj_id].type(torch.FloatTensor)\n                            a = networkOutput[concept][1][slot].detach().cpu()\n                            summed_kl += torch.cdist(a[:,None,:],b[:,None,:]).squeeze()\n\n                        kl_cost_matrix[obj_id, slot_idx] = summed_kl \n\n                kl_cost_matrix = np.einsum(\"abc->cab\", kl_cost_matrix.cpu().numpy())\n\n\n                indices = np.array(\n                list(map(scipy.optimize.linear_sum_assignment, kl_cost_matrix)))\n                \n\n                def slot_name_comb(x):\n                    return ''.join([\"slot_name_comb(o{}, s{}). \".format(i[0]+1, i[1]+1)  for i in x])\n\n                assignments = np.array(list(map(slot_name_comb, np.einsum(\"abc->acb\",indices))))\n                query_batch = list(map(str.__add__, query_batch, assignments))\n\n            \n            #stack all nn outputs in matrix M\n            big_M = torch.zeros([ len(self.mvpp['networkProb']),len(query_batch), self.max_n], device=self.grad_comp_device)\n\n            c = 0\n            for m in networkOutput:\n                for o in networkOutput[m]:\n                    for t in networkOutput[m][o]:\n                        big_M[c,:, :networkOutput[m][o][t].shape[1]]= networkOutput[m][o][t].detach().to('cpu')\n                        c+=1\n\n            \n            #set all matrices in the dmvpp class to the cpu for model computation\n            dmvpp.put_selection_mask_on_device('cpu')\n            big_M = big_M.to(device='cpu')\n            dmvpp.M =  dmvpp.M.to(device='cpu')\n\n            #normalize the SLASH copy of M \n            #big_M = normalize_M(big_M, dmvpp.non_binary_idx, dmvpp.selection_mask)\n\n\n            step1 = time.time()\n            forward_time.append(step1 - start_time)\n                        \n            #### Step 2: compute stable models and the gradients\n\n            #we split the network outputs such that we can put them on different processes\n            if vqa: \n                big_M_splits, query_batch_split, obj_filter_split, p_num = self.split_network_outputs(big_M, obj_filter, query_batch, p_num)\n            else:\n                big_M_splits, query_batch_split, _, p_num = self.split_network_outputs(big_M, None, query_batch, p_num)\n                obj_filter_split = [None] * len(query_batch_split)\n\n\n\n            split_outputs = Parallel(n_jobs=p_num,backend='loky')( #backend='loky')(\n                delayed(compute_models_splitwise)\n                (\n                    big_M_splits[i].detach(), query_batch_split[i],\n                        dmvpp, method, k_num, same_threshold, obj_filter_split[i], vqa_params\n                )\n                        for i in range(p_num))\n\n            del big_M_splits\n\n            #concatenate potential solutions, the atom indices and query p(Q) from all splits back into a single batch\n            model_batch_list_splits = []\n            models_idx_list = []\n\n            #collect the models and model computation times\n            for i in range(p_num):\n                model_batch_list_splits.extend(split_outputs[i][0])\n                models_idx_list.extend(split_outputs[i][1])   #batch, model, atoms\n\n            #amount of Stable models used for gradient computations per batch\n            sm_per_batch = np.sum([ len(sm_batch)  for splits in model_batch_list_splits for sm_batch in splits ])\n            sm_per_batch_list.append(sm_per_batch)\n\n            #save stable models\n            try:\n                model_batch_list = np.concatenate(model_batch_list_splits)\n\n                self.stableModels = model_batch_list\n\n            except ValueError as e:\n                pass\n                #print(\"fix later\")\n                #print(e)\n                #for i in range(0, len(model_batch_list_splits)):\n                #    print(\"NUM:\",i)\n                #    print(model_batch_list_splits[i])\n            \n\n            step2 = time.time()\n            asp_time.append(step2 - step1)\n\n            #compute gradients\n            dmvpp.put_selection_mask_on_device(self.grad_comp_device)\n            big_M = big_M.to(device=self.grad_comp_device)\n\n            gradients_batch_list = []\n            for bidx in count(start=0, step=1):\n                if bidx < model_batch_list_splits.__len__():\n                    dmvpp.M = big_M[:,bidx,:]\n                    dmvpp.normalize_M()\n                    gradients_batch_list.append(dmvpp.mvppLearn(model_batch_list_splits[bidx], models_idx_list[bidx], self.grad_comp_device))\n                else:\n                    break\n                \n            del big_M\n\n            #stack all gradients\n            gradient_tensor = torch.stack(gradients_batch_list)\n\n            #store the gradients, the stable models and p(Q) of the last batch processed\n            self.networkGradients = gradient_tensor\n\n\n            # Step 3: update parameters in neural networks\n            step3 = time.time()\n            gradient_time.append(step3 - step2)\n\n            \n            networkOutput_stacked = torch.zeros([gradient_tensor.shape[1], gradient_tensor.shape[0], gradient_tensor.shape[2]], device=torch.device('cuda:0'))\n            gradient_tensor = gradient_tensor.swapaxes(0,1)\n            org_idx = (gradient_tensor.sum(dim=2) != 0)#.flatten(0,1)\n\n            #add all NN outputs which have a gradient into tensor for backward pass\n            c = 0\n            for m in networkOutput:\n                for o in networkOutput[m]:\n                    for t in networkOutput[m][o]:\n                        for bidx in range(0,org_idx.shape[1]): #iterate over batch\n                            if org_idx[c, bidx]:\n                                networkOutput_stacked[c,bidx, :networkOutput[m][o][t].shape[1]]= networkOutput[m][o][t][bidx]\n                        c+=1\n\n\n            #multiply every probability with its gradient\n            gradient_tensor.requires_grad=True\n            result = torch.einsum(\"abc, abc -> abc\", gradient_tensor.to(device='cuda'),networkOutput_stacked)\n\n            not_used_npps = org_idx.sum() / (result.shape[0]* result.shape[1])\n            result = result[result.abs() != 0 ].sum()\n\n            #in the case vqa case we need to only use the npps over existing objects\n            if vqa:\n                total_obj = 0 \n                for of in obj_filter:\n                    total_obj += 2* of + of * (of-1)\n                not_used_npps = org_idx.sum() / total_obj\n\n            \n            #get the number of discrete properties, e.g. sum all npps times the atoms entailing the npps \n            sum_discrete_properties = sum([len(listElem) for listElem in self.mvpp['atom']]) * len(query_batch)\n            sum_discrete_properties = torch.Tensor([sum_discrete_properties]).to(device=self.device)\n\n            #scale to actualy used npps\n            sum_discrete_properties = sum_discrete_properties * not_used_npps\n\n            #get the mean over the sum of discrete properties\n            result_ll = result / sum_discrete_properties\n\n            #backward pass\n            #for gradient descent we minimize the negative log likelihood    \n            result_nll = -result_ll\n            \n            #reset optimizers\n            for  m in self.optimizers:\n                self.optimizers[m].zero_grad()\n\n\n            #append the loss value\n            total_loss.append(result_nll.cpu().detach().numpy())\n            \n            #backward pass\n            result_nll.backward(retain_graph=True)\n\n\n            \n            #apply gradients with each optimizer\n            for m in self.optimizers:\n                self.optimizers[m].step()\n\n\n            last_step = time.time()\n            backward_time.append(last_step - step3)\n\n                    \n            if writer is not None:\n                writer.add_scalar('train/loss_per_batch', result_nll.cpu().detach().numpy(), batch_idx+epoch*len(dataset_loader))\n                writer.add_scalar('train/sm_per_batch', sm_per_batch, batch_idx+epoch*len(dataset_loader))\n\n            pbar.update()\n        pbar.close()\n        \n        if writer is not None:\n\n            writer.add_scalar('train/forward_time', np.sum(forward_time), epoch)\n            writer.add_scalar('train/asp_time', np.sum(asp_time), epoch)\n            writer.add_scalar('train/gradient_time', np.sum(gradient_time), epoch)\n            writer.add_scalar('train/backward_time', np.sum(backward_time), epoch)\n            writer.add_scalar('train/sm_per_epoch', np.sum(sm_per_batch_list), epoch)\n\n\n        print(\"avg loss over batches:\", np.mean(total_loss))\n        print(\"1. forward time: \", np.sum(forward_time))\n        print(\"2. asp time:\", np.sum(asp_time))\n        print(\"3. gradient time:\", np.sum(gradient_time))\n        print(\"4. backward time: \", np.sum(backward_time))\n        print(\"SM processed\", np.sum(sm_per_batch_list))\n\n\n\n        return np.mean(total_loss), forward_time, asp_time, gradient_time, backward_time, sm_per_batch_list\n\n\n            \n\n    def testNetwork(self, network, testLoader, ret_confusion=False):\n        \"\"\"\n        Return a real number in [0,100] denoting accuracy\n        @network is the name of the neural network or probabilisitc circuit to check the accuracy. \n        @testLoader is the input and output pairs.\n        \"\"\"\n        self.networkMapping[network].eval()\n        # check if total prediction is correct\n        correct = 0\n        total = 0\n        # check if each single prediction is correct\n        singleCorrect = 0\n        singleTotal = 0\n        \n        #list to collect targets and predictions for confusion matrix\n        y_target = []\n        y_pred = []\n        with torch.no_grad():\n            for data, target in testLoader:\n                                \n                output = self.networkMapping[network](data.to(self.device))\n                if self.n[network] > 2 :\n                    pred = output.argmax(dim=-1, keepdim=True) # get the index of the max log-probability\n                    target = target.to(self.device).view_as(pred)\n                    \n                    correctionMatrix = (target.int() == pred.int()).view(target.shape[0], -1)\n                    y_target = np.concatenate( (y_target, target.int().flatten().cpu() ))\n                    y_pred = np.concatenate( (y_pred , pred.int().flatten().cpu()) )\n                    \n                    \n                    correct += correctionMatrix.all(1).sum().item()\n                    total += target.shape[0]\n                    singleCorrect += correctionMatrix.sum().item()\n                    singleTotal += target.numel()\n                else: \n                    pred = np.array([int(i[0]<0.5) for i in output.tolist()])\n                    target = target.numpy()\n                    \n                    \n                    correct += (pred.reshape(target.shape) == target).sum()\n                    total += len(pred)\n        accuracy = correct / total\n\n        if self.n[network] > 2:\n            singleAccuracy = singleCorrect / singleTotal\n        else:\n            singleAccuracy = 0\n        \n        if ret_confusion:\n            confusionMatrix = confusion_matrix(np.array(y_target), np.array(y_pred))\n            return accuracy, singleAccuracy, confusionMatrix\n\n        return accuracy, singleAccuracy\n    \n    # We interprete the most probable stable model(s) as the prediction of the inference mode\n    # and check the accuracy of the inference mode by checking whether the query is satisfied by the prediction\n    def testInferenceResults(self, dataset_loader):\n        \"\"\" Return a real number in [0,1] denoting the accuracy\n        @param dataset_loader: a dataloader object loading a dataset to test on\n        \"\"\"\n\n        correct = 0\n        len_dataset = 0\n        #iterate over batch\n        for data_batch, query_batch in dataset_loader:\n            len_dataset += len(query_batch)\n\n            #iterate over each entry in batch\n            for dataIdx in range(0, len(query_batch)):\n                models = self.infer(data_batch, query=':- mistake.', mvpp=self.mvpp['program_asp'],  dataIdx= dataIdx)\n\n                query,_ =  replace_plus_minus_occurences(query_batch[dataIdx])\n\n                for model in models:\n                    if self.satisfy(model, query):\n                        correct += 1\n                        break\n\n        accuracy = 100. * correct / len_dataset\n        return accuracy\n\n\n    def testConstraint(self, dataset_loader, mvppList):\n        \"\"\"\n        @param dataList: a list of dictionaries, where each dictionary maps terms to tensors/np-arrays\n        @param queryList: a list of strings, where each string is a set of constraints denoting a query\n        @param mvppList: a list of MVPP programs (each is a string)\n        \"\"\"\n\n        # we evaluate all nerual networks\n        for func in self.networkMapping:\n            self.networkMapping[func].eval()\n\n        # we count the correct prediction for each mvpp program\n        count = [0]*len(mvppList)\n\n\n        len_data = 0\n        for data_batch, query_batch in dataset_loader:\n            len_data += len(query_batch)\n\n            # data is a dictionary. we need to edit its key if the key contains a defined const c\n            # where c is defined in rule #const c=v.\n            data_batch_keys = list(data_batch.keys())\n            for key in data_batch_keys:\n                data_batch[self.constReplacement(key)] = data_batch.pop(key)\n\n            # Step 1: get the output of each neural network\n\n            for m in self.networkOutputs:\n                for o in self.networkOutputs[m]: #iterate over all output types and forwarded the input t trough the network\n                    for t in self.networkOutputs[m][o]:\n                        self.networkOutputs[m][o][t] = self.networkMapping[m].forward(\n                                                                                    data_batch[t].to(self.device),\n                                                                                    marg_idx=None,\n                                                                                    type=o)\n\n            # Step 2: turn the network outputs into a set of ASP facts            \n            aspFactsList = []\n            for bidx in range(len(query_batch)):\n                aspFacts = ''\n                for ruleIdx in range(self.mvpp['networkPrRuleNum']):\n                    \n                    #get the network outputs for the current element in the batch and put it into the correct rule\n                    probs = [self.networkOutputs[m][inf_type][t][bidx][i*self.n[m]+j] for (m, i, inf_type, t, j) in self.mvpp['networkProb'][ruleIdx]]\n\n                    if len(probs) == 1:\n                        atomIdx = int(probs[0] < 0.5) # t is of index 0 and f is of index 1\n                    else:\n                        atomIdx = probs.index(max(probs))\n                    aspFacts += self.mvpp['atom'][ruleIdx][atomIdx] + '.\\n'\n                aspFactsList.append(aspFacts)\n        \n\n            # Step 3: check whether each MVPP program is satisfied\n            for bidx in range(len(query_batch)):\n                for programIdx, program in enumerate(mvppList):\n\n                    query,_ =  replace_plus_minus_occurences(query_batch[bidx])\n                    program,_ = replace_plus_minus_occurences(program)\n\n                    # if the program has weak constraints\n                    if re.search(r':~.+\\.[ \\t]*\\[.+\\]', program) or re.search(r':~.+\\.[ \\t]*\\[.+\\]', query):\n                        choiceRules = ''\n                        for ruleIdx in range(self.mvpp['networkPrRuleNum']):\n                            choiceRules += '1{' + '; '.join(self.mvpp['atom'][ruleIdx]) + '}1.\\n'\n\n\n                        mvpp = MVPP(program+choiceRules)\n                        models = mvpp.find_all_opt_SM_under_query_WC(query=query)\n                        models = [set(model) for model in models] # each model is a set of atoms\n                        targetAtoms = aspFacts[bidx].split('.\\n')\n                        targetAtoms = set([atom.strip().replace(' ','') for atom in targetAtoms if atom.strip()])\n                        if any(targetAtoms.issubset(model) for model in models):\n                            count[programIdx] += 1\n                    else:\n                        mvpp = MVPP(aspFacts[bidx] + program)\n                        if mvpp.find_one_SM_under_query(query=query):\n                            count[programIdx] += 1\n        for programIdx, program in enumerate(mvppList):\n            print('The accuracy for constraint {} is {}({}/{})'.format(programIdx+1, float(count[programIdx])/len_data,float(count[programIdx]),len_data ))\n\n            \n            \n    \n    def forward_slot_attention_pipeline(self, slot_net, dataset_loader):\n        \"\"\"\n        Makes one forward pass trough the slot attention pipeline to obtain the probabilities/log likelihoods for all classes for each object. \n        The pipeline includes  the SlotAttention module followed by probabilisitc circuits for probabilites for the discrete properties.\n        @param slot_net: The SlotAttention module\n        @param dataset_loader: Dataloader containing a shapeworld/clevr dataset to be forwarded\n        \"\"\"\n        with torch.no_grad():\n\n            probabilities = {}  # map to store all output probabilities(posterior)\n            slot_map = {} #map to store all slot module outputs\n            \n            for data_batch, _,_ in dataset_loader:\n                \n                #forward img to get slots\n                dataTensor_after_slot = slot_net(data_batch['im'].to(self.device))#SHAPE [BS,SLOTS, SLOTSIZE]\n                \n                #dataTensor_after_slot has shape [bs, num_slots, slot_vector_length]\n                _, num_slots ,_ = dataTensor_after_slot.shape \n\n                \n                for sdx in range(0, num_slots):\n                    slot_map[\"s\"+str(sdx)] = dataTensor_after_slot[:,sdx,:]\n                \n\n                #iterate over all slots and forward them through all nets (shape + color + ... )\n                for key in slot_map:\n                    if key not in probabilities:\n                        probabilities[key] = {}\n                    \n                    for network in self.networkMapping:\n                        posterior= self.networkMapping[network].forward(slot_map[key])#[BS, num_discrete_props]\n                        if network not in probabilities[key]:\n                            probabilities[key][network] = posterior\n                        else: \n                            probabilities[key][network] = torch.cat((probabilities[key][network], posterior))\n\n\n            return probabilities\n\n    def get_recall(self, logits, labels,  topk=3):\n\n\n        # Calculate the recall\n        _, pred_idx = logits.topk(topk, 1, True, True) #get idx of the biggest k values in the prediction tensor\n\n\n        #gather gets the elements from a given indices tensor. Here we get the elements at the same positions from our (top5) predictions\n        #we then sum all entries up along the axis and therefore count the top-5 entries in the labels tensors at the prediction position indices\n        correct = torch.sum(labels.gather(1, pred_idx), dim=1)\n\n        #now we sum up all true labels. We clamp them to be maximum top5\n        correct_label = torch.clamp(torch.sum(labels, dim = 1), 0, topk)\n\n        #we now can compare if the number of correctly found top-5 labels on the predictions vector is the same as on the same positions as the GT vector\n        #if the num of gt labels is zero (question has no answer) then we get a nan value for the div by 0 -> we replace this with recall 1 \n        recall = torch.mean(torch.nan_to_num(correct / correct_label,1)).item()\n\n        return recall\n\n\n\n    def testVQA(self, test_loader, p_num = 1, k=5, vqa_params= None):\n        \"\"\"\n        @param test_loader: a dataloader object \n        @param p_num: integer denoting the number of processes to split the batches on\n        @param k: denotes how many targets pred are used for the recall metric\n        \"\"\"\n\n        start_test = time.time()\n        dmvpp = MVPP(self.mvpp['program'],prob_ground=True,binary_rule_belongings= self.mvpp['binary_rule_belongings'])\n        networkOutput = {}\n\n        # we train all neural network models\n        for m in self.networkMapping:\n            self.networkMapping[m].eval() #torch training mode\n            self.networkMapping[m].module.eval() #torch training mode\n\n        pred_vector_list = []\n        gt_vector_list = []\n\n        #iterate over all batches\n        #pbar = tqdm(total=len(test_loader))\n        for batch_idx, (batch) in enumerate(test_loader):            \n    \n            data_batch, query_batch, obj_filter, target = batch\n\n            # Step 1: get the output of each neural network\n            for m in self.networkOutputs:\n                if m not in networkOutput:\n                    networkOutput[m] = {}\n\n                #iterate over all inference types o and forwarded the input t trough the network\n                for o in self.networkOutputs[m]: \n                    if o not in networkOutput[m]:\n                        networkOutput[m][o] = {}\n                                        \n                    if self.networkTypes[m] == 'npp':\n\n                        #collect all inputs t for every network m \n                        dataTensor = [data_batch.get(key).to(device=self.device) for key in self.networkOutputs[m][o].keys()]\n                        dataTensor = torch.cat(dataTensor)\n                        len_keys = len(query_batch)\n\n                        output = self.networkMapping[m].forward(\n                                                        dataTensor.to(self.device),\n                                                        marg_idx=None,\n                                                        type=o)\n\n                        outputs = output.split(len_keys)\n                        networkOutput[m][o] = {**networkOutput[m][o], **dict(zip(self.networkOutputs[m][o].keys(), outputs))}\n\n\n\n            #stack all nn outputs in matrix M\n            big_M = torch.zeros([ len(self.mvpp['networkProb']),len(query_batch), self.max_n], device=self.grad_comp_device)\n\n\n            c = 0\n            for m in networkOutput:\n                for o in networkOutput[m]:\n                    for t in networkOutput[m][o]:\n                        big_M[c,:, :networkOutput[m][o][t].shape[1]]= networkOutput[m][o][t].detach().to(self.grad_comp_device)\n                        c+=1\n\n\n            big_M_splits, query_batch_split, obj_filter_split, p_num = self.split_network_outputs(big_M, obj_filter, query_batch, p_num)\n\n\n            # Step 2: compute stable models using the probabilities\n            # we get a list of pred vectors where each index represents if the object is a target and its corresponding target probability \n\n            dmvpp.put_selection_mask_on_device('cpu')\n            pred_vector_list_split = Parallel(n_jobs=p_num,backend='loky')( #backend='loky')(\n                delayed(compute_vqa_splitwise)\n                (\n                    big_M_splits[i].detach().cpu(), query_batch_split[i],\n                        dmvpp,  obj_filter_split[i], k, target.shape[1], vqa_params\n                )\n                        for i in range(p_num))\n            \n            #stack together the target vectors from the different procceses\n            pred_vector_list.append(torch.stack([torch.tensor(i) for p in pred_vector_list_split for i in p ]))\n            gt_vector_list.append(target.clone().detach())\n            #pbar.update()\n        #pbar.close()\n\n\n        #stack together the target vectors from the different batches\n        pred_vector = torch.cat(pred_vector_list, dim=0)\n        gt_vector = torch.cat(gt_vector_list, dim=0)\n\n\n\n        #compute the recall\n        recall = self.get_recall(pred_vector, gt_vector, 5)\n        #print(\"RECALL\",recall)\n\n        return recall, time.time() - start_test", ""]}
{"filename": "src/SLASH/mvpp.py", "chunked_list": ["\"\"\"\nThe source code is based on:\nNeurASP: Embracing Neural Networks into Answer Set Programming\nZhun Yang, Adam Ishay, Joohyung Lee. Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence\nMain track. Pages 1755-1762.\n\nhttps://github.com/azreasoners/NeurASP\n\"\"\"\n\nfrom doctest import FAIL_FAST", "\nfrom doctest import FAIL_FAST\nimport itertools\nfrom itertools import count\nimport math\nimport os.path\nimport os\n\nimport re\nimport sys", "import re\nimport sys\nimport time\n\nimport clingo\nimport numpy as np\nimport torch\n\nclass MVPP(object):                 \n    def __init__(self, program, k=1, eps=0.000001, prob_ground = False, binary_rule_belongings={}, max_n=0):\n        self.k = k\n        self.eps = eps\n\n        # each element in self.pc is a list of atoms (one list for one prob choice rule)\n        self.pc = []\n        # each element in self.parameters is a list of probabilities\n        self.parameters = []\n        # each element in self.learnable is a list of Boolean values\n        self.learnable = []\n        # self.asp is the ASP part of the LPMLN program\n        self.asp = \"\"\n        # self.pi_prime is the ASP program \\Pi' defined for the semantics\n        self.pi_prime = \"\"\n        # self.remain_probs is a list of probs, each denotes a remaining prob given those non-learnable probs\n        self.remain_probs = []\n        # self.ga_map is a dictionary containing the map ground atom <-> (ruleId, atomId).\n        # Thereby, ruleId is a NPP call and atomId is graound atom (outcome) index in a NPP call.\n        self.ga_map = {}\n        # self.M is the 2d tensor containig probabilities for all NPP calls\n        self.M = torch.empty((2,3), dtype=torch.float32)\n        self.max_n = max_n\n        self.device = 'cpu'\n\n\n        self.binary_rule_belongings = binary_rule_belongings\n        self.pc, self.parameters, self.learnable, self.asp, self.pi_prime, self.remain_probs, self.ga_map = self.parse(program, prob_ground)\n\n        self.get_non_binary_idx()\n        if self.parameters != []:\n            self.create_selection_mask()\n        \n    def debug_mvpp(self):\n        print(\"pc\",self.pc)\n        print(\"params\", self.parameters)\n        print(\"learnable\", self.learnable)\n        print(\"asp\", self.asp)\n        print(\"pi_prime\", self.pi_prime)\n        \n    def get_non_binary_idx(self):\n        non_binary_idx = []\n        \n        #from the idx to delete generate the ones to keep\n        for i in count(start=0, step=1):\n            if i < len(self.parameters):\n                if i not in self.binary_rule_belongings:\n                    non_binary_idx.append(i)\n            else:\n                break\n        self.non_binary_idx = non_binary_idx\n\n\n    def create_selection_mask(self):\n        selection_mask = torch.zeros([self.parameters.__len__(), max(par.__len__() for par in self.parameters)], dtype=torch.bool, device='cpu')\n        for rule_idx, rule in enumerate(self.parameters):\n            selection_mask[rule_idx,0:len(self.parameters[rule_idx])] = True\n        self.selection_mask = selection_mask\n\n    def put_selection_mask_on_device(self, device):\n        self.selection_mask = self.selection_mask.to(device=device)\n\n\n    def parse(self, program, prob_ground):\n\n        pc = []\n        parameters = []\n        learnable = []\n        asp = \"\"\n        pi_prime = \"\"\n        remain_probs = []\n        ga_map = {}\n        npp_choices = {}\n        npp_choices_inftype_sorted = []\n\n        lines = []\n        # if program is a file\n        if os.path.isfile(program):\n            with open(program, 'r') as program:\n                lines = program.readlines()\n        # if program is a string containing all rules of an LPMLN program\n\n        elif type(program) is str and re.sub(r'\\n%[^\\n]*', '\\n', program).strip().endswith(('.', ']')):\n            lines = program.split('\\n')\n        else:\n            print(program)\n            print(\"Error! The MVPP program {} is not valid.\".format(program))\n            sys.exit()\n\n        #iterate over all lines\n        # 1. build the choice rules 1{...}1\n        # 2. store which npp values are learnable\n        for line in lines:\n            \n            \n\n            #if re.match(r\".*[0-1]\\.?[0-9]*\\s.*;.*\", line): \n            if re.match(r'.*[0-1]\\.[0-9]*\\s.*(;|\\.).*', line):\n                #out = re.search(r'@[0-9]*\\.[0-9]*([a-z][a-zA-Z0-9_]*)\\(([0-9]*),([0-9]),([a-z]*[a-zA-Z0-9]*)', line.replace(\" \",\"\"), flags=0)\n\n                out = re.search(r'@[0-9]*\\.[0-9]*([a-z][a-zA-Z0-9_]*)\\(([0-9]*),([0-9]),([a-z]*[a-zA-Z0-9]*|[a-z]*[a-zA-Z0-9]*,[a-z]*[a-zA-Z0-9]*),([a-z]*[a-zA-Z0-9_]*\\))', line.replace(\" \",\"\"), flags=0)\n                npp_name = out.group(1)\n                npp_e = out.group(2)\n                npp_inftype = out.group(3)# is the identifier for the inference type e.g. 1,2,3,4 \n                npp_input = out.group(4)\n\n                list_of_atoms = []\n                list_of_probs = []\n                list_of_bools = []\n                choices = line.strip()[:-1].split(\";\")\n                for choice in choices:\n                    prob, atom = choice.strip().split(\" \", maxsplit=1)\n                    # Note that we remove all spaces in atom since clingo output does not contain space in atom\n                    list_of_atoms.append(atom.replace(\" \", \"\"))\n                    if prob.startswith(\"@\"):\n                        list_of_probs.append(float(prob[1:]))\n                        list_of_bools.append(True)\n                    else:\n                        list_of_probs.append(float(prob))\n                        list_of_bools.append(False)\n                pc.append(list_of_atoms)\n                parameters.append(list_of_probs)\n                learnable.append(list_of_bools)\n\n                #save a dictionary containing the npp and add all inference type instances to it\n                if (npp_name, npp_input, npp_e) not in npp_choices:\n                    npp_choices[(npp_name, npp_input, npp_e )]= []\n                npp_choices[(npp_name, npp_input,npp_e)] += list_of_atoms\n\n\n                #save npp to an ordered list to access them for probabilistic grounding\n                npp_choices_inftype_sorted.append([npp_name, npp_input, npp_inftype, npp_e, np.array(list_of_atoms) ])\n                \n                \n            else:\n                asp += (line.strip()+\"\\n\")\n\n        self.npp_choices_inftype_sorted = npp_choices_inftype_sorted\n\n        if prob_ground is False:\n            #create choice rules for npp atoms\n            for atom in npp_choices.keys():\n                if len(npp_choices[atom]) == 1:\n                    pi_prime += \"1{\"+\"; \".join(npp_choices[atom])+\"}1.\\n\"\n                else:\n                    pi_prime += \"0{\"+\";\".join(npp_choices[atom])+\"}1.\\n\"\n\n\n\n        pi_prime += asp\n\n        for ruleIdx, list_of_bools in enumerate(learnable):\n            remain_prob = 1\n            for atomIdx, b in enumerate(list_of_bools):\n                if b == False:\n                    remain_prob -= parameters[ruleIdx][atomIdx]\n            remain_probs.append(remain_prob)\n\n        for ruleIdx, list_of_atoms in enumerate(pc):\n            for atomIdx, atom in enumerate(list_of_atoms):\n                ga_map[atom] = (ruleIdx, atomIdx)\n\n        return pc, parameters, learnable, asp, pi_prime, remain_probs, ga_map\n\n\n\n    def normalize_M(self):\n        \n        tmp = self.M[self.non_binary_idx]\n        tmp[self.M[self.non_binary_idx] >=1] = 1-self.eps\n        tmp[self.M[self.non_binary_idx] <=self.eps] = self.eps\n\n        self.M[self.non_binary_idx] = tmp\n\n        self.M *= self.selection_mask\n        # determine denominator\n        denom = self.M[self.non_binary_idx].sum(dim=1)\n        self.M[self.non_binary_idx] /= denom[:,None]\n\n        \n\n\n    def prob_of_interpretation(self, I, model_idx_list = None):\n        prob = 1.0\n\n        #if we have indices\n        if model_idx_list is not None: \n            prob = 1.0           \n            for rule_idx, atom_idx in model_idx_list:\n                prob *= self.M[rule_idx][atom_idx]\n            return prob\n    \n        else:\n            # I must be a list of atoms, where each atom is a string\n            while not isinstance(I[0], str):\n                I = I[0]\n            for _, atom in enumerate(I):\n                if atom in self.ga_map:\n                    ruleIdx, atomIdx = self.ga_map[atom]\n                    prob *= self.M[ruleIdx][atomIdx]\n            return prob\n\n\n   # k = 0 means to find all stable models\n    def find_k_SM_under_query(self, query, k=3):\n\n        program = self.pi_prime + query\n        clingo_control = clingo.Control([\"--warn=none\", str(int(k))])\n        models = []\n\n\n\n        try:\n            clingo_control.add(\"base\", [], program)\n        except:\n            print(\"\\nPi': \\n{}\".format(program))\n\n        clingo_control.ground([(\"base\", [])])\n        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)))\n\n        if len(models) ==0:\n            exit()\n        models = [[str(atom) for atom in model] for model in models]\n\n        return models\n\n    # we assume query is a string containing a valid Clingo program, \n    # and each query is written in constraint form\n    def find_one_SM_under_query(self, query):\n        return self.find_k_SM_under_query(query, 1)\n\n    # we assume query is a string containing a valid Clingo program, \n    # and each query is written in constraint form\n    def find_all_SM_under_query(self, query):\n        return self.find_k_SM_under_query(query, 0)\n\n\n    def get_pareto_idx(self,arr, threshold, k=np.inf):\n        \"\"\"\n        Returns a list of indices ordered by ascending probability value that sum up to be bigger than a given threshold\n        \n        @param arr: array of probabilites\n        @param threshold: treshold of summed probabilities to keep\n        \"\"\"\n        sum = 0\n        idx_list = []\n\n        #sort probability indices in descending order\n        #sorted_idx = np.argsort(arr)[::-1]\n        sorted_idx = torch.argsort(arr,descending=True)\n        \n        \n        #iterate over indeces and add them to the list until the sum of the corresponding reaches the treshold\n        for i in sorted_idx:\n            sum += arr[i] \n            idx_list.append(i.cpu())\n            if idx_list.__len__() == k:\n                return idx_list\n            if sum >= threshold:\n                return idx_list\n\n        return sorted_idx\n\n\n\n    def find_SM_with_same(self, query, k=1, threshold=0.99):\n            \n        pi_prime = self.pi_prime\n        #create choice rules for npp atoms\n\n        # add choice rules for NPPs\n        for rule_idx, rule in enumerate(self.npp_choices_inftype_sorted):\n\n            #set threshold from dict or scalar value\n            if type(threshold) == dict:\n                t = threshold[rule[0]] #select npp specific threshold\n            else:\n                t = threshold\n\n            #if we have a binary npp\n            if rule_idx in self.binary_rule_belongings:\n                for atom_idx, atom in enumerate(self.pc[rule_idx]):\n                    if self.M[rule_idx][atom_idx] > 1 - t:\n                        pi_prime += \"0{\"+self.pc[rule_idx][atom_idx]+\"}1.\\n\"\n\n            else:\n                #get the ids after applying same with threshold t\n                k_idx = self.get_pareto_idx(self.M[rule_idx,0:len(rule[4])], t)\n                pi_prime += \"1{\"+\"; \".join(rule[4][np.sort(k_idx)])+\"}1.\\n\"\n\n\n        program = pi_prime + query\n\n\n        clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n        models = []\n        try:\n            clingo_control.add(\"base\", [], program)\n        except:\n            print(\"\\nPi': \\n{}\".format(program))\n        clingo_control.ground([(\"base\", [])])\n        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)))\n        models = [[str(atom) for atom in model] for model in models]  \n\n        if len(models) > 0:\n            return models[ -np.minimum(len(models),k) :]\n        else:\n            return []\n\n\n    \n\n    def find_k_most_probable_SM_with_same(self, query, k=1, threshold=0.99):\n            \n        pi_prime = self.pi_prime\n        #create choice rules for npp atoms\n\n        for rule_idx, rule in enumerate(self.npp_choices_inftype_sorted):\n\n            #set threshold from dict or scalar value\n            if type(threshold) == dict:\n                t = threshold[rule[0]] #select npp specific threshold\n            else:\n                t = threshold\n\n\n            #if we have a binary npp\n            if rule_idx in self.binary_rule_belongings:\n                for atom_idx, atom in enumerate(self.pc[rule_idx]):\n                    if self.M[rule_idx][atom_idx] > 1 - t:\n                        pi_prime += \"0{\"+self.pc[rule_idx][atom_idx]+\"}1.\\n\"\n\n            else:\n                #get the ids after applying same with threshold t\n                k_idx = self.get_pareto_idx(self.M[rule_idx,0:len(rule[4])], t)\n                pi_prime += \"1{\"+\"; \".join(rule[4][np.sort(k_idx)])+\"}1.\\n\"\n\n\n                # for each probabilistic rule with n atoms, add n weak constraints\n                for atomIdx in k_idx:\n                    if self.M[rule_idx][atomIdx] < 0.00674:\n                        penalty = -1000 * -5\n                    else:\n                        #penalty = int(-1000 * math.log(self.parameters[rule_idx][atomIdx]))\n                        penalty = int(-1000 * math.log(self.M[rule_idx][atomIdx]))\n\n                    pi_prime += ':~ {}. [{}, {}, {}]\\n'.format(self.pc[rule_idx][atomIdx], penalty, rule_idx, atomIdx)\n\n\n        program = pi_prime + query\n\n\n        clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n        models = []\n        try:\n            clingo_control.add(\"base\", [], program)\n        except:\n            print(\"\\nPi': \\n{}\".format(program))\n        clingo_control.ground([(\"base\", [])])\n        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)))\n        models = [[str(atom) for atom in model] for model in models]  \n\n        if len(models) > 0:\n            return models[ -np.minimum(len(models),k) :]\n        else:\n            return []\n\n\n \n    # there might be some duplications in SMs when optimization option is used\n    # and the duplications are removed by this method\n    def remove_duplicate_SM(self, models):\n        models.sort()\n        return list(models for models,_ in itertools.groupby(models))\n\n    # Note that the MVPP program cannot contain weak constraints\n    def find_all_most_probable_SM_under_query_noWC(self, query):\n        \"\"\"Return a list of stable models, each is a list of strings\n        @param query: a string of a set of constraints/facts\n        \"\"\"\n        program = self.pi_prime + query + '\\n'\n        # for each probabilistic rule with n atoms, add n weak constraints\n        for ruleIdx, atoms in enumerate(self.pc):\n            for atomIdx, atom in enumerate(atoms):\n                if self.M[ruleIdx][atomIdx] < 0.00674:\n                    penalty = -1000 * -5\n                else:\n                    penalty = int(-1000 * math.log(self.M[ruleIdx][atomIdx]))\n                program += ':~ {}. [{}, {}, {}]\\n'.format(atom, penalty, ruleIdx, atomIdx)\n\n        clingo_control = clingo.Control(['--warn=none', '--opt-mode=optN', '0', '-t', '8'])\n        models = []\n        clingo_control.add(\"base\", [], program)\n        clingo_control.ground([(\"base\", [])])\n        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)) if model.optimality_proven else None)\n        models = [[str(atom) for atom in model] for model in models]\n        return self.remove_duplicate_SM(models)\n\n\n    def find_k_most_probable_SM_under_query_noWC(self, query='', k = 1):\n        \"\"\"Return a list of a single stable model, which is a list of strings\n        @param query: a string of a set of constraints/facts\n        \"\"\"\n        \n        program = self.pi_prime + query + '\\n'\n        # for each probabilistic rule with n atoms, add n weak constraints\n        for ruleIdx, atoms in enumerate(self.pc):\n            for atomIdx, atom in enumerate(atoms):\n                if self.M[ruleIdx][atomIdx] < 0.00674: #< 0.00674:\n                    penalty = -1000 * -5\n                else:\n                    penalty = int(-1000 * math.log(self.M[ruleIdx][atomIdx]))\n                program += ':~ {}. [{}, {}, {}]\\n'.format(atom, penalty, ruleIdx, atomIdx)\n                \n        clingo_control = clingo.Control(['--warn=none', '-t', '8'])#8 parallel mode param\n        models = []\n\n        clingo_control.add(\"base\", [], program)\n        clingo_control.ground([(\"base\", [])])\n        clingo_control.solve([], lambda model: models.append(model.symbols(shown=True)))\n        models = np.array([[str(atom) for atom in model] for model in models], dtype=object)\n\n        return models[ -np.minimum(len(models),k) :] #return the last k models. The models are ordered by optimization values e.g how likely they are\n    \n\n\n\n    def find_SM_vqa_with_same(self, query='', k=1, obj_filter=np.inf, threshold=0.99, vqa_params= {}, train=True):\n        \"\"\"Return a list of a single stable model, which is a list of strings\n        @param query: a string of a set of constraints/facts\n        \"\"\"\n        pi_prime = self.pi_prime\n        temp = query.split(\"///\")\n        query = temp[0]\n        temp.pop(0)\n        attr_relation_filter = temp\n\n        l = vqa_params['l']\n        l_split = vqa_params['l_split'] #change to 20\n        num_names = vqa_params['num_names']\n        max_models = vqa_params[\"max_models\"]\n        asp_timeout = vqa_params[\"asp_timeout\"]\n\n\n        rel_atoms = {} #dictionary which contains all atom idx as keys and ruleIdx as values\n        for rule_idx, rule in enumerate(self.npp_choices_inftype_sorted):\n\n            #set threshold from dict or scalar value\n            if type(threshold) == dict:\n                t = threshold[rule[0]] #select npp specific threshold\n            else:\n                t = threshold\n\n            #check what the object number for filtering #relation (5, 7) min 5 max 7\n            obj_idx_min = np.array(rule[1].split(\",\")).astype(np.int).min()\n            obj_idx_max = np.array(rule[1].split(\",\")).astype(np.int).max()\n\n            #during training we only consider target objects \n            if obj_idx_min < obj_filter and obj_idx_max < obj_filter:\n                \n                #if we have a binary npp\n                if rule_idx  in self.binary_rule_belongings:\n                    for atom_idx, atom in enumerate(self.pc[rule_idx]):\n                        if self.M[rule_idx][atom_idx] > 1 - t:\n                            if any(\",\"+ar_filter+\")\" in self.pc[rule_idx][atom_idx] for ar_filter in attr_relation_filter):\n                                pi_prime += \"0{\"+self.pc[rule_idx][atom_idx]+\"}1.\\n\"\n                    \n                else:\n                    if 'relation' in self.pc[rule_idx][0]:\n                        for atom_idx, atom in enumerate(self.pc[rule_idx]):\n                            if any(\",\"+ar_filter+\")\" in atom for ar_filter in attr_relation_filter):\n                                if atom_idx not in rel_atoms.keys():\n                                    rel_atoms[atom_idx] = [rule_idx]\n                                else:\n                                    rel_atoms[atom_idx].append(rule_idx)\n                    else: \n                        #get the ids after applying same with threshold t\n                        if train: \n                            k_idx = self.get_pareto_idx(self.M[rule_idx,0:len(rule[4])], t, num_names)\n                        else:\n                            k_idx = self.get_pareto_idx(self.M[rule_idx,0:len(rule[4])], t, num_names)\n                            #on data generalization test we used -1\n\n                        pi_prime += \"0{\"+\"; \".join(rule[4][np.sort(k_idx)])+\"}1.\\n\"\n\n\n\n        program = pi_prime + query + '\\n'\n        models = []\n\n        #set number of relations to be considered\n        #check if relations exist\n        if rel_atoms:\n            #if all ruleIdx for relations are smaller than l use this number instead\n            if len(rel_atoms[list(rel_atoms.keys())[0]]) < l:\n                l = len(rel_atoms[list(rel_atoms.keys())[0]])\n                l_split = l # set l split also to l because we cant make splits m of a list with n elements if m > n \n                \n\n        #if we dont have any relations continue with solving as usual\n        else:\n            clingo_control = clingo.Control([str(0),'--warn=none', \"--project\"])#8 parallel mode param\n\n            #if we dont have have relations\n            clingo_control.add(\"base\", [], program)\n            clingo_control.ground([(\"base\", [])])\n            try:\n                clingo_control.solve([], lambda model: models.append(model.symbols(shown=True)))\n            except:\n                print(\"smth went wrong during solving\")\n\n            models = np.array([[str(atom) for atom in model] for model in models], dtype=object)\n            #print(\"one query done without relations\")\n            return models\n\n\n\n        #contains relation entries\n        rel_atoms_most_prob= {}\n        #for C2 keep all relataions in place\n        # l = len(rel_atoms[list(rel_atoms.keys())[0]])\n        #for every relation get the l likeliest relations\n        for key in rel_atoms:\n            top_l_idx = self.M[rel_atoms[key], key].topk(l)[1]\n\n            for i in np.array(rel_atoms[key])[top_l_idx]:\n                if key not in rel_atoms_most_prob:\n                    rel_atoms_most_prob[key] = [i]\n                else:\n                    rel_atoms_most_prob[key].append(i)\n            \n\n        rel_splits = np.array_split(np.arange(l), int(l / l_split))\n        # rel_splits = np.array_split(np.arange(l), 1)\n\n        #contains all relations that have to be considerd iteratively\n        atoms_rel = {}\n\n        ground_times = []\n        solve_times = []\n\n        models_prev_it = []\n        #iteratively add all different relations \n        solving_start = time.time()\n        for split in rel_splits:\n            models = []\n\n            # transform dict of atom_idx:rule_idx to dict of rule_idx:atom_idx\n            for key in rel_atoms_most_prob.keys():\n                for atom in np.array(rel_atoms_most_prob[key])[split]:\n\n                    if atom not in atoms_rel:\n                        atoms_rel[atom] = [key]\n                    else:\n                        atoms_rel[atom].append(key)\n\n\n            #build the relation string\n            rel_str = \"\"\n            for rule_idx in atoms_rel.keys():\n                atom_indices = atoms_rel[rule_idx]\n                rel_str += \"0{\"+\"; \".join([self.pc[rule_idx][atom_idx] for atom_idx in atom_indices])+\"}1.\\n\"\n\n\n            clingo_control = clingo.Control([str(max_models),'--warn=none', \"--project\"])#8 parallel mode param\n\n            program_with_rel = program + rel_str\n\n            t1 = time.time()\n            try:\n                clingo_control.add(\"base\", [], program_with_rel)\n                clingo_control.ground([(\"base\", [])])\n            except:\n                print(\"smth went wrong during grounding\")\n                f = open(\"dump/grounding_error\"+str(time.time())+\".txt\", \"a\")\n                f.write(program_with_rel)\n                f.close()\n                return models_prev_it\n\n            t2 = time.time()\n            ground_time = t2-t1\n            ground_times.append(ground_time)\n\n            try:\n                with clingo_control.solve([], lambda model: models.append(model.symbols(shown=True)), async_=True) as hnd:\n                    hnd.wait(asp_timeout)\n                    hnd.cancel()\n            except:\n                print(\"smth went wrong during solving\")\n                f = open(\"dump/solving_error\"+str(time.time())+\".txt\", \"a\")\n                f.write(program_with_rel)\n                f.close()\n                return models_prev_it\n                \n            t3 = time.time()\n            solve_time = t3-t2\n            solve_times.append(solve_time)\n\n            models = np.array([[str(atom) for atom in model] for model in models], dtype=object)\n\n            if len(models_prev_it) <= len(models):\n                models_prev_it = models #store models so we can return them when the solving fails\n            else:\n                return models_prev_it #if we have more models in the previous iteration return them\n\n            #if we have enough models or the time is over \n            if len(models) > max_models or (time.time()-solving_start) >= asp_timeout or ground_time >= asp_timeout:\n                #print(\"1.one query done with relations\",len(models), (time.time()-solving_start), \"ground times\", ground_time, \"solve times\", solve_time)\n                break\n\n\n\n\n        return models\n\n\n    def find_all_opt_SM_under_query_WC(self, query):\n        \"\"\" Return a list of stable models, each is a list of strings\n        @param query: a string of a set of constraints/facts\n\n        \"\"\"\n        program = self.pi_prime + query\n        clingo_control = clingo.Control(['--warn=none', '--opt-mode=optN', '0'])\n        models = []\n        try:\n            clingo_control.add(\"base\", [], program)\n        except:\n            print('\\nSyntax Error in Program: Pi\\': \\n{}'.format(program))\n            sys.exit()\n        clingo_control.ground([(\"base\", [])])\n        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)) if model.optimality_proven else None)\n        models = [[str(atom) for atom in model] for model in models]\n        return self.remove_duplicate_SM(models)\n\n    # compute P(Q)\n    def inference_query_exact(self, query):\n        prob = 0\n        models = self.find_all_SM_under_query(query)\n        for I in models:\n            prob += self.prob_of_interpretation(I)\n        return prob\n    \n    # computes P(Q) given a list of stable models satisfying Q\n    def sum_probability_for_stable_models(self, models):\n        prob = 0\n        for I in models:\n            prob += self.prob_of_interpretation(I)\n        return prob\n\n    def gradient(self, ruleIdx, atomIdx, query):\n        # we will compute P(I)/p_i where I satisfies query and c=v_i\n        p_query_i = 0\n        # we will compute P(I)/p_j where I satisfies query and c=v_j for i!=j\n        p_query_j = 0\n        # we will compute P(I) where I satisfies query\n        p_query = 0\n\n        # 1st, we generate all I that satisfies query\n        models = self.find_k_SM_under_query(query, k=3)\n        # 2nd, we iterate over each model I, and check if I satisfies c=v_i\n        c_equal_vi = self.pc[ruleIdx][atomIdx]\n        p_i = self.parameters[ruleIdx][atomIdx]\n        for I in models:\n            p_I = self.prob_of_interpretation(I)\n            p_query += p_I\n            if c_equal_vi in I:\n                p_query_i += p_I/p_i\n            else:\n                for atomIdx2, p_j in enumerate(self.parameters[ruleIdx]):\n                    c_equal_vj = self.pc[ruleIdx][atomIdx2]\n                    if c_equal_vj in I:\n                        p_query_j += p_I/p_j\n\n        # 3rd, we compute gradient\n        gradient = (p_query_i-p_query_j)/p_query\n        return gradient\n\n    def collect_atom_idx(self, models):\n\n        models_idx = []\n\n        for model in models:\n            model_idx = []\n            #add a one for every atom that is in the model\n            for i in count(start=0, step=1):\n                if i < model.__len__():\n                    if model[i] in self.ga_map:\n                        ruleIdx, atomIdx = self.ga_map[model[i]]\n                        model_idx.append((ruleIdx,atomIdx))                        \n                else:\n                    break\n            models_idx.append(model_idx)\n        return models_idx\n                    \n\n\n    # Function generates a 2d gradient mask for one model\n    def gen_grad_mask(self, model_idx_list, grad_device):\n        '''\n        generates a positive gradient mask and a negative gradient mask\n        '''\n\n        gradient_mask = torch.zeros(self.M.shape, dtype=torch.float, device=grad_device)\n        gradient_mask_neg = torch.zeros(self.M.shape, dtype=torch.float, device=grad_device)\n\n        #add a one for every atom that is in the model\n        for i in count(start=0, step=1):\n            if i < model_idx_list.__len__():\n                #ruleIdx, atomIdx = self.ga_map[model[i]]\n                ruleIdx, atomIdx = model_idx_list[i]\n                gradient_mask[ruleIdx][atomIdx] = 1\n\n                if ruleIdx not in self.binary_rule_belongings:\n                    gradient_mask_neg[ruleIdx] = -1\n                    gradient_mask_neg[ruleIdx][atomIdx] = 0\n            else:\n                break\n\n        return gradient_mask, gradient_mask_neg\n\n\n    def mvppLearn(self, models, model_idx_list, grad_device):\n        probs = []\n\n        #compute P(I) for every model I\n        for i in count(start=0, step=1):\n            if i < models.__len__():\n                probs.append(self.prob_of_interpretation(models[i], model_idx_list[i]))\n            else:\n                break\n\n        probs = torch.tensor(probs, dtype=torch.float, device=grad_device)\n        denominator = probs.sum()\n\n        #if the model is empty return an empty gradient matrix\n        if len(models) == 0 or denominator == 0:\n            return torch.zeros([len(self.parameters), self.max_n], dtype=torch.float, device=grad_device)\n\n\n        summed_numerator = torch.zeros([len(self.parameters), self.max_n], dtype=torch.float, device=grad_device)\n\n        #create an tensor for every model\n        splits = torch.split(torch.tensor(np.arange(0, len(models))),10)\n\n        #iterate over all splits\n        for s in count(start=0, step=1):\n            if s < splits.__len__():\n\n                # Calculate gradients in tensor fashion\n                gradient_mask = []\n                gradient_mask_neg=[]\n\n                #iterate over all splits in models\n                for i in count(start=0, step=1):\n                    if i < splits[s].__len__():\n                        pos, neg = self.gen_grad_mask(model_idx_list[splits[s][i]],grad_device)\n                        gradient_mask.append(pos)\n                        gradient_mask_neg.append(neg)\n                    else:\n                        break\n                \n                gradient_mask = torch.stack(gradient_mask) *  self.selection_mask\n                gradient_mask_neg = torch.stack(gradient_mask_neg) * self.selection_mask\n\n\n                if gradient_mask.dim() == 2: #if we only have one model\n                    gradient_mask.unsqueeze(0)\n                    gradient_mask_neg.unsqueeze(0)\n\n\n                #create the gradient tensor:\n                #generate c=vi\n                c_eq_vi = torch.einsum('kij,ij -> kij', gradient_mask, self.M)\n\n\n                #compute sum of atoms in c=vi \n                c_eq_vi_sum = torch.einsum('kij -> ki', c_eq_vi)\n                #generate c!=vi from the sum of atoms in c=vi\n                c_not_eq_vi = torch.einsum('kij,ki -> kij', gradient_mask_neg, c_eq_vi_sum)\n        \n\n                #numerator is the sum of both P(I)/c=vi and P(I)/c!=vi (no sign flip necessary due to the correct mask)\n                numerator = c_eq_vi + c_not_eq_vi\n\n                numerator[numerator != 0] = 1/ numerator[numerator != 0]  \n                numerator = torch.einsum('kij,k -> kij',numerator , probs[splits[s]])\n                #sum over all potential solutions\n                summed_numerator += torch.einsum('kij -> ij', numerator)\n                #gradient is the fraction of both\n            else:\n                break\n        \n        grad_tensor = summed_numerator/denominator\n\n        return grad_tensor\n\n\n    # gradients are stored in numpy array instead of list\n    # query is a string\n    def gradients_one_query(self, query, opt=False, k=0):\n        \"\"\"Return an np-array denoting the gradients\n        @param query: a string for query\n        @param opt: a Boolean denoting whether we use optimal stable models instead of stable models\n        \"\"\"\n        if opt:\n            models = self.find_all_opt_SM_under_query_WC(query)\n        else:\n            models = self.find_k_SM_under_query(query, k)\n        return self.mvppLearn(models), models\n\n    # gradients are stored in numpy array instead of list\n    def gradients_multi_query(self, list_of_query):\n        gradients = [[0.0 for item in l] for l in self.parameters]\n        for query in list_of_query:\n            gradients = [[c+d for c,d in zip(i,j)] for i,j in zip(gradients,self.gradients_one_query(query))]\n        return gradients\n\n    # list_of_query is either a list of strings or a file containing queries separated by \"#evidence\"\n    def learn_exact(self, list_of_query, lr=0.01, thres=0.0001, max_iter=None):\n        # if list_of_query is an evidence file, we need to first turn it into a list of strings\n        if type(list_of_query) is str and os.path.isfile(list_of_query):\n            with open(list_of_query, 'r') as f:\n                list_of_query = f.read().strip().strip(\"#evidence\").split(\"#evidence\")\n        print(\"Start learning by exact computation with {} queries...\\n\\nInitial parameters: {}\".format(len(list_of_query), self.parameters))\n        time_init = time.time()\n        check_continue = True\n        iteration = 1\n        while check_continue:\n            old_parameters = self.parameters\n            print(\"\\n#### Iteration {} ####\\n\".format(iteration))\n            check_continue = False\n            dif = [[lr*grad for grad in l] for l in self.gradients_multi_query(list_of_query)]\n\n            for ruleIdx, list_of_bools in enumerate(self.learnable):\n            # 1st, we turn each gradient into [-0.2, 0.2]\n                for atomIdx, b in enumerate(list_of_bools):\n                    if b == True:\n                        if dif[ruleIdx][atomIdx] > 0.2 :\n                            dif[ruleIdx][atomIdx] = 0.2\n                        elif dif[ruleIdx][atomIdx] < -0.2:\n                            dif[ruleIdx][atomIdx] = -0.2\n\n            self.parameters = [[c+d for c,d in zip(i,j)] for i,j in zip(dif,self.parameters)]\n            self.normalize_probs()\n\n            # we termintate if the change of the parameters is lower than thres\n            dif = [[abs(c-d) for c,d in zip(i,j)] for i,j in zip(old_parameters,self.parameters)]\n            print(\"After {} seconds of training (in total)\".format(time.time()-time_init))\n            print(\"Current parameters: {}\".format(self.parameters))\n            maxdif = max([max(l) for l in dif])\n            print(\"Max change on probabilities: {}\".format(maxdif))\n            iteration += 1\n            if maxdif > thres:\n                check_continue = True\n            if max_iter is not None:\n                if iteration > max_iter:\n                    check_continue = False\n        print(\"\\nFinal parameters: {}\".format(self.parameters))\n\n    ##############################\n    ####### Sampling Method ######\n    ##############################\n\n    # it will generate k sample stable models for a k-coherent program under a specific total choice\n    def k_sample(self):\n        asp_with_facts = self.asp\n        clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n        models = []\n        for ruleIdx,list_of_atoms in enumerate(self.pc):\n            tmp = np.random.choice(list_of_atoms, 1, p=self.parameters[ruleIdx])\n            asp_with_facts += tmp[0]+\".\\n\"\n        clingo_control.add(\"base\", [], asp_with_facts)\n        clingo_control.ground([(\"base\", [])])\n        result = clingo_control.solve([], lambda model: models.append(model.symbols(shown=True)))\n        models = [[str(atom) for atom in model] for model in models]\n        return models\n\n    # it will generate k*num sample stable models\n    def sample(self, num=1):\n        models = []\n        for i in range(num):\n            models = models + self.k_sample()\n        return models\n\n    # it will generate at least num of samples that satisfy query\n    def sample_query(self, query, num=50):\n        count = 0\n        models = []\n        while count < num:\n            asp_with_facts = self.asp\n            asp_with_facts += query\n            clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n            models_tmp = []\n            for ruleIdx,list_of_atoms in enumerate(self.pc):\n                p = np.array(self.parameters[ruleIdx]) \n                p /= p.sum()\n                tmp = np.random.choice(list_of_atoms, 1, p=p)\n                asp_with_facts += tmp[0]+\".\\n\"\n            clingo_control.add(\"base\", [], asp_with_facts)\n            clingo_control.ground([(\"base\", [])])\n            result = clingo_control.solve([], lambda model: models_tmp.append(model.symbols(shown=True)))\n            if str(result) == \"SAT\":\n                models_tmp = [[str(atom) for atom in model] for model in models_tmp]\n                count += len(models_tmp)\n                models = models + models_tmp\n            elif str(result) == \"UNSAT\":\n                pass\n            else:\n                print(\"Error! The result of a clingo call is not SAT nor UNSAT!\")\n        return models\n\n    # it will generate at least num of samples that satisfy query\n    def sample_query2(self, query, num=50):\n        count = 0\n        models = []\n        candidate_sm = []\n        # we first find out all stable models that satisfy query\n        program = self.pi_prime + query\n        clingo_control = clingo.Control(['0', '--warn=none'])\n        clingo_control.add('base', [], program)\n        clingo_control.ground([('base', [])])\n        clingo_control.solve([], lambda model: candidate_sm.append(model.symbols(shown=True)))\n        candidate_sm = [[str(atom) for atom in model] for model in candidate_sm]\n        probs = [self.prob_of_interpretation(model) for model in candidate_sm]\n\n        while count < num:\n            asp_with_facts = self.pi_prime\n            asp_with_facts += query\n            clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n            models_tmp = []\n            for ruleIdx,list_of_atoms in enumerate(self.pc):\n                tmp = np.random.choice(list_of_atoms, 1, p=self.parameters[ruleIdx])\n                asp_with_facts += tmp[0]+\".\\n\"\n            clingo_control.add(\"base\", [], asp_with_facts)\n            clingo_control.ground([(\"base\", [])])\n            result = clingo_control.solve([], lambda model: models_tmp.append(model.symbols(shown=True)))\n            if str(result) == \"SAT\":\n                models_tmp = [[str(atom) for atom in model] for model in models_tmp]\n                count += len(models_tmp)\n                models = models + models_tmp\n            elif str(result) == \"UNSAT\":\n                pass\n            else:\n                print(\"Error! The result of a clingo call is not SAT nor UNSAT!\")\n        return models\n\n    # we compute the gradients (numpy array) w.r.t. all probs in the ruleIdx-th rule\n    # given models that satisfy query\n    def gradient_given_models(self, ruleIdx, models):\n        arity = len(self.parameters[ruleIdx])\n\n        # we will compute N(O) and N(O,c=v_i)/p_i for each i\n        n_O = 0\n        n_i = [0]*arity\n\n        # 1st, we compute N(O)\n        n_O = len(models)\n\n        # 2nd, we compute N(O,c=v_i)/p_i for each i\n        for model in models:\n            for atomIdx, atom in enumerate(self.pc[ruleIdx]):\n                if atom in model:\n                    n_i[atomIdx] += 1\n        for atomIdx, p_i in enumerate(self.parameters[ruleIdx]):\n            n_i[atomIdx] = n_i[atomIdx]/p_i\n        \n        # 3rd, we compute the derivative of L'(O) w.r.t. p_i for each i\n        tmp = np.array(n_i) * (-1)\n        summation = np.sum(tmp)\n        gradients = np.array([summation]*arity)\n        for atomIdx, p_i in enumerate(self.parameters[ruleIdx]):\n            gradients[atomIdx] = gradients[atomIdx] + 2* n_i[atomIdx]\n        gradients = gradients / n_O\n        return gradients\n\n\n    # gradients are stored in numpy array instead of list\n    # query is a string\n    def gradients_one_query_by_sampling(self, query, num=50):\n        gradients = np.array([[0.0 for item in l] for l in self.parameters])\n        # 1st, we generate at least num of stable models that satisfy query\n        models = self.sample_query(query=query, num=num)\n\n        # 2nd, we compute the gradients w.r.t. the probs in each rule\n        for ruleIdx,list_of_bools in enumerate(self.learnable):\n            gradients[ruleIdx] = self.gradient_given_models(ruleIdx, models)\n            for atomIdx, b in enumerate(list_of_bools):\n                if b == False:\n                    gradients[ruleIdx][atomIdx] = 0\n        return gradients\n\n    # we compute the gradients (numpy array) w.r.t. all probs given list_of_query\n    def gradients_multi_query_by_sampling(self, list_of_query, num=50):\n        gradients = np.array([[0.0 for item in l] for l in self.parameters])\n\n        # we itereate over all query\n        for query in list_of_query:\n            # 1st, we generate at least num of stable models that satisfy query\n            models = self.sample_query(query=query, num=num) \n\n            # 2nd, we accumulate the gradients w.r.t. the probs in each rule\n            for ruleIdx,list_of_bools in enumerate(self.learnable):\n                gradients[ruleIdx] += self.gradient_given_models(ruleIdx, models)\n                for atomIdx, b in enumerate(list_of_bools):\n                    if b == False:\n                        gradients[ruleIdx][atomIdx] = 0\n        return gradients\n\n    # we compute the gradients (numpy array) w.r.t. all probs given list_of_query\n    # while we generate at least one sample without considering probability distribution\n    def gradients_multi_query_by_one_sample(self, list_of_query):\n        gradients = np.array([[0.0 for item in l] for l in self.parameters])\n\n        # we itereate over all query\n        for query in list_of_query:\n            # 1st, we generate one stable model that satisfy query\n            models = self.find_one_SM_under_query(query=query)\n\n            # 2nd, we accumulate the gradients w.r.t. the probs in each rule\n            for ruleIdx,list_of_bools in enumerate(self.learnable):\n                gradients[ruleIdx] += self.gradient_given_models(ruleIdx, models)\n                for atomIdx, b in enumerate(list_of_bools):\n                    if b == False:\n                        gradients[ruleIdx][atomIdx] = 0\n        return gradients\n\n    # list_of_query is either a list of strings or a file containing queries separated by \"#evidence\"\n    def learn_by_sampling(self, list_of_query, num_of_samples=50, lr=0.01, thres=0.0001, max_iter=None, num_pretrain=1):\n        # Step 0: Evidence Preprocessing: if list_of_query is an evidence file, \n        # we need to first turn it into a list of strings\n        if type(list_of_query) is str and os.path.isfile(list_of_query):\n            with open(list_of_query, 'r') as f:\n                list_of_query = f.read().strip().strip(\"#evidence\").split(\"#evidence\")\n\n        print(\"Start learning by sampling with {} queries...\\n\\nInitial parameters: {}\".format(len(list_of_queries), self.parameters))\n        time_init = time.time()\n\n        # Step 1: Parameter Pre-training: we pretrain the parameters \n        # so that it's easier to generate sample stable models\n        assert type(num_pretrain) is int\n        if num_pretrain >= 1:\n            print(\"\\n#######################################################\\nParameter Pre-training for {} iterations...\\n#######################################################\".format(num_pretrain))\n            for iteration in range(num_pretrain):\n                print(\"\\n#### Iteration {} for Pre-Training ####\\nGenerating 1 stable model for each query...\\n\".format(iteration+1))\n                dif = lr * self.gradients_multi_query_by_one_sample(list_of_query)\n                self.parameters = (np.array(self.parameters) + dif).tolist()\n                self.normalize_probs()\n\n                print(\"After {} seconds of training (in total)\".format(time.time()-time_init))\n                print(\"Current parameters: {}\".format(self.parameters))\n\n        # Step 2: Parameter Training: we train the parameters using \"list_of_query until\"\n        # (i) the max change on probabilities is lower than \"thres\", or\n        # (ii) the number of iterations is more than \"max_iter\"\n        print(\"\\n#######################################################\\nParameter Training for {} iterations or until converge...\\n#######################################################\".format(max_iter))\n        check_continue = True\n        iteration = 1\n        while check_continue:\n            print(\"\\n#### Iteration {} ####\".format(iteration))\n            old_parameters = np.array(self.parameters)            \n            check_continue = False\n\n            print(\"Generating {} stable model(s) for each query...\\n\".format(num_of_samples))\n            dif = lr * self.gradients_multi_query_by_sampling(list_of_query, num=num_of_samples)\n\n            self.parameters = (np.array(self.parameters) + dif).tolist()\n            self.normalize_probs()\n            \n            print(\"After {} seconds of training (in total)\".format(time.time()-time_init))\n            print(\"Current parameters: {}\".format(self.parameters))\n\n            # we termintate if the change of the parameters is lower than thres\n            dif = np.array(self.parameters) - old_parameters\n            dif = abs(max(dif.min(), dif.max(), key=abs))\n            print(\"Max change on probabilities: {}\".format(dif))\n\n            iteration += 1\n            if dif > thres:\n                check_continue = True\n            if max_iter is not None:\n                if iteration > max_iter:\n                    check_continue = False\n\n        print(\"\\nFinal parameters: {}\".format(self.parameters))", "class MVPP(object):                 \n    def __init__(self, program, k=1, eps=0.000001, prob_ground = False, binary_rule_belongings={}, max_n=0):\n        self.k = k\n        self.eps = eps\n\n        # each element in self.pc is a list of atoms (one list for one prob choice rule)\n        self.pc = []\n        # each element in self.parameters is a list of probabilities\n        self.parameters = []\n        # each element in self.learnable is a list of Boolean values\n        self.learnable = []\n        # self.asp is the ASP part of the LPMLN program\n        self.asp = \"\"\n        # self.pi_prime is the ASP program \\Pi' defined for the semantics\n        self.pi_prime = \"\"\n        # self.remain_probs is a list of probs, each denotes a remaining prob given those non-learnable probs\n        self.remain_probs = []\n        # self.ga_map is a dictionary containing the map ground atom <-> (ruleId, atomId).\n        # Thereby, ruleId is a NPP call and atomId is graound atom (outcome) index in a NPP call.\n        self.ga_map = {}\n        # self.M is the 2d tensor containig probabilities for all NPP calls\n        self.M = torch.empty((2,3), dtype=torch.float32)\n        self.max_n = max_n\n        self.device = 'cpu'\n\n\n        self.binary_rule_belongings = binary_rule_belongings\n        self.pc, self.parameters, self.learnable, self.asp, self.pi_prime, self.remain_probs, self.ga_map = self.parse(program, prob_ground)\n\n        self.get_non_binary_idx()\n        if self.parameters != []:\n            self.create_selection_mask()\n        \n    def debug_mvpp(self):\n        print(\"pc\",self.pc)\n        print(\"params\", self.parameters)\n        print(\"learnable\", self.learnable)\n        print(\"asp\", self.asp)\n        print(\"pi_prime\", self.pi_prime)\n        \n    def get_non_binary_idx(self):\n        non_binary_idx = []\n        \n        #from the idx to delete generate the ones to keep\n        for i in count(start=0, step=1):\n            if i < len(self.parameters):\n                if i not in self.binary_rule_belongings:\n                    non_binary_idx.append(i)\n            else:\n                break\n        self.non_binary_idx = non_binary_idx\n\n\n    def create_selection_mask(self):\n        selection_mask = torch.zeros([self.parameters.__len__(), max(par.__len__() for par in self.parameters)], dtype=torch.bool, device='cpu')\n        for rule_idx, rule in enumerate(self.parameters):\n            selection_mask[rule_idx,0:len(self.parameters[rule_idx])] = True\n        self.selection_mask = selection_mask\n\n    def put_selection_mask_on_device(self, device):\n        self.selection_mask = self.selection_mask.to(device=device)\n\n\n    def parse(self, program, prob_ground):\n\n        pc = []\n        parameters = []\n        learnable = []\n        asp = \"\"\n        pi_prime = \"\"\n        remain_probs = []\n        ga_map = {}\n        npp_choices = {}\n        npp_choices_inftype_sorted = []\n\n        lines = []\n        # if program is a file\n        if os.path.isfile(program):\n            with open(program, 'r') as program:\n                lines = program.readlines()\n        # if program is a string containing all rules of an LPMLN program\n\n        elif type(program) is str and re.sub(r'\\n%[^\\n]*', '\\n', program).strip().endswith(('.', ']')):\n            lines = program.split('\\n')\n        else:\n            print(program)\n            print(\"Error! The MVPP program {} is not valid.\".format(program))\n            sys.exit()\n\n        #iterate over all lines\n        # 1. build the choice rules 1{...}1\n        # 2. store which npp values are learnable\n        for line in lines:\n            \n            \n\n            #if re.match(r\".*[0-1]\\.?[0-9]*\\s.*;.*\", line): \n            if re.match(r'.*[0-1]\\.[0-9]*\\s.*(;|\\.).*', line):\n                #out = re.search(r'@[0-9]*\\.[0-9]*([a-z][a-zA-Z0-9_]*)\\(([0-9]*),([0-9]),([a-z]*[a-zA-Z0-9]*)', line.replace(\" \",\"\"), flags=0)\n\n                out = re.search(r'@[0-9]*\\.[0-9]*([a-z][a-zA-Z0-9_]*)\\(([0-9]*),([0-9]),([a-z]*[a-zA-Z0-9]*|[a-z]*[a-zA-Z0-9]*,[a-z]*[a-zA-Z0-9]*),([a-z]*[a-zA-Z0-9_]*\\))', line.replace(\" \",\"\"), flags=0)\n                npp_name = out.group(1)\n                npp_e = out.group(2)\n                npp_inftype = out.group(3)# is the identifier for the inference type e.g. 1,2,3,4 \n                npp_input = out.group(4)\n\n                list_of_atoms = []\n                list_of_probs = []\n                list_of_bools = []\n                choices = line.strip()[:-1].split(\";\")\n                for choice in choices:\n                    prob, atom = choice.strip().split(\" \", maxsplit=1)\n                    # Note that we remove all spaces in atom since clingo output does not contain space in atom\n                    list_of_atoms.append(atom.replace(\" \", \"\"))\n                    if prob.startswith(\"@\"):\n                        list_of_probs.append(float(prob[1:]))\n                        list_of_bools.append(True)\n                    else:\n                        list_of_probs.append(float(prob))\n                        list_of_bools.append(False)\n                pc.append(list_of_atoms)\n                parameters.append(list_of_probs)\n                learnable.append(list_of_bools)\n\n                #save a dictionary containing the npp and add all inference type instances to it\n                if (npp_name, npp_input, npp_e) not in npp_choices:\n                    npp_choices[(npp_name, npp_input, npp_e )]= []\n                npp_choices[(npp_name, npp_input,npp_e)] += list_of_atoms\n\n\n                #save npp to an ordered list to access them for probabilistic grounding\n                npp_choices_inftype_sorted.append([npp_name, npp_input, npp_inftype, npp_e, np.array(list_of_atoms) ])\n                \n                \n            else:\n                asp += (line.strip()+\"\\n\")\n\n        self.npp_choices_inftype_sorted = npp_choices_inftype_sorted\n\n        if prob_ground is False:\n            #create choice rules for npp atoms\n            for atom in npp_choices.keys():\n                if len(npp_choices[atom]) == 1:\n                    pi_prime += \"1{\"+\"; \".join(npp_choices[atom])+\"}1.\\n\"\n                else:\n                    pi_prime += \"0{\"+\";\".join(npp_choices[atom])+\"}1.\\n\"\n\n\n\n        pi_prime += asp\n\n        for ruleIdx, list_of_bools in enumerate(learnable):\n            remain_prob = 1\n            for atomIdx, b in enumerate(list_of_bools):\n                if b == False:\n                    remain_prob -= parameters[ruleIdx][atomIdx]\n            remain_probs.append(remain_prob)\n\n        for ruleIdx, list_of_atoms in enumerate(pc):\n            for atomIdx, atom in enumerate(list_of_atoms):\n                ga_map[atom] = (ruleIdx, atomIdx)\n\n        return pc, parameters, learnable, asp, pi_prime, remain_probs, ga_map\n\n\n\n    def normalize_M(self):\n        \n        tmp = self.M[self.non_binary_idx]\n        tmp[self.M[self.non_binary_idx] >=1] = 1-self.eps\n        tmp[self.M[self.non_binary_idx] <=self.eps] = self.eps\n\n        self.M[self.non_binary_idx] = tmp\n\n        self.M *= self.selection_mask\n        # determine denominator\n        denom = self.M[self.non_binary_idx].sum(dim=1)\n        self.M[self.non_binary_idx] /= denom[:,None]\n\n        \n\n\n    def prob_of_interpretation(self, I, model_idx_list = None):\n        prob = 1.0\n\n        #if we have indices\n        if model_idx_list is not None: \n            prob = 1.0           \n            for rule_idx, atom_idx in model_idx_list:\n                prob *= self.M[rule_idx][atom_idx]\n            return prob\n    \n        else:\n            # I must be a list of atoms, where each atom is a string\n            while not isinstance(I[0], str):\n                I = I[0]\n            for _, atom in enumerate(I):\n                if atom in self.ga_map:\n                    ruleIdx, atomIdx = self.ga_map[atom]\n                    prob *= self.M[ruleIdx][atomIdx]\n            return prob\n\n\n   # k = 0 means to find all stable models\n    def find_k_SM_under_query(self, query, k=3):\n\n        program = self.pi_prime + query\n        clingo_control = clingo.Control([\"--warn=none\", str(int(k))])\n        models = []\n\n\n\n        try:\n            clingo_control.add(\"base\", [], program)\n        except:\n            print(\"\\nPi': \\n{}\".format(program))\n\n        clingo_control.ground([(\"base\", [])])\n        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)))\n\n        if len(models) ==0:\n            exit()\n        models = [[str(atom) for atom in model] for model in models]\n\n        return models\n\n    # we assume query is a string containing a valid Clingo program, \n    # and each query is written in constraint form\n    def find_one_SM_under_query(self, query):\n        return self.find_k_SM_under_query(query, 1)\n\n    # we assume query is a string containing a valid Clingo program, \n    # and each query is written in constraint form\n    def find_all_SM_under_query(self, query):\n        return self.find_k_SM_under_query(query, 0)\n\n\n    def get_pareto_idx(self,arr, threshold, k=np.inf):\n        \"\"\"\n        Returns a list of indices ordered by ascending probability value that sum up to be bigger than a given threshold\n        \n        @param arr: array of probabilites\n        @param threshold: treshold of summed probabilities to keep\n        \"\"\"\n        sum = 0\n        idx_list = []\n\n        #sort probability indices in descending order\n        #sorted_idx = np.argsort(arr)[::-1]\n        sorted_idx = torch.argsort(arr,descending=True)\n        \n        \n        #iterate over indeces and add them to the list until the sum of the corresponding reaches the treshold\n        for i in sorted_idx:\n            sum += arr[i] \n            idx_list.append(i.cpu())\n            if idx_list.__len__() == k:\n                return idx_list\n            if sum >= threshold:\n                return idx_list\n\n        return sorted_idx\n\n\n\n    def find_SM_with_same(self, query, k=1, threshold=0.99):\n            \n        pi_prime = self.pi_prime\n        #create choice rules for npp atoms\n\n        # add choice rules for NPPs\n        for rule_idx, rule in enumerate(self.npp_choices_inftype_sorted):\n\n            #set threshold from dict or scalar value\n            if type(threshold) == dict:\n                t = threshold[rule[0]] #select npp specific threshold\n            else:\n                t = threshold\n\n            #if we have a binary npp\n            if rule_idx in self.binary_rule_belongings:\n                for atom_idx, atom in enumerate(self.pc[rule_idx]):\n                    if self.M[rule_idx][atom_idx] > 1 - t:\n                        pi_prime += \"0{\"+self.pc[rule_idx][atom_idx]+\"}1.\\n\"\n\n            else:\n                #get the ids after applying same with threshold t\n                k_idx = self.get_pareto_idx(self.M[rule_idx,0:len(rule[4])], t)\n                pi_prime += \"1{\"+\"; \".join(rule[4][np.sort(k_idx)])+\"}1.\\n\"\n\n\n        program = pi_prime + query\n\n\n        clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n        models = []\n        try:\n            clingo_control.add(\"base\", [], program)\n        except:\n            print(\"\\nPi': \\n{}\".format(program))\n        clingo_control.ground([(\"base\", [])])\n        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)))\n        models = [[str(atom) for atom in model] for model in models]  \n\n        if len(models) > 0:\n            return models[ -np.minimum(len(models),k) :]\n        else:\n            return []\n\n\n    \n\n    def find_k_most_probable_SM_with_same(self, query, k=1, threshold=0.99):\n            \n        pi_prime = self.pi_prime\n        #create choice rules for npp atoms\n\n        for rule_idx, rule in enumerate(self.npp_choices_inftype_sorted):\n\n            #set threshold from dict or scalar value\n            if type(threshold) == dict:\n                t = threshold[rule[0]] #select npp specific threshold\n            else:\n                t = threshold\n\n\n            #if we have a binary npp\n            if rule_idx in self.binary_rule_belongings:\n                for atom_idx, atom in enumerate(self.pc[rule_idx]):\n                    if self.M[rule_idx][atom_idx] > 1 - t:\n                        pi_prime += \"0{\"+self.pc[rule_idx][atom_idx]+\"}1.\\n\"\n\n            else:\n                #get the ids after applying same with threshold t\n                k_idx = self.get_pareto_idx(self.M[rule_idx,0:len(rule[4])], t)\n                pi_prime += \"1{\"+\"; \".join(rule[4][np.sort(k_idx)])+\"}1.\\n\"\n\n\n                # for each probabilistic rule with n atoms, add n weak constraints\n                for atomIdx in k_idx:\n                    if self.M[rule_idx][atomIdx] < 0.00674:\n                        penalty = -1000 * -5\n                    else:\n                        #penalty = int(-1000 * math.log(self.parameters[rule_idx][atomIdx]))\n                        penalty = int(-1000 * math.log(self.M[rule_idx][atomIdx]))\n\n                    pi_prime += ':~ {}. [{}, {}, {}]\\n'.format(self.pc[rule_idx][atomIdx], penalty, rule_idx, atomIdx)\n\n\n        program = pi_prime + query\n\n\n        clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n        models = []\n        try:\n            clingo_control.add(\"base\", [], program)\n        except:\n            print(\"\\nPi': \\n{}\".format(program))\n        clingo_control.ground([(\"base\", [])])\n        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)))\n        models = [[str(atom) for atom in model] for model in models]  \n\n        if len(models) > 0:\n            return models[ -np.minimum(len(models),k) :]\n        else:\n            return []\n\n\n \n    # there might be some duplications in SMs when optimization option is used\n    # and the duplications are removed by this method\n    def remove_duplicate_SM(self, models):\n        models.sort()\n        return list(models for models,_ in itertools.groupby(models))\n\n    # Note that the MVPP program cannot contain weak constraints\n    def find_all_most_probable_SM_under_query_noWC(self, query):\n        \"\"\"Return a list of stable models, each is a list of strings\n        @param query: a string of a set of constraints/facts\n        \"\"\"\n        program = self.pi_prime + query + '\\n'\n        # for each probabilistic rule with n atoms, add n weak constraints\n        for ruleIdx, atoms in enumerate(self.pc):\n            for atomIdx, atom in enumerate(atoms):\n                if self.M[ruleIdx][atomIdx] < 0.00674:\n                    penalty = -1000 * -5\n                else:\n                    penalty = int(-1000 * math.log(self.M[ruleIdx][atomIdx]))\n                program += ':~ {}. [{}, {}, {}]\\n'.format(atom, penalty, ruleIdx, atomIdx)\n\n        clingo_control = clingo.Control(['--warn=none', '--opt-mode=optN', '0', '-t', '8'])\n        models = []\n        clingo_control.add(\"base\", [], program)\n        clingo_control.ground([(\"base\", [])])\n        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)) if model.optimality_proven else None)\n        models = [[str(atom) for atom in model] for model in models]\n        return self.remove_duplicate_SM(models)\n\n\n    def find_k_most_probable_SM_under_query_noWC(self, query='', k = 1):\n        \"\"\"Return a list of a single stable model, which is a list of strings\n        @param query: a string of a set of constraints/facts\n        \"\"\"\n        \n        program = self.pi_prime + query + '\\n'\n        # for each probabilistic rule with n atoms, add n weak constraints\n        for ruleIdx, atoms in enumerate(self.pc):\n            for atomIdx, atom in enumerate(atoms):\n                if self.M[ruleIdx][atomIdx] < 0.00674: #< 0.00674:\n                    penalty = -1000 * -5\n                else:\n                    penalty = int(-1000 * math.log(self.M[ruleIdx][atomIdx]))\n                program += ':~ {}. [{}, {}, {}]\\n'.format(atom, penalty, ruleIdx, atomIdx)\n                \n        clingo_control = clingo.Control(['--warn=none', '-t', '8'])#8 parallel mode param\n        models = []\n\n        clingo_control.add(\"base\", [], program)\n        clingo_control.ground([(\"base\", [])])\n        clingo_control.solve([], lambda model: models.append(model.symbols(shown=True)))\n        models = np.array([[str(atom) for atom in model] for model in models], dtype=object)\n\n        return models[ -np.minimum(len(models),k) :] #return the last k models. The models are ordered by optimization values e.g how likely they are\n    \n\n\n\n    def find_SM_vqa_with_same(self, query='', k=1, obj_filter=np.inf, threshold=0.99, vqa_params= {}, train=True):\n        \"\"\"Return a list of a single stable model, which is a list of strings\n        @param query: a string of a set of constraints/facts\n        \"\"\"\n        pi_prime = self.pi_prime\n        temp = query.split(\"///\")\n        query = temp[0]\n        temp.pop(0)\n        attr_relation_filter = temp\n\n        l = vqa_params['l']\n        l_split = vqa_params['l_split'] #change to 20\n        num_names = vqa_params['num_names']\n        max_models = vqa_params[\"max_models\"]\n        asp_timeout = vqa_params[\"asp_timeout\"]\n\n\n        rel_atoms = {} #dictionary which contains all atom idx as keys and ruleIdx as values\n        for rule_idx, rule in enumerate(self.npp_choices_inftype_sorted):\n\n            #set threshold from dict or scalar value\n            if type(threshold) == dict:\n                t = threshold[rule[0]] #select npp specific threshold\n            else:\n                t = threshold\n\n            #check what the object number for filtering #relation (5, 7) min 5 max 7\n            obj_idx_min = np.array(rule[1].split(\",\")).astype(np.int).min()\n            obj_idx_max = np.array(rule[1].split(\",\")).astype(np.int).max()\n\n            #during training we only consider target objects \n            if obj_idx_min < obj_filter and obj_idx_max < obj_filter:\n                \n                #if we have a binary npp\n                if rule_idx  in self.binary_rule_belongings:\n                    for atom_idx, atom in enumerate(self.pc[rule_idx]):\n                        if self.M[rule_idx][atom_idx] > 1 - t:\n                            if any(\",\"+ar_filter+\")\" in self.pc[rule_idx][atom_idx] for ar_filter in attr_relation_filter):\n                                pi_prime += \"0{\"+self.pc[rule_idx][atom_idx]+\"}1.\\n\"\n                    \n                else:\n                    if 'relation' in self.pc[rule_idx][0]:\n                        for atom_idx, atom in enumerate(self.pc[rule_idx]):\n                            if any(\",\"+ar_filter+\")\" in atom for ar_filter in attr_relation_filter):\n                                if atom_idx not in rel_atoms.keys():\n                                    rel_atoms[atom_idx] = [rule_idx]\n                                else:\n                                    rel_atoms[atom_idx].append(rule_idx)\n                    else: \n                        #get the ids after applying same with threshold t\n                        if train: \n                            k_idx = self.get_pareto_idx(self.M[rule_idx,0:len(rule[4])], t, num_names)\n                        else:\n                            k_idx = self.get_pareto_idx(self.M[rule_idx,0:len(rule[4])], t, num_names)\n                            #on data generalization test we used -1\n\n                        pi_prime += \"0{\"+\"; \".join(rule[4][np.sort(k_idx)])+\"}1.\\n\"\n\n\n\n        program = pi_prime + query + '\\n'\n        models = []\n\n        #set number of relations to be considered\n        #check if relations exist\n        if rel_atoms:\n            #if all ruleIdx for relations are smaller than l use this number instead\n            if len(rel_atoms[list(rel_atoms.keys())[0]]) < l:\n                l = len(rel_atoms[list(rel_atoms.keys())[0]])\n                l_split = l # set l split also to l because we cant make splits m of a list with n elements if m > n \n                \n\n        #if we dont have any relations continue with solving as usual\n        else:\n            clingo_control = clingo.Control([str(0),'--warn=none', \"--project\"])#8 parallel mode param\n\n            #if we dont have have relations\n            clingo_control.add(\"base\", [], program)\n            clingo_control.ground([(\"base\", [])])\n            try:\n                clingo_control.solve([], lambda model: models.append(model.symbols(shown=True)))\n            except:\n                print(\"smth went wrong during solving\")\n\n            models = np.array([[str(atom) for atom in model] for model in models], dtype=object)\n            #print(\"one query done without relations\")\n            return models\n\n\n\n        #contains relation entries\n        rel_atoms_most_prob= {}\n        #for C2 keep all relataions in place\n        # l = len(rel_atoms[list(rel_atoms.keys())[0]])\n        #for every relation get the l likeliest relations\n        for key in rel_atoms:\n            top_l_idx = self.M[rel_atoms[key], key].topk(l)[1]\n\n            for i in np.array(rel_atoms[key])[top_l_idx]:\n                if key not in rel_atoms_most_prob:\n                    rel_atoms_most_prob[key] = [i]\n                else:\n                    rel_atoms_most_prob[key].append(i)\n            \n\n        rel_splits = np.array_split(np.arange(l), int(l / l_split))\n        # rel_splits = np.array_split(np.arange(l), 1)\n\n        #contains all relations that have to be considerd iteratively\n        atoms_rel = {}\n\n        ground_times = []\n        solve_times = []\n\n        models_prev_it = []\n        #iteratively add all different relations \n        solving_start = time.time()\n        for split in rel_splits:\n            models = []\n\n            # transform dict of atom_idx:rule_idx to dict of rule_idx:atom_idx\n            for key in rel_atoms_most_prob.keys():\n                for atom in np.array(rel_atoms_most_prob[key])[split]:\n\n                    if atom not in atoms_rel:\n                        atoms_rel[atom] = [key]\n                    else:\n                        atoms_rel[atom].append(key)\n\n\n            #build the relation string\n            rel_str = \"\"\n            for rule_idx in atoms_rel.keys():\n                atom_indices = atoms_rel[rule_idx]\n                rel_str += \"0{\"+\"; \".join([self.pc[rule_idx][atom_idx] for atom_idx in atom_indices])+\"}1.\\n\"\n\n\n            clingo_control = clingo.Control([str(max_models),'--warn=none', \"--project\"])#8 parallel mode param\n\n            program_with_rel = program + rel_str\n\n            t1 = time.time()\n            try:\n                clingo_control.add(\"base\", [], program_with_rel)\n                clingo_control.ground([(\"base\", [])])\n            except:\n                print(\"smth went wrong during grounding\")\n                f = open(\"dump/grounding_error\"+str(time.time())+\".txt\", \"a\")\n                f.write(program_with_rel)\n                f.close()\n                return models_prev_it\n\n            t2 = time.time()\n            ground_time = t2-t1\n            ground_times.append(ground_time)\n\n            try:\n                with clingo_control.solve([], lambda model: models.append(model.symbols(shown=True)), async_=True) as hnd:\n                    hnd.wait(asp_timeout)\n                    hnd.cancel()\n            except:\n                print(\"smth went wrong during solving\")\n                f = open(\"dump/solving_error\"+str(time.time())+\".txt\", \"a\")\n                f.write(program_with_rel)\n                f.close()\n                return models_prev_it\n                \n            t3 = time.time()\n            solve_time = t3-t2\n            solve_times.append(solve_time)\n\n            models = np.array([[str(atom) for atom in model] for model in models], dtype=object)\n\n            if len(models_prev_it) <= len(models):\n                models_prev_it = models #store models so we can return them when the solving fails\n            else:\n                return models_prev_it #if we have more models in the previous iteration return them\n\n            #if we have enough models or the time is over \n            if len(models) > max_models or (time.time()-solving_start) >= asp_timeout or ground_time >= asp_timeout:\n                #print(\"1.one query done with relations\",len(models), (time.time()-solving_start), \"ground times\", ground_time, \"solve times\", solve_time)\n                break\n\n\n\n\n        return models\n\n\n    def find_all_opt_SM_under_query_WC(self, query):\n        \"\"\" Return a list of stable models, each is a list of strings\n        @param query: a string of a set of constraints/facts\n\n        \"\"\"\n        program = self.pi_prime + query\n        clingo_control = clingo.Control(['--warn=none', '--opt-mode=optN', '0'])\n        models = []\n        try:\n            clingo_control.add(\"base\", [], program)\n        except:\n            print('\\nSyntax Error in Program: Pi\\': \\n{}'.format(program))\n            sys.exit()\n        clingo_control.ground([(\"base\", [])])\n        clingo_control.solve([], lambda model: models.append(model.symbols(atoms=True)) if model.optimality_proven else None)\n        models = [[str(atom) for atom in model] for model in models]\n        return self.remove_duplicate_SM(models)\n\n    # compute P(Q)\n    def inference_query_exact(self, query):\n        prob = 0\n        models = self.find_all_SM_under_query(query)\n        for I in models:\n            prob += self.prob_of_interpretation(I)\n        return prob\n    \n    # computes P(Q) given a list of stable models satisfying Q\n    def sum_probability_for_stable_models(self, models):\n        prob = 0\n        for I in models:\n            prob += self.prob_of_interpretation(I)\n        return prob\n\n    def gradient(self, ruleIdx, atomIdx, query):\n        # we will compute P(I)/p_i where I satisfies query and c=v_i\n        p_query_i = 0\n        # we will compute P(I)/p_j where I satisfies query and c=v_j for i!=j\n        p_query_j = 0\n        # we will compute P(I) where I satisfies query\n        p_query = 0\n\n        # 1st, we generate all I that satisfies query\n        models = self.find_k_SM_under_query(query, k=3)\n        # 2nd, we iterate over each model I, and check if I satisfies c=v_i\n        c_equal_vi = self.pc[ruleIdx][atomIdx]\n        p_i = self.parameters[ruleIdx][atomIdx]\n        for I in models:\n            p_I = self.prob_of_interpretation(I)\n            p_query += p_I\n            if c_equal_vi in I:\n                p_query_i += p_I/p_i\n            else:\n                for atomIdx2, p_j in enumerate(self.parameters[ruleIdx]):\n                    c_equal_vj = self.pc[ruleIdx][atomIdx2]\n                    if c_equal_vj in I:\n                        p_query_j += p_I/p_j\n\n        # 3rd, we compute gradient\n        gradient = (p_query_i-p_query_j)/p_query\n        return gradient\n\n    def collect_atom_idx(self, models):\n\n        models_idx = []\n\n        for model in models:\n            model_idx = []\n            #add a one for every atom that is in the model\n            for i in count(start=0, step=1):\n                if i < model.__len__():\n                    if model[i] in self.ga_map:\n                        ruleIdx, atomIdx = self.ga_map[model[i]]\n                        model_idx.append((ruleIdx,atomIdx))                        \n                else:\n                    break\n            models_idx.append(model_idx)\n        return models_idx\n                    \n\n\n    # Function generates a 2d gradient mask for one model\n    def gen_grad_mask(self, model_idx_list, grad_device):\n        '''\n        generates a positive gradient mask and a negative gradient mask\n        '''\n\n        gradient_mask = torch.zeros(self.M.shape, dtype=torch.float, device=grad_device)\n        gradient_mask_neg = torch.zeros(self.M.shape, dtype=torch.float, device=grad_device)\n\n        #add a one for every atom that is in the model\n        for i in count(start=0, step=1):\n            if i < model_idx_list.__len__():\n                #ruleIdx, atomIdx = self.ga_map[model[i]]\n                ruleIdx, atomIdx = model_idx_list[i]\n                gradient_mask[ruleIdx][atomIdx] = 1\n\n                if ruleIdx not in self.binary_rule_belongings:\n                    gradient_mask_neg[ruleIdx] = -1\n                    gradient_mask_neg[ruleIdx][atomIdx] = 0\n            else:\n                break\n\n        return gradient_mask, gradient_mask_neg\n\n\n    def mvppLearn(self, models, model_idx_list, grad_device):\n        probs = []\n\n        #compute P(I) for every model I\n        for i in count(start=0, step=1):\n            if i < models.__len__():\n                probs.append(self.prob_of_interpretation(models[i], model_idx_list[i]))\n            else:\n                break\n\n        probs = torch.tensor(probs, dtype=torch.float, device=grad_device)\n        denominator = probs.sum()\n\n        #if the model is empty return an empty gradient matrix\n        if len(models) == 0 or denominator == 0:\n            return torch.zeros([len(self.parameters), self.max_n], dtype=torch.float, device=grad_device)\n\n\n        summed_numerator = torch.zeros([len(self.parameters), self.max_n], dtype=torch.float, device=grad_device)\n\n        #create an tensor for every model\n        splits = torch.split(torch.tensor(np.arange(0, len(models))),10)\n\n        #iterate over all splits\n        for s in count(start=0, step=1):\n            if s < splits.__len__():\n\n                # Calculate gradients in tensor fashion\n                gradient_mask = []\n                gradient_mask_neg=[]\n\n                #iterate over all splits in models\n                for i in count(start=0, step=1):\n                    if i < splits[s].__len__():\n                        pos, neg = self.gen_grad_mask(model_idx_list[splits[s][i]],grad_device)\n                        gradient_mask.append(pos)\n                        gradient_mask_neg.append(neg)\n                    else:\n                        break\n                \n                gradient_mask = torch.stack(gradient_mask) *  self.selection_mask\n                gradient_mask_neg = torch.stack(gradient_mask_neg) * self.selection_mask\n\n\n                if gradient_mask.dim() == 2: #if we only have one model\n                    gradient_mask.unsqueeze(0)\n                    gradient_mask_neg.unsqueeze(0)\n\n\n                #create the gradient tensor:\n                #generate c=vi\n                c_eq_vi = torch.einsum('kij,ij -> kij', gradient_mask, self.M)\n\n\n                #compute sum of atoms in c=vi \n                c_eq_vi_sum = torch.einsum('kij -> ki', c_eq_vi)\n                #generate c!=vi from the sum of atoms in c=vi\n                c_not_eq_vi = torch.einsum('kij,ki -> kij', gradient_mask_neg, c_eq_vi_sum)\n        \n\n                #numerator is the sum of both P(I)/c=vi and P(I)/c!=vi (no sign flip necessary due to the correct mask)\n                numerator = c_eq_vi + c_not_eq_vi\n\n                numerator[numerator != 0] = 1/ numerator[numerator != 0]  \n                numerator = torch.einsum('kij,k -> kij',numerator , probs[splits[s]])\n                #sum over all potential solutions\n                summed_numerator += torch.einsum('kij -> ij', numerator)\n                #gradient is the fraction of both\n            else:\n                break\n        \n        grad_tensor = summed_numerator/denominator\n\n        return grad_tensor\n\n\n    # gradients are stored in numpy array instead of list\n    # query is a string\n    def gradients_one_query(self, query, opt=False, k=0):\n        \"\"\"Return an np-array denoting the gradients\n        @param query: a string for query\n        @param opt: a Boolean denoting whether we use optimal stable models instead of stable models\n        \"\"\"\n        if opt:\n            models = self.find_all_opt_SM_under_query_WC(query)\n        else:\n            models = self.find_k_SM_under_query(query, k)\n        return self.mvppLearn(models), models\n\n    # gradients are stored in numpy array instead of list\n    def gradients_multi_query(self, list_of_query):\n        gradients = [[0.0 for item in l] for l in self.parameters]\n        for query in list_of_query:\n            gradients = [[c+d for c,d in zip(i,j)] for i,j in zip(gradients,self.gradients_one_query(query))]\n        return gradients\n\n    # list_of_query is either a list of strings or a file containing queries separated by \"#evidence\"\n    def learn_exact(self, list_of_query, lr=0.01, thres=0.0001, max_iter=None):\n        # if list_of_query is an evidence file, we need to first turn it into a list of strings\n        if type(list_of_query) is str and os.path.isfile(list_of_query):\n            with open(list_of_query, 'r') as f:\n                list_of_query = f.read().strip().strip(\"#evidence\").split(\"#evidence\")\n        print(\"Start learning by exact computation with {} queries...\\n\\nInitial parameters: {}\".format(len(list_of_query), self.parameters))\n        time_init = time.time()\n        check_continue = True\n        iteration = 1\n        while check_continue:\n            old_parameters = self.parameters\n            print(\"\\n#### Iteration {} ####\\n\".format(iteration))\n            check_continue = False\n            dif = [[lr*grad for grad in l] for l in self.gradients_multi_query(list_of_query)]\n\n            for ruleIdx, list_of_bools in enumerate(self.learnable):\n            # 1st, we turn each gradient into [-0.2, 0.2]\n                for atomIdx, b in enumerate(list_of_bools):\n                    if b == True:\n                        if dif[ruleIdx][atomIdx] > 0.2 :\n                            dif[ruleIdx][atomIdx] = 0.2\n                        elif dif[ruleIdx][atomIdx] < -0.2:\n                            dif[ruleIdx][atomIdx] = -0.2\n\n            self.parameters = [[c+d for c,d in zip(i,j)] for i,j in zip(dif,self.parameters)]\n            self.normalize_probs()\n\n            # we termintate if the change of the parameters is lower than thres\n            dif = [[abs(c-d) for c,d in zip(i,j)] for i,j in zip(old_parameters,self.parameters)]\n            print(\"After {} seconds of training (in total)\".format(time.time()-time_init))\n            print(\"Current parameters: {}\".format(self.parameters))\n            maxdif = max([max(l) for l in dif])\n            print(\"Max change on probabilities: {}\".format(maxdif))\n            iteration += 1\n            if maxdif > thres:\n                check_continue = True\n            if max_iter is not None:\n                if iteration > max_iter:\n                    check_continue = False\n        print(\"\\nFinal parameters: {}\".format(self.parameters))\n\n    ##############################\n    ####### Sampling Method ######\n    ##############################\n\n    # it will generate k sample stable models for a k-coherent program under a specific total choice\n    def k_sample(self):\n        asp_with_facts = self.asp\n        clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n        models = []\n        for ruleIdx,list_of_atoms in enumerate(self.pc):\n            tmp = np.random.choice(list_of_atoms, 1, p=self.parameters[ruleIdx])\n            asp_with_facts += tmp[0]+\".\\n\"\n        clingo_control.add(\"base\", [], asp_with_facts)\n        clingo_control.ground([(\"base\", [])])\n        result = clingo_control.solve([], lambda model: models.append(model.symbols(shown=True)))\n        models = [[str(atom) for atom in model] for model in models]\n        return models\n\n    # it will generate k*num sample stable models\n    def sample(self, num=1):\n        models = []\n        for i in range(num):\n            models = models + self.k_sample()\n        return models\n\n    # it will generate at least num of samples that satisfy query\n    def sample_query(self, query, num=50):\n        count = 0\n        models = []\n        while count < num:\n            asp_with_facts = self.asp\n            asp_with_facts += query\n            clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n            models_tmp = []\n            for ruleIdx,list_of_atoms in enumerate(self.pc):\n                p = np.array(self.parameters[ruleIdx]) \n                p /= p.sum()\n                tmp = np.random.choice(list_of_atoms, 1, p=p)\n                asp_with_facts += tmp[0]+\".\\n\"\n            clingo_control.add(\"base\", [], asp_with_facts)\n            clingo_control.ground([(\"base\", [])])\n            result = clingo_control.solve([], lambda model: models_tmp.append(model.symbols(shown=True)))\n            if str(result) == \"SAT\":\n                models_tmp = [[str(atom) for atom in model] for model in models_tmp]\n                count += len(models_tmp)\n                models = models + models_tmp\n            elif str(result) == \"UNSAT\":\n                pass\n            else:\n                print(\"Error! The result of a clingo call is not SAT nor UNSAT!\")\n        return models\n\n    # it will generate at least num of samples that satisfy query\n    def sample_query2(self, query, num=50):\n        count = 0\n        models = []\n        candidate_sm = []\n        # we first find out all stable models that satisfy query\n        program = self.pi_prime + query\n        clingo_control = clingo.Control(['0', '--warn=none'])\n        clingo_control.add('base', [], program)\n        clingo_control.ground([('base', [])])\n        clingo_control.solve([], lambda model: candidate_sm.append(model.symbols(shown=True)))\n        candidate_sm = [[str(atom) for atom in model] for model in candidate_sm]\n        probs = [self.prob_of_interpretation(model) for model in candidate_sm]\n\n        while count < num:\n            asp_with_facts = self.pi_prime\n            asp_with_facts += query\n            clingo_control = clingo.Control([\"0\", \"--warn=none\"])\n            models_tmp = []\n            for ruleIdx,list_of_atoms in enumerate(self.pc):\n                tmp = np.random.choice(list_of_atoms, 1, p=self.parameters[ruleIdx])\n                asp_with_facts += tmp[0]+\".\\n\"\n            clingo_control.add(\"base\", [], asp_with_facts)\n            clingo_control.ground([(\"base\", [])])\n            result = clingo_control.solve([], lambda model: models_tmp.append(model.symbols(shown=True)))\n            if str(result) == \"SAT\":\n                models_tmp = [[str(atom) for atom in model] for model in models_tmp]\n                count += len(models_tmp)\n                models = models + models_tmp\n            elif str(result) == \"UNSAT\":\n                pass\n            else:\n                print(\"Error! The result of a clingo call is not SAT nor UNSAT!\")\n        return models\n\n    # we compute the gradients (numpy array) w.r.t. all probs in the ruleIdx-th rule\n    # given models that satisfy query\n    def gradient_given_models(self, ruleIdx, models):\n        arity = len(self.parameters[ruleIdx])\n\n        # we will compute N(O) and N(O,c=v_i)/p_i for each i\n        n_O = 0\n        n_i = [0]*arity\n\n        # 1st, we compute N(O)\n        n_O = len(models)\n\n        # 2nd, we compute N(O,c=v_i)/p_i for each i\n        for model in models:\n            for atomIdx, atom in enumerate(self.pc[ruleIdx]):\n                if atom in model:\n                    n_i[atomIdx] += 1\n        for atomIdx, p_i in enumerate(self.parameters[ruleIdx]):\n            n_i[atomIdx] = n_i[atomIdx]/p_i\n        \n        # 3rd, we compute the derivative of L'(O) w.r.t. p_i for each i\n        tmp = np.array(n_i) * (-1)\n        summation = np.sum(tmp)\n        gradients = np.array([summation]*arity)\n        for atomIdx, p_i in enumerate(self.parameters[ruleIdx]):\n            gradients[atomIdx] = gradients[atomIdx] + 2* n_i[atomIdx]\n        gradients = gradients / n_O\n        return gradients\n\n\n    # gradients are stored in numpy array instead of list\n    # query is a string\n    def gradients_one_query_by_sampling(self, query, num=50):\n        gradients = np.array([[0.0 for item in l] for l in self.parameters])\n        # 1st, we generate at least num of stable models that satisfy query\n        models = self.sample_query(query=query, num=num)\n\n        # 2nd, we compute the gradients w.r.t. the probs in each rule\n        for ruleIdx,list_of_bools in enumerate(self.learnable):\n            gradients[ruleIdx] = self.gradient_given_models(ruleIdx, models)\n            for atomIdx, b in enumerate(list_of_bools):\n                if b == False:\n                    gradients[ruleIdx][atomIdx] = 0\n        return gradients\n\n    # we compute the gradients (numpy array) w.r.t. all probs given list_of_query\n    def gradients_multi_query_by_sampling(self, list_of_query, num=50):\n        gradients = np.array([[0.0 for item in l] for l in self.parameters])\n\n        # we itereate over all query\n        for query in list_of_query:\n            # 1st, we generate at least num of stable models that satisfy query\n            models = self.sample_query(query=query, num=num) \n\n            # 2nd, we accumulate the gradients w.r.t. the probs in each rule\n            for ruleIdx,list_of_bools in enumerate(self.learnable):\n                gradients[ruleIdx] += self.gradient_given_models(ruleIdx, models)\n                for atomIdx, b in enumerate(list_of_bools):\n                    if b == False:\n                        gradients[ruleIdx][atomIdx] = 0\n        return gradients\n\n    # we compute the gradients (numpy array) w.r.t. all probs given list_of_query\n    # while we generate at least one sample without considering probability distribution\n    def gradients_multi_query_by_one_sample(self, list_of_query):\n        gradients = np.array([[0.0 for item in l] for l in self.parameters])\n\n        # we itereate over all query\n        for query in list_of_query:\n            # 1st, we generate one stable model that satisfy query\n            models = self.find_one_SM_under_query(query=query)\n\n            # 2nd, we accumulate the gradients w.r.t. the probs in each rule\n            for ruleIdx,list_of_bools in enumerate(self.learnable):\n                gradients[ruleIdx] += self.gradient_given_models(ruleIdx, models)\n                for atomIdx, b in enumerate(list_of_bools):\n                    if b == False:\n                        gradients[ruleIdx][atomIdx] = 0\n        return gradients\n\n    # list_of_query is either a list of strings or a file containing queries separated by \"#evidence\"\n    def learn_by_sampling(self, list_of_query, num_of_samples=50, lr=0.01, thres=0.0001, max_iter=None, num_pretrain=1):\n        # Step 0: Evidence Preprocessing: if list_of_query is an evidence file, \n        # we need to first turn it into a list of strings\n        if type(list_of_query) is str and os.path.isfile(list_of_query):\n            with open(list_of_query, 'r') as f:\n                list_of_query = f.read().strip().strip(\"#evidence\").split(\"#evidence\")\n\n        print(\"Start learning by sampling with {} queries...\\n\\nInitial parameters: {}\".format(len(list_of_queries), self.parameters))\n        time_init = time.time()\n\n        # Step 1: Parameter Pre-training: we pretrain the parameters \n        # so that it's easier to generate sample stable models\n        assert type(num_pretrain) is int\n        if num_pretrain >= 1:\n            print(\"\\n#######################################################\\nParameter Pre-training for {} iterations...\\n#######################################################\".format(num_pretrain))\n            for iteration in range(num_pretrain):\n                print(\"\\n#### Iteration {} for Pre-Training ####\\nGenerating 1 stable model for each query...\\n\".format(iteration+1))\n                dif = lr * self.gradients_multi_query_by_one_sample(list_of_query)\n                self.parameters = (np.array(self.parameters) + dif).tolist()\n                self.normalize_probs()\n\n                print(\"After {} seconds of training (in total)\".format(time.time()-time_init))\n                print(\"Current parameters: {}\".format(self.parameters))\n\n        # Step 2: Parameter Training: we train the parameters using \"list_of_query until\"\n        # (i) the max change on probabilities is lower than \"thres\", or\n        # (ii) the number of iterations is more than \"max_iter\"\n        print(\"\\n#######################################################\\nParameter Training for {} iterations or until converge...\\n#######################################################\".format(max_iter))\n        check_continue = True\n        iteration = 1\n        while check_continue:\n            print(\"\\n#### Iteration {} ####\".format(iteration))\n            old_parameters = np.array(self.parameters)            \n            check_continue = False\n\n            print(\"Generating {} stable model(s) for each query...\\n\".format(num_of_samples))\n            dif = lr * self.gradients_multi_query_by_sampling(list_of_query, num=num_of_samples)\n\n            self.parameters = (np.array(self.parameters) + dif).tolist()\n            self.normalize_probs()\n            \n            print(\"After {} seconds of training (in total)\".format(time.time()-time_init))\n            print(\"Current parameters: {}\".format(self.parameters))\n\n            # we termintate if the change of the parameters is lower than thres\n            dif = np.array(self.parameters) - old_parameters\n            dif = abs(max(dif.min(), dif.max(), key=abs))\n            print(\"Max change on probabilities: {}\".format(dif))\n\n            iteration += 1\n            if dif > thres:\n                check_continue = True\n            if max_iter is not None:\n                if iteration > max_iter:\n                    check_continue = False\n\n        print(\"\\nFinal parameters: {}\".format(self.parameters))", ""]}
{"filename": "src/experiments/__init__.py", "chunked_list": [""]}
{"filename": "src/experiments/mnist_top_k/train.py", "chunked_list": ["print(\"start importing...\")\n\nimport time\nimport sys\nimport argparse\nimport datetime\n\nsys.path.append('../../')\nsys.path.append('../../SLASH/')\nsys.path.append('../../EinsumNetworks/src/')", "sys.path.append('../../SLASH/')\nsys.path.append('../../EinsumNetworks/src/')\n\n\n#torch, numpy, ...\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.transforms import transforms\nimport torchvision\n", "import torchvision\n\nimport numpy as np\n\n#own modules\nfrom dataGen import MNIST_Addition\nfrom einsum_wrapper import EiNet\nfrom network_nn import Net_nn\n\n#import slash", "\n#import slash\nfrom slash import SLASH\n\nimport utils\nfrom utils import set_manual_seed\nfrom pathlib import Path\nfrom rtpt import RTPT\n\nprint(\"...done\")", "\nprint(\"...done\")\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--seed\", type=int, default=10, help=\"Random generator seed for all frameworks\"\n    )\n    parser.add_argument(\n        \"--epochs\", type=int, default=10, help=\"Number of epochs to train with\"\n    )\n    parser.add_argument(\n        \"--lr\", type=float, default=0.01, help=\"Learning rate of model\"\n    )\n    parser.add_argument(\n        \"--network-type\",\n        choices=[\"nn\",\"pc\"],\n        help=\"The type of external to be used e.g. neural net or probabilistic circuit\",\n    )\n    parser.add_argument(\n        \"--pc-structure\",\n        choices=[\"poon-domingos\",\"binary-trees\"],\n        help=\"The type of external to be used e.g. neural net or probabilistic circuit\",\n    )\n\n    parser.add_argument(\n        \"--method\",\n        choices=[\"exact\",\"top_k\",\"same\"],\n        help=\"How many images should be used in the addition\",\n    )\n    parser.add_argument(\n        \"--k\", type=int, default=0, help=\"Maximum number of stable model to be used\"\n    )\n    parser.add_argument(\n        \"--images-per-addition\",\n        choices=[\"2\",\"3\",\"4\",\"6\"],\n        help=\"How many images should be used in the addition\",\n    )\n    parser.add_argument(\n        \"--batch-size\", type=int, default=100, help=\"Batch size to train with\"\n    )\n    parser.add_argument(\n        \"--num-workers\", type=int, default=6, help=\"Number of threads for data loader\"\n    )\n\n    parser.add_argument(\n        \"--p-num\", type=int, default=8, help=\"Number of processes to devide the batch for parallel processing\"\n    )\n\n    parser.add_argument(\"--credentials\", type=str, help=\"Credentials for rtpt\")\n\n\n    args = parser.parse_args()\n\n    if args.network_type == 'pc':\n        args.use_pc = True\n    else:\n        args.use_pc = False\n\n    return args", "\n\ndef slash_mnist_addition():\n\n    args = get_args()\n    print(args)\n\n    \n    # Set the seeds for PRNG\n    set_manual_seed(args.seed)\n\n    # Create RTPT object\n    rtpt = RTPT(name_initials=args.credentials, experiment_name='SLASH MNIST pick-k', max_iterations=args.epochs)\n\n    # Start the RTPT tracking\n    rtpt.start()\n    \n    i_num = int(args.images_per_addition)\n    if i_num == 2:\n        program = '''\n        img(i1). img(i2).\n        addition(A,B,N):- digit(0,+A,-N1), digit(0,+B,-N2), N=N1+N2, A!=B.\n        npp(digit(1,X), [0,1,2,3,4,5,6,7,8,9]) :- img(X).\n        '''\n\n    elif i_num == 3:\n        program = '''\n        img(i1). img(i2). img(i3).\n        addition(A,B,C,N):- digit(0,+A,-N1), digit(0,+B,-N2), digit(0,+C,-N3), N=N1+N2+N3, A!=B, A!=C, B!=C.\n        npp(digit(1,X), [0,1,2,3,4,5,6,7,8,9]) :- img(X).\n        '''\n    \n    elif i_num == 4:\n        program = '''\n        img(i1). img(i2). img(i3). img(i4).\n        addition(A,B,C,D,N):- digit(0,+A,-N1), digit(0,+B,-N2), digit(0,+C,-N3), digit(0,+D,-N4), N=N1+N2+N3+N4, A != B, A!=C, A!=D, B!= C, B!= D, C!=D.\n        npp(digit(1,X), [0,1,2,3,4,5,6,7,8,9]) :- img(X).\n        '''\n\n    elif i_num == 6:\n        program = '''\n        img(i1). img(i2). img(i3). img(i4). img(i5). img(i6).\n        addition(i1,i2,i3,i4,i5,i6,N):- digit(0,+A,-N1), digit(0,+B,-N2), digit(0,+C,-N3), digit(0,+D,-N4), digit(0,+E,-N5), digit(0,+F,-N6), N=N1+N2+N3+N4+N5+N6, A != B, A!=C, A!=D, A!=E, A!=F, B!= C, B!= D, B!=E, B!=F, C!=D, C!=E, C!=F, D!=E, D!=F, E!=F.\n        npp(digit(1,X), [0,1,2,3,4,5,6,7,8,9]) :- img(X).\n        '''\n    \n    exp_name= str(args.method)+\"/\" +args.network_type+\"_i\"+str(i_num)+\"_k\"+ str(args.k)\n\n    saveModelPath = 'data/'+exp_name+'/slash_digit_addition_models_seed'+str(args.seed)+'.pt'\n    Path(\"data/\"+exp_name+\"/\").mkdir(parents=True, exist_ok=True)\n\n    \n    #use neural net or probabilisitc circuit\n    if args.network_type == 'pc':\n    \n        #setup new SLASH program given the network parameters\n        if args.pc_structure == 'binary-trees':\n            m = EiNet(structure = 'binary-trees',\n                      depth = 3,\n                      num_repetitions = 20,\n                      use_em = False,\n                      num_var = 784,\n                      class_count = 10,\n                      learn_prior = True)\n        elif args.pc_structure == 'poon-domingos': \n            m = EiNet(structure = 'poon-domingos',\n                      pd_num_pieces = [4,7,28],\n                      use_em = False,\n                      num_var = 784,\n                      class_count = 10,\n                      pd_width = 28,\n                      pd_height = 28,\n                      learn_prior = True)\n        else:\n            print(\"pc structure learner unknown\")\n\n    else:\n        m = Net_nn()    \n\n    \n    #trainable paramas\n    num_trainable_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n    num_params = sum(p.numel() for p in m.parameters())\n    print(\"training with {} trainable params and {} params in total\".format(num_trainable_params,num_params))\n            \n        \n    #create the SLASH Program\n    nnMapping = {'digit': m}\n    optimizers = {'digit': torch.optim.Adam(m.parameters(), lr=args.lr, eps=1e-7)}\n    SLASHobj = SLASH(program, nnMapping, optimizers)\n    SLASHobj.grad_comp_device ='cpu' #set gradient computation to cpu\n\n\n    #metric lists\n    train_accuracy_list = []\n    test_accuracy_list = []\n    confusion_matrix_list = []\n    loss_list = []\n    startTime = time.time()\n\n    forward_time_list = []\n    asp_time_list = []\n    backward_time_list = []\n    sm_per_batch_list = [] \n    train_test_times = []\n    \n    #load data\n    #if we are using spns we need to flatten the data(Tensor has form [bs, 784])\n    if args.use_pc: \n        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081, )), transforms.Lambda(lambda x: torch.flatten(x))])\n    #if not we can keep the dimensions(Tensor has form [bs,28,28])\n    else: \n        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081, ))]) \n    data_path = 'data/labels/train_data_s'+str(i_num)+'.txt'\n\n    mnist_addition_dataset = MNIST_Addition(torchvision.datasets.MNIST(root='./data/', train=True, download=True, transform=transform), data_path, i_num, args.use_pc)\n    train_dataset_loader = torch.utils.data.DataLoader(mnist_addition_dataset, shuffle=True,batch_size=args.batch_size,pin_memory=True, num_workers=8)\n    \n    test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('./data/', train=False, transform=transform), batch_size=100, shuffle=True)\n    train_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('./data/', train=True, transform=transform), batch_size=100, shuffle=True)\n\n    \n    # Evaluate the performanve directly after initialisation\n    time_test = time.time()\n    test_acc, _, confusion_matrix = SLASHobj.testNetwork('digit', test_loader, ret_confusion=True)\n    train_acc, _ = SLASHobj.testNetwork('digit', train_loader)\n    confusion_matrix_list.append(confusion_matrix)\n    train_accuracy_list.append([train_acc,0])\n    test_accuracy_list.append([test_acc, 0])\n    timestamp_test = utils.time_delta_now(time_test, simple_format=True)\n    timestamp_total = utils.time_delta_now(startTime, simple_format=True)\n\n    train_test_times.append([0.0, timestamp_test, timestamp_total])\n\n    # Save and print statistics\n    print('Train Acc: {:0.2f}%, Test Acc: {:0.2f}%'.format(train_acc, test_acc))\n    print('--- train time:  ---', 0)\n    print('--- test time:  ---' , timestamp_test)\n    print('--- total time from beginning:  ---', timestamp_total)\n\n    # Export results and networks\n    print('Storing the trained model into {}'.format(saveModelPath))\n    torch.save({\"addition_net\": m.state_dict(),\n                \"test_accuracy_list\": test_accuracy_list,\n                \"train_accuracy_list\":train_accuracy_list,\n                \"confusion_matrix_list\":confusion_matrix_list,\n                \"num_params\": num_trainable_params,\n                \"args\":args,\n                \"exp_name\":exp_name,\n                \"train_test_times\": train_test_times,\n                \"program\":program}, saveModelPath)\n    \n    start_e= 0\n\n    # Train and evaluate the performance\n    for e in range(start_e, args.epochs):\n        print('Epoch {}...'.format(e+1))\n\n        #one epoch of training\n        time_train= time.time()        \n        loss, forward_time, asp_time, backward_time, sm_per_batch, model_computation_time, gradient_computation_time = SLASHobj.learn(dataset_loader = train_dataset_loader,\n                       epoch=e, method=args.method, p_num=args.p_num, k_num = args.k , same_threshold=0.99)\n        timestamp_train = utils.time_delta_now(time_train, simple_format=True)\n\n        #store detailed timesteps per batch\n        forward_time_list.append(forward_time)\n        asp_time_list.append(asp_time)\n        backward_time_list.append(backward_time)\n        sm_per_batch_list.append(sm_per_batch)\n        \n\n        time_test = time.time()\n        test_acc, _, confusion_matrix = SLASHobj.testNetwork('digit', test_loader, ret_confusion=True)\n        confusion_matrix_list.append(confusion_matrix)\n        train_acc, _ = SLASHobj.testNetwork('digit', train_loader)        \n        train_accuracy_list.append([train_acc,e])\n        test_accuracy_list.append([test_acc, e])\n        timestamp_test = utils.time_delta_now(time_test, simple_format=True)\n        timestamp_total = utils.time_delta_now(startTime, simple_format=True)\n        loss_list.append(loss)\n        train_test_times.append([timestamp_train, timestamp_test, timestamp_total])\n\n        # Save and print statistics\n        print('Train Acc: {:0.2f}%, Test Acc: {:0.2f}%'.format(train_acc, test_acc))\n        print('--- train time:  ---', timestamp_train)\n        print('--- test time:  ---' , timestamp_test)\n        print('--- total time from beginning:  ---', timestamp_total)\n        \n        # Export results and networks\n        print('Storing the trained model into {}'.format(saveModelPath))\n        torch.save({\"addition_net\": m.state_dict(),\n                    \"resume\": {\n                        \"optimizer_digit\":optimizers['digit'].state_dict(),\n                        \"epoch\":e\n                            },\n                    \"test_accuracy_list\": test_accuracy_list,\n                    \"train_accuracy_list\":train_accuracy_list,\n                    \"confusion_matrix_list\":confusion_matrix_list,\n                    \"num_params\": num_trainable_params,\n                    \"args\":args,\n                    \"exp_name\":exp_name,\n                    \"train_test_times\": train_test_times,\n                    \"forward_time_list\":forward_time_list,\n                    \"asp_time_list\":asp_time_list,\n                    \"backward_time_list\":backward_time_list,\n                    \"sm_per_batch_list\":sm_per_batch_list,\n                    \"loss\": loss_list,\n                    \"program\":program}, saveModelPath)\n        \n        # Update the RTPT\n        rtpt.step(subtitle=f\"accuracy={test_acc:2.2f}\")", "\n\n\n\nif __name__ == \"__main__\":\n    slash_mnist_addition()"]}
{"filename": "src/experiments/mnist_top_k/network_nn.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nclass Net_nn(nn.Module):\n    def __init__(self):\n        super(Net_nn, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 6, 5),  # 6 is the output chanel size; 5 is the kernal size; 1 (chanel) 28 28 -> 6 24 24\n            nn.MaxPool2d(2, 2),  # kernal size 2; stride size 2; 6 24 24 -> 6 12 12\n            nn.ReLU(True),       # inplace=True means that it will modify the input directly thus save memory\n            nn.Conv2d(6, 16, 5), # 6 12 12 -> 16 8 8\n            nn.MaxPool2d(2, 2),  # 16 8 8 -> 16 4 4\n            nn.ReLU(True) \n        )\n        self.classifier =  nn.Sequential(\n            nn.Linear(16 * 4 * 4, 120),\n            nn.ReLU(),\n            nn.Linear(120, 84),\n            nn.ReLU(),\n            nn.Linear(84, 10),\n            nn.Softmax(1)\n        )\n\n    def forward(self, x, marg_idx=None, type=1):\n        \n        assert type == 1, \"only posterior computations are available for this network\"\n\n        # If the list of the pixel numbers to be marginalised is given,\n        # then genarate a marginalisation mask from it and apply to the\n        # tensor 'x'\n        if marg_idx:\n            batch_size = x.shape[0]\n            with torch.no_grad():\n                marg_mask = torch.ones_like(x, device=x.device).reshape(batch_size, 1, -1)\n                marg_mask[:, :, marg_idx] = 0\n                marg_mask = marg_mask.reshape_as(x)\n                marg_mask.requires_grad_(False)\n            x = torch.einsum('ijkl,ijkl->ijkl', x, marg_mask)\n        x = self.encoder(x)\n        x = x.view(-1, 16 * 4 * 4)\n        x = self.classifier(x)\n        return x", ""]}
{"filename": "src/experiments/mnist_top_k/__init__.py", "chunked_list": [""]}
{"filename": "src/experiments/mnist_top_k/dataGen.py", "chunked_list": ["import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\n\nclass MNIST_Addition(Dataset):\n\n    def __init__(self, dataset, examples, num_i, flat_for_pc):\n        self.data = list()\n        self.dataset = dataset\n        self.num_i = num_i\n        self.flat_for_pc = flat_for_pc\n\n        with open(examples) as f:\n            for line in f:\n                line = line.strip().split(' ')\n                self.data.append(tuple([int(i) for i in line]))\n\n    \n    def __getitem__(self, index):\n        if self.num_i == 2:\n            i1, i2, l = self.data[index]\n            l = ':- not addition(i1, i2, {}).'.format(l)\n            if self.flat_for_pc:\n                return {'i1': self.dataset[i1][0].flatten(), 'i2': self.dataset[i2][0].flatten()}, l\n            else:\n                return {'i1': self.dataset[i1][0], 'i2': self.dataset[i2][0]}, l\n\n        elif self.num_i == 3:\n            i1, i2, i3, l = self.data[index]\n            l = ':- not addition(i1, i2, i3, {}).'.format(l)\n            if self.flat_for_pc:\n                return {'i1': self.dataset[i1][0].flatten(), 'i2': self.dataset[i2][0].flatten(), 'i3': self.dataset[i3][0].flatten()}, l\n            else:\n                return {'i1': self.dataset[i1][0], 'i2': self.dataset[i2][0], 'i3': self.dataset[i3][0]}, l\n\n        elif self.num_i == 4:\n            i1, i2, i3, i4, l = self.data[index]\n            l = ':- not addition(i1, i2, i3, i4, {}).'.format(l)\n            if self.flat_for_pc:\n                return {'i1': self.dataset[i1][0].flatten(), 'i2': self.dataset[i2][0].flatten(), 'i3': self.dataset[i3][0].flatten(), 'i4': self.dataset[i4][0].flatten()}, l\n            else:\n                return {'i1': self.dataset[i1][0], 'i2': self.dataset[i2][0], 'i3': self.dataset[i3][0], 'i4': self.dataset[i4][0]}, l\n    \n        elif self.num_i == 6:\n            i1, i2, i3, i4, i5,i6, l = self.data[index]\n            l = ':- not addition(i1, i2, i3, i4, i5, i6, {}).'.format(l)\n            if self.flat_for_pc:\n                return {'i1': self.dataset[i1][0].flatten(), 'i2': self.dataset[i2][0].flatten(), 'i3': self.dataset[i3][0].flatten(), 'i4': self.dataset[i4][0].flatten(), 'i5': self.dataset[i5][0].flatten(), 'i6': self.dataset[i6][0].flatten()}, l\n            else:\n                return {'i1': self.dataset[i1][0], 'i2': self.dataset[i2][0], 'i3': self.dataset[i3][0], 'i4': self.dataset[i4][0], 'i5': self.dataset[i5][0], 'i6': self.dataset[i6][0]}, l\n    \n\n    def __len__(self):\n        return len(self.data)"]}
{"filename": "src/experiments/baseline_slot_attention/set_utils.py", "chunked_list": ["import scipy.optimize\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport os\n\n\ndef save_args(args, writer):\n\t# store args as txt file\n\twith open(os.path.join(writer.log_dir, 'args.txt'), 'w') as f:\n\t\tfor arg in vars(args):\n\t\t\tf.write(f\"\\n{arg}: {getattr(args, arg)}\")", "\n\ndef hungarian_matching(attrs, preds_attrs, verbose=0):\n\t\"\"\"\n\tReceives unordered predicted set and orders this to match the nearest GT set.\n\t:param attrs:\n\t:param preds_attrs:\n\t:param verbose:\n\t:return:\n\t\"\"\"\n\tassert attrs.shape[1] == preds_attrs.shape[1]\n\tassert attrs.shape == preds_attrs.shape\n\tfrom scipy.optimize import linear_sum_assignment\n\tmatched_preds_attrs = preds_attrs.clone()\n\tfor sample_id in range(attrs.shape[0]):\n\t\t# using euclidean distance\n\t\tcost_matrix = torch.cdist(attrs[sample_id], preds_attrs[sample_id]).detach().cpu()\n\n\t\tidx_mapping = linear_sum_assignment(cost_matrix)\n\t\t# convert to tuples of [(row_id, col_id)] of the cost matrix\n\t\tidx_mapping = [(idx_mapping[0][i], idx_mapping[1][i]) for i in range(len(idx_mapping[0]))]\n\n\t\tfor i, (row_id, col_id) in enumerate(idx_mapping):\n\t\t\tmatched_preds_attrs[sample_id, row_id, :] = preds_attrs[sample_id, col_id, :]\n\t\tif verbose:\n\t\t\tprint('GT: {}'.format(attrs[sample_id]))\n\t\t\tprint('Pred: {}'.format(preds_attrs[sample_id]))\n\t\t\tprint('Cost Matrix: {}'.format(cost_matrix))\n\t\t\tprint('idx mapping: {}'.format(idx_mapping))\n\t\t\tprint('Matched Pred: {}'.format(matched_preds_attrs[sample_id]))\n\t\t\tprint('\\n')\n\t\t\t# exit()\n\n\treturn matched_preds_attrs", "\n\ndef average_precision_shapeworld(pred, attributes, distance_threshold, dataset):\n\t\"\"\"Computes the average precision for CLEVR.\n\tThis function computes the average precision of the predictions specifically\n\tfor the CLEVR dataset. First, we sort the predictions of the model by\n\tconfidence (highest confidence first). Then, for each prediction we check\n\twhether there was a corresponding object in the input image. A prediction is\n\tconsidered a true positive if the discrete features are predicted correctly\n\tand the predicted position is within a certain distance from the ground truth\n\tobject.\n\tArgs:\n\t\t  pred: Tensor of shape [batch_size, num_elements, dimension] containing\n\t\t\tpredictions. The last dimension is expected to be the confidence of the\n\t\t\tprediction.\n\t\t  attributes: Tensor of shape [batch_size, num_elements, dimension] containing\n\t\t\tpredictions.\n\t\t  distance_threshold: Threshold to accept match. -1 indicates no threshold.\n\tReturns:\n\t\t  Average precision of the predictions.\n\t\"\"\"\n\n\t[batch_size, _, element_size] = attributes.shape\n\t[_, predicted_elements, _] = pred.shape\n\n\tdef unsorted_id_to_image(detection_id, predicted_elements):\n\t\t\"\"\"Find the index of the image from the unsorted detection index.\"\"\"\n\t\treturn int(detection_id // predicted_elements)\n\n\tflat_size = batch_size * predicted_elements\n\tflat_pred = np.reshape(pred, [flat_size, element_size])\n\t# sort_idx = np.argsort(flat_pred[:, -1], axis=0)[::-1]  # Reverse order.\n\tsort_idx = np.argsort(flat_pred[:, 0], axis=0)[::-1]  # Reverse order.\n\n\tsorted_predictions = np.take_along_axis(\n\t\tflat_pred, np.expand_dims(sort_idx, axis=1), axis=0)\n\tidx_sorted_to_unsorted = np.take_along_axis(\n\t\tnp.arange(flat_size), sort_idx, axis=0)\n\n\n\tdef process_targets_shapeworld4(target):\n\t\t\"\"\"Unpacks the target into the CLEVR properties.\"\"\"\n\t\t#col_enc + shape_enc + shade_enc + size_enc\n\t\treal_obj = target[0]\n\t\tcolor = np.argmax(target[1:9])\n\t\tshape = np.argmax(target[9:12])\n\t\tshade = np.argmax(target[12:14])\n\t\tsize = np.argmax(target[14:16])\n\t\treturn np.array([0,0,0]), size, shade, shape, color, real_obj\n\n\tdef process_targets_clevr(target):\n\t\t\"\"\"Unpacks the target into the CLEVR properties.\"\"\"\n\t\t#col_enc + shape_enc + shade_enc + size_enc\n  \n\n\t\treal_obj = target[0]\n\t\tsize = np.argmax(target[1:3])\n\t\tmaterial = np.argmax(target[3:5])\n\t\tshape = np.argmax(target[5:8])\n\t\tcolor = np.argmax(target[8:16]) \n  \n\t\treturn np.array([0,0,0]), size, material, shape, color, real_obj\n\n\n\tdef process_targets(target):\n\t\tif dataset == \"shapeworld4\":\n\t\t\treturn process_targets_shapeworld4(target)\n\t\telif dataset == \"clevr\":\n\t\t\treturn process_targets_clevr(target)\n\t\t\n\n\ttrue_positives = np.zeros(sorted_predictions.shape[0])\n\tfalse_positives = np.zeros(sorted_predictions.shape[0])\n\n\tdetection_set = set()\n\n\tfor detection_id in range(sorted_predictions.shape[0]):\n\t\t# Extract the current prediction.\n\t\tcurrent_pred = sorted_predictions[detection_id, :]\n\t\t# Find which image the prediction belongs to. Get the unsorted index from\n\t\t# the sorted one and then apply to unsorted_id_to_image function that undoes\n\t\t# the reshape.\n\t\toriginal_image_idx = unsorted_id_to_image(\n\t\t\tidx_sorted_to_unsorted[detection_id], predicted_elements)\n\t\t# Get the ground truth image.\n\t\tgt_image = attributes[original_image_idx, :, :]\n\n\t\t# Initialize the maximum distance and the id of the groud-truth object that\n\t\t# was found.\n\t\tbest_distance = 10000\n\t\tbest_id = None\n\n\t\t# Unpack the prediction by taking the argmax on the discrete\n\t\t# attributes.\n\t\t(pred_coords, pred_object_size, pred_material, pred_shape, pred_color,\n\t\t _) = process_targets(current_pred)\n\n\t\t# Loop through all objects in the ground-truth image to check for hits.\n\t\tfor target_object_id in range(gt_image.shape[0]):\n\t\t\ttarget_object = gt_image[target_object_id, :]\n\t\t\t# Unpack the targets taking the argmax on the discrete attributes.\n\t\t\t(target_coords, target_object_size, target_material, target_shape,\n\t\t\t target_color, target_real_obj) = process_targets(target_object)\n\t\t\t# Only consider real objects as matches.\n\t\t\tif target_real_obj:\n\t\t\t\t# For the match to be valid all attributes need to be correctly\n\t\t\t\t# predicted.\n\t\t\t\tpred_attr = [\n\t\t\t\t\tpred_object_size,\n\t\t\t\t\tpred_material,\n\t\t\t\t\tpred_shape,\n\t\t\t\t\tpred_color]\n\t\t\t\ttarget_attr = [\n\t\t\t\t\ttarget_object_size,\n\t\t\t\t\ttarget_material,\n\t\t\t\t\ttarget_shape,\n\t\t\t\t\ttarget_color]\n\t\t\t\tmatch = pred_attr == target_attr\n\t\t\t\tif match:\n\t\t\t\t\t# If a match was found, we check if the distance is below the\n\t\t\t\t\t# specified threshold. Recall that we have rescaled the coordinates\n\t\t\t\t\t# in the dataset from [-3, 3] to [0, 1], both for `target_coords` and\n\t\t\t\t\t# `pred_coords`. To compare in the original scale, we thus need to\n\t\t\t\t\t# multiply the distance values by 6 before applying the\n\t\t\t\t\t# norm.\n\t\t\t\t\tdistance = np.linalg.norm(\n\t\t\t\t\t\t(target_coords - pred_coords) * 6.)\n\n\t\t\t\t\t# If this is the best match we've found so far we remember\n\t\t\t\t\t# it.\n\t\t\t\t\tif distance < best_distance:\n\t\t\t\t\t\tbest_distance = distance\n\t\t\t\t\t\tbest_id = target_object_id\n\t\tif best_distance < distance_threshold or distance_threshold == -1:\n\t\t\t# We have detected an object correctly within the distance confidence.\n\t\t\t# If this object was not detected before it's a true positive.\n\t\t\tif best_id is not None:\n\t\t\t\tif (original_image_idx, best_id) not in detection_set:\n\t\t\t\t\ttrue_positives[detection_id] = 1\n\t\t\t\t\tdetection_set.add((original_image_idx, best_id))\n\t\t\t\telse:\n\t\t\t\t\tfalse_positives[detection_id] = 1\n\t\t\telse:\n\t\t\t\tfalse_positives[detection_id] = 1\n\t\telse:\n\t\t\tfalse_positives[detection_id] = 1\n\n\taccumulated_fp = np.cumsum(false_positives)\n\taccumulated_tp = np.cumsum(true_positives)\n\n\trecall_array = accumulated_tp / np.sum(attributes[:, :, 0])\n\tprecision_array = np.divide(\n\t\taccumulated_tp,\n\t\t(accumulated_fp + accumulated_tp))\n\n\treturn compute_average_precision(\n\t\tnp.array(precision_array, dtype=np.float32),\n\t\tnp.array(recall_array, dtype=np.float32))", "\n\ndef compute_average_precision(precision, recall):\n\t\"\"\"Computation of the average precision from precision and recall arrays.\"\"\"\n\trecall = recall.tolist()\n\tprecision = precision.tolist()\n\trecall = [0] + recall + [1]\n\tprecision = [0] + precision + [0]\n\n\tfor i in range(len(precision) - 1, -0, -1):\n\t\tprecision[i - 1] = max(precision[i - 1], precision[i])\n\n\tindices_recall = [\n\t\ti for i in range(len(recall) - 1) if recall[1:][i] != recall[:-1][i]\n\t]\n\n\taverage_precision = 0.\n\tfor i in indices_recall:\n\t\taverage_precision += precision[i + 1] * (recall[i + 1] - recall[i])\n\treturn average_precision", "\n\ndef hungarian_loss(predictions, targets):\n\t# permute dimensions for pairwise distance computation between all slots\n\tpredictions = predictions.permute(0, 2, 1)\n\ttargets = targets.permute(0, 2, 1)\n\n\t# predictions and targets shape :: (n, c, s)\n\tpredictions, targets = outer(predictions, targets)\n\t# squared_error shape :: (n, s, s)\n\tsquared_error = F.smooth_l1_loss(predictions, targets.expand_as(predictions), reduction=\"none\").mean(1)\n\n\tsquared_error_np = squared_error.detach().cpu().numpy()\n\tindices = map(hungarian_loss_per_sample, squared_error_np)\n\tlosses = [\n\t\tsample[row_idx, col_idx].mean()\n\t\tfor sample, (row_idx, col_idx) in zip(squared_error, indices)\n\t]\n\ttotal_loss = torch.mean(torch.stack(list(losses)))\n\treturn total_loss", "\n\n\ndef hungarian_loss_per_sample(sample_np):\n\treturn scipy.optimize.linear_sum_assignment(sample_np)\n\n\ndef scatter_masked(tensor, mask, binned=False, threshold=None):\n\ts = tensor[0].detach().cpu()\n\tmask = mask[0].detach().clamp(min=0, max=1).cpu()\n\tif binned:\n\t\ts = s * 128\n\t\ts = s.view(-1, s.size(-1))\n\t\tmask = mask.view(-1)\n\tif threshold is not None:\n\t\tkeep = mask.view(-1) > threshold\n\t\ts = s[:, keep]\n\t\tmask = mask[keep]\n\treturn s, mask", "\n\ndef outer(a, b=None):\n\t\"\"\" Compute outer product between a and b (or a and a if b is not specified). \"\"\"\n\tif b is None:\n\t\tb = a\n\tsize_a = tuple(a.size()) + (b.size()[-1],)\n\tsize_b = tuple(b.size()) + (a.size()[-1],)\n\ta = a.unsqueeze(dim=-1).expand(*size_a)\n\tb = b.unsqueeze(dim=-2).expand(*size_b)\n\treturn a, b"]}
{"filename": "src/experiments/baseline_slot_attention/train.py", "chunked_list": ["import os\nimport argparse\nfrom datetime import datetime\nimport time\nimport sys\nsys.path.append('../../')\n\nimport torch\nimport torch.nn.functional as F\nimport matplotlib", "import torch.nn.functional as F\nimport matplotlib\nmatplotlib.use(\"Agg\")\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torch.multiprocessing as mp\n\nimport scipy.optimize\nimport numpy as np", "import scipy.optimize\nimport numpy as np\n#from tqdm import tqdm\nfrom rtpt import RTPT\n\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom dataGen import SHAPEWORLD4,CLEVR, get_loader\nimport slot_attention_module as model", "from dataGen import SHAPEWORLD4,CLEVR, get_loader\nimport slot_attention_module as model\nimport set_utils as set_utils\n\nimport utils as misc_utils\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    # generic params\n    parser.add_argument(\n        \"--name\",\n        default=datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"),\n        help=\"Name to store the log file as\",\n    )\n    parser.add_argument(\"--resume\", help=\"Path to log file to resume from\")\n\n    parser.add_argument(\n        \"--seed\", type=int, default=10, help=\"Random generator seed for all frameworks\"\n    )\n    parser.add_argument(\n        \"--epochs\", type=int, default=10, help=\"Number of epochs to train with\"\n    )\n    parser.add_argument(\n        \"--ap-log\", type=int, default=10, help=\"Number of epochs before logging AP\"\n    )\n    parser.add_argument(\n        \"--lr\", type=float, default=1e-2, help=\"Outer learning rate of model\"\n    )\n    parser.add_argument(\n        \"--warmup-epochs\", type=int, default=10, help=\"Number of steps fpr learning rate warm up\"\n    )\n    parser.add_argument(\n        \"--decay-epochs\", type=int, default=10, help=\"Number of steps fpr learning rate decay\"\n    )\n    parser.add_argument(\n        \"--batch-size\", type=int, default=32, help=\"Batch size to train with\"\n    )\n    parser.add_argument(\n        \"--num-workers\", type=int, default=6, help=\"Number of threads for data loader\"\n    )\n    parser.add_argument(\n        \"--dataset\",\n        choices=[\"shapeworld4\", \"clevr\"],\n        help=\"Use shapeworld4 dataset\",\n    )\n    parser.add_argument(\n        \"--cogent\", action='store_true',\n        help=\"Evaluate on the CoGenT test of the dataset\",\n    )\n    parser.add_argument(\n        \"--no-cuda\",\n        action=\"store_true\",\n        help=\"Run on CPU instead of GPU (not recommended)\",\n    )\n    parser.add_argument(\n        \"--train-only\", action=\"store_true\", help=\"Only run training, no evaluation\"\n    )\n    parser.add_argument(\n        \"--eval-only\", action=\"store_true\", help=\"Only run evaluation, no training\"\n    )\n    parser.add_argument(\"--multi-gpu\", action=\"store_true\", help=\"Use multiple GPUs\")\n    parser.add_argument(\"--credentials\", type=str, help=\"Credentials for rtpt\")\n    parser.add_argument(\"--export-dir\", type=str, help=\"Directory to output samples to\")\n    parser.add_argument(\"--data-dir\", type=str, help=\"Directory to data\")\n\n    # Slot attention params\n    parser.add_argument('--n-slots', default=10, type=int,\n                        help='number of slots for slot attention module')\n    parser.add_argument('--n-iters-slot-att', default=3, type=int,\n                        help='number of iterations in slot attention module')\n    parser.add_argument('--n-attr', default=18, type=int,\n                        help='number of attributes per object')\n\n    args = parser.parse_args()\n\n    if args.no_cuda:\n        args.device = 'cpu'\n    else:\n        args.device = 'cuda:0'\n\n    misc_utils.set_manual_seed(args.seed)\n\n    args.name += f'-{args.seed}'\n\n    return args", "\n\ndef run(net, loader, optimizer, criterion, writer, args, test_cond = None, train=False, epoch=0):\n    if train:\n        net.train()\n        prefix = \"train\"\n        torch.set_grad_enabled(True)\n    else:\n        net.eval()\n        prefix = \"test\"\n        torch.set_grad_enabled(False)\n\n        preds_all = torch.zeros(0, args.n_slots, args.n_attr)\n        target_all = torch.zeros(0, args.n_slots, args.n_attr)\n\n    iters_per_epoch = len(loader)\n\n    \n    for i, sample in enumerate(loader, start=epoch * iters_per_epoch):\n\n        start = time.time()\n\n        # input is either a set or an image\n        if 'cuda' in args.device:\n            imgs, target_set = map(lambda x: x.cuda(), sample)\n        else:\n            imgs, target_set = sample\n\n        load = time.time()\n        #print(\"\\nload\", load-start)\n\n        output = net.forward(imgs)\n\n        loss = set_utils.hungarian_loss(output, target_set)\n\n        forward = time.time()\n        #print(\"forward\", forward-load)\n\n        if train:\n\n            # apply lr schedulers\n            if epoch < args.warmup_epochs:\n                lr = args.lr * ((epoch + 1) / args.warmup_epochs)\n            else:\n                lr = args.lr\n            lr = lr * 0.5 ** ((epoch + 1) / args.decay_epochs)\n            optimizer.param_groups[0]['lr'] = lr\n\n            optimizer.zero_grad()\n            loss.backward(retain_graph=True)\n            optimizer.step()\n            backward = time.time()\n            #print(\"backward\", backward-forward)\n\n        \n            writer.add_scalar(\"train/loss_baseline\", loss.item(), global_step=i)\n            log = time.time()\n            #print(\"log\", log-backward)\n\n            #writer.add_scalar(\"lr/\", optimizer.param_groups[0][\"lr\"], global_step=i)\n        else:\n            if i % iters_per_epoch == 0:\n\n                preds_all = torch.cat((preds_all, output.detach().cpu()), 0)\n                target_all = torch.cat((target_all, target_set.detach().cpu()), 0)\n\n                ap = [\n                    set_utils.average_precision_shapeworld(preds_all.detach().cpu().numpy(),\n                                                      target_all.detach().cpu().numpy(), d, args.dataset)\n                    for d in [-1]  # since we are not using 3D coords #[-1., 1., 0.5, 0.25, 0.125]\n                ]\n\n                print(f\"\\nCurrent AP: \", ap[0], \" %\\n\")\n                if test_cond == \"a\":\n                    writer.add_scalar(\"test/ap_cond_a\", ap[0], global_step=i)\n                elif test_cond == \"b\":\n                    writer.add_scalar(\"test/ap_cond_b\", ap[0], global_step=i)\n                else: \n                    writer.add_scalar(\"test/ap\", ap[0], global_step=i)\n                return ap\n            \n    if train:\n        print(f\"Epoch {epoch} Train Loss: {loss.item()}\")", "\n\ndef main():\n    args = get_args()\n\n    writer = SummaryWriter(os.path.join(\"runs\", args.name), purge_step=0)\n\n    if args.cogent:\n        if args.dataset == \"clevr\":\n            dataset_train = CLEVR(args.data_dir, \"trainA\")\n            dataset_test_a = CLEVR(args.data_dir, \"valA\")\n            dataset_test_b = CLEVR(args.data_dir, \"valB\")\n        elif args.dataset == \"shapeworld4\":\n            dataset_train = SHAPEWORLD4(args.data_dir, \"train_a\")\n            dataset_test_a = SHAPEWORLD4(args.data_dir, \"val_a\")\n            dataset_test_b = SHAPEWORLD4(args.data_dir, \"val_b\")\n    else:\n        if args.dataset == \"clevr\":\n            dataset_train = CLEVR(args.data_dir, \"train\")\n            dataset_test = CLEVR(args.data_dir, \"val\")\n        elif args.dataset == \"shapeworld4\":\n            dataset_train = SHAPEWORLD4(args.data_dir, \"train\")\n            dataset_test = SHAPEWORLD4(args.data_dir, \"val\")\n        \n    print('data loaded')\n\n    if not args.eval_only:\n        train_loader = get_loader(\n            dataset_train, batch_size=args.batch_size, num_workers=args.num_workers\n        )\n    if not args.train_only:\n        if args.cogent:\n            test_loader_a = get_loader(\n                dataset_test_a,\n                batch_size=5000,\n                num_workers=args.num_workers,\n                shuffle=False)\n            test_loader_b = get_loader(\n                dataset_test_b,\n                batch_size=5000,\n                num_workers=args.num_workers,\n                shuffle=False)\n        else:\n            test_loader = get_loader(\n                dataset_test,\n                batch_size=5000,\n                num_workers=args.num_workers,\n                shuffle=False)\n        \n\n    # print(torch.cuda.is_available())\n    if args.dataset == \"shapeworld4\":\n        net = model.SlotAttention_model(n_slots=4, n_iters=3, n_attr=15,\n                                        encoder_hidden_channels=32,\n                                        attention_hidden_channels=64,\n                                        mlp_prediction=True,\n                                        device=args.device)\n    elif args.dataset == \"clevr\":\n        net = model.SlotAttention_model(n_slots=10, n_iters=3, n_attr=15,\n                                        encoder_hidden_channels=64,\n                                        attention_hidden_channels=128,\n                                        mlp_prediction=True,\n                                        device=args.device,\n                                        clevr_encoding=True)\n        \n    args.n_attr = net.n_attr\n\n\n    if not args.no_cuda:\n        net = net.cuda()\n\n    optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n\n    criterion = torch.nn.SmoothL1Loss()\n    # Create RTPT object\n    rtpt = RTPT(name_initials=args.credentials, experiment_name=f\"Set prediction baseline \",\n                max_iterations=args.epochs)\n\n    # store args as txt file\n    set_utils.save_args(args, writer)\n\n\n\n    ap_list = []\n    ap_list_a = [] # for cogent\n    ap_list_b = [] # for cogent\n    total_train_time = 0\n    total_test_time = 0\n    time_list =[]\n    time_start = time.time()\n\n    start_epoch = 0\n    if args.resume:\n        print(\"Loading ckpt ...\")\n        log = torch.load(args.resume)\n        weights = log[\"weights\"]\n        net.load_state_dict(weights, strict=True)\n        start_epoch = log[\"args\"][\"epochs_trained\"] + 1\n\n        ap_list = log[\"ap_list\"]\n        ap_list_a = log[\"ap_list_a\"]\n        ap_list_b = log[\"ap_list_b\"]\n        time_list =log[\"time_list\"]\n        print(\"continue with epoch\", start_epoch)\n\n\n\n        # Start the RTPT tracking\n    rtpt.start()\n\n\n    for epoch in np.arange(start_epoch, args.epochs):\n        print(\"Epoch:\", epoch)\n        if not args.eval_only:\n            train_start = time.time()\n            run(net, train_loader, optimizer, criterion, writer, args, train=True, epoch=epoch)\n            time_train = time.time() - train_start\n            rtpt.step()\n        if not args.train_only:\n            test_start = time.time()\n            if args.cogent:\n                ap_a = run(net, test_loader_a, None, criterion, writer, args, test_cond=\"a\", train=False, epoch=epoch)\n                ap_list_a.append(ap_a)\n                ap_b = run(net, test_loader_b, None, criterion, writer, args, test_cond=\"b\", train=False, epoch=epoch)\n                ap_list_b.append(ap_b)\n            else:\n                ap = run(net, test_loader, None, criterion, writer, args, train=False, epoch=epoch)\n                ap_list.append(ap)\n            \n            \n            time_test = time.time() - test_start \n            if args.eval_only:\n                exit()\n        torch.cuda.empty_cache()\n        \n        args.epochs_trained = epoch\n\n        total_test_time += time_test\n        total_train_time += time_train\n        time_list.append([total_train_time, total_test_time, time_train, time_test])\n\n\n        results = {\n                \"name\": args.name,\n                \"weights\": net.state_dict(),\n                \"args\": vars(args),\n                \"ap_list\": ap_list, #empty for cogent\n                \"ap_list_a\": ap_list_a,\n                \"ap_list_b\": ap_list_b,\n                \"time\": time_list}\n\n        torch.save(results, os.path.join(\"runs\", args.name, args.name))\n        if args.eval_only:\n            break\n\n        print(\"total time\", misc_utils.time_delta_now(time_start))", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/experiments/baseline_slot_attention/dataGen.py", "chunked_list": ["import torch\nimport torchvision\n\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import transforms\n\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom skimage import io\nimport os", "from skimage import io\nimport os\nimport numpy as np\nimport random\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport json\nimport datasets as datasets", "import json\nimport datasets as datasets\n\n\ndef get_loader(dataset, batch_size, num_workers=8, shuffle=True):\n    '''\n    Returns and iterable dataset with specified batchsize and shuffling.\n    '''\n    return torch.utils.data.DataLoader(\n        dataset,\n        shuffle=shuffle,\n        batch_size=batch_size,\n        num_workers=num_workers\n)", "\n\ndef get_encoding_shapeworld(color, shape, shade, size):\n    \n    if color == 'red':\n        col_enc = [1,0,0,0,0,0,0,0]\n    elif color == 'blue':\n        col_enc = [0,1,0,0,0,0,0,0]\n    elif color == 'green':\n        col_enc = [0,0,1,0,0,0,0,0]\n    elif color == 'gray':\n        col_enc = [0,0,0,1,0,0,0,0]\n    elif color == 'brown':\n        col_enc = [0,0,0,0,1,0,0,0]\n    elif color == 'magenta':\n        col_enc = [0,0,0,0,0,1,0,0]\n    elif color == 'cyan':\n        col_enc = [0,0,0,0,0,0,1,0]\n    elif color == 'yellow':\n        col_enc = [0,0,0,0,0,0,0,1]\n\n    if shape == 'circle':\n        shape_enc = [1,0,0]\n    elif shape == 'triangle':\n        shape_enc = [0,1,0]\n    elif shape == 'square':\n        shape_enc = [0,0,1]    \n   \n    if shade == 'bright':\n        shade_enc = [1,0]\n    elif shade =='dark':\n        shade_enc = [0,1]\n\n             \n    if size == 'small':\n        size_enc = [1,0]\n    elif size == 'big':\n        size_enc = [0,1]\n    \n    return np.array([1]+ col_enc + shape_enc + shade_enc + size_enc)", "    \n    \nclass SHAPEWORLD4(Dataset):\n    def __init__(self, root, mode, learn_concept='default', bg_encoded=True):\n        \n        datasets.maybe_download_shapeworld4()\n\n        self.root = root\n        self.mode = mode\n        assert os.path.exists(root), 'Path {} does not exist'.format(root)\n\n        #dictionary of the form {'image_idx':'img_path'}\n        self.img_paths = {}\n        \n        \n        for file in os.scandir(os.path.join(root, 'images', mode)):\n            img_path = file.path\n            \n            img_path_idx =   img_path.split(\"/\")\n            img_path_idx = img_path_idx[-1]\n            img_path_idx = img_path_idx[:-4][6:]\n            try:\n                img_path_idx =  int(img_path_idx)\n                self.img_paths[img_path_idx] = img_path\n            except:\n                print(\"path:\",img_path_idx)\n                \n\n        \n        count = 0\n        \n        #target maps of the form {'target:idx': observation string} or {'target:idx': obj encoding}\n        self.obj_map = {}\n                \n        with open(os.path.join(root, 'labels', mode,\"world_model.json\")) as f:\n            worlds = json.load(f)\n            \n            \n            \n            #iterate over all objects\n            for world in worlds:\n                num_objects = 0\n                target_obs = \"\"\n                obj_enc = []\n                for entity in world['entities']:\n                    \n                    color = entity['color']['name']\n                    shape = entity['shape']['name']\n                    \n                    shade_val = entity['color']['shade']\n                    if shade_val == 0.0:\n                        shade = 'bright'\n                    else:\n                        shade = 'dark'\n                    \n                    size_val = entity['shape']['size']['x']\n                    if size_val == 0.075:\n                        size = 'small'\n                    elif size_val == 0.15:\n                        size = 'big'\n                    \n                    name = 'o' + str(num_objects+1)\n                    obj_enc.append(get_encoding_shapeworld(color, shape, shade, size))\n                    num_objects += 1\n                    \n                #bg encodings\n                for i in range(num_objects, 4):\n                    name = 'o' + str(num_objects+1)\n                    obj_enc.append(np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]))\n                    num_objects += 1\n\n                self.obj_map[count] = torch.Tensor(obj_enc)\n                count+=1\n\n    def __getitem__(self, index):\n        \n        #get the image\n        img_path = self.img_paths[index]\n        img = io.imread(img_path)[:, :, :3]\n        \n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            #transforms.CenterCrop(250),\n            #transforms.Resize((32, 32)),\n            transforms.ToTensor(),\n        ])\n        img = transform(img)\n        img = (img - 0.5) * 2.0  # Rescale to [-1, 1].\n\n        return img, self.obj_map[index]#, mask\n        \n    def __len__(self):\n        return len(self.img_paths)", "\ndef get_encoding_clevr(size, material, shape, color ):\n    \n    #size (small, large, bg)\n    if size == \"small\":\n        size_enc = [1,0]\n    elif size == \"large\":\n        size_enc = [0,1]\n    \n    #material (rubber, metal, bg)\n    if material == \"rubber\":\n        material_enc = [1,0]\n    elif material == \"metal\":\n        material_enc = [0,1]\n    \n    #shape (cube, sphere, cylinder, bg)\n    if shape == \"cube\":\n        shape_enc = [1,0,0]\n    elif shape == \"sphere\":\n        shape_enc = [0,1,0]\n    elif shape == \"cylinder\":\n        shape_enc = [0,0,1]\n\n    \n    #color (gray, red, blue, green, brown, purple, cyan, yellow, bg)\n    if color == \"gray\":\n        color_enc = [1,0,0,0,0,0,0,0]\n    elif color == \"red\":\n        color_enc = [0,1,0,0,0,0,0,0]\n    elif color == \"blue\":\n        color_enc = [0,0,1,0,0,0,0,0]\n    elif color == \"green\":\n        color_enc = [0,0,0,1,0,0,0,0]\n    elif color == \"brown\":\n        color_enc = [0,0,0,0,1,0,0,0]\n    elif color == \"purple\":\n        color_enc = [0,0,0,0,0,1,0,0]\n    elif color == \"cyan\":\n        color_enc = [0,0,0,0,0,0,1,0]\n    elif color == \"yellow\":\n        color_enc = [0,0,0,0,0,0,0,1]\n\n        \n    return np.array([1] + size_enc + material_enc + shape_enc + color_enc )", "\n\nclass CLEVR(Dataset):\n    def __init__(self, root, mode, img_paths=None, files_names=None, obj_num=None):\n        self.root = root  # The root folder of the dataset\n        self.mode = mode  # The mode of 'train' or 'val'\n        self.files_names = files_names # The list of the files names with correct nuber of objects\n        if obj_num is not None:\n            self.obj_num = obj_num  # The upper limit of number of objects \n        else:\n            self.obj_num = 10\n\n        assert os.path.exists(root), 'Path {} does not exist'.format(root)\n\n        #list of sorted image paths\n        self.img_paths = []\n        if img_paths:\n            self.img_paths = img_paths\n        else:                    \n            #open directory and save all image paths\n            for file in os.scandir(os.path.join(root, 'images', mode)):\n                img_path = file.path\n                if '.png' in img_path:\n                    self.img_paths.append(img_path)\n\n        self.img_paths.sort()\n        self.img_paths = np.array(self.img_paths, dtype=str)\n\n        count = 0\n        \n        #target maps of the form {'target:idx': query string} or {'target:idx': obj encoding}\n        #self.obj_map = {}\n\n        \n        count = 0        \n        #We have up to 10 objects in the image, load the json file\n        with open(os.path.join(root, 'scenes','CLEVR_'+ mode+\"_scenes.json\")) as f:\n            data = json.load(f)\n            \n            self.obj_map = np.empty((len(data['scenes']),10,16), dtype=np.float32)\n            \n            #iterate over each scene and create the query string and obj encoding\n            print(\"parsing scences\")\n            for scene in data['scenes']:\n                obj_encoding_list = []\n\n                if self.files_names:\n                    if any(scene['image_filename'] in file_name for file_name in files_names):                    \n                        num_objects = 0\n                        for idx, obj in enumerate(scene['objects']):\n                            obj_encoding_list.append(get_encoding_clevr(obj['size'], obj['material'], obj['shape'], obj['color']))\n                            num_objects = idx+1 #store the num of objects \n                        #fill in background objects\n                        for idx in range(num_objects, self.obj_num):\n                            obj_encoding_list.append([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n                        self.obj_map[count] = np.array(obj_encoding_list)\n                        count += 1\n                else:\n                    num_objects=0\n                    for idx, obj in enumerate(scene['objects']):\n                        obj_encoding_list.append(get_encoding_clevr(obj['size'], obj['material'], obj['shape'], obj['color']))\n                        num_objects = idx+1 #store the num of objects \n                    #fill in background objects\n                    for idx in range(num_objects, 10):\n                        obj_encoding_list.append([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n                    self.obj_map[scene['image_index']] = np.array(obj_encoding_list, dtype=np.float32)\n                \n            print(\"done\")\n        if self.files_names:\n            print(f'Correctly found images {count} out of {len(files_names)}')\n\n    def __getitem__(self, index):\n        \n        #get the image\n        img_path = self.img_paths[index]\n        img = io.imread(img_path)[:, :, :3]\n        img = Image.fromarray(img).resize((128,128)) #using transforms to resize gets us a shrared-memory leak :(\n\n        transform = transforms.Compose([\n            #transforms.ToPILImage(),\n            #transforms.CenterCrop((29, 221,64, 256)), #why do we need to crop?\n            #transforms.Resize((128, 128)),\n            transforms.ToTensor(),\n        ])\n        img = transform(img)\n        img = (img - 0.5) * 2.0  # Rescale to [-1, 1].\n\n        return img, self.obj_map[index]#, mask\n        \n    def __len__(self):\n        return self.img_paths.shape[0]"]}
{"filename": "src/experiments/vqa/cmd_args2.py", "chunked_list": ["\"\"\"\nThe source code is based on:\nScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\nJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\nAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\nhttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\"\"\"\n\n\nimport argparse", "\nimport argparse\nimport sys\nimport random\nimport numpy as np\nimport torch\nimport os\nimport logging\n\n# Utility function for take in yes/no and convert it to boolean\ndef convert_str_to_bool(cmd_args):\n    for key, val in vars(cmd_args).items():\n        if val == \"yes\":\n            setattr(cmd_args, key, True)\n        elif val == \"no\":\n            setattr(cmd_args, key, False)", "\n# Utility function for take in yes/no and convert it to boolean\ndef convert_str_to_bool(cmd_args):\n    for key, val in vars(cmd_args).items():\n        if val == \"yes\":\n            setattr(cmd_args, key, True)\n        elif val == \"no\":\n            setattr(cmd_args, key, False)\n\ndef str2bool(v):\n    if isinstance(v, bool):\n       return v\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')", "\ndef str2bool(v):\n    if isinstance(v, bool):\n       return v\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')", "\nclass LearningSetting(object):\n\n    def __init__(self):\n        data_dir = os.path.abspath(os.path.join(os.path.abspath(__file__), \"../../data\"))\n        dataset_dir = os.path.join(data_dir, \"dataset\")\n        knowledge_base_dir = os.path.join(data_dir, \"knowledge_base\")\n\n        self.parser = argparse.ArgumentParser(description='Argparser', allow_abbrev=True)\n        # Logistics\n        self.parser.add_argument(\"--seed\", default=1234, type=int, help=\"set random seed\")\n        self.parser.add_argument(\"--gpu\", default=0, type=int, help=\"GPU id\")\n        self.parser.add_argument('--timeout', default=600, type=int, help=\"Execution timeout\")\n\n        # Learning settings\n        self.parser.add_argument(\"--name_threshold\", default=0, type=float)\n        self.parser.add_argument(\"--attr_threshold\", default=0, type=float)\n        self.parser.add_argument(\"--rela_threshold\", default=0, type=float)\n        # self.parser.add_argument(\"--name_topk_n\", default=-1, type=int)\n        # self.parser.add_argument(\"--attr_topk_n\", default=-1, type=int)\n        # self.parser.add_argument(\"--rela_topk_n\", default=80, type=int)\n        self.parser.add_argument(\"--topk\", default=3, type=int)\n\n        # training settings\n        self.parser.add_argument('--feat_dim', type=int, default=2048)\n        self.parser.add_argument('--n_epochs', type=int, default=20)\n        self.parser.add_argument('--batch_size', type=int, default=1)\n        self.parser.add_argument('--max_workers', type=int, default=1)\n        self.parser.add_argument('--axiom_update_size', type=int, default=4)\n        self.parser.add_argument('--name_lr', type=float, default=0.0001)\n        self.parser.add_argument('--attr_lr', type=float, default=0.0001)\n        self.parser.add_argument('--rela_lr', type=float, default=0.0001)\n        self.parser.add_argument('--reinforce', type=str2bool, nargs='?', const=True, default=False) # reinforce only support single thread\n        self.parser.add_argument('--replays', type=int, default=5)\n\n        self.parser.add_argument('--model_dir', default=data_dir+'/model_ckpts_sg')\n        # self.parser.add_argument('--model_dir', default=None)\n        self.parser.add_argument('--log_name', default='model.log')\n        self.parser.add_argument('--feat_f', default=data_dir+'/features.npy')\n        self.parser.add_argument('--train_f', default=dataset_dir+'/task_list/train_tasks_c2_10.pkl')\n        self.parser.add_argument('--val_f', default=dataset_dir+'/task_list/val_tasks.pkl')\n        self.parser.add_argument('--test_f', default=dataset_dir+'/task_list/test_tasks_c2_1000.pkl')\n        self.parser.add_argument('--cul_prov', type=bool, default=False)\n\n        self.parser.add_argument('--meta_f', default=data_dir+'/gqa_info.json')\n        self.parser.add_argument('--scene_f', default=data_dir+'/gqa_formatted_scene_graph.pkl')\n        self.parser.add_argument('--image_data_f', default=data_dir+'/image_data.json')\n        self.parser.add_argument('--dataset_type', default='name') # name, relation, attr:<groupindex>\n\n        self.parser.add_argument('--function', default=None) #KG_Find / Hypernym_Find / Find_Name / Find_Attr / Relate / Relate_Reverse / And / Or\n        self.parser.add_argument('--knowledge_base_dir', default=knowledge_base_dir)\n        # self.parser.add_argument('--interp_size', type=int, default=2)\n\n        self.parser.add_argument('--save_dir', default=data_dir+\"/problog_data\")\n        self.args = self.parser.parse_args(sys.argv[1:])", "\nls = LearningSetting()\ncmd_args = ls.args\n# print(cmd_args)\n\n# Fix random seed for debugging purpose\nif (ls.args.seed != None):\n    random.seed(ls.args.seed)\n    np.random.seed(ls.args.seed)\n    torch.manual_seed(ls.args.seed)", "\nif not type(cmd_args.gpu) == None:\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(cmd_args.gpu)\n    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n\nif cmd_args.model_dir is not None:\n    if not os.path.exists(cmd_args.model_dir):\n        os.makedirs(cmd_args.model_dir)\n\n    log_path = os.path.join(cmd_args.model_dir, cmd_args.log_name)\n    logging.basicConfig(filename=log_path, filemode='w', level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s')\n    logging.info(cmd_args)\n    logging.info(\"start!\")", ""]}
{"filename": "src/experiments/vqa/vqa_utils.py", "chunked_list": ["\"\"\"\nThe source code is based on:\nScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\nJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\nAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\nhttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\"\"\"\n\nimport os\nimport json", "import os\nimport json\nimport argparse\nimport numpy as np\nimport torch\nimport random\nimport pickle\nfrom sklearn import metrics\n\nTIME_OUT = 120", "\nTIME_OUT = 120\n\n###########################################################################\n# basic utilities\n\n\ndef to_image_id_dict(dict_list):\n    ii_dict = {}\n    for dc in dict_list:\n        image_id = dc['image_id']\n        ii_dict[image_id] = dc\n    return ii_dict", "\n\narticles = ['a', 'an', 'the', 'some', 'it']\n\n\ndef remove_article(st):\n    st = st.split()\n    st = [word for word in st if word not in articles]\n    return \" \".join(st)\n", "\n################################################################################################################\n# AUC calculation\n\ndef get_recall(labels, logits, topk=2):\n    #a = torch.tensor([[0, 1, 1], [1, 1, 0], [0, 0, 1],[1, 1, 1]])\n    #b = torch.tensor([[0, 0, 0], [0, 1, 0], [0, 0, 1],[0, 1, 1]])\n\n    # Calculate the recall\n    _, pred = logits.topk(topk, 1, True, True) #get idx of the biggest k values in the tensor\n    print(pred)\n\n    pred = pred.t() #transpose\n\n    #gather gets the elements from a given indices tensor. Here we get the elements at the same positions from our (top5) predictions\n    #we then sum all entries up along the axis and therefore count the top-5 entries in the labels tensors at the prediction position indices\n    correct = torch.sum(labels.gather(1, pred.t()), dim=1)\n    print(\"correct\", correct)\n\n    #now we some up all true labels. We clamp them to be maximum top5\n    correct_label = torch.clamp(torch.sum(labels, dim = 1), 0, topk)\n\n    #we now can compare if the number of correctly  found top-5 labels on the predictions vector is the same as on the same positions as the GT vector\n    print(\"correct label\", correct_label)\n\n    accuracy = torch.mean(correct / correct_label).item()\n\n    return accuracy", "\ndef single_auc_score(label, pred):\n    label = label\n    pred = [p.item() if not (type(p) == float or type(p) == int)\n            else p for p in pred]\n    if len(set(label)) == 1:\n        auc = -1\n    else:\n        fpr, tpr, thresholds = metrics.roc_curve(label, pred, pos_label=1)\n        auc = metrics.auc(fpr, tpr)\n    return auc", "\n\ndef auc_score(labels, preds):\n    if type(labels) == torch.Tensor:\n        labels = labels.long().cpu().detach().numpy()\n    if type(preds) == torch.Tensor:\n        preds = preds.cpu().detach().numpy()\n\n    if (type(labels) == torch.Tensor or type(labels) == np.ndarray) and len(labels.shape) == 2:\n        aucs = []\n        for label, pred in zip(labels, preds):\n            auc_single = single_auc_score(label, pred)\n            if not auc_single < 0:\n                aucs.append(auc_single)\n        auc = np.array(aucs).mean()\n    else:\n        auc = single_auc_score(labels, preds)\n\n    return auc", "\n\ndef to_binary_labels(labels, attr_num):\n    if labels.shape[-1] == attr_num:\n        return labels\n\n    binary_labels = torch.zeros((labels.shape[0], attr_num))\n    for ct, label in enumerate(labels):\n        binary_labels[ct][label] = 1\n    return binary_labels", "\n\n##############################################################################\n# model loading\ndef get_default_args():\n\n    DATA_ROOT = os.path.abspath(os.path.join(\n        os.path.abspath(__file__), \"../../data\"))\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpu', type=int, default=2)\n    parser.add_argument('--seed', type=int, default=1234)\n    parser.add_argument('--feat_dim', type=int, default=2048)\n    # parser.add_argument('--type', default='name')\n    parser.add_argument('--model_dir', default=DATA_ROOT + '/model_ckpts')\n    parser.add_argument('--meta_f', default=DATA_ROOT + '/preprocessing/gqa_info.json')\n    args = parser.parse_args()\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)\n\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    random.seed(args.seed)\n\n    return args", "\n\ndef gen_task(formatted_scene_graph_path, vg_image_data_path, questions_path, features_path, n=100, q_length=None):\n\n    with open(questions_path, 'rb') as questions_file:\n        questions = pickle.load(questions_file)\n\n    with open(formatted_scene_graph_path, 'rb') as vg_scene_graphs_file:\n        scene_graphs = pickle.load(vg_scene_graphs_file)\n\n    with open(vg_image_data_path, 'r') as vg_image_data_file:\n        image_data = json.load(vg_image_data_file)\n\n    features = np.load(features_path, allow_pickle=True)\n    features = features.item()\n\n    image_data = to_image_id_dict(image_data)\n\n    for question in questions[:n]:\n\n        if q_length is not None:\n            current_len = len(question['clauses'])\n            if not current_len == q_length:\n                continue\n\n        # functions = question[\"clauses\"]\n        image_id = question[\"image_id\"]\n        scene_graph = scene_graphs[image_id]\n        cur_image_data = image_data[image_id]\n\n        if not scene_graph['image_id'] == image_id or not cur_image_data['image_id'] == image_id:\n            raise Exception(\"Mismatched image id\")\n\n        info = {}\n        info['image_id'] = image_id\n        info['scene_graph'] = scene_graph\n        info['url'] = cur_image_data['url']\n        info['question'] = question\n        info['object_feature'] = []\n        info['object_ids'] = []\n\n        for obj_id in scene_graph['names'].keys():\n            info['object_ids'].append(obj_id)\n            info['object_feature'].append(features.get(obj_id))\n\n        yield(info)", "\n"]}
{"filename": "src/experiments/vqa/train.py", "chunked_list": ["print(\"start importing...\")\n\nimport time\nimport sys\nimport argparse\nimport datetime\n\nsys.path.append('../../')\nsys.path.append('../../SLASH/')\nsys.path.append('../../EinsumNetworks/src/')", "sys.path.append('../../SLASH/')\nsys.path.append('../../EinsumNetworks/src/')\n\n\n#torch, numpy, ...\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.transforms import transforms\nimport torchvision\n", "import torchvision\n\nimport numpy as np\n\nimport json\n\n#own modules\nfrom dataGen import VQA\nfrom einsum_wrapper import EiNet\nfrom network_nn import Net_nn", "from einsum_wrapper import EiNet\nfrom network_nn import Net_nn\n\nfrom tqdm import tqdm\n\n#import slash\nfrom slash import SLASH\nimport os\nfrom datetime import date\n", "from datetime import date\n\n\n\n\nimport utils\nfrom utils import set_manual_seed\nfrom pathlib import Path\nfrom rtpt import RTPT\nimport pickle", "from rtpt import RTPT\nimport pickle\n\n\nfrom knowledge_graph import RULES, KG\nfrom dataGen import name_npp, relation_npp, attribute_npp\nfrom models import name_clf, rela_clf, attr_clf, rela_einet, name_einet, attr_einet\n\nprint(\"...done\")\n", "print(\"...done\")\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--seed\", type=int, default=10, help=\"Random generator seed for all frameworks\"\n    )\n    parser.add_argument(\n        \"--epochs\", type=int, default=10, help=\"Number of epochs to train with\"\n    )\n    parser.add_argument(\n        \"--lr\", type=float, default=0.01, help=\"Learning rate of model\"\n    )\n    parser.add_argument(\n        \"--network-type\",\n        choices=[\"nn\",\"pc\"],\n        help=\"The type of external to be used e.g. neural net or probabilistic circuit\",\n    )\n    parser.add_argument(\n        \"--pc-structure\",\n        choices=[\"poon-domingos\",\"binary-trees\"],\n        help=\"The type of external to be used e.g. neural net or probabilistic circuit\",\n    )\n    parser.add_argument(\n        \"--batch-size\", type=int, default=100, help=\"Batch size to train with\"\n    )\n    parser.add_argument(\n        \"--num-workers\", type=int, default=6, help=\"Number of threads for data loader\"\n    )\n\n    parser.add_argument(\n        \"--p-num\", type=int, default=8, help=\"Number of processes to devide the batch for parallel processing\"\n    )\n\n    parser.add_argument(\n        \"--exp-name\", type=str, default=\"vqa\", help=\"Name of the experiment\"\n    )\n\n    parser.add_argument(\"--credentials\", type=str, help=\"Credentials for rtpt\")\n\n\n    args = parser.parse_args()\n\n    if args.network_type == 'pc':\n        args.use_pc = True\n    else:\n        args.use_pc = False\n\n    return args", "\n\ndef determine_max_objects(task_file):\n\n    with open (task_file, 'rb') as tf:\n        tasks = pickle.load(tf)\n        print(\"taskfile len\",len(tasks))\n\n    #get the biggest number of objects in the image\n    max_objects = 0\n    for tidx, task in enumerate(tasks):\n        all_oid = task['question']['input']\n        len_all_oid = len(all_oid)\n\n        #store the biggest number of objects in an image\n        if len_all_oid > max_objects:\n            max_objects = len_all_oid\n    return max_objects", "\n\n \n\ndef slash_vqa():\n\n    args = get_args()\n    print(args)\n\n    # Set the seeds for PRNG\n    set_manual_seed(args.seed)\n\n    # Create RTPT object\n    rtpt = RTPT(name_initials=args.credentials, experiment_name='SLASH VQA', max_iterations=args.epochs)\n\n    # Start the RTPT tracking\n    rtpt.start()\n    exp_name = args.exp_name+ \"_\"+date.today().strftime(\"%d_%m_%Y\")\n\n    writer = SummaryWriter(os.path.join(\"runs\", exp_name, str(args.seed)), purge_step=0)\n    print(exp_name)\n\n    Path(\"data/\"+exp_name+\"/\").mkdir(parents=True, exist_ok=True)\n\n    #TODO workaround that adds +- notation \n    program_example = \"\"\"\n%scallop conversion rules\nname(O,N) :-  name(0,+O,-N).\nattr(O,A) :-  attr(0, +O, -A).\nrelation(O1,O2,N) :-  relation(0, +(O1,O2), -N).\n    \"\"\"\n\n    # datasets\n    #train_f = \"dataset/task_list/train_tasks.pkl\"  # Training dataset\n    train_f = \"dataset/task_list/train_tasks_c2_10000.pkl\"\n    val_f = \"dataset/task_list/val_tasks.pkl\"  # Test datset\n    test_f = \"dataset/task_list/test_tasks.pkl\"  # Test datset\n    # test_f = {\"c2\":\"dataset/task_list/test_tasks_c2_1000.pkl\",\n    #           \"c3\":\"dataset/task_list/test_tasks_c3_1000.pkl\",\n    #           \"c4\":\"dataset/task_list/test_tasks_c4_1000.pkl\",\n    #           \"c5\":\"dataset/task_list/test_tasks_c5_1000.pkl\",\n    #           \"c6\":\"dataset/task_list/test_tasks_c6_1000.pkl\"}\n    \n\n    vqa_params = {\"l\":100,\n                \"l_split\":5,\n                \"num_names\":500,\n                \"max_models\":10000,\n                \"asp_timeout\": 30 }\n\n    num_obj = []\n    num_obj.append(determine_max_objects(train_f))\n    num_obj.append(determine_max_objects(val_f))\n\n    if type(test_f) == str: \n        num_obj.append(determine_max_objects(test_f))\n    #if we have multiple test files\n    elif type(test_f) == dict:\n        for key in test_f:   \n            num_obj.append(determine_max_objects(test_f[key]))\n\n\n    NUM_OBJECTS = np.max(num_obj)\n    NUM_OBJECTS = 70\n\n    train_data = VQA(\"train\", train_f, NUM_OBJECTS)\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=4)\n\n    val_data = VQA(\"val\", val_f, NUM_OBJECTS)\n    val_loader = torch.utils.data.DataLoader(val_data, batch_size=args.batch_size, shuffle=False, num_workers=4)\n\n    if type(test_f) == str: \n        test_data = VQA(\"test\", test_f, NUM_OBJECTS)\n        test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=4)\n    #if we have multiple test files\n    elif type(test_f) == dict: \n        for key in test_f:\n            test_data = VQA(\"test\", test_f[key], NUM_OBJECTS)\n            test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=4)\n            test_f[key] = test_loader\n     \n    #create the SLASH Program\n    # nnMapping = {'relation': rela_einet, 'name':name_einet , \"attr\":attr_einet}\n    # optimizers = {'relation': torch.optim.Adam(rela_einet.parameters(), lr=args.lr, eps=1e-7),\n    #                   'name': torch.optim.Adam(name_einet.parameters(), lr=args.lr, eps=1e-7),\n    #                   'attr': torch.optim.Adam(attr_einet.parameters(), lr=args.lr, eps=1e-7)}\n\n    nnMapping = {'relation':rela_clf, 'name':name_clf , \"attr\":attr_clf}\n    optimizers = {'relation': torch.optim.Adam(rela_clf.parameters(), lr=args.lr, eps=1e-7),\n                      'name': torch.optim.Adam(name_clf.parameters(), lr=args.lr, eps=1e-7),\n                      'attr': torch.optim.Adam(attr_clf.parameters(), lr=args.lr, eps=1e-7)}\n\n    num_trainable_params = [sum(p.numel() for p in rela_clf.parameters() if p.requires_grad),\n                            sum(p.numel() for p in name_clf.parameters() if p.requires_grad),\n                            sum(p.numel() for p in attr_clf.parameters() if p.requires_grad)]\n\n    print(\"num traiable params:\", num_trainable_params)\n    \n    loss_list = []\n    forward_time_list = []\n    asp_time_list = []\n    gradient_time_list = []\n    backward_time_list = []\n\n    train_recall_list = []\n    val_recall_list = []\n    test_recall_list = []\n\n    sm_per_batch_list = []\n\n    best_test_recall = torch.zeros((1), dtype=torch.float)\n    best_val_recall = torch.zeros((1), dtype=torch.float)\n\n\n    #setup dataloader for the datasets\n    all_oid = np.arange(0,NUM_OBJECTS)\n    object_string = \"\".join([ f\"object({oid1},{oid2}). \" for oid1 in all_oid for oid2 in all_oid if oid1 != oid2])\n    object_string = \"\".join([\"\".join([f\"object({oid}). \" for oid in all_oid]), object_string])\n\n    #parse the SLASH program\n    print(\"parse all train programs\")\n    program = \"\".join([KG, RULES, object_string, name_npp, relation_npp, attribute_npp, program_example])\n    SLASHobj = SLASH(program, nnMapping, optimizers)\n\n    for e in range(0, args.epochs):\n\n        print(\"---TRAIN---\")\n        loss, forward_time, gradient_time, asp_time, backward_time, sm_per_batch = SLASHobj.learn(dataset_loader=train_loader,\n            epoch=e, method=\"same\", k_num=0, p_num=args.p_num, same_threshold={\"relation\":0.999, \"name\":0.999, \"attr\":0.99}, writer=writer, \n            vqa=True, batched_pass=True, vqa_params= vqa_params)\n\n        loss_list.append(loss)\n        forward_time_list.append(forward_time)\n        asp_time_list.append(asp_time)\n        gradient_time_list.append(gradient_time)\n        backward_time_list.append(backward_time)\n        sm_per_batch_list.append(sm_per_batch)\n\n        time_metrics = {\"forward_time\":forward_time_list,\n                        \"asp_time\":asp_time_list,\n                        \"backward_time\":backward_time_list,\n                        \"gradient_time\":gradient_time_list,\n                        \"sm_per_batch\":sm_per_batch_list\n                        }\n\n        writer.add_scalar('train/loss', loss, e)\n\n        saveModelPath = 'data/'+exp_name+'/slash_vqa_models_seed'+str(args.seed)+'_epoch_'+str(e)+'.pt'\n        # Export results and networks\n        print('Storing the trained model into {}'.format(saveModelPath))\n\n        # print(\"---TRAIN---\")\n        # recall_5_train = SLASHobj.testVQA(train_loader_test_mode, args.p_num, vqa_params=vqa_params)\n        # writer.add_scalar('train/recall', recall_5_train, e)\n        # print(\"recall@5 train\", recall_5_train)\n\n        print(\"---VAL---\")\n        recall_5_val, val_time = SLASHobj.testVQA(val_loader, args.p_num, vqa_params=vqa_params)\n        writer.add_scalar('val/recall', recall_5_val, e)\n        val_recall_list.append(recall_5_val)\n        print(\"val--recall@5:\", recall_5_val, \", val_time:\", val_time )\n        val_recall_list.append(recall_5_val)\n\n\n        print(\"---TEST---\")\n        if type(test_f) == str:\n            recall_5_test, test_time = SLASHobj.testVQA(test_loader, args.p_num, vqa_params=vqa_params)\n            writer.add_scalar('test/recall', recall_5_test, e)\n            print(\"test-recall@5\", recall_5_test)\n            test_recall_list.append(recall_5_test)\n\n\n        elif type(test_f) == dict:\n            test_time = 0\n            recalls = []\n            for key in test_f:\n                recall_5_test, tt = SLASHobj.testVQA(test_f[key], args.p_num, vqa_params=vqa_params)\n                test_time += tt\n                recalls.append(recall_5_test)\n                print(\"test-recall@5_{}\".format(key), recall_5_test, \", test_time:\", tt )\n                writer.add_scalar('test/recall_{}'.format(key), recall_5_test, e)\n            writer.add_scalar('test/recall_c_all', np.array(recalls).mean(), e)\n            print(\"test-recall@5_c_all\", np.mean(recalls), \", test_time:\", test_time)\n\n            test_recall_list.append(recalls)\n                \n\n\n        # Check if the new best value for recall@5 on val dataset is reached.\n        # If so, then save the new best performing model and test performance\n        if best_val_recall < recall_5_val:\n            best_val_recall = recall_5_val\n\n            print(\"Found the new best performing model and store it!\")\n            saveBestModelPath = 'data/'+exp_name+'/slash_vqa_models_seed'+str(args.seed)+'best.pt'\n            torch.save({\"relation_clf\": rela_clf.state_dict(),\n                    \"name_clf\":  name_clf.state_dict(),\n                    \"attr_clf\":attr_clf.state_dict(),\n                    \"resume\": {\n                        \"optimizer_rela\":optimizers['relation'].state_dict(),\n                        \"optimizer_name\":optimizers['name'].state_dict(),\n                        \"optimizer_attr\":optimizers['attr'].state_dict(),\n                        \"epoch\":e\n                            },\n                    \"args\":args,\n                    \"loss\": loss_list,\n                    \"train_recall_list\":train_recall_list,\n                    \"val_recall_list\":val_recall_list,\n                    \"test_recall_list\":test_recall_list,\n                    \"time_metrics\":time_metrics,\n                    \"program\":program,\n                    \"vqa_params\":vqa_params}, saveBestModelPath)\n\n\n        print(\"storing the model\")\n        torch.save({\"relation_clf\": rela_clf.state_dict(),\n                    \"name_clf\":  name_clf.state_dict(),\n                    \"attr_clf\":attr_clf.state_dict(),\n                    \"resume\": {\n                        \"optimizer_rela\":optimizers['relation'].state_dict(),\n                        \"optimizer_name\":optimizers['name'].state_dict(),\n                        \"optimizer_attr\":optimizers['attr'].state_dict(),\n                        \"epoch\":e\n                            },\n                    \"args\":args,\n                    \"loss\": loss_list,\n                    \"train_recall_list\":train_recall_list,\n                    \"val_recall_list\":val_recall_list,\n                    \"test_recall_list\":test_recall_list,\n                    \"time_metrics\":time_metrics,\n                    \"program\":program,\n                    \"vqa_params\":vqa_params}, saveModelPath)\n            \n        # Update the RTPT\n        rtpt.step(subtitle=f\"loss={loss:2.2f};recall@5={recall_5_test:2.2f}\")", "        \nif __name__ == \"__main__\":\n    slash_vqa()\n"]}
{"filename": "src/experiments/vqa/knowledge_graph.py", "chunked_list": ["\"\"\"\nThe source code is based on:\nScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\nJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\nAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\nhttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\"\"\"\n\nimport re \n", "import re \n\nKG_FACTS = '''\n%FACTS\nis_a(\"boat\", \"watercraft\").\nis_a(\"photograph\", \"artwork\").\nis_a(\"chalk\", \"writing implement\").\nis_a(\"tv\", \"electrical appliance\").\nis_a(\"screen\", \"electronic device\").\nis_a(\"chocolate\", \"dessert\").", "is_a(\"screen\", \"electronic device\").\nis_a(\"chocolate\", \"dessert\").\nis_a(\"pig\", \"even-toed ungulate\").\nis_a(\"pig\", \"mammal\").\nis_a(\"alcohol\", \"alcoholic beverage\").\nis_a(\"wine\", \"alcoholic beverage\").\nis_a(\"picture\", \"artwork\").\nis_a(\"zebra\", \"herbivore\").\nis_a(\"skillet\", \"kitchen utensil\").\nis_a(\"horse\", \"mammal\").", "is_a(\"skillet\", \"kitchen utensil\").\nis_a(\"horse\", \"mammal\").\nis_a(\"rice\", \"staple food\").\nis_a(\"rice\", \"grains\").\nis_a(\"zebra\", \"mammal\").\nis_a(\"refrigerator\", \"electrical appliance\").\nis_a(\"soda\", \"soft drinks\").\nis_a(\"horse\", \"herbivore\").\nis_a(\"air conditioner\", \"electrical appliance\").\nis_a(\"container\", \"kitchen utensil\").", "is_a(\"air conditioner\", \"electrical appliance\").\nis_a(\"container\", \"kitchen utensil\").\nis_a(\"cat\", \"feline\").\nis_a(\"cat\", \"mammal\").\nis_a(\"zebra\", \"odd-toed ungulate\").\nis_a(\"spoon\", \"eating utensil\").\nis_a(\"lemon\", \"citrus fruit\").\nis_a(\"lime\", \"citrus fruit\").\nis_a(\"soap\", \"toiletry\").\nis_a(\"dog\", \"mammal\").", "is_a(\"soap\", \"toiletry\").\nis_a(\"dog\", \"mammal\").\nis_a(\"dog\", \"carnivora\").\nis_a(\"giraffe\", \"even-toed ungulate\").\nis_a(\"giraffe\", \"mammal\").\nis_a(\"giraffe\", \"herbivore\").\nis_a(\"elephant\", \"mammal\").\nis_a(\"bear\", \"carnivora\").\nis_a(\"duck\", \"bird\").\nis_a(\"sheep\", \"mammal\").", "is_a(\"duck\", \"bird\").\nis_a(\"sheep\", \"mammal\").\nis_a(\"sheep\", \"even-toed ungulate\").\nis_a(\"computer\", \"electronic device\").\nis_a(\"fork\", \"eating utensil\").\nis_a(\"pen\", \"writing implement\").\nis_a(\"speaker\", \"electronic device\").\nis_a(\"camera\", \"electronic device\").\nis_a(\"bus\", \"public transports\").\nis_a(\"tiger\", \"feline\").", "is_a(\"bus\", \"public transports\").\nis_a(\"tiger\", \"feline\").\nis_a(\"painting\", \"artwork\").\nis_a(\"headphones\", \"electronic device\").\nis_a(\"ostrich\", \"bird\").\nis_a(\"projector\", \"electronic device\").\nis_a(\"mousepad\", \"electronic device\").\nis_a(\"monitor\", \"electronic device\").\nis_a(\"orange\", \"citrus fruit\").\nis_a(\"keyboard\", \"electronic device\").", "is_a(\"orange\", \"citrus fruit\").\nis_a(\"keyboard\", \"electronic device\").\nis_a(\"airplane\", \"public transports\").\nis_a(\"airplane\", \"aircraft\").\nis_a(\"pot\", \"kitchen utensil\").\nis_a(\"microwave\", \"electrical appliance\").\nis_a(\"bird\", \"class\").\nis_a(\"fox\", \"canidae\").\nis_a(\"oven\", \"electrical appliance\").\nis_a(\"kettle\", \"kitchen utensil\").", "is_a(\"oven\", \"electrical appliance\").\nis_a(\"kettle\", \"kitchen utensil\").\nis_a(\"tea pot\", \"kitchen utensil\").\nis_a(\"washing machine\", \"electrical appliance\").\nis_a(\"donkey\", \"mammal\").\nis_a(\"macaroni\", \"main course\").\nis_a(\"propeller\", \"aircraft\").\nis_a(\"sink\", \"kitchen utensil\").\nis_a(\"landscape\", \"artwork\").\nis_a(\"blade\", \"kitchen utensil\").", "is_a(\"landscape\", \"artwork\").\nis_a(\"blade\", \"kitchen utensil\").\nis_a(\"marker\", \"writing implement\").\nis_a(\"tv\", \"electronic device\").\nis_a(\"radio\", \"electronic device\").\nis_a(\"printer\", \"electronic device\").\nis_a(\"horse\", \"odd-toed ungulate\").\nis_a(\"train\", \"public transports\").\nis_a(\"sheep\", \"herbivore\").\nis_a(\"knife\", \"eating utensil\").", "is_a(\"sheep\", \"herbivore\").\nis_a(\"knife\", \"eating utensil\").\nis_a(\"wii\", \"electronic device\").\nis_a(\"guitar\", \"instruments\").\nis_a(\"bull\", \"mammal\").\nis_a(\"pan\", \"kitchen utensil\").\nis_a(\"grapefruit\", \"citrus fruit\").\nis_a(\"lizard\", \"reptile\").\nis_a(\"jet\", \"aircraft\").\nis_a(\"mouse\", \"electronic device\").", "is_a(\"jet\", \"aircraft\").\nis_a(\"mouse\", \"electronic device\").\nis_a(\"toothbrush\", \"toiletry\").\nis_a(\"lion\", \"feline\").\nis_a(\"lion\", \"carnivora\").\nis_a(\"tea kettle\", \"kitchen utensil\").\nis_a(\"cake\", \"dessert\").\nis_a(\"laptop\", \"electronic device\").\nis_a(\"cow\", \"mammal\").\nis_a(\"calf\", \"mammal\").", "is_a(\"cow\", \"mammal\").\nis_a(\"calf\", \"mammal\").\nis_a(\"phone\", \"electronic device\").\nis_a(\"pencil\", \"writing implement\").\nis_a(\"game controller\", \"electronic device\").\nis_a(\"pizza\", \"main course\").\nis_a(\"pizza\", \"fast food\").\nis_a(\"calculator\", \"electronic device\").\nis_a(\"ice cream\", \"dessert\").\nis_a(\"cookie\", \"dessert\").", "is_a(\"ice cream\", \"dessert\").\nis_a(\"cookie\", \"dessert\").\nis_a(\"grill\", \"kitchen utensil\").\nis_a(\"goat\", \"mammal\").\nis_a(\"microphone\", \"electronic device\").\nis_a(\"polar bear\", \"mammal\").\nis_a(\"statue\", \"artwork\").\nis_a(\"cow\", \"even-toed ungulate\").\nis_a(\"chopsticks\", \"eating utensil\").\nis_a(\"cow\", \"herbivore\").", "is_a(\"chopsticks\", \"eating utensil\").\nis_a(\"cow\", \"herbivore\").\nis_a(\"cattle\", \"even-toed ungulate\").\nis_a(\"dishwasher\", \"electrical appliance\").\nis_a(\"blender\", \"kitchen utensil\").\nis_a(\"coffee pot\", \"kitchen utensil\").\nis_a(\"cooking pot\", \"kitchen utensil\").\nis_a(\"coffee maker\", \"electrical appliance\").\nis_a(\"water\", \"liquid\").\nis_a(\"dvd\", \"electronic device\").", "is_a(\"water\", \"liquid\").\nis_a(\"dvd\", \"electronic device\").\nis_a(\"ship\", \"watercraft\").\nis_a(\"fax machine\", \"electronic device\").\nis_a(\"lotion\", \"toiletry\").\nis_a(\"pepper\", \"condiment\").\nis_a(\"noodles\", \"main course\").\nis_a(\"bread\", \"staple food\").\nis_a(\"cereal\", \"grains\").\nis_a(\"drum\", \"instruments\").", "is_a(\"cereal\", \"grains\").\nis_a(\"drum\", \"instruments\").\nis_a(\"graffiti\", \"artwork\").\nis_a(\"deer\", \"herbivore\").\nis_a(\"deer\", \"even-toed ungulate\").\nis_a(\"cutting board\", \"kitchen utensil\").\nis_a(\"knife block\", \"kitchen utensil\").\nis_a(\"whisk\", \"kitchen utensil\").\nis_a(\"burger\", \"fast food\").\nis_a(\"burger\", \"main course\").", "is_a(\"burger\", \"fast food\").\nis_a(\"burger\", \"main course\").\nis_a(\"heater\", \"electrical appliance\").\nis_a(\"rolling pin\", \"kitchen utensil\").\nis_a(\"dog\", \"canidae\").\nis_a(\"cattle\", \"mammal\").\nis_a(\"harp\", \"instruments\").\nis_a(\"cat\", \"carnivora\").\nis_a(\"pigeon\", \"bird\").\nis_a(\"juicer\", \"kitchen utensil\").", "is_a(\"pigeon\", \"bird\").\nis_a(\"juicer\", \"kitchen utensil\").\nis_a(\"subway\", \"public transports\").\nis_a(\"tiger\", \"mammal\").\nis_a(\"penguin\", \"bird\").\nis_a(\"pizza oven\", \"electrical appliance\").\nis_a(\"pizza cutter\", \"kitchen utensil\").\nis_a(\"bear\", \"mammal\").\nis_a(\"seagull\", \"bird\").\nis_a(\"ladle\", \"kitchen utensil\").", "is_a(\"seagull\", \"bird\").\nis_a(\"ladle\", \"kitchen utensil\").\nis_a(\"foil\", \"kitchen utensil\").\nis_a(\"wii controller\", \"electronic device\").\nis_a(\"router\", \"electronic device\").\nis_a(\"panda\", \"mammal\").\nis_a(\"monkey\", \"mammal\").\nis_a(\"spatula\", \"kitchen utensil\").\nis_a(\"rabbit\", \"herbivore\").\nis_a(\"bunny\", \"mammal\").", "is_a(\"rabbit\", \"herbivore\").\nis_a(\"bunny\", \"mammal\").\nis_a(\"mixing bowl\", \"kitchen utensil\").\nis_a(\"cola\", \"soft drinks\").\nis_a(\"turtle\", \"reptile\").\nis_a(\"frying pan\", \"kitchen utensil\").\nis_a(\"cheesecake\", \"dessert\").\nis_a(\"baking sheet\", \"kitchen utensil\").\nis_a(\"dryer\", \"electrical appliance\").\nis_a(\"beer\", \"alcoholic beverage\").", "is_a(\"dryer\", \"electrical appliance\").\nis_a(\"beer\", \"alcoholic beverage\").\nis_a(\"calf\", \"herbivore\").\nis_a(\"sailboat\", \"watercraft\").\nis_a(\"toaster oven\", \"electrical appliance\").\nis_a(\"bull\", \"herbivore\").\nis_a(\"lion\", \"mammal\").\nis_a(\"dvd player\", \"electronic device\").\nis_a(\"toothpick\", \"eating utensil\").\nis_a(\"squirrel\", \"mammal\").", "is_a(\"toothpick\", \"eating utensil\").\nis_a(\"squirrel\", \"mammal\").\nis_a(\"squirrel\", \"rodent\").\nis_a(\"tongs\", \"kitchen utensil\").\nis_a(\"moose\", \"even-toed ungulate\").\nis_a(\"moose\", \"mammal\").\nis_a(\"pasta\", \"main course\").\nis_a(\"pony\", \"mammal\").\nis_a(\"sword\", \"weapon\").\nis_a(\"fox\", \"mammal\").", "is_a(\"sword\", \"weapon\").\nis_a(\"fox\", \"mammal\").\nis_a(\"toaster\", \"electrical appliance\").\nis_a(\"eagle\", \"bird\").\nis_a(\"lipstick\", \"cosmetic\").\nis_a(\"calf\", \"even-toed ungulate\").\nis_a(\"rifle\", \"weapon\").\nis_a(\"piano\", \"instruments\").\nis_a(\"cupcake\", \"dessert\").\nis_a(\"feline\", \"family\").", "is_a(\"cupcake\", \"dessert\").\nis_a(\"feline\", \"family\").\nis_a(\"chicken\", \"bird\").\nis_a(\"pie\", \"dessert\").\nis_a(\"dish drainer\", \"kitchen utensil\").\nis_a(\"lamb\", \"even-toed ungulate\").\nis_a(\"lamb\", \"mammal\").\nis_a(\"goat\", \"even-toed ungulate\").\nis_a(\"goat\", \"herbivore\").\nis_a(\"lamb\", \"herbivore\").", "is_a(\"goat\", \"herbivore\").\nis_a(\"lamb\", \"herbivore\").\nis_a(\"puppy\", \"canidae\").\nis_a(\"basil\", \"condiment\").\nis_a(\"fries\", \"fast food\").\nis_a(\"grater\", \"kitchen utensil\").\nis_a(\"sauce\", \"condiment\").\nis_a(\"kitten\", \"feline\").\nis_a(\"hippo\", \"even-toed ungulate\").\nis_a(\"hippo\", \"mammal\").", "is_a(\"hippo\", \"even-toed ungulate\").\nis_a(\"hippo\", \"mammal\").\nis_a(\"bass\", \"instruments\").\nis_a(\"antelope\", \"herbivore\").\nis_a(\"polar bear\", \"carnivora\").\nis_a(\"corn\", \"grains\").\nis_a(\"corn\", \"staple food\").\nis_a(\"brownie\", \"dessert\").\nis_a(\"cash\", \"currency\").\nis_a(\"cereal\", \"staple food\").", "is_a(\"cash\", \"currency\").\nis_a(\"cereal\", \"staple food\").\nis_a(\"owl\", \"bird\").\nis_a(\"flamingo\", \"bird\").\nis_a(\"monkey\", \"primate\").\nis_a(\"bomb\", \"weapon\").\nis_a(\"pizza pan\", \"kitchen utensil\").\nis_a(\"bull\", \"even-toed ungulate\").\nis_a(\"rice cooker\", \"electrical appliance\").\nis_a(\"sushi\", \"main course\").", "is_a(\"rice cooker\", \"electrical appliance\").\nis_a(\"sushi\", \"main course\").\nis_a(\"vacuum\", \"electrical appliance\").\nis_a(\"toothpaste\", \"toiletry\").\nis_a(\"frog\", \"amphibian\").\nis_a(\"antelope\", \"mammal\").\nis_a(\"hen\", \"bird\").\nis_a(\"puppy\", \"mammal\").\nis_a(\"puppy\", \"carnivora\").\nis_a(\"double decker\", \"public transports\").", "is_a(\"puppy\", \"carnivora\").\nis_a(\"double decker\", \"public transports\").\nis_a(\"alligator\", \"reptile\").\nis_a(\"deer\", \"mammal\").\nis_a(\"cinnamon\", \"condiment\").\nis_a(\"tongs\", \"eating utensil\").\nis_a(\"kitten\", \"carnivora\").\nis_a(\"kitten\", \"mammal\").\nis_a(\"cattle\", \"herbivore\").\nis_a(\"antelope\", \"even-toed ungulate\").", "is_a(\"cattle\", \"herbivore\").\nis_a(\"antelope\", \"even-toed ungulate\").\nis_a(\"parrot\", \"bird\").\nis_a(\"ipod\", \"electronic device\").\nis_a(\"dolphin\", \"even-toed ungulate\").\nis_a(\"leopard\", \"mammal\").\nis_a(\"fox\", \"carnivora\").\nis_a(\"gun\", \"weapon\").\nis_a(\"canoe\", \"watercraft\").\nis_a(\"food processor\", \"electrical appliance\").", "is_a(\"canoe\", \"watercraft\").\nis_a(\"food processor\", \"electrical appliance\").\nis_a(\"hamburger\", \"main course\").\nis_a(\"hamburger\", \"fast food\").\nis_a(\"console\", \"electronic device\").\nis_a(\"xbox controller\", \"electronic device\").\nis_a(\"video camera\", \"electronic device\").\nis_a(\"mammal\", \"class\").\nis_a(\"helicopter\", \"aircraft\").\nis_a(\"spear\", \"weapon\").", "is_a(\"helicopter\", \"aircraft\").\nis_a(\"spear\", \"weapon\").\nis_a(\"hummingbird\", \"bird\").\nis_a(\"goose\", \"bird\").\nis_a(\"rodent\", \"order\").\nis_a(\"shampoo\", \"toiletry\").\nis_a(\"violin\", \"instruments\").\nis_a(\"coin\", \"currency\").\nis_a(\"dove\", \"bird\").\nis_a(\"champagne\", \"alcoholic beverage\").", "is_a(\"dove\", \"bird\").\nis_a(\"champagne\", \"alcoholic beverage\").\nis_a(\"camel\", \"even-toed ungulate\").\nis_a(\"bison\", \"even-toed ungulate\").\nis_a(\"bison\", \"herbivore\").\nis_a(\"trumpet\", \"instruments\").\nis_a(\"rabbit\", \"mammal\").\nis_a(\"kangaroo\", \"mammal\").\nis_a(\"wolf\", \"carnivora\").\nis_a(\"guacamole\", \"condiment\").", "is_a(\"wolf\", \"carnivora\").\nis_a(\"guacamole\", \"condiment\").\nis_a(\"wok\", \"kitchen utensil\").\nis_a(\"tiger\", \"carnivora\").\nis_a(\"panda bear\", \"mammal\").\nis_a(\"syrup\", \"condiment\").\nis_a(\"moose\", \"herbivore\").\nis_a(\"hawk\", \"bird\").\nis_a(\"beaver\", \"mammal\").\nis_a(\"liquor\", \"alcoholic beverage\").", "is_a(\"beaver\", \"mammal\").\nis_a(\"liquor\", \"alcoholic beverage\").\nis_a(\"shaving cream\", \"toiletry\").\nis_a(\"utensil holder\", \"kitchen utensil\").\nis_a(\"cake pan\", \"kitchen utensil\").\nis_a(\"seal\", \"mammal\").\nis_a(\"seal\", \"carnivora\").\nis_a(\"poodle\", \"mammal\").\nis_a(\"raccoon\", \"carnivora\").\nis_a(\"ice maker\", \"electrical appliance\").", "is_a(\"raccoon\", \"carnivora\").\nis_a(\"ice maker\", \"electrical appliance\").\nis_a(\"hamster\", \"rodent\").\nis_a(\"rat\", \"mammal\").\nis_a(\"crow\", \"bird\").\nis_a(\"swan\", \"bird\").\nis_a(\"whale\", \"even-toed ungulate\").\nis_a(\"garlic\", \"condiment\").\nis_a(\"wolf\", \"mammal\").\nis_a(\"mustard\", \"condiment\").", "is_a(\"wolf\", \"mammal\").\nis_a(\"mustard\", \"condiment\").\nis_a(\"deodorant\", \"toiletry\").\nis_a(\"hand dryer\", \"electrical appliance\").\nis_a(\"wheat\", \"staple food\").\nis_a(\"wheat\", \"grains\").\nis_a(\"pudding\", \"dessert\").\nis_a(\"accordion\", \"instruments\").\nis_a(\"peacock\", \"bird\").\nis_a(\"crayon\", \"writing implement\").", "is_a(\"peacock\", \"bird\").\nis_a(\"crayon\", \"writing implement\").\nis_a(\"gorilla\", \"primate\").\nis_a(\"cheetah\", \"carnivora\").\nis_a(\"cheetah\", \"mammal\").\nis_a(\"leopard\", \"feline\").\nis_a(\"baking pan\", \"kitchen utensil\").\nis_a(\"mascara\", \"cosmetic\").\nis_a(\"gorilla\", \"mammal\").\nis_a(\"fingernail polish\", \"cosmetic\").", "is_a(\"gorilla\", \"mammal\").\nis_a(\"fingernail polish\", \"cosmetic\").\nis_a(\"rhino\", \"odd-toed ungulate\").\nis_a(\"hamster\", \"mammal\").\nis_a(\"rat\", \"rodent\").\nis_a(\"sugar\", \"condiment\").\nis_a(\"finch\", \"bird\").\nis_a(\"skewer\", \"eating utensil\").\nis_a(\"cheeseburger\", \"main course\").\nis_a(\"snake\", \"reptile\").", "is_a(\"cheeseburger\", \"main course\").\nis_a(\"snake\", \"reptile\").\nis_a(\"bald eagle\", \"bird\").\nis_a(\"cheetah\", \"feline\").\nis_a(\"bison\", \"mammal\").\nis_a(\"rhino\", \"mammal\").\nis_a(\"camel\", \"herbivore\").\nis_a(\"camel\", \"mammal\").\nis_a(\"ketchup\", \"condiment\").\nis_a(\"ape\", \"primate\").", "is_a(\"ketchup\", \"condiment\").\nis_a(\"ape\", \"primate\").\nis_a(\"ape\", \"mammal\").\nis_a(\"gravy\", \"condiment\").\nis_a(\"barley\", \"grains\").\nis_a(\"butter\", \"condiment\").\nis_a(\"copier\", \"electronic device\").\nis_a(\"robin\", \"bird\").\nis_a(\"reptile\", \"class\").\nis_a(\"beaver\", \"rodent\").", "is_a(\"reptile\", \"class\").\nis_a(\"beaver\", \"rodent\").\nis_a(\"perfume\", \"cosmetic\").\nis_a(\"eyeshadow\", \"cosmetic\").\nis_a(\"wolf\", \"canidae\").\nis_a(\"badger\", \"mammal\").\nis_a(\"leopard\", \"carnivora\").\nis_a(\"eyeliner\", \"cosmetic\").\nis_a(\"people\", \"mammal\").\nis_a(\"salt\", \"condiment\").", "is_a(\"people\", \"mammal\").\nis_a(\"salt\", \"condiment\").\nis_a(\"grey cat\", \"mammal\").\nis_a(\"alpaca\", \"even-toed ungulate\").\nis_a(\"raccoon\", \"mammal\").\nis_a(\"perfume\", \"toiletry\").\nis_a(\"caramel\", \"condiment\").\nis_a(\"clarinet\", \"instruments\").\nis_a(\"otter\", \"carnivora\").\nis_a(\"athlete\", \"person\").", "is_a(\"otter\", \"carnivora\").\nis_a(\"athlete\", \"person\").\nis_a(\"occupation\", \"person\").\nis_a(\"baseball position\", \"person\").\nis_a(\"tennis player\", \"athlete\").\nis_a(\"baseball player\", \"athlete\").\nis_a(\"soccer player\", \"athlete\").\nis_a(\"basketball player\", \"athlete\").\nis_a(\"frisbee player\", \"athlete\").\nis_a(\"football player\", \"athlete\").", "is_a(\"frisbee player\", \"athlete\").\nis_a(\"football player\", \"athlete\").\nis_a(\"volleyball player\", \"athlete\").\nis_a(\"billiards player\", \"athlete\").\nis_a(\"hockey player\", \"athlete\").\nis_a(\"golfer\", \"athlete\").\nis_a(\"surfer\", \"athlete\").\nis_a(\"biker\", \"athlete\").\nis_a(\"swimmer\", \"athlete\").\nis_a(\"runner\", \"athlete\").", "is_a(\"swimmer\", \"athlete\").\nis_a(\"runner\", \"athlete\").\nis_a(\"jogger\", \"athlete\").\nis_a(\"skier\", \"athlete\").\nis_a(\"skateboarder\", \"athlete\").\nis_a(\"skater\", \"athlete\").\nis_a(\"snowboarder\", \"athlete\").\nis_a(\"police\", \"occupation\").\nis_a(\"teacher\", \"occupation\").\nis_a(\"student\", \"occupation\").", "is_a(\"teacher\", \"occupation\").\nis_a(\"student\", \"occupation\").\nis_a(\"pilot\", \"occupation\").\nis_a(\"cowboy\", \"occupation\").\nis_a(\"soldier\", \"occupation\").\nis_a(\"fisherman\", \"occupation\").\nis_a(\"worker\", \"occupation\").\nis_a(\"photographer\", \"occupation\").\nis_a(\"performer\", \"occupation\").\nis_a(\"farmer\", \"occupation\").", "is_a(\"performer\", \"occupation\").\nis_a(\"farmer\", \"occupation\").\nis_a(\"policeman\", \"occupation\").\nis_a(\"officer\", \"occupation\").\nis_a(\"vendor\", \"occupation\").\nis_a(\"shopper\", \"occupation\").\nis_a(\"bus driver\", \"occupation\").\nis_a(\"driver\", \"occupation\").\nis_a(\"jockey\", \"occupation\").\nis_a(\"engineer\", \"occupation\").", "is_a(\"jockey\", \"occupation\").\nis_a(\"engineer\", \"occupation\").\nis_a(\"doctor\", \"occupation\").\nis_a(\"chef\", \"occupation\").\nis_a(\"baker\", \"occupation\").\nis_a(\"bartender\", \"occupation\").\nis_a(\"waiter\", \"occupation\").\nis_a(\"waitress\", \"occupation\").\nis_a(\"customer\", \"occupation\").\nis_a(\"player\", \"occupation\").", "is_a(\"customer\", \"occupation\").\nis_a(\"player\", \"occupation\").\nis_a(\"athlete\", \"occupation\").\nis_a(\"coach\", \"occupation\").\nis_a(\"actor\", \"occupation\").\nis_a(\"batter\", \"baseball position\").\nis_a(\"catcher\", \"baseball position\").\nis_a(\"pitcher\", \"baseball position\").\nis_a(\"umpire\", \"baseball position\").\nis_a(\"santa\", \"character\").", "is_a(\"umpire\", \"baseball position\").\nis_a(\"santa\", \"character\").\nis_a(\"mickey mouse\", \"character\").\nis_a(\"snoopy\", \"character\").\nis_a(\"pikachu\", \"character\").\nis_a(\"leg\", \"part of body\").\nis_a(\"tail\", \"part of body\").\nis_a(\"lap\", \"part of body\").\nis_a(\"neck\", \"part of body\").\nis_a(\"foot\", \"part of body\").", "is_a(\"neck\", \"part of body\").\nis_a(\"foot\", \"part of body\").\nis_a(\"face\", \"part of body\").\nis_a(\"arm\", \"part of body\").\nis_a(\"hand\", \"part of body\").\nis_a(\"wrist\", \"part of body\").\nis_a(\"shoulder\", \"part of body\").\nis_a(\"head\", \"part of body\").\nis_a(\"horn\", \"part of body\").\nis_a(\"tusk\", \"part of body\").", "is_a(\"horn\", \"part of body\").\nis_a(\"tusk\", \"part of body\").\nis_a(\"racing\", \"sport\").\nis_a(\"baseball\", \"sport\").\nis_a(\"soccer\", \"sport\").\nis_a(\"skiing\", \"sport\").\nis_a(\"basketball\", \"sport\").\nis_a(\"polo\", \"sport\").\nis_a(\"tennis\", \"sport\").\nis_a(\"surfing\", \"sport\").", "is_a(\"tennis\", \"sport\").\nis_a(\"surfing\", \"sport\").\nis_a(\"riding\", \"sport\").\nis_a(\"skateboarding\", \"sport\").\nis_a(\"skate\", \"sport\").\nis_a(\"swimming\", \"sport\").\nis_a(\"snowboarding\", \"sport\").\nis_a(\"christmas\", \"event\").\nis_a(\"thanksgiving\", \"event\").\nis_a(\"wedding\", \"event\").", "is_a(\"thanksgiving\", \"event\").\nis_a(\"wedding\", \"event\").\nis_a(\"birthday\", \"event\").\nis_a(\"halloween\", \"event\").\nis_a(\"party\", \"event\").\nis_a(\"cat\", \"animal\").\nis_a(\"kitten\", \"animal\").\nis_a(\"dog\", \"animal\").\nis_a(\"puppy\", \"animal\").\nis_a(\"poodle\", \"animal\").", "is_a(\"puppy\", \"animal\").\nis_a(\"poodle\", \"animal\").\nis_a(\"bull\", \"animal\").\nis_a(\"cow\", \"animal\").\nis_a(\"cattle\", \"animal\").\nis_a(\"bison\", \"animal\").\nis_a(\"calf\", \"animal\").\nis_a(\"pig\", \"animal\").\nis_a(\"ape\", \"animal\").\nis_a(\"monkey\", \"animal\").", "is_a(\"ape\", \"animal\").\nis_a(\"monkey\", \"animal\").\nis_a(\"gorilla\", \"animal\").\nis_a(\"rat\", \"animal\").\nis_a(\"squirrel\", \"animal\").\nis_a(\"hamster\", \"animal\").\nis_a(\"deer\", \"animal\").\nis_a(\"moose\", \"animal\").\nis_a(\"alpaca\", \"animal\").\nis_a(\"elephant\", \"animal\").", "is_a(\"alpaca\", \"animal\").\nis_a(\"elephant\", \"animal\").\nis_a(\"goat\", \"animal\").\nis_a(\"sheep\", \"animal\").\nis_a(\"lamb\", \"animal\").\nis_a(\"antelope\", \"animal\").\nis_a(\"rhino\", \"animal\").\nis_a(\"hippo\", \"animal\").\nis_a(\"giraffe\", \"animal\").\nis_a(\"zebra\", \"animal\").", "is_a(\"giraffe\", \"animal\").\nis_a(\"zebra\", \"animal\").\nis_a(\"horse\", \"animal\").\nis_a(\"pony\", \"animal\").\nis_a(\"donkey\", \"animal\").\nis_a(\"camel\", \"animal\").\nis_a(\"panda\", \"animal\").\nis_a(\"panda bear\", \"animal\").\nis_a(\"bear\", \"animal\").\nis_a(\"polar bear\", \"animal\").", "is_a(\"bear\", \"animal\").\nis_a(\"polar bear\", \"animal\").\nis_a(\"seal\", \"animal\").\nis_a(\"fox\", \"animal\").\nis_a(\"raccoon\", \"animal\").\nis_a(\"tiger\", \"animal\").\nis_a(\"wolf\", \"animal\").\nis_a(\"lion\", \"animal\").\nis_a(\"leopard\", \"animal\").\nis_a(\"cheetah\", \"animal\").", "is_a(\"leopard\", \"animal\").\nis_a(\"cheetah\", \"animal\").\nis_a(\"badger\", \"animal\").\nis_a(\"rabbit\", \"animal\").\nis_a(\"bunny\", \"animal\").\nis_a(\"beaver\", \"animal\").\nis_a(\"kangaroo\", \"animal\").\nis_a(\"dinosaur\", \"animal\").\nis_a(\"dragon\", \"animal\").\nis_a(\"fish\", \"animal\").", "is_a(\"dragon\", \"animal\").\nis_a(\"fish\", \"animal\").\nis_a(\"whale\", \"animal\").\nis_a(\"dolphin\", \"animal\").\nis_a(\"crab\", \"animal\").\nis_a(\"shark\", \"animal\").\nis_a(\"octopus\", \"animal\").\nis_a(\"lobster\", \"animal\").\nis_a(\"oyster\", \"animal\").\nis_a(\"butterfly\", \"animal\").", "is_a(\"oyster\", \"animal\").\nis_a(\"butterfly\", \"animal\").\nis_a(\"bee\", \"animal\").\nis_a(\"fly\", \"animal\").\nis_a(\"ant\", \"animal\").\nis_a(\"firefly\", \"animal\").\nis_a(\"snail\", \"animal\").\nis_a(\"spider\", \"animal\").\nis_a(\"bird\", \"animal\").\nis_a(\"penguin\", \"animal\").", "is_a(\"bird\", \"animal\").\nis_a(\"penguin\", \"animal\").\nis_a(\"pigeon\", \"animal\").\nis_a(\"seagull\", \"animal\").\nis_a(\"finch\", \"animal\").\nis_a(\"robin\", \"animal\").\nis_a(\"ostrich\", \"animal\").\nis_a(\"goose\", \"animal\").\nis_a(\"owl\", \"animal\").\nis_a(\"duck\", \"animal\").", "is_a(\"owl\", \"animal\").\nis_a(\"duck\", \"animal\").\nis_a(\"hawk\", \"animal\").\nis_a(\"eagle\", \"animal\").\nis_a(\"swan\", \"animal\").\nis_a(\"chicken\", \"animal\").\nis_a(\"hen\", \"animal\").\nis_a(\"hummingbird\", \"animal\").\nis_a(\"parrot\", \"animal\").\nis_a(\"crow\", \"animal\").", "is_a(\"parrot\", \"animal\").\nis_a(\"crow\", \"animal\").\nis_a(\"flamingo\", \"animal\").\nis_a(\"peacock\", \"animal\").\nis_a(\"bald eagle\", \"animal\").\nis_a(\"dove\", \"animal\").\nis_a(\"snake\", \"animal\").\nis_a(\"lizard\", \"animal\").\nis_a(\"alligator\", \"animal\").\nis_a(\"turtle\", \"animal\").", "is_a(\"alligator\", \"animal\").\nis_a(\"turtle\", \"animal\").\nis_a(\"frog\", \"animal\").\nis_a(\"butterfly\", \"insect\").\nis_a(\"bee\", \"insect\").\nis_a(\"fly\", \"insect\").\nis_a(\"ant\", \"insect\").\nis_a(\"firefly\", \"insect\").\nis_a(\"swan\", \"aquatic bird\").\nis_a(\"penguin\", \"aquatic bird\").", "is_a(\"swan\", \"aquatic bird\").\nis_a(\"penguin\", \"aquatic bird\").\nis_a(\"duck\", \"aquatic bird\").\nis_a(\"goose\", \"aquatic bird\").\nis_a(\"seagull\", \"aquatic bird\").\nis_a(\"flamingo\", \"aquatic bird\").\nis_a(\"tree\", \"plant\").\nis_a(\"flower\", \"plant\").\nis_a(\"bamboo\", \"tree\").\nis_a(\"palm tree\", \"tree\").", "is_a(\"bamboo\", \"tree\").\nis_a(\"palm tree\", \"tree\").\nis_a(\"pine\", \"tree\").\nis_a(\"pine tree\", \"tree\").\nis_a(\"oak tree\", \"tree\").\nis_a(\"christmas tree\", \"tree\").\nis_a(\"flower\", \"flower\").\nis_a(\"sunflower\", \"flower\").\nis_a(\"daisy\", \"flower\").\nis_a(\"orchid\", \"flower\").", "is_a(\"daisy\", \"flower\").\nis_a(\"orchid\", \"flower\").\nis_a(\"seaweed\", \"flower\").\nis_a(\"blossom\", \"flower\").\nis_a(\"lily\", \"flower\").\nis_a(\"rose\", \"flower\").\nis_a(\"sweater\", \"tops\").\nis_a(\"pullover\", \"tops\").\nis_a(\"blouse\", \"tops\").\nis_a(\"blazer\", \"tops\").", "is_a(\"blouse\", \"tops\").\nis_a(\"blazer\", \"tops\").\nis_a(\"cardigan\", \"tops\").\nis_a(\"halter\", \"tops\").\nis_a(\"parka\", \"tops\").\nis_a(\"turtleneck\", \"tops\").\nis_a(\"hoodie\", \"tops\").\nis_a(\"bikini\", \"tops\").\nis_a(\"tank top\", \"tops\").\nis_a(\"vest\", \"tops\").", "is_a(\"tank top\", \"tops\").\nis_a(\"vest\", \"tops\").\nis_a(\"jersey\", \"tops\").\nis_a(\"t shirt\", \"tops\").\nis_a(\"polo shirt\", \"tops\").\nis_a(\"dress shirt\", \"tops\").\nis_a(\"undershirt\", \"tops\").\nis_a(\"shirt\", \"tops\").\nis_a(\"sweatshirt\", \"tops\").\nis_a(\"jacket\", \"tops\").", "is_a(\"sweatshirt\", \"tops\").\nis_a(\"jacket\", \"tops\").\nis_a(\"jeans\", \"pants\").\nis_a(\"khaki\", \"pants\").\nis_a(\"denim\", \"pants\").\nis_a(\"jogger\", \"pants\").\nis_a(\"capris\", \"pants\").\nis_a(\"trunks\", \"pants\").\nis_a(\"leggings\", \"pants\").\nis_a(\"trousers\", \"pants\").", "is_a(\"leggings\", \"pants\").\nis_a(\"trousers\", \"pants\").\nis_a(\"shorts\", \"pants\").\nis_a(\"snow pants\", \"pants\").\nis_a(\"shoe\", \"shoes\").\nis_a(\"heel\", \"shoes\").\nis_a(\"tennis shoe\", \"shoes\").\nis_a(\"boot\", \"shoes\").\nis_a(\"sneaker\", \"shoes\").\nis_a(\"sandal\", \"shoes\").", "is_a(\"sneaker\", \"shoes\").\nis_a(\"sandal\", \"shoes\").\nis_a(\"cleat\", \"shoes\").\nis_a(\"slipper\", \"shoes\").\nis_a(\"flip flop\", \"shoes\").\nis_a(\"flipper\", \"shoes\").\nis_a(\"nightdress\", \"dress\").\nis_a(\"kimono\", \"dress\").\nis_a(\"onesie\", \"dress\").\nis_a(\"sundress\", \"dress\").", "is_a(\"onesie\", \"dress\").\nis_a(\"sundress\", \"dress\").\nis_a(\"wedding dress\", \"dress\").\nis_a(\"strapless\", \"dress\").\nis_a(\"pants\", \"clothing\").\nis_a(\"tops\", \"clothing\").\nis_a(\"dress\", \"clothing\").\nis_a(\"skirt\", \"clothing\").\nis_a(\"coat\", \"clothing\").\nis_a(\"suit\", \"clothing\").", "is_a(\"coat\", \"clothing\").\nis_a(\"suit\", \"clothing\").\nis_a(\"jumpsuit\", \"clothing\").\nis_a(\"gown\", \"clothing\").\nis_a(\"robe\", \"clothing\").\nis_a(\"bathrobe\", \"clothing\").\nis_a(\"socks\", \"clothing\").\nis_a(\"helmet\", \"hat\").\nis_a(\"cap\", \"hat\").\nis_a(\"beanie\", \"hat\").", "is_a(\"cap\", \"hat\").\nis_a(\"beanie\", \"hat\").\nis_a(\"visor\", \"hat\").\nis_a(\"hood\", \"hat\").\nis_a(\"bandana\", \"hat\").\nis_a(\"baseball cap\", \"hat\").\nis_a(\"headband\", \"hat\").\nis_a(\"cowboy hat\", \"hat\").\nis_a(\"chef hat\", \"hat\").\nis_a(\"mitt\", \"glove\").", "is_a(\"chef hat\", \"hat\").\nis_a(\"mitt\", \"glove\").\nis_a(\"baseball glove\", \"glove\").\nis_a(\"baseball mitt\", \"glove\").\nis_a(\"mitten\", \"glove\").\nis_a(\"necklace\", \"jewelry\").\nis_a(\"ring\", \"jewelry\").\nis_a(\"earring\", \"jewelry\").\nis_a(\"bracelet\", \"jewelry\").\nis_a(\"shoes\", \"accessory\").", "is_a(\"bracelet\", \"jewelry\").\nis_a(\"shoes\", \"accessory\").\nis_a(\"scarf\", \"accessory\").\nis_a(\"tie\", \"accessory\").\nis_a(\"veil\", \"accessory\").\nis_a(\"mask\", \"accessory\").\nis_a(\"apron\", \"accessory\").\nis_a(\"poncho\", \"accessory\").\nis_a(\"cape\", \"accessory\").\nis_a(\"belt\", \"accessory\").", "is_a(\"cape\", \"accessory\").\nis_a(\"belt\", \"accessory\").\nis_a(\"handkerchief\", \"accessory\").\nis_a(\"hairband\", \"accessory\").\nis_a(\"life jacket\", \"accessory\").\nis_a(\"lanyard\", \"accessory\").\nis_a(\"crown\", \"accessory\").\nis_a(\"garland\", \"accessory\").\nis_a(\"wristband\", \"accessory\").\nis_a(\"watch\", \"accessory\").", "is_a(\"wristband\", \"accessory\").\nis_a(\"watch\", \"accessory\").\nis_a(\"wristwatch\", \"accessory\").\nis_a(\"pocket watch\", \"accessory\").\nis_a(\"hat\", \"accessory\").\nis_a(\"glove\", \"accessory\").\nis_a(\"jewelry\", \"accessory\").\nis_a(\"glasses\", \"accessory\").\nis_a(\"eye glasses\", \"glasses\").\nis_a(\"sunglasses\", \"glasses\").", "is_a(\"eye glasses\", \"glasses\").\nis_a(\"sunglasses\", \"glasses\").\nis_a(\"goggles\", \"glasses\").\nis_a(\"shrimp\", \"meat\").\nis_a(\"ribs\", \"meat\").\nis_a(\"steak\", \"meat\").\nis_a(\"beef\", \"meat\").\nis_a(\"egg\", \"meat\").\nis_a(\"egg shell\", \"meat\").\nis_a(\"chicken\", \"meat\").", "is_a(\"egg shell\", \"meat\").\nis_a(\"chicken\", \"meat\").\nis_a(\"chicken breast\", \"meat\").\nis_a(\"pepperoni\", \"meat\").\nis_a(\"bacon\", \"meat\").\nis_a(\"ham\", \"meat\").\nis_a(\"sausage\", \"meat\").\nis_a(\"pork\", \"meat\").\nis_a(\"bacon\", \"pork\").\nis_a(\"ham\", \"pork\").", "is_a(\"bacon\", \"pork\").\nis_a(\"ham\", \"pork\").\nis_a(\"pepperoni\", \"sausage\").\nis_a(\"salami\", \"sausage\").\nis_a(\"apple\", \"fruit\").\nis_a(\"pineapple\", \"fruit\").\nis_a(\"banana\", \"fruit\").\nis_a(\"olives\", \"fruit\").\nis_a(\"orange\", \"fruit\").\nis_a(\"grapes\", \"fruit\").", "is_a(\"orange\", \"fruit\").\nis_a(\"grapes\", \"fruit\").\nis_a(\"strawberry\", \"fruit\").\nis_a(\"cherry\", \"fruit\").\nis_a(\"lemon\", \"fruit\").\nis_a(\"lime\", \"fruit\").\nis_a(\"mango\", \"fruit\").\nis_a(\"peach\", \"fruit\").\nis_a(\"tangerine\", \"fruit\").\nis_a(\"grape\", \"fruit\").", "is_a(\"tangerine\", \"fruit\").\nis_a(\"grape\", \"fruit\").\nis_a(\"kiwi\", \"fruit\").\nis_a(\"pear\", \"fruit\").\nis_a(\"watermelon\", \"fruit\").\nis_a(\"berry\", \"fruit\").\nis_a(\"blueberry\", \"fruit\").\nis_a(\"raspberry\", \"fruit\").\nis_a(\"cranberry\", \"fruit\").\nis_a(\"raisin\", \"fruit\").", "is_a(\"cranberry\", \"fruit\").\nis_a(\"raisin\", \"fruit\").\nis_a(\"gourd\", \"fruit\").\nis_a(\"grapefruit\", \"fruit\").\nis_a(\"melon\", \"fruit\").\nis_a(\"pomegranate\", \"fruit\").\nis_a(\"papaya\", \"fruit\").\nis_a(\"coconut\", \"fruit\").\nis_a(\"citrus fruit\", \"fruit\").\nis_a(\"tangerine\", \"citrus fruit\").", "is_a(\"citrus fruit\", \"fruit\").\nis_a(\"tangerine\", \"citrus fruit\").\nis_a(\"onion\", \"vegetable\").\nis_a(\"pumpkin\", \"vegetable\").\nis_a(\"spinach\", \"vegetable\").\nis_a(\"broccoli\", \"vegetable\").\nis_a(\"mushroom\", \"vegetable\").\nis_a(\"carrot\", \"vegetable\").\nis_a(\"cabbage\", \"vegetable\").\nis_a(\"potato\", \"vegetable\").", "is_a(\"cabbage\", \"vegetable\").\nis_a(\"potato\", \"vegetable\").\nis_a(\"lettuce\", \"vegetable\").\nis_a(\"tomato\", \"vegetable\").\nis_a(\"beans\", \"vegetable\").\nis_a(\"squash\", \"vegetable\").\nis_a(\"cucumber\", \"vegetable\").\nis_a(\"eggplant\", \"vegetable\").\nis_a(\"celery\", \"vegetable\").\nis_a(\"pepper\", \"vegetable\").", "is_a(\"celery\", \"vegetable\").\nis_a(\"pepper\", \"vegetable\").\nis_a(\"chili\", \"vegetable\").\nis_a(\"parsley\", \"vegetable\").\nis_a(\"sweet potato\", \"vegetable\").\nis_a(\"olive\", \"vegetable\").\nis_a(\"zucchini\", \"vegetable\").\nis_a(\"artichoke\", \"vegetable\").\nis_a(\"cauliflower\", \"vegetable\").\nis_a(\"avocado\", \"vegetable\").", "is_a(\"cauliflower\", \"vegetable\").\nis_a(\"avocado\", \"vegetable\").\nis_a(\"herb\", \"vegetable\").\nis_a(\"beet\", \"vegetable\").\nis_a(\"pea\", \"vegetable\").\nis_a(\"nut\", \"nut\").\nis_a(\"walnut\", \"nut\").\nis_a(\"pecan\", \"nut\").\nis_a(\"peanut\", \"nut\").\nis_a(\"pistachio\", \"nut\").", "is_a(\"peanut\", \"nut\").\nis_a(\"pistachio\", \"nut\").\nis_a(\"almond\", \"nut\").\nis_a(\"dip\", \"condiment\").\nis_a(\"pesto\", \"condiment\").\nis_a(\"hummus\", \"condiment\").\nis_a(\"peanut butter\", \"condiment\").\nis_a(\"ginger\", \"condiment\").\nis_a(\"toast\", \"breakfast food\").\nis_a(\"cereal\", \"breakfast food\").", "is_a(\"toast\", \"breakfast food\").\nis_a(\"cereal\", \"breakfast food\").\nis_a(\"doughnut\", \"breakfast food\").\nis_a(\"waffle\", \"breakfast food\").\nis_a(\"egg\", \"breakfast food\").\nis_a(\"pancake\", \"breakfast food\").\nis_a(\"beans\", \"side dishes\").\nis_a(\"broccoli\", \"side dishes\").\nis_a(\"potato\", \"side dishes\").\nis_a(\"salad\", \"side dishes\").", "is_a(\"potato\", \"side dishes\").\nis_a(\"salad\", \"side dishes\").\nis_a(\"cabbage\", \"side dishes\").\nis_a(\"squash\", \"side dishes\").\nis_a(\"mushroom\", \"side dishes\").\nis_a(\"fries\", \"side dishes\").\nis_a(\"maize\", \"staple food\").\nis_a(\"millet\", \"staple food\").\nis_a(\"sorghum\", \"staple food\").\nis_a(\"rice\", \"soft food\").", "is_a(\"sorghum\", \"staple food\").\nis_a(\"rice\", \"soft food\").\nis_a(\"ice cream\", \"soft food\").\nis_a(\"chocolate\", \"soft food\").\nis_a(\"cake\", \"soft food\").\nis_a(\"cupcake\", \"soft food\").\nis_a(\"cheesecake\", \"soft food\").\nis_a(\"pie\", \"soft food\").\nis_a(\"pudding\", \"soft food\").\nis_a(\"sauce\", \"soft food\").", "is_a(\"pudding\", \"soft food\").\nis_a(\"sauce\", \"soft food\").\nis_a(\"dip\", \"soft food\").\nis_a(\"sugar\", \"soft food\").\nis_a(\"caramel\", \"soft food\").\nis_a(\"ketchup\", \"soft food\").\nis_a(\"pesto\", \"soft food\").\nis_a(\"gravy\", \"soft food\").\nis_a(\"guacamole\", \"soft food\").\nis_a(\"hummus\", \"soft food\").", "is_a(\"guacamole\", \"soft food\").\nis_a(\"hummus\", \"soft food\").\nis_a(\"peanut butter\", \"soft food\").\nis_a(\"butter\", \"soft food\").\nis_a(\"syrup\", \"soft food\").\nis_a(\"mustard\", \"soft food\").\nis_a(\"meat\", \"solid food\").\nis_a(\"side dishes\", \"solid food\").\nis_a(\"fruit\", \"solid food\").\nis_a(\"main course\", \"solid food\").", "is_a(\"fruit\", \"solid food\").\nis_a(\"main course\", \"solid food\").\nis_a(\"vegetable\", \"solid food\").\nis_a(\"pizza\", \"food\").\nis_a(\"sandwich\", \"food\").\nis_a(\"hot dog\", \"food\").\nis_a(\"noodles\", \"food\").\nis_a(\"pasta\", \"food\").\nis_a(\"donut\", \"food\").\nis_a(\"cupcake\", \"food\").", "is_a(\"donut\", \"food\").\nis_a(\"cupcake\", \"food\").\nis_a(\"bread\", \"food\").\nis_a(\"rice\", \"food\").\nis_a(\"cereal\", \"food\").\nis_a(\"chips\", \"food\").\nis_a(\"bun\", \"food\").\nis_a(\"cake\", \"food\").\nis_a(\"doughnut\", \"food\").\nis_a(\"fries\", \"food\").", "is_a(\"doughnut\", \"food\").\nis_a(\"fries\", \"food\").\nis_a(\"burger\", \"food\").\nis_a(\"hamburger\", \"food\").\nis_a(\"porridge\", \"food\").\nis_a(\"pie\", \"food\").\nis_a(\"vegetable\", \"food\").\nis_a(\"nut\", \"food\").\nis_a(\"meat\", \"food\").\nis_a(\"fruit\", \"food\").", "is_a(\"meat\", \"food\").\nis_a(\"fruit\", \"food\").\nis_a(\"grains\", \"food\").\nis_a(\"side dishes\", \"food\").\nis_a(\"dessert\", \"food\").\nis_a(\"main course\", \"food\").\nis_a(\"breakfast food\", \"food\").\nis_a(\"milk\", \"drinks\").\nis_a(\"juice\", \"drinks\").\nis_a(\"soda\", \"drinks\").", "is_a(\"juice\", \"drinks\").\nis_a(\"soda\", \"drinks\").\nis_a(\"cola\", \"drinks\").\nis_a(\"cappuccino\", \"drinks\").\nis_a(\"milkshake\", \"drinks\").\nis_a(\"lemonade\", \"drinks\").\nis_a(\"liquor\", \"drinks\").\nis_a(\"alcohol\", \"drinks\").\nis_a(\"beer\", \"drinks\").\nis_a(\"wine\", \"drinks\").", "is_a(\"beer\", \"drinks\").\nis_a(\"wine\", \"drinks\").\nis_a(\"champagne\", \"drinks\").\nis_a(\"coffee\", \"drinks\").\nis_a(\"tea\", \"drinks\").\nis_a(\"water\", \"drinks\").\nis_a(\"soft drinks\", \"drinks\").\nis_a(\"alcoholic beverage\", \"drinks\").\nis_a(\"milk\", \"beverage\").\nis_a(\"juice\", \"beverage\").", "is_a(\"milk\", \"beverage\").\nis_a(\"juice\", \"beverage\").\nis_a(\"soda\", \"beverage\").\nis_a(\"cola\", \"beverage\").\nis_a(\"cappuccino\", \"beverage\").\nis_a(\"milkshake\", \"beverage\").\nis_a(\"lemonade\", \"beverage\").\nis_a(\"liquor\", \"beverage\").\nis_a(\"alcohol\", \"beverage\").\nis_a(\"beer\", \"beverage\").", "is_a(\"alcohol\", \"beverage\").\nis_a(\"beer\", \"beverage\").\nis_a(\"wine\", \"beverage\").\nis_a(\"champagne\", \"beverage\").\nis_a(\"coffee\", \"beverage\").\nis_a(\"tea\", \"beverage\").\nis_a(\"water\", \"beverage\").\nis_a(\"shelf\", \"furniture\").\nis_a(\"bookshelf\", \"furniture\").\nis_a(\"bookcase\", \"furniture\").", "is_a(\"bookshelf\", \"furniture\").\nis_a(\"bookcase\", \"furniture\").\nis_a(\"drawer\", \"furniture\").\nis_a(\"entertainment center\", \"furniture\").\nis_a(\"medicine cabinet\", \"furniture\").\nis_a(\"table\", \"furniture\").\nis_a(\"end table\", \"furniture\").\nis_a(\"dining table\", \"furniture\").\nis_a(\"picnic table\", \"furniture\").\nis_a(\"side table\", \"furniture\").", "is_a(\"picnic table\", \"furniture\").\nis_a(\"side table\", \"furniture\").\nis_a(\"coffee table\", \"furniture\").\nis_a(\"banquet table\", \"furniture\").\nis_a(\"desk\", \"furniture\").\nis_a(\"computer desk\", \"furniture\").\nis_a(\"tv stand\", \"furniture\").\nis_a(\"bed\", \"furniture\").\nis_a(\"mattress\", \"furniture\").\nis_a(\"nightstand\", \"furniture\").", "is_a(\"mattress\", \"furniture\").\nis_a(\"nightstand\", \"furniture\").\nis_a(\"counter\", \"furniture\").\nis_a(\"blind\", \"furniture\").\nis_a(\"cabinet\", \"furniture\").\nis_a(\"wardrobe\", \"furniture\").\nis_a(\"chair\", \"furniture\").\nis_a(\"armchair\", \"furniture\").\nis_a(\"folding chair\", \"furniture\").\nis_a(\"beach chair\", \"furniture\").", "is_a(\"folding chair\", \"furniture\").\nis_a(\"beach chair\", \"furniture\").\nis_a(\"office chair\", \"furniture\").\nis_a(\"recliner\", \"furniture\").\nis_a(\"bench\", \"furniture\").\nis_a(\"stool\", \"furniture\").\nis_a(\"bar stool\", \"furniture\").\nis_a(\"seat\", \"furniture\").\nis_a(\"couch\", \"furniture\").\nis_a(\"sofa\", \"furniture\").", "is_a(\"couch\", \"furniture\").\nis_a(\"sofa\", \"furniture\").\nis_a(\"ottoman\", \"furniture\").\nis_a(\"closet\", \"furniture\").\nis_a(\"dresser\", \"furniture\").\nis_a(\"cupboard\", \"furniture\").\nis_a(\"lamp\", \"furniture\").\nis_a(\"spatula\", \"kitchenware\").\nis_a(\"colander\", \"kitchenware\").\nis_a(\"tongs\", \"kitchenware\").", "is_a(\"colander\", \"kitchenware\").\nis_a(\"tongs\", \"kitchenware\").\nis_a(\"blade\", \"kitchenware\").\nis_a(\"cutting board\", \"kitchenware\").\nis_a(\"foil\", \"kitchenware\").\nis_a(\"dishwasher\", \"kitchenware\").\nis_a(\"sink\", \"kitchenware\").\nis_a(\"microwave\", \"kitchenware\").\nis_a(\"blender\", \"kitchenware\").\nis_a(\"toaster\", \"kitchenware\").", "is_a(\"blender\", \"kitchenware\").\nis_a(\"toaster\", \"kitchenware\").\nis_a(\"oven\", \"kitchenware\").\nis_a(\"stove\", \"kitchenware\").\nis_a(\"grill\", \"kitchenware\").\nis_a(\"fridge\", \"kitchenware\").\nis_a(\"container\", \"kitchenware\").\nis_a(\"pot\", \"kitchenware\").\nis_a(\"kettle\", \"kitchenware\").\nis_a(\"mixer\", \"kitchenware\").", "is_a(\"kettle\", \"kitchenware\").\nis_a(\"mixer\", \"kitchenware\").\nis_a(\"electrical appliance\", \"home appliance\").\nis_a(\"kitchenware\", \"home appliance\").\nis_a(\"radiator\", \"home appliance\").\nis_a(\"stove\", \"home appliance\").\nis_a(\"gas stove\", \"home appliance\").\nis_a(\"spoon\", \"tableware\").\nis_a(\"knife\", \"tableware\").\nis_a(\"fork\", \"tableware\").", "is_a(\"knife\", \"tableware\").\nis_a(\"fork\", \"tableware\").\nis_a(\"chopsticks\", \"tableware\").\nis_a(\"tray\", \"tableware\").\nis_a(\"pizza tray\", \"tableware\").\nis_a(\"placemat\", \"tableware\").\nis_a(\"dishes\", \"tableware\").\nis_a(\"napkin\", \"tableware\").\nis_a(\"plate\", \"tableware\").\nis_a(\"saucer\", \"tableware\").", "is_a(\"plate\", \"tableware\").\nis_a(\"saucer\", \"tableware\").\nis_a(\"cup\", \"tableware\").\nis_a(\"coffee cup\", \"tableware\").\nis_a(\"glass\", \"tableware\").\nis_a(\"wine glass\", \"tableware\").\nis_a(\"water glass\", \"tableware\").\nis_a(\"mug\", \"tableware\").\nis_a(\"beer mug\", \"tableware\").\nis_a(\"coffee mug\", \"tableware\").", "is_a(\"beer mug\", \"tableware\").\nis_a(\"coffee mug\", \"tableware\").\nis_a(\"bowl\", \"tableware\").\nis_a(\"straw\", \"tableware\").\nis_a(\"tablecloth\", \"tableware\").\nis_a(\"cloth\", \"tableware\").\nis_a(\"basket\", \"tableware\").\nis_a(\"candle\", \"tableware\").\nis_a(\"can\", \"tableware\").\nis_a(\"salt shaker\", \"tableware\").", "is_a(\"can\", \"tableware\").\nis_a(\"salt shaker\", \"tableware\").\nis_a(\"pepper shaker\", \"tableware\").\nis_a(\"vessel\", \"tableware\").\nis_a(\"vase\", \"tableware\").\nis_a(\"eating utensil\", \"utensil\").\nis_a(\"kitchen utensil\", \"utensil\").\nis_a(\"shampoo bottle\", \"bottle\").\nis_a(\"perfume bottle\", \"bottle\").\nis_a(\"soap bottle\", \"bottle\").", "is_a(\"perfume bottle\", \"bottle\").\nis_a(\"soap bottle\", \"bottle\").\nis_a(\"ketchup bottle\", \"bottle\").\nis_a(\"spray bottle\", \"bottle\").\nis_a(\"mustard bottle\", \"bottle\").\nis_a(\"water bottle\", \"bottle\").\nis_a(\"wine bottle\", \"bottle\").\nis_a(\"soda bottle\", \"bottle\").\nis_a(\"beer bottle\", \"bottle\").\nis_a(\"adidas\", \"logo\").", "is_a(\"beer bottle\", \"bottle\").\nis_a(\"adidas\", \"logo\").\nis_a(\"nike\", \"logo\").\nis_a(\"apple logo\", \"logo\").\nis_a(\"adidas\", \"brand\").\nis_a(\"nike\", \"brand\").\nis_a(\"laptop\", \"computer\").\nis_a(\"bedspread\", \"bedding\").\nis_a(\"pillow\", \"bedding\").\nis_a(\"duvet\", \"bedding\").", "is_a(\"pillow\", \"bedding\").\nis_a(\"duvet\", \"bedding\").\nis_a(\"duvet cover\", \"bedding\").\nis_a(\"quilt\", \"bedding\").\nis_a(\"sheet\", \"bedding\").\nis_a(\"blanket\", \"bedding\").\nis_a(\"mattress\", \"bedding\").\nis_a(\"teddy bear\", \"toy\").\nis_a(\"rubber duck\", \"toy\").\nis_a(\"lego\", \"toy\").", "is_a(\"rubber duck\", \"toy\").\nis_a(\"lego\", \"toy\").\nis_a(\"stuffed bear\", \"toy\").\nis_a(\"stuffed dog\", \"toy\").\nis_a(\"stuffed animal\", \"toy\").\nis_a(\"toy car\", \"toy\").\nis_a(\"balloon\", \"toy\").\nis_a(\"doll\", \"toy\").\nis_a(\"kite\", \"toy\").\nis_a(\"frisbee\", \"toy\").", "is_a(\"kite\", \"toy\").\nis_a(\"frisbee\", \"toy\").\nis_a(\"teddy bear\", \"stuffed animal\").\nis_a(\"stuffed bear\", \"stuffed animal\").\nis_a(\"stuffed dog\", \"stuffed animal\").\nis_a(\"cable\", \"electronic device\").\nis_a(\"hard drive\", \"electronic device\").\nis_a(\"charger\", \"electronic device\").\nis_a(\"cd\", \"electronic device\").\nis_a(\"remote\", \"electronic device\").", "is_a(\"cd\", \"electronic device\").\nis_a(\"remote\", \"electronic device\").\nis_a(\"controller\", \"electronic device\").\nis_a(\"telephone\", \"electronic device\").\nis_a(\"bicycle\", \"vehicle\").\nis_a(\"cart\", \"vehicle\").\nis_a(\"wagon\", \"vehicle\").\nis_a(\"carriage\", \"vehicle\").\nis_a(\"stroller\", \"vehicle\").\nis_a(\"motorcycle\", \"vehicle\").", "is_a(\"stroller\", \"vehicle\").\nis_a(\"motorcycle\", \"vehicle\").\nis_a(\"scooter\", \"vehicle\").\nis_a(\"subway\", \"vehicle\").\nis_a(\"train\", \"vehicle\").\nis_a(\"car\", \"vehicle\").\nis_a(\"planter\", \"vehicle\").\nis_a(\"tractor\", \"vehicle\").\nis_a(\"crane\", \"vehicle\").\nis_a(\"aircraft\", \"vehicle\").", "is_a(\"crane\", \"vehicle\").\nis_a(\"aircraft\", \"vehicle\").\nis_a(\"watercraft\", \"vehicle\").\nis_a(\"trailer\", \"vehicle\").\nis_a(\"truck\", \"vehicle\").\nis_a(\"fire truck\", \"vehicle\").\nis_a(\"bus\", \"vehicle\").\nis_a(\"school bus\", \"vehicle\").\nis_a(\"ambulance\", \"vehicle\").\nis_a(\"double decker\", \"vehicle\").", "is_a(\"ambulance\", \"vehicle\").\nis_a(\"double decker\", \"vehicle\").\nis_a(\"taxi\", \"vehicle\").\nis_a(\"sedan\", \"car\").\nis_a(\"minivan\", \"car\").\nis_a(\"van\", \"car\").\nis_a(\"pickup\", \"car\").\nis_a(\"jeep\", \"car\").\nis_a(\"suv\", \"car\").\nis_a(\"micro\", \"car\").", "is_a(\"suv\", \"car\").\nis_a(\"micro\", \"car\").\nis_a(\"hatchback\", \"car\").\nis_a(\"coupe\", \"car\").\nis_a(\"station wagon\", \"car\").\nis_a(\"roadster\", \"car\").\nis_a(\"cabriolet\", \"car\").\nis_a(\"muscle car\", \"car\").\nis_a(\"sport car\", \"car\").\nis_a(\"super car\", \"car\").", "is_a(\"sport car\", \"car\").\nis_a(\"super car\", \"car\").\nis_a(\"limousine\", \"car\").\nis_a(\"cuv\", \"car\").\nis_a(\"campervan\", \"car\").\nis_a(\"engine\", \"part of vehicle\").\nis_a(\"cargo\", \"part of vehicle\").\nis_a(\"steering wheel\", \"part of vehicle\").\nis_a(\"kickstand\", \"part of vehicle\").\nis_a(\"wheel\", \"part of vehicle\").", "is_a(\"kickstand\", \"part of vehicle\").\nis_a(\"wheel\", \"part of vehicle\").\nis_a(\"tire\", \"part of vehicle\").\nis_a(\"windshield\", \"part of vehicle\").\nis_a(\"propeller\", \"part of vehicle\").\nis_a(\"paddle\", \"part of vehicle\").\nis_a(\"locomotive\", \"part of vehicle\").\nis_a(\"letters\", \"symbol\").\nis_a(\"words\", \"symbol\").\nis_a(\"numbers\", \"symbol\").", "is_a(\"words\", \"symbol\").\nis_a(\"numbers\", \"symbol\").\nis_a(\"roman numerals\", \"symbol\").\nis_a(\"snowboard\", \"sports equipment\").\nis_a(\"skateboard\", \"sports equipment\").\nis_a(\"surfboard\", \"sports equipment\").\nis_a(\"skis\", \"sports equipment\").\nis_a(\"frisbee\", \"sports equipment\").\nis_a(\"ball\", \"sports equipment\").\nis_a(\"tennis ball\", \"sports equipment\").", "is_a(\"ball\", \"sports equipment\").\nis_a(\"tennis ball\", \"sports equipment\").\nis_a(\"soccer ball\", \"sports equipment\").\nis_a(\"kite\", \"sports equipment\").\nis_a(\"hurdle\", \"sports equipment\").\nis_a(\"racket\", \"sports equipment\").\nis_a(\"baseball bat\", \"sports equipment\").\nis_a(\"baseball helmet\", \"sports equipment\").\nis_a(\"baseball glove\", \"sports equipment\").\nis_a(\"baseball mitt\", \"sports equipment\").", "is_a(\"baseball glove\", \"sports equipment\").\nis_a(\"baseball mitt\", \"sports equipment\").\nis_a(\"goggles\", \"sports equipment\").\nis_a(\"bedroom\", \"place\").\nis_a(\"living room\", \"place\").\nis_a(\"dining room\", \"place\").\nis_a(\"kitchen\", \"place\").\nis_a(\"bathroom\", \"place\").\nis_a(\"alcove\", \"place\").\nis_a(\"attic\", \"place\").", "is_a(\"alcove\", \"place\").\nis_a(\"attic\", \"place\").\nis_a(\"basement\", \"place\").\nis_a(\"closet\", \"place\").\nis_a(\"home office\", \"place\").\nis_a(\"pantry\", \"place\").\nis_a(\"shower\", \"place\").\nis_a(\"staircase\", \"place\").\nis_a(\"tent\", \"place\").\nis_a(\"hall\", \"place\").", "is_a(\"tent\", \"place\").\nis_a(\"hall\", \"place\").\nis_a(\"balcony\", \"place\").\nis_a(\"patio\", \"place\").\nis_a(\"factory\", \"place\").\nis_a(\"lab\", \"place\").\nis_a(\"office\", \"place\").\nis_a(\"classroom\", \"place\").\nis_a(\"building\", \"place\").\nis_a(\"apartment\", \"place\").", "is_a(\"building\", \"place\").\nis_a(\"apartment\", \"place\").\nis_a(\"restaurant\", \"place\").\nis_a(\"alley\", \"place\").\nis_a(\"bar\", \"place\").\nis_a(\"supermarket\", \"place\").\nis_a(\"shop\", \"place\").\nis_a(\"market\", \"place\").\nis_a(\"store\", \"place\").\nis_a(\"mall\", \"place\").", "is_a(\"store\", \"place\").\nis_a(\"mall\", \"place\").\nis_a(\"plaza\", \"place\").\nis_a(\"theater\", \"place\").\nis_a(\"courtyard\", \"place\").\nis_a(\"gas station\", \"place\").\nis_a(\"restroom\", \"place\").\nis_a(\"library\", \"place\").\nis_a(\"dormitory\", \"place\").\nis_a(\"aquarium\", \"place\").", "is_a(\"dormitory\", \"place\").\nis_a(\"aquarium\", \"place\").\nis_a(\"school\", \"place\").\nis_a(\"bank\", \"place\").\nis_a(\"hospital\", \"place\").\nis_a(\"casino\", \"place\").\nis_a(\"baseball field\", \"place\").\nis_a(\"bleachers\", \"place\").\nis_a(\"dugout\", \"place\").\nis_a(\"football field\", \"place\").", "is_a(\"dugout\", \"place\").\nis_a(\"football field\", \"place\").\nis_a(\"soccer field\", \"place\").\nis_a(\"golf course\", \"place\").\nis_a(\"stadium\", \"place\").\nis_a(\"court\", \"place\").\nis_a(\"tennis court\", \"place\").\nis_a(\"ski lift\", \"place\").\nis_a(\"ski slope\", \"place\").\nis_a(\"swimming pool\", \"place\").", "is_a(\"ski slope\", \"place\").\nis_a(\"swimming pool\", \"place\").\nis_a(\"pool\", \"place\").\nis_a(\"playground\", \"place\").\nis_a(\"park\", \"place\").\nis_a(\"resort\", \"place\").\nis_a(\"skate park\", \"place\").\nis_a(\"station\", \"place\").\nis_a(\"bus station\", \"place\").\nis_a(\"train station\", \"place\").", "is_a(\"bus station\", \"place\").\nis_a(\"train station\", \"place\").\nis_a(\"bus stop\", \"place\").\nis_a(\"airport\", \"place\").\nis_a(\"tarmac\", \"place\").\nis_a(\"freeway\", \"place\").\nis_a(\"street\", \"place\").\nis_a(\"tunnel\", \"place\").\nis_a(\"highway\", \"place\").\nis_a(\"driveway\", \"place\").", "is_a(\"highway\", \"place\").\nis_a(\"driveway\", \"place\").\nis_a(\"road\", \"place\").\nis_a(\"crosswalk\", \"place\").\nis_a(\"overpass\", \"place\").\nis_a(\"runway\", \"place\").\nis_a(\"railway\", \"place\").\nis_a(\"parking lot\", \"place\").\nis_a(\"track\", \"place\").\nis_a(\"bridge\", \"place\").", "is_a(\"track\", \"place\").\nis_a(\"bridge\", \"place\").\nis_a(\"airfield\", \"place\").\nis_a(\"shore\", \"place\").\nis_a(\"beach\", \"place\").\nis_a(\"harbor\", \"place\").\nis_a(\"jetty\", \"place\").\nis_a(\"dock\", \"place\").\nis_a(\"pier\", \"place\").\nis_a(\"sidewalk\", \"place\").", "is_a(\"pier\", \"place\").\nis_a(\"sidewalk\", \"place\").\nis_a(\"lane\", \"place\").\nis_a(\"curb\", \"place\").\nis_a(\"crosswalkhouse\", \"place\").\nis_a(\"home\", \"place\").\nis_a(\"hotel\", \"place\").\nis_a(\"farm\", \"place\").\nis_a(\"barn\", \"place\").\nis_a(\"garage\", \"place\").", "is_a(\"barn\", \"place\").\nis_a(\"garage\", \"place\").\nis_a(\"corn field\", \"place\").\nis_a(\"corral\", \"place\").\nis_a(\"garden\", \"place\").\nis_a(\"orchard\", \"place\").\nis_a(\"tower\", \"place\").\nis_a(\"windmill\", \"place\").\nis_a(\"church\", \"place\").\nis_a(\"temple\", \"place\").", "is_a(\"church\", \"place\").\nis_a(\"temple\", \"place\").\nis_a(\"chapel\", \"place\").\nis_a(\"shrine\", \"place\").\nis_a(\"lighthouse\", \"place\").\nis_a(\"clock tower\", \"place\").\nis_a(\"arch\", \"place\").\nis_a(\"dam\", \"place\").\nis_a(\"zoo\", \"place\").\nis_a(\"ocean\", \"place\").", "is_a(\"zoo\", \"place\").\nis_a(\"ocean\", \"place\").\nis_a(\"lake\", \"place\").\nis_a(\"pond\", \"place\").\nis_a(\"river\", \"place\").\nis_a(\"raft\", \"place\").\nis_a(\"creekswamp\", \"place\").\nis_a(\"waterfall\", \"place\").\nis_a(\"wave\", \"place\").\nis_a(\"canyon\", \"place\").", "is_a(\"wave\", \"place\").\nis_a(\"canyon\", \"place\").\nis_a(\"cliff\", \"place\").\nis_a(\"desert\", \"place\").\nis_a(\"mountain\", \"place\").\nis_a(\"hill\", \"place\").\nis_a(\"valley\", \"place\").\nis_a(\"plain\", \"place\").\nis_a(\"air\", \"place\").\nis_a(\"land\", \"place\").", "is_a(\"air\", \"place\").\nis_a(\"land\", \"place\").\nis_a(\"sky\", \"place\").\nis_a(\"bamboo forest\", \"place\").\nis_a(\"forest\", \"place\").\nis_a(\"jungle\", \"place\").\nis_a(\"yard\", \"place\").\nis_a(\"field\", \"place\").\nis_a(\"savanna\", \"place\").\nis_a(\"bayou\", \"place\").", "is_a(\"savanna\", \"place\").\nis_a(\"bayou\", \"place\").\nis_a(\"city\", \"place\").\nis_a(\"downtown\", \"place\").\nis_a(\"wild\", \"place\").\nis_a(\"bedroom\", \"room\").\nis_a(\"living room\", \"room\").\nis_a(\"dining room\", \"room\").\nis_a(\"kitchen\", \"room\").\nis_a(\"bathroom\", \"room\").", "is_a(\"kitchen\", \"room\").\nis_a(\"bathroom\", \"room\").\nis_a(\"alcove\", \"room\").\nis_a(\"attic\", \"room\").\nis_a(\"basement\", \"room\").\nis_a(\"closet\", \"room\").\nis_a(\"home office\", \"room\").\nis_a(\"office\", \"room\").\nis_a(\"pantry\", \"room\").\nis_a(\"shower\", \"room\").", "is_a(\"pantry\", \"room\").\nis_a(\"shower\", \"room\").\nis_a(\"staircase\", \"room\").\nis_a(\"hall\", \"room\").\nis_a(\"balcony\", \"room\").\nis_a(\"hydrant\", \"object\").\nis_a(\"manhole cover\", \"object\").\nis_a(\"fountain\", \"object\").\nis_a(\"line\", \"object\").\nis_a(\"parking meter\", \"object\").", "is_a(\"line\", \"object\").\nis_a(\"parking meter\", \"object\").\nis_a(\"mailbox\", \"object\").\nis_a(\"pole\", \"object\").\nis_a(\"street light\", \"object\").\nis_a(\"sign\", \"object\").\nis_a(\"street sign\", \"object\").\nis_a(\"stop sign\", \"object\").\nis_a(\"traffic sign\", \"object\").\nis_a(\"parking signtraffic light\", \"object\").", "is_a(\"traffic sign\", \"object\").\nis_a(\"parking signtraffic light\", \"object\").\nis_a(\"bench\", \"object\").\nis_a(\"trash can\", \"object\").\nis_a(\"cone\", \"object\").\nis_a(\"dispenser\", \"object\").\nis_a(\"vending machine\", \"object\").\nis_a(\"toolbox\", \"object\").\nis_a(\"buoy\", \"object\").\nis_a(\"dumpster\", \"object\").", "is_a(\"buoy\", \"object\").\nis_a(\"dumpster\", \"object\").\nis_a(\"garbage\", \"object\").\nis_a(\"umbrella\", \"object\").\nis_a(\"canopy\", \"object\").\nis_a(\"backpack\", \"object\").\nis_a(\"luggage\", \"object\").\nis_a(\"purse\", \"object\").\nis_a(\"wallet\", \"object\").\nis_a(\"bag\", \"object\").", "is_a(\"wallet\", \"object\").\nis_a(\"bag\", \"object\").\nis_a(\"handbag\", \"object\").\nis_a(\"shopping bag\", \"object\").\nis_a(\"trash bag\", \"object\").\nis_a(\"pouch\", \"object\").\nis_a(\"suitcase\", \"object\").\nis_a(\"box\", \"object\").\nis_a(\"crate\", \"object\").\nis_a(\"sack\", \"object\").", "is_a(\"crate\", \"object\").\nis_a(\"sack\", \"object\").\nis_a(\"cardboard\", \"object\").\nis_a(\"light bulb\", \"object\").\nis_a(\"christmas light\", \"object\").\nis_a(\"ceiling light\", \"object\").\nis_a(\"clock\", \"object\").\nis_a(\"alarm clock\", \"object\").\nis_a(\"gift\", \"object\").\nis_a(\"wheelchair\", \"object\").", "is_a(\"gift\", \"object\").\nis_a(\"wheelchair\", \"object\").\nis_a(\"beach umbrella\", \"object\").\nis_a(\"parachute\", \"object\").\nis_a(\"feeder\", \"object\").\nis_a(\"fire extinguisher\", \"object\").\nis_a(\"tissue box\", \"object\").\nis_a(\"paper dispenser\", \"object\").\nis_a(\"soap dispenser\", \"object\").\nis_a(\"napkin dispenser\", \"object\").", "is_a(\"soap dispenser\", \"object\").\nis_a(\"napkin dispenser\", \"object\").\nis_a(\"towel dispenser\", \"object\").\nis_a(\"spray can\", \"object\").\nis_a(\"paint brush\", \"object\").\nis_a(\"cash register\", \"object\").\nis_a(\"candle holder\", \"object\").\nis_a(\"bell\", \"object\").\nis_a(\"lock\", \"object\").\nis_a(\"cigarette\", \"object\").", "is_a(\"lock\", \"object\").\nis_a(\"cigarette\", \"object\").\nis_a(\"curtain\", \"object\").\nis_a(\"carpet\", \"object\").\nis_a(\"rug\", \"object\").\nis_a(\"thermometer\", \"object\").\nis_a(\"fence\", \"object\").\nis_a(\"barrier\", \"object\").\nis_a(\"stick\", \"object\").\nis_a(\"rope\", \"object\").", "is_a(\"stick\", \"object\").\nis_a(\"rope\", \"object\").\nis_a(\"chain\", \"object\").\nis_a(\"hook\", \"object\").\nis_a(\"cage\", \"object\").\nis_a(\"chalk\", \"object\").\nis_a(\"chalkboard\", \"object\").\nis_a(\"money\", \"object\").\nis_a(\"coin\", \"object\").\nis_a(\"shield\", \"object\").", "is_a(\"coin\", \"object\").\nis_a(\"shield\", \"object\").\nis_a(\"armor\", \"object\").\nis_a(\"seat belt\", \"object\").\nis_a(\"chimney\", \"object\").\nis_a(\"fishing pole\", \"object\").\nis_a(\"bottle\", \"object\").\nis_a(\"bandage\", \"object\").\nis_a(\"lipstick\", \"object\").\nis_a(\"wig\", \"object\").", "is_a(\"lipstick\", \"object\").\nis_a(\"wig\", \"object\").\nis_a(\"shaving cream\", \"object\").\nis_a(\"deodorant\", \"object\").\nis_a(\"lotion\", \"object\").\nis_a(\"sink\", \"object\").\nis_a(\"faucet\", \"object\").\nis_a(\"fireplace\", \"object\").\nis_a(\"shower\", \"object\").\nis_a(\"fan\", \"object\").", "is_a(\"shower\", \"object\").\nis_a(\"fan\", \"object\").\nis_a(\"light switch\", \"object\").\nis_a(\"figure\", \"object\").\nis_a(\"frame\", \"object\").\nis_a(\"picture frame\", \"object\").\nis_a(\"door frame\", \"object\").\nis_a(\"window frame\", \"object\").\nis_a(\"lamp\", \"object\").\nis_a(\"table lamp\", \"object\").", "is_a(\"lamp\", \"object\").\nis_a(\"table lamp\", \"object\").\nis_a(\"desk lamp\", \"object\").\nis_a(\"lamps\", \"object\").\nis_a(\"floor lamp\", \"object\").\nis_a(\"sconce\", \"object\").\nis_a(\"chandelier\", \"object\").\nis_a(\"bathtub\", \"object\").\nis_a(\"urinal\", \"object\").\nis_a(\"soap dish\", \"object\").", "is_a(\"urinal\", \"object\").\nis_a(\"soap dish\", \"object\").\nis_a(\"fans\", \"object\").\nis_a(\"string\", \"object\").\nis_a(\"shade\", \"object\").\nis_a(\"tarp\", \"object\").\nis_a(\"handle\", \"object\").\nis_a(\"knob\", \"object\").\nis_a(\"hammer\", \"object\").\nis_a(\"screw\", \"object\").", "is_a(\"hammer\", \"object\").\nis_a(\"screw\", \"object\").\nis_a(\"broom\", \"object\").\nis_a(\"sponge\", \"object\").\nis_a(\"cane\", \"object\").\nis_a(\"knife block\", \"object\").\nis_a(\"waste basket\", \"object\").\nis_a(\"satellite dish\", \"object\").\nis_a(\"shopping cart\", \"object\").\nis_a(\"tape\", \"object\").", "is_a(\"shopping cart\", \"object\").\nis_a(\"tape\", \"object\").\nis_a(\"cord\", \"object\").\nis_a(\"power line\", \"object\").\nis_a(\"book\", \"object\").\nis_a(\"newspaper\", \"object\").\nis_a(\"magazine\", \"object\").\nis_a(\"paper\", \"object\").\nis_a(\"notebook\", \"object\").\nis_a(\"notepad\", \"object\").", "is_a(\"notebook\", \"object\").\nis_a(\"notepad\", \"object\").\nis_a(\"cookbook\", \"object\").\nis_a(\"map\", \"object\").\nis_a(\"envelope\", \"object\").\nis_a(\"pen\", \"object\").\nis_a(\"pencil\", \"object\").\nis_a(\"marker\", \"object\").\nis_a(\"crayon\", \"object\").\nis_a(\"pencil sharpener\", \"object\").", "is_a(\"crayon\", \"object\").\nis_a(\"pencil sharpener\", \"object\").\nis_a(\"ruler\", \"object\").\nis_a(\"binder\", \"object\").\nis_a(\"scissors\", \"object\").\nis_a(\"stapler\", \"object\").\nis_a(\"staples\", \"object\").\nis_a(\"glue stick\", \"object\").\nis_a(\"clip\", \"object\").\nis_a(\"folder\", \"object\").", "is_a(\"clip\", \"object\").\nis_a(\"folder\", \"object\").\nis_a(\"briefcase\", \"object\").\nis_a(\"vehicle\", \"object\").\nis_a(\"sports equipment\", \"object\").\nis_a(\"artwork\", \"object\").\nis_a(\"writing implement\", \"object\").\nis_a(\"electronic device\", \"object\").\nis_a(\"instruments\", \"object\").\nis_a(\"toy\", \"object\").", "is_a(\"instruments\", \"object\").\nis_a(\"toy\", \"object\").\nis_a(\"bedding\", \"object\").\nis_a(\"weapon\", \"object\").\nis_a(\"utensil\", \"object\").\nis_a(\"tableware\", \"object\").\nis_a(\"home appliance\", \"object\").\nis_a(\"kitchenware\", \"object\").\nis_a(\"furniture\", \"object\").\nis_a(\"food\", \"object\").", "is_a(\"furniture\", \"object\").\nis_a(\"food\", \"object\").\nis_a(\"drinks\", \"object\").\nis_a(\"beverage\", \"object\").\nis_a(\"accessory\", \"object\").\nis_a(\"clothing\", \"object\").\nis_a(\"animal\", \"object\").\nis_a(\"plant\", \"object\").\nis_a(\"alpaca\", \"herbivore\").\nis_a(\"amphibian\", \"class\").", "is_a(\"alpaca\", \"herbivore\").\nis_a(\"amphibian\", \"class\").\nis_a(\"odd-toed ungulate\", \"order\").\nis_a(\"even-toed ungulate\", \"order\").\nis_a(\"primate\", \"order\").\nis_a(\"carnivoran\", \"order\").\nis_a(\"canidae\", \"family\").\nis_a(\"perfume\", \"liquid\").\nis_a(\"cosmetic\", \"toiletry\").\nis_a(\"antiperspirant\", \"toiletry\").", "is_a(\"cosmetic\", \"toiletry\").\nis_a(\"antiperspirant\", \"toiletry\").\nis_a(\"eyebrow pencil\", \"cosmetic\").\nis_a(\"face powder\", \"cosmetic\").\nis_a(\"facial moisturizer\", \"cosmetic\").\nis_a(\"rouge\", \"cosmetic\").\n'''\n\nKG_REL= '''\noa_rel(\"is used for\", \"floor\", \"standing on\").", "KG_REL= '''\noa_rel(\"is used for\", \"floor\", \"standing on\").\noa_rel(\"is used for\", \"shelf\", \"storing foods\").\noa_rel(\"is used for\", \"wall\", \"holding up roof\").\noa_rel(\"is used for\", \"chair\", \"sitting on\").\noa_rel(\"is used for\", \"table\", \"holding things\").\noa_rel(\"is used for\", \"bookshelf\", \"storing magazines\").\noa_rel(\"is used for\", \"bookshelf\", \"holding books\").\noa_rel(\"can\", \"vase\", \"holds flowers\").\noa_rel(\"usually appears in\", \"book\", \"office\").", "oa_rel(\"can\", \"vase\", \"holds flowers\").\noa_rel(\"usually appears in\", \"book\", \"office\").\noa_rel(\"is used for\", \"fireplace\", \"burning things\").\noa_rel(\"is used for\", \"window\", \"letting light in\").\noa_rel(\"can\", \"grass\", \"turn brown\").\noa_rel(\"can\", \"fish\", \"navigate via polarised light\").\noa_rel(\"can\", \"cat\", \"eat fish\").\noa_rel(\"is used for\", \"bridge\", \"crossing valley\").\noa_rel(\"is used for\", \"stool\", \"reaching high places\").\noa_rel(\"is\", \"screen\", \"electric\").", "oa_rel(\"is used for\", \"stool\", \"reaching high places\").\noa_rel(\"is\", \"screen\", \"electric\").\noa_rel(\"is used for\", \"carpet\", \"saving floor\").\noa_rel(\"is used for\", \"store\", \"buying and selling\").\noa_rel(\"is used for\", \"box\", \"putting things in\").\noa_rel(\"is used for\", \"dresser\", \"supporting mirror\").\noa_rel(\"can\", \"tree\", \"shade car\").\noa_rel(\"can be\", \"paper\", \"cut\").\noa_rel(\"is used for\", \"table\", \"writing at\").\noa_rel(\"is used for\", \"pot\", \"cooking stew\").", "oa_rel(\"is used for\", \"table\", \"writing at\").\noa_rel(\"is used for\", \"pot\", \"cooking stew\").\noa_rel(\"can\", \"gas stove\", \"heat pot\").\noa_rel(\"is used for\", \"patio\", \"sitting outside\").\noa_rel(\"can\", \"umbrella\", \"protect you from sun\").\noa_rel(\"can be\", \"door\", \"opened or closed\").\noa_rel(\"is used for\", \"seat\", \"sitting on\").\noa_rel(\"can\", \"remote\", \"control tv\").\noa_rel(\"is used for\", \"remote\", \"remotely controlling TV\").\noa_rel(\"is\", \"console\", \"electric\").", "oa_rel(\"is used for\", \"remote\", \"remotely controlling TV\").\noa_rel(\"is\", \"console\", \"electric\").\noa_rel(\"can\", \"car\", \"travel on road\").\noa_rel(\"is used for\", \"window\", \"letting fresh air in\").\noa_rel(\"is used for\", \"car\", \"transporting handful of people\").\noa_rel(\"is used for\", \"road\", \"driving car on\").\noa_rel(\"can\", \"cart\", \"follow horse\").\noa_rel(\"can\", \"horse\", \"pull cart\").\noa_rel(\"is used for\", \"luggage\", \"carrying things\").\noa_rel(\"usually appears in\", \"shelf\", \"bedroom\").", "oa_rel(\"is used for\", \"luggage\", \"carrying things\").\noa_rel(\"usually appears in\", \"shelf\", \"bedroom\").\noa_rel(\"has\", \"cupcake\", \"starch\").\noa_rel(\"is used for\", \"container\", \"holding foods\").\noa_rel(\"is\", \"chocolate\", \"sticky\").\noa_rel(\"is used for\", \"tray\", \"holding food\").\noa_rel(\"is used for\", \"paper\", \"drawing on\").\noa_rel(\"is made from\", \"hot dog\", \"flour\").\noa_rel(\"usually appears in\", \"blanket\", \"bedroom\").\noa_rel(\"is made from\", \"cinnamon roll\", \"flour\").", "oa_rel(\"usually appears in\", \"blanket\", \"bedroom\").\noa_rel(\"is made from\", \"cinnamon roll\", \"flour\").\noa_rel(\"is used for\", \"sugar\", \"sweetening food\").\noa_rel(\"is used for\", \"basket\", \"carrying something\").\noa_rel(\"usually appears in\", \"tray\", \"restaurant\").\noa_rel(\"is made from\", \"dough\", \"flour\").\noa_rel(\"requires\", \"sauteing\", \"pan\").\noa_rel(\"is used for\", \"table\", \"eating at\").\noa_rel(\"is used for\", \"window\", \"looking outside\").\noa_rel(\"is used for\", \"bowl\", \"holding fruit\").", "oa_rel(\"is used for\", \"window\", \"looking outside\").\noa_rel(\"is used for\", \"bowl\", \"holding fruit\").\noa_rel(\"is used for\", \"wall\", \"hanging picture\").\noa_rel(\"is used for\", \"glass\", \"holding drinks\").\noa_rel(\"can\", \"hammer\", \"break glass\").\noa_rel(\"can\", \"knife\", \"cut you\").\noa_rel(\"requires\", \"slicing\", \"knife\").\noa_rel(\"has\", \"cake\", \"starch\").\noa_rel(\"usually appears in\", \"tub\", \"bathroom\").\noa_rel(\"is used for\", \"plate\", \"holding pizza\").", "oa_rel(\"usually appears in\", \"tub\", \"bathroom\").\noa_rel(\"is used for\", \"plate\", \"holding pizza\").\noa_rel(\"is used for\", \"garage\", \"parking car\").\noa_rel(\"is used for\", \"bar\", \"meeting friends\").\noa_rel(\"can\", \"suv\", \"travel on road\").\noa_rel(\"is used for\", \"bar\", \"getting drunk\").\noa_rel(\"is\", \"alcohol\", \"harmful\").\noa_rel(\"can\", \"bottle\", \"hold liquid\").\noa_rel(\"is used for\", \"drinks\", \"satisfying thirst\").\noa_rel(\"is used for\", \"stool\", \"tying shoes\").", "oa_rel(\"is used for\", \"drinks\", \"satisfying thirst\").\noa_rel(\"is used for\", \"stool\", \"tying shoes\").\noa_rel(\"can\", \"vacuum\", \"clean floor\").\noa_rel(\"can\", \"wine\", \"age in bottle\").\noa_rel(\"is\", \"wine\", \"liquid\").\noa_rel(\"is used for\", \"lamp\", \"lighting room\").\noa_rel(\"usually appears in\", \"wine glass\", \"restaurant\").\noa_rel(\"is used for\", \"tablecloth\", \"keeping table clean\").\noa_rel(\"is used for\", \"tablecloth\", \"decoration\").\noa_rel(\"usually appears in\", \"plate\", \"restaurant\").", "oa_rel(\"is used for\", \"tablecloth\", \"decoration\").\noa_rel(\"usually appears in\", \"plate\", \"restaurant\").\noa_rel(\"is used for\", \"barn\", \"keeping animals\").\noa_rel(\"is used for\", \"bedroom\", \"sleeping\").\noa_rel(\"usually appears in\", \"bed\", \"bedroom\").\noa_rel(\"is used for\", \"pillow\", \"make seat softer\").\noa_rel(\"usually appears in\", \"mirror\", \"bathroom\").\noa_rel(\"can\", \"curtain\", \"keep light out of room\").\noa_rel(\"can\", \"tv\", \"display images\").\noa_rel(\"is used for\", \"dresser\", \"storing cloth\").", "oa_rel(\"can\", \"tv\", \"display images\").\noa_rel(\"is used for\", \"dresser\", \"storing cloth\").\noa_rel(\"is used for\", \"door\", \"making room private\").\noa_rel(\"is used for\", \"wall\", \"divide open space into smaller areas\").\noa_rel(\"is\", \"lamp\", \"electric\").\noa_rel(\"is used for\", \"tv\", \"entertainment\").\noa_rel(\"can\", \"truck\", \"pull cars\").\noa_rel(\"can\", \"cart\", \"transport things\").\noa_rel(\"is used for\", \"boat\", \"transporting people\").\noa_rel(\"can\", \"tree\", \"shade lawn\").", "oa_rel(\"is used for\", \"boat\", \"transporting people\").\noa_rel(\"can\", \"tree\", \"shade lawn\").\noa_rel(\"is\", \"water\", \"fluid\").\noa_rel(\"is used for\", \"bicycle\", \"transporting people\").\noa_rel(\"is used for\", \"street\", \"transportation\").\noa_rel(\"is used for\", \"bottle\", \"holding juice\").\noa_rel(\"is used for\", \"table\", \"playing game at\").\noa_rel(\"is used for\", \"fireplace\", \"heating home\").\noa_rel(\"is used for\", \"car\", \"transporting people\").\noa_rel(\"is used for\", \"driveway\", \"transportation\").", "oa_rel(\"is used for\", \"car\", \"transporting people\").\noa_rel(\"is used for\", \"driveway\", \"transportation\").\noa_rel(\"is\", \"computer\", \"electric\").\noa_rel(\"is used for\", \"table\", \"putting things on\").\noa_rel(\"is used for\", \"clock\", \"measuring passage of time\").\noa_rel(\"is used for\", \"necklace\", \"decoration\").\noa_rel(\"is used for\", \"book\", \"learning\").\noa_rel(\"is used for\", \"fence\", \"keeping pets in\").\noa_rel(\"is used for\", \"field\", \"grazing animals\").\noa_rel(\"is used for\", \"floor\", \"walking on\").", "oa_rel(\"is used for\", \"field\", \"grazing animals\").\noa_rel(\"is used for\", \"floor\", \"walking on\").\noa_rel(\"can\", \"grass\", \"grow on hill\").\noa_rel(\"can\", \"car\", \"transport people\").\noa_rel(\"usually appears in\", \"coffee cup\", \"dining room\").\noa_rel(\"is used for\", \"couch\", \"lying on\").\noa_rel(\"usually appears in\", \"switch\", \"bedroom\").\noa_rel(\"is used for\", \"rug\", \"covering area near front door\").\noa_rel(\"is used for\", \"rug\", \"covering floor\").\noa_rel(\"requires\", \"sauteing\", \"skillet\").", "oa_rel(\"is used for\", \"rug\", \"covering floor\").\noa_rel(\"requires\", \"sauteing\", \"skillet\").\noa_rel(\"is used for\", \"stove\", \"boiling water\").\noa_rel(\"is used for\", \"fence\", \"enclosing space\").\noa_rel(\"can\", \"bear\", \"climb tree\").\noa_rel(\"is used for\", \"book\", \"reading for pleasure\").\noa_rel(\"is used for\", \"bar\", \"meeting people\").\noa_rel(\"is used for\", \"road\", \"transportation\").\noa_rel(\"can\", \"bus\", \"carry passengers\").\noa_rel(\"can\", \"bus\", \"travel on road\").", "oa_rel(\"can\", \"bus\", \"carry passengers\").\noa_rel(\"can\", \"bus\", \"travel on road\").\noa_rel(\"usually appears in\", \"mirror\", \"bedroom\").\noa_rel(\"is used for\", \"van\", \"transporting goods\").\noa_rel(\"is used for\", \"phone\", \"communicating\").\noa_rel(\"usually appears in\", \"paper\", \"office\").\noa_rel(\"can be\", \"food\", \"eaten\").\noa_rel(\"is used for\", \"spoon\", \"eating food that isn't very solid\").\noa_rel(\"is used for\", \"spoon\", \"scooping food\").\noa_rel(\"is\", \"dessert\", \"sweet\").", "oa_rel(\"is used for\", \"spoon\", \"scooping food\").\noa_rel(\"is\", \"dessert\", \"sweet\").\noa_rel(\"usually appears in\", \"steak\", \"dinner\").\noa_rel(\"is\", \"water\", \"liquid\").\noa_rel(\"can\", \"bowl\", \"keep water in\").\noa_rel(\"requires\", \"cooking\", \"cooking utensils\").\noa_rel(\"has\", \"watermelon\", \"vitamin C\").\noa_rel(\"can be\", \"fruit\", \"eaten\").\noa_rel(\"is used for\", \"wall\", \"hanging art work\").\noa_rel(\"is\", \"ocean\", \"fluid\").", "oa_rel(\"is used for\", \"wall\", \"hanging art work\").\noa_rel(\"is\", \"ocean\", \"fluid\").\noa_rel(\"can\", \"chicken\", \"be pet\").\noa_rel(\"is used for\", \"bowl\", \"holding cereal\").\noa_rel(\"usually appears in\", \"carrot\", \"salad\").\noa_rel(\"usually appears in\", \"lettuce\", \"salad\").\noa_rel(\"is\", \"sauce\", \"sticky\").\noa_rel(\"is used for\", \"plate\", \"holding food\").\noa_rel(\"is used for\", \"food\", \"eating\").\noa_rel(\"has\", \"cabbage\", \"vitamin C\").", "oa_rel(\"is used for\", \"food\", \"eating\").\noa_rel(\"has\", \"cabbage\", \"vitamin C\").\noa_rel(\"is used for\", \"bar\", \"drinking alcohol\").\noa_rel(\"has\", \"cola\", \"water\").\noa_rel(\"usually appears in\", \"laptop\", \"office\").\noa_rel(\"is used for\", \"desk\", \"putting computer on\").\noa_rel(\"is used for\", \"bed\", \"lying down\").\noa_rel(\"is used for\", \"mouse\", \"interfacing with computer\").\noa_rel(\"is used for\", \"refrigerator\", \"chilling drinks\").\noa_rel(\"is used for\", \"drinks\", \"drinking\").", "oa_rel(\"is used for\", \"refrigerator\", \"chilling drinks\").\noa_rel(\"is used for\", \"drinks\", \"drinking\").\noa_rel(\"is used for\", \"street sign\", \"giving instructions to road users\").\noa_rel(\"can\", \"tree\", \"grow new branches\").\noa_rel(\"requires\", \"cooling off\", \"air conditioner\").\noa_rel(\"can\", \"water\", \"feel wet\").\noa_rel(\"is used for\", \"umbrella\", \"protection from rain\").\noa_rel(\"is\", \"apple\", \"healthy\").\noa_rel(\"can\", \"car\", \"move quickly\").\noa_rel(\"can\", \"glass\", \"hold liquid\").", "oa_rel(\"can\", \"car\", \"move quickly\").\noa_rel(\"can\", \"glass\", \"hold liquid\").\noa_rel(\"can\", \"horse\", \"pull wagon\").\noa_rel(\"usually appears in\", \"candle\", \"dining room\").\noa_rel(\"is used for\", \"bench\", \"lying down\").\noa_rel(\"can\", \"cup\", \"store liquid\").\noa_rel(\"is used for\", \"curtain\", \"covering window\").\noa_rel(\"can\", \"umbrella\", \"shield one from rain or sun\").\noa_rel(\"is used for\", \"curtain\", \"blocking light\").\noa_rel(\"is used for\", \"bowl\", \"holding beans\").", "oa_rel(\"is used for\", \"curtain\", \"blocking light\").\noa_rel(\"is used for\", \"bowl\", \"holding beans\").\noa_rel(\"can\", \"cat\", \"kill birds\").\noa_rel(\"is used for\", \"sidewalk\", \"skating on\").\noa_rel(\"is used for\", \"sidewalk\", \"walking dog\").\noa_rel(\"usually appears in\", \"clip\", \"office\").\noa_rel(\"usually appears in\", \"lid\", \"bathroom\").\noa_rel(\"is\", \"zebra\", \"herbivorous\").\noa_rel(\"is used for\", \"paper\", \"writing on\").\noa_rel(\"is used for\", \"pen\", \"signing checks\").", "oa_rel(\"is used for\", \"paper\", \"writing on\").\noa_rel(\"is used for\", \"pen\", \"signing checks\").\noa_rel(\"usually appears in\", \"monitor\", \"office\").\noa_rel(\"is used for\", \"glove\", \"protecting hand\").\noa_rel(\"can\", \"trailer\", \"travel on road\").\noa_rel(\"can\", \"truck\", \"pull trailer\").\noa_rel(\"has\", \"shrimp\", \"iron\").\noa_rel(\"has\", \"shrimp\", \"vitamin D\").\noa_rel(\"is\", \"lime\", \"sour\").\noa_rel(\"has\", \"meat\", \"vitamin B\").", "oa_rel(\"is\", \"lime\", \"sour\").\noa_rel(\"has\", \"meat\", \"vitamin B\").\noa_rel(\"is made from\", \"bacon\", \"pork\").\noa_rel(\"requires\", \"making pizza\", \"sauce\").\noa_rel(\"is used for\", \"bus station\", \"waiting for bus\").\noa_rel(\"is used for\", \"van\", \"transporting handful of people\").\noa_rel(\"can\", \"car\", \"spend gas\").\noa_rel(\"is used for\", \"track\", \"subway to run on\").\noa_rel(\"is used for\", \"train\", \"transporting goods\").\noa_rel(\"is used for\", \"blinds\", \"keep out light from houses\").", "oa_rel(\"is used for\", \"train\", \"transporting goods\").\noa_rel(\"is used for\", \"blinds\", \"keep out light from houses\").\noa_rel(\"can\", \"computer\", \"help people\").\noa_rel(\"usually appears in\", \"holder\", \"bathroom\").\noa_rel(\"usually appears in\", \"mug\", \"restaurant\").\noa_rel(\"is used for\", \"glasses\", \"improving eyesight\").\noa_rel(\"is used for\", \"horse\", \"riding\").\noa_rel(\"can\", \"tree\", \"grow branch\").\noa_rel(\"has\", \"donut\", \"starch\").\noa_rel(\"is\", \"frosting\", \"sweet\").", "oa_rel(\"has\", \"donut\", \"starch\").\noa_rel(\"is\", \"frosting\", \"sweet\").\noa_rel(\"is used for\", \"shoes\", \"protecting feet\").\noa_rel(\"is used for\", \"handbag\", \"carrying things\").\noa_rel(\"can\", \"cat\", \"see well in dark\").\noa_rel(\"can\", \"bat\", \"hit baseball\").\noa_rel(\"is used for\", \"tennis ball\", \"hitting with racket\").\noa_rel(\"is used for\", \"sink\", \"washing up face\").\noa_rel(\"can\", \"towel\", \"dry hair\").\noa_rel(\"usually appears in\", \"soap\", \"bathroom\").", "oa_rel(\"can\", \"towel\", \"dry hair\").\noa_rel(\"usually appears in\", \"soap\", \"bathroom\").\noa_rel(\"is\", \"cake\", \"sweet\").\noa_rel(\"is made from\", \"cheese\", \"milk\").\noa_rel(\"is used for\", \"bowl\", \"holding food\").\noa_rel(\"is\", \"dog\", \"soft\").\noa_rel(\"is used for\", \"seat\", \"resting\").\noa_rel(\"is used for\", \"hotel\", \"sleeping away from home\").\noa_rel(\"is used for\", \"track\", \"trains to run on\").\noa_rel(\"is used for\", \"motorcycle\", \"transporting people\").", "oa_rel(\"is used for\", \"track\", \"trains to run on\").\noa_rel(\"is used for\", \"motorcycle\", \"transporting people\").\noa_rel(\"can\", \"cat\", \"clean itself often\").\noa_rel(\"is used for\", \"ring\", \"decoration\").\noa_rel(\"is used for\", \"umbrella\", \"keeping sun off you\").\noa_rel(\"is used for\", \"boat\", \"floating and moving on water\").\noa_rel(\"has\", \"bun\", \"starch\").\noa_rel(\"is used for\", \"rug\", \"making floor warmer\").\noa_rel(\"is used for\", \"rug\", \"covering just outside shower\").\noa_rel(\"is used for\", \"cabinet\", \"storing glasses\").", "oa_rel(\"is used for\", \"rug\", \"covering just outside shower\").\noa_rel(\"is used for\", \"cabinet\", \"storing glasses\").\noa_rel(\"usually appears in\", \"fridge\", \"kitchen\").\noa_rel(\"is used for\", \"kitchen\", \"cooking food\").\noa_rel(\"can\", \"butterfly\", \"fly\").\noa_rel(\"is used for\", \"blanket\", \"sleeping under\").\noa_rel(\"is used for\", \"pillow\", \"sleeping\").\noa_rel(\"is used for\", \"bench\", \"sitting on\").\noa_rel(\"can\", \"elephant\", \"lift logs from ground\").\noa_rel(\"has\", \"elephant\", \"trunk\").", "oa_rel(\"can\", \"elephant\", \"lift logs from ground\").\noa_rel(\"has\", \"elephant\", \"trunk\").\noa_rel(\"can\", \"grass\", \"stain pants\").\noa_rel(\"can\", \"grass\", \"continue to grow\").\noa_rel(\"can\", \"elephant\", \"carry trunk\").\noa_rel(\"can\", \"bear\", \"fish with it's paw\").\noa_rel(\"is used for\", \"fence\", \"containing animals\").\noa_rel(\"can\", \"snow\", \"be packed into ball\").\noa_rel(\"can\", \"horse\", \"rest standing up\").\noa_rel(\"can\", \"jeep\", \"climb hills\").", "oa_rel(\"can\", \"horse\", \"rest standing up\").\noa_rel(\"can\", \"jeep\", \"climb hills\").\noa_rel(\"is used for\", \"watch\", \"measuring passage of time\").\noa_rel(\"is used for\", \"computer\", \"studying\").\noa_rel(\"is\", \"laptop\", \"electric\").\noa_rel(\"is used for\", \"candle\", \"decoration\").\noa_rel(\"is used for\", \"mouse\", \"controlling computer\").\noa_rel(\"has\", \"giraffe\", \"marsupium\").\noa_rel(\"is used for\", \"hot dog\", \"eating\").\noa_rel(\"has\", \"beer\", \"alcohol\").", "oa_rel(\"is used for\", \"hot dog\", \"eating\").\noa_rel(\"has\", \"beer\", \"alcohol\").\noa_rel(\"is used for\", \"napkin\", \"wiping mouth\").\noa_rel(\"is used for\", \"carpet\", \"decorating apartment\").\noa_rel(\"is used for\", \"keyboard\", \"interfacing with computer\").\noa_rel(\"is used for\", \"monitor\", \"displaying images\").\noa_rel(\"is used for\", \"stool\", \"sitting on\").\noa_rel(\"is used for\", \"handbag\", \"storing things\").\noa_rel(\"is used for\", \"lane\", \"driving car on\").\noa_rel(\"is used for\", \"desk\", \"reading at\").", "oa_rel(\"is used for\", \"lane\", \"driving car on\").\noa_rel(\"is used for\", \"desk\", \"reading at\").\noa_rel(\"is used for\", \"mat\", \"protection something\").\noa_rel(\"is used for\", \"bus\", \"transporting people\").\noa_rel(\"is used for\", \"bus\", \"mass transit for city\").\noa_rel(\"is used for\", \"trash can\", \"storing trash\").\noa_rel(\"can\", \"turtle\", \"live much longer than people\").\noa_rel(\"is\", \"candy\", \"sweet\").\noa_rel(\"usually appears in\", \"chair\", \"bedroom\").\noa_rel(\"is used for\", \"carpet\", \"walking on\").", "oa_rel(\"usually appears in\", \"chair\", \"bedroom\").\noa_rel(\"is used for\", \"carpet\", \"walking on\").\noa_rel(\"usually appears in\", \"chalkboard\", \"classroom\").\noa_rel(\"is used for\", \"scooter\", \"transporting handful of people\").\noa_rel(\"is used for\", \"toy\", \"having fun\").\noa_rel(\"is used for\", \"fan\", \"circulating air\").\noa_rel(\"is\", \"charger\", \"electric\").\noa_rel(\"is used for\", \"chair\", \"resting\").\noa_rel(\"is used for\", \"bookshelf\", \"storing books\").\noa_rel(\"can\", \"bottle\", \"store wine\").", "oa_rel(\"is used for\", \"bookshelf\", \"storing books\").\noa_rel(\"can\", \"bottle\", \"store wine\").\noa_rel(\"can\", \"monitor\", \"display images\").\noa_rel(\"is used for\", \"keyboard\", \"controlling computer\").\noa_rel(\"is used for\", \"keyboard\", \"entering text\").\noa_rel(\"can\", \"radiator\", \"heat room\").\noa_rel(\"can\", \"fan\", \"cool air\").\noa_rel(\"usually appears in\", \"desk\", \"bedroom\").\noa_rel(\"is\", \"projector\", \"electric\").\noa_rel(\"is used for\", \"computer\", \"data storage\").", "oa_rel(\"is\", \"projector\", \"electric\").\noa_rel(\"is used for\", \"computer\", \"data storage\").\noa_rel(\"is used for\", \"desk\", \"placing something on\").\noa_rel(\"is used for\", \"keyboard\", \"entering data\").\noa_rel(\"is\", \"headphones\", \"electric\").\noa_rel(\"is used for\", \"window\", \"keeping cold air out\").\noa_rel(\"can\", \"computer\", \"save files on disk\").\noa_rel(\"is\", \"monitor\", \"electric\").\noa_rel(\"is used for\", \"office\", \"business\").\noa_rel(\"is used for\", \"door\", \"entering or exiting area\").", "oa_rel(\"is used for\", \"office\", \"business\").\noa_rel(\"is used for\", \"door\", \"entering or exiting area\").\noa_rel(\"is used for\", \"classroom\", \"learning\").\noa_rel(\"can\", \"monitors\", \"show text\").\noa_rel(\"is used for\", \"theater\", \"watching movie\").\noa_rel(\"is used for\", \"balcony\", \"viewing, resting, or eating at\").\noa_rel(\"is used for\", \"spatula\", \"turning food\").\noa_rel(\"is used for\", \"candle\", \"creating ambience\").\noa_rel(\"can\", \"car\", \"carry few persons\").\noa_rel(\"can\", \"rat\", \"eat wires\").", "oa_rel(\"can\", \"car\", \"carry few persons\").\noa_rel(\"can\", \"rat\", \"eat wires\").\noa_rel(\"usually appears in\", \"office chair\", \"office\").\noa_rel(\"can\", \"cup\", \"hold liquids\").\noa_rel(\"is used for\", \"restaurant\", \"drinking\").\noa_rel(\"is used for\", \"restaurant\", \"meeting people\").\noa_rel(\"is made from\", \"paper\", \"wood\").\noa_rel(\"is used for\", \"shirt\", \"covering upperbody\").\noa_rel(\"can\", \"printer\", \"print pictures\").\noa_rel(\"can\", \"minivan\", \"travel on road\").", "oa_rel(\"can\", \"printer\", \"print pictures\").\noa_rel(\"can\", \"minivan\", \"travel on road\").\noa_rel(\"is used for\", \"fountain\", \"decoration\").\noa_rel(\"can\", \"poodle\", \"live in house\").\noa_rel(\"usually appears in\", \"pot\", \"kitchen\").\noa_rel(\"is used for\", \"kitchen\", \"storing food\").\noa_rel(\"is used for\", \"sheet\", \"covering bed\").\noa_rel(\"is used for\", \"hospital\", \"delivering babies\").\noa_rel(\"is used for\", \"bed\", \"sitting on\").\noa_rel(\"is used for\", \"cabinet\", \"storing dishes\").", "oa_rel(\"is used for\", \"bed\", \"sitting on\").\noa_rel(\"is used for\", \"cabinet\", \"storing dishes\").\noa_rel(\"can\", \"bus\", \"transport people\").\noa_rel(\"can\", \"bottle\", \"hold water\").\noa_rel(\"can\", \"horse\", \"be tamed\").\noa_rel(\"can\", \"airplane\", \"arrive at airport\").\noa_rel(\"is used for\", \"truck\", \"carrying cargo\").\noa_rel(\"is used for\", \"airport\", \"waiting for airplane\").\noa_rel(\"is used for\", \"stop sign\", \"controlling traffic\").\noa_rel(\"is used for\", \"boat\", \"transportation at sea\").", "oa_rel(\"is used for\", \"stop sign\", \"controlling traffic\").\noa_rel(\"is used for\", \"boat\", \"transportation at sea\").\noa_rel(\"can\", \"dog\", \"guard house\").\noa_rel(\"requires\", \"washing dishes\", \"faucet\").\noa_rel(\"is used for\", \"straw\", \"drinking beverage\").\noa_rel(\"is made from\", \"donut\", \"flour\").\noa_rel(\"is made from\", \"pastry\", \"flour\").\noa_rel(\"usually appears in\", \"glass\", \"restaurant\").\noa_rel(\"is used for\", \"phone\", \"sending email\").\noa_rel(\"can\", \"truck\", \"travel on road\").", "oa_rel(\"is used for\", \"phone\", \"sending email\").\noa_rel(\"can\", \"truck\", \"travel on road\").\noa_rel(\"can\", \"water\", \"act as reflector\").\noa_rel(\"is used for\", \"raft\", \"keeping people out of water\").\noa_rel(\"is used for\", \"raft\", \"traveling on water\").\noa_rel(\"can\", \"dog\", \"detect odors better than humans can\").\noa_rel(\"can\", \"bird\", \"live in house\").\noa_rel(\"usually appears in\", \"customer\", \"bar\").\noa_rel(\"is\", \"banana\", \"sweet\").\noa_rel(\"is used for\", \"sofa\", \"lying down\").", "oa_rel(\"is\", \"banana\", \"sweet\").\noa_rel(\"is used for\", \"sofa\", \"lying down\").\noa_rel(\"is\", \"oven\", \"electric\").\noa_rel(\"requires\", \"making pizza\", \"oven\").\noa_rel(\"is used for\", \"lamp\", \"illuminating area\").\noa_rel(\"is used for\", \"vase\", \"holding flowers\").\noa_rel(\"can be\", \"cake\", \"cut\").\noa_rel(\"can\", \"bus\", \"run\").\noa_rel(\"is used for\", \"bus stop\", \"waiting for bus\").\noa_rel(\"is used for\", \"street light\", \"illuminating area\").", "oa_rel(\"is used for\", \"bus stop\", \"waiting for bus\").\noa_rel(\"is used for\", \"street light\", \"illuminating area\").\noa_rel(\"can\", \"traffic light\", \"stop cars\").\noa_rel(\"can\", \"horse\", \"carry people\").\noa_rel(\"is used for\", \"motorcycle\", \"riding\").\noa_rel(\"can\", \"ship\", \"carry cargo\").\noa_rel(\"is used for\", \"ship\", \"keeping people out of water\").\noa_rel(\"is used for\", \"boat\", \"traveling on water\").\noa_rel(\"is made from\", \"chips\", \"potato\").\noa_rel(\"requires\", \"measuring up\", \"measuring cup\").", "oa_rel(\"is made from\", \"chips\", \"potato\").\noa_rel(\"requires\", \"measuring up\", \"measuring cup\").\noa_rel(\"usually appears in\", \"dish soap\", \"bathroom\").\noa_rel(\"is a sub-event of\", \"cleaning clothing\", \"operating washing machine\").\noa_rel(\"can\", \"horse\", \"pull buggy to picnic\").\noa_rel(\"can\", \"cart\", \"travel on road\").\noa_rel(\"is used for\", \"street\", \"driving car on\").\noa_rel(\"can\", \"bus\", \"drive down street\").\noa_rel(\"is used for\", \"shelf\", \"holding books\").\noa_rel(\"is used for\", \"restaurant\", \"selling food\").", "oa_rel(\"is used for\", \"shelf\", \"holding books\").\noa_rel(\"is used for\", \"restaurant\", \"selling food\").\noa_rel(\"can\", \"refrigerator\", \"stock food\").\noa_rel(\"is used for\", \"refrigerator\", \"freezing food\").\noa_rel(\"is used for\", \"couch\", \"sitting on\").\noa_rel(\"usually appears in\", \"sofa\", \"living room\").\noa_rel(\"can\", \"dog\", \"live in house\").\noa_rel(\"usually appears in\", \"wine glass\", \"bar\").\noa_rel(\"has\", \"corn\", \"vitamin B\").\noa_rel(\"can\", \"airplane\", \"go fast\").", "oa_rel(\"has\", \"corn\", \"vitamin B\").\noa_rel(\"can\", \"airplane\", \"go fast\").\noa_rel(\"is used for\", \"wine\", \"getting drunk\").\noa_rel(\"is used for\", \"sink\", \"washing hands\").\noa_rel(\"is used for\", \"horse\", \"transporting people\").\noa_rel(\"can\", \"horse\", \"jump higher than people\").\noa_rel(\"usually appears in\", \"plate\", \"kitchen\").\noa_rel(\"usually appears in\", \"lamp\", \"bedroom\").\noa_rel(\"is used for\", \"bed\", \"napping on\").\noa_rel(\"is used for\", \"sheet\", \"covering mattress\").", "oa_rel(\"is used for\", \"bed\", \"napping on\").\noa_rel(\"is used for\", \"sheet\", \"covering mattress\").\noa_rel(\"is used for\", \"fan\", \"cooling air\").\noa_rel(\"can\", \"refrigerator\", \"keep food cold\").\noa_rel(\"is used for\", \"blanket\", \"keeping warm at night\").\noa_rel(\"is used for\", \"door\", \"controlling access\").\noa_rel(\"usually appears in\", \"notebook\", \"office\").\noa_rel(\"is used for\", \"kitchen\", \"preparing food\").\noa_rel(\"requires\", \"cleaning house\", \"pail\").\noa_rel(\"can\", \"pitcher\", \"throw fast ball\").", "oa_rel(\"requires\", \"cleaning house\", \"pail\").\noa_rel(\"can\", \"pitcher\", \"throw fast ball\").\noa_rel(\"is\", \"radiator\", \"electric\").\noa_rel(\"usually appears in\", \"christmas tree\", \"christmas\").\noa_rel(\"has\", \"wine\", \"alcohol\").\noa_rel(\"is used for\", \"sidewalk\", \"riding skateboards\").\noa_rel(\"is used for\", \"bottle\", \"holding drinks\").\noa_rel(\"is used for\", \"luggage\", \"storing clothes for trip\").\noa_rel(\"is used for\", \"sunglasses\", \"protecting eyes from sunlight\").\noa_rel(\"is used for\", \"sidewalk\", \"walking on\").", "oa_rel(\"is used for\", \"sunglasses\", \"protecting eyes from sunlight\").\noa_rel(\"is used for\", \"sidewalk\", \"walking on\").\noa_rel(\"is used for\", \"clock\", \"knowing time\").\noa_rel(\"is used for\", \"bridge\", \"crossing river\").\noa_rel(\"is used for\", \"river\", \"canoeing\").\noa_rel(\"is used for\", \"curtain\", \"decorating room\").\noa_rel(\"is used for\", \"bathroom\", \"brushing teeth\").\noa_rel(\"can\", \"fire truck\", \"travel on road\").\noa_rel(\"is used for\", \"market\", \"buying and selling\").\noa_rel(\"is used for\", \"shelf\", \"storing items\").", "oa_rel(\"is used for\", \"market\", \"buying and selling\").\noa_rel(\"is used for\", \"shelf\", \"storing items\").\noa_rel(\"can\", \"boat\", \"travel over water\").\noa_rel(\"is\", \"printer\", \"electric\").\noa_rel(\"is used for\", \"teddy bear\", \"cuddling\").\noa_rel(\"can\", \"umbrella\", \"fold up\").\noa_rel(\"is used for\", \"train\", \"transporting people\").\noa_rel(\"can\", \"jeep\", \"travel on road\").\noa_rel(\"is used for\", \"toy\", \"entertainment\").\noa_rel(\"can\", \"batter\", \"strike out\").", "oa_rel(\"is used for\", \"toy\", \"entertainment\").\noa_rel(\"can\", \"batter\", \"strike out\").\noa_rel(\"is used for\", \"helmet\", \"protecting head\").\noa_rel(\"can\", \"horse\", \"be pet\").\noa_rel(\"is used for\", \"motorcycle\", \"transporting handful of people\").\noa_rel(\"is used for\", \"baseball bat\", \"hitting something\").\noa_rel(\"is used for\", \"house\", \"dwelling\").\noa_rel(\"can\", \"train\", \"arrive at station\").\noa_rel(\"can\", \"bus\", \"transport many people at once\").\noa_rel(\"is used for\", \"crane\", \"transporting people\").", "oa_rel(\"can\", \"bus\", \"transport many people at once\").\noa_rel(\"is used for\", \"crane\", \"transporting people\").\noa_rel(\"can\", \"taxi\", \"travel on road\").\noa_rel(\"is used for\", \"cabinet\", \"storing pills\").\noa_rel(\"usually appears in\", \"bucket\", \"bathroom\").\noa_rel(\"is\", \"xbox controller\", \"electric\").\noa_rel(\"is used for\", \"goggles\", \"preventing particulates from striking eyes\").\noa_rel(\"can\", \"helmet\", \"protect head from impact\").\noa_rel(\"is used for\", \"watch\", \"knowing time\").\noa_rel(\"usually appears in\", \"calculator\", \"office\").", "oa_rel(\"is used for\", \"watch\", \"knowing time\").\noa_rel(\"usually appears in\", \"calculator\", \"office\").\noa_rel(\"can be\", \"window\", \"opened or closed\").\noa_rel(\"is\", \"light bulb\", \"electric\").\noa_rel(\"is used for\", \"snow\", \"skiing on\").\noa_rel(\"usually appears in\", \"napkin\", \"dining room\").\noa_rel(\"usually appears in\", \"dishes\", \"restaurant\").\noa_rel(\"can\", \"dog\", \"guard building\").\noa_rel(\"is used for\", \"steering wheel\", \"controlling direction car turns\").\noa_rel(\"can\", \"van\", \"travel on road\").", "oa_rel(\"is used for\", \"steering wheel\", \"controlling direction car turns\").\noa_rel(\"can\", \"van\", \"travel on road\").\noa_rel(\"can\", \"airplane\", \"transport many people at once\").\noa_rel(\"usually appears in\", \"fireplace\", \"living room\").\noa_rel(\"is\", \"wii controller\", \"electric\").\noa_rel(\"is used for\", \"guitar\", \"playing music\").\noa_rel(\"is\", \"speaker\", \"electric\").\noa_rel(\"is used for\", \"boat\", \"fishing\").\noa_rel(\"can\", \"steering wheel\", \"control direction of car\").\noa_rel(\"is used for\", \"car\", \"transportation\").", "oa_rel(\"can\", \"steering wheel\", \"control direction of car\").\noa_rel(\"is used for\", \"car\", \"transportation\").\noa_rel(\"is used for\", \"fork\", \"moving food to mouth\").\noa_rel(\"usually appears in\", \"water glass\", \"bar\").\noa_rel(\"is used for\", \"bench\", \"resting\").\noa_rel(\"can\", \"boat\", \"swim on water\").\noa_rel(\"usually appears in\", \"menu\", \"restaurant\").\noa_rel(\"can\", \"bull\", \"charge matador\").\noa_rel(\"is used for\", \"speaker\", \"listening to music\").\noa_rel(\"can\", \"hammers\", \"nail wood\").", "oa_rel(\"is used for\", \"speaker\", \"listening to music\").\noa_rel(\"can\", \"hammers\", \"nail wood\").\noa_rel(\"can\", \"monkeys\", \"use tool\").\noa_rel(\"is used for\", \"pot\", \"holding water\").\noa_rel(\"is\", \"elephant\", \"herbivorous\").\noa_rel(\"is used for\", \"vehicle\", \"transportation\").\noa_rel(\"is used for\", \"bicycle\", \"riding\").\noa_rel(\"can\", \"truck\", \"carry cargo\").\noa_rel(\"can\", \"stethoscopes\", \"listen to heart\").\noa_rel(\"is used for\", \"suitcase\", \"packing clothes for trip\").", "oa_rel(\"can\", \"stethoscopes\", \"listen to heart\").\noa_rel(\"is used for\", \"suitcase\", \"packing clothes for trip\").\noa_rel(\"is\", \"fruit\", \"healthy\").\noa_rel(\"has\", \"grapefruit\", \"vitamin C\").\noa_rel(\"can\", \"airplane\", \"cross ocean\").\noa_rel(\"is used for\", \"runway\", \"aircraft takeoff\").\noa_rel(\"is used for\", \"shower\", \"cleaning body\").\noa_rel(\"is used for\", \"bathroom\", \"taking bath\").\noa_rel(\"is used for\", \"sink\", \"cleaning dishes\").\noa_rel(\"is used for\", \"menu\", \"ordering food\").", "oa_rel(\"is used for\", \"sink\", \"cleaning dishes\").\noa_rel(\"is used for\", \"menu\", \"ordering food\").\noa_rel(\"can\", \"kite\", \"fly\").\noa_rel(\"can\", \"truck\", \"move heavy loads\").\noa_rel(\"is used for\", \"blanket\", \"covering things\").\noa_rel(\"is used for\", \"church\", \"weddings\").\noa_rel(\"is used for\", \"court\", \"playing tennis\").\noa_rel(\"is used for\", \"bus\", \"transporting lots of people\").\noa_rel(\"can\", \"lamp\", \"illuminate\").\noa_rel(\"is used for\", \"computer\", \"finding information\").", "oa_rel(\"can\", \"lamp\", \"illuminate\").\noa_rel(\"is used for\", \"computer\", \"finding information\").\noa_rel(\"usually appears in\", \"salt shaker\", \"dining room\").\noa_rel(\"is used for\", \"rug\", \"standing on\").\noa_rel(\"is used for\", \"camera\", \"taking pictures\").\noa_rel(\"usually appears in\", \"candle\", \"restaurant\").\noa_rel(\"is used for\", \"door\", \"separating rooms\").\noa_rel(\"can\", \"knife\", \"cut that cake\").\noa_rel(\"usually appears in\", \"spoon\", \"dining room\").\noa_rel(\"is used for\", \"bowl\", \"holding apples\").", "oa_rel(\"usually appears in\", \"spoon\", \"dining room\").\noa_rel(\"is used for\", \"bowl\", \"holding apples\").\noa_rel(\"can\", \"knives\", \"cut food\").\noa_rel(\"can\", \"oven\", \"roast\").\noa_rel(\"is a sub-event of\", \"baking cake\", \"preheating oven\").\noa_rel(\"is used for\", \"pot\", \"holding liquid\").\noa_rel(\"is used for\", \"soap\", \"cleaning somethings\").\noa_rel(\"requires\", \"cleaning clothing\", \"soap\").\noa_rel(\"has\", \"avocado\", \"vitamin B\").\noa_rel(\"can be\", \"vegetable\", \"eaten\").", "oa_rel(\"has\", \"avocado\", \"vitamin B\").\noa_rel(\"can be\", \"vegetable\", \"eaten\").\noa_rel(\"is\", \"mouse\", \"electric\").\noa_rel(\"is used for\", \"desk\", \"writing upon\").\noa_rel(\"is used for\", \"keyboard\", \"typing\").\noa_rel(\"usually appears in\", \"toothbrush\", \"bathroom\").\noa_rel(\"usually appears in\", \"bathtub\", \"bathroom\").\noa_rel(\"is used for\", \"luggage\", \"carrying clothes on trip\").\noa_rel(\"is used for\", \"microwave\", \"cooking food fast\").\noa_rel(\"can\", \"oven\", \"warm meal\").", "oa_rel(\"is used for\", \"microwave\", \"cooking food fast\").\noa_rel(\"can\", \"oven\", \"warm meal\").\noa_rel(\"requires\", \"roasting\", \"oven\").\noa_rel(\"can be\", \"shirt\", \"hung\").\noa_rel(\"is used for\", \"home\", \"dwelling\").\noa_rel(\"is used for\", \"fence\", \"marking property lines\").\noa_rel(\"can\", \"baseball\", \"travel very fast\").\noa_rel(\"is used for\", \"platform\", \"standing on\").\noa_rel(\"is\", \"wine\", \"fluid\").\noa_rel(\"is used for\", \"glasses\", \"correcting vision\").", "oa_rel(\"is\", \"wine\", \"fluid\").\noa_rel(\"is used for\", \"glasses\", \"correcting vision\").\noa_rel(\"can\", \"bench\", \"seat people\").\noa_rel(\"usually appears in\", \"toilet brush\", \"bathroom\").\noa_rel(\"is used for\", \"bathroom\", \"peeing\").\noa_rel(\"requires\", \"washing dishes\", \"sink\").\noa_rel(\"is used for\", \"refrigerator\", \"keeping food cold\").\noa_rel(\"is\", \"stove\", \"electric\").\noa_rel(\"usually appears in\", \"veil\", \"wedding\").\noa_rel(\"is\", \"cigarette\", \"harmful\").", "oa_rel(\"usually appears in\", \"veil\", \"wedding\").\noa_rel(\"is\", \"cigarette\", \"harmful\").\noa_rel(\"is used for\", \"beverage\", \"drinking\").\noa_rel(\"usually appears in\", \"soup\", \"dinner\").\noa_rel(\"is used for\", \"bowl\", \"holding soup\").\noa_rel(\"is used for\", \"airplane\", \"traversing skies\").\noa_rel(\"is used for\", \"cup\", \"holding drinks\").\noa_rel(\"has\", \"bread\", \"starch\").\noa_rel(\"is used for\", \"phone\", \"surfing internet\").\noa_rel(\"can\", \"truck\", \"ship goods\").", "oa_rel(\"is used for\", \"phone\", \"surfing internet\").\noa_rel(\"can\", \"truck\", \"ship goods\").\noa_rel(\"is used for\", \"gas station\", \"buying gas\").\noa_rel(\"can\", \"sedan\", \"travel on road\").\noa_rel(\"is used for\", \"baseball field\", \"playing baseball\").\noa_rel(\"is used for\", \"hospital\", \"healing sick people\").\noa_rel(\"can\", \"heater\", \"heat room\").\noa_rel(\"usually appears in\", \"drink\", \"bar\").\noa_rel(\"usually appears in\", \"folder\", \"office\").\noa_rel(\"is\", \"ocean\", \"liquid\").", "oa_rel(\"usually appears in\", \"folder\", \"office\").\noa_rel(\"is\", \"ocean\", \"liquid\").\noa_rel(\"is used for\", \"meat\", \"eating\").\noa_rel(\"is used for\", \"toilet\", \"depositing human waste\").\noa_rel(\"is used for\", \"blanket\", \"covering bed\").\noa_rel(\"can\", \"water\", \"dribble\").\noa_rel(\"is used for\", \"bicycle\", \"transporting handful of people\").\noa_rel(\"is used for\", \"belt\", \"holding pants\").\noa_rel(\"can\", \"donkey\", \"carry load of supplies\").\noa_rel(\"usually appears in\", \"closet\", \"bedroom\").", "oa_rel(\"can\", \"donkey\", \"carry load of supplies\").\noa_rel(\"usually appears in\", \"closet\", \"bedroom\").\noa_rel(\"is used for\", \"couch\", \"sleeping\").\noa_rel(\"is\", \"radio\", \"electric\").\noa_rel(\"can\", \"computer\", \"cost lot of money\").\noa_rel(\"is used for\", \"bed\", \"resting\").\noa_rel(\"can be\", \"umbrella\", \"opened or closed\").\noa_rel(\"is used for\", \"dumpster\", \"storing trash\").\noa_rel(\"is used for\", \"frisbee\", \"exercise\").\noa_rel(\"can\", \"boat\", \"sail on pond\").", "oa_rel(\"is used for\", \"frisbee\", \"exercise\").\noa_rel(\"can\", \"boat\", \"sail on pond\").\noa_rel(\"requires\", \"baking bread\", \"oven\").\noa_rel(\"is\", \"ice cream\", \"sticky\").\noa_rel(\"can\", \"cup\", \"hold coffee\").\noa_rel(\"has\", \"coffee\", \"water\").\noa_rel(\"can\", \"dog\", \"shake hands\").\noa_rel(\"is used for\", \"mall\", \"gathering shops\").\noa_rel(\"is used for\", \"canoe\", \"traveling on water\").\noa_rel(\"is used for\", \"canoe\", \"transporting people\").", "oa_rel(\"is used for\", \"canoe\", \"traveling on water\").\noa_rel(\"is used for\", \"canoe\", \"transporting people\").\noa_rel(\"is used for\", \"ship\", \"transporting people\").\noa_rel(\"usually appears in\", \"cup\", \"restaurant\").\noa_rel(\"usually appears in\", \"mug\", \"dining room\").\noa_rel(\"is\", \"bear\", \"carnivorous\").\noa_rel(\"is used for\", \"couch\", \"taking nap\").\noa_rel(\"is used for\", \"beverage\", \"satisfying thirst\").\noa_rel(\"is used for\", \"airplane\", \"transporting people\").\noa_rel(\"can\", \"airplane\", \"seat passengers\").", "oa_rel(\"is used for\", \"airplane\", \"transporting people\").\noa_rel(\"can\", \"airplane\", \"seat passengers\").\noa_rel(\"usually appears in\", \"chair\", \"restaurant\").\noa_rel(\"has\", \"broccoli\", \"vitamin B\").\noa_rel(\"is\", \"ketchup\", \"fluid\").\noa_rel(\"usually appears in\", \"elephant\", \"grassland\").\noa_rel(\"is used for\", \"living room\", \"entertaining guests\").\noa_rel(\"is used for\", \"sofa\", \"resting\").\noa_rel(\"is used for\", \"ship\", \"transporting goods\").\noa_rel(\"can\", \"ruler\", \"measure distance\").", "oa_rel(\"is used for\", \"ship\", \"transporting goods\").\noa_rel(\"can\", \"ruler\", \"measure distance\").\noa_rel(\"can\", \"docks\", \"shore boats\").\noa_rel(\"is used for\", \"calculator\", \"doing mathematical calculations\").\noa_rel(\"is made from\", \"ice cream\", \"milk\").\noa_rel(\"is used for\", \"shelf\", \"storing dishes\").\noa_rel(\"has\", \"juice\", \"water\").\noa_rel(\"has\", \"milk\", \"vitamin B\").\noa_rel(\"is\", \"ketchup\", \"sticky\").\noa_rel(\"can\", \"hammer\", \"hit nail\").", "oa_rel(\"is\", \"ketchup\", \"sticky\").\noa_rel(\"can\", \"hammer\", \"hit nail\").\noa_rel(\"can\", \"axe\", \"chop wood\").\noa_rel(\"is used for\", \"scissors\", \"cutting ribbons\").\noa_rel(\"usually appears in\", \"grapes\", \"salad\").\noa_rel(\"is used for\", \"traffic light\", \"controlling flows of traffic\").\noa_rel(\"can\", \"washing machine\", \"clean clothes\").\noa_rel(\"is used for\", \"desk\", \"working\").\noa_rel(\"requires\", \"barbecue\", \"grill\").\noa_rel(\"is used for\", \"blender\", \"mixing food\").", "oa_rel(\"requires\", \"barbecue\", \"grill\").\noa_rel(\"is used for\", \"blender\", \"mixing food\").\noa_rel(\"is used for\", \"highway\", \"driving car on\").\noa_rel(\"is used for\", \"frisbee\", \"catching\").\noa_rel(\"can\", \"rain\", \"cause floods\").\noa_rel(\"usually appears in\", \"candle\", \"birthday party\").\noa_rel(\"is used for\", \"sailboat\", \"floating and moving on water\").\noa_rel(\"is used for\", \"pot\", \"boiling water\").\noa_rel(\"is used for\", \"pot\", \"planting plant\").\noa_rel(\"is used for\", \"pen\", \"writing\").", "oa_rel(\"is used for\", \"pot\", \"planting plant\").\noa_rel(\"is used for\", \"pen\", \"writing\").\noa_rel(\"is used for\", \"fruit\", \"eating\").\noa_rel(\"can\", \"steering wheel\", \"control car\").\noa_rel(\"is used for\", \"airplane\", \"transporting goods\").\noa_rel(\"is used for\", \"bathroom\", \"washing hands\").\noa_rel(\"usually appears in\", \"tray\", \"dining room\").\noa_rel(\"can\", \"pitcher\", \"strike out baseball hitter\").\noa_rel(\"is used for\", \"earring\", \"decoration\").\noa_rel(\"is used for\", \"bathtub\", \"cleaning body\").", "oa_rel(\"is used for\", \"earring\", \"decoration\").\noa_rel(\"is used for\", \"bathtub\", \"cleaning body\").\noa_rel(\"is used for\", \"truck\", \"transporting goods\").\noa_rel(\"is used for\", \"doll\", \"playing with child\").\noa_rel(\"is used for\", \"binder\", \"binding papers together\").\noa_rel(\"usually appears in\", \"cup\", \"dining room\").\noa_rel(\"is used for\", \"engine\", \"powering\").\noa_rel(\"is used for\", \"home\", \"housing family\").\noa_rel(\"is used for\", \"book\", \"getting knowledge\").\noa_rel(\"can\", \"cat\", \"live in house\").", "oa_rel(\"is used for\", \"book\", \"getting knowledge\").\noa_rel(\"can\", \"cat\", \"live in house\").\noa_rel(\"usually appears in\", \"tissue box\", \"bathroom\").\noa_rel(\"is used for\", \"bowl\", \"holding cream\").\noa_rel(\"is\", \"tea\", \"liquid\").\noa_rel(\"is used for\", \"bottle\", \"storing liquids\").\noa_rel(\"can\", \"bird\", \"sing\").\noa_rel(\"is used for\", \"fence\", \"creating privacy\").\noa_rel(\"is\", \"apple\", \"sweet\").\noa_rel(\"can\", \"knife\", \"cut apple\").", "oa_rel(\"is\", \"apple\", \"sweet\").\noa_rel(\"can\", \"knife\", \"cut apple\").\noa_rel(\"is used for\", \"seat\", \"waiting\").\noa_rel(\"is used for\", \"fruit\", \"making juice\").\noa_rel(\"can\", \"fish\", \"swim\").\noa_rel(\"can\", \"microwave\", \"heat food\").\noa_rel(\"is used for\", \"cup\", \"handling liquid\").\noa_rel(\"is used for\", \"blanket\", \"keeping warm when sleeping\").\noa_rel(\"is used for\", \"carpet\", \"covering floor\").\noa_rel(\"is used for\", \"market\", \"selling foodstuffs\").", "oa_rel(\"is used for\", \"carpet\", \"covering floor\").\noa_rel(\"is used for\", \"market\", \"selling foodstuffs\").\noa_rel(\"is used for\", \"wii\", \"playing video games\").\noa_rel(\"can\", \"knife\", \"hurt you\").\noa_rel(\"requires\", \"dicing\", \"knife\").\noa_rel(\"can\", \"corn\", \"provide complex carbohydrates\").\noa_rel(\"can\", \"truck\", \"use diesel fuel\").\noa_rel(\"is used for\", \"stove\", \"heating food\").\noa_rel(\"can\", \"boat\", \"take you to island\").\noa_rel(\"can\", \"train\", \"carry freight\").", "oa_rel(\"can\", \"boat\", \"take you to island\").\noa_rel(\"can\", \"train\", \"carry freight\").\noa_rel(\"is used for\", \"cake\", \"celebrating someones special event\").\noa_rel(\"is used for\", \"bell\", \"getting people's attention\").\noa_rel(\"can\", \"noodle\", \"provide complex carbohydrates\").\noa_rel(\"is used for\", \"train\", \"transporting lots of people\").\noa_rel(\"is used for\", \"refrigerator\", \"chilling food\").\noa_rel(\"requires\", \"washing dishes\", \"dishwasher\").\noa_rel(\"is used for\", \"menu\", \"listing choices\").\noa_rel(\"is used for\", \"phone\", \"talking to someone\").", "oa_rel(\"is used for\", \"menu\", \"listing choices\").\noa_rel(\"is used for\", \"phone\", \"talking to someone\").\noa_rel(\"is used for\", \"doll\", \"having fun\").\noa_rel(\"is used for\", \"shelf\", \"storing books\").\noa_rel(\"can\", \"toilet\", \"flush\").\noa_rel(\"can\", \"stove\", \"heat pot\").\noa_rel(\"usually appears in\", \"bowl\", \"kitchen\").\noa_rel(\"is used for\", \"kitchen\", \"eating food\").\noa_rel(\"usually appears in\", \"sink\", \"kitchen\").\noa_rel(\"is used for\", \"kettle\", \"boiling water\").", "oa_rel(\"usually appears in\", \"sink\", \"kitchen\").\noa_rel(\"is used for\", \"kettle\", \"boiling water\").\noa_rel(\"is used for\", \"sink\", \"soaking dishes\").\noa_rel(\"is used for\", \"soap\", \"washing dishes\").\noa_rel(\"usually appears in\", \"rag\", \"living room\").\noa_rel(\"is used for\", \"sink\", \"washing dishes\").\noa_rel(\"usually appears in\", \"blender\", \"kitchen\").\noa_rel(\"requires\", \"blending\", \"blender\").\noa_rel(\"usually appears in\", \"plate\", \"dining room\").\noa_rel(\"requires\", \"toasting\", \"toaster\").", "oa_rel(\"usually appears in\", \"plate\", \"dining room\").\noa_rel(\"requires\", \"toasting\", \"toaster\").\noa_rel(\"can\", \"refrigerator\", \"store food for long times\").\noa_rel(\"usually appears in\", \"oven\", \"kitchen\").\noa_rel(\"requires\", \"baking\", \"oven\").\noa_rel(\"usually appears in\", \"dishwasher\", \"kitchen\").\noa_rel(\"is used for\", \"stove\", \"cooking stew\").\noa_rel(\"requires\", \"making coffee\", \"coffee maker\").\noa_rel(\"usually appears in\", \"towel rack\", \"bathroom\").\noa_rel(\"is used for\", \"stove\", \"grilling steak\").", "oa_rel(\"usually appears in\", \"towel rack\", \"bathroom\").\noa_rel(\"is used for\", \"stove\", \"grilling steak\").\noa_rel(\"can\", \"oven\", \"warm pie\").\noa_rel(\"requires\", \"baking cake\", \"oven\").\noa_rel(\"can\", \"stove\", \"heat pot of water\").\noa_rel(\"requires\", \"boiling\", \"cooking pot\").\noa_rel(\"usually appears in\", \"stove\", \"kitchen\").\noa_rel(\"usually appears in\", \"placemat\", \"dining room\").\noa_rel(\"usually appears in\", \"microwave\", \"kitchen\").\noa_rel(\"is used for\", \"mug\", \"holding drinks\").", "oa_rel(\"usually appears in\", \"microwave\", \"kitchen\").\noa_rel(\"is used for\", \"mug\", \"holding drinks\").\noa_rel(\"usually appears in\", \"pan\", \"kitchen\").\noa_rel(\"usually appears in\", \"towel\", \"bathroom\").\noa_rel(\"is\", \"dryer\", \"electric\").\noa_rel(\"is used for\", \"living room\", \"having party\").\noa_rel(\"is a sub-event of\", \"cleaning clothing\", \"putting clothing in washer\").\noa_rel(\"can\", \"coffee maker\", \"making coffee\").\noa_rel(\"can\", \"microwave\", \"warm up coffee\").\noa_rel(\"is used for\", \"camera\", \"photography\").", "oa_rel(\"can\", \"microwave\", \"warm up coffee\").\noa_rel(\"is used for\", \"camera\", \"photography\").\noa_rel(\"requires\", \"mincing\", \"knife\").\noa_rel(\"can\", \"bridge\", \"cross river\").\noa_rel(\"usually appears in\", \"brush\", \"bathroom\").\noa_rel(\"is used for\", \"river\", \"transportation\").\noa_rel(\"can\", \"bird\", \"land on branch\").\noa_rel(\"usually appears in\", \"coffee cup\", \"restaurant\").\noa_rel(\"is\", \"coffee maker\", \"electric\").\noa_rel(\"is\", \"dishwasher\", \"electric\").", "oa_rel(\"is\", \"coffee maker\", \"electric\").\noa_rel(\"is\", \"dishwasher\", \"electric\").\noa_rel(\"is used for\", \"closet\", \"storing clothes\").\noa_rel(\"can\", \"water\", \"be liquid, ice, or steam\").\noa_rel(\"is used for\", \"temple\", \"praying\").\noa_rel(\"is used for\", \"carpet\", \"covering ugly floor\").\noa_rel(\"is a sub-event of\", \"cleaning house\", \"vacuuming carpet\").\noa_rel(\"can\", \"airplane\", \"land in field\").\noa_rel(\"is used for\", \"closet\", \"hanging clothes\").\noa_rel(\"can\", \"dog\", \"guide blind\").", "oa_rel(\"is used for\", \"closet\", \"hanging clothes\").\noa_rel(\"can\", \"dog\", \"guide blind\").\noa_rel(\"can\", \"umbrella\", \"shade you from sun\").\noa_rel(\"is used for\", \"pot\", \"making soup\").\noa_rel(\"is used for\", \"sidewalk\", \"riding bike on\").\noa_rel(\"is\", \"fax machine\", \"electric\").\noa_rel(\"is used for\", \"keyboard\", \"coding\").\noa_rel(\"is\", \"sheep\", \"herbivorous\").\noa_rel(\"is used for\", \"pen\", \"drawing\").\noa_rel(\"usually appears in\", \"mouse\", \"office\").", "oa_rel(\"is used for\", \"pen\", \"drawing\").\noa_rel(\"usually appears in\", \"mouse\", \"office\").\noa_rel(\"is used for\", \"dining room\", \"eating\").\noa_rel(\"usually appears in\", \"bowl\", \"restaurant\").\noa_rel(\"has\", \"orange\", \"vitamin C\").\noa_rel(\"is used for\", \"sheet\", \"sleeping on\").\noa_rel(\"is used for\", \"fork\", \"eating pie\").\noa_rel(\"requires\", \"making pizza\", \"salt\").\noa_rel(\"is\", \"knife\", \"dangerous\").\noa_rel(\"is used for\", \"knife\", \"cutting food\").", "oa_rel(\"is\", \"knife\", \"dangerous\").\noa_rel(\"is used for\", \"knife\", \"cutting food\").\noa_rel(\"is used for\", \"fork\", \"lifting food from plate to mouth\").\noa_rel(\"usually appears in\", \"saucer\", \"dining room\").\noa_rel(\"is\", \"cream\", \"fluid\").\noa_rel(\"requires\", \"beating egg\", \"spoon\").\noa_rel(\"usually appears in\", \"straw\", \"restaurant\").\noa_rel(\"is used for\", \"salt\", \"salting food\").\noa_rel(\"usually appears in\", \"placemat\", \"restaurant\").\noa_rel(\"is made from\", \"pasta\", \"flour\").", "oa_rel(\"usually appears in\", \"placemat\", \"restaurant\").\noa_rel(\"is made from\", \"pasta\", \"flour\").\noa_rel(\"requires\", \"baking cake\", \"egg\").\noa_rel(\"is\", \"salt\", \"salty\").\noa_rel(\"has\", \"strawberry\", \"vitamin C\").\noa_rel(\"is made from\", \"butter\", \"milk\").\noa_rel(\"is a sub-event of\", \"baking cake\", \"mixing butter with sugar\").\noa_rel(\"is\", \"oatmeal\", \"fluid\").\noa_rel(\"is made from\", \"juice\", \"fruit\").\noa_rel(\"is made from\", \"casserole\", \"flour\").", "oa_rel(\"is made from\", \"juice\", \"fruit\").\noa_rel(\"is made from\", \"casserole\", \"flour\").\noa_rel(\"requires\", \"cutting\", \"knife\").\noa_rel(\"has\", \"rice\", \"starch\").\noa_rel(\"requires\", \"chopping\", \"knife\").\noa_rel(\"usually appears in\", \"stool\", \"bar\").\noa_rel(\"is used for\", \"refrigerator\", \"making ice\").\noa_rel(\"can\", \"oven\", \"heat meal\").\noa_rel(\"is a sub-event of\", \"baking bread\", \"preheating oven\").\noa_rel(\"can\", \"boat\", \"sail through sea\").", "oa_rel(\"is a sub-event of\", \"baking bread\", \"preheating oven\").\noa_rel(\"can\", \"boat\", \"sail through sea\").\noa_rel(\"is used for\", \"scissors\", \"cutting string\").\noa_rel(\"requires\", \"mincing\", \"scissors\").\noa_rel(\"is used for\", \"living room\", \"watching tv\").\noa_rel(\"usually appears in\", \"couch\", \"living room\").\noa_rel(\"is used for\", \"stove\", \"frying burgers\").\noa_rel(\"is used for\", \"ballon\", \"decoration\").\noa_rel(\"is used for\", \"drum\", \"banging\").\noa_rel(\"is\", \"beer\", \"harmful\").", "oa_rel(\"is used for\", \"drum\", \"banging\").\noa_rel(\"is\", \"beer\", \"harmful\").\noa_rel(\"can\", \"bicycle\", \"travel on road\").\noa_rel(\"is used for\", \"computer\", \"doing mathematical calculations\").\noa_rel(\"usually appears in\", \"chair\", \"living room\").\noa_rel(\"is used for\", \"projector\", \"showing presentations\").\noa_rel(\"can\", \"computer\", \"save information\").\noa_rel(\"is used for\", \"lobby\", \"meeting guests\").\noa_rel(\"can\", \"fence\", \"divide property\").\noa_rel(\"is used for\", \"container\", \"holding something\").", "oa_rel(\"can\", \"fence\", \"divide property\").\noa_rel(\"is used for\", \"container\", \"holding something\").\noa_rel(\"is used for\", \"sailboat\", \"traveling on water\").\noa_rel(\"can\", \"sailboat\", \"travel over water\").\noa_rel(\"is used for\", \"mailbox\", \"sending letters\").\noa_rel(\"is used for\", \"factory\", \"manufacture goods\").\noa_rel(\"is used for\", \"broom\", \"sweeping floors\").\noa_rel(\"requires\", \"cleaning house\", \"broom\").\noa_rel(\"is used for\", \"scooter\", \"riding\").\noa_rel(\"can\", \"airplanes\", \"carry people\").", "oa_rel(\"is used for\", \"scooter\", \"riding\").\noa_rel(\"can\", \"airplanes\", \"carry people\").\noa_rel(\"can\", \"hammer\", \"strike nail\").\noa_rel(\"is used for\", \"van\", \"transporting people\").\noa_rel(\"can\", \"refrigerator\", \"cool milk\").\noa_rel(\"is used for\", \"bracelet\", \"decoration\").\noa_rel(\"can\", \"ruler\", \"guide lines\").\noa_rel(\"usually appears in\", \"dresser\", \"bedroom\").\noa_rel(\"can\", \"snake\", \"eat egg\").\noa_rel(\"requires\", \"beating egg\", \"mixer\").", "oa_rel(\"can\", \"snake\", \"eat egg\").\noa_rel(\"requires\", \"beating egg\", \"mixer\").\noa_rel(\"is used for\", \"spoon\", \"moving liquid food to mouth\").\noa_rel(\"is used for\", \"spoon\", \"moving food to mouth\").\noa_rel(\"is made from\", \"sandwich\", \"flour\").\noa_rel(\"is used for\", \"elephant\", \"transporting people\").\noa_rel(\"is used for\", \"house\", \"housing family\").\noa_rel(\"can\", \"rat\", \"live in house\").\noa_rel(\"is used for\", \"carpet\", \"protecting feet from floor\").\noa_rel(\"is used for\", \"church\", \"worship\").", "oa_rel(\"is used for\", \"carpet\", \"protecting feet from floor\").\noa_rel(\"is used for\", \"church\", \"worship\").\noa_rel(\"can\", \"elephant\", \"weight 1000 kilos\").\noa_rel(\"is used for\", \"fan\", \"cooling people\").\noa_rel(\"is used for\", \"curtain\", \"get privacy\").\noa_rel(\"is made from\", \"pizza slice\", \"flour\").\noa_rel(\"is used for\", \"stapler\", \"stapling papers together\").\noa_rel(\"usually appears in\", \"mug\", \"bar\").\noa_rel(\"is used for\", \"boat\", \"keeping people out of water\").\noa_rel(\"is made from\", \"chocolate\", \"cacao seeds\").", "oa_rel(\"is used for\", \"boat\", \"keeping people out of water\").\noa_rel(\"is made from\", \"chocolate\", \"cacao seeds\").\noa_rel(\"can\", \"squirrel\", \"store nuts for winter\").\noa_rel(\"is used for\", \"airplane\", \"carrying cargo\").\noa_rel(\"usually appears in\", \"tractor\", \"farm\").\noa_rel(\"can\", \"stove\", \"heat food\").\noa_rel(\"has\", \"banana\", \"vitamin B\").\noa_rel(\"is a sub-event of\", \"making juice\", \"cutting fruit\").\noa_rel(\"is used for\", \"oven\", \"preparing food\").\noa_rel(\"is used for\", \"museum\", \"displaying old objects\").", "oa_rel(\"is used for\", \"oven\", \"preparing food\").\noa_rel(\"is used for\", \"museum\", \"displaying old objects\").\noa_rel(\"is used for\", \"train station\", \"waiting train\").\noa_rel(\"can\", \"frisbee\", \"descend slowly by hovering\").\noa_rel(\"can\", \"cat\", \"eat meat\").\noa_rel(\"can\", \"batter\", \"hit baseball\").\noa_rel(\"is used for\", \"computer\", \"storing information\").\noa_rel(\"can\", \"camera\", \"record scene\").\noa_rel(\"usually appears in\", \"cattle\", \"farm\").\noa_rel(\"can\", \"hammer\", \"nail board\").", "oa_rel(\"usually appears in\", \"cattle\", \"farm\").\noa_rel(\"can\", \"hammer\", \"nail board\").\noa_rel(\"can\", \"bird\", \"spread wings\").\noa_rel(\"is\", \"keyboard\", \"electric\").\noa_rel(\"is used for\", \"stapler\", \"holding papers together\").\noa_rel(\"can\", \"computer\", \"power down\").\noa_rel(\"is used for\", \"thermometer\", \"measuring temperature\").\noa_rel(\"is used for\", \"runway\", \"landing airplanes\").\noa_rel(\"has\", \"zebra\", \"stripes\").\noa_rel(\"can\", \"baseball bat\", \"hit baseball\").", "oa_rel(\"has\", \"zebra\", \"stripes\").\noa_rel(\"can\", \"baseball bat\", \"hit baseball\").\noa_rel(\"usually appears in\", \"hair dryer\", \"bathroom\").\noa_rel(\"can\", \"ambulance\", \"travel on road\").\noa_rel(\"is used for\", \"sofa\", \"sitting on\").\noa_rel(\"can\", \"dog\", \"please human\").\noa_rel(\"can\", \"raft\", \"travel over water\").\noa_rel(\"is used for\", \"freeway\", \"driving car on\").\noa_rel(\"is a sub-event of\", \"cleaning clothing\", \"hanging clothing up\").\noa_rel(\"is used for\", \"lighthouse\", \"signaling danger\").", "oa_rel(\"is a sub-event of\", \"cleaning clothing\", \"hanging clothing up\").\noa_rel(\"is used for\", \"lighthouse\", \"signaling danger\").\noa_rel(\"is used for\", \"airplane\", \"travelling long distances\").\noa_rel(\"can\", \"monkeys\", \"climb tree\").\noa_rel(\"is used for\", \"train\", \"carrying cargo\").\noa_rel(\"is used for\", \"printer\", \"printing pictures\").\noa_rel(\"is used for\", \"printer\", \"printing books\").\noa_rel(\"is used for\", \"train\", \"travelling long distances\").\noa_rel(\"can\", \"elephant\", \"life tree\").\noa_rel(\"can\", \"cat\", \"wash itself\").", "oa_rel(\"can\", \"elephant\", \"life tree\").\noa_rel(\"can\", \"cat\", \"wash itself\").\noa_rel(\"can\", \"train\", \"arrive at city\").\noa_rel(\"is used for\", \"library\", \"finding information\").\noa_rel(\"can\", \"turtle\", \"live in house\").\noa_rel(\"can\", \"pigeon\", \"fly\").\noa_rel(\"can\", \"axe\", \"hurt people\").\noa_rel(\"can\", \"cat\", \"please humans\").\noa_rel(\"has\", \"hot chocolate\", \"caffeine\").\noa_rel(\"is\", \"sponge\", \"soft\").", "oa_rel(\"has\", \"hot chocolate\", \"caffeine\").\noa_rel(\"is\", \"sponge\", \"soft\").\noa_rel(\"usually appears in\", \"stapler\", \"office\").\noa_rel(\"is used for\", \"scooter\", \"transporting people\").\noa_rel(\"usually appears in\", \"telephone\", \"office\").\noa_rel(\"can\", \"toaster\", \"brown toast\").\noa_rel(\"can\", \"bird\", \"lay eggs\").\noa_rel(\"is used for\", \"mailbox\", \"receiving packages\").\noa_rel(\"is used for\", \"restaurant\", \"meeting friends\").\noa_rel(\"can\", \"rabbit\", \"live in house\").", "oa_rel(\"is used for\", \"restaurant\", \"meeting friends\").\noa_rel(\"can\", \"rabbit\", \"live in house\").\noa_rel(\"usually appears in\", \"tv\", \"living room\").\noa_rel(\"usually appears in\", \"straw\", \"dining room\").\noa_rel(\"has\", \"pineapple\", \"vitamin C\").\noa_rel(\"usually appears in\", \"potato\", \"salad\").\noa_rel(\"is\", \"orange\", \"sour\").\noa_rel(\"is used for\", \"stool\", \"resting\").\noa_rel(\"is used for\", \"piano\", \"playing music\").\noa_rel(\"can\", \"suv\", \"transport people\").", "oa_rel(\"is used for\", \"piano\", \"playing music\").\noa_rel(\"can\", \"suv\", \"transport people\").\noa_rel(\"is used for\", \"grill\", \"grilling hamburgers\").\noa_rel(\"can\", \"ship\", \"near island\").\noa_rel(\"is used for\", \"phones\", \"listening to music\").\noa_rel(\"is\", \"refrigerator\", \"electric\").\noa_rel(\"usually appears in\", \"guitar\", \"rock band\").\noa_rel(\"requires\", \"making juice\", \"juicer\").\noa_rel(\"is used for\", \"subway\", \"transporting people\").\noa_rel(\"is used for\", \"train car\", \"transporting people\").", "oa_rel(\"is used for\", \"subway\", \"transporting people\").\noa_rel(\"is used for\", \"train car\", \"transporting people\").\noa_rel(\"is made from\", \"fries\", \"potato\").\noa_rel(\"is\", \"chicken\", \"omnivorous\").\noa_rel(\"is used for\", \"oven\", \"heating food\").\noa_rel(\"is used for\", \"oven\", \"baking food\").\noa_rel(\"usually appears in\", \"container\", \"kitchen\").\noa_rel(\"is used for\", \"fork\", \"eating solid food\").\noa_rel(\"requires\", \"making pizza\", \"vegetables\").\noa_rel(\"is\", \"puppy\", \"soft\").", "oa_rel(\"requires\", \"making pizza\", \"vegetables\").\noa_rel(\"is\", \"puppy\", \"soft\").\noa_rel(\"can\", \"owl\", \"see at night\").\noa_rel(\"can\", \"pilot\", \"fly airplane\").\noa_rel(\"is used for\", \"keyboard\", \"typing letters onto windows\").\noa_rel(\"can\", \"charger\", \"charge battery\").\noa_rel(\"can\", \"computer\", \"stream video\").\noa_rel(\"is used for\", \"dresser\", \"holding cloth\").\noa_rel(\"usually appears in\", \"hay\", \"barn\").\noa_rel(\"is used for\", \"mailbox\", \"receiving bills\").", "oa_rel(\"usually appears in\", \"hay\", \"barn\").\noa_rel(\"is used for\", \"mailbox\", \"receiving bills\").\noa_rel(\"is used for\", \"frisbee\", \"entertainment\").\noa_rel(\"usually appears in\", \"water glass\", \"dining room\").\noa_rel(\"can\", \"train\", \"transport many people at once\").\noa_rel(\"can\", \"horse\", \"jump over objects\").\noa_rel(\"can\", \"dog\", \"follow its master\").\noa_rel(\"usually appears in\", \"penguin\", \"ocean\").\noa_rel(\"can\", \"bear\", \"eat most types of food\").\noa_rel(\"is used for\", \"couch\", \"relaxing\").", "oa_rel(\"can\", \"bear\", \"eat most types of food\").\noa_rel(\"is used for\", \"couch\", \"relaxing\").\noa_rel(\"is used for\", \"fireplace\", \"getting warm\").\noa_rel(\"usually appears in\", \"refrigerator\", \"kitchen\").\noa_rel(\"is used for\", \"phone\", \"finding information\").\noa_rel(\"is\", \"horse\", \"herbivorous\").\noa_rel(\"requires\", \"making juice\", \"fruit\").\noa_rel(\"has\", \"egg\", \"vitamin B\").\noa_rel(\"is a sub-event of\", \"baking cake\", \"mixing egg with sugar\").\noa_rel(\"usually appears in\", \"turkey\", \"thanksgiving\").", "oa_rel(\"is a sub-event of\", \"baking cake\", \"mixing egg with sugar\").\noa_rel(\"usually appears in\", \"turkey\", \"thanksgiving\").\noa_rel(\"is used for\", \"waste basket\", \"storing trash\").\noa_rel(\"is used for\", \"cake\", \"celebrating birthday\").\noa_rel(\"is used for\", \"cake\", \"eating\").\noa_rel(\"usually appears in\", \"tissue\", \"bathroom\").\noa_rel(\"is used for\", \"speaker\", \"producing sound\").\noa_rel(\"usually appears in\", \"bear\", \"jungle\").\noa_rel(\"has\", \"monkey\", \"two legs\").\noa_rel(\"usually appears in\", \"mat\", \"living room\").", "oa_rel(\"has\", \"monkey\", \"two legs\").\noa_rel(\"usually appears in\", \"mat\", \"living room\").\noa_rel(\"is used for\", \"fork\", \"piercing food\").\noa_rel(\"is\", \"juice\", \"liquid\").\noa_rel(\"can\", \"kitten\", \"live in house\").\noa_rel(\"is used for\", \"barn\", \"storing farming equipment\").\noa_rel(\"is used for\", \"chopsticks\", \"moving food to mouth\").\noa_rel(\"is\", \"soup\", \"fluid\").\noa_rel(\"is used for\", \"fruit stand\", \"buying and selling fruit\").\noa_rel(\"can\", \"horse\", \"carry riders\").", "oa_rel(\"is used for\", \"fruit stand\", \"buying and selling fruit\").\noa_rel(\"can\", \"horse\", \"carry riders\").\noa_rel(\"usually appears in\", \"curtain\", \"bedroom\").\noa_rel(\"is used for\", \"dog\", \"herding sheep\").\noa_rel(\"can\", \"cat\", \"be companion\").\noa_rel(\"usually appears in\", \"dining table\", \"dining room\").\noa_rel(\"is made from\", \"bun\", \"flour\").\noa_rel(\"has\", \"egg\", \"iron\").\noa_rel(\"is\", \"microwave\", \"electric\").\noa_rel(\"can\", \"bear\", \"hunt rabbit\").", "oa_rel(\"is\", \"microwave\", \"electric\").\noa_rel(\"can\", \"bear\", \"hunt rabbit\").\noa_rel(\"is\", \"juice\", \"fluid\").\noa_rel(\"requires\", \"mixing\", \"mixing bowl\").\noa_rel(\"is used for\", \"cutting board\", \"cutting food\").\noa_rel(\"requires\", \"baking cake\", \"butter\").\noa_rel(\"is used for\", \"bread\", \"making toast\").\noa_rel(\"has\", \"beans\", \"starch\").\noa_rel(\"is made from\", \"toast\", \"flour\").\noa_rel(\"usually appears in\", \"mousepad\", \"office\").", "oa_rel(\"is made from\", \"toast\", \"flour\").\noa_rel(\"usually appears in\", \"mousepad\", \"office\").\noa_rel(\"can\", \"dog\", \"dig holes in yard\").\noa_rel(\"has\", \"nut\", \"vitamin B\").\noa_rel(\"requires\", \"frying\", \"frying pan\").\noa_rel(\"is made from\", \"macaroni\", \"flour\").\noa_rel(\"usually appears in\", \"glass\", \"dining room\").\noa_rel(\"is used for\", \"bathroom\", \"clean humans\").\noa_rel(\"is used for\", \"frisbee\", \"throwing\").\noa_rel(\"usually appears in\", \"bedspread\", \"bedroom\").", "oa_rel(\"is used for\", \"frisbee\", \"throwing\").\noa_rel(\"usually appears in\", \"bedspread\", \"bedroom\").\noa_rel(\"is used for\", \"tongs\", \"grasping food\").\noa_rel(\"is used for\", \"vegetable\", \"eating\").\noa_rel(\"requires\", \"mixing\", \"spoon\").\noa_rel(\"has\", \"tomato\", \"vitamin C\").\noa_rel(\"is\", \"peach\", \"sweet\").\noa_rel(\"is a sub-event of\", \"making pizza\", \"baking pizza in oven\").\noa_rel(\"is\", \"heater\", \"electric\").\noa_rel(\"is used for\", \"guitar\", \"playing chords\").", "oa_rel(\"is\", \"heater\", \"electric\").\noa_rel(\"is used for\", \"guitar\", \"playing chords\").\noa_rel(\"can\", \"jellyfish\", \"hurt person\").\noa_rel(\"is used for\", \"ipod\", \"listening to music\").\noa_rel(\"can\", \"vacuum\", \"clean carpet\").\noa_rel(\"is used for\", \"harbor\", \"store boats\").\noa_rel(\"is\", \"butter\", \"sticky\").\noa_rel(\"can\", \"refrigerator\", \"cool warm food\").\noa_rel(\"is used for\", \"binder\", \"holding papers together\").\noa_rel(\"is\", \"vegetables\", \"healthy\").", "oa_rel(\"is used for\", \"binder\", \"holding papers together\").\noa_rel(\"is\", \"vegetables\", \"healthy\").\noa_rel(\"usually appears in\", \"napkin\", \"restaurant\").\noa_rel(\"is\", \"beer\", \"liquid\").\noa_rel(\"has\", \"pancake\", \"starch\").\noa_rel(\"can\", \"frisbee\", \"fly\").\noa_rel(\"is used for\", \"bathroom\", \"washing up\").\noa_rel(\"is used for\", \"restaurant\", \"eating\").\noa_rel(\"can\", \"van\", \"spend gas\").\noa_rel(\"can\", \"rain\", \"wet clothes\").", "oa_rel(\"can\", \"van\", \"spend gas\").\noa_rel(\"can\", \"rain\", \"wet clothes\").\noa_rel(\"is used for\", \"ship\", \"transportation at sea\").\noa_rel(\"is\", \"ice cream\", \"sweet\").\noa_rel(\"is made from\", \"cake\", \"flour\").\noa_rel(\"can\", \"school bus\", \"travel on road\").\noa_rel(\"can\", \"airplane\", \"circle airfield\").\noa_rel(\"usually appears in\", \"tablecloth\", \"restaurant\").\noa_rel(\"is used for\", \"airplane\", \"transporting lots of people\").\noa_rel(\"can\", \"turtle\", \"hide in its shell\").", "oa_rel(\"is used for\", \"airplane\", \"transporting lots of people\").\noa_rel(\"can\", \"turtle\", \"hide in its shell\").\noa_rel(\"can\", \"oven\", \"brown chicken\").\noa_rel(\"usually appears in\", \"nightstand\", \"bedroom\").\noa_rel(\"usually appears in\", \"bowl\", \"dining room\").\noa_rel(\"is used for\", \"pencil\", \"drawing\").\noa_rel(\"has\", \"cheese\", \"calcium\").\noa_rel(\"is a sub-event of\", \"cleaning house\", \"vacuuming floors\").\noa_rel(\"can\", \"cat\", \"sleep most of day\").\noa_rel(\"usually appears in\", \"faucet\", \"bathroom\").", "oa_rel(\"can\", \"cat\", \"sleep most of day\").\noa_rel(\"usually appears in\", \"faucet\", \"bathroom\").\noa_rel(\"has\", \"soda\", \"water\").\noa_rel(\"is used for\", \"bell\", \"making noise\").\noa_rel(\"usually appears in\", \"hair clip\", \"bathroom\").\noa_rel(\"can\", \"dishwasher\", \"wash dirty dishes\").\noa_rel(\"is used for\", \"shop\", \"buying and selling\").\noa_rel(\"usually appears in\", \"rolling pin\", \"kitchen\").\noa_rel(\"usually appears in\", \"toaster oven\", \"kitchen\").\noa_rel(\"usually appears in\", \"waiter\", \"restaurant\").", "oa_rel(\"usually appears in\", \"toaster oven\", \"kitchen\").\noa_rel(\"usually appears in\", \"waiter\", \"restaurant\").\noa_rel(\"usually appears in\", \"lobster\", \"water\").\noa_rel(\"has\", \"lemon\", \"vitamin C\").\noa_rel(\"is used for\", \"condiment\", \"flavoring food\").\noa_rel(\"can\", \"ice maker\", \"making ice\").\noa_rel(\"has\", \"beer\", \"water\").\noa_rel(\"usually appears in\", \"wine glass\", \"dining room\").\noa_rel(\"can\", \"computer\", \"boot from hard drive\").\noa_rel(\"can\", \"cell phone\", \"ring\").", "oa_rel(\"can\", \"computer\", \"boot from hard drive\").\noa_rel(\"can\", \"cell phone\", \"ring\").\noa_rel(\"is used for\", \"rug\", \"prevent scratches on floor\").\noa_rel(\"usually appears in\", \"keyboard\", \"office\").\noa_rel(\"is used for\", \"hair dryer\", \"dry hair\").\noa_rel(\"is used for\", \"apartment\", \"housing family\").\noa_rel(\"usually appears in\", \"cutting board\", \"kitchen\").\noa_rel(\"is made from\", \"pizza pie\", \"flour\").\noa_rel(\"usually appears in\", \"shower curtain\", \"bathroom\").\noa_rel(\"is used for\", \"pantry\", \"keeping food organized\").", "oa_rel(\"usually appears in\", \"shower curtain\", \"bathroom\").\noa_rel(\"is used for\", \"pantry\", \"keeping food organized\").\noa_rel(\"has\", \"elephant\", \"long nose\").\noa_rel(\"is used for\", \"spoon\", \"people to eat soup with\").\noa_rel(\"is made from\", \"bagel\", \"flour\").\noa_rel(\"usually appears in\", \"bill\", \"restaurant\").\noa_rel(\"can\", \"knife\", \"cut potato\").\noa_rel(\"has\", \"mushroom\", \"vitamin D\").\noa_rel(\"can\", \"computer\", \"speed up research\").\noa_rel(\"usually appears in\", \"pumpkin\", \"halloween\").", "oa_rel(\"can\", \"computer\", \"speed up research\").\noa_rel(\"usually appears in\", \"pumpkin\", \"halloween\").\noa_rel(\"usually appears in\", \"salt shaker\", \"restaurant\").\noa_rel(\"can\", \"van\", \"carry few persons\").\noa_rel(\"usually appears in\", \"pen\", \"office\").\noa_rel(\"usually appears in\", \"coffee mug\", \"restaurant\").\noa_rel(\"has\", \"kiwi\", \"vitamin C\").\noa_rel(\"is\", \"pear\", \"sweet\").\noa_rel(\"can\", \"grain\", \"provide complex carbohydrates\").\noa_rel(\"is used for\", \"screw\", \"attaching item to something else\").", "oa_rel(\"can\", \"grain\", \"provide complex carbohydrates\").\noa_rel(\"is used for\", \"screw\", \"attaching item to something else\").\noa_rel(\"is used for\", \"computer\", \"playing games\").\noa_rel(\"usually appears in\", \"coffee mug\", \"dining room\").\noa_rel(\"is used for\", \"soap\", \"washing clothes\").\noa_rel(\"is used for\", \"bookshelf\", \"storing novels\").\noa_rel(\"is used for\", \"hotel\", \"temporary residence\").\noa_rel(\"is used for\", \"computer\", \"surfing internet\").\noa_rel(\"usually appears in\", \"planter\", \"farm\").\noa_rel(\"is\", \"desert\", \"dangerous\").", "oa_rel(\"usually appears in\", \"planter\", \"farm\").\noa_rel(\"is\", \"desert\", \"dangerous\").\noa_rel(\"can\", \"bird\", \"attempt to fly\").\noa_rel(\"is used for\", \"air conditioner\", \"cooling air\").\noa_rel(\"usually appears in\", \"piano\", \"orchestra\").\noa_rel(\"requires\", \"making pizza\", \"cheese\").\noa_rel(\"is\", \"blender\", \"electric\").\noa_rel(\"is\", \"cliff\", \"dangerous\").\noa_rel(\"is used for\", \"restaurant\", \"purchasing meals\").\noa_rel(\"is\", \"soda\", \"fluid\").", "oa_rel(\"is used for\", \"restaurant\", \"purchasing meals\").\noa_rel(\"is\", \"soda\", \"fluid\").\noa_rel(\"is used for\", \"mailbox\", \"storing mail\").\noa_rel(\"can\", \"knife\", \"cut cheese\").\noa_rel(\"requires\", \"crushing\", \"knife\").\noa_rel(\"usually appears in\", \"binder\", \"office\").\noa_rel(\"usually appears in\", \"toilet paper\", \"bathroom\").\noa_rel(\"usually appears in\", \"sheet\", \"bedroom\").\noa_rel(\"can\", \"air conditioner\", \"cool air\").\noa_rel(\"is\", \"sauce\", \"fluid\").", "oa_rel(\"can\", \"air conditioner\", \"cool air\").\noa_rel(\"is\", \"sauce\", \"fluid\").\noa_rel(\"can\", \"gas stove\", \"heat food\").\noa_rel(\"usually appears in\", \"toilet\", \"bathroom\").\noa_rel(\"can\", \"refrigerator\", \"keep ice cold\").\noa_rel(\"usually appears in\", \"pillow\", \"bedroom\").\noa_rel(\"is used for\", \"dining room\", \"drinking\").\noa_rel(\"is used for\", \"bread\", \"eating\").\noa_rel(\"is a sub-event of\", \"juicing\", \"cutting fruit in half\").\noa_rel(\"usually appears in\", \"fish\", \"water\").", "oa_rel(\"is a sub-event of\", \"juicing\", \"cutting fruit in half\").\noa_rel(\"usually appears in\", \"fish\", \"water\").\noa_rel(\"can\", \"oven\", \"bake\").\noa_rel(\"is used for\", \"oven\", \"cooking\").\noa_rel(\"usually appears in\", \"saucer\", \"restaurant\").\noa_rel(\"is used for\", \"office\", \"holding meeting\").\noa_rel(\"is made from\", \"beer\", \"hops\").\noa_rel(\"can\", \"computer\", \"process information\").\noa_rel(\"is used for\", \"comb\", \"removing tangles from hair\").\noa_rel(\"requires\", \"washing dishes\", \"water\").", "oa_rel(\"is used for\", \"comb\", \"removing tangles from hair\").\noa_rel(\"requires\", \"washing dishes\", \"water\").\noa_rel(\"is used for\", \"office\", \"working\").\noa_rel(\"usually appears in\", \"tap\", \"bathroom\").\noa_rel(\"usually appears in\", \"foil\", \"kitchen\").\noa_rel(\"can\", \"dog\", \"be companion\").\noa_rel(\"is used for\", \"raft\", \"transporting people\").\noa_rel(\"can\", \"helmets\", \"prevent head injuries\").\noa_rel(\"is\", \"sword\", \"dangerous\").\noa_rel(\"has\", \"beans\", \"iron\").", "oa_rel(\"is\", \"sword\", \"dangerous\").\noa_rel(\"has\", \"beans\", \"iron\").\noa_rel(\"is\", \"pepper\", \"spicy\").\noa_rel(\"is used for\", \"elephant\", \"riding\").\noa_rel(\"is used for\", \"toothbrush\", \"cleaning teeth\").\noa_rel(\"has\", \"broccoli\", \"vitamin C\").\noa_rel(\"can\", \"rice\", \"provide complex carbohydrates\").\noa_rel(\"can\", \"dog\", \"learn to fetch things\").\noa_rel(\"can be\", \"steak\", \"cut\").\noa_rel(\"is made from\", \"onion ring\", \"onion\").", "oa_rel(\"can be\", \"steak\", \"cut\").\noa_rel(\"is made from\", \"onion ring\", \"onion\").\noa_rel(\"has\", \"cheese\", \"vitamin D\").\noa_rel(\"is a sub-event of\", \"making juice\", \"running ingredients through juicer\").\noa_rel(\"is used for\", \"spoon\", \"eating liquids\").\noa_rel(\"usually appears in\", \"knife\", \"kitchen\").\noa_rel(\"is\", \"pony\", \"herbivorous\").\noa_rel(\"can\", \"grain\", \"provide energy\").\noa_rel(\"is a sub-event of\", \"making coffee\", \"brewing coffee\").\noa_rel(\"has\", \"pastry\", \"starch\").", "oa_rel(\"is a sub-event of\", \"making coffee\", \"brewing coffee\").\noa_rel(\"has\", \"pastry\", \"starch\").\noa_rel(\"usually appears in\", \"mat\", \"bathroom\").\noa_rel(\"is used for\", \"teddy bear\", \"having fun\").\noa_rel(\"is made from\", \"cream\", \"milk\").\noa_rel(\"has\", \"blueberry\", \"vitamin C\").\noa_rel(\"is\", \"juice\", \"healthy\").\noa_rel(\"can be\", \"chicken\", \"roasted\").\noa_rel(\"can\", \"waiter\", \"serve food\").\noa_rel(\"can\", \"bird\", \"be pet\").", "oa_rel(\"can\", \"waiter\", \"serve food\").\noa_rel(\"can\", \"bird\", \"be pet\").\noa_rel(\"is used for\", \"spoon\", \"drinking\").\noa_rel(\"can\", \"airplane\", \"fly\").\noa_rel(\"is used for\", \"sailboat\", \"transportation at sea\").\noa_rel(\"is\", \"lamb\", \"herbivorous\").\noa_rel(\"is used for\", \"barn\", \"feeding animals\").\noa_rel(\"usually appears in\", \"fork\", \"restaurant\").\noa_rel(\"is made from\", \"bread\", \"flour\").\noa_rel(\"has\", \"spinach\", \"vitamin B\").", "oa_rel(\"is made from\", \"bread\", \"flour\").\noa_rel(\"has\", \"spinach\", \"vitamin B\").\noa_rel(\"requires\", \"making pizza\", \"meat\").\noa_rel(\"can\", \"cat\", \"be pet\").\noa_rel(\"is used for\", \"baseball\", \"hitting\").\noa_rel(\"requires\", \"coring\", \"knife\").\noa_rel(\"is used for\", \"comb\", \"styling hair\").\noa_rel(\"can\", \"computer\", \"stream media\").\noa_rel(\"can\", \"suv\", \"carry few persons\").\noa_rel(\"requires\", \"cleaning clothing\", \"water\").", "oa_rel(\"can\", \"suv\", \"carry few persons\").\noa_rel(\"requires\", \"cleaning clothing\", \"water\").\noa_rel(\"can\", \"cat\", \"jump onto table or chair\").\noa_rel(\"is\", \"strawberry\", \"sweet\").\noa_rel(\"can\", \"catcher\", \"catch\").\noa_rel(\"is\", \"dog\", \"omnivorous\").\noa_rel(\"is used for\", \"toaster\", \"toasting bread\").\noa_rel(\"can be\", \"drinks\", \"drunk\").\noa_rel(\"can\", \"police\", \"carry gun while at work\").\noa_rel(\"is made from\", \"wine\", \"grapes\").", "oa_rel(\"can\", \"police\", \"carry gun while at work\").\noa_rel(\"is made from\", \"wine\", \"grapes\").\noa_rel(\"is\", \"cat\", \"carnivorous\").\noa_rel(\"is a sub-event of\", \"cleaning house\", \"polishing furniture\").\noa_rel(\"can\", \"bird\", \"fly\").\noa_rel(\"is\", \"eagle\", \"carnivorous\").\noa_rel(\"is used for\", \"rug\", \"walking on\").\noa_rel(\"is\", \"sweet potato\", \"sweet\").\noa_rel(\"is used for\", \"oven\", \"roasting\").\noa_rel(\"is used for\", \"dog\", \"providing friendship\").", "oa_rel(\"is used for\", \"oven\", \"roasting\").\noa_rel(\"is used for\", \"dog\", \"providing friendship\").\noa_rel(\"is\", \"gravy\", \"sticky\").\noa_rel(\"is used for\", \"grill\", \"grilling steak\").\noa_rel(\"can\", \"bear\", \"stand on their hind legs\").\noa_rel(\"is used for\", \"farm\", \"raising crops\").\noa_rel(\"can\", \"cow\", \"supply humans with milk\").\noa_rel(\"is made from\", \"biscuit\", \"flour\").\noa_rel(\"usually appears in\", \"seagull\", \"ocean\").\noa_rel(\"is\", \"chili\", \"spicy\").", "oa_rel(\"usually appears in\", \"seagull\", \"ocean\").\noa_rel(\"is\", \"chili\", \"spicy\").\noa_rel(\"usually appears in\", \"toaster\", \"kitchen\").\noa_rel(\"can\", \"bread\", \"provide complex carbohydrates\").\noa_rel(\"is used for\", \"lipstick\", \"coloring lips\").\noa_rel(\"can\", \"computer\", \"run programs\").\noa_rel(\"is\", \"broth\", \"fluid\").\noa_rel(\"has\", \"broccoli\", \"calcium\").\noa_rel(\"is used for\", \"baseball\", \"pitching\").\noa_rel(\"is used for\", \"apple\", \"making juice\").", "oa_rel(\"is used for\", \"baseball\", \"pitching\").\noa_rel(\"is used for\", \"apple\", \"making juice\").\noa_rel(\"has\", \"toast\", \"starch\").\noa_rel(\"usually appears in\", \"tv stand\", \"living room\").\noa_rel(\"can be\", \"chicken\", \"fried\").\noa_rel(\"can\", \"dog\", \"come to its master\").\noa_rel(\"can\", \"frog\", \"spring out of pond\").\noa_rel(\"is used for\", \"computer\", \"sending email\").\noa_rel(\"is a sub-event of\", \"juicing\", \"pressing halves on juicer\").\noa_rel(\"is used for\", \"air conditioner\", \"cooling people\").", "oa_rel(\"is a sub-event of\", \"juicing\", \"pressing halves on juicer\").\noa_rel(\"is used for\", \"air conditioner\", \"cooling people\").\noa_rel(\"is used for\", \"scissors\", \"cutting paper or cloth\").\noa_rel(\"can\", \"dog\", \"sleep long time\").\noa_rel(\"usually appears in\", \"spatula\", \"kitchen\").\noa_rel(\"has\", \"alcohol\", \"water\").\noa_rel(\"is used for\", \"bank\", \"saving money\").\noa_rel(\"can\", \"bird\", \"fly high\").\noa_rel(\"can\", \"screwdriver\", \"turn screw\").\noa_rel(\"is\", \"cat\", \"soft\").", "oa_rel(\"can\", \"screwdriver\", \"turn screw\").\noa_rel(\"is\", \"cat\", \"soft\").\noa_rel(\"is used for\", \"meat\", \"getting protein\").\noa_rel(\"is used for\", \"canoe\", \"floating and moving on water\").\noa_rel(\"is used for\", \"tunnel\", \"transportation\").\noa_rel(\"has\", \"sweet potato\", \"vitamin C\").\noa_rel(\"is\", \"giraffe\", \"herbivorous\").\noa_rel(\"is used for\", \"screw\", \"fastening two objects together\").\noa_rel(\"has\", \"tea\", \"water\").\noa_rel(\"has\", \"tea\", \"caffeine\").", "oa_rel(\"has\", \"tea\", \"water\").\noa_rel(\"has\", \"tea\", \"caffeine\").\noa_rel(\"can\", \"cat\", \"mind getting wet\").\noa_rel(\"can\", \"jeep\", \"spend gas\").\noa_rel(\"is made from\", \"noodles\", \"flour\").\noa_rel(\"is used for\", \"sugar\", \"adding taste to food\").\noa_rel(\"is used for\", \"cow\", \"milking\").\noa_rel(\"is\", \"butter\", \"fluid\").\noa_rel(\"usually appears in\", \"knife\", \"dining room\").\noa_rel(\"is\", \"coffee\", \"liquid\").", "oa_rel(\"usually appears in\", \"knife\", \"dining room\").\noa_rel(\"is\", \"coffee\", \"liquid\").\noa_rel(\"is\", \"coffee\", \"fluid\").\noa_rel(\"is\", \"milk\", \"fluid\").\noa_rel(\"can\", \"airplane\", \"travel through many time zones\").\noa_rel(\"is used for\", \"canoe\", \"keeping people out of water\").\noa_rel(\"has\", \"giraffe\", \"long neck\").\noa_rel(\"has\", \"beef\", \"iron\").\noa_rel(\"can\", \"ship\", \"go across sea\").\noa_rel(\"can\", \"seagull\", \"fly\").", "oa_rel(\"can\", \"ship\", \"go across sea\").\noa_rel(\"can\", \"seagull\", \"fly\").\noa_rel(\"is used for\", \"baseball field\", \"playing baseball with team\").\noa_rel(\"can\", \"hats\", \"go on hat rack\").\noa_rel(\"requires\", \"cooking\", \"food\").\noa_rel(\"is\", \"oil\", \"liquid\").\noa_rel(\"requires\", \"sauteing\", \"oil\").\noa_rel(\"is\", \"toaster\", \"electric\").\noa_rel(\"can\", \"computer\", \"mine data\").\noa_rel(\"can\", \"bird\", \"chirp\").", "oa_rel(\"can\", \"computer\", \"mine data\").\noa_rel(\"can\", \"bird\", \"chirp\").\noa_rel(\"is\", \"cow\", \"herbivorous\").\noa_rel(\"can\", \"knife\", \"be both tools and weapons\").\noa_rel(\"usually appears in\", \"chef\", \"restaurant\").\noa_rel(\"can be\", \"clothing\", \"hung\").\noa_rel(\"is\", \"milk\", \"nutritious\").\noa_rel(\"usually appears in\", \"bookshelf\", \"bedroom\").\noa_rel(\"is\", \"cream\", \"sticky\").\noa_rel(\"is made from\", \"burrito\", \"flour\").", "oa_rel(\"is\", \"cream\", \"sticky\").\noa_rel(\"is made from\", \"burrito\", \"flour\").\noa_rel(\"is a sub-event of\", \"baking bread\", \"gathering ingredients\").\noa_rel(\"has\", \"coffee\", \"caffeine\").\noa_rel(\"can be\", \"cardigan\", \"hung\").\noa_rel(\"can\", \"bear\", \"swim\").\noa_rel(\"is used for\", \"tattoo\", \"decoration\").\noa_rel(\"can be\", \"pizza\", \"eaten\").\noa_rel(\"can\", \"bird\", \"feed worms to its young\").\noa_rel(\"is used for\", \"museum\", \"displaying historical artifacts\").", "oa_rel(\"can\", \"bird\", \"feed worms to its young\").\noa_rel(\"is used for\", \"museum\", \"displaying historical artifacts\").\noa_rel(\"can be\", \"pizza\", \"cut\").\noa_rel(\"usually appears in\", \"kettle\", \"kitchen\").\noa_rel(\"is used for\", \"milk\", \"feeding baby\").\noa_rel(\"is\", \"air conditioner\", \"electric\").\noa_rel(\"usually appears in\", \"duck\", \"water\").\noa_rel(\"is used for\", \"mailbox\", \"sending packages\").\noa_rel(\"requires\", \"grating\", \"grater\").\noa_rel(\"has\", \"wine\", \"water\").", "oa_rel(\"requires\", \"grating\", \"grater\").\noa_rel(\"has\", \"wine\", \"water\").\noa_rel(\"is\", \"milk\", \"liquid\").\noa_rel(\"can\", \"horse\", \"finish race\").\noa_rel(\"usually appears in\", \"vegetable\", \"dinner\").\noa_rel(\"is made from\", \"cupcake\", \"flour\").\noa_rel(\"is\", \"polar bear\", \"carnivorous\").\noa_rel(\"is used for\", \"pickup\", \"transporting goods\").\noa_rel(\"is used for\", \"hotel\", \"staying overnight\").\noa_rel(\"is used for\", \"headphones\", \"listening to music\").", "oa_rel(\"is used for\", \"hotel\", \"staying overnight\").\noa_rel(\"is used for\", \"headphones\", \"listening to music\").\noa_rel(\"can\", \"pony\", \"be pet\").\noa_rel(\"can\", \"horse\", \"jump over hurdles\").\noa_rel(\"has\", \"peacock\", \"large\").\noa_rel(\"is used for\", \"ship\", \"traveling on water\").\noa_rel(\"is used for\", \"ship\", \"carrying cargo\").\noa_rel(\"usually appears in\", \"ladle\", \"kitchen\").\noa_rel(\"has\", \"beans\", \"calcium\").\noa_rel(\"is\", \"marshmallow\", \"sweet\").", "oa_rel(\"has\", \"beans\", \"calcium\").\noa_rel(\"is\", \"marshmallow\", \"sweet\").\noa_rel(\"usually appears in\", \"pepper shaker\", \"restaurant\").\noa_rel(\"usually appears in\", \"pencil\", \"office\").\noa_rel(\"is\", \"soda\", \"liquid\").\noa_rel(\"is made from\", \"lemonade\", \"lemon\").\noa_rel(\"is\", \"lemonade\", \"liquid\").\noa_rel(\"can\", \"dog\", \"smell drugs\").\noa_rel(\"usually appears in\", \"blade\", \"kitchen\").\noa_rel(\"is\", \"lemon\", \"bitter\").", "oa_rel(\"usually appears in\", \"blade\", \"kitchen\").\noa_rel(\"is\", \"lemon\", \"bitter\").\noa_rel(\"is used for\", \"soap\", \"washing hands\").\noa_rel(\"is a sub-event of\", \"baking cake\", \"pouring batter in cake pan\").\noa_rel(\"is made from\", \"pizza\", \"flour\").\noa_rel(\"is made from\", \"coffee\", \"coffee beans\").\noa_rel(\"is used for\", \"temple\", \"worship\").\noa_rel(\"is used for\", \"ship\", \"floating and moving on water\").\noa_rel(\"has\", \"cheese\", \"vitamin B\").\noa_rel(\"is made from\", \"pancake\", \"flour\").", "oa_rel(\"has\", \"cheese\", \"vitamin B\").\noa_rel(\"is made from\", \"pancake\", \"flour\").\noa_rel(\"can\", \"cat\", \"hunt mice\").\noa_rel(\"can\", \"owl\", \"hear slightest rustle\").\noa_rel(\"usually appears in\", \"giraffe\", \"grassland\").\noa_rel(\"usually appears in\", \"urinal\", \"bathroom\").\noa_rel(\"has\", \"egg\", \"vitamin D\").\noa_rel(\"is used for\", \"pencil\", \"writing\").\noa_rel(\"usually appears in\", \"carpet\", \"bedroom\").\noa_rel(\"requires\", \"juicing\", \"juicer\").", "oa_rel(\"usually appears in\", \"carpet\", \"bedroom\").\noa_rel(\"requires\", \"juicing\", \"juicer\").\noa_rel(\"is\", \"coffee\", \"bitter\").\noa_rel(\"can\", \"owl\", \"fly\").\noa_rel(\"is used for\", \"office\", \"conducting business\").\noa_rel(\"can\", \"screw\", \"hold things together\").\noa_rel(\"can\", \"police\", \"arrest\").\noa_rel(\"has\", \"spinach\", \"iron\").\noa_rel(\"has\", \"beef\", \"vitamin B\").\noa_rel(\"can\", \"squirrel\", \"store nuts\").", "oa_rel(\"has\", \"beef\", \"vitamin B\").\noa_rel(\"can\", \"squirrel\", \"store nuts\").\noa_rel(\"requires\", \"making pizza\", \"pizza pan\").\noa_rel(\"requires\", \"coring\", \"slicer\").\noa_rel(\"can\", \"thermometer\", \"measure temperature\").\noa_rel(\"is made from\", \"cookie\", \"flour\").\noa_rel(\"is\", \"beer\", \"fluid\").\noa_rel(\"has\", \"pepper\", \"vitamin C\").\noa_rel(\"is\", \"gun\", \"dangerous\").\noa_rel(\"can\", \"soldier\", \"fight battle\").", "oa_rel(\"is\", \"gun\", \"dangerous\").\noa_rel(\"can\", \"soldier\", \"fight battle\").\noa_rel(\"is used for\", \"apartment\", \"dwelling\").\noa_rel(\"has\", \"spinach\", \"vitamin C\").\noa_rel(\"usually appears in\", \"pizza tray\", \"dining room\").\noa_rel(\"is used for\", \"vacuum\", \"cleaning carpet\").\noa_rel(\"is a sub-event of\", \"cleaning house\", \"getting vacuum out\").\noa_rel(\"is used for\", \"vending machine\", \"buying drinks\").\noa_rel(\"is used for\", \"ground\", \"standing on\").\noa_rel(\"is\", \"dip\", \"fluid\").", "oa_rel(\"is used for\", \"ground\", \"standing on\").\noa_rel(\"is\", \"dip\", \"fluid\").\noa_rel(\"can\", \"bird\", \"learn to fly\").\noa_rel(\"usually appears in\", \"sheep\", \"meadow\").\noa_rel(\"usually appears in\", \"turkey\", \"christmas\").\noa_rel(\"usually appears in\", \"bread\", \"dinner\").\noa_rel(\"has\", \"monkey\", \"two arms\").\noa_rel(\"has\", \"milk\", \"water\").\noa_rel(\"is\", \"honey\", \"sweet\").\noa_rel(\"is\", \"lemon\", \"sour\").", "oa_rel(\"is\", \"honey\", \"sweet\").\noa_rel(\"is\", \"lemon\", \"sour\").\noa_rel(\"usually appears in\", \"fork\", \"dining room\").\noa_rel(\"is used for\", \"highway\", \"transportation\").\noa_rel(\"has\", \"almond\", \"vitamin B\").\noa_rel(\"is made from\", \"yogurt\", \"milk\").\noa_rel(\"can\", \"waitress\", \"serve food\").\noa_rel(\"can\", \"pickup\", \"travel on road\").\noa_rel(\"can\", \"bird\", \"build nest\").\noa_rel(\"can\", \"jeep\", \"transport people\").", "oa_rel(\"can\", \"bird\", \"build nest\").\noa_rel(\"can\", \"jeep\", \"transport people\").\noa_rel(\"is used for\", \"sheep\", \"shearing\").\noa_rel(\"is used for\", \"library\", \"reading books\").\noa_rel(\"can\", \"banjo\", \"play bluegrass music\").\noa_rel(\"is used for\", \"sugar\", \"making drinks sweet\").\noa_rel(\"usually appears in\", \"crab\", \"ocean\").\noa_rel(\"is used for\", \"toothbrush\", \"keeping you teeth clean\").\noa_rel(\"requires\", \"cleaning clothing\", \"washing machine\").\noa_rel(\"usually appears in\", \"lobster\", \"ocean\").", "oa_rel(\"requires\", \"cleaning clothing\", \"washing machine\").\noa_rel(\"usually appears in\", \"lobster\", \"ocean\").\noa_rel(\"is\", \"game controller\", \"electric\").\noa_rel(\"is used for\", \"doll\", \"entertainment\").\noa_rel(\"is used for\", \"computer\", \"doing calculation\").\noa_rel(\"can\", \"horse\", \"run faster than most humans\").\noa_rel(\"is used for\", \"drum\", \"banging out rhythms\").\noa_rel(\"has\", \"spinach\", \"vitamin D\").\noa_rel(\"is used for\", \"kettle\", \"heating water\").\noa_rel(\"can\", \"snail\", \"wave their antennae\").", "oa_rel(\"is used for\", \"kettle\", \"heating water\").\noa_rel(\"can\", \"snail\", \"wave their antennae\").\noa_rel(\"is used for\", \"oil\", \"frying food in\").\noa_rel(\"usually appears in\", \"baking sheet\", \"kitchen\").\noa_rel(\"usually appears in\", \"whale\", \"water\").\noa_rel(\"can\", \"whale\", \"swim\").\noa_rel(\"is\", \"dip\", \"sticky\").\noa_rel(\"has\", \"juice\", \"vitamin C\").\noa_rel(\"can\", \"canoe\", \"travel over water\").\noa_rel(\"can\", \"eagle\", \"fly\").", "oa_rel(\"can\", \"canoe\", \"travel over water\").\noa_rel(\"can\", \"eagle\", \"fly\").\noa_rel(\"has\", \"sweet potato\", \"calcium\").\noa_rel(\"is used for\", \"museum\", \"preserving historical artifacts\").\noa_rel(\"is made from\", \"omelette\", \"flour\").\noa_rel(\"is used for\", \"toaster\", \"making toast\").\noa_rel(\"usually appears in\", \"cake\", \"birthday party\").\noa_rel(\"has\", \"rice\", \"vitamin B\").\noa_rel(\"is\", \"gravy\", \"fluid\").\noa_rel(\"usually appears in\", \"wedding cake\", \"wedding\").", "oa_rel(\"is\", \"gravy\", \"fluid\").\noa_rel(\"usually appears in\", \"wedding cake\", \"wedding\").\noa_rel(\"is used for\", \"kettle\", \"making tea\").\noa_rel(\"usually appears in\", \"dolphin\", \"ocean\").\noa_rel(\"is\", \"shark\", \"dangerous\").\noa_rel(\"is\", \"ipod\", \"electric\").\noa_rel(\"usually appears in\", \"notepad\", \"office\").\noa_rel(\"usually appears in\", \"tablecloth\", \"dining room\").\noa_rel(\"can be\", \"lemon\", \"squeezed\").\noa_rel(\"can\", \"horse\", \"be raced and ridden by humans\").", "oa_rel(\"can be\", \"lemon\", \"squeezed\").\noa_rel(\"can\", \"horse\", \"be raced and ridden by humans\").\noa_rel(\"has\", \"fish\", \"vitamin D\").\noa_rel(\"usually appears in\", \"speaker\", \"office\").\noa_rel(\"is used for\", \"ship\", \"travelling long distances\").\noa_rel(\"can\", \"wine\", \"be ingredient in recipe\").\noa_rel(\"requires\", \"baking bread\", \"dough\").\noa_rel(\"is used for\", \"classroom\", \"teaching\").\noa_rel(\"requires\", \"processing\", \"food processor\").\noa_rel(\"has\", \"chocolate\", \"caffeine\").", "oa_rel(\"requires\", \"processing\", \"food processor\").\noa_rel(\"has\", \"chocolate\", \"caffeine\").\noa_rel(\"can\", \"cat\", \"sense with their whiskers\").\noa_rel(\"is made from\", \"tortilla\", \"flour\").\noa_rel(\"can\", \"fly\", \"fly\").\noa_rel(\"can\", \"bat\", \"fly\").\noa_rel(\"is\", \"tea\", \"fluid\").\noa_rel(\"is used for\", \"axe\", \"chopping wood\").\noa_rel(\"is used for\", \"double decker\", \"transporting lots of people\").\noa_rel(\"is\", \"sugar\", \"sweet\").", "oa_rel(\"is used for\", \"double decker\", \"transporting lots of people\").\noa_rel(\"is\", \"sugar\", \"sweet\").\noa_rel(\"has\", \"raspberry\", \"vitamin C\").\noa_rel(\"is\", \"monkey\", \"omnivorous\").\noa_rel(\"usually appears in\", \"student\", \"classroom\").\noa_rel(\"requires\", \"making pizza\", \"olive oil\").\noa_rel(\"is used for\", \"knife\", \"slicing\").\noa_rel(\"has\", \"tiger\", \"stripes\").\noa_rel(\"usually appears in\", \"pencil sharpener\", \"office\").\noa_rel(\"can\", \"dog\", \"sense danger\").", "oa_rel(\"usually appears in\", \"pencil sharpener\", \"office\").\noa_rel(\"can\", \"dog\", \"sense danger\").\noa_rel(\"can\", \"shuttle\", \"fly\").\noa_rel(\"is used for\", \"condiment\", \"adding taste to food\").\noa_rel(\"can be\", \"lemon\", \"eaten\").\noa_rel(\"has\", \"corn\", \"starch\").\noa_rel(\"can\", \"cat\", \"purr\").\noa_rel(\"is used for\", \"mailbox\", \"receiving letters\").\noa_rel(\"is used for\", \"frisbee\", \"having fun\").\noa_rel(\"can\", \"police\", \"direct traffic\").", "oa_rel(\"is used for\", \"frisbee\", \"having fun\").\noa_rel(\"can\", \"police\", \"direct traffic\").\noa_rel(\"usually appears in\", \"goat\", \"barn\").\noa_rel(\"usually appears in\", \"grill\", \"kitchen\").\noa_rel(\"is used for\", \"sausage\", \"getting protein\").\noa_rel(\"is used for\", \"cafe\", \"having snack\").\noa_rel(\"usually appears in\", \"zebra\", \"grassland\").\noa_rel(\"is\", \"pig\", \"omnivorous\").\noa_rel(\"has\", \"cranberry\", \"vitamin C\").\noa_rel(\"can\", \"suv\", \"move quickly\").", "oa_rel(\"has\", \"cranberry\", \"vitamin C\").\noa_rel(\"can\", \"suv\", \"move quickly\").\noa_rel(\"can\", \"frog\", \"catch fly\").\noa_rel(\"usually appears in\", \"wine\", \"bar\").\noa_rel(\"is used for\", \"factory\", \"manufacture things\").\noa_rel(\"is\", \"olive oil\", \"liquid\").\noa_rel(\"requires\", \"making pizza\", \"mixing bowl\").\noa_rel(\"is used for\", \"sugar\", \"imparting specific flavor\").\noa_rel(\"is made from\", \"pita\", \"flour\").\noa_rel(\"is\", \"banana\", \"healthy\").", "oa_rel(\"is made from\", \"pita\", \"flour\").\noa_rel(\"is\", \"banana\", \"healthy\").\noa_rel(\"can\", \"laptop\", \"save files on disk\").\noa_rel(\"usually appears in\", \"hand soap\", \"bathroom\").\noa_rel(\"is used for\", \"air conditioner\", \"lowering air temperature\").\noa_rel(\"is used for\", \"bus\", \"transportation\").\noa_rel(\"can be\", \"jeans\", \"hung\").\noa_rel(\"is used for\", \"teddy bear\", \"entertainment\").\noa_rel(\"is used for\", \"supermarket\", \"buying and selling\").\noa_rel(\"can\", \"dog\", \"be pet\").", "oa_rel(\"is used for\", \"supermarket\", \"buying and selling\").\noa_rel(\"can\", \"dog\", \"be pet\").\noa_rel(\"is made from\", \"burger\", \"flour\").\noa_rel(\"is made from\", \"hamburger\", \"flour\").\noa_rel(\"is made from\", \"tea\", \"leaves of camellia sinensis\").\noa_rel(\"is\", \"peanut butter\", \"fluid\").\noa_rel(\"can\", \"subway\", \"transport many people at once\").\noa_rel(\"usually appears in\", \"toothpaste\", \"bathroom\").\noa_rel(\"is used for\", \"sailboat\", \"keeping people out of water\").\noa_rel(\"is a sub-event of\", \"making pizza\", \"rising dough\").", "oa_rel(\"is used for\", \"sailboat\", \"keeping people out of water\").\noa_rel(\"is a sub-event of\", \"making pizza\", \"rising dough\").\noa_rel(\"can\", \"monkeys\", \"throw things\").\noa_rel(\"is used for\", \"backyard\", \"planting flowers\").\noa_rel(\"is a sub-event of\", \"baking bread\", \"rising dough\").\noa_rel(\"requires\", \"cooking\", \"heat\").\noa_rel(\"can\", \"shark\", \"swim\").\noa_rel(\"can\", \"frog\", \"catch flies with its tongue\").\noa_rel(\"is used for\", \"computer\", \"enjoyment\").\noa_rel(\"is used for\", \"double decker\", \"transporting people\").", "oa_rel(\"is used for\", \"computer\", \"enjoyment\").\noa_rel(\"is used for\", \"double decker\", \"transporting people\").\noa_rel(\"usually appears in\", \"chalk\", \"classroom\").\noa_rel(\"can\", \"flamingo\", \"fly\").\noa_rel(\"requires\", \"peeling\", \"peeler\").\noa_rel(\"is used for\", \"condiment\", \"imparting specific flavor\").\noa_rel(\"usually appears in\", \"pork\", \"dinner\").\noa_rel(\"usually appears in\", \"knife block\", \"kitchen\").\noa_rel(\"is made from\", \"cheesecake\", \"flour\").\noa_rel(\"has\", \"lime\", \"vitamin C\").", "oa_rel(\"is made from\", \"cheesecake\", \"flour\").\noa_rel(\"has\", \"lime\", \"vitamin C\").\noa_rel(\"can\", \"ship\", \"travel over water\").\noa_rel(\"can\", \"bird\", \"perch\").\noa_rel(\"has\", \"wheat\", \"starch\").\noa_rel(\"is a sub-event of\", \"baking bread\", \"put batter in oven\").\noa_rel(\"usually appears in\", \"groom\", \"wedding\").\noa_rel(\"is\", \"video camera\", \"electric\").\noa_rel(\"is\", \"cream\", \"sweet\").\noa_rel(\"is used for\", \"cafe\", \"meeting people\").", "oa_rel(\"is\", \"cream\", \"sweet\").\noa_rel(\"is used for\", \"cafe\", \"meeting people\").\noa_rel(\"is used for\", \"freeway\", \"transportation\").\noa_rel(\"is used for\", \"toothpaste\", \"cleaning teeth\").\noa_rel(\"usually appears in\", \"teacher\", \"classroom\").\noa_rel(\"can\", \"dove\", \"fly\").\noa_rel(\"is used for\", \"laptop\", \"studying\").\noa_rel(\"has\", \"cereal\", \"starch\").\noa_rel(\"can be\", \"jacket\", \"hung\").\noa_rel(\"usually appears in\", \"monkey\", \"jungle\").", "oa_rel(\"can be\", \"jacket\", \"hung\").\noa_rel(\"usually appears in\", \"monkey\", \"jungle\").\noa_rel(\"is used for\", \"scooter\", \"transportation\").\noa_rel(\"is\", \"toothpaste\", \"sticky\").\noa_rel(\"is\", \"lotion\", \"fluid\").\noa_rel(\"usually appears in\", \"chopsticks\", \"restaurant\").\noa_rel(\"can\", \"hammer\", \"nail nail\").\noa_rel(\"is used for\", \"soap\", \"bathing\").\noa_rel(\"can be\", \"food\", \"cut\").\noa_rel(\"has\", \"mango\", \"vitamin C\").", "oa_rel(\"can be\", \"food\", \"cut\").\noa_rel(\"has\", \"mango\", \"vitamin C\").\noa_rel(\"is\", \"calf\", \"herbivorous\").\noa_rel(\"is\", \"cola\", \"fluid\").\noa_rel(\"usually appears in\", \"spoon\", \"restaurant\").\noa_rel(\"is used for\", \"drum\", \"making rhythm\").\noa_rel(\"is used for\", \"sugar\", \"flavoring food\").\noa_rel(\"is used for\", \"salt\", \"imparting specific flavor\").\noa_rel(\"can\", \"sedan\", \"transport people\").\noa_rel(\"usually appears in\", \"drinks\", \"dinner\").", "oa_rel(\"can\", \"sedan\", \"transport people\").\noa_rel(\"usually appears in\", \"drinks\", \"dinner\").\noa_rel(\"is used for\", \"comb\", \"combing hair\").\noa_rel(\"is\", \"donkey\", \"herbivorous\").\noa_rel(\"usually appears in\", \"tongs\", \"kitchen\").\noa_rel(\"is used for\", \"hairbrush\", \"styling hair\").\noa_rel(\"usually appears in\", \"comb\", \"bathroom\").\noa_rel(\"usually appears in\", \"tiger\", \"jungle\").\noa_rel(\"usually appears in\", \"mill\", \"barn\").\noa_rel(\"is a sub-event of\", \"baking cake\", \"gathering ingredients\").", "oa_rel(\"usually appears in\", \"mill\", \"barn\").\noa_rel(\"is a sub-event of\", \"baking cake\", \"gathering ingredients\").\noa_rel(\"can\", \"duck\", \"fly\").\noa_rel(\"can be\", \"duck\", \"roasted\").\noa_rel(\"usually appears in\", \"swan\", \"water\").\noa_rel(\"can\", \"minivan\", \"carry few persons\").\noa_rel(\"usually appears in\", \"santa\", \"christmas\").\noa_rel(\"is used for\", \"couch\", \"resting\").\noa_rel(\"is\", \"peanut butter\", \"sticky\").\noa_rel(\"requires\", \"mixing\", \"electric mixer\").", "oa_rel(\"is\", \"peanut butter\", \"sticky\").\noa_rel(\"requires\", \"mixing\", \"electric mixer\").\noa_rel(\"can\", \"duck\", \"swim\").\noa_rel(\"usually appears in\", \"pizza tray\", \"restaurant\").\noa_rel(\"is made from\", \"whipped cream\", \"milk\").\noa_rel(\"is\", \"guacamole\", \"fluid\").\noa_rel(\"can\", \"cat\", \"jump amazingly high\").\noa_rel(\"can\", \"swan\", \"fly\").\noa_rel(\"is used for\", \"bread\", \"eating\").\noa_rel(\"can be\", \"toast\", \"toasted\").", "oa_rel(\"is used for\", \"bread\", \"eating\").\noa_rel(\"can be\", \"toast\", \"toasted\").\noa_rel(\"usually appears in\", \"menu\", \"bar\").\noa_rel(\"can\", \"parrot\", \"imitate human voices\").\noa_rel(\"has\", \"milk\", \"calcium\").\noa_rel(\"has\", \"cappuccino\", \"caffeine\").\noa_rel(\"is\", \"caramel\", \"fluid\").\noa_rel(\"usually appears in\", \"computer\", \"office\").\noa_rel(\"is used for\", \"attic\", \"storing books\").\noa_rel(\"is used for\", \"phone\", \"listening to music\").", "oa_rel(\"is used for\", \"attic\", \"storing books\").\noa_rel(\"is used for\", \"phone\", \"listening to music\").\noa_rel(\"is\", \"hard drive\", \"electric\").\noa_rel(\"can be\", \"wine\", \"drunk\").\noa_rel(\"can\", \"kitten\", \"be pet\").\noa_rel(\"is used for\", \"cafe\", \"meeting friends\").\noa_rel(\"usually appears in\", \"wii\", \"living room\").\noa_rel(\"can\", \"flamingo\", \"be pet\").\noa_rel(\"is made from\", \"loaf\", \"flour\").\noa_rel(\"is used for\", \"laptop\", \"doing calculation\").", "oa_rel(\"is made from\", \"loaf\", \"flour\").\noa_rel(\"is used for\", \"laptop\", \"doing calculation\").\noa_rel(\"has\", \"milkshake\", \"water\").\noa_rel(\"can\", \"alligator\", \"swim\").\noa_rel(\"can\", \"laptop\", \"process information\").\noa_rel(\"can\", \"swan\", \"swim\").\noa_rel(\"is used for\", \"library\", \"borrowing books\").\noa_rel(\"usually appears in\", \"knife\", \"restaurant\").\noa_rel(\"is used for\", \"truck\", \"transporting people\").\noa_rel(\"is used for\", \"truck\", \"transportation\").", "oa_rel(\"is used for\", \"truck\", \"transporting people\").\noa_rel(\"is used for\", \"truck\", \"transportation\").\noa_rel(\"can\", \"goose\", \"swim\").\noa_rel(\"is\", \"ice cream\", \"fluid\").\noa_rel(\"requires\", \"baking bread\", \"flour\").\noa_rel(\"has\", \"champagne\", \"alcohol\").\noa_rel(\"is used for\", \"salt\", \"flavoring food\").\noa_rel(\"usually appears in\", \"goose\", \"water\").\noa_rel(\"usually appears in\", \"ostrich\", \"grassland\").\noa_rel(\"is used for\", \"camel\", \"transporting people\").", "oa_rel(\"usually appears in\", \"ostrich\", \"grassland\").\noa_rel(\"is used for\", \"camel\", \"transporting people\").\noa_rel(\"is used for\", \"grill\", \"cooking foods\").\noa_rel(\"is used for\", \"laptop\", \"finding information\").\noa_rel(\"is\", \"deer\", \"herbivorous\").\noa_rel(\"is used for\", \"backyard\", \"growing garden\").\noa_rel(\"can\", \"frog\", \"jump very high\").\noa_rel(\"can\", \"parrot\", \"fly\").\noa_rel(\"is used for\", \"drum\", \"hitting\").\noa_rel(\"has\", \"pepperoni\", \"vitamin B\").", "oa_rel(\"is used for\", \"drum\", \"hitting\").\noa_rel(\"has\", \"pepperoni\", \"vitamin B\").\noa_rel(\"is\", \"milk\", \"healthy\").\noa_rel(\"is used for\", \"mall\", \"meeting friends\").\noa_rel(\"can\", \"double decker\", \"travel on road\").\noa_rel(\"can\", \"van\", \"move quickly\").\noa_rel(\"is\", \"spear\", \"dangerous\").\noa_rel(\"usually appears in\", \"alligator\", \"water\").\noa_rel(\"has\", \"tofu\", \"iron\").\noa_rel(\"is used for\", \"chicken\", \"getting protein\").", "oa_rel(\"has\", \"tofu\", \"iron\").\noa_rel(\"is used for\", \"chicken\", \"getting protein\").\noa_rel(\"is used for\", \"motorcycle\", \"transportation\").\noa_rel(\"is used for\", \"grill\", \"barbecuing foods\").\noa_rel(\"usually appears in\", \"whisk\", \"kitchen\").\noa_rel(\"is used for\", \"hotel\", \"sleeping\").\noa_rel(\"is used for\", \"laptop\", \"playing games\").\noa_rel(\"is used for\", \"hammer\", \"pounding in nails\").\noa_rel(\"is used for\", \"shoe\", \"protecting feet\").\noa_rel(\"usually appears in\", \"toiletries\", \"bathroom\").", "oa_rel(\"is used for\", \"shoe\", \"protecting feet\").\noa_rel(\"usually appears in\", \"toiletries\", \"bathroom\").\noa_rel(\"is used for\", \"restaurant\", \"eating meal without cooking\").\noa_rel(\"requires\", \"cleaning house\", \"vacuum\").\noa_rel(\"can\", \"bee\", \"fly\").\noa_rel(\"is used for\", \"refrigerator\", \"storing foods\").\noa_rel(\"is\", \"goat\", \"herbivorous\").\noa_rel(\"requires\", \"baking cake\", \"cake pan\").\noa_rel(\"is made from\", \"pretzel\", \"flour\").\noa_rel(\"usually appears in\", \"alcohol\", \"bar\").", "oa_rel(\"is made from\", \"pretzel\", \"flour\").\noa_rel(\"usually appears in\", \"alcohol\", \"bar\").\noa_rel(\"is\", \"alcohol\", \"liquid\").\noa_rel(\"is\", \"champagne\", \"fluid\").\noa_rel(\"usually appears in\", \"dishes\", \"dining room\").\noa_rel(\"is used for\", \"laptop\", \"doing mathematical calculations\").\noa_rel(\"is\", \"owl\", \"carnivorous\").\noa_rel(\"can\", \"suv\", \"spend gas\").\noa_rel(\"usually appears in\", \"beer\", \"bar\").\noa_rel(\"can\", \"toaster\", \"brown bread\").", "oa_rel(\"usually appears in\", \"beer\", \"bar\").\noa_rel(\"can\", \"toaster\", \"brown bread\").\noa_rel(\"is used for\", \"water\", \"drinking\").\noa_rel(\"is used for\", \"screwdriver\", \"inserting screw\").\noa_rel(\"is used for\", \"salt\", \"adding taste to food\").\noa_rel(\"usually appears in\", \"octopus\", \"ocean\").\noa_rel(\"is\", \"blood\", \"liquid\").\noa_rel(\"has\", \"yogurt\", \"calcium\").\noa_rel(\"can\", \"helicopter\", \"fly\").\noa_rel(\"usually appears in\", \"computer desk\", \"office\").", "oa_rel(\"can\", \"helicopter\", \"fly\").\noa_rel(\"usually appears in\", \"computer desk\", \"office\").\noa_rel(\"is made from\", \"waffle\", \"flour\").\noa_rel(\"is made from\", \"brownie\", \"flour\").\noa_rel(\"is used for\", \"lotion\", \"moisturizing skin\").\noa_rel(\"has\", \"bagel\", \"starch\").\noa_rel(\"has\", \"peanut butter\", \"iron\").\noa_rel(\"is\", \"food processor\", \"electric\").\noa_rel(\"is\", \"olive oil\", \"fluid\").\noa_rel(\"can\", \"hummingbird\", \"fly\").", "oa_rel(\"is\", \"olive oil\", \"fluid\").\noa_rel(\"can\", \"hummingbird\", \"fly\").\noa_rel(\"usually appears in\", \"pepper shaker\", \"dining room\").\noa_rel(\"can\", \"frog\", \"be pet\").\noa_rel(\"is used for\", \"hotel\", \"staying on vacations\").\noa_rel(\"is used for\", \"train\", \"transportation\").\noa_rel(\"usually appears in\", \"waitress\", \"restaurant\").\noa_rel(\"is used for\", \"metal stand\", \"sitting on\").\noa_rel(\"requires\", \"coring\", \"corer\").\noa_rel(\"can\", \"van\", \"transport people\").", "oa_rel(\"requires\", \"coring\", \"corer\").\noa_rel(\"can\", \"van\", \"transport people\").\noa_rel(\"is\", \"champagne\", \"liquid\").\noa_rel(\"is used for\", \"backyard\", \"growing vegetables\").\noa_rel(\"can\", \"beaker\", \"measure liquid\").\noa_rel(\"is\", \"shampoo\", \"sticky\").\noa_rel(\"is\", \"moose\", \"herbivorous\").\noa_rel(\"requires\", \"baking cake\", \"dough\").\noa_rel(\"is\", \"whipped cream\", \"sweet\").\noa_rel(\"can\", \"gas stove\", \"heat eater\").", "oa_rel(\"is\", \"whipped cream\", \"sweet\").\noa_rel(\"can\", \"gas stove\", \"heat eater\").\noa_rel(\"usually appears in\", \"octopus\", \"water\").\noa_rel(\"is used for\", \"bicycle\", \"transportation\").\noa_rel(\"is\", \"kitten\", \"soft\").\noa_rel(\"can\", \"bartender\", \"mix classic cocktails\").\noa_rel(\"is\", \"cattle\", \"herbivorous\").\noa_rel(\"can\", \"cell phone\", \"make call\").\noa_rel(\"can\", \"can opener\", \"open cans\").\noa_rel(\"is\", \"rice cooker\", \"electric\").", "oa_rel(\"can\", \"can opener\", \"open cans\").\noa_rel(\"is\", \"rice cooker\", \"electric\").\noa_rel(\"is used for\", \"pantry\", \"storing food\").\noa_rel(\"is used for\", \"dishwasher\", \"washing dishes\").\noa_rel(\"is\", \"peach\", \"healthy\").\noa_rel(\"has\", \"peacock\", \"elaborate plumage\").\noa_rel(\"can be\", \"juice\", \"drunk\").\noa_rel(\"can\", \"rabbit\", \"be pet\").\noa_rel(\"is used for\", \"wine\", \"drinking\").\noa_rel(\"requires\", \"making coffee\", \"coffee beans\").", "oa_rel(\"is used for\", \"wine\", \"drinking\").\noa_rel(\"requires\", \"making coffee\", \"coffee beans\").\noa_rel(\"is used for\", \"drum\", \"playing music\").\noa_rel(\"is a sub-event of\", \"baking bread\", \"kneading dough\").\noa_rel(\"can\", \"police\", \"tail suspect\").\noa_rel(\"is used for\", \"suv\", \"transporting people\").\noa_rel(\"is used for\", \"pickup\", \"transporting handful of people\").\noa_rel(\"is\", \"milkshake\", \"liquid\").\noa_rel(\"is\", \"router\", \"electric\").\noa_rel(\"is a sub-event of\", \"making pizza\", \"shaping dough into thin circle\").", "oa_rel(\"is\", \"router\", \"electric\").\noa_rel(\"is a sub-event of\", \"making pizza\", \"shaping dough into thin circle\").\noa_rel(\"is\", \"bull\", \"herbivorous\").\noa_rel(\"usually appears in\", \"shark\", \"water\").\noa_rel(\"is used for\", \"stuffed animal\", \"entertainment\").\noa_rel(\"is\", \"hawk\", \"carnivorous\").\noa_rel(\"is used for\", \"sailboat\", \"transporting people\").\noa_rel(\"is\", \"shaving cream\", \"sticky\").\noa_rel(\"is used for\", \"banana\", \"eating\").\noa_rel(\"can\", \"chicken\", \"sing\").", "oa_rel(\"is used for\", \"banana\", \"eating\").\noa_rel(\"can\", \"chicken\", \"sing\").\noa_rel(\"is used for\", \"mitt\", \"protecting hand\").\noa_rel(\"usually appears in\", \"crab\", \"water\").\noa_rel(\"is\", \"lion\", \"carnivorous\").\noa_rel(\"usually appears in\", \"water glass\", \"restaurant\").\noa_rel(\"usually appears in\", \"dvd player\", \"living room\").\noa_rel(\"is used for\", \"listening to music\", \"entertainment\").\noa_rel(\"can\", \"robin\", \"winter down south\").\noa_rel(\"is\", \"shampoo\", \"fluid\").", "oa_rel(\"can\", \"robin\", \"winter down south\").\noa_rel(\"is\", \"shampoo\", \"fluid\").\noa_rel(\"is\", \"oil\", \"fluid\").\noa_rel(\"is\", \"tiger\", \"carnivorous\").\noa_rel(\"is\", \"bomb\", \"dangerous\").\noa_rel(\"can\", \"cereal\", \"provide complex carbohydrates\").\noa_rel(\"usually appears in\", \"printer\", \"office\").\noa_rel(\"has\", \"alcohol\", \"alcohol\").\noa_rel(\"is used for\", \"hairbrush\", \"combing hair\").\noa_rel(\"has\", \"almond\", \"calcium\").", "oa_rel(\"is used for\", \"hairbrush\", \"combing hair\").\noa_rel(\"has\", \"almond\", \"calcium\").\noa_rel(\"is\", \"lion\", \"dangerous\").\noa_rel(\"is used for\", \"projector\", \"showing films\").\noa_rel(\"usually appears in\", \"champagne\", \"wedding\").\noa_rel(\"is\", \"milk\", \"good for baby\").\noa_rel(\"usually appears in\", \"wok\", \"kitchen\").\noa_rel(\"is used for\", \"sugar\", \"sweetening coffee\").\noa_rel(\"usually appears in\", \"dish drainer\", \"kitchen\").\noa_rel(\"usually appears in\", \"baking pan\", \"kitchen\").", "oa_rel(\"usually appears in\", \"dish drainer\", \"kitchen\").\noa_rel(\"usually appears in\", \"baking pan\", \"kitchen\").\noa_rel(\"is\", \"alcohol\", \"fluid\").\noa_rel(\"is used for\", \"ruler\", \"measuring lengths\").\noa_rel(\"is used for\", \"beer\", \"drinking\").\noa_rel(\"can\", \"poodle\", \"be pet\").\noa_rel(\"is used for\", \"cafe\", \"drinking coffee\").\noa_rel(\"can\", \"theater\", \"show movie\").\noa_rel(\"requires\", \"washing dishes\", \"dish cloth\").\noa_rel(\"can\", \"puppy\", \"be pet\").", "oa_rel(\"requires\", \"washing dishes\", \"dish cloth\").\noa_rel(\"can\", \"puppy\", \"be pet\").\noa_rel(\"usually appears in\", \"shampoo\", \"bathroom\").\noa_rel(\"is\", \"blood\", \"fluid\").\noa_rel(\"is\", \"toothpaste\", \"fluid\").\noa_rel(\"usually appears in\", \"penguin\", \"water\").\noa_rel(\"can\", \"heater\", \"warm feet\").\noa_rel(\"is\", \"pear\", \"healthy\").\noa_rel(\"is\", \"toaster oven\", \"electric\").\noa_rel(\"is used for\", \"helicopter\", \"traversing skies\").", "oa_rel(\"is\", \"toaster oven\", \"electric\").\noa_rel(\"is used for\", \"helicopter\", \"traversing skies\").\noa_rel(\"is\", \"cooking oil\", \"fluid\").\noa_rel(\"requires\", \"frying\", \"cooking oil\").\noa_rel(\"is a sub-event of\", \"baking cake\", \"put cake pan in oven\").\noa_rel(\"usually appears in\", \"bartender\", \"bar\").\noa_rel(\"is\", \"guacamole\", \"sticky\").\noa_rel(\"can\", \"seal\", \"position itself on rock\").\noa_rel(\"is used for\", \"screwdriver\", \"installing or removing screws\").\noa_rel(\"usually appears in\", \"glue stick\", \"office\").", "oa_rel(\"is used for\", \"screwdriver\", \"installing or removing screws\").\noa_rel(\"usually appears in\", \"glue stick\", \"office\").\noa_rel(\"is made from\", \"egg roll\", \"flour\").\noa_rel(\"usually appears in\", \"pizza pan\", \"kitchen\").\noa_rel(\"is\", \"hummus\", \"fluid\").\noa_rel(\"can be\", \"hot dog\", \"eaten\").\noa_rel(\"usually appears in\", \"ruler\", \"office\").\noa_rel(\"is used for\", \"donut\", \"eating\").\noa_rel(\"has\", \"grains\", \"vitamin B\").\noa_rel(\"is\", \"liquor\", \"liquid\").", "oa_rel(\"has\", \"grains\", \"vitamin B\").\noa_rel(\"is\", \"liquor\", \"liquid\").\noa_rel(\"is\", \"liquor\", \"harmful\").\noa_rel(\"is used for\", \"helicopter\", \"transporting people\").\noa_rel(\"is used for\", \"backyard\", \"family activities\").\noa_rel(\"is\", \"lotion\", \"sticky\").\noa_rel(\"usually appears in\", \"staples\", \"office\").\noa_rel(\"can\", \"goose\", \"fly\").\noa_rel(\"is\", \"milkshake\", \"fluid\").\noa_rel(\"can\", \"eagle\", \"feed worms to its young\").", "oa_rel(\"is\", \"milkshake\", \"fluid\").\noa_rel(\"can\", \"eagle\", \"feed worms to its young\").\noa_rel(\"can\", \"peacock\", \"fly\").\noa_rel(\"usually appears in\", \"bride\", \"wedding\").\noa_rel(\"is a sub-event of\", \"mincing\", \"cutting ingredients into tinier pieces\").\noa_rel(\"is\", \"cola\", \"liquid\").\noa_rel(\"is\", \"rabbit\", \"herbivorous\").\noa_rel(\"is used for\", \"backyard\", \"children to play in\").\noa_rel(\"can\", \"penguin\", \"swim\").\noa_rel(\"is\", \"hand dryer\", \"electric\").", "oa_rel(\"can\", \"penguin\", \"swim\").\noa_rel(\"is\", \"hand dryer\", \"electric\").\noa_rel(\"is\", \"kitten\", \"carnivorous\").\noa_rel(\"has\", \"oyster\", \"iron\").\noa_rel(\"is used for\", \"camel\", \"riding\").\noa_rel(\"is\", \"gorilla\", \"omnivorous\").\noa_rel(\"can\", \"rat\", \"be pet\").\noa_rel(\"is used for\", \"truck\", \"transporting handful of people\").\noa_rel(\"is used for\", \"instruments\", \"making music\").\noa_rel(\"is\", \"lemonade\", \"fluid\").", "oa_rel(\"is used for\", \"instruments\", \"making music\").\noa_rel(\"is\", \"lemonade\", \"fluid\").\noa_rel(\"is used for\", \"backyard\", \"having barbecues\").\noa_rel(\"usually appears in\", \"trumpet\", \"orchestra\").\noa_rel(\"is used for\", \"laptop\", \"surfing internet\").\noa_rel(\"has\", \"cappuccino\", \"water\").\noa_rel(\"is\", \"caramel\", \"sticky\").\noa_rel(\"has\", \"cheesecake\", \"starch\").\noa_rel(\"is\", \"chocolate\", \"sweet\").\noa_rel(\"can\", \"bicycle\", \"travel on road\").", "oa_rel(\"is\", \"chocolate\", \"sweet\").\noa_rel(\"can\", \"bicycle\", \"travel on road\").\noa_rel(\"is used for\", \"backyard\", \"dogs to run around in\").\noa_rel(\"is used for\", \"wine\", \"satisfying thirst\").\noa_rel(\"is\", \"perfume\", \"liquid\").\noa_rel(\"usually appears in\", \"beer mug\", \"bar\").\noa_rel(\"can\", \"snake\", \"be pet\").\noa_rel(\"is\", \"rifle\", \"dangerous\").\noa_rel(\"can be\", \"coffee\", \"drunk\").\noa_rel(\"has\", \"tofu\", \"calcium\").", "oa_rel(\"can be\", \"coffee\", \"drunk\").\noa_rel(\"has\", \"tofu\", \"calcium\").\noa_rel(\"can\", \"air conditioner\", \"warm air\").\noa_rel(\"is used for\", \"pantry\", \"storing kitchen items\").\noa_rel(\"requires\", \"making pizza\", \"flour\").\noa_rel(\"is\", \"strawberry\", \"healthy\").\noa_rel(\"is used for\", \"laptop\", \"enjoyment\").\noa_rel(\"has\", \"waffle\", \"starch\").\noa_rel(\"usually appears in\", \"lion\", \"grassland\").\noa_rel(\"can\", \"eagle\", \"spot prey from afar\").", "oa_rel(\"usually appears in\", \"lion\", \"grassland\").\noa_rel(\"can\", \"eagle\", \"spot prey from afar\").\noa_rel(\"requires\", \"cleaning clothing\", \"detergent\").\noa_rel(\"usually appears in\", \"dolphin\", \"water\").\noa_rel(\"is\", \"pesto\", \"sticky\").\noa_rel(\"can\", \"duck\", \"attempt to fly\").\noa_rel(\"is\", \"tiger\", \"dangerous\").\noa_rel(\"is used for\", \"backyard\", \"having party\").\noa_rel(\"is\", \"lime\", \"healthy\").\noa_rel(\"has\", \"lemonade\", \"water\").", "oa_rel(\"is\", \"lime\", \"healthy\").\noa_rel(\"has\", \"lemonade\", \"water\").\noa_rel(\"can\", \"wheat\", \"provide complex carbohydrates\").\noa_rel(\"is\", \"yogurt\", \"fluid\").\noa_rel(\"usually appears in\", \"frog\", \"water\").\noa_rel(\"usually appears in\", \"bass\", \"rock band\").\noa_rel(\"is used for\", \"couch\", \"lying down\").\noa_rel(\"usually appears in\", \"lizard\", \"dessert\").\noa_rel(\"has\", \"liquor\", \"water\").\noa_rel(\"can\", \"jeep\", \"move quickly\").", "oa_rel(\"has\", \"liquor\", \"water\").\noa_rel(\"can\", \"jeep\", \"move quickly\").\noa_rel(\"usually appears in\", \"shark\", \"ocean\").\noa_rel(\"usually appears in\", \"mixing bowl\", \"kitchen\").\noa_rel(\"is\", \"ice maker\", \"electric\").\noa_rel(\"usually appears in\", \"hamburger\", \"lunch\").\noa_rel(\"is made from\", \"cheeseburger\", \"flour\").\noa_rel(\"is used for\", \"accordion\", \"polka music\").\noa_rel(\"is used for\", \"hammer\", \"pulling out nails\").\noa_rel(\"is a sub-event of\", \"baking cake\", \"stirring flour\").", "oa_rel(\"is used for\", \"hammer\", \"pulling out nails\").\noa_rel(\"is a sub-event of\", \"baking cake\", \"stirring flour\").\noa_rel(\"can be\", \"cake\", \"eaten\").\noa_rel(\"has\", \"chicken\", \"vitamin B\").\noa_rel(\"is used for\", \"water bottle\", \"holding drinks\").\noa_rel(\"is\", \"gummy bear\", \"sweet\").\noa_rel(\"is\", \"fast food\", \"unhealthy\").\noa_rel(\"usually appears in\", \"ape\", \"jungle\").\noa_rel(\"is used for\", \"lemon\", \"making juice\").\noa_rel(\"usually appears in\", \"hairbrush\", \"bathroom\").", "oa_rel(\"is used for\", \"lemon\", \"making juice\").\noa_rel(\"usually appears in\", \"hairbrush\", \"bathroom\").\noa_rel(\"can be\", \"water\", \"drunk\").\noa_rel(\"is used for\", \"cleat\", \"protecting feet\").\noa_rel(\"can\", \"penguin\", \"be pet\").\noa_rel(\"usually appears in\", \"pizza oven\", \"kitchen\").\noa_rel(\"has\", \"kangaroo\", \"two legs\").\noa_rel(\"is used for\", \"perfume\", \"aroma\").\noa_rel(\"requires\", \"baking cake\", \"flour\").\noa_rel(\"can be\", \"tea\", \"drunk\").", "oa_rel(\"requires\", \"baking cake\", \"flour\").\noa_rel(\"can be\", \"tea\", \"drunk\").\noa_rel(\"usually appears in\", \"whale\", \"ocean\").\noa_rel(\"is\", \"cappuccino\", \"fluid\").\noa_rel(\"is\", \"cappuccino\", \"liquid\").\noa_rel(\"is\", \"washing machine\", \"electric\").\noa_rel(\"can\", \"turtle\", \"be pet\").\noa_rel(\"can\", \"frog\", \"swim\").\noa_rel(\"is used for\", \"subway\", \"transporting lots of people\").\noa_rel(\"usually appears in\", \"champagne\", \"bar\").", "oa_rel(\"is used for\", \"subway\", \"transporting lots of people\").\noa_rel(\"usually appears in\", \"champagne\", \"bar\").\noa_rel(\"usually appears in\", \"skillet\", \"kitchen\").\noa_rel(\"is\", \"cotton candy\", \"sweet\").\noa_rel(\"usually appears in\", \"otter\", \"water\").\noa_rel(\"can\", \"robin\", \"fly\").\noa_rel(\"is used for\", \"boat\", \"transportation\").\noa_rel(\"is used for\", \"crane\", \"lifting heavy weight\").\noa_rel(\"usually appears in\", \"grater\", \"kitchen\").\noa_rel(\"is used for\", \"trumpet\", \"playing music\").", "oa_rel(\"usually appears in\", \"grater\", \"kitchen\").\noa_rel(\"is used for\", \"trumpet\", \"playing music\").\noa_rel(\"is used for\", \"cafeteria\", \"eating\").\noa_rel(\"is\", \"yogurt\", \"sweet\").\noa_rel(\"is used for\", \"mall\", \"shopping\").\noa_rel(\"is used for\", \"boot\", \"protecting feet\").\noa_rel(\"can\", \"laptop\", \"run programs\").\noa_rel(\"can be\", \"vest\", \"hung\").\noa_rel(\"is used for\", \"helmet\", \"protecting head\").\noa_rel(\"is\", \"vinegar\", \"liquid\").", "oa_rel(\"is used for\", \"helmet\", \"protecting head\").\noa_rel(\"is\", \"vinegar\", \"liquid\").\noa_rel(\"is used for\", \"beans\", \"eating\").\noa_rel(\"can\", \"lizard\", \"sun to warm up\").\noa_rel(\"is used for\", \"milk\", \"satisfying thirst\").\noa_rel(\"can\", \"pickup\", \"spend gas\").\noa_rel(\"can\", \"hawk\", \"fly\").\noa_rel(\"can\", \"police\", \"tail criminal\").\noa_rel(\"is\", \"fast food\", \"cheap\").\noa_rel(\"can\", \"violin\", \"play beautiful music\").", "oa_rel(\"is\", \"fast food\", \"cheap\").\noa_rel(\"can\", \"violin\", \"play beautiful music\").\noa_rel(\"usually appears in\", \"eagle\", \"jungle\").\noa_rel(\"usually appears in\", \"beer mug\", \"restaurant\").\noa_rel(\"is used for\", \"hairbrush\", \"removing tangles from hair\").\noa_rel(\"is\", \"seal\", \"carnivorous\").\noa_rel(\"is used for\", \"factory\", \"making products\").\noa_rel(\"usually appears in\", \"chopsticks\", \"dining room\").\noa_rel(\"is\", \"pudding\", \"sweet\").\noa_rel(\"is used for\", \"shampoo\", \"washing hair\").", "oa_rel(\"is\", \"pudding\", \"sweet\").\noa_rel(\"is used for\", \"shampoo\", \"washing hair\").\noa_rel(\"has\", \"oatmeal\", \"starch\").\noa_rel(\"has\", \"kangaroo\", \"two arms\").\noa_rel(\"can\", \"jeep\", \"carry few persons\").\noa_rel(\"can\", \"lizard\", \"be pet\").\noa_rel(\"is used for\", \"attic\", \"storing dishes\").\noa_rel(\"is\", \"liquor\", \"fluid\").\noa_rel(\"can\", \"minivan\", \"spend gas\").\noa_rel(\"usually appears in\", \"fax machine\", \"office\").", "oa_rel(\"can\", \"minivan\", \"spend gas\").\noa_rel(\"usually appears in\", \"fax machine\", \"office\").\noa_rel(\"can be\", \"polo shirt\", \"hung\").\noa_rel(\"is\", \"pizza oven\", \"electric\").\noa_rel(\"has\", \"gorilla\", \"two legs\").\noa_rel(\"can\", \"minivan\", \"transport people\").\noa_rel(\"can\", \"snake\", \"hurt\").\noa_rel(\"requires\", \"making pizza\", \"yeast\").\noa_rel(\"can\", \"snake\", \"be dangerous\").\noa_rel(\"usually appears in\", \"cake pan\", \"kitchen\").", "oa_rel(\"can\", \"snake\", \"be dangerous\").\noa_rel(\"usually appears in\", \"cake pan\", \"kitchen\").\noa_rel(\"usually appears in\", \"shaving cream\", \"bathroom\").\noa_rel(\"is\", \"crow\", \"omnivorous\").\noa_rel(\"usually appears in\", \"turtle\", \"water\").\noa_rel(\"is used for\", \"hairbrush\", \"brushing hair\").\noa_rel(\"is used for\", \"sneaker\", \"protecting feet\").\noa_rel(\"can\", \"rice cooker\", \"cooking rice\").\noa_rel(\"is used for\", \"attic\", \"storing things that are not used\").\noa_rel(\"can\", \"hen\", \"fly\").", "oa_rel(\"is used for\", \"attic\", \"storing things that are not used\").\noa_rel(\"can\", \"hen\", \"fly\").\noa_rel(\"is used for\", \"water\", \"satisfying thirst\").\noa_rel(\"can\", \"camel\", \"work for days without water\").\noa_rel(\"has\", \"champagne\", \"water\").\noa_rel(\"is made from\", \"milkshake\", \"milk\").\noa_rel(\"can\", \"crow\", \"fly\").\noa_rel(\"is used for\", \"beer\", \"satisfying thirst\").\noa_rel(\"can\", \"parrot\", \"be pet\").\noa_rel(\"is used for\", \"shampoo\", \"curing dandruff\").", "oa_rel(\"can\", \"parrot\", \"be pet\").\noa_rel(\"is used for\", \"shampoo\", \"curing dandruff\").\noa_rel(\"is used for\", \"screwdriver\", \"fixing loose screws\").\noa_rel(\"is used for\", \"jet\", \"transporting people\").\noa_rel(\"is used for\", \"guitar\", \"making music\").\noa_rel(\"can\", \"sedan\", \"spend gas\").\noa_rel(\"is used for\", \"sedan\", \"transporting people\").\noa_rel(\"is used for\", \"attic\", \"keeping stuff in\").\noa_rel(\"is\", \"pesto\", \"fluid\").\noa_rel(\"is used for\", \"violin\", \"playing music\").", "oa_rel(\"is\", \"pesto\", \"fluid\").\noa_rel(\"is used for\", \"violin\", \"playing music\").\noa_rel(\"can be\", \"soda\", \"drunk\").\noa_rel(\"can\", \"dolphin\", \"swim\").\noa_rel(\"is\", \"dark chocolate\", \"bitter\").\noa_rel(\"is used for\", \"cereal\", \"eating\").\noa_rel(\"has\", \"liquor\", \"alcohol\").\noa_rel(\"is\", \"wolf\", \"dangerous\").\noa_rel(\"has\", \"steak\", \"vitamin B\").\noa_rel(\"can\", \"bartender\", \"serve alcoholic or soft drink beverages\").", "oa_rel(\"has\", \"steak\", \"vitamin B\").\noa_rel(\"can\", \"bartender\", \"serve alcoholic or soft drink beverages\").\noa_rel(\"is used for\", \"drum\", \"making music\").\noa_rel(\"is made from\", \"bread loaf\", \"flour\").\noa_rel(\"is\", \"alpaca\", \"herbivorous\").\noa_rel(\"is\", \"vacuum\", \"electric\").\noa_rel(\"is used for\", \"fork\", \"moving food\").\noa_rel(\"is used for\", \"mall\", \"buying and selling\").\noa_rel(\"can\", \"chicken\", \"fly\").\noa_rel(\"usually appears in\", \"electric toothbrush\", \"bathroom\").", "oa_rel(\"can\", \"chicken\", \"fly\").\noa_rel(\"usually appears in\", \"electric toothbrush\", \"bathroom\").\noa_rel(\"usually appears in\", \"leopard\", \"jungle\").\noa_rel(\"usually appears in\", \"utensil holder\", \"kitchen\").\noa_rel(\"usually appears in\", \"drum\", \"orchestra\").\noa_rel(\"requires\", \"cooking\", \"ingredients\").\noa_rel(\"can\", \"fork\", \"lift food\").\noa_rel(\"can\", \"laptop\", \"save information\").\noa_rel(\"is used for\", \"baseball glove\", \"protecting hand\").\noa_rel(\"is used for\", \"attic\", \"storing clothes\").", "oa_rel(\"is used for\", \"baseball glove\", \"protecting hand\").\noa_rel(\"is used for\", \"attic\", \"storing clothes\").\noa_rel(\"can\", \"bald eagle\", \"fly\").\noa_rel(\"usually appears in\", \"banquet table\", \"restaurant\").\noa_rel(\"has\", \"bacon\", \"vitamin B\").\noa_rel(\"is used for\", \"laptop\", \"sending email\").\noa_rel(\"has\", \"ape\", \"two legs\").\noa_rel(\"can\", \"chicken\", \"spread wings\").\noa_rel(\"is\", \"shaving cream\", \"fluid\").\noa_rel(\"can\", \"duck\", \"lay eggs\").", "oa_rel(\"is\", \"shaving cream\", \"fluid\").\noa_rel(\"can\", \"duck\", \"lay eggs\").\noa_rel(\"has\", \"gorilla\", \"two arms\").\noa_rel(\"can be\", \"shorts\", \"hung\").\noa_rel(\"is\", \"vinegar\", \"fluid\").\noa_rel(\"can be\", \"beer\", \"drunk\").\noa_rel(\"is used for\", \"accordion\", \"playing music\").\noa_rel(\"can be\", \"pants\", \"hung\").\noa_rel(\"can\", \"pickup\", \"move quickly\").\noa_rel(\"is made from\", \"oreo\", \"flour\").", "oa_rel(\"can\", \"pickup\", \"move quickly\").\noa_rel(\"is made from\", \"oreo\", \"flour\").\noa_rel(\"is\", \"rabbit\", \"soft\").\noa_rel(\"is\", \"silk\", \"soft\").\noa_rel(\"is used for\", \"laptop\", \"data storage\").\noa_rel(\"can be\", \"milk\", \"drunk\").\noa_rel(\"usually appears in\", \"drum\", \"rock band\").\noa_rel(\"usually appears in\", \"wedding gown\", \"wedding\").\noa_rel(\"is used for\", \"soda\", \"satisfying thirst\").\noa_rel(\"is used for\", \"sandal\", \"protecting feet\").", "oa_rel(\"is used for\", \"soda\", \"satisfying thirst\").\noa_rel(\"is used for\", \"sandal\", \"protecting feet\").\noa_rel(\"is\", \"hippo\", \"herbivorous\").\noa_rel(\"is used for\", \"cafe\", \"eating cookies\").\noa_rel(\"can be\", \"skirt\", \"hung\").\noa_rel(\"can\", \"turtle\", \"live to be 200 years old\").\noa_rel(\"can\", \"laptop\", \"speed up research\").\noa_rel(\"has\", \"chicken liver\", \"iron\").\noa_rel(\"has\", \"beef liver\", \"iron\").\noa_rel(\"has\", \"rabbit\", \"two long ears\").", "oa_rel(\"has\", \"beef liver\", \"iron\").\noa_rel(\"has\", \"rabbit\", \"two long ears\").\noa_rel(\"has\", \"ape\", \"two arms\").\noa_rel(\"requires\", \"cleaning clothing\", \"washing powders\").\noa_rel(\"requires\", \"cleaning house\", \"mop\").\noa_rel(\"requires\", \"baking bread\", \"yeast\").\noa_rel(\"requires\", \"baking bread\", \"bread pan\").\noa_rel(\"requires\", \"beating egg\", \"electric mixer\").\noa_rel(\"is a sub-event of\", \"making pizza\", \"mixing flour, yeast, olive oil\").\noa_rel(\"is a sub-event of\", \"making pizza\", \"add sauce, cheese, meats, vegetables\").", "oa_rel(\"is a sub-event of\", \"making pizza\", \"mixing flour, yeast, olive oil\").\noa_rel(\"is a sub-event of\", \"making pizza\", \"add sauce, cheese, meats, vegetables\").\noa_rel(\"is a sub-event of\", \"making coffee\", \"grinding coffee beans\").\noa_rel(\"can be\", \"\", \"diced\").\noa_rel(\"can be\", \"\", \"sliced\").\noa_rel(\"can be\", \"\", \"minced\").\noa_rel(\"can be\", \"\", \"shred\").\noa_rel(\"can be\", \"\", \"baked\").\noa_rel(\"is made of\", \"sushi\", \"rice\").\noa_rel(\"is made from\", \"whey\", \"milk\").", "oa_rel(\"is made of\", \"sushi\", \"rice\").\noa_rel(\"is made from\", \"whey\", \"milk\").\noa_rel(\"is created by\", \"\", \"baking\").\noa_rel(\"is\", \"fast food\", \"bad for health\").\noa_rel(\"is\", \"doughnut\", \"sweet\").\noa_rel(\"is\", \"fudge\", \"sweet\").\noa_rel(\"is\", \"milk chocolate\", \"sweet\").\noa_rel(\"is\", \"cooking oil\", \"liquid\").\noa_rel(\"is\", \"hummus\", \"sticky\").\noa_rel(\"is\", \"leopard\", \"dangerous\").", "oa_rel(\"is\", \"hummus\", \"sticky\").\noa_rel(\"is\", \"leopard\", \"dangerous\").\noa_rel(\"is\", \"cheetah\", \"dangerous\").\noa_rel(\"is\", \"bison\", \"herbivorous\").\noa_rel(\"is\", \"ape\", \"omnivorous\").\noa_rel(\"is\", \"cheetah\", \"carnivorous\").\noa_rel(\"is\", \"raccoon\", \"carnivorous\").\noa_rel(\"is\", \"otter\", \"carnivorous\").\noa_rel(\"is\", \"copier\", \"electric\").\noa_rel(\"is used for\", \"fork\", \"piercing solid food\").", "oa_rel(\"is\", \"copier\", \"electric\").\noa_rel(\"is used for\", \"fork\", \"piercing solid food\").\noa_rel(\"is used for\", \"fork\", \"moving solid food\").\noa_rel(\"is used for\", \"spoon\", \"eating soft food\").\noa_rel(\"is used for\", \"spoon\", \"moving soft food to mouth\").\noa_rel(\"is used for\", \"spoon\", \"moving soft food\").\noa_rel(\"is used for\", \"spoon\", \"scooping soft food\").\noa_rel(\"is capable of\", \"microwave\", \"heating food\").\noa_rel(\"is used for\", \"harp\", \"playing music\").\noa_rel(\"can be used for\", \"drum\", \"playing in orchestra\").", "oa_rel(\"is used for\", \"harp\", \"playing music\").\noa_rel(\"can be used for\", \"drum\", \"playing in orchestra\").\noa_rel(\"can be used for\", \"wood\", \"campfires\").\noa_rel(\"is used for\", \"perfume\", \"perfuming\").\noa_rel(\"can\", \"steering wheel\", \"control vehicle\").\noa_rel(\"can\", \"steering wheel\", \"control direction of vehicle\").\noa_rel(\"can\", \"finch\", \"fly\").\noa_rel(\"can\", \"firefly\", \"fly\").\noa_rel(\"can\", \"snake\", \"bite\").\noa_rel(\"can\", \"lizard\", \"sun itself on rock\").", "oa_rel(\"can\", \"snake\", \"bite\").\noa_rel(\"can\", \"lizard\", \"sun itself on rock\").\noa_rel(\"can\", \"turtle\", \"swim\").\noa_rel(\"can\", \"otter\", \"swim\").\noa_rel(\"can\", \"starch\", \"provide complex carbohydrates\").\noa_rel(\"can\", \"starch\", \"provide energy\").\noa_rel(\"is good at\", \"chef\", \"cook food\").\noa_rel(\"is good at\", \"chef\", \"prepare food\").\noa_rel(\"is good at\", \"chef\", \"season food\").\noa_rel(\"is good at\", \"chef\", \"cook fish\").", "oa_rel(\"is good at\", \"chef\", \"season food\").\noa_rel(\"is good at\", \"chef\", \"cook fish\").\noa_rel(\"is good at\", \"chef\", \"season meat\").\noa_rel(\"can\", \"catcher\", \"catch baseball\").\noa_rel(\"can\", \"snowplows\", \"clear snow from roads\").\noa_rel(\"can\", \"cell phone\", \"vibrate\").\noa_rel(\"can\", \"helmet\", \"prevent head injuries\").\noa_rel(\"can\", \"spoon\", \"scoop food\").\noa_rel(\"usually appears in\", \"main course\", \"dinner\").\noa_rel(\"usually appears in\", \"side dishes\", \"dinner\").", "oa_rel(\"usually appears in\", \"main course\", \"dinner\").\noa_rel(\"usually appears in\", \"side dishes\", \"dinner\").\noa_rel(\"usually appears in\", \"fast food\", \"lunch\").\noa_rel(\"usually appears in\", \"soft drinks\", \"lunch\").\noa_rel(\"usually appears in\", \"violin\", \"orchestra\").\noa_rel(\"usually appears in\", \"clarinet\", \"orchestra\").\noa_rel(\"usually appears in\", \"trumpet\", \"brass band\").\noa_rel(\"usually appears in\", \"gas stove\", \"kitchen\").\noa_rel(\"usually appears in\", \"cooking pot\", \"kitchen\").\noa_rel(\"usually appears in\", \"beer mug\", \"dining room\").", "oa_rel(\"usually appears in\", \"cooking pot\", \"kitchen\").\noa_rel(\"usually appears in\", \"beer mug\", \"dining room\").\noa_rel(\"usually appears in\", \"barkeeper\", \"bar\").\noa_rel(\"usually appears in\", \"cocktail cabinet\", \"bar\").\noa_rel(\"usually appears in\", \"liquor\", \"bar\").\noa_rel(\"usually appears in\", \"restaurant table\", \"restaurant\").\noa_rel(\"usually appears in\", \"copier\", \"office\").\noa_rel(\"usually appears in\", \"camel\", \"dessert\").\noa_rel(\"usually appears in\", \"jaguar\", \"jungle\").\noa_rel(\"usually appears in\", \"gorilla\", \"jungle\").", "oa_rel(\"usually appears in\", \"jaguar\", \"jungle\").\noa_rel(\"usually appears in\", \"gorilla\", \"jungle\").\noa_rel(\"usually appears in\", \"bison\", \"grassland\").\noa_rel(\"usually appears in\", \"rhino\", \"grassland\").\noa_rel(\"usually appears in\", \"moose\", \"meadow\").\nis_a(person_type_01, person).\nis_a(person_type_02, person).\nis_a(person_type_03, person).\nis_a(person_type_04, person).\nis_a(person_type_05, person).", "is_a(person_type_04, person).\nis_a(person_type_05, person).\nis_a(man, person_type_01).\nis_a(woman, person_type_01).\nis_a(lady, person_type_02).\nis_a(gentleman, person_type_02).\nis_a(boy, person_type_03).\nis_a(girl, person_type_03).\nis_a(baby, person_type_04).\nis_a(toddler, person_type_04).", "is_a(baby, person_type_04).\nis_a(toddler, person_type_04).\nis_a(child, person_type_04).\nis_a(teenager, person_type_04).\nis_a(adult, person_type_04).\nis_a(elder, person_type_04).\nis_a(pedestrian, person_type_05).\nis_a(passenger, person_type_05).\nis_a(spectator, person_type_05).\nis_a(tourist, person_type_05).", "is_a(spectator, person_type_05).\nis_a(tourist, person_type_05).\nis_a(spectators, person_type_05).\nis_a(customers, person_type_05).\nis_a(visitor, person_type_05).\n\n'''\n\ndef npp_sub(match):\n\n    #print(match.group(0))\n    #print(match.group(0).replace(\" \", \"_\").replace('\"','').replace(\"-\",\"\"))\n    return match.group(0).replace(\" \", \"_\").replace('\"','').replace(\"-\",\"_\").replace(\",\",\"\")", "def npp_sub(match):\n\n    #print(match.group(0))\n    #print(match.group(0).replace(\" \", \"_\").replace('\"','').replace(\"-\",\"\"))\n    return match.group(0).replace(\" \", \"_\").replace('\"','').replace(\"-\",\"_\").replace(\",\",\"\")\n\n\nKG_REL = KG_REL.replace('\", \"','\"; \"').lower()\nKG_REL = re.sub(r\"\\\"[a-z \\-,]*\\\"\", npp_sub, KG_REL).replace(\";\",\",\").replace(\", ,\",\", \")\nKG_FACTS= KG_FACTS.replace('\", \"','\"; \"').lower()", "KG_REL = re.sub(r\"\\\"[a-z \\-,]*\\\"\", npp_sub, KG_REL).replace(\";\",\",\").replace(\", ,\",\", \")\nKG_FACTS= KG_FACTS.replace('\", \"','\"; \"').lower()\nKG_FACTS = re.sub(r\"\\\"[a-z \\-,]*\\\"\", npp_sub, KG_FACTS).replace(\";\",\",\").replace(\", ,\",\", \")\n\nKG = KG_REL + KG_FACTS\n\n\nRULES_old = '''is_a(A, C) :- is_a(A, B), is_a(B, C).\nis_a(N, \"thing\") :- name(N, T).\nname(N1, Oid) :- name(N2, Oid), is_a(N2, N1).", "is_a(N, \"thing\") :- name(N, T).\nname(N1, Oid) :- name(N2, Oid), is_a(N2, N1).\n\noa_rel(\"is a type of\", Obj, Attr) :- is_a(Obj, Attr).\noa_rel(R, Obj, Attr) :- in_oa_rel(R, Obj, Attr).\noa_rel(R, Obj, Attr) :- is_a(Obj, T), oa_rel(R, T, Attr).'''\n\n\n# RULES_OLD = '''\n# %sixRULES", "# RULES_OLD = '''\n# %sixRULES\n# is_a(A, C) :- is_a(A, B), is_a(B, C). \n# is_a(N, thing) :- name(N, T).\n# name(N1, Oid) :- name(N2, Oid), is_a(N2, N1).\n\n# oa_rel(is_a_type_of, Obj, Attr) :- is_a(Obj, Attr).\n# oa_rel(R, Obj, Attr) :- in_oa_rel(R, Obj, Attr).\n# oa_rel(R, Obj, Attr) :- is_a(Obj, T), oa_rel(R, T, Attr).\n# '''", "# oa_rel(R, Obj, Attr) :- is_a(Obj, T), oa_rel(R, T, Attr).\n# '''\n\nRULES = '''\n%sixRULES\nis_a(A, C) :- is_a(A, B), is_a(B, C). \nis_a(N, thing) :- name(T,N).\nname(Oid,N1) :- name(Oid,N2), is_a(N2,N1).\n\noa_rel(is_a_type_of, Obj, Attr) :- is_a(Obj, Attr).", "\noa_rel(is_a_type_of, Obj, Attr) :- is_a(Obj, Attr).\n%oa_rel(R, Obj, Attr) :- in_oa_rel(R, Obj, Attr).\noa_rel(R, Obj, Attr) :- is_a(Obj, T), oa_rel(R, T, Attr).\n'''\n\n\n\n\n", "\n\n\"\"\"\n['window', 'man', 'shirt', 'tree', 'wall', 'person', 'building', 'ground', 'sky', 'sign', 'head', 'pole', 'hand', 'grass', 'hair', 'leg', 'car', 'woman', 'leaves', 'table', 'trees', 'ear', 'pants', 'people', 'eye', 'door', 'water', 'fence', 'wheel', 'nose', 'chair', 'floor', 'arm', 'jacket', 'hat', 'shoe', 'tail', 'face', 'leaf', 'clouds', 'number', 'letter', 'plate', 'windows', 'shorts', 'road', 'sidewalk', 'flower', 'bag', 'helmet', 'snow', 'rock', 'boy', 'tire', 'logo', 'cloud', 'roof', 'glass', 'street', 'foot', 'legs', 'umbrella', 'post', 'jeans', 'mouth', 'boat', 'cap', 'bottle', 'girl', 'bush', 'shoes', 'flowers', 'glasses', 'field', 'picture', 'mirror', 'bench', 'box', 'bird', 'dirt', 'clock', 'neck', 'food', 'letters', 'bowl', 'shelf', 'bus', 'train', 'pillow', 'trunk', 'horse', 'plant', 'coat', 'airplane', 'lamp', 'wing', 'kite', 'elephant', 'paper', 'seat', 'dog', 'cup', 'house', 'counter', 'sheep', 'street light', 'glove', 'banana', 'branch', 'giraffe', 'rocks', 'cow', 'book', 'truck', 'racket', 'flag', 'ceiling', 'skateboard', 'cabinet', 'eyes', 'ball', 'zebra', 'bike', 'wheels', 'sand', 'hands', 'surfboard', 'frame', 'feet', 'windshield', 'finger', 'motorcycle', 'player', 'bushes', 'hill', 'child', 'bed', 'cat', 'sink', 'container', 'sock', 'tie', 'towel', 'traffic light', 'pizza', 'paw', 'backpack', 'collar', 'mountain', 'lid', 'basket', 'vase', 'phone', 'animal', 'sticker', 'branches', 'donut', 'lady', 'mane', 'license plate', 'cheese', 'fur', 'laptop', 'uniform', 'wire', 'fork', 'beach', 'wrist', 'buildings', 'word', 'desk', 'toilet', 'cars', 'curtain', 'pot', 'bear', 'ears', 'tag', 'dress', 'tower', 'faucet', 'screen', 'cell phone', 'watch', 'keyboard', 'arrow', 'sneakers', 'stone', 'blanket', 'broccoli', 'orange', 'numbers', 'drawer', 'knife', 'fruit', 'ocean', 't-shirt', 'cord', 'guy', 'spots', 'apple', 'napkin', 'cone', 'bread', 'bananas', 'sweater', 'cake', 'bicycle', 'skis', 'vehicle', 'room', 'couch', 'frisbee', 'horn', 'air', 'plants', 'trash can', 'camera', 'paint', 'ski', 'tomato', 'tiles', 'belt', 'words', 'television', 'wires', 'tray', 'socks', 'pipe', 'bat', 'rope', 'bathroom', 'carrot', 'suit', 'books', 'boot', 'sauce', 'ring', 'spoon', 'bricks', 'meat', 'van', 'bridge', 'goggles', 'platform', 'gravel', 'vest', 'label', 'stick', 'pavement', 'beak', 'refrigerator', 'computer', 'wetsuit', 'mountains', 'gloves', 'balcony', 'tree trunk', 'carpet', 'skirt', 'palm tree', 'fire hydrant', 'chain', 'kitchen', 'jersey', 'candle', 'remote control', 'shore', 'boots', 'rug', 'suitcase', 'computer mouse', 'clothes', 'street sign', 'pocket', 'outlet', 'can', 'snowboard', 'net', 'horns', 'pepper', 'doors', 'stairs', 'scarf', 'gate', 'graffiti', 'purse', 'luggage', 'beard', 'vegetables', 'bracelet', 'necklace', 'wristband', 'parking lot', 'park', 'train tracks', 'onion', 'dish', 'statue', 'sun', 'vegetable', 'sandwich', 'arms', 'star', 'doorway', 'wine', 'path', 'skier', 'teeth', 'men', 'stove', 'crust', 'weeds', 'chimney', 'chairs', 'feathers', 'monitor', 'home plate', 'speaker', 'fingers', 'steps', 'catcher', 'trash', 'forest', 'blinds', 'log', 'bathtub', 'eye glasses', 'outfit', 'cabinets', 'countertop', 'lettuce', 'pine tree', 'oven', 'city', 'walkway', 'jar', 'cart', 'tent', 'curtains', 'painting', 'dock', 'cockpit', 'step', 'pen', 'poster', 'light switch', 'hot dog', 'frosting', 'bucket', 'bun', 'pepperoni', 'crowd', 'thumb', 'propeller', 'symbol', 'liquid', 'tablecloth', 'pan', 'runway', 'train car', 'teddy bear', 'baby', 'microwave', 'headband', 'earring', 'store', 'spectator', 'fan', 'headboard', 'straw', 'stop sign', 'skin', 'pillows', 'display', 'couple', 'ladder', 'bottles', 'sprinkles', 'power lines', 'tennis ball', 'papers', 'decoration', 'burner', 'birds', 'umpire', 'grill', 'brush', 'cable', 'tongue', 'smoke', 'canopy', 'wings', 'controller', 'carrots', 'river', 'taxi', 'drain', 'spectators', 'sheet', 'game', 'chicken', 'american flag', 'mask', 'stones', 'batter', 'topping', 'wine glass', 'suv', 'tank top', 'scissors', 'barrier', 'hills', 'olive', 'planter', 'animals', 'plates', 'cross', 'mug', 'baseball', 'boats', 'telephone pole', 'mat', 'crosswalk', 'lips', 'mushroom', 'hay', 'toilet paper', 'tape', 'hillside', 'surfer', 'apples', 'donuts', 'station', 'mustache', 'children', 'pond', 'tires', 'toy', 'walls', 'flags', 'boxes', 'sofa', 'tomatoes', 'bags', 'sandals', 'onions', 'sandal', 'oranges', 'paws', 'pitcher', 'houses', 'baseball bat', 'moss', 'duck', 'apron', 'airport', 'light bulb', 'drawers', 'toothbrush', 'shelves', 'potato', 'light fixture', 'umbrellas', 'drink', 'heart', 'fence post', 'egg', 'hose', 'power line', 'fireplace', 'icing', 'nightstand', 'vehicles', 'magnet', 'beer', 'hook', 'comforter', 'lake', 'bookshelf', 'fries', 'peppers', 'coffee table', 'sweatshirt', 'shower', 'jet', 'water bottle', 'cows', 'entrance', 'driver', 'towels', 'soap', 'sail', 'crate', 'utensil', 'salad', 'kites', 'paddle', 'mound', 'tree branch']\n['white', 'black', 'green', 'blue', 'brown', 'red', 'gray', 'large', 'small', 'wooden', 'yellow', 'tall', 'metal', 'orange', 'long', 'dark', 'silver', 'pink', 'standing', 'clear', 'round', 'glass', 'open', 'sitting', 'short', 'parked', 'plastic', 'walking', 'brick', 'tan', 'purple', 'striped', 'colorful', 'cloudy', 'hanging', 'concrete', 'blond', 'bare', 'empty', 'young', 'old', 'closed', 'baseball', 'happy', 'bright', 'wet', 'gold', 'stone', 'smiling', 'light', 'dirty', 'flying', 'shiny', 'plaid', 'on', 'thin', 'square', 'tennis', 'little', 'sliced', 'leafy', 'playing', 'thick', 'beige', 'steel', 'calm', 'rectangular', 'dry', 'tiled', 'leather', 'eating', 'painted', 'ceramic', 'pointy', 'lying', 'surfing', 'snowy', 'paved', 'clean', 'fluffy', 'electric', 'cooked', 'grassy', 'stacked', 'full', 'covered', 'paper', 'framed', 'lit', 'blurry', 'grazing', 'flat', 'leafless', 'skiing', 'curved', 'light brown', 'beautiful', 'decorative', 'up', 'folded', 'sandy', 'chain-link', 'arched', 'overcast', 'cut', 'wide', 'running', 'waiting', 'ripe', 'long sleeved', 'furry', 'rusty', 'short sleeved', 'down', 'light blue', 'cloudless', 'dark brown', 'high', 'hazy', 'fresh', 'chocolate', 'cream colored', 'baby', 'worn', 'bent', 'light colored', 'rocky', 'skinny', 'curly', 'patterned', 'driving', 'jumping', 'maroon', 'riding', 'raised', 'lush', 'dark blue', 'off', 'cardboard', 'reflective', 'bald', 'iron', 'floral', 'black and white', 'melted', 'piled', 'skateboarding', 'rubber', 'talking', 'chrome', 'wire', 'puffy', 'broken', 'smooth', 'low', 'evergreen', 'narrow', 'denim', 'grouped', 'wicker', 'straight', 'triangular', 'sunny', 'dried', 'bushy', 'resting', 'sleeping', 'wrinkled', 'adult', 'dark colored', 'hairy', 'khaki', 'stuffed', 'wavy', 'nike', 'chopped', 'curled', 'shirtless', 'splashing', 'posing', 'upside down', 'ski', 'water', 'pointing', 'double decker', 'glazed', 'male', 'marble', 'fried', 'rock', 'ornate', 'wild', 'shining', 'tinted', 'asphalt', 'filled', 'floating', 'burnt', 'crossed', 'fuzzy', 'outdoors', 'overhead', 'potted', 'muddy', 'pale', 'decorated', 'swinging', 'asian', 'sharp', 'floppy', 'outstretched', 'rough', 'drinking', 'displayed', 'lined', 'having meeting', 'reflected', 'delicious', 'barefoot', 'plain', 'healthy', 'printed', 'frosted', 'crouching', 'written', 'illuminated', 'aluminum', 'digital', 'skating', 'trimmed', 'patchy', 'bending', 'soft', 'toasted', 'neon', 'choppy', 'fake', 'wispy', 'toy', 'knit', 'uncooked', 'vertical', 'tied', 'female', 'straw', 'grilled', 'rolled', 'rounded', 'wrapped', 'attached', 'messy', 'bathroom', 'swimming', 'deep', 'pretty', 'sleeveless', 'granite', 'rainbow colored', 'fallen', 'modern', 'kneeling', 'kitchen', 'turned', 'used', 'murky', 'busy', 'snowboarding', 'still', 'soccer', 'mounted', 'antique', 'street', 'watching', 'slanted', 'faded', 'glowing', 'teal', 'gravel', 'balding', 'outdoor', 'heavy', 'cracked', 'snow', 'baked', 'fancy', 'transparent', 'roman', 'vintage', 'old fashioned', 'cooking', 'horizontal', 'crouched', 'shredded', 'computer', 'docked', 'navy', 'octagonal', 'shallow', 'sparse', 'hard', 'cotton', 'fat', 'waving', 'carpeted', 'electronic', 'protective', 'foggy', 'rippled', 'cloth', 'squatting', 'shaggy', 'scattered', 'new', 'barren', 'stained', 'wireless', 'designed', 'reading', 'overgrown', 'apple', 'looking down', 'sliding', 'ocean', 'plush', 'tilted', 'shaded', 'dusty', 'flowered', 'crumpled', 'dotted', 'mesh', 'collared', 'traffic', 'woven', 'athletic', 'lighted', 'power', 'clay', 'made', 'crispy', 'elderly', 'bunched', 'warm', 'cluttered', 'brunette', 'textured', 'spread', 'brass', 'chinese', 'styrofoam', 'elevated', 'palm', 'blowing', 'copper', 'foamy', 'unripe', 'cheese', 'sheer', 'padded', 'half full', 'shaped', 'laughing', 'winter', 'manicured', 'indoors', 'speckled', 'batting', 'loose', 'caucasian', 'peeling', 'diced', 'dense', 'adidas', 'military', 'tin', 'halved', 'jeans', 'wired', 'mixed', 'bronze', 'alert', 'neat', 'public', 'drawn', 'cobblestone', 'massive', 'real', 'peeled', 'assorted', 'sunlit', 'chipped', 'blooming', 'american', 'perched', 'shaved', 'spiky', 'wool', 'bamboo', 'gloved', 'spraying', 'commercial', 'steep', 'braided', 'steamed', 'tight', 'partly cloudy', 'unpeeled', 'carved', 'damaged', 'edged', 'directional', 'strong', 'pulled back', 'wrinkly', 'muscular', 'tabby', 'mowed', 'disposable', 'portable', 'shut', 'looking up', 'sad', 'packed', 'rippling', 'vanilla', 'eaten', 'barbed', 'safety', 'shingled', 'roasted', 'juicy', 'pine', 'shadowed', 'diamond', 'crystal', 'angled', 'weathered', 'homemade', 'seasoned', 'paneled', 'sloped', 'sweet', 'miniature', 'domed', 'pizza', 'torn', 'crowded', 'tail', 'beaded', 'suspended', 'cushioned', 'tangled', 'stormy', 'blank', 'tasty', 'hitting', 'deciduous', 'frozen', 'license', 'fenced', 'sprinkled', 'feathered', 'dull', 'tropical', 'twisted', 'crooked', 'wine', 'browned', 'cylindrical', 'greasy', 'abandoned', 'capital', 'toilet', 'unlit', 'iced', 'upholstered', 'gloomy', 'professional', 'creamy', 'analog', 'jagged', 'oblong', 'ruffled', 'connected', 'ivory', 'christmas', 'upper', 'coffee', 'lace', 'ridged', 'comfortable', 'foreign', 'knotted', 'french', 'support', 'performing trick', 'artificial', 'spiral', 'crumbled', 'dangling', 'cordless', 'wedding', 'fire', 'rustic', 'trash', 'uneven', 'grated', 'curvy', 'telephone', 'unoccupied', 'curious', 'rotten', 'oriental', 'folding', 'crusty', 'rimmed', 'birthday', 'polished', 'funny', 'crashing', 'powdered', 'lower', 'winding', 'calico', 'banana', 'angry', 'inflated', 'scrambled', 'wii', 'vast', 'sleepy', 'rainy', 'groomed', 'glossy', 'wooded', 'sturdy', 'vibrant', 'misty', 'beer', 'office', 'melting', 'silk', 'discolored', 'corded', 'formal', 'vacant', 'oversized', 'vinyl', 'spinning', 'simple', 'translucent', 'mature', 'tomato', 'crisp', 'boiled', 'recessed', 'gas', 'handmade', 'clumped', 'railroad', 'rugged', 'sculpted', 'burning', 'patched', 'industrial', 'chubby', 'ugly', 'garbage', 'powerful', 'elongated', 'pepper', 'abundant', 'opaque', 'ornamental', 'fluorescent', 'soda', 'packaged', 'inflatable', 'unpaved', 'soap', 'strawberry', 'park', 'exterior', 'intricate', 'sealed', 'quilted', 'hollow', 'breakable', 'pastel', 'urban', 'wrist', 'breaking', 'regular', 'forested', 'abstract', 'unhappy', 'incomplete', 'fine', 'chalk', 'coarse', 'polo', 'unhealthy', 'irregular', 'polar', 'uncomfortable', 'typical', 'complete', 'scarce', 'immature']\n['right', 'left', 'on', 'wearing', 'near', 'in', 'of', 'behind', 'in front of', 'under', 'holding', 'with', 'by', 'on the side of', 'watching', 'inside', 'carrying', 'eating', 'at', 'riding', 'playing', 'covered by', 'covering', 'touching', 'on the front of', 'full of', 'riding on', 'using', 'covered in', 'sitting at', 'playing with', 'crossing', 'on the back of', 'surrounded by', 'reflected in', 'covered with', 'flying', 'pulled by', 'contain', 'standing by', 'driving on', 'surrounding', 'walking down', 'printed on', 'attached to', 'talking on', 'facing', 'driving', 'worn on', 'floating in', 'grazing on', 'on the bottom of', 'standing in front of', 'topped with', 'playing in', 'walking with', 'swimming in', 'driving down', 'pushed by', 'playing on', 'close to', 'waiting for', 'between', 'running on', 'tied to', 'on the edge of', 'holding onto', 'talking to', 'eating from', 'perched on', 'parked by', 'reaching for', 'connected to', 'grazing in', 'floating on', 'wrapped around', 'skiing on', 'stacked on', 'parked at', 'standing at', 'walking across', 'plugged into', 'stuck on', 'stuck in', 'drinking from', 'seen through', 'sitting by', 'sitting in front of', 'looking out', 'parked in front of', 'petting', 'wrapped in', 'selling', 'parked along', 'coming from', 'lying inside', 'sitting inside', 'walking by', 'sitting with', 'making', 'walking through', 'hung on', 'walking along', 'going down', 'leaving', 'running in', 'flying through', 'mounted to', 'on the other side of', 'licking', 'sniffing', 'followed by', 'following', 'riding in', 'biting', 'parked alongside', 'chasing', 'leading', 'hanging off', 'helping', 'coming out of', 'hanging out of', 'staring at', 'walking toward', 'served on', 'hugging', 'entering', 'skiing in', 'looking in', 'draped over', 'tied around', 'exiting', 'looking down at', 'looking into', 'drawn on', 'balancing on', 'jumping over', 'posing with', 'reflecting in', 'eating at', 'walking up', 'sewn on', 'reflected on', 'about to hit', 'getting on', 'observing', 'approaching', 'traveling on', 'walking towards', 'wading in', 'growing by', 'displayed on', 'growing along', 'mixed with', 'grabbing', 'jumping on', 'scattered on', 'climbing', 'pointing at', 'opening', 'taller than', 'going into', 'decorated by', 'decorating', 'preparing', 'coming down', 'tossing', 'eating in', 'growing from', 'chewing', 'washing', 'herding', 'skiing down', 'looking through', 'picking up', 'trying to catch', 'standing against', 'looking over', 'typing on', 'piled on', 'tying', 'shining through', 'smoking', 'cleaning', 'walking to', 'smiling at', 'guiding', 'dragging', 'chained to', 'going through', 'enclosing', 'adjusting', 'running through', 'skating on', 'photographing', 'smelling', 'kissing', 'falling off', 'decorated with', 'walking into', 'worn around', 'walking past', 'blowing out', 'towing', 'jumping off', 'sprinkled on', 'moving', 'running across', 'hidden by', 'traveling down', 'splashing', 'hang from', 'looking toward', 'kept in', 'cooked in', 'displayed in', 'sitting atop', 'brushing', 'buying', 'in between', 'larger than', 'smaller than', 'pouring', 'playing at', 'longer than', 'higher than', 'jumping in', 'shorter than', 'bigger than']\n\n\"\"\"\n\n\"\"\" NAMES", "\n\"\"\" NAMES\n0 window\n1 man\n2 shirt\n3 tree\n4 wall\n5 person\n6 building\n7 ground", "6 building\n7 ground\n8 sky\n9 sign\n10 head\n11 pole\n12 hand\n13 grass\n14 hair\n15 leg", "14 hair\n15 leg\n16 car\n17 woman\n18 leaves\n19 table\n20 trees\n21 ear\n22 pants\n23 people", "22 pants\n23 people\n24 eye\n25 door\n26 water\n27 fence\n28 wheel\n29 nose\n30 chair\n31 floor", "30 chair\n31 floor\n32 arm\n33 jacket\n34 hat\n35 shoe\n36 tail\n37 face\n38 leaf\n39 clouds", "38 leaf\n39 clouds\n40 number\n41 letter\n42 plate\n43 windows\n44 shorts\n45 road\n46 sidewalk\n47 flower", "46 sidewalk\n47 flower\n48 bag\n49 helmet\n50 snow\n51 rock\n52 boy\n53 tire\n54 logo\n55 cloud", "54 logo\n55 cloud\n56 roof\n57 glass\n58 street\n59 foot\n60 legs\n61 umbrella\n62 post\n63 jeans", "62 post\n63 jeans\n64 mouth\n65 boat\n66 cap\n67 bottle\n68 girl\n69 bush\n70 shoes\n71 flowers", "70 shoes\n71 flowers\n72 glasses\n73 field\n74 picture\n75 mirror\n76 bench\n77 box\n78 bird\n79 dirt", "78 bird\n79 dirt\n80 clock\n81 neck\n82 food\n83 letters\n84 bowl\n85 shelf\n86 bus\n87 train", "86 bus\n87 train\n88 pillow\n89 trunk\n90 horse\n91 plant\n92 coat\n93 airplane\n94 lamp\n95 wing", "94 lamp\n95 wing\n96 kite\n97 elephant\n98 paper\n99 seat\n100 dog\n101 cup\n102 house\n103 counter", "102 house\n103 counter\n104 sheep\n105 street light\n106 glove\n107 banana\n108 branch\n109 giraffe\n110 rocks\n111 cow", "110 rocks\n111 cow\n112 book\n113 truck\n114 racket\n115 flag\n116 ceiling\n117 skateboard\n118 cabinet\n119 eyes", "118 cabinet\n119 eyes\n120 ball\n121 zebra\n122 bike\n123 wheels\n124 sand\n125 hands\n126 surfboard\n127 frame", "126 surfboard\n127 frame\n128 feet\n129 windshield\n130 finger\n131 motorcycle\n132 player\n133 bushes\n134 hill\n135 child", "134 hill\n135 child\n136 bed\n137 cat\n138 sink\n139 container\n140 sock\n141 tie\n142 towel\n143 traffic light", "142 towel\n143 traffic light\n144 pizza\n145 paw\n146 backpack\n147 collar\n148 mountain\n149 lid\n150 basket\n151 vase", "150 basket\n151 vase\n152 phone\n153 animal\n154 sticker\n155 branches\n156 donut\n157 lady\n158 mane\n159 license plate", "158 mane\n159 license plate\n160 cheese\n161 fur\n162 laptop\n163 uniform\n164 wire\n165 fork\n166 beach\n167 wrist", "166 beach\n167 wrist\n168 buildings\n169 word\n170 desk\n171 toilet\n172 cars\n173 curtain\n174 pot\n175 bear", "174 pot\n175 bear\n176 ears\n177 tag\n178 dress\n179 tower\n180 faucet\n181 screen\n182 cell phone\n183 watch", "182 cell phone\n183 watch\n184 keyboard\n185 arrow\n186 sneakers\n187 stone\n188 blanket\n189 broccoli\n190 orange\n191 numbers", "190 orange\n191 numbers\n192 drawer\n193 knife\n194 fruit\n195 ocean\n196 t-shirt\n197 cord\n198 guy\n199 spots", "198 guy\n199 spots\n200 apple\n201 napkin\n202 cone\n203 bread\n204 bananas\n205 sweater\n206 cake\n207 bicycle", "206 cake\n207 bicycle\n208 skis\n209 vehicle\n210 room\n211 couch\n212 frisbee\n213 horn\n214 air\n215 plants", "214 air\n215 plants\n216 trash can\n217 camera\n218 paint\n219 ski\n220 tomato\n221 tiles\n222 belt\n223 words", "222 belt\n223 words\n224 television\n225 wires\n226 tray\n227 socks\n228 pipe\n229 bat\n230 rope\n231 bathroom", "230 rope\n231 bathroom\n232 carrot\n233 suit\n234 books\n235 boot\n236 sauce\n237 ring\n238 spoon\n239 bricks", "238 spoon\n239 bricks\n240 meat\n241 van\n242 bridge\n243 goggles\n244 platform\n245 gravel\n246 vest\n247 label", "246 vest\n247 label\n248 stick\n249 pavement\n250 beak\n251 refrigerator\n252 computer\n253 wetsuit\n254 mountains\n255 gloves", "254 mountains\n255 gloves\n256 balcony\n257 tree trunk\n258 carpet\n259 skirt\n260 palm tree\n261 fire hydrant\n262 chain\n263 kitchen", "262 chain\n263 kitchen\n264 jersey\n265 candle\n266 remote control\n267 shore\n268 boots\n269 rug\n270 suitcase\n271 computer mouse", "270 suitcase\n271 computer mouse\n272 clothes\n273 street sign\n274 pocket\n275 outlet\n276 can\n277 snowboard\n278 net\n279 horns", "278 net\n279 horns\n280 pepper\n281 doors\n282 stairs\n283 scarf\n284 gate\n285 graffiti\n286 purse\n287 luggage", "286 purse\n287 luggage\n288 beard\n289 vegetables\n290 bracelet\n291 necklace\n292 wristband\n293 parking lot\n294 park\n295 train tracks", "294 park\n295 train tracks\n296 onion\n297 dish\n298 statue\n299 sun\n300 vegetable\n301 sandwich\n302 arms\n303 star", "302 arms\n303 star\n304 doorway\n305 wine\n306 path\n307 skier\n308 teeth\n309 men\n310 stove\n311 crust", "310 stove\n311 crust\n312 weeds\n313 chimney\n314 chairs\n315 feathers\n316 monitor\n317 home plate\n318 speaker\n319 fingers", "318 speaker\n319 fingers\n320 steps\n321 catcher\n322 trash\n323 forest\n324 blinds\n325 log\n326 bathtub\n327 eye glasses", "326 bathtub\n327 eye glasses\n328 outfit\n329 cabinets\n330 countertop\n331 lettuce\n332 pine tree\n333 oven\n334 city\n335 walkway", "334 city\n335 walkway\n336 jar\n337 cart\n338 tent\n339 curtains\n340 painting\n341 dock\n342 cockpit\n343 step", "342 cockpit\n343 step\n344 pen\n345 poster\n346 light switch\n347 hot dog\n348 frosting\n349 bucket\n350 bun\n351 pepperoni", "350 bun\n351 pepperoni\n352 crowd\n353 thumb\n354 propeller\n355 symbol\n356 liquid\n357 tablecloth\n358 pan\n359 runway", "358 pan\n359 runway\n360 train car\n361 teddy bear\n362 baby\n363 microwave\n364 headband\n365 earring\n366 store\n367 spectator", "366 store\n367 spectator\n368 fan\n369 headboard\n370 straw\n371 stop sign\n372 skin\n373 pillows\n374 display\n375 couple", "374 display\n375 couple\n376 ladder\n377 bottles\n378 sprinkles\n379 power lines\n380 tennis ball\n381 papers\n382 decoration\n383 burner", "382 decoration\n383 burner\n384 birds\n385 umpire\n386 grill\n387 brush\n388 cable\n389 tongue\n390 smoke\n391 canopy", "390 smoke\n391 canopy\n392 wings\n393 controller\n394 carrots\n395 river\n396 taxi\n397 drain\n398 spectators\n399 sheet", "398 spectators\n399 sheet\n400 game\n401 chicken\n402 american flag\n403 mask\n404 stones\n405 batter\n406 topping\n407 wine glass", "406 topping\n407 wine glass\n408 suv\n409 tank top\n410 scissors\n411 barrier\n412 hills\n413 olive\n414 planter\n415 animals", "414 planter\n415 animals\n416 plates\n417 cross\n418 mug\n419 baseball\n420 boats\n421 telephone pole\n422 mat\n423 crosswalk", "422 mat\n423 crosswalk\n424 lips\n425 mushroom\n426 hay\n427 toilet paper\n428 tape\n429 hillside\n430 surfer\n431 apples", "430 surfer\n431 apples\n432 donuts\n433 station\n434 mustache\n435 children\n436 pond\n437 tires\n438 toy\n439 walls", "438 toy\n439 walls\n440 flags\n441 boxes\n442 sofa\n443 tomatoes\n444 bags\n445 sandals\n446 onions\n447 sandal", "446 onions\n447 sandal\n448 oranges\n449 paws\n450 pitcher\n451 houses\n452 baseball bat\n453 moss\n454 duck\n455 apron", "454 duck\n455 apron\n456 airport\n457 light bulb\n458 drawers\n459 toothbrush\n460 shelves\n461 potato\n462 light fixture\n463 umbrellas", "462 light fixture\n463 umbrellas\n464 drink\n465 heart\n466 fence post\n467 egg\n468 hose\n469 power line\n470 fireplace\n471 icing", "470 fireplace\n471 icing\n472 nightstand\n473 vehicles\n474 magnet\n475 beer\n476 hook\n477 comforter\n478 lake\n479 bookshelf", "478 lake\n479 bookshelf\n480 fries\n481 peppers\n482 coffee table\n483 sweatshirt \n484 shower\n485 jet\n486 water bottle\n487 cows", "486 water bottle\n487 cows\n488 entrance\n489 driver\n490 towels\n491 soap\n492 sail\n493 crate\n494 utensil\n495 salad", "494 utensil\n495 salad\n496 kites\n497 paddle\n498 mound\n499 tree branch\n\"\"\"\n\n\"\"\" attributes\n0 white", "\"\"\" attributes\n0 white\n1 black\n2 green\n3 blue\n4 brown\n5 red\n6 gray\n7 large\n8 small", "7 large\n8 small\n9 wooden\n10 yellow\n11 tall\n12 metal\n13 orange\n14 long\n15 dark\n16 silver", "15 dark\n16 silver\n17 pink\n18 standing\n19 clear\n20 round\n21 glass\n22 open\n23 sitting\n24 short", "23 sitting\n24 short\n25 parked\n26 plastic\n27 walking\n28 brick\n29 tan\n30 purple\n31 striped\n32 colorful", "31 striped\n32 colorful\n33 cloudy\n34 hanging\n35 concrete\n36 blond\n37 bare\n38 empty\n39 young\n40 old", "39 young\n40 old\n41 closed\n42 baseball\n43 happy\n44 bright\n45 wet\n46 gold\n47 stone\n48 smiling", "47 stone\n48 smiling\n49 light\n50 dirty\n51 flying\n52 shiny\n53 plaid\n54 on\n55 thin\n56 square", "55 thin\n56 square\n57 tennis\n58 little\n59 sliced\n60 leafy\n61 playing\n62 thick\n63 beige\n64 steel", "63 beige\n64 steel\n65 calm\n66 rectangular\n67 dry\n68 tiled\n69 leather\n70 eating\n71 painted\n72 ceramic", "71 painted\n72 ceramic\n73 pointy\n74 lying\n75 surfing\n76 snowy\n77 paved\n78 clean\n79 fluffy\n80 electric", "79 fluffy\n80 electric\n81 cooked\n82 grassy\n83 stacked\n84 full\n85 covered\n86 paper\n87 framed\n88 lit", "87 framed\n88 lit\n89 blurry\n90 grazing\n91 flat\n92 leafless\n93 skiing\n94 curved\n95 light brown\n96 beautiful", "95 light brown\n96 beautiful\n97 decorative\n98 up\n99 folded\n100 sandy\n101 chain-link\n102 arched\n103 overcast\n104 cut", "103 overcast\n104 cut\n105 wide\n106 running\n107 waiting\n108 ripe\n109 long sleeved\n110 furry\n111 rusty\n112 short sleeved", "111 rusty\n112 short sleeved\n113 down\n114 light blue\n115 cloudless\n116 dark brown\n117 high\n118 hazy\n119 fresh\n120 chocolate", "119 fresh\n120 chocolate\n121 cream colored\n122 baby\n123 worn\n124 bent\n125 light colored\n126 rocky\n127 skinny\n128 curly", "127 skinny\n128 curly\n129 patterned\n130 driving\n131 jumping\n132 maroon\n133 riding\n134 raised\n135 lush\n136 dark blue", "135 lush\n136 dark blue\n137 off\n138 cardboard\n139 reflective\n140 bald\n141 iron\n142 floral\n143 black and white\n144 melted", "143 black and white\n144 melted\n145 piled\n146 skateboarding\n147 rubber\n148 talking\n149 chrome\n150 wire\n151 puffy\n152 broken", "151 puffy\n152 broken\n153 smooth\n154 low\n155 evergreen\n156 narrow\n157 denim\n158 grouped\n159 wicker\n160 straight", "159 wicker\n160 straight\n161 triangular\n162 sunny\n163 dried\n164 bushy\n165 resting\n166 sleeping\n167 wrinkled\n168 adult", "167 wrinkled\n168 adult\n169 dark colored\n170 hairy\n171 khaki\n172 stuffed\n173 wavy\n174 nike\n175 chopped\n176 curled", "175 chopped\n176 curled\n177 shirtless\n178 splashing\n179 posing\n180 upside down\n181 ski\n182 water\n183 pointing\n184 double decker", "183 pointing\n184 double decker\n185 glazed\n186 male\n187 marble\n188 fried\n189 rock\n190 ornate\n191 wild\n192 shining", "191 wild\n192 shining\n193 tinted\n194 asphalt\n195 filled\n196 floating\n197 burnt\n198 crossed\n199 fuzzy\n200 outdoors", "199 fuzzy\n200 outdoors\n201 overhead\n202 potted\n203 muddy\n204 pale\n205 decorated\n206 swinging\n207 asian\n208 sharp", "207 asian\n208 sharp\n209 floppy\n210 outstretched\n211 rough\n212 drinking\n213 displayed\n214 lined\n215 having meeting\n216 reflected", "215 having meeting\n216 reflected\n217 delicious\n218 barefoot\n219 plain\n220 healthy\n221 printed\n222 frosted\n223 crouching\n224 written", "223 crouching\n224 written\n225 illuminated\n226 aluminum\n227 digital\n228 skating\n229 trimmed\n230 patchy\n231 bending\n232 soft", "231 bending\n232 soft\n233 toasted\n234 neon\n235 choppy\n236 fake\n237 wispy\n238 toy\n239 knit\n240 uncooked", "239 knit\n240 uncooked\n241 vertical\n242 tied\n243 female\n244 straw\n245 grilled\n246 rolled\n247 rounded\n248 wrapped", "247 rounded\n248 wrapped\n249 attached\n250 messy\n251 bathroom\n252 swimming\n253 deep\n254 pretty\n255 sleeveless\n256 granite", "255 sleeveless\n256 granite\n257 rainbow colored\n258 fallen\n259 modern\n260 kneeling\n261 kitchen\n262 turned\n263 used\n264 murky", "263 used\n264 murky\n265 busy\n266 snowboarding\n267 still\n268 soccer\n269 mounted\n270 antique\n271 street\n272 watching", "271 street\n272 watching\n273 slanted\n274 faded\n275 glowing\n276 teal\n277 gravel\n278 balding\n279 outdoor\n280 heavy", "279 outdoor\n280 heavy\n281 cracked\n282 snow\n283 baked\n284 fancy\n285 transparent\n286 roman\n287 vintage\n288 old fashioned", "287 vintage\n288 old fashioned\n289 cooking\n290 horizontal\n291 crouched\n292 shredded\n293 computer\n294 docked\n295 navy\n296 octagonal", "295 navy\n296 octagonal\n297 shallow\n298 sparse\n299 hard\n300 cotton\n301 fat\n302 waving\n303 carpeted\n304 electronic", "303 carpeted\n304 electronic\n305 protective\n306 foggy\n307 rippled\n308 cloth\n309 squatting\n310 shaggy\n311 scattered\n312 new", "311 scattered\n312 new\n313 barren\n314 stained\n315 wireless\n316 designed\n317 reading\n318 overgrown\n319 apple\n320 looking down", "319 apple\n320 looking down\n321 sliding\n322 ocean\n323 plush\n324 tilted\n325 shaded\n326 dusty\n327 flowered\n328 crumpled", "327 flowered\n328 crumpled\n329 dotted\n330 mesh\n331 collared\n332 traffic\n333 woven\n334 athletic\n335 lighted\n336 power", "335 lighted\n336 power\n337 clay\n338 made\n339 crispy\n340 elderly\n341 bunched\n342 warm\n343 cluttered\n344 brunette", "343 cluttered\n344 brunette\n345 textured\n346 spread\n347 brass\n348 chinese\n349 styrofoam\n350 elevated\n351 palm\n352 blowing", "351 palm\n352 blowing\n353 copper\n354 foamy\n355 unripe\n356 cheese\n357 sheer\n358 padded\n359 half full\n360 shaped", "359 half full\n360 shaped\n361 laughing\n362 winter\n363 manicured\n364 indoors\n365 speckled\n366 batting\n367 loose\n368 caucasian", "367 loose\n368 caucasian\n369 peeling\n370 diced\n371 dense\n372 adidas\n373 military\n374 tin\n375 halved\n376 jeans", "375 halved\n376 jeans\n377 wired\n378 mixed\n379 bronze\n380 alert\n381 neat\n382 public\n383 drawn\n384 cobblestone", "383 drawn\n384 cobblestone\n385 massive\n386 real\n387 peeled\n388 assorted\n389 sunlit\n390 chipped\n391 blooming\n392 american", "391 blooming\n392 american\n393 perched\n394 shaved\n395 spiky\n396 wool\n397 bamboo\n398 gloved\n399 spraying\n400 commercial", "399 spraying\n400 commercial\n401 steep\n402 braided\n403 steamed\n404 tight\n405 partly cloudy\n406 unpeeled\n407 carved\n408 damaged", "407 carved\n408 damaged\n409 edged\n410 directional\n411 strong\n412 pulled back\n413 wrinkly\n414 muscular\n415 tabby\n416 mowed", "415 tabby\n416 mowed\n417 disposable\n418 portable\n419 shut\n420 looking up\n421 sad\n422 packed\n423 rippling\n424 vanilla", "423 rippling\n424 vanilla\n425 eaten\n426 barbed\n427 safety\n428 shingled\n429 roasted\n430 juicy\n431 pine\n432 shadowed", "431 pine\n432 shadowed\n433 diamond\n434 crystal\n435 angled\n436 weathered\n437 homemade\n438 seasoned\n439 paneled\n440 sloped", "439 paneled\n440 sloped\n441 sweet\n442 miniature\n443 domed\n444 pizza\n445 torn\n446 crowded\n447 tail\n448 beaded", "447 tail\n448 beaded\n449 suspended\n450 cushioned\n451 tangled\n452 stormy\n453 blank\n454 tasty\n455 hitting\n456 deciduous", "455 hitting\n456 deciduous\n457 frozen\n458 license\n459 fenced\n460 sprinkled\n461 feathered\n462 dull\n463 tropical\n464 twisted", "463 tropical\n464 twisted\n465 crooked\n466 wine\n467 browned\n468 cylindrical\n469 greasy\n470 abandoned\n471 capital\n472 toilet", "471 capital\n472 toilet\n473 unlit\n474 iced\n475 upholstered\n476 gloomy\n477 professional\n478 creamy\n479 analog\n480 jagged", "479 analog\n480 jagged\n481 oblong\n482 ruffled\n483 connected\n484 ivory\n485 christmas\n486 upper\n487 coffee\n488 lace", "487 coffee\n488 lace\n489 ridged\n490 comfortable\n491 foreign\n492 knotted\n493 french\n494 support\n495 performing trick\n496 artificial", "495 performing trick\n496 artificial\n497 spiral\n498 crumbled\n499 dangling\n500 cordless\n501 wedding\n502 fire\n503 rustic\n504 trash", "503 rustic\n504 trash\n505 uneven\n506 grated\n507 curvy\n508 telephone\n509 unoccupied\n510 curious\n511 rotten\n512 oriental", "511 rotten\n512 oriental\n513 folding\n514 crusty\n515 rimmed\n516 birthday\n517 polished\n518 funny\n519 crashing\n520 powdered", "519 crashing\n520 powdered\n521 lower\n522 winding\n523 calico\n524 banana\n525 angry\n526 inflated\n527 scrambled\n528 wii", "527 scrambled\n528 wii\n529 vast\n530 sleepy\n531 rainy\n532 groomed\n533 glossy\n534 wooded\n535 sturdy\n536 vibrant", "535 sturdy\n536 vibrant\n537 misty\n538 beer\n539 office\n540 melting\n541 silk\n542 discolored\n543 corded\n544 formal", "543 corded\n544 formal\n545 vacant\n546 oversized\n547 vinyl\n548 spinning\n549 simple\n550 translucent\n551 mature\n552 tomato", "551 mature\n552 tomato\n553 crisp\n554 boiled\n555 recessed\n556 gas\n557 handmade\n558 clumped\n559 railroad\n560 rugged", "559 railroad\n560 rugged\n561 sculpted\n562 burning\n563 patched\n564 industrial\n565 chubby\n566 ugly\n567 garbage\n568 powerful", "567 garbage\n568 powerful\n569 elongated\n570 pepper\n571 abundant\n572 opaque\n573 ornamental\n574 fluorescent\n575 soda\n576 packaged", "575 soda\n576 packaged\n577 inflatable\n578 unpaved\n579 soap\n580 strawberry\n581 park\n582 exterior\n583 intricate\n584 sealed", "583 intricate\n584 sealed\n585 quilted\n586 hollow\n587 breakable\n588 pastel\n589 urban\n590 wrist\n591 breaking\n592 regular", "591 breaking\n592 regular\n593 forested\n594 abstract\n595 unhappy\n596 incomplete\n597 fine\n598 chalk\n599 coarse\n600 polo", "599 coarse\n600 polo\n601 unhealthy\n602 irregular\n603 polar\n604 uncomfortable\n605 typical\n606 complete\n607 scarce\n608 immature", "607 scarce\n608 immature\n\"\"\"\n\n\"\"\" RELATIONS\n0 right\n1 left\n2 on\n3 wearing\n4 near", "3 wearing\n4 near\n5 in\n6 of\n7 behind\n8 in front of\n9 under\n10 holding\n11 with\n12 by", "11 with\n12 by\n13 on the side of\n14 watching\n15 inside\n16 carrying\n17 eating\n18 at\n19 riding\n20 playing", "19 riding\n20 playing\n21 covered by\n22 covering\n23 touching\n24 on the front of\n25 full of\n26 riding on\n27 using\n28 covered in", "27 using\n28 covered in\n29 sitting at\n30 playing with\n31 crossing\n32 on the back of\n33 surrounded by\n34 reflected in\n35 covered with\n36 flying", "35 covered with\n36 flying\n37 pulled by\n38 contain\n39 standing by\n40 driving on\n41 surrounding\n42 walking down\n43 printed on\n44 attached to", "43 printed on\n44 attached to\n45 talking on\n46 facing\n47 driving\n48 worn on\n49 floating in\n50 grazing on\n51 on the bottom of\n52 standing in front of", "51 on the bottom of\n52 standing in front of\n53 topped with\n54 playing in\n55 walking with\n56 swimming in\n57 driving down\n58 pushed by\n59 playing on\n60 close to", "59 playing on\n60 close to\n61 waiting for\n62 between\n63 running on\n64 tied to\n65 on the edge of\n66 holding onto\n67 talking to\n68 eating from", "67 talking to\n68 eating from\n69 perched on\n70 parked by\n71 reaching for\n72 connected to\n73 grazing in\n74 floating on\n75 wrapped around\n76 skiing on", "75 wrapped around\n76 skiing on\n77 stacked on\n78 parked at\n79 standing at\n80 walking across\n81 plugged into\n82 stuck on\n83 stuck in\n84 drinking from", "83 stuck in\n84 drinking from\n85 seen through\n86 sitting by\n87 sitting in front of\n88 looking out\n89 parked in front of\n90 petting\n91 wrapped in\n92 selling", "91 wrapped in\n92 selling\n93 parked along\n94 coming from\n95 lying inside\n96 sitting inside\n97 walking by\n98 sitting with\n99 making\n100 walking through", "99 making\n100 walking through\n101 hung on\n102 walking along\n103 going down\n104 leaving\n105 running in\n106 flying through\n107 mounted to\n108 on the other side of", "107 mounted to\n108 on the other side of\n109 licking\n110 sniffing\n111 followed by\n112 following\n113 riding in\n114 biting\n115 parked alongside\n116 chasing", "115 parked alongside\n116 chasing\n117 leading\n118 hanging off\n119 helping\n120 coming out of\n121 hanging out of\n122 staring at\n123 walking toward\n124 served on", "123 walking toward\n124 served on\n125 hugging\n126 entering\n127 skiing in\n128 looking in\n129 draped over\n130 tied around\n131 exiting\n132 looking down at", "131 exiting\n132 looking down at\n133 looking into\n134 drawn on\n135 balancing on\n136 jumping over\n137 posing with\n138 reflecting in\n139 eating at\n140 walking up", "139 eating at\n140 walking up\n141 sewn on\n142 reflected on\n143 about to hit\n144 getting on\n145 observing\n146 approaching\n147 traveling on\n148 walking towards", "147 traveling on\n148 walking towards\n149 wading in\n150 growing by\n151 displayed on\n152 growing along\n153 mixed with\n154 grabbing\n155 jumping on\n156 scattered on", "155 jumping on\n156 scattered on\n157 climbing\n158 pointing at\n159 opening\n160 taller than\n161 going into\n162 decorated by\n163 decorating\n164 preparing", "163 decorating\n164 preparing\n165 coming down\n166 tossing\n167 eating in\n168 growing from\n169 chewing\n170 washing\n171 herding\n172 skiing down", "171 herding\n172 skiing down\n173 looking through\n174 picking up\n175 trying to catch\n176 standing against\n177 looking over\n178 typing on\n179 piled on\n180 tying", "179 piled on\n180 tying\n181 shining through\n182 smoking\n183 cleaning\n184 walking to\n185 smiling at\n186 guiding\n187 dragging\n188 chained to", "187 dragging\n188 chained to\n189 going through\n190 enclosing\n191 adjusting\n192 running through\n193 skating on\n194 photographing\n195 smelling\n196 kissing", "195 smelling\n196 kissing\n197 falling off\n198 decorated with\n199 walking into\n200 worn around\n201 walking past\n202 blowing out\n203 towing\n204 jumping off", "203 towing\n204 jumping off\n205 sprinkled on\n206 moving\n207 running across\n208 hidden by\n209 traveling down\n210 splashing\n211 hang from\n212 looking toward", "211 hang from\n212 looking toward\n213 kept in\n214 cooked in\n215 displayed in\n216 sitting atop\n217 brushing\n218 buying\n219 in between\n220 larger than", "219 in between\n220 larger than\n221 smaller than\n222 pouring\n223 playing at\n224 longer than\n225 higher than\n226 jumping in\n227 shorter than\n228 bigger than", "227 shorter than\n228 bigger than\n\"\"\""]}
{"filename": "src/experiments/vqa/network_nn.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nclass Net_nn(nn.Module):\n    def __init__(self):\n        super(Net_nn, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 6, 5),  # 6 is the output chanel size; 5 is the kernal size; 1 (chanel) 28 28 -> 6 24 24\n            nn.MaxPool2d(2, 2),  # kernal size 2; stride size 2; 6 24 24 -> 6 12 12\n            nn.ReLU(True),       # inplace=True means that it will modify the input directly thus save memory\n            nn.Conv2d(6, 16, 5), # 6 12 12 -> 16 8 8\n            nn.MaxPool2d(2, 2),  # 16 8 8 -> 16 4 4\n            nn.ReLU(True) \n        )\n        self.classifier =  nn.Sequential(\n            nn.Linear(16 * 4 * 4, 120),\n            nn.ReLU(),\n            nn.Linear(120, 84),\n            nn.ReLU(),\n            nn.Linear(84, 10),\n            nn.Softmax(1)\n        )\n\n    def forward(self, x, marg_idx=None, type=1):\n        \n        assert type == 1, \"only posterior computations are available for this network\"\n\n        # If the list of the pixel numbers to be marginalised is given,\n        # then genarate a marginalisation mask from it and apply to the\n        # tensor 'x'\n        if marg_idx:\n            batch_size = x.shape[0]\n            with torch.no_grad():\n                marg_mask = torch.ones_like(x, device=x.device).reshape(batch_size, 1, -1)\n                marg_mask[:, :, marg_idx] = 0\n                marg_mask = marg_mask.reshape_as(x)\n                marg_mask.requires_grad_(False)\n            x = torch.einsum('ijkl,ijkl->ijkl', x, marg_mask)\n        x = self.encoder(x)\n        x = x.view(-1, 16 * 4 * 4)\n        x = self.classifier(x)\n        return x", ""]}
{"filename": "src/experiments/vqa/models.py", "chunked_list": ["\"\"\"\nThe source code is based on:\nScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\nJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\nAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\nhttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\"\"\"\n\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport sys\ntorch.autograd.set_detect_anomaly(True)   \n\nsys.path.append('../../EinsumNetworks/src/')\nfrom einsum_wrapper import EiNet", "sys.path.append('../../EinsumNetworks/src/')\nfrom einsum_wrapper import EiNet\n\n\nclass MLPClassifier(nn.Module):\n    def __init__(self, input_dim, latent_dim, output_dim, n_layers, dropout_rate, softmax):\n        super(MLPClassifier, self).__init__()\n\n        self.softmax = softmax\n        self.output_dim = output_dim\n\n        layers = []\n        layers.append(nn.Linear(input_dim, latent_dim))\n        layers.append(nn.ReLU())\n        layers.append(nn.BatchNorm1d(latent_dim))\n        #layers.append(nn.InstanceNorm1d(latent_dim))\n        layers.append(nn.Dropout(dropout_rate))\n        for _ in range(n_layers - 1):\n            layers.append(nn.Linear(latent_dim, latent_dim))\n            layers.append(nn.ReLU())\n            layers.append(nn.BatchNorm1d(latent_dim))\n            #layers.append(nn.InstanceNorm1d(latent_dim))\n            layers.append(nn.Dropout(dropout_rate))\n        layers.append(nn.Linear(latent_dim, output_dim))\n\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x, marg_idx=None, type=None):\n        if x.sum() == 0:\n            return torch.ones([x.shape[0], self.output_dim], device='cuda')\n\n        idx = x.sum(dim=1)!=0 # get idx of true objects\n        logits = torch.zeros(x.shape[0], self.output_dim, device='cuda')\n\n        logits[idx] = self.net(x[idx])\n        \n\n        if self.softmax:\n            probs = F.softmax(logits, dim=1)\n        else:\n            probs = torch.sigmoid(logits)\n\n        return probs", "\n# FasterCNN object feature size\nfeature_dim = 2048 \n\nname_clf = MLPClassifier(\n    input_dim=feature_dim,\n    output_dim=500,\n    latent_dim=1024,\n    n_layers=2,\n    dropout_rate=0.3,", "    n_layers=2,\n    dropout_rate=0.3,\n    softmax=True\n)\n\nrela_clf = MLPClassifier(\n    input_dim=(feature_dim+4)*2,  \n    output_dim=229,       \n    latent_dim=1024,\n    n_layers=1,", "    latent_dim=1024,\n    n_layers=1,\n    dropout_rate=0.5,\n    softmax=True\n)\n\n\nattr_clf = MLPClassifier(\n    input_dim=feature_dim,\n    output_dim=609,", "    input_dim=feature_dim,\n    output_dim=609,\n    latent_dim=1024,\n    n_layers=1,\n    dropout_rate=0.3,\n    softmax=False\n)\n\n\n", "\n\nname_einet = EiNet(structure = 'poon-domingos',\n                      pd_num_pieces = [4],\n                      use_em = False,\n                      num_var = 2048,\n                      class_count = 500,\n                      pd_width = 32,\n                      pd_height = 64,\n                      learn_prior = True)", "                      pd_height = 64,\n                      learn_prior = True)\nrela_einet = EiNet(structure = 'poon-domingos',\n                      pd_num_pieces = [4],\n                      use_em = False,\n                      num_var = 4104,\n                      class_count = 229,\n                      pd_width = 72,\n                      pd_height = 57,\n                      learn_prior = True)", "                      pd_height = 57,\n                      learn_prior = True)\n\nattr_einet = EiNet(structure = 'poon-domingos',\n                      pd_num_pieces = [4],\n                      use_em = False,\n                      num_var = 2048,\n                      class_count = 609,\n                      pd_width = 32,\n                      pd_height = 64,", "                      pd_width = 32,\n                      pd_height = 64,\n                      learn_prior = True)"]}
{"filename": "src/experiments/vqa/__init__.py", "chunked_list": [""]}
{"filename": "src/experiments/vqa/dataGen.py", "chunked_list": ["import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport json\nimport pickle\nimport re\nimport itertools\n\nfrom trainer import ClauseNTrainer  # the class for training, testing, etc.\nfrom preprocess import Query  # the abstract class to transform the strings of VQA to an abstraction of a query", "from trainer import ClauseNTrainer  # the class for training, testing, etc.\nfrom preprocess import Query  # the abstract class to transform the strings of VQA to an abstraction of a query\n\nfrom tqdm import tqdm\n\ndef get_SLASH_npp(outcomes:list, tag:str) -> str:\n    \"\"\" Function to generate a valid NPP given a list of outcomes\n        Inputs:\n            outcomes<list>: list of strings entailing the outcomes\n            tag<str>: the name tag of the npp\n        Outputs:\n            npp<str>: string descibing a valid NPP\n    \"\"\"\n    if tag == \"relation\":\n        str_outcomes = \", \"\n        str_outcomes = str_outcomes.join([f'{outcome.replace(\" \", \"_\").replace(\"-\", \"_\")}' for outcome in outcomes])\n        npp = \"\".join([f\"npp({tag}(1,X), \", \"[\", str_outcomes , \"]) :- object(X1,X2).\"])\n\n    else: \n        str_outcomes = \", \"\n        str_outcomes = str_outcomes.join([f'{outcome.replace(\" \", \"_\").replace(\"-\", \"_\")}' for outcome in outcomes])\n        npp = \"\".join([f\"npp({tag}(1,X), \", \"[\", str_outcomes , \"]) :- object(X).\"])\n\n\n\n    return npp", "\ndef get_SLASH_query(query_content:str , target_objects= None,non_target_objects=None, show_filter=None, num_objects=0) -> str:\n    \"\"\" Given the string of SCALLOP query rewrite it to a SLASH variant.\n        Inputs:\n            query_content<str>: string entailing SCALLOP's version of a query.\n        Outputs:\n            main_rule_and_query<str>: SLASH' version of the given query\n    \"\"\"\n\n    #adjust VQAR scallop format to SLASH by deleting the query(target(O)). entry.\n    tmp = query_content.split(\"\\n\")\n    main_rule = \"\\n\".join([t for t in tmp[:-1] if t!=''])\n    \n    #store attributes and relations from the query here\n    attr_rel_filter = \"\"\n\n    #add target constraints for training.\n    query = \":- target(X), target(Y), X != Y.\\n\" #display only one target per solution\n    if target_objects is not None:\n\n        #if an answer exists add them as a constraint. One of them has to be in a solution\n        if len(target_objects) > 0:\n            query += \":-\"\n\n            for target_object in target_objects:\n                query += \" not target({});\".format(int(target_object))\n\n            query= query[:-1] +\".\\n\"\n\n        #if non answer objects exist add them as a constraint.\n        #Solution is UNSAT if it contains one of these answers.\n        for non_target_object in non_target_objects: \n            query += \":- target({}).\".format(int(non_target_object))\n        \n        query += \"\\n\"\n\n        #show statement to disable all printing\n        show_statements = \"#show. \\n\"\n\n    #add show statements for testing\n    else: \n        show_statements = \"#show target/1.\\n\"\n\n\n    #add show statements for training and testing\n    #to build showstatements we use a list of tuples with target head and body as a tuple\n    # example: ('O2', ['attr(dirty, O2)', 'relation(left, O1, O2)',...]) as an intermediate \n    # we call this intermediate representation show filter and transform it afterwards to the show statements\n\n    #if we have two different rules (and/or)\n    if len(show_filter)== 2:\n        if show_filter[0][2] != show_filter[1][2]:\n            show_filter[0][1].extend(show_filter[1][1])\n            show_filter = [show_filter[0]]\n\n\n    #iterate over all show filters\n    for filter in show_filter: #filter[0] is the target variable\n\n        #iterate over the body\n        for var_descr in filter[1]:\n            \n            #filter attributes and relation predicates\n            matches_attr_rel = re.findall(r'(relation|attr)\\(([ a-zA-Z0-9]*|[ a-zA-Z0-9]*,[ a-zA-Z0-9]*),([-_ a-z]*)\\)',var_descr )\n\n            #if we found a relation or attribute iterate over them\n            for match in matches_attr_rel:\n\n                #every show statement/filter corresponds to one subgoal of the target query.\n                #We want to have them seperate so we use a choice rule to show subgoals seperatly\n\n                attr_rel_filter = attr_rel_filter + \"///\" +str(match[2]).replace(\" \",\"\")#extend the attribute/relation filter\n\n                #for relations we only want one relation combination to fulfills the target requirement\n                #we add a count constraint for this\n                if match[0] == 'relation' :#and target_objects is not None:\n                    query += \":- #count {{ {}(0,1,{},{}): target({}), {}}} > 1.\\n\".format(match[0],match[1],match[2],filter[0], \", \".join(filter[1]))\n\n                #now build the ASP readable show statement\n                show_statements += \"#show {}(0,1,{},{}): target({}), {}.\\n\".format(match[0],match[1],match[2],filter[0], \", \".join(filter[1]))\n\n            #filter names and add the showstatements\n            matches_name = re.findall(r'name\\(([ a-zA-Z0-9]*),([ -_a-zA-Z0-9]*)\\)',var_descr)\n            for match in matches_name:\n                show_statements += \"#show name(0,1,{}, X) : target({}), name(0,1,{},X), {}, {}.\\n\".format(match[0],  filter[0], match[0], \", \".join(filter[1]), \"s(0)\")\n\n\n    #correct wrong KG translations\n    def correct_kg_intput(str):\n        return str.replace(\"vitamin_B\", \"vitamin_b\").replace(\"even-toed\",\"even_toed\").replace(\"sub-event\",\"sub_event\").replace(\"odd-toed\",\"odd_toed\").replace('t-shirt','t_shirt').replace(\"chain-link\",\"chain_link\").replace(\"-ungulate\",\"_ungulate\")\n\n    query = correct_kg_intput(query)\n\n    #join all together in one query\n    query_rule_and_show_statements = \"\".join([\"%\\t Target rule: \\n\", main_rule,\"\\n\",query,  \"\\n%\\tShow statement:\\n\", show_statements,\"\\n\"] )\n\n    query_rule_and_show_statements = correct_kg_intput(query_rule_and_show_statements)\n\n    return query_rule_and_show_statements, attr_rel_filter", "\n\nmeta_f = \"dataset/gqa_info.json\"  # Meta file - Graph Question Answering information\nwith open (meta_f, 'r') as meta_f:\n    meta_info = json.load(meta_f)\n\n\n\n#load the set of names, attributes and relations\nnames = list(meta_info['name']['canon'].keys())", "#load the set of names, attributes and relations\nnames = list(meta_info['name']['canon'].keys())\nattributes = list(meta_info['attr']['canon'].keys())\nrelations = list(meta_info['rel']['canon'].keys())\n\n#names= [\"giraffe\",\"tigger\",\"lion\"] #debug\n#relations = [\"left\", \"right\",\"above\"] #debug\n#attributes = [\"gren\", \"blue\",\"striped\",\"shiny\"] #debug\n\n", "\n\n#NPPS\nname_npp = get_SLASH_npp(names, \"name\")\nrelation_npp = get_SLASH_npp(relations, \"relation\")\nattribute_npp = get_SLASH_npp(attributes, \"attr\")\n\n\nclass VQA(Dataset):\n    '''\n    VQA dataset consisting of:\n    objects\n    flag_addinf: The flag for getting dictionary with additional informations for visualisations.\n    '''\n\n\n    def __init__(self, dataset,  task_file, num_objects =None, flag_addinf=False):\n        self.data = list()\n        self.dataset = dataset\n        self.flag_addinf = flag_addinf\n        self.max_objects = 0\n\n        self.dataset_size = 0\n\n        with open (task_file, 'rb') as tf:\n            tasks = pickle.load(tf)\n\n        print(\"dataloading done\")\n        \n\n        self.object_list = []\n        self.query_list = []\n        self.feature_and_bb_list = []\n        self.num_true_objects = []\n        self.targets = []\n        self.additional_information_list = []\n        \n        self.attr_rel_filter_list = []\n\n        #if num_objects is not None:\n        self.num_objects = num_objects\n        print(\"taskfile len:{}, working with {} objects\".format(len(tasks), self.num_objects))\n\n        #some statistics\n        self.num_rela=0\n        self.num_attr=0\n        #hist = np.zeros(self.num_objects+1)\n        obj_list = np.arange(0, self.num_objects)\n\n\n        #CREATE and empty scene dict \n        empty_scene_dict = {}\n\n        #add empty objects for relations\n        for id1 in obj_list:\n            for id2 in obj_list:\n                key = str(id1)+\",\"+str(id2)\n                if key not in empty_scene_dict:\n                    empty_scene_dict[key] = torch.zeros([4104])\n\n        #add empty objects for names and attributes\n            for id in obj_list:\n                empty_scene_dict[str(id)] = torch.zeros([2048])\n\n        no_ans = 0\n        for tidx, task in enumerate(tqdm(tasks)):\n\n            # if task['image_id'] != 2337789:\n            #    continue\n\n\n            target = np.zeros(self.num_objects)\n            clause_n_trainer = ClauseNTrainer(train_data_loader=tasks, val_data_loader=tasks, test_data_loader=tasks)\n\n            #OBJECTS\n            all_oid = task['question']['input']\n            all_oid = [int(oid) for oid in all_oid]\n            len_all_oid = len(all_oid)\n\n\n            #only consider images with less then num_objects\n            if self.num_objects is not None:\n                if len_all_oid > self.num_objects:\n                    continue\n                else:\n                    self.dataset_size +=1\n            \n\n            #map object ids to ids lower than num_objects\n            correct_oids = task['question']['output']\n\n            if len(correct_oids) == 0 :\n                no_ans +=1\n            len_correct_oid = len(correct_oids)\n            not_correct_oids = [x for x in all_oid if (x not in correct_oids)]\n            all_oid = correct_oids + not_correct_oids #this list contains all objects but puts the true objects first\n\n\n            #map object ids to the inverval [0,25]\n            oid_map = {}\n            for idx, oid in enumerate(all_oid):\n                oid_map[oid] = idx\n\n            #map the correct answers\n            correct_oids_mapped = [oid_map[correct_oid] for correct_oid in correct_oids]\n            not_correct_oids_mapped = [oid_map[not_correct_oid] for not_correct_oid in not_correct_oids]\n            #print(\"the correct answers are:\", correct_oids)\n\n\n            #set gt vector true for correct objects\n            for oid in correct_oids_mapped:\n                target[oid] = 1\n\n            #create the QUERY\n            query = task[\"question\"][\"clauses\"]\n            query = Query(query)\n            query_content = clause_n_trainer.query_manager.transformer.transform(query)\n\n\n            \n            if 'rela'  in clause_n_trainer.query_manager.transformer.show_filter[0][1][0]:\n               self.num_rela += 1\n               #continue\n\n            if 'attr'  in clause_n_trainer.query_manager.transformer.show_filter[0][1][0]:\n               self.num_attr += 1\n               #continue\n            \n            #if \"hand\" not in query_content and self.dataset==\"test\":\n            #    self.dataset_size -=1\n            #    continue\n\n            #if (\"controlling_flows_of_traffic\" not in query_content) and self.dataset==\"test\":\n            #    self.dataset_size -=1\n            #    continue\n\n\n\n            #collect bounding boxes and object features \n            #scene_dict stores all combinations while data_dict contains only the query-relevant ones\n            scene_dict = empty_scene_dict.copy()\n\n\n            # Put objects into scene dict\n            #feature maps for attributes and names are of size 2048, bounding boxes are of size 4 \n            for  oid in all_oid:\n                scene_dict[str(oid_map[oid])] = torch.tensor(task['object_feature'][task['object_ids'].index(oid)])\n            \n            #feature maps for relations\n            for  oid1 in all_oid:\n                for  oid2 in all_oid:\n                    key = str(oid_map[oid1])+\",\"+str(oid_map[oid2])\n                    scene_dict[key] = torch.cat([torch.tensor(task['object_feature'][task['object_ids'].index(oid1)]), torch.tensor(task['scene_graph']['bboxes'][oid1]),\n                                                torch.tensor(task['object_feature'][task['object_ids'].index(oid2)]), torch.tensor(task['scene_graph']['bboxes'][oid2])])    \n\n\n            \n            #use only objects in the query for training\n            if dataset == \"train\":\n                query_rule_and_show_statement, attr_rel_filter = get_SLASH_query(query_content, np.array(correct_oids_mapped), np.array(not_correct_oids_mapped), clause_n_trainer.query_manager.transformer.show_filter,num_objects = len_all_oid) # Translate the query to SLASH\n                \n\n            #use all objects for evaluation\n            else:\n                query_rule_and_show_statement, attr_rel_filter= get_SLASH_query(query_content, show_filter =clause_n_trainer.query_manager.transformer.show_filter, num_objects = len_all_oid) # Translate the query to SLASH\n            \n\n\n            # if 'relation' in query_rule_and_show_statement:\n            #    self.dataset_size -=1\n            #    continue\n\n            # if 'rule_1' in query_rule_and_show_statement:\n            #    self.dataset_size -=1\n            #    continue\n\n            #queries without any relations or rule work up to 25 objects\n\n            # if 'attr' in query_rule_and_show_statement:\n            #    self.dataset_size -=1\n            #    continue\n\n            self.attr_rel_filter_list.append(attr_rel_filter)\n\n            #we add the number of true objects for each image\n            #later we can then remove NPPs which are not true objects\n            self.num_true_objects.append(np.array(len_all_oid))\n\n\n\n            #if 'flag_addinf' is True, generate \"additional_information_dict\"\n            #and fill it with entries from \"task\" dictionary.\n            if flag_addinf:\n                additional_information_dict = {}\n                #add image_id\n                additional_information_dict['image_id'] = task['image_id']\n                #add image's URL\n                additional_information_dict['image_URL'] = task['url']\n                #add original object ids\n                additional_information_dict['original_object_ids'] = task['question']['input']\n                #add original answer object's id\n                additional_information_dict['original_answer_object_id'] = task['question']['output']\n                #add the mapping from obj_ids to obj*\n                additional_information_dict['mapping'] = oid_map\n                #add the answers in form of obj*\n                additional_information_dict['correct_oids_mapped'] = correct_oids_mapped\n                #add the rest of obj*, which are not answers to the question\n                additional_information_dict['not_correct_oids_mapped'] = not_correct_oids_mapped\n                #add bounding boxes of every object in the image\n                additional_information_dict['bboxes'] = task['scene_graph']['bboxes']\n                #add query in the FOL format\n                additional_information_dict['query'] = query_rule_and_show_statement\n                additional_information_dict['scene_graph'] = task['scene_graph']\n                self.additional_information_list.append(additional_information_dict)\n                # print(self.additional_information_list[0]['image_id'])\n\n            # add data, objects, queries and query rules to the dataset\n            self.feature_and_bb_list.append(scene_dict)\n            #self.object_list.append(object_string)\n            self.query_list.append(query_rule_and_show_statement)\n            self.targets.append(target)\n\n            # if self.dataset_size >= 100 and self.dataset == \"train\":\n            #   break\n            # elif self.dataset_size >= 100 and self.dataset == \"test\":\n            #   break\n            # elif self.dataset_size >= 100 and self.dataset == \"val\":\n            #   break\n            \n\n        # temp = np.cumsum(hist)\n        # for hidx, h in enumerate(hist):\n        #     if h != 0:\n        #         print(hidx, h, temp[hidx]/tasks.__len__())\n\n\n\n\n        print(len(self.query_list),len(self.targets))\n        print(\"dataset size \", self.dataset,\":\",self.dataset_size)\n        print(\"queries with no answer\", no_ans)\n        print(\"Num relations\", self.num_rela)\n        print(\"Num attr\", self.num_attr)\n\n\n\n\n    \n    def __getitem__(self, index):\n\n        if self.flag_addinf:\n            return self.feature_and_bb_list[index], self.query_list[index], self.num_true_objects[index], self.targets[index] , self.additional_information_list[index]\n        else:\n            return self.feature_and_bb_list[index], self.query_list[index]+self.attr_rel_filter_list[index] , self.num_true_objects[index], self.targets[index]\n        \n\n    def __len__(self):\n        return self.dataset_size", "class VQA(Dataset):\n    '''\n    VQA dataset consisting of:\n    objects\n    flag_addinf: The flag for getting dictionary with additional informations for visualisations.\n    '''\n\n\n    def __init__(self, dataset,  task_file, num_objects =None, flag_addinf=False):\n        self.data = list()\n        self.dataset = dataset\n        self.flag_addinf = flag_addinf\n        self.max_objects = 0\n\n        self.dataset_size = 0\n\n        with open (task_file, 'rb') as tf:\n            tasks = pickle.load(tf)\n\n        print(\"dataloading done\")\n        \n\n        self.object_list = []\n        self.query_list = []\n        self.feature_and_bb_list = []\n        self.num_true_objects = []\n        self.targets = []\n        self.additional_information_list = []\n        \n        self.attr_rel_filter_list = []\n\n        #if num_objects is not None:\n        self.num_objects = num_objects\n        print(\"taskfile len:{}, working with {} objects\".format(len(tasks), self.num_objects))\n\n        #some statistics\n        self.num_rela=0\n        self.num_attr=0\n        #hist = np.zeros(self.num_objects+1)\n        obj_list = np.arange(0, self.num_objects)\n\n\n        #CREATE and empty scene dict \n        empty_scene_dict = {}\n\n        #add empty objects for relations\n        for id1 in obj_list:\n            for id2 in obj_list:\n                key = str(id1)+\",\"+str(id2)\n                if key not in empty_scene_dict:\n                    empty_scene_dict[key] = torch.zeros([4104])\n\n        #add empty objects for names and attributes\n            for id in obj_list:\n                empty_scene_dict[str(id)] = torch.zeros([2048])\n\n        no_ans = 0\n        for tidx, task in enumerate(tqdm(tasks)):\n\n            # if task['image_id'] != 2337789:\n            #    continue\n\n\n            target = np.zeros(self.num_objects)\n            clause_n_trainer = ClauseNTrainer(train_data_loader=tasks, val_data_loader=tasks, test_data_loader=tasks)\n\n            #OBJECTS\n            all_oid = task['question']['input']\n            all_oid = [int(oid) for oid in all_oid]\n            len_all_oid = len(all_oid)\n\n\n            #only consider images with less then num_objects\n            if self.num_objects is not None:\n                if len_all_oid > self.num_objects:\n                    continue\n                else:\n                    self.dataset_size +=1\n            \n\n            #map object ids to ids lower than num_objects\n            correct_oids = task['question']['output']\n\n            if len(correct_oids) == 0 :\n                no_ans +=1\n            len_correct_oid = len(correct_oids)\n            not_correct_oids = [x for x in all_oid if (x not in correct_oids)]\n            all_oid = correct_oids + not_correct_oids #this list contains all objects but puts the true objects first\n\n\n            #map object ids to the inverval [0,25]\n            oid_map = {}\n            for idx, oid in enumerate(all_oid):\n                oid_map[oid] = idx\n\n            #map the correct answers\n            correct_oids_mapped = [oid_map[correct_oid] for correct_oid in correct_oids]\n            not_correct_oids_mapped = [oid_map[not_correct_oid] for not_correct_oid in not_correct_oids]\n            #print(\"the correct answers are:\", correct_oids)\n\n\n            #set gt vector true for correct objects\n            for oid in correct_oids_mapped:\n                target[oid] = 1\n\n            #create the QUERY\n            query = task[\"question\"][\"clauses\"]\n            query = Query(query)\n            query_content = clause_n_trainer.query_manager.transformer.transform(query)\n\n\n            \n            if 'rela'  in clause_n_trainer.query_manager.transformer.show_filter[0][1][0]:\n               self.num_rela += 1\n               #continue\n\n            if 'attr'  in clause_n_trainer.query_manager.transformer.show_filter[0][1][0]:\n               self.num_attr += 1\n               #continue\n            \n            #if \"hand\" not in query_content and self.dataset==\"test\":\n            #    self.dataset_size -=1\n            #    continue\n\n            #if (\"controlling_flows_of_traffic\" not in query_content) and self.dataset==\"test\":\n            #    self.dataset_size -=1\n            #    continue\n\n\n\n            #collect bounding boxes and object features \n            #scene_dict stores all combinations while data_dict contains only the query-relevant ones\n            scene_dict = empty_scene_dict.copy()\n\n\n            # Put objects into scene dict\n            #feature maps for attributes and names are of size 2048, bounding boxes are of size 4 \n            for  oid in all_oid:\n                scene_dict[str(oid_map[oid])] = torch.tensor(task['object_feature'][task['object_ids'].index(oid)])\n            \n            #feature maps for relations\n            for  oid1 in all_oid:\n                for  oid2 in all_oid:\n                    key = str(oid_map[oid1])+\",\"+str(oid_map[oid2])\n                    scene_dict[key] = torch.cat([torch.tensor(task['object_feature'][task['object_ids'].index(oid1)]), torch.tensor(task['scene_graph']['bboxes'][oid1]),\n                                                torch.tensor(task['object_feature'][task['object_ids'].index(oid2)]), torch.tensor(task['scene_graph']['bboxes'][oid2])])    \n\n\n            \n            #use only objects in the query for training\n            if dataset == \"train\":\n                query_rule_and_show_statement, attr_rel_filter = get_SLASH_query(query_content, np.array(correct_oids_mapped), np.array(not_correct_oids_mapped), clause_n_trainer.query_manager.transformer.show_filter,num_objects = len_all_oid) # Translate the query to SLASH\n                \n\n            #use all objects for evaluation\n            else:\n                query_rule_and_show_statement, attr_rel_filter= get_SLASH_query(query_content, show_filter =clause_n_trainer.query_manager.transformer.show_filter, num_objects = len_all_oid) # Translate the query to SLASH\n            \n\n\n            # if 'relation' in query_rule_and_show_statement:\n            #    self.dataset_size -=1\n            #    continue\n\n            # if 'rule_1' in query_rule_and_show_statement:\n            #    self.dataset_size -=1\n            #    continue\n\n            #queries without any relations or rule work up to 25 objects\n\n            # if 'attr' in query_rule_and_show_statement:\n            #    self.dataset_size -=1\n            #    continue\n\n            self.attr_rel_filter_list.append(attr_rel_filter)\n\n            #we add the number of true objects for each image\n            #later we can then remove NPPs which are not true objects\n            self.num_true_objects.append(np.array(len_all_oid))\n\n\n\n            #if 'flag_addinf' is True, generate \"additional_information_dict\"\n            #and fill it with entries from \"task\" dictionary.\n            if flag_addinf:\n                additional_information_dict = {}\n                #add image_id\n                additional_information_dict['image_id'] = task['image_id']\n                #add image's URL\n                additional_information_dict['image_URL'] = task['url']\n                #add original object ids\n                additional_information_dict['original_object_ids'] = task['question']['input']\n                #add original answer object's id\n                additional_information_dict['original_answer_object_id'] = task['question']['output']\n                #add the mapping from obj_ids to obj*\n                additional_information_dict['mapping'] = oid_map\n                #add the answers in form of obj*\n                additional_information_dict['correct_oids_mapped'] = correct_oids_mapped\n                #add the rest of obj*, which are not answers to the question\n                additional_information_dict['not_correct_oids_mapped'] = not_correct_oids_mapped\n                #add bounding boxes of every object in the image\n                additional_information_dict['bboxes'] = task['scene_graph']['bboxes']\n                #add query in the FOL format\n                additional_information_dict['query'] = query_rule_and_show_statement\n                additional_information_dict['scene_graph'] = task['scene_graph']\n                self.additional_information_list.append(additional_information_dict)\n                # print(self.additional_information_list[0]['image_id'])\n\n            # add data, objects, queries and query rules to the dataset\n            self.feature_and_bb_list.append(scene_dict)\n            #self.object_list.append(object_string)\n            self.query_list.append(query_rule_and_show_statement)\n            self.targets.append(target)\n\n            # if self.dataset_size >= 100 and self.dataset == \"train\":\n            #   break\n            # elif self.dataset_size >= 100 and self.dataset == \"test\":\n            #   break\n            # elif self.dataset_size >= 100 and self.dataset == \"val\":\n            #   break\n            \n\n        # temp = np.cumsum(hist)\n        # for hidx, h in enumerate(hist):\n        #     if h != 0:\n        #         print(hidx, h, temp[hidx]/tasks.__len__())\n\n\n\n\n        print(len(self.query_list),len(self.targets))\n        print(\"dataset size \", self.dataset,\":\",self.dataset_size)\n        print(\"queries with no answer\", no_ans)\n        print(\"Num relations\", self.num_rela)\n        print(\"Num attr\", self.num_attr)\n\n\n\n\n    \n    def __getitem__(self, index):\n\n        if self.flag_addinf:\n            return self.feature_and_bb_list[index], self.query_list[index], self.num_true_objects[index], self.targets[index] , self.additional_information_list[index]\n        else:\n            return self.feature_and_bb_list[index], self.query_list[index]+self.attr_rel_filter_list[index] , self.num_true_objects[index], self.targets[index]\n        \n\n    def __len__(self):\n        return self.dataset_size", "        #return len(self.feature_and_bb_list)"]}
{"filename": "src/experiments/vqa/preprocess.py", "chunked_list": ["\"\"\"\nThe source code is based on:\nScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\nJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\nAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\nhttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\"\"\"\n\n# Prog = Conj | Logic\n# Logic = biOp Prog Prog", "# Prog = Conj | Logic\n# Logic = biOp Prog Prog\n# Conj = and Des Conj | Des\n# Des = rela Relaname O1 O2 | attr Attr O\n\nclass Variable():\n    def __init__(self, id):\n        self.var_id = f\"O{id}\"\n        self.name_id = f\"N{id}\"\n        self.name = []\n        self.hypernyms = []\n        self.attrs = []\n        self.kgs = []\n        # The relations where this object functions as a subject\n        self.sub_relas = []\n        self.obj_relas = []\n\n    def has_rela(self):\n        if len(self.sub_relas) == 0 and len(self.obj_relas) == 0:\n            return False\n        return True\n\n    def get_name_id(self):\n        # if (not len(self.hypernyms) == 0) and len(self.name) == 0:\n        #     return True, self.name_id\n        # if (not len(self.kgs) == 0) and len(self.name) == 0:\n        #     return True, self.name_id\n        return False, self.name_id\n\n    def set_name(self, name):\n        if name in self.name:\n            return\n\n        self.name.append(name)\n\n    def set_kg(self, kg):\n        if kg not in self.kgs:\n            self.kgs.append(kg)\n\n    def set_hypernym(self, hypernym):\n        if hypernym not in self.hypernyms:\n            self.hypernyms.append(hypernym)\n\n    def set_attr(self, attr):\n        if attr not in self.attrs:\n            self.attrs.append(attr)\n\n    def set_obj_relas(self, obj_rela):\n        if obj_rela not in self.obj_relas:\n            self.obj_relas.append(obj_rela)\n\n    def set_sub_relas(self, sub_rela):\n        if sub_rela not in self.sub_relas:\n            self.sub_relas.append(sub_rela)\n\n    def get_neighbor(self):\n        neighbors = []\n        for rela in self.sub_relas:\n            neighbors.append(rela.obj)\n        for rela in self.obj_relas:\n            neighbors.append(rela.sub)\n        return neighbors\n\n    def update(self, other):\n\n        self.hypernyms = list(set(self.name + other.name))\n        self.hypernyms = list(set(self.hypernyms + other.hypernyms))\n        self.attrs = list(set(self.attrs + other.attrs))\n        self.kgs = list(set(self.kgs + other.kgs))\n\n\n    def to_datalog(self, with_name=False, with_rela=True):\n\n        name_query = []\n\n        if (len(self.name) == 0) and with_name:\n            name_query.append(\"name({}, {})\".format(self.name_id.replace(\" \",\"_\").replace(\".\",\"_\"), self.var_id.replace(\" \",\"_\").replace(\".\",\"_\")))\n\n        if (not len(self.name) == 0):\n            for n in self.name:\n                #name_query.append(f\"name(\\\"{n}\\\", {self.var_id})\")\n                n = n.replace(\" \",\"_\").replace(\".\",\"_\")\n                #name_query.append(f\"name(0,1,{self.var_id},{n})\")\n                name_query.append(f\"name({self.var_id},{n})\")\n\n\n        #attr_query = [ f\"attr(\\\"{attr}\\\", {self.var_id})\" for attr in self.attrs]\n        #attr_query = [ \"attr(0,1,{}, {})\".format(self.var_id.replace(\" \",\"_\"),attr.replace(\" \",\"_\")) for attr in self.attrs]\n        attr_query = [ \"attr({}, {})\".format(self.var_id.replace(\" \",\"_\"),attr.replace(\" \",\"_\")) for attr in self.attrs]\n\n        #hypernym_query = [f\"name(\\\"{hypernym}\\\", {self.var_id})\" for hypernym in self.hypernyms]\n        #hypernym_query = [\"name(0,1,{}, {})\".format(self.var_id.replace(\" \",\"_\").replace(\".\",\"_\"),hypernym.replace(\" \",\"_\").replace(\".\",\"_\")) for hypernym in self.hypernyms]\n        hypernym_query = [\"name({}, {})\".format(self.var_id.replace(\" \",\"_\").replace(\".\",\"_\"),hypernym.replace(\" \",\"_\").replace(\".\",\"_\")) for hypernym in self.hypernyms]\n\n        kg_query = []\n\n        for kg in self.kgs:\n            restriction = list(filter(lambda x: not x == 'BLANK' and not x == '', kg))\n            assert (len(restriction) == 2)\n            rel = restriction[0].replace(\" \",\"_\")\n            usage = restriction[1].replace(\" \",\"_\")\n            #kg_query += [f\"name({self.name_id}, {self.var_id}), oa_rel({rel}, {self.name_id}, {usage})\"]\n            #kg_query +=     [\"name({}, {}), oa_rel({}, {}, {})\".format(self.name_id.replace(\" \",\"_\").replace(\".\",\"_\") ,self.var_id.replace(\" \",\"_\").replace(\".\",\"_\") ,rel.replace(\" \",\"_\").replace(\".\",\"_\"), self.name_id.replace(\" \",\"_\").replace(\".\",\"_\"), usage.replace(\" \",\"_\").replace(\".\",\"_\"))]\n            #kg_query +=     [\"name(0,1,{}, {}), oa_rel({}, {}, {})\".format(self.var_id.replace(\" \",\"_\").replace(\".\",\"_\") ,self.name_id.replace(\" \",\"_\").replace(\".\",\"_\"), rel.replace(\" \",\"_\").replace(\".\",\"_\"), self.name_id.replace(\" \",\"_\").replace(\".\",\"_\"), usage.replace(\" \",\"_\").replace(\".\",\"_\"))]\n            kg_query +=     [\"name({}, {}), oa_rel({}, {}, {})\".format(self.var_id.replace(\" \",\"_\").replace(\".\",\"_\") ,self.name_id.replace(\" \",\"_\").replace(\".\",\"_\"), rel.replace(\" \",\"_\").replace(\".\",\"_\"), self.name_id.replace(\" \",\"_\").replace(\".\",\"_\"), usage.replace(\" \",\"_\").replace(\".\",\"_\"))]\n\n        if with_rela:\n            rela_query = [rela.to_datalog() for rela in self.sub_relas]\n        else:\n            rela_query = []\n\n        program = name_query + attr_query + hypernym_query + kg_query + rela_query\n\n        #print(program)\n        return program", "\nclass Relation():\n    def __init__(self, rela_name, sub, obj):\n        self.rela_name = rela_name\n        self.sub = sub\n        self.obj = obj\n        self.sub.set_sub_relas(self)\n        self.obj.set_obj_relas(self)\n\n    def substitute(self, v1, v2):\n        if self.sub == v1:\n            self.sub = v2\n        if self.obj == v1:\n            self.obj = v2\n\n    def to_datalog(self):\n        #rela_query = f\"relation(\\\"{self.rela_name}\\\", {self.sub.var_id},  {self.obj.var_id})\"\n        #rela_query = \"relation(0,1,{}, {},  {})\".format( self.sub.var_id.replace(\" \",\"_\"), self.obj.var_id.replace(\" \",\"_\"),self.rela_name.replace(\" \",\"_\"))\n        rela_query = \"relation({}, {},  {})\".format( self.sub.var_id.replace(\" \",\"_\"), self.obj.var_id.replace(\" \",\"_\"),self.rela_name.replace(\" \",\"_\"))\n\n        return rela_query", "\n\n# This is for binary operations on variables\nclass BiOp():\n    def __init__(self, op_name, v1, v2):\n        self.op_name = op_name\n        self.v1 = v1\n        self.v2 = v2\n\n    def to_datalog(self):\n        raise NotImplementedError", "\nclass Or(BiOp):\n    def __init__(self, v1, v2):\n        super().__init__('or', v1, v2)\n\n    def to_datalog(self):\n        pass\n\nclass And(BiOp):\n    def __init__(self, v1, v2):\n        super().__init__('and', v1, v2)\n\n    def to_datalog(self):\n        pass", "class And(BiOp):\n    def __init__(self, v1, v2):\n        super().__init__('and', v1, v2)\n\n    def to_datalog(self):\n        pass\n\nclass Query():\n\n    def __init__(self, query):\n        self.vars = []\n        self.relations = []\n        self.operations = []\n        self.stack = []\n        self.preprocess(query)\n\n\n    def get_target(self):\n        pass\n\n    def get_new_var(self):\n        self.vars.append(Variable(len(self.vars)))\n\n    def preprocess(self, query):\n\n        # for clause in question[\"program\"]:\n        for clause in query:\n\n            if clause['function'] == \"Initial\":\n                if not len(self.vars) == 0:\n                    self.stack.append(self.vars[-1])\n                self.get_new_var()\n                self.root = self.vars[-1]\n\n            # logic operations\n            elif clause['function'] == \"And\":\n                v = self.stack.pop()\n                self.operations.append(And(v, self.vars[-1]))\n                self.root = self.operations[-1]\n\n            elif clause['function'] == \"Or\":\n                v = self.stack.pop()\n                self.operations.append(Or(v, self.vars[-1]))\n                self.root = self.operations[-1]\n\n            # find operations\n            elif clause['function'] == \"KG_Find\":\n                self.vars[-1].set_kg(clause['text_input'])\n\n            elif clause['function'] == \"Hypernym_Find\":\n                self.vars[-1].set_hypernym(clause['text_input'])\n\n            elif clause['function'] == \"Find_Name\":\n                self.vars[-1].set_name(clause['text_input'])\n\n            elif clause['function'] == \"Find_Attr\":\n                self.vars[-1].set_attr(clause['text_input'])\n\n            elif clause['function'] == \"Relate_Reverse\":\n                self.get_new_var()\n                self.root = self.vars[-1]\n                obj = self.vars[-2]\n                sub = self.vars[-1]\n                rela_name = clause['text_input']\n                relation = Relation(rela_name, sub, obj)\n                self.relations.append(relation)\n\n            elif clause['function'] == \"Relate\":\n                self.get_new_var()\n                self.root = self.vars[-1]\n                sub = self.vars[-2]\n                obj = self.vars[-1]\n                rela_name = clause['text_input']\n                relation = Relation(rela_name, sub, obj)\n                self.relations.append(relation)\n\n            else:\n                raise Exception(f\"Not handled function: {clause['function']}\")", "\n\n# Optimizers for optimization\nclass QueryOptimizer():\n\n    def __init__(self, name):\n        self.name = name\n\n    def optimize(self, query):\n        raise NotImplementedError", "\n# This only works for one and operation at the end\n# This is waited for update\n# class AndQueryOptimizer(QueryOptimizer):\n\n#     def __init__(self):\n#         super().__init__(\"AndQueryOptimizer\")\n\n#     # For any and operation, this can be rewritten as a single object\n#     def optimize(self, query):", "#     # For any and operation, this can be rewritten as a single object\n#     def optimize(self, query):\n\n#         if len(query.operations) == 0:\n#             return query\n\n#         assert(len(query.operations) == 1)\n\n#         operation = query.operations[0]\n#         # merge every subtree into one", "#         operation = query.operations[0]\n#         # merge every subtree into one\n#         if operation.name == \"and\":\n#             v1 = operation.v1\n#             v2 = operation.v2\n#             v1.merge(v2)\n\n#             for relation in query.relations:\n#                 relation.substitute(v2, v1)\n", "#                 relation.substitute(v2, v1)\n\n#             query.vars.remove(v2)\n\n#             if query.root == operation:\n#                 query.root = v1\n\n#         return query\n\n\nclass HypernymOptimizer(QueryOptimizer):\n\n    def __init__(self):\n        super().__init__(\"HypernymOptimizer\")\n\n    def optimize(self, query):\n\n        if (query.name is not None and not query.hypernyms == []):\n            query.hypernyms = []\n\n        return query", "\n\nclass HypernymOptimizer(QueryOptimizer):\n\n    def __init__(self):\n        super().__init__(\"HypernymOptimizer\")\n\n    def optimize(self, query):\n\n        if (query.name is not None and not query.hypernyms == []):\n            query.hypernyms = []\n\n        return query", "\n\nclass KGOptimizer(QueryOptimizer):\n\n    def __init__(self):\n        super().__init__(\"HypernymOptimizer\")\n\n    def optimize(self, query):\n\n        if (query.name is not None and not query.kgs == []):\n            query.kgs = []\n\n        return query", ""]}
{"filename": "src/experiments/vqa/query_lib.py", "chunked_list": ["\"\"\"\nThe source code is based on:\nScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\nJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\nAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\nhttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\"\"\"\n\nimport pickle\nimport os", "import pickle\nimport os\nimport subprocess\n\nfrom transformer import DetailTransformer, SimpleTransformer\nfrom preprocess import Query\nfrom knowledge_graph import KG, RULES\n\n# animals = [\"giraffe\", \"cat\", \"kitten\", \"dog\", \"puppy\", \"poodle\", \"bull\", \"cow\", \"cattle\", \"bison\", \"calf\", \"pig\", \"ape\", \"monkey\", \"gorilla\", \"rat\", \"squirrel\", \"hamster\", \"deer\", \"moose\", \"alpaca\", \"elephant\", \"goat\", \"sheep\", \"lamb\", \"antelope\", \"rhino\", \"hippo\",  \"zebra\", \"horse\", \"pony\", \"donkey\", \"camel\", \"panda\", \"panda bear\", \"bear\", \"polar bear\", \"seal\", \"fox\", \"raccoon\", \"tiger\", \"wolf\", \"lion\", \"leopard\", \"cheetah\", \"badger\", \"rabbit\", \"bunny\", \"beaver\", \"kangaroo\", \"dinosaur\", \"dragon\", \"fish\", \"whale\", \"dolphin\", \"crab\", \"shark\", \"octopus\", \"lobster\", \"oyster\", \"butterfly\", \"bee\", \"fly\", \"ant\", \"firefly\", \"snail\", \"spider\", \"bird\", \"penguin\", \"pigeon\", \"seagull\", \"finch\", \"robin\", \"ostrich\", \"goose\", \"owl\", \"duck\", \"hawk\", \"eagle\", \"swan\", \"chicken\", \"hen\", \"hummingbird\", \"parrot\", \"crow\", \"flamingo\", \"peacock\", \"bald eagle\", \"dove\", \"snake\", \"lizard\", \"alligator\", \"turtle\", \"frog\", \"animal\"]\nclass QueryManager():\n    def __init__(self, save_dir):\n        self.save_dir = save_dir\n        self.transformer = SimpleTransformer()\n\n    def save_file(self, file_name, content):\n        save_path = os.path.join (self.save_dir, file_name)\n        with open (save_path, \"w\") as save_file:\n            save_file.write(content)\n\n    def delete_file(self, file_name):\n        save_path = os.path.join (self.save_dir, file_name)\n        if os.path.exists(save_path):\n            os.remove(save_path)\n\n    def fact_prob_to_file(self, fact_tps, fact_probs):\n        scene_tps = []\n\n        (name_tps, attr_tps, rela_tps) = fact_tps\n        (name_probs, attr_probs, rela_probs) = fact_probs\n\n        cluster_ntp = {}\n        for (oid, name), prob in zip(name_tps, name_probs):\n            if not oid in cluster_ntp:\n                cluster_ntp[oid] = [(name, prob)]\n            else:\n                cluster_ntp[oid].append((name, prob))\n\n\n        for oid, name_list in cluster_ntp.items():\n            name_tps = []\n            for (name, prob) in name_list:\n                # if not name in animals[:5]:\n                #     continue\n                name_tps.append(f'{prob}::name(\"{name}\", {int(oid)})')\n            name_content = \";\\n\".join(name_tps) + \".\"\n            scene_tps.append(name_content)\n\n        for attr_tp, prob in zip(attr_tps, attr_probs):\n            # if not attr_tp[1] == \"tall\":\n            #     continue\n            scene_tps.append(f'{prob}::attr(\"{attr_tp[1]}\", {int(attr_tp[0])}).')\n\n        for rela_tp, prob in zip(rela_tps, rela_probs):\n            # if not rela_tp[0] == \"left\":\n            #     continue\n            scene_tps.append(f'{prob}::relation(\"{rela_tp[0]}\", {int(rela_tp[1])}, {int(rela_tp[2])}).')\n\n        return \"\\n\".join(scene_tps)\n\n    def process_result(self, result):\n        output = result.stdout.decode()\n        lines = output.split(\"\\n\")\n        targets = {}\n        for line in lines:\n            if line == '':\n                continue\n            if not '\\t' in line:\n                continue\n            info = line.split('\\t')\n            # No target found\n            if 'X' in info[0]:\n                break\n            target_name = int(info[0][7:-2])\n            target_prob = float(info[1])\n            targets[target_name] = target_prob\n        return targets\n\n    def get_result(self, task, fact_tps, fact_probs):\n        timeout = False\n        question = task[\"question\"][\"clauses\"]\n        file_name = f\"{task['question']['question_id']}.pl\"\n        save_path = os.path.join (self.save_dir, file_name)\n        query = Query(question)\n        query_content = self.transformer.transform(query)\n        scene_content = self.fact_prob_to_file(fact_tps, fact_probs)\n\n        content = KG+ \"\\n\" + RULES + \"\\n\" + scene_content + \"\\n\" + query_content\n        self.save_file(file_name, content)\n        try:\n            result = subprocess.run([\"problog\", save_path], capture_output=True, timeout=10)\n            targets = self.process_result(result)\n        except:\n            # time out here\n            timeout = True\n            targets = {}\n\n        # self.delete_file(file_name)\n        return targets, timeout\n\n    def get_relas(self, query):\n        relations = []\n        for clause in query:\n            if 'Relate' in clause['function']:\n                relations.append(clause['text_input'])\n        return relations", "# animals = [\"giraffe\", \"cat\", \"kitten\", \"dog\", \"puppy\", \"poodle\", \"bull\", \"cow\", \"cattle\", \"bison\", \"calf\", \"pig\", \"ape\", \"monkey\", \"gorilla\", \"rat\", \"squirrel\", \"hamster\", \"deer\", \"moose\", \"alpaca\", \"elephant\", \"goat\", \"sheep\", \"lamb\", \"antelope\", \"rhino\", \"hippo\",  \"zebra\", \"horse\", \"pony\", \"donkey\", \"camel\", \"panda\", \"panda bear\", \"bear\", \"polar bear\", \"seal\", \"fox\", \"raccoon\", \"tiger\", \"wolf\", \"lion\", \"leopard\", \"cheetah\", \"badger\", \"rabbit\", \"bunny\", \"beaver\", \"kangaroo\", \"dinosaur\", \"dragon\", \"fish\", \"whale\", \"dolphin\", \"crab\", \"shark\", \"octopus\", \"lobster\", \"oyster\", \"butterfly\", \"bee\", \"fly\", \"ant\", \"firefly\", \"snail\", \"spider\", \"bird\", \"penguin\", \"pigeon\", \"seagull\", \"finch\", \"robin\", \"ostrich\", \"goose\", \"owl\", \"duck\", \"hawk\", \"eagle\", \"swan\", \"chicken\", \"hen\", \"hummingbird\", \"parrot\", \"crow\", \"flamingo\", \"peacock\", \"bald eagle\", \"dove\", \"snake\", \"lizard\", \"alligator\", \"turtle\", \"frog\", \"animal\"]\nclass QueryManager():\n    def __init__(self, save_dir):\n        self.save_dir = save_dir\n        self.transformer = SimpleTransformer()\n\n    def save_file(self, file_name, content):\n        save_path = os.path.join (self.save_dir, file_name)\n        with open (save_path, \"w\") as save_file:\n            save_file.write(content)\n\n    def delete_file(self, file_name):\n        save_path = os.path.join (self.save_dir, file_name)\n        if os.path.exists(save_path):\n            os.remove(save_path)\n\n    def fact_prob_to_file(self, fact_tps, fact_probs):\n        scene_tps = []\n\n        (name_tps, attr_tps, rela_tps) = fact_tps\n        (name_probs, attr_probs, rela_probs) = fact_probs\n\n        cluster_ntp = {}\n        for (oid, name), prob in zip(name_tps, name_probs):\n            if not oid in cluster_ntp:\n                cluster_ntp[oid] = [(name, prob)]\n            else:\n                cluster_ntp[oid].append((name, prob))\n\n\n        for oid, name_list in cluster_ntp.items():\n            name_tps = []\n            for (name, prob) in name_list:\n                # if not name in animals[:5]:\n                #     continue\n                name_tps.append(f'{prob}::name(\"{name}\", {int(oid)})')\n            name_content = \";\\n\".join(name_tps) + \".\"\n            scene_tps.append(name_content)\n\n        for attr_tp, prob in zip(attr_tps, attr_probs):\n            # if not attr_tp[1] == \"tall\":\n            #     continue\n            scene_tps.append(f'{prob}::attr(\"{attr_tp[1]}\", {int(attr_tp[0])}).')\n\n        for rela_tp, prob in zip(rela_tps, rela_probs):\n            # if not rela_tp[0] == \"left\":\n            #     continue\n            scene_tps.append(f'{prob}::relation(\"{rela_tp[0]}\", {int(rela_tp[1])}, {int(rela_tp[2])}).')\n\n        return \"\\n\".join(scene_tps)\n\n    def process_result(self, result):\n        output = result.stdout.decode()\n        lines = output.split(\"\\n\")\n        targets = {}\n        for line in lines:\n            if line == '':\n                continue\n            if not '\\t' in line:\n                continue\n            info = line.split('\\t')\n            # No target found\n            if 'X' in info[0]:\n                break\n            target_name = int(info[0][7:-2])\n            target_prob = float(info[1])\n            targets[target_name] = target_prob\n        return targets\n\n    def get_result(self, task, fact_tps, fact_probs):\n        timeout = False\n        question = task[\"question\"][\"clauses\"]\n        file_name = f\"{task['question']['question_id']}.pl\"\n        save_path = os.path.join (self.save_dir, file_name)\n        query = Query(question)\n        query_content = self.transformer.transform(query)\n        scene_content = self.fact_prob_to_file(fact_tps, fact_probs)\n\n        content = KG+ \"\\n\" + RULES + \"\\n\" + scene_content + \"\\n\" + query_content\n        self.save_file(file_name, content)\n        try:\n            result = subprocess.run([\"problog\", save_path], capture_output=True, timeout=10)\n            targets = self.process_result(result)\n        except:\n            # time out here\n            timeout = True\n            targets = {}\n\n        # self.delete_file(file_name)\n        return targets, timeout\n\n    def get_relas(self, query):\n        relations = []\n        for clause in query:\n            if 'Relate' in clause['function']:\n                relations.append(clause['text_input'])\n        return relations", "\n\n\n"]}
{"filename": "src/experiments/vqa/test.py", "chunked_list": ["print(\"start importing...\")\n\nimport time\nimport sys\nimport argparse\nimport datetime\n\nsys.path.append('../../')\nsys.path.append('../../SLASH/')\nsys.path.append('../../EinsumNetworks/src/')", "sys.path.append('../../SLASH/')\nsys.path.append('../../EinsumNetworks/src/')\n\n\n#torch, numpy, ...\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.transforms import transforms\nimport torchvision\n", "import torchvision\n\nimport numpy as np\n\nimport json\n\n#own modules\nfrom dataGen import VQA\nfrom einsum_wrapper import EiNet\nfrom network_nn import Net_nn", "from einsum_wrapper import EiNet\nfrom network_nn import Net_nn\n\nfrom tqdm import tqdm\n\n#import slash\nfrom slash import SLASH\nimport os\n\n", "\n\n\n\nimport utils\nfrom utils import set_manual_seed\nfrom pathlib import Path\nfrom rtpt import RTPT\nimport pickle\n", "import pickle\n\n\nfrom knowledge_graph import RULES, KG\nfrom dataGen import name_npp, relation_npp, attribute_npp\nfrom models import name_clf, rela_clf, attr_clf \n\nprint(\"...done\")\n\n", "\n\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--seed\", type=int, default=10, help=\"Random generator seed for all frameworks\"\n    )\n\n    parser.add_argument(\n        \"--network-type\",\n        choices=[\"nn\",\"pc\"],\n        help=\"The type of external to be used e.g. neural net or probabilistic circuit\",\n    )\n    parser.add_argument(\n        \"--pc-structure\",\n        choices=[\"poon-domingos\",\"binary-trees\"],\n        help=\"The type of external to be used e.g. neural net or probabilistic circuit\",\n    )\n    parser.add_argument(\n        \"--batch-size\", type=int, default=100, help=\"Batch size to train with\"\n    )\n    parser.add_argument(\n        \"--num-workers\", type=int, default=6, help=\"Number of threads for data loader\"\n    )\n\n    parser.add_argument(\n        \"--p-num\", type=int, default=8, help=\"Number of processes to devide the batch for parallel processing\"\n    )\n\n    parser.add_argument(\"--credentials\", type=str, help=\"Credentials for rtpt\")\n\n\n    args = parser.parse_args()\n\n    if args.network_type == 'pc':\n        args.use_pc = True\n    else:\n        args.use_pc = False\n\n    return args", "\n\ndef determine_max_objects(task_file):\n\n    with open (task_file, 'rb') as tf:\n        tasks = pickle.load(tf)\n        print(\"taskfile len\",len(tasks))\n\n    #get the biggest number of objects in the image\n    max_objects = 0\n    for tidx, task in enumerate(tasks):\n        all_oid = task['question']['input']\n        len_all_oid = len(all_oid)\n\n        #store the biggest number of objects in an image\n        if len_all_oid > max_objects:\n            max_objects = len_all_oid\n    return max_objects", "\ndef slash_vqa():\n\n    args = get_args()\n    print(args)\n\n    \n    # Set the seeds for PRNG\n    set_manual_seed(args.seed)\n\n    # Create RTPT object\n    rtpt = RTPT(name_initials=args.credentials, experiment_name='SLASH VQA', max_iterations=1)\n\n    # Start the RTPT tracking\n    rtpt.start()\n    #writer = SummaryWriter(os.path.join(\"runs\",\"vqa\", str(args.seed)), purge_step=0)\n\n    #exp_name = 'vqa3'\n    #Path(\"data/\"+exp_name+\"/\").mkdir(parents=True, exist_ok=True)\n    #saveModelPath = 'data/'+exp_name+'/slash_vqa_models_seed'+str(args.seed)+'.pt'\n\n\n    #TODO workaround that adds +- notation \n    program_example = \"\"\"\n%scallop conversion rules\nname(O,N) :-  name(0,+O,-N).\nattr(O,A) :-  attr(0, +O, -A).\nrelation(O1,O2,N) :-  relation(0, +(O1,O2), -N).\n\"\"\"\n\n    #test_f = \"dataset/task_list/test_tasks_c3_1000.pkl\"  # Test datset\n\n    test_f = {\"c2\":\"dataset/task_list/test_tasks_c2_1000.pkl\",\n              \"c3\":\"dataset/task_list/test_tasks_c3_1000.pkl\",\n              \"c4\":\"dataset/task_list/test_tasks_c4_1000.pkl\",\n              \"c5\":\"dataset/task_list/test_tasks_c5_1000.pkl\",\n              \"c6\":\"dataset/task_list/test_tasks_c6_1000.pkl\"\n              }\n    \n\n    num_obj = []\n    if type(test_f) == str: \n        num_obj.append(determine_max_objects(test_f))\n    #if we have multiple test files\n    elif type(test_f) == dict:\n        for key in test_f:   \n            num_obj.append(determine_max_objects(test_f[key]))\n\n\n    NUM_OBJECTS = np.max(num_obj)\n    NUM_OBJECTS = 70\n\n\n    vqa_params = {\"l\":200,\n            \"l_split\":100,\n            \"num_names\":500,\n            \"max_models\":10000,\n            \"asp_timeout\": 60}\n\n\n\n    #load models #data/vqa18_10/slash_vqa_models_seed0_epoch_0.pt\n    #src/experiments/vqa/\n    #saved_models = torch.load(\"data/test/slash_vqa_models_seed42_epoch_9.pt\")\n    saved_models = torch.load(\"data/vqa_debug_relations_17_04_2023/slash_vqa_models_seed0_epoch_2.pt\")\n\n    print(saved_models.keys())\n    rela_clf.load_state_dict(saved_models['relation_clf'])\n    name_clf.load_state_dict(saved_models['name_clf'])\n    attr_clf.load_state_dict(saved_models['attr_clf'])\n\n    #create the SLASH Program , ,\n    nnMapping = {'relation': rela_clf, 'name':name_clf , \"attr\":attr_clf}\n    optimizers = {'relation': torch.optim.Adam(rela_clf.parameters(), lr=0.001, eps=1e-7),\n                      'name': torch.optim.Adam(name_clf.parameters(), lr=0.001, eps=1e-7),\n                      'attr': torch.optim.Adam(attr_clf.parameters(), lr=0.001, eps=1e-7)}\n\n\n\n    all_oid = np.arange(0,NUM_OBJECTS)\n    object_string = \"\".join([ f\"object({oid1},{oid2}). \" for oid1 in all_oid for oid2 in all_oid if oid1 != oid2])\n    object_string = \"\".join([\"\".join([f\"object({oid}). \" for oid in all_oid]), object_string])\n\n    #parse the SLASH program\n    print(\"create SLASH program\")\n    program = \"\".join([KG, RULES, object_string, name_npp, relation_npp, attribute_npp, program_example])\n    SLASHobj = SLASH(program, nnMapping, optimizers)\n\n    #load the data\n    if type(test_f) == str: \n        test_data = VQA(\"test\", test_f, NUM_OBJECTS)\n        test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=4)\n    #if we have multiple test files\n    elif type(test_f) == dict: \n        for key in test_f:\n            test_data = VQA(\"test\", test_f[key], NUM_OBJECTS)\n            test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=4)\n            test_f[key] = test_loader\n\n\n\n    print(\"---TEST---\")\n    if type(test_f) == str:\n        recall_5_test, test_time = SLASHobj.testVQA(test_loader, args.p_num, vqa_params=vqa_params)\n        print(\"test-recall@5\", recall_5_test)\n\n    elif type(test_f) == dict:\n        test_time = 0\n        recalls = []\n        for key in test_f:\n            recall_5_test, tt = SLASHobj.testVQA(test_f[key], args.p_num, vqa_params=vqa_params)\n            test_time += tt\n            recalls.append(recall_5_test)\n            print(\"test-recall@5_{}\".format(key), recall_5_test, \", test_time:\", tt )\n        print(\"test-recall@5_c_all\", np.mean(recalls), \", test_time:\", test_time)", "\n\nif __name__ == \"__main__\":\n    slash_vqa()\n"]}
{"filename": "src/experiments/vqa/sg_model.py", "chunked_list": ["\"\"\"\nThe source code is based on:\nScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\nJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\nAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\nhttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\"\"\"\n\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\ntorch.autograd.set_detect_anomaly(True)\n\ndef load_model(model, model_f, device):\n    print('loading model from %s' % model_f)\n    model.load_state_dict(torch.load(model_f, map_location=device))\n    model.eval()", "\nclass MLPClassifier(nn.Module):\n    def __init__(self, input_dim, latent_dim, output_dim, n_layers, dropout_rate):\n        super(MLPClassifier, self).__init__()\n\n        layers = []\n        layers.append(nn.Linear(input_dim, latent_dim))\n        layers.append(nn.ReLU())\n        layers.append(nn.BatchNorm1d(latent_dim))\n        layers.append(nn.Dropout(dropout_rate))\n        for _ in range(n_layers - 1):\n            layers.append(nn.Linear(latent_dim, latent_dim))\n            layers.append(nn.ReLU())\n            layers.append(nn.BatchNorm1d(latent_dim))\n            layers.append(nn.Dropout(dropout_rate))\n        layers.append(nn.Linear(latent_dim, output_dim))\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        logits = self.net(x)\n        return logits", "\nclass SceneGraphModel:\n    def __init__(self, feat_dim, n_names, n_attrs, n_rels, device, model_dir=None):\n        self.feat_dim = feat_dim\n        self.n_names = n_names\n        self.n_attrs = n_attrs\n        self.n_rels = n_rels\n        self.device = device\n\n        self._init_models()\n        if model_dir is not None:\n            self._load_models(model_dir)\n\n    def _load_models(self, model_dir):\n        for type in ['name', 'relation', 'attribute']:\n            load_model(\n                model=self.models[type],\n                model_f=model_dir+'/%s_best_epoch.pt' % type,\n                device=self.device\n            )\n\n    def _init_models(self):\n        name_clf = MLPClassifier(\n            input_dim=self.feat_dim,\n            output_dim=self.n_names,\n            latent_dim=1024,\n            n_layers=2,\n            dropout_rate=0.3\n        )\n\n        rela_clf = MLPClassifier(\n            input_dim=(self.feat_dim+4)*2,  # 4: bbox\n            output_dim=self.n_rels+1,       # 1: None\n            latent_dim=1024,\n            n_layers=1,\n            dropout_rate=0.5\n        )\n\n        attr_clf = MLPClassifier(\n            input_dim=self.feat_dim,\n            output_dim=self.n_attrs,\n            latent_dim=1024,\n            n_layers=1,\n            dropout_rate=0.3\n        )\n\n        self.models = {\n            'name': name_clf,\n            'attribute': attr_clf,\n            'relation': rela_clf\n        }\n\n    def predict(self, type, inputs):\n        # type == 'name', inputs == (obj_feat_np_array)\n        # type == 'relation', inputs == (sub_feat_np_array, obj_feat_np_array, sub_bbox_np_array, obj_bbox_np_array)\n        # type == 'attribute', inputs == (obj_feat_np_array)\n\n        model = self.models[type].to(self.device)\n        inputs = torch.cat([torch.from_numpy(x).float() for x in inputs]).reshape(len(inputs), -1).to(self.device)\n        logits = model(inputs)\n\n        if type == 'attribute':\n            probs = torch.sigmoid(logits)\n        else:\n            probs = F.softmax(logits, dim=1)\n\n        return logits, probs\n\n    def batch_predict(self, type, inputs, batch_split):\n\n        model = self.models[type].to(self.device)\n        inputs = torch.cat([torch.from_numpy(x).float() for x in inputs]).reshape(len(inputs), -1).to(self.device)\n        logits = model(inputs)\n\n        if type == 'attribute':\n            probs = torch.sigmoid(logits)\n        else:\n            current_split = 0\n            probs = []\n            for split in batch_split:\n                current_logits = logits[current_split:split]\n                # batched_logits = logits.reshape(batch_shape[0], batch_shape[1], -1)\n                current_probs = F.softmax(current_logits, dim=1)\n                # probs = probs.reshape(inputs.shape[0], -1)\n                probs.append(current_probs)\n                current_split = split\n\n            probs = torch.cat(probs).reshape(inputs.shape[0], -1)\n        return logits, probs", ""]}
{"filename": "src/experiments/vqa/word_idx_translator.py", "chunked_list": ["\"\"\"\nThe source code is based on:\nScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\nJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\nAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\nhttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\"\"\"\nimport json\n\n\nclass Idx2Word():\n\n    def __init__(self, meta_info, use_canon=False):\n        self.setup(meta_info)\n        self.attr_canon = meta_info['attr']['canon']\n        self.name_canon = meta_info['name']['canon']\n        self.rela_canon = meta_info['rel']['canon']\n\n        self.attr_alias = meta_info['attr']['alias']\n        self.rela_alias = meta_info['rel']['alias']\n\n        self.attr_to_idx_dict = meta_info['attr']['idx']\n        self.rela_to_idx_dict = meta_info['rel']['idx']\n        self.name_to_idx_dict = meta_info['name']['idx']\n        self.use_canon = use_canon\n        # print(\"here\")\n\n    def setup(self, meta_info):\n\n        attr_to_idx = meta_info['attr']['idx']\n        rela_to_idx = meta_info['rel']['idx']\n        name_to_idx = meta_info['name']['idx']\n\n        attr_freq = meta_info['attr']['freq']\n        rela_freq = meta_info['rel']['freq']\n        name_freq = meta_info['name']['freq']\n\n        # attr_group = meta_info['attr']['group']\n\n        def setup_single(to_idx, freq, group=None):\n            idx_to_name = {}\n            for name in freq:\n                if name not in to_idx:\n                    continue\n                idx = to_idx[name]\n                if type(idx) == list:\n                    if not idx[0] in idx_to_name.keys():\n                        idx_to_name[idx[0]] = {}\n                    idx_to_name[idx[0]][idx[1]] = name\n                else:\n                    idx_to_name[idx] = name\n            return idx_to_name\n\n        self.idx_to_name_dict = setup_single(name_to_idx, name_freq)\n        self.idx_to_rela_dict = setup_single(rela_to_idx, rela_freq)\n        self.idx_to_attr_dict = setup_single(attr_to_idx, attr_freq)\n        # self.idx_to_attr_dict = setup_single(attr_to_idx, attr_freq, attr_group)\n\n    def get_name_ct(self):\n        return len(self.idx_to_name_dict)\n\n    def get_rela_ct(self):\n        return len(self.idx_to_rela_dict)\n\n    def get_attr_ct(self):\n        return len(self.idx_to_attr_dict)\n\n    def get_names(self):\n        return list(self.idx_to_name_dict.values())\n\n    def idx_to_name(self, idx):\n        if idx is None:\n            return None\n        if type(idx) == str:\n            return idx\n        if len(self.idx_to_name_dict) == idx:\n            return None\n        if idx == -1:\n            return None\n        return self.idx_to_name_dict[idx]\n\n    def idx_to_rela(self, idx):\n        if idx is None:\n            return None\n        if idx == -1:\n            return None\n        if type(idx) == str:\n            return idx\n        if len(self.idx_to_rela_dict) == idx:\n            return None\n        return self.idx_to_rela_dict[idx]\n\n    def idx_to_attr(self, idx):\n        if idx is None:\n            return None\n        if type(idx) == str:\n            return idx\n        if len(self.idx_to_attr_dict) == idx:\n            return None\n        if idx == -1:\n            return None\n        # return self.idx_to_attr_dict[idx[0]][idx[1]]\n        return self.idx_to_attr_dict[idx]\n\n    def attr_to_idx(self, attr):\n        if attr is None:\n            return attr\n\n        if self.use_canon:\n            if attr in self.attr_canon.keys():\n                attr = self.attr_canon[attr]\n\n        if attr in self.attr_alias.keys():\n            attr = self.attr_alias[attr]\n\n        if attr not in self.attr_to_idx_dict.keys():\n            return None\n\n        return self.attr_to_idx_dict[attr]\n\n    def name_to_idx(self, name):\n\n        if name is None:\n            return name\n\n        if self.use_canon:\n            if name in self.name_canon.keys():\n                name = self.name_canon[name]\n\n        if name not in self.name_to_idx_dict.keys():\n            return None\n\n        return self.name_to_idx_dict[name]\n\n    def rela_to_idx(self, rela):\n        if rela is None:\n            return rela\n\n        if self.use_canon:\n            if rela in self.rela_canon.keys():\n                rela = self.rela_canon[rela]\n\n        if rela in self.rela_alias.keys():\n            rela = self.rela_alias[rela]\n\n        if rela not in self.rela_to_idx_dict.keys():\n            return None\n\n        return self.rela_to_idx_dict[rela]", "\n\nclass Idx2Word():\n\n    def __init__(self, meta_info, use_canon=False):\n        self.setup(meta_info)\n        self.attr_canon = meta_info['attr']['canon']\n        self.name_canon = meta_info['name']['canon']\n        self.rela_canon = meta_info['rel']['canon']\n\n        self.attr_alias = meta_info['attr']['alias']\n        self.rela_alias = meta_info['rel']['alias']\n\n        self.attr_to_idx_dict = meta_info['attr']['idx']\n        self.rela_to_idx_dict = meta_info['rel']['idx']\n        self.name_to_idx_dict = meta_info['name']['idx']\n        self.use_canon = use_canon\n        # print(\"here\")\n\n    def setup(self, meta_info):\n\n        attr_to_idx = meta_info['attr']['idx']\n        rela_to_idx = meta_info['rel']['idx']\n        name_to_idx = meta_info['name']['idx']\n\n        attr_freq = meta_info['attr']['freq']\n        rela_freq = meta_info['rel']['freq']\n        name_freq = meta_info['name']['freq']\n\n        # attr_group = meta_info['attr']['group']\n\n        def setup_single(to_idx, freq, group=None):\n            idx_to_name = {}\n            for name in freq:\n                if name not in to_idx:\n                    continue\n                idx = to_idx[name]\n                if type(idx) == list:\n                    if not idx[0] in idx_to_name.keys():\n                        idx_to_name[idx[0]] = {}\n                    idx_to_name[idx[0]][idx[1]] = name\n                else:\n                    idx_to_name[idx] = name\n            return idx_to_name\n\n        self.idx_to_name_dict = setup_single(name_to_idx, name_freq)\n        self.idx_to_rela_dict = setup_single(rela_to_idx, rela_freq)\n        self.idx_to_attr_dict = setup_single(attr_to_idx, attr_freq)\n        # self.idx_to_attr_dict = setup_single(attr_to_idx, attr_freq, attr_group)\n\n    def get_name_ct(self):\n        return len(self.idx_to_name_dict)\n\n    def get_rela_ct(self):\n        return len(self.idx_to_rela_dict)\n\n    def get_attr_ct(self):\n        return len(self.idx_to_attr_dict)\n\n    def get_names(self):\n        return list(self.idx_to_name_dict.values())\n\n    def idx_to_name(self, idx):\n        if idx is None:\n            return None\n        if type(idx) == str:\n            return idx\n        if len(self.idx_to_name_dict) == idx:\n            return None\n        if idx == -1:\n            return None\n        return self.idx_to_name_dict[idx]\n\n    def idx_to_rela(self, idx):\n        if idx is None:\n            return None\n        if idx == -1:\n            return None\n        if type(idx) == str:\n            return idx\n        if len(self.idx_to_rela_dict) == idx:\n            return None\n        return self.idx_to_rela_dict[idx]\n\n    def idx_to_attr(self, idx):\n        if idx is None:\n            return None\n        if type(idx) == str:\n            return idx\n        if len(self.idx_to_attr_dict) == idx:\n            return None\n        if idx == -1:\n            return None\n        # return self.idx_to_attr_dict[idx[0]][idx[1]]\n        return self.idx_to_attr_dict[idx]\n\n    def attr_to_idx(self, attr):\n        if attr is None:\n            return attr\n\n        if self.use_canon:\n            if attr in self.attr_canon.keys():\n                attr = self.attr_canon[attr]\n\n        if attr in self.attr_alias.keys():\n            attr = self.attr_alias[attr]\n\n        if attr not in self.attr_to_idx_dict.keys():\n            return None\n\n        return self.attr_to_idx_dict[attr]\n\n    def name_to_idx(self, name):\n\n        if name is None:\n            return name\n\n        if self.use_canon:\n            if name in self.name_canon.keys():\n                name = self.name_canon[name]\n\n        if name not in self.name_to_idx_dict.keys():\n            return None\n\n        return self.name_to_idx_dict[name]\n\n    def rela_to_idx(self, rela):\n        if rela is None:\n            return rela\n\n        if self.use_canon:\n            if rela in self.rela_canon.keys():\n                rela = self.rela_canon[rela]\n\n        if rela in self.rela_alias.keys():\n            rela = self.rela_alias[rela]\n\n        if rela not in self.rela_to_idx_dict.keys():\n            return None\n\n        return self.rela_to_idx_dict[rela]", "\n\ndef process_program(program, meta_info):\n\n    new_program = []\n\n    for clause in program:\n        new_clause = {}\n        new_clause['function'] = clause['function']\n\n        if 'output' in clause.keys():\n            new_clause['output'] = clause['output']\n\n        if clause['function'] == \"Hypernym_Find\":\n            name = clause['text_input'][0]\n            attr = clause['text_input'][1]\n\n            new_clause['text_input'] = [name, attr] + clause['text_input'][2:]\n\n        elif clause['function'] == \"Find\":\n            name = clause['text_input'][0]\n            new_clause['text_input'] = [name] + clause['text_input'][1:]\n\n        elif clause['function'] == \"Relate_Reverse\":\n            relation = clause['text_input']\n            new_clause['text_input'] = relation\n\n        elif clause['function'] == \"Relate\":\n            relation = clause['text_input']\n            new_clause['text_input'] = relation\n\n        else:\n            if 'text_input' in clause.keys():\n                new_clause['text_input'] = clause['text_input']\n\n        new_program.append(new_clause)\n\n    return new_program", "\n\ndef process_questions(questions_path, new_question_path, meta_info):\n    new_questions = {}\n\n    with open(questions_path, 'r') as questions_file:\n        questions = json.load(questions_file)\n\n    # process questions\n    for question in questions:\n\n        image_id = question[\"image_id\"]\n\n        # process questions\n        if image_id not in new_questions.keys():\n            new_questions[image_id] = {}\n        new_question = new_questions[image_id]\n\n        new_question['question_id'] = question['question_id']\n        new_question['program'] = process_program(\n            question[\"program\"], meta_info)\n\n        program = question['program']\n        new_question['target'] = program[-2][\"output\"]\n        new_question['question'] = question['question']\n        new_question['answer'] = question['answer']\n\n    with open(new_question_path, 'w') as new_question_file:\n        json.dump(new_questions, new_question_file)", "\n"]}
{"filename": "src/experiments/vqa/trainer.py", "chunked_list": ["\"\"\"\nThe source code is based on:\nScallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning\nJiani Huang, Ziyang Li, Binghong Chen, Karan Samel, Mayur Naik, Le Song, Xujie Si\nAdvances in Neural Information Processing Systems 34 (NeurIPS 2021)\nhttps://proceedings.neurips.cc/paper/2021/hash/d367eef13f90793bd8121e2f675f0dc2-Abstract.html\n\"\"\"\n\nimport os\nimport json", "import os\nimport json\nimport numpy as np\nimport sys\nfrom tqdm import tqdm\nimport torch\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.nn import BCELoss\nimport time", "from torch.nn import BCELoss\nimport time\nimport math\nimport statistics\n# import concurrent.futures\n\nsupervised_learning_path = os.path.abspath(os.path.join(\n    os.path.abspath(__file__), \"../../supervised_learning\"))\nsys.path.insert(0, supervised_learning_path)\n", "sys.path.insert(0, supervised_learning_path)\n\ncommon_path = os.path.abspath(os.path.join(os.path.abspath(__file__), \"../..\"))\nsys.path.insert(0, common_path)\n\nfrom query_lib import QueryManager\n#from src.experiments.vqa.cmd_args2 import cmd_args\nfrom word_idx_translator import Idx2Word\nfrom sg_model import SceneGraphModel\nfrom learning import get_fact_probs", "from sg_model import SceneGraphModel\nfrom learning import get_fact_probs\nfrom vqa_utils import auc_score, get_recall\n\ndef prog_analysis(datapoint):\n    knowledge_op = ['Hypernym_Find', 'KG_Find']\n    rela_op = ['Relate', 'Relate_Reverse']\n    clauses = datapoint['question']['clauses']\n    ops = [ clause['function'] for clause in clauses ]\n    kg_num = sum([1 if op in knowledge_op else 0 for op in ops])\n    rela_num = sum([1 if op in rela_op else 0 for op in ops])\n    return (kg_num, rela_num)", "\nclass ClauseNTrainer():\n\n    def __init__(self,\n                 train_data_loader,\n                 val_data_loader,\n                 test_data_loader=None,\n                 #model_dir=cmd_args.model_dir, n_epochs=cmd_args.n_epochs,\n                 #save_dir=cmd_args.save_dir,\n                 #meta_f=cmd_args.meta_f, knowledge_base_dir=cmd_args.knowledge_base_dir,\n                 #axiom_update_size=cmd_args.axiom_update_size):\n                 model_dir=\"data_model/\", n_epochs=2,\n                 save_dir=\"data_save/\",\n                 meta_f=\"dataset/gqa_info.json\", knowledge_base_dir=\"\",\n                 axiom_update_size=\"\"):\n\n\n        self.model_dir = model_dir\n        model_exists = self._model_exists(model_dir)\n\n        if not model_exists:\n            load_model_dir = None\n        else:\n            load_model_dir = model_dir\n\n        if train_data_loader is not None:\n            self.train_data = train_data_loader\n        if val_data_loader is not None:\n            self.val_data = val_data_loader\n        if test_data_loader is not None:\n            self.val_data = test_data_loader\n\n        self.is_train = test_data_loader is None\n        meta_info = json.load(open(meta_f, 'r'))\n        self.idx2word = Idx2Word(meta_info)\n        self.n_epochs = n_epochs\n        self.query_manager = QueryManager(save_dir)\n        self.axiom_update_size = axiom_update_size\n        self.wmc_funcs = {}\n\n        # load dictionary from previous training results\n        self.sg_model = SceneGraphModel(\n            feat_dim=64,\n            n_names=meta_info['name']['num'],\n            n_attrs=meta_info['attr']['num'],\n            n_rels=meta_info['rel']['num'],\n            device=torch.device('cuda'),\n            model_dir=load_model_dir\n        )\n\n        self.sg_model_dict = self.sg_model.models\n\n        self.loss_func = BCELoss()\n\n        if self.is_train:\n            self.optimizers = {}\n            self.schedulers = {}\n\n            for model_type, model_info in self.sg_model_dict.items():\n                if model_type == 'name':\n                    self.optimizers['name'] = optim.Adam(\n                        model_info.parameters(), lr=0.01)\n                    self.schedulers['name'] = StepLR(\n                        self.optimizers['name'], step_size=10, gamma=0.1)\n                    # self.loss_func['name'] = F.cross_entropy\n                if model_type == 'relation':\n                    self.optimizers['rel'] = optim.Adam(\n                        model_info.parameters(), lr=0.01)\n                    self.schedulers['rel'] = StepLR(\n                        self.optimizers['rel'], step_size=10, gamma=0.1)\n                if model_type == 'attribute':\n                    self.optimizers['attr'] = optim.Adam(\n                        model_info.parameters(), lr=0.01)\n                    self.schedulers['attr'] = StepLR(\n                        self.optimizers['attr'], step_size=10, gamma=0.1)\n\n        # self.pool = mp.Pool(cmd_args.max_workers)\n        # self.batch = cmd_args.trainer_batch\n\n    def _model_exists(self, model_dir):\n        name_f = os.path.join(self.model_dir, 'name_best_epoch.pt')\n        rela_f = os.path.join(self.model_dir, 'relation_best_epoch.pt')\n        attr_f = os.path.join(self.model_dir, 'attribute_best_epoch.pt')\n\n        if not os.path.exists(name_f):\n            return False\n        if not os.path.exists(rela_f):\n            return False\n        if not os.path.exists(attr_f):\n            return False\n        return True\n\n    def _get_optimizer(self, data_type):\n        optimizer = self.optimizers[data_type]\n        return optimizer\n\n    def _step_all(self):\n        for optim_type, optim_info in self.optimizers.items():\n            optim_info.step()\n\n    def _step_scheduler(self):\n        for scheduler_type, scheduler_info in self.schedulers.items():\n            scheduler_info.step()\n\n    def _zero_all(self):\n        for optim_type, optim_info in self.optimizers.items():\n            optim_info.zero_grad()\n\n    def _train_all(self):\n        for model_type, model_info in self.sg_model_dict.items():\n            if model_type == 'name':\n                model_info.train()\n                model_info.share_memory()\n            if model_type == 'relation':\n                model_info.train()\n                model_info.share_memory()\n            if model_type == 'attribute':\n                model_info.train()\n                model_info.share_memory()\n\n    def _eval_all(self):\n        for model_type, model_info in self.sg_model_dict.items():\n            if model_type == 'name':\n                model_info.eval()\n                model_info.share_memory()\n            if model_type == 'relation':\n                model_info.eval()\n                model_info.share_memory()\n            if model_type == 'attribute':\n                model_info.eval()\n                model_info.share_memory()\n\n    def _save_all(self):\n        for model_type, model_info in self.sg_model_dict.items():\n            if model_type == 'name':\n                save_f = os.path.join(self.model_dir, 'name_best_epoch.pt')\n                torch.save(model_info.state_dict(), save_f)\n            if model_type == 'relation':\n                save_f = os.path.join(self.model_dir, 'relation_best_epoch.pt')\n                torch.save(model_info.state_dict(), save_f)\n            if model_type == 'attribute':\n                save_f = os.path.join(self.model_dir, 'attribute_best_epoch.pt')\n                torch.save(model_info.state_dict(), save_f)\n\n    def loss_acc(self, targets, correct, all_oids, is_train=True):\n\n        pred = []\n        for oid in all_oids:\n            if oid in targets:\n                pred.append(targets[oid])\n            else:\n                pred.append(0)\n\n        labels = [1 if obj in correct else 0 for obj in all_oids]\n\n        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n        pred_tensor = torch.tensor(pred, dtype=torch.float32)\n\n        pred_tensor = pred_tensor.reshape(1, -1)\n        labels_tensor = labels_tensor.reshape(1, -1)\n\n        loss = self.loss_func(pred_tensor, labels_tensor)\n        recall = get_recall(labels_tensor, pred_tensor)\n\n        if math.isnan(recall):\n            recall = -1\n\n        return loss.item(), recall\n\n    def _pass(self, datapoint, is_train=True):\n\n        correct = datapoint['question']['output']\n        all_oid = datapoint['question']['input']\n        fact_tps, fact_probs = get_fact_probs(self.sg_model, datapoint, self.idx2word, self.query_manager)\n        result, timeout = self.query_manager.get_result(datapoint, fact_tps, fact_probs)\n        if not timeout:\n            loss, acc = self.loss_acc(result, correct, all_oid)\n        else:\n            loss = -1\n            acc = -1\n\n        return acc, loss, timeout\n\n    def _train_epoch(self, ct):\n\n        self._train_all()\n        aucs = []\n        losses = []\n        timeouts = 0\n        pbar = tqdm(self.train_data)\n\n        for datapoint in pbar:\n            auc, loss, timeout = self._pass(datapoint, is_train=True)\n            if not timeout:\n                if auc >= 0:\n                    aucs.append(auc)\n                losses.append(loss)\n            else:\n                timeouts += 1\n\n            pbar.set_description(\n                f'[loss: {np.array(losses).mean()}, auc: {np.array(aucs).mean()}, timeouts: {timeouts}]')\n            torch.cuda.empty_cache()\n\n            self._step_all()\n            self._zero_all()\n\n        return np.mean(losses), np.mean(aucs)\n\n    def _val_epoch(self):\n        self._eval_all()\n\n        timeouts = 0\n        aucs = []\n        losses = []\n        time_out_prog_kg = {}\n        time_out_prog_rela = {}\n        success_prog_kg = {}\n        success_prog_rela = {}\n\n        pbar = tqdm(self.val_data)\n        with torch.no_grad():\n            for datapoint in pbar:\n                kg_num, rela_num = prog_analysis(datapoint)\n\n                auc, loss, timeout = self._pass(datapoint, is_train=False)\n                if not timeout:\n                    aucs.append(auc)\n                    losses.append(loss)\n                    if not kg_num in success_prog_kg:\n                        success_prog_kg[kg_num] = 0\n                    if not rela_num in success_prog_rela:\n                        success_prog_rela[rela_num] = 0\n                    success_prog_kg[kg_num] += 1\n                    success_prog_rela[rela_num] += 1\n\n                else:\n                    timeouts += 1\n                    if not kg_num in time_out_prog_kg:\n                        time_out_prog_kg[kg_num] = 0\n                    if not rela_num in time_out_prog_rela:\n                        time_out_prog_rela[rela_num] = 0\n                    time_out_prog_kg[kg_num] += 1\n                    time_out_prog_rela[rela_num] += 1\n\n                if not len(aucs) == 0:\n                    pbar.set_description(\n                        f'[loss: {np.array(losses).mean()}, auc: {np.array(aucs).mean()}, timeouts: {timeouts}]')\n\n        print(f\"succ kg: {success_prog_kg}, succ rela: {success_prog_rela}\")\n        print(f\"timeout kg: {time_out_prog_kg}, timeout rela: {time_out_prog_rela}\")\n        return np.mean(losses), np.mean(aucs)\n\n\n    def train(self):\n        assert self.is_train\n        best_val_loss = np.inf\n\n        for epoch in range(self.n_epochs):\n            train_loss, train_acc = self._train_epoch(epoch)\n            val_loss, val_acc = self._val_epoch()\n            self._step_scheduler()\n\n            print(\n                '[Epoch %d/%d] [training loss: %.2f, auc: %.2f] [validation loss: %.2f, auc: %.2f]' %\n                (epoch, self.n_epochs, train_loss, train_acc, val_loss, val_acc)\n            )\n\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                print('saving best models')\n                self._save_all()\n\n    def test(self):\n        assert not self.is_train\n        test_loss, test_acc = self._val_epoch()\n        print('[test loss: %.2f, acc: %.2f]' % (test_loss, test_acc))", ""]}
{"filename": "src/experiments/slash_attention/ap_utils.py", "chunked_list": ["import numpy as np\nimport torch \nimport time\nimport random\n\n'''\nAll helper functions to compute the average precision for different datasets\n'''\n\n\ndef get_obj_encodings(dataloader):\n    '''\n    Collects all object encodings in a single tensor for the test dataset\n    @param dataloader: Dataloader object for a shapeworld/clevr like dataset that returns an object encoding \n    '''\n    obj_list = []\n    for _,_,obj_encoding in dataloader:\n        for i in range(0, len(obj_encoding)):\n            obj_list.append(obj_encoding[i])\n\n    obj_list = torch.stack(obj_list)\n    \n    return obj_list", "\n\ndef get_obj_encodings(dataloader):\n    '''\n    Collects all object encodings in a single tensor for the test dataset\n    @param dataloader: Dataloader object for a shapeworld/clevr like dataset that returns an object encoding \n    '''\n    obj_list = []\n    for _,_,obj_encoding in dataloader:\n        for i in range(0, len(obj_encoding)):\n            obj_list.append(obj_encoding[i])\n\n    obj_list = torch.stack(obj_list)\n    \n    return obj_list", "\n\n\ndef inference_map_to_array(inference, only_color=False, only_shape=False, only_shade=False, only_size=False, only_material=False):\n    '''\n    Returns an encoding of the output objects having shape [bs, num_slots, properties] for the slot attention experiment. The encoding can be used then for the AP metric.\n    properties depends on the number of classes + 1 for the prediction confidence  predicted e.g. red,green,blue + prediction confidence -> 3+1 = 4\n    @param inference: Hashmap of the form {'slot':{'color' : [bs,num_colors], ...}, ...}\n    '''\n    \n    all_object_encodings = None #contains the encoding all slots over the batchsize\n    \n    #iterate over all slots and collect the properties \n    for slot in inference:\n        slot_object_encoding = torch.Tensor([]) #contains the encoding for one slot over the batchsize\n        \n        prediction_confidence = None\n        \n        \n        #iterate over all properties and put them together into one vector\n        for prop in inference[slot]:\n            slot_object_encoding = torch.cat((slot_object_encoding, inference[slot][prop].cpu()), axis=1)\n            \n\n            #select the prediction confidence. #We want to select the highest prediction value for the selected properties e.g. col, shape, col+shape\n            if (prop == 'color' and only_color) or (prop == 'shape' and only_shape) or (prop == 'shade' and only_shade) or (prop == 'size' and only_size) or (prop == 'material' and only_material) or (only_color is False and only_shape is False and only_shade is False and only_size is False): \n                #collect the argmax prediction values for each property\n\n                if prediction_confidence is None:\n                    prediction_confidence = torch.max(inference[slot][prop].cpu(), axis=1)[0][None,:]\n                else:\n                    prediction_confidence = torch.cat((prediction_confidence, torch.max(inference[slot][prop].cpu(), axis=1)[0][None,:]))\n\n                    \n        #take the mean over the prediction confidence if we have more than one property and then append it to the properties tensor\n        if only_color is False and only_shape is False:\n            prediction_confidence = prediction_confidence.mean(axis=0)\n        else:\n            prediction_confidence = prediction_confidence.squeeze()\n        \n        slot_object_encoding = torch.cat((slot_object_encoding, prediction_confidence[:,None]), axis=1)\n\n        \n        #concatenate all slot encodings\n        if all_object_encodings is None:\n            all_object_encodings = slot_object_encoding[None,:,:]\n        else:\n            all_object_encodings = torch.cat((all_object_encodings, slot_object_encoding[None,:,:]), axis=0)\n    return torch.einsum(\"abc->bac\", all_object_encodings)", "\n        \ndef average_precision(pred, attributes, distance_threshold, dataset='', subset_size=None, only_color=False, only_shape=False, only_shade=False, only_size=False, only_material=False):\n    \"\"\"Computes the average precision for CLEVR or Shapeworld.\n\n    This function computes the average precision of the predictions specifically\n    for the CLEVR dataset. First, we sort the predictions of the model by\n    confidence (highest confidence first). Then, for each prediction we check\n    whether there was a corresponding object in the input image. A prediction is\n    considered a true positive if the discrete features are predicted correctly\n    and the predicted position is within a certain distance from the ground truth\n    object.\n\n    Args:\n    pred: Tensor of shape [batch_size, num_elements, dimension] containing\n      predictions. The last dimension is expected to be the confidence of the\n      prediction.\n    attributes: Tensor of shape [batch_size, num_elements, dimension] containing\n      ground-truth object properties.\n    distance_threshold: Threshold to accept match. -1 indicates no threshold.\n    only_color: Only consider color as a relevant property for a match.\n    only_shape: Only consider shape as a relevant property for a match.\n    only_shade: Consider only shade as a relevant property for a match. SHAPEWORLD\n    only_material: Consider only material as a relevant property for a match. CLEVR\n\n    Returns:\n    Average precision of the predictions.\n    \"\"\"\n\n    if subset_size is not None:\n        idx = random.sample(list(np.arange(0,attributes.shape[0])), subset_size)\n        attributes = attributes[idx,:,:]\n        pred = pred[idx,:,:]\n\n\n\n    [batch_size, _, element_size] = attributes.shape\n    [_, predicted_elements, _] = pred.shape\n\n\n    def unsorted_id_to_image(detection_id, predicted_elements):\n        \"\"\"Find the index of the image from the unsorted detection index.\"\"\"\n        return int(detection_id // predicted_elements)\n\n    flat_size = batch_size * predicted_elements\n    flat_pred = np.reshape(pred, [flat_size, element_size])\n    sort_idx = np.argsort(flat_pred[:, -1], axis=0)[::-1]  # Reverse order.\n\n    sorted_predictions = np.take_along_axis(\n        flat_pred, np.expand_dims(sort_idx, axis=1), axis=0)\n    idx_sorted_to_unsorted = np.take_along_axis(\n      np.arange(flat_size), sort_idx, axis=0)\n    \n\n    def process_targets_CLEVR(target):\n        \"\"\"Unpacks the target into the CLEVR properties.\"\"\"\n        size = np.argmax(target[:3])\n        material = np.argmax(target[3:6])\n        shape = np.argmax(target[6:10])\n        color = np.argmax(target[10:19])\n        real_obj = target[19]\n        \n        attr = [size, material, shape, color]\n        \n        is_bg = False\n        if color == 8 and shape == 3 and material == 2 and size == 2:\n            is_bg = True\n           \n        if only_color: #consider only the color properties for comparison\n            attr = [color]                \n        if only_shape: #consider only the shape properties for comparison\n            attr = [shape]                \n        if only_material: #consider only the shade properties for comparison\n            attr = [material]                \n        if only_size: #consider only the size properties for comparison\n            attr = [size]             \n\n        coords = np.array([0,0,0]) # We don't learn the coordinates of the objects\n        # return coords, object_size, material, shape, color, real_obj\n        return coords, attr, is_bg, real_obj\n    \n      \n    \n    \n    def process_targets_SHAPEWORLD4(target):\n        \"\"\"Unpacks the target into the Shapeworld properties.\"\"\"\n        \n        #[c,c,c,c,c,c,c,c,c , s,s,s,s , h,h,h, z,z,z, confidence]\n            \n        color = np.argmax(target[:9])\n        shape = np.argmax(target[9:13])\n        shade = np.argmax(target[13:16])\n        size = np.argmax(target[16:19])\n        real_obj = target[19]\n        \n        \n        is_bg= False\n        attr = [shape, color, shade, size]\n\n        if color == 8 and shape == 3 and shade == 2 and size == 2:\n            is_bg = True\n                \n        if only_color: #consider only the color properties for comparison\n            attr = [color]                \n        if only_shape: #consider only the shape properties for comparison\n            attr = [shape]                \n        if only_shade: #consider only the shade properties for comparison\n            attr = [shade]                \n        if only_size: #consider only the size properties for comparison\n            attr = [size]                \n        \n        coords = np.array([0,0,0])\n        return coords, attr, is_bg, real_obj\n    \n    \n    def process_targets_SHAPEWORLD_OOD(target):\n        \"\"\"Unpacks the target into the Shapeworld ood properties.\"\"\"\n        \n        #[c,c,c,c,c,c,c,c,c , s,s,s,s,s,s , h,h,h, z,z,z, confidence]\n            \n        color = np.argmax(target[:9])\n        shape = np.argmax(target[9:15])\n        shade = np.argmax(target[15:18])\n        size = np.argmax(target[18:21])\n        real_obj = target[21]\n        \n        \n        is_bg= False\n        attr = [shape, color, shade, size]\n\n        if color == 8 and shape == 5 and shade == 2 and size == 2:\n            is_bg = True\n                \n        if only_color: #consider only the color properties for comparison\n            attr = [color]                \n        if only_shape: #consider only the shape properties for comparison\n            attr = [shape]                \n        if only_shade: #consider only the shade properties for comparison\n            attr = [shade]                \n        if only_size: #consider only the size properties for comparison\n            attr = [size]                \n        \n        coords = np.array([0,0,0])\n        return coords, attr, is_bg, real_obj\n        \n    #switch for different dataset encodings\n    def process_targets(target):\n        if dataset == 'SHAPEWORLD4':\n            return process_targets_SHAPEWORLD4(target)\n        elif dataset == 'SHAPEWORLD_OOD':\n            return process_targets_SHAPEWORLD_OOD(target)\n        elif dataset == 'CLEVR':\n            return process_targets_CLEVR(target)\n        else:\n            raise RuntimeError('AP metric not implemented for dataset '+dataset)\n\n    true_positives = np.zeros(sorted_predictions.shape[0])\n    false_positives = np.zeros(sorted_predictions.shape[0])\n    true_negatives = np.zeros(sorted_predictions.shape[0]) \n    \n    detection_set = set()\n    match_count = 0\n    \n    for detection_id in range(sorted_predictions.shape[0]):\n        # Extract the current prediction.\n        current_pred = sorted_predictions[detection_id, :]\n        # Find which image the prediction belongs to. Get the unsorted index from\n        # the sorted one and then apply to unsorted_id_to_image function that undoes\n        # the reshape.\n        original_image_idx = unsorted_id_to_image(\n            idx_sorted_to_unsorted[detection_id], predicted_elements)\n        # Get the ground truth image.\n        gt_image = attributes[original_image_idx, :, :]\n\n        # Initialize the maximum distance and the id of the groud-truth object that\n        # was found.\n        best_distance = 10000\n        best_id = None\n          \n\n        # Unpack the prediction by taking the argmax on the discrete attributes.\n        #(pred_coords, pred_object_size, pred_material, pred_shape, pred_color,_) = process_targets(current_pred)\n        (pred_coords, pred_attr, is_bg, _) = process_targets(current_pred)\n        \n        \n        # Loop through all objects in the ground-truth image to check for hits.\n        for target_object_id in range(gt_image.shape[0]):\n            target_object = gt_image[target_object_id, :]\n            \n            \n            # Unpack the targets taking the argmax on the discrete attributes.\n            #(target_coords, target_object_size, target_material, target_shape,\n            #   target_color, target_real_obj) = process_targets(target_object)\n            \n            (target_coords, target_attr ,_, target_real_obj) = process_targets(target_object)\n                        \n            # Only consider real objects as matches.\n            if target_real_obj:\n                # For the match to be valid all attributes need to be correctly\n                # predicted.\n            \n                match = pred_attr == target_attr\n                \n                if match:\n                    # If a match was found, we check if the distance is below the\n                    # specified threshold. Recall that we have rescaled the coordinates\n                    # in the dataset from [-3, 3] to [0, 1], both for `target_coords` and\n                    # `pred_coords`. To compare in the original scale, we thus need to\n                    # multiply the distance values by 6 before applying the norm.\n                    distance = np.linalg.norm((target_coords - pred_coords) * 6.)\n\n                    match_count +=1\n                    # If this is the best match we've found so far in terms of distance we remember it.\n                    if distance < best_distance:\n                        best_distance = distance\n                        best_id = target_object_id\n                        \n                    #If we found a match we need to check if another object with the same attributes was already assigned to it.\n                    elif distance_threshold == -1 and (original_image_idx,target_object_id) not in detection_set:\n                        best_id = target_object_id\n        \n        if best_distance < distance_threshold or distance_threshold == -1:\n            # We have detected an object correctly within the distance confidence.\n            # If this object was not detected before it's a true positive.\n            if best_id is not None:\n                if (original_image_idx, best_id) not in detection_set and is_bg:\n                    true_negatives[detection_id] = 1\n                    detection_set.add((original_image_idx, best_id))\n                elif (original_image_idx, best_id) not in detection_set:\n                    true_positives[detection_id] = 1\n                    detection_set.add((original_image_idx, best_id))\n\n                else:\n                    false_positives[detection_id] = 1\n            else:\n                false_positives[detection_id] = 1\n        else:\n            false_positives[detection_id] = 1\n    \n\n    accumulated_fp = np.cumsum(false_positives)\n    accumulated_tp = np.cumsum(true_positives)\n    accumulated_tn = np.cumsum(true_negatives)\n    \n    #save tp, fp, tn\n    true_positives = accumulated_tp[-1]\n    false_positives = accumulated_fp[-1]\n    true_negatives = accumulated_tn[-1]\n    \n    \n    #the relevant examples is the amount of object substracted by the background detection per image(true negative)\n    relevant_examples = batch_size * predicted_elements - true_negatives\n\n    recall_array = accumulated_tp / relevant_examples\n    precision_array = np.divide(accumulated_tp, (accumulated_fp + accumulated_tp))\n    #print(detection_set)\n\n    \n    objects_detected = np.zeros((batch_size,predicted_elements)) \n    for detection in detection_set:\n        objects_detected[detection[0], detection[1]] = 1\n\n    \n    #check if all detections are true and then count all correctly classified images\n    correctly_classified = np.sum(np.all(objects_detected, axis=1))\n    \n    \n    return compute_average_precision(\n      np.array(precision_array, dtype=np.float32),\n      np.array(recall_array, dtype=np.float32)), true_positives, false_positives, true_negatives, correctly_classified", "\n\n\ndef compute_average_precision(precision, recall):\n    \"\"\"Computation of the average precision from precision and recall arrays.\"\"\"\n    recall = recall.tolist()\n    precision = precision.tolist()\n    recall = [0] + recall + [1]\n    precision = [0] + precision + [0]\n\n    for i in range(len(precision) - 1, -0, -1):\n        precision[i - 1] = max(precision[i - 1], precision[i])\n\n    indices_recall = [\n      i for i in range(len(recall) - 1) if recall[1:][i] != recall[:-1][i]\n    ]\n\n    average_precision = 0.\n    for i in indices_recall:\n        average_precision += precision[i + 1] * (recall[i + 1] - recall[i])\n    return average_precision"]}
{"filename": "src/experiments/slash_attention/clevr/train.py", "chunked_list": ["print(\"start importing...\")\n\nimport time\nimport sys\nimport os\nimport json\nsys.path.append('../../../')\nsys.path.append('../../../SLASH/')\nsys.path.append('../../../EinsumNetworks/src/')\n", "sys.path.append('../../../EinsumNetworks/src/')\n\nwb = True\nimport wandb\n\n#torch, numpy, ...\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\ntorch.cuda.empty_cache()\n", "torch.cuda.empty_cache()\n\n\nimport numpy as np\nimport importlib\n\n#own modules\n\nfrom dataGen import CLEVR\nfrom auxiliary import get_files_names_and_paths, get_slash_program", "from dataGen import CLEVR\nfrom auxiliary import get_files_names_and_paths, get_slash_program\nfrom einsum_wrapper import EiNet\nfrom slash import SLASH\nimport utils\nimport ap_utils\nfrom utils import set_manual_seed\nfrom slot_attention_module import SlotAttention_model\nfrom slot_attention_module import SlotAttention_model\nfrom pathlib import Path", "from slot_attention_module import SlotAttention_model\nfrom pathlib import Path\nfrom rtpt import RTPT\nprint(\"...done\")\n\n\n\ndef slash_slot_attention(exp_name , exp_dict):\n    \n    # Set the seeds for PRNG\n    set_manual_seed(exp_dict['seed'])\n\n    # Create RTPT object\n    rtpt = RTPT(name_initials=exp_dict['credentials'], experiment_name=f'SLASH Attention CLEVR %s' % exp_dict['obj_num'], max_iterations=int(exp_dict['epochs']))\n    \n    # Start the RTPT tracking\n    rtpt.start()\n    \n\n    # create save paths and tensorboard writer\n    writer = SummaryWriter(os.path.join(\"runs\", exp_name,exp_dict['method'], str(exp_dict['seed'])), purge_step=0)\n    saveModelPath = 'data/'+exp_name+'_'+exp_dict['method']+'/slash_slot_models_seed'+str(exp_dict['seed'])+'.pt'\n    Path(\"data/\"+exp_name+'_'+exp_dict['method']+\"/\").mkdir(parents=True, exist_ok=True)\n\n    # save args\n    with open(os.path.join(\"runs\", exp_name, 'args.json'), 'w') as json_file:\n        json.dump(exp_dict, json_file, indent=4)\n\n    print(\"Experiment parameters:\", exp_dict)\n\n    if wb:\n        #start a new wandb run to track this script\n        wandb.init(\n        # set the wandb project where this run will be logged\n        project=\"slash_attention_clevr\",\n        \n        # track hyperparameters and run metadata\n        config=exp_dict)\n\n\n    #setup new SLASH program given the network parameters\n    program = get_slash_program(exp_dict['obj_num'])\n    \n    #setup new SLASH program given the network parameters\n    if exp_dict['structure'] == 'poon-domingos':\n        exp_dict['depth'] = None\n        exp_dict['num_repetitions'] = None\n        print(\"using poon-domingos\")\n\n    elif exp_dict['structure'] == 'binary-trees':\n        exp_dict['pd_num_pieces'] = None\n        print(\"using binary-trees\")\n              \n    #size network\n    size_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 64,\n        pd_width = 8,\n        pd_height = 8,\n        class_count = 3,\n        use_em = exp_dict['use_em'],\n        learn_prior = exp_dict['learn_prior'])\n    \n    #material network\n    material_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 64,\n        pd_width = 8,\n        pd_height = 8,\n        class_count = 3,\n        use_em = exp_dict['use_em'],\n        learn_prior = exp_dict['learn_prior'])\n    \n    #shape network\n    shape_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 64,\n        pd_width = 8,\n        pd_height = 8,\n        class_count = 4,\n        use_em = exp_dict['use_em'],\n        learn_prior = exp_dict['learn_prior'])\n    \n    #color network\n    color_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 64,\n        pd_width = 8,\n        pd_height = 8,\n        class_count = 9,\n        use_em = exp_dict['use_em'],\n        learn_prior = exp_dict['learn_prior'])\n        \n    \n    \n    #create the Slot Attention network\n    slot_net = SlotAttention_model(n_slots=exp_dict['obj_num'], n_iters=3, n_attr=18,\n                                   encoder_hidden_channels=64, attention_hidden_channels=128, clevr_encoding=True)\n    slot_net = slot_net.to(device='cuda')\n        \n\n\n    #trainable params\n    num_trainable_params = [sum(p.numel() for p in size_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in material_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in shape_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in color_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in slot_net.parameters() if p.requires_grad)]\n    num_params = [sum(p.numel() for p in size_net.parameters()), \n                  sum(p.numel() for p in material_net.parameters()), \n                  sum(p.numel() for p in shape_net.parameters()), \n                  sum(p.numel() for p in color_net.parameters()),\n                  sum(p.numel() for p in slot_net.parameters())]\n    \n    print(\"training with {}({}) trainable params and {}({}) params in total\".format(np.sum(num_trainable_params),num_trainable_params,np.sum(num_params),num_params))\n         \n            \n    slot_net_params = list(slot_net.parameters())\n    smsc_params = list(size_net.parameters()) + list(material_net.parameters()) + list(shape_net.parameters())  + list(color_net.parameters()) \n    \n    #create the SLASH Program\n    nnMapping = {'size': size_net,\n                 'material': material_net,\n                 'shape': shape_net,\n                 'color': color_net\n                }\n        \n    \n    #OPTIMIZERS\n    optimizers = {'smsc': torch.optim.Adam([\n                                        {'params':smsc_params}],\n                                        lr=exp_dict['lr'], eps=1e-7),\n                 'slot': torch.optim.Adam([\n                                        {'params': slot_net_params}],\n                                        lr=0.0004, eps=1e-7)}\n\n    \n    SLASHobj = SLASH(program, nnMapping, optimizers)\n    SLASHobj.grad_comp_device ='cpu' #set gradient computation to cpu\n\n    \n\n    print(\"using learning rate warmup and decay\")\n    warmup_epochs = exp_dict['lr_warmup_steps'] #warmup for x epochs\n    decay_epochs = exp_dict['lr_decay_steps']\n    slot_base_lr = 0.0004\n        \n        \n    #metric lists\n    test_ap_list = [] #stores average precsion values\n    test_metric_list = [] #stores tp, fp, tn values\n    lr_list = [] # store learning rate\n    loss_list = []  # store training loss\n    startTime = time.time()\n    train_test_times = []\n    sm_per_batch_list = []\n\n    forward_time_list = []\n    asp_time_list = []\n    gradient_time_list = []\n    backward_time_list = []\n    \n    # Load data\n    obj_num = exp_dict['obj_num']\n    root = '/SLASH/data/CLEVR_v1.0/'\n    mode = 'train'\n    img_paths, files_names = get_files_names_and_paths(root=root, mode=mode, obj_num=obj_num)\n    \n    train_dataset_loader = torch.utils.data.DataLoader(CLEVR(root,mode,img_paths,files_names,obj_num), shuffle=True,batch_size=exp_dict['bs'],num_workers=8)\n    \n    mode = 'val'\n    img_paths, files_names = get_files_names_and_paths(root=root, mode=mode, obj_num=obj_num)\n    test_dataset_loader= torch.utils.data.DataLoader(CLEVR(root,mode,img_paths,files_names,obj_num), shuffle=False,batch_size=exp_dict['bs'], num_workers=8)\n\n    obj_encodings_gt = ap_utils.get_obj_encodings(test_dataset_loader)\n\n    print(\"loaded data\")\n    \n    \n    # Resume the training if requested\n    start_e= 0\n    if exp_dict['resume']:\n        print(\"resuming experiment\")\n        saved_model = torch.load(saveModelPath)\n        \n        #load pytorch models\n        color_net.load_state_dict(saved_model['color_net'])\n        shape_net.load_state_dict(saved_model['shape_net'])\n        material_net.load_state_dict(saved_model['material_net'])\n        size_net.load_state_dict(saved_model['size_net'])\n        slot_net.load_state_dict(saved_model['slot_net'])\n        \n        \n        #optimizers and shedulers\n        optimizers['smsc'].load_state_dict(saved_model['resume']['optimizer_smsc'])\n        optimizers['slot'].load_state_dict(saved_model['resume']['optimizer_slot'])\n        start_e = saved_model['resume']['epoch']\n    \n        #metrics\n        test_ap_list = saved_model['test_ap_list']\n        test_metric_list = saved_model['test_metric_list']\n        lr_list = saved_model['lr_list']\n        loss_list = saved_model['loss_list']\n        train_test_times = saved_model['train_test_times']\n        sm_per_batch_list = saved_model['sm_per_batch_list']  \n    \n\n    \n\n    # train the network and evaluate the performance\n    for e in range(start_e, exp_dict['epochs']):\n        #we have three datasets right now train, val and test with 20k, 5k and 100 samples\n                \n        #TRAIN\n        print('Epoch {}/{}...'.format(e+1, exp_dict['epochs']))\n        time_train = time.time()\n        \n        #apply lr schedulers\n        if e < warmup_epochs:\n            lr = slot_base_lr * ((e+1)/warmup_epochs)\n        else:\n            lr = slot_base_lr\n        lr = lr * 0.5**((e+1)/decay_epochs)\n        optimizers['slot'].param_groups[0]['lr'] = lr\n        lr_list.append([lr,e])\n        print(\"LR SAm:\", \"{:.6f}\".format(lr), optimizers['slot'].param_groups[0]['lr'])\n\n\n        loss, forward_time, asp_time, gradient_time, backward_time, sm_per_batch  = SLASHobj.learn(dataset_loader = train_dataset_loader, slot_net=slot_net,hungarian_matching = True, method=exp_dict['method'], p_num=exp_dict['p_num'], k_num=1,\n                              epoch=e, writer = writer, batched_pass = True)\n        \n        forward_time_list.append(forward_time)\n        asp_time_list.append(asp_time)\n        gradient_time_list.append(gradient_time)\n        backward_time_list.append(backward_time)\n        sm_per_batch_list.append(sm_per_batch)\n        loss_list.append(loss)\n        writer.add_scalar(\"train/loss\", loss, global_step=e)\n\n        \n        timestamp_train, time_train_sec = utils.time_delta_now(time_train)\n                \n        #TEST\n        time_test = time.time()\n\n        \n        #forward test batch\n        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader)\n\n        #compute the average precision, tp, fp, tn for color+shape+material+size\n        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n\n\n        #use only a fraction of the test data, except every 100th epoch\n        if e % 100 == 0:\n            subset_size = None\n        else:\n            subset_size = 1500\n\n        ap, true_positives,false_positives, true_negatives, correctly_classified  = ap_utils.average_precision(pred, obj_encodings_gt, -1, \"CLEVR\",subset_size = subset_size)\n        print(\"avg precision \", ap, \"tp\", true_positives, \"fp\", false_positives, \"tn\", true_negatives, \"correctly classified\", correctly_classified)\n\n        '''\n        #color\n        pred_c = ap_utils.inference_map_to_array(inference, only_color=True).cpu().numpy()\n        ap_c, true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c = ap_utils.average_precision(pred, obj_encodings_gt, -1, \"CLEVR\", only_color=True)\n        print(\"avg precision color\", ap_c, \"tp\", true_positives_c, \"fp\", false_positives_c, \"tn\", true_negatives_c, \"correctly classified\", correctly_classified_c)\n\n        #shape              \n        pred_s = ap_utils.inference_map_to_array(inference, only_shape=True).cpu().numpy()\n        ap_s, true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s = ap_utils.average_precision(pred_s, obj_encodings_gt, -1, \"CLEVR\", only_shape=True)\n        print(\"avg precision shape\", ap_s, \"tp\", true_positives_s, \"fp\", false_positives_s, \"tn\", true_negatives_s, \"correctly classified\", correctly_classified_s)\n        \n        #material              \n        pred_m = ap_utils.inference_map_to_array(inference, only_material=True).cpu().numpy()\n        ap_m, true_positives_m, false_positives_m, true_negatives_m, correctly_classified_m = ap_utils.average_precision(pred_m, obj_encodings_gt, -1, \"CLEVR\", only_material=True)\n        print(\"avg precision material\", ap_m, \"tp\", true_positives_m, \"fp\", false_positives_m, \"tn\", true_negatives_m, \"correctly classified\", correctly_classified_m)\n        \n        #size              \n        pred_x = ap_utils.inference_map_to_array(inference, only_size=True).cpu().numpy()\n        ap_x, true_positives_x, false_positives_x, true_negatives_x, correctly_classified_x = ap_utils.average_precision(pred_x, obj_encodings_gt, -1, \"CLEVR\", only_size=True)\n        print(\"avg precision size\", ap_x, \"tp\", true_positives_x, \"fp\", false_positives_x, \"tn\", true_negatives_x, \"correctly classified\", correctly_classified_x)\n\n        \n        #store ap, tp, fp, tn\n        test_ap_list.append([ap, ap_c, ap_s, ap_m, ap_x, e])                    \n        test_metric_list.append([true_positives, false_positives, true_negatives, correctly_classified,\n                                 true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c,\n                                 true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s,\n                                 true_positives_m, false_positives_m, true_negatives_m, correctly_classified_m,\n                                 true_positives_x, false_positives_x, true_negatives_x, correctly_classified_x])\n        '''\n\n        if wb:\n            wandb.log({\"ap\": ap,\n                        \"loss\":loss,\n                        \"forward_time\":np.sum(forward_time),\n                        \"asp_time\":np.sum(asp_time),\n                        \"gradient_time\": np.sum(gradient_time),\n                        \"backward_time\": np.sum(backward_time),\n                        \"sm_per_batch\": np.sum(sm_per_batch)})\n\n        test_ap_list.append([ap, e])                    \n        test_metric_list.append([true_positives, false_positives, true_negatives, correctly_classified])\n        \n        #Tensorboard outputs\n        writer.add_scalar(\"test/ap\", ap, global_step=e)\n        #writer.add_scalar(\"test/ap_c\", ap_c, global_step=e)\n        #writer.add_scalar(\"test/ap_s\", ap_s, global_step=e)\n        #writer.add_scalar(\"test/ap_m\", ap_m, global_step=e)\n        #writer.add_scalar(\"test/ap_x\", ap_x, global_step=e)\n\n\n        #Time measurements\n        timestamp_test,time_test_sec = utils.time_delta_now(time_test)\n        timestamp_total,time_total_sec =  utils.time_delta_now(startTime)\n        \n        train_test_times.append([time_train_sec, time_test_sec, time_total_sec])\n        train_test_times_np = np.array(train_test_times)\n        print('--- train time:  ---', timestamp_train, '--- total: ---',train_test_times_np[:,0].sum())\n        print('--- test time:  ---' , timestamp_test, '--- total: ---',train_test_times_np[:,1].sum())\n        print('--- total time from beginning:  ---', timestamp_total )\n        \n        #save the neural network  such that we can use it later\n        print('Storing the trained model into {}'.format(saveModelPath))\n        torch.save({\"color_net\":color_net.state_dict(),\n                    \"shape_net\":shape_net.state_dict(),                    \n                    \"material_net\":material_net.state_dict(),\n                    \"size_net\":size_net.state_dict(),\n                    \"slot_net\":slot_net.state_dict(),\n                    \"resume\": {\n                        \"optimizer_smsc\":optimizers['smsc'].state_dict(),\n                        \"optimizer_slot\":optimizers['slot'].state_dict(),\n                        \"epoch\":e\n                    },\n                    \"test_ap_list\":test_ap_list,\n                    \"loss_list\":loss_list,                    \n                    \"sm_per_batch_list\":sm_per_batch_list,\n                    \"test_metric_list\":test_metric_list,\n                    \"lr_list\":lr_list,\n                    \"num_params\":num_params,\n                    \"train_test_times\": train_test_times,\n                    \"time\": {\n                        \"forward_time\":forward_time_list,\n                        \"asp_time\": asp_time_list,\n                        \"gradient_time\":gradient_time_list,\n                        \"backward_time\":backward_time_list,\n                    },\n                    \"exp_dict\":exp_dict,\n                    \"program\":program}, saveModelPath)\n        \n        # Update the RTPT\n        rtpt.step()\n    if wb:\n        wandb.finish()", "        "]}
{"filename": "src/experiments/slash_attention/clevr/__init__.py", "chunked_list": [""]}
{"filename": "src/experiments/slash_attention/clevr/dataGen.py", "chunked_list": ["import torch\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import transforms\n\n# from torch.utils.data import Dataset\n# from torchvision import transforms\nfrom skimage import io\nimport os\nimport numpy as np", "import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport json\n\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\n    \ndef object_encoding(size, material, shape, color ):\n    \n    #size (small, large, bg)\n    if size == \"small\":\n        size_enc = [1,0,0]\n    elif size == \"large\":\n        size_enc = [0,1,0]\n    elif size == \"bg\":\n        size_enc = [0,0,1]\n    \n    #material (rubber, metal, bg)\n    if material == \"rubber\":\n        material_enc = [1,0,0]\n    elif material == \"metal\":\n        material_enc = [0,1,0]\n    elif material == \"bg\":\n        material_enc = [0,0,1]\n    \n    #shape (cube, sphere, cylinder, bg)\n    if shape == \"cube\":\n        shape_enc = [1,0,0,0]\n    elif shape == \"sphere\":\n        shape_enc = [0,1,0,0]\n    elif shape == \"cylinder\":\n        shape_enc = [0,0,1,0]\n    elif shape == \"bg\":\n        shape_enc = [0,0,0,1]\n    \n    #color (gray, red, blue, green, brown, purple, cyan, yellow, bg)\n    #color (gray, red, blue, green, brown, purple, cyan, yellow, bg)\n    #color  1      2     3     4     5     6        7     8      9\n    if color == \"gray\":\n        color_enc = [1,0,0,0,0,0,0,0,0]\n    elif color == \"red\":\n        color_enc = [0,1,0,0,0,0,0,0,0]\n    elif color == \"blue\":\n        color_enc = [0,0,1,0,0,0,0,0,0]\n    elif color == \"green\":\n        color_enc = [0,0,0,1,0,0,0,0,0]\n    elif color == \"brown\":\n        color_enc = [0,0,0,0,1,0,0,0,0]\n    elif color == \"purple\":\n        color_enc = [0,0,0,0,0,1,0,0,0]\n    elif color == \"cyan\":\n        color_enc = [0,0,0,0,0,0,1,0,0]\n    elif color == \"yellow\":\n        color_enc = [0,0,0,0,0,0,0,1,0]\n    elif color == \"bg\":\n        color_enc = [0,0,0,0,0,0,0,0,1]\n        \n    return size_enc + material_enc + shape_enc + color_enc +[1]", "       \n\n\n    \n    \nclass CLEVR(Dataset):\n    def __init__(self, root, mode, img_paths=None, files_names=None, obj_num=None):\n        self.root = root  # The root folder of the dataset\n        self.mode = mode  # The mode of 'train' or 'val'\n        self.files_names = files_names # The list of the files names with correct nuber of objects\n        if obj_num is not None:\n            self.obj_num = obj_num  # The upper limit of number of objects \n        else:\n            self.obj_num = 10\n\n        assert os.path.exists(root), 'Path {} does not exist'.format(root)\n\n        #list of sorted image paths\n        self.img_paths = []\n        if img_paths:\n            self.img_paths = img_paths\n        else:                    \n            #open directory and save all image paths\n            for file in os.scandir(os.path.join(root, 'images', mode)):\n                img_path = file.path\n                if '.png' in img_path:\n                    self.img_paths.append(img_path)\n\n        self.img_paths.sort()\n        count = 0\n        \n        #target maps of the form {'target:idx': query string} or {'target:idx': obj encoding}\n        self.query_map = {}\n        self.obj_map = {}\n        \n        count = 0        \n        #We have up to 10 objects in the image, load the json file\n        with open(os.path.join(root, 'scenes','CLEVR_'+ mode+\"_scenes.json\")) as f:\n            data = json.load(f)\n            \n            #iterate over each scene and create the query string and obj encoding\n            print(\"parsing scences\")\n            for scene in tqdm(data['scenes']):\n                target_query = \"\"\n                obj_encoding_list = []\n\n                if self.files_names:\n                    if any(scene['image_filename'] in file_name for file_name in files_names):                    \n                        num_objects = 0\n                        for idx, obj in enumerate(scene['objects']):\n                            target_query += \" :- not object(o{}, {}, {}, {}, {}).\".format(idx+1, obj['size'], obj['material'], obj['shape'], obj['color'])\n                            obj_encoding_list.append(object_encoding(obj['size'], obj['material'], obj['shape'], obj['color']))\n                            num_objects = idx+1 #store the num of objects \n                        #fill in background objects\n                        for idx in range(num_objects, self.obj_num):\n                            target_query += \" :- not object(o{}, bg, bg, bg, bg).\".format(idx+1)\n                            obj_encoding_list.append([0,0,1, 0,0,1, 0,0,0,1, 0,0,0,0,0,0,0,0,1, 1])\n                        self.query_map[count] = target_query\n                        self.obj_map[count] = np.array(obj_encoding_list)\n                        count += 1\n                else:\n                    num_objects=0\n                    for idx, obj in enumerate(scene['objects']):\n                        target_query += \" :- not object(o{}, {}, {}, {}, {}).\".format(idx+1, obj['size'], obj['material'], obj['shape'], obj['color'])\n                        obj_encoding_list.append(object_encoding(obj['size'], obj['material'], obj['shape'], obj['color']))\n                        num_objects = idx+1 #store the num of objects \n                    #fill in background objects\n                    for idx in range(num_objects, 10):\n                        target_query += \" :- not object(o{}, bg, bg, bg, bg).\".format(idx+1)\n                        obj_encoding_list.append([0,0,1, 0,0,1, 0,0,0,1, 0,0,0,0,0,0,0,0,1, 1])\n                    self.query_map[scene['image_index']] = target_query\n                    self.obj_map[scene['image_index']] = np.array(obj_encoding_list)\n                \n            print(\"done\")\n        if self.files_names:\n            print(f'Correctly found images {count} out of {len(files_names)}')\n                    \n        \n        #print(np.array(list(self.obj_map.values()))[0:20])\n    def __getitem__(self, index):\n        #get the image\n        img_path = self.img_paths[index]\n        img = io.imread(img_path)[:, :, :3]\n        \n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            #transforms.CenterCrop((29, 221,64, 256)), #why do we need to crop?\n            transforms.Resize((128, 128)),\n            transforms.ToTensor(),\n        ])\n\n        img = transform(img)\n        img = (img - 0.5) * 2.0  # Rescale to [-1, 1].\n\n        return {'im':img}, self.query_map[index] ,self.obj_map[index]\n\n\n                \n    def __len__(self):\n        return len(self.img_paths)", "\n"]}
{"filename": "src/experiments/slash_attention/clevr/slash_attention_clevr.py", "chunked_list": ["#!/usr/bin/env python\n# coding: utf-8\n\n\n\nimport train\nimport datetime\n\n\nseed = 0", "\nseed = 0\nobj_num = 10\ndate_string = datetime.datetime.today().strftime('%d-%m-%Y')\nexperiments = {f'CLEVR{obj_num}': {'structure':'poon-domingos', 'pd_num_pieces':[4], 'learn_prior':True,\n                         'lr': 0.01, 'bs':512, 'epochs':1000,\n                        'lr_warmup_steps':8, 'lr_decay_steps':360, 'use_em':False, 'resume':False,\n                        'method':'most_prob',\n                         'start_date':date_string, 'credentials':'DO', 'p_num':16, 'seed':seed, 'obj_num':obj_num\n                        }}", "                         'start_date':date_string, 'credentials':'DO', 'p_num':16, 'seed':seed, 'obj_num':obj_num\n                        }}\n\n\nfor exp_name in experiments:\n    print(exp_name)\n    train.slash_slot_attention(exp_name, experiments[exp_name])\n\n\n", "\n\n"]}
{"filename": "src/experiments/slash_attention/clevr/auxiliary.py", "chunked_list": ["import json\nimport os\nfrom pathlib import Path\n\n\ndef get_files_names_and_paths(root:str='./data/CLEVR_v1.0/', mode:str='val', obj_num:int=4):\n    data_file = Path(os.path.join(root, 'scenes','CLEVR_'+ mode+\"_scenes.json\"))\n    data_file.parent.mkdir(parents=True, exist_ok=True)\n    if data_file.exists():\n        print('File exists. Parsing file...')\n    else:\n        print(f'The JSON file {data_file} does not exist!')\n        quit()\n    img_paths = []\n    files_names = []\n    with open(data_file, 'r') as json_file:\n        json_data = json.load(json_file)\n\n        for scene in json_data['scenes']:\n            if len(scene['objects']) <= obj_num:\n                img_paths.append(Path(os.path.join(root,'images/'+mode+'/'+scene['image_filename'])))\n                files_names.append(scene['image_filename'])\n\n    print(\"...done \")\n    return img_paths, files_names", "\n\ndef get_slash_program(obj_num:int=4):\n    program = ''\n    if obj_num == 10:\n        program ='''\n    slot(s1).\n    slot(s2).\n    slot(s3).\n    slot(s4).\n    slot(s5).\n    slot(s6).\n    slot(s7).\n    slot(s8).\n    slot(s9).\n    slot(s10).\n\n    name(o1).\n    name(o2).\n    name(o3).\n    name(o4).\n    name(o5).\n    name(o6).\n    name(o7).\n    name(o8).\n    name(o9).\n    name(o10).\n        '''\n    elif obj_num ==4:\n        program ='''\n    slot(s1).\n    slot(s2).\n    slot(s3).\n    slot(s4).\n\n    name(o1).\n    name(o2).\n    name(o3).\n    name(o4).\n        '''\n    elif obj_num ==6:\n        program ='''\n    slot(s1).\n    slot(s2).\n    slot(s3).\n    slot(s4).\n    slot(s5).\n    slot(s6).\n\n    name(o1).\n    name(o2).\n    name(o3).\n    name(o4).\n    name(o5).\n    name(o6).\n        '''\n    else:\n        print(f'The number of objects {obj_num} is wrong!')\n        quit()\n    program +='''        \n    %assign each name a slot\n    %{slot_name_comb(N,X): slot(X)}=1 :- name(N). %problem we have dublicated slots\n\n    %remove each model which has multiple slots asigned to the same name\n    %:- slot_name_comb(N1,X1), slot_name_comb(N2,X2), X1 == X2, N1 != N2. \n\n    %build the object ontop of the slot assignment\n    object(N, S, M, P, C) :- size(0, +X, -S), material(0, +X, -M), shape(0, +X, -P), color(0, +X, -C), slot(X), name(N), slot_name_comb(N,X).\n\n    %define the SPNs\n    npp(size(1,X),[small, large, bg]) :- slot(X).\n    npp(material(1,X),[rubber, metal, bg]) :- slot(X).\n    npp(shape(1,X),[cube, sphere, cylinder, bg]) :- slot(X).\n    npp(color(1,X),[gray, red, blue, green, brown, purple, cyan, yellow, bg]) :- slot(X).\n\n    '''\n    return program"]}
{"filename": "src/experiments/slash_attention/shapeworld4/slash_attention_shapeworld4.py", "chunked_list": ["#!/usr/bin/env python\n# coding: utf-8\n\n\nimport train\nimport datetime\n\n\n#Python script to start the shapeworld4 slot attention experiment\n#Define your experiment(s) parameters as a hashmap having the following parameters", "#Python script to start the shapeworld4 slot attention experiment\n#Define your experiment(s) parameters as a hashmap having the following parameters\nexample_structure = {'experiment_name': \n                   {'structure': 'poon-domingos',\n                    'pd_num_pieces': [4],\n                    'lr': 0.01, #the learning rate to train the SPNs with, the slot attention module has a fixed lr=0.0004  \n                    'bs':50, #the batchsize\n                    'epochs':1000, #number of epochs to train\n                    'lr_warmup':True, #boolean indicating the use of learning rate warm up\n                    'lr_warmup_steps':25, #number of epochs to warm up the slot attention module, warmup does not apply to the SPNs", "                    'lr_warmup':True, #boolean indicating the use of learning rate warm up\n                    'lr_warmup_steps':25, #number of epochs to warm up the slot attention module, warmup does not apply to the SPNs\n                    'start_date':\"01-01-0001\", #current date\n                    'resume':False, #you can stop the experiment and set this parameter to true to load the last state and continue learning\n                    'credentials':'DO', #your credentials for the rtpt class\n                    'hungarian_matching': True,\n                    'explanation': \"\"\"Running the whole SlotAttention+Slash pipeline using poon-domingos as SPN structure learner.\"\"\"}}\n\n\n", "\n\n\n#EXPERIMENTS\ndate_string = datetime.datetime.today().strftime('%d-%m-%Y')\n\n\nfor seed in [0,1,2,3,4]:\n    experiments = {'shapeworld4': \n                   {'structure': 'poon-domingos', 'pd_num_pieces': [4],\n                    'lr': 0.01, 'bs':512, 'epochs':1000, \n                    'lr_warmup_steps':8, 'lr_decay_steps':360,\n                    'start_date':date_string, 'resume':False, 'credentials':'DO','seed':seed,\n                    'p_num':16, 'method':'same_top_k', 'hungarian_matching': False,\n                    'explanation': \"\"\"Running the whole SlotAttention+Slash pipeline using poon-domingos as SPN structure learner.\"\"\"}\n                    }\n    \n\n    print(\"shapeworld4\")\n    train.slash_slot_attention(\"shapeworld4\", experiments[\"shapeworld4\"])", "\n\n\n\n\n"]}
{"filename": "src/experiments/slash_attention/shapeworld4/train.py", "chunked_list": ["print(\"start importing...\")\n\nimport time\nimport sys\nimport os\nimport json\n\nwb = True\nimport wandb\nsys.path.append('../../../')", "import wandb\nsys.path.append('../../../')\nsys.path.append('../../../SLASH/')\nsys.path.append('../../../EinsumNetworks/src/')\n\n#torch, numpy, ...\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n", "import numpy as np\n\n#own modules\nfrom dataGen import SHAPEWORLD4\nfrom einsum_wrapper import EiNet\nfrom slash import SLASH\nimport utils\nimport ap_utils\nfrom utils import set_manual_seed\nfrom slot_attention_module import SlotAttention_model", "from utils import set_manual_seed\nfrom slot_attention_module import SlotAttention_model\nfrom pathlib import Path\n\nfrom rtpt import RTPT\nprint(\"...done\")\n\n\n\ndef slash_slot_attention(exp_name , exp_dict):\n\n    program =\"\"\"\n    slot(s1).\n    slot(s2).\n    slot(s3).\n    slot(s4).\n    name(o1).\n    name(o2).\n    name(o3).\n    name(o4).\n\n    %assign each name a slot\n    %build the object ontop of the slot assignment\n    object(N,C,S,H,Z) :- color(0, +X, -C), shape(0, +X, -S), shade(0, +X, -H), size(0, +X, -Z), slot(X), name(N), slot_name_comb(N,X).\n\n    npp(shade(1,X),[bright, dark, bg]) :- slot(X).\n    npp(color(1,X),[red, blue, green, gray, brown, magenta, cyan, yellow, black]) :- slot(X).\n    npp(shape(1,X),[circle, triangle, square, bg]) :- slot(X).\n    npp(size(1,X),[small, big, bg]) :- slot(X).\n    \"\"\"\n    \n    #solve the matching problem in the logic program\n    if exp_dict['hungarian_matching'] == False:\n        program += \"\"\"\n        %assign each name a slot\n        {slot_name_comb(N,X): slot(X) }=1 :- name(N). %problem we have dublicated slots\n        %remove each model which has multiple slots asigned to the same name\n        %:-  slot_name_comb(N1,X1), slot_name_comb(N2,X2), X1 == X2, N1 != N2.\n        %remove the duplicates\n        {slot_name_comb(N,X): name(N) }=1 :- slot(X). \n        \"\"\"\n\n    # Set the seeds for PRNG\n    set_manual_seed(exp_dict['seed'])\n    \n    # Create RTPT object\n    rtpt = RTPT(name_initials=exp_dict['credentials'], experiment_name='SLASH Attention Shapeworld4', max_iterations=int(exp_dict['epochs']))\n\n    # Start the RTPT tracking\n    rtpt.start()\n\n    # create save paths and tensorboard writer\n    writer = SummaryWriter(os.path.join(\"runs\", exp_name,exp_dict['method'], str(exp_dict['seed'])), purge_step=0)\n    saveModelPath = 'data/'+exp_name+'_'+exp_dict['method']+'/slash_slot_models_seed'+str(exp_dict['seed'])+'.pt'\n    Path(\"data/\"+exp_name+'_'+exp_dict['method']+\"/\").mkdir(parents=True, exist_ok=True)\n\n\n    if wb:\n        #start a new wandb run to track this script\n        wandb.init(\n        # set the wandb project where this run will be logged\n        project=\"slash_attention\",\n        \n        # track hyperparameters and run metadata\n        config=exp_dict)\n\n    # save args\n    with open(os.path.join(\"runs\", exp_name, 'args.json'), 'w') as json_file:\n        json.dump(exp_dict, json_file, indent=4)\n\n    print(\"Experiment parameters:\", exp_dict)\n\n\n    \n    #NETWORKS\n    if exp_dict['structure'] == 'poon-domingos':\n        exp_dict['depth'] = None\n        exp_dict['num_repetitions'] = None\n        print(\"using poon-domingos\")\n\n    elif exp_dict['structure'] == 'binary-trees':\n        exp_dict['pd_num_pieces'] = None\n        print(\"using binary-trees\")\n\n    \n    #color network\n    color_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 32,\n        class_count=9,\n        pd_width=8,pd_height=4,\n        use_em= False)\n\n    #shape network\n    shape_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 32,\n        class_count=4,\n        pd_width=8,pd_height=4,\n        use_em= False)\n    \n    #shade network\n    shade_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 32,\n        class_count=3,\n        pd_width=8,pd_height=4,\n        use_em= False)\n    \n    #size network\n    size_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 32,\n        class_count=3,\n        pd_width=8,pd_height=4,\n        use_em= False)\n    \n    \n    \n    #create the Slot Attention network\n    slot_net = SlotAttention_model(n_slots=4, n_iters=3, n_attr=18,\n                                encoder_hidden_channels=32, attention_hidden_channels=64)\n    slot_net = slot_net.to(device='cuda')\n    \n    \n\n    #count trainable params\n    num_trainable_params = [sum(p.numel() for p in color_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in shape_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in shade_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in size_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in slot_net.parameters() if p.requires_grad)]\n    num_params = [sum(p.numel() for p in color_net.parameters()), \n                  sum(p.numel() for p in shape_net.parameters()),\n                  sum(p.numel() for p in shade_net.parameters()),\n                  sum(p.numel() for p in size_net.parameters()),\n                  sum(p.numel() for p in slot_net.parameters())]\n    \n    print(\"training with {}({}) trainable params and {}({}) params in total\".format(np.sum(num_trainable_params),num_trainable_params,np.sum(num_params),num_params))\n    \n        \n    slot_net_params = list(slot_net.parameters())\n    csss_params = list(color_net.parameters()) + list(shape_net.parameters()) + list(shade_net.parameters()) + list(size_net.parameters())\n    \n    #create the SLASH Program\n    nnMapping = {'color': color_net,\n                 'shape':shape_net,\n                 'shade':shade_net,\n                 'size':size_net}\n    \n    \n\n    #OPTIMIZERS\n    optimizers = {'csss': torch.optim.Adam([\n                                            {'params':csss_params}],\n                                            lr=exp_dict['lr'], eps=1e-7),\n                 'slot': torch.optim.Adam([\n                                            {'params': slot_net_params}],\n                                            lr=0.0004, eps=1e-7)}\n\n    \n    SLASHobj = SLASH(program, nnMapping, optimizers)\n    SLASHobj.grad_comp_device ='cpu' #set gradient computation to cpu\n\n\n    print(\"using learning rate warmup and decay\")\n    warmup_epochs = exp_dict['lr_warmup_steps'] #warmup for x epochs\n    decay_epochs = exp_dict['lr_decay_steps']\n    slot_base_lr = 0.0004\n\n    \n    \n    #metric lists\n    test_ap_list = [] #stores average precsion values\n    test_metric_list = [] #stores tp, fp, tn values\n    lr_list = [] # store learning rate\n    loss_list = []  # store training loss\n    startTime = time.time()\n    train_test_times = []\n    sm_per_batch_list = []\n\n    forward_time_list = []\n    asp_time_list = []\n    gradient_time_list = []\n    backward_time_list = []\n    \n    print(\"loading data...\")\n    #if the hungarian matching algorithm is used we need to pass the object encodings to SLASH\n    if exp_dict['hungarian_matching']:\n        train_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD4('../../data/shapeworld4/',\"train\", ret_obj_encoding=True), shuffle=True,batch_size=exp_dict['bs'], num_workers=8)\n    else:\n        train_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD4('../../data/shapeworld4/',\"train\"), shuffle=True,batch_size=exp_dict['bs'], num_workers=8)\n\n    test_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD4('../../data/shapeworld4/',\"val\", ret_obj_encoding=True), shuffle=False,batch_size=exp_dict['bs'], num_workers=8)\n    obj_encodings_gt = ap_utils.get_obj_encodings(test_dataset_loader)\n    print(\"...done\")\n\n    \n    start_e= 0\n    if exp_dict['resume']:\n        print(\"resuming experiment\")\n        saved_model = torch.load(saveModelPath)\n        \n        #load pytorch models\n        color_net.load_state_dict(saved_model['color_net'])\n        shape_net.load_state_dict(saved_model['shape_net'])\n        shade_net.load_state_dict(saved_model['shade_net'])\n        size_net.load_state_dict(saved_model['size_net'])\n        slot_net.load_state_dict(saved_model['slot_net'])\n        \n        \n        #optimizers and shedulers\n        optimizers['csss'].load_state_dict(saved_model['resume']['optimizer_csss'])\n        optimizers['slot'].load_state_dict(saved_model['resume']['optimizer_slot'])\n        start_e = saved_model['resume']['epoch']\n                \n        #metrics\n        test_ap_list = saved_model['test_ap_list']\n        test_metric_list = saved_model['test_metric_list']\n        lr_list = saved_model['lr_list']\n        loss_list = saved_model['loss_list']\n        train_test_times = saved_model['train_test_times']\n        sm_per_batch_list = saved_model['sm_per_batch_list']\n\n\n\n\n    for e in range(start_e, exp_dict['epochs']):\n                     \n        #TRAIN\n        print('Epoch {}/{}...'.format(e+1, exp_dict['epochs']))\n        time_train= time.time()\n        \n        #apply lr schedulers to the SAm\n        if e < warmup_epochs:\n            lr = slot_base_lr * ((e+1)/warmup_epochs)\n        else:\n            lr = slot_base_lr\n        lr = lr * 0.5**((e+1)/decay_epochs)\n        optimizers['slot'].param_groups[0]['lr'] = lr\n        lr_list.append([lr,e])\n\n        print(\"LR SAm:\", \"{:.6f}\".format(lr), optimizers['slot'].param_groups[0]['lr'])\n        \n        #SLASH training\n        loss, forward_time, asp_time, gradient_time, backward_time, sm_per_batch= SLASHobj.learn(dataset_loader = train_dataset_loader, slot_net=slot_net, hungarian_matching=exp_dict['hungarian_matching'], method=exp_dict['method'], p_num = exp_dict['p_num'], k_num=1, batched_pass = True,\n                        epoch=e, writer = writer)\n        \n        forward_time_list.append(forward_time)\n        asp_time_list.append(asp_time)\n        gradient_time_list.append(gradient_time)\n        backward_time_list.append(backward_time)\n\n        sm_per_batch_list.append(sm_per_batch)\n        loss_list.append(loss)\n        writer.add_scalar(\"train/loss\", loss, global_step=e)\n\n        timestamp_train, time_train_sec = utils.time_delta_now(time_train)\n        \n        \n        #TEST\n        time_test = time.time()\n\n        #forward test batch\n        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader)                          \n                            \n        #compute the average precision, tp, fp, tn for color+shape+shade+size, color, shape, shade, size\n        #color+shape+shade+size\n        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n        ap, true_positives,false_positives, true_negatives, correctly_classified = ap_utils.average_precision(pred, obj_encodings_gt,-1, \"SHAPEWORLD4\")\n        print(\"avg precision \",ap, \"tp\", true_positives, \"fp\", false_positives, \"tn\", true_negatives, \"correctly classified\",correctly_classified)\n            \n        \n        #color\n        pred_c = ap_utils.inference_map_to_array(inference, only_color=True).cpu().numpy()\n        ap_c, true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c= ap_utils.average_precision(pred, obj_encodings_gt,-1, \"SHAPEWORLD4\", only_color = True)\n        print(\"avg precision color\",ap_c, \"tp\", true_positives_c, \"fp\", false_positives_c, \"tn\", true_negatives_c, \"correctly classified\",correctly_classified_c)\n\n        #shape              \n        pred_s = ap_utils.inference_map_to_array(inference, only_shape=True).cpu().numpy()\n        ap_s, true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s= ap_utils.average_precision(pred_s, obj_encodings_gt,-1, \"SHAPEWORLD4\", only_shape = True)\n        print(\"avg precision shape\",ap_s, \"tp\", true_positives_s, \"fp\", false_positives_s, \"tn\", true_negatives_s, \"correctly classified\",correctly_classified_s)\n        \n        #shade              \n        pred_h = ap_utils.inference_map_to_array(inference, only_shade=True).cpu().numpy()\n        ap_h, true_positives_h, false_positives_h, true_negatives_h, correctly_classified_h= ap_utils.average_precision(pred_h, obj_encodings_gt,-1, \"SHAPEWORLD4\", only_shade = True)\n        print(\"avg precision shade\",ap_h, \"tp\", true_positives_h, \"fp\", false_positives_h, \"tn\", true_negatives_h, \"correctly classified\",correctly_classified_h)\n        \n        #shape              \n        pred_x = ap_utils.inference_map_to_array(inference, only_size=True).cpu().numpy()\n        ap_x, true_positives_x, false_positives_x, true_negatives_x, correctly_classified_x= ap_utils.average_precision(pred_x, obj_encodings_gt,-1, \"SHAPEWORLD4\", only_size = True)\n        print(\"avg precision size\",ap_x, \"tp\", true_positives_x, \"fp\", false_positives_x, \"tn\", true_negatives_x, \"correctly classified\",correctly_classified_x)\n\n        \n        #store ap, tp, fp, tn\n        test_ap_list.append([ap, ap_c, ap_s, ap_h, ap_x, e])                    \n        test_metric_list.append([true_positives, false_positives, true_negatives,correctly_classified,\n                                 true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c,\n                                 true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s,\n                                 true_positives_h, false_positives_h, true_negatives_h, correctly_classified_h,\n                                 true_positives_x, false_positives_x, true_negatives_x, correctly_classified_x])\n\n        #Tensorboard outputs\n        writer.add_scalar(\"test/ap\", ap, global_step=e)\n        writer.add_scalar(\"test/ap_c\", ap_c, global_step=e)\n        writer.add_scalar(\"test/ap_s\", ap_s, global_step=e)\n        writer.add_scalar(\"test/ap_h\", ap_h, global_step=e)\n        writer.add_scalar(\"test/ap_x\", ap_x, global_step=e)\n\n        if wb:\n            wandb.log({\"ap\": ap,\n                        \"ap_c\":ap_c,\n                        \"ap_s\":ap_s,\n                        \"ap_h\":ap_h,\n                        \"ap_x\":ap_x,\n                        \"loss\":loss,\n                        \"forward_time\":forward_time,\n                        \"asp_time\":asp_time,\n                        \"gradient_time\": gradient_time,\n                        \"backward_time\": backward_time,\n                        \"sm_per_batch\": sm_per_batch})\n\n        \n        #Time measurements\n        timestamp_test,time_test_sec = utils.time_delta_now(time_test)\n        timestamp_total,time_total_sec =  utils.time_delta_now(startTime)\n        \n        train_test_times.append([time_train_sec, time_test_sec, time_total_sec])\n        train_test_times_np = np.array(train_test_times)\n        print('--- train time:  ---', timestamp_train, '--- total: ---',train_test_times_np[:,0].sum())\n        print('--- test time:  ---' , timestamp_test, '--- total: ---',train_test_times_np[:,1].sum())\n        print('--- total time from beginning:  ---', timestamp_total )\n        \n        #save the neural network  such that we can use it later\n        print('Storing the trained model into {}'.format(saveModelPath))\n        torch.save({\"shape_net\":  shape_net.state_dict(), \n                    \"color_net\": color_net.state_dict(),\n                    \"shade_net\": shade_net.state_dict(),\n                    \"size_net\": size_net.state_dict(),\n                    \"slot_net\": slot_net.state_dict(),\n                    \"resume\": {\n                        \"optimizer_csss\":optimizers['csss'].state_dict(),\n                        \"optimizer_slot\": optimizers['slot'].state_dict(),\n                        \"epoch\":e\n                    },\n                    \"test_ap_list\":test_ap_list,\n                    \"loss_list\":loss_list,\n                    \"sm_per_batch_list\":sm_per_batch_list,\n                    \"test_metric_list\":test_metric_list,\n                    \"lr_list\": lr_list,\n                    \"num_params\": num_params,\n                    \"train_test_times\": train_test_times,\n                    \"exp_dict\":exp_dict,\n                    \"time\": {\n                        \"forward_time\":forward_time_list,\n                        \"asp_time\": asp_time_list,\n                        \"gradient_time\":gradient_time_list,\n                        \"backward_time\":backward_time_list,\n                    },\n                    \"program\":program}, saveModelPath)\n        \n        # Update the RTPT\n        rtpt.step(subtitle=f\"ap={ap:2.2f}\")\n    if wb:\n        wandb.finish()", "\ndef slash_slot_attention(exp_name , exp_dict):\n\n    program =\"\"\"\n    slot(s1).\n    slot(s2).\n    slot(s3).\n    slot(s4).\n    name(o1).\n    name(o2).\n    name(o3).\n    name(o4).\n\n    %assign each name a slot\n    %build the object ontop of the slot assignment\n    object(N,C,S,H,Z) :- color(0, +X, -C), shape(0, +X, -S), shade(0, +X, -H), size(0, +X, -Z), slot(X), name(N), slot_name_comb(N,X).\n\n    npp(shade(1,X),[bright, dark, bg]) :- slot(X).\n    npp(color(1,X),[red, blue, green, gray, brown, magenta, cyan, yellow, black]) :- slot(X).\n    npp(shape(1,X),[circle, triangle, square, bg]) :- slot(X).\n    npp(size(1,X),[small, big, bg]) :- slot(X).\n    \"\"\"\n    \n    #solve the matching problem in the logic program\n    if exp_dict['hungarian_matching'] == False:\n        program += \"\"\"\n        %assign each name a slot\n        {slot_name_comb(N,X): slot(X) }=1 :- name(N). %problem we have dublicated slots\n        %remove each model which has multiple slots asigned to the same name\n        %:-  slot_name_comb(N1,X1), slot_name_comb(N2,X2), X1 == X2, N1 != N2.\n        %remove the duplicates\n        {slot_name_comb(N,X): name(N) }=1 :- slot(X). \n        \"\"\"\n\n    # Set the seeds for PRNG\n    set_manual_seed(exp_dict['seed'])\n    \n    # Create RTPT object\n    rtpt = RTPT(name_initials=exp_dict['credentials'], experiment_name='SLASH Attention Shapeworld4', max_iterations=int(exp_dict['epochs']))\n\n    # Start the RTPT tracking\n    rtpt.start()\n\n    # create save paths and tensorboard writer\n    writer = SummaryWriter(os.path.join(\"runs\", exp_name,exp_dict['method'], str(exp_dict['seed'])), purge_step=0)\n    saveModelPath = 'data/'+exp_name+'_'+exp_dict['method']+'/slash_slot_models_seed'+str(exp_dict['seed'])+'.pt'\n    Path(\"data/\"+exp_name+'_'+exp_dict['method']+\"/\").mkdir(parents=True, exist_ok=True)\n\n\n    if wb:\n        #start a new wandb run to track this script\n        wandb.init(\n        # set the wandb project where this run will be logged\n        project=\"slash_attention\",\n        \n        # track hyperparameters and run metadata\n        config=exp_dict)\n\n    # save args\n    with open(os.path.join(\"runs\", exp_name, 'args.json'), 'w') as json_file:\n        json.dump(exp_dict, json_file, indent=4)\n\n    print(\"Experiment parameters:\", exp_dict)\n\n\n    \n    #NETWORKS\n    if exp_dict['structure'] == 'poon-domingos':\n        exp_dict['depth'] = None\n        exp_dict['num_repetitions'] = None\n        print(\"using poon-domingos\")\n\n    elif exp_dict['structure'] == 'binary-trees':\n        exp_dict['pd_num_pieces'] = None\n        print(\"using binary-trees\")\n\n    \n    #color network\n    color_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 32,\n        class_count=9,\n        pd_width=8,pd_height=4,\n        use_em= False)\n\n    #shape network\n    shape_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 32,\n        class_count=4,\n        pd_width=8,pd_height=4,\n        use_em= False)\n    \n    #shade network\n    shade_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 32,\n        class_count=3,\n        pd_width=8,pd_height=4,\n        use_em= False)\n    \n    #size network\n    size_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 32,\n        class_count=3,\n        pd_width=8,pd_height=4,\n        use_em= False)\n    \n    \n    \n    #create the Slot Attention network\n    slot_net = SlotAttention_model(n_slots=4, n_iters=3, n_attr=18,\n                                encoder_hidden_channels=32, attention_hidden_channels=64)\n    slot_net = slot_net.to(device='cuda')\n    \n    \n\n    #count trainable params\n    num_trainable_params = [sum(p.numel() for p in color_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in shape_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in shade_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in size_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in slot_net.parameters() if p.requires_grad)]\n    num_params = [sum(p.numel() for p in color_net.parameters()), \n                  sum(p.numel() for p in shape_net.parameters()),\n                  sum(p.numel() for p in shade_net.parameters()),\n                  sum(p.numel() for p in size_net.parameters()),\n                  sum(p.numel() for p in slot_net.parameters())]\n    \n    print(\"training with {}({}) trainable params and {}({}) params in total\".format(np.sum(num_trainable_params),num_trainable_params,np.sum(num_params),num_params))\n    \n        \n    slot_net_params = list(slot_net.parameters())\n    csss_params = list(color_net.parameters()) + list(shape_net.parameters()) + list(shade_net.parameters()) + list(size_net.parameters())\n    \n    #create the SLASH Program\n    nnMapping = {'color': color_net,\n                 'shape':shape_net,\n                 'shade':shade_net,\n                 'size':size_net}\n    \n    \n\n    #OPTIMIZERS\n    optimizers = {'csss': torch.optim.Adam([\n                                            {'params':csss_params}],\n                                            lr=exp_dict['lr'], eps=1e-7),\n                 'slot': torch.optim.Adam([\n                                            {'params': slot_net_params}],\n                                            lr=0.0004, eps=1e-7)}\n\n    \n    SLASHobj = SLASH(program, nnMapping, optimizers)\n    SLASHobj.grad_comp_device ='cpu' #set gradient computation to cpu\n\n\n    print(\"using learning rate warmup and decay\")\n    warmup_epochs = exp_dict['lr_warmup_steps'] #warmup for x epochs\n    decay_epochs = exp_dict['lr_decay_steps']\n    slot_base_lr = 0.0004\n\n    \n    \n    #metric lists\n    test_ap_list = [] #stores average precsion values\n    test_metric_list = [] #stores tp, fp, tn values\n    lr_list = [] # store learning rate\n    loss_list = []  # store training loss\n    startTime = time.time()\n    train_test_times = []\n    sm_per_batch_list = []\n\n    forward_time_list = []\n    asp_time_list = []\n    gradient_time_list = []\n    backward_time_list = []\n    \n    print(\"loading data...\")\n    #if the hungarian matching algorithm is used we need to pass the object encodings to SLASH\n    if exp_dict['hungarian_matching']:\n        train_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD4('../../data/shapeworld4/',\"train\", ret_obj_encoding=True), shuffle=True,batch_size=exp_dict['bs'], num_workers=8)\n    else:\n        train_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD4('../../data/shapeworld4/',\"train\"), shuffle=True,batch_size=exp_dict['bs'], num_workers=8)\n\n    test_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD4('../../data/shapeworld4/',\"val\", ret_obj_encoding=True), shuffle=False,batch_size=exp_dict['bs'], num_workers=8)\n    obj_encodings_gt = ap_utils.get_obj_encodings(test_dataset_loader)\n    print(\"...done\")\n\n    \n    start_e= 0\n    if exp_dict['resume']:\n        print(\"resuming experiment\")\n        saved_model = torch.load(saveModelPath)\n        \n        #load pytorch models\n        color_net.load_state_dict(saved_model['color_net'])\n        shape_net.load_state_dict(saved_model['shape_net'])\n        shade_net.load_state_dict(saved_model['shade_net'])\n        size_net.load_state_dict(saved_model['size_net'])\n        slot_net.load_state_dict(saved_model['slot_net'])\n        \n        \n        #optimizers and shedulers\n        optimizers['csss'].load_state_dict(saved_model['resume']['optimizer_csss'])\n        optimizers['slot'].load_state_dict(saved_model['resume']['optimizer_slot'])\n        start_e = saved_model['resume']['epoch']\n                \n        #metrics\n        test_ap_list = saved_model['test_ap_list']\n        test_metric_list = saved_model['test_metric_list']\n        lr_list = saved_model['lr_list']\n        loss_list = saved_model['loss_list']\n        train_test_times = saved_model['train_test_times']\n        sm_per_batch_list = saved_model['sm_per_batch_list']\n\n\n\n\n    for e in range(start_e, exp_dict['epochs']):\n                     \n        #TRAIN\n        print('Epoch {}/{}...'.format(e+1, exp_dict['epochs']))\n        time_train= time.time()\n        \n        #apply lr schedulers to the SAm\n        if e < warmup_epochs:\n            lr = slot_base_lr * ((e+1)/warmup_epochs)\n        else:\n            lr = slot_base_lr\n        lr = lr * 0.5**((e+1)/decay_epochs)\n        optimizers['slot'].param_groups[0]['lr'] = lr\n        lr_list.append([lr,e])\n\n        print(\"LR SAm:\", \"{:.6f}\".format(lr), optimizers['slot'].param_groups[0]['lr'])\n        \n        #SLASH training\n        loss, forward_time, asp_time, gradient_time, backward_time, sm_per_batch= SLASHobj.learn(dataset_loader = train_dataset_loader, slot_net=slot_net, hungarian_matching=exp_dict['hungarian_matching'], method=exp_dict['method'], p_num = exp_dict['p_num'], k_num=1, batched_pass = True,\n                        epoch=e, writer = writer)\n        \n        forward_time_list.append(forward_time)\n        asp_time_list.append(asp_time)\n        gradient_time_list.append(gradient_time)\n        backward_time_list.append(backward_time)\n\n        sm_per_batch_list.append(sm_per_batch)\n        loss_list.append(loss)\n        writer.add_scalar(\"train/loss\", loss, global_step=e)\n\n        timestamp_train, time_train_sec = utils.time_delta_now(time_train)\n        \n        \n        #TEST\n        time_test = time.time()\n\n        #forward test batch\n        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader)                          \n                            \n        #compute the average precision, tp, fp, tn for color+shape+shade+size, color, shape, shade, size\n        #color+shape+shade+size\n        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n        ap, true_positives,false_positives, true_negatives, correctly_classified = ap_utils.average_precision(pred, obj_encodings_gt,-1, \"SHAPEWORLD4\")\n        print(\"avg precision \",ap, \"tp\", true_positives, \"fp\", false_positives, \"tn\", true_negatives, \"correctly classified\",correctly_classified)\n            \n        \n        #color\n        pred_c = ap_utils.inference_map_to_array(inference, only_color=True).cpu().numpy()\n        ap_c, true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c= ap_utils.average_precision(pred, obj_encodings_gt,-1, \"SHAPEWORLD4\", only_color = True)\n        print(\"avg precision color\",ap_c, \"tp\", true_positives_c, \"fp\", false_positives_c, \"tn\", true_negatives_c, \"correctly classified\",correctly_classified_c)\n\n        #shape              \n        pred_s = ap_utils.inference_map_to_array(inference, only_shape=True).cpu().numpy()\n        ap_s, true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s= ap_utils.average_precision(pred_s, obj_encodings_gt,-1, \"SHAPEWORLD4\", only_shape = True)\n        print(\"avg precision shape\",ap_s, \"tp\", true_positives_s, \"fp\", false_positives_s, \"tn\", true_negatives_s, \"correctly classified\",correctly_classified_s)\n        \n        #shade              \n        pred_h = ap_utils.inference_map_to_array(inference, only_shade=True).cpu().numpy()\n        ap_h, true_positives_h, false_positives_h, true_negatives_h, correctly_classified_h= ap_utils.average_precision(pred_h, obj_encodings_gt,-1, \"SHAPEWORLD4\", only_shade = True)\n        print(\"avg precision shade\",ap_h, \"tp\", true_positives_h, \"fp\", false_positives_h, \"tn\", true_negatives_h, \"correctly classified\",correctly_classified_h)\n        \n        #shape              \n        pred_x = ap_utils.inference_map_to_array(inference, only_size=True).cpu().numpy()\n        ap_x, true_positives_x, false_positives_x, true_negatives_x, correctly_classified_x= ap_utils.average_precision(pred_x, obj_encodings_gt,-1, \"SHAPEWORLD4\", only_size = True)\n        print(\"avg precision size\",ap_x, \"tp\", true_positives_x, \"fp\", false_positives_x, \"tn\", true_negatives_x, \"correctly classified\",correctly_classified_x)\n\n        \n        #store ap, tp, fp, tn\n        test_ap_list.append([ap, ap_c, ap_s, ap_h, ap_x, e])                    \n        test_metric_list.append([true_positives, false_positives, true_negatives,correctly_classified,\n                                 true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c,\n                                 true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s,\n                                 true_positives_h, false_positives_h, true_negatives_h, correctly_classified_h,\n                                 true_positives_x, false_positives_x, true_negatives_x, correctly_classified_x])\n\n        #Tensorboard outputs\n        writer.add_scalar(\"test/ap\", ap, global_step=e)\n        writer.add_scalar(\"test/ap_c\", ap_c, global_step=e)\n        writer.add_scalar(\"test/ap_s\", ap_s, global_step=e)\n        writer.add_scalar(\"test/ap_h\", ap_h, global_step=e)\n        writer.add_scalar(\"test/ap_x\", ap_x, global_step=e)\n\n        if wb:\n            wandb.log({\"ap\": ap,\n                        \"ap_c\":ap_c,\n                        \"ap_s\":ap_s,\n                        \"ap_h\":ap_h,\n                        \"ap_x\":ap_x,\n                        \"loss\":loss,\n                        \"forward_time\":forward_time,\n                        \"asp_time\":asp_time,\n                        \"gradient_time\": gradient_time,\n                        \"backward_time\": backward_time,\n                        \"sm_per_batch\": sm_per_batch})\n\n        \n        #Time measurements\n        timestamp_test,time_test_sec = utils.time_delta_now(time_test)\n        timestamp_total,time_total_sec =  utils.time_delta_now(startTime)\n        \n        train_test_times.append([time_train_sec, time_test_sec, time_total_sec])\n        train_test_times_np = np.array(train_test_times)\n        print('--- train time:  ---', timestamp_train, '--- total: ---',train_test_times_np[:,0].sum())\n        print('--- test time:  ---' , timestamp_test, '--- total: ---',train_test_times_np[:,1].sum())\n        print('--- total time from beginning:  ---', timestamp_total )\n        \n        #save the neural network  such that we can use it later\n        print('Storing the trained model into {}'.format(saveModelPath))\n        torch.save({\"shape_net\":  shape_net.state_dict(), \n                    \"color_net\": color_net.state_dict(),\n                    \"shade_net\": shade_net.state_dict(),\n                    \"size_net\": size_net.state_dict(),\n                    \"slot_net\": slot_net.state_dict(),\n                    \"resume\": {\n                        \"optimizer_csss\":optimizers['csss'].state_dict(),\n                        \"optimizer_slot\": optimizers['slot'].state_dict(),\n                        \"epoch\":e\n                    },\n                    \"test_ap_list\":test_ap_list,\n                    \"loss_list\":loss_list,\n                    \"sm_per_batch_list\":sm_per_batch_list,\n                    \"test_metric_list\":test_metric_list,\n                    \"lr_list\": lr_list,\n                    \"num_params\": num_params,\n                    \"train_test_times\": train_test_times,\n                    \"exp_dict\":exp_dict,\n                    \"time\": {\n                        \"forward_time\":forward_time_list,\n                        \"asp_time\": asp_time_list,\n                        \"gradient_time\":gradient_time_list,\n                        \"backward_time\":backward_time_list,\n                    },\n                    \"program\":program}, saveModelPath)\n        \n        # Update the RTPT\n        rtpt.step(subtitle=f\"ap={ap:2.2f}\")\n    if wb:\n        wandb.finish()", "        \n\n\n\n"]}
{"filename": "src/experiments/slash_attention/shapeworld4/__init__.py", "chunked_list": [""]}
{"filename": "src/experiments/slash_attention/shapeworld4/dataGen.py", "chunked_list": ["import torch\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import transforms\n\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom skimage import io\nimport os\nimport numpy as np", "import os\nimport numpy as np\nimport torch\nfrom PIL import  ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport json\nimport datasets\n\n       \ndef get_encoding(color, shape, shade, size):\n    \n    if color == 'red':\n        col_enc = [1,0,0,0,0,0,0,0,0]\n    elif color == 'blue':\n        col_enc = [0,1,0,0,0,0,0,0,0]\n    elif color == 'green':\n        col_enc = [0,0,1,0,0,0,0,0,0]\n    elif color == 'gray':\n        col_enc = [0,0,0,1,0,0,0,0,0]\n    elif color == 'brown':\n        col_enc = [0,0,0,0,1,0,0,0,0]\n    elif color == 'magenta':\n        col_enc = [0,0,0,0,0,1,0,0,0]\n    elif color == 'cyan':\n        col_enc = [0,0,0,0,0,0,1,0,0]\n    elif color == 'yellow':\n        col_enc = [0,0,0,0,0,0,0,1,0]\n    elif color == 'black':\n        col_enc = [0,0,0,0,0,0,0,0,1]\n        \n\n    if shape == 'circle':\n        shape_enc = [1,0,0,0]\n    elif shape == 'triangle':\n        shape_enc = [0,1,0,0]\n    elif shape == 'square':\n        shape_enc = [0,0,1,0]    \n    elif shape == 'bg':\n        shape_enc = [0,0,0,1]\n        \n    if shade == 'bright':\n        shade_enc = [1,0,0]\n    elif shade =='dark':\n        shade_enc = [0,1,0]\n    elif shade == 'bg':\n        shade_enc = [0,0,1]\n        \n        \n    if size == 'small':\n        size_enc = [1,0,0]\n    elif size == 'big':\n        size_enc = [0,1,0]\n    elif size == 'bg':\n        size_enc = [0,0,1]\n    \n    return col_enc + shape_enc + shade_enc + size_enc + [1]", "       \ndef get_encoding(color, shape, shade, size):\n    \n    if color == 'red':\n        col_enc = [1,0,0,0,0,0,0,0,0]\n    elif color == 'blue':\n        col_enc = [0,1,0,0,0,0,0,0,0]\n    elif color == 'green':\n        col_enc = [0,0,1,0,0,0,0,0,0]\n    elif color == 'gray':\n        col_enc = [0,0,0,1,0,0,0,0,0]\n    elif color == 'brown':\n        col_enc = [0,0,0,0,1,0,0,0,0]\n    elif color == 'magenta':\n        col_enc = [0,0,0,0,0,1,0,0,0]\n    elif color == 'cyan':\n        col_enc = [0,0,0,0,0,0,1,0,0]\n    elif color == 'yellow':\n        col_enc = [0,0,0,0,0,0,0,1,0]\n    elif color == 'black':\n        col_enc = [0,0,0,0,0,0,0,0,1]\n        \n\n    if shape == 'circle':\n        shape_enc = [1,0,0,0]\n    elif shape == 'triangle':\n        shape_enc = [0,1,0,0]\n    elif shape == 'square':\n        shape_enc = [0,0,1,0]    \n    elif shape == 'bg':\n        shape_enc = [0,0,0,1]\n        \n    if shade == 'bright':\n        shade_enc = [1,0,0]\n    elif shade =='dark':\n        shade_enc = [0,1,0]\n    elif shade == 'bg':\n        shade_enc = [0,0,1]\n        \n        \n    if size == 'small':\n        size_enc = [1,0,0]\n    elif size == 'big':\n        size_enc = [0,1,0]\n    elif size == 'bg':\n        size_enc = [0,0,1]\n    \n    return col_enc + shape_enc + shade_enc + size_enc + [1]", "    \n    \nclass SHAPEWORLD4(Dataset):\n    def __init__(self, root, mode, ret_obj_encoding=False):\n        \n        datasets.maybe_download_shapeworld4()\n\n        self.ret_obj_encoding = ret_obj_encoding\n        self.root = root\n        self.mode = mode\n        assert os.path.exists(root), 'Path {} does not exist'.format(root)\n\n        #dictionary of the form {'image_idx':'img_path'}\n        self.img_paths = {}\n        \n        \n        for file in os.scandir(os.path.join(root, 'images', mode)):\n            img_path = file.path\n            \n            img_path_idx =   img_path.split(\"/\")\n            img_path_idx = img_path_idx[-1]\n            img_path_idx = img_path_idx[:-4][6:]\n            try:\n                img_path_idx =  int(img_path_idx)\n                self.img_paths[img_path_idx] = img_path\n            except:\n                print(\"path:\",img_path_idx)\n                \n        \n        count = 0\n        \n        #target maps of the form {'target:idx': query string} or {'target:idx': obj encoding}\n        self.query_map = {}\n        self.obj_map = {}\n                \n        with open(os.path.join(root, 'labels', mode,\"world_model.json\")) as f:\n            worlds = json.load(f)\n            \n            #iterate over all objects\n            for world in worlds:\n                num_objects = 0\n                target_query = \"\"\n                obj_enc = []\n                for entity in world['entities']:\n                    \n                    color = entity['color']['name']\n                    shape = entity['shape']['name']\n                    \n                    shade_val = entity['color']['shade']\n                    if shade_val == 0.0:\n                        shade = 'bright'\n                    else:\n                        shade = 'dark'\n                    \n                    size_val = entity['shape']['size']['x']\n                    if size_val == 0.075:\n                        size = 'small'\n                    elif size_val == 0.15:\n                        size = 'big'\n                    \n                    name = 'o' + str(num_objects+1)\n                    target_query = target_query+ \":- not object({},{},{},{},{}). \".format(name, color, shape, shade, size)\n                    obj_enc.append(get_encoding(color, shape, shade, size))\n                    num_objects += 1\n                    \n                #bg encodings\n                for i in range(num_objects, 4):\n                    name = 'o' + str(num_objects+1)\n                    target_query = target_query+ \":- not object({},black,bg, bg, bg). \".format(name)\n                    obj_enc.append(get_encoding(\"black\",\"bg\",\"bg\",\"bg\"))\n                    num_objects += 1\n\n\n                self.query_map[count] = target_query\n                self.obj_map[count] = np.array(obj_enc)\n                count+=1\n            \n            \n                    \n    def __getitem__(self, index):\n        \n        #get the image\n        img_path = self.img_paths[index]\n        img = io.imread(img_path)[:, :, :3]\n        \n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.ToTensor(),\n        ])\n        img = transform(img)\n        img = (img - 0.5) * 2.0  # Rescale to [-1, 1].\n\n        if self.ret_obj_encoding:\n            return {'im':img}, self.query_map[index] ,self.obj_map[index]\n        else:\n            return {'im':img}, self.query_map[index]\n\n    def __len__(self):\n        return len(self.img_paths)", ""]}
{"filename": "src/experiments/slash_attention/clevr_cogent/train.py", "chunked_list": ["print(\"start importing...\")\n\nimport time\nimport sys\nimport os\nimport json\nsys.path.append('../../../')\nsys.path.append('../../../SLASH/')\nsys.path.append('../../../EinsumNetworks/src/')\n", "sys.path.append('../../../EinsumNetworks/src/')\n\n#torch, numpy, ...\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\ntorch.cuda.empty_cache()\n\n\nimport numpy as np\nimport importlib", "import numpy as np\nimport importlib\n\n#own modules\n\nfrom dataGen import CLEVR\nfrom auxiliary import get_files_names_and_paths, get_slash_program\nfrom einsum_wrapper import EiNet\nfrom slash import SLASH\nimport utils", "from slash import SLASH\nimport utils\nimport ap_utils\nfrom utils import set_manual_seed\nfrom slot_attention_module import SlotAttention_model\nfrom slot_attention_module import SlotAttention_model\nfrom pathlib import Path\nfrom rtpt import RTPT\nprint(\"...done\")\n", "print(\"...done\")\n\n\n\ndef slash_slot_attention(exp_name , exp_dict):\n    \n    # Set the seeds for PRNG\n    set_manual_seed(exp_dict['seed'])\n\n    # Create RTPT object\n    rtpt = RTPT(name_initials=exp_dict['credentials'], experiment_name=f'SLASH Attention CLEVR-Cogent %s' % exp_dict['obj_num'], max_iterations=int(exp_dict['epochs']))\n    \n    # Start the RTPT tracking\n    rtpt.start()\n    \n\n    # create save paths and tensorboard writer\n    writer = SummaryWriter(os.path.join(\"runs\", exp_name,exp_dict['method'], str(exp_dict['seed'])), purge_step=0)\n    saveModelPath = 'data/'+exp_name+'_'+exp_dict['method']+'/slash_slot_models_seed'+str(exp_dict['seed'])+'.pt'\n    Path(\"data/\"+exp_name+'_'+exp_dict['method']+\"/\").mkdir(parents=True, exist_ok=True)\n\n    # save args\n    with open(os.path.join(\"runs\", exp_name, 'args.json'), 'w') as json_file:\n        json.dump(exp_dict, json_file, indent=4)\n\n    print(\"Experiment parameters:\", exp_dict)\n\n\n    #setup new SLASH program given the network parameters\n    program = get_slash_program(exp_dict['obj_num'])\n    \n    #setup new SLASH program given the network parameters\n    if exp_dict['structure'] == 'poon-domingos':\n        exp_dict['depth'] = None\n        exp_dict['num_repetitions'] = None\n        print(\"using poon-domingos\")\n\n    elif exp_dict['structure'] == 'binary-trees':\n        exp_dict['pd_num_pieces'] = None\n        print(\"using binary-trees\")\n              \n    #size network\n    size_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 64,\n        pd_width = 8,\n        pd_height = 8,\n        class_count = 3,\n        use_em = exp_dict['use_em'],\n        learn_prior = exp_dict['learn_prior'])\n    \n    #material network\n    material_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 64,\n        pd_width = 8,\n        pd_height = 8,\n        class_count = 3,\n        use_em = exp_dict['use_em'],\n        learn_prior = exp_dict['learn_prior'])\n    \n    #shape network\n    shape_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 64,\n        pd_width = 8,\n        pd_height = 8,\n        class_count = 4,\n        use_em = exp_dict['use_em'],\n        learn_prior = exp_dict['learn_prior'])\n    \n    #color network\n    color_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 64,\n        pd_width = 8,\n        pd_height = 8,\n        class_count = 9,\n        use_em = exp_dict['use_em'],\n        learn_prior = exp_dict['learn_prior'])\n        \n    \n    \n    #create the Slot Attention network\n    slot_net = SlotAttention_model(n_slots=exp_dict['obj_num'], n_iters=3, n_attr=18,\n                                   encoder_hidden_channels=64, attention_hidden_channels=128, clevr_encoding=True)\n    slot_net = slot_net.to(device='cuda')\n        \n\n\n    #trainable params\n    num_trainable_params = [sum(p.numel() for p in size_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in material_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in shape_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in color_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in slot_net.parameters() if p.requires_grad)]\n    num_params = [sum(p.numel() for p in size_net.parameters()), \n                  sum(p.numel() for p in material_net.parameters()), \n                  sum(p.numel() for p in shape_net.parameters()), \n                  sum(p.numel() for p in color_net.parameters()),\n                  sum(p.numel() for p in slot_net.parameters())]\n    \n    print(\"training with {}({}) trainable params and {}({}) params in total\".format(np.sum(num_trainable_params),num_trainable_params,np.sum(num_params),num_params))\n         \n            \n    slot_net_params = list(slot_net.parameters())\n    smsc_params = list(size_net.parameters()) + list(material_net.parameters()) + list(shape_net.parameters())  + list(color_net.parameters()) \n    \n    #create the SLASH Program\n    nnMapping = {'size': size_net,\n                 'material': material_net,\n                 'shape': shape_net,\n                 'color': color_net\n                }\n        \n    \n    #OPTIMIZERS\n    optimizers = {'smsc': torch.optim.Adam([\n                                        {'params':smsc_params}],\n                                        lr=exp_dict['lr'], eps=1e-7),\n                 'slot': torch.optim.Adam([\n                                        {'params': slot_net_params}],\n                                        lr=0.0004, eps=1e-7)}\n\n    \n    SLASHobj = SLASH(program, nnMapping, optimizers)\n\n    \n\n    print(\"using learning rate warmup and decay\")\n    warmup_epochs = exp_dict['lr_warmup_steps'] #warmup for x epochs\n    decay_epochs = exp_dict['lr_decay_steps']\n    slot_base_lr = 0.0004\n        \n        \n    #metric lists\n    test_ap_list_a = [] #stores average precsion values\n    test_ap_list_b = [] #stores average precsion values\n\n    test_metric_list_a = [] #stores tp, fp, tn values\n    test_metric_list_b = [] #stores tp, fp, tn values\n\n    lr_list = [] # store learning rate\n    loss_list = []  # store training loss\n    \n    startTime = time.time()\n    train_test_times = []\n    sm_per_batch_list = []\n    \n    # Load data\n    obj_num = exp_dict['obj_num']\n    root = '/SLASH/data/CLEVR_CoGenT_v1.0/'\n    \n    mode = 'trainA'\n    img_paths, files_names = get_files_names_and_paths(root=root, mode=mode, obj_num=obj_num)\n    train_dataset_loader = torch.utils.data.DataLoader(CLEVR(root,mode,img_paths,files_names,obj_num), shuffle=True,batch_size=exp_dict['bs'],pin_memory=True, num_workers=4)\n    \n    mode = 'valA'\n    img_paths, files_names = get_files_names_and_paths(root=root, mode=mode, obj_num=obj_num)\n    test_dataset_loader_a= torch.utils.data.DataLoader(CLEVR(root,mode,img_paths,files_names,obj_num), shuffle=False,batch_size=exp_dict['bs'],pin_memory=True, num_workers=4)\n    obj_encodings_gt_a = ap_utils.get_obj_encodings(test_dataset_loader_a)\n\n    mode = 'valB'\n    img_paths, files_names = get_files_names_and_paths(root=root, mode=mode, obj_num=obj_num)\n    test_dataset_loader_b= torch.utils.data.DataLoader(CLEVR(root,mode,img_paths,files_names,obj_num), shuffle=False,batch_size=exp_dict['bs'],pin_memory=True, num_workers=4)\n    obj_encodings_gt_b = ap_utils.get_obj_encodings(test_dataset_loader_b)\n\n    print(\"loaded data\")\n    \n    \n    # Resume the training if requested\n    start_e= 0\n    if exp_dict['resume']:\n        print(\"resuming experiment\")\n        saved_model = torch.load(saveModelPath)\n        \n        #load pytorch models\n        color_net.load_state_dict(saved_model['color_net'])\n        shape_net.load_state_dict(saved_model['shape_net'])\n        material_net.load_state_dict(saved_model['material_net'])\n        size_net.load_state_dict(saved_model['size_net'])\n        slot_net.load_state_dict(saved_model['slot_net'])\n        \n        \n        #optimizers and shedulers\n        optimizers['smsc'].load_state_dict(saved_model['resume']['optimizer_smsc'])\n        optimizers['slot'].load_state_dict(saved_model['resume']['optimizer_slot'])\n        start_e = saved_model['resume']['epoch']\n    \n        #metrics\n        test_ap_list_a = saved_model['test_ap_list_a']\n        test_ap_list_b = saved_model['test_ap_list_b']\n\n        test_metric_list_a = saved_model['test_metric_list_a']\n        test_metric_list_b = saved_model['test_metric_list_b']\n\n        lr_list = saved_model['lr_list']\n        loss_list = saved_model['loss_list']\n        train_test_times = saved_model['train_test_times']\n        sm_per_batch_list = saved_model['sm_per_batch_list']  \n    \n\n\n    # train the network and evaluate the performance\n    for e in range(start_e, exp_dict['epochs']):\n        #we have three datasets right now train, val and test with 20k, 5k and 100 samples\n                \n        #TRAIN\n        print('Epoch {}/{}...'.format(e+1, exp_dict['epochs']))\n        time_train = time.time()\n        \n        #apply lr schedulers\n        if e < warmup_epochs:\n            lr = slot_base_lr * ((e+1)/warmup_epochs)\n        else:\n            lr = slot_base_lr\n        lr = lr * 0.5**((e+1)/decay_epochs)\n        optimizers['slot'].param_groups[0]['lr'] = lr\n        lr_list.append([lr,e])\n        print(\"LR SAm:\", \"{:.6f}\".format(lr), optimizers['slot'].param_groups[0]['lr'])\n\n\n        loss,_,_,_,sm_per_batch  = SLASHobj.learn(dataset_loader = train_dataset_loader, slot_net=slot_net,hungarian_matching = True, method=exp_dict['method'], p_num=exp_dict['p_num'], k_num=1,\n                              epoch=e, writer = writer)\n        \n\n        sm_per_batch_list.append(sm_per_batch)\n        loss_list.append(loss)\n        writer.add_scalar(\"train/loss\", loss, global_step=e)\n\n        \n        timestamp_train, time_train_sec = utils.time_delta_now(time_train)\n                \n        #TEST\n        time_test = time.time()\n\n        #use only a fraction of the test data, except every 100th epoch\n        if e % 100 == 0:\n            subset_size = None\n        else:\n            subset_size = 1500\n\n        ### CONDITION A\n        print(\"condition a\")\n        \n        #forward test batch\n        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader_a)\n\n        #compute the average precision, tp, fp, tn for color+shape+material+size\n        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n        ap_a, true_positives_a,false_positives_a, true_negatives_a, correctly_classified_a  = ap_utils.average_precision(pred, obj_encodings_gt_a, -1, \"CLEVR\",subset_size = subset_size)\n        print(\"avg precision A \", ap_a, \"tp\", true_positives_a, \"fp\", false_positives_a, \"tn\", true_negatives_a, \"correctly classified\", correctly_classified_a)\n\n        test_ap_list_a.append([ap_a, e])                    \n        test_metric_list_a.append([true_positives_a, false_positives_a, true_negatives_a, correctly_classified_a])\n        \n        ### CONDITION B\n        print(\"condition b\")\n        \n        #forward test batch\n        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader_b)\n\n        #compute the average precision, tp, fp, tn for color+shape+material+size\n        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n        ap_b, true_positives_b,false_positives_b, true_negatives_b, correctly_classified_b  = ap_utils.average_precision(pred, obj_encodings_gt_b, -1, \"CLEVR\",subset_size = subset_size)\n        print(\"avg precision A \", ap_b, \"tp\", true_positives_b, \"fp\", false_positives_b, \"tn\", true_negatives_b, \"correctly classified\", correctly_classified_b)\n\n        test_ap_list_b.append([ap_b, e])                    \n        test_metric_list_b.append([true_positives_b, false_positives_b, true_negatives_b, correctly_classified_b])\n        \n        \n        #Tensorboard outputs\n        writer.add_scalar(\"test/ap_a\", ap_a, global_step=e)\n        writer.add_scalar(\"test/ap_b\", ap_b, global_step=e)\n\n\n        #Time measurements\n        timestamp_test,time_test_sec = utils.time_delta_now(time_test)\n        timestamp_total,time_total_sec =  utils.time_delta_now(startTime)\n        \n        train_test_times.append([time_train_sec, time_test_sec, time_total_sec])\n        train_test_times_np = np.array(train_test_times)\n        print('--- train time:  ---', timestamp_train, '--- total: ---',train_test_times_np[:,0].sum())\n        print('--- test time:  ---' , timestamp_test, '--- total: ---',train_test_times_np[:,1].sum())\n        print('--- total time from beginning:  ---', timestamp_total )\n        \n        #save the neural network  such that we can use it later\n        print('Storing the trained model into {}'.format(saveModelPath))\n        torch.save({\"color_net\":color_net.state_dict(),\n                    \"shape_net\":shape_net.state_dict(),                    \n                    \"material_net\":material_net.state_dict(),\n                    \"size_net\":size_net.state_dict(),\n                    \"slot_net\":slot_net.state_dict(),\n                    \"resume\": {\n                        \"optimizer_smsc\":optimizers['smsc'].state_dict(),\n                        \"optimizer_slot\":optimizers['slot'].state_dict(),\n                        \"epoch\":e\n                    },\n                    \"test_ap_list_a\":test_ap_list_a,\n                    \"test_ap_list_b\":test_ap_list_b,\n                    \"loss_list\":loss_list,                    \n                    \"sm_per_batch_list\":sm_per_batch_list,\n                    \"test_metric_list_a\":test_metric_list_a,\n                    \"test_metric_list_b\":test_metric_list_b,\n                    \"lr_list\":lr_list,\n                    \"num_params\":num_params,\n                    \"train_test_times\": train_test_times,\n                    \"exp_dict\":exp_dict,\n                    \"program\":program}, saveModelPath)\n        \n        # Update the RTPT\n        rtpt.step(subtitle=f\"ap_b={ap_b:2.2f}\")", "        "]}
{"filename": "src/experiments/slash_attention/clevr_cogent/__init__.py", "chunked_list": [""]}
{"filename": "src/experiments/slash_attention/clevr_cogent/dataGen.py", "chunked_list": ["import torch\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import transforms\n\n# from torch.utils.data import Dataset\n# from torchvision import transforms\nfrom skimage import io\nimport os\nimport numpy as np", "import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport json\n\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\n\ndef class_lookup(idx):\n    '''\n    Shapeworld dataset generation generates a class id for each class. Returns color, shape and object encoding given the class id.\n    The class order was generated by ShapeWorld.\n    '''\n    \n    #obj encoding ['red','blue','green','black',  'circle' , 'triangle','square', 'bg', 'object_confidence(1)']\n\n    if  idx == 0:\n        return \"red\", \"circle\", [1,0,0,0  ,1,0,0,0 , 1]\n    elif idx ==1:\n        return \"green\", \"circle\" ,[0,0,1,0  ,1,0,0,0 , 1]\n    elif idx ==2:\n        return \"blue\", \"circle\", [0,1,0,0  ,1,0,0,0 , 1]\n    elif idx ==3:\n        return \"red\", \"square\", [1,0,0,0  ,0,0,1,0 , 1]\n    elif idx ==4:\n        return \"green\", \"square\", [0,0,1,0  ,0,0,1,0 , 1]\n    elif idx ==5:\n        return \"blue\", \"square\" , [0,1,0,0  ,0,0,1,0 , 1]\n    elif idx ==6:\n        return \"red\", \"triangle\", [1,0,0,0  ,0,1,0,0 , 1]\n    elif idx ==7:\n        return \"green\", \"triangle\", [0,0,1,0  ,0,1,0,0 , 1]\n    elif idx ==8:\n        return \"blue\", \"triangle\", [0,1,0,0  ,0,1,0,0 , 1]", "    \ndef object_encoding(size, material, shape, color ):\n    \n    #size (small, large, bg)\n    if size == \"small\":\n        size_enc = [1,0,0]\n    elif size == \"large\":\n        size_enc = [0,1,0]\n    elif size == \"bg\":\n        size_enc = [0,0,1]\n    \n    #material (rubber, metal, bg)\n    if material == \"rubber\":\n        material_enc = [1,0,0]\n    elif material == \"metal\":\n        material_enc = [0,1,0]\n    elif material == \"bg\":\n        material_enc = [0,0,1]\n    \n    #shape (cube, sphere, cylinder, bg)\n    if shape == \"cube\":\n        shape_enc = [1,0,0,0]\n    elif shape == \"sphere\":\n        shape_enc = [0,1,0,0]\n    elif shape == \"cylinder\":\n        shape_enc = [0,0,1,0]\n    elif shape == \"bg\":\n        shape_enc = [0,0,0,1]\n    \n    #color (gray, red, blue, green, brown, purple, cyan, yellow, bg)\n    #color (gray, red, blue, green, brown, purple, cyan, yellow, bg)\n    #color  1      2     3     4     5     6        7     8      9\n    if color == \"gray\":\n        color_enc = [1,0,0,0,0,0,0,0,0]\n    elif color == \"red\":\n        color_enc = [0,1,0,0,0,0,0,0,0]\n    elif color == \"blue\":\n        color_enc = [0,0,1,0,0,0,0,0,0]\n    elif color == \"green\":\n        color_enc = [0,0,0,1,0,0,0,0,0]\n    elif color == \"brown\":\n        color_enc = [0,0,0,0,1,0,0,0,0]\n    elif color == \"purple\":\n        color_enc = [0,0,0,0,0,1,0,0,0]\n    elif color == \"cyan\":\n        color_enc = [0,0,0,0,0,0,1,0,0]\n    elif color == \"yellow\":\n        color_enc = [0,0,0,0,0,0,0,1,0]\n    elif color == \"bg\":\n        color_enc = [0,0,0,0,0,0,0,0,1]\n        \n    return size_enc + material_enc + shape_enc + color_enc +[1]", "       \n\n\n    \n    \nclass CLEVR(Dataset):\n    def __init__(self, root, mode, img_paths=None, files_names=None, obj_num=None):\n        self.root = root  # The root folder of the dataset\n        self.mode = mode  # The mode of 'train' or 'val'\n        self.files_names = files_names # The list of the files names with correct nuber of objects\n        if obj_num is not None:\n            self.obj_num = obj_num  # The upper limit of number of objects \n        else:\n            self.obj_num = 10\n\n        assert os.path.exists(root), 'Path {} does not exist'.format(root)\n\n        #list of sorted image paths\n        self.img_paths = []\n        if img_paths:\n            self.img_paths = img_paths\n        else:                    \n            #open directory and save all image paths\n            for file in os.scandir(os.path.join(root, 'images', mode)):\n                img_path = file.path\n                if '.png' in img_path:\n                    self.img_paths.append(img_path)\n\n        self.img_paths.sort()\n        count = 0\n        \n        #target maps of the form {'target:idx': query string} or {'target:idx': obj encoding}\n        self.query_map = {}\n        self.obj_map = {}\n        \n        count = 0        \n        #We have up to 10 objects in the image, load the json file\n        with open(os.path.join(root, 'scenes','CLEVR_'+ mode+\"_scenes.json\")) as f:\n            data = json.load(f)\n            \n            #iterate over each scene and create the query string and obj encoding\n            print(\"parsing scences\")\n            for scene in tqdm(data['scenes']):\n                target_query = \"\"\n                obj_encoding_list = []\n\n                if self.files_names:\n                    if any(scene['image_filename'] in file_name for file_name in files_names):                    \n                        num_objects = 0\n                        for idx, obj in enumerate(scene['objects']):\n                            target_query += \" :- not object(o{}, {}, {}, {}, {}).\".format(idx+1, obj['size'], obj['material'], obj['shape'], obj['color'])\n                            obj_encoding_list.append(object_encoding(obj['size'], obj['material'], obj['shape'], obj['color']))\n                            num_objects = idx+1 #store the num of objects \n                        #fill in background objects\n                        for idx in range(num_objects, self.obj_num):\n                            target_query += \" :- not object(o{}, bg, bg, bg, bg).\".format(idx+1)\n                            obj_encoding_list.append([0,0,1, 0,0,1, 0,0,0,1, 0,0,0,0,0,0,0,0,1, 1])\n                        self.query_map[count] = target_query\n                        self.obj_map[count] = np.array(obj_encoding_list)\n                        count += 1\n                else:\n                    num_objects=0\n                    for idx, obj in enumerate(scene['objects']):\n                        target_query += \" :- not object(o{}, {}, {}, {}, {}).\".format(idx+1, obj['size'], obj['material'], obj['shape'], obj['color'])\n                        obj_encoding_list.append(object_encoding(obj['size'], obj['material'], obj['shape'], obj['color']))\n                        num_objects = idx+1 #store the num of objects \n                    #fill in background objects\n                    for idx in range(num_objects, 10):\n                        target_query += \" :- not object(o{}, bg, bg, bg, bg).\".format(idx+1)\n                        obj_encoding_list.append([0,0,1, 0,0,1, 0,0,0,1, 0,0,0,0,0,0,0,0,1, 1])\n                    self.query_map[scene['image_index']] = target_query\n                    self.obj_map[scene['image_index']] = np.array(obj_encoding_list)\n                \n            print(\"done\")\n        if self.files_names:\n            print(f'Correctly found images {count} out of {len(files_names)}')\n                    \n        \n        #print(np.array(list(self.obj_map.values()))[0:20])\n    def __getitem__(self, index):\n        #get the image\n        img_path = self.img_paths[index]\n        img = io.imread(img_path)[:, :, :3]\n        \n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            #transforms.CenterCrop((29, 221,64, 256)), #why do we need to crop?\n            transforms.Resize((128, 128)),\n            transforms.ToTensor(),\n        ])\n\n        img = transform(img)\n        img = (img - 0.5) * 2.0  # Rescale to [-1, 1].\n\n        return {'im':img}, self.query_map[index] ,self.obj_map[index]\n\n\n                \n    def __len__(self):\n        return len(self.img_paths)", "\n"]}
{"filename": "src/experiments/slash_attention/clevr_cogent/slash_attention_clevr.py", "chunked_list": ["#!/usr/bin/env python\n# coding: utf-8\n\n\n\nimport train\n\nimport datetime\n\n", "\n\n\n\n\nseed = 4\nobj_num = 10\ndate_string = datetime.datetime.today().strftime('%d-%m-%Y')\nexperiments = {f'CLEVR{obj_num}_seed_{seed}': {'structure':'poon-domingos', 'pd_num_pieces':[4], 'learn_prior':True,\n                         'lr': 0.01, 'bs':512, 'epochs':1000,", "experiments = {f'CLEVR{obj_num}_seed_{seed}': {'structure':'poon-domingos', 'pd_num_pieces':[4], 'learn_prior':True,\n                         'lr': 0.01, 'bs':512, 'epochs':1000,\n                        'lr_warmup_steps':8, 'lr_decay_steps':360, 'use_em':False, 'resume':False,\n                        'method':'most_prob',\n                         'start_date':date_string, 'credentials':'DO', 'p_num':16, 'seed':seed, 'obj_num':obj_num\n                        }}\n\n\nfor exp_name in experiments:\n    print(exp_name)\n    train.slash_slot_attention(exp_name, experiments[exp_name])", "for exp_name in experiments:\n    print(exp_name)\n    train.slash_slot_attention(exp_name, experiments[exp_name])\n\n\n\n"]}
{"filename": "src/experiments/slash_attention/clevr_cogent/auxiliary.py", "chunked_list": ["import json\nimport os\nfrom pathlib import Path\n\n\ndef get_files_names_and_paths(root:str='./data/CLEVR_v1.0/', mode:str='val', obj_num:int=4):\n    data_file = Path(os.path.join(root, 'scenes','CLEVR_'+ mode+\"_scenes.json\"))\n    data_file.parent.mkdir(parents=True, exist_ok=True)\n    if data_file.exists():\n        print('File exists. Parsing file...')\n    else:\n        print(f'The JSON file {data_file} does not exist!')\n        quit()\n    img_paths = []\n    files_names = []\n    with open(data_file, 'r') as json_file:\n        json_data = json.load(json_file)\n\n        for scene in json_data['scenes']:\n            if len(scene['objects']) <= obj_num:\n                img_paths.append(Path(os.path.join(root,'images/'+mode+'/'+scene['image_filename'])))\n                files_names.append(scene['image_filename'])\n\n    print(\"...done \")\n    return img_paths, files_names", "\n\ndef get_slash_program(obj_num:int=4):\n    program = ''\n    if obj_num == 10:\n        program ='''\n    slot(s1).\n    slot(s2).\n    slot(s3).\n    slot(s4).\n    slot(s5).\n    slot(s6).\n    slot(s7).\n    slot(s8).\n    slot(s9).\n    slot(s10).\n\n    name(o1).\n    name(o2).\n    name(o3).\n    name(o4).\n    name(o5).\n    name(o6).\n    name(o7).\n    name(o8).\n    name(o9).\n    name(o10).\n        '''\n    elif obj_num ==4:\n        program ='''\n    slot(s1).\n    slot(s2).\n    slot(s3).\n    slot(s4).\n\n    name(o1).\n    name(o2).\n    name(o3).\n    name(o4).\n        '''\n    elif obj_num ==6:\n        program ='''\n    slot(s1).\n    slot(s2).\n    slot(s3).\n    slot(s4).\n    slot(s5).\n    slot(s6).\n\n    name(o1).\n    name(o2).\n    name(o3).\n    name(o4).\n    name(o5).\n    name(o6).\n        '''\n    else:\n        print(f'The number of objects {obj_num} is wrong!')\n        quit()\n    program +='''        \n    %assign each name a slot\n    %{slot_name_comb(N,X): slot(X)}=1 :- name(N). %problem we have dublicated slots\n\n    %remove each model which has multiple slots asigned to the same name\n    %:- slot_name_comb(N1,X1), slot_name_comb(N2,X2), X1 == X2, N1 != N2. \n\n    %build the object ontop of the slot assignment\n    object(N, S, M, P, C) :- size(0, +X, -S), material(0, +X, -M), shape(0, +X, -P), color(0, +X, -C), slot(X), name(N), slot_name_comb(N,X).\n\n    %define the SPNs\n    npp(size(1,X),[small, large, bg]) :- slot(X).\n    npp(material(1,X),[rubber, metal, bg]) :- slot(X).\n    npp(shape(1,X),[cube, sphere, cylinder, bg]) :- slot(X).\n    npp(color(1,X),[gray, red, blue, green, brown, purple, cyan, yellow, bg]) :- slot(X).\n\n    '''\n    return program"]}
{"filename": "src/experiments/slash_attention/cogent/train.py", "chunked_list": ["print(\"start importing...\")\n\nimport os\nimport time\nimport sys\nimport json\nsys.path.append('../../../')\nsys.path.append('../../../SLASH/')\nsys.path.append('../../../EinsumNetworks/src/')\n", "sys.path.append('../../../EinsumNetworks/src/')\n\n#torch, numpy, ...\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom warmup_scheduler import GradualWarmupScheduler\n\n", "\n\nimport numpy as np\n\n#own modules\nfrom dataGen import SHAPEWORLD_COGENT\nfrom einsum_wrapper import EiNet\nfrom slash import SLASH\nimport ap_utils\nimport utils", "import ap_utils\nimport utils\nfrom utils import set_manual_seed\n\nfrom slot_attention_module import SlotAttention_model\nfrom pathlib import Path\n\nfrom rtpt import RTPT\n\n#seeds", "\n#seeds\ntorch.manual_seed(42)\nnp.random.seed(42)\nprint(\"...done\")\n\n\n\n\n", "\n\n\ndef slash_slot_attention(exp_name , exp_dict):\n    program ='''\n    slot(s1).\n    slot(s2).\n    slot(s3).\n    slot(s4).\n    name(o1).\n    name(o2).\n    name(o3).\n    name(o4).\n\n    %build the object ontop of the slot assignment\n    object(N,C,S,H,Z) :- color(0, +X, -C), shape(0, +X, -S), shade(0, +X, -H), size(0, +X, -Z), slot(X), name(N), slot_name_comb(N,X).\n\n    npp(color(1,X),[red, blue, green, gray, brown, magenta, cyan, yellow, black]) :- slot(X).\n    npp(shape(1,X),[circle, triangle, square, bg]) :- slot(X).\n    npp(shade(1,X),[bright, dark, bg]) :- slot(X).\n    npp(size(1,X),[small, big, bg]) :- slot(X).\n    '''\n\n    if exp_dict['hungarian_matching'] == False:\n        program += \"\"\"\n        %assign each name a slot\n        {slot_name_comb(N,X): slot(X) }=1 :- name(N). %problem we have dublicated slots\n        %remove each model which has multiple slots asigned to the same name\n        %:-  slot_name_comb(N1,X1), slot_name_comb(N2,X2), X1 == X2, N1 != N2.\n        {slot_name_comb(N,X): name(N) }=1 :- slot(X). %problem we have dublicated slots\n        \"\"\"\n\n    # Set the seeds for PRNG\n    set_manual_seed(exp_dict['seed'])\n    \n    # Create RTPT object\n    rtpt = RTPT(name_initials=exp_dict['credentials'], experiment_name='SLASH Shapeworld4 CoGenT', max_iterations=int(exp_dict['epochs']))\n\n    # Start the RTPT tracking\n    rtpt.start()\n    \n    \n    # create save paths and tensorboard writer\n    writer = SummaryWriter(os.path.join(\"runs\", exp_name,exp_dict['method'], str(exp_dict['seed'])), purge_step=0)\n    saveModelPath = 'data/'+exp_name+'_'+exp_dict['method']+'/slash_slot_models_seed'+str(exp_dict['seed'])+'.pt'\n    Path(\"data/\"+exp_name+'_'+exp_dict['method']+\"/\").mkdir(parents=True, exist_ok=True)\n\n\n    # save args\n    with open(os.path.join(\"runs\", exp_name, 'args.json'), 'w') as json_file:\n        json.dump(exp_dict, json_file, indent=4)\n\n    print(\"Experiment parameters:\", exp_dict)\n\n\n    #setup new SLASH program given the network parameters\n    if exp_dict['structure'] == 'poon-domingos':\n        exp_dict['depth'] = None\n        exp_dict['num_repetitions'] = None\n        print(\"using poon-domingos\")\n\n    elif exp_dict['structure'] == 'binary-trees':\n        exp_dict['pd_num_pieces'] = None\n        print(\"using binary-trees\")\n\n    \n    #NETWORKS\n        \n    #color network\n    color_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 64,\n        class_count=9,\n        pd_width=8,pd_height=8,\n        use_em= False)\n\n    #shape network\n    shape_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 64,\n        class_count=4,\n        pd_width=8,pd_height=8,\n        use_em= False)\n    \n    #shade network\n    shade_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 64,\n        class_count=3,\n        pd_width=8,pd_height=8,\n        use_em= False)\n    \n    #size network\n    size_net = EiNet(structure = exp_dict['structure'],\n        pd_num_pieces = exp_dict['pd_num_pieces'],\n        depth = exp_dict['depth'],\n        num_repetitions = exp_dict['num_repetitions'],\n        num_var = 64,\n        class_count=3,\n        pd_width=8,pd_height=8,\n        use_em= False)\n    \n    \n    \n    #create the Slot Attention network\n    slot_net = SlotAttention_model(n_slots=4, n_iters=3, n_attr=18,\n                                encoder_hidden_channels=64, attention_hidden_channels=64)\n    slot_net = slot_net.to(device='cuda')\n    \n    \n\n    #count trainable params\n    num_trainable_params = [sum(p.numel() for p in color_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in shape_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in shade_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in size_net.parameters() if p.requires_grad),\n                            sum(p.numel() for p in slot_net.parameters() if p.requires_grad)]\n    num_params = [sum(p.numel() for p in color_net.parameters()), \n                  sum(p.numel() for p in shape_net.parameters()),\n                  sum(p.numel() for p in shade_net.parameters()),\n                  sum(p.numel() for p in size_net.parameters()),\n                  sum(p.numel() for p in slot_net.parameters())]\n    \n    print(\"training with {}({}) trainable params and {}({}) params in total\".format(np.sum(num_trainable_params),num_trainable_params,np.sum(num_params),num_params))\n    \n     \n    slot_net_params = list(slot_net.parameters())\n    csss_params = list(color_net.parameters()) + list(shape_net.parameters()) + list(shade_net.parameters()) + list(size_net.parameters())\n    \n    #create the SLASH Program\n    nnMapping = {'color': color_net,\n                 'shape':shape_net,\n                 'shade':shade_net,\n                 'size':size_net}\n    \n    \n\n    #OPTIMIZERS and LEARNING RATE SHEDULING\n    print(\"training probabilisitc circuits and SlotAttention module\")\n    optimizers = {'csss': torch.optim.Adam([\n                                            {'params':csss_params}],\n                                            lr=exp_dict['lr'], eps=1e-7),\n                 'slot': torch.optim.Adam([\n                                            {'params': slot_net_params}],\n                                            lr=0.0004, eps=1e-7)}\n   \n    \n    SLASHobj = SLASH(program, nnMapping, optimizers)\n    \n    print(\"using learning rate warmup and decay\")\n    warmup_epochs = exp_dict['lr_warmup_steps'] #warmup for x epochs\n    decay_epochs = exp_dict['lr_decay_steps']\n    slot_base_lr = 0.0004\n    \n\n    \n    #metric lists\n    test_ap_list_a = [] #stores average precsion values for test set with condition a\n    test_ap_list_b = [] #stores average precsion values for test set with condition b\n\n    test_metric_list_a = [] #stores tp, fp, tn values for test set with condition a\n    test_metric_list_b = [] #stores tp, fp, tn values for test set with condition b\n\n    lr_list = [] # store learning rate\n    loss_list = []  # store training loss\n\n    startTime = time.time()\n    train_test_times = []\n    sm_per_batch_list = []\n    \n    # load datasets\n    print(\"loading data...\")\n\n    #if the hungarian matching algorithm is used we need to pass the object encodings to SLASH\n    if exp_dict['hungarian_matching']:\n        train_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD_COGENT('../../data/shapeworld_cogent/',\"train_a\", ret_obj_encoding=True), shuffle=True,batch_size=exp_dict['bs'],pin_memory=False, num_workers=4)\n    else:\n        train_dataset_loader = torch.utils.data.DataLoader(SHAPEWORLD_COGENT('../../data/shapeworld_cogent/',\"train_a\"), shuffle=True,batch_size=exp_dict['bs'],pin_memory=False, num_workers=4)\n    \n    test_dataset_loader_a = torch.utils.data.DataLoader(SHAPEWORLD_COGENT('../../data/shapeworld_cogent/',\"val_a\", ret_obj_encoding=True), shuffle=False,batch_size=exp_dict['bs'],pin_memory=False, num_workers=4)\n    test_dataset_loader_b = torch.utils.data.DataLoader(SHAPEWORLD_COGENT('../../data/shapeworld_cogent/',\"val_b\", ret_obj_encoding=True), shuffle=False,batch_size=exp_dict['bs'],pin_memory=False, num_workers=4)\n\n\n    obj_encodings_gt_a = ap_utils.get_obj_encodings(test_dataset_loader_a)\n    obj_encodings_gt_b = ap_utils.get_obj_encodings(test_dataset_loader_b)\n    print(\"...done\")\n\n\n    \n    start_e= 0\n    if exp_dict['resume']:\n        print(\"resuming experiment\")\n        saved_model = torch.load(saveModelPath)\n        \n        #load pytorch models\n        color_net.load_state_dict(saved_model['color_net'])\n        shape_net.load_state_dict(saved_model['shape_net'])\n        shade_net.load_state_dict(saved_model['shade_net'])\n        size_net.load_state_dict(saved_model['size_net'])\n        slot_net.load_state_dict(saved_model['slot_net'])\n        \n        \n        #optimizers and shedulers\n        optimizers['csss'].load_state_dict(saved_model['resume']['optimizer_csss'])\n        optimizers['slot'].load_state_dict(saved_model['resume']['optimizer_slot'])\n        start_e = saved_model['resume']['epoch']\n\n        \n        #metrics\n        test_ap_list_a = saved_model['test_ap_list_a']\n        test_ap_list_b = saved_model['test_ap_list_b']\n\n        test_metric_list_a = saved_model['test_metric_list_a']\n        test_metric_list_b = saved_model['test_metric_list_b']\n\n        lr_list = saved_model['lr_list']\n        loss_list = saved_model['loss_list']\n        train_test_times = saved_model['train_test_times']\n        sm_per_batch_list = saved_model['sm_per_batch_list']\n        \n            \n        \n    for e in range(start_e, exp_dict['epochs']):\n                \n        #TRAIN\n        print('Epoch {}/{}...'.format(e+1, exp_dict['epochs']))\n        time_train= time.time()\n        \n        #apply lr schedulers to the SAm\n        if e < warmup_epochs:\n            lr = slot_base_lr * ((e+1)/warmup_epochs)\n        else:\n            lr = slot_base_lr\n        lr = lr * 0.5**((e+1)/decay_epochs)\n        optimizers['slot'].param_groups[0]['lr'] = lr\n        lr_list.append([lr,e])\n        \n        loss,_,_,_,sm_per_batch = SLASHobj.learn(dataset_loader = train_dataset_loader, slot_net=slot_net, hungarian_matching=exp_dict['hungarian_matching'], method=exp_dict['method'], p_num = exp_dict['p_num'], k_num=1,\n                        epoch=e, writer = writer)\n        \n        sm_per_batch_list.append(sm_per_batch)\n        loss_list.append(loss)\n        writer.add_scalar(\"train/loss\", loss, global_step=e)\n\n        timestamp_train, time_train_sec = utils.time_delta_now(time_train)\n        \n        \n        #TEST\n        time_test = time.time()\n\n        ### CONDITION A\n        print(\"condition a\")\n\n        #forward test batch a\n        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader_a)\n                            \n        #compute the average precision, tp, fp, tn for color+shape+shade+size, color, shape\n        #color+shape+shade+size\n        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n        ap, true_positives,false_positives, true_negatives, correctly_classified = ap_utils.average_precision(pred, obj_encodings_gt_a,-1, \"SHAPEWORLD4\")\n        print(\"avg precision(a) \",ap, \"tp\", true_positives, \"fp\", false_positives, \"tn\", true_negatives, \"correctly classified\",correctly_classified)\n                   \n        #color\n        pred_c = ap_utils.inference_map_to_array(inference, only_color=True).cpu().numpy()\n        ap_c, true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c= ap_utils.average_precision(pred_c, obj_encodings_gt_a,-1, \"SHAPEWORLD4\", only_color = True)\n        print(\"avg precision(a) color\",ap_c, \"tp\", true_positives_c, \"fp\", false_positives_c, \"tn\", true_negatives_c, \"correctly classified\",correctly_classified_c)\n\n        #shape              \n        pred_s = ap_utils.inference_map_to_array(inference, only_shape=True).cpu().numpy()\n        ap_s, true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s= ap_utils.average_precision(pred_s, obj_encodings_gt_a,-1, \"SHAPEWORLD4\", only_shape = True)\n        print(\"avg precision(a) shape\",ap_s, \"tp\", true_positives_s, \"fp\", false_positives_s, \"tn\", true_negatives_s, \"correctly classified\",correctly_classified_s)\n        \n    \n        #store ap, tp, fp, tn\n        test_ap_list_a.append([ap, ap_c, ap_s, e])                    \n        test_metric_list_a.append([true_positives, false_positives, true_negatives,correctly_classified,\n                                 true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c,\n                                 true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s])\n\n        \n        writer.add_scalar(\"test/ap_cond_a\", ap, global_step=e)\n        writer.add_scalar(\"test/ap_c_cond_a\", ap_c, global_step=e)\n        writer.add_scalar(\"test/ap_s_cond_a\", ap_s, global_step=e)\n\n    \n    \n        ### CONDITION B\n        print(\"condition b\")\n         #forward test batch a\n        inference = SLASHobj.forward_slot_attention_pipeline(slot_net=slot_net, dataset_loader= test_dataset_loader_b)\n             \n                            \n        #compute the average precision, tp, fp, tn for color+shape+shade+size, color, shap\n        #color+shape+shade+size\n        pred = ap_utils.inference_map_to_array(inference).cpu().numpy()\n        ap, true_positives,false_positives, true_negatives, correctly_classified = ap_utils.average_precision(pred, obj_encodings_gt_b,-1, \"SHAPEWORLD4\")\n        print(\"avg precision(b) \",ap, \"tp\", true_positives, \"fp\", false_positives, \"tn\", true_negatives, \"correctly classified\",correctly_classified)\n                   \n        #color\n        pred_c = ap_utils.inference_map_to_array(inference, only_color=True).cpu().numpy()\n        ap_c, true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c= ap_utils.average_precision(pred, obj_encodings_gt_b,-1, \"SHAPEWORLD4\", only_color = True)\n        print(\"avg precision(b) color\",ap_c, \"tp\", true_positives_c, \"fp\", false_positives_c, \"tn\", true_negatives_c, \"correctly classified\",correctly_classified_c)\n\n        #shape              \n        pred_s = ap_utils.inference_map_to_array(inference, only_shape=True).cpu().numpy()\n        ap_s, true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s= ap_utils.average_precision(pred_s, obj_encodings_gt_b,-1, \"SHAPEWORLD4\", only_shape = True)\n        print(\"avg precision(b) shape\",ap_s, \"tp\", true_positives_s, \"fp\", false_positives_s, \"tn\", true_negatives_s, \"correctly classified\",correctly_classified_s)\n            \n        \n        #store ap, tp, fp, tn\n        test_ap_list_b.append([ap, ap_c, ap_s, e])                    \n        test_metric_list_b.append([true_positives, false_positives, true_negatives,correctly_classified,\n                                 true_positives_c, false_positives_c, true_negatives_c, correctly_classified_c,\n                                 true_positives_s, false_positives_s, true_negatives_s, correctly_classified_s])\n                            \n        writer.add_scalar(\"test/ap_cond_b\", ap, global_step=e)\n        writer.add_scalar(\"test/ap_c_cond_b\", ap_c, global_step=e)\n        writer.add_scalar(\"test/ap_s_cond_b\", ap_s, global_step=e)\n\n        #Time measurements\n        timestamp_test,time_test_sec = utils.time_delta_now(time_test)\n        timestamp_total,time_total_sec =  utils.time_delta_now(startTime)\n        \n        train_test_times.append([time_train_sec, time_test_sec, time_total_sec])\n        train_test_times_np = np.array(train_test_times)        \n\n        print('--- train time:  ---', timestamp_train, '--- total: ---',train_test_times_np[:,0].sum())\n        print('--- test time:  ---' , timestamp_test, '--- total: ---',train_test_times_np[:,1].sum())\n        print('--- total time from beginning:  ---', timestamp_total )\n        \n\n        #save the neural network  such that we can use it later\n        print('Storing the trained model into {}'.format(saveModelPath))\n        torch.save({\"shape_net\":  shape_net.state_dict(), \n                    \"color_net\": color_net.state_dict(),\n                    \"shade_net\": shade_net.state_dict(),\n                    \"size_net\": size_net.state_dict(),\n                    \"slot_net\": slot_net.state_dict(),\n                    \"resume\": {\n                        \"optimizer_csss\":optimizers['csss'].state_dict(),\n                        \"optimizer_slot\": optimizers['slot'].state_dict(),\n                        \"epoch\":e\n                    },\n                    \"test_ap_list_a\":test_ap_list_a,\n                    \"test_ap_list_b\":test_ap_list_b,\n                    \"loss_list\":loss_list,\n                    \"sm_per_batch_list\":sm_per_batch_list,\n                    \"test_metric_list_a\":test_metric_list_a,\n                    \"test_metric_list_b\":test_metric_list_b,\n                    \"lr_list\": lr_list,\n                    \"num_params\": num_params,\n                    \"train_test_times\": train_test_times,\n                    \"exp_dict\":exp_dict,\n                    \"program\":program}, saveModelPath)\n        \n        # Update the RTPT\n        rtpt.step()", "\n        \n\n\n\n"]}
{"filename": "src/experiments/slash_attention/cogent/__init__.py", "chunked_list": [""]}
{"filename": "src/experiments/slash_attention/cogent/dataGen.py", "chunked_list": ["import torch\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import transforms\n\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom skimage import io\nimport os\nimport numpy as np", "import os\nimport numpy as np\nimport torch\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport json\nimport datasets\n    \n       \ndef get_encoding(color, shape, shade, size):\n    \n    if color == 'red':\n        col_enc = [1,0,0,0,0,0,0,0,0]\n    elif color == 'blue':\n        col_enc = [0,1,0,0,0,0,0,0,0]\n    elif color == 'green':\n        col_enc = [0,0,1,0,0,0,0,0,0]\n    elif color == 'gray':\n        col_enc = [0,0,0,1,0,0,0,0,0]\n    elif color == 'brown':\n        col_enc = [0,0,0,0,1,0,0,0,0]\n    elif color == 'magenta':\n        col_enc = [0,0,0,0,0,1,0,0,0]\n    elif color == 'cyan':\n        col_enc = [0,0,0,0,0,0,1,0,0]\n    elif color == 'yellow':\n        col_enc = [0,0,0,0,0,0,0,1,0]\n    elif color == 'black':\n        col_enc = [0,0,0,0,0,0,0,0,1]\n        \n\n    if shape == 'circle':\n        shape_enc = [1,0,0,0]\n    elif shape == 'triangle':\n        shape_enc = [0,1,0,0]\n    elif shape == 'square':\n        shape_enc = [0,0,1,0]    \n    elif shape == 'bg':\n        shape_enc = [0,0,0,1]\n        \n    if shade == 'bright':\n        shade_enc = [1,0,0]\n    elif shade =='dark':\n        shade_enc = [0,1,0]\n    elif shade == 'bg':\n        shade_enc = [0,0,1]\n        \n        \n    if size == 'small':\n        size_enc = [1,0,0]\n    elif size == 'big':\n        size_enc = [0,1,0]\n    elif size == 'bg':\n        size_enc = [0,0,1]\n    \n    return col_enc + shape_enc + shade_enc + size_enc + [1]", "       \ndef get_encoding(color, shape, shade, size):\n    \n    if color == 'red':\n        col_enc = [1,0,0,0,0,0,0,0,0]\n    elif color == 'blue':\n        col_enc = [0,1,0,0,0,0,0,0,0]\n    elif color == 'green':\n        col_enc = [0,0,1,0,0,0,0,0,0]\n    elif color == 'gray':\n        col_enc = [0,0,0,1,0,0,0,0,0]\n    elif color == 'brown':\n        col_enc = [0,0,0,0,1,0,0,0,0]\n    elif color == 'magenta':\n        col_enc = [0,0,0,0,0,1,0,0,0]\n    elif color == 'cyan':\n        col_enc = [0,0,0,0,0,0,1,0,0]\n    elif color == 'yellow':\n        col_enc = [0,0,0,0,0,0,0,1,0]\n    elif color == 'black':\n        col_enc = [0,0,0,0,0,0,0,0,1]\n        \n\n    if shape == 'circle':\n        shape_enc = [1,0,0,0]\n    elif shape == 'triangle':\n        shape_enc = [0,1,0,0]\n    elif shape == 'square':\n        shape_enc = [0,0,1,0]    \n    elif shape == 'bg':\n        shape_enc = [0,0,0,1]\n        \n    if shade == 'bright':\n        shade_enc = [1,0,0]\n    elif shade =='dark':\n        shade_enc = [0,1,0]\n    elif shade == 'bg':\n        shade_enc = [0,0,1]\n        \n        \n    if size == 'small':\n        size_enc = [1,0,0]\n    elif size == 'big':\n        size_enc = [0,1,0]\n    elif size == 'bg':\n        size_enc = [0,0,1]\n    \n    return col_enc + shape_enc + shade_enc + size_enc + [1]", "    \n    \nclass SHAPEWORLD_COGENT(Dataset):\n    def __init__(self, root, mode, ret_obj_encoding=False):\n        \n        datasets.maybe_download_shapeworld_cogent()\n        \n        self.ret_obj_encoding= ret_obj_encoding\n        self.root = root\n        self.mode = mode\n        assert os.path.exists(root), 'Path {} does not exist'.format(root)\n\n        #dictionary of the form {'image_idx':'img_path'}\n        self.img_paths = {}\n        \n        \n        for file in os.scandir(os.path.join(root, 'images', mode)):\n            img_path = file.path\n            \n            img_path_idx =   img_path.split(\"/\")\n            img_path_idx = img_path_idx[-1]\n            img_path_idx = img_path_idx[:-4][6:]\n            try:\n                img_path_idx =  int(img_path_idx)\n                self.img_paths[img_path_idx] = img_path\n            except:\n                print(\"path:\",img_path_idx)\n                \n\n        \n        count = 0\n        \n        #target maps of the form {'target:idx': query string} or {'target:idx': obj encoding}\n        self.query_map = {}\n        self.obj_map = {}\n                \n        with open(os.path.join(root, 'labels', mode,\"world_model.json\")) as f:\n            worlds = json.load(f)\n            \n            objects = 0\n            bgs = 0\n            #iterate over all objects\n            for world in worlds:\n                num_objects = 0\n                target_query = \"\"\n                obj_enc = []\n                for entity in world['entities']:\n                    \n                    color = entity['color']['name']\n                    shape = entity['shape']['name']\n                    \n                    shade_val = entity['color']['shade']\n                    if shade_val == 0.0:\n                        shade = 'bright'\n                    else:\n                        shade = 'dark'\n                    \n                    size_val = entity['shape']['size']['x']\n                    if size_val == 0.075:\n                        size = 'small'\n                    elif size_val == 0.15:\n                        size = 'big'\n                    \n                    name = 'o' + str(num_objects+1)\n                    target_query = target_query+ \":- not object({},{},{},{},{}). \".format(name, color, shape, shade, size)\n                    obj_enc.append(get_encoding(color, shape, shade, size))\n                    num_objects += 1\n                    objects +=1 \n                    \n                #bg encodings\n                for i in range(num_objects, 4):\n                    name = 'o' + str(num_objects+1)\n                    target_query = target_query+ \":- not object({},black,bg, bg, bg). \".format(name)\n                    obj_enc.append(get_encoding(\"black\",\"bg\",\"bg\",\"bg\"))\n                    num_objects += 1\n                    bgs +=1\n\n                self.query_map[count] = target_query\n                self.obj_map[count] = np.array(obj_enc)\n                count+=1\n            print(\"num objects\",objects)\n            print(\"num bgs\",bgs)\n\n            \n                    \n    def __getitem__(self, index):\n        \n        #get the image\n        img_path = self.img_paths[index]\n        img = io.imread(img_path)[:, :, :3]\n        \n        transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.ToTensor(),\n        ])\n        img = transform(img)\n        img = (img - 0.5) * 2.0  # Rescale to [-1, 1].\n\n        if self.ret_obj_encoding:\n            return {'im':img}, self.query_map[index] ,self.obj_map[index]\n        else:\n            return {'im':img}, self.query_map[index]        \n    def __len__(self):\n        return len(self.img_paths)", "\n"]}
{"filename": "src/experiments/slash_attention/cogent/slash_attention_cogent.py", "chunked_list": ["#!/usr/bin/env python\n# coding: utf-8\n\n\nimport train\nimport numpy as np\nimport torch\nimport torchvision\nimport datetime\n", "import datetime\n\ndate_string = datetime.datetime.today().strftime('%d-%m-%Y')\n\n#Python script to start the shapeworld4 slot attention experiment\n#Define your experiment(s) parameters as a hashmap having the following parameters\nexample_structure = {'experiment_name': \n                   {'structure': 'poon-domingos',\n                    'pd_num_pieces': [4],\n                    'lr': 0.01, #the learning rate to train the SPNs with, the slot attention module has a fixed lr=0.0004  ", "                    'pd_num_pieces': [4],\n                    'lr': 0.01, #the learning rate to train the SPNs with, the slot attention module has a fixed lr=0.0004  \n                    'bs':512, #the batchsize\n                    'epochs':1000, #number of epochs to train\n                    'lr_warmup_steps':25, #number of epochs to warm up the slot attention module, warmup does not apply to the SPNs\n                    'lr_decay_steps':100, #number of epochs it takes to decay to 50% of the specified lr\n                    'start_date':\"01-01-0001\", #current date\n                    'resume':False, #you can stop the experiment and set this parameter to true to load the last state and continue learning\n                    'credentials':'AS', #your credentials for the rtpt class\n                    'explanation': \"\"\"Training on Condtion A, Testing on Condtion A and B to evaluate generalization of the model.\"\"\"}}", "                    'credentials':'AS', #your credentials for the rtpt class\n                    'explanation': \"\"\"Training on Condtion A, Testing on Condtion A and B to evaluate generalization of the model.\"\"\"}}\n\n\n\nexperiments ={'shapeworld4_cogent_hung': \n                   {'structure': 'poon-domingos', 'pd_num_pieces': [4],\n                    'lr': 0.01, 'bs':512, 'epochs':1000,\n                    'lr_warmup_steps':8, 'lr_decay_steps':360,\n                    'start_date':date_string, 'resume':False,", "                    'lr_warmup_steps':8, 'lr_decay_steps':360,\n                    'start_date':date_string, 'resume':False,\n                    'credentials':'DO', 'seed':3, 'learn_prior':True,\n                    'p_num':16, 'hungarian_matching':True, 'method':'probabilistic_grounding_top_k',\n                    'explanation': \"\"\"Training on Condtion A, Testing on Condtion A and B to evaluate generalization of the model.\"\"\"}}\n\n\n\n#train the network\nfor exp_name in experiments:\n    print(exp_name)\n    train.slash_slot_attention(exp_name, experiments[exp_name])", "#train the network\nfor exp_name in experiments:\n    print(exp_name)\n    train.slash_slot_attention(exp_name, experiments[exp_name])\n\n\n\n\n\n", "\n"]}
