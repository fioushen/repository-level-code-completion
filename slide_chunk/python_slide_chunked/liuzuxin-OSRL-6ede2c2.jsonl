{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport os\n\nfrom setuptools import find_packages, setup\n\n\ndef get_version() -> str:\n    # https://packaging.python.org/guides/single-sourcing-package-version/\n    init = open(os.path.join(\"osrl\", \"__init__.py\"), \"r\").read().split()\n    return init[init.index(\"__version__\") + 2][1:-1]", "def get_version() -> str:\n    # https://packaging.python.org/guides/single-sourcing-package-version/\n    init = open(os.path.join(\"osrl\", \"__init__.py\"), \"r\").read().split()\n    return init[init.index(\"__version__\") + 2][1:-1]\n\n\ndef get_install_requires() -> str:\n    return [\n        \"dsrl\",\n        \"fast-safe-rl\",\n        \"pyrallis==0.3.1\",\n        \"pyyaml~=6.0\",\n        \"scipy~=1.10.1\",\n        \"tqdm\",\n        \"numpy>1.16.0\",  # https://github.com/numpy/numpy/issues/12793\n        \"tensorboard>=2.5.0\",\n        \"torch~=1.13.0\",\n        \"numba>=0.51.0\",\n        \"wandb~=0.14.0\",\n        \"h5py>=2.10.0\",\n        \"protobuf~=3.19.0\",  # breaking change, sphinx fail\n        \"python-dateutil==2.8.2\",\n        \"easy_runner\",\n        \"swig==4.1.1\",\n    ]", "\n\ndef get_extras_require() -> str:\n    req = {\n        \"dev\": [\n            \"sphinx==6.2.1\",\n            \"sphinx_rtd_theme==1.2.0\",\n            \"jinja2==3.0.3\",  # temporary fix\n            \"sphinxcontrib-bibtex==2.5.0\",\n            \"flake8\",\n            \"flake8-bugbear\",\n            \"yapf\",\n            \"isort\",\n            \"pytest~=7.3.1\",\n            \"pytest-cov~=4.0.0\",\n            \"networkx\",\n            \"mypy\",\n            \"pydocstyle\",\n            \"doc8==0.11.2\",\n            \"scipy\",\n            \"pre-commit\",\n        ]\n    }\n    return req", "\n\nsetup(\n    name=\"osrl-lib\",\n    version=get_version(),\n    description=\n    \"Elegant Implementations of Offline Safe Reinforcement Learning Algorithms\",\n    long_description=open(\"README.md\", encoding=\"utf8\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/liuzuxin/offline-safe-rl-baselines.git\",", "    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/liuzuxin/offline-safe-rl-baselines.git\",\n    author=\"Zijian Guo; Zuxin Liu\",\n    author_email=\"zuxin1997@gmail.com\",\n    license=\"MIT\",\n    python_requires=\">=3.8\",\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta", "        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        \"Development Status :: 3 - Alpha\",\n        # Indicate who your project is intended for\n        \"Intended Audience :: Science/Research\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n        # Pick your license as you wish (should match \"license\" above)\n        \"License :: OSI Approved :: MIT License\",", "        # Pick your license as you wish (should match \"license\" above)\n        \"License :: OSI Approved :: MIT License\",\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n    ],\n    keywords=\"offline safe reinforcement learning algorithms pytorch\",\n    packages=find_packages(", "    keywords=\"offline safe reinforcement learning algorithms pytorch\",\n    packages=find_packages(\n        exclude=[\"test\", \"test.*\", \"examples\", \"examples.*\", \"docs\", \"docs.*\"]),\n    install_requires=get_install_requires(),\n    extras_require=get_extras_require(),\n)\n"]}
{"filename": "osrl/__init__.py", "chunked_list": ["__version__ = \"0.1.0\"\n\n__all__ = [\n    \"algorithms\",\n    \"common\",\n]\n"]}
{"filename": "osrl/common/net.py", "chunked_list": ["import math\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import distributions as pyd\nfrom torch.distributions.normal import Normal\n", "from torch.distributions.normal import Normal\n\n\ndef mlp(sizes, activation, output_activation=nn.Identity):\n    \"\"\"\n    Creates a multi-layer perceptron with the specified sizes and activations.\n\n    Args:\n        sizes (list): A list of integers specifying the size of each layer in the MLP.\n        activation (nn.Module): The activation function to use for all layers except the output layer.\n        output_activation (nn.Module): The activation function to use for the output layer. Defaults to nn.Identity.\n\n    Returns:\n        nn.Sequential: A PyTorch Sequential model representing the MLP.\n    \"\"\"\n\n    layers = []\n    for j in range(len(sizes) - 1):\n        act = activation if j < len(sizes) - 2 else output_activation\n        layer = nn.Linear(sizes[j], sizes[j + 1])\n        layers += [layer, act()]\n    return nn.Sequential(*layers)", "\n\nclass MLPGaussianPerturbationActor(nn.Module):\n    \"\"\"\n    A MLP actor that adds Gaussian noise to the output.\n\n    Args:\n        obs_dim (int): The dimension of the observation space.\n        act_dim (int): The dimension of the action space.\n        hidden_sizes (List[int]): The sizes of the hidden layers in the neural network.\n        activation (Type[nn.Module]): The activation function to use between layers.\n        phi (float): The standard deviation of the Gaussian noise to add to the output.\n        act_limit (float): The absolute value of the limits of the action space.\n    \"\"\"\n\n    def __init__(self,\n                 obs_dim,\n                 act_dim,\n                 hidden_sizes,\n                 activation,\n                 phi=0.05,\n                 act_limit=1):\n        super().__init__()\n        pi_sizes = [obs_dim + act_dim] + list(hidden_sizes) + [act_dim]\n        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n        self.act_limit = act_limit\n        self.phi = phi\n\n    def forward(self, obs, act):\n        # Return output from network scaled to action space limits.\n        a = self.phi * self.act_limit * self.pi(torch.cat([obs, act], 1))\n        return (a + act).clamp(-self.act_limit, self.act_limit)", "\n\nclass MLPActor(nn.Module):\n    \"\"\"\n    A MLP actor\n    \n    Args:\n        obs_dim (int): The dimension of the observation space.\n        act_dim (int): The dimension of the action space.\n        hidden_sizes (List[int]): The sizes of the hidden layers in the neural network.\n        activation (Type[nn.Module]): The activation function to use between layers.\n        act_limit (float, optional): The upper limit of the action space.\n    \"\"\"\n\n    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit=1):\n        super().__init__()\n        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n        self.act_limit = act_limit\n\n    def forward(self, obs):\n        # Return output from network scaled to action space limits.\n        return self.act_limit * self.pi(obs)", "\n\nclass MLPGaussianActor(nn.Module):\n    \"\"\"\n    A MLP Gaussian actor\n    \n    Args:\n        obs_dim (int): The dimension of the observation space.\n        act_dim (int): The dimension of the action space.\n        action_low (np.ndarray): A 1D numpy array of lower bounds for each action dimension.\n        action_high (np.ndarray): A 1D numpy array of upper bounds for each action dimension.\n        hidden_sizes (List[int]): The sizes of the hidden layers in the neural network.\n        activation (Type[nn.Module]): The activation function to use between layers.\n        device (str): The device to use for computation (cpu or cuda).\n    \"\"\"\n\n    def __init__(self,\n                 obs_dim,\n                 act_dim,\n                 action_low,\n                 action_high,\n                 hidden_sizes,\n                 activation,\n                 device=\"cpu\"):\n        super().__init__()\n        self.device = device\n        self.action_low = torch.nn.Parameter(torch.tensor(action_low,\n                                                          device=device)[None, ...],\n                                             requires_grad=False)  # (1, act_dim)\n        self.action_high = torch.nn.Parameter(torch.tensor(action_high,\n                                                           device=device)[None, ...],\n                                              requires_grad=False)  # (1, act_dim)\n        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n        self.mu_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n\n    def _distribution(self, obs):\n        mu = torch.sigmoid(self.mu_net(obs))\n        mu = self.action_low + (self.action_high - self.action_low) * mu\n        std = torch.exp(self.log_std)\n        return mu, Normal(mu, std)\n\n    def _log_prob_from_distribution(self, pi, act):\n        return pi.log_prob(act).sum(\n            axis=-1)  # Last axis sum needed for Torch Normal distribution\n\n    def forward(self, obs, act=None, deterministic=False):\n        '''\n        Produce action distributions for given observations, and\n        optionally compute the log likelihood of given actions under\n        those distributions.\n        If act is None, sample an action\n        '''\n        mu, pi = self._distribution(obs)\n        if act is None:\n            act = pi.sample()\n        if deterministic:\n            act = mu\n        logp_a = self._log_prob_from_distribution(pi, act)\n        return pi, act, logp_a", "\n\nLOG_STD_MAX = 2\nLOG_STD_MIN = -20\n\n\nclass SquashedGaussianMLPActor(nn.Module):\n    '''\n    A MLP Gaussian actor, can also be used as a deterministic actor\n    \n    Args:\n        obs_dim (int): The dimension of the observation space.\n        act_dim (int): The dimension of the action space.\n        hidden_sizes (List[int]): The sizes of the hidden layers in the neural network.\n        activation (Type[nn.Module]): The activation function to use between layers.\n    '''\n\n    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n        super().__init__()\n        self.net = mlp([obs_dim] + list(hidden_sizes), activation, activation)\n        self.mu_layer = nn.Linear(hidden_sizes[-1], act_dim)\n        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)\n\n    def forward(self,\n                obs,\n                deterministic=False,\n                with_logprob=True,\n                with_distribution=False,\n                return_pretanh_value=False):\n        net_out = self.net(obs)\n        mu = self.mu_layer(net_out)\n        log_std = self.log_std_layer(net_out)\n        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n        std = torch.exp(log_std)\n\n        # Pre-squash distribution and sample\n        pi_distribution = Normal(mu, std)\n        if deterministic:\n            # Only used for evaluating policy at test time.\n            pi_action = mu\n        else:\n            pi_action = pi_distribution.rsample()\n\n        if with_logprob:\n            # Compute logprob from Gaussian, and then apply correction for Tanh squashing.\n            logp_pi = pi_distribution.log_prob(pi_action).sum(axis=-1)\n            logp_pi -= (2 *\n                        (np.log(2) - pi_action - F.softplus(-2 * pi_action))).sum(axis=1)\n        else:\n            logp_pi = None\n\n        # for BEARL only\n        if return_pretanh_value:\n            return torch.tanh(pi_action), pi_action\n\n        pi_action = torch.tanh(pi_action)\n\n        if with_distribution:\n            return pi_action, logp_pi, pi_distribution\n        return pi_action, logp_pi", "\n\nclass EnsembleQCritic(nn.Module):\n    '''\n    An ensemble of Q network to address the overestimation issue.\n    \n    Args:\n        obs_dim (int): The dimension of the observation space.\n        act_dim (int): The dimension of the action space.\n        hidden_sizes (List[int]): The sizes of the hidden layers in the neural network.\n        activation (Type[nn.Module]): The activation function to use between layers.\n        num_q (float): The number of Q networks to include in the ensemble.\n    '''\n\n    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, num_q=2):\n        super().__init__()\n        assert num_q >= 1, \"num_q param should be greater than 1\"\n        self.q_nets = nn.ModuleList([\n            mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], nn.ReLU)\n            for i in range(num_q)\n        ])\n\n    def forward(self, obs, act=None):\n        # Squeeze is critical to ensure value has the right shape.\n        # Without squeeze, the training stability will be greatly affected!\n        # For instance, shape [3] - shape[3,1] = shape [3, 3] instead of shape [3]\n        data = obs if act is None else torch.cat([obs, act], dim=-1)\n        return [torch.squeeze(q(data), -1) for q in self.q_nets]\n\n    def predict(self, obs, act):\n        q_list = self.forward(obs, act)\n        qs = torch.vstack(q_list)  # [num_q, batch_size]\n        return torch.min(qs, dim=0).values, q_list\n\n    def loss(self, target, q_list=None):\n        losses = [((q - target)**2).mean() for q in q_list]\n        return sum(losses)", "\n\nclass EnsembleDoubleQCritic(nn.Module):\n    '''\n    An ensemble of double Q network to address the overestimation issue.\n    \n    Args:\n        obs_dim (int): The dimension of the observation space.\n        act_dim (int): The dimension of the action space.\n        hidden_sizes (List[int]): The sizes of the hidden layers in the neural network.\n        activation (Type[nn.Module]): The activation function to use between layers.\n        num_q (float): The number of Q networks to include in the ensemble.\n    '''\n\n    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, num_q=2):\n        super().__init__()\n        assert num_q >= 1, \"num_q param should be greater than 1\"\n        self.q1_nets = nn.ModuleList([\n            mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], nn.ReLU)\n            for i in range(num_q)\n        ])\n        self.q2_nets = nn.ModuleList([\n            mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], nn.ReLU)\n            for i in range(num_q)\n        ])\n\n    def forward(self, obs, act):\n        # Squeeze is critical to ensure value has the right shape.\n        # Without squeeze, the training stability will be greatly affected!\n        # For instance, shape [3] - shape[3,1] = shape [3, 3] instead of shape [3]\n        data = torch.cat([obs, act], dim=-1)\n        q1 = [torch.squeeze(q(data), -1) for q in self.q1_nets]\n        q2 = [torch.squeeze(q(data), -1) for q in self.q2_nets]\n        return q1, q2\n\n    def predict(self, obs, act):\n        q1_list, q2_list = self.forward(obs, act)\n        qs1, qs2 = torch.vstack(q1_list), torch.vstack(q2_list)\n        # qs = torch.vstack(q_list)  # [num_q, batch_size]\n        qs1_min, qs2_min = torch.min(qs1, dim=0).values, torch.min(qs2, dim=0).values\n        return qs1_min, qs2_min, q1_list, q2_list\n\n    def loss(self, target, q_list=None):\n        losses = [((q - target)**2).mean() for q in q_list]\n        return sum(losses)", "\n\nclass VAE(nn.Module):\n    \"\"\"\n    Variational Auto-Encoder\n    \n    Args:\n        obs_dim (int): The dimension of the observation space.\n        act_dim (int): The dimension of the action space.\n        hidden_size (int): The number of hidden units in the encoder and decoder networks.\n        latent_dim (int): The dimensionality of the latent space.\n        act_lim (float): The upper limit of the action space.\n        device (str): The device to use for computation (cpu or cuda).\n    \"\"\"\n\n    def __init__(self, obs_dim, act_dim, hidden_size, latent_dim, act_lim, device=\"cpu\"):\n        super(VAE, self).__init__()\n        self.e1 = nn.Linear(obs_dim + act_dim, hidden_size)\n        self.e2 = nn.Linear(hidden_size, hidden_size)\n\n        self.mean = nn.Linear(hidden_size, latent_dim)\n        self.log_std = nn.Linear(hidden_size, latent_dim)\n\n        self.d1 = nn.Linear(obs_dim + latent_dim, hidden_size)\n        self.d2 = nn.Linear(hidden_size, hidden_size)\n        self.d3 = nn.Linear(hidden_size, act_dim)\n\n        self.act_lim = act_lim\n        self.latent_dim = latent_dim\n        self.device = device\n\n    def forward(self, obs, act):\n        z = F.relu(self.e1(torch.cat([obs, act], 1)))\n        z = F.relu(self.e2(z))\n\n        mean = self.mean(z)\n        # Clamped for numerical stability\n        log_std = self.log_std(z).clamp(-4, 15)\n        std = torch.exp(log_std)\n        z = mean + std * torch.randn_like(std)\n\n        u = self.decode(obs, z)\n        return u, mean, std\n\n    def decode(self, obs, z=None):\n        if z is None:\n            z = torch.randn((obs.shape[0], self.latent_dim)).clamp(-0.5,\n                                                                   0.5).to(self.device)\n\n        a = F.relu(self.d1(torch.cat([obs, z], 1)))\n        a = F.relu(self.d2(a))\n        return self.act_lim * torch.tanh(self.d3(a))\n\n    # for BEARL only\n    def decode_multiple(self, obs, z=None, num_decode=10):\n        if z is None:\n            z = torch.randn(\n                (obs.shape[0], num_decode, self.latent_dim)).clamp(-0.5,\n                                                                   0.5).to(self.device)\n\n        a = F.relu(\n            self.d1(\n                torch.cat(\n                    [obs.unsqueeze(0).repeat(num_decode, 1, 1).permute(1, 0, 2), z], 2)))\n        a = F.relu(self.d2(a))\n        return torch.tanh(self.d3(a)), self.d3(a)", "\n\nclass LagrangianPIDController:\n    '''\n    Lagrangian multiplier controller\n    \n    Args:\n        KP (float): The proportional gain.\n        KI (float): The integral gain.\n        KD (float): The derivative gain.\n        thres (float): The setpoint for the controller.\n    '''\n\n    def __init__(self, KP, KI, KD, thres) -> None:\n        super().__init__()\n        self.KP = KP\n        self.KI = KI\n        self.KD = KD\n        self.thres = thres\n        self.error_old = 0\n        self.error_integral = 0\n\n    def control(self, qc):\n        '''\n        @param qc [batch,]\n        '''\n        error_new = torch.mean(qc - self.thres)  # [batch]\n        error_diff = F.relu(error_new - self.error_old)\n        self.error_integral = torch.mean(F.relu(self.error_integral + error_new))\n        self.error_old = error_new\n\n        multiplier = F.relu(self.KP * F.relu(error_new) + self.KI * self.error_integral +\n                            self.KD * error_diff)\n        return torch.mean(multiplier)", "\n\n# Decision Transformer implementation\nclass TransformerBlock(nn.Module):\n\n    def __init__(\n        self,\n        seq_len: int,\n        embedding_dim: int,\n        num_heads: int,\n        attention_dropout: float,\n        residual_dropout: float,\n    ):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embedding_dim)\n        self.norm2 = nn.LayerNorm(embedding_dim)\n        self.drop = nn.Dropout(residual_dropout)\n\n        self.attention = nn.MultiheadAttention(embedding_dim,\n                                               num_heads,\n                                               attention_dropout,\n                                               batch_first=True)\n        self.mlp = nn.Sequential(\n            nn.Linear(embedding_dim, 4 * embedding_dim),\n            nn.GELU(),\n            nn.Linear(4 * embedding_dim, embedding_dim),\n            nn.Dropout(residual_dropout),\n        )\n        # True value indicates that the corresponding position is not allowed to attend\n        self.register_buffer(\"causal_mask\",\n                             ~torch.tril(torch.ones(seq_len, seq_len)).to(bool))\n        self.seq_len = seq_len\n\n    # [batch_size, seq_len, emb_dim] -> [batch_size, seq_len, emb_dim]\n    def forward(self,\n                x: torch.Tensor,\n                padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        causal_mask = self.causal_mask[:x.shape[1], :x.shape[1]]\n\n        norm_x = self.norm1(x)\n        attention_out = self.attention(\n            query=norm_x,\n            key=norm_x,\n            value=norm_x,\n            attn_mask=causal_mask,\n            key_padding_mask=padding_mask,\n            need_weights=False,\n        )[0]\n        # by default pytorch attention does not use dropout\n        # after final attention weights projection, while minGPT does:\n        # https://github.com/karpathy/minGPT/blob/7218bcfa527c65f164de791099de715b81a95106/mingpt/model.py#L70 # noqa\n        x = x + self.drop(attention_out)\n        x = x + self.mlp(self.norm2(x))\n        return x", "\n\nclass TanhTransform(pyd.transforms.Transform):\n    domain = pyd.constraints.real\n    codomain = pyd.constraints.interval(-1.0, 1.0)\n    bijective = True\n    sign = +1\n\n    def __init__(self, cache_size=1):\n        super().__init__(cache_size=cache_size)\n\n    @staticmethod\n    def atanh(x):\n        return 0.5 * (x.log1p() - (-x).log1p())\n\n    def __eq__(self, other):\n        return isinstance(other, TanhTransform)\n\n    def _call(self, x):\n        return x.tanh()\n\n    def _inverse(self, y):\n        # We do not clamp to the boundary here as it may degrade the performance of certain algorithms.\n        # one should use `cache_size=1` instead\n        return self.atanh(y)\n\n    def log_abs_det_jacobian(self, x, y):\n        # We use a formula that is more numerically stable, see details in the following link\n        # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n        return 2.0 * (math.log(2.0) - x - F.softplus(-2.0 * x))", "\n\nclass SquashedNormal(pyd.transformed_distribution.TransformedDistribution):\n    \"\"\"\n    Squashed Normal Distribution(s)\n    If loc/std is of size (batch_size, sequence length, d),\n    this returns batch_size * sequence length * d\n    independent squashed univariate normal distributions.\n    \"\"\"\n\n    def __init__(self, loc, std):\n        self.loc = loc\n        self.std = std\n        self.base_dist = pyd.Normal(loc, std)\n\n        transforms = [TanhTransform()]\n        super().__init__(self.base_dist, transforms)\n\n    @property\n    def mean(self):\n        mu = self.loc\n        for tr in self.transforms:\n            mu = tr(mu)\n        return mu\n\n    def entropy(self, N=1):\n        # sample from the distribution and then compute the empirical entropy:\n        x = self.rsample((N, ))\n        log_p = self.log_prob(x)\n\n        return -log_p.mean(axis=0).sum(axis=2)\n\n    def log_likelihood(self, x):\n        # sum up along the action dimensions\n        return self.log_prob(x).sum(axis=2)", "\n\nclass DiagGaussianActor(nn.Module):\n    \"\"\"\n    torch.distributions implementation of an diagonal Gaussian policy.\n    \"\"\"\n\n    def __init__(self, hidden_dim, act_dim, log_std_bounds=[-5.0, 2.0]):\n        super().__init__()\n\n        self.mu = torch.nn.Linear(hidden_dim, act_dim)\n        self.log_std = torch.nn.Linear(hidden_dim, act_dim)\n        self.log_std_bounds = log_std_bounds\n\n        def weight_init(m):\n            \"\"\"Custom weight init for Conv2D and Linear layers.\"\"\"\n            if isinstance(m, torch.nn.Linear):\n                nn.init.orthogonal_(m.weight.data)\n                if hasattr(m.bias, \"data\"):\n                    m.bias.data.fill_(0.0)\n\n        self.apply(weight_init)\n\n    def forward(self, obs):\n        mu, log_std = self.mu(obs), self.log_std(obs)\n        std = log_std.exp()\n        return Normal(mu, std)", ""]}
{"filename": "osrl/common/exp_util.py", "chunked_list": ["import os\nimport os.path as osp\nimport random\nimport uuid\nfrom typing import Dict, Optional, Sequence\n\nimport numpy as np\nimport torch\nimport yaml\n", "import yaml\n\n\ndef seed_all(seed=1029, others: Optional[list] = None):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    # torch.use_deterministic_algorithms(True)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    if others is not None:\n        if hasattr(others, \"seed\"):\n            others.seed(seed)\n            return True\n        try:\n            for item in others:\n                if hasattr(item, \"seed\"):\n                    item.seed(seed)\n        except:\n            pass", "\n\ndef get_cfg_value(config, key):\n    if key in config:\n        value = config[key]\n        if isinstance(value, list):\n            suffix = \"\"\n            for i in value:\n                suffix += str(i)\n            return suffix\n        return str(value)\n    for k in config.keys():\n        if isinstance(config[k], dict):\n            res = get_cfg_value(config[k], key)\n            if res is not None:\n                return res\n    return \"None\"", "\n\ndef load_config_and_model(path: str, best: bool = False):\n    '''\n    Load the configuration and trained model from a specified directory.\n\n    :param path: the directory path where the configuration and trained model are stored.\n    :param best: whether to load the best-performing model or the most recent one. Defaults to False.\n\n    :return: a tuple containing the configuration dictionary and the trained model.\n    :raises ValueError: if the specified directory does not exist.\n    '''\n    if osp.exists(path):\n        config_file = osp.join(path, \"config.yaml\")\n        print(f\"load config from {config_file}\")\n        with open(config_file) as f:\n            config = yaml.load(f.read(), Loader=yaml.FullLoader)\n        model_file = \"model.pt\"\n        if best:\n            model_file = \"model_best.pt\"\n        model_path = osp.join(path, \"checkpoint/\" + model_file)\n        print(f\"load model from {model_path}\")\n        model = torch.load(model_path)\n        return config, model\n    else:\n        raise ValueError(f\"{path} doesn't exist!\")", "\n\ndef to_string(values):\n    '''\n    Recursively convert a sequence or dictionary of values to a string representation.\n    :param values: the sequence or dictionary of values to be converted to a string.\n    :return: a string representation of the input values.\n    '''\n    name = \"\"\n    if isinstance(values, Sequence) and not isinstance(values, str):\n        for i, v in enumerate(values):\n            prefix = \"\" if i == 0 else \"_\"\n            name += prefix + to_string(v)\n        return name\n    elif isinstance(values, Dict):\n        for i, k in enumerate(sorted(values.keys())):\n            prefix = \"\" if i == 0 else \"_\"\n            name += prefix + to_string(values[k])\n        return name\n    else:\n        return str(values)", "\n\nDEFAULT_SKIP_KEY = [\n    \"task\", \"reward_threshold\", \"logdir\", \"worker\", \"project\", \"group\", \"name\", \"prefix\",\n    \"suffix\", \"save_interval\", \"render\", \"verbose\", \"save_ckpt\", \"training_num\",\n    \"testing_num\", \"epoch\", \"device\", \"thread\"\n]\n\nDEFAULT_KEY_ABBRE = {\n    \"cost_limit\": \"cost\",", "DEFAULT_KEY_ABBRE = {\n    \"cost_limit\": \"cost\",\n    \"mstep_iter_num\": \"mnum\",\n    \"estep_iter_num\": \"enum\",\n    \"estep_kl\": \"ekl\",\n    \"mstep_kl_mu\": \"kl_mu\",\n    \"mstep_kl_std\": \"kl_std\",\n    \"mstep_dual_lr\": \"mlr\",\n    \"estep_dual_lr\": \"elr\",\n    \"update_per_step\": \"update\"", "    \"estep_dual_lr\": \"elr\",\n    \"update_per_step\": \"update\"\n}\n\n\ndef auto_name(default_cfg: dict,\n              current_cfg: dict,\n              prefix: str = \"\",\n              suffix: str = \"\",\n              skip_keys: list = DEFAULT_SKIP_KEY,\n              key_abbre: dict = DEFAULT_KEY_ABBRE) -> str:\n    '''\n    Automatic generate the experiment name by comparing the current config with the default one.\n\n    :param dict default_cfg: a dictionary containing the default configuration values.\n    :param dict current_cfg: a dictionary containing the current configuration values.\n    :param str prefix: (optional) a string to be added at the beginning of the generated name.\n    :param str suffix: (optional) a string to be added at the end of the generated name.\n    :param list skip_keys: (optional) a list of keys to be skipped when generating the name.\n    :param dict key_abbre: (optional) a dictionary containing abbreviations for keys in the generated name.\n\n    :return str: a string representing the generated experiment name.\n    '''\n    name = prefix\n    for i, k in enumerate(sorted(default_cfg.keys())):\n        if default_cfg[k] == current_cfg[k] or k in skip_keys:\n            continue\n        prefix = \"_\" if len(name) else \"\"\n        value = to_string(current_cfg[k])\n        # replace the name with abbreviation if key has abbreviation in key_abbre\n        if k in key_abbre:\n            k = key_abbre[k]\n        # Add the key-value pair to the name variable with the prefix\n        name += prefix + k + value\n    if len(suffix):\n        name = name + \"_\" + suffix if len(name) else suffix\n\n    name = \"default\" if not len(name) else name\n    name = f\"{name}-{str(uuid.uuid4())[:4]}\"\n    return name", ""]}
{"filename": "osrl/common/dataset.py", "chunked_list": ["import copy\nimport heapq\nimport random\nfrom collections import Counter, defaultdict\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\n\ntry:\n    import oapackage\nexcept ImportError:\n    print(\"OApackage is not installed, can not use CDT.\")", "try:\n    import oapackage\nexcept ImportError:\n    print(\"OApackage is not installed, can not use CDT.\")\nfrom scipy.optimize import minimize\nfrom torch.nn import functional as F  # noqa\nfrom torch.utils.data import IterableDataset\nfrom tqdm.auto import trange  # noqa\n\n\ndef discounted_cumsum(x: np.ndarray, gamma: float) -> np.ndarray:\n    \"\"\"\n    Calculate the discounted cumulative sum of x (can be rewards or costs).\n    \"\"\"\n    cumsum = np.zeros_like(x)\n    cumsum[-1] = x[-1]\n    for t in reversed(range(x.shape[0] - 1)):\n        cumsum[t] = x[t] + gamma * cumsum[t + 1]\n    return cumsum", "\n\ndef discounted_cumsum(x: np.ndarray, gamma: float) -> np.ndarray:\n    \"\"\"\n    Calculate the discounted cumulative sum of x (can be rewards or costs).\n    \"\"\"\n    cumsum = np.zeros_like(x)\n    cumsum[-1] = x[-1]\n    for t in reversed(range(x.shape[0] - 1)):\n        cumsum[t] = x[t] + gamma * cumsum[t + 1]\n    return cumsum", "\n\ndef process_bc_dataset(dataset: dict, cost_limit: float, gamma: float, bc_mode: str):\n    \"\"\"\n    Processes a givne dataset for behavior cloning and its variants.\n\n    Args:\n        dataset (dict): A dictionary containing the dataset to be processed.\n        cost_limit (float): The maximum cost allowed for the dataset.\n        gamma (float): The discount factor used to compute the returns.\n        bc_mode (str): The behavior cloning mode to use. Can be one of:\n            - \"all\": All trajectories are used for behavior cloning.\n            - \"multi-task\": All trajectories are used for behavior cloning, and the cost is appended as a feature.\n            - \"safe\": Only trajectories with cost below the cost limit are used for behavior cloning.\n            - \"risky\": Only trajectories with cost above twice the cost limit are used for behavior cloning.\n            - \"frontier\": Only trajectories near the Pareto frontier are used for behavior cloning.\n            - \"boundary\": Only trajectories near the cost limit are used for behavior cloning.\n        frontier_fn (function, optional): A function used to compute the frontier. \n                                          Required if bc_mode is \"frontier\".\n        frontier_range (float, optional): The range around the frontier to use for selecting trajectories. \n                                           Required if bc_mode is \"frontier\".\n    \n    Returns:\n        dict: A dictionary containing the processed dataset.\n\n    \"\"\"\n\n    # get the indices of the transitions after terminal states or timeouts\n    done_idx = np.where((dataset[\"terminals\"] == 1) | (dataset[\"timeouts\"] == 1))[0]\n\n    n_transitions = dataset[\"observations\"].shape[0]\n    dataset[\"cost_returns\"] = np.zeros_like(dataset[\"costs\"])\n    dataset[\"rew_returns\"] = np.zeros_like(dataset[\"rewards\"])\n    cost_ret, rew_ret = [], []\n    pareto_frontier, pf_mask = None, None\n\n    # compute episode returns\n    for i in range(done_idx.shape[0]):\n        start = 0 if i == 0 else done_idx[i - 1] + 1\n        end = done_idx[i] + 1\n        # compute the cost and reward returns for the segment\n        cost_returns = discounted_cumsum(dataset[\"costs\"][start:end], gamma=gamma)\n        reward_returns = discounted_cumsum(dataset[\"rewards\"][start:end], gamma=gamma)\n        dataset[\"cost_returns\"][start:end] = cost_returns[0]\n        dataset[\"rew_returns\"][start:end] = reward_returns[0]\n        cost_ret.append(cost_returns[0])\n        rew_ret.append(reward_returns[0])\n\n    # compute Pareto Frontier\n    if bc_mode == \"frontier\":\n        cost_ret = np.array(cost_ret, dtype=np.float64)\n        rew_ret = np.array(rew_ret, dtype=np.float64)\n        rmax, rmin = np.max(rew_ret), np.min(rew_ret)\n\n        pareto = oapackage.ParetoDoubleLong()\n        for i in range(rew_ret.shape[0]):\n            w = oapackage.doubleVector((-cost_ret[i], rew_ret[i]))\n            pareto.addvalue(w, i)\n        pareto.show(verbose=1)\n        pareto_idx = list(pareto.allindices())\n        cost_ret_pareto = cost_ret[pareto_idx]\n        rew_ret_pareto = rew_ret[pareto_idx]\n\n        for deg in [0, 1, 2]:\n            pareto_frontier = np.poly1d(\n                np.polyfit(cost_ret_pareto, rew_ret_pareto, deg=deg))\n            pf_rew_ret = pareto_frontier(cost_ret_pareto)\n            ss_total = np.sum((rew_ret_pareto - np.mean(rew_ret_pareto))**2)\n            ss_residual = np.sum((rew_ret_pareto - pf_rew_ret)**2)\n            r_squared = 1 - (ss_residual / ss_total)\n            if r_squared >= 0.9:\n                break\n\n        pf_rew_ret = pareto_frontier(dataset[\"cost_returns\"])\n        pf_mask = np.logical_and(\n            pf_rew_ret - (rmax - rmin) / 5 <= dataset[\"rew_returns\"],\n            dataset[\"rew_returns\"] <= pf_rew_ret + (rmax - rmin) / 5)\n\n    # select the transitions for behavior cloning based on the mode\n    selected_transition = np.zeros((n_transitions, ), dtype=int)\n    if bc_mode == \"all\" or bc_mode == \"multi-task\":\n        selected_transition = np.ones((n_transitions, ), dtype=int)\n    elif bc_mode == \"safe\":\n        # safe trajectories\n        selected_transition[dataset[\"cost_returns\"] <= cost_limit] = 1\n    elif bc_mode == \"risky\":\n        # high cost trajectories\n        selected_transition[dataset[\"cost_returns\"] >= 2 * cost_limit] = 1\n    elif bc_mode == \"boundary\":\n        # trajectories that are near the cost limit\n        mask = np.logical_and(0.5 * cost_limit < dataset[\"cost_returns\"],\n                              dataset[\"cost_returns\"] <= 1.5 * cost_limit)\n        selected_transition[mask] = 1\n    elif bc_mode == \"frontier\":\n        selected_transition[pf_mask] = 1\n    else:\n        raise NotImplementedError\n\n    for k, v in dataset.items():\n        dataset[k] = v[selected_transition == 1]\n    if bc_mode == \"multi-task\":\n        dataset[\"observations\"] = np.hstack(\n            (dataset[\"observations\"], dataset[\"cost_returns\"].reshape(-1, 1)))\n\n    print(\n        f\"original size = {n_transitions}, cost limit = {cost_limit}, filtered size = {np.sum(selected_transition == 1)}\"\n    )", "\n\ndef process_sequence_dataset(dataset: dict, cost_reverse: bool = False):\n    '''\n    Processe a given dataset into a list of trajectories, each containing information about \n    the observations, actions, rewards, costs, returns, and cost returns for a single episode.\n    \n    Args:\n\n        dataset (dict): A dictionary representing the dataset, \n                        with keys \"observations\", \"actions\", \"rewards\", \"costs\", \"terminals\", and \"timeouts\", \n                        each containing numpy arrays of corresponding data.\n        cost_reverse (bool): An optional boolean parameter that indicates whether the cost should be reversed.\n        \n    Returns:\n        traj (list): A list of dictionaries, each representing a trajectory.\n        info (dict): A dictionary containing additional information about the trajectories\n\n    '''\n    traj, traj_len = [], []\n    data_, episode_step = defaultdict(list), 0\n    for i in trange(dataset[\"rewards\"].shape[0], desc=\"Processing trajectories\"):\n        data_[\"observations\"].append(dataset[\"observations\"][i])\n        data_[\"actions\"].append(dataset[\"actions\"][i])\n        data_[\"rewards\"].append(dataset[\"rewards\"][i])\n        if cost_reverse:\n            data_[\"costs\"].append(1.0 - dataset[\"costs\"][i])\n        else:\n            data_[\"costs\"].append(dataset[\"costs\"][i])\n\n        if dataset[\"terminals\"][i] or dataset[\"timeouts\"][i]:\n            episode_data = {k: np.array(v, dtype=np.float32) for k, v in data_.items()}\n            # return-to-go if gamma=1.0, just discounted returns else\n            episode_data[\"returns\"] = discounted_cumsum(episode_data[\"rewards\"], gamma=1)\n            episode_data[\"cost_returns\"] = discounted_cumsum(episode_data[\"costs\"],\n                                                             gamma=1)\n            traj.append(episode_data)\n            traj_len.append(episode_step)\n            # reset trajectory buffer\n            data_, episode_step = defaultdict(list), 0\n        episode_step += 1\n\n    # needed for normalization, weighted sampling, other stats can be added also\n    info = {\n        \"obs_mean\": dataset[\"observations\"].mean(0, keepdims=True),\n        \"obs_std\": dataset[\"observations\"].std(0, keepdims=True) + 1e-6,\n        \"traj_lens\": np.array(traj_len),\n    }\n    return traj, info", "\n\ndef get_nearest_point(original_data: np.ndarray,\n                      sampled_data: np.ndarray,\n                      max_rew_decrease: float = 1,\n                      beta: float = 1):\n    \"\"\"\n    Given two arrays of data, finds the indices of the original data that are closest\n    to each sample in the sampled data, and returns a list of those indices.\n    \n    Args:\n        original_data: A 2D numpy array of the original data.\n        sampled_data: A 2D numpy array of the sampled data.\n        max_rew_decrease: A float representing the maximum reward decrease allowed.\n        beta: A float used in calculating the distance between points.\n    \n    Returns:\n        A list of integers representing the indices of the original data that are closest\n        to each sample in the sampled data.\n    \"\"\"\n\n    idxes = []\n    original_idx = np.arange(0, original_data.shape[0])\n    # for i in trange(sampled_data.shape[0], desc=\"Calculating nearest point\"):\n    for i in range(sampled_data.shape[0]):\n        p = sampled_data[i, :]\n        mask = original_data[:, 0] <= p[0]\n        # mask = np.logical_and(original_data[:, 0] <= p[0], original_data[:, 0] >= p[0] - 5)\n        delta = original_data[mask, :] - p\n        dist = np.hypot(delta[:, 0], delta[:, 1])\n        idx = np.argmin(dist)\n        idxes.append(original_idx[mask][idx])\n    counts = dict(Counter(idxes))\n\n    new_idxes = []\n    dist_fun = lambda x: 1 / (x + beta)\n    for idx, num in counts.items():\n        new_idxes.append(idx)\n        if num > 1:\n            p = original_data[idx, :]\n            mask = original_data[:, 0] <= p[0]\n\n            # the associated data should be: 1) smaller than the current cost 2) greater than certain reward\n            mask = np.logical_and(original_data[:, 0] <= p[0],\n                                  original_data[:, 1] >= p[1] - max_rew_decrease)\n            delta = original_data[mask, :] - p\n            dist = np.hypot(delta[:, 0], delta[:, 1])\n            dist = dist_fun(dist)\n            sample_idx = np.random.choice(dist.shape[0],\n                                          size=num - 1,\n                                          p=dist / np.sum(dist))\n            new_idxes.extend(original_idx[mask][sample_idx.tolist()])\n    return new_idxes", "\n\ndef grid_filter(x,\n                y,\n                xmin=-np.inf,\n                xmax=np.inf,\n                ymin=-np.inf,\n                ymax=np.inf,\n                xbins=10,\n                ybins=10,\n                max_num_per_bin=10,\n                min_num_per_bin=1):\n    xmin, xmax = max(min(x), xmin), min(max(x), xmax)\n    ymin, ymax = max(min(y), ymin), min(max(y), ymax)\n    xbin_step = (xmax - xmin) / xbins\n    ybin_step = (ymax - ymin) / ybins\n    # the key is x y bin index, the value is a list of indices\n    bin_hashmap = defaultdict(list)\n    for i in range(len(x)):\n        if x[i] < xmin or x[i] > xmax or y[i] < ymin or y[i] > ymax:\n            continue\n        x_bin_idx = (x[i] - xmin) // xbin_step\n        y_bin_idx = (y[i] - ymin) // ybin_step\n        bin_hashmap[(x_bin_idx, y_bin_idx)].append(i)\n    # start filtering\n    indices = []\n    for v in bin_hashmap.values():\n        if len(v) > max_num_per_bin:\n            # random sample max_num_per_bin indices\n            indices += random.sample(v, max_num_per_bin)\n        elif len(v) <= min_num_per_bin:\n            continue\n        else:\n            indices += v\n    return indices", "\n\ndef filter_trajectory(cost,\n                      rew,\n                      traj,\n                      cost_min=-np.inf,\n                      cost_max=np.inf,\n                      rew_min=-np.inf,\n                      rew_max=np.inf,\n                      cost_bins=60,\n                      rew_bins=50,\n                      max_num_per_bin=10,\n                      min_num_per_bin=1):\n    indices = grid_filter(cost,\n                          rew,\n                          cost_min,\n                          cost_max,\n                          rew_min,\n                          rew_max,\n                          xbins=cost_bins,\n                          ybins=rew_bins,\n                          max_num_per_bin=max_num_per_bin,\n                          min_num_per_bin=min_num_per_bin)\n    cost2, rew2, traj2 = [], [], []\n    for i in indices:\n        cost2.append(cost[i])\n        rew2.append(rew[i])\n        traj2.append(traj[i])\n    return cost2, rew2, traj2, indices", "\n\ndef augmentation(trajs: list,\n                 deg: int = 3,\n                 max_rew_decrease: float = 1,\n                 beta: float = 1,\n                 augment_percent: float = 0.3,\n                 max_reward: float = 1000.0,\n                 min_reward: float = 0.0):\n    \"\"\"\n    Applies data augmentation to a list of trajectories, \n    returning the augmented trajectories along with their indices \n    and the Pareto frontier of the original data.\n\n    Args:\n        trajs: A list of dictionaries representing the original trajectories.\n        deg: The degree of the polynomial used to fit the Pareto frontier.\n        max_rew_decrease: The maximum amount by which the reward of an augmented trajectory can decrease compared to the original.\n        beta: The scaling factor used to weigh the distance between cost and reward when finding nearest neighbors.\n        augment_percent: The percentage of original trajectories to use for augmentation.\n        max_reward: The maximum reward value for augmented trajectories.\n        min_reward: The minimum reward value for augmented trajectories.\n\n    Returns:\n        nearest_idx: A list of indices of the original trajectories that are nearest to each augmented trajectory.\n        aug_trajs: A list of dictionaries representing the augmented trajectories.\n        pareto_frontier: A polynomial function representing the Pareto frontier of the original data.\n    \"\"\"\n\n    rew_ret, cost_ret = [], []\n    for i, traj in enumerate(trajs):\n        r, c = traj[\"returns\"][0], traj[\"cost_returns\"][0]\n        rew_ret.append(r)\n        cost_ret.append(c)\n    rew_ret = np.array(rew_ret, dtype=np.float64)\n    cost_ret = np.array(cost_ret, dtype=np.float64)\n\n    # grid filer to filter outliers\n    cmin, cmax = np.min(cost_ret), np.max(cost_ret)\n    rmin, rmax = np.min(rew_ret), np.max(rew_ret)\n    cbins, rbins = 10, 50\n    max_npb, min_npb = 10, 2\n    cost_ret, rew_ret, trajs, indices = filter_trajectory(cost_ret,\n                                                          rew_ret,\n                                                          trajs,\n                                                          cost_min=cmin,\n                                                          cost_max=cmax,\n                                                          rew_min=rmin,\n                                                          rew_max=rmax,\n                                                          cost_bins=cbins,\n                                                          rew_bins=rbins,\n                                                          max_num_per_bin=max_npb,\n                                                          min_num_per_bin=min_npb)\n    print(f\"after filter {len(trajs)}\")\n    rew_ret = np.array(rew_ret, dtype=np.float64)\n    cost_ret = np.array(cost_ret, dtype=np.float64)\n\n    pareto = oapackage.ParetoDoubleLong()\n    for i in range(rew_ret.shape[0]):\n        w = oapackage.doubleVector((-cost_ret[i], rew_ret[i]))\n        pareto.addvalue(w, i)\n\n    # print pareto number\n    pareto.show(verbose=1)\n    pareto_idx = list(pareto.allindices())\n\n    cost_ret_pareto = cost_ret[pareto_idx]\n    rew_ret_pareto = rew_ret[pareto_idx]\n    pareto_frontier = np.poly1d(np.polyfit(cost_ret_pareto, rew_ret_pareto, deg=deg))\n\n    sample_num = int(augment_percent * cost_ret.shape[0])\n    # the augmented data should be within the cost return range of the dataset\n    cost_ret_range = np.linspace(np.min(cost_ret), np.max(cost_ret), sample_num)\n    pf_rew_ret = pareto_frontier(cost_ret_range)\n    max_reward = max_reward * np.ones(pf_rew_ret.shape)\n    min_reward = min_reward * np.ones(pf_rew_ret.shape)\n    # sample the rewards that are above the pf curve and within the max_reward\n    sampled_rew_ret = np.random.uniform(low=pf_rew_ret + min_reward,\n                                        high=max_reward,\n                                        size=sample_num)\n\n    # associate each sampled (cost, reward) pair with a trajectory index\n    original_data = np.hstack([cost_ret[:, None], rew_ret[:, None]])\n    sampled_data = np.hstack([cost_ret_range[:, None], sampled_rew_ret[:, None]])\n    nearest_idx = get_nearest_point(original_data, sampled_data, max_rew_decrease, beta)\n\n    # relabel the dataset\n    aug_trajs = []\n    for i, target in zip(nearest_idx, sampled_data):\n        target_cost_ret, target_rew_ret = target[0], target[1]\n        associated_traj = copy.deepcopy(trajs[i])\n        cost_ret, rew_ret = associated_traj[\"cost_returns\"], associated_traj[\"returns\"]\n        cost_ret += target_cost_ret - cost_ret[0]\n        rew_ret += target_rew_ret - rew_ret[0]\n        aug_trajs.append(associated_traj)\n    return nearest_idx, aug_trajs, pareto_frontier, indices", "\n\ndef compute_sample_prob(dataset, pareto_frontier, beta):\n    \"\"\"\n    Computes the probability of sampling each trajectory in a given dataset.\n\n    Args:\n        dataset (list): A list of dictionaries containing the trajectories \n                        to compute the sample probabilities for.\n        pareto_frontier (callable): A function that takes in a cost value and \n                                    returns the corresponding maximum reward value \n                                    on the Pareto frontier.\n        beta (float): A hyperparameter that controls the shape of the probability distribution.\n\n    Returns:\n        np.ndarray: A 1D numpy array of the same length as the dataset, \n                    containing the probability of sampling each trajectory.\n\n    \"\"\"\n\n    rew_ret, cost_ret = [], []\n    for i, traj in enumerate(dataset):\n        r, c = traj[\"returns\"][0], traj[\"cost_returns\"][0]\n        rew_ret.append(r)\n        cost_ret.append(c)\n    rew_ret = np.array(rew_ret, dtype=np.float64)  # type should be float64\n    cost_ret = np.array(cost_ret, dtype=np.float64)\n\n    prob_fun = lambda x: 1 / (x + beta)\n    sample_prob = []\n    for i in trange(cost_ret.shape[0], desc=\"Calculating sample prob\"):\n        r, c = rew_ret[i], cost_ret[i]\n        dist_fun = lambda x: (x - c)**2 + (pareto_frontier(x) - r)**2\n        sol = minimize(dist_fun, x0=c, method=\"bfgs\", tol=1e-4)\n        x = np.max([0, (sol.x)[0]])\n        dist = np.sqrt(dist_fun(x))\n        prob = prob_fun(dist)\n        sample_prob.append(prob)\n    sample_prob /= np.sum(sample_prob)\n    return sample_prob", "\n\ndef compute_cost_sample_prob(dataset, cost_transform=lambda x: 50 - x):\n    \"\"\"\n    Computes the sample probabilities for a given dataset based on its costs.\n\n    Args:\n        dataset (list): A list of trajectories, where each trajectory is a dictionary containing\n                        a \"cost_returns\" key with a list of cost values.\n        cost_transform (function): A function that transforms cost values.\n\n    Returns:\n        np.ndarray: A 1D numpy array of sample probabilities, normalized to sum to 1.\n    \"\"\"\n\n    sample_prob = []\n    for i, traj in enumerate(dataset):\n        c = cost_transform(traj[\"cost_returns\"][0])\n        sample_prob.append(c)\n    sample_prob = np.array(sample_prob)\n    sample_prob[sample_prob < 0] = 0\n    sample_prob /= np.sum(sample_prob)\n    return sample_prob", "\n\ndef gauss_kernel(size, std=1.0):\n    \"\"\"\n    Computes a 1D Gaussian kernel with the given size and standard deviation.\n    \"\"\"\n    size = int(size)\n    x = np.linspace(-size, size, 2 * size + 1)\n    g = np.exp(-(x**2 / std))\n    return g", "\n\ndef compute_start_index_sample_prob(dataset, prob=0.4):\n    \"\"\"\n    computes every trajectories start index sampling probability\n    \"\"\"\n\n    sample_prob_list = []\n    for i, traj in enumerate(dataset):\n        n = np.sum(traj[\"costs\"])\n        l = len(traj[\"costs\"])\n        if prob * l - n <= 0:\n            x = 100\n        else:\n            x = n * (1 - prob) / (prob * l - n)\n        # in case n=0\n        if x <= 0:\n            x = 1\n        costs = np.array(traj[\"costs\"])\n        kernel = gauss_kernel(10, 10)\n        costs = np.convolve(costs, kernel)\n        costs = costs[10:-10] + x\n        sample_prob = costs / costs.sum()\n        sample_prob_list.append(sample_prob)\n    return sample_prob_list", "\n\n# some utils functionalities specific for Decision Transformer\ndef pad_along_axis(arr: np.ndarray,\n                   pad_to: int,\n                   axis: int = 0,\n                   fill_value: float = 0.0) -> np.ndarray:\n    pad_size = pad_to - arr.shape[axis]\n    if pad_size <= 0:\n        return arr\n\n    npad = [(0, 0)] * arr.ndim\n    npad[axis] = (0, pad_size)\n    return np.pad(arr, pad_width=npad, mode=\"constant\", constant_values=fill_value)", "\n\ndef select_optimal_trajectory(trajs, rmin=0, cost_bins=60, max_num_per_bin=1):\n    \"\"\"\n    Selects the optimal trajectories from a list of trajectories based on their returns and costs.\n\n    Args:\n        trajs (list): A list of dictionaries, where each dictionary represents a trajectory and contains\n                      the keys \"returns\" and \"cost_returns\".\n        rmin (float): The minimum return that a trajectory must have in order to be considered optimal.\n        cost_bins (int): The number of bins to divide the cost range into.\n        max_num_per_bin (int): The maximum number of trajectories to select from each cost bin.\n\n    Returns:\n        list: A list of dictionaries representing the optimal trajectories.\n    \"\"\"\n\n    rew, cost = [], []\n    for i, traj in enumerate(trajs):\n        r, c = traj[\"returns\"][0], traj[\"cost_returns\"][0]\n        rew.append(r)\n        cost.append(c)\n\n    xmin, xmax = min(cost), max(cost)\n    xbin_step = (xmax - xmin) / cost_bins\n    # the key is x y bin index, the value is a list of indices\n    bin_hashmap = defaultdict(list)\n    for i in range(len(cost)):\n        if rew[i] < rmin:\n            continue\n        x_bin_idx = (cost[i] - xmin) // xbin_step\n        bin_hashmap[x_bin_idx].append(i)\n\n    # start filtering\n    def sort_index(idx):\n        return rew[idx]\n\n    indices = []\n    for v in bin_hashmap.values():\n        idx = heapq.nlargest(max_num_per_bin, v, key=sort_index)\n        indices += idx\n\n    traj2 = []\n    for i in indices:\n        traj2.append(trajs[i])\n    return traj2", "\n\ndef random_augmentation(trajs: list,\n                        augment_percent: float = 0.3,\n                        aug_rmin: float = 0,\n                        aug_rmax: float = 600,\n                        aug_cmin: float = 5,\n                        aug_cmax: float = 50,\n                        cgap: float = 5,\n                        rstd: float = 1,\n                        cstd: float = 0.25):\n    \"\"\"\n    Augments a list of trajectories with random noise.\n    \n    Args:\n        trajs (list): A list of dictionaries, where each dictionary represents a trajectory\n            and contains \"returns\" and \"cost_returns\" keys that hold the returns and cost returns\n            for each time step of the trajectory.\n        augment_percent (float, optional): The percentage of trajectories to augment.\n        aug_rmin (float, optional): The minimum value for the augmented returns.\n        aug_rmax (float, optional): The maximum value for the augmented returns.\n        aug_cmin (float, optional): The minimum value for the augmented cost returns.\n        aug_cmax (float, optional): The maximum value for the augmented cost returns.\n        cgap (float, optional): The minimum distance between the augmented cost returns\n        rstd (float, optional): The standard deviation of the noise to add to the returns.\n        cstd (float, optional): The standard deviation of the noise to add to the cost returns.\n\n    Returns:\n        Tuple[List[int], List[Dict]]: A tuple containing two lists. The first list contains\n            the indices of the original trajectories that were augmented. The second list contains\n            the augmented trajectories, represented as dictionaries with \"returns\" and \"cost_returns\"\n            keys.\n    \"\"\"\n\n    rew_ret, cost_ret = [], []\n    for i, traj in enumerate(trajs):\n        r, c = traj[\"returns\"][0], traj[\"cost_returns\"][0]\n        rew_ret.append(r)\n        cost_ret.append(c)\n\n    # [traj_num]\n    rew_ret = np.array(rew_ret, dtype=np.float64)  # type should be float64\n    cost_ret = np.array(cost_ret, dtype=np.float64)\n    cmin = np.min(cost_ret)\n\n    num = int(augment_percent * cost_ret.shape[0])\n    sampled_cr = np.random.uniform(low=(aug_cmin, aug_rmin),\n                                   high=(aug_cmax, aug_rmax),\n                                   size=(num, 2))\n\n    idxes = []\n    original_data = np.hstack([cost_ret[:, None], rew_ret[:, None]])\n    original_idx = np.arange(0, original_data.shape[0])\n    # for i in trange(sampled_data.shape[0], desc=\"Calculating nearest point\"):\n    for i in range(sampled_cr.shape[0]):\n        p = sampled_cr[i, :]\n        boundary = max(p[0] - cgap, cmin + 1)\n        mask = original_data[:, 0] <= boundary\n        # mask = np.logical_and(original_data[:, 0] <= p[0], original_data[:, 0] >= p[0] - 5)\n        delta = original_data[mask, :] - p\n        dist = np.hypot(delta[:, 0], delta[:, 1])\n        idx = np.argmin(dist)\n        idxes.append(original_idx[mask][idx])\n\n    # relabel the dataset\n    aug_trajs = []\n    for i, target in zip(idxes, sampled_cr):\n        target_cost_ret, target_rew_ret = target[0], target[1]\n        associated_traj = copy.deepcopy(trajs[i])\n        cost_ret, rew_ret = associated_traj[\"cost_returns\"], associated_traj[\"returns\"]\n        cost_ret += target_cost_ret - cost_ret[0] + np.random.normal(\n            loc=0, scale=cstd, size=cost_ret.shape)\n        rew_ret += target_rew_ret - rew_ret[0] + np.random.normal(\n            loc=0, scale=rstd, size=rew_ret.shape)\n        aug_trajs.append(associated_traj)\n    return idxes, aug_trajs", "\n\nclass SequenceDataset(IterableDataset):\n    \"\"\"\n    A dataset of sequential data.\n\n    Args:\n        dataset (dict): Input dataset, containing trajectory IDs and sequences of observations.\n        seq_len (int): Length of sequence to use for training.\n        reward_scale (float): Scaling factor for reward values.\n        cost_scale (float): Scaling factor for cost values.\n        deg (int): Degree of polynomial used for Pareto frontier augmentation.\n        pf_sample (bool): Whether to sample data from the Pareto frontier.\n        max_rew_decrease (float): Maximum reward decrease for Pareto frontier augmentation.\n        beta (float): Parameter used for cost-based augmentation.\n        augment_percent (float): Percentage of data to augment.\n        max_reward (float): Maximum reward value for augmentation.\n        min_reward (float): Minimum reward value for augmentation.\n        cost_reverse (bool): Whether to reverse the cost values.\n        pf_only (bool): Whether to use only Pareto frontier data points.\n        rmin (float): Minimum reward value for random augmentation.\n        cost_bins (int): Number of cost bins for random augmentation.\n        npb (int): Number of data points to select from each cost bin for random augmentation.\n        cost_sample (bool): Whether to sample data based on cost.\n        cost_transform (callable): Function used to transform cost values.\n        prob (float): Probability of sampling from each trajectory start index.\n        start_sampling (bool): Whether to sample from each trajectory start index.\n        random_aug (float): Percentage of data to augment randomly.\n        aug_rmin (float): Minimum reward value for random augmentation.\n        aug_rmax (float): Maximum reward value for random augmentation.\n        aug_cmin (float): Minimum cost value for random augmentation.\n        aug_cmax (float): Maximum cost value for random augmentation.\n        cgap (float): Cost gap for random augmentation.\n        rstd (float): Standard deviation of reward values for random augmentation.\n        cstd (float): Standard deviation of cost values for random augmentation.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: dict,\n        seq_len: int = 10,\n        reward_scale: float = 1.0,\n        cost_scale: float = 1.0,\n        deg: int = 3,\n        pf_sample: bool = False,\n        max_rew_decrease: float = 1.0,\n        beta: float = 1.0,\n        augment_percent: float = 0,\n        max_reward: float = 1000.0,\n        min_reward: float = 5,\n        cost_reverse: bool = False,\n        pf_only: bool = False,\n        rmin: float = 0,\n        cost_bins: int = 60,\n        npb: int = 5,\n        cost_sample: bool = False,\n        cost_transform=lambda x: 50 - x,\n        prob: float = 0.4,\n        start_sampling: bool = False,\n        random_aug: float = 0,\n        aug_rmin: float = 0,\n        aug_rmax: float = 600,\n        aug_cmin: float = 5,\n        aug_cmax: float = 50,\n        cgap: float = 5,\n        rstd: float = 1,\n        cstd: float = 0.2,\n    ):\n        self.original_data, info = process_sequence_dataset(dataset, cost_reverse)\n        self.reward_scale = reward_scale\n        self.cost_scale = cost_scale\n        self.seq_len = seq_len\n        self.start_sampling = start_sampling\n\n        self.aug_data = []\n        if pf_only:\n            print(\"*\" * 100)\n            print(\"Using pareto frontier data points only!!!!!\")\n            print(\"*\" * 100)\n            self.dataset = select_optimal_trajectory(self.original_data, rmin, cost_bins,\n                                                     npb)\n        elif random_aug > 0:\n            self.idx, self.aug_data = random_augmentation(\n                self.original_data,\n                random_aug,\n                aug_rmin,\n                aug_rmax,\n                aug_cmin,\n                aug_cmax,\n                cgap,\n                rstd,\n                cstd,\n            )\n        elif augment_percent > 0:\n            # sampled data and the index of its \"nearest\" point in the dataset\n            self.idx, self.aug_data, self.pareto_frontier, self.indices = augmentation(\n                self.original_data, deg, max_rew_decrease, beta, augment_percent,\n                max_reward, min_reward)\n        self.dataset = self.original_data + self.aug_data\n        print(\n            f\"original data: {len(self.original_data)}, augment data: {len(self.aug_data)}, total: {len(self.dataset)}\"\n        )\n\n        if cost_sample:\n            self.sample_prob = compute_cost_sample_prob(self.dataset, cost_transform)\n        elif pf_sample:\n            self.sample_prob = compute_sample_prob(self.dataset, self.pareto_frontier, 1)\n        else:\n            self.sample_prob = None\n\n        # compute every trajectories start index sampling prob:\n        if start_sampling:\n            self.start_idx_sample_prob = compute_start_index_sample_prob(\n                dataset=self.dataset, prob=prob)\n\n    def compute_pareto_return(self, cost):\n        return self.pareto_frontier(cost)\n\n    def __prepare_sample(self, traj_idx, start_idx):\n        traj = self.dataset[traj_idx]\n        states = traj[\"observations\"][start_idx:start_idx + self.seq_len]\n        actions = traj[\"actions\"][start_idx:start_idx + self.seq_len]\n        returns = traj[\"returns\"][start_idx:start_idx + self.seq_len]\n        cost_returns = traj[\"cost_returns\"][start_idx:start_idx + self.seq_len]\n        time_steps = np.arange(start_idx, start_idx + self.seq_len)\n\n        costs = traj[\"costs\"][start_idx:start_idx + self.seq_len]\n\n        episode_cost = traj[\"cost_returns\"][0] * self.cost_scale\n\n        # states = (states - self.state_mean) / self.state_std\n        returns = returns * self.reward_scale\n        cost_returns = cost_returns * self.cost_scale\n        # pad up to seq_len if needed\n        mask = np.hstack(\n            [np.ones(states.shape[0]),\n             np.zeros(self.seq_len - states.shape[0])])\n        if states.shape[0] < self.seq_len:\n            states = pad_along_axis(states, pad_to=self.seq_len)\n            actions = pad_along_axis(actions, pad_to=self.seq_len)\n            returns = pad_along_axis(returns, pad_to=self.seq_len)\n            cost_returns = pad_along_axis(cost_returns, pad_to=self.seq_len)\n            costs = pad_along_axis(costs, pad_to=self.seq_len)\n\n        return states, actions, returns, cost_returns, time_steps, mask, episode_cost, costs\n\n    def __iter__(self):\n        while True:\n            traj_idx = np.random.choice(len(self.dataset), p=self.sample_prob)\n            # compute start index sampling prob\n            if self.start_sampling:\n                start_idx = np.random.choice(self.dataset[traj_idx][\"rewards\"].shape[0],\n                                             p=self.start_idx_sample_prob[traj_idx])\n            else:\n                start_idx = random.randint(\n                    0, self.dataset[traj_idx][\"rewards\"].shape[0] - 1)\n            yield self.__prepare_sample(traj_idx, start_idx)", "\n\nclass TransitionDataset(IterableDataset):\n    \"\"\"\n    A dataset of transitions (state, action, reward, next state) used for training RL agents.\n    \n    Args:\n        dataset (dict): A dictionary of NumPy arrays containing the observations, actions, rewards, etc.\n        reward_scale (float): The scale factor for the rewards.\n        cost_scale (float): The scale factor for the costs.\n        state_init (bool): If True, the dataset will include an \"is_init\" flag indicating if a transition\n            corresponds to the initial state of an episode.\n\n    \"\"\"\n\n    def __init__(self,\n                 dataset: dict,\n                 reward_scale: float = 1.0,\n                 cost_scale: float = 1.0,\n                 state_init: bool = False):\n        self.dataset = dataset\n        self.reward_scale = reward_scale\n        self.cost_scale = cost_scale\n        self.sample_prob = None\n        self.state_init = state_init\n        self.dataset_size = self.dataset[\"observations\"].shape[0]\n\n        self.dataset[\"done\"] = np.logical_or(self.dataset[\"terminals\"],\n                                             self.dataset[\"timeouts\"]).astype(np.float32)\n        if self.state_init:\n            self.dataset[\"is_init\"] = self.dataset[\"done\"].copy()\n            self.dataset[\"is_init\"][1:] = self.dataset[\"is_init\"][:-1]\n            self.dataset[\"is_init\"][0] = 1.0\n\n    def get_dataset_states(self):\n        \"\"\"\n        Returns the proportion of initial states in the dataset, \n        as well as the standard deviations of the observation and action spaces.\n        \"\"\"\n        init_state_propotion = self.dataset[\"is_init\"].mean()\n        obs_std = self.dataset[\"observations\"].std(0, keepdims=True)\n        act_std = self.dataset[\"actions\"].std(0, keepdims=True)\n        return init_state_propotion, obs_std, act_std\n\n    def __prepare_sample(self, idx):\n        observations = self.dataset[\"observations\"][idx, :]\n        next_observations = self.dataset[\"next_observations\"][idx, :]\n        actions = self.dataset[\"actions\"][idx, :]\n        rewards = self.dataset[\"rewards\"][idx] * self.reward_scale\n        costs = self.dataset[\"costs\"][idx] * self.cost_scale\n        done = self.dataset[\"done\"][idx]\n        if self.state_init:\n            is_init = self.dataset[\"is_init\"][idx]\n            return observations, next_observations, actions, rewards, costs, done, is_init\n        return observations, next_observations, actions, rewards, costs, done\n\n    def __iter__(self):\n        while True:\n            idx = np.random.choice(self.dataset_size, p=self.sample_prob)\n            yield self.__prepare_sample(idx)", ""]}
{"filename": "osrl/common/__init__.py", "chunked_list": ["from osrl.common.dataset import SequenceDataset, TransitionDataset\nfrom osrl.common.exp_util import *\nfrom osrl.common.net import *\n"]}
{"filename": "osrl/algorithms/bcql.py", "chunked_list": ["# reference: https://github.com/sfujim/BCQ\nfrom copy import deepcopy\n\nimport gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom fsrl.utils import DummyLogger, WandbLogger\nfrom tqdm.auto import trange  # noqa\n", "from tqdm.auto import trange  # noqa\n\nfrom osrl.common.net import (VAE, EnsembleDoubleQCritic, LagrangianPIDController,\n                             MLPGaussianPerturbationActor)\n\n\nclass BCQL(nn.Module):\n    \"\"\"\n        Batch-Constrained deep Q-learning with PID Lagrangian (BCQL)\n\n    Args:\n        state_dim (int): dimension of the state space.\n        action_dim (int): dimension of the action space.\n        max_action (float): Maximum action value.\n        a_hidden_sizes (list): List of integers specifying the sizes \n            of the layers in the actor network.\n        c_hidden_sizes (list): List of integers specifying the sizes \n            of the layers in the critic network.\n        vae_hidden_sizes (int): Number of hidden units in the VAE. \n        sample_action_num (int): Number of action samples to draw. \n        gamma (float): Discount factor for the reward.\n        tau (float): Soft update coefficient for the target networks. \n        phi (float): Scale parameter for the Gaussian perturbation \n            applied to the actor's output.\n        lmbda (float): Weight of the Lagrangian term.\n        beta (float): Weight of the KL divergence term.\n        PID (list): List of three floats containing the coefficients \n            of the PID controller.\n        num_q (int): Number of Q networks in the ensemble.\n        num_qc (int): Number of cost Q networks in the ensemble.\n        cost_limit (int): Upper limit on the cost per episode.\n        episode_len (int): Maximum length of an episode.\n        device (str): Device to run the model on (e.g. 'cpu' or 'cuda:0'). \n    \"\"\"\n\n    def __init__(self,\n                 state_dim: int,\n                 action_dim: int,\n                 max_action: float,\n                 a_hidden_sizes: list = [128, 128],\n                 c_hidden_sizes: list = [128, 128],\n                 vae_hidden_sizes: int = 64,\n                 sample_action_num: int = 10,\n                 gamma: float = 0.99,\n                 tau: float = 0.005,\n                 phi: float = 0.05,\n                 lmbda: float = 0.75,\n                 beta: float = 0.5,\n                 PID: list = [0.1, 0.003, 0.001],\n                 num_q: int = 1,\n                 num_qc: int = 1,\n                 cost_limit: int = 10,\n                 episode_len: int = 300,\n                 device: str = \"cpu\"):\n\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.max_action = max_action\n        self.latent_dim = self.action_dim * 2\n        self.a_hidden_sizes = a_hidden_sizes\n        self.c_hidden_sizes = c_hidden_sizes\n        self.vae_hidden_sizes = vae_hidden_sizes\n        self.sample_action_num = sample_action_num\n        self.gamma = gamma\n        self.tau = tau\n        self.phi = phi\n        self.lmbda = lmbda\n        self.beta = beta\n        self.KP, self.KI, self.KD = PID\n        self.num_q = num_q\n        self.num_qc = num_qc\n        self.cost_limit = cost_limit\n        self.episode_len = episode_len\n        self.device = device\n\n        ################ create actor critic model ###############\n        self.actor = MLPGaussianPerturbationActor(self.state_dim, self.action_dim,\n                                                  self.a_hidden_sizes, nn.Tanh, self.phi,\n                                                  self.max_action).to(self.device)\n        self.critic = EnsembleDoubleQCritic(self.state_dim,\n                                            self.action_dim,\n                                            self.c_hidden_sizes,\n                                            nn.ReLU,\n                                            num_q=self.num_q).to(self.device)\n        self.cost_critic = EnsembleDoubleQCritic(self.state_dim,\n                                                 self.action_dim,\n                                                 self.c_hidden_sizes,\n                                                 nn.ReLU,\n                                                 num_q=self.num_qc).to(self.device)\n        self.vae = VAE(self.state_dim, self.action_dim, self.vae_hidden_sizes,\n                       self.latent_dim, self.max_action, self.device).to(self.device)\n\n        self.actor_old = deepcopy(self.actor)\n        self.actor_old.eval()\n        self.critic_old = deepcopy(self.critic)\n        self.critic_old.eval()\n        self.cost_critic_old = deepcopy(self.cost_critic)\n        self.cost_critic_old.eval()\n\n        self.qc_thres = cost_limit * (1 - self.gamma**self.episode_len) / (\n            1 - self.gamma) / self.episode_len\n        self.controller = LagrangianPIDController(self.KP, self.KI, self.KD,\n                                                  self.qc_thres)\n\n    def _soft_update(self, tgt: nn.Module, src: nn.Module, tau: float) -> None:\n        \"\"\"\n        Softly update the parameters of target module \n        towards the parameters of source module.\n        \"\"\"\n        for tgt_param, src_param in zip(tgt.parameters(), src.parameters()):\n            tgt_param.data.copy_(tau * src_param.data + (1 - tau) * tgt_param.data)\n\n    def vae_loss(self, observations, actions):\n        recon, mean, std = self.vae(observations, actions)\n        recon_loss = nn.functional.mse_loss(recon, actions)\n        KL_loss = -0.5 * (1 + torch.log(std.pow(2)) - mean.pow(2) - std.pow(2)).mean()\n        loss_vae = recon_loss + self.beta * KL_loss\n\n        self.vae_optim.zero_grad()\n        loss_vae.backward()\n        self.vae_optim.step()\n        stats_vae = {\"loss/loss_vae\": loss_vae.item()}\n        return loss_vae, stats_vae\n\n    def critic_loss(self, observations, next_observations, actions, rewards, done):\n        _, _, q1_list, q2_list = self.critic.predict(observations, actions)\n        with torch.no_grad():\n            batch_size = next_observations.shape[0]\n            obs_next = torch.repeat_interleave(next_observations, self.sample_action_num,\n                                               0).to(self.device)\n\n            act_targ_next = self.actor_old(obs_next, self.vae.decode(obs_next))\n            q1_targ, q2_targ, _, _ = self.critic_old.predict(obs_next, act_targ_next)\n\n            q_targ = self.lmbda * torch.min(\n                q1_targ, q2_targ) + (1. - self.lmbda) * torch.max(q1_targ, q2_targ)\n            q_targ = q_targ.reshape(batch_size, -1).max(1)[0]\n\n            backup = rewards + self.gamma * (1 - done) * q_targ\n        loss_critic = self.critic.loss(backup, q1_list) + self.critic.loss(\n            backup, q2_list)\n        self.critic_optim.zero_grad()\n        loss_critic.backward()\n        self.critic_optim.step()\n        stats_critic = {\"loss/critic_loss\": loss_critic.item()}\n        return loss_critic, stats_critic\n\n    def cost_critic_loss(self, observations, next_observations, actions, costs, done):\n        _, _, q1_list, q2_list = self.cost_critic.predict(observations, actions)\n        with torch.no_grad():\n            batch_size = next_observations.shape[0]\n            obs_next = torch.repeat_interleave(next_observations, self.sample_action_num,\n                                               0).to(self.device)\n\n            act_targ_next = self.actor_old(obs_next, self.vae.decode(obs_next))\n            q1_targ, q2_targ, _, _ = self.cost_critic_old.predict(\n                obs_next, act_targ_next)\n\n            q_targ = self.lmbda * torch.min(\n                q1_targ, q2_targ) + (1. - self.lmbda) * torch.max(q1_targ, q2_targ)\n            q_targ = q_targ.reshape(batch_size, -1).max(1)[0]\n\n            backup = costs + self.gamma * q_targ\n        loss_cost_critic = self.cost_critic.loss(\n            backup, q1_list) + self.cost_critic.loss(backup, q2_list)\n        self.cost_critic_optim.zero_grad()\n        loss_cost_critic.backward()\n        self.cost_critic_optim.step()\n        stats_cost_critic = {\"loss/cost_critic_loss\": loss_cost_critic.item()}\n        return loss_cost_critic, stats_cost_critic\n\n    def actor_loss(self, observations):\n        for p in self.critic.parameters():\n            p.requires_grad = False\n        for p in self.cost_critic.parameters():\n            p.requires_grad = False\n        for p in self.vae.parameters():\n            p.requires_grad = False\n\n        actions = self.actor(observations, self.vae.decode(observations))\n        q1_pi, q2_pi, _, _ = self.critic.predict(observations, actions)  # [batch_size]\n        qc1_pi, qc2_pi, _, _ = self.cost_critic.predict(observations, actions)\n        qc_pi = torch.min(qc1_pi, qc2_pi)\n        q_pi = torch.min(q1_pi, q2_pi)\n\n        with torch.no_grad():\n            multiplier = self.controller.control(qc_pi).detach()\n        qc_penalty = ((qc_pi - self.qc_thres) * multiplier).mean()\n        loss_actor = -q_pi.mean() + qc_penalty\n\n        self.actor_optim.zero_grad()\n        loss_actor.backward()\n        self.actor_optim.step()\n\n        stats_actor = {\n            \"loss/actor_loss\": loss_actor.item(),\n            \"loss/qc_penalty\": qc_penalty.item(),\n            \"loss/lagrangian\": multiplier.item()\n        }\n\n        for p in self.critic.parameters():\n            p.requires_grad = True\n        for p in self.cost_critic.parameters():\n            p.requires_grad = True\n        for p in self.vae.parameters():\n            p.requires_grad = True\n        return loss_actor, stats_actor\n\n    def setup_optimizers(self, actor_lr, critic_lr, vae_lr):\n        \"\"\"\n        Sets up optimizers for the actor, critic, cost critic, and VAE models.\n        \"\"\"\n        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n        self.cost_critic_optim = torch.optim.Adam(self.cost_critic.parameters(),\n                                                  lr=critic_lr)\n        self.vae_optim = torch.optim.Adam(self.vae.parameters(), lr=vae_lr)\n\n    def sync_weight(self):\n        \"\"\"\n        Soft-update the weight for the target network.\n        \"\"\"\n        self._soft_update(self.critic_old, self.critic, self.tau)\n        self._soft_update(self.cost_critic_old, self.cost_critic, self.tau)\n        self._soft_update(self.actor_old, self.actor, self.tau)\n\n    def act(self, obs, deterministic=False, with_logprob=False):\n        '''\n        Given a single obs, return the action, value, logp.\n        '''\n        obs = torch.tensor(obs[None, ...], dtype=torch.float32).to(self.device)\n        act = self.actor(obs, self.vae.decode(obs))\n        act = act.data.numpy() if self.device == \"cpu\" else act.data.cpu().numpy()\n        return np.squeeze(act, axis=0), None", "\n\nclass BCQLTrainer:\n    \"\"\"\n    Constraints Penalized Q-learning Trainer\n    \n    Args:\n        model (BCQL): The BCQL model to be trained.\n        env (gym.Env): The OpenAI Gym environment to train the model in.\n        logger (WandbLogger or DummyLogger): The logger to use for tracking training progress.\n        actor_lr (float): learning rate for actor\n        critic_lr (float): learning rate for critic\n        vae_lr (float): learning rate for vae\n        reward_scale (float): The scaling factor for the reward signal.\n        cost_scale (float): The scaling factor for the constraint cost.\n        device (str): The device to use for training (e.g. \"cpu\" or \"cuda\").\n    \"\"\"\n\n    def __init__(\n            self,\n            model: BCQL,\n            env: gym.Env,\n            logger: WandbLogger = DummyLogger(),\n            # training params\n            actor_lr: float = 1e-4,\n            critic_lr: float = 1e-4,\n            vae_lr: float = 1e-4,\n            reward_scale: float = 1.0,\n            cost_scale: float = 1.0,\n            device=\"cpu\"):\n\n        self.model = model\n        self.logger = logger\n        self.env = env\n        self.reward_scale = reward_scale\n        self.cost_scale = cost_scale\n        self.device = device\n        self.model.setup_optimizers(actor_lr, critic_lr, vae_lr)\n\n    def train_one_step(self, observations, next_observations, actions, rewards, costs,\n                       done):\n        \"\"\"\n        Trains the model by updating the VAE, critic, cost critic, and actor.\n        \"\"\"\n\n        # update VAE\n        loss_vae, stats_vae = self.model.vae_loss(observations, actions)\n        # update critic\n        loss_critic, stats_critic = self.model.critic_loss(observations,\n                                                           next_observations, actions,\n                                                           rewards, done)\n        # update cost critic\n        loss_cost_critic, stats_cost_critic = self.model.cost_critic_loss(\n            observations, next_observations, actions, costs, done)\n        # update actor\n        loss_actor, stats_actor = self.model.actor_loss(observations)\n\n        self.model.sync_weight()\n\n        self.logger.store(**stats_vae)\n        self.logger.store(**stats_critic)\n        self.logger.store(**stats_cost_critic)\n        self.logger.store(**stats_actor)\n\n    def evaluate(self, eval_episodes):\n        \"\"\"\n        Evaluates the performance of the model on a number of episodes.\n        \"\"\"\n        self.model.eval()\n        episode_rets, episode_costs, episode_lens = [], [], []\n        for _ in trange(eval_episodes, desc=\"Evaluating...\", leave=False):\n            epi_ret, epi_len, epi_cost = self.rollout()\n            episode_rets.append(epi_ret)\n            episode_lens.append(epi_len)\n            episode_costs.append(epi_cost)\n        self.model.train()\n        return np.mean(episode_rets) / self.reward_scale, np.mean(\n            episode_costs) / self.cost_scale, np.mean(episode_lens)\n\n    @torch.no_grad()\n    def rollout(self):\n        \"\"\"\n        Evaluates the performance of the model on a single episode.\n        \"\"\"\n        obs, info = self.env.reset()\n        episode_ret, episode_cost, episode_len = 0.0, 0.0, 0\n        for _ in range(self.model.episode_len):\n            act, _ = self.model.act(obs)\n            obs_next, reward, terminated, truncated, info = self.env.step(act)\n            cost = info[\"cost\"] * self.cost_scale\n            obs = obs_next\n            episode_ret += reward\n            episode_len += 1\n            episode_cost += cost\n            if terminated or truncated:\n                break\n        return episode_ret, episode_len, episode_cost", ""]}
{"filename": "osrl/algorithms/coptidice.py", "chunked_list": ["# reference: https://github.com/deepmind/constrained_optidice.git\nimport gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom fsrl.utils import DummyLogger, WandbLogger\nfrom torch import distributions as pyd\nfrom torch.distributions.beta import Beta\nfrom torch.nn import functional as F  # noqa\nfrom tqdm.auto import trange  # noqa", "from torch.nn import functional as F  # noqa\nfrom tqdm.auto import trange  # noqa\n\nfrom osrl.common.net import EnsembleQCritic, SquashedGaussianMLPActor\n\n\ndef get_f_div_fn(f_type: str):\n    \"\"\"\n    Returns a function that computes the provided f-divergence type.\n    \"\"\"\n    f_fn = None\n    f_prime_inv_fn = None\n\n    if f_type == 'chi2':\n        f_fn = lambda x: 0.5 * (x - 1)**2\n        f_prime_inv_fn = lambda x: x + 1\n\n    elif f_type == 'softchi':\n        f_fn = lambda x: torch.where(x < 1,\n                                     x * (torch.log(x + 1e-10) - 1) + 1, 0.5 *\n                                     (x - 1)**2)\n        f_prime_inv_fn = lambda x: torch.where(x < 0, torch.exp(x.clamp(max=0.0)), x + 1)\n\n    elif f_type == 'kl':\n        f_fn = lambda x: x * torch.log(x + 1e-10)\n        f_prime_inv_fn = lambda x: torch.exp(x - 1)\n    else:\n        raise NotImplementedError('Not implemented f_fn:', f_type)\n\n    return f_fn, f_prime_inv_fn", "\n\nclass COptiDICE(nn.Module):\n    \"\"\"\n    Offline Constrained Policy Optimization \n    via stationary DIstribution Correction Estimation (COptiDICE)\n    \n    Args:\n        state_dim (int): dimension of the state space.\n        action_dim (int): dimension of the action space.\n        max_action (float): Maximum action value.\n        f_type (str): The type of f-divergence function to use.\n        init_state_propotion (float): The proportion of initial states to include in the optimization.\n        observations_std (np.ndarray): The standard deviation of the observation space.\n        actions_std (np.ndarray): The standard deviation of the action space.\n        a_hidden_sizes (list): List of integers specifying the sizes \n                               of the layers in the actor network.\n        c_hidden_sizes (list): List of integers specifying the sizes \n                               of the layers in the critic network (nu and chi networks).\n        gamma (float): Discount factor for the reward.\n        alpha (float): The coefficient for the cost term in the loss function.\n        cost_ub_epsilon (float): A small value added to the upper bound on the cost term.\n        num_nu (int): The number of critics to use for the nu-network.\n        num_chi (int): The number of critics to use for the chi-network.\n        cost_limit (int): Upper limit on the cost per episode.\n        episode_len (int): Maximum length of an episode.\n        device (str): Device to run the model on (e.g. 'cpu' or 'cuda:0'). \n    \"\"\"\n\n    def __init__(self,\n                 state_dim: int,\n                 action_dim: int,\n                 max_action: float,\n                 f_type: str,\n                 init_state_propotion: float,\n                 observations_std: np.ndarray,\n                 actions_std: np.ndarray,\n                 a_hidden_sizes: list = [128, 128],\n                 c_hidden_sizes: list = [128, 128],\n                 gamma: float = 0.99,\n                 alpha: float = 0.5,\n                 cost_ub_epsilon: float = 0.01,\n                 num_nu: int = 1,\n                 num_chi: int = 1,\n                 cost_limit: int = 10,\n                 episode_len: int = 300,\n                 device: str = \"cpu\"):\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.max_action = max_action\n        self.a_hidden_sizes = a_hidden_sizes\n        self.c_hidden_sizes = c_hidden_sizes\n        self.gamma = gamma\n        self.alpha = alpha\n        self.cost_ub_epsilon = cost_ub_epsilon\n        self.num_nu = num_nu\n        self.num_chi = num_chi\n        self.cost_limit = cost_limit\n        self.episode_len = episode_len\n        self.device = device\n\n        self.qc_thres = cost_limit * (1 - self.gamma**self.episode_len) / (\n            1 - self.gamma) / self.episode_len\n        self.tau = torch.ones(1, requires_grad=True, device=self.device)\n        self.lmbda = torch.ones(1, requires_grad=True, device=self.device)\n        self.actor = SquashedGaussianMLPActor(self.state_dim, self.action_dim,\n                                              self.a_hidden_sizes,\n                                              nn.ReLU).to(self.device)\n        self.nu_network = EnsembleQCritic(self.state_dim,\n                                          0,\n                                          self.c_hidden_sizes,\n                                          nn.ReLU,\n                                          num_q=self.num_nu).to(self.device)\n        self.chi_network = EnsembleQCritic(self.state_dim,\n                                           0,\n                                           self.c_hidden_sizes,\n                                           nn.ReLU,\n                                           num_q=self.num_chi).to(self.device)\n\n        self.f_fn, self.f_prime_inv_fn = get_f_div_fn(f_type)\n\n        self.init_state_propotion = init_state_propotion\n        self.observations_std = torch.tensor(observations_std, device=self.device)\n        self.actions_std = torch.tensor(actions_std, device=self.device)\n\n    def _optimal_w(self, observations, next_observations, rewards, costs, done):\n        nu_s, _ = self.nu_network.predict(observations, None)\n        nu_s_next, _ = self.nu_network.predict(next_observations, None)\n        # \\hat{e}_{\\lambda, \\nu}(s,a,s')\n        e_nu_lambda = rewards - self._lmbda.detach() * costs\n        e_nu_lambda += self.gamma * (1.0 - done) * nu_s_next - nu_s\n        # w_{lambda,\\nu}^*(s,a)\n        w_sa = F.relu(self.f_prime_inv_fn(e_nu_lambda / self.alpha))\n        return nu_s, nu_s_next, e_nu_lambda, w_sa\n\n    def update(self, batch):\n        observations, next_observations, actions, rewards, costs, done, is_init = batch\n        # 1. Learn the optimal distribution\n        self._lmbda = F.softplus(self.lmbda)  # lmbda >= 0\n\n        nu_s, nu_s_next, e_nu_lambda, w_sa = self._optimal_w(observations,\n                                                             next_observations, rewards,\n                                                             costs, done)\n        nu_init = nu_s * is_init / self.init_state_propotion\n        w_sa_no_grad = w_sa.detach()\n        # divergence between distributions of policy & dataset\n        Df = self.f_fn(w_sa_no_grad).mean()\n\n        # 1.1 (chi, tau) loss\n        if self.cost_ub_epsilon == 0:\n            weighted_c = (w_sa_no_grad * costs).mean()\n            chi_loss = torch.zeros(1, device=self.device)\n            tau_loss = torch.zeros(1, device=self.device)\n            D_kl = torch.zeros(1, device=self.device)\n            self._tau = F.softplus(self.tau)\n\n        else:\n            self._tau = F.softplus(self.tau)\n            batch_size = observations.shape[0]\n\n            chi_s, _ = self.chi_network.predict(observations, None)\n            chi_s_next, _ = self.chi_network.predict(next_observations, None)\n            chi_init = chi_s * is_init / self.init_state_propotion\n\n            ell = (1- self.gamma) * chi_init + \\\n                    w_sa_no_grad * (costs + self.gamma * (1 - done) * chi_s_next - chi_s)\n            logits = ell / self._tau.detach()\n            weights = torch.softmax(logits, dim=0) * batch_size\n            log_weights = torch.log_softmax(logits, dim=0) + np.log(batch_size)\n            D_kl = (weights * log_weights - weights + 1).mean()\n\n            # an upper bound estimation\n            weighted_c = (weights * w_sa_no_grad * costs).mean()\n\n            chi_loss = (weights * ell).mean()\n            self.chi_optim.zero_grad()\n            chi_loss.backward(retain_graph=True)\n            self.chi_optim.step()\n\n            tau_loss = self._tau * (self.cost_ub_epsilon - D_kl.detach())\n            self.tau_optim.zero_grad()\n            tau_loss.backward()\n            self.tau_optim.step()\n\n        # 1.2 nu loss\n        nu_loss = (1 - self.gamma) * nu_init.mean() + \\\n            (w_sa * e_nu_lambda - self.alpha * self.f_fn(w_sa)).mean()\n        td_error = e_nu_lambda.pow(2).mean()\n\n        self.nu_optim.zero_grad()\n        nu_loss.backward(retain_graph=True)\n        self.nu_optim.step()\n\n        # 1.3 lambda loss\n        lmbda_loss = self._lmbda * (self.qc_thres - weighted_c.detach())\n\n        self.lmbda_optim.zero_grad()\n        lmbda_loss.backward()\n        self.lmbda_optim.step()\n\n        # 2. Extract policy\n        obs_eps = torch.randn_like(observations) * self.observations_std * 0.1\n        act_eps = torch.randn_like(actions) * self.actions_std * 0.1\n\n        _, _, dist = self.actor.forward(observations + obs_eps, False, True, True)\n\n        with torch.no_grad():\n            _, _, e_nu_lambda, w_sa = self._optimal_w(observations, next_observations,\n                                                      rewards, costs, done)\n\n        actor_loss = -(w_sa * dist.log_prob(actions + act_eps).sum(axis=-1)).mean()\n        self.actor_optim.zero_grad()\n        actor_loss.backward()\n        self.actor_optim.step()\n\n        stats_loss = {\n            \"loss/chi_loss\": chi_loss.item(),\n            \"loss/tau_loss\": tau_loss.item(),\n            \"loss/D_kl\": D_kl.item(),\n            \"loss/Df\": Df.item(),\n            \"loss/td_error\": td_error.item(),\n            \"loss/nu_loss\": nu_loss.item(),\n            \"loss/lmbda_loss\": lmbda_loss.item(),\n            \"loss/actor_loss\": actor_loss.item(),\n            \"loss/tau\": self._tau.item(),\n            \"loss/lmbda\": self._lmbda.item()\n        }\n        return stats_loss\n\n    def setup_optimizers(self, actor_lr, critic_lr, scalar_lr):\n        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n        self.nu_optim = torch.optim.Adam(self.nu_network.parameters(), lr=critic_lr)\n        self.chi_optim = torch.optim.Adam(self.chi_network.parameters(), lr=critic_lr)\n        self.lmbda_optim = torch.optim.Adam([self.lmbda], lr=scalar_lr)\n        self.tau_optim = torch.optim.Adam([self.tau], lr=scalar_lr)\n\n    def act(self,\n            obs: np.ndarray,\n            deterministic: bool = False,\n            with_logprob: bool = False):\n        \"\"\"\n        Given a single obs, return the action, logp.\n        \"\"\"\n        obs = torch.tensor(obs[None, ...], dtype=torch.float32).to(self.device)\n        a, logp_a = self.actor.forward(obs, deterministic, with_logprob)\n        a = a.data.numpy() if self.device == \"cpu\" else a.data.cpu().numpy()\n        logp_a = logp_a.data.numpy() if self.device == \"cpu\" else logp_a.data.cpu(\n        ).numpy()\n        return np.squeeze(a, axis=0), np.squeeze(logp_a)", "\n\nclass COptiDICETrainer:\n    \"\"\"\n    COptiDICE trainer\n    \n    Args:\n        model (COptiDICE): The COptiDICE model to train.\n        env (gym.Env): The OpenAI Gym environment to train the model in.\n        logger (WandbLogger or DummyLogger): The logger to use for tracking training progress.\n        actor_lr (float): learning rate for actor\n        critic_lr (float): learning rate for critic (nu and chi networks)\n        scalar_lr (float, optional): The learning rate for the scalar (tau, lmbda).\n        reward_scale (float): The scaling factor for the reward signal.\n        cost_scale (float): The scaling factor for the constraint cost.\n        device (str): The device to use for training (e.g. \"cpu\" or \"cuda\").\n    \"\"\"\n\n    def __init__(self,\n                 model: COptiDICE,\n                 env: gym.Env,\n                 logger: WandbLogger = DummyLogger(),\n                 actor_lr: float = 1e-3,\n                 critic_lr: float = 1e-3,\n                 scalar_lr: float = 1e-3,\n                 reward_scale: float = 1.0,\n                 cost_scale: float = 1.0,\n                 device=\"cpu\"):\n        self.model = model\n        self.logger = logger\n        self.env = env\n        self.reward_scale = reward_scale\n        self.cost_scale = cost_scale\n        self.device = device\n        self.model.setup_optimizers(actor_lr, critic_lr, scalar_lr)\n\n    def train_one_step(self, batch):\n        stats_loss = self.model.update(batch)\n        self.logger.store(**stats_loss)\n\n    def evaluate(self, eval_episodes):\n        \"\"\"\n        Evaluates the performance of the model on a number of episodes.\n        \"\"\"\n        self.model.eval()\n        episode_rets, episode_costs, episode_lens = [], [], []\n        for _ in trange(eval_episodes, desc=\"Evaluating...\", leave=False):\n            epi_ret, epi_len, epi_cost = self.rollout()\n            episode_rets.append(epi_ret)\n            episode_lens.append(epi_len)\n            episode_costs.append(epi_cost)\n        self.model.train()\n        return np.mean(episode_rets) / self.reward_scale, np.mean(\n            episode_costs) / self.cost_scale, np.mean(episode_lens)\n\n    @torch.no_grad()\n    def rollout(self):\n        \"\"\"\n        Evaluates the performance of the model on a single episode.\n        \"\"\"\n        obs, info = self.env.reset()\n        episode_ret, episode_cost, episode_len = 0.0, 0.0, 0\n        for _ in range(self.model.episode_len):\n            act, _ = self.model.act(obs, True, True)\n            obs_next, reward, terminated, truncated, info = self.env.step(act)\n            cost = info[\"cost\"] * self.cost_scale\n            obs = obs_next\n            episode_ret += reward\n            episode_len += 1\n            episode_cost += cost\n            if terminated or truncated:\n                break\n        return episode_ret, episode_len, episode_cost", ""]}
{"filename": "osrl/algorithms/__init__.py", "chunked_list": ["from .bc import BC, BCTrainer\nfrom .bcql import BCQL, BCQLTrainer\nfrom .bearl import BEARL, BEARLTrainer\nfrom .cdt import CDT, CDTTrainer\nfrom .coptidice import COptiDICE, COptiDICETrainer\nfrom .cpq import CPQ, CPQTrainer"]}
{"filename": "osrl/algorithms/cpq.py", "chunked_list": ["from copy import deepcopy\n\nimport gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom fsrl.utils import DummyLogger, WandbLogger\nfrom tqdm.auto import trange  # noqa\n\nfrom osrl.common.net import VAE, EnsembleQCritic, SquashedGaussianMLPActor", "\nfrom osrl.common.net import VAE, EnsembleQCritic, SquashedGaussianMLPActor\n\n\nclass CPQ(nn.Module):\n    \"\"\"\n    Constraints Penalized Q-Learning (CPQ)\n\n    Args:\n        state_dim (int): dimension of the state space.\n        action_dim (int): dimension of the action space.\n        max_action (float): Maximum action value.\n        a_hidden_sizes (list): List of integers specifying the sizes \n                               of the layers in the actor network.\n        c_hidden_sizes (list): List of integers specifying the sizes \n                               of the layers in the critic network.\n        vae_hidden_sizes (int): Number of hidden units in the VAE. \n        sample_action_num (int): Number of action samples to draw. \n        gamma (float): Discount factor for the reward.\n        tau (float): Soft update coefficient for the target networks. \n        beta (float): Weight of the KL divergence term.\n        num_q (int): Number of Q networks in the ensemble.\n        num_qc (int): Number of cost Q networks in the ensemble.\n        qc_scalar (float): Scaling factor for the cost critic threshold.\n        cost_limit (int): Upper limit on the cost per episode.\n        episode_len (int): Maximum length of an episode.\n        device (str): Device to run the model on (e.g. 'cpu' or 'cuda:0'). \n    \"\"\"\n\n    def __init__(self,\n                 state_dim: int,\n                 action_dim: int,\n                 max_action: float,\n                 a_hidden_sizes: list = [128, 128],\n                 c_hidden_sizes: list = [128, 128],\n                 vae_hidden_sizes: int = 64,\n                 sample_action_num: int = 10,\n                 gamma: float = 0.99,\n                 tau: float = 0.005,\n                 beta: float = 1.5,\n                 num_q: int = 1,\n                 num_qc: int = 1,\n                 qc_scalar: float = 1.5,\n                 cost_limit: int = 10,\n                 episode_len: int = 300,\n                 device: str = \"cpu\"):\n\n        super().__init__()\n        self.a_hidden_sizes = a_hidden_sizes\n        self.c_hidden_sizes = c_hidden_sizes\n        self.vae_hidden_sizes = vae_hidden_sizes\n        self.gamma = gamma\n        self.tau = tau\n        self.beta = beta\n        self.cost_limit = cost_limit\n        self.num_q = num_q\n        self.num_qc = num_qc\n        self.qc_scalar = qc_scalar\n        self.sample_action_num = sample_action_num\n\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.latent_dim = self.action_dim * 2\n        self.episode_len = episode_len\n        self.max_action = max_action\n\n        self.device = device\n\n        ################ create actor critic model ###############\n        self.actor = SquashedGaussianMLPActor(self.state_dim, self.action_dim,\n                                              self.a_hidden_sizes,\n                                              nn.ReLU).to(self.device)\n        self.critic = EnsembleQCritic(self.state_dim,\n                                      self.action_dim,\n                                      self.c_hidden_sizes,\n                                      nn.ReLU,\n                                      num_q=self.num_q).to(self.device)\n        self.vae = VAE(self.state_dim, self.action_dim, self.vae_hidden_sizes,\n                       self.latent_dim, self.max_action, self.device).to(self.device)\n        self.cost_critic = EnsembleQCritic(self.state_dim,\n                                           self.action_dim,\n                                           self.c_hidden_sizes,\n                                           nn.ReLU,\n                                           num_q=self.num_qc).to(self.device)\n        self.log_alpha = torch.tensor(0.0, device=self.device)\n\n        self.actor_old = deepcopy(self.actor)\n        self.actor_old.eval()\n        self.critic_old = deepcopy(self.critic)\n        self.critic_old.eval()\n        self.cost_critic_old = deepcopy(self.cost_critic)\n        self.cost_critic_old.eval()\n\n        # set critic and cost critic threshold\n        self.q_thres = cost_limit * (1 - self.gamma**self.episode_len) / (\n            1 - self.gamma) / self.episode_len\n        self.qc_thres = qc_scalar * self.q_thres\n\n    def _soft_update(self, tgt: nn.Module, src: nn.Module, tau: float) -> None:\n        \"\"\"\n        Softly update the parameters of target module \n        towards the parameters of source module.\n        \"\"\"\n        for tgt_param, src_param in zip(tgt.parameters(), src.parameters()):\n            tgt_param.data.copy_(tau * src_param.data + (1 - tau) * tgt_param.data)\n\n    def _actor_forward(self,\n                       obs: torch.tensor,\n                       deterministic: bool = False,\n                       with_logprob: bool = True):\n        \"\"\"\n        Return action distribution and action log prob [optional].\n        \"\"\"\n        a, logp = self.actor(obs, deterministic, with_logprob)\n        return a * self.max_action, logp\n\n    def vae_loss(self, observations, actions):\n        recon, mean, std = self.vae(observations, actions)\n        recon_loss = nn.functional.mse_loss(recon, actions)\n        KL_loss = -0.5 * (1 + torch.log(std.pow(2)) - mean.pow(2) - std.pow(2)).mean()\n        loss_vae = recon_loss + self.beta * KL_loss\n\n        self.vae_optim.zero_grad()\n        loss_vae.backward()\n        self.vae_optim.step()\n        stats_vae = {\"loss/loss_vae\": loss_vae.item()}\n        return loss_vae, stats_vae\n\n    def critic_loss(self, observations, next_observations, actions, rewards, done):\n        _, q_list = self.critic.predict(observations, actions)\n        # Bellman backup for Q functions\n        with torch.no_grad():\n            next_actions, _ = self._actor_forward(next_observations, False, True)\n            q_targ, _ = self.critic_old.predict(next_observations, next_actions)\n            qc_targ, _ = self.cost_critic_old.predict(next_observations, next_actions)\n            # Constraints Penalized Bellman operator\n            backup = rewards + self.gamma * (1 -\n                                             done) * (qc_targ <= self.q_thres) * q_targ\n        # MSE loss against Bellman backup\n        loss_critic = self.critic.loss(backup, q_list)\n        self.critic_optim.zero_grad()\n        loss_critic.backward()\n        self.critic_optim.step()\n        stats_critic = {\"loss/critic_loss\": loss_critic.item()}\n        return loss_critic, stats_critic\n\n    def cost_critic_loss(self, observations, next_observations, actions, costs, done):\n        _, qc_list = self.cost_critic.predict(observations, actions)\n        # Bellman backup for Q functions\n        with torch.no_grad():\n            next_actions, _ = self._actor_forward(next_observations, False, True)\n            qc_targ, _ = self.cost_critic_old.predict(next_observations, next_actions)\n            backup = costs + self.gamma * qc_targ\n\n            batch_size = observations.shape[0]\n            _, _, pi_dist = self.actor(observations, False, True, True)\n            # sample actions\n            sampled_actions = pi_dist.sample(\n                [self.sample_action_num])  # [sample_action_num, batch_size, act_dim]\n            sampled_actions = sampled_actions.reshape(\n                self.sample_action_num * batch_size, self.action_dim)\n            stacked_obs = torch.tile(observations[None, :, :],\n                                     (self.sample_action_num, 1,\n                                      1))  # [sample_action_num, batch_size, obs_dim]\n            stacked_obs = stacked_obs.reshape(self.sample_action_num * batch_size,\n                                              self.state_dim)\n            qc_sampled, _ = self.cost_critic_old.predict(stacked_obs, sampled_actions)\n            qc_sampled = qc_sampled.reshape(self.sample_action_num, batch_size)\n            # get latent mean and std\n            _, mean, std = self.vae(stacked_obs, sampled_actions)\n            mean = mean.reshape(self.sample_action_num, batch_size, self.latent_dim)\n            std = std.reshape(self.sample_action_num, batch_size, self.latent_dim)\n            KL_loss = -0.5 * (1 + torch.log(std.pow(2)) - mean.pow(2) - std.pow(2)).mean(\n                2)  # [sample_action_num, batch_size]\n            quantile = torch.quantile(KL_loss, 0.75)\n            qc_ood = ((KL_loss >= quantile) * qc_sampled).mean(0)\n\n        loss_cost_critic = self.cost_critic.loss(\n            backup, qc_list) - self.log_alpha.exp() * (qc_ood.mean() - self.qc_thres)\n        self.cost_critic_optim.zero_grad()\n        loss_cost_critic.backward()\n        self.cost_critic_optim.step()\n\n        # update alpha\n        self.log_alpha += self.alpha_lr * self.log_alpha.exp() * (\n            self.qc_thres - qc_ood.mean()).detach()\n        self.log_alpha.data.clamp_(min=-5.0, max=5.0)\n\n        stats_cost_critic = {\n            \"loss/cost_critic_loss\": loss_cost_critic.item(),\n            \"loss/alpha_value\": self.log_alpha.exp().item()\n        }\n        return loss_cost_critic, stats_cost_critic\n\n    def actor_loss(self, observations):\n        for p in self.critic.parameters():\n            p.requires_grad = False\n        for p in self.cost_critic.parameters():\n            p.requires_grad = False\n\n        actions, _ = self._actor_forward(observations, False, True)\n        q_pi, _ = self.critic.predict(observations, actions)\n        qc_pi, _ = self.cost_critic.predict(observations, actions)\n        loss_actor = -((qc_pi <= self.q_thres) * q_pi).mean()\n        self.actor_optim.zero_grad()\n        loss_actor.backward()\n        self.actor_optim.step()\n        stats_actor = {\"loss/actor_loss\": loss_actor.item()}\n\n        for p in self.critic.parameters():\n            p.requires_grad = True\n        for p in self.cost_critic.parameters():\n            p.requires_grad = True\n        return loss_actor, stats_actor\n\n    def sync_weight(self):\n        \"\"\"\n        Soft-update the weight for the target network.\n        \"\"\"\n        self._soft_update(self.critic_old, self.critic, self.tau)\n        self._soft_update(self.cost_critic_old, self.cost_critic, self.tau)\n        self._soft_update(self.actor_old, self.actor, self.tau)\n\n    def setup_optimizers(self, actor_lr, critic_lr, alpha_lr, vae_lr):\n        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n        self.cost_critic_optim = torch.optim.Adam(self.cost_critic.parameters(),\n                                                  lr=critic_lr)\n        self.vae_optim = torch.optim.Adam(self.vae.parameters(), lr=vae_lr)\n        self.alpha_lr = alpha_lr\n\n    def act(self,\n            obs: np.ndarray,\n            deterministic: bool = False,\n            with_logprob: bool = False):\n        \"\"\"\n        Given a single obs, return the action, logp.\n        \"\"\"\n        obs = torch.tensor(obs[None, ...], dtype=torch.float32).to(self.device)\n        a, logp_a = self._actor_forward(obs, deterministic, with_logprob)\n        a = a.data.numpy() if self.device == \"cpu\" else a.data.cpu().numpy()\n        logp_a = logp_a.data.numpy() if self.device == \"cpu\" else logp_a.data.cpu(\n        ).numpy()\n        return np.squeeze(a, axis=0), np.squeeze(logp_a)", "\n\nclass CPQTrainer:\n    \"\"\"\n    Constraints Penalized Q-learning Trainer\n    \n    Args:\n        model (CPQ): The CPQ model to be trained.\n        env (gym.Env): The OpenAI Gym environment to train the model in.\n        logger (WandbLogger or DummyLogger): The logger to use for tracking training progress.\n        actor_lr (float): learning rate for actor\n        critic_lr (float): learning rate for critic\n        alpha_lr (float): learning rate for alpha\n        vae_lr (float): learning rate for vae\n        reward_scale (float): The scaling factor for the reward signal.\n        cost_scale (float): The scaling factor for the constraint cost.\n        device (str): The device to use for training (e.g. \"cpu\" or \"cuda\").\n    \"\"\"\n\n    def __init__(\n            self,\n            model: CPQ,\n            env: gym.Env,\n            logger: WandbLogger = DummyLogger(),\n            # training params\n            actor_lr: float = 1e-4,\n            critic_lr: float = 1e-4,\n            alpha_lr: float = 1e-4,\n            vae_lr: float = 1e-4,\n            reward_scale: float = 1.0,\n            cost_scale: float = 1.0,\n            device=\"cpu\") -> None:\n\n        self.model = model\n        self.logger = logger\n        self.env = env\n        self.reward_scale = reward_scale\n        self.cost_scale = cost_scale\n        self.device = device\n        self.model.setup_optimizers(actor_lr, critic_lr, alpha_lr, vae_lr)\n\n    def train_one_step(self, observations, next_observations, actions, rewards, costs,\n                       done):\n        # update VAE\n        loss_vae, stats_vae = self.model.vae_loss(observations, actions)\n        # update critic\n        loss_critic, stats_critic = self.model.critic_loss(observations,\n                                                           next_observations, actions,\n                                                           rewards, done)\n        # update cost critic\n        loss_cost_critic, stats_cost_critic = self.model.cost_critic_loss(\n            observations, next_observations, actions, costs, done)\n        # update actor\n        loss_actor, stats_actor = self.model.actor_loss(observations)\n\n        self.model.sync_weight()\n\n        self.logger.store(**stats_vae)\n        self.logger.store(**stats_critic)\n        self.logger.store(**stats_cost_critic)\n        self.logger.store(**stats_actor)\n\n    def evaluate(self, eval_episodes):\n        \"\"\"\n        Evaluates the performance of the model on a number of episodes.\n        \"\"\"\n        self.model.eval()\n        episode_rets, episode_costs, episode_lens = [], [], []\n        for _ in trange(eval_episodes, desc=\"Evaluating...\", leave=False):\n            epi_ret, epi_len, epi_cost = self.rollout()\n            episode_rets.append(epi_ret)\n            episode_lens.append(epi_len)\n            episode_costs.append(epi_cost)\n        self.model.train()\n        return np.mean(episode_rets) / self.reward_scale, np.mean(\n            episode_costs) / self.cost_scale, np.mean(episode_lens)\n\n    @torch.no_grad()\n    def rollout(self):\n        \"\"\"\n        Evaluates the performance of the model on a single episode.\n        \"\"\"\n        obs, info = self.env.reset()\n        episode_ret, episode_cost, episode_len = 0.0, 0.0, 0\n        for _ in range(self.model.episode_len):\n            act, _ = self.model.act(obs, True, True)\n            obs_next, reward, terminated, truncated, info = self.env.step(act)\n            cost = info[\"cost\"] * self.cost_scale\n            obs = obs_next\n            episode_ret += reward\n            episode_len += 1\n            episode_cost += cost\n            if terminated or truncated:\n                break\n        return episode_ret, episode_len, episode_cost", ""]}
{"filename": "osrl/algorithms/cdt.py", "chunked_list": ["from typing import Optional, Tuple\n\nimport gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom fsrl.utils import DummyLogger, WandbLogger\nfrom torch.distributions.beta import Beta\nfrom torch.nn import functional as F  # noqa\nfrom tqdm.auto import trange  # noqa", "from torch.nn import functional as F  # noqa\nfrom tqdm.auto import trange  # noqa\n\nfrom osrl.common.net import DiagGaussianActor, TransformerBlock, mlp\n\n\nclass CDT(nn.Module):\n    \"\"\"\n    Constrained Decision Transformer (CDT)\n    \n    Args:\n        state_dim (int): dimension of the state space.\n        action_dim (int): dimension of the action space.\n        max_action (float): Maximum action value.\n        seq_len (int): The length of the sequence to process.\n        episode_len (int): The length of the episode.\n        embedding_dim (int): The dimension of the embeddings.\n        num_layers (int): The number of transformer layers to use.\n        num_heads (int): The number of heads to use in the multi-head attention.\n        attention_dropout (float): The dropout probability for attention layers.\n        residual_dropout (float): The dropout probability for residual layers.\n        embedding_dropout (float): The dropout probability for embedding layers.\n        time_emb (bool): Whether to include time embeddings.\n        use_rew (bool): Whether to include return embeddings.\n        use_cost (bool): Whether to include cost embeddings.\n        cost_transform (bool): Whether to transform the cost values.\n        add_cost_feat (bool): Whether to add cost features.\n        mul_cost_feat (bool): Whether to multiply cost features.\n        cat_cost_feat (bool): Whether to concatenate cost features.\n        action_head_layers (int): The number of layers in the action head.\n        cost_prefix (bool): Whether to include a cost prefix.\n        stochastic (bool): Whether to use stochastic actions.\n        init_temperature (float): The initial temperature value for stochastic actions.\n        target_entropy (float): The target entropy value for stochastic actions.\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim: int,\n        action_dim: int,\n        max_action: float,\n        seq_len: int = 10,\n        episode_len: int = 1000,\n        embedding_dim: int = 128,\n        num_layers: int = 4,\n        num_heads: int = 8,\n        attention_dropout: float = 0.0,\n        residual_dropout: float = 0.0,\n        embedding_dropout: float = 0.0,\n        time_emb: bool = True,\n        use_rew: bool = False,\n        use_cost: bool = False,\n        cost_transform: bool = False,\n        add_cost_feat: bool = False,\n        mul_cost_feat: bool = False,\n        cat_cost_feat: bool = False,\n        action_head_layers: int = 1,\n        cost_prefix: bool = False,\n        stochastic: bool = False,\n        init_temperature=0.1,\n        target_entropy=None,\n    ):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding_dim = embedding_dim\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.episode_len = episode_len\n        self.max_action = max_action\n        if cost_transform:\n            self.cost_transform = lambda x: 50 - x\n        else:\n            self.cost_transform = None\n        self.add_cost_feat = add_cost_feat\n        self.mul_cost_feat = mul_cost_feat\n        self.cat_cost_feat = cat_cost_feat\n        self.stochastic = stochastic\n\n        self.emb_drop = nn.Dropout(embedding_dropout)\n        self.emb_norm = nn.LayerNorm(embedding_dim)\n\n        self.out_norm = nn.LayerNorm(embedding_dim)\n        # additional seq_len embeddings for padding timesteps\n        self.time_emb = time_emb\n        if self.time_emb:\n            self.timestep_emb = nn.Embedding(episode_len + seq_len, embedding_dim)\n\n        self.state_emb = nn.Linear(state_dim, embedding_dim)\n        self.action_emb = nn.Linear(action_dim, embedding_dim)\n\n        self.seq_repeat = 2\n        self.use_rew = use_rew\n        self.use_cost = use_cost\n        if self.use_cost:\n            self.cost_emb = nn.Linear(1, embedding_dim)\n            self.seq_repeat += 1\n        if self.use_rew:\n            self.return_emb = nn.Linear(1, embedding_dim)\n            self.seq_repeat += 1\n\n        dt_seq_len = self.seq_repeat * seq_len\n\n        self.cost_prefix = cost_prefix\n        if self.cost_prefix:\n            self.prefix_emb = nn.Linear(1, embedding_dim)\n            dt_seq_len += 1\n\n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                seq_len=dt_seq_len,\n                embedding_dim=embedding_dim,\n                num_heads=num_heads,\n                attention_dropout=attention_dropout,\n                residual_dropout=residual_dropout,\n            ) for _ in range(num_layers)\n        ])\n\n        action_emb_dim = 2 * embedding_dim if self.cat_cost_feat else embedding_dim\n\n        if self.stochastic:\n            if action_head_layers >= 2:\n                self.action_head = nn.Sequential(\n                    nn.Linear(action_emb_dim, action_emb_dim), nn.GELU(),\n                    DiagGaussianActor(action_emb_dim, action_dim))\n            else:\n                self.action_head = DiagGaussianActor(action_emb_dim, action_dim)\n        else:\n            self.action_head = mlp([action_emb_dim] * action_head_layers + [action_dim],\n                                   activation=nn.GELU,\n                                   output_activation=nn.Identity)\n        self.state_pred_head = nn.Linear(embedding_dim, state_dim)\n        # a classification problem\n        self.cost_pred_head = nn.Linear(embedding_dim, 2)\n\n        if self.stochastic:\n            self.log_temperature = torch.tensor(np.log(init_temperature))\n            self.log_temperature.requires_grad = True\n            self.target_entropy = target_entropy\n\n        self.apply(self._init_weights)\n\n    def temperature(self):\n        if self.stochastic:\n            return self.log_temperature.exp()\n        else:\n            return None\n\n    @staticmethod\n    def _init_weights(module: nn.Module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.LayerNorm):\n            torch.nn.init.zeros_(module.bias)\n            torch.nn.init.ones_(module.weight)\n\n    def forward(\n            self,\n            states: torch.Tensor,  # [batch_size, seq_len, state_dim]\n            actions: torch.Tensor,  # [batch_size, seq_len, action_dim]\n            returns_to_go: torch.Tensor,  # [batch_size, seq_len]\n            costs_to_go: torch.Tensor,  # [batch_size, seq_len]\n            time_steps: torch.Tensor,  # [batch_size, seq_len]\n            padding_mask: Optional[torch.Tensor] = None,  # [batch_size, seq_len]\n            episode_cost: torch.Tensor = None,  # [batch_size, ]\n    ) -> torch.FloatTensor:\n        batch_size, seq_len = states.shape[0], states.shape[1]\n        # [batch_size, seq_len, emb_dim]\n        if self.time_emb:\n            timestep_emb = self.timestep_emb(time_steps)\n        else:\n            timestep_emb = 0.0\n        state_emb = self.state_emb(states) + timestep_emb\n        act_emb = self.action_emb(actions) + timestep_emb\n\n        seq_list = [state_emb, act_emb]\n\n        if self.cost_transform is not None:\n            costs_to_go = self.cost_transform(costs_to_go.detach())\n\n        if self.use_cost:\n            costs_emb = self.cost_emb(costs_to_go.unsqueeze(-1)) + timestep_emb\n            seq_list.insert(0, costs_emb)\n        if self.use_rew:\n            returns_emb = self.return_emb(returns_to_go.unsqueeze(-1)) + timestep_emb\n            seq_list.insert(0, returns_emb)\n\n        # [batch_size, seq_len, 2-4, emb_dim], (c_0 s_0, a_0, c_1, s_1, a_1, ...)\n        sequence = torch.stack(seq_list, dim=1).permute(0, 2, 1, 3)\n        sequence = sequence.reshape(batch_size, self.seq_repeat * seq_len,\n                                    self.embedding_dim)\n\n        if padding_mask is not None:\n            # [batch_size, seq_len * self.seq_repeat], stack mask identically to fit the sequence\n            padding_mask = torch.stack([padding_mask] * self.seq_repeat,\n                                       dim=1).permute(0, 2, 1).reshape(batch_size, -1)\n\n        if self.cost_prefix:\n            episode_cost = episode_cost.unsqueeze(-1).unsqueeze(-1)\n\n            episode_cost = episode_cost.to(states.dtype)\n            # [batch, 1, emb_dim]\n            episode_cost_emb = self.prefix_emb(episode_cost)\n            # [batch, 1+seq_len * self.seq_repeat, emb_dim]\n            sequence = torch.cat([episode_cost_emb, sequence], dim=1)\n            if padding_mask is not None:\n                # [batch_size, 1+ seq_len * self.seq_repeat]\n                padding_mask = torch.cat([padding_mask[:, :1], padding_mask], dim=1)\n\n        # LayerNorm and Dropout (!!!) as in original implementation,\n        # while minGPT & huggingface uses only embedding dropout\n        out = self.emb_norm(sequence)\n        out = self.emb_drop(out)\n\n        for block in self.blocks:\n            out = block(out, padding_mask=padding_mask)\n\n        # [batch_size, seq_len * self.seq_repeat, embedding_dim]\n        out = self.out_norm(out)\n        if self.cost_prefix:\n            # [batch_size, seq_len * seq_repeat, embedding_dim]\n            out = out[:, 1:]\n\n        # [batch_size, seq_len, self.seq_repeat, embedding_dim]\n        out = out.reshape(batch_size, seq_len, self.seq_repeat, self.embedding_dim)\n        # [batch_size, self.seq_repeat, seq_len, embedding_dim]\n        out = out.permute(0, 2, 1, 3)\n\n        # [batch_size, seq_len, embedding_dim]\n        action_feature = out[:, self.seq_repeat - 1]\n        state_feat = out[:, self.seq_repeat - 2]\n\n        if self.add_cost_feat and self.use_cost:\n            state_feat = state_feat + costs_emb.detach()\n        if self.mul_cost_feat and self.use_cost:\n            state_feat = state_feat * costs_emb.detach()\n        if self.cat_cost_feat and self.use_cost:\n            # cost_prefix feature, deprecated\n            # episode_cost_emb = episode_cost_emb.repeat_interleave(seq_len, dim=1)\n            # [batch_size, seq_len, 2 * embedding_dim]\n            state_feat = torch.cat([state_feat, costs_emb.detach()], dim=2)\n\n        # get predictions\n\n        action_preds = self.action_head(\n            state_feat\n        )  # predict next action given state, [batch_size, seq_len, action_dim]\n        # [batch_size, seq_len, 2]\n        cost_preds = self.cost_pred_head(\n            action_feature)  # predict next cost return given state and action\n        cost_preds = F.log_softmax(cost_preds, dim=-1)\n\n        state_preds = self.state_pred_head(\n            action_feature)  # predict next state given state and action\n\n        return action_preds, cost_preds, state_preds", "\n\nclass CDTTrainer:\n    \"\"\"\n    Constrained Decision Transformer Trainer\n    \n    Args:\n        model (CDT): A CDT model to train.\n        env (gym.Env): The OpenAI Gym environment to train the model in.\n        logger (WandbLogger or DummyLogger): The logger to use for tracking training progress.\n        learning_rate (float): The learning rate for the optimizer.\n        weight_decay (float): The weight decay for the optimizer.\n        betas (Tuple[float, ...]): The betas for the optimizer.\n        clip_grad (float): The clip gradient value.\n        lr_warmup_steps (int): The number of warmup steps for the learning rate scheduler.\n        reward_scale (float): The scaling factor for the reward signal.\n        cost_scale (float): The scaling factor for the constraint cost.\n        loss_cost_weight (float): The weight for the cost loss.\n        loss_state_weight (float): The weight for the state loss.\n        cost_reverse (bool): Whether to reverse the cost.\n        no_entropy (bool): Whether to use entropy.\n        device (str): The device to use for training (e.g. \"cpu\" or \"cuda\").\n\n    \"\"\"\n\n    def __init__(\n            self,\n            model: CDT,\n            env: gym.Env,\n            logger: WandbLogger = DummyLogger(),\n            # training params\n            learning_rate: float = 1e-4,\n            weight_decay: float = 1e-4,\n            betas: Tuple[float, ...] = (0.9, 0.999),\n            clip_grad: float = 0.25,\n            lr_warmup_steps: int = 10000,\n            reward_scale: float = 1.0,\n            cost_scale: float = 1.0,\n            loss_cost_weight: float = 0.0,\n            loss_state_weight: float = 0.0,\n            cost_reverse: bool = False,\n            no_entropy: bool = False,\n            device=\"cpu\") -> None:\n        self.model = model\n        self.logger = logger\n        self.env = env\n        self.clip_grad = clip_grad\n        self.reward_scale = reward_scale\n        self.cost_scale = cost_scale\n        self.device = device\n        self.cost_weight = loss_cost_weight\n        self.state_weight = loss_state_weight\n        self.cost_reverse = cost_reverse\n        self.no_entropy = no_entropy\n\n        self.optim = torch.optim.AdamW(\n            self.model.parameters(),\n            lr=learning_rate,\n            weight_decay=weight_decay,\n            betas=betas,\n        )\n        self.scheduler = torch.optim.lr_scheduler.LambdaLR(\n            self.optim,\n            lambda steps: min((steps + 1) / lr_warmup_steps, 1),\n        )\n        self.stochastic = self.model.stochastic\n        if self.stochastic:\n            self.log_temperature_optimizer = torch.optim.Adam(\n                [self.model.log_temperature],\n                lr=1e-4,\n                betas=[0.9, 0.999],\n            )\n        self.max_action = self.model.max_action\n\n        self.beta_dist = Beta(torch.tensor(2, dtype=torch.float, device=self.device),\n                              torch.tensor(5, dtype=torch.float, device=self.device))\n\n    def train_one_step(self, states, actions, returns, costs_return, time_steps, mask,\n                       episode_cost, costs):\n        # True value indicates that the corresponding key value will be ignored\n        padding_mask = ~mask.to(torch.bool)\n        action_preds, cost_preds, state_preds = self.model(\n            states=states,\n            actions=actions,\n            returns_to_go=returns,\n            costs_to_go=costs_return,\n            time_steps=time_steps,\n            padding_mask=padding_mask,\n            episode_cost=episode_cost,\n        )\n\n        if self.stochastic:\n            log_likelihood = action_preds.log_prob(actions)[mask > 0].mean()\n            entropy = action_preds.entropy()[mask > 0].mean()\n            entropy_reg = self.model.temperature().detach()\n            entropy_reg_item = entropy_reg.item()\n            if self.no_entropy:\n                entropy_reg = 0.0\n                entropy_reg_item = 0.0\n            act_loss = -(log_likelihood + entropy_reg * entropy)\n            self.logger.store(tab=\"train\",\n                              nll=-log_likelihood.item(),\n                              ent=entropy.item(),\n                              ent_reg=entropy_reg_item)\n        else:\n            act_loss = F.mse_loss(action_preds, actions.detach(), reduction=\"none\")\n            # [batch_size, seq_len, action_dim] * [batch_size, seq_len, 1]\n            act_loss = (act_loss * mask.unsqueeze(-1)).mean()\n\n        # cost_preds: [batch_size * seq_len, 2], costs: [batch_size * seq_len]\n        cost_preds = cost_preds.reshape(-1, 2)\n        costs = costs.flatten().long().detach()\n        cost_loss = F.nll_loss(cost_preds, costs, reduction=\"none\")\n        # cost_loss = F.mse_loss(cost_preds, costs.detach(), reduction=\"none\")\n        cost_loss = (cost_loss * mask.flatten()).mean()\n        # compute the accuracy, 0 value, 1 indice, [batch_size, seq_len]\n        pred = cost_preds.data.max(dim=1)[1]\n        correct = pred.eq(costs.data.view_as(pred)) * mask.flatten()\n        correct = correct.sum()\n        total_num = mask.sum()\n        acc = correct / total_num\n\n        # [batch_size, seq_len, state_dim]\n        state_loss = F.mse_loss(state_preds[:, :-1],\n                                states[:, 1:].detach(),\n                                reduction=\"none\")\n        state_loss = (state_loss * mask[:, :-1].unsqueeze(-1)).mean()\n\n        loss = act_loss + self.cost_weight * cost_loss + self.state_weight * state_loss\n\n        self.optim.zero_grad()\n        loss.backward()\n        if self.clip_grad is not None:\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad)\n        self.optim.step()\n\n        if self.stochastic:\n            self.log_temperature_optimizer.zero_grad()\n            temperature_loss = (self.model.temperature() *\n                                (entropy - self.model.target_entropy).detach())\n            temperature_loss.backward()\n            self.log_temperature_optimizer.step()\n\n        self.scheduler.step()\n        self.logger.store(\n            tab=\"train\",\n            all_loss=loss.item(),\n            act_loss=act_loss.item(),\n            cost_loss=cost_loss.item(),\n            cost_acc=acc.item(),\n            state_loss=state_loss.item(),\n            train_lr=self.scheduler.get_last_lr()[0],\n        )\n\n    def evaluate(self, num_rollouts, target_return, target_cost):\n        \"\"\"\n        Evaluates the performance of the model on a number of episodes.\n        \"\"\"\n        self.model.eval()\n        episode_rets, episode_costs, episode_lens = [], [], []\n        for _ in trange(num_rollouts, desc=\"Evaluating...\", leave=False):\n            epi_ret, epi_len, epi_cost = self.rollout(self.model, self.env,\n                                                      target_return, target_cost)\n            episode_rets.append(epi_ret)\n            episode_lens.append(epi_len)\n            episode_costs.append(epi_cost)\n        self.model.train()\n        return np.mean(episode_rets) / self.reward_scale, np.mean(\n            episode_costs) / self.cost_scale, np.mean(episode_lens)\n\n    @torch.no_grad()\n    def rollout(\n        self,\n        model: CDT,\n        env: gym.Env,\n        target_return: float,\n        target_cost: float,\n    ) -> Tuple[float, float]:\n        \"\"\"\n        Evaluates the performance of the model on a single episode.\n        \"\"\"\n        states = torch.zeros(1,\n                             model.episode_len + 1,\n                             model.state_dim,\n                             dtype=torch.float,\n                             device=self.device)\n        actions = torch.zeros(1,\n                              model.episode_len,\n                              model.action_dim,\n                              dtype=torch.float,\n                              device=self.device)\n        returns = torch.zeros(1,\n                              model.episode_len + 1,\n                              dtype=torch.float,\n                              device=self.device)\n        costs = torch.zeros(1,\n                            model.episode_len + 1,\n                            dtype=torch.float,\n                            device=self.device)\n        time_steps = torch.arange(model.episode_len,\n                                  dtype=torch.long,\n                                  device=self.device)\n        time_steps = time_steps.view(1, -1)\n\n        obs, info = env.reset()\n        states[:, 0] = torch.as_tensor(obs, device=self.device)\n        returns[:, 0] = torch.as_tensor(target_return, device=self.device)\n        costs[:, 0] = torch.as_tensor(target_cost, device=self.device)\n\n        epi_cost = torch.tensor(np.array([target_cost]),\n                                dtype=torch.float,\n                                device=self.device)\n\n        # cannot step higher than model episode len, as timestep embeddings will crash\n        episode_ret, episode_cost, episode_len = 0.0, 0.0, 0\n        for step in range(model.episode_len):\n            # first select history up to step, then select last seq_len states,\n            # step + 1 as : operator is not inclusive, last action is dummy with zeros\n            # (as model will predict last, actual last values are not important) # fix this noqa!!!\n            s = states[:, :step + 1][:, -model.seq_len:]  # noqa\n            a = actions[:, :step + 1][:, -model.seq_len:]  # noqa\n            r = returns[:, :step + 1][:, -model.seq_len:]  # noqa\n            c = costs[:, :step + 1][:, -model.seq_len:]  # noqa\n            t = time_steps[:, :step + 1][:, -model.seq_len:]  # noqa\n\n            acts, _, _ = model(s, a, r, c, t, None, epi_cost)\n            if self.stochastic:\n                acts = acts.mean\n            acts = acts.clamp(-self.max_action, self.max_action)\n            act = acts[0, -1].cpu().numpy()\n            # act = self.get_ensemble_action(1, model, s, a, r, c, t, epi_cost)\n\n            obs_next, reward, terminated, truncated, info = env.step(act)\n            if self.cost_reverse:\n                cost = (1.0 - info[\"cost\"]) * self.cost_scale\n            else:\n                cost = info[\"cost\"] * self.cost_scale\n            # at step t, we predict a_t, get s_{t + 1}, r_{t + 1}\n            actions[:, step] = torch.as_tensor(act)\n            states[:, step + 1] = torch.as_tensor(obs_next)\n            returns[:, step + 1] = torch.as_tensor(returns[:, step] - reward)\n            costs[:, step + 1] = torch.as_tensor(costs[:, step] - cost)\n\n            obs = obs_next\n\n            episode_ret += reward\n            episode_len += 1\n            episode_cost += info[\"cost\"]\n\n            if terminated or truncated:\n                break\n\n        return episode_ret, episode_len, episode_cost\n\n    def get_ensemble_action(self, size: int, model, s, a, r, c, t, epi_cost):\n        # [size, seq_len, state_dim]\n        s = torch.repeat_interleave(s, size, 0)\n        # [size, seq_len, act_dim]\n        a = torch.repeat_interleave(a, size, 0)\n        # [size, seq_len]\n        r = torch.repeat_interleave(r, size, 0)\n        c = torch.repeat_interleave(c, size, 0)\n        t = torch.repeat_interleave(t, size, 0)\n        epi_cost = torch.repeat_interleave(epi_cost, size, 0)\n\n        acts, _, _ = model(s, a, r, c, t, None, epi_cost)\n        if self.stochastic:\n            acts = acts.mean\n\n        # [size, seq_len, act_dim]\n        acts = torch.mean(acts, dim=0, keepdim=True)\n        acts = acts.clamp(-self.max_action, self.max_action)\n        act = acts[0, -1].cpu().numpy()\n        return act\n\n    def collect_random_rollouts(self, num_rollouts):\n        episode_rets = []\n        for _ in range(num_rollouts):\n            obs, info = self.env.reset()\n            episode_ret = 0.0\n            for step in range(self.model.episode_len):\n                act = self.env.action_space.sample()\n                obs_next, reward, terminated, truncated, info = self.env.step(act)\n                obs = obs_next\n                episode_ret += reward\n                if terminated or truncated:\n                    break\n            episode_rets.append(episode_ret)\n        return np.mean(episode_rets)", ""]}
{"filename": "osrl/algorithms/bc.py", "chunked_list": ["import gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fsrl.utils import DummyLogger, WandbLogger\nfrom tqdm.auto import trange  # noqa\n\nfrom osrl.common.net import MLPActor\n", "from osrl.common.net import MLPActor\n\n\nclass BC(nn.Module):\n    \"\"\"\n    Behavior Cloning (BC)\n    \n    Args:\n        state_dim (int): dimension of the state space.\n        action_dim (int): dimension of the action space.\n        max_action (float): Maximum action value.\n        a_hidden_sizes (list, optional): List of integers specifying the sizes \n            of the layers in the actor network.\n        episode_len (int, optional): Maximum length of an episode.\n        device (str, optional): Device to run the model on (e.g. 'cpu' or 'cuda:0'). \n    \"\"\"\n\n    def __init__(self,\n                 state_dim: int,\n                 action_dim: int,\n                 max_action: float,\n                 a_hidden_sizes: list = [128, 128],\n                 episode_len: int = 300,\n                 device: str = \"cpu\"):\n\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.max_action = max_action\n        self.a_hidden_sizes = a_hidden_sizes\n        self.episode_len = episode_len\n        self.device = device\n\n        self.actor = MLPActor(self.state_dim, self.action_dim, self.a_hidden_sizes,\n                              nn.ReLU, self.max_action).to(self.device)\n\n    def actor_loss(self, observations, actions):\n        pred_actions = self.actor(observations)\n        loss_actor = F.mse_loss(pred_actions, actions)\n        self.actor_optim.zero_grad()\n        loss_actor.backward()\n        self.actor_optim.step()\n        stats_actor = {\"loss/actor_loss\": loss_actor.item()}\n        return loss_actor, stats_actor\n\n    def setup_optimizers(self, actor_lr):\n        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n\n    def act(self, obs):\n        '''\n        Given a single obs, return the action.\n        '''\n        obs = torch.tensor(obs[None, ...], dtype=torch.float32).to(self.device)\n        act = self.actor(obs)\n        act = act.data.numpy() if self.device == \"cpu\" else act.data.cpu().numpy()\n        return np.squeeze(act, axis=0)", "\n\nclass BCTrainer:\n    \"\"\"\n    Behavior Cloning Trainer\n    \n    Args:\n        model (BC): The BC model to be trained.\n        env (gym.Env): The OpenAI Gym environment to train the model in.\n        logger (WandbLogger or DummyLogger): The logger to use for tracking training progress.\n        actor_lr (float): learning rate for actor\n        bc_mode (str): specify bc mode\n        cost_limit (int): Upper limit on the cost per episode.\n        device (str): The device to use for training (e.g. \"cpu\" or \"cuda\").\n    \"\"\"\n\n    def __init__(\n            self,\n            model: BC,\n            env: gym.Env,\n            logger: WandbLogger = DummyLogger(),\n            # training params\n            actor_lr: float = 1e-4,\n            bc_mode: str = \"all\",\n            cost_limit: int = 10,\n            device=\"cpu\"):\n\n        self.model = model\n        self.logger = logger\n        self.env = env\n        self.device = device\n        self.bc_mode = bc_mode\n        self.cost_limit = cost_limit\n        self.model.setup_optimizers(actor_lr)\n\n    def set_target_cost(self, target_cost):\n        self.cost_limit = target_cost\n\n    def train_one_step(self, observations, actions):\n        \"\"\"\n        Trains the model by updating the actor.\n        \"\"\"\n        # update actor\n        loss_actor, stats_actor = self.model.actor_loss(observations, actions)\n        self.logger.store(**stats_actor)\n\n    def evaluate(self, eval_episodes):\n        \"\"\"\n        Evaluates the performance of the model on a number of episodes.\n        \"\"\"\n        self.model.eval()\n        episode_rets, episode_costs, episode_lens = [], [], []\n        for _ in trange(eval_episodes, desc=\"Evaluating...\", leave=False):\n            epi_ret, epi_len, epi_cost = self.rollout()\n            episode_rets.append(epi_ret)\n            episode_lens.append(epi_len)\n            episode_costs.append(epi_cost)\n        self.model.train()\n        return np.mean(episode_rets), np.mean(episode_costs), np.mean(episode_lens)\n\n    @torch.no_grad()\n    def rollout(self):\n        \"\"\"\n        Evaluates the performance of the model on a single episode.\n        \"\"\"\n        episode_ret, episode_cost, episode_len = 0.0, 0.0, 0\n        obs, info = self.env.reset()\n        if self.bc_mode == \"multi-task\":\n            obs = np.append(obs, self.cost_limit)\n        for _ in range(self.model.episode_len):\n            act = self.model.act(obs)\n            obs_next, reward, terminated, truncated, info = self.env.step(act)\n            if self.bc_mode == \"multi-task\":\n                obs_next = np.append(obs_next, self.cost_limit)\n            obs = obs_next\n            episode_ret += reward\n            episode_len += 1\n            episode_cost += info[\"cost\"]\n            if terminated or truncated:\n                break\n        return episode_ret, episode_len, episode_cost", ""]}
{"filename": "osrl/algorithms/bearl.py", "chunked_list": ["# reference: https://github.com/aviralkumar2907/BEAR\nfrom copy import deepcopy\n\nimport gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom fsrl.utils import DummyLogger, WandbLogger\nfrom tqdm.auto import trange  # noqa\n", "from tqdm.auto import trange  # noqa\n\nfrom osrl.common.net import (VAE, EnsembleDoubleQCritic, LagrangianPIDController,\n                             SquashedGaussianMLPActor)\n\n\nclass BEARL(nn.Module):\n    \"\"\"\n    Bootstrapping Error Accumulation Reduction with PID Lagrangian (BEARL)\n    \n    Args:\n        state_dim (int): dimension of the state space.\n        action_dim (int): dimension of the action space.\n        max_action (float): Maximum action value.\n        a_hidden_sizes (list): List of integers specifying the sizes \n            of the layers in the actor network.\n        c_hidden_sizes (list): List of integers specifying the sizes \n            of the layers in the critic network.\n        vae_hidden_sizes (int): Number of hidden units in the VAE. \n            sample_action_num (int): Number of action samples to draw. \n        gamma (float): Discount factor for the reward.\n        tau (float): Soft update coefficient for the target networks. \n        beta (float): Weight of the KL divergence term.            \n        lmbda (float): Weight of the Lagrangian term.\n        mmd_sigma (float): Width parameter for the Gaussian kernel used in the MMD loss.\n        target_mmd_thresh (float): Target threshold for the MMD loss.\n        num_samples_mmd_match (int): Number of samples to use in the MMD loss calculation.\n        PID (list): List of three floats containing the coefficients of the PID controller.\n        kernel (str): Kernel function to use in the MMD loss calculation.\n        num_q (int): Number of Q networks in the ensemble.\n        num_qc (int): Number of cost Q networks in the ensemble.\n        cost_limit (int): Upper limit on the cost per episode.\n        episode_len (int): Maximum length of an episode.\n        start_update_policy_step (int): Number of steps to wait before updating the policy.\n        device (str): Device to run the model on (e.g. 'cpu' or 'cuda:0'). \n    \"\"\"\n\n    def __init__(self,\n                 state_dim: int,\n                 action_dim: int,\n                 max_action: float,\n                 a_hidden_sizes: list = [128, 128],\n                 c_hidden_sizes: list = [128, 128],\n                 vae_hidden_sizes: int = 64,\n                 sample_action_num: int = 10,\n                 gamma: float = 0.99,\n                 tau: float = 0.005,\n                 beta: float = 0.5,\n                 lmbda: float = 0.75,\n                 mmd_sigma: float = 50,\n                 target_mmd_thresh: float = 0.05,\n                 num_samples_mmd_match: int = 10,\n                 PID: list = [0.1, 0.003, 0.001],\n                 kernel: str = \"gaussian\",\n                 num_q: int = 1,\n                 num_qc: int = 1,\n                 cost_limit: int = 10,\n                 episode_len: int = 300,\n                 start_update_policy_step: int = 20_000,\n                 device: str = \"cpu\"):\n\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.latent_dim = self.action_dim * 2\n        self.max_action = max_action\n        self.a_hidden_sizes = a_hidden_sizes\n        self.c_hidden_sizes = c_hidden_sizes\n        self.vae_hidden_sizes = vae_hidden_sizes\n        self.sample_action_num = sample_action_num\n        self.gamma = gamma\n        self.tau = tau\n        self.beta = beta\n        self.lmbda = lmbda\n        self.mmd_sigma = mmd_sigma\n        self.target_mmd_thresh = target_mmd_thresh\n        self.num_samples_mmd_match = num_samples_mmd_match\n        self.start_update_policy_step = start_update_policy_step\n        self.KP, self.KI, self.KD = PID\n        self.kernel = kernel\n        self.num_q = num_q\n        self.num_qc = num_qc\n        self.cost_limit = cost_limit\n        self.episode_len = episode_len\n        self.device = device\n        self.n_train_steps = 0\n\n        ################ create actor critic model ###############\n        self.actor = SquashedGaussianMLPActor(self.state_dim, self.action_dim,\n                                              self.a_hidden_sizes,\n                                              nn.ReLU).to(self.device)\n        self.critic = EnsembleDoubleQCritic(self.state_dim,\n                                            self.action_dim,\n                                            self.c_hidden_sizes,\n                                            nn.ReLU,\n                                            num_q=self.num_q).to(self.device)\n        self.cost_critic = EnsembleDoubleQCritic(self.state_dim,\n                                                 self.action_dim,\n                                                 self.c_hidden_sizes,\n                                                 nn.ReLU,\n                                                 num_q=self.num_qc).to(self.device)\n        self.vae = VAE(self.state_dim, self.action_dim, self.vae_hidden_sizes,\n                       self.latent_dim, self.max_action, self.device).to(self.device)\n        self.log_alpha = torch.tensor(0.0, device=self.device)\n\n        self.actor_old = deepcopy(self.actor)\n        self.actor_old.eval()\n        self.critic_old = deepcopy(self.critic)\n        self.critic_old.eval()\n        self.cost_critic_old = deepcopy(self.cost_critic)\n        self.cost_critic_old.eval()\n\n        self.qc_thres = cost_limit * (1 - self.gamma**self.episode_len) / (\n            1 - self.gamma) / self.episode_len\n        self.controller = LagrangianPIDController(self.KP, self.KI, self.KD,\n                                                  self.qc_thres)\n\n    def _soft_update(self, tgt: nn.Module, src: nn.Module, tau: float) -> None:\n        \"\"\"\n        Softly update the parameters of target module towards the parameters\n        of source module.\n        \"\"\"\n        for tgt_param, src_param in zip(tgt.parameters(), src.parameters()):\n            tgt_param.data.copy_(tau * src_param.data + (1 - tau) * tgt_param.data)\n\n    def _actor_forward(self,\n                       obs: torch.tensor,\n                       deterministic: bool = False,\n                       with_logprob: bool = True):\n        \"\"\"\n        Return action distribution and action log prob [optional].\n        \"\"\"\n        a, logp = self.actor(obs, deterministic, with_logprob)\n        return a * self.max_action, logp\n\n    def vae_loss(self, observations, actions):\n        recon, mean, std = self.vae(observations, actions)\n        recon_loss = nn.functional.mse_loss(recon, actions)\n        KL_loss = -0.5 * (1 + torch.log(std.pow(2)) - mean.pow(2) - std.pow(2)).mean()\n        loss_vae = recon_loss + self.beta * KL_loss\n\n        self.vae_optim.zero_grad()\n        loss_vae.backward()\n        self.vae_optim.step()\n        stats_vae = {\"loss/loss_vae\": loss_vae.item()}\n        return loss_vae, stats_vae\n\n    def critic_loss(self, observations, next_observations, actions, rewards, done):\n        _, _, q1_list, q2_list = self.critic.predict(observations, actions)\n        with torch.no_grad():\n            batch_size = next_observations.shape[0]\n            obs_next = torch.repeat_interleave(next_observations, self.sample_action_num,\n                                               0).to(self.device)\n\n            act_targ_next, _ = self.actor_old(obs_next, False, True, False)\n            q1_targ, q2_targ, _, _ = self.critic_old.predict(obs_next, act_targ_next)\n\n            q_targ = self.lmbda * torch.min(\n                q1_targ, q2_targ) + (1. - self.lmbda) * torch.max(q1_targ, q2_targ)\n            q_targ = q_targ.reshape(batch_size, -1).max(1)[0]\n\n            backup = rewards + self.gamma * (1 - done) * q_targ\n\n        loss_critic = self.critic.loss(backup, q1_list) + self.critic.loss(\n            backup, q2_list)\n        self.critic_optim.zero_grad()\n        loss_critic.backward()\n        self.critic_optim.step()\n\n        stats_critic = {\"loss/critic_loss\": loss_critic.item()}\n        return loss_critic, stats_critic\n\n    def cost_critic_loss(self, observations, next_observations, actions, costs, done):\n        _, _, qc1_list, qc2_list = self.cost_critic.predict(observations, actions)\n        with torch.no_grad():\n            batch_size = next_observations.shape[0]\n            obs_next = torch.repeat_interleave(next_observations, self.sample_action_num,\n                                               0).to(self.device)\n\n            act_targ_next, _ = self.actor_old(obs_next, False, True, False)\n            qc1_targ, qc2_targ, _, _ = self.cost_critic_old.predict(\n                obs_next, act_targ_next)\n\n            qc_targ = self.lmbda * torch.min(\n                qc1_targ, qc2_targ) + (1. - self.lmbda) * torch.max(qc1_targ, qc2_targ)\n            qc_targ = qc_targ.reshape(batch_size, -1).max(1)[0]\n\n            backup = costs + self.gamma * qc_targ\n\n        loss_cost_critic = self.cost_critic.loss(\n            backup, qc1_list) + self.cost_critic.loss(backup, qc2_list)\n\n        self.cost_critic_optim.zero_grad()\n        loss_cost_critic.backward()\n        self.cost_critic_optim.step()\n\n        stats_cost_critic = {\"loss/cost_critic_loss\": loss_cost_critic.item()}\n        return loss_cost_critic, stats_cost_critic\n\n    def actor_loss(self, observations):\n        for p in self.critic.parameters():\n            p.requires_grad = False\n        for p in self.cost_critic.parameters():\n            p.requires_grad = False\n        for p in self.vae.parameters():\n            p.requires_grad = False\n\n        _, raw_sampled_actions = self.vae.decode_multiple(\n            observations, num_decode=self.num_samples_mmd_match)\n\n        batch_size = observations.shape[0]\n        stacked_obs = torch.repeat_interleave(\n            observations, self.num_samples_mmd_match,\n            0)  # [batch_size*num_samples_mmd_match, obs_dim]\n        actor_samples, raw_actor_actions = self.actor(stacked_obs,\n                                                      return_pretanh_value=True)\n        actor_samples = actor_samples.reshape(batch_size, self.num_samples_mmd_match,\n                                              self.action_dim)\n        raw_actor_actions = raw_actor_actions.view(batch_size,\n                                                   self.num_samples_mmd_match,\n                                                   self.action_dim)\n\n        if self.kernel == 'laplacian':\n            mmd_loss = self.mmd_loss_laplacian(raw_sampled_actions,\n                                               raw_actor_actions,\n                                               sigma=self.mmd_sigma)\n        elif self.kernel == 'gaussian':\n            mmd_loss = self.mmd_loss_gaussian(raw_sampled_actions,\n                                              raw_actor_actions,\n                                              sigma=self.mmd_sigma)\n\n        q_val1, q_val2, _, _ = self.critic.predict(observations, actor_samples[:, 0, :])\n        qc_val1, qc_val2, _, _ = self.cost_critic.predict(observations,\n                                                          actor_samples[:, 0, :])\n        qc_val = torch.min(qc_val1, qc_val2)\n        with torch.no_grad():\n            multiplier = self.controller.control(qc_val).detach()\n        qc_penalty = ((qc_val - self.qc_thres) * multiplier).mean()\n        q_val = torch.min(q_val1, q_val2)\n\n        if self.n_train_steps >= self.start_update_policy_step:\n            loss_actor = (-q_val + self.log_alpha.exp() *\n                          (mmd_loss - self.target_mmd_thresh)).mean()\n        else:\n            loss_actor = (self.log_alpha.exp() *\n                          (mmd_loss - self.target_mmd_thresh)).mean()\n        loss_actor += qc_penalty\n\n        self.actor_optim.zero_grad()\n        loss_actor.backward()\n        self.actor_optim.step()\n\n        self.log_alpha += self.alpha_lr * self.log_alpha.exp() * (\n            mmd_loss - self.target_mmd_thresh).mean().detach()\n        self.log_alpha.data.clamp_(min=-5.0, max=5.0)\n        self.n_train_steps += 1\n\n        stats_actor = {\n            \"loss/actor_loss\": loss_actor.item(),\n            \"loss/mmd_loss\": mmd_loss.mean().item(),\n            \"loss/qc_penalty\": qc_penalty.item(),\n            \"loss/lagrangian\": multiplier.item(),\n            \"loss/alpha_value\": self.log_alpha.exp().item()\n        }\n\n        for p in self.critic.parameters():\n            p.requires_grad = True\n        for p in self.cost_critic.parameters():\n            p.requires_grad = True\n        for p in self.vae.parameters():\n            p.requires_grad = True\n        return loss_actor, stats_actor\n\n    # from https://github.com/Farama-Foundation/D4RL-Evaluations\n    def mmd_loss_laplacian(self, samples1, samples2, sigma=0.2):\n        \"\"\"MMD constraint with Laplacian kernel for support matching\"\"\"\n        # sigma is set to 20.0 for hopper, cheetah and 50 for walker/ant\n        diff_x_x = samples1.unsqueeze(2) - samples1.unsqueeze(1)  # B x N x N x d\n        diff_x_x = torch.mean((-(diff_x_x.abs()).sum(-1) / (2.0 * sigma)).exp(),\n                              dim=(1, 2))\n\n        diff_x_y = samples1.unsqueeze(2) - samples2.unsqueeze(1)\n        diff_x_y = torch.mean((-(diff_x_y.abs()).sum(-1) / (2.0 * sigma)).exp(),\n                              dim=(1, 2))\n\n        diff_y_y = samples2.unsqueeze(2) - samples2.unsqueeze(1)  # B x N x N x d\n        diff_y_y = torch.mean((-(diff_y_y.abs()).sum(-1) / (2.0 * sigma)).exp(),\n                              dim=(1, 2))\n\n        overall_loss = (diff_x_x + diff_y_y - 2.0 * diff_x_y + 1e-6).sqrt()\n        return overall_loss\n\n    # from https://github.com/Farama-Foundation/D4RL-Evaluations\n    def mmd_loss_gaussian(self, samples1, samples2, sigma=0.2):\n        \"\"\"MMD constraint with Gaussian Kernel support matching\"\"\"\n        # sigma is set to 20.0 for hopper, cheetah and 50 for walker/ant\n        diff_x_x = samples1.unsqueeze(2) - samples1.unsqueeze(1)  # B x N x N x d\n        diff_x_x = torch.mean((-(diff_x_x.pow(2)).sum(-1) / (2.0 * sigma)).exp(),\n                              dim=(1, 2))\n\n        diff_x_y = samples1.unsqueeze(2) - samples2.unsqueeze(1)\n        diff_x_y = torch.mean((-(diff_x_y.pow(2)).sum(-1) / (2.0 * sigma)).exp(),\n                              dim=(1, 2))\n\n        diff_y_y = samples2.unsqueeze(2) - samples2.unsqueeze(1)  # B x N x N x d\n        diff_y_y = torch.mean((-(diff_y_y.pow(2)).sum(-1) / (2.0 * sigma)).exp(),\n                              dim=(1, 2))\n\n        overall_loss = (diff_x_x + diff_y_y - 2.0 * diff_x_y + 1e-6).sqrt()\n        return overall_loss\n\n    def setup_optimizers(self, actor_lr, critic_lr, vae_lr, alpha_lr):\n        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n        self.cost_critic_optim = torch.optim.Adam(self.cost_critic.parameters(),\n                                                  lr=critic_lr)\n        self.vae_optim = torch.optim.Adam(self.vae.parameters(), lr=vae_lr)\n        # self.alpha_optim = torch.optim.Adam([self.log_alpha], lr=alpha_lr)\n        self.alpha_lr = alpha_lr\n\n    def sync_weight(self):\n        \"\"\"\n        Soft-update the weight for the target network.\n        \"\"\"\n        self._soft_update(self.critic_old, self.critic, self.tau)\n        self._soft_update(self.cost_critic_old, self.cost_critic, self.tau)\n        self._soft_update(self.actor_old, self.actor, self.tau)\n\n    def act(self,\n            obs: np.ndarray,\n            deterministic: bool = False,\n            with_logprob: bool = False):\n        \"\"\"\n        Given a single obs, return the action, logp.\n        \"\"\"\n        obs = torch.tensor(obs[None, ...], dtype=torch.float32).to(self.device)\n        a, logp_a = self._actor_forward(obs, deterministic, with_logprob)\n        a = a.data.numpy() if self.device == \"cpu\" else a.data.cpu().numpy()\n        logp_a = logp_a.data.numpy() if self.device == \"cpu\" else logp_a.data.cpu(\n        ).numpy()\n        return np.squeeze(a, axis=0), np.squeeze(logp_a)", "\n\nclass BEARLTrainer:\n    \"\"\"\n    BEARL Trainer\n    \n    Args:\n        model (BEARL): The BEARL model to be trained.\n        env (gym.Env): The OpenAI Gym environment to train the model in.\n        logger (WandbLogger or DummyLogger): The logger to use for tracking training progress.\n        actor_lr (float): learning rate for actor\n        critic_lr (float): learning rate for critic\n        alpha_lr (float): learning rate for alpha\n        vae_lr (float): learning rate for vae\n        reward_scale (float): The scaling factor for the reward signal.\n        cost_scale (float): The scaling factor for the constraint cost.\n        device (str): The device to use for training (e.g. \"cpu\" or \"cuda\").\n    \"\"\"\n\n    def __init__(self,\n                 model: BEARL,\n                 env: gym.Env,\n                 logger: WandbLogger = DummyLogger(),\n                 actor_lr: float = 1e-3,\n                 critic_lr: float = 1e-3,\n                 alpha_lr: float = 1e-3,\n                 vae_lr: float = 1e-3,\n                 reward_scale: float = 1.0,\n                 cost_scale: float = 1.0,\n                 device=\"cpu\"):\n\n        self.model = model\n        self.logger = logger\n        self.env = env\n        self.reward_scale = reward_scale\n        self.cost_scale = cost_scale\n        self.device = device\n        self.model.setup_optimizers(actor_lr, critic_lr, vae_lr, alpha_lr)\n\n    def train_one_step(self, observations, next_observations, actions, rewards, costs,\n                       done):\n        \"\"\"\n        Trains the model by updating the VAE, critic, cost critic, and actor.\n        \"\"\"\n\n        # update VAE\n        loss_vae, stats_vae = self.model.vae_loss(observations, actions)\n        # update critic\n        loss_critic, stats_critic = self.model.critic_loss(observations,\n                                                           next_observations, actions,\n                                                           rewards, done)\n        # update cost critic\n        loss_cost_critic, stats_cost_critic = self.model.cost_critic_loss(\n            observations, next_observations, actions, costs, done)\n        # update actor\n        loss_actor, stats_actor = self.model.actor_loss(observations)\n\n        self.model.sync_weight()\n\n        self.logger.store(**stats_vae)\n        self.logger.store(**stats_critic)\n        self.logger.store(**stats_cost_critic)\n        self.logger.store(**stats_actor)\n\n    def evaluate(self, eval_episodes):\n        \"\"\"\n        Evaluates the performance of the model on a number of episodes.\n        \"\"\"\n        self.model.eval()\n        episode_rets, episode_costs, episode_lens = [], [], []\n        for _ in trange(eval_episodes, desc=\"Evaluating...\", leave=False):\n            epi_ret, epi_len, epi_cost = self.rollout()\n            episode_rets.append(epi_ret)\n            episode_lens.append(epi_len)\n            episode_costs.append(epi_cost)\n        self.model.train()\n        return np.mean(episode_rets) / self.reward_scale, np.mean(\n            episode_costs) / self.cost_scale, np.mean(episode_lens)\n\n    @torch.no_grad()\n    def rollout(self):\n        \"\"\"\n        Evaluates the performance of the model on a single episode.\n        \"\"\"\n        obs, info = self.env.reset()\n        episode_ret, episode_cost, episode_len = 0.0, 0.0, 0\n        for _ in range(self.model.episode_len):\n            act, _ = self.model.act(obs, True, True)\n            obs_next, reward, terminated, truncated, info = self.env.step(act)\n            cost = info[\"cost\"] * self.cost_scale\n            obs = obs_next\n            episode_ret += reward\n            episode_len += 1\n            episode_cost += cost\n            if terminated or truncated:\n                break\n        return episode_ret, episode_len, episode_cost", ""]}
{"filename": "examples/__init__.py", "chunked_list": [""]}
{"filename": "examples/train_all_tasks.py", "chunked_list": ["from easy_runner import EasyRunner\n\nif __name__ == \"__main__\":\n\n    exp_name = \"benchmark\"\n    runner = EasyRunner(log_name=exp_name)\n\n    task = [\n        # bullet safety gym envs\n        \"OfflineAntCircle-v0\",\n        \"OfflineAntRun-v0\",\n        \"OfflineCarCircle-v0\",\n        \"OfflineDroneCircle-v0\",\n        \"OfflineDroneRun-v0\",\n        \"OfflineBallCircle-v0\",\n        \"OfflineBallRun-v0\",\n        \"OfflineCarRun-v0\",\n        # safety gymnasium: car\n        \"OfflineCarButton1Gymnasium-v0\",\n        \"OfflineCarButton2Gymnasium-v0\",\n        \"OfflineCarCircle1Gymnasium-v0\",\n        \"OfflineCarCircle2Gymnasium-v0\",\n        \"OfflineCarGoal1Gymnasium-v0\",\n        \"OfflineCarGoal2Gymnasium-v0\",\n        \"OfflineCarPush1Gymnasium-v0\",\n        \"OfflineCarPush2Gymnasium-v0\",\n        # safety gymnasium: point\n        \"OfflinePointButton1Gymnasium-v0\",\n        \"OfflinePointButton2Gymnasium-v0\",\n        \"OfflinePointCircle1Gymnasium-v0\",\n        \"OfflinePointCircle2Gymnasium-v0\",\n        \"OfflinePointGoal1Gymnasium-v0\",\n        \"OfflinePointGoal2Gymnasium-v0\",\n        \"OfflinePointPush1Gymnasium-v0\",\n        \"OfflinePointPush2Gymnasium-v0\",\n        # safety gymnasium: velocity\n        \"OfflineAntVelocityGymnasium-v1\",\n        \"OfflineHalfCheetahVelocityGymnasium-v1\",\n        \"OfflineHopperVelocityGymnasium-v1\",\n        \"OfflineSwimmerVelocityGymnasium-v1\",\n        \"OfflineWalker2dVelocityGymnasium-v1\",\n        # metadrive envs\n        \"OfflineMetadrive-easysparse-v0\",\n        \"OfflineMetadrive-easymean-v0\",\n        \"OfflineMetadrive-easydense-v0\",\n        \"OfflineMetadrive-mediumsparse-v0\",\n        \"OfflineMetadrive-mediummean-v0\",\n        \"OfflineMetadrive-mediumdense-v0\",\n        \"OfflineMetadrive-hardsparse-v0\",\n        \"OfflineMetadrive-hardmean-v0\",\n        \"OfflineMetadrive-harddense-v0\",\n    ]\n\n    policy = [\"train_bc\", \"train_bcql\", \"train_bearl\", \"train_coptidice\", \"train_cpq\"]\n\n    # Do not write & to the end of the command, it will be added automatically.\n    template = \"nohup python examples/train/{}.py --task {} --device cpu\"\n\n    train_instructions = runner.compose(template, [policy, task])\n    runner.start(train_instructions, max_parallel=15)", ""]}
{"filename": "examples/configs/coptidice_configs.py", "chunked_list": ["from dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nfrom pyrallis import field\n\n\n@dataclass\nclass COptiDICETrainConfig:\n    # wandb params\n    project: str = \"OSRL-baselines\"\n    group: str = None\n    name: Optional[str] = None\n    prefix: Optional[str] = \"COptiDICE\"\n    suffix: Optional[str] = \"\"\n    logdir: Optional[str] = \"logs\"\n    verbose: bool = True\n    # dataset params\n    outliers_percent: float = None\n    noise_scale: float = None\n    inpaint_ranges: Tuple[Tuple[float, float, float, float], ...] = None\n    epsilon: float = None\n    density: float = 1.0\n    # training params\n    task: str = \"OfflineCarCircle-v0\"\n    dataset: str = None\n    seed: int = 0\n    device: str = \"cpu\"\n    threads: int = 4\n    reward_scale: float = 0.1\n    cost_scale: float = 1\n    actor_lr: float = 0.0001\n    critic_lr: float = 0.0001\n    scalar_lr: float = 0.0001\n    cost_limit: int = 10\n    episode_len: int = 300\n    batch_size: int = 512\n    update_steps: int = 100_000\n    num_workers: int = 8\n    # model params\n    a_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n    c_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n    alpha: float = 0.5\n    gamma: float = 0.99\n    cost_ub_epsilon: float = 0.01\n    f_type: str = \"softchi\"\n    num_nu: int = 2\n    num_chi: int = 2\n    # evaluation params\n    eval_episodes: int = 10\n    eval_every: int = 2500", "\n\n@dataclass\nclass COptiDICECarCircleConfig(COptiDICETrainConfig):\n    pass\n\n\n@dataclass\nclass COptiDICEAntRunConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineAntRun-v0\"\n    episode_len: int = 200", "class COptiDICEAntRunConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineAntRun-v0\"\n    episode_len: int = 200\n\n\n@dataclass\nclass COptiDICEDroneRunConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineDroneRun-v0\"\n    episode_len: int = 200", "\n\n@dataclass\nclass COptiDICEDroneCircleConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineDroneCircle-v0\"\n    episode_len: int = 300\n\n\n@dataclass\nclass COptiDICECarRunConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarRun-v0\"\n    episode_len: int = 200", "\n@dataclass\nclass COptiDICECarRunConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarRun-v0\"\n    episode_len: int = 200\n\n\n@dataclass\nclass COptiDICEAntCircleConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineAntCircle-v0\"\n    episode_len: int = 500", "@dataclass\nclass COptiDICEAntCircleConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineAntCircle-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass COptiDICEBallRunConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineBallRun-v0\"\n    episode_len: int = 100", "class COptiDICEBallRunConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineBallRun-v0\"\n    episode_len: int = 100\n\n\n@dataclass\nclass COptiDICEBallCircleConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineBallCircle-v0\"\n    episode_len: int = 200", "\n\n@dataclass\nclass COptiDICECarButton1Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarButton1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICECarButton2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarButton2Gymnasium-v0\"\n    episode_len: int = 1000", "\n@dataclass\nclass COptiDICECarButton2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarButton2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICECarCircle1Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n    episode_len: int = 500", "@dataclass\nclass COptiDICECarCircle1Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass COptiDICECarCircle2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n    episode_len: int = 500", "class COptiDICECarCircle2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass COptiDICECarGoal1Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarGoal1Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass COptiDICECarGoal2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarGoal2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICECarPush1Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarPush1Gymnasium-v0\"\n    episode_len: int = 1000", "\n@dataclass\nclass COptiDICECarPush1Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarPush1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICECarPush2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarPush2Gymnasium-v0\"\n    episode_len: int = 1000", "@dataclass\nclass COptiDICECarPush2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineCarPush2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICEPointButton1Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflinePointButton1Gymnasium-v0\"\n    episode_len: int = 1000", "class COptiDICEPointButton1Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflinePointButton1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICEPointButton2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflinePointButton2Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass COptiDICEPointCircle1Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle1Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass COptiDICEPointCircle2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n    episode_len: int = 500", "\n@dataclass\nclass COptiDICEPointCircle2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass COptiDICEPointGoal1Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n    episode_len: int = 1000", "@dataclass\nclass COptiDICEPointGoal1Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICEPointGoal2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n    episode_len: int = 1000", "class COptiDICEPointGoal2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICEPointPush1Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflinePointPush1Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass COptiDICEPointPush2Config(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflinePointPush2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICEAntVelocityConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineAntVelocityGymnasium-v1\"\n    episode_len: int = 1000", "\n@dataclass\nclass COptiDICEAntVelocityConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineAntVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICEHalfCheetahVelocityConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n    episode_len: int = 1000", "@dataclass\nclass COptiDICEHalfCheetahVelocityConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICEHopperVelocityConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n    episode_len: int = 1000", "class COptiDICEHopperVelocityConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICESwimmerVelocityConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineSwimmerVelocityGymnasium-v1\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass COptiDICEWalker2dVelocityConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineWalker2dVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass COptiDICEEasySparseConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easysparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass COptiDICEEasySparseConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easysparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass COptiDICEEasyMeanConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easymean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass COptiDICEEasyMeanConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easymean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass COptiDICEEasyDenseConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easydense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass COptiDICEEasyDenseConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easydense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass COptiDICEMediumSparseConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass COptiDICEMediumSparseConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass COptiDICEMediumMeanConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediummean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass COptiDICEMediumMeanConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediummean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass COptiDICEMediumDenseConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumdense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass COptiDICEMediumDenseConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumdense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass COptiDICEHardSparseConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass COptiDICEHardSparseConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass COptiDICEHardMeanConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardmean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass COptiDICEHardMeanConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardmean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass COptiDICEHardDenseConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-harddense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass COptiDICEHardDenseConfig(COptiDICETrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-harddense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\nCOptiDICE_DEFAULT_CONFIG = {", "\nCOptiDICE_DEFAULT_CONFIG = {\n    # bullet_safety_gym\n    \"OfflineCarCircle-v0\": COptiDICECarCircleConfig,\n    \"OfflineAntRun-v0\": COptiDICEAntRunConfig,\n    \"OfflineDroneRun-v0\": COptiDICEDroneRunConfig,\n    \"OfflineDroneCircle-v0\": COptiDICEDroneCircleConfig,\n    \"OfflineCarRun-v0\": COptiDICECarRunConfig,\n    \"OfflineAntCircle-v0\": COptiDICEAntCircleConfig,\n    \"OfflineBallCircle-v0\": COptiDICEBallCircleConfig,", "    \"OfflineAntCircle-v0\": COptiDICEAntCircleConfig,\n    \"OfflineBallCircle-v0\": COptiDICEBallCircleConfig,\n    \"OfflineBallRun-v0\": COptiDICEBallRunConfig,\n    # safety_gymnasium\n    \"OfflineCarButton1Gymnasium-v0\": COptiDICECarButton1Config,\n    \"OfflineCarButton2Gymnasium-v0\": COptiDICECarButton2Config,\n    \"OfflineCarCircle1Gymnasium-v0\": COptiDICECarCircle1Config,\n    \"OfflineCarCircle2Gymnasium-v0\": COptiDICECarCircle2Config,\n    \"OfflineCarGoal1Gymnasium-v0\": COptiDICECarGoal1Config,\n    \"OfflineCarGoal2Gymnasium-v0\": COptiDICECarGoal2Config,", "    \"OfflineCarGoal1Gymnasium-v0\": COptiDICECarGoal1Config,\n    \"OfflineCarGoal2Gymnasium-v0\": COptiDICECarGoal2Config,\n    \"OfflineCarPush1Gymnasium-v0\": COptiDICECarPush1Config,\n    \"OfflineCarPush2Gymnasium-v0\": COptiDICECarPush2Config,\n    # safety_gymnasium: point\n    \"OfflinePointButton1Gymnasium-v0\": COptiDICEPointButton1Config,\n    \"OfflinePointButton2Gymnasium-v0\": COptiDICEPointButton2Config,\n    \"OfflinePointCircle1Gymnasium-v0\": COptiDICEPointCircle1Config,\n    \"OfflinePointCircle2Gymnasium-v0\": COptiDICEPointCircle2Config,\n    \"OfflinePointGoal1Gymnasium-v0\": COptiDICEPointGoal1Config,", "    \"OfflinePointCircle2Gymnasium-v0\": COptiDICEPointCircle2Config,\n    \"OfflinePointGoal1Gymnasium-v0\": COptiDICEPointGoal1Config,\n    \"OfflinePointGoal2Gymnasium-v0\": COptiDICEPointGoal2Config,\n    \"OfflinePointPush1Gymnasium-v0\": COptiDICEPointPush1Config,\n    \"OfflinePointPush2Gymnasium-v0\": COptiDICEPointPush2Config,\n    # safety_gymnasium: velocity\n    \"OfflineAntVelocityGymnasium-v1\": COptiDICEAntVelocityConfig,\n    \"OfflineHalfCheetahVelocityGymnasium-v1\": COptiDICEHalfCheetahVelocityConfig,\n    \"OfflineHopperVelocityGymnasium-v1\": COptiDICEHopperVelocityConfig,\n    \"OfflineSwimmerVelocityGymnasium-v1\": COptiDICESwimmerVelocityConfig,", "    \"OfflineHopperVelocityGymnasium-v1\": COptiDICEHopperVelocityConfig,\n    \"OfflineSwimmerVelocityGymnasium-v1\": COptiDICESwimmerVelocityConfig,\n    \"OfflineWalker2dVelocityGymnasium-v1\": COptiDICEWalker2dVelocityConfig,\n    # safe_metadrive\n    \"OfflineMetadrive-easysparse-v0\": COptiDICEEasySparseConfig,\n    \"OfflineMetadrive-easymean-v0\": COptiDICEEasyMeanConfig,\n    \"OfflineMetadrive-easydense-v0\": COptiDICEEasyDenseConfig,\n    \"OfflineMetadrive-mediumsparse-v0\": COptiDICEMediumSparseConfig,\n    \"OfflineMetadrive-mediummean-v0\": COptiDICEMediumMeanConfig,\n    \"OfflineMetadrive-mediumdense-v0\": COptiDICEMediumDenseConfig,", "    \"OfflineMetadrive-mediummean-v0\": COptiDICEMediumMeanConfig,\n    \"OfflineMetadrive-mediumdense-v0\": COptiDICEMediumDenseConfig,\n    \"OfflineMetadrive-hardsparse-v0\": COptiDICEHardSparseConfig,\n    \"OfflineMetadrive-hardmean-v0\": COptiDICEHardMeanConfig,\n    \"OfflineMetadrive-harddense-v0\": COptiDICEHardDenseConfig\n}\n"]}
{"filename": "examples/configs/bc_configs.py", "chunked_list": ["from dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nfrom pyrallis import field\n\n\n@dataclass\nclass BCTrainConfig:\n    # wandb params\n    project: str = \"OSRL-baselines\"\n    group: str = None\n    name: Optional[str] = None\n    prefix: Optional[str] = \"BC\"\n    suffix: Optional[str] = \"\"\n    logdir: Optional[str] = \"logs\"\n    verbose: bool = True\n    # dataset params\n    outliers_percent: float = None\n    noise_scale: float = None\n    inpaint_ranges: Tuple[Tuple[float, float, float, float], ...] = None\n    epsilon: float = None\n    density: float = 1.0\n    # training params\n    task: str = \"OfflineCarCircle-v0\"\n    dataset: str = None\n    seed: int = 0\n    device: str = \"cpu\"\n    threads: int = 4\n    actor_lr: float = 0.001\n    cost_limit: int = 10\n    episode_len: int = 300\n    batch_size: int = 512\n    update_steps: int = 100_000\n    num_workers: int = 8\n    bc_mode: str = \"all\"  # \"all\", \"safe\", \"risky\", \"frontier\", \"boundary\", \"multi-task\"\n    # model params\n    a_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n    gamma: float = 1.0\n    # evaluation params\n    eval_episodes: int = 10\n    eval_every: int = 2500", "\n\n@dataclass\nclass BCCarCircleConfig(BCTrainConfig):\n    pass\n\n\n@dataclass\nclass BCAntRunConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineAntRun-v0\"\n    episode_len: int = 200", "class BCAntRunConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineAntRun-v0\"\n    episode_len: int = 200\n\n\n@dataclass\nclass BCDroneRunConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineDroneRun-v0\"\n    episode_len: int = 200", "\n\n@dataclass\nclass BCDroneCircleConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineDroneCircle-v0\"\n    episode_len: int = 300\n\n\n@dataclass\nclass BCCarRunConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarRun-v0\"\n    episode_len: int = 200", "\n@dataclass\nclass BCCarRunConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarRun-v0\"\n    episode_len: int = 200\n\n\n@dataclass\nclass BCAntCircleConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineAntCircle-v0\"\n    episode_len: int = 500", "@dataclass\nclass BCAntCircleConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineAntCircle-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BCBallRunConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineBallRun-v0\"\n    episode_len: int = 100", "class BCBallRunConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineBallRun-v0\"\n    episode_len: int = 100\n\n\n@dataclass\nclass BCBallCircleConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineBallCircle-v0\"\n    episode_len: int = 200", "\n\n@dataclass\nclass BCCarButton1Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarButton1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCCarButton2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarButton2Gymnasium-v0\"\n    episode_len: int = 1000", "\n@dataclass\nclass BCCarButton2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarButton2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCCarCircle1Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n    episode_len: int = 500", "@dataclass\nclass BCCarCircle1Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BCCarCircle2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n    episode_len: int = 500", "class BCCarCircle2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BCCarGoal1Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarGoal1Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass BCCarGoal2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarGoal2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCCarPush1Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush1Gymnasium-v0\"\n    episode_len: int = 1000", "\n@dataclass\nclass BCCarPush1Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCCarPush2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush2Gymnasium-v0\"\n    episode_len: int = 1000", "@dataclass\nclass BCCarPush2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCPointButton1Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflinePointButton1Gymnasium-v0\"\n    episode_len: int = 1000", "class BCPointButton1Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflinePointButton1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCPointButton2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflinePointButton2Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass BCPointCircle1Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle1Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BCPointCircle2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n    episode_len: int = 500", "\n@dataclass\nclass BCPointCircle2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BCPointGoal1Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n    episode_len: int = 1000", "@dataclass\nclass BCPointGoal1Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCPointGoal2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n    episode_len: int = 1000", "class BCPointGoal2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCPointPush1Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflinePointPush1Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass BCPointPush2Config(BCTrainConfig):\n    # training params\n    task: str = \"OfflinePointPush2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCAntVelocityConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineAntVelocityGymnasium-v1\"\n    episode_len: int = 1000", "\n@dataclass\nclass BCAntVelocityConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineAntVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCHalfCheetahVelocityConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n    episode_len: int = 1000", "@dataclass\nclass BCHalfCheetahVelocityConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCHopperVelocityConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n    episode_len: int = 1000", "class BCHopperVelocityConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCSwimmerVelocityConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineSwimmerVelocityGymnasium-v1\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass BCWalker2dVelocityConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineWalker2dVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCEasySparseConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easysparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCEasySparseConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easysparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCEasyMeanConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easymean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCEasyMeanConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easymean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCEasyDenseConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easydense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCEasyDenseConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easydense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCMediumSparseConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCMediumSparseConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCMediumMeanConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediummean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCMediumMeanConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediummean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCMediumDenseConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumdense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCMediumDenseConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumdense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCHardSparseConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCHardSparseConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCHardMeanConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardmean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCHardMeanConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardmean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCHardDenseConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-harddense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCHardDenseConfig(BCTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-harddense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\nBC_DEFAULT_CONFIG = {", "\nBC_DEFAULT_CONFIG = {\n    # bullet_safety_gym\n    \"OfflineCarCircle-v0\": BCCarCircleConfig,\n    \"OfflineAntRun-v0\": BCAntRunConfig,\n    \"OfflineDroneRun-v0\": BCDroneRunConfig,\n    \"OfflineDroneCircle-v0\": BCDroneCircleConfig,\n    \"OfflineCarRun-v0\": BCCarRunConfig,\n    \"OfflineAntCircle-v0\": BCAntCircleConfig,\n    \"OfflineBallCircle-v0\": BCBallCircleConfig,", "    \"OfflineAntCircle-v0\": BCAntCircleConfig,\n    \"OfflineBallCircle-v0\": BCBallCircleConfig,\n    \"OfflineBallRun-v0\": BCBallRunConfig,\n    # safety_gymnasium: car\n    \"OfflineCarButton1Gymnasium-v0\": BCCarButton1Config,\n    \"OfflineCarButton2Gymnasium-v0\": BCCarButton2Config,\n    \"OfflineCarCircle1Gymnasium-v0\": BCCarCircle1Config,\n    \"OfflineCarCircle2Gymnasium-v0\": BCCarCircle2Config,\n    \"OfflineCarGoal1Gymnasium-v0\": BCCarGoal1Config,\n    \"OfflineCarGoal2Gymnasium-v0\": BCCarGoal2Config,", "    \"OfflineCarGoal1Gymnasium-v0\": BCCarGoal1Config,\n    \"OfflineCarGoal2Gymnasium-v0\": BCCarGoal2Config,\n    \"OfflineCarPush1Gymnasium-v0\": BCCarPush1Config,\n    \"OfflineCarPush2Gymnasium-v0\": BCCarPush2Config,\n    # safety_gymnasium: point\n    \"OfflinePointButton1Gymnasium-v0\": BCPointButton1Config,\n    \"OfflinePointButton2Gymnasium-v0\": BCPointButton2Config,\n    \"OfflinePointCircle1Gymnasium-v0\": BCPointCircle1Config,\n    \"OfflinePointCircle2Gymnasium-v0\": BCPointCircle2Config,\n    \"OfflinePointGoal1Gymnasium-v0\": BCPointGoal1Config,", "    \"OfflinePointCircle2Gymnasium-v0\": BCPointCircle2Config,\n    \"OfflinePointGoal1Gymnasium-v0\": BCPointGoal1Config,\n    \"OfflinePointGoal2Gymnasium-v0\": BCPointGoal2Config,\n    \"OfflinePointPush1Gymnasium-v0\": BCPointPush1Config,\n    \"OfflinePointPush2Gymnasium-v0\": BCPointPush2Config,\n    # safety_gymnasium: velocity\n    \"OfflineAntVelocityGymnasium-v1\": BCAntVelocityConfig,\n    \"OfflineHalfCheetahVelocityGymnasium-v1\": BCHalfCheetahVelocityConfig,\n    \"OfflineHopperVelocityGymnasium-v1\": BCHopperVelocityConfig,\n    \"OfflineSwimmerVelocityGymnasium-v1\": BCSwimmerVelocityConfig,", "    \"OfflineHopperVelocityGymnasium-v1\": BCHopperVelocityConfig,\n    \"OfflineSwimmerVelocityGymnasium-v1\": BCSwimmerVelocityConfig,\n    \"OfflineWalker2dVelocityGymnasium-v1\": BCWalker2dVelocityConfig,\n    # safe_metadrive\n    \"OfflineMetadrive-easysparse-v0\": BCEasySparseConfig,\n    \"OfflineMetadrive-easymean-v0\": BCEasyMeanConfig,\n    \"OfflineMetadrive-easydense-v0\": BCEasyDenseConfig,\n    \"OfflineMetadrive-mediumsparse-v0\": BCMediumSparseConfig,\n    \"OfflineMetadrive-mediummean-v0\": BCMediumMeanConfig,\n    \"OfflineMetadrive-mediumdense-v0\": BCMediumDenseConfig,", "    \"OfflineMetadrive-mediummean-v0\": BCMediumMeanConfig,\n    \"OfflineMetadrive-mediumdense-v0\": BCMediumDenseConfig,\n    \"OfflineMetadrive-hardsparse-v0\": BCHardSparseConfig,\n    \"OfflineMetadrive-hardmean-v0\": BCHardMeanConfig,\n    \"OfflineMetadrive-harddense-v0\": BCHardDenseConfig\n}"]}
{"filename": "examples/configs/__init__.py", "chunked_list": [""]}
{"filename": "examples/configs/cpq_configs.py", "chunked_list": ["from dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nfrom pyrallis import field\n\n\n@dataclass\nclass CPQTrainConfig:\n    # wandb params\n    project: str = \"OSRL-baselines\"\n    group: str = None\n    name: Optional[str] = None\n    prefix: Optional[str] = \"CPQ\"\n    suffix: Optional[str] = \"\"\n    logdir: Optional[str] = \"logs\"\n    verbose: bool = True\n    # dataset params\n    outliers_percent: float = None\n    noise_scale: float = None\n    inpaint_ranges: Tuple[Tuple[float, float, float, float], ...] = None\n    epsilon: float = None\n    density: float = 1.0\n    # training params\n    task: str = \"OfflineCarCircle-v0\"\n    dataset: str = None\n    seed: int = 0\n    device: str = \"cpu\"\n    threads: int = 4\n    reward_scale: float = 0.1\n    cost_scale: float = 1\n    actor_lr: float = 0.0001\n    critic_lr: float = 0.001\n    alpha_lr: float = 0.0001\n    vae_lr: float = 0.001\n    cost_limit: int = 10\n    episode_len: int = 300\n    batch_size: int = 512\n    update_steps: int = 100_000\n    num_workers: int = 8\n    # model params\n    a_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n    c_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n    vae_hidden_sizes: int = 400\n    sample_action_num: int = 10\n    gamma: float = 0.99\n    tau: float = 0.005\n    beta: float = 0.5\n    num_q: int = 2\n    num_qc: int = 2\n    qc_scalar: float = 1.5\n    # evaluation params\n    eval_episodes: int = 10\n    eval_every: int = 2500", "\n\n@dataclass\nclass CPQCarCircleConfig(CPQTrainConfig):\n    pass\n\n\n@dataclass\nclass CPQAntRunConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineAntRun-v0\"\n    episode_len: int = 200", "class CPQAntRunConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineAntRun-v0\"\n    episode_len: int = 200\n\n\n@dataclass\nclass CPQDroneRunConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineDroneRun-v0\"\n    episode_len: int = 200", "\n\n@dataclass\nclass CPQDroneCircleConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineDroneCircle-v0\"\n    episode_len: int = 300\n\n\n@dataclass\nclass CPQCarRunConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarRun-v0\"\n    episode_len: int = 200", "\n@dataclass\nclass CPQCarRunConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarRun-v0\"\n    episode_len: int = 200\n\n\n@dataclass\nclass CPQAntCircleConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineAntCircle-v0\"\n    episode_len: int = 500", "@dataclass\nclass CPQAntCircleConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineAntCircle-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass CPQBallRunConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineBallRun-v0\"\n    episode_len: int = 100", "class CPQBallRunConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineBallRun-v0\"\n    episode_len: int = 100\n\n\n@dataclass\nclass CPQBallCircleConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineBallCircle-v0\"\n    episode_len: int = 200", "\n\n@dataclass\nclass CPQCarButton1Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarButton1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQCarButton2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarButton2Gymnasium-v0\"\n    episode_len: int = 1000", "\n@dataclass\nclass CPQCarButton2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarButton2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQCarCircle1Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n    episode_len: int = 500", "@dataclass\nclass CPQCarCircle1Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass CPQCarCircle2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n    episode_len: int = 500", "class CPQCarCircle2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass CPQCarGoal1Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarGoal1Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass CPQCarGoal2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarGoal2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQCarPush1Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush1Gymnasium-v0\"\n    episode_len: int = 1000", "\n@dataclass\nclass CPQCarPush1Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQCarPush2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush2Gymnasium-v0\"\n    episode_len: int = 1000", "@dataclass\nclass CPQCarPush2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQPointButton1Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflinePointButton1Gymnasium-v0\"\n    episode_len: int = 1000", "class CPQPointButton1Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflinePointButton1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQPointButton2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflinePointButton2Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass CPQPointCircle1Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle1Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass CPQPointCircle2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n    episode_len: int = 500", "\n@dataclass\nclass CPQPointCircle2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass CPQPointGoal1Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n    episode_len: int = 1000", "@dataclass\nclass CPQPointGoal1Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQPointGoal2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n    episode_len: int = 1000", "class CPQPointGoal2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQPointPush1Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflinePointPush1Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass CPQPointPush2Config(CPQTrainConfig):\n    # training params\n    task: str = \"OfflinePointPush2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQAntVelocityConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineAntVelocityGymnasium-v1\"\n    episode_len: int = 1000", "\n@dataclass\nclass CPQAntVelocityConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineAntVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQHalfCheetahVelocityConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n    episode_len: int = 1000", "@dataclass\nclass CPQHalfCheetahVelocityConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQHopperVelocityConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n    episode_len: int = 1000", "class CPQHopperVelocityConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQSwimmerVelocityConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineSwimmerVelocityGymnasium-v1\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass CPQWalker2dVelocityConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineWalker2dVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass CPQEasySparseConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easysparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass CPQEasySparseConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easysparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass CPQEasyMeanConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easymean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass CPQEasyMeanConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easymean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass CPQEasyDenseConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easydense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass CPQEasyDenseConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easydense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass CPQMediumSparseConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass CPQMediumSparseConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass CPQMediumMeanConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediummean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass CPQMediumMeanConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediummean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass CPQMediumDenseConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumdense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass CPQMediumDenseConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumdense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass CPQHardSparseConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass CPQHardSparseConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass CPQHardMeanConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardmean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass CPQHardMeanConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardmean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass CPQHardDenseConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-harddense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass CPQHardDenseConfig(CPQTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-harddense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\nCPQ_DEFAULT_CONFIG = {", "\nCPQ_DEFAULT_CONFIG = {\n    # bullet_safety_gym\n    \"OfflineCarCircle-v0\": CPQCarCircleConfig,\n    \"OfflineAntRun-v0\": CPQAntRunConfig,\n    \"OfflineDroneRun-v0\": CPQDroneRunConfig,\n    \"OfflineDroneCircle-v0\": CPQDroneCircleConfig,\n    \"OfflineCarRun-v0\": CPQCarRunConfig,\n    \"OfflineAntCircle-v0\": CPQAntCircleConfig,\n    \"OfflineBallCircle-v0\": CPQBallCircleConfig,", "    \"OfflineAntCircle-v0\": CPQAntCircleConfig,\n    \"OfflineBallCircle-v0\": CPQBallCircleConfig,\n    \"OfflineBallRun-v0\": CPQBallRunConfig,\n    # safety_gymnasium\n    \"OfflineCarButton1Gymnasium-v0\": CPQCarButton1Config,\n    \"OfflineCarButton2Gymnasium-v0\": CPQCarButton2Config,\n    \"OfflineCarCircle1Gymnasium-v0\": CPQCarCircle1Config,\n    \"OfflineCarCircle2Gymnasium-v0\": CPQCarCircle2Config,\n    \"OfflineCarGoal1Gymnasium-v0\": CPQCarGoal1Config,\n    \"OfflineCarGoal2Gymnasium-v0\": CPQCarGoal2Config,", "    \"OfflineCarGoal1Gymnasium-v0\": CPQCarGoal1Config,\n    \"OfflineCarGoal2Gymnasium-v0\": CPQCarGoal2Config,\n    \"OfflineCarPush1Gymnasium-v0\": CPQCarPush1Config,\n    \"OfflineCarPush2Gymnasium-v0\": CPQCarPush2Config,\n    # safety_gymnasium: point\n    \"OfflinePointButton1Gymnasium-v0\": CPQPointButton1Config,\n    \"OfflinePointButton2Gymnasium-v0\": CPQPointButton2Config,\n    \"OfflinePointCircle1Gymnasium-v0\": CPQPointCircle1Config,\n    \"OfflinePointCircle2Gymnasium-v0\": CPQPointCircle2Config,\n    \"OfflinePointGoal1Gymnasium-v0\": CPQPointGoal1Config,", "    \"OfflinePointCircle2Gymnasium-v0\": CPQPointCircle2Config,\n    \"OfflinePointGoal1Gymnasium-v0\": CPQPointGoal1Config,\n    \"OfflinePointGoal2Gymnasium-v0\": CPQPointGoal2Config,\n    \"OfflinePointPush1Gymnasium-v0\": CPQPointPush1Config,\n    \"OfflinePointPush2Gymnasium-v0\": CPQPointPush2Config,\n    # safety_gymnasium: velocity\n    \"OfflineAntVelocityGymnasium-v1\": CPQAntVelocityConfig,\n    \"OfflineHalfCheetahVelocityGymnasium-v1\": CPQHalfCheetahVelocityConfig,\n    \"OfflineHopperVelocityGymnasium-v1\": CPQHopperVelocityConfig,\n    \"OfflineSwimmerVelocityGymnasium-v1\": CPQSwimmerVelocityConfig,", "    \"OfflineHopperVelocityGymnasium-v1\": CPQHopperVelocityConfig,\n    \"OfflineSwimmerVelocityGymnasium-v1\": CPQSwimmerVelocityConfig,\n    \"OfflineWalker2dVelocityGymnasium-v1\": CPQWalker2dVelocityConfig,\n    # safe_metadrive\n    \"OfflineMetadrive-easysparse-v0\": CPQEasySparseConfig,\n    \"OfflineMetadrive-easymean-v0\": CPQEasyMeanConfig,\n    \"OfflineMetadrive-easydense-v0\": CPQEasyDenseConfig,\n    \"OfflineMetadrive-mediumsparse-v0\": CPQMediumSparseConfig,\n    \"OfflineMetadrive-mediummean-v0\": CPQMediumMeanConfig,\n    \"OfflineMetadrive-mediumdense-v0\": CPQMediumDenseConfig,", "    \"OfflineMetadrive-mediummean-v0\": CPQMediumMeanConfig,\n    \"OfflineMetadrive-mediumdense-v0\": CPQMediumDenseConfig,\n    \"OfflineMetadrive-hardsparse-v0\": CPQHardSparseConfig,\n    \"OfflineMetadrive-hardmean-v0\": CPQHardMeanConfig,\n    \"OfflineMetadrive-harddense-v0\": CPQHardDenseConfig\n}"]}
{"filename": "examples/configs/cdt_configs.py", "chunked_list": ["from dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\n\n@dataclass\nclass CDTTrainConfig:\n    # wandb params\n    project: str = \"OSRL-baselines\"\n    group: str = None\n    name: Optional[str] = None\n    prefix: Optional[str] = \"CDT\"\n    suffix: Optional[str] = \"\"\n    logdir: Optional[str] = \"logs\"\n    verbose: bool = True\n    # dataset params\n    outliers_percent: float = None\n    noise_scale: float = None\n    inpaint_ranges: Tuple[Tuple[float, float], ...] = None\n    epsilon: float = None\n    density: float = 1.0\n    # model params\n    embedding_dim: int = 128\n    num_layers: int = 3\n    num_heads: int = 8\n    action_head_layers: int = 1\n    seq_len: int = 10\n    episode_len: int = 300\n    attention_dropout: float = 0.1\n    residual_dropout: float = 0.1\n    embedding_dropout: float = 0.1\n    time_emb: bool = True\n    # training params\n    task: str = \"OfflineCarCircle-v0\"\n    dataset: str = None\n    learning_rate: float = 1e-4\n    betas: Tuple[float, float] = (0.9, 0.999)\n    weight_decay: float = 1e-4\n    clip_grad: Optional[float] = 0.25\n    batch_size: int = 2048\n    update_steps: int = 100_000\n    lr_warmup_steps: int = 500\n    reward_scale: float = 0.1\n    cost_scale: float = 1\n    num_workers: int = 8\n    # evaluation params\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((450.0, 10), (500.0, 20), (550.0, 50))  # reward, cost\n    cost_limit: int = 10\n    eval_episodes: int = 10\n    eval_every: int = 2500\n    # general params\n    seed: int = 0\n    device: str = \"cuda:2\"\n    threads: int = 6\n    # augmentation param\n    deg: int = 4\n    pf_sample: bool = False\n    beta: float = 1.0\n    augment_percent: float = 0.2\n    # maximum absolute value of reward for the augmented trajs\n    max_reward: float = 600.0\n    # minimum reward above the PF curve\n    min_reward: float = 1.0\n    # the max drecrease of ret between the associated traj\n    # w.r.t the nearest pf traj\n    max_rew_decrease: float = 100.0\n    # model mode params\n    use_rew: bool = True\n    use_cost: bool = True\n    cost_transform: bool = True\n    cost_prefix: bool = False\n    add_cost_feat: bool = False\n    mul_cost_feat: bool = False\n    cat_cost_feat: bool = False\n    loss_cost_weight: float = 0.02\n    loss_state_weight: float = 0\n    cost_reverse: bool = False\n    # pf only mode param\n    pf_only: bool = False\n    rmin: float = 300\n    cost_bins: int = 60\n    npb: int = 5\n    cost_sample: bool = True\n    linear: bool = True  # linear or inverse\n    start_sampling: bool = False\n    prob: float = 0.2\n    stochastic: bool = True\n    init_temperature: float = 0.1\n    no_entropy: bool = False\n    # random augmentation\n    random_aug: float = 0\n    aug_rmin: float = 400\n    aug_rmax: float = 500\n    aug_cmin: float = -2\n    aug_cmax: float = 25\n    cgap: float = 5\n    rstd: float = 1\n    cstd: float = 0.2", "\n\n@dataclass\nclass CDTCarCircleConfig(CDTTrainConfig):\n    pass\n\n\n@dataclass\nclass CDTAntRunConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 200\n    # training params\n    task: str = \"OfflineAntRun-v0\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((700.0, 10), (750.0, 20), (800.0, 40))\n    # augmentation param\n    deg: int = 3\n    max_reward: float = 1000.0\n    max_rew_decrease: float = 150\n    device: str = \"cuda:2\"", "class CDTAntRunConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 200\n    # training params\n    task: str = \"OfflineAntRun-v0\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((700.0, 10), (750.0, 20), (800.0, 40))\n    # augmentation param\n    deg: int = 3\n    max_reward: float = 1000.0\n    max_rew_decrease: float = 150\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTDroneRunConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 200\n    # training params\n    task: str = \"OfflineDroneRun-v0\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((400.0, 10), (500.0, 20), (600.0, 40))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 700.0\n    max_rew_decrease: float = 100\n    min_reward: float = 1\n    device: str = \"cuda:3\"", "\n\n@dataclass\nclass CDTDroneCircleConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 300\n    # training params\n    task: str = \"OfflineDroneCircle-v0\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((700.0, 10), (750.0, 20), (800.0, 40))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 1000.0\n    max_rew_decrease: float = 100\n    min_reward: float = 1\n    device: str = \"cuda:3\"", "\n\n@dataclass\nclass CDTCarRunConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 200\n    # training params\n    task: str = \"OfflineCarRun-v0\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((575.0, 10), (575.0, 20), (575.0, 40))\n    # augmentation param\n    deg: int = 0\n    max_reward: float = 600.0\n    max_rew_decrease: float = 100\n    min_reward: float = 1\n    device: str = \"cuda:3\"", "\n\n@dataclass\nclass CDTAntCircleConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 500\n    # training params\n    task: str = \"OfflineAntCircle-v0\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n    # augmentation param\n    deg: int = 2\n    max_reward: float = 500.0\n    max_rew_decrease: float = 100\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTBallRunConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 100\n    # training params\n    task: str = \"OfflineBallRun-v0\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((500.0, 10), (500.0, 20), (700.0, 40))\n    # augmentation param\n    deg: int = 2\n    max_reward: float = 1400.0\n    max_rew_decrease: float = 200\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTBallCircleConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 200\n    # training params\n    task: str = \"OfflineBallCircle-v0\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((700.0, 10), (750.0, 20), (800.0, 40))\n    # augmentation param\n    deg: int = 2\n    max_reward: float = 1000.0\n    max_rew_decrease: float = 200\n    min_reward: float = 1\n    device: str = \"cuda:1\"", "\n\n@dataclass\nclass CDTCarButton1Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineCarButton1Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((35.0, 20), (35.0, 40), (35.0, 80))\n    # augmentation param\n    deg: int = 0\n    max_reward: float = 45.0\n    max_rew_decrease: float = 10\n    min_reward: float = 1\n    device: str = \"cuda:0\"", "\n\n@dataclass\nclass CDTCarButton2Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineCarButton2Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((40.0, 20), (40.0, 40), (40.0, 80))\n    # augmentation param\n    deg: int = 0\n    max_reward: float = 50.0\n    max_rew_decrease: float = 10\n    min_reward: float = 1\n    device: str = \"cuda:0\"", "\n\n@dataclass\nclass CDTCarCircle1Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 500\n    # training params\n    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((20.0, 20), (22.5, 40), (25.0, 80))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 30.0\n    max_rew_decrease: float = 10\n    min_reward: float = 1\n    device: str = \"cuda:0\"", "\n\n@dataclass\nclass CDTCarCircle2Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 500\n    # training params\n    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((20.0, 20), (21.0, 40), (22.0, 80))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 30.0\n    max_rew_decrease: float = 10\n    min_reward: float = 1\n    device: str = \"cuda:0\"", "\n\n@dataclass\nclass CDTCarGoal1Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineCarGoal1Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((40.0, 20), (40.0, 40), (40.0, 80))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 50.0\n    max_rew_decrease: float = 5\n    min_reward: float = 1\n    device: str = \"cuda:1\"", "\n\n@dataclass\nclass CDTCarGoal2Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineCarGoal2Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((30.0, 20), (30.0, 40), (30.0, 80))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 35.0\n    max_rew_decrease: float = 5\n    min_reward: float = 1\n    device: str = \"cuda:1\"", "\n\n@dataclass\nclass CDTCarPush1Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineCarPush1Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((15.0, 20), (15.0, 40), (15.0, 80))\n    # augmentation param\n    deg: int = 0\n    max_reward: float = 20.0\n    max_rew_decrease: float = 5\n    min_reward: float = 1\n    device: str = \"cuda:1\"", "\n\n@dataclass\nclass CDTCarPush2Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineCarPush2Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((12.0, 20), (12.0, 40), (12.0, 80))\n    # augmentation param\n    deg: int = 0\n    max_reward: float = 15.0\n    max_rew_decrease: float = 3\n    min_reward: float = 1\n    device: str = \"cuda:1\"", "\n\n@dataclass\nclass CDTPointButton1Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflinePointButton1Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((40.0, 20), (40.0, 40), (40.0, 80))\n    # augmentation param\n    deg: int = 0\n    max_reward: float = 45.0\n    max_rew_decrease: float = 5\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTPointButton2Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflinePointButton2Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((40.0, 20), (40.0, 40), (40.0, 80))\n    # augmentation param\n    deg: int = 0\n    max_reward: float = 50.0\n    max_rew_decrease: float = 10\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTPointCircle1Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 500\n    # training params\n    task: str = \"OfflinePointCircle1Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((50.0, 20), (52.5, 40), (55.0, 80))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 65.0\n    max_rew_decrease: float = 5\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTPointCircle2Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 500\n    # training params\n    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((45.0, 20), (47.5, 40), (50.0, 80))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 55.0\n    max_rew_decrease: float = 5\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTPointGoal1Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((30.0, 20), (30.0, 40), (30.0, 80))\n    # augmentation param\n    deg: int = 0\n    max_reward: float = 35.0\n    max_rew_decrease: float = 5\n    min_reward: float = 1\n    device: str = \"cuda:3\"", "\n\n@dataclass\nclass CDTPointGoal2Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((30.0, 20), (30.0, 40), (30.0, 80))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 35.0\n    max_rew_decrease: float = 5\n    min_reward: float = 1\n    device: str = \"cuda:3\"", "\n\n@dataclass\nclass CDTPointPush1Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflinePointPush1Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((15.0, 20), (15.0, 40), (15.0, 80))\n    # augmentation param\n    deg: int = 0\n    max_reward: float = 20.0\n    max_rew_decrease: float = 5\n    min_reward: float = 1\n    device: str = \"cuda:3\"", "\n\n@dataclass\nclass CDTPointPush2Config(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflinePointPush2Gymnasium-v0\"\n    target_returns: Tuple[Tuple[float, ...], ...] = ((12.0, 20), (12.0, 40), (12.0, 80))\n    # augmentation param\n    deg: int = 0\n    max_reward: float = 15.0\n    max_rew_decrease: float = 3\n    min_reward: float = 1\n    device: str = \"cuda:3\"", "\n\n@dataclass\nclass CDTAntVelocityConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineAntVelocityGymnasium-v1\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((2800.0, 20), (2800.0, 40), (2800.0, 80))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 3000.0\n    max_rew_decrease: float = 500\n    min_reward: float = 1\n    device: str = \"cuda:1\"", "\n\n@dataclass\nclass CDTHalfCheetahVelocityConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((3000.0, 20), (3000.0, 40), (3000.0, 80))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 3000.0\n    max_rew_decrease: float = 500\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTHopperVelocityConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((1750.0, 20), (1750.0, 40), (1750.0, 80))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 2000.0\n    max_rew_decrease: float = 300\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTSwimmerVelocityConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineSwimmerVelocityGymnasium-v1\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((160.0, 20), (160.0, 40), (160.0, 80))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 250.0\n    max_rew_decrease: float = 50\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTWalker2dVelocityConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineWalker2dVelocityGymnasium-v1\"\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((2800.0, 20), (2800.0, 40), (2800.0, 80))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 3600.0\n    max_rew_decrease: float = 800\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTEasySparseConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineMetadrive-easysparse-v0\"\n    update_steps: int = 200_000\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n    # augmentation param\n    deg: int = 2\n    max_reward: float = 500.0\n    max_rew_decrease: float = 100\n    min_reward: float = 1\n    device: str = \"cuda:3\"", "\n\n@dataclass\nclass CDTEasyMeanConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineMetadrive-easymean-v0\"\n    update_steps: int = 200_000\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n    # augmentation param\n    deg: int = 2\n    max_reward: float = 500.0\n    max_rew_decrease: float = 100\n    min_reward: float = 1\n    device: str = \"cuda:3\"", "\n\n@dataclass\nclass CDTEasyDenseConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineMetadrive-easydense-v0\"\n    update_steps: int = 200_000\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n    # augmentation param\n    deg: int = 2\n    max_reward: float = 500.0\n    max_rew_decrease: float = 100\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTMediumSparseConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n    update_steps: int = 200_000\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((300.0, 10), (300.0, 20), (300.0, 40))\n    # augmentation param\n    deg: int = 0\n    max_reward: float = 300.0\n    max_rew_decrease: float = 100\n    min_reward: float = 1\n    device: str = \"cuda:3\"", "\n\n@dataclass\nclass CDTMediumMeanConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineMetadrive-mediummean-v0\"\n    update_steps: int = 200_000\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((300.0, 10), (300.0, 20), (300.0, 40))\n    # augmentation param\n    deg: int = 0\n    max_reward: float = 300.0\n    max_rew_decrease: float = 100\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTMediumDenseConfig(CDTTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumdense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n", "\n\n@dataclass\nclass CDTHardSparseConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineMetadrive-hardsparse-v0\"\n    update_steps: int = 200_000\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 500.0\n    max_rew_decrease: float = 100\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTHardMeanConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineMetadrive-hardmean-v0\"\n    update_steps: int = 200_000\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 500.0\n    max_rew_decrease: float = 100\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\n@dataclass\nclass CDTHardDenseConfig(CDTTrainConfig):\n    # model params\n    seq_len: int = 10\n    episode_len: int = 1000\n    # training params\n    task: str = \"OfflineMetadrive-harddense-v0\"\n    update_steps: int = 200_000\n    target_returns: Tuple[Tuple[float, ...],\n                          ...] = ((300.0, 10), (350.0, 20), (400.0, 40))\n    # augmentation param\n    deg: int = 1\n    max_reward: float = 500.0\n    max_rew_decrease: float = 100\n    min_reward: float = 1\n    device: str = \"cuda:2\"", "\n\nCDT_DEFAULT_CONFIG = {\n    # bullet_safety_gym\n    \"OfflineCarCircle-v0\": CDTCarCircleConfig,\n    \"OfflineAntRun-v0\": CDTAntRunConfig,\n    \"OfflineDroneRun-v0\": CDTDroneRunConfig,\n    \"OfflineDroneCircle-v0\": CDTDroneCircleConfig,\n    \"OfflineCarRun-v0\": CDTCarRunConfig,\n    \"OfflineAntCircle-v0\": CDTAntCircleConfig,", "    \"OfflineCarRun-v0\": CDTCarRunConfig,\n    \"OfflineAntCircle-v0\": CDTAntCircleConfig,\n    \"OfflineBallCircle-v0\": CDTBallCircleConfig,\n    \"OfflineBallRun-v0\": CDTBallRunConfig,\n    # safety_gymnasium\n    \"OfflineCarButton1Gymnasium-v0\": CDTCarButton1Config,\n    \"OfflineCarButton2Gymnasium-v0\": CDTCarButton2Config,\n    \"OfflineCarCircle1Gymnasium-v0\": CDTCarCircle1Config,\n    \"OfflineCarCircle2Gymnasium-v0\": CDTCarCircle2Config,\n    \"OfflineCarGoal1Gymnasium-v0\": CDTCarGoal1Config,", "    \"OfflineCarCircle2Gymnasium-v0\": CDTCarCircle2Config,\n    \"OfflineCarGoal1Gymnasium-v0\": CDTCarGoal1Config,\n    \"OfflineCarGoal2Gymnasium-v0\": CDTCarGoal2Config,\n    \"OfflineCarPush1Gymnasium-v0\": CDTCarPush1Config,\n    \"OfflineCarPush2Gymnasium-v0\": CDTCarPush2Config,\n    # safety_gymnasium: point\n    \"OfflinePointButton1Gymnasium-v0\": CDTPointButton1Config,\n    \"OfflinePointButton2Gymnasium-v0\": CDTPointButton2Config,\n    \"OfflinePointCircle1Gymnasium-v0\": CDTPointCircle1Config,\n    \"OfflinePointCircle2Gymnasium-v0\": CDTPointCircle2Config,", "    \"OfflinePointCircle1Gymnasium-v0\": CDTPointCircle1Config,\n    \"OfflinePointCircle2Gymnasium-v0\": CDTPointCircle2Config,\n    \"OfflinePointGoal1Gymnasium-v0\": CDTPointGoal1Config,\n    \"OfflinePointGoal2Gymnasium-v0\": CDTPointGoal2Config,\n    \"OfflinePointPush1Gymnasium-v0\": CDTPointPush1Config,\n    \"OfflinePointPush2Gymnasium-v0\": CDTPointPush2Config,\n    # safety_gymnasium: velocity\n    \"OfflineAntVelocityGymnasium-v1\": CDTAntVelocityConfig,\n    \"OfflineHalfCheetahVelocityGymnasium-v1\": CDTHalfCheetahVelocityConfig,\n    \"OfflineHopperVelocityGymnasium-v1\": CDTHopperVelocityConfig,", "    \"OfflineHalfCheetahVelocityGymnasium-v1\": CDTHalfCheetahVelocityConfig,\n    \"OfflineHopperVelocityGymnasium-v1\": CDTHopperVelocityConfig,\n    \"OfflineSwimmerVelocityGymnasium-v1\": CDTSwimmerVelocityConfig,\n    \"OfflineWalker2dVelocityGymnasium-v1\": CDTWalker2dVelocityConfig,\n    # safe_metadrive\n    \"OfflineMetadrive-easysparse-v0\": CDTEasySparseConfig,\n    \"OfflineMetadrive-easymean-v0\": CDTEasyMeanConfig,\n    \"OfflineMetadrive-easydense-v0\": CDTEasyDenseConfig,\n    \"OfflineMetadrive-mediumsparse-v0\": CDTMediumSparseConfig,\n    \"OfflineMetadrive-mediummean-v0\": CDTMediumMeanConfig,", "    \"OfflineMetadrive-mediumsparse-v0\": CDTMediumSparseConfig,\n    \"OfflineMetadrive-mediummean-v0\": CDTMediumMeanConfig,\n    \"OfflineMetadrive-mediumdense-v0\": CDTMediumDenseConfig,\n    \"OfflineMetadrive-hardsparse-v0\": CDTHardSparseConfig,\n    \"OfflineMetadrive-hardmean-v0\": CDTHardMeanConfig,\n    \"OfflineMetadrive-harddense-v0\": CDTHardDenseConfig\n}\n"]}
{"filename": "examples/configs/bearl_configs.py", "chunked_list": ["from dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nfrom pyrallis import field\n\n\n@dataclass\nclass BEARLTrainConfig:\n    # wandb params\n    project: str = \"OSRL-baselines\"\n    group: str = None\n    name: Optional[str] = None\n    prefix: Optional[str] = \"BEARL\"\n    suffix: Optional[str] = \"\"\n    logdir: Optional[str] = \"logs\"\n    verbose: bool = True\n    # dataset params\n    outliers_percent: float = None\n    noise_scale: float = None\n    inpaint_ranges: Tuple[Tuple[float, float, float, float], ...] = None\n    epsilon: float = None\n    density: float = 1.0\n    # training params\n    task: str = \"OfflineCarCircle-v0\"\n    dataset: str = None\n    seed: int = 0\n    device: str = \"cpu\"\n    threads: int = 4\n    reward_scale: float = 0.1\n    cost_scale: float = 1\n    actor_lr: float = 0.001\n    critic_lr: float = 0.001\n    vae_lr: float = 0.001\n    cost_limit: int = 10\n    episode_len: int = 300\n    batch_size: int = 512\n    update_steps: int = 300_000\n    num_workers: int = 8\n    # model params\n    a_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n    c_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n    vae_hidden_sizes: int = 400\n    sample_action_num: int = 10\n    gamma: float = 0.99\n    tau: float = 0.005\n    beta: float = 0.5\n    lmbda: float = 0.75\n    mmd_sigma: float = 50\n    target_mmd_thresh: float = 0.05\n    num_samples_mmd_match: int = 10\n    start_update_policy_step: int = 0\n    kernel: str = \"gaussian\"  # or \"laplacian\"\n    num_q: int = 2\n    num_qc: int = 2\n    PID: List[float] = field(default=[0.1, 0.003, 0.001], is_mutable=True)\n    # evaluation params\n    eval_episodes: int = 10\n    eval_every: int = 2500", "\n\n@dataclass\nclass BEARLCarCircleConfig(BEARLTrainConfig):\n    pass\n\n\n@dataclass\nclass BEARLAntRunConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineAntRun-v0\"\n    episode_len: int = 200", "class BEARLAntRunConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineAntRun-v0\"\n    episode_len: int = 200\n\n\n@dataclass\nclass BEARLDroneRunConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineDroneRun-v0\"\n    episode_len: int = 200", "\n\n@dataclass\nclass BEARLDroneCircleConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineDroneCircle-v0\"\n    episode_len: int = 300\n\n\n@dataclass\nclass BEARLCarRunConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarRun-v0\"\n    episode_len: int = 200", "\n@dataclass\nclass BEARLCarRunConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarRun-v0\"\n    episode_len: int = 200\n\n\n@dataclass\nclass BEARLAntCircleConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineAntCircle-v0\"\n    episode_len: int = 500", "@dataclass\nclass BEARLAntCircleConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineAntCircle-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BEARLBallRunConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineBallRun-v0\"\n    episode_len: int = 100", "class BEARLBallRunConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineBallRun-v0\"\n    episode_len: int = 100\n\n\n@dataclass\nclass BEARLBallCircleConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineBallCircle-v0\"\n    episode_len: int = 200", "\n\n@dataclass\nclass BEARLCarButton1Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarButton1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLCarButton2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarButton2Gymnasium-v0\"\n    episode_len: int = 1000", "\n@dataclass\nclass BEARLCarButton2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarButton2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLCarCircle1Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n    episode_len: int = 500", "@dataclass\nclass BEARLCarCircle1Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BEARLCarCircle2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n    episode_len: int = 500", "class BEARLCarCircle2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BEARLCarGoal1Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarGoal1Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass BEARLCarGoal2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarGoal2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLCarPush1Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush1Gymnasium-v0\"\n    episode_len: int = 1000", "\n@dataclass\nclass BEARLCarPush1Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLCarPush2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush2Gymnasium-v0\"\n    episode_len: int = 1000", "@dataclass\nclass BEARLCarPush2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLPointButton1Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflinePointButton1Gymnasium-v0\"\n    episode_len: int = 1000", "class BEARLPointButton1Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflinePointButton1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLPointButton2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflinePointButton2Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass BEARLPointCircle1Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle1Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BEARLPointCircle2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n    episode_len: int = 500", "\n@dataclass\nclass BEARLPointCircle2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BEARLPointGoal1Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n    episode_len: int = 1000", "@dataclass\nclass BEARLPointGoal1Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLPointGoal2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n    episode_len: int = 1000", "class BEARLPointGoal2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLPointPush1Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflinePointPush1Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass BEARLPointPush2Config(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflinePointPush2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLAntVelocityConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineAntVelocityGymnasium-v1\"\n    episode_len: int = 1000", "\n@dataclass\nclass BEARLAntVelocityConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineAntVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLHalfCheetahVelocityConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n    episode_len: int = 1000", "@dataclass\nclass BEARLHalfCheetahVelocityConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLHopperVelocityConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n    episode_len: int = 1000", "class BEARLHopperVelocityConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLSwimmerVelocityConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineSwimmerVelocityGymnasium-v1\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass BEARLWalker2dVelocityConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineWalker2dVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BEARLEasySparseConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easysparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BEARLEasySparseConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easysparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BEARLEasyMeanConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easymean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BEARLEasyMeanConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easymean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BEARLEasyDenseConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easydense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BEARLEasyDenseConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easydense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BEARLMediumSparseConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BEARLMediumSparseConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BEARLMediumMeanConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediummean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BEARLMediumMeanConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediummean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BEARLMediumDenseConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumdense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BEARLMediumDenseConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumdense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BEARLHardSparseConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BEARLHardSparseConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BEARLHardMeanConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardmean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BEARLHardMeanConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardmean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BEARLHardDenseConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-harddense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BEARLHardDenseConfig(BEARLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-harddense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\nBEARL_DEFAULT_CONFIG = {", "\nBEARL_DEFAULT_CONFIG = {\n    # bullet_safety_gym\n    \"OfflineCarCircle-v0\": BEARLCarCircleConfig,\n    \"OfflineAntRun-v0\": BEARLAntRunConfig,\n    \"OfflineDroneRun-v0\": BEARLDroneRunConfig,\n    \"OfflineDroneCircle-v0\": BEARLDroneCircleConfig,\n    \"OfflineCarRun-v0\": BEARLCarRunConfig,\n    \"OfflineAntCircle-v0\": BEARLAntCircleConfig,\n    \"OfflineBallCircle-v0\": BEARLBallCircleConfig,", "    \"OfflineAntCircle-v0\": BEARLAntCircleConfig,\n    \"OfflineBallCircle-v0\": BEARLBallCircleConfig,\n    \"OfflineBallRun-v0\": BEARLBallRunConfig,\n    # safety_gymnasium\n    \"OfflineCarButton1Gymnasium-v0\": BEARLCarButton1Config,\n    \"OfflineCarButton2Gymnasium-v0\": BEARLCarButton2Config,\n    \"OfflineCarCircle1Gymnasium-v0\": BEARLCarCircle1Config,\n    \"OfflineCarCircle2Gymnasium-v0\": BEARLCarCircle2Config,\n    \"OfflineCarGoal1Gymnasium-v0\": BEARLCarGoal1Config,\n    \"OfflineCarGoal2Gymnasium-v0\": BEARLCarGoal2Config,", "    \"OfflineCarGoal1Gymnasium-v0\": BEARLCarGoal1Config,\n    \"OfflineCarGoal2Gymnasium-v0\": BEARLCarGoal2Config,\n    \"OfflineCarPush1Gymnasium-v0\": BEARLCarPush1Config,\n    \"OfflineCarPush2Gymnasium-v0\": BEARLCarPush2Config,\n    # safety_gymnasium: point\n    \"OfflinePointButton1Gymnasium-v0\": BEARLPointButton1Config,\n    \"OfflinePointButton2Gymnasium-v0\": BEARLPointButton2Config,\n    \"OfflinePointCircle1Gymnasium-v0\": BEARLPointCircle1Config,\n    \"OfflinePointCircle2Gymnasium-v0\": BEARLPointCircle2Config,\n    \"OfflinePointGoal1Gymnasium-v0\": BEARLPointGoal1Config,", "    \"OfflinePointCircle2Gymnasium-v0\": BEARLPointCircle2Config,\n    \"OfflinePointGoal1Gymnasium-v0\": BEARLPointGoal1Config,\n    \"OfflinePointGoal2Gymnasium-v0\": BEARLPointGoal2Config,\n    \"OfflinePointPush1Gymnasium-v0\": BEARLPointPush1Config,\n    \"OfflinePointPush2Gymnasium-v0\": BEARLPointPush2Config,\n    # safety_gymnasium: velocity\n    \"OfflineAntVelocityGymnasium-v1\": BEARLAntVelocityConfig,\n    \"OfflineHalfCheetahVelocityGymnasium-v1\": BEARLHalfCheetahVelocityConfig,\n    \"OfflineHopperVelocityGymnasium-v1\": BEARLHopperVelocityConfig,\n    \"OfflineSwimmerVelocityGymnasium-v1\": BEARLSwimmerVelocityConfig,", "    \"OfflineHopperVelocityGymnasium-v1\": BEARLHopperVelocityConfig,\n    \"OfflineSwimmerVelocityGymnasium-v1\": BEARLSwimmerVelocityConfig,\n    \"OfflineWalker2dVelocityGymnasium-v1\": BEARLWalker2dVelocityConfig,\n    # safe_metadrive\n    \"OfflineMetadrive-easysparse-v0\": BEARLEasySparseConfig,\n    \"OfflineMetadrive-easymean-v0\": BEARLEasyMeanConfig,\n    \"OfflineMetadrive-easydense-v0\": BEARLEasyDenseConfig,\n    \"OfflineMetadrive-mediumsparse-v0\": BEARLMediumSparseConfig,\n    \"OfflineMetadrive-mediummean-v0\": BEARLMediumMeanConfig,\n    \"OfflineMetadrive-mediumdense-v0\": BEARLMediumDenseConfig,", "    \"OfflineMetadrive-mediummean-v0\": BEARLMediumMeanConfig,\n    \"OfflineMetadrive-mediumdense-v0\": BEARLMediumDenseConfig,\n    \"OfflineMetadrive-hardsparse-v0\": BEARLHardSparseConfig,\n    \"OfflineMetadrive-hardmean-v0\": BEARLHardMeanConfig,\n    \"OfflineMetadrive-harddense-v0\": BEARLHardDenseConfig\n}\n"]}
{"filename": "examples/configs/bcql_configs.py", "chunked_list": ["from dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nfrom pyrallis import field\n\n\n@dataclass\nclass BCQLTrainConfig:\n    # wandb params\n    project: str = \"OSRL-baselines\"\n    group: str = None\n    name: Optional[str] = None\n    prefix: Optional[str] = \"BCQL\"\n    suffix: Optional[str] = \"\"\n    logdir: Optional[str] = \"logs\"\n    verbose: bool = True\n    # dataset params\n    outliers_percent: float = None\n    noise_scale: float = None\n    inpaint_ranges: Tuple[Tuple[float, float, float, float], ...] = None\n    epsilon: float = None\n    density: float = 1.0\n    # training params\n    task: str = \"OfflineCarCircle-v0\"\n    dataset: str = None\n    seed: int = 0\n    device: str = \"cpu\"\n    threads: int = 4\n    reward_scale: float = 0.1\n    cost_scale: float = 1\n    actor_lr: float = 0.001\n    critic_lr: float = 0.001\n    vae_lr: float = 0.001\n    phi: float = 0.05\n    lmbda: float = 0.75\n    beta: float = 0.5\n    cost_limit: int = 10\n    episode_len: int = 300\n    batch_size: int = 512\n    update_steps: int = 100_000\n    num_workers: int = 8\n    # model params\n    a_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n    c_hidden_sizes: List[float] = field(default=[256, 256], is_mutable=True)\n    vae_hidden_sizes: int = 400\n    sample_action_num: int = 10\n    gamma: float = 0.99\n    tau: float = 0.005\n    num_q: int = 2\n    num_qc: int = 2\n    PID: List[float] = field(default=[0.1, 0.003, 0.001], is_mutable=True)\n    # evaluation params\n    eval_episodes: int = 10\n    eval_every: int = 2500", "\n\n@dataclass\nclass BCQLCarCircleConfig(BCQLTrainConfig):\n    pass\n\n\n@dataclass\nclass BCQLAntRunConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineAntRun-v0\"\n    episode_len: int = 200", "class BCQLAntRunConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineAntRun-v0\"\n    episode_len: int = 200\n\n\n@dataclass\nclass BCQLDroneRunConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineDroneRun-v0\"\n    episode_len: int = 200", "\n\n@dataclass\nclass BCQLDroneCircleConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineDroneCircle-v0\"\n    episode_len: int = 300\n\n\n@dataclass\nclass BCQLCarRunConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarRun-v0\"\n    episode_len: int = 200", "\n@dataclass\nclass BCQLCarRunConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarRun-v0\"\n    episode_len: int = 200\n\n\n@dataclass\nclass BCQLAntCircleConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineAntCircle-v0\"\n    episode_len: int = 500", "@dataclass\nclass BCQLAntCircleConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineAntCircle-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BCQLBallRunConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineBallRun-v0\"\n    episode_len: int = 100", "class BCQLBallRunConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineBallRun-v0\"\n    episode_len: int = 100\n\n\n@dataclass\nclass BCQLBallCircleConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineBallCircle-v0\"\n    episode_len: int = 200", "\n\n@dataclass\nclass BCQLCarButton1Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarButton1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLCarButton2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarButton2Gymnasium-v0\"\n    episode_len: int = 1000", "\n@dataclass\nclass BCQLCarButton2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarButton2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLCarCircle1Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n    episode_len: int = 500", "@dataclass\nclass BCQLCarCircle1Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle1Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BCQLCarCircle2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n    episode_len: int = 500", "class BCQLCarCircle2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarCircle2Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BCQLCarGoal1Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarGoal1Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass BCQLCarGoal2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarGoal2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLCarPush1Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush1Gymnasium-v0\"\n    episode_len: int = 1000", "\n@dataclass\nclass BCQLCarPush1Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLCarPush2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush2Gymnasium-v0\"\n    episode_len: int = 1000", "@dataclass\nclass BCQLCarPush2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineCarPush2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLPointButton1Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflinePointButton1Gymnasium-v0\"\n    episode_len: int = 1000", "class BCQLPointButton1Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflinePointButton1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLPointButton2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflinePointButton2Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass BCQLPointCircle1Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle1Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BCQLPointCircle2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n    episode_len: int = 500", "\n@dataclass\nclass BCQLPointCircle2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflinePointCircle2Gymnasium-v0\"\n    episode_len: int = 500\n\n\n@dataclass\nclass BCQLPointGoal1Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n    episode_len: int = 1000", "@dataclass\nclass BCQLPointGoal1Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal1Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLPointGoal2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n    episode_len: int = 1000", "class BCQLPointGoal2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflinePointGoal2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLPointPush1Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflinePointPush1Gymnasium-v0\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass BCQLPointPush2Config(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflinePointPush2Gymnasium-v0\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLAntVelocityConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineAntVelocityGymnasium-v1\"\n    episode_len: int = 1000", "\n@dataclass\nclass BCQLAntVelocityConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineAntVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLHalfCheetahVelocityConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n    episode_len: int = 1000", "@dataclass\nclass BCQLHalfCheetahVelocityConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineHalfCheetahVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLHopperVelocityConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n    episode_len: int = 1000", "class BCQLHopperVelocityConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineHopperVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLSwimmerVelocityConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineSwimmerVelocityGymnasium-v1\"\n    episode_len: int = 1000", "\n\n@dataclass\nclass BCQLWalker2dVelocityConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineWalker2dVelocityGymnasium-v1\"\n    episode_len: int = 1000\n\n\n@dataclass\nclass BCQLEasySparseConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easysparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCQLEasySparseConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easysparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCQLEasyMeanConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easymean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCQLEasyMeanConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easymean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCQLEasyDenseConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easydense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCQLEasyDenseConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-easydense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCQLMediumSparseConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCQLMediumSparseConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCQLMediumMeanConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediummean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCQLMediumMeanConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediummean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCQLMediumDenseConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumdense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCQLMediumDenseConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-mediumdense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCQLHardSparseConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCQLHardSparseConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardsparse-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCQLHardMeanConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardmean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCQLHardMeanConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-hardmean-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\n@dataclass\nclass BCQLHardDenseConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-harddense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000", "\n@dataclass\nclass BCQLHardDenseConfig(BCQLTrainConfig):\n    # training params\n    task: str = \"OfflineMetadrive-harddense-v0\"\n    episode_len: int = 1000\n    update_steps: int = 200_000\n\n\nBCQL_DEFAULT_CONFIG = {", "\nBCQL_DEFAULT_CONFIG = {\n    # bullet_safety_gym\n    \"OfflineCarCircle-v0\": BCQLCarCircleConfig,\n    \"OfflineAntRun-v0\": BCQLAntRunConfig,\n    \"OfflineDroneRun-v0\": BCQLDroneRunConfig,\n    \"OfflineDroneCircle-v0\": BCQLDroneCircleConfig,\n    \"OfflineCarRun-v0\": BCQLCarRunConfig,\n    \"OfflineAntCircle-v0\": BCQLAntCircleConfig,\n    \"OfflineBallCircle-v0\": BCQLBallCircleConfig,", "    \"OfflineAntCircle-v0\": BCQLAntCircleConfig,\n    \"OfflineBallCircle-v0\": BCQLBallCircleConfig,\n    \"OfflineBallRun-v0\": BCQLBallRunConfig,\n    # safety_gymnasium: car\n    \"OfflineCarButton1Gymnasium-v0\": BCQLCarButton1Config,\n    \"OfflineCarButton2Gymnasium-v0\": BCQLCarButton2Config,\n    \"OfflineCarCircle1Gymnasium-v0\": BCQLCarCircle1Config,\n    \"OfflineCarCircle2Gymnasium-v0\": BCQLCarCircle2Config,\n    \"OfflineCarGoal1Gymnasium-v0\": BCQLCarGoal1Config,\n    \"OfflineCarGoal2Gymnasium-v0\": BCQLCarGoal2Config,", "    \"OfflineCarGoal1Gymnasium-v0\": BCQLCarGoal1Config,\n    \"OfflineCarGoal2Gymnasium-v0\": BCQLCarGoal2Config,\n    \"OfflineCarPush1Gymnasium-v0\": BCQLCarPush1Config,\n    \"OfflineCarPush2Gymnasium-v0\": BCQLCarPush2Config,\n    # safety_gymnasium: point\n    \"OfflinePointButton1Gymnasium-v0\": BCQLPointButton1Config,\n    \"OfflinePointButton2Gymnasium-v0\": BCQLPointButton2Config,\n    \"OfflinePointCircle1Gymnasium-v0\": BCQLPointCircle1Config,\n    \"OfflinePointCircle2Gymnasium-v0\": BCQLPointCircle2Config,\n    \"OfflinePointGoal1Gymnasium-v0\": BCQLPointGoal1Config,", "    \"OfflinePointCircle2Gymnasium-v0\": BCQLPointCircle2Config,\n    \"OfflinePointGoal1Gymnasium-v0\": BCQLPointGoal1Config,\n    \"OfflinePointGoal2Gymnasium-v0\": BCQLPointGoal2Config,\n    \"OfflinePointPush1Gymnasium-v0\": BCQLPointPush1Config,\n    \"OfflinePointPush2Gymnasium-v0\": BCQLPointPush2Config,\n    # safety_gymnasium: velocity\n    \"OfflineAntVelocityGymnasium-v1\": BCQLAntVelocityConfig,\n    \"OfflineHalfCheetahVelocityGymnasium-v1\": BCQLHalfCheetahVelocityConfig,\n    \"OfflineHopperVelocityGymnasium-v1\": BCQLHopperVelocityConfig,\n    \"OfflineSwimmerVelocityGymnasium-v1\": BCQLSwimmerVelocityConfig,", "    \"OfflineHopperVelocityGymnasium-v1\": BCQLHopperVelocityConfig,\n    \"OfflineSwimmerVelocityGymnasium-v1\": BCQLSwimmerVelocityConfig,\n    \"OfflineWalker2dVelocityGymnasium-v1\": BCQLWalker2dVelocityConfig,\n    # safe_metadrive\n    \"OfflineMetadrive-easysparse-v0\": BCQLEasySparseConfig,\n    \"OfflineMetadrive-easymean-v0\": BCQLEasyMeanConfig,\n    \"OfflineMetadrive-easydense-v0\": BCQLEasyDenseConfig,\n    \"OfflineMetadrive-mediumsparse-v0\": BCQLMediumSparseConfig,\n    \"OfflineMetadrive-mediummean-v0\": BCQLMediumMeanConfig,\n    \"OfflineMetadrive-mediumdense-v0\": BCQLMediumDenseConfig,", "    \"OfflineMetadrive-mediummean-v0\": BCQLMediumMeanConfig,\n    \"OfflineMetadrive-mediumdense-v0\": BCQLMediumDenseConfig,\n    \"OfflineMetadrive-hardsparse-v0\": BCQLHardSparseConfig,\n    \"OfflineMetadrive-hardmean-v0\": BCQLHardMeanConfig,\n    \"OfflineMetadrive-harddense-v0\": BCQLHardDenseConfig\n}"]}
{"filename": "examples/eval/eval_cpq.py", "chunked_list": ["from dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nimport dsrl\nimport gymnasium as gym  # noqa\nimport numpy as np\nimport pyrallis\nimport torch\nfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom pyrallis import field", "from dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom pyrallis import field\n\nfrom osrl.algorithms import CPQ, CPQTrainer\nfrom osrl.common.exp_util import load_config_and_model, seed_all\n\n\n@dataclass\nclass EvalConfig:\n    path: str = \"log/.../checkpoint/model.pt\"\n    noise_scale: List[float] = None\n    eval_episodes: int = 20\n    best: bool = False\n    device: str = \"cpu\"\n    threads: int = 4", "class EvalConfig:\n    path: str = \"log/.../checkpoint/model.pt\"\n    noise_scale: List[float] = None\n    eval_episodes: int = 20\n    best: bool = False\n    device: str = \"cpu\"\n    threads: int = 4\n\n\n@pyrallis.wrap()\ndef eval(args: EvalConfig):\n\n    cfg, model = load_config_and_model(args.path, args.best)\n    seed_all(cfg[\"seed\"])\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    env = wrap_env(\n        env=gym.make(cfg[\"task\"]),\n        reward_scale=cfg[\"reward_scale\"],\n    )\n    env = OfflineEnvWrapper(env)\n    env.set_target_cost(cfg[\"cost_limit\"])\n\n    cpq_model = CPQ(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n        c_hidden_sizes=cfg[\"c_hidden_sizes\"],\n        vae_hidden_sizes=cfg[\"vae_hidden_sizes\"],\n        sample_action_num=cfg[\"sample_action_num\"],\n        gamma=cfg[\"gamma\"],\n        tau=cfg[\"tau\"],\n        beta=cfg[\"beta\"],\n        num_q=cfg[\"num_q\"],\n        num_qc=cfg[\"num_qc\"],\n        qc_scalar=cfg[\"qc_scalar\"],\n        cost_limit=cfg[\"cost_limit\"],\n        episode_len=cfg[\"episode_len\"],\n        device=args.device,\n    )\n    cpq_model.load_state_dict(model[\"model_state\"])\n    cpq_model.to(args.device)\n\n    trainer = CPQTrainer(cpq_model,\n                         env,\n                         reward_scale=cfg[\"reward_scale\"],\n                         cost_scale=cfg[\"cost_scale\"],\n                         device=args.device)\n\n    ret, cost, length = trainer.evaluate(args.eval_episodes)\n    normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n    print(\n        f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n    )", "\n@pyrallis.wrap()\ndef eval(args: EvalConfig):\n\n    cfg, model = load_config_and_model(args.path, args.best)\n    seed_all(cfg[\"seed\"])\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    env = wrap_env(\n        env=gym.make(cfg[\"task\"]),\n        reward_scale=cfg[\"reward_scale\"],\n    )\n    env = OfflineEnvWrapper(env)\n    env.set_target_cost(cfg[\"cost_limit\"])\n\n    cpq_model = CPQ(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n        c_hidden_sizes=cfg[\"c_hidden_sizes\"],\n        vae_hidden_sizes=cfg[\"vae_hidden_sizes\"],\n        sample_action_num=cfg[\"sample_action_num\"],\n        gamma=cfg[\"gamma\"],\n        tau=cfg[\"tau\"],\n        beta=cfg[\"beta\"],\n        num_q=cfg[\"num_q\"],\n        num_qc=cfg[\"num_qc\"],\n        qc_scalar=cfg[\"qc_scalar\"],\n        cost_limit=cfg[\"cost_limit\"],\n        episode_len=cfg[\"episode_len\"],\n        device=args.device,\n    )\n    cpq_model.load_state_dict(model[\"model_state\"])\n    cpq_model.to(args.device)\n\n    trainer = CPQTrainer(cpq_model,\n                         env,\n                         reward_scale=cfg[\"reward_scale\"],\n                         cost_scale=cfg[\"cost_scale\"],\n                         device=args.device)\n\n    ret, cost, length = trainer.evaluate(args.eval_episodes)\n    normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n    print(\n        f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n    )", "\n\nif __name__ == \"__main__\":\n    eval()\n"]}
{"filename": "examples/eval/eval_bearl.py", "chunked_list": ["from dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nimport dsrl\nimport gymnasium as gym  # noqa\nimport numpy as np\nimport pyrallis\nimport torch\nfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom pyrallis import field", "from dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom pyrallis import field\n\nfrom osrl.algorithms import BEARL, BEARLTrainer\nfrom osrl.common.exp_util import load_config_and_model, seed_all\n\n\n@dataclass\nclass EvalConfig:\n    path: str = \"log/.../checkpoint/model.pt\"\n    noise_scale: List[float] = None\n    eval_episodes: int = 20\n    best: bool = False\n    device: str = \"cpu\"\n    threads: int = 4", "class EvalConfig:\n    path: str = \"log/.../checkpoint/model.pt\"\n    noise_scale: List[float] = None\n    eval_episodes: int = 20\n    best: bool = False\n    device: str = \"cpu\"\n    threads: int = 4\n\n\n@pyrallis.wrap()\ndef eval(args: EvalConfig):\n\n    cfg, model = load_config_and_model(args.path, args.best)\n    seed_all(cfg[\"seed\"])\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    env = wrap_env(\n        env=gym.make(cfg[\"task\"]),\n        reward_scale=cfg[\"reward_scale\"],\n    )\n    env = OfflineEnvWrapper(env)\n    env.set_target_cost(cfg[\"cost_limit\"])\n\n    # model & optimizer & scheduler setup\n    bear_model = BEARL(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n        c_hidden_sizes=cfg[\"c_hidden_sizes\"],\n        vae_hidden_sizes=cfg[\"vae_hidden_sizes\"],\n        sample_action_num=cfg[\"sample_action_num\"],\n        gamma=cfg[\"gamma\"],\n        tau=cfg[\"tau\"],\n        beta=cfg[\"beta\"],\n        lmbda=cfg[\"lmbda\"],\n        mmd_sigma=cfg[\"mmd_sigma\"],\n        target_mmd_thresh=cfg[\"target_mmd_thresh\"],\n        start_update_policy_step=cfg[\"start_update_policy_step\"],\n        num_q=cfg[\"num_q\"],\n        num_qc=cfg[\"num_qc\"],\n        PID=cfg[\"PID\"],\n        cost_limit=cfg[\"cost_limit\"],\n        episode_len=cfg[\"episode_len\"],\n        device=args.device,\n    )\n    bear_model.load_state_dict(model[\"model_state\"])\n    bear_model.to(args.device)\n\n    trainer = BEARLTrainer(bear_model,\n                           env,\n                           reward_scale=cfg[\"reward_scale\"],\n                           cost_scale=cfg[\"cost_scale\"],\n                           device=args.device)\n\n    ret, cost, length = trainer.evaluate(args.eval_episodes)\n    normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n    print(\n        f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n    )", "\n@pyrallis.wrap()\ndef eval(args: EvalConfig):\n\n    cfg, model = load_config_and_model(args.path, args.best)\n    seed_all(cfg[\"seed\"])\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    env = wrap_env(\n        env=gym.make(cfg[\"task\"]),\n        reward_scale=cfg[\"reward_scale\"],\n    )\n    env = OfflineEnvWrapper(env)\n    env.set_target_cost(cfg[\"cost_limit\"])\n\n    # model & optimizer & scheduler setup\n    bear_model = BEARL(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n        c_hidden_sizes=cfg[\"c_hidden_sizes\"],\n        vae_hidden_sizes=cfg[\"vae_hidden_sizes\"],\n        sample_action_num=cfg[\"sample_action_num\"],\n        gamma=cfg[\"gamma\"],\n        tau=cfg[\"tau\"],\n        beta=cfg[\"beta\"],\n        lmbda=cfg[\"lmbda\"],\n        mmd_sigma=cfg[\"mmd_sigma\"],\n        target_mmd_thresh=cfg[\"target_mmd_thresh\"],\n        start_update_policy_step=cfg[\"start_update_policy_step\"],\n        num_q=cfg[\"num_q\"],\n        num_qc=cfg[\"num_qc\"],\n        PID=cfg[\"PID\"],\n        cost_limit=cfg[\"cost_limit\"],\n        episode_len=cfg[\"episode_len\"],\n        device=args.device,\n    )\n    bear_model.load_state_dict(model[\"model_state\"])\n    bear_model.to(args.device)\n\n    trainer = BEARLTrainer(bear_model,\n                           env,\n                           reward_scale=cfg[\"reward_scale\"],\n                           cost_scale=cfg[\"cost_scale\"],\n                           device=args.device)\n\n    ret, cost, length = trainer.evaluate(args.eval_episodes)\n    normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n    print(\n        f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n    )", "\n\nif __name__ == \"__main__\":\n    eval()\n"]}
{"filename": "examples/eval/eval_bc.py", "chunked_list": ["from dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nimport dsrl\nimport gymnasium as gym  # noqa\nimport numpy as np\nimport pyrallis\nimport torch\nfrom pyrallis import field\n", "from pyrallis import field\n\nfrom osrl.algorithms import BC, BCTrainer\nfrom osrl.common.exp_util import load_config_and_model, seed_all\n\n\n@dataclass\nclass EvalConfig:\n    path: str = \"log/.../checkpoint/model.pt\"\n    noise_scale: List[float] = None\n    costs: List[float] = field(default=[1, 10, 20, 30, 40], is_mutable=True)\n    eval_episodes: int = 20\n    best: bool = False\n    device: str = \"cpu\"\n    threads: int = 4", "\n\n@pyrallis.wrap()\ndef eval(args: EvalConfig):\n\n    cfg, model = load_config_and_model(args.path, args.best)\n    seed_all(cfg[\"seed\"])\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    env = gym.make(cfg[\"task\"])\n    env.set_target_cost(cfg[\"cost_limit\"])\n\n    # model & optimizer & scheduler setup\n    state_dim = env.observation_space.shape[0]\n    if cfg[\"bc_mode\"] == \"multi-task\":\n        state_dim += 1\n    bc_model = BC(\n        state_dim=state_dim,\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n        episode_len=cfg[\"episode_len\"],\n        device=args.device,\n    )\n    bc_model.load_state_dict(model[\"model_state\"])\n    bc_model.to(args.device)\n\n    trainer = BCTrainer(bc_model,\n                        env,\n                        bc_mode=cfg[\"bc_mode\"],\n                        cost_limit=cfg[\"cost_limit\"],\n                        device=args.device)\n\n    if cfg[\"bc_mode\"] == \"multi-task\":\n        for target_cost in args.costs:\n            env.set_target_cost(target_cost)\n            trainer.set_target_cost(target_cost)\n            ret, cost, length = trainer.evaluate(args.eval_episodes)\n            normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n            print(\n                f\"Eval reward: {ret}, normalized reward: {normalized_ret}; target cost {target_cost}, real cost {cost}, normalized cost: {normalized_cost}\"\n            )\n    else:\n        ret, cost, length = trainer.evaluate(args.eval_episodes)\n        normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n        print(\n            f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n        )", "\n\nif __name__ == \"__main__\":\n    eval()\n"]}
{"filename": "examples/eval/eval_bcql.py", "chunked_list": ["from dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nimport dsrl\nimport gymnasium as gym  # noqa\nimport numpy as np\nimport pyrallis\nimport torch\nfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom pyrallis import field", "from dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom pyrallis import field\n\nfrom osrl.algorithms import BCQL, BCQLTrainer\nfrom osrl.common.exp_util import load_config_and_model, seed_all\n\n\n@dataclass\nclass EvalConfig:\n    path: str = \"log/.../checkpoint/model.pt\"\n    noise_scale: List[float] = None\n    eval_episodes: int = 20\n    best: bool = False\n    device: str = \"cpu\"\n    threads: int = 4", "class EvalConfig:\n    path: str = \"log/.../checkpoint/model.pt\"\n    noise_scale: List[float] = None\n    eval_episodes: int = 20\n    best: bool = False\n    device: str = \"cpu\"\n    threads: int = 4\n\n\n@pyrallis.wrap()\ndef eval(args: EvalConfig):\n\n    cfg, model = load_config_and_model(args.path, args.best)\n    seed_all(cfg[\"seed\"])\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    env = wrap_env(\n        env=gym.make(cfg[\"task\"]),\n        reward_scale=cfg[\"reward_scale\"],\n    )\n    env = OfflineEnvWrapper(env)\n    env.set_target_cost(cfg[\"cost_limit\"])\n\n    bcql_model = BCQL(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n        c_hidden_sizes=cfg[\"c_hidden_sizes\"],\n        vae_hidden_sizes=cfg[\"vae_hidden_sizes\"],\n        sample_action_num=cfg[\"sample_action_num\"],\n        PID=cfg[\"PID\"],\n        gamma=cfg[\"gamma\"],\n        tau=cfg[\"tau\"],\n        lmbda=cfg[\"lmbda\"],\n        beta=cfg[\"beta\"],\n        phi=cfg[\"phi\"],\n        num_q=cfg[\"num_q\"],\n        num_qc=cfg[\"num_qc\"],\n        cost_limit=cfg[\"cost_limit\"],\n        episode_len=cfg[\"episode_len\"],\n        device=args.device,\n    )\n    bcql_model.load_state_dict(model[\"model_state\"])\n    bcql_model.to(args.device)\n\n    trainer = BCQLTrainer(bcql_model,\n                          env,\n                          reward_scale=cfg[\"reward_scale\"],\n                          cost_scale=cfg[\"cost_scale\"],\n                          device=args.device)\n\n    ret, cost, length = trainer.evaluate(args.eval_episodes)\n    normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n    print(\n        f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n    )", "\n@pyrallis.wrap()\ndef eval(args: EvalConfig):\n\n    cfg, model = load_config_and_model(args.path, args.best)\n    seed_all(cfg[\"seed\"])\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    env = wrap_env(\n        env=gym.make(cfg[\"task\"]),\n        reward_scale=cfg[\"reward_scale\"],\n    )\n    env = OfflineEnvWrapper(env)\n    env.set_target_cost(cfg[\"cost_limit\"])\n\n    bcql_model = BCQL(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n        c_hidden_sizes=cfg[\"c_hidden_sizes\"],\n        vae_hidden_sizes=cfg[\"vae_hidden_sizes\"],\n        sample_action_num=cfg[\"sample_action_num\"],\n        PID=cfg[\"PID\"],\n        gamma=cfg[\"gamma\"],\n        tau=cfg[\"tau\"],\n        lmbda=cfg[\"lmbda\"],\n        beta=cfg[\"beta\"],\n        phi=cfg[\"phi\"],\n        num_q=cfg[\"num_q\"],\n        num_qc=cfg[\"num_qc\"],\n        cost_limit=cfg[\"cost_limit\"],\n        episode_len=cfg[\"episode_len\"],\n        device=args.device,\n    )\n    bcql_model.load_state_dict(model[\"model_state\"])\n    bcql_model.to(args.device)\n\n    trainer = BCQLTrainer(bcql_model,\n                          env,\n                          reward_scale=cfg[\"reward_scale\"],\n                          cost_scale=cfg[\"cost_scale\"],\n                          device=args.device)\n\n    ret, cost, length = trainer.evaluate(args.eval_episodes)\n    normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n    print(\n        f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n    )", "\n\nif __name__ == \"__main__\":\n    eval()\n"]}
{"filename": "examples/eval/eval_coptidice.py", "chunked_list": ["from dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nimport dsrl\nimport gymnasium as gym  # noqa\nimport numpy as np\nimport pyrallis\nimport torch\nfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom pyrallis import field", "from dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom pyrallis import field\n\nfrom osrl.algorithms import COptiDICE, COptiDICETrainer\nfrom osrl.common.exp_util import load_config_and_model, seed_all\n\n\n@dataclass\nclass EvalConfig:\n    path: str = \"log/.../checkpoint/model.pt\"\n    noise_scale: List[float] = None\n    eval_episodes: int = 20\n    best: bool = False\n    device: str = \"cpu\"\n    threads: int = 4", "class EvalConfig:\n    path: str = \"log/.../checkpoint/model.pt\"\n    noise_scale: List[float] = None\n    eval_episodes: int = 20\n    best: bool = False\n    device: str = \"cpu\"\n    threads: int = 4\n\n\n@pyrallis.wrap()\ndef eval(args: EvalConfig):\n\n    cfg, model = load_config_and_model(args.path, args.best)\n    seed_all(cfg[\"seed\"])\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    env = wrap_env(\n        env=gym.make(cfg[\"task\"]),\n        reward_scale=cfg[\"reward_scale\"],\n    )\n    env = OfflineEnvWrapper(env)\n    env.set_target_cost(cfg[\"cost_limit\"])\n\n    # setup model\n    coptidice_model = COptiDICE(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        f_type=cfg[\"f_type\"],\n        init_state_propotion=1.0,\n        observations_std=np.array([0]),\n        actions_std=np.array([0]),\n        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n        c_hidden_sizes=cfg[\"c_hidden_sizes\"],\n        gamma=cfg[\"gamma\"],\n        alpha=cfg[\"alpha\"],\n        cost_ub_epsilon=cfg[\"cost_ub_epsilon\"],\n        num_nu=cfg[\"num_nu\"],\n        num_chi=cfg[\"num_chi\"],\n        cost_limit=cfg[\"cost_limit\"],\n        episode_len=cfg[\"episode_len\"],\n        device=args.device,\n    )\n    coptidice_model.load_state_dict(model[\"model_state\"])\n    coptidice_model.to(args.device)\n    trainer = COptiDICETrainer(coptidice_model,\n                               env,\n                               reward_scale=cfg[\"reward_scale\"],\n                               cost_scale=cfg[\"cost_scale\"],\n                               device=args.device)\n\n    ret, cost, length = trainer.evaluate(args.eval_episodes)\n    normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n    print(\n        f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n    )", "\n@pyrallis.wrap()\ndef eval(args: EvalConfig):\n\n    cfg, model = load_config_and_model(args.path, args.best)\n    seed_all(cfg[\"seed\"])\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    env = wrap_env(\n        env=gym.make(cfg[\"task\"]),\n        reward_scale=cfg[\"reward_scale\"],\n    )\n    env = OfflineEnvWrapper(env)\n    env.set_target_cost(cfg[\"cost_limit\"])\n\n    # setup model\n    coptidice_model = COptiDICE(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        f_type=cfg[\"f_type\"],\n        init_state_propotion=1.0,\n        observations_std=np.array([0]),\n        actions_std=np.array([0]),\n        a_hidden_sizes=cfg[\"a_hidden_sizes\"],\n        c_hidden_sizes=cfg[\"c_hidden_sizes\"],\n        gamma=cfg[\"gamma\"],\n        alpha=cfg[\"alpha\"],\n        cost_ub_epsilon=cfg[\"cost_ub_epsilon\"],\n        num_nu=cfg[\"num_nu\"],\n        num_chi=cfg[\"num_chi\"],\n        cost_limit=cfg[\"cost_limit\"],\n        episode_len=cfg[\"episode_len\"],\n        device=args.device,\n    )\n    coptidice_model.load_state_dict(model[\"model_state\"])\n    coptidice_model.to(args.device)\n    trainer = COptiDICETrainer(coptidice_model,\n                               env,\n                               reward_scale=cfg[\"reward_scale\"],\n                               cost_scale=cfg[\"cost_scale\"],\n                               device=args.device)\n\n    ret, cost, length = trainer.evaluate(args.eval_episodes)\n    normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n    print(\n        f\"Eval reward: {ret}, normalized reward: {normalized_ret}; cost: {cost}, normalized cost: {normalized_cost}; length: {length}\"\n    )", "\n\nif __name__ == \"__main__\":\n    eval()\n"]}
{"filename": "examples/eval/eval_cdt.py", "chunked_list": ["from dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nimport dsrl\nimport gymnasium as gym  # noqa\nimport numpy as np\nimport pyrallis\nimport torch\nfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom pyrallis import field", "from dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom pyrallis import field\n\nfrom osrl.algorithms import CDT, CDTTrainer\nfrom osrl.common.exp_util import load_config_and_model, seed_all\n\n\n@dataclass\nclass EvalConfig:\n    path: str = \"log/.../checkpoint/model.pt\"\n    returns: List[float] = field(default=[300, 400, 500], is_mutable=True)\n    costs: List[float] = field(default=[10, 10, 10], is_mutable=True)\n    noise_scale: List[float] = None\n    eval_episodes: int = 20\n    best: bool = False\n    device: str = \"cpu\"\n    threads: int = 4", "class EvalConfig:\n    path: str = \"log/.../checkpoint/model.pt\"\n    returns: List[float] = field(default=[300, 400, 500], is_mutable=True)\n    costs: List[float] = field(default=[10, 10, 10], is_mutable=True)\n    noise_scale: List[float] = None\n    eval_episodes: int = 20\n    best: bool = False\n    device: str = \"cpu\"\n    threads: int = 4\n", "\n\n@pyrallis.wrap()\ndef eval(args: EvalConfig):\n\n    cfg, model = load_config_and_model(args.path, args.best)\n    seed_all(cfg[\"seed\"])\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    env = wrap_env(\n        env=gym.make(cfg[\"task\"]),\n        reward_scale=cfg[\"reward_scale\"],\n    )\n    env = OfflineEnvWrapper(env)\n    env.set_target_cost(cfg[\"cost_limit\"])\n\n    target_entropy = -env.action_space.shape[0]\n\n    # model & optimizer & scheduler setup\n    cdt_model = CDT(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        embedding_dim=cfg[\"embedding_dim\"],\n        seq_len=cfg[\"seq_len\"],\n        episode_len=cfg[\"episode_len\"],\n        num_layers=cfg[\"num_layers\"],\n        num_heads=cfg[\"num_heads\"],\n        attention_dropout=cfg[\"attention_dropout\"],\n        residual_dropout=cfg[\"residual_dropout\"],\n        embedding_dropout=cfg[\"embedding_dropout\"],\n        time_emb=cfg[\"time_emb\"],\n        use_rew=cfg[\"use_rew\"],\n        use_cost=cfg[\"use_cost\"],\n        cost_transform=cfg[\"cost_transform\"],\n        add_cost_feat=cfg[\"add_cost_feat\"],\n        mul_cost_feat=cfg[\"mul_cost_feat\"],\n        cat_cost_feat=cfg[\"cat_cost_feat\"],\n        action_head_layers=cfg[\"action_head_layers\"],\n        cost_prefix=cfg[\"cost_prefix\"],\n        stochastic=cfg[\"stochastic\"],\n        init_temperature=cfg[\"init_temperature\"],\n        target_entropy=target_entropy,\n    )\n    cdt_model.load_state_dict(model[\"model_state\"])\n    cdt_model.to(args.device)\n\n    trainer = CDTTrainer(cdt_model,\n                         env,\n                         reward_scale=cfg[\"reward_scale\"],\n                         cost_scale=cfg[\"cost_scale\"],\n                         cost_reverse=cfg[\"cost_reverse\"],\n                         device=args.device)\n\n    rets = args.returns\n    costs = args.costs\n    assert len(rets) == len(\n        costs\n    ), f\"The length of returns {len(rets)} should be equal to costs {len(costs)}!\"\n    for target_ret, target_cost in zip(rets, costs):\n        seed_all(cfg[\"seed\"])\n        ret, cost, length = trainer.evaluate(args.eval_episodes,\n                                             target_ret * cfg[\"reward_scale\"],\n                                             target_cost * cfg[\"cost_scale\"])\n        normalized_ret, normalized_cost = env.get_normalized_score(ret, cost)\n        print(\n            f\"Target reward {target_ret}, real reward {ret}, normalized reward: {normalized_ret}; target cost {target_cost}, real cost {cost}, normalized cost: {normalized_cost}\"\n        )", "\n\nif __name__ == \"__main__\":\n    eval()\n"]}
{"filename": "examples/train/train_bc.py", "chunked_list": ["import os\nimport uuid\nimport types\nfrom dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nimport bullet_safety_gym  # noqa\nimport dsrl\nimport gymnasium as gym  # noqa\nimport numpy as np", "import gymnasium as gym  # noqa\nimport numpy as np\nimport pyrallis\nimport torch\nfrom dsrl.infos import DENSITY_CFG\nfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom fsrl.utils import WandbLogger\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import trange  # noqa\n", "from tqdm.auto import trange  # noqa\n\nfrom examples.configs.bc_configs import BC_DEFAULT_CONFIG, BCTrainConfig\nfrom osrl.algorithms import BC, BCTrainer\nfrom osrl.common import TransitionDataset\nfrom osrl.common.dataset import process_bc_dataset\nfrom osrl.common.exp_util import auto_name, seed_all\n\n\n@pyrallis.wrap()\ndef train(args: BCTrainConfig):\n    # update config\n    cfg, old_cfg = asdict(args), asdict(BCTrainConfig())\n    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n    cfg = asdict(BC_DEFAULT_CONFIG[args.task]())\n    cfg.update(differing_values)\n    args = types.SimpleNamespace(**cfg)\n\n    # setup logger\n    default_cfg = asdict(BC_DEFAULT_CONFIG[args.task]())\n    if args.name is None:\n        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n    if args.group is None:\n        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n    if args.logdir is not None:\n        args.logdir = os.path.join(args.logdir, args.group, args.name)\n    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n    logger.save_config(cfg, verbose=args.verbose)\n\n    # set seed\n    seed_all(args.seed)\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    # the cost scale is down in trainer rollout\n    env = gym.make(args.task)\n    data = env.get_dataset()\n    env.set_target_cost(args.cost_limit)\n\n    cbins, rbins, max_npb, min_npb = None, None, None, None\n    if args.density != 1.0:\n        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n        cbins = density_cfg[\"cbins\"]\n        rbins = density_cfg[\"rbins\"]\n        max_npb = density_cfg[\"max_npb\"]\n        min_npb = density_cfg[\"min_npb\"]\n    data = env.pre_process_data(data,\n                                args.outliers_percent,\n                                args.noise_scale,\n                                args.inpaint_ranges,\n                                args.epsilon,\n                                args.density,\n                                cbins=cbins,\n                                rbins=rbins,\n                                max_npb=max_npb,\n                                min_npb=min_npb)\n\n    process_bc_dataset(data, args.cost_limit, args.gamma, args.bc_mode)\n\n    # model & optimizer & scheduler setup\n    state_dim = env.observation_space.shape[0]\n    if args.bc_mode == \"multi-task\":\n        state_dim += 1\n    model = BC(\n        state_dim=state_dim,\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=args.a_hidden_sizes,\n        episode_len=args.episode_len,\n        device=args.device,\n    )\n    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    def checkpoint_fn():\n        return {\"model_state\": model.state_dict()}\n\n    logger.setup_checkpoint_fn(checkpoint_fn)\n\n    trainer = BCTrainer(model,\n                        env,\n                        logger=logger,\n                        actor_lr=args.actor_lr,\n                        bc_mode=args.bc_mode,\n                        cost_limit=args.cost_limit,\n                        device=args.device)\n\n    trainloader = DataLoader(\n        TransitionDataset(data),\n        batch_size=args.batch_size,\n        pin_memory=True,\n        num_workers=args.num_workers,\n    )\n    trainloader_iter = iter(trainloader)\n\n    # for saving the best\n    best_reward = -np.inf\n    best_cost = np.inf\n    best_idx = 0\n\n    for step in trange(args.update_steps, desc=\"Training\"):\n        batch = next(trainloader_iter)\n        observations, _, actions, _, _, _ = [b.to(args.device) for b in batch]\n        trainer.train_one_step(observations, actions)\n\n        # evaluation\n        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n            ret, cost, length = trainer.evaluate(args.eval_episodes)\n            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\n            # save the current weight\n            logger.save_checkpoint()\n            # save the best weight\n            if cost < best_cost or (cost == best_cost and ret > best_reward):\n                best_cost = cost\n                best_reward = ret\n                best_idx = step\n                logger.save_checkpoint(suffix=\"best\")\n\n            logger.store(tab=\"train\", best_idx=best_idx)\n            logger.write(step, display=False)\n\n        else:\n            logger.write_without_reset(step)", "\n@pyrallis.wrap()\ndef train(args: BCTrainConfig):\n    # update config\n    cfg, old_cfg = asdict(args), asdict(BCTrainConfig())\n    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n    cfg = asdict(BC_DEFAULT_CONFIG[args.task]())\n    cfg.update(differing_values)\n    args = types.SimpleNamespace(**cfg)\n\n    # setup logger\n    default_cfg = asdict(BC_DEFAULT_CONFIG[args.task]())\n    if args.name is None:\n        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n    if args.group is None:\n        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n    if args.logdir is not None:\n        args.logdir = os.path.join(args.logdir, args.group, args.name)\n    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n    logger.save_config(cfg, verbose=args.verbose)\n\n    # set seed\n    seed_all(args.seed)\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    # the cost scale is down in trainer rollout\n    env = gym.make(args.task)\n    data = env.get_dataset()\n    env.set_target_cost(args.cost_limit)\n\n    cbins, rbins, max_npb, min_npb = None, None, None, None\n    if args.density != 1.0:\n        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n        cbins = density_cfg[\"cbins\"]\n        rbins = density_cfg[\"rbins\"]\n        max_npb = density_cfg[\"max_npb\"]\n        min_npb = density_cfg[\"min_npb\"]\n    data = env.pre_process_data(data,\n                                args.outliers_percent,\n                                args.noise_scale,\n                                args.inpaint_ranges,\n                                args.epsilon,\n                                args.density,\n                                cbins=cbins,\n                                rbins=rbins,\n                                max_npb=max_npb,\n                                min_npb=min_npb)\n\n    process_bc_dataset(data, args.cost_limit, args.gamma, args.bc_mode)\n\n    # model & optimizer & scheduler setup\n    state_dim = env.observation_space.shape[0]\n    if args.bc_mode == \"multi-task\":\n        state_dim += 1\n    model = BC(\n        state_dim=state_dim,\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=args.a_hidden_sizes,\n        episode_len=args.episode_len,\n        device=args.device,\n    )\n    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    def checkpoint_fn():\n        return {\"model_state\": model.state_dict()}\n\n    logger.setup_checkpoint_fn(checkpoint_fn)\n\n    trainer = BCTrainer(model,\n                        env,\n                        logger=logger,\n                        actor_lr=args.actor_lr,\n                        bc_mode=args.bc_mode,\n                        cost_limit=args.cost_limit,\n                        device=args.device)\n\n    trainloader = DataLoader(\n        TransitionDataset(data),\n        batch_size=args.batch_size,\n        pin_memory=True,\n        num_workers=args.num_workers,\n    )\n    trainloader_iter = iter(trainloader)\n\n    # for saving the best\n    best_reward = -np.inf\n    best_cost = np.inf\n    best_idx = 0\n\n    for step in trange(args.update_steps, desc=\"Training\"):\n        batch = next(trainloader_iter)\n        observations, _, actions, _, _, _ = [b.to(args.device) for b in batch]\n        trainer.train_one_step(observations, actions)\n\n        # evaluation\n        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n            ret, cost, length = trainer.evaluate(args.eval_episodes)\n            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\n            # save the current weight\n            logger.save_checkpoint()\n            # save the best weight\n            if cost < best_cost or (cost == best_cost and ret > best_reward):\n                best_cost = cost\n                best_reward = ret\n                best_idx = step\n                logger.save_checkpoint(suffix=\"best\")\n\n            logger.store(tab=\"train\", best_idx=best_idx)\n            logger.write(step, display=False)\n\n        else:\n            logger.write_without_reset(step)", "\n\nif __name__ == \"__main__\":\n    train()\n"]}
{"filename": "examples/train/train_cpq.py", "chunked_list": ["import os\nimport uuid\nimport types\nfrom dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nimport bullet_safety_gym  # noqa\nimport dsrl\nimport gymnasium as gym  # noqa\nimport numpy as np", "import gymnasium as gym  # noqa\nimport numpy as np\nimport pyrallis\nimport torch\nfrom dsrl.infos import DENSITY_CFG\nfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom fsrl.utils import WandbLogger\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import trange  # noqa\n", "from tqdm.auto import trange  # noqa\n\nfrom examples.configs.cpq_configs import CPQ_DEFAULT_CONFIG, CPQTrainConfig\nfrom osrl.algorithms import CPQ, CPQTrainer\nfrom osrl.common import TransitionDataset\nfrom osrl.common.exp_util import auto_name, seed_all\n\n\n@pyrallis.wrap()\ndef train(args: CPQTrainConfig):\n    # update config\n    cfg, old_cfg = asdict(args), asdict(CPQTrainConfig())\n    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n    cfg = asdict(CPQ_DEFAULT_CONFIG[args.task]())\n    cfg.update(differing_values)\n    args = types.SimpleNamespace(**cfg)\n\n    # setup logger\n    default_cfg = asdict(CPQ_DEFAULT_CONFIG[args.task]())\n    if args.name is None:\n        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n    if args.group is None:\n        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n    if args.logdir is not None:\n        args.logdir = os.path.join(args.logdir, args.group, args.name)\n    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n    logger.save_config(cfg, verbose=args.verbose)\n\n    # set seed\n    seed_all(args.seed)\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    # initialize environment\n    env = gym.make(args.task)\n\n    # pre-process offline dataset\n    data = env.get_dataset()\n    env.set_target_cost(args.cost_limit)\n\n    cbins, rbins, max_npb, min_npb = None, None, None, None\n    if args.density != 1.0:\n        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n        cbins = density_cfg[\"cbins\"]\n        rbins = density_cfg[\"rbins\"]\n        max_npb = density_cfg[\"max_npb\"]\n        min_npb = density_cfg[\"min_npb\"]\n    data = env.pre_process_data(data,\n                                args.outliers_percent,\n                                args.noise_scale,\n                                args.inpaint_ranges,\n                                args.epsilon,\n                                args.density,\n                                cbins=cbins,\n                                rbins=rbins,\n                                max_npb=max_npb,\n                                min_npb=min_npb)\n\n    # wrapper\n    env = wrap_env(\n        env=env,\n        reward_scale=args.reward_scale,\n    )\n    env = OfflineEnvWrapper(env)\n\n    # model & optimizer setup\n    model = CPQ(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=args.a_hidden_sizes,\n        c_hidden_sizes=args.c_hidden_sizes,\n        vae_hidden_sizes=args.vae_hidden_sizes,\n        sample_action_num=args.sample_action_num,\n        gamma=args.gamma,\n        tau=args.tau,\n        beta=args.beta,\n        num_q=args.num_q,\n        num_qc=args.num_qc,\n        qc_scalar=args.qc_scalar,\n        cost_limit=args.cost_limit,\n        episode_len=args.episode_len,\n        device=args.device,\n    )\n    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    def checkpoint_fn():\n        return {\"model_state\": model.state_dict()}\n\n    logger.setup_checkpoint_fn(checkpoint_fn)\n\n    trainer = CPQTrainer(model,\n                         env,\n                         logger=logger,\n                         actor_lr=args.actor_lr,\n                         critic_lr=args.critic_lr,\n                         alpha_lr=args.alpha_lr,\n                         vae_lr=args.vae_lr,\n                         reward_scale=args.reward_scale,\n                         cost_scale=args.cost_scale,\n                         device=args.device)\n\n    dataset = TransitionDataset(data,\n                                reward_scale=args.reward_scale,\n                                cost_scale=args.cost_scale)\n    trainloader = DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        num_workers=args.num_workers,\n    )\n    trainloader_iter = iter(trainloader)\n\n    # for saving the best\n    best_reward = -np.inf\n    best_cost = np.inf\n    best_idx = 0\n\n    for step in trange(args.update_steps, desc=\"Training\"):\n        batch = next(trainloader_iter)\n        observations, next_observations, actions, rewards, costs, done = [\n            b.to(args.device) for b in batch\n        ]\n        trainer.train_one_step(observations, next_observations, actions, rewards, costs,\n                               done)\n\n        # evaluation\n        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n            ret, cost, length = trainer.evaluate(args.eval_episodes)\n            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\n            # save the current weight\n            logger.save_checkpoint()\n            # save the best weight\n            if cost < best_cost or (cost == best_cost and ret > best_reward):\n                best_cost = cost\n                best_reward = ret\n                best_idx = step\n                logger.save_checkpoint(suffix=\"best\")\n\n            logger.store(tab=\"train\", best_idx=best_idx)\n            logger.write(step, display=False)\n\n        else:\n            logger.write_without_reset(step)", "@pyrallis.wrap()\ndef train(args: CPQTrainConfig):\n    # update config\n    cfg, old_cfg = asdict(args), asdict(CPQTrainConfig())\n    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n    cfg = asdict(CPQ_DEFAULT_CONFIG[args.task]())\n    cfg.update(differing_values)\n    args = types.SimpleNamespace(**cfg)\n\n    # setup logger\n    default_cfg = asdict(CPQ_DEFAULT_CONFIG[args.task]())\n    if args.name is None:\n        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n    if args.group is None:\n        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n    if args.logdir is not None:\n        args.logdir = os.path.join(args.logdir, args.group, args.name)\n    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n    logger.save_config(cfg, verbose=args.verbose)\n\n    # set seed\n    seed_all(args.seed)\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    # initialize environment\n    env = gym.make(args.task)\n\n    # pre-process offline dataset\n    data = env.get_dataset()\n    env.set_target_cost(args.cost_limit)\n\n    cbins, rbins, max_npb, min_npb = None, None, None, None\n    if args.density != 1.0:\n        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n        cbins = density_cfg[\"cbins\"]\n        rbins = density_cfg[\"rbins\"]\n        max_npb = density_cfg[\"max_npb\"]\n        min_npb = density_cfg[\"min_npb\"]\n    data = env.pre_process_data(data,\n                                args.outliers_percent,\n                                args.noise_scale,\n                                args.inpaint_ranges,\n                                args.epsilon,\n                                args.density,\n                                cbins=cbins,\n                                rbins=rbins,\n                                max_npb=max_npb,\n                                min_npb=min_npb)\n\n    # wrapper\n    env = wrap_env(\n        env=env,\n        reward_scale=args.reward_scale,\n    )\n    env = OfflineEnvWrapper(env)\n\n    # model & optimizer setup\n    model = CPQ(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=args.a_hidden_sizes,\n        c_hidden_sizes=args.c_hidden_sizes,\n        vae_hidden_sizes=args.vae_hidden_sizes,\n        sample_action_num=args.sample_action_num,\n        gamma=args.gamma,\n        tau=args.tau,\n        beta=args.beta,\n        num_q=args.num_q,\n        num_qc=args.num_qc,\n        qc_scalar=args.qc_scalar,\n        cost_limit=args.cost_limit,\n        episode_len=args.episode_len,\n        device=args.device,\n    )\n    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    def checkpoint_fn():\n        return {\"model_state\": model.state_dict()}\n\n    logger.setup_checkpoint_fn(checkpoint_fn)\n\n    trainer = CPQTrainer(model,\n                         env,\n                         logger=logger,\n                         actor_lr=args.actor_lr,\n                         critic_lr=args.critic_lr,\n                         alpha_lr=args.alpha_lr,\n                         vae_lr=args.vae_lr,\n                         reward_scale=args.reward_scale,\n                         cost_scale=args.cost_scale,\n                         device=args.device)\n\n    dataset = TransitionDataset(data,\n                                reward_scale=args.reward_scale,\n                                cost_scale=args.cost_scale)\n    trainloader = DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        num_workers=args.num_workers,\n    )\n    trainloader_iter = iter(trainloader)\n\n    # for saving the best\n    best_reward = -np.inf\n    best_cost = np.inf\n    best_idx = 0\n\n    for step in trange(args.update_steps, desc=\"Training\"):\n        batch = next(trainloader_iter)\n        observations, next_observations, actions, rewards, costs, done = [\n            b.to(args.device) for b in batch\n        ]\n        trainer.train_one_step(observations, next_observations, actions, rewards, costs,\n                               done)\n\n        # evaluation\n        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n            ret, cost, length = trainer.evaluate(args.eval_episodes)\n            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\n            # save the current weight\n            logger.save_checkpoint()\n            # save the best weight\n            if cost < best_cost or (cost == best_cost and ret > best_reward):\n                best_cost = cost\n                best_reward = ret\n                best_idx = step\n                logger.save_checkpoint(suffix=\"best\")\n\n            logger.store(tab=\"train\", best_idx=best_idx)\n            logger.write(step, display=False)\n\n        else:\n            logger.write_without_reset(step)", "\n\nif __name__ == \"__main__\":\n    train()\n"]}
{"filename": "examples/train/__init__.py", "chunked_list": [""]}
{"filename": "examples/train/train_bcql.py", "chunked_list": ["import os\nimport uuid\nimport types\nfrom dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nimport bullet_safety_gym  # noqa\nimport dsrl\nimport gymnasium as gym  # noqa\nimport numpy as np", "import gymnasium as gym  # noqa\nimport numpy as np\nimport pyrallis\nimport torch\nfrom dsrl.infos import DENSITY_CFG\nfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom fsrl.utils import WandbLogger\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import trange  # noqa\n", "from tqdm.auto import trange  # noqa\n\nfrom examples.configs.bcql_configs import BCQL_DEFAULT_CONFIG, BCQLTrainConfig\nfrom osrl.algorithms import BCQL, BCQLTrainer\nfrom osrl.common import TransitionDataset\nfrom osrl.common.exp_util import auto_name, seed_all\n\n\n@pyrallis.wrap()\ndef train(args: BCQLTrainConfig):\n    # update config\n    cfg, old_cfg = asdict(args), asdict(BCQLTrainConfig())\n    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n    cfg = asdict(BCQL_DEFAULT_CONFIG[args.task]())\n    cfg.update(differing_values)\n    args = types.SimpleNamespace(**cfg)\n\n    # setup logger\n    default_cfg = asdict(BCQL_DEFAULT_CONFIG[args.task]())\n    if args.name is None:\n        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n    if args.group is None:\n        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n    if args.logdir is not None:\n        args.logdir = os.path.join(args.logdir, args.group, args.name)\n    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n    logger.save_config(cfg, verbose=args.verbose)\n\n    # set seed\n    seed_all(args.seed)\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    # initialize environment\n    env = gym.make(args.task)\n\n    # pre-process offline dataset\n    data = env.get_dataset()\n    env.set_target_cost(args.cost_limit)\n\n    cbins, rbins, max_npb, min_npb = None, None, None, None\n    if args.density != 1.0:\n        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n        cbins = density_cfg[\"cbins\"]\n        rbins = density_cfg[\"rbins\"]\n        max_npb = density_cfg[\"max_npb\"]\n        min_npb = density_cfg[\"min_npb\"]\n    data = env.pre_process_data(data,\n                                args.outliers_percent,\n                                args.noise_scale,\n                                args.inpaint_ranges,\n                                args.epsilon,\n                                args.density,\n                                cbins=cbins,\n                                rbins=rbins,\n                                max_npb=max_npb,\n                                min_npb=min_npb)\n\n    # wrapper\n    env = wrap_env(\n        env=env,\n        reward_scale=args.reward_scale,\n    )\n    env = OfflineEnvWrapper(env)\n\n    # model & optimizer setup\n    model = BCQL(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=args.a_hidden_sizes,\n        c_hidden_sizes=args.c_hidden_sizes,\n        vae_hidden_sizes=args.vae_hidden_sizes,\n        sample_action_num=args.sample_action_num,\n        PID=args.PID,\n        gamma=args.gamma,\n        tau=args.tau,\n        lmbda=args.lmbda,\n        beta=args.beta,\n        phi=args.phi,\n        num_q=args.num_q,\n        num_qc=args.num_qc,\n        cost_limit=args.cost_limit,\n        episode_len=args.episode_len,\n        device=args.device,\n    )\n    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    def checkpoint_fn():\n        return {\"model_state\": model.state_dict()}\n\n    logger.setup_checkpoint_fn(checkpoint_fn)\n\n    # trainer\n    trainer = BCQLTrainer(model,\n                          env,\n                          logger=logger,\n                          actor_lr=args.actor_lr,\n                          critic_lr=args.critic_lr,\n                          vae_lr=args.vae_lr,\n                          reward_scale=args.reward_scale,\n                          cost_scale=args.cost_scale,\n                          device=args.device)\n\n    # initialize pytorch dataloader\n    dataset = TransitionDataset(data,\n                                reward_scale=args.reward_scale,\n                                cost_scale=args.cost_scale)\n    trainloader = DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        num_workers=args.num_workers,\n    )\n    trainloader_iter = iter(trainloader)\n\n    # for saving the best\n    best_reward = -np.inf\n    best_cost = np.inf\n    best_idx = 0\n\n    # training\n    for step in trange(args.update_steps, desc=\"Training\"):\n        batch = next(trainloader_iter)\n        observations, next_observations, actions, rewards, costs, done = [\n            b.to(args.device) for b in batch\n        ]\n        trainer.train_one_step(observations, next_observations, actions, rewards, costs,\n                               done)\n\n        # evaluation\n        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n            ret, cost, length = trainer.evaluate(args.eval_episodes)\n            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\n            # save the current weight\n            logger.save_checkpoint()\n            # save the best weight\n            if cost < best_cost or (cost == best_cost and ret > best_reward):\n                best_cost = cost\n                best_reward = ret\n                best_idx = step\n                logger.save_checkpoint(suffix=\"best\")\n\n            logger.store(tab=\"train\", best_idx=best_idx)\n            logger.write(step, display=False)\n\n        else:\n            logger.write_without_reset(step)", "@pyrallis.wrap()\ndef train(args: BCQLTrainConfig):\n    # update config\n    cfg, old_cfg = asdict(args), asdict(BCQLTrainConfig())\n    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n    cfg = asdict(BCQL_DEFAULT_CONFIG[args.task]())\n    cfg.update(differing_values)\n    args = types.SimpleNamespace(**cfg)\n\n    # setup logger\n    default_cfg = asdict(BCQL_DEFAULT_CONFIG[args.task]())\n    if args.name is None:\n        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n    if args.group is None:\n        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n    if args.logdir is not None:\n        args.logdir = os.path.join(args.logdir, args.group, args.name)\n    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n    logger.save_config(cfg, verbose=args.verbose)\n\n    # set seed\n    seed_all(args.seed)\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    # initialize environment\n    env = gym.make(args.task)\n\n    # pre-process offline dataset\n    data = env.get_dataset()\n    env.set_target_cost(args.cost_limit)\n\n    cbins, rbins, max_npb, min_npb = None, None, None, None\n    if args.density != 1.0:\n        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n        cbins = density_cfg[\"cbins\"]\n        rbins = density_cfg[\"rbins\"]\n        max_npb = density_cfg[\"max_npb\"]\n        min_npb = density_cfg[\"min_npb\"]\n    data = env.pre_process_data(data,\n                                args.outliers_percent,\n                                args.noise_scale,\n                                args.inpaint_ranges,\n                                args.epsilon,\n                                args.density,\n                                cbins=cbins,\n                                rbins=rbins,\n                                max_npb=max_npb,\n                                min_npb=min_npb)\n\n    # wrapper\n    env = wrap_env(\n        env=env,\n        reward_scale=args.reward_scale,\n    )\n    env = OfflineEnvWrapper(env)\n\n    # model & optimizer setup\n    model = BCQL(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=args.a_hidden_sizes,\n        c_hidden_sizes=args.c_hidden_sizes,\n        vae_hidden_sizes=args.vae_hidden_sizes,\n        sample_action_num=args.sample_action_num,\n        PID=args.PID,\n        gamma=args.gamma,\n        tau=args.tau,\n        lmbda=args.lmbda,\n        beta=args.beta,\n        phi=args.phi,\n        num_q=args.num_q,\n        num_qc=args.num_qc,\n        cost_limit=args.cost_limit,\n        episode_len=args.episode_len,\n        device=args.device,\n    )\n    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    def checkpoint_fn():\n        return {\"model_state\": model.state_dict()}\n\n    logger.setup_checkpoint_fn(checkpoint_fn)\n\n    # trainer\n    trainer = BCQLTrainer(model,\n                          env,\n                          logger=logger,\n                          actor_lr=args.actor_lr,\n                          critic_lr=args.critic_lr,\n                          vae_lr=args.vae_lr,\n                          reward_scale=args.reward_scale,\n                          cost_scale=args.cost_scale,\n                          device=args.device)\n\n    # initialize pytorch dataloader\n    dataset = TransitionDataset(data,\n                                reward_scale=args.reward_scale,\n                                cost_scale=args.cost_scale)\n    trainloader = DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        num_workers=args.num_workers,\n    )\n    trainloader_iter = iter(trainloader)\n\n    # for saving the best\n    best_reward = -np.inf\n    best_cost = np.inf\n    best_idx = 0\n\n    # training\n    for step in trange(args.update_steps, desc=\"Training\"):\n        batch = next(trainloader_iter)\n        observations, next_observations, actions, rewards, costs, done = [\n            b.to(args.device) for b in batch\n        ]\n        trainer.train_one_step(observations, next_observations, actions, rewards, costs,\n                               done)\n\n        # evaluation\n        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n            ret, cost, length = trainer.evaluate(args.eval_episodes)\n            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\n            # save the current weight\n            logger.save_checkpoint()\n            # save the best weight\n            if cost < best_cost or (cost == best_cost and ret > best_reward):\n                best_cost = cost\n                best_reward = ret\n                best_idx = step\n                logger.save_checkpoint(suffix=\"best\")\n\n            logger.store(tab=\"train\", best_idx=best_idx)\n            logger.write(step, display=False)\n\n        else:\n            logger.write_without_reset(step)", "\n\nif __name__ == \"__main__\":\n    train()\n"]}
{"filename": "examples/train/train_bearl.py", "chunked_list": ["import os\nimport uuid\nimport types\nfrom dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nimport bullet_safety_gym  # noqa\nimport dsrl\nimport gymnasium as gym  # noqa\nimport numpy as np", "import gymnasium as gym  # noqa\nimport numpy as np\nimport pyrallis\nimport torch\nfrom dsrl.infos import DENSITY_CFG\nfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom fsrl.utils import WandbLogger\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import trange  # noqa\n", "from tqdm.auto import trange  # noqa\n\nfrom examples.configs.bearl_configs import BEARL_DEFAULT_CONFIG, BEARLTrainConfig\nfrom osrl.algorithms import BEARL, BEARLTrainer\nfrom osrl.common import TransitionDataset\nfrom osrl.common.exp_util import auto_name, seed_all\n\n\n@pyrallis.wrap()\ndef train(args: BEARLTrainConfig):\n    # update config\n    cfg, old_cfg = asdict(args), asdict(BEARLTrainConfig())\n    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n    cfg = asdict(BEARL_DEFAULT_CONFIG[args.task]())\n    cfg.update(differing_values)\n    args = types.SimpleNamespace(**cfg)\n\n    # setup logger\n    default_cfg = asdict(BEARL_DEFAULT_CONFIG[args.task]())\n    if args.name is None:\n        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n    if args.group is None:\n        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n    if args.logdir is not None:\n        args.logdir = os.path.join(args.logdir, args.group, args.name)\n    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n    logger.save_config(cfg, verbose=args.verbose)\n\n    # set seed\n    seed_all(args.seed)\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    # initialize environment\n    env = gym.make(args.task)\n\n    # pre-process offline dataset\n    data = env.get_dataset()\n    env.set_target_cost(args.cost_limit)\n\n    cbins, rbins, max_npb, min_npb = None, None, None, None\n    if args.density != 1.0:\n        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n        cbins = density_cfg[\"cbins\"]\n        rbins = density_cfg[\"rbins\"]\n        max_npb = density_cfg[\"max_npb\"]\n        min_npb = density_cfg[\"min_npb\"]\n    data = env.pre_process_data(data,\n                                args.outliers_percent,\n                                args.noise_scale,\n                                args.inpaint_ranges,\n                                args.epsilon,\n                                args.density,\n                                cbins=cbins,\n                                rbins=rbins,\n                                max_npb=max_npb,\n                                min_npb=min_npb)\n\n    # wrapper\n    env = wrap_env(\n        env=env,\n        reward_scale=args.reward_scale,\n    )\n    env = OfflineEnvWrapper(env)\n\n    # model & optimizer setup\n    model = BEARL(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=args.a_hidden_sizes,\n        c_hidden_sizes=args.c_hidden_sizes,\n        vae_hidden_sizes=args.vae_hidden_sizes,\n        sample_action_num=args.sample_action_num,\n        gamma=args.gamma,\n        tau=args.tau,\n        beta=args.beta,\n        lmbda=args.lmbda,\n        mmd_sigma=args.mmd_sigma,\n        target_mmd_thresh=args.target_mmd_thresh,\n        start_update_policy_step=args.start_update_policy_step,\n        num_q=args.num_q,\n        num_qc=args.num_qc,\n        PID=args.PID,\n        cost_limit=args.cost_limit,\n        episode_len=args.episode_len,\n        device=args.device,\n    )\n    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    def checkpoint_fn():\n        return {\"model_state\": model.state_dict()}\n\n    logger.setup_checkpoint_fn(checkpoint_fn)\n\n    # trainer\n    trainer = BEARLTrainer(model,\n                           env,\n                           logger=logger,\n                           actor_lr=args.actor_lr,\n                           critic_lr=args.critic_lr,\n                           vae_lr=args.vae_lr,\n                           reward_scale=args.reward_scale,\n                           cost_scale=args.cost_scale,\n                           device=args.device)\n\n    # initialize pytorch dataloader\n    dataset = TransitionDataset(data,\n                                reward_scale=args.reward_scale,\n                                cost_scale=args.cost_scale)\n    trainloader = DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        num_workers=args.num_workers,\n    )\n    trainloader_iter = iter(trainloader)\n\n    # for saving the best\n    best_reward = -np.inf\n    best_cost = np.inf\n    best_idx = 0\n\n    # training\n    for step in trange(args.update_steps, desc=\"Training\"):\n        batch = next(trainloader_iter)\n        observations, next_observations, actions, rewards, costs, done = [\n            b.to(args.device) for b in batch\n        ]\n        trainer.train_one_step(observations, next_observations, actions, rewards, costs,\n                               done)\n\n        # evaluation\n        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n            ret, cost, length = trainer.evaluate(args.eval_episodes)\n            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\n            # save the current weight\n            logger.save_checkpoint()\n            # save the best weight\n            if cost < best_cost or (cost == best_cost and ret > best_reward):\n                best_cost = cost\n                best_reward = ret\n                best_idx = step\n                logger.save_checkpoint(suffix=\"best\")\n\n            logger.store(tab=\"train\", best_idx=best_idx)\n            logger.write(step, display=False)\n\n        else:\n            logger.write_without_reset(step)", "@pyrallis.wrap()\ndef train(args: BEARLTrainConfig):\n    # update config\n    cfg, old_cfg = asdict(args), asdict(BEARLTrainConfig())\n    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n    cfg = asdict(BEARL_DEFAULT_CONFIG[args.task]())\n    cfg.update(differing_values)\n    args = types.SimpleNamespace(**cfg)\n\n    # setup logger\n    default_cfg = asdict(BEARL_DEFAULT_CONFIG[args.task]())\n    if args.name is None:\n        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n    if args.group is None:\n        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n    if args.logdir is not None:\n        args.logdir = os.path.join(args.logdir, args.group, args.name)\n    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n    logger.save_config(cfg, verbose=args.verbose)\n\n    # set seed\n    seed_all(args.seed)\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    # initialize environment\n    env = gym.make(args.task)\n\n    # pre-process offline dataset\n    data = env.get_dataset()\n    env.set_target_cost(args.cost_limit)\n\n    cbins, rbins, max_npb, min_npb = None, None, None, None\n    if args.density != 1.0:\n        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n        cbins = density_cfg[\"cbins\"]\n        rbins = density_cfg[\"rbins\"]\n        max_npb = density_cfg[\"max_npb\"]\n        min_npb = density_cfg[\"min_npb\"]\n    data = env.pre_process_data(data,\n                                args.outliers_percent,\n                                args.noise_scale,\n                                args.inpaint_ranges,\n                                args.epsilon,\n                                args.density,\n                                cbins=cbins,\n                                rbins=rbins,\n                                max_npb=max_npb,\n                                min_npb=min_npb)\n\n    # wrapper\n    env = wrap_env(\n        env=env,\n        reward_scale=args.reward_scale,\n    )\n    env = OfflineEnvWrapper(env)\n\n    # model & optimizer setup\n    model = BEARL(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        a_hidden_sizes=args.a_hidden_sizes,\n        c_hidden_sizes=args.c_hidden_sizes,\n        vae_hidden_sizes=args.vae_hidden_sizes,\n        sample_action_num=args.sample_action_num,\n        gamma=args.gamma,\n        tau=args.tau,\n        beta=args.beta,\n        lmbda=args.lmbda,\n        mmd_sigma=args.mmd_sigma,\n        target_mmd_thresh=args.target_mmd_thresh,\n        start_update_policy_step=args.start_update_policy_step,\n        num_q=args.num_q,\n        num_qc=args.num_qc,\n        PID=args.PID,\n        cost_limit=args.cost_limit,\n        episode_len=args.episode_len,\n        device=args.device,\n    )\n    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    def checkpoint_fn():\n        return {\"model_state\": model.state_dict()}\n\n    logger.setup_checkpoint_fn(checkpoint_fn)\n\n    # trainer\n    trainer = BEARLTrainer(model,\n                           env,\n                           logger=logger,\n                           actor_lr=args.actor_lr,\n                           critic_lr=args.critic_lr,\n                           vae_lr=args.vae_lr,\n                           reward_scale=args.reward_scale,\n                           cost_scale=args.cost_scale,\n                           device=args.device)\n\n    # initialize pytorch dataloader\n    dataset = TransitionDataset(data,\n                                reward_scale=args.reward_scale,\n                                cost_scale=args.cost_scale)\n    trainloader = DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        num_workers=args.num_workers,\n    )\n    trainloader_iter = iter(trainloader)\n\n    # for saving the best\n    best_reward = -np.inf\n    best_cost = np.inf\n    best_idx = 0\n\n    # training\n    for step in trange(args.update_steps, desc=\"Training\"):\n        batch = next(trainloader_iter)\n        observations, next_observations, actions, rewards, costs, done = [\n            b.to(args.device) for b in batch\n        ]\n        trainer.train_one_step(observations, next_observations, actions, rewards, costs,\n                               done)\n\n        # evaluation\n        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n            ret, cost, length = trainer.evaluate(args.eval_episodes)\n            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\n            # save the current weight\n            logger.save_checkpoint()\n            # save the best weight\n            if cost < best_cost or (cost == best_cost and ret > best_reward):\n                best_cost = cost\n                best_reward = ret\n                best_idx = step\n                logger.save_checkpoint(suffix=\"best\")\n\n            logger.store(tab=\"train\", best_idx=best_idx)\n            logger.write(step, display=False)\n\n        else:\n            logger.write_without_reset(step)", "\n\nif __name__ == \"__main__\":\n    train()\n"]}
{"filename": "examples/train/train_cdt.py", "chunked_list": ["import os\nimport uuid\nimport types\nfrom dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nimport bullet_safety_gym  # noqa\nimport dsrl\nimport gymnasium as gym  # noqa\nimport numpy as np", "import gymnasium as gym  # noqa\nimport numpy as np\nimport pyrallis\nimport torch\nfrom dsrl.infos import DENSITY_CFG\nfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom fsrl.utils import WandbLogger\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import trange  # noqa\n", "from tqdm.auto import trange  # noqa\n\nfrom examples.configs.cdt_configs import CDT_DEFAULT_CONFIG, CDTTrainConfig\nfrom osrl.algorithms import CDT, CDTTrainer\nfrom osrl.common import SequenceDataset\nfrom osrl.common.exp_util import auto_name, seed_all\n\n\n@pyrallis.wrap()\ndef train(args: CDTTrainConfig):\n    # update config\n    cfg, old_cfg = asdict(args), asdict(CDTTrainConfig())\n    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n    cfg = asdict(CDT_DEFAULT_CONFIG[args.task]())\n    cfg.update(differing_values)\n    args = types.SimpleNamespace(**cfg)\n\n    # setup logger\n    default_cfg = asdict(CDT_DEFAULT_CONFIG[args.task]())\n    if args.name is None:\n        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n    if args.group is None:\n        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n    if args.logdir is not None:\n        args.logdir = os.path.join(args.logdir, args.group, args.name)\n    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n    logger.save_config(cfg, verbose=args.verbose)\n\n    # set seed\n    seed_all(args.seed)\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    # initialize environment\n    env = gym.make(args.task)\n\n    # pre-process offline dataset\n    data = env.get_dataset()\n    env.set_target_cost(args.cost_limit)\n\n    cbins, rbins, max_npb, min_npb = None, None, None, None\n    if args.density != 1.0:\n        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n        cbins = density_cfg[\"cbins\"]\n        rbins = density_cfg[\"rbins\"]\n        max_npb = density_cfg[\"max_npb\"]\n        min_npb = density_cfg[\"min_npb\"]\n    data = env.pre_process_data(data,\n                                args.outliers_percent,\n                                args.noise_scale,\n                                args.inpaint_ranges,\n                                args.epsilon,\n                                args.density,\n                                cbins=cbins,\n                                rbins=rbins,\n                                max_npb=max_npb,\n                                min_npb=min_npb)\n\n    # wrapper\n    env = wrap_env(\n        env=env,\n        reward_scale=args.reward_scale,\n    )\n    env = OfflineEnvWrapper(env)\n\n    # model & optimizer & scheduler setup\n    model = CDT(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        embedding_dim=args.embedding_dim,\n        seq_len=args.seq_len,\n        episode_len=args.episode_len,\n        num_layers=args.num_layers,\n        num_heads=args.num_heads,\n        attention_dropout=args.attention_dropout,\n        residual_dropout=args.residual_dropout,\n        embedding_dropout=args.embedding_dropout,\n        time_emb=args.time_emb,\n        use_rew=args.use_rew,\n        use_cost=args.use_cost,\n        cost_transform=args.cost_transform,\n        add_cost_feat=args.add_cost_feat,\n        mul_cost_feat=args.mul_cost_feat,\n        cat_cost_feat=args.cat_cost_feat,\n        action_head_layers=args.action_head_layers,\n        cost_prefix=args.cost_prefix,\n        stochastic=args.stochastic,\n        init_temperature=args.init_temperature,\n        target_entropy=-env.action_space.shape[0],\n    ).to(args.device)\n    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    def checkpoint_fn():\n        return {\"model_state\": model.state_dict()}\n\n    logger.setup_checkpoint_fn(checkpoint_fn)\n\n    # trainer\n    trainer = CDTTrainer(model,\n                         env,\n                         logger=logger,\n                         learning_rate=args.learning_rate,\n                         weight_decay=args.weight_decay,\n                         betas=args.betas,\n                         clip_grad=args.clip_grad,\n                         lr_warmup_steps=args.lr_warmup_steps,\n                         reward_scale=args.reward_scale,\n                         cost_scale=args.cost_scale,\n                         loss_cost_weight=args.loss_cost_weight,\n                         loss_state_weight=args.loss_state_weight,\n                         cost_reverse=args.cost_reverse,\n                         no_entropy=args.no_entropy,\n                         device=args.device)\n\n    ct = lambda x: 70 - x if args.linear else 1 / (x + 10)\n\n    dataset = SequenceDataset(\n        data,\n        seq_len=args.seq_len,\n        reward_scale=args.reward_scale,\n        cost_scale=args.cost_scale,\n        deg=args.deg,\n        pf_sample=args.pf_sample,\n        max_rew_decrease=args.max_rew_decrease,\n        beta=args.beta,\n        augment_percent=args.augment_percent,\n        cost_reverse=args.cost_reverse,\n        max_reward=args.max_reward,\n        min_reward=args.min_reward,\n        pf_only=args.pf_only,\n        rmin=args.rmin,\n        cost_bins=args.cost_bins,\n        npb=args.npb,\n        cost_sample=args.cost_sample,\n        cost_transform=ct,\n        start_sampling=args.start_sampling,\n        prob=args.prob,\n        random_aug=args.random_aug,\n        aug_rmin=args.aug_rmin,\n        aug_rmax=args.aug_rmax,\n        aug_cmin=args.aug_cmin,\n        aug_cmax=args.aug_cmax,\n        cgap=args.cgap,\n        rstd=args.rstd,\n        cstd=args.cstd,\n    )\n\n    trainloader = DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        num_workers=args.num_workers,\n    )\n    trainloader_iter = iter(trainloader)\n\n    # for saving the best\n    best_reward = -np.inf\n    best_cost = np.inf\n    best_idx = 0\n\n    for step in trange(args.update_steps, desc=\"Training\"):\n        batch = next(trainloader_iter)\n        states, actions, returns, costs_return, time_steps, mask, episode_cost, costs = [\n            b.to(args.device) for b in batch\n        ]\n        trainer.train_one_step(states, actions, returns, costs_return, time_steps, mask,\n                               episode_cost, costs)\n\n        # evaluation\n        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n            average_reward, average_cost = [], []\n            log_cost, log_reward, log_len = {}, {}, {}\n            for target_return in args.target_returns:\n                reward_return, cost_return = target_return\n                if args.cost_reverse:\n                    # critical step, rescale the return!\n                    ret, cost, length = trainer.evaluate(\n                        args.eval_episodes, reward_return * args.reward_scale,\n                        (args.episode_len - cost_return) * args.cost_scale)\n                else:\n                    ret, cost, length = trainer.evaluate(\n                        args.eval_episodes, reward_return * args.reward_scale,\n                        cost_return * args.cost_scale)\n                average_cost.append(cost)\n                average_reward.append(ret)\n\n                name = \"c_\" + str(int(cost_return)) + \"_r_\" + str(int(reward_return))\n                log_cost.update({name: cost})\n                log_reward.update({name: ret})\n                log_len.update({name: length})\n\n            logger.store(tab=\"cost\", **log_cost)\n            logger.store(tab=\"ret\", **log_reward)\n            logger.store(tab=\"length\", **log_len)\n\n            # save the current weight\n            logger.save_checkpoint()\n            # save the best weight\n            mean_ret = np.mean(average_reward)\n            mean_cost = np.mean(average_cost)\n            if mean_cost < best_cost or (mean_cost == best_cost\n                                         and mean_ret > best_reward):\n                best_cost = mean_cost\n                best_reward = mean_ret\n                best_idx = step\n                logger.save_checkpoint(suffix=\"best\")\n\n            logger.store(tab=\"train\", best_idx=best_idx)\n            logger.write(step, display=False)\n\n        else:\n            logger.write_without_reset(step)", "@pyrallis.wrap()\ndef train(args: CDTTrainConfig):\n    # update config\n    cfg, old_cfg = asdict(args), asdict(CDTTrainConfig())\n    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n    cfg = asdict(CDT_DEFAULT_CONFIG[args.task]())\n    cfg.update(differing_values)\n    args = types.SimpleNamespace(**cfg)\n\n    # setup logger\n    default_cfg = asdict(CDT_DEFAULT_CONFIG[args.task]())\n    if args.name is None:\n        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n    if args.group is None:\n        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n    if args.logdir is not None:\n        args.logdir = os.path.join(args.logdir, args.group, args.name)\n    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n    logger.save_config(cfg, verbose=args.verbose)\n\n    # set seed\n    seed_all(args.seed)\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    # initialize environment\n    env = gym.make(args.task)\n\n    # pre-process offline dataset\n    data = env.get_dataset()\n    env.set_target_cost(args.cost_limit)\n\n    cbins, rbins, max_npb, min_npb = None, None, None, None\n    if args.density != 1.0:\n        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n        cbins = density_cfg[\"cbins\"]\n        rbins = density_cfg[\"rbins\"]\n        max_npb = density_cfg[\"max_npb\"]\n        min_npb = density_cfg[\"min_npb\"]\n    data = env.pre_process_data(data,\n                                args.outliers_percent,\n                                args.noise_scale,\n                                args.inpaint_ranges,\n                                args.epsilon,\n                                args.density,\n                                cbins=cbins,\n                                rbins=rbins,\n                                max_npb=max_npb,\n                                min_npb=min_npb)\n\n    # wrapper\n    env = wrap_env(\n        env=env,\n        reward_scale=args.reward_scale,\n    )\n    env = OfflineEnvWrapper(env)\n\n    # model & optimizer & scheduler setup\n    model = CDT(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        embedding_dim=args.embedding_dim,\n        seq_len=args.seq_len,\n        episode_len=args.episode_len,\n        num_layers=args.num_layers,\n        num_heads=args.num_heads,\n        attention_dropout=args.attention_dropout,\n        residual_dropout=args.residual_dropout,\n        embedding_dropout=args.embedding_dropout,\n        time_emb=args.time_emb,\n        use_rew=args.use_rew,\n        use_cost=args.use_cost,\n        cost_transform=args.cost_transform,\n        add_cost_feat=args.add_cost_feat,\n        mul_cost_feat=args.mul_cost_feat,\n        cat_cost_feat=args.cat_cost_feat,\n        action_head_layers=args.action_head_layers,\n        cost_prefix=args.cost_prefix,\n        stochastic=args.stochastic,\n        init_temperature=args.init_temperature,\n        target_entropy=-env.action_space.shape[0],\n    ).to(args.device)\n    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    def checkpoint_fn():\n        return {\"model_state\": model.state_dict()}\n\n    logger.setup_checkpoint_fn(checkpoint_fn)\n\n    # trainer\n    trainer = CDTTrainer(model,\n                         env,\n                         logger=logger,\n                         learning_rate=args.learning_rate,\n                         weight_decay=args.weight_decay,\n                         betas=args.betas,\n                         clip_grad=args.clip_grad,\n                         lr_warmup_steps=args.lr_warmup_steps,\n                         reward_scale=args.reward_scale,\n                         cost_scale=args.cost_scale,\n                         loss_cost_weight=args.loss_cost_weight,\n                         loss_state_weight=args.loss_state_weight,\n                         cost_reverse=args.cost_reverse,\n                         no_entropy=args.no_entropy,\n                         device=args.device)\n\n    ct = lambda x: 70 - x if args.linear else 1 / (x + 10)\n\n    dataset = SequenceDataset(\n        data,\n        seq_len=args.seq_len,\n        reward_scale=args.reward_scale,\n        cost_scale=args.cost_scale,\n        deg=args.deg,\n        pf_sample=args.pf_sample,\n        max_rew_decrease=args.max_rew_decrease,\n        beta=args.beta,\n        augment_percent=args.augment_percent,\n        cost_reverse=args.cost_reverse,\n        max_reward=args.max_reward,\n        min_reward=args.min_reward,\n        pf_only=args.pf_only,\n        rmin=args.rmin,\n        cost_bins=args.cost_bins,\n        npb=args.npb,\n        cost_sample=args.cost_sample,\n        cost_transform=ct,\n        start_sampling=args.start_sampling,\n        prob=args.prob,\n        random_aug=args.random_aug,\n        aug_rmin=args.aug_rmin,\n        aug_rmax=args.aug_rmax,\n        aug_cmin=args.aug_cmin,\n        aug_cmax=args.aug_cmax,\n        cgap=args.cgap,\n        rstd=args.rstd,\n        cstd=args.cstd,\n    )\n\n    trainloader = DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        num_workers=args.num_workers,\n    )\n    trainloader_iter = iter(trainloader)\n\n    # for saving the best\n    best_reward = -np.inf\n    best_cost = np.inf\n    best_idx = 0\n\n    for step in trange(args.update_steps, desc=\"Training\"):\n        batch = next(trainloader_iter)\n        states, actions, returns, costs_return, time_steps, mask, episode_cost, costs = [\n            b.to(args.device) for b in batch\n        ]\n        trainer.train_one_step(states, actions, returns, costs_return, time_steps, mask,\n                               episode_cost, costs)\n\n        # evaluation\n        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n            average_reward, average_cost = [], []\n            log_cost, log_reward, log_len = {}, {}, {}\n            for target_return in args.target_returns:\n                reward_return, cost_return = target_return\n                if args.cost_reverse:\n                    # critical step, rescale the return!\n                    ret, cost, length = trainer.evaluate(\n                        args.eval_episodes, reward_return * args.reward_scale,\n                        (args.episode_len - cost_return) * args.cost_scale)\n                else:\n                    ret, cost, length = trainer.evaluate(\n                        args.eval_episodes, reward_return * args.reward_scale,\n                        cost_return * args.cost_scale)\n                average_cost.append(cost)\n                average_reward.append(ret)\n\n                name = \"c_\" + str(int(cost_return)) + \"_r_\" + str(int(reward_return))\n                log_cost.update({name: cost})\n                log_reward.update({name: ret})\n                log_len.update({name: length})\n\n            logger.store(tab=\"cost\", **log_cost)\n            logger.store(tab=\"ret\", **log_reward)\n            logger.store(tab=\"length\", **log_len)\n\n            # save the current weight\n            logger.save_checkpoint()\n            # save the best weight\n            mean_ret = np.mean(average_reward)\n            mean_cost = np.mean(average_cost)\n            if mean_cost < best_cost or (mean_cost == best_cost\n                                         and mean_ret > best_reward):\n                best_cost = mean_cost\n                best_reward = mean_ret\n                best_idx = step\n                logger.save_checkpoint(suffix=\"best\")\n\n            logger.store(tab=\"train\", best_idx=best_idx)\n            logger.write(step, display=False)\n\n        else:\n            logger.write_without_reset(step)", "\n\nif __name__ == \"__main__\":\n    train()\n"]}
{"filename": "examples/train/train_coptidice.py", "chunked_list": ["import os\nimport uuid\nimport types\nfrom dataclasses import asdict, dataclass\nfrom typing import Any, DefaultDict, Dict, List, Optional, Tuple\n\nimport bullet_safety_gym  # noqa\nimport dsrl\nimport gymnasium as gym  # noqa\nimport numpy as np", "import gymnasium as gym  # noqa\nimport numpy as np\nimport pyrallis\nimport torch\nfrom dsrl.infos import DENSITY_CFG\nfrom dsrl.offline_env import OfflineEnvWrapper, wrap_env  # noqa\nfrom fsrl.utils import WandbLogger\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import trange  # noqa\n", "from tqdm.auto import trange  # noqa\n\nfrom examples.configs.coptidice_configs import (COptiDICE_DEFAULT_CONFIG,\n                                                COptiDICETrainConfig)\nfrom osrl.algorithms import COptiDICE, COptiDICETrainer\nfrom osrl.common import TransitionDataset\nfrom osrl.common.exp_util import auto_name, seed_all\n\n\n@pyrallis.wrap()\ndef train(args: COptiDICETrainConfig):\n    # update config\n    cfg, old_cfg = asdict(args), asdict(COptiDICETrainConfig())\n    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n    cfg = asdict(COptiDICE_DEFAULT_CONFIG[args.task]())\n    cfg.update(differing_values)\n    args = types.SimpleNamespace(**cfg)\n\n    # setup logger\n    default_cfg = asdict(COptiDICE_DEFAULT_CONFIG[args.task]())\n    if args.name is None:\n        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n    if args.group is None:\n        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n    if args.logdir is not None:\n        args.logdir = os.path.join(args.logdir, args.group, args.name)\n    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n    logger.save_config(cfg, verbose=args.verbose)\n\n    # set seed\n    seed_all(args.seed)\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    # initialize environment\n    env = gym.make(args.task)\n\n    # pre-process offline dataset\n    data = env.get_dataset()\n    env.set_target_cost(args.cost_limit)\n\n    cbins, rbins, max_npb, min_npb = None, None, None, None\n    if args.density != 1.0:\n        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n        cbins = density_cfg[\"cbins\"]\n        rbins = density_cfg[\"rbins\"]\n        max_npb = density_cfg[\"max_npb\"]\n        min_npb = density_cfg[\"min_npb\"]\n    data = env.pre_process_data(data,\n                                args.outliers_percent,\n                                args.noise_scale,\n                                args.inpaint_ranges,\n                                args.epsilon,\n                                args.density,\n                                cbins=cbins,\n                                rbins=rbins,\n                                max_npb=max_npb,\n                                min_npb=min_npb)\n\n    # wrapper\n    env = wrap_env(\n        env=env,\n        reward_scale=args.reward_scale,\n    )\n    env = OfflineEnvWrapper(env)\n\n    # setup dataset\n    dataset = TransitionDataset(data,\n                                reward_scale=args.reward_scale,\n                                cost_scale=args.cost_scale,\n                                state_init=True)\n    trainloader = DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        num_workers=args.num_workers,\n    )\n    trainloader_iter = iter(trainloader)\n    init_s_propotion, obs_std, act_std = dataset.get_dataset_states()\n\n    # setup model\n    model = COptiDICE(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        f_type=args.f_type,\n        init_state_propotion=init_s_propotion,\n        observations_std=obs_std,\n        actions_std=act_std,\n        a_hidden_sizes=args.a_hidden_sizes,\n        c_hidden_sizes=args.c_hidden_sizes,\n        gamma=args.gamma,\n        alpha=args.alpha,\n        cost_ub_epsilon=args.cost_ub_epsilon,\n        num_nu=args.num_nu,\n        num_chi=args.num_chi,\n        cost_limit=args.cost_limit,\n        episode_len=args.episode_len,\n        device=args.device,\n    )\n\n    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    def checkpoint_fn():\n        return {\"model_state\": model.state_dict()}\n\n    logger.setup_checkpoint_fn(checkpoint_fn)\n\n    trainer = COptiDICETrainer(model,\n                               env,\n                               logger=logger,\n                               actor_lr=args.actor_lr,\n                               critic_lr=args.critic_lr,\n                               scalar_lr=args.scalar_lr,\n                               reward_scale=args.reward_scale,\n                               cost_scale=args.cost_scale,\n                               device=args.device)\n\n    # for saving the best\n    best_reward = -np.inf\n    best_cost = np.inf\n    best_idx = 0\n\n    for step in trange(args.update_steps, desc=\"Training\"):\n        batch = next(trainloader_iter)\n        batch = [b.to(args.device) for b in batch]\n        trainer.train_one_step(batch)\n\n        # evaluation\n        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n            ret, cost, length = trainer.evaluate(args.eval_episodes)\n            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\n            # save the current weight\n            logger.save_checkpoint()\n            # save the best weight\n            if cost < best_cost or (cost == best_cost and ret > best_reward):\n                best_cost = cost\n                best_reward = ret\n                best_idx = step\n                logger.save_checkpoint(suffix=\"best\")\n\n            logger.store(tab=\"train\", best_idx=best_idx)\n            logger.write(step, display=False)\n\n        else:\n            logger.write_without_reset(step)", "\n@pyrallis.wrap()\ndef train(args: COptiDICETrainConfig):\n    # update config\n    cfg, old_cfg = asdict(args), asdict(COptiDICETrainConfig())\n    differing_values = {key: cfg[key] for key in cfg.keys() if cfg[key] != old_cfg[key]}\n    cfg = asdict(COptiDICE_DEFAULT_CONFIG[args.task]())\n    cfg.update(differing_values)\n    args = types.SimpleNamespace(**cfg)\n\n    # setup logger\n    default_cfg = asdict(COptiDICE_DEFAULT_CONFIG[args.task]())\n    if args.name is None:\n        args.name = auto_name(default_cfg, cfg, args.prefix, args.suffix)\n    if args.group is None:\n        args.group = args.task + \"-cost-\" + str(int(args.cost_limit))\n    if args.logdir is not None:\n        args.logdir = os.path.join(args.logdir, args.group, args.name)\n    logger = WandbLogger(cfg, args.project, args.group, args.name, args.logdir)\n    # logger = TensorboardLogger(args.logdir, log_txt=True, name=args.name)\n    logger.save_config(cfg, verbose=args.verbose)\n\n    # set seed\n    seed_all(args.seed)\n    if args.device == \"cpu\":\n        torch.set_num_threads(args.threads)\n\n    # initialize environment\n    env = gym.make(args.task)\n\n    # pre-process offline dataset\n    data = env.get_dataset()\n    env.set_target_cost(args.cost_limit)\n\n    cbins, rbins, max_npb, min_npb = None, None, None, None\n    if args.density != 1.0:\n        density_cfg = DENSITY_CFG[args.task + \"_density\" + str(args.density)]\n        cbins = density_cfg[\"cbins\"]\n        rbins = density_cfg[\"rbins\"]\n        max_npb = density_cfg[\"max_npb\"]\n        min_npb = density_cfg[\"min_npb\"]\n    data = env.pre_process_data(data,\n                                args.outliers_percent,\n                                args.noise_scale,\n                                args.inpaint_ranges,\n                                args.epsilon,\n                                args.density,\n                                cbins=cbins,\n                                rbins=rbins,\n                                max_npb=max_npb,\n                                min_npb=min_npb)\n\n    # wrapper\n    env = wrap_env(\n        env=env,\n        reward_scale=args.reward_scale,\n    )\n    env = OfflineEnvWrapper(env)\n\n    # setup dataset\n    dataset = TransitionDataset(data,\n                                reward_scale=args.reward_scale,\n                                cost_scale=args.cost_scale,\n                                state_init=True)\n    trainloader = DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        pin_memory=True,\n        num_workers=args.num_workers,\n    )\n    trainloader_iter = iter(trainloader)\n    init_s_propotion, obs_std, act_std = dataset.get_dataset_states()\n\n    # setup model\n    model = COptiDICE(\n        state_dim=env.observation_space.shape[0],\n        action_dim=env.action_space.shape[0],\n        max_action=env.action_space.high[0],\n        f_type=args.f_type,\n        init_state_propotion=init_s_propotion,\n        observations_std=obs_std,\n        actions_std=act_std,\n        a_hidden_sizes=args.a_hidden_sizes,\n        c_hidden_sizes=args.c_hidden_sizes,\n        gamma=args.gamma,\n        alpha=args.alpha,\n        cost_ub_epsilon=args.cost_ub_epsilon,\n        num_nu=args.num_nu,\n        num_chi=args.num_chi,\n        cost_limit=args.cost_limit,\n        episode_len=args.episode_len,\n        device=args.device,\n    )\n\n    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    def checkpoint_fn():\n        return {\"model_state\": model.state_dict()}\n\n    logger.setup_checkpoint_fn(checkpoint_fn)\n\n    trainer = COptiDICETrainer(model,\n                               env,\n                               logger=logger,\n                               actor_lr=args.actor_lr,\n                               critic_lr=args.critic_lr,\n                               scalar_lr=args.scalar_lr,\n                               reward_scale=args.reward_scale,\n                               cost_scale=args.cost_scale,\n                               device=args.device)\n\n    # for saving the best\n    best_reward = -np.inf\n    best_cost = np.inf\n    best_idx = 0\n\n    for step in trange(args.update_steps, desc=\"Training\"):\n        batch = next(trainloader_iter)\n        batch = [b.to(args.device) for b in batch]\n        trainer.train_one_step(batch)\n\n        # evaluation\n        if (step + 1) % args.eval_every == 0 or step == args.update_steps - 1:\n            ret, cost, length = trainer.evaluate(args.eval_episodes)\n            logger.store(tab=\"eval\", Cost=cost, Reward=ret, Length=length)\n\n            # save the current weight\n            logger.save_checkpoint()\n            # save the best weight\n            if cost < best_cost or (cost == best_cost and ret > best_reward):\n                best_cost = cost\n                best_reward = ret\n                best_idx = step\n                logger.save_checkpoint(suffix=\"best\")\n\n            logger.store(tab=\"train\", best_idx=best_idx)\n            logger.write(step, display=False)\n\n        else:\n            logger.write_without_reset(step)", "\n\nif __name__ == \"__main__\":\n    train()\n"]}
