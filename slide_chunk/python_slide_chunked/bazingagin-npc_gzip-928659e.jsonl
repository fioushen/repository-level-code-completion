{"filename": "original_codebase/compressors.py", "chunked_list": ["# Compressor Framework\n\nfrom importlib import import_module\n\nimport numpy as np\n\n\nclass DefaultCompressor:\n    \"\"\"For non-neural-based compressor\"\"\"\n\n    def __init__(self, compressor, typ=\"text\"):\n        try:\n            self.compressor = import_module(compressor)\n        except ModuleNotFoundError:\n            raise RuntimeError(\"Unsupported compressor\")\n        self.type = typ\n\n    def get_compressed_len(self, x: str) -> int:\n        \"\"\"\n        Calculates the size of `x` once compressed.\n\n        Arguments:\n            x (str): String to be compressed.\n\n        Returns:\n            int: Length of x after compression.\n        \"\"\"\n        if self.type == \"text\":\n            return len(self.compressor.compress(x.encode(\"utf-8\")))\n        else:\n            return len(self.compressor.compress(np.array(x).tobytes()))\n\n    def get_bits_per_character(self, original_fn: str) -> float:\n        \"\"\"\n        Returns the compressed size of the original function\n        in bits.\n\n        Arguments:\n            original_fn (str): Function name to be compressed.\n\n        Returns:\n            int: Compressed size of original_fn content in bits.\n        \"\"\"\n        with open(original_fn) as fo:\n            data = fo.read()\n            compressed_str = self.compressor.compress(data.encode(\"utf-8\"))\n            return len(compressed_str) * 8 / len(data)", "\n\n\"\"\"Test Compressors\"\"\"\nif __name__ == \"__main__\":\n    comp = DefaultCompressor(\"gzip\")\n    print(comp.get_compressed_len(\"Hello world\"))\n"]}
{"filename": "original_codebase/main_text.py", "chunked_list": ["import argparse\nimport time\nfrom functools import partial\nfrom typing import Callable\n\nfrom compressors import *\nfrom data import *\nfrom experiments import *\nfrom pathos.multiprocessing import ProcessingPool as Pool\nfrom torchtext.datasets import (", "from pathos.multiprocessing import ProcessingPool as Pool\nfrom torchtext.datasets import (\n    AG_NEWS,\n    IMDB,\n    AmazonReviewPolarity,\n    DBpedia,\n    SogouNews,\n    YahooAnswers,\n    YelpReviewPolarity,\n)", "    YelpReviewPolarity,\n)\nfrom utils import *\n\n# np.random.seed(6)\n\n\ndef non_neural_knn_exp(\n    compressor_name: str,\n    test_data: list,\n    test_label: list,\n    train_data: list,\n    train_label: list,\n    agg_func: Callable,\n    dis_func: Callable,\n    k: int,\n    para: bool = True,\n):\n    print(\"KNN with compressor={}\".format(compressor_name))\n    cp = DefaultCompressor(compressor_name)\n    knn_exp_ins = KnnExpText(agg_func, cp, dis_func)\n    start = time.time()\n    if para:\n        with Pool(5) as p:\n            pred_correct_pair = p.map(\n                partial(knn_exp_ins.combine_dis_acc_single, k, train_data, train_label),\n                test_data,\n                test_label,\n            )\n        print(\n            \"accuracy:{}\".format(\n                np.average(np.array(pred_correct_pair, dtype=np.int32)[:, 1])\n            )\n        )\n        # print('accuracy:{}'.format(np.average(np.array(pred_correct_pair, dtype=np.object_)[:, 1])))\n    else:\n        knn_exp_ins.calc_dis(test_data, train_data=train_data)\n        knn_exp_ins.calc_acc(k, test_label, train_label=train_label)\n    print(\"spent: {}\".format(time.time() - start))", "\n\ndef record_distance(\n    compressor_name,\n    test_data,\n    test_portion_name,\n    train_data,\n    agg_func,\n    dis_func,\n    out_dir,\n    para=True,\n):\n    print(\"compressor={}\".format(compressor_name))\n    numpy_dir = os.path.join(out_dir, compressor_name)\n    if not os.path.exists(numpy_dir):\n        os.makedirs(numpy_dir)\n    out_fn = os.path.join(numpy_dir, test_portion_name)\n    cp = DefaultCompressor(compressor_name)\n    knn_exp = KnnExpText(agg_func, cp, dis_func)\n    start = time.time()\n    if para:\n        with Pool(6) as p:\n            distance_for_selected_test = p.map(\n                partial(knn_exp.calc_dis_single_multi, train_data), test_data\n            )\n        np.save(out_fn, np.array(distance_for_selected_test))\n        del distance_for_selected_test\n    else:\n        knn_exp.calc_dis(test_data, train_data=train_data)\n        np.save(out_fn, np.array(knn_exp.distance_matrix))\n    print(\"spent: {}\".format(time.time() - start))", "\n\ndef non_neurl_knn_exp_given_dis(dis_matrix, k, test_label, train_label):\n    knn_exp = KnnExpText(None, None, None)\n    _, correct = knn_exp.calc_acc(\n        k,\n        test_label,\n        train_label=train_label,\n        provided_distance_matrix=dis_matrix,\n        rand=args.random,\n    )\n    return correct", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data_dir\", default=\"data\")\n    parser.add_argument(\"--dataset\", default=\"AG_NEWS\")\n    parser.add_argument(\"--num_test\", type=int, default=100)\n    parser.add_argument(\"--num_train\", type=int, default=100)\n    parser.add_argument(\"--compressor\", default=\"gzip\")\n    parser.add_argument(\"--all_test\", action=\"store_true\", default=False)\n    parser.add_argument(\"--all_train\", action=\"store_true\", default=False)\n    parser.add_argument(\"--para\", action=\"store_true\", default=False)\n    parser.add_argument(\n        \"--record\",\n        action=\"store_true\",\n        default=False,\n        help=\"if record the distance into numpy\",\n    )\n    parser.add_argument(\"--output_dir\", default=\"text_exp_output\")\n    parser.add_argument(\"--test_idx_fn\", default=None)\n    parser.add_argument(\"--test_idx_start\", type=int, default=None)\n    parser.add_argument(\"--test_idx_end\", type=int, default=None)\n    parser.add_argument(\"--distance_fn\", default=None)\n    parser.add_argument(\"--score\", action=\"store_true\", default=False)\n    parser.add_argument(\"--k\", default=2, type=int)\n    parser.add_argument(\"--class_num\", default=5, type=int)\n    parser.add_argument(\"--random\", action=\"store_true\", default=False)\n    args = parser.parse_args()\n    # create output dir\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n    train_idx_fn = os.path.join(\n        args.output_dir,\n        \"{}_train_indicies_{}per_class\".format(args.dataset, args.num_train),\n    )\n    test_idx_fn = os.path.join(\n        args.output_dir,\n        \"{}_test_indicies_{}per_class\".format(args.dataset, args.num_test),\n    )\n    # all dataset class number\n    ds2classes = {\n        \"AG_NEWS\": 4,\n        \"SogouNews\": 5,\n        \"DBpedia\": 14,\n        \"YahooAnswers\": 10,\n        \"20News\": 20,\n        \"Ohsumed\": 23,\n        \"Ohsumed_single\": 23,\n        \"R8\": 8,\n        \"R52\": 52,\n        \"kinnews\": 14,\n        \"swahili\": 6,\n        \"filipino\": 5,\n        \"kirnews\": 14,\n        \"custom\": args.class_num,\n    }\n    # load dataset\n    data_dir = os.path.join(args.data_dir, args.dataset)\n    if args.dataset not in [\n        \"20News\",\n        \"Ohsumed\",\n        \"Ohsumed_single\",\n        \"R8\",\n        \"R52\",\n        \"kinnews\",\n        \"swahili\",\n        \"filipino\",\n        \"kirnews\",\n        \"custom\",\n    ]:\n        dataset_pair = eval(args.dataset)(root=args.data_dir)\n    else:\n        if args.dataset == \"20News\":\n            dataset_pair = load_20news()\n        elif args.dataset == \"Ohsumed\":\n            dataset_pair = load_ohsumed(args.data_dir)\n        elif args.dataset == \"Ohsumed_single\":\n            dataset_pair = load_ohsumed_single(args.data_dir)\n        elif args.dataset == \"R8\" or args.dataset == \"R52\":\n            dataset_pair = load_r8(args.data_dir)\n        elif args.dataset == \"kinnews\":\n            dataset_pair = load_kinnews_kirnews(\n                dataset_name=\"kinnews_kirnews\", data_split=\"kinnews_cleaned\"\n            )\n        elif args.dataset == \"kirnews\":\n            dataset_pair = load_kinnews_kirnews(\n                dataset_name=\"kinnews_kirnews\", data_split=\"kirnews_cleaned\"\n            )\n        elif args.dataset == \"swahili\":\n            dataset_pair = load_swahili()\n        elif args.dataset == \"filipino\":\n            dataset_pair = load_filipino(args.data_dir)\n        else:\n            dataset_pair = load_custom_dataset(args.data_dir)\n    num_classes = ds2classes[args.dataset]\n    # choose indices\n    if not args.all_test:\n        # pick certain number per class\n        if args.test_idx_fn is not None:\n            try:\n                test_idx = np.load(args.test_idx_fn)\n                test_data, test_labels = read_torch_text_labels(\n                    dataset_pair[1], test_idx\n                )\n            except FileNotFoundError:\n                print(\"No generated indices file for test set provided\")\n        elif args.test_idx_start is not None:\n            test_idx = list(range(args.test_idx_start, args.test_idx_end))\n            test_data, test_labels = read_torch_text_labels(dataset_pair[1], test_idx)\n        else:\n            test_data, test_labels = pick_n_sample_from_each_class_given_dataset(\n                dataset_pair[1], args.num_test, test_idx_fn\n            )\n    else:\n        train_pair, test_pair = dataset_pair[0], dataset_pair[1]\n        test_data, test_labels = read_torch_text_labels(\n            test_pair, range(len(test_pair))\n        )\n    if not args.all_train:\n        if args.test_idx_fn is not None or args.test_idx_start is not None:\n            train_idx = np.load(train_idx_fn + \".npy\")\n            train_data, train_labels = read_torch_text_labels(\n                dataset_pair[0], train_idx\n            )\n        else:\n            train_data, train_labels = pick_n_sample_from_each_class_given_dataset(\n                dataset_pair[0], args.num_train, train_idx_fn\n            )\n    else:\n        train_pair, test_pair = dataset_pair[0], dataset_pair[1]\n        train_data, train_labels = read_torch_text_labels(\n            train_pair, range(len(train_pair))\n        )\n    if not args.record:\n        non_neural_knn_exp(\n            args.compressor,\n            test_data,\n            test_labels,\n            train_data,\n            train_labels,\n            agg_by_concat_space,\n            NCD,\n            args.k,\n            para=args.para,\n        )\n    else:\n        if not args.score:\n            if args.test_idx_start is None:\n                start_idx = 0\n            else:\n                start_idx = args.test_idx_start\n            for i in range(0, len(test_data), 100):\n                print(\"from {} to {}\".format(start_idx + i, start_idx + i + 100))\n                output_rel_fn = \"test_dis_idx_from_{}_to_{}\".format(\n                    start_idx + i, start_idx + i + 100\n                )\n                output_dir = os.path.join(\n                    args.output_dir, os.path.join(\"distance\", args.dataset)\n                )\n                record_distance(\n                    args.compressor,\n                    np.array(test_data)[i : i + 100],\n                    output_rel_fn,\n                    train_data,\n                    agg_by_concat_space,\n                    NCD,\n                    output_dir,\n                    para=args.para,\n                )\n        else:\n            if os.path.isdir(args.distance_fn):\n                all_correct = 0\n                total_num = 0\n                for fn in tqdm(os.listdir(args.distance_fn)):\n                    if fn.endswith(\".npy\"):\n                        dis_matrix = np.load(os.path.join(args.distance_fn, fn))\n                        start_idx, end_idx = int(fn.split(\".\")[0].split(\"_\")[-3]), int(\n                            fn.split(\".\")[0].split(\"_\")[-1]\n                        )\n                        sub_test_labels = test_labels[\n                            start_idx:end_idx\n                        ]  # assume all_test=True, all_train=True\n                        correct = non_neurl_knn_exp_given_dis(\n                            dis_matrix, args.k, sub_test_labels, train_labels\n                        )\n                        all_correct += sum(correct)\n                        total_num += len(correct)\n                        del dis_matrix\n                print(\"Altogether Accuracy is: {}\".format(all_correct / total_num))\n            else:\n                dis_matrix = np.load(args.distance_fn)\n                non_neurl_knn_exp_given_dis(\n                    dis_matrix, args.k, test_labels, train_labels\n                )", ""]}
{"filename": "original_codebase/utils.py", "chunked_list": ["from collections.abc import Sequence\n\nimport numpy as np\nimport scipy.stats\nimport torch\n\n\ndef NCD(c1: float, c2: float, c12: float) -> float:\n    \"\"\"\n    Calculates Normalized Compression Distance (NCD).\n\n    Arguments:\n        c1 (float): The compressed length of the first object.\n        c2 (float): The compressed length of the second object.\n        c12 (float): The compressed length of the concatenation of the first\n                     and second objects.\n\n    Returns:\n        float: The Normalized Compression Distance c1 and c2.\n\n    Formula:\n        NCD(c1, c2, c12) = (c12 - min(c1, c2)) / max(c1, c2)\n    \"\"\"\n\n    distance = (c12 - min(c1, c2)) / max(c1, c2)\n    return distance", "\n\ndef CLM(c1, c2, c12):\n    \"\"\"\n    Calculates Compression-based Length Measure (CLM).\n\n    Arguments:\n        c1: The compressed length of the first object.\n        c2: The compressed length of the second object.\n        c12: The compressed length of the concatenation of the first and second objects.\n\n    Returns:\n        float: The Compression-based Length Measure value between c1 and c2.\n\n    Formula:\n        CLM(c1, c2, c12) = 1 - (c1 + c2 - c12) / c12\n    \"\"\"\n    dis = 1 - (c1 + c2 - c12) / c12\n    return dis", "\n\ndef CDM(c1: float, c2: float, c12: float) -> float:\n    \"\"\"\n    Calculates Compound Dissimilarity Measure (CDM).\n\n    Arguments:\n        c1 (float): The compressed length of the first object.\n        c2 (float): The compressed length of the second object.\n        c12 (float): The compressed length of the concatenation of the first\n                     and second objects.\n\n    Returns:\n        float: The Compound Dissimilarity Measure value between c1 and c2.\n\n    Formula:\n        CDM(c1, c2, c12) = c12 / (c1 + c2)\n    \"\"\"\n    dis = c12 / (c1 + c2)\n    return dis", "\n\ndef MSE(v1: np.ndarray, v2: np.ndarray) -> float:\n    \"\"\"\n    Calculates Mean Squared Error (MSE).\n\n    Arguments:\n        v1 (np.ndarray): The first array.\n        v2 (np.ndarray): The second array.\n\n    Returns:\n        float: The Mean Squared Error value, representing the average squared\n               difference between v1 and v2.\n\n    Formula:\n        MSE(v1, v2) = \u03a3((v1 - v2) ** 2) / len(v1)\n    \"\"\"\n\n    return np.sum((v1 - v2) ** 2) / len(v1)", "\n\ndef agg_by_concat_space(t1: str, t2: str) -> str:\n    \"\"\"\n    Combines `t1` and `t2` with a space.\n\n    Arguments:\n        t1 (str): First item.\n        t2 (str): Second item.\n\n    Returns:\n        str: `{t1} {t2}`\n    \"\"\"\n\n    return t1 + \" \" + t2", "\n\ndef agg_by_jag_word(t1: str, t2: str) -> str:\n    \"\"\"\n    # TODO: Better description\n\n    Arguments:\n        t1 (str): First item.\n        t2 (str): Second item.\n\n    Returns:\n        str:\n    \"\"\"\n\n    t1_list = t1.split(\" \")\n    t2_list = t2.split(\" \")\n\n    combined = []\n    minimum_list_size = min([len(t1_list), len(t2_list)])\n    for i in range(0, minimum_list_size - 1, 2):\n        combined.append(t1_list[i])\n        combined.append(t2_list[i + 1])\n\n    if len(t1_list) > len(t2_list):\n        combined += t1_list[i:]\n    return \" \".join(combined)", "\n\ndef agg_by_jag_char(t1: str, t2: str):\n    \"\"\"\n    # TODO: Better description\n\n    Arguments:\n        t1 (str): First item.\n        t2 (str): Second item.\n\n    Returns:\n        str:\n    \"\"\"\n\n    t1_list = list(t1)\n    t2_list = list(t2)\n    combined = []\n    minimum_list_size = min([len(t1_list), len(t2_list)])\n    for i in range(0, minimum_list_size - 1, 2):\n        combined.append(t1_list[i])\n        combined.append(t2_list[i + 1])\n    if len(t1_list) > len(t2_list):\n        combined += t1_list[i:]\n\n    return \"\".join(combined)", "\n\ndef aggregate_strings(stringa: str, stringb: str, by_character: bool = False) -> str:\n    \"\"\"\n    Aggregates strings.\n\n    Arguments:\n        stringa (str): First item.\n        stringb (str): Second item.\n        by_character (bool): True if you want to join the combined string by character,\n                             Else combines by word\n\n    Returns:\n        str: combination of stringa and stringb\n    \"\"\"\n\n    lista = list(stringa)\n    listb = list(stringb)\n    combined = []\n    minimum_list_size = min([len(lista), len(listb)])\n    for i in range(0, minimum_list_size - 1, 2):\n        combined.append(lista[i])\n        combined.append(listb[i + 1])\n    if len(lista) > len(listb):\n        combined += lista[i:]\n\n    if by_character:\n        return \"\".join(combined)\n    return \" \".join(combined)", "\n\ndef agg_by_avg(i1: torch.Tensor, i2: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Calculates the average of i1 and i2, rounding to the shortest.\n\n    Arguments:\n        i1 (torch.Tensor): First series of numbers.\n        i2 (torch.Tensor): Second series of numbers.\n\n    Returns:\n        torch.Tensor: Average of the two series of numbers.\n    \"\"\"\n\n    return torch.div(i1 + i2, 2, rounding_mode=\"trunc\")", "\n\ndef agg_by_min_or_max(\n    i1: torch.Tensor, i2: torch.Tensor, aggregate_by_minimum: bool = False\n) -> torch.Tensor:\n    \"\"\"\n    Calculates the average of i1 and i2, rounding to the shortest.\n\n    Arguments:\n        i1 (torch.Tensor): First series of numbers.\n        i2 (torch.Tensor): Second series of numbers.\n        aggregate_by_minimum (bool): True to take the minimum of the two series.\n                                     False to take the maximum instead.\n\n    Returns:\n        torch.Tensor: Average of the two series.\n    \"\"\"\n\n    stacked = torch.stack([i1, i2], axis=0)\n    if aggregate_by_minimum:\n        return torch.min(stacked, axis=0)[0]\n\n    return torch.max(stacked, axis=0)[0]", "\n\ndef agg_by_stack(i1: torch.Tensor, i2: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Combines `i1` and `i2` via `torch.stack`.\n\n    Arguments:\n        i1 (torch.Tensor): First series of numbers.\n        i2 (torch.Tensor): Second series of numbers.\n\n    Returns:\n        torch.Tensor: Stack of the two series.\n    \"\"\"\n\n    return torch.stack([i1, i2])", "\n\ndef mean_confidence_interval(data: Sequence, confidence: float = 0.95) -> tuple:\n    \"\"\"\n    Computes the mean confidence interval of `data` with `confidence`\n\n    Arguments:\n        data (Sequence): Data to compute a confidence interval over.\n        confidence (float): Level to compute confidence.\n\n    Returns:\n        tuple: (Mean, quantile-error-size)\n    \"\"\"\n\n    if isinstance(data, np.ndarray):\n        array = data\n    else:\n        array = np.array(data, dtype=np.float32)\n\n    n = array.shape[0]\n\n    mean = np.mean(array)\n    standard_error = scipy.stats.sem(array)\n    quantile = scipy.stats.t.ppf((1 + confidence) / 2.0, n - 1)\n    return mean, standard_error * quantile", ""]}
{"filename": "original_codebase/data.py", "chunked_list": ["import csv\nimport os\nimport random\nfrom collections import defaultdict\nfrom collections.abc import Iterable\nfrom typing import Optional, Sequence, Union\n\nimport numpy as np\nimport unidecode\nfrom datasets import load_dataset", "import unidecode\nfrom datasets import load_dataset\nfrom sklearn.datasets import fetch_20newsgroups\n\n\ndef _load_csv_filepath(csv_filepath: str) -> list:\n    \"\"\"\n    Loads three elements from a csv file and appends them to a list.\n\n    Arguments:\n        csv_filepath (str): Filepath to .csv file.\n\n    Returns:\n        list: 2-dimensional list containing three elements.\n    \"\"\"\n\n    data = []\n    with open(csv_filepath, \"r\") as file:\n        reader = csv.reader(file, delimiter=\",\", quotechar='\"')\n        for row in reader:\n            data.append([row[0], row[1], row[2]])\n    return data", "\n\ndef read_fn_label(filename: str) -> dict:\n    \"\"\"\n    Reads a csv file and returns a dictionary containing\n    title+description: label pairs.\n\n    Arguments:\n        filename (str): Filepath to a csv file containing label, title, description.\n\n    Returns:\n        dict: {title. description: label} pairings.\n    \"\"\"\n\n    text2label = {}\n    data = _load_csv_filepath(filename)\n    for row in data:\n        label, title, desc = row[0], row[1], row[2]\n        text = \". \".join([title, desc])\n        text2label[text] = label\n\n    return text2label", "\n\ndef read_label(filename: str) -> list:\n    \"\"\"\n    Reads the first item from the `filename` csv filepath in each row.\n\n    Arguments:\n        filename (str): Filepath to a csv file containing label, title, description.\n\n    Returns:\n        list: Labels from the `fn` filepath.\n    \"\"\"\n\n    labels = [row[0] for row in _load_csv_filepath(filename)]\n    return labels", "\n\ndef read_fn_compress(filename: str) -> list:\n    \"\"\"\n    Opens a compressed file and returns the contents\n    and delimits the contents on new lines.\n\n    Arguments:\n        filename (str): Filepath to a compressed file.\n\n    Returns:\n        list: Compressed file contents line separated.\n    \"\"\"\n\n    text = unidecode.unidecode(open(filename).read())\n    text_list = text.strip().split(\"\\n\")\n    return text_list", "\n\ndef read_torch_text_labels(dataset: list, indices: Sequence[int]) -> tuple:\n    \"\"\"\n    Extracts the text and labels lists from a pytorch\n    `dataset` on `indices`.\n\n    Arguments:\n        dataset (list): List of lists containing text and labels.\n        indices (list): List of list indices to extract text and\n                         labels on from `dataset`.\n\n    Returns:\n        (list, list): Text and Label pairs from `dataset` on `indices`.\n\n    \"\"\"\n    text_list = []\n    label_list = []\n\n    for index in indices:\n        try:\n            row = dataset[index]\n        except IndexError:\n            row = None\n            pass\n\n        if row:\n            label_list.append(row[0])\n            text_list.append(row[1])\n\n    return text_list, label_list", "\n\ndef load_20news() -> tuple:\n    \"\"\"\n    Loads the 20NewsGroups dataset from `torchtext`.\n\n    Returns:\n        tuple: Tuple of Lists, with training data at index 0 and test at\n               index 1.\n\n    \"\"\"\n\n    def process(dataset):\n        pairs = []\n        for i in range(len(dataset.data)):\n            text = dataset.data[i]\n            label = dataset.target[i]\n            pairs.append((label, text))\n        return pairs\n\n    newsgroups_train = fetch_20newsgroups(subset=\"train\")\n    newsgroups_test = fetch_20newsgroups(subset=\"test\")\n    train_ds, test_ds = process(newsgroups_train), process(newsgroups_test)\n    return train_ds, test_ds", "\n\ndef load_ohsumed_single(local_directory: str) -> tuple:\n    \"\"\"\n    Loads the Ohsumed dataset from `local_directory`.\n    Assumes the existence of subdirectories `training` and `test`.\n\n    :ref: https://paperswithcode.com/dataset/ohsumed\n\n    Arguments:\n        local_directory (str): Local path to directory containing the Ohsumed\n                               `training` and `test` subdirectories.\n\n    Returns:\n        tuple: Pair of training and testing datasets.\n    \"\"\"\n\n    def process(data_directory: str) -> list:\n        dataset = []\n\n        # TODO: Replace with `glob` to crawl files into a list.\n        for directory_name in os.listdir(data_directory):\n            subdirectory_path = os.path.join(data_directory, directory_name)\n            if os.path.isdir(subdirectory_path):\n                label = directory_name\n                for filename in os.listdir(subdirectory_path):\n                    filepath = os.path.join(subdirectory_path, filename)\n\n                    if os.path.isfile(filepath):\n                        text = open(filepath).read().strip()\n                        dataset.append((label, text))\n        return dataset\n\n    train_dir = os.path.join(local_directory, \"training\")\n    test_dir = os.path.join(local_directory, \"test\")\n    train_ds, test_ds = process(train_dir), process(test_dir)\n    return train_ds, test_ds", "\n\ndef load_ohsumed(data_directory: str, split: float = 0.9) -> tuple:\n    \"\"\"\n    Loads the Ohsumed dataset and performs a train-test-split.\n\n    Arguments:\n        data_directory (str): Directory containing the ohsumed dataset.\n        split (float): % train size split.\n\n    Returns:\n        tuple: Tuple of lists containing the training and testing datasets respectively.\n\n    \"\"\"\n    train_ds = []\n    test_ds = []\n\n    for directory_name in os.listdir(data_directory):\n        if os.path.isdir(os.path.join(data_directory, directory_name)):\n            label = directory_name\n            subdirectory = os.path.join(data_directory, directory_name)\n            subdirectory_files = list(os.listdir(subdirectory))\n\n            for filename in subdirectory_files:\n                text = open(os.path.join(subdirectory, filename), \"r\").read().strip()\n                if random.random() <= split:\n                    train_ds.append((label, text))\n                else:\n                    test_ds.append((label, text))\n\n    return train_ds, test_ds", "\n\ndef load_r8(data_directory: str, delimiter: str = \"\\t\") -> tuple:\n    \"\"\"\n    Loads the R8 dataset.\n\n    Arguments:\n        data_directory (str): Directory containing the R8 dataset.\n        delimiter (str): File delimiter to parse on.\n\n    Returns:\n        tuple: Tuple of lists containing the training and testing datasets respectively.\n    \"\"\"\n\n    def process(filename: str) -> list:\n        processed_data = []\n        text_list = open(filename, \"r\").read().strip().split(\"\\n\")\n        for row in text_list:\n            label, text = row.split(delimiter)\n            processed_data.append((label, text))\n        return processed_data\n\n    test_fn = os.path.join(data_directory, \"test.txt\")\n    train_fn = os.path.join(data_directory, \"train.txt\")\n    train_ds, test_ds = process(train_fn), process(test_fn)\n    return train_ds, test_ds", "\n\ndef load_trec(data_directory: str) -> tuple:\n    \"\"\"\n    Loads the TREC dataset from a directory.\n\n    Arguments:\n        data_directory (str): Directory containing the TREC dataset.\n\n    Returns:\n        tuple: Tuple of lists containing the training and testing datasets respectively.\n    \"\"\"\n\n    def process(filename: str) -> list:\n        processed_data = []\n        with open(filename, encoding=\"ISO-8859-1\") as file:\n            reader = csv.reader(file, delimiter=\":\")\n            for row in reader:\n                label, text = row[0], row[1]\n                processed_data.append((label, text))\n        return processed_data\n\n    test_fn = os.path.join(data_directory, \"test.txt\")\n    train_fn = os.path.join(data_directory, \"train.txt\")\n    train_ds, test_ds = process(train_fn), process(test_fn)\n    return train_ds, test_ds", "\n\ndef load_kinnews_kirnews(\n    dataset_name: str = \"kinnews_kirnews\", data_split: str = \"kinnews_cleaned\"\n) -> tuple:\n    \"\"\"\n    Loads the KINNEWS and KIRNEWS datasets.\n\n    :ref: https://huggingface.co/datasets/kinnews_kirnews\n\n    Arguments:\n        dataset_name (str): Name of the dataset to be loaded.\n        data_split (str): The data split to be loaded.\n\n    Returns:\n        tuple: Tuple of lists containing the training and testing datasets respectively.\n    \"\"\"\n\n    def process(dataset: Iterable) -> list:\n        pairs = []\n        for pair in dataset:\n            label = pair[\"label\"]\n            title = pair[\"title\"]\n            content = pair[\"content\"]\n            pairs.append((label, title + \" \" + content))\n        return pairs\n\n    ds = load_dataset(dataset_name, data_split)\n    train_ds, test_ds = process(ds[\"train\"]), process(ds[\"test\"])\n    return train_ds, test_ds", "\n\ndef load_swahili() -> tuple:\n    \"\"\"\n    Loads the Swahili dataset\n\n    Returns:\n        tuple: Tuple of lists containing the training and testing datasets respectively.\n    \"\"\"\n\n    def process(dataset: Iterable) -> list:\n        pairs = []\n        for pair in dataset:\n            label = pair[\"label\"]\n            text = pair[\"text\"]\n            pairs.append((label, text))\n        return pairs\n\n    ds = load_dataset(\"swahili_news\")\n    train_ds, test_ds = process(ds[\"train\"]), process(ds[\"test\"])\n    return train_ds, test_ds", "\n\ndef load_filipino(data_directory) -> tuple:\n    \"\"\"\n    Loads the Dengue Filipino dataset from local directory\n\n    :ref: https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks#datasets\n\n    Arguments:\n        data_directory (str): Directory containing Dengue Filipino dataset\n    Returns:\n        tuple: Tuple of lists containing the training and testing datasets respectively.\n    \"\"\"\n\n    def process(fn):\n        pairs = []\n        with open(fn, \"r\") as f:\n            reader = csv.reader(f, delimiter=\",\", quotechar='\"')\n            for row in reader:\n                text = row[0]\n                for i in range(1, 6):\n                    if row[i] == \"1\":\n                        label = i - 1\n                        pairs.append((label, text))\n                        break\n        return pairs\n\n    train_ds, test_ds = process(os.path.join(data_directory, \"train.csv\")), process(\n        os.path.join(data_directory, \"test.csv\")\n    )\n    return train_ds, test_ds", "\n\ndef read_img_with_label(\n    dataset: list, indices: Sequence[int], flatten: bool = True\n) -> tuple:\n    \"\"\"\n    Loads items from `dataset` based on the indices listed in `indices`\n    and optionally flattens them.\n\n    Arguments:\n        dataset (list): List of images.\n        indices (list): indices of `dataset` to be returned.\n        flatten (bool): [Optional] Optionally flatten the image.\n\n    Returns:\n        tuple: (np.ndarray, np.ndarray) of images and labels respectively\n\n    \"\"\"\n    imgs = []\n    labels = []\n    for idx in indices:\n        img = np.array(dataset[idx][0])\n        label = dataset[idx][1]\n        if flatten:\n            img = img.flatten()\n        imgs.append(img)\n        labels.append(label)\n\n    return np.array(imgs), np.array(labels)", "\n\ndef read_img_label(dataset: list, indices: Sequence[int]) -> list:\n    \"\"\"\n    Given an image dataset and a list of indices,\n    this function returns the labels from the dataset.\n\n    Arguments:\n        dataset (list): List of images.\n        indices (list): indices of `dataset` to be returned.\n\n    Returns:\n        list: Image labels.\n    \"\"\"\n\n    labels = []\n    for idx in indices:\n        label = dataset[idx][1]\n        labels.append(label)\n    return labels", "\n\ndef pick_n_sample_from_each_class(\n    filename: str, n_samples: int, idx_only: bool = False\n) -> Union[list, tuple]:\n    \"\"\"\n    Grabs a random sample of size `n_samples` for each label from the csv file\n    at `filename`.\n\n    Arguments:\n        filename (str): Relative path to the file you want to load.\n        n_samples (int): Number of samples to load and return for each label.\n        idx_only (bool): True if you only want to return the indices of the rows\n                         to load.\n\n    Returns:\n        list | tuple: List if idx_only, else tuple of samples and labels.\n\n    \"\"\"\n\n    label2text = defaultdict(list)\n    label2idx = defaultdict(list)\n    class2count = {}\n    result = []\n    labels = []\n    recorded_idx = []\n\n    data = _load_csv_filepath(filename)\n    for i, (label, title, description) in enumerate(data):\n        text = \". \".join([title, description])\n        label2text[label].append(text)\n        label2idx[label].append(i)\n\n    for class_ in label2text:\n        class2count[class_] = len(label2text[class_])\n\n    for c in class2count:\n        select_idx = np.random.choice(class2count[c], size=n_samples, replace=False)\n        select_text = np.array(label2text[c])[select_idx]\n        select_text_idx = np.array(label2idx[c])[select_idx]\n        recorded_idx += list(select_text_idx)\n        result += list(select_text)\n        labels += [c] * n_samples\n\n    if idx_only:\n        return recorded_idx\n\n    return result, labels", "\n\ndef pick_n_sample_from_each_class_given_dataset(\n    dataset: Iterable,\n    n_samples: int,\n    output_filename: Optional[str] = None,\n    index_only: bool = False,\n) -> tuple:\n    \"\"\"\n    Grabs a random sample of size `n_samples` for each label from the dataset\n    `dataset`.\n\n    Arguments:\n        dataset (Iterable): Labeled data, in ``label, text`` pairs.\n        n_samples (int): Number of samples to load and return for each label.\n        output_filename (str): [Optional] Where to save the recorded indices.\n        index_only (bool): True if you only want to return the indices of the rows\n                           to load.\n\n    Returns:\n        list | tuple: List if idx_only, else tuple of samples and labels.\n    \"\"\"\n\n    label2text = defaultdict(list)\n    label2idx = defaultdict(list)\n    class2count = {}\n    result = []\n    labels = []\n    recorded_idx = []\n\n    for i, (label, text) in enumerate(dataset):\n        label2text[label].append(text)\n        label2idx[label].append(i)\n\n    for cl in label2text:\n        class2count[cl] = len(label2text[cl])\n\n    for c in class2count:\n        select_idx = np.random.choice(class2count[c], size=n_samples, replace=False)\n        select_text = np.array(label2text[c])[select_idx]\n        select_text_idx = np.array(label2idx[c])[select_idx]\n        recorded_idx += list(select_text_idx)\n        result += list(select_text)\n        labels += [c] * n_samples\n\n    if output_filename is not None:\n        np.save(output_filename, np.array(recorded_idx))\n\n    if index_only:\n        return np.array(recorded_idx), labels\n    return result, labels", "\n\ndef pick_n_sample_from_each_class_img(\n    dataset: list, n_samples: int, flatten: bool = False\n) -> tuple:\n    \"\"\"\n    Grabs a random sample of size `n_samples` for each label from the dataset\n    `dataset`.\n\n    Arguments:\n        dataset (list): Relative path to the file you want to load.\n        n_samples (int): Number of samples to load and return for each label.\n        flatten (bool): True if you want to flatten the images.\n\n    Returns:\n        tuple: Tuple of samples, labels, and the recorded indices.\n    \"\"\"\n\n    label2img = defaultdict(list)\n    label2idx = defaultdict(list)\n    class2count = {}\n    result = []\n    labels = []\n    recorded_idx = []  # for replication\n    for i, pair in enumerate(dataset):\n        img, label = pair\n        if flatten:\n            img = np.array(img).flatten()\n        label2img[label].append(img)\n        label2idx[label].append(i)\n\n    for cl in label2img:\n        class2count[cl] = len(label2img[cl])\n\n    for c in class2count:\n        select_idx = np.random.choice(class2count[c], size=n_samples, replace=False)\n        select_img = np.array(label2img[c])[select_idx]\n        select_img_idx = np.array(label2idx[c])[select_idx]\n        recorded_idx += list(select_img_idx)\n        result += list(select_img)\n        labels += [c] * n_samples\n    return result, labels, recorded_idx", "\n\ndef load_custom_dataset(directory: str, delimiter: str = \"\\t\") -> tuple:\n    def process(filename: str) -> list:\n        pairs = []\n        text_list = open(filename).read().strip().split(\"\\n\")\n        for t in text_list:\n            label, text = t.split(delimiter)\n            pairs.append((label, text))\n        return pairs\n\n    test_fn = os.path.join(directory, \"test.txt\")\n    train_fn = os.path.join(directory, \"train.txt\")\n    train_ds, test_ds = process(train_fn), process(test_fn)\n    return train_ds, test_ds", ""]}
{"filename": "original_codebase/experiments.py", "chunked_list": ["# Experiment framework\nimport operator\nimport random\nfrom collections import defaultdict\nfrom typing import Any, Callable, Optional\n\nimport numpy as np\nfrom compressors import DefaultCompressor\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\n\nclass KnnExpText:\n    def __init__(\n        self,\n        aggregation_function: Callable,\n        compressor: DefaultCompressor,\n        distance_function: Callable,\n    ) -> None:\n        self.aggregation_func = aggregation_function\n        self.compressor = compressor\n        self.distance_func = distance_function\n        self.distance_matrix: list = []\n\n    def calc_dis(\n        self, data: list, train_data: Optional[list] = None, fast: bool = False\n    ) -> None:\n        \"\"\"\n        Calculates the distance between either `data` and itself or `data` and\n        `train_data` and appends the distance to `self.distance_matrix`.\n\n        Arguments:\n            data (list): Data to compute distance between.\n            train_data (list): [Optional] Training data to compute distance from `data`.\n            fast (bool): [Optional] Uses the _fast compression length function\n                                    of `self.compressor`.\n\n        Returns:\n            None: None\n        \"\"\"\n\n        data_to_compare = data\n        if train_data is not None:\n            data_to_compare = train_data\n\n        for i, t1 in tqdm(enumerate(data)):\n            distance4i = []\n            if fast:\n                t1_compressed = self.compressor.get_compressed_len_fast(t1)\n            else:\n                t1_compressed = self.compressor.get_compressed_len(t1)\n            for j, t2 in enumerate(data_to_compare):\n                if fast:\n                    t2_compressed = self.compressor.get_compressed_len_fast(t2)\n                    t1t2_compressed = self.compressor.get_compressed_len_fast(\n                        self.aggregation_func(t1, t2)\n                    )\n                else:\n                    t2_compressed = self.compressor.get_compressed_len(t2)\n                    t1t2_compressed = self.compressor.get_compressed_len(\n                        self.aggregation_func(t1, t2)\n                    )\n                distance = self.distance_func(\n                    t1_compressed, t2_compressed, t1t2_compressed\n                )\n                distance4i.append(distance)\n            self.distance_matrix.append(distance4i)\n\n    def calc_dis_with_single_compressed_given(\n        self, data: list, data_len: list = None, train_data: Optional[list] = None\n    ) -> None:\n        \"\"\"\n        Calculates the distance between either `data`, `data_len`, or\n        `train_data` and appends the distance to `self.distance_matrix`.\n\n        Arguments:\n            data (list): Data to compute distance between.\n            train_data (list): [Optional] Training data to compute distance from `data`.\n            fast (bool): [Optional] Uses the _fast compression length function\n                                    of `self.compressor`.\n\n        Returns:\n            None: None\n        \"\"\"\n\n        data_to_compare = data\n        if train_data is not None:\n            data_to_compare = train_data\n\n        for i, t1 in tqdm(enumerate(data)):\n            distance4i = []\n            t1_compressed = self.compressor.get_compressed_len_given_prob(\n                t1, data_len[i]\n            )\n            for j, t2 in tqdm(enumerate(data_to_compare)):\n                t2_compressed = self.compressor.get_compressed_len_given_prob(\n                    t2, data_len[j]\n                )\n                t1t2_compressed = self.compressor.get_compressed_len(\n                    self.aggregation_func(t1, t2)\n                )\n                distance = self.distance_func(\n                    t1_compressed, t2_compressed, t1t2_compressed\n                )\n                distance4i.append(distance)\n            self.distance_matrix.append(distance4i)\n\n    def calc_dis_single(self, t1: str, t2: str) -> float:\n        \"\"\"\n        Calculates the distance between `t1` and `t2` and returns\n        that distance value as a float-like object.\n\n        Arguments:\n            t1 (str): Data 1.\n            t2 (str): Data 2.\n\n        Returns:\n            float-like: Distance between `t1` and `t2`.\n        \"\"\"\n\n        t1_compressed = self.compressor.get_compressed_len(t1)\n        t2_compressed = self.compressor.get_compressed_len(t2)\n        t1t2_compressed = self.compressor.get_compressed_len(\n            self.aggregation_func(t1, t2)\n        )\n        distance = self.distance_func(t1_compressed, t2_compressed, t1t2_compressed)\n        return distance\n\n    def calc_dis_single_multi(self, train_data: list, datum: str) -> list:\n        \"\"\"\n        Calculates the distance between `train_data` and `datum` and returns\n        that distance value as a float-like object.\n\n        Arguments:\n            train_data (list): Training data as a list-like object.\n            datum (str): Data to compare against `train_data`.\n\n        Returns:\n            list: Distance between `t1` and `t2`.\n        \"\"\"\n\n        distance4i = []\n        t1_compressed = self.compressor.get_compressed_len(datum)\n        for j, t2 in tqdm(enumerate(train_data)):\n            t2_compressed = self.compressor.get_compressed_len(t2)\n            t1t2_compressed = self.compressor.get_compressed_len(\n                self.aggregation_func(datum, t2)\n            )\n            distance = self.distance_func(t1_compressed, t2_compressed, t1t2_compressed)\n            distance4i.append(distance)\n        return distance4i\n\n    def calc_dis_with_vector(self, data: list, train_data: Optional[list] = None):\n        \"\"\"\n        Calculates the distance between `train_data` and `data` and returns\n        that distance value as a float-like object.\n\n        Arguments:\n            train_data (list): Training data as a list-like object.\n            datum (str): Data to compare against `train_data`.\n\n        Returns:\n            float-like: Distance between `t1` and `t2`.\n        \"\"\"\n\n        if train_data is not None:\n            data_to_compare = train_data\n        else:\n            data_to_compare = data\n        for i, t1 in tqdm(enumerate(data)):\n            distance4i = []\n            for j, t2 in enumerate(data_to_compare):\n                distance = self.distance_func(t1, t2)\n                distance4i.append(distance)\n            self.distance_matrix.append(distance4i)\n\n    def calc_acc(\n        self,\n        k: int,\n        label: list,\n        train_label: Optional[list] = None,\n        provided_distance_matrix: Optional[list] = None,\n        rand: bool = False,\n    ) -> tuple:\n        \"\"\"\n        Calculates the accuracy of the algorithm.\n\n        Arguments:\n            k (int?): TODO\n            label (list): Predicted Labels.\n            train_label (list): Correct Labels.\n            provided_distance_matrix (list): Calculated Distance Matrix to use\n                                             instead of `self.distance_matrix`.\n            rand (bool): TODO\n\n        Returns:\n            tuple: predictions, and list of bools indicating prediction correctness.\n\n        \"\"\"\n        if provided_distance_matrix is not None:\n            self.distance_matrix = provided_distance_matrix\n        correct = []\n        pred = []\n        if train_label is not None:\n            compare_label = train_label\n            start = 0\n            end = k\n        else:\n            compare_label = label\n            start = 1\n            end = k + 1\n\n        for i in range(len(self.distance_matrix)):\n            sorted_idx = np.argsort(np.array(self.distance_matrix[i]))\n            pred_labels = defaultdict(int)\n            for j in range(start, end):\n                pred_l = compare_label[sorted_idx[j]]\n                pred_labels[pred_l] += 1\n            sorted_pred_lab = sorted(\n                pred_labels.items(), key=operator.itemgetter(1), reverse=True\n            )\n            most_count = sorted_pred_lab[0][1]\n            if_right = 0\n            most_label = sorted_pred_lab[0][0]\n            most_voted_labels = []\n            for pair in sorted_pred_lab:\n                if pair[1] < most_count:\n                    break\n                if not rand:\n                    if pair[0] == label[i]:\n                        if_right = 1\n                        most_label = pair[0]\n                else:\n                    most_voted_labels.append(pair[0])\n            if rand:\n                most_label = random.choice(most_voted_labels)\n                if_right = 1 if most_label == label[i] else 0\n            pred.append(most_label)\n            correct.append(if_right)\n        print(\"Accuracy is {}\".format(sum(correct) / len(correct)))\n        return pred, correct\n\n    def combine_dis_acc(\n        self,\n        k: int,\n        data: list,\n        label: list,\n        train_data: Optional[list] = None,\n        train_label: Optional[list] = None,\n    ) -> tuple:\n        \"\"\"\n        Calculates the distance and the accuracy of the algorithm for data with\n        training.\n\n        Arguments:\n            k (int?): TODO\n            data (list): Data used for predictions.\n            label (list): Predicted Labels.\n            train_data (list): Training data to compare distances.\n            train_label (list): Correct Labels.\n\n        Returns:\n            tuple: predictions, and list of bools indicating prediction correctness.\n        \"\"\"\n        correct = []\n        pred = []\n        if train_label is not None:\n            compare_label = train_label\n            start = 0\n            end = k\n        else:\n            compare_label = label\n            start = 1\n            end = k + 1\n        if train_data is not None:\n            data_to_compare = train_data\n        else:\n            data_to_compare = data\n        for i, t1 in tqdm(enumerate(data)):\n            distance4i = self.calc_dis_single_multi(data_to_compare, t1)\n            sorted_idx = np.argsort(np.array(distance4i))\n            pred_labels = defaultdict(int)\n            for j in range(start, end):\n                pred_l = compare_label[sorted_idx[j]]\n                pred_labels[pred_l] += 1\n            sorted_pred_lab = sorted(\n                pred_labels.items(), key=operator.itemgetter(1), reverse=True\n            )\n            most_count = sorted_pred_lab[0][1]\n            if_right = 0\n            most_label = sorted_pred_lab[0][0]\n            for pair in sorted_pred_lab:\n                if pair[1] < most_count:\n                    break\n                if pair[0] == label[i]:\n                    if_right = 1\n                    most_label = pair[0]\n            pred.append(most_label)\n            correct.append(if_right)\n        print(\"Accuracy is {}\".format(sum(correct) / len(correct)))\n        return pred, correct\n\n    def combine_dis_acc_single(\n        self,\n        k: int,\n        train_data: list,\n        train_label: list,\n        datum: str,\n        label: Any,  # int, as used in this application\n    ) -> tuple:\n        \"\"\"\n        Calculates the distance and the accuracy of the algorithm for a single\n        datum with training.\n\n        Arguments:\n            k (int?): TODO\n            train_data (list): Training data to compare distances.\n            train_label (list): Correct Labels.\n            datum (str): Datum used for predictions.\n            label (Any): Correct label of datum.\n\n        Returns:\n            tuple: prediction, and a bool indicating prediction correctness.\n        \"\"\"\n        # Support multi processing - must provide train data and train label\n        distance4i = self.calc_dis_single_multi(train_data, datum)\n        sorted_idx = np.argpartition(np.array(distance4i), range(k))\n        pred_labels = defaultdict(int)\n        for j in range(k):\n            pred_l = train_label[sorted_idx[j]]\n            pred_labels[pred_l] += 1\n        sorted_pred_lab = sorted(\n            pred_labels.items(), key=operator.itemgetter(1), reverse=True\n        )\n        most_count = sorted_pred_lab[0][1]\n        if_right = 0\n        most_label = sorted_pred_lab[0][0]\n        for pair in sorted_pred_lab:\n            if pair[1] < most_count:\n                break\n            if pair[0] == label:\n                if_right = 1\n                most_label = pair[0]\n        pred = most_label\n        correct = if_right\n        return pred, correct", ""]}
{"filename": "npc_gzip/knn_classifier.py", "chunked_list": ["from typing import Optional, Sequence\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom npc_gzip.aggregations import concatenate_with_space\nfrom npc_gzip.compressors.base import BaseCompressor\nfrom npc_gzip.distance import Distance\nfrom npc_gzip.exceptions import (\n    InputLabelEqualLengthException,", "from npc_gzip.exceptions import (\n    InputLabelEqualLengthException,\n    InvalidObjectTypeException,\n    UnsupportedDistanceMetricException,\n)\n\n\nclass KnnClassifier:\n    \"\"\"\n    Given the training input and optional\n    training labels data, this class stores\n    the data in memory\n\n    >>> import random\n    >>> from npc_gzip.compressors.gzip_compressor import GZipCompressor\n\n    >>> training_data = [\"hey\", \"hi\", \"how are you?\", \"not too bad\"]\n\n    >>> training_labels = [random.randint(0, 1) for _ in range(len(training_data))]\n    >>> assert len(training_data) == len(training_labels)\n\n    >>> model = KnnClassifier(\n    ...     compressor=GZipCompressor(),\n    ...     training_inputs=training_data,\n    ...     training_labels=training_labels,\n    ...     distance_metric=\"ncd\",\n    ... )\n\n    >>> test = np.array([\"hey\", \"you are a real pain in my ass\", \"go away please\"])\n\n    >>> top_k = 1\n    >>> distances, labels, similar_samples = model.predict(test, top_k=top_k)\n    >>> assert distances.shape == (test.shape[0], len(training_data))\n    >>> assert labels.shape == (test.shape[0], )\n    >>> assert distances.shape[0] == labels.shape[0] == similar_samples.shape[0]\n    \"\"\"\n\n    def __init__(\n        self,\n        compressor: BaseCompressor,\n        training_inputs: Sequence,\n        training_labels: Optional[Sequence] = None,\n        distance_metric: str = \"ncd\",\n    ) -> None:\n        self.compressor = compressor\n\n        if isinstance(training_inputs, list) and isinstance(training_labels, list):\n            if training_labels is not None:\n                if len(training_inputs) != len(training_labels):\n                    raise InputLabelEqualLengthException(\n                        len(training_inputs),\n                        len(training_labels),\n                        function_name=\"KnnGzip.__init__\",\n                    )\n\n            self.training_inputs: np.ndarray = np.array(training_inputs).reshape(-1)\n            self.training_labels: np.ndarray = np.array(training_labels).reshape(-1)\n\n        elif isinstance(training_inputs, np.ndarray) or isinstance(\n            training_labels, np.ndarray\n        ):\n            self.training_inputs: np.ndarray = (\n                np.array(training_inputs)\n                if isinstance(training_inputs, list)\n                else training_inputs\n            )\n            self.training_labels: np.ndarray = (\n                np.array(training_labels)\n                if isinstance(training_labels, list)\n                else training_labels\n            )\n\n            self.training_inputs = self.training_inputs.reshape(-1)\n            self.training_labels = self.training_labels.reshape(-1)\n\n        else:\n            raise InvalidObjectTypeException(\n                type(training_inputs),\n                supported_types=[list, np.ndarray],\n                function_name=\"KnnGzip.__init__\",\n            )\n\n        assert (\n            self.training_inputs.shape == self.training_labels.shape\n        ), f\"\"\"\n        Training Inputs and Labels did not maintain their\n        shape during the conversion from lists to numpy arrays.\n        This is most likely a bug in the numpy package:\n\n        self.training_inputs.shape: {self.training_inputs.shape}\n        self.training_labels.shape: {self.training_labels.shape}\n        \"\"\"\n\n        self.supported_distance_metrics: list = [\"ncd\", \"clm\", \"cdm\", \"mse\"]\n\n        if distance_metric not in self.supported_distance_metrics:\n            raise UnsupportedDistanceMetricException(\n                distance_metric,\n                self.supported_distance_metrics,\n                function_name=\"KnnGzip.__init__\",\n            )\n\n        self.distance_metric = distance_metric\n\n        self.compressed_training_inputs: list = [\n            self.compressor.get_compressed_length(data) for data in self.training_inputs\n        ]\n        self.compressed_training_inputs: np.ndarray = np.array(\n            self.compressed_training_inputs\n        ).reshape(-1)\n\n    def _calculate_distance(\n        self,\n        compressed_input: np.ndarray,\n        compressed_combined: np.ndarray,\n        compressed_training: Optional[np.ndarray] = None,\n    ) -> np.ndarray:\n        \"\"\"\n        Helper function that converts the string representation\n        of `self.distance_metric` to the actual\n        `npc_gzip.distance.Distance.[distance_metric]`. Then\n        that distance metric is calculated using\n        `self.compressed_training_inputs`, `compressed_input` and\n        `compressed_combined`.\n\n        Arguments:\n            compressed_input (np.ndarray): Numpy array representing the\n                                       compressed lengths of the input\n                                       data to be scored.\n            compressed_combined (np.ndarray): Numpy array representing the\n                                              compressed lengths of the input\n                                              data combined with\n                                              each training sample to be scored.\n        Returns:\n            np.ndarray: Numpy array containing the distance metric.\n        \"\"\"\n\n        if compressed_training is None:\n            distance = Distance(\n                np.resize(self.compressed_training_inputs, compressed_input.shape),\n                compressed_input,\n                compressed_combined,\n            )\n        else:\n            distance = Distance(\n                np.resize(compressed_training, compressed_input.shape),\n                compressed_input,\n                compressed_combined,\n            )\n\n        if self.distance_metric == \"ncd\":\n            return distance.ncd\n        elif self.distance_metric == \"clm\":\n            return distance.clm\n        elif self.distance_metric == \"cdm\":\n            return distance.cdm\n        elif self.distance_metric == \"mse\":\n            return distance.mse\n        else:\n            raise UnsupportedDistanceMetricException(\n                self.distance_metric,\n                supported_distance_metrics=self.supported_distance_metrics,\n                function_name=\"_calculate_distance\",\n            )\n\n    def _compress_sample(\n        self,\n        x: str,\n        training_inputs: Optional[np.ndarray] = None,\n    ) -> tuple:\n        \"\"\"\n        Helper method that compresses `x` against each\n        item in `self.training_inputs` and returns the\n        distance between each sample using the\n        self.distance_metric from the\n        `npc_gzip.distance.Distance` object.\n\n        Arguments:\n            x (np.ndarray): The sample data to compare against\n                            the `training_inputs`.\n            training_inputs (np.ndarray): [Optional] If provided, this\n                                          method will use `training_inputs`\n                                          when calculating the distance matrix\n                                          rather than `self.training_inputs`.\n\n        Returns:\n            np.ndarray: Compressed length of `x` as an array of shape\n                        [self.compressed_training_inputs.shape[0]].\n            np.ndarray: Compressed length of the combination of `x` and\n                        each training sample as an array of shape\n                        [self.compressed_training_inputs.shape[0]].\n        \"\"\"\n\n        assert isinstance(\n            x, str\n        ), f\"Non-string was passed to self._compress_sample: {x}\"\n\n        if training_inputs is None:\n            training_inputs = self.training_inputs\n\n        compressed_input_length: int = self.compressor.get_compressed_length(x)\n        compressed_input: list = [\n            compressed_input_length for _ in range(training_inputs.shape[0])\n        ]\n        compressed_input: np.ndarray = np.array(compressed_input).reshape(-1)\n        assert compressed_input.shape == training_inputs.shape\n\n        combined: list = []\n        for training_sample in training_inputs:\n            train_and_x: str = concatenate_with_space(training_sample, x)\n            combined_compressed: int = self.compressor.get_compressed_length(\n                train_and_x\n            )\n            combined.append(combined_compressed)\n\n        combined: np.ndarray = np.array(combined).reshape(-1)\n        assert training_inputs.shape == compressed_input.shape == combined.shape\n\n        return (compressed_input, combined)\n\n    def sample_data(\n        self, sampling_percentage: float = 1.0\n    ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Given a `sampling_percentage`, this method randomly\n        samples data from `self.training_inputs` &\n        `self.training_labels` (if exists) without replacement\n        and returns two numpy arrays containing the randomly\n        sampled data.\n\n        Arguments:\n            sampling_percentage (float): (0, 1.0] % of data to\n                                         randomly sample from the\n                                         training inputs and labels.\n\n        Returns:\n            np.ndarray: Randomly sampled training inputs.\n            np.ndarray: Randomly sampled training labels.\n            np.ndarray: Indices that the training inputs &\n                        labels were sampled.\n        \"\"\"\n\n        total_inputs: int = self.training_inputs.shape[0]\n        sample_size: int = int(sampling_percentage * total_inputs)\n        randomly_sampled_indices: np.ndarray = np.random.choice(\n            total_inputs, sample_size, replace=False\n        )\n\n        randomly_sampled_inputs: np.ndarray = self.training_inputs[\n            randomly_sampled_indices\n        ]\n        randomly_sampled_labels: np.ndarray = np.array([])\n        if self.training_labels is not None:\n            randomly_sampled_labels = self.training_labels[randomly_sampled_indices]\n\n        return (\n            randomly_sampled_inputs,\n            randomly_sampled_labels,\n            randomly_sampled_indices,\n        )\n\n    def predict(\n        self,\n        x: Sequence,\n        top_k: int = 1,\n        sampling_percentage: float = 1.0,\n    ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Faster version of `predict`. This method\n        will compare `x` against a sample of the\n        training data provided and will return the\n        best matching label if `training_labels` was\n        passed during instantiation.\n\n        If `training_labels` was not passed during\n        instantiation, or if `top_k` is not None, the most\n        similar samples from `training_inputs` will be\n        returned.\n\n        resulting array is [batch_size, self.training_inputs.shape[0] ]\n\n        Arguments:\n            x (Sequence): The sample data to compare against\n                                                  the `training_inputs`.\n            top_k (int): [Optional] If not None, the most similar\n                         `k` number of samples will be returned.\n                         `top_k` must be greater than zero.\n                         [default: top_k=1]\n            sampling_percentage (float): (0.0, 1.0] % of `self.training_inputs`\n                                         to sample predictions against.\n\n        Returns:\n            np.ndarray: The distance-metrics matrix computed on the test\n                        set.\n            np.ndarray: The `top_k` most similar `self.training_labels`.\n            np.ndarray: The `top_k` best matching samples\n                           from `self.training_inputs`.\n        \"\"\"\n\n        if not isinstance(x, np.ndarray):\n            x: np.ndarray = np.array(x)\n\n        assert top_k > 0, f\"top_k ({top_k}) must be greater than zero.\"\n\n        x: np.ndarray = x.reshape(-1)\n        assert (\n            top_k <= x.shape[0]\n        ), f\"\"\"\n        top_k ({top_k}) must be less or equal to than the number of\n        samples provided to be predicted on ({x.shape[0]})\n\n        \"\"\"\n\n        # sample training inputs and labels\n        training_inputs, training_labels, randomly_sampled_indices = self.sample_data(\n            sampling_percentage\n        )\n\n        samples: list = []\n        combined: list = []\n        for sample in tqdm(x, desc=\"Compressing input...\"):\n            compressed_sample, combined_length = self._compress_sample(\n                sample, training_inputs=training_inputs\n            )\n            samples.append(compressed_sample)\n            combined.append(combined_length)\n\n        compressed_samples: np.ndarray = np.array(samples)\n        compressed_combined: np.ndarray = np.array(combined)\n\n        assert isinstance(training_inputs, np.ndarray)\n        assert isinstance(compressed_samples, np.ndarray)\n        assert isinstance(compressed_combined, np.ndarray)\n        assert isinstance(self.compressed_training_inputs, np.ndarray)\n\n        compressed_training: np.ndarray = self.compressed_training_inputs[\n            randomly_sampled_indices\n        ]\n\n        distances: np.ndarray = self._calculate_distance(\n            compressed_samples, compressed_combined, compressed_training\n        )\n\n        # top matching training samples and labels by\n        # minimum distance.\n\n        # get indicies of minimum top_k distances.\n        minimum_distance_indices = np.argpartition(distances, top_k)[:, :top_k]\n\n        similar_samples: np.ndarray = training_inputs[minimum_distance_indices]\n        labels: np.ndarray = training_labels[minimum_distance_indices]\n        predicted_labels = np.apply_along_axis(\n            lambda x: np.bincount(x).argmax(), axis=1, arr=labels\n        )\n\n        return distances, predicted_labels, similar_samples", ""]}
{"filename": "npc_gzip/aggregations.py", "chunked_list": ["import itertools\n\n\ndef concatenate_with_space(stringa: str, stringb: str) -> str:\n    \"\"\"\n    Combines `stringa` and `stringb` with a space.\n\n    Arguments:\n        stringa (str): First item.\n        stringb (str): Second item.\n\n    Returns:\n        str: `{stringa} {stringb}`\n    \"\"\"\n    return stringa + \" \" + stringb", "\n\ndef aggregate_strings(stringa: str, stringb: str, by_character: bool = False) -> str:\n    \"\"\"\n    Aggregates strings.\n\n    (replaces agg_by_jag_char, agg_by_jag_word)\n\n    Arguments:\n        stringa (str): First item.\n        stringb (str): Second item.\n        by_character (bool): True if you want to join the combined string by character,\n                             Else combines by word\n\n    Returns:\n        str: combination of stringa and stringb\n    \"\"\"\n\n    stringa_list: list = stringa.split()\n    stringb_list: list = stringb.split()\n\n    zipped_lists: list = list(zip(stringa_list, stringb_list))\n    out: list = list(itertools.chain(*zipped_lists))\n\n    aggregated: str = (\"\" if by_character else \" \").join(out)\n    return aggregated", ""]}
{"filename": "npc_gzip/__init__.py", "chunked_list": ["from . import compressors\nfrom .distance import *\nfrom .exceptions import *\nfrom .utils import *\n"]}
{"filename": "npc_gzip/utils.py", "chunked_list": ["\"\"\"\nHelper functions used primarily in testing the\nrest of the codebase.\n\n>>> number_of_sentences = random.randint(1, 100)\n>>> dataset = generate_dataset(number_of_sentences)\n>>> assert len(dataset) == number_of_sentences\n\"\"\"\n\nimport random", "\nimport random\nimport string\n\n\ndef generate_sentence(number_of_words: int = 10) -> str:\n    \"\"\"\n    Generates a sentence of random\n    numbers and letters, with\n    `number_of_words` words in the\n    sentence such that len(out.split()) \\\n    == `number_of_words`.\n\n    Arguments:\n        number_of_words (int): The number of words you\n                               want in the sentence.\n\n    Returns:\n        str: Sentence of random numbers and letters.\n    \"\"\"\n\n    assert number_of_words > 0, \"`number_of_words` must be greater than zero.\"\n\n    words = []\n    for word in range(number_of_words):\n        # initializing size of string\n        N = random.randint(1, 50)\n\n        words.append(\n            \"\".join(random.choices(string.ascii_uppercase + string.digits, k=N))\n        )\n\n    out = \" \".join(words)\n    return out", "\n\ndef generate_dataset(number_of_sentences: int) -> list:\n    \"\"\"\n    Loops over `range(number_of_sentences)` that\n    utilizes `generate_sentence()` to generate\n    a dataset of randomly sized sentences.\n\n    Arguments:\n        number_of_sentences (int): The number of\n                                   sentences you\n                                   want in your\n                                   dataset.\n\n    Returns:\n        list: List of sentences (str).\n    \"\"\"\n\n    assert number_of_sentences > 0, \"`number_of_sentences` must be greater than zero.\"\n\n    dataset = []\n    for sentence in range(number_of_sentences):\n        number_of_words = random.randint(1, 100)\n        dataset.append(generate_sentence(number_of_words))\n\n    return dataset", ""]}
{"filename": "npc_gzip/distance.py", "chunked_list": ["from typing import Sequence\n\nimport numpy as np\n\nfrom npc_gzip.exceptions import CompressedValuesEqualZero, InvalidShapeException\n\n\nclass Distance:\n    \"\"\"\n    Used to calculate the distance between compressed\n    objects. Typical usage is your training data as\n    `compressed_values_a`, the data you want to predict\n    on as `compressed_values_b` and the two values\n    concatenated then compressed as `compressed_values_ab`.\n\n    >>> import random\n\n    >>> a = random.random()\n    >>> b = random.random()\n    >>> ab = random.random()\n\n    >>> distance = Distance(a, b, ab)\n    >>> ncd: float = distance._ncd(a, b, ab)\n    >>> cdm: float = distance._cdm(a, b, ab)\n    >>> clm: float = distance._clm(a, b, ab)\n    >>> mse: float = distance._mse(a, b)\n\n    >>> assert isinstance(ncd, float)\n    >>> assert isinstance(cdm, float)\n    >>> assert isinstance(clm, float)\n    >>> assert isinstance(mse, float)\n\n    >>> a = np.random.rand(3, 10)\n    >>> b = np.random.rand(3, 10)\n    >>> ab = np.random.rand(3, 10)\n\n    >>> distance = Distance(a, b, ab)\n\n    >>> ncd: np.ndarray = distance.ncd\n    >>> cdm: np.ndarray = distance.cdm\n    >>> clm: np.ndarray = distance.clm\n    >>> mse: np.ndarray = distance.mse\n\n    >>> assert isinstance(ncd, np.ndarray)\n    >>> assert isinstance(cdm, np.ndarray)\n    >>> assert isinstance(clm, np.ndarray)\n    >>> assert isinstance(mse, np.ndarray)\n    \"\"\"\n\n    def __init__(\n        self,\n        compressed_values_a: Sequence,\n        compressed_values_b: Sequence,\n        compressed_values_ab: Sequence,\n    ) -> None:\n        if not isinstance(compressed_values_a, np.ndarray):\n            compressed_values_a = np.array(compressed_values_a)\n\n        if not isinstance(compressed_values_b, np.ndarray):\n            compressed_values_b = np.array(compressed_values_b)\n\n        if not isinstance(compressed_values_ab, np.ndarray):\n            compressed_values_ab = np.array(compressed_values_ab)\n\n        if (\n            compressed_values_a.shape\n            == compressed_values_b.shape\n            == compressed_values_ab.shape\n        ):\n            self.compressed_values_a = compressed_values_a\n            self.compressed_values_b = compressed_values_b\n            self.compressed_values_ab = compressed_values_ab\n        else:\n            raise InvalidShapeException(\n                compressed_values_a,\n                compressed_values_b,\n                compressed_values_ab,\n                function_name=\"Distance.__init__\",\n            )\n\n    def _ncd(\n        self,\n        compressed_value_a: float,\n        compressed_value_b: float,\n        compressed_value_ab: float,\n    ) -> float:\n        denominator = max(compressed_value_a, compressed_value_b)\n        if denominator == 0:\n            raise CompressedValuesEqualZero(\n                compressed_value_a, compressed_value_b, function_name=\"Distance._ncd\"\n            )\n\n        numerator = compressed_value_ab - min(compressed_value_a, compressed_value_b)\n        distance = numerator / denominator\n        return distance\n\n    def _cdm(\n        self,\n        compressed_value_a: float,\n        compressed_value_b: float,\n        compressed_value_ab: float,\n    ) -> float:\n        denominator = compressed_value_a + compressed_value_b\n        if denominator == 0:\n            raise CompressedValuesEqualZero(\n                compressed_value_a, compressed_value_b, function_name=\"Distance._cdm\"\n            )\n\n        numerator = compressed_value_ab\n\n        distance = numerator / denominator\n        return distance\n\n    def _clm(\n        self,\n        compressed_value_a: float,\n        compressed_value_b: float,\n        compressed_value_ab: float,\n    ) -> float:\n        denominator = compressed_value_ab\n        if denominator == 0:\n            raise CompressedValuesEqualZero(\n                compressed_value_ab, function_name=\"Distance._clm\"\n            )\n\n        numerator = 1 - (compressed_value_a + compressed_value_b - compressed_value_ab)\n\n        distance = numerator / denominator\n        return distance\n\n    def _mse(\n        self,\n        compressed_value_a: Sequence,\n        compressed_value_b: Sequence,\n    ) -> np.ndarray:\n        \"\"\"\n        Computes the mean squared error between two\n        values.\n\n        \"\"\"\n\n        if not isinstance(compressed_value_a, np.ndarray):\n            compressed_value_a = np.array(compressed_value_a)\n\n        if not isinstance(compressed_value_b, np.ndarray):\n            compressed_value_b = np.array(compressed_value_b)\n\n        compressed_value_a = compressed_value_a.reshape(-1)\n        compressed_value_b = compressed_value_b.reshape(-1)\n\n        numerator = np.sum((compressed_value_a - compressed_value_b) ** 2)\n        denominator = compressed_value_a.shape[0]\n\n        if denominator == 0:\n            raise CompressedValuesEqualZero(\n                compressed_value_a, compressed_value_b, function_name=\"Distance._mse\"\n            )\n\n        mse = numerator / denominator\n        return mse\n\n    @property\n    def ncd(self) -> np.ndarray:\n        \"\"\"\n        A numpy vectorized form of self._ncd.\n\n        \"\"\"\n\n        out = np.vectorize(self._ncd)(\n            self.compressed_values_a,\n            self.compressed_values_b,\n            self.compressed_values_ab,\n        )\n        out = out.reshape(self.compressed_values_a.shape)\n\n        return out\n\n    @property\n    def cdm(self) -> np.ndarray:\n        \"\"\"\n        A numpy vectorized form of self._cdm.\n\n        \"\"\"\n\n        out = np.vectorize(self._cdm)(\n            self.compressed_values_a,\n            self.compressed_values_b,\n            self.compressed_values_ab,\n        )\n        out = out.reshape(self.compressed_values_a.shape)\n\n        return out\n\n    @property\n    def clm(self) -> np.ndarray:\n        \"\"\"\n        A numpy vectorized form of self._clm.\n\n        \"\"\"\n\n        out = np.vectorize(self._clm)(\n            self.compressed_values_a,\n            self.compressed_values_b,\n            self.compressed_values_ab,\n        )\n        out = out.reshape(self.compressed_values_a.shape)\n\n        return out\n\n    @property\n    def mse(self) -> np.ndarray:\n        \"\"\"\n        A numpy vectorized form of self._mse.\n\n        \"\"\"\n\n        out = np.vectorize(self._mse)(\n            self.compressed_values_a,\n            self.compressed_values_b,\n        )\n        out = out.reshape(self.compressed_values_a.shape)\n\n        return out", ""]}
{"filename": "npc_gzip/exceptions.py", "chunked_list": ["from typing import Any, Optional\n\nimport numpy as np\n\n\nclass InvalidCompressorException(Exception):\n    \"\"\"\n    Is raised when a user is trying to use a compression\n    library that is not supported.\n    \"\"\"\n\n    def __init__(self, compression_library: str) -> None:\n        self.message = f\"\"\"\n        Compression Library ({compression_library})\n        is not currently supported.\n        \"\"\"\n        super().__init__(self.message)", "\n\nclass MissingDependencyException(Exception):\n    \"\"\"\n    Is raised when an underlying dependency is not\n    found when loading a library.\n    \"\"\"\n\n    def __init__(self, compression_library: str) -> None:\n        self.message = f\"\"\"\n        Compression Library ({compression_library})\n        is missing an underlying dependency. Try\n        installing those missing dependencies and\n        load this again.\n\n        Common missing dependencies for:\n\n        * lzma:\n            * brew install xz\n            * sudo apt-get install lzma liblzma-dev libbz2-dev\n\n        * bz2:\n            * sudo apt-get install lzma liblzma-dev libbz2-dev\n\n        \"\"\"\n        super().__init__(self.message)", "\n\nclass StringTooShortException(Exception):\n    def __init__(\n        self, stringa: str, stringb: str, function_name: Optional[str] = None\n    ) -> None:\n        self.message = f\"\"\"\n        Unable to aggregate ({stringa}) and ({stringb}).\n        One or both of the two strings are too short to concatenate.\n\n        \"\"\"\n\n        if function_name is not None:\n            self.message += f\"function_name: {function_name}\"\n\n        super().__init__(self.message)", "\n\nclass CompressedValuesEqualZero(Exception):\n    def __init__(\n        self,\n        compressed_value_a: float,\n        compressed_value_b: Optional[float] = None,\n        function_name: Optional[str] = None,\n    ) -> None:\n        self.message = \"\"\"\n        The combination of compressed values passed equal zero.\n        This will result in a divide by zero error.\n\n\n        \"\"\"\n\n        if function_name is not None:\n            self.message += f\"function_name: {function_name}\"\n        super().__init__(self.message)", "\n\nclass AllOrNoneException(Exception):\n    def __init__(\n        self,\n        a: Any,\n        b: Any,\n        c: Any,\n        function_name: Optional[str] = None,\n    ) -> None:\n        self.message = f\"\"\"\n        The passed values must either all be None or not None.\n            arg1: {type(a)}\n            arg2: {type(b)}\n            arg3: {type(c)}\n\n        \"\"\"\n\n        if function_name is not None:\n            self.message += f\"function_name: {function_name}\"\n        super().__init__(self.message)", "\n\nclass InvalidShapeException(Exception):\n    def __init__(\n        self,\n        array_a: np.ndarray,\n        array_b: np.ndarray,\n        array_c: np.ndarray,\n        function_name: Optional[str] = None,\n    ) -> None:\n        self.message = f\"\"\"\n        The passed values must either all of the same shape.\n            arg1: {array_a.shape}\n            arg2: {array_b.shape}\n            arg3: {array_c.shape}\n\n        \"\"\"\n\n        if function_name is not None:\n            self.message += f\"function_name: {function_name}\"\n        super().__init__(self.message)", "\n\nclass UnsupportedDistanceMetricException(Exception):\n    def __init__(\n        self,\n        distance_metric: str,\n        supported_distance_metrics: Optional[list] = None,\n        function_name: Optional[str] = None,\n    ) -> None:\n        self.message = f\"\"\"\n        The `distance_metric` ({distance_metric}) provided is not\n        currently supported. Please submit an Issue and/or\n        Pull Request here to add support:\n        https://github.com/bazingagin/npc_gzip\n\n        \"\"\"\n\n        if supported_distance_metrics is not None:\n            self.message += (\n                f\"supported_distance_metrics: {supported_distance_metrics}\\n\"\n            )\n\n        if function_name is not None:\n            self.message += f\"function_name: {function_name}\"\n        super().__init__(self.message)", "\n\nclass InvalidObjectTypeException(TypeError):\n    def __init__(\n        self,\n        passed_type: str,\n        supported_types: Optional[list] = None,\n        function_name: Optional[str] = None,\n    ) -> None:\n        self.message = f\"\"\"\n        The type passed ({passed_type}) provided is not\n        currently supported.\n\n        \"\"\"\n\n        if supported_types is not None:\n            self.message += f\"supported types: {supported_types}\\n\"\n\n        if function_name is not None:\n            self.message += f\"function_name: {function_name}\"\n        super().__init__(self.message)", "\n\nclass InputLabelEqualLengthException(Exception):\n    def __init__(\n        self,\n        training_samples: int,\n        label_samples: int,\n        function_name: Optional[str] = None,\n    ) -> None:\n        self.message = f\"\"\"\n        If training labels are passed, the number\n        of training data samples must equal the\n        number of training label samples\n\n        training_samples: {training_samples}\n        label_samples: {label_samples}\n\n        \"\"\"\n\n        if function_name is not None:\n            self.message += f\"function_name: {function_name}\"\n        super().__init__(self.message)", ""]}
{"filename": "npc_gzip/compressors/base.py", "chunked_list": ["import os\nfrom types import ModuleType\n\nfrom npc_gzip.exceptions import InvalidCompressorException\n\n\nclass BaseCompressor:\n    \"\"\"\n    Default compressor class that other compressors inherit\n    from.\n\n    >>> import gzip\n\n    >>> compressor = BaseCompressor(compressor=gzip)\n\n    >>> example = \"Hello there!\"\n    >>> compressed_length: int = compressor.get_compressed_length(example)\n    >>> bits_per_character: float = compressor.get_bits_per_character(example)\n    >>> assert isinstance(compressed_length, int)\n    >>> assert isinstance(bits_per_character, float)\n    \"\"\"\n\n    def __init__(self, compressor: ModuleType) -> None:\n        if not isinstance(compressor, (ModuleType, BaseCompressor)):\n            raise InvalidCompressorException(\n                f\"Not a function passed: {type(compressor)}\"\n            )\n        self.compressor = compressor\n\n    def _open_file(self, filepath: str, as_bytes: bool = False) -> str:\n        \"\"\"\n        Helper function that loads and returns the contents\n        of a file at `filepath`. Optional `as_bytes` parameter\n        will load the file contents with `open(filepath, 'rb')`\n        if True.\n\n        Arguments:\n            filepath (str): Path to the file to read contents from.\n            as_bytes (bool): [Optional] If true, opens the file as bytes.\n\n        Returns:\n            str: File contents as a string.\n        \"\"\"\n\n        assert os.path.isfile(filepath), f\"Filepath ({filepath}) does not exist.\"\n        file_contents = None\n        open_mode = \"r\"\n        if as_bytes:\n            open_mode = \"rb\"\n\n        with open(filepath, open_mode) as f:\n            file_contents = f.read()\n\n        if as_bytes:\n            file_contents = file_contents.decode(\"utf-8\")\n\n        return file_contents\n\n    def _compress(self, x: str) -> bytes:\n        \"\"\"\n        Applies the compression algorithm to `x` and\n        returns the results as bytes.\n\n        Arguments:\n            x (str): The string you want to compress.\n\n        Returns:\n            bytes: The compressed bytes representation of `x`.\n        \"\"\"\n\n        x: bytes = x.encode(\"utf-8\")\n        compressed: bytes = self.compressor.compress(x)\n        return compressed\n\n    def get_compressed_length(self, x: str) -> int:\n        \"\"\"\n        Calculates the size of `x` once compressed.\n\n        Arguments:\n            x (str): String you want to compute the compressed length of.\n\n        Returns:\n            int: Length of `x` once compressed.\n        \"\"\"\n\n        compressed_value: bytes = self._compress(x)\n        compressed_length: int = len(compressed_value)\n        return compressed_length\n\n    def get_bits_per_character(self, x: str) -> float:\n        \"\"\"\n        Returns the compressed size of `x` relative to\n        the number of characters in string `x`.\n\n        Arguments:\n            x (str): String you want to compute the compressed length per\n                     character in.\n\n        Returns:\n            float: Length of `x` once compressed divided by the number of\n                 characters in `x`.\n        \"\"\"\n\n        compressed_value: bytes = self._compress(x)\n        compressed_length: int = len(compressed_value)\n        compressed_length_in_bits: int = compressed_length * 8\n        number_of_characters: int = len(x)\n\n        compressed_length_per_number_of_characters: float = (\n            compressed_length_in_bits / number_of_characters\n        )\n        return compressed_length_per_number_of_characters", ""]}
{"filename": "npc_gzip/compressors/lzma_compressor.py", "chunked_list": ["from npc_gzip.compressors.base import BaseCompressor\nfrom npc_gzip.exceptions import MissingDependencyException\n\n\nclass LzmaCompressor(BaseCompressor):\n    \"\"\"\n    lzma compressor that inherits from\n    `npc_gzip.compressors.base.BaseCompressor`\n\n    >>> compressor: BaseCompressor = LzmaCompressor()\n    >>> example: str = \"Hello there!\"\n    >>> compressed_length: int = compressor.get_compressed_length(example)\n    >>> bits_per_character: float = compressor.get_bits_per_character(example)\n    >>> assert isinstance(compressed_length, int)\n    >>> assert isinstance(bits_per_character, float)\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__(self)\n\n        try:\n            import lzma\n        except ModuleNotFoundError as e:\n            raise MissingDependencyException(\"lzma\") from e\n\n        self.compressor = lzma", ""]}
{"filename": "npc_gzip/compressors/bz2_compressor.py", "chunked_list": ["from npc_gzip.compressors.base import BaseCompressor\nfrom npc_gzip.exceptions import MissingDependencyException\n\n\nclass Bz2Compressor(BaseCompressor):\n    \"\"\"\n    bz2 compressor that inherits from\n    `npc_gzip.compressors.base.BaseCompressor`\n\n    >>> compressor: BaseCompressor = Bz2Compressor()\n    >>> example: str = \"Hello there!\"\n    >>> compressed_length: int = compressor.get_compressed_length(example)\n    >>> bits_per_character: float = compressor.get_bits_per_character(example)\n    >>> assert isinstance(compressed_length, int)\n    >>> assert isinstance(bits_per_character, float)\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__(self)\n\n        try:\n            import bz2\n        except ModuleNotFoundError as e:\n            raise MissingDependencyException(\"bz2\") from e\n\n        self.compressor = bz2", ""]}
{"filename": "npc_gzip/compressors/__init__.py", "chunked_list": [""]}
{"filename": "npc_gzip/compressors/gzip_compressor.py", "chunked_list": ["from npc_gzip.compressors.base import BaseCompressor\nfrom npc_gzip.exceptions import MissingDependencyException\n\n\nclass GZipCompressor(BaseCompressor):\n    \"\"\"\n    gzip compressor that inherits from\n    `npc_gzip.compressors.base.BaseCompressor`\n\n    >>> compressor: BaseCompressor = GZipCompressor()\n    >>> example: str = \"Hello there!\"\n    >>> compressed_length: int = compressor.get_compressed_length(example)\n    >>> bits_per_character: float = compressor.get_bits_per_character(example)\n    >>> assert isinstance(compressed_length, int)\n    >>> assert isinstance(bits_per_character, float)\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__(self)\n\n        try:\n            import gzip\n        except ModuleNotFoundError as e:\n            raise MissingDependencyException(\"gzip\") from e\n\n        self.compressor = gzip", ""]}
{"filename": "tests/test_distance.py", "chunked_list": ["import random\n\nimport numpy as np\nimport pytest\n\nfrom npc_gzip.distance import Distance\nfrom npc_gzip.exceptions import CompressedValuesEqualZero, InvalidShapeException\n\n\nclass TestDistance:\n    array_a = np.random.rand(3, 10)\n    array_b = np.random.rand(3, 10)\n    array_ab = np.random.rand(3, 10)\n\n    float_a = random.random()\n    float_b = random.random()\n    float_ab = random.random()\n\n    def test_types(self) -> None:\n        distance = Distance(self.array_a, self.array_b, self.array_ab)\n\n        assert isinstance(distance.compressed_values_a, np.ndarray)\n        assert isinstance(distance.compressed_values_b, np.ndarray)\n        assert isinstance(distance.compressed_values_ab, np.ndarray)\n\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        assert isinstance(distance.compressed_values_a, np.ndarray)\n        assert isinstance(distance.compressed_values_b, np.ndarray)\n        assert isinstance(distance.compressed_values_ab, np.ndarray)\n\n    def test_invalid_shapes(self) -> None:\n        array_a_shape = self.array_a.shape\n        invalid_shape_array = np.random.rand(array_a_shape[0] + 1, array_a_shape[1] + 1)\n        assert invalid_shape_array.shape != self.array_a.shape\n\n        with pytest.raises(InvalidShapeException):\n            Distance(self.array_a, self.array_b, invalid_shape_array)\n\n    def test__ncd(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance._ncd(self.float_a, self.float_b, self.float_ab)\n        assert isinstance(out, float)\n\n        a = b = ab = 0\n        with pytest.raises(CompressedValuesEqualZero):\n            distance._ncd(a, b, ab)\n\n        with pytest.raises(ValueError):\n            distance._ncd(self.array_a, self.array_b, self.array_ab)\n\n    def test__cdm(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance._cdm(self.float_a, self.float_b, self.float_ab)\n        assert isinstance(out, float)\n\n        a = b = ab = 0\n        with pytest.raises(CompressedValuesEqualZero):\n            distance._cdm(a, b, ab)\n\n        with pytest.raises(ValueError):\n            distance._ncd(self.array_a, self.array_b, self.array_ab)\n\n    def test__clm(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance._clm(self.float_a, self.float_b, self.float_ab)\n        assert isinstance(out, float)\n\n        a = b = ab = 0\n        with pytest.raises(CompressedValuesEqualZero):\n            distance._clm(a, b, ab)\n\n        with pytest.raises(ValueError):\n            distance._ncd(self.array_a, self.array_b, self.array_ab)\n\n    def test__mse(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance._mse(self.float_a, self.float_b)\n        assert isinstance(out, float)\n\n        a = b = 0\n        out = distance._mse(a, b)\n        assert isinstance(out, float)\n\n    def test_ncd(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance.ncd\n        assert isinstance(out, np.ndarray)\n\n        distance = Distance(self.array_a, self.array_b, self.array_ab)\n        out = distance.ncd\n        assert isinstance(out, np.ndarray)\n\n    def test_cdm(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance.cdm\n        assert isinstance(out, np.ndarray)\n\n        distance = Distance(self.array_a, self.array_b, self.array_ab)\n        out = distance.cdm\n        assert isinstance(out, np.ndarray)\n\n    def test_clm(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance.clm\n        assert isinstance(out, np.ndarray)\n\n        distance = Distance(self.array_a, self.array_b, self.array_ab)\n        out = distance.clm\n        assert isinstance(out, np.ndarray)\n\n    def test_mse(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance.mse\n        assert isinstance(out, np.ndarray)\n\n        distance = Distance(self.array_a, self.array_b, self.array_ab)\n        out = distance.mse\n        assert isinstance(out, np.ndarray)", "\nclass TestDistance:\n    array_a = np.random.rand(3, 10)\n    array_b = np.random.rand(3, 10)\n    array_ab = np.random.rand(3, 10)\n\n    float_a = random.random()\n    float_b = random.random()\n    float_ab = random.random()\n\n    def test_types(self) -> None:\n        distance = Distance(self.array_a, self.array_b, self.array_ab)\n\n        assert isinstance(distance.compressed_values_a, np.ndarray)\n        assert isinstance(distance.compressed_values_b, np.ndarray)\n        assert isinstance(distance.compressed_values_ab, np.ndarray)\n\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        assert isinstance(distance.compressed_values_a, np.ndarray)\n        assert isinstance(distance.compressed_values_b, np.ndarray)\n        assert isinstance(distance.compressed_values_ab, np.ndarray)\n\n    def test_invalid_shapes(self) -> None:\n        array_a_shape = self.array_a.shape\n        invalid_shape_array = np.random.rand(array_a_shape[0] + 1, array_a_shape[1] + 1)\n        assert invalid_shape_array.shape != self.array_a.shape\n\n        with pytest.raises(InvalidShapeException):\n            Distance(self.array_a, self.array_b, invalid_shape_array)\n\n    def test__ncd(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance._ncd(self.float_a, self.float_b, self.float_ab)\n        assert isinstance(out, float)\n\n        a = b = ab = 0\n        with pytest.raises(CompressedValuesEqualZero):\n            distance._ncd(a, b, ab)\n\n        with pytest.raises(ValueError):\n            distance._ncd(self.array_a, self.array_b, self.array_ab)\n\n    def test__cdm(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance._cdm(self.float_a, self.float_b, self.float_ab)\n        assert isinstance(out, float)\n\n        a = b = ab = 0\n        with pytest.raises(CompressedValuesEqualZero):\n            distance._cdm(a, b, ab)\n\n        with pytest.raises(ValueError):\n            distance._ncd(self.array_a, self.array_b, self.array_ab)\n\n    def test__clm(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance._clm(self.float_a, self.float_b, self.float_ab)\n        assert isinstance(out, float)\n\n        a = b = ab = 0\n        with pytest.raises(CompressedValuesEqualZero):\n            distance._clm(a, b, ab)\n\n        with pytest.raises(ValueError):\n            distance._ncd(self.array_a, self.array_b, self.array_ab)\n\n    def test__mse(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance._mse(self.float_a, self.float_b)\n        assert isinstance(out, float)\n\n        a = b = 0\n        out = distance._mse(a, b)\n        assert isinstance(out, float)\n\n    def test_ncd(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance.ncd\n        assert isinstance(out, np.ndarray)\n\n        distance = Distance(self.array_a, self.array_b, self.array_ab)\n        out = distance.ncd\n        assert isinstance(out, np.ndarray)\n\n    def test_cdm(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance.cdm\n        assert isinstance(out, np.ndarray)\n\n        distance = Distance(self.array_a, self.array_b, self.array_ab)\n        out = distance.cdm\n        assert isinstance(out, np.ndarray)\n\n    def test_clm(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance.clm\n        assert isinstance(out, np.ndarray)\n\n        distance = Distance(self.array_a, self.array_b, self.array_ab)\n        out = distance.clm\n        assert isinstance(out, np.ndarray)\n\n    def test_mse(self) -> None:\n        distance = Distance(self.float_a, self.float_b, self.float_ab)\n        out = distance.mse\n        assert isinstance(out, np.ndarray)\n\n        distance = Distance(self.array_a, self.array_b, self.array_ab)\n        out = distance.mse\n        assert isinstance(out, np.ndarray)", ""]}
{"filename": "tests/test_aggregations.py", "chunked_list": ["from npc_gzip.aggregations import aggregate_strings, concatenate_with_space\n\n\nclass TestAggregations:\n    stringa: str = \"hey there how are you?\"\n    stringb: str = \"I am just hanging out!\"\n\n    def test_concatenate_with_space(self) -> None:\n        out = concatenate_with_space(self.stringa, self.stringb)\n\n        assert len(out) == len(self.stringa) + len(self.stringb) + 1\n        assert out == f\"{self.stringa} {self.stringb}\"\n\n    def test_aggregate_strings(self) -> None:\n        out = aggregate_strings(self.stringa, self.stringb, by_character=False)\n        assert len(out) == len(self.stringa) + len(self.stringb) + 1\n\n    def test_aggregate_strings_by_character(self) -> None:\n        out = aggregate_strings(self.stringa, self.stringb, by_character=True)\n        total_length = len(\"\".join(self.stringa.split()))\n        total_length += len(\"\".join(self.stringb.split()))\n        assert len(out) == total_length", ""]}
{"filename": "tests/test_base_compressor.py", "chunked_list": ["import gzip\n\nimport pytest\n\nfrom npc_gzip.compressors.base import BaseCompressor\nfrom npc_gzip.exceptions import InvalidCompressorException\n\n\nclass TestBaseCompressor:\n    compressor = BaseCompressor(compressor=gzip)\n    example_input = \"hello there!\"\n\n    def test_types(self) -> None:\n        invalid_compressors = [\"test\", 0.1, 123, (\"hello\", \"there\"), {\"hey\": \"hi\"}]\n\n        for compressor in invalid_compressors:\n            with pytest.raises(InvalidCompressorException):\n                BaseCompressor(compressor)\n\n        valid_compressor = gzip\n        BaseCompressor(valid_compressor)\n\n    def test__open_file(self) -> None:\n        valid_filepath = \"tests/test_base_compressor.py\"\n        invalid_filepath = \"tests/test_base_compressor.py.xyz\"\n\n        with pytest.raises(AssertionError):\n            self.compressor._open_file(invalid_filepath)\n\n        with pytest.raises(AssertionError):\n            self.compressor._open_file(invalid_filepath, as_bytes=True)\n\n        file_contents = self.compressor._open_file(valid_filepath, as_bytes=False)\n        assert isinstance(file_contents, str)\n\n        file_contents = self.compressor._open_file(valid_filepath, as_bytes=True)\n        assert isinstance(file_contents, str)\n\n    def test__compress(self) -> None:\n        compressed_bytes = self.compressor._compress(self.example_input)\n        assert isinstance(compressed_bytes, bytes)\n\n    def test_get_compressed_length(self) -> None:\n        example_input_length = self.compressor.get_compressed_length(self.example_input)\n        assert isinstance(example_input_length, int)\n        assert example_input_length > 0\n\n    def test_get_bits_per_character(self) -> None:\n        example_bits_per_character = self.compressor.get_bits_per_character(\n            self.example_input\n        )\n        assert isinstance(example_bits_per_character, float)", "class TestBaseCompressor:\n    compressor = BaseCompressor(compressor=gzip)\n    example_input = \"hello there!\"\n\n    def test_types(self) -> None:\n        invalid_compressors = [\"test\", 0.1, 123, (\"hello\", \"there\"), {\"hey\": \"hi\"}]\n\n        for compressor in invalid_compressors:\n            with pytest.raises(InvalidCompressorException):\n                BaseCompressor(compressor)\n\n        valid_compressor = gzip\n        BaseCompressor(valid_compressor)\n\n    def test__open_file(self) -> None:\n        valid_filepath = \"tests/test_base_compressor.py\"\n        invalid_filepath = \"tests/test_base_compressor.py.xyz\"\n\n        with pytest.raises(AssertionError):\n            self.compressor._open_file(invalid_filepath)\n\n        with pytest.raises(AssertionError):\n            self.compressor._open_file(invalid_filepath, as_bytes=True)\n\n        file_contents = self.compressor._open_file(valid_filepath, as_bytes=False)\n        assert isinstance(file_contents, str)\n\n        file_contents = self.compressor._open_file(valid_filepath, as_bytes=True)\n        assert isinstance(file_contents, str)\n\n    def test__compress(self) -> None:\n        compressed_bytes = self.compressor._compress(self.example_input)\n        assert isinstance(compressed_bytes, bytes)\n\n    def test_get_compressed_length(self) -> None:\n        example_input_length = self.compressor.get_compressed_length(self.example_input)\n        assert isinstance(example_input_length, int)\n        assert example_input_length > 0\n\n    def test_get_bits_per_character(self) -> None:\n        example_bits_per_character = self.compressor.get_bits_per_character(\n            self.example_input\n        )\n        assert isinstance(example_bits_per_character, float)", ""]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_lzma_compressor.py", "chunked_list": ["import lzma\n\nfrom npc_gzip.compressors.base import BaseCompressor\nfrom npc_gzip.compressors.lzma_compressor import LzmaCompressor\n\n\nclass TestBz2Compressor:\n    base_compressor = BaseCompressor(lzma)\n    compressor = LzmaCompressor()\n    example_input = \"hello there!\"\n\n    def test__compress(self) -> None:\n        compressed_bytes = self.compressor._compress(self.example_input)\n        base_compressed_bytes = self.base_compressor._compress(self.example_input)\n        assert compressed_bytes == base_compressed_bytes\n\n    def test_get_compressed_length(self) -> None:\n        example_input_length = self.compressor.get_compressed_length(self.example_input)\n        assert isinstance(example_input_length, int)\n        assert example_input_length > 0\n\n    def test_get_bits_per_character(self) -> None:\n        example_bits_per_character = self.compressor.get_bits_per_character(\n            self.example_input\n        )\n        assert isinstance(example_bits_per_character, float)", ""]}
{"filename": "tests/test_knn_classifier.py", "chunked_list": ["import random\n\nimport numpy as np\nimport pytest\n\nfrom npc_gzip.compressors.base import BaseCompressor\nfrom npc_gzip.compressors.bz2_compressor import Bz2Compressor\nfrom npc_gzip.compressors.gzip_compressor import GZipCompressor\nfrom npc_gzip.compressors.lzma_compressor import LzmaCompressor\nfrom npc_gzip.exceptions import (", "from npc_gzip.compressors.lzma_compressor import LzmaCompressor\nfrom npc_gzip.exceptions import (\n    InputLabelEqualLengthException,\n    UnsupportedDistanceMetricException,\n)\nfrom npc_gzip.knn_classifier import KnnClassifier\nfrom npc_gzip.utils import generate_dataset\n\n\nclass TestKnnClassifier:\n    gzip_compressor: BaseCompressor = GZipCompressor()\n    bz2_compressor: BaseCompressor = Bz2Compressor()\n    lzma_compressor: BaseCompressor = LzmaCompressor()\n\n    dataset_size: int = 100\n    training_dataset: list = generate_dataset(dataset_size)\n    training_labels: list = [random.randint(0, 10) for _ in range(dataset_size)]\n\n    model = KnnClassifier(\n        compressor=gzip_compressor,\n        training_inputs=training_dataset,\n        training_labels=training_labels,\n        distance_metric=\"ncd\",\n    )\n\n    sample_input: str = \"hello there\"\n\n    def test_init(self) -> None:\n        assert self.model.distance_metric == \"ncd\"\n        assert isinstance(self.model.training_inputs, np.ndarray)\n        assert isinstance(self.model.compressed_training_inputs, np.ndarray)\n\n        assert (\n            self.model.training_inputs.shape[0]\n            == self.model.training_labels.shape[0]\n            == self.model.compressed_training_inputs.shape[0]\n        )\n\n        assert self.model.supported_distance_metrics == [\"ncd\", \"clm\", \"cdm\", \"mse\"]\n\n        model = KnnClassifier(\n            compressor=self.gzip_compressor,\n            training_inputs=np.array(self.training_dataset),\n            training_labels=np.array(self.training_labels),\n            distance_metric=\"ncd\",\n        )\n\n        assert isinstance(model.training_inputs, np.ndarray)\n        assert isinstance(model.compressed_training_inputs, np.ndarray)\n        assert (\n            model.training_inputs.shape[0]\n            == model.training_labels.shape[0]\n            == model.compressed_training_inputs.shape[0]\n        )\n\n    def test_invalid_metric(self) -> None:\n        with pytest.raises(UnsupportedDistanceMetricException):\n            KnnClassifier(\n                compressor=self.gzip_compressor,\n                training_inputs=np.array(self.training_dataset),\n                training_labels=np.array(self.training_labels),\n                distance_metric=\"hoopla\",\n            )\n\n    def test_invalid_data_and_label_size(self) -> None:\n        training_data_size = 10\n        training_label_size = training_data_size - 1\n\n        dataset = generate_dataset(training_data_size)\n        labels = [random.randint(0, 10) for _ in range(training_label_size)]\n\n        with pytest.raises(InputLabelEqualLengthException):\n            KnnClassifier(\n                compressor=self.gzip_compressor,\n                training_inputs=dataset,\n                training_labels=labels,\n                distance_metric=\"ncd\",\n            )\n\n    def test__compress_sample(self) -> None:\n        compressed_input, compressed_combined = self.model._compress_sample(\n            self.sample_input\n        )\n\n        assert isinstance(compressed_input, np.ndarray)\n        assert isinstance(compressed_combined, np.ndarray)\n        assert compressed_input.shape == compressed_combined.shape\n\n    def test__calculate_distance(self) -> None:\n        compressed_input, compressed_combined = self.model._compress_sample(\n            self.sample_input\n        )\n        distance = self.model._calculate_distance(compressed_input, compressed_combined)\n\n        assert isinstance(distance, np.ndarray)\n        assert distance.shape == compressed_input.shape == compressed_combined.shape\n\n    def test_predict(self) -> None:\n        top_k = 1\n        (distance, labels, similar_samples) = self.model.predict(\n            self.sample_input, top_k\n        )\n\n        assert distance.shape == (\n            1,\n            self.model.training_inputs.shape[0],\n        )\n        assert labels.shape == (1,)\n        assert similar_samples.shape == (top_k, 1)\n\n        test_set_size = random.randint(2, 50)\n        test_set = [self.sample_input for _ in range(test_set_size)]\n        top_k = 2\n        (distance, labels, similar_samples) = self.model.predict(test_set, top_k)\n\n        assert distance.shape == (test_set_size, self.model.training_inputs.shape[0])\n        assert labels.shape == (test_set_size,)\n        assert similar_samples.shape == (test_set_size, top_k)\n\n    def test_negative_top_k(self) -> None:\n        test_set_size = random.randint(1, 50)\n        test_set = [self.sample_input for _ in range(test_set_size)]\n        top_k = -1\n\n        with pytest.raises(AssertionError):\n            self.model.predict(test_set, top_k)\n\n    def test_top_k_bigger_than_test_set(self) -> None:\n        test_set_size = random.randint(1, 10)\n        test_set = [self.sample_input for _ in range(test_set_size)]\n        top_k = test_set_size + 1\n        with pytest.raises(AssertionError):\n            self.model.predict(test_set, top_k)\n\n    def test_sampling_percentage(self) -> None:\n        test_set_size = random.randint(1, 10)\n        test_set = [self.sample_input for _ in range(test_set_size)]\n        top_k = 1\n        (full_distance, full_labels, full_similar_samples) = self.model.predict(\n            test_set, top_k, sampling_percentage=1.0\n        )\n        (distance, labels, similar_samples) = self.model.predict(\n            test_set, top_k, sampling_percentage=0.1\n        )\n\n        assert full_labels.shape == labels.shape\n        assert full_similar_samples.shape == similar_samples.shape\n\n    def test_sample_data(self) -> None:\n        sampling_percentage: float = 0.8\n        (\n            randomly_sampled_inputs,\n            randomly_sampled_labels,\n            randomly_sampled_indices,\n        ) = self.model.sample_data(sampling_percentage)\n\n        assert (\n            randomly_sampled_inputs.shape\n            == randomly_sampled_labels.shape\n            == randomly_sampled_indices.shape\n        )\n        assert randomly_sampled_inputs.shape[0] == int(\n            self.model.training_inputs.shape[0] * sampling_percentage\n        )\n\n        assert (\n            self.model.training_inputs[randomly_sampled_indices]\n            == randomly_sampled_inputs\n        ).all()", "\nclass TestKnnClassifier:\n    gzip_compressor: BaseCompressor = GZipCompressor()\n    bz2_compressor: BaseCompressor = Bz2Compressor()\n    lzma_compressor: BaseCompressor = LzmaCompressor()\n\n    dataset_size: int = 100\n    training_dataset: list = generate_dataset(dataset_size)\n    training_labels: list = [random.randint(0, 10) for _ in range(dataset_size)]\n\n    model = KnnClassifier(\n        compressor=gzip_compressor,\n        training_inputs=training_dataset,\n        training_labels=training_labels,\n        distance_metric=\"ncd\",\n    )\n\n    sample_input: str = \"hello there\"\n\n    def test_init(self) -> None:\n        assert self.model.distance_metric == \"ncd\"\n        assert isinstance(self.model.training_inputs, np.ndarray)\n        assert isinstance(self.model.compressed_training_inputs, np.ndarray)\n\n        assert (\n            self.model.training_inputs.shape[0]\n            == self.model.training_labels.shape[0]\n            == self.model.compressed_training_inputs.shape[0]\n        )\n\n        assert self.model.supported_distance_metrics == [\"ncd\", \"clm\", \"cdm\", \"mse\"]\n\n        model = KnnClassifier(\n            compressor=self.gzip_compressor,\n            training_inputs=np.array(self.training_dataset),\n            training_labels=np.array(self.training_labels),\n            distance_metric=\"ncd\",\n        )\n\n        assert isinstance(model.training_inputs, np.ndarray)\n        assert isinstance(model.compressed_training_inputs, np.ndarray)\n        assert (\n            model.training_inputs.shape[0]\n            == model.training_labels.shape[0]\n            == model.compressed_training_inputs.shape[0]\n        )\n\n    def test_invalid_metric(self) -> None:\n        with pytest.raises(UnsupportedDistanceMetricException):\n            KnnClassifier(\n                compressor=self.gzip_compressor,\n                training_inputs=np.array(self.training_dataset),\n                training_labels=np.array(self.training_labels),\n                distance_metric=\"hoopla\",\n            )\n\n    def test_invalid_data_and_label_size(self) -> None:\n        training_data_size = 10\n        training_label_size = training_data_size - 1\n\n        dataset = generate_dataset(training_data_size)\n        labels = [random.randint(0, 10) for _ in range(training_label_size)]\n\n        with pytest.raises(InputLabelEqualLengthException):\n            KnnClassifier(\n                compressor=self.gzip_compressor,\n                training_inputs=dataset,\n                training_labels=labels,\n                distance_metric=\"ncd\",\n            )\n\n    def test__compress_sample(self) -> None:\n        compressed_input, compressed_combined = self.model._compress_sample(\n            self.sample_input\n        )\n\n        assert isinstance(compressed_input, np.ndarray)\n        assert isinstance(compressed_combined, np.ndarray)\n        assert compressed_input.shape == compressed_combined.shape\n\n    def test__calculate_distance(self) -> None:\n        compressed_input, compressed_combined = self.model._compress_sample(\n            self.sample_input\n        )\n        distance = self.model._calculate_distance(compressed_input, compressed_combined)\n\n        assert isinstance(distance, np.ndarray)\n        assert distance.shape == compressed_input.shape == compressed_combined.shape\n\n    def test_predict(self) -> None:\n        top_k = 1\n        (distance, labels, similar_samples) = self.model.predict(\n            self.sample_input, top_k\n        )\n\n        assert distance.shape == (\n            1,\n            self.model.training_inputs.shape[0],\n        )\n        assert labels.shape == (1,)\n        assert similar_samples.shape == (top_k, 1)\n\n        test_set_size = random.randint(2, 50)\n        test_set = [self.sample_input for _ in range(test_set_size)]\n        top_k = 2\n        (distance, labels, similar_samples) = self.model.predict(test_set, top_k)\n\n        assert distance.shape == (test_set_size, self.model.training_inputs.shape[0])\n        assert labels.shape == (test_set_size,)\n        assert similar_samples.shape == (test_set_size, top_k)\n\n    def test_negative_top_k(self) -> None:\n        test_set_size = random.randint(1, 50)\n        test_set = [self.sample_input for _ in range(test_set_size)]\n        top_k = -1\n\n        with pytest.raises(AssertionError):\n            self.model.predict(test_set, top_k)\n\n    def test_top_k_bigger_than_test_set(self) -> None:\n        test_set_size = random.randint(1, 10)\n        test_set = [self.sample_input for _ in range(test_set_size)]\n        top_k = test_set_size + 1\n        with pytest.raises(AssertionError):\n            self.model.predict(test_set, top_k)\n\n    def test_sampling_percentage(self) -> None:\n        test_set_size = random.randint(1, 10)\n        test_set = [self.sample_input for _ in range(test_set_size)]\n        top_k = 1\n        (full_distance, full_labels, full_similar_samples) = self.model.predict(\n            test_set, top_k, sampling_percentage=1.0\n        )\n        (distance, labels, similar_samples) = self.model.predict(\n            test_set, top_k, sampling_percentage=0.1\n        )\n\n        assert full_labels.shape == labels.shape\n        assert full_similar_samples.shape == similar_samples.shape\n\n    def test_sample_data(self) -> None:\n        sampling_percentage: float = 0.8\n        (\n            randomly_sampled_inputs,\n            randomly_sampled_labels,\n            randomly_sampled_indices,\n        ) = self.model.sample_data(sampling_percentage)\n\n        assert (\n            randomly_sampled_inputs.shape\n            == randomly_sampled_labels.shape\n            == randomly_sampled_indices.shape\n        )\n        assert randomly_sampled_inputs.shape[0] == int(\n            self.model.training_inputs.shape[0] * sampling_percentage\n        )\n\n        assert (\n            self.model.training_inputs[randomly_sampled_indices]\n            == randomly_sampled_inputs\n        ).all()", ""]}
{"filename": "tests/test_bz2_compressor.py", "chunked_list": ["import bz2\n\nfrom npc_gzip.compressors.base import BaseCompressor\nfrom npc_gzip.compressors.bz2_compressor import Bz2Compressor\n\n\nclass TestBz2Compressor:\n    base_compressor = BaseCompressor(bz2)\n    compressor = Bz2Compressor()\n    example_input = \"hello there!\"\n\n    def test__compress(self) -> None:\n        compressed_bytes = self.compressor._compress(self.example_input)\n        base_compressed_bytes = self.base_compressor._compress(self.example_input)\n        assert compressed_bytes == base_compressed_bytes\n\n    def test_get_compressed_length(self) -> None:\n        example_input_length = self.compressor.get_compressed_length(self.example_input)\n        assert isinstance(example_input_length, int)\n        assert example_input_length > 0\n\n    def test_get_bits_per_character(self) -> None:\n        example_bits_per_character = self.compressor.get_bits_per_character(\n            self.example_input\n        )\n        assert isinstance(example_bits_per_character, float)", ""]}
{"filename": "tests/test_utils.py", "chunked_list": ["import random\n\nimport pytest\n\nfrom npc_gzip.utils import generate_dataset, generate_sentence\n\n\nclass TestUtils:\n    def test_generate_sentence(self) -> None:\n        for _ in range(100):\n            number_of_words = random.randint(1, 100)\n            sentence = generate_sentence(number_of_words)\n            assert len(sentence.split()) == number_of_words\n\n        with pytest.raises(TypeError):\n            generate_sentence(\"hey there\")\n\n        with pytest.raises(AssertionError):\n            generate_sentence(-1)\n\n    def test_generate_dataset(self) -> None:\n        for _ in range(100):\n            number_of_sentences = random.randint(1, 100)\n            dataset = generate_dataset(number_of_sentences)\n            assert len(dataset) == number_of_sentences\n\n        with pytest.raises(TypeError):\n            generate_dataset(\"hey there\")\n\n        with pytest.raises(AssertionError):\n            generate_dataset(-1)", ""]}
{"filename": "tests/test_gzip_compressor.py", "chunked_list": ["import gzip\n\nfrom npc_gzip.compressors.base import BaseCompressor\nfrom npc_gzip.compressors.gzip_compressor import GZipCompressor\n\n\nclass TestBz2Compressor:\n    base_compressor = BaseCompressor(gzip)\n    compressor = GZipCompressor()\n    example_input = \"hello there!\"\n\n    def test__compress(self) -> None:\n        compressed_bytes = self.compressor._compress(self.example_input)\n        base_compressed_bytes = self.base_compressor._compress(self.example_input)\n        assert compressed_bytes == base_compressed_bytes\n\n    def test_get_compressed_length(self) -> None:\n        example_input_length = self.compressor.get_compressed_length(self.example_input)\n        assert isinstance(example_input_length, int)\n        assert example_input_length > 0\n\n    def test_get_bits_per_character(self) -> None:\n        example_bits_per_character = self.compressor.get_bits_per_character(\n            self.example_input\n        )\n        assert isinstance(example_bits_per_character, float)", ""]}
{"filename": "examples/ag_news.py", "chunked_list": ["import numpy as np\nfrom sklearn.metrics import classification_report\nfrom torchtext.datasets import AG_NEWS\n\nfrom npc_gzip.compressors.base import BaseCompressor\nfrom npc_gzip.compressors.gzip_compressor import GZipCompressor\nfrom npc_gzip.knn_classifier import KnnClassifier\n\n\ndef get_data() -> tuple:\n    \"\"\"\n    Pulls the AG_NEWS dataset\n    and returns two tuples the first being the\n    training data and the second being the test\n    data. Each tuple contains the text and label\n    respectively as numpy arrays.\n\n    \"\"\"\n\n    train_iter, test_iter = AG_NEWS(split=(\"train\", \"test\"))\n\n    train_text = []\n    train_labels = []\n    for label, text in train_iter:\n        train_labels.append(label)\n        train_text.append(text)\n\n    test_text = []\n    test_labels = []\n    for label, text in test_iter:\n        test_labels.append(label)\n        test_text.append(text)\n\n    train_text = np.array(train_text)\n    train_labels = np.array(train_labels)\n\n    test_text = np.array(test_text)\n    test_labels = np.array(test_labels)\n\n    train = (train_text, train_labels)\n    test = (test_text, test_labels)\n\n    return (train, test)", "\ndef get_data() -> tuple:\n    \"\"\"\n    Pulls the AG_NEWS dataset\n    and returns two tuples the first being the\n    training data and the second being the test\n    data. Each tuple contains the text and label\n    respectively as numpy arrays.\n\n    \"\"\"\n\n    train_iter, test_iter = AG_NEWS(split=(\"train\", \"test\"))\n\n    train_text = []\n    train_labels = []\n    for label, text in train_iter:\n        train_labels.append(label)\n        train_text.append(text)\n\n    test_text = []\n    test_labels = []\n    for label, text in test_iter:\n        test_labels.append(label)\n        test_text.append(text)\n\n    train_text = np.array(train_text)\n    train_labels = np.array(train_labels)\n\n    test_text = np.array(test_text)\n    test_labels = np.array(test_labels)\n\n    train = (train_text, train_labels)\n    test = (test_text, test_labels)\n\n    return (train, test)", "\n\ndef fit_model(\n    train_text: np.ndarray, train_labels: np.ndarray, distance_metric: str = \"ncd\"\n) -> KnnClassifier:\n    \"\"\"\n    Fits a Knn-GZip compressor on the train\n    data and returns it.\n\n    Arguments:\n        train_text (np.ndarray): Training dataset as a numpy array.\n        train_labels (np.ndarray): Training labels as a numpy array.\n\n    Returns:\n        KnnClassifier: Trained Knn-Compressor model ready to make predictions.\n    \"\"\"\n\n    compressor: BaseCompressor = GZipCompressor()\n    model: KnnClassifier = KnnClassifier(\n        compressor=compressor,\n        training_inputs=train_text,\n        training_labels=train_labels,\n        distance_metric=distance_metric,\n    )\n\n    return model", "\n\ndef main() -> None:\n    print(\"Fetching data...\")\n    ((train_text, train_labels), (test_text, test_labels)) = get_data()\n\n    print(\"Fitting model...\")\n    model = fit_model(train_text, train_labels)\n    random_indicies = np.random.choice(test_text.shape[0], 1000, replace=False)\n\n    sample_test_text = test_text[random_indicies]\n    sample_test_labels = test_labels[random_indicies]\n\n    print(\"Generating predictions...\")\n    top_k = 1\n\n    # Here we use the `sampling_percentage` to save time\n    # at the expense of worse predictions. This\n    # `sampling_percentage` selects a random % of training\n    # data to compare `sample_test_text` against rather\n    # than comparing it against the entire training dataset.\n    (distances, labels, similar_samples) = model.predict(\n        sample_test_text, top_k, sampling_percentage=0.01\n    )\n\n    print(classification_report(sample_test_labels, labels.reshape(-1)))", "\n\nif __name__ == \"__main__\":\n    main()\n\n\n#               precision    recall  f1-score   support\n\n#            1       0.67      0.80      0.73       246\n#            2       0.78      0.74      0.76       246", "#            1       0.67      0.80      0.73       246\n#            2       0.78      0.74      0.76       246\n#            3       0.64      0.61      0.62       249\n#            4       0.65      0.59      0.62       259\n\n#     accuracy                           0.68      1000\n#    macro avg       0.68      0.68      0.68      1000\n# weighted avg       0.68      0.68      0.68      1000\n", ""]}
{"filename": "examples/imdb.py", "chunked_list": ["import numpy as np\nfrom sklearn.metrics import classification_report\nfrom torchtext.datasets import IMDB\n\nfrom npc_gzip.compressors.base import BaseCompressor\nfrom npc_gzip.compressors.gzip_compressor import GZipCompressor\nfrom npc_gzip.knn_classifier import KnnClassifier\n\n\ndef get_data() -> tuple:\n    \"\"\"\n    Pulls the IMDB sentiment analysis dataset\n    and returns two tuples the first being the\n    training data and the second being the test\n    data. Each tuple contains the text and label\n    respectively as numpy arrays.\n\n    \"\"\"\n\n    train_iter, test_iter = IMDB(split=(\"train\", \"test\"))\n\n    train_text = []\n    train_labels = []\n    for label, text in train_iter:\n        train_labels.append(label)\n        train_text.append(text)\n\n    test_text = []\n    test_labels = []\n    for label, text in test_iter:\n        test_labels.append(label)\n        test_text.append(text)\n\n    train_text = np.array(train_text)\n    train_labels = np.array(train_labels)\n\n    test_text = np.array(test_text)\n    test_labels = np.array(test_labels)\n\n    train = (train_text, train_labels)\n    test = (test_text, test_labels)\n\n    return (train, test)", "\ndef get_data() -> tuple:\n    \"\"\"\n    Pulls the IMDB sentiment analysis dataset\n    and returns two tuples the first being the\n    training data and the second being the test\n    data. Each tuple contains the text and label\n    respectively as numpy arrays.\n\n    \"\"\"\n\n    train_iter, test_iter = IMDB(split=(\"train\", \"test\"))\n\n    train_text = []\n    train_labels = []\n    for label, text in train_iter:\n        train_labels.append(label)\n        train_text.append(text)\n\n    test_text = []\n    test_labels = []\n    for label, text in test_iter:\n        test_labels.append(label)\n        test_text.append(text)\n\n    train_text = np.array(train_text)\n    train_labels = np.array(train_labels)\n\n    test_text = np.array(test_text)\n    test_labels = np.array(test_labels)\n\n    train = (train_text, train_labels)\n    test = (test_text, test_labels)\n\n    return (train, test)", "\n\ndef fit_model(\n    train_text: np.ndarray, train_labels: np.ndarray, distance_metric: str = \"ncd\"\n) -> KnnClassifier:\n    \"\"\"\n    Fits a Knn-GZip compressor on the train\n    data and returns it.\n\n    Arguments:\n        train_text (np.ndarray): Training dataset as a numpy array.\n        train_labels (np.ndarray): Training labels as a numpy array.\n\n    Returns:\n        KnnClassifier: Trained Knn-Compressor model ready to make predictions.\n    \"\"\"\n\n    compressor: BaseCompressor = GZipCompressor()\n    model: KnnClassifier = KnnClassifier(\n        compressor=compressor,\n        training_inputs=train_text,\n        training_labels=train_labels,\n        distance_metric=distance_metric,\n    )\n\n    return model", "\n\ndef main() -> None:\n    print(\"Fetching data...\")\n    ((train_text, train_labels), (test_text, test_labels)) = get_data()\n\n    print(\"Fitting model...\")\n    model = fit_model(train_text, train_labels)\n\n    # Randomly sampling from the test set.\n    # The IMDb test data comes in with all of the\n    # `1` labels first, then all of the `2` labels\n    # last, so we're shuffling so that our model\n    # has something to predict other than `1`.\n\n    random_indicies = np.random.choice(test_text.shape[0], 1000, replace=False)\n\n    sample_test_text = test_text[random_indicies]\n    sample_test_labels = test_labels[random_indicies]\n\n    print(\"Generating predictions...\")\n    top_k = 1\n\n    # Here we use the `sampling_percentage` to save time\n    # at the expense of worse predictions. This\n    # `sampling_percentage` selects a random % of training\n    # data to compare `sample_test_text` against rather\n    # than comparing it against the entire training dataset.\n    (distances, labels, similar_samples) = model.predict(\n        sample_test_text, top_k, sampling_percentage=0.01\n    )\n\n    print(classification_report(sample_test_labels, labels.reshape(-1)))", "\n\nif __name__ == \"__main__\":\n    main()\n\n\n#               precision    recall  f1-score   support\n\n#            1       0.57      0.63      0.60       495\n#            2       0.59      0.53      0.56       505", "#            1       0.57      0.63      0.60       495\n#            2       0.59      0.53      0.56       505\n\n#     accuracy                           0.58      1000\n#    macro avg       0.58      0.58      0.58      1000\n# weighted avg       0.58      0.58      0.58      1000\n"]}
