{"filename": "evaluation.py", "chunked_list": ["import logging\nfrom tqdm import tqdm\nimport torch\nfrom beir.retrieval.evaluation import EvaluateRetrieval\nfrom models import compute_loss\nfrom utils import Metrics\n\nlogger = logging.getLogger()\n\n\ndef eval_datasets(args, dev_loader, test_loader, model=None):\n    if dev_loader:\n        logger.info(f\"{'*' * 10} Evaluating on dev set {'*' * 10}\")\n        dev_metric = evaluate(args, dev_loader, model)\n    else:\n        dev_metric = None\n    \n    if test_loader:\n        logger.info(f\"{'*' * 10} Evaluating on test set {'*' * 10}\")\n        test_metric = evaluate(args, test_loader, model)\n    else:\n        test_metric = None\n    \n    return dev_metric, test_metric", "\n\ndef eval_datasets(args, dev_loader, test_loader, model=None):\n    if dev_loader:\n        logger.info(f\"{'*' * 10} Evaluating on dev set {'*' * 10}\")\n        dev_metric = evaluate(args, dev_loader, model)\n    else:\n        dev_metric = None\n    \n    if test_loader:\n        logger.info(f\"{'*' * 10} Evaluating on test set {'*' * 10}\")\n        test_metric = evaluate(args, test_loader, model)\n    else:\n        test_metric = None\n    \n    return dev_metric, test_metric", "\n\n@torch.no_grad()\ndef evaluate(args, dataloader, model=None):\n    if model:\n        model.eval()\n    metrics = Metrics()\n    dataloader.dataset.data['pytrec_results'] = dict()\n    dataloader.dataset.data['pytrec_qrels'] = dict()\n    for idx, (qids, sim_matrix, targets) in tqdm(enumerate(dataloader), desc=\"Evaluating\"):\n        sim_matrix, targets = sim_matrix.to(args.device), targets.to(args.device)\n\n        if model:\n            scores = model(sim_matrix)\n            loss = compute_loss(scores, targets)\n            metrics.update(loss=(loss.item(), len(sim_matrix)))\n        else:\n            scores = torch.arange(sim_matrix.shape[-1], 0, -1, \n                                  device=sim_matrix.device)[None, :].expand(sim_matrix.shape[0], -1)\n\n        for qid, score_list in zip(qids, scores):\n            qdata = dataloader.dataset.data[qid]\n            \n            pytrec_results = {pid:score.item() for pid, score in zip(qdata['retrieved_ctxs'], score_list)}\n            dataloader.dataset.data['pytrec_results'][qid] = pytrec_results\n            \n            pytrec_qrels = {pid:1 for pid in qdata['positive_ctxs']}\n            if 'has_answer' in qdata:\n                pytrec_qrels.update({pid:1 for pid, has_answer in zip(qdata['retrieved_ctxs'], qdata['has_answer']) if has_answer})\n            dataloader.dataset.data['pytrec_qrels'][qid] = pytrec_qrels\n\n    if model:\n        logger.info(\"loss: \" + str(metrics.meters['loss']))\n    ndcg, _map, recall, precision = EvaluateRetrieval.evaluate(dataloader.dataset.data['pytrec_qrels'], dataloader.dataset.data['pytrec_results'], [1, 5, 10, 20, 50, 100], ignore_identical_ids=False)\n    mrr = EvaluateRetrieval.evaluate_custom(dataloader.dataset.data['pytrec_qrels'], dataloader.dataset.data['pytrec_results'], [1, 5, 10, 20, 50, 100], metric=\"mrr@k\")\n    accuracy = EvaluateRetrieval.evaluate_custom(dataloader.dataset.data['pytrec_qrels'], dataloader.dataset.data['pytrec_results'], [1, 5, 10, 20, 50, 100], metric=\"accuracy@k\")\n    metrics.update(**ndcg, **mrr, **accuracy)\n    logger.info(\"\\n\")\n    return metrics", ""]}
{"filename": "main.py", "chunked_list": ["import os\nimport logging\nimport torch\nfrom torch import optim\nfrom torch.cuda.amp.grad_scaler import GradScaler\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom config import get_args, print_args", "\nfrom config import get_args, print_args\nfrom dataset import RetrievalSim\nfrom models import Hybrank, compute_loss\nfrom evaluation import eval_datasets\nfrom utils import Metrics, setup_logger\n\nlogger = logging.getLogger()\ntorch.set_num_threads(1)\n", "torch.set_num_threads(1)\n\n\ndef main():\n    args = get_args()\n    if args.device == 'cuda':\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    else:\n        device = torch.device('cpu')\n\n    # make directories\n    args.output_dir = os.path.join('experiments', args.exp_name)\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir, exist_ok=True)\n\n    setup_logger(logger, args)\n    print_args(args)\n\n    logger.info(f\"{'*' * 30} Loading datasets and model {'*' * 30}\")\n    # load datasets\n    assert any([x in args.data_path for x in [\"NQ\", \"MSMARCO\", \"TRECDL2019\", \"TRECDL2020\"]]), \"Dataset unknown.\"\n    data_files = os.listdir(args.data_path)\n\n    if not args.only_eval:\n        train_dataset = RetrievalSim(root=args.data_path, split='train', \n                                    list_len=args.list_len, num_anchors=args.num_anchors)\n        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, \n                                num_workers=args.num_workers, drop_last=False)\n    \n    if any([\"dev\" in x for x in data_files]):\n        dev_dataset = RetrievalSim(root=args.data_path, split=\"dev\", \n                                list_len=args.list_len, num_anchors=args.num_anchors)\n        dev_loader = DataLoader(dev_dataset, batch_size=args.batch_size, shuffle=False, \n                                num_workers=args.num_workers, drop_last=False)\n    else:\n        dev_dataset, dev_loader = None, None\n    \n    if any([\"test\" in x for x in data_files]):\n        test_dataset = RetrievalSim(root=args.data_path, split=\"test\", \n                                    list_len=args.list_len, num_anchors=args.num_anchors)\n        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, \n                                 num_workers=args.num_workers, drop_last=False)\n    else:\n        test_dataset, test_loader = None, None\n\n    if \"NQ\" in args.data_path:\n        select_criterion_key = \"Accuracy@1\"\n    elif \"MSMARCO\" in args.data_path:\n        select_criterion_key = \"MRR@10\"\n    else:\n        select_criterion_key = \"NDCG@10\"\n\n    # define model\n    model = Hybrank(in_dim=args.num_sim_type, embed_dim=args.embed_dim,\n                    depth=args.depth, num_heads=args.num_heads).to(device)\n\n    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    logger.info('Number of params: {:.2f}M'.format(n_parameters / 1e6))\n\n    start_epoch = 0\n    if args.resume is not None:\n        assert os.path.isfile(args.resume), \"No checkpoint found at '{}'\".format(args.resume)\n        checkpoint = torch.load(args.resume, map_location='cpu')\n        start_epoch = checkpoint['epoch'] + 1\n        model.load_state_dict(checkpoint['state_dict'], strict=False)\n        logger.info(\"Loaded checkpoint: >>>> '{}' (epoch {})\".format(args.resume, checkpoint['epoch']))\n\n    logger.info(f\"{'*' * 30} Initial retrieval list {'*' * 30}\")\n    eval_datasets(args, dev_loader, test_loader)\n\n    logger.info(f\"{'*' * 30} Initialized model performance {'*' * 30}\")\n    eval_datasets(args, dev_loader, test_loader, model)\n\n    if not args.only_eval:\n        no_decay = ['bias', 'LayerNorm.weight', 'norm1.weight', 'norm2.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in model.named_parameters() if\n                        p.requires_grad and not any(nd in n for nd in no_decay)],\n            'weight_decay': args.weight_decay},\n            {'params': [p for n, p in model.named_parameters() if\n                        p.requires_grad and any(nd in n for nd in no_decay)],\n            'weight_decay': 0.0}\n        ]\n        optimizer = optim.Adam(optimizer_grouped_parameters, lr=args.lr, weight_decay=args.weight_decay)\n\n        lr_scheduler = get_cosine_schedule_with_warmup(\n            optimizer=optimizer,\n            num_warmup_steps=args.warmup_epochs * (len(train_loader) // args.gradient_accumulation_steps),\n            num_training_steps=args.num_epochs * (len(train_loader) // args.gradient_accumulation_steps),\n        )\n\n        # Start training\n        logger.info(f\"{'*' * 30} Start training {'*' * 30}\")\n        metrics = Metrics()\n        scaler = GradScaler()\n        tb_writer = SummaryWriter(log_dir=os.path.join(args.output_dir, 'tensorboard'))\n        best_select_criterion, best_epoch, best_epoch_metric = 0, -1, Metrics()\n        for epoch in range(start_epoch, args.num_epochs):\n            logger.info(f\"{'*' * 30} Epoch {epoch} {'*' * 30}\")\n            metrics.reset_meters()\n            optimizer.zero_grad()\n            model.train()\n            for batch_idx, (_, sim_matrix, targets) in enumerate(train_loader):\n                sim_matrix, targets = sim_matrix.to(device), targets.to(device)\n                scores = model(sim_matrix)\n                loss = compute_loss(scores, targets)\n\n                scaler.scale(loss).backward()\n                metrics.meters['loss'].update(loss.item(), n=len(sim_matrix))\n                metrics.meters['lr'].reset()\n                metrics.meters['lr'].update(lr_scheduler.get_last_lr()[0])\n                n_iter = epoch * len(train_loader) + batch_idx\n                tb_writer.add_scalar('train/batch_loss', loss.item(), n_iter)\n                tb_writer.add_scalar('train/learning_rate', metrics.meters['lr'].avg, n_iter)\n\n                if (batch_idx + 1) % args.gradient_accumulation_steps == 0:\n                    if args.max_grad_norm > 0:\n                        scaler.unscale_(optimizer)\n                        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                    scaler.step(optimizer)\n                    scaler.update()\n                    lr_scheduler.step()\n                    optimizer.zero_grad()\n\n                if batch_idx % args.log_per_steps == 0:\n                    batch_num = len(train_loader)\n                    logger.info(f\"Epoch: {epoch}, Batch: {batch_idx:{len(str(batch_num))}d}/{batch_num}, {metrics}\")\n\n            # Save checkpoint\n            model_path = os.path.join(args.output_dir, 'checkpoint-epoch{}.pth'.format(epoch))\n            torch.save({'epoch': epoch, 'state_dict': model.state_dict()}, model_path)\n\n            logger.info(f\"Evaluting model performance for epoch {epoch}\")\n            n_iter = (epoch + 1) * len(train_loader)\n            epoch_dev_metric, epoch_test_metric = eval_datasets(args, dev_loader, test_loader, model)\n            if epoch_dev_metric:\n                for key, meters in epoch_dev_metric.meters.items():\n                    tb_writer.add_scalar(f'dev/{key}', meters.avg, n_iter)\n            elif epoch_test_metric:\n                for key, meters in epoch_test_metric.meters.items():\n                    tb_writer.add_scalar(f'test/{key}', meters.avg, n_iter)\n\n            if epoch_dev_metric is not None:\n                select_criterion = epoch_dev_metric.meters[select_criterion_key].avg\n            else:\n                select_criterion = epoch_test_metric.meters[select_criterion_key].avg\n            \n            if select_criterion > best_select_criterion:\n                best_select_criterion = select_criterion\n                best_epoch_metric = epoch_test_metric if test_loader is not None else epoch_dev_metric\n                best_epoch = epoch\n\n            logger.info(f\">>>>>> Best Epoch: {best_epoch} {best_epoch_metric} <<<<<<\")", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "config.py", "chunked_list": ["import argparse\nimport random\nimport logging\nimport torch\nfrom torch import cuda\nimport numpy as np\n\nlogger = logging.getLogger()\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    # training experiment name\n    parser.add_argument('--exp_name', type=str, default='test')\n    parser.add_argument('--data_path', type=str, default=None)\n\n    # model related params\n    parser.add_argument('--num_sim_type', type=int, default=2)\n    parser.add_argument('--embed_dim', type=int, default=64)\n    parser.add_argument('--depth', type=int, default=2)\n    parser.add_argument('--num_heads', type=int, default=8)\n\n    # training specific args\n    parser.add_argument('--only_eval', action='store_true', default=False)\n    parser.add_argument('--resume', type=str, default=None)\n\n    parser.add_argument('--list_len', type=int, default=100)\n    parser.add_argument('--num_anchors', type=int, default=100)\n\n    parser.add_argument('--batch_size', type=int, default=32)\n\n    parser.add_argument('--num_workers', type=int, default=0)\n    parser.add_argument('--device', type=str, default='cuda' if cuda.is_available() else 'cpu')\n\n    parser.add_argument('--num_epochs', type=int, default=100)\n    parser.add_argument('--warmup_epochs', type=int, default=10)\n    parser.add_argument('--lr', type=float, default=1e-3)\n    parser.add_argument('--weight_decay', type=float, default=1e-6)\n\n    parser.add_argument('--max_grad_norm', type=float, default=2.0)\n    parser.add_argument('--seed', type=int, default=12345)\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1)\n    parser.add_argument('--log_per_steps', type=int, default=100)\n    args = parser.parse_args()\n\n    if args.seed is not None:\n        random.seed(args.seed)\n        np.random.seed(args.seed)\n        torch.manual_seed(args.seed)\n        torch.cuda.manual_seed_all(args.seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n    return args", "\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    # training experiment name\n    parser.add_argument('--exp_name', type=str, default='test')\n    parser.add_argument('--data_path', type=str, default=None)\n\n    # model related params\n    parser.add_argument('--num_sim_type', type=int, default=2)\n    parser.add_argument('--embed_dim', type=int, default=64)\n    parser.add_argument('--depth', type=int, default=2)\n    parser.add_argument('--num_heads', type=int, default=8)\n\n    # training specific args\n    parser.add_argument('--only_eval', action='store_true', default=False)\n    parser.add_argument('--resume', type=str, default=None)\n\n    parser.add_argument('--list_len', type=int, default=100)\n    parser.add_argument('--num_anchors', type=int, default=100)\n\n    parser.add_argument('--batch_size', type=int, default=32)\n\n    parser.add_argument('--num_workers', type=int, default=0)\n    parser.add_argument('--device', type=str, default='cuda' if cuda.is_available() else 'cpu')\n\n    parser.add_argument('--num_epochs', type=int, default=100)\n    parser.add_argument('--warmup_epochs', type=int, default=10)\n    parser.add_argument('--lr', type=float, default=1e-3)\n    parser.add_argument('--weight_decay', type=float, default=1e-6)\n\n    parser.add_argument('--max_grad_norm', type=float, default=2.0)\n    parser.add_argument('--seed', type=int, default=12345)\n    parser.add_argument('--gradient_accumulation_steps', type=int, default=1)\n    parser.add_argument('--log_per_steps', type=int, default=100)\n    args = parser.parse_args()\n\n    if args.seed is not None:\n        random.seed(args.seed)\n        np.random.seed(args.seed)\n        torch.manual_seed(args.seed)\n        torch.cuda.manual_seed_all(args.seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\n    return args", "\n\ndef print_args(args):\n    logger.info(f\"{'*' * 30} CONFIGURATION {'*' * 30}\")\n    for key, val in sorted(vars(args).items()):\n        keystr = \"{}\".format(key) + (\" \" * (30 - len(key)))\n        logger.info(\"%s -->   %s\", keystr, val)\n    logger.info(f\"{'*' * 30} CONFIGURATION {'*' * 30}\")\n", ""]}
{"filename": "models.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import BertConfig, BertModel\n\n\nclass Hybrank(nn.Module):\n    def __init__(self, in_dim=2, embed_dim=64, depth=2, num_heads=8):\n        super().__init__()\n        self.cnn = nn.Conv2d(in_dim, embed_dim, 1)\n        self.embed_dim = embed_dim\n\n        config = BertConfig(vocab_size=1, hidden_size=embed_dim, num_hidden_layers=depth,\n                            num_attention_heads=num_heads, intermediate_size=int(embed_dim * 4))\n        self.col_transformer = BertModel(config=config)\n\n        self.out_cls_token = nn.Parameter(torch.empty(1, 1, embed_dim))\n        nn.init.kaiming_uniform_(self.out_cls_token)\n        config = BertConfig(vocab_size=1, hidden_size=embed_dim, num_hidden_layers=1,\n                            num_attention_heads=num_heads, intermediate_size=int(embed_dim * 4))\n        self.out_transformer = BertModel(config=config)\n\n        self.t = 0.07\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = x.permute(0, 2, 3, 1)\n\n        B, Nrow, Ncol, D = x.shape\n        assert D == self.embed_dim\n\n        x = x.transpose(1, 2).reshape(-1, Nrow, self.embed_dim)\n        x = self.col_transformer(inputs_embeds=x)[0]\n        x = x.reshape(B, Ncol, Nrow, self.embed_dim).transpose(1, 2)\n\n        x = x.reshape(-1, Ncol, self.embed_dim)\n        x = torch.cat((self.out_cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n        x = self.out_transformer(inputs_embeds=x)[0]\n        x = x[:, 0, :]\n        x = x.reshape(B, Nrow, self.embed_dim)\n        x = F.normalize(x, dim=-1)\n        q, p = x[:, :1], x[:, 1:]\n        out = (q @ p.transpose(1, 2)).squeeze(1) / self.t\n        return out", "\n\ndef compute_loss(input, target):\n    \"\"\"\n    :param input: Float tensor of size (B, L)\n    :param target: Bool tensor of size (B, L)\n    :return: loss: Scalar tensor\n    \"\"\"\n    # remove samples without positive\n    mask = target.sum(dim=-1).bool()\n    input, target = input[mask], target[mask]\n\n    target = target.float()\n    log_softmax = input.log_softmax(dim=-1)\n    loss = -((log_softmax * target).sum(-1) / target.sum(-1)).mean()\n    return loss", ""]}
{"filename": "dataset.py", "chunked_list": ["import os\nimport h5py\nimport pickle\nimport logging\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\n\nlogger = logging.getLogger()", "\nlogger = logging.getLogger()\n\n\nclass RetrievalSim(Dataset):\n    r\"\"\"\n    :returns\n        sim_matrix: FloatTensor with size [C, 1 + list_len, num_anchors]\n        label: BoolTensor with size [list_len], relevant or not\n    \"\"\"\n\n    def __init__(self, root='data', split='train', list_len=100, num_anchors=100,\n                 shuffle_retrieval_list=False, perm_feat_dim=False):\n        self.root = root\n        self.split = split\n        self.list_len = int(list_len)\n        self.num_anchors = int(num_anchors)\n        self.shuffle_retrieval_list = shuffle_retrieval_list\n        self.perm_feat_dim = perm_feat_dim\n\n        logger.info(f'Loading {split} dataset...')\n        with open(os.path.join(root, split + '_ground_truth.pkl'), 'rb') as f:\n            self.data = pickle.load(f)\n\n        logger.info('Preparing features...')\n        self.prepare_features()\n\n        logger.info('Preparing labels...')\n        self.prepare_labels()\n\n        self.filter_data()\n\n        self.qs = sorted(list(self.data.keys()))  # sort to guarantee reproducibility\n        logger.info(f'Total number of questions: {len(self.qs)}')\n\n        pos_num = []\n        for q in self.data:\n            pos_num.append(self.data[q]['label'].sum().item())\n        logger.info(f'Average positive number: {sum(pos_num) / len(pos_num)}')\n\n    def filter_data(self):\n        # remove samples without positive in training\n        if self.split == 'train':\n            unlabeled_qs = [q for q in self.data if self.data[q]['label'].sum() == 0]\n            for q in unlabeled_qs:\n                self.data.pop(q)\n\n    def prepare_features(self):\n        with h5py.File(os.path.join(self.root, self.split + '_scores.hdf5'), 'r') as f:\n            for q in tqdm(self.data):\n                bm25_matrix = torch.FloatTensor(np.concatenate((f[q]['query_ctx_bm25_score'][()],\n                                                                f[q]['ctx_ctx_bm25_score'][()]), axis=0))\n                bm25_matrix = bm25_matrix[:int(1 + self.list_len), :int(self.num_anchors)]\n                bm25_matrix = self.norm_feature(bm25_matrix, 100)\n                dense_matrix = torch.FloatTensor(np.concatenate((f[q]['query_ctx_dense_score'][()],\n                                                                 f[q]['ctx_ctx_dense_score'][()]), axis=0))\n                dense_matrix = dense_matrix[:int(1 + self.list_len), :int(self.num_anchors)]\n                dense_matrix = self.norm_feature(dense_matrix, 10)\n                self.data[q]['feature'] = torch.stack([bm25_matrix, dense_matrix], dim=0)\n\n    @staticmethod\n    def norm_feature(x, norm_temperature=None):\n        if norm_temperature is not None:\n            x = (x / norm_temperature).softmax(dim=-1)\n\n        # max-min normalization\n        norm_min = x.min(dim=-1, keepdim=True).values\n        norm_max = x.max(dim=-1, keepdim=True).values\n        x = (x - norm_min) / (norm_max - norm_min + 1e-10)\n        x = x * 2.0 - 1\n        return x\n\n    def prepare_labels(self):\n        for q, d in self.data.items():\n            if 'has_answer' in d and self.split != 'train':\n                # for NQ evaluation\n                has_answer = d['has_answer']\n                label = [hit for hit in has_answer]\n            else:\n                positive_ctxs = d['positive_ctxs']\n                retrieved_ctxs = d['retrieved_ctxs']\n                label = [pid in positive_ctxs for pid in retrieved_ctxs]\n            self.data[q]['label'] = torch.BoolTensor(label)[:int(self.list_len)]\n\n    def __getitem__(self, index):\n        q = self.qs[index]\n        sim_matrix = self.data[q]['feature']\n        label = self.data[q]['label']\n\n        if self.shuffle_retrieval_list:\n            label_perm_idx = torch.randperm(label.shape[0])\n            label = label[label_perm_idx]\n\n            matrix_perm_idx = torch.cat((torch.zeros(1, dtype=torch.long),\n                                         label_perm_idx + torch.scalar_tensor(1, dtype=torch.long)))\n            sim_matrix = sim_matrix[matrix_perm_idx]\n            if self.perm_feat_dim:\n                sim_matrix = sim_matrix[:, matrix_perm_idx]\n\n        return q, sim_matrix, label\n\n    def __len__(self):\n        return len(self.qs)", ""]}
{"filename": "utils.py", "chunked_list": ["import os\nimport math\nfrom collections import defaultdict\nimport logging\nimport torch\n\n\nclass AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0.0\n        self.avg = 0.0\n        self.sum = 0.0\n        self.count = 0\n\n    def update(self, val, n=1):\n        if not math.isfinite(val):\n            val = 10000.0\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        num = self.avg\n        if abs(num) > 1e-4:\n            return f\"{num:.4f}\"\n        else:\n            return f\"{num:.4e}\"", "\n\nclass Metrics:\n    def __init__(self, delimiter=\" \"):\n        self.meters = defaultdict(AverageMeter)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, list) or isinstance(v, tuple):\n                v, n = v\n            else:\n                v, n = v, 1\n\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            if not math.isfinite(v):\n                v = 10000.0\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v, n)\n\n    def reset_meters(self):\n        for k in self.meters:\n            if isinstance(self.meters[k], AverageMeter):\n                self.meters[k].reset()\n\n    def __str__(self):\n        meters_str = [name + \": \" + str(meter) for name, meter in self.meters.items()]\n        return self.delimiter.join(meters_str)", "\n\ndef setup_logger(logger, args, filename='log'):\n    logger.setLevel(logging.INFO)\n    if logger.hasHandlers():\n        logger.handlers.clear()\n    log_formatter = logging.Formatter(fmt=\"[%(asctime)s][%(levelname)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n    console = logging.StreamHandler()\n    console.setFormatter(log_formatter)\n    file_handler = logging.FileHandler(os.path.join(args.output_dir, filename))\n    file_handler.setFormatter(log_formatter)\n    logger.addHandler(console)\n    logger.addHandler(file_handler)", ""]}
{"filename": "data/generate_MSMARCO_data.py", "chunked_list": ["import os\nimport pickle\nimport h5py\nimport argparse\nfrom tqdm import tqdm\nfrom pyserini.search import FaissSearcher\nfrom pyserini.search.faiss.__main__ import init_query_encoder\nfrom utils import BM25SimComputerCached, DenseSimComputer\n\n\"\"\"", "\n\"\"\"\n******************************************************************\nground_truth.pkl\n{\n    \"qid\":\n    {\n        \"question\": raw text of question\n        \"positive_ctxs\": [pid1, pid2, pid3, ...], string list, variable length\n        \"retrieved_ctxs\": [pid1, pid2, pid3, ...], string list, length of K", "        \"positive_ctxs\": [pid1, pid2, pid3, ...], string list, variable length\n        \"retrieved_ctxs\": [pid1, pid2, pid3, ...], string list, length of K\n    }\n}\n******************************************************************\nscore.hdf5\n{\n    \"qid\":\n    {\n        \"query_ctx_dense_score\": float array, size of [1, K]", "    {\n        \"query_ctx_dense_score\": float array, size of [1, K]\n        \"ctx_ctx_dense_score\": float array, size of [K, K]\n        \"query_ctx_bm25_score\": float array, size of [1, K]\n        \"ctx_ctx_bm25_score\": float array, size of [K, K]\n    }\n}\n******************************************************************\n\"\"\"\n", "\"\"\"\n\n\ndef generate_positive_list(input_path, output_path):\n    # qid, question and positive_ctxs\n    for split in ('train', 'dev'):\n        data = dict()\n        qid2q = dict()\n        file_name = 'queries.train.tsv' if split == 'train' else 'queries.dev.small.tsv'\n        with open(os.path.join(input_path, file_name), 'r') as f:\n            for line in f:\n                qid, q = line.rstrip().split('\\t')\n                qid2q[qid] = q\n\n        file_name = 'qrels.train.tsv' if split == 'train' else 'qrels.dev.small.tsv'\n        with open(os.path.join(input_path, file_name), 'r') as f:\n            for line in tqdm(f, desc='generating relevance list'):\n                qid, _, pid, rel = line.rstrip().split('\\t')\n                assert rel == '1'\n                question = qid2q[qid]\n                if qid not in data:\n                    data[qid] = {'question': question, 'positive_ctxs': []}\n                data[qid]['positive_ctxs'].append(pid)\n\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'wb') as f:\n            pickle.dump(data, f)", "\n\ndef generate_retrieval_list(index, query_encoder, output_path, results_path_prefix=None):\n    # retrieved_ctxs\n    query_encoder = init_query_encoder(query_encoder, None, None, None, None, 'cpu', None)\n    searcher = FaissSearcher.from_prebuilt_index(index, query_encoder)\n\n    for split in ('train', 'dev'):\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n            data = pickle.load(f)\n\n        if results_path_prefix is not None:\n            with open(results_path_prefix + split, 'r') as f:\n                for i, line in enumerate(tqdm(f, desc='generating retrieval list')):\n                    qid, pid, rank, score = line.rstrip().split('\\t')\n                    assert qid in data\n                    if 'retrieved_ctxs' in data[qid]:\n                        data[qid]['retrieved_ctxs'].append(pid)\n                    else:\n                        data[qid]['retrieved_ctxs'] = [pid]\n        else:\n            batch_size, threads = 36, 30\n            qids = list(data.keys())\n            qs = [data[qid]['question'] for qid in qids]\n            batch_qids, batch_qs = list(), list()\n            for index, (qid, q) in enumerate(tqdm(zip(qids, qs), desc='generating retrieval list')):\n                batch_qids.append(qid)\n                batch_qs.append(q)\n                if (index + 1) % batch_size == 0 or index == len(qids) - 1:\n                    batch_hits = searcher.batch_search(batch_qs, batch_qids, k=100, threads=threads)\n                    assert len(batch_qids) == len(batch_hits)\n\n                    for qid_in_batch, hits in batch_hits.items():\n                        data[qid_in_batch]['retrieved_ctxs'] = [hit.docid for hit in hits]\n\n                    batch_qids.clear()\n                    batch_qs.clear()\n                else:\n                    continue\n\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'wb') as f:\n            pickle.dump(data, f)", "\n\ndef generate_dense_score(index, query_encoder, output_path):\n    # query_ctx_dense_score and ctx_ctx_dense_score\n    query_encoder = init_query_encoder(query_encoder, None, None, None, None, 'cpu', None)\n    simcomputer = DenseSimComputer.from_prebuilt_index(index, query_encoder)\n\n    for split in ('train', 'dev'):\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n            data = pickle.load(f)\n\n        with h5py.File(os.path.join(output_path, split + '_scores.hdf5'), 'a') as f:\n            for qid in tqdm(data, desc='generating dense scores'):\n                question = data[qid]['question']\n                retrieved_pids = data[qid]['retrieved_ctxs']\n                query_ctx_dense_score, ctx_ctx_dense_score = simcomputer.compute(question, retrieved_pids)\n\n                if qid not in f.keys():\n                    f.create_group(qid)\n                f[qid]['query_ctx_dense_score'] = query_ctx_dense_score.astype(float)\n                f[qid]['ctx_ctx_dense_score'] = ctx_ctx_dense_score.astype(float)", "\n\ndef generate_bm25_score(lucene_index_path, output_path):\n    # query_ctx_bm25score and ctx_ctx_bm25score\n    simcomputer = BM25SimComputerCached(lucene_index_path)\n\n    for split in ('train', 'dev'):\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n            data = pickle.load(f)\n\n        with h5py.File(os.path.join(output_path, split + '_scores.hdf5'), 'a') as f:\n            for qid in tqdm(data, desc='generating BM25 scores'):\n                question = data[qid]['question']\n                retrieved_pids = data[qid]['retrieved_ctxs']\n                query_ctx_bm25_score, ctx_ctx_bm25_score = simcomputer.compute(question, retrieved_pids)\n\n                if qid not in f.keys():\n                    f.create_group(qid)\n                f[qid]['query_ctx_bm25_score'] = query_ctx_bm25_score.astype(float)\n                f[qid]['ctx_ctx_bm25_score'] = ctx_ctx_bm25_score.astype(float)", "\n\nretriever_dense_index_map = {\n    'ANCE': 'msmarco-passage-ance-bf',\n    'DistilBERT-KD': 'msmarco-passage-distilbert-dot-margin_mse-T2-bf',\n    'TAS-B': 'msmarco-passage-distilbert-dot-tas_b-b256-bf',\n    'TCT-ColBERT-v1': 'msmarco-passage-tct_colbert-bf',\n    'TCT-ColBERT-v2': 'msmarco-passage-tct_colbert-v2-hnp-bf',\n    'RocketQA-retriever': 'msmarco-passage-ance-bf',\n    'RocketQAv2-retriever': 'msmarco-passage-ance-bf',", "    'RocketQA-retriever': 'msmarco-passage-ance-bf',\n    'RocketQAv2-retriever': 'msmarco-passage-ance-bf',\n    'RocketQA-reranker': 'msmarco-passage-ance-bf',\n    'RocketQAv2-reranker': 'msmarco-passage-ance-bf',\n}\n\nretriever_query_encoder_map = {\n    'ANCE': 'castorini/ance-msmarco-passage',\n    'DistilBERT-KD': 'sebastian-hofstaetter/distilbert-dot-margin_mse-T2-msmarco',\n    'TAS-B': 'sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco',", "    'DistilBERT-KD': 'sebastian-hofstaetter/distilbert-dot-margin_mse-T2-msmarco',\n    'TAS-B': 'sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco',\n    'TCT-ColBERT-v1': 'castorini/tct_colbert-msmarco',\n    'TCT-ColBERT-v2': 'castorini/tct_colbert-v2-hnp-msmarco',\n    'RocketQA-retriever': 'castorini/ance-msmarco-passage',\n    'RocketQAv2-retriever': 'castorini/ance-msmarco-passage',\n    'RocketQA-reranker': 'castorini/ance-msmarco-passage',\n    'RocketQAv2-reranker': 'castorini/ance-msmarco-passage',\n}\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Generate MSMARCO dataset.')\n    parser.add_argument('--msmarco_data_path', type=str, required=False, default='raw/MSMARCO',\n                        help='Original MSMARCO retriever dataset path.')\n    parser.add_argument('--retriever', type=str, required=True,\n                        choices=[\"ANCE\", \"DistilBERT-KD\", \"TAS-B\", \"TCT-ColBERT-v1\", \"TCT-ColBERT-v2\",\n                                 \"RocketQA-retriever\", \"RocketQAv2-retriever\", \"RocketQA-reranker\", \"RocketQAv2-reranker\"],\n                        default=None, help='Which retriever method to use.')\n    parser.add_argument('--lucene_index_path', type=str, required=False,\n                        default='pyserini/indexes/lucene-index-msmarco-passage',\n                        help='Lucene index path.')\n    parser.add_argument('--output_path', type=str, required=False, default=None,\n                        help='Generated dataset path.')\n    args = parser.parse_args()\n\n    msmarco_data_path = args.msmarco_data_path\n    dense_index_name = retriever_dense_index_map[args.retriever]\n    query_encoder = retriever_query_encoder_map[args.retriever]\n    lucene_index_path = args.lucene_index_path\n    if args.output_path is None:\n        output_path = 'MSMARCO_' + args.retriever\n    else:\n        output_path = args.output_path\n\n    if 'RocketQA' in args.retriever:\n        results_path_prefix = f'RocketQA_baselines/MSMARCO/{args.retriever.split(\"-\")[0]}/res.top100.'\n        if 'reranker' in args.retriever:\n            results_path = results_path_prefix.replace('res.top100', 'rerank.res.top100')\n    else:\n        results_path_prefix = None\n\n    os.makedirs(output_path, exist_ok=True)\n    generate_positive_list(msmarco_data_path, output_path)\n    generate_retrieval_list(dense_index_name, query_encoder, output_path, results_path_prefix)  # 5.5h\n    generate_dense_score(dense_index_name, query_encoder, output_path)  # 2h\n    generate_bm25_score(lucene_index_path, output_path)  # 15h", "}\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Generate MSMARCO dataset.')\n    parser.add_argument('--msmarco_data_path', type=str, required=False, default='raw/MSMARCO',\n                        help='Original MSMARCO retriever dataset path.')\n    parser.add_argument('--retriever', type=str, required=True,\n                        choices=[\"ANCE\", \"DistilBERT-KD\", \"TAS-B\", \"TCT-ColBERT-v1\", \"TCT-ColBERT-v2\",\n                                 \"RocketQA-retriever\", \"RocketQAv2-retriever\", \"RocketQA-reranker\", \"RocketQAv2-reranker\"],\n                        default=None, help='Which retriever method to use.')\n    parser.add_argument('--lucene_index_path', type=str, required=False,\n                        default='pyserini/indexes/lucene-index-msmarco-passage',\n                        help='Lucene index path.')\n    parser.add_argument('--output_path', type=str, required=False, default=None,\n                        help='Generated dataset path.')\n    args = parser.parse_args()\n\n    msmarco_data_path = args.msmarco_data_path\n    dense_index_name = retriever_dense_index_map[args.retriever]\n    query_encoder = retriever_query_encoder_map[args.retriever]\n    lucene_index_path = args.lucene_index_path\n    if args.output_path is None:\n        output_path = 'MSMARCO_' + args.retriever\n    else:\n        output_path = args.output_path\n\n    if 'RocketQA' in args.retriever:\n        results_path_prefix = f'RocketQA_baselines/MSMARCO/{args.retriever.split(\"-\")[0]}/res.top100.'\n        if 'reranker' in args.retriever:\n            results_path = results_path_prefix.replace('res.top100', 'rerank.res.top100')\n    else:\n        results_path_prefix = None\n\n    os.makedirs(output_path, exist_ok=True)\n    generate_positive_list(msmarco_data_path, output_path)\n    generate_retrieval_list(dense_index_name, query_encoder, output_path, results_path_prefix)  # 5.5h\n    generate_dense_score(dense_index_name, query_encoder, output_path)  # 2h\n    generate_bm25_score(lucene_index_path, output_path)  # 15h", ""]}
{"filename": "data/generate_TRECDL_data.py", "chunked_list": ["import os\nimport pickle\nimport h5py\nimport argparse\nfrom tqdm import tqdm\nfrom pyserini.search import FaissSearcher\nfrom pyserini.search.faiss.__main__ import init_query_encoder\nfrom utils import BM25SimComputerCached, DenseSimComputer\n\n\"\"\"", "\n\"\"\"\n******************************************************************\nground_truth.pkl\n{\n    \"qid\":\n    {\n        \"question\": raw text of question\n        \"positive_ctxs\": [pid1, pid2, pid3, ...], string list, variable length\n        \"retrieved_ctxs\": [pid1, pid2, pid3, ...], string list, length of K", "        \"positive_ctxs\": [pid1, pid2, pid3, ...], string list, variable length\n        \"retrieved_ctxs\": [pid1, pid2, pid3, ...], string list, length of K\n    }\n}\n******************************************************************\nscore.hdf5\n{\n    \"qid\":\n    {\n        \"query_ctx_dense_score\": float array, size of [1, K]", "    {\n        \"query_ctx_dense_score\": float array, size of [1, K]\n        \"ctx_ctx_dense_score\": float array, size of [K, K]\n        \"query_ctx_bm25_score\": float array, size of [1, K]\n        \"ctx_ctx_bm25_score\": float array, size of [K, K]\n    }\n}\n******************************************************************\n\"\"\"\n", "\"\"\"\n\n\ndef generate_positive_list(input_path, split_year, output_path):\n    # qid, question and positive_ctxs\n    msmarco_path = output_path.replace(f\"TRECDL{split_year}\", \"MSMARCO\")\n\n    for split in ('test',):\n        data = dict()\n        qid2q = dict()\n        file_name = f'msmarco-test{split_year}-queries.tsv'\n        with open(os.path.join(input_path, file_name), 'r') as f:\n            for line in f:\n                qid, q = line.rstrip().split('\\t')\n                qid2q[qid] = q\n\n        file_name = f'{split_year}qrels-pass.txt'\n        with open(os.path.join(input_path, file_name), 'r') as f:\n            for line in tqdm(f, desc='generating relevance list'):\n                qid, _, pid, rel = line.rstrip().split(' ')\n                if rel == '0' or rel == '1':\n                    continue\n                question = qid2q[qid]\n                if qid not in data:\n                    data[qid] = {'question': question, 'positive_ctxs': []}\n                data[qid]['positive_ctxs'].append(pid)\n\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'wb') as f:\n            pickle.dump(data, f)", "\n\ndef generate_retrieval_list(index, query_encoder, output_path, results_path=None):\n    # retrieved_ctxs\n    query_encoder = init_query_encoder(query_encoder, None, None, None, None, 'cpu', None)\n    searcher = FaissSearcher.from_prebuilt_index(index, query_encoder)\n\n    for split in ('test',):\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n            data = pickle.load(f)\n\n        if results_path is not None:\n            with open(results_path, 'r') as f:\n                for i, line in enumerate(tqdm(f, desc='generating retrieval list')):\n                    qid, pid, rank, score = line.rstrip().split('\\t')\n                    assert qid in data\n                    if 'retrieved_ctxs' in data[qid]:\n                        data[qid]['retrieved_ctxs'].append(pid)\n                    else:\n                        data[qid]['retrieved_ctxs'] = [pid]\n        else:\n            batch_size, threads = 36, 30\n            qids = list(data.keys())\n            qs = [data[qid]['question'] for qid in qids]\n            batch_qids, batch_qs = list(), list()\n            for index, (qid, q) in enumerate(tqdm(zip(qids, qs), desc='generating retrieval list')):\n                batch_qids.append(qid)\n                batch_qs.append(q)\n                if (index + 1) % batch_size == 0 or index == len(qids) - 1:\n                    batch_hits = searcher.batch_search(batch_qs, batch_qids, k=100, threads=threads)\n                    assert len(batch_qids) == len(batch_hits)\n\n                    for qid_in_batch, hits in batch_hits.items():\n                        data[qid_in_batch]['retrieved_ctxs'] = [hit.docid for hit in hits]\n\n                    batch_qids.clear()\n                    batch_qs.clear()\n                else:\n                    continue\n\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'wb') as f:\n            pickle.dump(data, f)", "\n\ndef generate_dense_score(index, query_encoder, output_path):\n    # query_ctx_dense_score and ctx_ctx_dense_score\n    query_encoder = init_query_encoder(query_encoder, None, None, None, None, 'cpu', None)\n    simcomputer = DenseSimComputer.from_prebuilt_index(index, query_encoder)\n\n    for split in ('test',):\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n            data = pickle.load(f)\n\n        with h5py.File(os.path.join(output_path, split + '_scores.hdf5'), 'a') as f:\n            for qid in tqdm(data, desc='generating dense scores'):\n                question = data[qid]['question']\n                retrieved_pids = data[qid]['retrieved_ctxs']\n                query_ctx_dense_score, ctx_ctx_dense_score = simcomputer.compute(question, retrieved_pids)\n\n                if qid not in f.keys():\n                    f.create_group(qid)\n                f[qid]['query_ctx_dense_score'] = query_ctx_dense_score.astype(float)\n                f[qid]['ctx_ctx_dense_score'] = ctx_ctx_dense_score.astype(float)", "\n\ndef generate_bm25_score(lucene_index_path, output_path):\n    # query_ctx_bm25score and ctx_ctx_bm25score\n    simcomputer = BM25SimComputerCached(lucene_index_path)\n\n    for split in ('test',):\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n            data = pickle.load(f)\n\n        with h5py.File(os.path.join(output_path, split + '_scores.hdf5'), 'a') as f:\n            for qid in tqdm(data, desc='generating BM25 scores'):\n                question = data[qid]['question']\n                retrieved_pids = data[qid]['retrieved_ctxs']\n                query_ctx_bm25_score, ctx_ctx_bm25_score = simcomputer.compute(question, retrieved_pids)\n\n                if qid not in f.keys():\n                    f.create_group(qid)\n                f[qid]['query_ctx_bm25_score'] = query_ctx_bm25_score.astype(float)\n                f[qid]['ctx_ctx_bm25_score'] = ctx_ctx_bm25_score.astype(float)", "\n\nretriever_dense_index_map = {\n    'ANCE': 'msmarco-passage-ance-bf',\n    'DistilBERT-KD': 'msmarco-passage-distilbert-dot-margin_mse-T2-bf',\n    'TAS-B': 'msmarco-passage-distilbert-dot-tas_b-b256-bf',\n    'TCT-ColBERT-v1': 'msmarco-passage-tct_colbert-bf',\n    'TCT-ColBERT-v2': 'msmarco-passage-tct_colbert-v2-hnp-bf',\n    'RocketQA-retriever': 'msmarco-passage-ance-bf',\n    'RocketQAv2-retriever': 'msmarco-passage-ance-bf',", "    'RocketQA-retriever': 'msmarco-passage-ance-bf',\n    'RocketQAv2-retriever': 'msmarco-passage-ance-bf',\n    'RocketQA-reranker': 'msmarco-passage-ance-bf',\n    'RocketQAv2-reranker': 'msmarco-passage-ance-bf',\n}\n\nretriever_query_encoder_map = {\n    'ANCE': 'castorini/ance-msmarco-passage',\n    'DistilBERT-KD': 'sebastian-hofstaetter/distilbert-dot-margin_mse-T2-msmarco',\n    'TAS-B': 'sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco',", "    'DistilBERT-KD': 'sebastian-hofstaetter/distilbert-dot-margin_mse-T2-msmarco',\n    'TAS-B': 'sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco',\n    'TCT-ColBERT-v1': 'castorini/tct_colbert-msmarco',\n    'TCT-ColBERT-v2': 'castorini/tct_colbert-v2-hnp-msmarco',\n    'RocketQA-retriever': 'castorini/ance-msmarco-passage',\n    'RocketQAv2-retriever': 'castorini/ance-msmarco-passage',\n    'RocketQA-reranker': 'castorini/ance-msmarco-passage',\n    'RocketQAv2-reranker': 'castorini/ance-msmarco-passage',\n}\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Generate TREC-DL dataset.')\n    parser.add_argument('--trecdl_data_path', type=str, required=False, default='raw/TRECDL',\n                        help='Original TREC-DL retriever dataset path.')\n    parser.add_argument('--split', type=str, required=True, choices=['2019', '2020'], default=None,\n                        help=\"Which split of TREC-DL to use.\")\n    parser.add_argument('--retriever', type=str, required=True,\n                        choices=[\"ANCE\", \"DistilBERT-KD\", \"TAS-B\", \"TCT-ColBERT-v1\", \"TCT-ColBERT-v2\",\n                                 \"RocketQA-retriever\", \"RocketQAv2-retriever\", \"RocketQA-reranker\", \"RocketQAv2-reranker\"],\n                        default=None, help='Which retriever method to use.')\n    parser.add_argument('--lucene_index_path', type=str, required=False,\n                        default='pyserini/indexes/lucene-index-msmarco-passage',\n                        help='Lucene index path.')\n    parser.add_argument('--output_path', type=str, required=False, default=None,\n                        help='Generated dataset path.')\n    args = parser.parse_args()\n\n    trecdl_data_path = args.trecdl_data_path\n    dense_index_name = retriever_dense_index_map[args.retriever]\n    query_encoder = retriever_query_encoder_map[args.retriever]\n    lucene_index_path = args.lucene_index_path\n    if args.output_path is None:\n        output_path = f'TRECDL{args.split}_{args.retriever}'\n    else:\n        output_path = args.output_path\n\n    if 'RocketQA' in args.retriever:\n        results_path = f'RocketQA_baselines/TRECDL/{args.retriever.split(\"-\")[0]}/res.top100.{args.split}'\n        if 'reranker' in args.retriever:\n            results_path = results_path.replace('res.top100', 'rerank.res.top100')\n    else:\n        results_path = None\n\n    os.makedirs(output_path, exist_ok=True)\n    generate_positive_list(trecdl_data_path, args.split, output_path)\n    generate_retrieval_list(dense_index_name, query_encoder, output_path, results_path)  # 5.5h\n    generate_dense_score(dense_index_name, query_encoder, output_path)  # 2h\n    generate_bm25_score(lucene_index_path, output_path)  # 15h", "}\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Generate TREC-DL dataset.')\n    parser.add_argument('--trecdl_data_path', type=str, required=False, default='raw/TRECDL',\n                        help='Original TREC-DL retriever dataset path.')\n    parser.add_argument('--split', type=str, required=True, choices=['2019', '2020'], default=None,\n                        help=\"Which split of TREC-DL to use.\")\n    parser.add_argument('--retriever', type=str, required=True,\n                        choices=[\"ANCE\", \"DistilBERT-KD\", \"TAS-B\", \"TCT-ColBERT-v1\", \"TCT-ColBERT-v2\",\n                                 \"RocketQA-retriever\", \"RocketQAv2-retriever\", \"RocketQA-reranker\", \"RocketQAv2-reranker\"],\n                        default=None, help='Which retriever method to use.')\n    parser.add_argument('--lucene_index_path', type=str, required=False,\n                        default='pyserini/indexes/lucene-index-msmarco-passage',\n                        help='Lucene index path.')\n    parser.add_argument('--output_path', type=str, required=False, default=None,\n                        help='Generated dataset path.')\n    args = parser.parse_args()\n\n    trecdl_data_path = args.trecdl_data_path\n    dense_index_name = retriever_dense_index_map[args.retriever]\n    query_encoder = retriever_query_encoder_map[args.retriever]\n    lucene_index_path = args.lucene_index_path\n    if args.output_path is None:\n        output_path = f'TRECDL{args.split}_{args.retriever}'\n    else:\n        output_path = args.output_path\n\n    if 'RocketQA' in args.retriever:\n        results_path = f'RocketQA_baselines/TRECDL/{args.retriever.split(\"-\")[0]}/res.top100.{args.split}'\n        if 'reranker' in args.retriever:\n            results_path = results_path.replace('res.top100', 'rerank.res.top100')\n    else:\n        results_path = None\n\n    os.makedirs(output_path, exist_ok=True)\n    generate_positive_list(trecdl_data_path, args.split, output_path)\n    generate_retrieval_list(dense_index_name, query_encoder, output_path, results_path)  # 5.5h\n    generate_dense_score(dense_index_name, query_encoder, output_path)  # 2h\n    generate_bm25_score(lucene_index_path, output_path)  # 15h", ""]}
{"filename": "data/utils.py", "chunked_list": ["import numpy as np\nfrom typing import List, Optional, Union\nfrom sklearn.preprocessing import normalize\nfrom scipy.sparse import csr_matrix, vstack\n\nimport transformers\ntransformers.logging.set_verbosity_error()\n\nfrom pyserini.index.lucene import IndexReader\nfrom pyserini.vectorizer import BM25Vectorizer", "from pyserini.index.lucene import IndexReader\nfrom pyserini.vectorizer import BM25Vectorizer\nfrom pyserini.search import FaissSearcher, QueryEncoder\n\n\nclass BM25SimComputerCached(BM25Vectorizer):\n    \"\"\"Faster than BM25SimComputer when involve the same doc multiple times\"\"\"\n    def __init__(self, lucene_index_path: str, min_df: int = 1, verbose: bool = False):\n        super().__init__(lucene_index_path, min_df, verbose)\n        self.index_reader = IndexReader(lucene_index_path)\n        self.tf_vector_cache = dict()\n        self.bm25_vector_cache = dict()\n\n    def compute(self, q: str, docids: List[str], norm: Optional[str] = None):\n        query_doc_matrix = np.array([[self.index_reader.compute_query_document_score(docid, q) for docid in docids]])\n        doc_tf_vectors = self.get_tf_vectors(docids, norm)\n        doc_bm25_vectors = self.get_bm25_vectors(docids, norm)\n        doc_doc_matrix = doc_tf_vectors.dot(doc_bm25_vectors.T).toarray()\n        return query_doc_matrix, doc_doc_matrix\n\n    def get_tf_vectors(self, docids: List[str], norm: Optional[str] = None):\n        \"\"\"Get the tf vectors given a list of docids\n\n        Parameters\n        ----------\n        norm : str\n            Normalize the sparse matrix\n        docids : List[str]\n            The piece of text to analyze.\n\n        Returns\n        -------\n        csr_matrix\n            Sparse matrix representation of tf vectors\n        \"\"\"\n        num_docs = len(docids)\n\n        vectors = []\n        for doc_id in docids:\n            if doc_id in self.tf_vector_cache:\n                vector = self.tf_vector_cache[doc_id]\n            else:\n                matrix_col, matrix_data = [], []\n                # Term Frequency\n                tf = self.index_reader.get_document_vector(doc_id)\n                if tf is None:\n                    vector = csr_matrix((1, self.vocabulary_size))\n                else:\n                    # Filter out in-eligible terms\n                    tf = {t: tf[t] for t in tf if t in self.term_to_index}\n                    # Convert from dict to sparse matrix\n                    for term in tf:\n                        tfidf = tf[term]\n                        matrix_col.append(self.term_to_index[term])\n                        matrix_data.append(tfidf)\n                    matrix_row = [0] * len(matrix_col)\n                    vector = csr_matrix((matrix_data, (matrix_row, matrix_col)), shape=(1, self.vocabulary_size))\n                self.tf_vector_cache[doc_id] = vector\n\n            vectors.append(vector)\n\n        assert num_docs == len(vectors)\n        vectors = vstack(vectors)\n        if norm:\n            return normalize(vectors, norm=norm)\n        return vectors\n\n    def get_bm25_vectors(self, docids: List[str], norm: Optional[str] = None):\n        \"\"\"Get the BM25 vectors given a list of docids\n\n        Parameters\n        ----------\n        norm : str\n            Normalize the sparse matrix\n        docids : List[str]\n            The piece of text to analyze.\n\n        Returns\n        -------\n        csr_matrix\n            Sparse matrix representation of BM25 vectors\n        \"\"\"\n        num_docs = len(docids)\n\n        vectors = []\n        for doc_id in docids:\n            if doc_id in self.bm25_vector_cache:\n                vector = self.bm25_vector_cache[doc_id]\n            else:\n                matrix_col, matrix_data = [], []\n                # Term Frequency\n                tf = self.index_reader.get_document_vector(doc_id)\n                if tf is None:\n                    vector = csr_matrix((1, self.vocabulary_size))\n                else:\n                    # Filter out in-eligible terms\n                    tf = {t: tf[t] for t in tf if t in self.term_to_index}\n                    # Convert from dict to sparse matrix\n                    for term in tf:\n                        bm25_weight = self.index_reader.compute_bm25_term_weight(doc_id, term, analyzer=None)\n                        matrix_col.append(self.term_to_index[term])\n                        matrix_data.append(bm25_weight)\n                    matrix_row = [0] * len(matrix_col)\n                    vector = csr_matrix((matrix_data, (matrix_row, matrix_col)), shape=(1, self.vocabulary_size))\n                self.bm25_vector_cache[doc_id] = vector\n\n            vectors.append(vector)\n\n        assert num_docs == len(vectors)\n        vectors = vstack(vectors)\n        if norm:\n            return normalize(vectors, norm=norm)\n        return vectors", "\n\nclass BM25SimComputer(BM25Vectorizer):\n    def __init__(self, lucene_index_path: str, min_df: int = 1, verbose: bool = False):\n        super().__init__(lucene_index_path, min_df, verbose)\n        self.index_reader = IndexReader(lucene_index_path)\n\n    def compute(self, q: str, docids: List[str], norm: Optional[str] = None):\n        query_doc_matrix = np.array([[self.index_reader.compute_query_document_score(docid, q) for docid in docids]])\n        doc_tf_vectors = self.get_tf_vectors(docids, norm)\n        doc_bm25_vectors = self.get_bm25_vectors(docids, norm)\n        doc_doc_matrix = doc_tf_vectors.dot(doc_bm25_vectors.T).toarray()\n        return query_doc_matrix, doc_doc_matrix\n\n    def get_tf_vectors(self, docids: List[str], norm: Optional[str] = None):\n        \"\"\"Get the tf vectors given a list of docids\n\n        Parameters\n        ----------\n        norm : str\n            Normalize the sparse matrix\n        docids : List[str]\n            The piece of text to analyze.\n\n        Returns\n        -------\n        csr_matrix\n            Sparse matrix representation of tf vectors\n        \"\"\"\n        matrix_row, matrix_col, matrix_data = [], [], []\n        num_docs = len(docids)\n\n        for index, doc_id in enumerate(docids):\n            # Term Frequency\n            tf = self.index_reader.get_document_vector(doc_id)\n            if tf is None:\n                continue\n\n            # Filter out in-eligible terms\n            tf = {t: tf[t] for t in tf if t in self.term_to_index}\n\n            # Convert from dict to sparse matrix\n            for term in tf:\n                tfidf = tf[term]\n                matrix_row.append(index)\n                matrix_col.append(self.term_to_index[term])\n                matrix_data.append(tfidf)\n\n        vectors = csr_matrix((matrix_data, (matrix_row, matrix_col)), shape=(num_docs, self.vocabulary_size))\n\n        if norm:\n            return normalize(vectors, norm=norm)\n        return vectors\n\n    def get_bm25_vectors(self, docids: List[str], norm: Optional[str] = None):\n        \"\"\"Get the BM25 vectors given a list of docids\n\n        Parameters\n        ----------\n        norm : str\n            Normalize the sparse matrix\n        docids : List[str]\n            The piece of text to analyze.\n\n        Returns\n        -------\n        csr_matrix\n            Sparse matrix representation of BM25 vectors\n        \"\"\"\n        matrix_row, matrix_col, matrix_data = [], [], []\n        num_docs = len(docids)\n\n        for index, doc_id in enumerate(docids):\n\n            # Term Frequency\n            tf = self.index_reader.get_document_vector(doc_id)\n            if tf is None:\n                continue\n\n            # Filter out in-eligible terms\n            tf = {t: tf[t] for t in tf if t in self.term_to_index}\n\n            # Convert from dict to sparse matrix\n            for term in tf:\n                bm25_weight = self.index_reader.compute_bm25_term_weight(doc_id, term, analyzer=None)\n                matrix_row.append(index)\n                matrix_col.append(self.term_to_index[term])\n                matrix_data.append(bm25_weight)\n\n        vectors = csr_matrix((matrix_data, (matrix_row, matrix_col)), shape=(num_docs, self.vocabulary_size))\n\n        if norm:\n            return normalize(vectors, norm=norm)\n        return vectors", "\n\nclass DenseSimComputer(FaissSearcher):\n    def __init__(self, index_dir: str, query_encoder: Union[QueryEncoder, str],\n                 prebuilt_index_name: Optional[str] = None):\n        super().__init__(index_dir, query_encoder, prebuilt_index_name)\n        self.docid2idx = {docid: i for i, docid in enumerate(self.docids)}\n\n    def compute(self, q: str, docids: List[str], norm: Optional[str] = None):\n        query_vectors = self.get_query_vectors(q, norm)\n        doc_vectors = self.get_doc_vectors(docids, norm)\n        query_doc_matrix = query_vectors @ doc_vectors.T\n        doc_doc_matrix = doc_vectors @ doc_vectors.T\n        return query_doc_matrix, doc_doc_matrix\n\n    def get_query_vectors(self, q: str, norm: Optional[str] = None):\n        \"\"\"Get the dense vectors given a query text\n\n        Parameters\n        ----------\n        norm : str\n            Normalize the matrix\n        q : str\n            The raw query text to analyze.\n\n        Returns\n        -------\n        vectors\n            Numpy matrix representation of dense vectors\n        \"\"\"\n        vectors = self.query_encoder.encode(q)\n        vectors = vectors.reshape((1, -1))\n\n        if norm:\n            return normalize(vectors, norm=norm)\n        return vectors\n\n    def get_doc_vectors(self, docids: List[str], norm: Optional[str] = None):\n        \"\"\"Get the dense vectors given a list of docids\n\n        Parameters\n        ----------\n        norm : str\n            Normalize the matrix\n        docids : List[str]\n            The piece of text to analyze.\n\n        Returns\n        -------\n        vectors\n            Numpy matrix representation of dense vectors\n        \"\"\"\n        indices = [self.docid2idx[docid] for docid in docids]\n        vectors = [self.index.reconstruct(i) for i in indices]\n        vectors = np.vstack(vectors)\n\n        if norm:\n            return normalize(vectors, norm=norm)\n        return vectors", ""]}
{"filename": "data/download_DPR_data.py", "chunked_list": ["#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\n Command line tool to download various preprocessed data sources & checkpoints for DPR\n\"\"\"", " Command line tool to download various preprocessed data sources & checkpoints for DPR\n\"\"\"\n\nimport argparse\nimport gzip\nimport logging\nimport os\nimport pathlib\nimport wget\n", "import wget\n\nfrom typing import Tuple\n\nlogger = logging.getLogger(__name__)\n\n# TODO: move to hydra config group\n\nNQ_LICENSE_FILES = [\n    \"https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\",", "NQ_LICENSE_FILES = [\n    \"https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\",\n    \"https://dl.fbaipublicfiles.com/dpr/nq_license/README\",\n]\n\nRESOURCES_MAP = {\n    \"data.wikipedia_split.psgs_w100\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\",\n        \"original_ext\": \".tsv\",\n        \"compressed\": True,", "        \"original_ext\": \".tsv\",\n        \"compressed\": True,\n        \"desc\": \"Entire wikipedia passages set obtain by splitting all pages into 100-word segments (no overlap)\",\n    },\n    \"data.retriever.nq-dev\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"NQ dev subset with passages pools for the Retriever train time validation\",\n        \"license_files\": NQ_LICENSE_FILES,", "        \"desc\": \"NQ dev subset with passages pools for the Retriever train time validation\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.retriever.nq-train\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-train.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"NQ train subset with passages pools for the Retriever training\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },", "        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.retriever.nq-adv-hn-train\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-adv-hn-train.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"NQ train subset with hard negative passages mined using the baseline DPR NQ encoders & wikipedia index\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.retriever.trivia-dev\": {", "    },\n    \"data.retriever.trivia-dev\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-trivia-dev.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"TriviaQA dev subset with passages pools for the Retriever train time validation\",\n    },\n    \"data.retriever.trivia-train\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-trivia-train.json.gz\",\n        \"original_ext\": \".json\",", "        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-trivia-train.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"TriviaQA train subset with passages pools for the Retriever training\",\n    },\n    \"data.retriever.squad1-train\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-squad1-train.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"SQUAD 1.1 train subset with passages pools for the Retriever training\",", "        \"compressed\": True,\n        \"desc\": \"SQUAD 1.1 train subset with passages pools for the Retriever training\",\n    },\n    \"data.retriever.squad1-dev\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-squad1-dev.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"SQUAD 1.1 dev subset with passages pools for the Retriever train time validation\",\n    },\n    \"data.retriever.webq-train\": {", "    },\n    \"data.retriever.webq-train\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-webquestions-train.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"WebQuestions dev subset with passages pools for the Retriever train time validation\",\n    },\n    \"data.retriever.webq-dev\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-webquestions-dev.json.gz\",\n        \"original_ext\": \".json\",", "        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-webquestions-dev.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"WebQuestions dev subset with passages pools for the Retriever train time validation\",\n    },\n    \"data.retriever.curatedtrec-train\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-curatedtrec-train.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"CuratedTrec dev subset with passages pools for the Retriever train time validation\",", "        \"compressed\": True,\n        \"desc\": \"CuratedTrec dev subset with passages pools for the Retriever train time validation\",\n    },\n    \"data.retriever.curatedtrec-dev\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-curatedtrec-dev.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"CuratedTrec dev subset with passages pools for the Retriever train time validation\",\n    },\n    \"data.retriever.qas.nq-dev\": {", "    },\n    \"data.retriever.qas.nq-dev\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-dev.qa.csv\",\n        \"original_ext\": \".csv\",\n        \"compressed\": False,\n        \"desc\": \"NQ dev subset for Retriever validation and IR results generation\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.retriever.qas.nq-test\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-test.qa.csv\",", "    \"data.retriever.qas.nq-test\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-test.qa.csv\",\n        \"original_ext\": \".csv\",\n        \"compressed\": False,\n        \"desc\": \"NQ test subset for Retriever validation and IR results generation\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.retriever.qas.nq-train\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-train.qa.csv\",\n        \"original_ext\": \".csv\",", "        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/nq-train.qa.csv\",\n        \"original_ext\": \".csv\",\n        \"compressed\": False,\n        \"desc\": \"NQ train subset for Retriever validation and IR results generation\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    #\n    \"data.retriever.qas.trivia-dev\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/trivia-dev.qa.csv.gz\",\n        \"original_ext\": \".csv\",", "        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/trivia-dev.qa.csv.gz\",\n        \"original_ext\": \".csv\",\n        \"compressed\": True,\n        \"desc\": \"Trivia dev subset for Retriever validation and IR results generation\",\n    },\n    \"data.retriever.qas.trivia-test\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/trivia-test.qa.csv.gz\",\n        \"original_ext\": \".csv\",\n        \"compressed\": True,\n        \"desc\": \"Trivia test subset for Retriever validation and IR results generation\",", "        \"compressed\": True,\n        \"desc\": \"Trivia test subset for Retriever validation and IR results generation\",\n    },\n    \"data.retriever.qas.trivia-train\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/trivia-train.qa.csv.gz\",\n        \"original_ext\": \".csv\",\n        \"compressed\": True,\n        \"desc\": \"Trivia train subset for Retriever validation and IR results generation\",\n    },\n    \"data.retriever.qas.squad1-test\": {", "    },\n    \"data.retriever.qas.squad1-test\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/squad1-test.qa.csv\",\n        \"original_ext\": \".csv\",\n        \"compressed\": False,\n        \"desc\": \"Trivia test subset for Retriever validation and IR results generation\",\n    },\n    \"data.retriever.qas.webq-test\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/webquestions-test.qa.csv\",\n        \"original_ext\": \".csv\",", "        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/webquestions-test.qa.csv\",\n        \"original_ext\": \".csv\",\n        \"compressed\": False,\n        \"desc\": \"WebQuestions test subset for Retriever validation and IR results generation\",\n    },\n    \"data.retriever.qas.curatedtrec-test\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever/curatedtrec-test.qa.csv\",\n        \"original_ext\": \".csv\",\n        \"compressed\": False,\n        \"desc\": \"CuratedTrec test subset for Retriever validation and IR results generation\",", "        \"compressed\": False,\n        \"desc\": \"CuratedTrec test subset for Retriever validation and IR results generation\",\n    },\n    \"data.gold_passages_info.nq_train\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/nq_gold_info/nq-train_gold_info.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"Original NQ (our train subset) gold positive passages and alternative question tokenization\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },", "        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.gold_passages_info.nq_dev\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/nq_gold_info/nq-dev_gold_info.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"Original NQ (our dev subset) gold positive passages and alternative question tokenization\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.gold_passages_info.nq_test\": {", "    },\n    \"data.gold_passages_info.nq_test\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/nq_gold_info/nq-test_gold_info.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"Original NQ (our test, original dev subset) gold positive passages and alternative question \"\n        \"tokenization\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"pretrained.fairseq.roberta-base.dict\": {", "    },\n    \"pretrained.fairseq.roberta-base.dict\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/pretrained/fairseq/roberta/dict.txt\",\n        \"original_ext\": \".txt\",\n        \"compressed\": False,\n        \"desc\": \"Dictionary for pretrained fairseq roberta model\",\n    },\n    \"pretrained.fairseq.roberta-base.model\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/pretrained/fairseq/roberta/model.pt\",\n        \"original_ext\": \".pt\",", "        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/pretrained/fairseq/roberta/model.pt\",\n        \"original_ext\": \".pt\",\n        \"compressed\": False,\n        \"desc\": \"Weights for pretrained fairseq roberta base model\",\n    },\n    \"pretrained.pytext.bert-base.model\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/pretrained/pytext/bert/bert-base-uncased.pt\",\n        \"original_ext\": \".pt\",\n        \"compressed\": False,\n        \"desc\": \"Weights for pretrained pytext bert base model\",", "        \"compressed\": False,\n        \"desc\": \"Weights for pretrained pytext bert base model\",\n    },\n    \"data.retriever_results.nq.single.wikipedia_passages\": {\n        \"s3_url\": [\n            \"https://dl.fbaipublicfiles.com/dpr/data/wiki_encoded/single/nq/wiki_passages_{}\".format(i)\n            for i in range(50)\n        ],\n        \"original_ext\": \".pkl\",\n        \"compressed\": False,", "        \"original_ext\": \".pkl\",\n        \"compressed\": False,\n        \"desc\": \"Encoded wikipedia files using a biencoder checkpoint(\"\n        \"checkpoint.retriever.single.nq.bert-base-encoder) trained on NQ dataset \",\n    },\n    \"data.retriever_results.nq.single-adv-hn.wikipedia_passages\": {\n        \"s3_url\": [\n            \"https://dl.fbaipublicfiles.com/dpr/data/wiki_encoded/single-adv-hn/nq/wiki_passages_{}\".format(i)\n            for i in range(50)\n        ],", "            for i in range(50)\n        ],\n        \"original_ext\": \".pkl\",\n        \"compressed\": False,\n        \"desc\": \"Encoded wikipedia files using a biencoder checkpoint(\"\n        \"checkpoint.retriever.single-adv-hn.nq.bert-base-encoder) trained on NQ dataset + adversarial hard negatives\",\n    },\n    \"data.retriever_results.nq.single.test\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever_results/single/nq-test.json.gz\",\n        \"original_ext\": \".json\",", "        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever_results/single/nq-test.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"Retrieval results of NQ test dataset for the encoder trained on NQ\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.retriever_results.nq.single.dev\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever_results/single/nq-dev.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,", "        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"Retrieval results of NQ dev dataset for the encoder trained on NQ\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.retriever_results.nq.single.train\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever_results/single/nq-train.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"Retrieval results of NQ train dataset for the encoder trained on NQ\",", "        \"compressed\": True,\n        \"desc\": \"Retrieval results of NQ train dataset for the encoder trained on NQ\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.retriever_results.nq.single-adv-hn.test\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/retriever_results/single-adv-hn/nq-test.json.gz\",\n        \"original_ext\": \".json\",\n        \"compressed\": True,\n        \"desc\": \"Retrieval results of NQ test dataset for the encoder trained on NQ + adversarial hard negatives\",\n        \"license_files\": NQ_LICENSE_FILES,", "        \"desc\": \"Retrieval results of NQ test dataset for the encoder trained on NQ + adversarial hard negatives\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"checkpoint.retriever.single.nq.bert-base-encoder\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/retriever/single/nq/hf_bert_base.cp\",\n        \"original_ext\": \".cp\",\n        \"compressed\": False,\n        \"desc\": \"Biencoder weights trained on NQ data and HF bert-base-uncased model\",\n    },\n    \"checkpoint.retriever.multiset.bert-base-encoder\": {", "    },\n    \"checkpoint.retriever.multiset.bert-base-encoder\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/retriver/multiset/hf_bert_base.cp\",\n        \"original_ext\": \".cp\",\n        \"compressed\": False,\n        \"desc\": \"Biencoder weights trained on multi set data and HF bert-base-uncased model\",\n    },\n    \"checkpoint.retriever.single-adv-hn.nq.bert-base-encoder\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/retriver/single-adv-hn/nq/hf_bert_base.cp\",\n        \"original_ext\": \".cp\",", "        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/retriver/single-adv-hn/nq/hf_bert_base.cp\",\n        \"original_ext\": \".cp\",\n        \"compressed\": False,\n        \"desc\": \"Biencoder weights trained on the original DPR NQ data combined with adversarial hard negatives (See data.retriever.nq-adv-hn-train resource). \"\n        \"The model is HF bert-base-uncased\",\n    },\n    \"data.reader.nq.single.train\": {\n        \"s3_url\": [\"https://dl.fbaipublicfiles.com/dpr/data/reader/nq/single/train.{}.pkl\".format(i) for i in range(8)],\n        \"original_ext\": \".pkl\",\n        \"compressed\": False,", "        \"original_ext\": \".pkl\",\n        \"compressed\": False,\n        \"desc\": \"Reader model NQ train dataset input data preprocessed from retriever results (also trained on NQ)\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.reader.nq.single.dev\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/reader/nq/single/dev.0.pkl\",\n        \"original_ext\": \".pkl\",\n        \"compressed\": False,\n        \"desc\": \"Reader model NQ dev dataset input data preprocessed from retriever results (also trained on NQ)\",", "        \"compressed\": False,\n        \"desc\": \"Reader model NQ dev dataset input data preprocessed from retriever results (also trained on NQ)\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.reader.nq.single.test\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/reader/nq/single/test.0.pkl\",\n        \"original_ext\": \".pkl\",\n        \"compressed\": False,\n        \"desc\": \"Reader model NQ test dataset input data preprocessed from retriever results (also trained on NQ)\",\n        \"license_files\": NQ_LICENSE_FILES,", "        \"desc\": \"Reader model NQ test dataset input data preprocessed from retriever results (also trained on NQ)\",\n        \"license_files\": NQ_LICENSE_FILES,\n    },\n    \"data.reader.trivia.multi-hybrid.train\": {\n        \"s3_url\": [\n            \"https://dl.fbaipublicfiles.com/dpr/data/reader/trivia/multi-hybrid/train.{}.pkl\".format(i)\n            for i in range(8)\n        ],\n        \"original_ext\": \".pkl\",\n        \"compressed\": False,", "        \"original_ext\": \".pkl\",\n        \"compressed\": False,\n        \"desc\": \"Reader model Trivia train dataset input data preprocessed from hybrid retriever results \"\n        \"(where dense part is trained on multiset)\",\n    },\n    \"data.reader.trivia.multi-hybrid.dev\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/reader/trivia/multi-hybrid/dev.0.pkl\",\n        \"original_ext\": \".pkl\",\n        \"compressed\": False,\n        \"desc\": \"Reader model Trivia dev dataset input data preprocessed from hybrid retriever results \"", "        \"compressed\": False,\n        \"desc\": \"Reader model Trivia dev dataset input data preprocessed from hybrid retriever results \"\n        \"(where dense part is trained on multiset)\",\n    },\n    \"data.reader.trivia.multi-hybrid.test\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/data/reader/trivia/multi-hybrid/test.0.pkl\",\n        \"original_ext\": \".pkl\",\n        \"compressed\": False,\n        \"desc\": \"Reader model Trivia test dataset input data preprocessed from hybrid retriever results \"\n        \"(where dense part is trained on multiset)\",", "        \"desc\": \"Reader model Trivia test dataset input data preprocessed from hybrid retriever results \"\n        \"(where dense part is trained on multiset)\",\n    },\n    \"checkpoint.reader.nq-single.hf-bert-base\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/reader/nq-single/hf_bert_base.cp\",\n        \"original_ext\": \".cp\",\n        \"compressed\": False,\n        \"desc\": \"Reader weights trained on NQ-single retriever results and HF bert-base-uncased model\",\n    },\n    \"checkpoint.reader.nq-trivia-hybrid.hf-bert-base\": {", "    },\n    \"checkpoint.reader.nq-trivia-hybrid.hf-bert-base\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/reader/nq-trivia-hybrid/hf_bert_base.cp\",\n        \"original_ext\": \".cp\",\n        \"compressed\": False,\n        \"desc\": \"Reader weights trained on Trivia multi hybrid retriever results and HF bert-base-uncased model\",\n    },\n    # extra checkpoints for EfficientQA competition\n    \"checkpoint.reader.nq-single-subset.hf-bert-base\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/reader/nq-single-seen_only/hf_bert_base.cp\",", "    \"checkpoint.reader.nq-single-subset.hf-bert-base\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/reader/nq-single-seen_only/hf_bert_base.cp\",\n        \"original_ext\": \".cp\",\n        \"compressed\": False,\n        \"desc\": \"Reader weights trained on NQ-single retriever results and HF bert-base-uncased model, when only Wikipedia pages seen during training are considered\",\n    },\n    \"checkpoint.reader.nq-tfidf.hf-bert-base\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/reader/nq-drqa/hf_bert_base.cp\",\n        \"original_ext\": \".cp\",\n        \"compressed\": False,", "        \"original_ext\": \".cp\",\n        \"compressed\": False,\n        \"desc\": \"Reader weights trained on TFIDF results and HF bert-base-uncased model\",\n    },\n    \"checkpoint.reader.nq-tfidf-subset.hf-bert-base\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/reader/nq-drqa-seen_only/hf_bert_base.cp\",\n        \"original_ext\": \".cp\",\n        \"compressed\": False,\n        \"desc\": \"Reader weights trained on TFIDF results and HF bert-base-uncased model, when only Wikipedia pages seen during training are considered\",\n    },", "        \"desc\": \"Reader weights trained on TFIDF results and HF bert-base-uncased model, when only Wikipedia pages seen during training are considered\",\n    },\n    # retrieval indexes\n    \"indexes.single.nq.full.index\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/single/nq/full.index.dpr\",\n        \"original_ext\": \".dpr\",\n        \"compressed\": False,\n        \"desc\": \"DPR index on NQ-single retriever\",\n    },\n    \"indexes.single.nq.full.index_meta\": {", "    },\n    \"indexes.single.nq.full.index_meta\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/single/nq/full.index_meta.dpr\",\n        \"original_ext\": \".dpr\",\n        \"compressed\": False,\n        \"desc\": \"DPR index on NQ-single retriever (metadata)\",\n    },\n    \"indexes.single.nq.subset.index\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/single/nq/seen_only.index.dpr\",\n        \"original_ext\": \".dpr\",", "        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/single/nq/seen_only.index.dpr\",\n        \"original_ext\": \".dpr\",\n        \"compressed\": False,\n        \"desc\": \"DPR index on NQ-single retriever when only Wikipedia pages seen during training are considered\",\n    },\n    \"indexes.single.nq.subset.index_meta\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/single/nq/seen_only.index_meta.dpr\",\n        \"original_ext\": \".dpr\",\n        \"compressed\": False,\n        \"desc\": \"DPR index on NQ-single retriever when only Wikipedia pages seen during training are considered (metadata)\",", "        \"compressed\": False,\n        \"desc\": \"DPR index on NQ-single retriever when only Wikipedia pages seen during training are considered (metadata)\",\n    },\n    \"indexes.tfidf.nq.full\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/drqa/nq/full-tfidf.npz\",\n        \"original_ext\": \".npz\",\n        \"compressed\": False,\n        \"desc\": \"TFIDF index\",\n    },\n    \"indexes.tfidf.nq.subset\": {", "    },\n    \"indexes.tfidf.nq.subset\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/checkpoint/indexes/drqa/nq/seen_only-tfidf.npz\",\n        \"original_ext\": \".npz\",\n        \"compressed\": False,\n        \"desc\": \"TFIDF index when only Wikipedia pages seen during training are considered\",\n    },\n    # Universal retriever project data\n    \"data.wikipedia_split.psgs_w100\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\",", "    \"data.wikipedia_split.psgs_w100\": {\n        \"s3_url\": \"https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz\",\n        \"original_ext\": \".tsv\",\n        \"compressed\": True,\n        \"desc\": \"Entire wikipedia passages set obtain by splitting all pages into 100-word segments (no overlap)\",\n    },\n}\n\n\ndef unpack(gzip_file: str, out_file: str):\n    logger.info(\"Uncompressing %s\", gzip_file)\n    input = gzip.GzipFile(gzip_file, \"rb\")\n    s = input.read()\n    input.close()\n    output = open(out_file, \"wb\")\n    output.write(s)\n    output.close()\n    logger.info(\" Saved to %s\", out_file)", "\ndef unpack(gzip_file: str, out_file: str):\n    logger.info(\"Uncompressing %s\", gzip_file)\n    input = gzip.GzipFile(gzip_file, \"rb\")\n    s = input.read()\n    input.close()\n    output = open(out_file, \"wb\")\n    output.write(s)\n    output.close()\n    logger.info(\" Saved to %s\", out_file)", "\n\ndef download_resource(\n    s3_url: str, original_ext: str, compressed: bool, resource_key: str, out_dir: str\n) -> Tuple[str, str]:\n    logger.info(\"Requested resource from %s\", s3_url)\n    path_names = resource_key.split(\".\")\n\n    if out_dir:\n        root_dir = out_dir\n    else:\n        # since hydra overrides the location for the 'current dir' for every run and we don't want to duplicate\n        # resources multiple times, remove the current folder's volatile part\n        root_dir = os.path.abspath(\"./\")\n        if \"/outputs/\" in root_dir:\n            root_dir = root_dir[: root_dir.index(\"/outputs/\")]\n\n    logger.info(\"Download root_dir %s\", root_dir)\n\n    save_root = os.path.join(root_dir, \"downloads\", *path_names[:-1])  # last segment is for file name\n\n    pathlib.Path(save_root).mkdir(parents=True, exist_ok=True)\n\n    local_file_uncompressed = os.path.abspath(os.path.join(save_root, path_names[-1] + original_ext))\n    logger.info(\"File to be downloaded as %s\", local_file_uncompressed)\n\n    if os.path.exists(local_file_uncompressed):\n        logger.info(\"File already exist %s\", local_file_uncompressed)\n        return save_root, local_file_uncompressed\n\n    local_file = os.path.abspath(os.path.join(save_root, path_names[-1] + (\".tmp\" if compressed else original_ext)))\n\n    wget.download(s3_url, out=local_file)\n\n    logger.info(\"Downloaded to %s\", local_file)\n\n    if compressed:\n        uncompressed_file = os.path.join(save_root, path_names[-1] + original_ext)\n        unpack(local_file, uncompressed_file)\n        os.remove(local_file)\n        local_file = uncompressed_file\n    return save_root, local_file", "\n\ndef download_file(s3_url: str, out_dir: str, file_name: str):\n    logger.info(\"Loading from %s\", s3_url)\n    local_file = os.path.join(out_dir, file_name)\n\n    if os.path.exists(local_file):\n        logger.info(\"File already exist %s\", local_file)\n        return\n\n    wget.download(s3_url, out=local_file)\n    logger.info(\"Downloaded to %s\", local_file)", "\n\ndef download(resource_key: str, out_dir: str = None):\n    if resource_key not in RESOURCES_MAP:\n        # match by prefix\n        resources = [k for k in RESOURCES_MAP.keys() if k.startswith(resource_key)]\n        logger.info(\"matched by prefix resources: %s\", resources)\n        if resources:\n            for key in resources:\n                download(key, out_dir)\n        else:\n            logger.info(\"no resources found for specified key\")\n        return []\n    download_info = RESOURCES_MAP[resource_key]\n\n    s3_url = download_info[\"s3_url\"]\n\n    save_root_dir = None\n    data_files = []\n    if isinstance(s3_url, list):\n        for i, url in enumerate(s3_url):\n            save_root_dir, local_file = download_resource(\n                url,\n                download_info[\"original_ext\"],\n                download_info[\"compressed\"],\n                \"{}_{}\".format(resource_key, i),\n                out_dir,\n            )\n            data_files.append(local_file)\n    else:\n        save_root_dir, local_file = download_resource(\n            s3_url,\n            download_info[\"original_ext\"],\n            download_info[\"compressed\"],\n            resource_key,\n            out_dir,\n        )\n        data_files.append(local_file)\n\n    license_files = download_info.get(\"license_files\", None)\n    if license_files:\n        download_file(license_files[0], save_root_dir, \"LICENSE\")\n        download_file(license_files[1], save_root_dir, \"README\")\n    return data_files", "\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--output_dir\",\n        default=\"./\",\n        type=str,\n        help=\"The output directory to download file\",\n    )\n    parser.add_argument(\n        \"--resource\",\n        type=str,\n        help=\"Resource name. See RESOURCES_MAP for all possible values\",\n    )\n    args = parser.parse_args()\n    if args.resource:\n        download(args.resource, args.output_dir)\n    else:\n        logger.warning(\"Please specify resource value. Possible options are:\")\n        for k, v in RESOURCES_MAP.items():\n            logger.warning(\"Resource key=%s  :  %s\", k, v[\"desc\"])", "\n\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "data/generate_NQ_data.py", "chunked_list": ["import os\nimport json\nimport pickle\nimport h5py\nimport argparse\nfrom tqdm import tqdm\nfrom pyserini.search import FaissSearcher\nfrom pyserini.search.faiss.__main__ import init_query_encoder\nfrom pyserini.eval.evaluate_dpr_retrieval import has_answers, SimpleTokenizer\nfrom utils import BM25SimComputerCached, DenseSimComputer", "from pyserini.eval.evaluate_dpr_retrieval import has_answers, SimpleTokenizer\nfrom utils import BM25SimComputerCached, DenseSimComputer\n\n\"\"\"\n******************************************************************\nground_truth.pkl\n{\n    \"qid\":\n    {\n        \"question\": raw text of question", "    {\n        \"question\": raw text of question\n        \"answers\": [answer1, answer2, ...], string list, variable length\n        \"positive_ctxs\": [pid1, pid2, pid3, ...], string list, variable length\n        \"retrieved_ctxs\": [pid1, pid2, pid3, ...], string list, length of K\n        \"has_answer\": [True, False, True, ...], bool list, length of K\n    }\n}\n******************************************************************\nscore.hdf5", "******************************************************************\nscore.hdf5\n{\n    \"qid\":\n    {\n        \"query_ctx_dense_score\": float array, size of [1, K]\n        \"ctx_ctx_dense_score\": float array, size of [K, K]\n        \"query_ctx_bm25_score\": float array, size of [1, K]\n        \"ctx_ctx_bm25_score\": float array, size of [K, K]\n    }", "        \"ctx_ctx_bm25_score\": float array, size of [K, K]\n    }\n}\n******************************************************************\n\"\"\"\n\n\ndef generate_positive_list(input_path, output_path):\n    # qid, question, answers and positive_ctxs\n    for split in ('train', 'dev', 'test'):\n        data = dict()\n        q2id = dict()\n        with open(os.path.join(input_path, 'qas', 'nq-' + split + '.csv'), 'r') as f:\n            for i, line in enumerate(tqdm(f, desc='reading questions')):\n                question, answer_str = line.rstrip().split('\\t')\n                data[str(i)] = {'question': question, 'answers': eval(answer_str), 'positive_ctxs': []}\n                q2id[question] = str(i)\n\n        if split != 'test':\n            with open(os.path.join(input_path, 'nq-' + split + '.json'), 'r') as f:\n                anno = json.load(f)\n                for sample in tqdm(anno, desc='generating relevance list'):\n                    qid = q2id[sample['question']]\n                    data[qid]['positive_ctxs'] = [x['passage_id'] for x in sample['positive_ctxs']]\n\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'wb') as f:\n            pickle.dump(data, f)", "\n\ndef generate_retrieval_list(index, query_encoder, output_path, results_path_prefix=None):\n    # retrieved_ctxs and has_answer\n    query_encoder = init_query_encoder(query_encoder, None, None, None, None, 'cpu', None)\n    searcher = FaissSearcher.from_prebuilt_index(index, query_encoder)\n    tokenizer = SimpleTokenizer()\n\n    for split in ('train', 'dev', 'test'):\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n            data = pickle.load(f)\n\n        if results_path_prefix is not None:\n            with open(results_path_prefix + split, 'r') as f:\n                for i, line in enumerate(tqdm(f, desc='generating retrieval list')):\n                    qid, pid, rank, score = line.rstrip().split('\\t')\n                    assert qid in data\n                    if 'retrieved_ctxs' in data[qid]:\n                        data[qid]['retrieved_ctxs'].append(pid)\n                    else:\n                        data[qid]['retrieved_ctxs'] = [pid]\n        else:\n            batch_size, threads = 36, 30\n            qids = list(data.keys())\n            qs = [data[qid]['question'] for qid in qids]\n            batch_qids, batch_qs = list(), list()\n            for index, (qid, q) in enumerate(tqdm(zip(qids, qs), desc='generating retrieval list')):\n                batch_qids.append(qid)\n                batch_qs.append(q)\n                if (index + 1) % batch_size == 0 or index == len(qids) - 1:\n                    batch_hits = searcher.batch_search(batch_qs, batch_qids, k=100, threads=threads)\n                    assert len(batch_qids) == len(batch_hits)\n\n                    for qid_in_batch, hits in batch_hits.items():\n                        data[qid_in_batch]['retrieved_ctxs'] = [hit.docid for hit in hits]\n\n                    batch_qids.clear()\n                    batch_qs.clear()\n                else:\n                    continue\n\n        for qid in tqdm(data, desc='computing has_answer'):\n            answers = data[qid]['answers']\n            data[qid]['has_answer'] = []\n            for docid in data[qid]['retrieved_ctxs']:\n                text = json.loads(searcher.doc(docid).raw())['contents'].split('\\n')[1]\n                has_answer = has_answers(text, answers, tokenizer)\n                data[qid]['has_answer'].append(has_answer)\n\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'wb') as f:\n            pickle.dump(data, f)", "\n\ndef generate_dense_score(index, query_encoder, output_path):\n    # query_ctx_dense_score and ctx_ctx_dense_score\n    query_encoder = init_query_encoder(query_encoder, None, None, None, None, 'cpu', None)\n    simcomputer = DenseSimComputer.from_prebuilt_index(index, query_encoder)\n\n    for split in ('train', 'dev', 'test'):\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n            data = pickle.load(f)\n\n        with h5py.File(os.path.join(output_path, split + '_scores.hdf5'), 'a') as f:\n            for qid in tqdm(data, desc='generating dense scores'):\n                question = data[qid]['question']\n                retrieved_pids = data[qid]['retrieved_ctxs']\n                query_ctx_dense_score, ctx_ctx_dense_score = simcomputer.compute(question, retrieved_pids)\n\n                if qid not in f.keys():\n                    f.create_group(qid)\n                f[qid]['query_ctx_dense_score'] = query_ctx_dense_score.astype(float)\n                f[qid]['ctx_ctx_dense_score'] = ctx_ctx_dense_score.astype(float)", "\n\ndef generate_bm25_score(lucene_index_path, output_path):\n    # query_ctx_bm25score and ctx_ctx_bm25score\n    simcomputer = BM25SimComputerCached(lucene_index_path)\n\n    for split in ('train', 'dev', 'test'):\n        with open(os.path.join(output_path, split + '_ground_truth.pkl'), 'rb') as f:\n            data = pickle.load(f)\n\n        with h5py.File(os.path.join(output_path, split + '_scores.hdf5'), 'a') as f:\n            for qid in tqdm(data, desc='generating BM25 scores'):\n                question = data[qid]['question']\n                retrieved_pids = data[qid]['retrieved_ctxs']\n                query_ctx_bm25_score, ctx_ctx_bm25_score = simcomputer.compute(question, retrieved_pids)\n\n                if qid not in f.keys():\n                    f.create_group(qid)\n                f[qid]['query_ctx_bm25_score'] = query_ctx_bm25_score.astype(float)\n                f[qid]['ctx_ctx_bm25_score'] = ctx_ctx_bm25_score.astype(float)", "\n\nretriever_dense_index_map = {\n    'DPR-Multi': 'wikipedia-dpr-multi-bf',\n    'DPR-Single': 'wikipedia-dpr-single-nq-bf',\n    'ANCE': 'wikipedia-ance-multi-bf',\n    'FiD-KD': 'wikipedia-dpr-dkrr-nq',\n    'RocketQA-retriever': 'wikipedia-dpr-dkrr-nq',\n    'RocketQAv2-retriever': 'wikipedia-dpr-dkrr-nq',\n    'RocketQA-reranker': 'wikipedia-dpr-dkrr-nq',", "    'RocketQAv2-retriever': 'wikipedia-dpr-dkrr-nq',\n    'RocketQA-reranker': 'wikipedia-dpr-dkrr-nq',\n    'RocketQAv2-reranker': 'wikipedia-dpr-dkrr-nq',\n}\n\nretriever_query_encoder_map = {\n    'DPR-Multi': 'facebook/dpr-question_encoder-multiset-base',\n    'DPR-Single': 'facebook/dpr-question_encoder-single-nq-base',\n    'ANCE': 'castorini/ance-dpr-question-multi',\n    'FiD-KD': 'castorini/dkrr-dpr-nq-retriever',", "    'ANCE': 'castorini/ance-dpr-question-multi',\n    'FiD-KD': 'castorini/dkrr-dpr-nq-retriever',\n    'RocketQA-retriever': 'castorini/dkrr-dpr-nq-retriever',\n    'RocketQAv2-retriever': 'castorini/dkrr-dpr-nq-retriever',\n    'RocketQA-reranker': 'castorini/dkrr-dpr-nq-retriever',\n    'RocketQAv2-reranker': 'castorini/dkrr-dpr-nq-retriever',\n}\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Generate NQ dataset.')\n    parser.add_argument('--nq_data_path', type=str, required=False, default='raw/NQ/data/retriever',\n                        help='Original NQ retriever dataset path.')\n    parser.add_argument('--retriever', type=str, required=True,\n                        choices=[\"DPR-Multi\", \"DPR-Single\", \"ANCE\", \"FiD-KD\", \"RocketQA-retriever\", \n                                 \"RocketQAv2-retriever\", \"RocketQA-reranker\", \"RocketQAv2-reranker\"],\n                        default=None, help='Which retriever method to use.')\n    parser.add_argument('--lucene_index_path', type=str, required=False,\n                        default='pyserini/indexes/wikipedia_doc',\n                        help='Lucene index path.')\n    parser.add_argument('--output_path', type=str, required=False, default=None,\n                        help='Generated dataset path.')\n    args = parser.parse_args()\n\n    nq_data_path = args.nq_data_path\n    dense_index_name = retriever_dense_index_map[args.retriever]\n    query_encoder = retriever_query_encoder_map[args.retriever]\n    lucene_index_path = args.lucene_index_path\n    if args.output_path is None:\n        output_path = 'NQ_' + args.retriever\n    else:\n        output_path = args.output_path\n\n    if 'RocketQA' in args.retriever:\n        results_path_prefix = f'RocketQA_baselines/NQ/{args.retriever.split(\"-\")[0]}/res.top100.{args.split}'\n        if 'reranker' in args.retriever:\n            results_path_prefix = results_path_prefix.replace('res.top100', 'rerank.res.top100')\n    else:\n        results_path_prefix = None\n\n    os.makedirs(output_path, exist_ok=True)\n    generate_positive_list(nq_data_path, output_path)\n    generate_retrieval_list(dense_index_name, query_encoder, output_path, results_path_prefix)  # 4.5h\n    generate_dense_score(dense_index_name, query_encoder, output_path)  # 1h\n    generate_bm25_score(lucene_index_path, output_path)  # 4.5h", "if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Generate NQ dataset.')\n    parser.add_argument('--nq_data_path', type=str, required=False, default='raw/NQ/data/retriever',\n                        help='Original NQ retriever dataset path.')\n    parser.add_argument('--retriever', type=str, required=True,\n                        choices=[\"DPR-Multi\", \"DPR-Single\", \"ANCE\", \"FiD-KD\", \"RocketQA-retriever\", \n                                 \"RocketQAv2-retriever\", \"RocketQA-reranker\", \"RocketQAv2-reranker\"],\n                        default=None, help='Which retriever method to use.')\n    parser.add_argument('--lucene_index_path', type=str, required=False,\n                        default='pyserini/indexes/wikipedia_doc',\n                        help='Lucene index path.')\n    parser.add_argument('--output_path', type=str, required=False, default=None,\n                        help='Generated dataset path.')\n    args = parser.parse_args()\n\n    nq_data_path = args.nq_data_path\n    dense_index_name = retriever_dense_index_map[args.retriever]\n    query_encoder = retriever_query_encoder_map[args.retriever]\n    lucene_index_path = args.lucene_index_path\n    if args.output_path is None:\n        output_path = 'NQ_' + args.retriever\n    else:\n        output_path = args.output_path\n\n    if 'RocketQA' in args.retriever:\n        results_path_prefix = f'RocketQA_baselines/NQ/{args.retriever.split(\"-\")[0]}/res.top100.{args.split}'\n        if 'reranker' in args.retriever:\n            results_path_prefix = results_path_prefix.replace('res.top100', 'rerank.res.top100')\n    else:\n        results_path_prefix = None\n\n    os.makedirs(output_path, exist_ok=True)\n    generate_positive_list(nq_data_path, output_path)\n    generate_retrieval_list(dense_index_name, query_encoder, output_path, results_path_prefix)  # 4.5h\n    generate_dense_score(dense_index_name, query_encoder, output_path)  # 1h\n    generate_bm25_score(lucene_index_path, output_path)  # 4.5h", ""]}
{"filename": "data/convert_NQ_collection_to_jsonl.py", "chunked_list": ["import os\nimport json\nimport argparse\nfrom tqdm import tqdm\n\n\ndef convert_collection(args):\n    print('Converting collection...')\n    with open(args.collection_path, 'r') as f:\n        with open(os.path.join(args.output_folder, 'DPR_wikipedia_corpus.jsonl'), 'w') as fw:\n            next(f)\n            for line in tqdm(f):\n                pid, text, title = line.split('\\t')\n                contents = title + text[1:-1]\n                doc = json.dumps({\"id\": pid, \"contents\": contents})\n                fw.write(doc + '\\n')", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Convert NQ wikipedia tsv passage collection into jsonl files for Anserini.')\n    parser.add_argument('--collection-path', required=True, help='Path to NQ wikipedia tsv collection.')\n    parser.add_argument('--output-folder', required=True, help='Output folder.')\n\n    args = parser.parse_args()\n\n    if not os.path.exists(args.output_folder):\n        os.makedirs(args.output_folder)\n\n    convert_collection(args)\n    print('Done!')", ""]}
{"filename": "data/convert_MSMARCO_collection_to_jsonl.py", "chunked_list": ["#\n# Pyserini: Python interface to the Anserini IR toolkit built on Lucene\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport json\nimport os", "import json\nimport os\nimport argparse\n\n\ndef convert_collection(args):\n    print('Converting collection...')\n    file_index = 0\n    with open(args.collection_path, encoding='utf-8') as f:\n        for i, line in enumerate(f):\n            doc_id, doc_text = line.rstrip().split('\\t')\n\n            if i % args.max_docs_per_file == 0:\n                if i > 0:\n                    output_jsonl_file.close()\n                output_path = os.path.join(args.output_folder, 'docs{:02d}.json'.format(file_index))\n                output_jsonl_file = open(output_path, 'w', encoding='utf-8', newline='\\n')\n                file_index += 1\n            output_dict = {'id': doc_id, 'contents': doc_text}\n            output_jsonl_file.write(json.dumps(output_dict) + '\\n')\n\n            if i % 100000 == 0:\n                print(f'Converted {i:,} docs, writing into file {file_index}')\n\n    output_jsonl_file.close()", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Convert MSMARCO tsv passage collection into jsonl files for Anserini.')\n    parser.add_argument('--collection-path', required=True, help='Path to MS MARCO tsv collection.')\n    parser.add_argument('--output-folder', required=True, help='Output folder.')\n    parser.add_argument('--max-docs-per-file', default=1000000, type=int,\n                        help='Maximum number of documents in each jsonl file.')\n\n    args = parser.parse_args()\n\n    if not os.path.exists(args.output_folder):\n        os.makedirs(args.output_folder)\n\n    convert_collection(args)\n    print('Done!')", ""]}
{"filename": "data/RocketQA_baselines/TRECDL/generate_embeddings.py", "chunked_list": ["import os\nimport sys\nimport logging\nimport pickle\nimport rocketqa\n\n\nlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\nshard_id = 0 if sys.argv[1] == 'q' else int(sys.argv[1])", "\nshard_id = 0 if sys.argv[1] == 'q' else int(sys.argv[1])\nshard_num, device_id, batch_size = [int(x) for x in sys.argv[2:5]]\nmodel_name, output_folder = sys.argv[5:7]\n\nif sys.argv[1] == 'q':\n    model = rocketqa.load_model(model_name, use_cuda=True, device_id=device_id, batch_size=batch_size)\n\n    for split in ['2019', '2020']:\n        qrels_file = f'../../raw/TRECDL/{split}qrels-pass.txt'\n        queries_file = f'../../raw/TRECDL/msmarco-test{split}-queries.tsv'\n\n        qid2q = dict()\n        with open(queries_file, 'r') as f:\n            for line in f:\n                qid, q = line.strip().split('\\t')\n                qid2q[qid] = q\n        \n        results = []\n\n        with open(qrels_file, 'r') as f:\n            lines = [line.strip().split() for line in f.readlines()]\n            qids = sorted(list(set([x[0] for x in lines])))\n            texts = [qid2q[qid] for qid in qids]\n            q_num = len(qids)\n            batch_qids, batch_texts = [], []\n            for idx, (qid, text) in enumerate(zip(qids, texts)):                \n                batch_qids.append(qid)\n                batch_texts.append(text)\n                if len(batch_texts) == batch_size or idx == q_num - 1:\n                    batch_outs = list(model.encode_query(query=batch_texts))\n                    assert len(batch_outs) == len(batch_qids)\n                    results.extend([(qid, out) for qid, out in zip(batch_qids, batch_outs)])\n                    batch_qids, batch_texts = [], []\n                    logging.info(\"%d query encoded.\" % idx)\n\n        os.makedirs(output_folder, exist_ok=True)\n        with open(f'{output_folder}/q_embed_' + split + '.pkl', 'wb') as f:\n            pickle.dump(results, f)\n\nelse:\n    logging.info(\"Passage already encoded in MSMARCO.\")"]}
{"filename": "data/RocketQA_baselines/TRECDL/rerank.py", "chunked_list": ["import os\nimport sys\nfrom tqdm import tqdm\nimport rocketqa\n\n\nmodel_name, output_folder = sys.argv[1], sys.argv[2]\nmodel = rocketqa.load_model(model_name, use_cuda=True, device_id=0, batch_size=100)\n\npid2p = dict()\nwith open('../../raw/MSMARCO/collection.tsv', 'r') as f:\n    next(f)\n    for line in tqdm(f, desc='loading passages'):\n        pid, text = line.strip().split('\\t')\n        pid2p[pid] = text", "\npid2p = dict()\nwith open('../../raw/MSMARCO/collection.tsv', 'r') as f:\n    next(f)\n    for line in tqdm(f, desc='loading passages'):\n        pid, text = line.strip().split('\\t')\n        pid2p[pid] = text\n\nfor split in ['2019', '2020']:\n    queries_file = f'../../raw/TRECDL/msmarco-test{split}-queries.tsv'\n\n    qid2q = dict()\n    with open(queries_file, 'r') as f:\n        for line in f:\n            qid, q = line.strip().split('\\t')\n            qid2q[qid] = q\n\n    qid2pids = dict()\n    with open(f'{output_folder}/res.top100.' + split, 'r') as f:\n        for line in tqdm(f, desc='loading ' + split + ' retrieval results'):\n            qid, pid, rank, score = line.strip().split()\n            if qid in qid2pids:\n                qid2pids[qid].append(pid)\n            else:\n                qid2pids[qid] = [pid]\n    \n    with open(f'{output_folder}/rerank.res.top100.' + split, 'w') as f:\n        for qid, pids in tqdm(qid2pids.items(), desc='reranking'):\n            if len(pids) != 100:\n                pids = pids[:100]\n                # print(f'retrieval list for question {qid} truncated.')\n            \n            query = [qid2q[qid]] * 100\n            para = [pid2p[pid] for pid in pids]\n            \n            scores = list(model.matching(query=query, para=para))\n            \n            sorted_index = [idx for idx, x in sorted(list(enumerate(scores)), key=lambda x:x[1], reverse=True)]\n            sorted_pids = [pids[idx] for idx in sorted_index]\n            sorted_scores = [scores[idx] for idx in sorted_index]\n            for rank, (pid, score) in enumerate(zip(sorted_pids, sorted_scores)):\n                f.write('%s\\t%s\\t%d\\t%s\\n' % (qid, pid, rank, score))", "for split in ['2019', '2020']:\n    queries_file = f'../../raw/TRECDL/msmarco-test{split}-queries.tsv'\n\n    qid2q = dict()\n    with open(queries_file, 'r') as f:\n        for line in f:\n            qid, q = line.strip().split('\\t')\n            qid2q[qid] = q\n\n    qid2pids = dict()\n    with open(f'{output_folder}/res.top100.' + split, 'r') as f:\n        for line in tqdm(f, desc='loading ' + split + ' retrieval results'):\n            qid, pid, rank, score = line.strip().split()\n            if qid in qid2pids:\n                qid2pids[qid].append(pid)\n            else:\n                qid2pids[qid] = [pid]\n    \n    with open(f'{output_folder}/rerank.res.top100.' + split, 'w') as f:\n        for qid, pids in tqdm(qid2pids.items(), desc='reranking'):\n            if len(pids) != 100:\n                pids = pids[:100]\n                # print(f'retrieval list for question {qid} truncated.')\n            \n            query = [qid2q[qid]] * 100\n            para = [pid2p[pid] for pid in pids]\n            \n            scores = list(model.matching(query=query, para=para))\n            \n            sorted_index = [idx for idx, x in sorted(list(enumerate(scores)), key=lambda x:x[1], reverse=True)]\n            sorted_pids = [pids[idx] for idx in sorted_index]\n            sorted_scores = [scores[idx] for idx in sorted_index]\n            for rank, (pid, score) in enumerate(zip(sorted_pids, sorted_scores)):\n                f.write('%s\\t%s\\t%d\\t%s\\n' % (qid, pid, rank, score))", "\n"]}
{"filename": "data/RocketQA_baselines/TRECDL/retrieval.py", "chunked_list": ["import os\nimport sys\nimport glob\nimport pickle\nimport logging\nimport numpy as np\nimport faiss\n\n\nlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')", "\nlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\ntopk = 100\nmodel_name, output_folder = sys.argv[1], sys.argv[2]\nos.makedirs(output_folder, exist_ok=True)\nindex_path = f'../MSMARCO/{output_folder}/index'\nindex_meta_path = f'../MSMARCO/{output_folder}/index_meta'\nq_embed_pattern = f'{output_folder}/q_embed_*.pkl'\np_embed_pattern = f'../MSMARCO/{output_folder}/p_embed_*.pkl'", "q_embed_pattern = f'{output_folder}/q_embed_*.pkl'\np_embed_pattern = f'../MSMARCO/{output_folder}/p_embed_*.pkl'\noutput_path = f'{output_folder}/res.top100'\n\nif os.path.exists(index_path) and os.path.exists(index_meta_path):\n    logging.info('reading index ' + index_path)\n    with open(index_meta_path, 'rb') as f:\n        ids = pickle.load(f)\n    index = faiss.read_index(index_path)\n    assert len(ids) == index.ntotal\n\nelse:\n    ids = []\n    index = faiss.IndexFlatIP(768)\n    for p_embed_file in glob.glob(p_embed_pattern):\n        logging.info('indexing ' + p_embed_file)\n        with open(p_embed_file, \"rb\") as f:\n            data = pickle.load(f)\n        buffer = []\n        buffer_size = 50000\n        for db_id, doc_vector in data:\n            buffer.append((db_id, doc_vector))\n            if buffer_size == len(buffer):\n                ids.extend([x[0] for x in buffer])\n                index.add(np.concatenate([np.reshape(t[1], (1, -1)) for t in buffer], axis=0))\n                buffer = []\n        if len(buffer) > 0:\n            ids.extend([x[0] for x in buffer])\n            index.add(np.concatenate([np.reshape(t[1], (1, -1)) for t in buffer], axis=0))\n            buffer = []\n        logging.info('done. ntotal = ' + str(index.ntotal))\n    \n    faiss.write_index(index, index_path)\n    with open(index_meta_path, mode='wb') as f:\n        pickle.dump(ids, f)", "\nlogging.info(\"indexing done.\")\n\nbuffer_size = 32\nfaiss.omp_set_num_threads(30)\nfor split in ['2019', '2020']:\n    with open(q_embed_pattern.replace('*', split), 'rb') as f:\n        data = pickle.load(f)\n    \n    new_output_path = output_path + '.' + split\n    if os.path.exists(new_output_path):\n        os.remove(new_output_path)\n    for start_idx in range(0, len(data), buffer_size):\n        qids = [x[0] for x in data[start_idx:(start_idx + buffer_size)]]\n        qvecs = np.stack([x[1] for x in data[start_idx:(start_idx + buffer_size)]], axis=0)\n        scores, indexes = index.search(qvecs, topk)\n        retrieved_lists = [[ids[i] for i in query_top_idxs] for query_top_idxs in indexes]\n        assert len(scores) == len(retrieved_lists) == len(qids)\n\n        with open(new_output_path, 'a') as f:\n            for qid, pid_list, score_list in zip(qids, retrieved_lists, scores):\n                for rank, (pid, score) in enumerate(zip(pid_list, score_list), 1):\n                    f.write(\"%s\\t%s\\t%d\\t%f\\n\" % (qid, pid, rank, score))\n        \n        logging.info(f\"{start_idx} queries searched.\")", "\n"]}
{"filename": "data/RocketQA_baselines/NQ/generate_embeddings.py", "chunked_list": ["import os\nimport sys\nimport logging\nimport pickle\nimport rocketqa\n\n\nlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\nshard_id = 0 if sys.argv[1] == 'q' else int(sys.argv[1])", "\nshard_id = 0 if sys.argv[1] == 'q' else int(sys.argv[1])\nshard_num, device_id, batch_size = [int(x) for x in sys.argv[2:5]]\nmodel_name, output_folder = sys.argv[5:7]\n\nif sys.argv[1] == 'q':\n    model = rocketqa.load_model(model_name, use_cuda=True, device_id=device_id, batch_size=batch_size)\n\n    for split in ['train', 'dev', 'test']:\n        query_file = '../../raw/NQ/data/retriever/qas/nq-' + split + '.csv'\n        results = []\n\n        with open(query_file, 'r') as f:\n            batch_qids, batch_texts = [], []\n            lines = f.readlines()\n            line_num = len(lines)\n            for idx, line in enumerate(lines):\n                query, answers = line.strip().split('\\t')\n                \n                batch_qids.append(str(idx))\n                batch_texts.append(query)\n                if len(batch_texts) == batch_size or idx == line_num - 1:\n                    logging.info(\"encoding query %s to %s ...\" % (batch_qids[0], batch_qids[-1]))\n                    batch_outs = list(model.encode_query(query=batch_texts))\n                    assert len(batch_outs) == len(batch_qids)\n                    results.extend([(qid, out) for qid, out in zip(batch_qids, batch_outs)])\n                    batch_qids, batch_texts = [], []\n\n        os.makedirs(output_folder, exist_ok=True)\n        with open(f'{output_folder}/q_embed_' + split + '.pkl', 'wb') as f:\n            pickle.dump(results, f)\n\nelse:\n    logging.info((shard_id, shard_num, device_id, batch_size))\n    assert shard_id < shard_num\n\n    model = rocketqa.load_model(model_name, use_cuda=True, device_id=device_id, batch_size=batch_size)\n\n    para_num = 21015324\n    shard_size = para_num // shard_num\n    start_idx = shard_id * shard_size\n    end_idx = para_num + 1 if shard_id == shard_num - 1 else start_idx + shard_size\n\n    corpus_file = '../../raw/NQ/data/wikipedia_split/psgs_w100.tsv'\n\n    results = []\n\n    with open(corpus_file, 'r') as f:\n        next(f)\n        batch_pids, batch_texts, batch_titles = [], [], []\n        for idx, line in enumerate(f):\n            if idx >= start_idx and idx < end_idx:\n                pid, text, title = line.strip().split('\\t')\n                text = text[1:-1]\n                \n                batch_pids.append(pid)\n                batch_texts.append(text)\n                batch_titles.append(title)\n                if len(batch_texts) == batch_size or idx == end_idx - 1:\n                    logging.info(\"encoding passage %s to %s ...\" % (batch_pids[0], batch_pids[-1]))\n                    batch_outs = list(model.encode_para(para=batch_texts, title=batch_titles))\n                    assert len(batch_outs) == len(batch_pids)\n                    results.extend([(pid, out) for pid, out in zip(batch_pids, batch_outs)])\n                    batch_pids, batch_texts, batch_titles = [], [], []\n\n    os.makedirs(output_folder, exist_ok=True)\n    with open(f'{output_folder}/p_embed_' + str(shard_id) + '.pkl', 'wb') as f:\n        pickle.dump(results, f)"]}
{"filename": "data/RocketQA_baselines/NQ/rerank.py", "chunked_list": ["import os\nimport sys\nfrom tqdm import tqdm\nimport rocketqa\n\n\nif sys.argv[1] == 'v1_nq_ce':\n    model_name = 'v1_nq_ce'\nelif sys.argv[1] == 'v2_nq_ce':\n    model_name = 'v2_nq_ce/config.json'\nelse:\n    raise NotImplementedError", "output_folder = sys.argv[2]\nmodel = rocketqa.load_model(model_name, use_cuda=True, device_id=0, batch_size=100)\n\npid2p = dict()\nwith open('../../raw/NQ/data/wikipedia_split/psgs_w100.tsv', 'r') as f:\n    for line in tqdm(f, desc='loading passages'):\n        pid, text, title = line.strip().split('\\t')\n        pid2p[pid] = text[1:-1]\n\nfor split in ['train', 'dev', 'test']:\n    qid2q = dict()\n    with open(os.path.join('../../raw/NQ/data/retriever/qas', 'nq-' + split + '.csv'), 'r') as f:\n        for qid, line in enumerate(tqdm(f, desc='loading ' + split + ' queries')):\n            query, answers = line.strip().split('\\t')\n            qid2q[str(qid)] = query\n\n    qid2pids = dict()\n    with open(f'{output_folder}/res.top100.' + split, 'r') as f:\n        for line in tqdm(f, desc='loading ' + split + ' retrieval results'):\n            qid, pid, rank, score = line.strip().split()\n            if qid in qid2pids:\n                qid2pids[qid].append(pid)\n            else:\n                qid2pids[qid] = [pid]\n    \n    with open(f'{output_folder}/rerank.res.top100.' + split, 'w') as f:\n        for qid, pids in tqdm(qid2pids.items(), desc='reranking'):\n            assert len(pids) == 100\n            \n            query = [qid2q[qid]] * 100\n            para = [pid2p[pid] for pid in pids]\n            \n            scores = list(model.matching(query=query, para=para))\n            \n            sorted_index = [idx for idx, x in sorted(list(enumerate(scores)), key=lambda x:x[1], reverse=True)]\n            sorted_pids = [pids[idx] for idx in sorted_index]\n            sorted_scores = [scores[idx] for idx in sorted_index]\n            for rank, (pid, score) in enumerate(zip(sorted_pids, sorted_scores)):\n                f.write('%s\\t%s\\t%d\\t%s\\n' % (qid, pid, rank, score))", "\nfor split in ['train', 'dev', 'test']:\n    qid2q = dict()\n    with open(os.path.join('../../raw/NQ/data/retriever/qas', 'nq-' + split + '.csv'), 'r') as f:\n        for qid, line in enumerate(tqdm(f, desc='loading ' + split + ' queries')):\n            query, answers = line.strip().split('\\t')\n            qid2q[str(qid)] = query\n\n    qid2pids = dict()\n    with open(f'{output_folder}/res.top100.' + split, 'r') as f:\n        for line in tqdm(f, desc='loading ' + split + ' retrieval results'):\n            qid, pid, rank, score = line.strip().split()\n            if qid in qid2pids:\n                qid2pids[qid].append(pid)\n            else:\n                qid2pids[qid] = [pid]\n    \n    with open(f'{output_folder}/rerank.res.top100.' + split, 'w') as f:\n        for qid, pids in tqdm(qid2pids.items(), desc='reranking'):\n            assert len(pids) == 100\n            \n            query = [qid2q[qid]] * 100\n            para = [pid2p[pid] for pid in pids]\n            \n            scores = list(model.matching(query=query, para=para))\n            \n            sorted_index = [idx for idx, x in sorted(list(enumerate(scores)), key=lambda x:x[1], reverse=True)]\n            sorted_pids = [pids[idx] for idx in sorted_index]\n            sorted_scores = [scores[idx] for idx in sorted_index]\n            for rank, (pid, score) in enumerate(zip(sorted_pids, sorted_scores)):\n                f.write('%s\\t%s\\t%d\\t%s\\n' % (qid, pid, rank, score))", "\n"]}
{"filename": "data/RocketQA_baselines/NQ/retrieval.py", "chunked_list": ["import os\nimport sys\nimport glob\nimport pickle\nimport logging\nimport numpy as np\nimport faiss\n\n\nlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')", "\nlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\ntopk = 100\nmodel_name, output_folder = sys.argv[1], sys.argv[2]\nos.makedirs(output_folder, exist_ok=True)\nindex_path = f'{output_folder}/index'\nindex_meta_path = f'{output_folder}/index_meta'\nq_embed_pattern = f'{output_folder}/q_embed_*.pkl'\np_embed_pattern = f'{output_folder}/p_embed_*.pkl'", "q_embed_pattern = f'{output_folder}/q_embed_*.pkl'\np_embed_pattern = f'{output_folder}/p_embed_*.pkl'\noutput_path = f'{output_folder}/res.top100'\n\nif os.path.exists(index_path) and os.path.exists(index_meta_path):\n    logging.info('reading index ' + index_path)\n    with open(index_meta_path, 'rb') as f:\n        ids = pickle.load(f)\n    index = faiss.read_index(index_path)\n    assert len(ids) == index.ntotal\n\nelse:\n    ids = []\n    index = faiss.IndexFlatIP(768)\n    for p_embed_file in glob.glob(p_embed_pattern):\n        logging.info('indexing ' + p_embed_file)\n        with open(p_embed_file, \"rb\") as f:\n            data = pickle.load(f)\n        buffer = []\n        buffer_size = 50000\n        for db_id, doc_vector in data:\n            buffer.append((db_id, doc_vector))\n            if buffer_size == len(buffer):\n                ids.extend([x[0] for x in buffer])\n                index.add(np.concatenate([np.reshape(t[1], (1, -1)) for t in buffer], axis=0))\n                buffer = []\n        if len(buffer) > 0:\n            ids.extend([x[0] for x in buffer])\n            index.add(np.concatenate([np.reshape(t[1], (1, -1)) for t in buffer], axis=0))\n            buffer = []\n        logging.info('done. ntotal = ' + str(index.ntotal))\n    \n    faiss.write_index(index, index_path)\n    with open(index_meta_path, mode='wb') as f:\n        pickle.dump(ids, f)", "\nlogging.info(\"indexing done.\")\n\nbuffer_size = 32\nfaiss.omp_set_num_threads(30)\nfor split in ['train', 'dev', 'test']:\n    with open(q_embed_pattern.replace('*', split), 'rb') as f:\n        data = pickle.load(f)\n    \n    new_output_path = output_path + '.' + split\n    if os.path.exists(new_output_path):\n        os.remove(new_output_path)\n    for start_idx in range(0, len(data), buffer_size):\n        qids = [x[0] for x in data[start_idx:(start_idx + buffer_size)]]\n        qvecs = np.stack([x[1] for x in data[start_idx:(start_idx + buffer_size)]], axis=0)\n        scores, indexes = index.search(qvecs, topk)\n        retrieved_lists = [[ids[i] for i in query_top_idxs] for query_top_idxs in indexes]\n        assert len(scores) == len(retrieved_lists) == len(qids)\n\n        with open(new_output_path, 'a') as f:\n            for qid, pid_list, score_list in zip(qids, retrieved_lists, scores):\n                for rank, (pid, score) in enumerate(zip(pid_list, score_list), 1):\n                    f.write(\"%s\\t%s\\t%d\\t%f\\n\" % (qid, pid, rank, score))\n        \n        logging.info(f\"{start_idx} queries searched.\")", "\n"]}
{"filename": "data/RocketQA_baselines/MSMARCO/generate_embeddings.py", "chunked_list": ["import os\nimport sys\nimport logging\nimport pickle\nimport rocketqa\n\n\nlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\nshard_id = 0 if sys.argv[1] == 'q' else int(sys.argv[1])", "\nshard_id = 0 if sys.argv[1] == 'q' else int(sys.argv[1])\nshard_num, device_id, batch_size = [int(x) for x in sys.argv[2:5]]\nmodel_name, output_folder = sys.argv[5:7]\n\nif sys.argv[1] == 'q':\n    model = rocketqa.load_model(model_name, use_cuda=True, device_id=device_id, batch_size=batch_size)\n\n    for split in ['train', 'dev']:\n        if split == 'train':\n            qrels_file = '../../raw/MSMARCO/qrels.train.tsv'\n            queries_file = '../../raw/MSMARCO/queries.train.tsv'\n        else:\n            qrels_file = '../../raw/MSMARCO/qrels.dev.small.tsv'\n            queries_file = '../../raw/MSMARCO/queries.dev.small.tsv'\n\n        qid2q = dict()\n        with open(queries_file, 'r') as f:\n            for line in f:\n                qid, q = line.strip().split('\\t')\n                qid2q[qid] = q\n        \n        results = []\n\n        with open(qrels_file, 'r') as f:\n            lines = [line.strip().split('\\t') for line in f.readlines()]\n            qids = sorted(list(set([x[0] for x in lines])))\n            texts = [qid2q[qid] for qid in qids]\n            q_num = len(qids)\n            batch_qids, batch_texts = [], []\n            for idx, (qid, text) in enumerate(zip(qids, texts)):                \n                batch_qids.append(qid)\n                batch_texts.append(text)\n                if len(batch_texts) == batch_size or idx == q_num - 1:\n                    batch_outs = list(model.encode_query(query=batch_texts))\n                    assert len(batch_outs) == len(batch_qids)\n                    results.extend([(qid, out) for qid, out in zip(batch_qids, batch_outs)])\n                    batch_qids, batch_texts = [], []\n                    logging.info(\"%d query encoded.\" % idx)\n\n        os.makedirs(output_folder, exist_ok=True)\n        with open(f'{output_folder}/q_embed_' + split + '.pkl', 'wb') as f:\n            pickle.dump(results, f)\n\nelse:\n    logging.info((shard_id, shard_num, device_id, batch_size))\n    assert shard_id < shard_num\n\n    model = rocketqa.load_model(model_name, use_cuda=True, device_id=device_id, batch_size=batch_size)\n\n    corpus_file = '../../raw/MSMARCO/collection.tsv'\n\n    with open(corpus_file) as f:\n        para_num = len([idx for idx, _ in enumerate(f)])\n    shard_size = para_num // shard_num\n    start_idx = shard_id * shard_size\n    end_idx = para_num + 1 if shard_id == shard_num - 1 else start_idx + shard_size\n\n    results = []\n\n    with open(corpus_file, 'r') as f:\n        next(f)\n        batch_pids, batch_texts = [], []\n        for idx, line in enumerate(f):\n            if idx >= start_idx and idx < end_idx:\n                pid, text = line.strip().split('\\t')\n                \n                batch_pids.append(pid)\n                batch_texts.append(text)\n                if len(batch_texts) == batch_size or idx == end_idx - 1:\n                    logging.info(\"encoding passage %s to %s ...\" % (batch_pids[0], batch_pids[-1]))\n                    batch_outs = list(model.encode_para(para=batch_texts))\n                    assert len(batch_outs) == len(batch_pids)\n                    results.extend([(pid, out) for pid, out in zip(batch_pids, batch_outs)])\n                    batch_pids, batch_texts = [], []\n\n    os.makedirs(output_folder, exist_ok=True)\n    with open(f'{output_folder}/p_embed_' + str(shard_id) + '.pkl', 'wb') as f:\n        pickle.dump(results, f)"]}
{"filename": "data/RocketQA_baselines/MSMARCO/rerank.py", "chunked_list": ["import os\nimport sys\nfrom tqdm import tqdm\nimport rocketqa\n\n\nmodel_name, output_folder = sys.argv[1], sys.argv[2]\nmodel = rocketqa.load_model(model_name, use_cuda=True, device_id=0, batch_size=100)\n\npid2p = dict()\nwith open('../../raw/MSMARCO/collection.tsv', 'r') as f:\n    next(f)\n    for line in tqdm(f, desc='loading passages'):\n        pid, text = line.strip().split('\\t')\n        pid2p[pid] = text", "\npid2p = dict()\nwith open('../../raw/MSMARCO/collection.tsv', 'r') as f:\n    next(f)\n    for line in tqdm(f, desc='loading passages'):\n        pid, text = line.strip().split('\\t')\n        pid2p[pid] = text\n\nfor split in ['train', 'dev']:\n    if split == 'train':\n        queries_file = '../../raw/MSMARCO/queries.train.tsv'\n    else:\n        queries_file = '../../raw/MSMARCO/queries.dev.small.tsv'\n\n    qid2q = dict()\n    with open(queries_file, 'r') as f:\n        for line in f:\n            qid, q = line.strip().split('\\t')\n            qid2q[qid] = q\n\n    qid2pids = dict()\n    with open(f'{output_folder}/res.top100.' + split, 'r') as f:\n        for line in tqdm(f, desc='loading ' + split + ' retrieval results'):\n            qid, pid, rank, score = line.strip().split()\n            if qid in qid2pids:\n                qid2pids[qid].append(pid)\n            else:\n                qid2pids[qid] = [pid]\n    \n    with open(f'{output_folder}/rerank.res.top100.' + split, 'w') as f:\n        for qid, pids in tqdm(qid2pids.items(), desc='reranking'):\n            if len(pids) != 100:\n                pids = pids[:100]\n                # print(f'retrieval list for question {qid} truncated.')\n            \n            query = [qid2q[qid]] * 100\n            para = [pid2p[pid] for pid in pids]\n            \n            scores = list(model.matching(query=query, para=para))\n            \n            sorted_index = [idx for idx, x in sorted(list(enumerate(scores)), key=lambda x:x[1], reverse=True)]\n            sorted_pids = [pids[idx] for idx in sorted_index]\n            sorted_scores = [scores[idx] for idx in sorted_index]\n            for rank, (pid, score) in enumerate(zip(sorted_pids, sorted_scores)):\n                f.write('%s\\t%s\\t%d\\t%s\\n' % (qid, pid, rank, score))", "for split in ['train', 'dev']:\n    if split == 'train':\n        queries_file = '../../raw/MSMARCO/queries.train.tsv'\n    else:\n        queries_file = '../../raw/MSMARCO/queries.dev.small.tsv'\n\n    qid2q = dict()\n    with open(queries_file, 'r') as f:\n        for line in f:\n            qid, q = line.strip().split('\\t')\n            qid2q[qid] = q\n\n    qid2pids = dict()\n    with open(f'{output_folder}/res.top100.' + split, 'r') as f:\n        for line in tqdm(f, desc='loading ' + split + ' retrieval results'):\n            qid, pid, rank, score = line.strip().split()\n            if qid in qid2pids:\n                qid2pids[qid].append(pid)\n            else:\n                qid2pids[qid] = [pid]\n    \n    with open(f'{output_folder}/rerank.res.top100.' + split, 'w') as f:\n        for qid, pids in tqdm(qid2pids.items(), desc='reranking'):\n            if len(pids) != 100:\n                pids = pids[:100]\n                # print(f'retrieval list for question {qid} truncated.')\n            \n            query = [qid2q[qid]] * 100\n            para = [pid2p[pid] for pid in pids]\n            \n            scores = list(model.matching(query=query, para=para))\n            \n            sorted_index = [idx for idx, x in sorted(list(enumerate(scores)), key=lambda x:x[1], reverse=True)]\n            sorted_pids = [pids[idx] for idx in sorted_index]\n            sorted_scores = [scores[idx] for idx in sorted_index]\n            for rank, (pid, score) in enumerate(zip(sorted_pids, sorted_scores)):\n                f.write('%s\\t%s\\t%d\\t%s\\n' % (qid, pid, rank, score))", "\n"]}
{"filename": "data/RocketQA_baselines/MSMARCO/retrieval.py", "chunked_list": ["import os\nimport sys\nimport glob\nimport pickle\nimport logging\nimport numpy as np\nimport faiss\n\n\nlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')", "\nlogging.basicConfig(level=logging.DEBUG, format=\"[%(asctime)s] - %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n\ntopk = 100\nmodel_name, output_folder = sys.argv[1], sys.argv[2]\nos.makedirs(output_folder, exist_ok=True)\nindex_path = f'{output_folder}/index'\nindex_meta_path = f'{output_folder}/index_meta'\nq_embed_pattern = f'{output_folder}/q_embed_*.pkl'\np_embed_pattern = f'{output_folder}/p_embed_*.pkl'", "q_embed_pattern = f'{output_folder}/q_embed_*.pkl'\np_embed_pattern = f'{output_folder}/p_embed_*.pkl'\noutput_path = f'{output_folder}/res.top100'\n\nif os.path.exists(index_path) and os.path.exists(index_meta_path):\n    logging.info('reading index ' + index_path)\n    with open(index_meta_path, 'rb') as f:\n        ids = pickle.load(f)\n    index = faiss.read_index(index_path)\n    assert len(ids) == index.ntotal\n\nelse:\n    ids = []\n    index = faiss.IndexFlatIP(768)\n    for p_embed_file in glob.glob(p_embed_pattern):\n        logging.info('indexing ' + p_embed_file)\n        with open(p_embed_file, \"rb\") as f:\n            data = pickle.load(f)\n        buffer = []\n        buffer_size = 50000\n        for db_id, doc_vector in data:\n            buffer.append((db_id, doc_vector))\n            if buffer_size == len(buffer):\n                ids.extend([x[0] for x in buffer])\n                index.add(np.concatenate([np.reshape(t[1], (1, -1)) for t in buffer], axis=0))\n                buffer = []\n        if len(buffer) > 0:\n            ids.extend([x[0] for x in buffer])\n            index.add(np.concatenate([np.reshape(t[1], (1, -1)) for t in buffer], axis=0))\n            buffer = []\n        logging.info('done. ntotal = ' + str(index.ntotal))\n    \n    faiss.write_index(index, index_path)\n    with open(index_meta_path, mode='wb') as f:\n        pickle.dump(ids, f)", "\nlogging.info(\"indexing done.\")\n\nbuffer_size = 32\nfaiss.omp_set_num_threads(30)\nfor split in ['train', 'dev']:\n    with open(q_embed_pattern.replace('*', split), 'rb') as f:\n        data = pickle.load(f)\n    \n    new_output_path = output_path + '.' + split\n    if os.path.exists(new_output_path):\n        os.remove(new_output_path)\n    for start_idx in range(0, len(data), buffer_size):\n        qids = [x[0] for x in data[start_idx:(start_idx + buffer_size)]]\n        qvecs = np.stack([x[1] for x in data[start_idx:(start_idx + buffer_size)]], axis=0)\n        scores, indexes = index.search(qvecs, topk)\n        retrieved_lists = [[ids[i] for i in query_top_idxs] for query_top_idxs in indexes]\n        assert len(scores) == len(retrieved_lists) == len(qids)\n\n        with open(new_output_path, 'a') as f:\n            for qid, pid_list, score_list in zip(qids, retrieved_lists, scores):\n                for rank, (pid, score) in enumerate(zip(pid_list, score_list), 1):\n                    f.write(\"%s\\t%s\\t%d\\t%f\\n\" % (qid, pid, rank, score))\n        \n        logging.info(f\"{start_idx} queries searched.\")", "\n"]}
