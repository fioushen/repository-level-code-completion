{"filename": "tfold_msa_tools.py", "chunked_list": ["#Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2021-2022\n\n#MSA and pdb search tools from the AlphaFold data pipeline\n\nimport os\nimport numpy as np\nimport pickle\nimport time\nimport shutil", "import time\nimport shutil\n\nfrom alphafold.data.tools import jackhmmer,hhblits,hmmsearch\nfrom alphafold.data import parsers\n\nfrom tfold_config import uniref90_database_path,mgnify_database_path,bfd_database_path,uniclust30_database_path\nfrom tfold_config import pdb_seqres_database_path\nfrom tfold_config import jackhmmer_binary_path,hhblits_binary_path,hmmsearch_binary_path,hmmbuild_binary_path\n", "from tfold_config import jackhmmer_binary_path,hhblits_binary_path,hmmsearch_binary_path,hmmbuild_binary_path\n\n#max hits, as set in the AF-M pipeline\nUNIREF_MAX_HITS=10000\nMGNIFY_MAX_HITS=  501\n\ndef process_seq(seq,tmp_dir,msa_output_dir):\n    '''\n    takes a sequence seq, tmp_dir and output_dir;\n    runs alignment tools on uniref90, mgnify, bfd, uniclust30 \n    and saves the output\n    '''    \n    #make unique tmp_id, write query fasta, make output_dir\n    tmp_id=seq[:10]+''.join([str(x) for x in np.random.randint(10,size=10)])    \n    input_fasta_path=tmp_dir+f'/{tmp_id}.fasta'    \n    with open(input_fasta_path,'w',encoding='utf8',newline='') as f:\n        f.write('>seq\\n'+seq)         \n    os.makedirs(msa_output_dir,exist_ok=True)\n        \n    #uniref90 via jackhmmer\n    jackhmmer_uniref90_runner=jackhmmer.Jackhmmer(binary_path=jackhmmer_binary_path,\n                                                  database_path=uniref90_database_path)\n    print(f'running jackhmmer on {input_fasta_path} fasta')\n    jackhmmer_uniref90_result=jackhmmer_uniref90_runner.query(input_fasta_path)[0]\n    #preprocess uniref90 result: truncate, deduplicate, remove empty columns\n    uniref90_result_sto=jackhmmer_uniref90_result['sto']\n    uniref90_result_sto=parsers.truncate_stockholm_msa(uniref90_result_sto,max_sequences=UNIREF_MAX_HITS)\n    uniref90_result_sto=parsers.deduplicate_stockholm_msa(uniref90_result_sto)\n    uniref90_result_sto=parsers.remove_empty_columns_from_stockholm_msa(uniref90_result_sto)\n    uniref90_result_a3m=parsers.convert_stockholm_to_a3m(uniref90_result_sto)\n    uniref90_out_path_sto=os.path.join(msa_output_dir,'uniref90_hits.sto')        \n    with open(uniref90_out_path_sto,'w') as f: #for hmmsearch\n        f.write(uniref90_result_sto)\n    uniref90_out_path_a3m=os.path.join(msa_output_dir,'uniref90_hits.a3m')  \n    with open(uniref90_out_path_a3m,'w') as f: #for MSA input\n        f.write(uniref90_result_a3m)\n             \n    #mgnify via jackhmmer\n    jackhmmer_mgnify_runner=jackhmmer.Jackhmmer(binary_path=jackhmmer_binary_path,\n                                                database_path=mgnify_database_path)\n    jackhmmer_mgnify_result=jackhmmer_mgnify_runner.query(input_fasta_path)[0]\n    #preprocess mgnify result: truncate, deduplicate, remove empty columns\n    mgnify_result_sto=jackhmmer_mgnify_result['sto']\n    mgnify_result_sto=parsers.truncate_stockholm_msa(mgnify_result_sto,max_sequences=MGNIFY_MAX_HITS)\n    mgnify_result_sto=parsers.deduplicate_stockholm_msa(mgnify_result_sto)\n    mgnify_result_sto=parsers.remove_empty_columns_from_stockholm_msa(mgnify_result_sto)\n    mgnify_result_a3m=parsers.convert_stockholm_to_a3m(mgnify_result_sto)\n    mgnify_out_path=os.path.join(msa_output_dir,'mgnify_hits.a3m')\n    with open(mgnify_out_path,'w') as f:\n        f.write(mgnify_result_a3m)\n\n    #bfd and uniclust30 via hhblits\n    hhblits_bfd_uniclust_runner=hhblits.HHBlits(binary_path=hhblits_binary_path,\n                                                  databases=[bfd_database_path,uniclust30_database_path])\n    #if using data.tools.from AF-M, use query(...)[0]\n    #if using data.tools from the old AF, use query(...)\n    hhblits_bfd_uniclust_result=hhblits_bfd_uniclust_runner.query(input_fasta_path)[0] \n    bfd_out_path=os.path.join(msa_output_dir,'bfd_uniclust_hits.a3m')\n    with open(bfd_out_path,'w') as f:\n        f.write(hhblits_bfd_uniclust_result['a3m'])\n\n    #remove the input fasta\n    os.remove(input_fasta_path)", "    \ndef search_pdb(input_msa,output_dir):\n    #hhsearch takes a3m, hmmsearch takes sto; here use hmmsearch, hence input must be sto\n    with open(input_msa) as f:\n        msa=f.read()        \n    template_searcher=hmmsearch.Hmmsearch(binary_path=hmmsearch_binary_path,hmmbuild_binary_path=hmmbuild_binary_path,\n                                          database_path=pdb_seqres_database_path)    \n    pdb_result=template_searcher.query(msa)  \n    pdb_result=parsers.convert_stockholm_to_a3m(pdb_result)\n    os.makedirs(output_dir,exist_ok=True)\n    pdb_hits_out_path=os.path.join(output_dir, f'pdb_hits.a3m')\n    with open(pdb_hits_out_path, 'w') as f:\n        f.write(pdb_result)", "        \nif __name__=='__main__':               \n    from argparse import ArgumentParser\n    import csv\n    t0=time.time() \n    parser=ArgumentParser()\n    parser.add_argument('input_file', type=str, help='path to input file')    \n    parser.add_argument('task', type=str, help='msa or pdb')            \n    parser.add_argument('output_dir', type=str, help='where to put results')   \n    parser.add_argument('--tmp_dir', default=None, type=str, help='where to store tmp fastas')   \n    args=parser.parse_args()  \n    if args.task=='msa':\n        if args.tmp_dir is None:\n            raise ValueError('tmp dir must be provided for MSA building')                \n        os.makedirs(args.tmp_dir,exist_ok=True)\n        inputs=[]\n        with open(args.input_file) as f:\n            f_csv=csv.reader(f,delimiter='\\t')\n            inputs=[x for x in f_csv]        \n        print(f'processing {len(inputs)} tasks...')\n        for x in inputs:\n            seq,name=x\n            process_seq(seq,args.tmp_dir,args.output_dir+'/'+name)\n        print('finished {} tasks in {} s'.format(len(inputs),time.time()-t0))\n    elif args.task=='pdb':\n        search_pdb(args.input_file,args.output_dir)\n    else:\n        raise ValueError(f'task {args.task} not recognized')    ", ""]}
{"filename": "collect_results.py", "chunked_list": ["from argparse import ArgumentParser\nimport pandas as pd\nfrom tfold.modeling import result_parse_tools\n\nif __name__=='__main__':               \n    parser=ArgumentParser()    \n    parser.add_argument('working_dir',type=str,\n                        help='Path to a directory where AlphaFold inputs and outputs will be stored')\n    args=parser.parse_args() \n    working_dir=args.working_dir\n    #collect results\n    result_parse_tools.parse_results(working_dir)\n    result_df=pd.read_pickle(working_dir+'/result_df.pckl')\n    #reduce to best models (lowest predicted score) for each pMHC and save\n    best_model_df=result_parse_tools.reduce_to_best(result_df,['pmhc_id'],'score',how='min')   \n    best_model_df=best_model_df.drop(['tpl_tails', 'best_score', 'best_mhc_score',\n                                      'pep_lddt', 'mhc_lddt', 'mhc_a','mhc_b','tails_prefiltered', \n                                      'af_n_reg', 'seqnn_logkd'],axis=1)\n    best_model_df.to_csv(working_dir+'/best_models.csv',index=False)"]}
{"filename": "model_pmhcs.py", "chunked_list": ["import os,pickle\nfrom argparse import ArgumentParser\nimport pandas as pd\nfrom tfold.modeling import make_inputs\n\nif __name__=='__main__':               \n    parser=ArgumentParser()\n    parser.add_argument('input',type=str, \n                         help='Path to input csv file with columns \"pep\" and \"MHC allele\" or \"MHC sequence\", and optionally, \"pmhc_id\", \"pdb_id\", and \"exclude_pdb\". (See details.ipynb for the details.)')    \n    parser.add_argument('working_dir',type=str,\n                        help='Path to a directory where AlphaFold inputs and outputs will be stored')\n    parser.add_argument('--date_cutoff',type=str,default=None,help='Optionally, date cutoff for templates, YYYY-MM-DD.')\n    args=parser.parse_args() \n    df_to_model=pd.read_csv(args.input)\n    print(f'Need to model {len(df_to_model)} pMHCs.')\n    working_dir=args.working_dir\n    date_cutoff=args.date_cutoff                        \n    #make numbered MHC objects and run seqnn\n    df_to_model=make_inputs.preprocess_df(df_to_model)\n\n    #make AF inputs\n    af_inputs=make_inputs.make_inputs(df_to_model,date_cutoff=date_cutoff,print_stats=False)\n    print('total AF models to be produced:',len(af_inputs))\n\n    #make folders\n    input_dir=working_dir+'/inputs'\n    output_dir=working_dir+'/outputs'\n    os.makedirs(working_dir,exist_ok=True)\n    os.makedirs(input_dir,exist_ok=True) \n    os.makedirs(output_dir,exist_ok=True)\n\n    #save AF inputs and input dataframe\n    with open(input_dir+'/input.pckl','wb') as f: \n        pickle.dump(af_inputs,f) \n    df_to_model.to_pickle(working_dir+'/target_df.pckl')", "    \n    #input_dir=os.path.abspath(input_dir)\n    #output_dir=os.path.abspath(output_dir)\n    #print('#############################################################')\n    #print('Next, run tfold_run_alphafold.py on a GPU as follows:')\n    #print(f'python tfold_run_alphafold.py --inputs {input_dir}/input.pckl --output_dir {output_dir}') \n    #print('#############################################################')"]}
{"filename": "tfold_run_alphafold.py", "chunked_list": ["#patch by: Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2021-2023\n\n# Copyright 2021 DeepMind Technologies Limited\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0", "#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys", "\nimport sys\nfrom tfold_patch.tfold_config import data_dir,mmcif_dir,kalign_binary_path,af_params,alphafold_dir\nsys.path.append(alphafold_dir) #path to AlphaFold for import\n\n\"\"\"Full AlphaFold protein structure prediction script.\"\"\"\nimport json\nimport os\nimport pathlib\nimport pickle", "import pathlib\nimport pickle\nimport random\nimport time\nfrom typing import Dict, Union, Optional\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nfrom alphafold.common import protein", "from absl import logging\nfrom alphafold.common import protein\nfrom alphafold.common import residue_constants\n\nfrom alphafold.data import templates\nfrom alphafold.model import config\nfrom alphafold.model import model\nimport numpy as np\n\nfrom alphafold.model import data", "\nfrom alphafold.model import data\n# Internal import (7716).\n\nlogging.set_verbosity(logging.INFO)\n\nimport tfold_patch.tfold_pipeline as pipeline\nimport tfold_patch.postprocessing as postprocessing\n\nflags.DEFINE_string('inputs',None,'path to a .pkl input file with a list of inputs')", "\nflags.DEFINE_string('inputs',None,'path to a .pkl input file with a list of inputs')\nflags.DEFINE_string('output_dir',None,'where to put outputs')\nflags.DEFINE_boolean('benchmark', False, 'Run multiple JAX model evaluations '\n                     'to obtain a timing that excludes the compilation time, '\n                     'which should be more indicative of the time required for '\n                     'inferencing many proteins.')\nflags.DEFINE_integer('random_seed', None, 'The random seed for the data '\n                     'pipeline. By default, this is randomly generated. Note '\n                     'that even if this is set, Alphafold may still not be '", "                     'pipeline. By default, this is randomly generated. Note '\n                     'that even if this is set, Alphafold may still not be '\n                     'deterministic, because processes like GPU inference are '\n                     'nondeterministic.')\nFLAGS = flags.FLAGS\n\nMAX_TEMPLATE_HITS=20            #default 20; later reduced to 4 anyway (?)\nMAX_TEMPLATE_DATE='9999-12-31'  #set no limit here\n\ndef renumber_pdb(pdb,renumber_list):   \n    '''\n    note: AF numbers residues from 1 sequentially but with interchain shifts    \n    '''\n    lines=[]      \n    i_current=-1\n    chain_pdbnum_prev='xxxxxx'\n    for line in pdb.split('\\n'):\n        if line.startswith(('ATOM','TER')):               \n            chain_pdbnum=line[21:27]\n            if chain_pdbnum!=chain_pdbnum_prev:                \n                chain_pdbnum_prev=chain_pdbnum\n                i_current+=1\n            new_chain_pdbnum=renumber_list[i_current]\n            line=line[:21]+new_chain_pdbnum+line[27:]\n        lines.append(line)    \n    return '\\n'.join(lines)", "\ndef renumber_pdb(pdb,renumber_list):   \n    '''\n    note: AF numbers residues from 1 sequentially but with interchain shifts    \n    '''\n    lines=[]      \n    i_current=-1\n    chain_pdbnum_prev='xxxxxx'\n    for line in pdb.split('\\n'):\n        if line.startswith(('ATOM','TER')):               \n            chain_pdbnum=line[21:27]\n            if chain_pdbnum!=chain_pdbnum_prev:                \n                chain_pdbnum_prev=chain_pdbnum\n                i_current+=1\n            new_chain_pdbnum=renumber_list[i_current]\n            line=line[:21]+new_chain_pdbnum+line[27:]\n        lines.append(line)    \n    return '\\n'.join(lines)", "\ndef predict_structure(sequences,msas,template_hits,renumber_list,\n                      current_id,output_dir,                          \n                      data_pipeline,model_runners,benchmark,random_seed,true_pdb=None):  \n    logging.info(f'Predicting for id {current_id}')\n    timings = {}        \n    os.makedirs(output_dir,exist_ok=True)    \n    # Get features.\n    t_0=time.time()    \n    feature_dict=data_pipeline.process(sequences,msas,template_hits)\n    timings['features']=time.time()-t_0    \n    # Run the models.    \n    num_models=len(model_runners)\n    for model_index,(model_name,model_runner) in enumerate(model_runners.items()):\n        logging.info('Running model %s on %s',model_name,current_id)\n        t_0=time.time()\n        model_random_seed=model_index+random_seed*num_models\n        processed_feature_dict=model_runner.process_features(feature_dict,random_seed=model_random_seed)\n        timings[f'process_features_{model_name}']=time.time()-t_0\n        t_0=time.time()\n        prediction_result=model_runner.predict(processed_feature_dict,random_seed=model_random_seed)\n        t_diff=time.time()-t_0\n        timings[f'predict_and_compile_{model_name}']=t_diff\n        logging.info('Total JAX model %s on %s predict time (includes compilation time, see --benchmark): %.1fs',\n                     model_name,current_id,t_diff)\n        if benchmark:\n            t_0=time.time()\n            model_runner.predict(processed_feature_dict,random_seed=model_random_seed)\n            t_diff=time.time()-t_0\n            timings[f'predict_benchmark_{model_name}']=t_diff\n            logging.info('Total JAX model %s on %s predict time (excludes compilation time): %.1fs',\n                         model_name,current_id,t_diff)               \n        # Add the predicted LDDT in the b-factor column.\n        # Note that higher predicted LDDT value means higher model confidence.\n        plddt=prediction_result['plddt']\n        plddt_b_factors=np.repeat(plddt[:, None],residue_constants.atom_type_num,axis=-1)\n        unrelaxed_protein=protein.from_prediction(features=processed_feature_dict,result=prediction_result,\n                                                  b_factors=plddt_b_factors,remove_leading_feature_dimension=True)\n        unrelaxed_pdb=protein.to_pdb(unrelaxed_protein)        \n        unrelaxed_pdb_renumbered=renumber_pdb(unrelaxed_pdb,renumber_list)        \n        #renumber peptide\n        unrelaxed_pdb_renumbered,pep_pdbnum,pep_tails,success=postprocessing.renumber_pep(unrelaxed_pdb_renumbered)        \n        prediction_result['pep_renumbered']=success\n        prediction_result['pep_tails']=pep_tails\n        prediction_result['pdbnum_list']=['P'+p for p in pep_pdbnum]+renumber_list[len(sequences[0]):]                        \n        #compute rmsd if true structure provided\n        if true_pdb:\n            rmsds=postprocessing.compute_rmsds(unrelaxed_pdb_renumbered,true_pdb)\n            prediction_result={**prediction_result,**rmsds}\n        #save results and pdb\n        result_output_path=os.path.join(output_dir,f'result_{model_name}_{current_id}.pkl')\n        with open(result_output_path,'wb') as f:\n            pickle.dump(prediction_result, f, protocol=4)\n        unrelaxed_pdb_path=os.path.join(output_dir,f'structure_{model_name}_{current_id}.pdb')\n        with open(unrelaxed_pdb_path,'w') as f:\n            f.write(unrelaxed_pdb_renumbered)                 \n    logging.info('Final timings for %s: %s', current_id, timings)", "    #timings_output_path=os.path.join(output_dir,f'timings_{current_id}.json')\n    #with open(timings_output_path, 'w') as f:\n    #    f.write(json.dumps(timings,indent=4))\n\ndef main(argv):    \n    t_start=time.time()    \n    with open(FLAGS.inputs,'rb') as f:\n        inputs=pickle.load(f)            #list of dicts [{param_name : value_for_input_0},..]     \n    if len(inputs)==0:\n        raise ValueError('input list of zero length provided')\n    output_dir=FLAGS.output_dir\n    logging.info(f'processing {len(inputs)} inputs...')           \n    #set parameters#   \n    params=af_params #from tfold.config\n    num_ensemble      =params['num_ensemble']   \n    model_names       =params['model_names']   \n    chain_break_shift =params['chain_break_shift']\n    ##################        \n    template_featurizer=templates.HhsearchHitFeaturizer(mmcif_dir=mmcif_dir,\n                                                        max_template_date=MAX_TEMPLATE_DATE,\n                                                        max_hits=MAX_TEMPLATE_HITS,\n                                                        kalign_binary_path=kalign_binary_path,\n                                                        release_dates_path=None,\n                                                        obsolete_pdbs_path=None)\n    data_pipeline=pipeline.DataPipeline(template_featurizer=template_featurizer,chain_break_shift=chain_break_shift)\n    model_runners={}    \n    for model_name in model_names:\n        model_config=config.model_config(model_name)\n        model_config.data.eval.num_ensemble=num_ensemble\n        model_params=data.get_model_haiku_params(model_name=model_name,data_dir=data_dir)\n        model_runner=model.RunModel(model_config,model_params)\n        model_runners[model_name]=model_runner\n    logging.info('Have %d models: %s',len(model_runners),list(model_runners.keys()))\n    random_seed=FLAGS.random_seed\n    if random_seed is None:\n        random_seed = random.randrange(sys.maxsize // len(model_names))\n    logging.info('Using random seed %d for the data pipeline',random_seed)  \n    for x in inputs:\n        sequences        =x['sequences']            #(seq_chain1,seq_chain2,..)\n        msas             =x['msas']                 #list of dicts {chain_number:path to msa in a3m format,..}\n        template_hits    =x['template_hits']        #list of dicts for template hits\n        renumber_list    =x['renumber_list']        #e.g. ['P   1 ','P   2 ',..,'M   5 ',..]\n        target_id        =str(x['target_id'])       #id or name of the target\n        current_id       =str(x['current_id'])      #id of the run (for a given target, all run ids should be distinct)\n        true_pdb         =x.get('true_pdb')         #pdb_id of true structure, for rmsd computation\n        output_dir_target=output_dir+'/'+target_id\n        predict_structure(sequences=sequences,msas=msas,template_hits=template_hits,renumber_list=renumber_list,\n                          current_id=current_id,output_dir=output_dir_target,                          \n                          data_pipeline=data_pipeline,model_runners=model_runners,\n                          benchmark=FLAGS.benchmark,random_seed=random_seed,true_pdb=true_pdb)\n    t_delta=time.time()-t_start\n    print('Processed {:3d} inputs in {:4.1f} minutes.'.format(len(inputs),t_delta/60))\n    print('time per input: {:5.1f}'.format(t_delta/len(inputs)))    ", "\nif __name__ == '__main__':\n    flags.mark_flags_as_required(['inputs','output_dir'])\n    app.run(main)\n"]}
{"filename": "tfold_patch/tfold_config.py", "chunked_list": ["#Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2021-2023\n\n### SET THESE DIRECTORIES ################################################################################\n\n#AF folder containing '/params' (AF params), e.g. '/data/vmikhayl/alphafold-multimer/databases' \ndata_dir      =#\n\n#AF folder containing run_alphafold.py, e.g. '/data/vmikhayl/alphafold-multimer/alphafold' \nalphafold_dir =#", "#AF folder containing run_alphafold.py, e.g. '/data/vmikhayl/alphafold-multimer/alphafold' \nalphafold_dir =#\n\n#folder with TFold data, e.g. '/data/vmikhayl/tfold-release/data'      \ntfold_data_dir=#   \n\n#path to kalign (used by AF to realign templates);\n#if you installed conda environment tfold-env, kalign should be in the /envs/tfold-env/bin/kalign in you conda folder\n#e.g.:'/home/vmikhayl/anaconda3/envs/tfold-env/bin/kalign'\nkalign_binary_path=#", "#e.g.:'/home/vmikhayl/anaconda3/envs/tfold-env/bin/kalign'\nkalign_binary_path=#\n\n##########################################################################################################\n##########################################################################################################\n##########################################################################################################\n\n#folder with processed mmcif files to use as templates\nmmcif_dir=tfold_data_dir+'/experimental_structures/processed_updated/mmcif'\n#folder with pdbs for true structures, for rmsd computations", "mmcif_dir=tfold_data_dir+'/experimental_structures/processed_updated/mmcif'\n#folder with pdbs for true structures, for rmsd computations\ntrue_pdb_dir=tfold_data_dir+'/experimental_structures/processed_updated/pdb_rotated'\n\n##the following data are not needed unless you want to build new MSAs and search PDB##\n##protein databases\n#uniref90_database_path  =data_dir+'/uniref90/uniref90.fasta' \n#mgnify_database_path    =data_dir+'/mgnify/mgy_clusters_2018_12.fa'\n#bfd_database_path       =data_dir+'/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt'\n#uniclust30_database_path=data_dir+'/uniclust30/uniclust30_2018_08/uniclust30_2018_08'", "#bfd_database_path       =data_dir+'/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt'\n#uniclust30_database_path=data_dir+'/uniclust30/uniclust30_2018_08/uniclust30_2018_08'\n#pdb_seqres_database_path=data_dir+'/pdb_seqres/pdb_seqres.txt'\n##bins for alignment tools\n#search_tools_dir='/home/vmikhayl/anaconda3/envs/tfold-env/bin'   #tfold-env conda environment folder\n#jackhmmer_binary_path=search_tools_dir+'/jackhmmer'    \n#hhblits_binary_path  =search_tools_dir+'/hhblits'\n#hmmsearch_binary_path=search_tools_dir+'/hmmsearch'\n#hmmbuild_binary_path =search_tools_dir+'/hmmbuild'\n", "#hmmbuild_binary_path =search_tools_dir+'/hmmbuild'\n\n#AlphaFold parameters\naf_params={'num_ensemble':1,               #ensembling\n           'model_names':('model_1',),     #which AF models to use; monomer models with templates are okay, \n                                           #i.e. model_1, model_2, model_1_ptm, model_2_ptm (see alphafold.model.config for details)           \n           'chain_break_shift':200         #chain index break in multi-chain protein\n          }\n\n", "\n"]}
{"filename": "tfold_patch/postprocessing.py", "chunked_list": ["#Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2022\n\n#Tools for postprocessing pdb files after AF modeling: renumber peptides, compute RMSDs\n\nimport os\nimport re\nimport numpy as np\nimport pickle\nimport time", "import pickle\nimport time\n\nfrom Bio import pairwise2\n\nimport tfold_patch.tfold_pdb_tools as pdb_tools\nfrom tfold_patch.tfold_config import true_pdb_dir\n\n#reference structures for pep renumbering\nimport importlib.resources", "#reference structures for pep renumbering\nimport importlib.resources\nimport tfold_patch.ref_structures as ref_structures\n\n#pep renumbering\ncl_I_resnum_template_left=['   0{:1d}'.format(x) for x in range(1,10)]+['{:4d} '.format(x) for x in range(1,6)]\ncl_I_resnum_template_insert=['   5{:1d}'.format(x) for x in range(1,10)]\ncl_I_resnum_template_right=['{:4d} '.format(x) for x in range(6,10000)]\ncl_II_resnum_template_ext=['   0{}'.format(x) for x in 'abcdefghijklmnopqrstuvwxyz']\ncl_II_resnum_template=['   0{:1d}'.format(x) for x in range(1,10)]+['{:4d} '.format(x) for x in range(1,10000)]\ndef _make_pep_pdbnums_I(pep_len,left_tail,right_tail):\n    '''\n    pdbnums for a class I peptide\n    '''\n    assert -1<=left_tail<=9,    'cl I pep: left tail length should be between -1 and 9;'\n    assert 0<=right_tail<=9999, 'cl I pep: right tail length must be between 0 and 9999;'    \n    core_len=pep_len-left_tail-right_tail    #core length\n    assert core_len>=8, 'cl I pep: core length must be at least 8;'   \n    assert core_len<=18, 'cl I pep: core too long;' #cannot index cores longer than 18\n    left_part=cl_I_resnum_template_left[9-left_tail:] #e.g. [9:] for no tail, [10:] for tail=-1 (i.e. from res 2)\n    l_insert=max(0,core_len-9)\n    l_insert_right=l_insert//2\n    l_insert_left=l_insert-l_insert_right\n    center_part=cl_I_resnum_template_insert[:l_insert_left]+cl_I_resnum_template_insert[9-l_insert_right:]\n    right_part=cl_I_resnum_template_right[max(0,9-core_len):] #remove res 6 if core length 8\n    pdbnum=left_part+center_part+right_part        \n    return pdbnum[:pep_len]", "cl_II_resnum_template_ext=['   0{}'.format(x) for x in 'abcdefghijklmnopqrstuvwxyz']\ncl_II_resnum_template=['   0{:1d}'.format(x) for x in range(1,10)]+['{:4d} '.format(x) for x in range(1,10000)]\ndef _make_pep_pdbnums_I(pep_len,left_tail,right_tail):\n    '''\n    pdbnums for a class I peptide\n    '''\n    assert -1<=left_tail<=9,    'cl I pep: left tail length should be between -1 and 9;'\n    assert 0<=right_tail<=9999, 'cl I pep: right tail length must be between 0 and 9999;'    \n    core_len=pep_len-left_tail-right_tail    #core length\n    assert core_len>=8, 'cl I pep: core length must be at least 8;'   \n    assert core_len<=18, 'cl I pep: core too long;' #cannot index cores longer than 18\n    left_part=cl_I_resnum_template_left[9-left_tail:] #e.g. [9:] for no tail, [10:] for tail=-1 (i.e. from res 2)\n    l_insert=max(0,core_len-9)\n    l_insert_right=l_insert//2\n    l_insert_left=l_insert-l_insert_right\n    center_part=cl_I_resnum_template_insert[:l_insert_left]+cl_I_resnum_template_insert[9-l_insert_right:]\n    right_part=cl_I_resnum_template_right[max(0,9-core_len):] #remove res 6 if core length 8\n    pdbnum=left_part+center_part+right_part        \n    return pdbnum[:pep_len]", "def _make_pep_pdbnums_II(pep_len,left_tail):  \n    '''\n    pdbnums for a class II peptide\n    '''    \n    assert pep_len-left_tail>=9, 'cl II pep: core too short;'\n    left_tail_ext=max(left_tail-9,0) #part with letter insertion codes\n    pdbnum=cl_II_resnum_template_ext[len(cl_II_resnum_template_ext)-left_tail_ext:]+cl_II_resnum_template[max(0,9-left_tail):]\n    return pdbnum[:pep_len]\ndef _get_CA_coord_w_default(res):\n    '''residue CA coordinates; replaces by average over all coords if no CA'''\n    if res is None:\n        return np.array([0.,0.,0.])\n    elif 'CA' in res:\n        return res['CA']\n    else:\n        return np.average(list(res.values()),axis=0)  ", "def _get_CA_coord_w_default(res):\n    '''residue CA coordinates; replaces by average over all coords if no CA'''\n    if res is None:\n        return np.array([0.,0.,0.])\n    elif 'CA' in res:\n        return res['CA']\n    else:\n        return np.average(list(res.values()),axis=0)  \ndef _pdbnum_to_tails(pdbnum):\n    pdbnum=np.array(pdbnum)\n    return np.sum(pdbnum<'   2 ')-1,np.sum(pdbnum>'   9 ')   #res 2 and 9 always present                ", "def _pdbnum_to_tails(pdbnum):\n    pdbnum=np.array(pdbnum)\n    return np.sum(pdbnum<'   2 ')-1,np.sum(pdbnum>'   9 ')   #res 2 and 9 always present                \ndef renumber_pep(pdb):\n    '''\n    superimpose a structure onto a reference structure and renumber the peptide accordingly;\n    (largely borrows from process_pdbs.py);\n    if no output_filename is given, will overwrite the input pdb;\n    returns a list of errors\n    '''\n    errors=[]\n    chainmaps={'I':[['M','M']],'II':[['M','M'],['N','N']]}    \n    #load structure, determine class    \n    structure,_=pdb_tools.parse_pdb_from_str(pdb,'query')   \n    chains=[x.get_id() for x in structure.get_chains()]\n    if 'N' in chains:\n        cl='II'\n    else:\n        cl='I'\n    #load ref structure\n    if cl=='I':               \n        ref_pdb=importlib.resources.read_text(ref_structures, '3mrePA___.pdb')\n    else:\n        ref_pdb=importlib.resources.read_text(ref_structures, '4x5wCAB__.pdb')       \n    ref_structure,_=pdb_tools.parse_pdb_from_str(ref_pdb,'refpdb')       \n    ref_structure_dict=pdb_tools.get_structure_dict(ref_structure,True)\n    ref_pep_resnums,ref_pep_coords=[],[]\n    for k,v in ref_structure_dict['P'].items():        \n        ref_pep_resnums.append(k)\n        ref_pep_coords.append(v['CA'])\n    ref_pep_resnums=np.array(ref_pep_resnums)\n    ref_pep_coords=np.array(ref_pep_coords)\n    #superimpose\n    pdb_tools.superimpose_by_chainmap(structure,ref_structure,chainmaps[cl])   \n    structure_dict=pdb_tools.get_structure_dict(structure,True)\n    p_pdbnum=list(structure_dict['P'].keys()) #add sort? no, in case very long pep with [a-z] indexed left tail\n    pep_len=len(p_pdbnum)\n    pep_coords=np.array([_get_CA_coord_w_default(structure_dict['P'][k]) for k in p_pdbnum])\n    #pep-pep distance matrix\n    d2matrix=pdb_tools.distance2_matrix(pep_coords,ref_pep_coords)            \n    closest_refs=[]    \n    for i in range(d2matrix.shape[0]):\n        i0=np.argmin(d2matrix[i,:])        \n        closest_refs.append(ref_pep_resnums[i0])\n    refs_symbol=','.join(closest_refs)\n    p12_str='   1 ,   2 '\n    p23_str='   2 ,   3 '\n    p89_str='   8 ,   9 '\n    if refs_symbol.count(p12_str)==1:\n        i123=refs_symbol.find(p12_str)//6\n    elif refs_symbol.count(p23_str)==1:\n        i123=refs_symbol.find(p23_str)//6-1\n    else:\n        errors.append(f'bad refs_symbol |{refs_symbol}|;')                                \n    if refs_symbol.count(p89_str)>=1:\n        i789=refs_symbol.find(p89_str)//6-1\n    else:\n        errors.append(f'bad refs_symbol |{refs_symbol}|;')\n    if errors: #anchor residues not identified\n        return pdb,p_pdbnum,_pdbnum_to_tails(p_pdbnum),False    \n    left_tail=i123\n    right_tail=pep_len-i789-3\n    core_len=pep_len-left_tail-right_tail\n    \n    if (cl=='I') and (-1<=left_tail<=9) and (0<=right_tail) and (8<=core_len<=18):\n        new_pdbnum=_make_pep_pdbnums_I(pep_len,left_tail,right_tail)\n    elif (cl=='II') and (core_len==9):\n        new_pdbnum=_make_pep_pdbnums_II(pep_len,left_tail)\n    else:\n        return pdb,p_pdbnum,_pdbnum_to_tails(p_pdbnum),False        \n    renum_dict=dict(zip(p_pdbnum,new_pdbnum))\n    pdb_new=[]\n    for line in pdb.split('\\n'):\n        if line.startswith(('ATOM','HETATM','TER')):\n            chain=line[21]\n            pdbnum=line[22:27]\n            if chain=='P':                \n                new_line=line[:22]+renum_dict[pdbnum]+line[27:]                \n            else:\n                new_line=line            \n        else:\n            new_line=line\n        pdb_new.append(new_line)\n    return '\\n'.join(pdb_new),new_pdbnum,_pdbnum_to_tails(new_pdbnum),True", "\n#rmsds\ndef compute_rmsds(pdb,pdb_id):\n    '''\n    compute pep and mhc rmsds;\n    pep rmsds computed over all residues for cl I and over 0.9-9 for cl II;\n    mhc rmsds computed over all residues;\n    okay with missing residues at the tails, e.g. pdb has 'AAYGILGFVFTL' and pdb_id has 'AA(gap)GILGFVFTL'\n    '''    \n    chainmaps={'I':[['M','M']],'II':[['M','M'],['N','N']]}\n    structure,_=pdb_tools.parse_pdb_from_str(pdb,'modeled')\n    pepseq=''.join([pdb_tools.aa_dict.get(x.get_resname(),'X') for x in structure['P'].get_residues()])\n    true_pdb_path=true_pdb_dir+'/'+pdb_id+'.pdb'\n    structure_ref,_=pdb_tools.parse_pdb(true_pdb_path,'true')\n    pepseq_ref=''.join([pdb_tools.aa_dict.get(x.get_resname(),'X') for x in structure_ref['P'].get_residues()])\n    structure_dict=pdb_tools.get_structure_dict(structure,False)\n    pdbnum_current=['P'+x for x in structure_dict['P'].keys()] \n    structure_ref_dict=pdb_tools.get_structure_dict(structure_ref,False)\n    pdbnum_true=['P'+x for x in structure_ref_dict['P'].keys()]\n    if 'N' in structure_ref_dict:\n        cl='II'\n    else:\n        cl='I'    \n    pdb_tools.superimpose_by_chainmap(structure,structure_ref,chainmaps[cl],CA_only=True,verbose=False)\n    #align peptide sequences, make resmap                    \n    y=pairwise2.align.globalms(pepseq,pepseq_ref,match=1,mismatch=-1,open=-1,extend=-1)[0]\n    i1,i2=0,0\n    resmap=[]\n    for i,x in enumerate(zip(y.seqA,y.seqB)):\n        if x[0]!='-' and x[1]!='-':\n            resmap.append([pdbnum_current[i1],pdbnum_true[i2]])\n        if x[0]!='-':\n            i1+=1\n        if x[1]!='-':\n            i2+=1\n    if cl=='II': #restrict to ext core\n        resmap=[a for a in resmap if (a[1]>='P   09') and (a[1]<='P   9 ') and not re.search('[a-z]',a[1])]    \n    pep_rmsd=pdb_tools.rmsd_by_resmap(structure,structure_ref,resmap,allow_missing_res=True,verbose=False)\n    mhc_rmsd=pdb_tools.rmsd_by_chainmap(structure,structure_ref,chainmaps[cl],verbose=False) \n    return {'pep_CA':pep_rmsd['CA'],'pep_all':pep_rmsd['all'],'mhc_CA':mhc_rmsd['CA'],'mhc_all':mhc_rmsd['all']}", ""]}
{"filename": "tfold_patch/__init__.py", "chunked_list": [""]}
{"filename": "tfold_patch/tfold_pdb_tools.py", "chunked_list": ["#Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2021-2022\n\nimport os\nimport io\nimport pickle\nimport re\n\nimport numpy as np\nimport Bio.PDB as PDB", "import numpy as np\nimport Bio.PDB as PDB\n\naa_dict={'ARG':'R','HIS':'H','LYS':'K','ASP':'D','GLU':'E','SER':'S','THR':'T','ASN':'N','GLN':'Q','CYS':'C',\n         'GLY':'G','PRO':'P','ALA':'A','VAL':'V','ILE':'I','LEU':'L','MET':'M','PHE':'F','TYR':'Y','TRP':'W'}\n\n#### parsing ####\n\ndef parse_pdb_from_str(pdb,name):\n    pdb_handle=io.StringIO(pdb)\n    pdb_parser=PDB.PDBParser(PERMISSIVE=False,QUIET=True)\n    structure=pdb_parser.get_structure(name,pdb_handle)[0]\n    header=pdb_parser.get_header()    \n    return structure,header  ", "def parse_pdb_from_str(pdb,name):\n    pdb_handle=io.StringIO(pdb)\n    pdb_parser=PDB.PDBParser(PERMISSIVE=False,QUIET=True)\n    structure=pdb_parser.get_structure(name,pdb_handle)[0]\n    header=pdb_parser.get_header()    \n    return structure,header  \ndef parse_pdb(filename,name=None):\n    if name is None: #'/../../X.pdb' -> 'X'\n        name=filename.split('/')[-1].split('.')[0]        \n    pdb_parser=PDB.PDBParser(PERMISSIVE=False,QUIET=True)\n    structure=pdb_parser.get_structure(name,filename)[0]\n    header=pdb_parser.get_header()    \n    return structure,header   ", "\ndef _int_or_repl(x,replacement=-100):\n    '''convert to int or return replacement (default -100)'''\n    try:\n        return int(x)\n    except ValueError:\n        return replacement \ndef _atom_to_chain_pdbnum(a):\n    '''takes Bio.PDB atom, returns Cnnnni'''\n    chain,res_id,___=a.get_full_id()[-3:] #note: can be len 5 or len 4 (first entry is pdb_id, drops upon copying)\n    _,num,ins=res_id\n    num=_int_or_repl(num)\n    return '{:1s}{:4d}{:1s}'.format(chain,num,ins)", "def get_structure_dict(structure,include_coords,keep_hetero=True):\n    '''\n    takes a Bio.PDB object with get_atoms method;\n    if include_coords: returns dict {chain:{pdbnum:{atom_name:array_xyz,..},..},..},\n    otherwise:         returns dict {chain:{pdbnum:[atom_name,..],..},..};    \n    if keep_hetero (default True), keeps hetero res/atoms, otherwise drops them; #SWITCHED TO DEFAULT TRUE!!!\n    (always drops waters)\n    '''\n    structure_dict={}\n    for a in structure.get_atoms():\n        chain,res_id,atom_id=a.get_full_id()[-3:]\n        het,num,ins=res_id\n        if not (het.strip()=='W'): #drop waters\n            atomname,_=atom_id\n            pdbnum='{:4d}{:1s}'.format(_int_or_repl(num),ins)\n            if keep_hetero or not het.strip():\n                if include_coords:\n                    structure_dict.setdefault(chain,{}).setdefault(pdbnum,{})[atomname]=a.get_coord()\n                else:                \n                    structure_dict.setdefault(chain,{}).setdefault(pdbnum,[]).append(atomname)\n    return structure_dict    ", "\n#### maps ####\n\ndef chainmap_to_resmap(structure1,structure2,chainmap,verbose=False):\n    '''\n    takes two structures and a chainmap, e.g. [['M','N'],['M','M'], ['A','A'], ['P','P']]; \n    returns resmap which matches residues with identical pdbnums in each chain pair,\n    e.g. [['M1070 ','N1070 '],['P   5 ','P   5 ']]\n    '''    \n    structure1_dict=get_structure_dict(structure1,include_coords=False)\n    structure2_dict=get_structure_dict(structure2,include_coords=False)\n    resmap=[]\n    for x,y in chainmap:\n        res1=structure1_dict.get(x)\n        res2=structure2_dict.get(y)\n        if (res1 is not None) and (res2 is not None):\n            res1=set(res1.keys())\n            res2=set(res2.keys())\n            res_both=res1&res2\n            delta1=res1-res_both\n            delta2=res2-res_both\n            if verbose:\n                if delta1:\n                    print(f'res {delta1} present in structure 1 chain {x} but missing in structure 2 chain {y};')\n                if delta2:\n                    print(f'res {delta2} present in structure 2 chain {y} but missing in structure 1 chain {x};')\n            for r in res_both:\n                resmap.append([x+r,y+r])\n        elif verbose:\n            if res1 is None:\n                print(f'chain {x} missing in structure 1;')\n            if res2 is None:\n                print(f'chain {y} missing in structure 2;')    \n    return resmap", "def resmap_to_atommap(structure1,structure2,resmap,CA_only=False,allow_missing_res=False,verbose=False):\n    '''\n    if allow_missing_res==False, will raise error when residues from resmap are missing in structure,\n    otherwise will skip those residue pairs\n    '''\n    structure1_dict=get_structure_dict(structure1,include_coords=False)\n    structure2_dict=get_structure_dict(structure2,include_coords=False)\n    atoms1=[]\n    atoms2=[]    \n    for x,y in resmap:\n        chain1=x[0]\n        pdbnum1=x[1:]\n        chain2=y[0]\n        pdbnum2=y[1:]\n        #assume resnum was properly generated. If not, raise errors\n        if (chain1 not in structure1_dict) or (chain2 not in structure2_dict):\n            raise ValueError('defective resmap: chains missing in structure;')        \n        res1_dict=structure1_dict[chain1]\n        res2_dict=structure2_dict[chain2]\n        if (pdbnum1 not in res1_dict) or (pdbnum2 not in res2_dict):\n            if allow_missing_res:\n                continue\n            else:\n                raise ValueError('defective resmap: pdbnums missing in structure;')\n        atom_names1=res1_dict[pdbnum1]\n        atom_names2=res2_dict[pdbnum2]        \n        if CA_only:\n            if ('CA' in atom_names1) and ('CA' in atom_names2):\n                atoms1.append((x,'CA'))\n                atoms2.append((y,'CA'))\n            elif verbose:\n                if 'CA' not in atom_names1:\n                    print(f'CA missing in structure 1 residue {x}')\n                if 'CA' not in atom_names2:\n                    print(f'CA missing in structure 2 residue {y}')\n        else:            \n            atoms_both=set(atom_names1)&set(atom_names2)\n            for a in atoms_both:\n                atoms1.append((x,a))\n                atoms2.append((y,a)) \n    #make atommap with atom objects\n    atoms=[atoms1,atoms2]    \n    atommap=[[None,None] for x in atoms1]\n    for i,structure in enumerate([structure1,structure2]):\n        for a in structure.get_atoms():\n            x=_atom_to_chain_pdbnum(a)\n            if (x,a.name) in atoms[i]:\n                ind=atoms[i].index((x,a.name))\n                atommap[ind][i]=a    \n    return atommap", "\n#### distances and contacts ####\n\ndef distance2_matrix(x1,x2):\n    '''\n    takes two non-empty np arrays of coordinates.\n    Returns d_ij^2 matrix\n    '''    \n    delta_x=np.tile(x1[:,np.newaxis,:],[1,len(x2),1])-np.tile(x2[np.newaxis,:,:],[len(x1),1,1])\n    return np.sum(delta_x**2,axis=2)", "\n#### superimposing ####\n\n#a quick fix for a bug in transforming disordered atoms\n#(from https://github.com/biopython/biopython/issues/455)\n#NOTE: only keeps position A of disordered atoms and drops the rest\ndef get_unpacked_list_patch(self):\n    '''\n    Returns all atoms from the residue;\n    in case of disordered, keep only first alt loc and remove the alt-loc tag\n    '''\n    atom_list = self.get_list()\n    undisordered_atom_list = []\n    for atom in atom_list:\n        if atom.is_disordered():\n            atom.altloc=\" \"\n            undisordered_atom_list.append(atom)\n        else:\n            undisordered_atom_list.append(atom)\n    return undisordered_atom_list", "PDB.Residue.Residue.get_unpacked_list=get_unpacked_list_patch\ndef superimpose_by_resmap(structure1,structure2,resmap,CA_only=True,allow_missing_res=False,verbose=False):\n    '''\n    superimpose structure1 onto structure2 according to given resmap;\n    resmap should be a list of pairs ['Cnnnni','Cnnnni'] of corresponding residues;\n    if CA_only=True (default), only uses CA atoms;    \n    if allow_missing_res (default False), does not raise error when residue in resmap are missing in structure;\n    transforms structure1 in place; returns rmsd\n    '''\n    atommap=resmap_to_atommap(structure1,structure2,resmap,CA_only,allow_missing_res,verbose)    \n    if verbose:\n        print(f'superimposing on {len(atommap)} atoms...')     \n    atoms1,atoms2=zip(*atommap)\n    sup=PDB.Superimposer()\n    sup.set_atoms(atoms2,atoms1)\n    sup.apply(structure1)\n    return sup.rms            ", "def superimpose_by_chainmap(structure1,structure2,chainmap,CA_only=True,verbose=False):\n    '''\n    superimpose structure1 onto structure2 according to a chainmap;\n    chainmap is e.g. [['M','N'],['M','M'], ['A','A'], ['P','P']]; matching pdbnums in each pair of chains used;    \n    if CA_only=True (default), only uses CA atoms;    \n    transforms structure1 in place; returns rmsd    \n    '''    \n    resmap=chainmap_to_resmap(structure1,structure2,chainmap,verbose)\n    rmsd=superimpose_by_resmap(structure1,structure2,resmap,CA_only,verbose)\n    return rmsd", "    \n#### rmsd ####\ndef rmsd_by_resmap(structure1,structure2,resmap,allow_missing_res=False,verbose=False):\n    '''\n    compute rmsds (CA and all-atom) according to resmap;\n    note: resmap should be list, not zip!\n    does not superimpose!\n    '''\n    result={}\n    for name in ['CA','all']:\n        CA_only=name=='CA'        \n        atommap=resmap_to_atommap(structure1,structure2,resmap,CA_only=CA_only,\n                                  allow_missing_res=allow_missing_res,verbose=verbose) \n        if verbose:\n            print(f'rmsd_{name} over {len(atommap)} atoms...')\n        d2s=[]\n        for a,b in atommap:\n            delta=a.get_coord()-b.get_coord()\n            d2s.append(np.dot(delta,delta))\n        result[name]=np.average(d2s)**0.5\n    return result        ", "def rmsd_by_chainmap(structure1,structure2,chainmap,verbose=False):\n    '''\n    compute rmsds (CA and all-atom) according to chainmap;\n    does not superimpose!\n    '''\n    resmap=chainmap_to_resmap(structure1,structure2,chainmap,verbose=verbose)\n    if verbose:\n        print(f'rmsd over {len(resmap)} residues...')\n    return rmsd_by_resmap(structure1,structure2,resmap,verbose)\n", ""]}
{"filename": "tfold_patch/tfold_pipeline.py", "chunked_list": ["#Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2021-2022\n\n# Copyright 2021 DeepMind Technologies Limited\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0", "#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Functions for building the input features for the AlphaFold model.\"\"\"", "\n\"\"\"Functions for building the input features for the AlphaFold model.\"\"\"\n\nimport os\nfrom typing import Any, Mapping, MutableMapping, Optional, Sequence, Union\nfrom absl import logging\nfrom alphafold.common import residue_constants\nfrom alphafold.data import msa_identifiers\nfrom alphafold.data import parsers\nfrom alphafold.data import templates", "from alphafold.data import parsers\nfrom alphafold.data import templates\nimport numpy as np\n\n#overwrite a function in templates so that can pass any filename instead of 4-letter pdbid\n#(also set chain to 'A' always)\ndef _get_pdb_id_and_chain(hit: parsers.TemplateHit):\n    return hit.name,'A'\ntemplates._get_pdb_id_and_chain=_get_pdb_id_and_chain\n", "templates._get_pdb_id_and_chain=_get_pdb_id_and_chain\n\n# Internal import (7716).\n\nFeatureDict = MutableMapping[str, np.ndarray]\n\ndef make_sequence_features(sequences,chain_break_shift) -> FeatureDict:\n    \"\"\"Constructs a feature dict of sequence features.\"\"\"    \n    sequence=''.join(sequences)\n    num_res=len(sequence)\n    features={}\n    features['aatype']=residue_constants.sequence_to_onehot(sequence=sequence,\n                                                            mapping=residue_constants.restype_order_with_x,\n                                                            map_unknown_to_x=True)\n    features['between_segment_residues']=np.zeros((num_res,),dtype=np.int32)\n    features['domain_name']=np.array(['some_protein'.encode('utf-8')],dtype=np.object_)\n    resindex=np.arange(num_res)+np.concatenate([np.ones(len(seq))*i*chain_break_shift for i,seq in enumerate(sequences)])\n    features['residue_index']=np.array(resindex,dtype=np.int32)\n    features['seq_length']=np.array([num_res]*num_res, dtype=np.int32)\n    features['sequence']=np.array([sequence.encode('utf-8')], dtype=np.object_)\n    return features", "\ndef read_and_preprocess_msas(sequences,msas):\n    '''\n    input is a list of sequences and a list of dicts {chain_number:path_to_msa_for_chain};\n    read a3m MSA files, combine chains, add gaps for missing chains, add sequence as the first entry\n    '''    \n    sequence=''.join(sequences)\n    n_chains=len(sequences)    \n    msas_str=['\\n'.join(('>joined_target_sequence',sequence))]        \n    for msa in msas:        \n        msa_components=[]    #list of lists, each contains str sequences\n        for i in range(n_chains):\n            if i in msa:\n                msa_headers=[]       #list of lines, each starts with '>'; if multiple chains, headers from last entry will be kept\n                msa_sequences=[]\n                msa_filename=msa[i]\n                if not msa_filename.endswith('.a3m'):\n                    raise ValueError('MSAs must be in the a3m format')                \n                with open(msa_filename) as f:\n                    s=f.read().split('\\n')\n                for line in s:\n                    if line: #drop empty lines\n                        if line.startswith('>'):\n                            msa_headers.append(line)\n                        else:\n                            msa_sequences.append(line)\n                n_sequences=len(msa_sequences)\n                msa_components.append(msa_sequences)\n            else:\n                msa_components.append(None)\n        for i in range(n_chains):\n            if i not in msa:\n                msa_components[i]=['-'*len(sequences[i])]*n_sequences\n        #verify length is the same in MSAs for all chains and in headers\n        assert len(set([len(x) for x in msa_components]+[len(msa_headers)]))==1, 'components of an MSA are of unequal length;'\n        #join sequences for different chains, interlace with headers\n        msa_sequences=[''.join(x) for x in zip(*msa_components)]\n        msas_str.append('\\n'.join([a for x in zip(msa_headers,msa_sequences) for a in x]))                          \n    return [parsers.parse_a3m(msa) for msa in msas_str]", "\ndef make_msa_features(msas) -> FeatureDict:\n    \"\"\"Constructs a feature dict of MSA features.\"\"\"\n    if not msas:\n        raise ValueError('At least one MSA must be provided.')\n    int_msa=[]\n    deletion_matrix=[]\n    uniprot_accession_ids=[]\n    species_ids=[]\n    seen_sequences=set()\n    for msa_index,msa in enumerate(msas):\n        if not msa:\n            raise ValueError(f'MSA {msa_index} must contain at least one sequence.')\n        for sequence_index, sequence in enumerate(msa.sequences):\n            if sequence in seen_sequences:\n                continue\n            seen_sequences.add(sequence)\n            int_msa.append([residue_constants.HHBLITS_AA_TO_ID[res] for res in sequence])\n            deletion_matrix.append(msa.deletion_matrix[sequence_index])\n            identifiers=msa_identifiers.get_identifiers(msa.descriptions[sequence_index])\n            uniprot_accession_ids.append(identifiers.uniprot_accession_id.encode('utf-8'))\n            species_ids.append(identifiers.species_id.encode('utf-8'))\n    num_res=len(msas[0].sequences[0])\n    num_alignments=len(int_msa)\n    features={}\n    features['deletion_matrix_int']=np.array(deletion_matrix, dtype=np.int32)\n    features['msa']=np.array(int_msa, dtype=np.int32)\n    features['num_alignments']=np.array([num_alignments]*num_res, dtype=np.int32)\n    features['msa_uniprot_accession_identifiers'] = np.array(uniprot_accession_ids, dtype=np.object_)\n    features['msa_species_identifiers']=np.array(species_ids, dtype=np.object_)\n    return features", "\nclass DataPipeline:\n    \"\"\"assembles the input features.\"\"\"\n    def __init__(self,template_featurizer: templates.TemplateHitFeaturizer,chain_break_shift: int):    \n        self.template_featurizer=template_featurizer   \n        self.chain_break_shift  =chain_break_shift\n    def process(self,sequences,msas,template_hits) -> FeatureDict:  \n        #sequence features, incl. chain shifts\n        sequence_features=make_sequence_features(sequences,self.chain_break_shift)                          \n        #msa features, incl. adding gaps for chains\n        msas=read_and_preprocess_msas(sequences,msas)\n        msa_features=make_msa_features(msas)\n        #template features                \n        hhsearch_hits=[]        \n        for x in template_hits:\n            hhsearch_hits.append(parsers.TemplateHit(**x))          \n        input_sequence=''.join(sequences)\n        templates_result=self.template_featurizer.get_templates(query_sequence=input_sequence,hits=hhsearch_hits)\n        logging.info('Final (deduplicated) MSA size: %d sequences.',msa_features['num_alignments'][0])\n        logging.info('Total number of templates (NB: this can include bad '\n                     'templates and is later filtered to top 4): %d.',\n                     templates_result.features['template_domain_names'].shape[0])\n        return {**sequence_features, **msa_features, **templates_result.features}", ""]}
{"filename": "tfold_patch/ref_structures/__init__.py", "chunked_list": [""]}
{"filename": "tfold/config.py", "chunked_list": ["\n### SET THESE DIRECTORIES ################################################################################\n\n#tfold data folder\n#e.g. '/data/vmikhayl/tfold-release/data'\ndata_dir=#\n\n#a tmp folder to store data from blastp alignments\nseq_tools_tmp_dir=#\n##########################################################################################################", "seq_tools_tmp_dir=#\n##########################################################################################################\n##########################################################################################################\n##########################################################################################################\n\n\n\n\n\n", "\n\n\n#seq_tools\nseq_tools_data_dir=data_dir+'/seq_tools'                  #path to data\n\n#TFold comes with a python wrapper for netMHCpan-4.1/IIpan-3.4,4.0. \n#If you want to use it, set variables below. Otherwise, these variables aren't be used.\n\n#netmhc_tools", "\n#netmhc_tools\n# path to netMHCpan-4.1 and netMHCIIpan-4.1\nnetmhcpanI_dir ='/home/vmikhayl/netMHCpan-4.1'  \nnetmhcpanII_dir='/home/vmikhayl/netMHCIIpan-4.1'\nnetmhcpanII_old_dir='/home/vmikhayl/netMHCIIpan-3.2'\n\n#path to templates\ntemplate_source_dir=data_dir+'/experimental_structures/processed_updated'\n", "template_source_dir=data_dir+'/experimental_structures/processed_updated'\n\n#seqnn settings\nseqnn_params={'max_core_len_I':12,    #peptide input length is this +2  #(core truncation, indep from reg number)\n              'max_pep_len_I' :15,    #to set max number of registers   #(reg number)\n              'max_pep_len_II':25,    #to set max number of registers\n              'n_mhc_I':26,           #mhc pseudoseq length\n              'n_mhc_II':30,          #mhc pseudoseq length\n              'n_tail_bits':3         #n bits to encode tails; shared between cl 1 and 2\n             } ", "              'n_tail_bits':3         #n bits to encode tails; shared between cl 1 and 2\n             } \n\n#seqnn params, weights and model lists\nseqnn_obj_dir=data_dir+'/obj/seqnn'\n\n#parameters for making AlphaFold inputs\naf_input_params={'I':\n                 {\n                  'templates_per_register':20,", "                 {\n                  'templates_per_register':20,\n                  'pep_gap_penalty':1,\n                  'mhc_cutoff':20,  \n                  'score_cutoff':None,\n                  'kd_threshold':10., \n                  'use_mhc_msa':False,\n                  'use_paired_msa':True,\n                  'tile_registers':False,\n                  'shuffle':False", "                  'tile_registers':False,\n                  'shuffle':False\n                 },\n                 'II':\n                 {\n                  'templates_per_register':20,\n                  'pep_gap_penalty':1,\n                  'mhc_cutoff':25,\n                  'score_cutoff':None,\n                  'kd_threshold':100.,  ", "                  'score_cutoff':None,\n                  'kd_threshold':100.,  \n                  'use_mhc_msa':False,\n                  'use_paired_msa':True, #added 2022-11-22 (discovery run_11)\n                  'tile_registers':False,\n                  'shuffle':False\n                 }\n                }\n\n", "\n"]}
{"filename": "tfold/__init__.py", "chunked_list": [""]}
{"filename": "tfold/structure_database/__init__.py", "chunked_list": [""]}
{"filename": "tfold/structure_database/process_pdbs.py", "chunked_list": ["#Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2021-2022\n\nimport numpy as np\nimport os\nimport pickle\nimport time\nimport re\n\nfrom tfold.utils import seq_tools, pdb_tools", "\nfrom tfold.utils import seq_tools, pdb_tools\nseq_tools.load_mhcs()\nseq_tools.load_tcrs()\n\naa_set=set(list('ACDEFGHIKLMNPQRSTVWY'))\n\n#### blossum score thresholds for blast hits ####\n\nblossum_thresholds={'MHC_1_A':250,'B2M':300,'MHC_2_A':400,'MHC_2_B':400,", "\nblossum_thresholds={'MHC_1_A':250,'B2M':300,'MHC_2_A':400,'MHC_2_B':400,\n                    'TCR_A_V':300,'TCR_B_V':300,'TCR_G_V':300,'TCR_D_V':300,'TCR_A/D_V':300}\n\n#### peptide numbering conventions ####\n\n#class I\n#l(core)     numbering\n#l=8         1 2 3 4 5   7 8 9\n#l=9         1 2 3 4 5 6 7 8 9", "#l=8         1 2 3 4 5   7 8 9\n#l=9         1 2 3 4 5 6 7 8 9\n#10<=l<=18   1 2 3 4 5 5.1-5.9 6 7 8 9\n#insertion order: [5.1], [5.1,5.9], [5.1,5.2,5.9], ... ,[5.1,..,5.9]\n#tails\n# ...,0.7,0.8,0.9, [core], 10,11,...\n#left tail max len 9\n\n#class II\n# ...,0.7,0.8,0.9,1,..,9,10,11,... anchored to template on res 9", "#class II\n# ...,0.7,0.8,0.9,1,..,9,10,11,... anchored to template on res 9\n#left tail max len 9\n#e.g.:\n#  V   S   K WRMATPLLM  Q  A  L\n#0.7 0.8 0.9 123456789 10 11 12\n#formally, the core is 1-9, but 0.8 and 0.9 seem to align well\n\n#class I - class II correspondence\n#   123 456789      II", "#class I - class II correspondence\n#   123 456789      II\n#VSKWRM ATPLLM QAL  II\n#    GLCTLVAML       I\n#    123456789       I\n#     x    xxx      (res that superimpose well)\n\ncl_I_resnum_template_left=['   0{:1d}'.format(x) for x in range(1,10)]+['{:4d} '.format(x) for x in range(1,6)]\ncl_I_resnum_template_insert=['   5{:1d}'.format(x) for x in range(1,10)]\ncl_I_resnum_template_right=['{:4d} '.format(x) for x in range(6,10000)]", "cl_I_resnum_template_insert=['   5{:1d}'.format(x) for x in range(1,10)]\ncl_I_resnum_template_right=['{:4d} '.format(x) for x in range(6,10000)]\ncl_II_resnum_template=['   0{:1d}'.format(x) for x in range(1,10)]+['{:4d} '.format(x) for x in range(1,10000)]\n\n#### fix gaps, blast and realign ####\n\ndef _repair_chain_gaps(chain_obj,seqres_seq):\n    '''\n    repair gaps in chains using SEQRES data; \n    takes NUMSEQ object for chain and a str for the corresponding seqres sequence;\n    returns updated NUMSEQ object with small-letter gaps, and error list\n    '''\n    chain=chain_obj.info['chain']           \n    errors=[]        \n    y=seq_tools.pairwise2.align.globalmd(chain_obj.seq(),seqres_seq,match=1,mismatch=-10,\n                                   openA=0,extendA=0,openB=-15,extendB=-15,penalize_end_gaps=(False,False))\n    max_score=max([y0.score for y0 in y])\n    y=[y0 for y0 in y if y0.score==max_score] #restrict to max score (does align output non-max scores at all?)    \n    y1=seq_tools.pairwise2.align.globalmd(chain_obj.seq(),seqres_seq,match=1,mismatch=-10,\n                                   openA=-1,extendA=0,openB=-15,extendB=-15,penalize_end_gaps=(False,False))\n    y+=y1 #add alignments with openA penalty, to be sure they are never missed\n    datas=[]\n    n_errors=[]\n    for y0 in y:               \n        c_n_errors=0\n        seqA,seqB=y0.seqA,y0.seqB    \n        x=re.search('^-+',seqB)\n        if x:\n            i1=x.end()\n        else:\n            i1=0        \n        x=re.search('-+$',seqB)\n        if x:\n            i2=x.start()\n        else:\n            i2=len(seqB)   \n        seqA=seqA[i1:i2]\n        seqB=seqB[i1:i2]\n        data=chain_obj.data.copy()[i1:i2]\n        if '-' in seqB:            \n            errors.append(f'_repair_chain_gaps: gap in aligned seqres for chain {chain};')            \n            continue\n        if '-' not in seqA: #no gaps, can return result\n            datas.append(data)\n            n_errors.append(0)\n            break\n        i_current=-1        \n        x=re.search('^-+',seqA)    \n        if x:\n            n_left=x.end()\n        else:\n            n_left=0               \n        n_prev=data[0]['num']-n_left-1\n        n_prev_nongap=n_prev\n        new_seq=''\n        new_num=[]\n        new_ins=[]                \n        for i,x in enumerate(seqA):\n            if x=='-':\n                new_seq+=seqB[i].lower()\n                new_num.append(n_prev+1)  \n                new_ins.append('')\n                n_prev+=1\n            else:\n                i_current+=1\n                n_current=data[i_current]['num']\n                new_seq+=data[i_current]['seq']\n                new_num.append(n_current)  \n                new_ins.append(data[i_current]['ins'])                \n                if ((n_prev==n_prev_nongap) or (n_prev==n_current-1) or \n                    (n_prev==n_current-2 and n_prev_nongap<0 and n_current>0)):\n                    pass\n                else: #gap in numbering persists  \n                    c_n_errors+=1                    \n                n_prev=data[i_current]['num'] \n                n_prev_nongap=n_prev \n        datas.append(seq_tools.NUMSEQ(seq=new_seq,num=new_num,ins=new_ins).data)\n        n_errors.append(c_n_errors)\n        if c_n_errors==0:            \n            break\n    if len(datas)==0:\n        errors.append(f'_repair_chain_gaps: no good alignments found for chain {chain};')\n        return chain_obj,errors\n    i=np.argmin(n_errors)\n    e=np.min(n_errors)\n    if e>0:\n        errors.append(f'_repair_chain_gaps: min error num {e} for chain {chain};')\n    data=datas[i]\n    chain_obj=seq_tools.NUMSEQ(data=data,info=chain_obj.info.copy())     \n    return chain_obj,errors       ", "\ndef process_chain(chain,require_mhc_for_pep=True):\n    '''\n    takes a NUMSEQ object for chain;\n    returns a list of realigned objects and a list of peptide candidates;\n    also returns a debug dict which includes blast hits (all and filtered);\n    also returns a list of non-fatal errors\n    '''\n    #note: gaps made back into uppercase when passing to blast and realign;\n    #then gap positions made small in realigned fragments\n    errors=[]    \n    debug={}\n    ###blast search and filtering###\n    try:\n        hits=seq_tools.blast_prot(chain.seq().upper()) \n    except Exception as e:\n        c=chain.info['chain']\n        errors.append(f'blast search for chain {c} resulted in error {e};')\n        hits=[]    \n    #drop TRJ hits since hard to filter and not used anyway\n    hits=[h for h in hits if not (h['protein'].startswith('TCR') and h['protein'].endswith('J'))]\n    #keep top 5 results for each protein type                \n    hits=seq_tools.filter_blast_hits_to_multiple(hits,keep=5) \n    debug['hits_prefiltered']=hits\n    #impose thresholds\n    hits=[h for h in hits if h['score']>=blossum_thresholds[h['protein']]]                \n    #filter overlapping hits\n    hits=seq_tools.filter_blast_hits_for_chain(hits,threshold=-10)\n    debug['hits_filtered']=hits\n    #sort hits left to right\n    ind=np.argsort([h['query_start'] for h in hits])\n    hits=[hits[i] for i in ind]    \n    ###realign hits###\n    includes_mhc=False\n    includes_tcr=False        \n    aligned_objects=[]    \n    fragment_boundaries=[]\n    for i,h in enumerate(hits):        \n        #boundaries of hit and next hit (need for tcrs)\n        il=h['query_start']\n        ir=h['query_end']  \n        if i<len(hits)-1:\n            il_next=hits[i+1]['query_start']\n        else:\n            il_next=len(chain.data)                \n        #realign hit                \n        if h['protein'].startswith('MHC'):             \n            includes_mhc=True                    \n            query=chain.get_fragment_by_i(il,ir)\n            try:                \n                mhc,il0,ir0=seq_tools.mhc_from_seq(query.seq().upper(),return_boundaries=True)\n                source=query.get_fragment_by_i(il0,ir0)\n                if mhc.seq()!=source.seq().upper():\n                    raise ValueError('MHC-source seq mismatch!')\n                aligned_objects.append([mhc,source])  \n                bdry_left=il+il0\n                if len(query.data)-1-ir0>20: #assume constant region present, use i from blast\n                    bdry_right=ir\n                else:                        #assume no contant region present, use i from realigned\n                    bdry_right=il+ir0                \n                fragment_boundaries.append([bdry_left,bdry_right])\n            except Exception as e:\n                c=chain.info['chain']\n                errors.append(f'MHC realign for chain {c} resulted in error: {e}')\n        elif h['protein'].startswith('TCR'):\n            includes_tcr=True\n            query=chain.get_fragment_by_i(il,il_next-1) #use all space to the next hit, to accomodate for TRJ            \n            try:\n                tcr,il0,ir0=seq_tools.tcr_from_seq(query.seq().upper(),return_boundaries=True)\n                source=query.get_fragment_by_i(il0,ir0)\n                if tcr.seq()!=source.seq().upper():\n                    raise ValueError('TCR-source seq mismatch!')\n                aligned_objects.append([tcr,source]) \n                fragment_boundaries.append([il+il0,il+ir0])\n            except Exception as e:\n                c=chain.info['chain']\n                errors.append(f'TCR realign for chain {c} resulted in error: {e}')\n        elif h['protein'].startswith('B2M'):\n            includes_mhc=True #B2M is a sure sign there is MHC in the structure\n            fragment_boundaries.append([il,ir])\n        else:    #no other protein types currently\n            pass\n    ###make peptide candidates###\n    #note: pep can be tethered to TCR! (e.g. 4may: to the beginning of tcr chain);    \n    peptide_candidates=[]\n    #indices: from blast, except right end of mhc when there is constant region (check mismatch)\n    if includes_mhc or not require_mhc_for_pep:\n        fragment_boundaries=[[-2,-1]]+fragment_boundaries+[[len(chain.data),len(chain.data)+1]]        \n        for x,y in zip(fragment_boundaries[:-1],fragment_boundaries[1:]):\n            fragment=chain.get_fragment_by_i(x[1]+1,y[0]-1)\n            if len(fragment.data)>0:\n                linker=[False,False]\n                if x[1]+1>0:\n                    linker[0]=True\n                if y[0]-1<len(chain.data)-1:\n                    linker[1]=True\n                fragment.info['linker']=linker\n                peptide_candidates.append(fragment)\n    #put gaps into realigned objects\n    for x_new,x_old in aligned_objects:\n        assert len(x_new.data)==len(x_old.data), 'source and realigned fragment length mismatch;'\n        for i in range(len(x_new.data)):\n            if x_old.data[i]['seq'].islower():\n                x_new.data[i]['seq']=x_new.data[i]['seq'].lower()            \n    return aligned_objects,peptide_candidates,includes_mhc,includes_tcr,debug,errors   ", "\n#### assemble complexes ####\ndef _ungap_pair(x):\n    return [x[0].ungap_small(),x[1].ungap_small()]\ndef _get_fragment_coords(fragment,structure_dict,pdbnum_l=None,pdbnum_r=None,CA_only=False):\n    '''\n    takes a fragment which is [obj_aligned,obj_source], a structure_dict with coords, \n    and a pair of boundaries pdbnum_l, pdbnum_r (default None, boundary not imposed);\n    pdbnums should be according to numbering in obj_aligned (i.e. canonical);    \n    returns an array of coordinates of atoms in the fragment;    \n    if set flag CA_only, only CA atom coords; otherwise (default) all atoms\n    '''\n    pdbnum_l=pdbnum_l or fragment[0].data['pdbnum'][0]\n    pdbnum_r=pdbnum_r or fragment[0].data['pdbnum'][-1]\n    ind=((pdbnum_l<=fragment[0].data['pdbnum'])&(fragment[0].data['pdbnum']<=pdbnum_r))\n    pdbnums=fragment[1].data['pdbnum'][ind]\n    chain=fragment[1].info['chain']        \n    coords=[]\n    for x in pdbnums:        \n        if CA_only:\n            if 'CA' in structure_dict[chain][x]:  #quietly skip missing CAs\n                coords.append(structure_dict[chain][x]['CA'])\n        else:\n            coords+=list(structure_dict[chain][x].values())\n    return np.array(coords)", "\ndef assemble_mhcs_II(mhcs,structure_dict,cutoff=7.):    \n    '''\n    takes a list of aligned fragments for cl II mhcs and a structure dict;\n    returns a list of matched pairs and a list of non-fatal errors\n    '''\n    if len(mhcs)==0:\n        return [],[]\n    errors=[]    \n    mhcs_a=[]\n    mhcs_b=[]\n    for x in mhcs:\n        chain=x[0].info['chain']\n        if chain=='A':\n            mhcs_a.append(x)\n        elif chain=='B':\n            mhcs_b.append(x)\n        else:\n            raise ValueError(f'mhc chain {chain} not recognized;')        \n    l_mhcs_b=len(mhcs_b)\n    mhcs_assembled=[]\n    mhcs_a_dropped=[] #keep ones for which no pair was found\n    for x in mhcs_a: \n        x_ungapped=_ungap_pair(x)\n        coord_x=_get_fragment_coords(x_ungapped,structure_dict,'   4 ','  11 ')\n        ds=[]        \n        for i,y in enumerate(mhcs_b):  \n            y_ungapped=_ungap_pair(y)\n            coord_y=_get_fragment_coords(y_ungapped,structure_dict,'1004 ','1011 ')            \n            ds.append(np.min(pdb_tools.distance2_matrix(coord_x,coord_y))**0.5)\n        ds=np.array(ds)        \n        ind=np.nonzero(ds<cutoff)[0]\n        if len(ind)==1:\n            i=ind[0]\n            mhcs_assembled.append([x,mhcs_b[i]])\n            del mhcs_b[i]\n        else:\n            mhcs_a_dropped.append(x)\n            if len(ind)==0:\n                pass #do nothing; error added later            \n            else:\n                bad_chain=x[1].info['chain']\n                errors.append(f'MHC II assembly: multiple matches for chain {bad_chain};')\n    if len(mhcs_a_dropped)>0:\n        errors.append(f'MHC II assembly: {len(mhcs_a_dropped)} of {len(mhcs_a)} chains A unpaired;')\n    if len(mhcs_b)>0: #all that remain are dropped        \n        errors.append(f'MHC II assembly: {len(mhcs_b)} of {l_mhcs_b} chains B unpaired;')    \n    return mhcs_assembled,errors", "def assemble_tcrs(tcrs,structure_dict,cutoff=10.):    \n    '''\n    takes a list of aligned fragments for tcrs and a structure dict;\n    returns a list of pairs which are either matched [tcr1,tcr2] or single chain [tcr1,None],\n    and a list of non-fatal errors;\n    within pairs, chains ordered according to A<B<G<D\n    '''\n    if len(tcrs)==0:\n        return [],[]\n    chain_order=['A','B','G','D']\n    errors=[]            \n    tcrs_assembled=[]\n    used=[False]*len(tcrs) #which already paired    \n    for i,x in enumerate(tcrs):\n        if not used[i]:         \n            x_ungapped=_ungap_pair(x)\n            coord_x=_get_fragment_coords(x_ungapped,structure_dict,' 115 ',' 118 ')\n            if len(coord_x)==0: # e.g. 3omz has too many gaps\n                continue\n            ds=[]        \n            for j,y in enumerate(tcrs[i+1:]):\n                if not used[i+1+j]:            \n                    y_ungapped=_ungap_pair(y)\n                    coord_y=_get_fragment_coords(y_ungapped,structure_dict,'  49 ','  53 ') \n                    if len(coord_y)==0: # e.g. 3omz has too many gaps\n                        continue\n                    ds.append(np.min(pdb_tools.distance2_matrix(coord_x,coord_y))**0.5)\n                else:\n                    ds.append(1000.)                   \n            ds=np.array(ds)        \n            ind=np.nonzero(ds<cutoff)[0]\n            if len(ind)==1:\n                k=ind[0]\n                y=tcrs[i+1+k]\n                if chain_order.index(x[0].info['chain'])<chain_order.index(y[0].info['chain']):\n                    tcrs_assembled.append([x,y])\n                else:\n                    tcrs_assembled.append([y,x])\n                used[i]=True\n                used[i+1+k]=True                \n            else:                \n                if len(ind)==0:\n                    pass #do nothing; error added later            \n                else:\n                    bad_chain=x[1].info['chain']\n                    errors.append(f'TCR assembler: multiple matches for chain {bad_chain};')\n    tcrs_unpaired=[[x,None] for i,x in enumerate(tcrs) if not used[i]]\n    n_unpaired=len(tcrs_unpaired)\n    n_total=len(tcrs)\n    if n_unpaired>0:\n        errors.append(f'TCR assembler: {n_unpaired} of {n_total} TCR chains unpaired;')    \n    return tcrs_assembled+tcrs_unpaired,errors", "\ndef _make_resmap_from_aligned_obj(obj,ref_structure_dict,chain):    \n    pdbnum1=[obj[1].info['chain']+x for x in obj[1].data['pdbnum']]\n    pdbnum2=[chain+x for x in obj[0].data['pdbnum']]\n    if len(pdbnum1)!=len(pdbnum2):\n        raise ValueError('len mismatch in _make_resmap_from_aligned_obj')\n    resmap=list(zip(pdbnum1,pdbnum2))\n    #print('resmap prefiltered',resmap)\n    resmap=[x for x in resmap if x[1][1:] in ref_structure_dict[chain]] #filter\n    #print('resmap filtered',resmap)\n    return resmap", "def _default_for_none(x,default):\n    if x is None:\n        return default\n    else:\n        return x        \ndef _make_pep_pdbnums_I(i123,core_len,pep_len):\n    assert i123<10, \"_make_pep_pdbnums_I:i123 too large;\"      \n    assert pep_len>=8, \"_make_pep_pdbnums_I:pep_len too small;\"\n    assert core_len>=8, \"_make_pep_pdbnums_I:pep core_len too small;\"\n    left_part=cl_I_resnum_template_left[max(0,9-i123):]\n    l_insert=max(0,core_len-9)\n    l_insert_right=l_insert//2\n    l_insert_left=l_insert-l_insert_right\n    center_part=cl_I_resnum_template_insert[:l_insert_left]+cl_I_resnum_template_insert[9-l_insert_right:]\n    right_part=cl_I_resnum_template_right[max(0,9-core_len):]\n    pdbnum=left_part+center_part+right_part        \n    return pdbnum[:pep_len]", "def _make_pep_pdbnums_II(i789,len_pep):\n    assert i789<16, \"_make_pep_pdbnums_II: i789 too large;\"\n    pdbnum=cl_II_resnum_template[max(15-i789,0):]\n    return pdbnum[:len_pep]\ndef _renumber_pep(pep,i123,i789,cl):        \n    errors=[]    \n    #check full 1-9 core for cl II\n    if (cl=='II') and (i789-i123!=6):\n        errors.append(f'unconventional core length for cl II: i123 {i123}, i789 {i789};')\n    #cut on the left, if necessary\n    if i123>9:\n        #errors.append(f'pep with i123={i123} too long, cutting on the left;')\n        pep.data=pep.data[i123-9:]\n        pep.info['linker'][0]|=True  \n        i789-=(i123-9)\n        i123=9            \n    #make aligned object    \n    if cl=='I':\n        pep_len=len(pep.data)\n        core_len=i789-i123+3\n        if pep_len<8 or core_len<8:\n            raise ValueError(f'cl I pep too short: pep_len {pep_len}, core_len {core_len}')        \n        pdbnums=_make_pep_pdbnums_I(i123,core_len,pep_len)\n    elif cl=='II':                \n        pdbnums=_make_pep_pdbnums_II(i789,len(pep.data))\n    else:\n        raise ValueError('pep renumbering: MHC class not recognized;')    \n    pep0=seq_tools.NUMSEQ(seq=pep.seq(),pdbnum=pdbnums,info=pep.info.copy())    \n    pep0.info['chain']='P'    \n    #cut on the right\n    ind=np.nonzero(pep0.data['num']>=20)[0]\n    if len(ind)>0:\n        i_r=np.amin(ind)\n        tail=set(list(pep0.seq()[i_r:]))\n        if tail&aa_set:\n            pep0.info['linker'][1]=True\n            pep.info['linker'][1]=True   #(not necessary)\n        pep0.data=pep0.data[:i_r]\n        pep.data=pep.data[:i_r]                   \n    return [pep0,pep],errors", "def _get_CA_coord_w_default(res):\n    if res is None:\n        return np.array([0.,0.,0.])\n    elif 'CA' in res:\n        return res['CA']\n    else:\n        return np.average(list(res.values()),axis=0)    \ndef assemble_pmhcs(mhcs,pep_candidates,structure,pep9_threshold=2.):\n    '''\n    finds pep-mhc pairs, renumbers pep\n    '''\n    p12_str='   1 ,   2 '\n    p23_str='   2 ,   3 '\n    p89_str='   8 ,   9 '\n    errors=[]\n    dump_info={}\n    if len(mhcs)==0:\n        return [],{},[]\n    cl=mhcs[0][0][0].info['class']\n    #load ref structures\n    if cl=='I':       \n        ref_filename='./data/experimental_structures/ref_structures/3mrePA___.pdb'\n    elif cl=='II':\n        ref_filename='./data/experimental_structures/ref_structures/4x5wCAB__.pdb'        \n    else:\n        raise ValueError(f'mhc class {cl} not recognized;')    \n    ref_structure,_=pdb_tools.parse_pdb(ref_filename)       \n    ref_structure_dict=pdb_tools.get_structure_dict(ref_structure,True)\n    ref_pep_resnums,ref_pep_coords=[],[]\n    for k,v in ref_structure_dict['P'].items():        \n        ref_pep_resnums.append(k)\n        ref_pep_coords.append(v['CA'])\n    ref_pep_resnums=np.array(ref_pep_resnums)\n    ref_pep_coords=np.array(ref_pep_coords)\n    #assemble\n    pmhcs=[]\n    mhcs_unpaired=[]\n    for m in mhcs:\n        #make mhc resmap        \n        if cl=='I':\n            resmap=_make_resmap_from_aligned_obj(_ungap_pair(m[0]),ref_structure_dict,'M')            \n        else:\n            resmap=_make_resmap_from_aligned_obj(_ungap_pair(m[0]),ref_structure_dict,'M')\n            resmap+=_make_resmap_from_aligned_obj(_ungap_pair(m[1]),ref_structure_dict,'N')          \n        for i,p in enumerate(pep_candidates):\n            #superimpose\n            pdb_tools.superimpose_by_resmap(structure,ref_structure,resmap)            \n            structure_dict=pdb_tools.get_structure_dict(structure,True)\n            #indices of gap res\n            gap=np.array([resletter.islower() for resletter in p.data['seq']])\n            #get coords; note: use av of all atoms as default for missing CA            \n            pep_coords=np.array([_get_CA_coord_w_default(structure_dict[p.info['chain']].get(x)) for x in p.data['pdbnum']])                        \n            d2matrix=pdb_tools.distance2_matrix(pep_coords,ref_pep_coords)            \n            d2matrix_reduced=d2matrix[~gap,:]   #matrix for actual non-gap residues            \n            if np.prod(d2matrix_reduced.shape)==0 or np.min(d2matrix_reduced)>pep9_threshold**2:                \n                continue\n            closest_refs=[]\n            ds=[]\n            for i in range(len(p.data['pdbnum'])):\n                if gap[i]:\n                    closest_refs.append('-----')\n                    ds.append(10000.)\n                else:\n                    i0=np.argmin(d2matrix[i,:])\n                    ds.append(d2matrix[i,i0]**0.5)\n                    closest_refs.append(ref_pep_resnums[i0])\n            dump_info.setdefault('peptide_maps',[]).append([cl,closest_refs,ds])            \n            refs_symbol=','.join(closest_refs)\n            if refs_symbol.count(p12_str)==1:\n                i123=refs_symbol.find(p12_str)//6\n            elif refs_symbol.count(p23_str)==1:\n                i123=refs_symbol.find(p23_str)//6-1\n            else:\n                errors.append(f'pmhc assembler: bad refs_symbol |{refs_symbol}|;')\n                continue                                    \n            if refs_symbol.count(p89_str)>=1: #okay if more than one; then take first occurence; (see e.g. 2qri)\n                i789=refs_symbol.find(p89_str)//6-1\n            else:\n                errors.append(f'pmhc assembler: bad refs_symbol |{refs_symbol}|;')\n                continue            \n            try:\n                p,c_error=_renumber_pep(p,i123,i789,cl)\n                errors+=c_error\n            except Exception as e:\n                errors.append(f'error {e} in pep renumbering;')\n                continue\n            pmhcs.append([p,m])                               \n            break\n        else:            \n            mhcs_unpaired.append(m)\n    if len(mhcs_unpaired)>0:\n        errors.append(f'pmhc assembler: cl {cl}, {len(mhcs_unpaired)} of {len(mhcs)} mhcs left unpaired;')    \n    return pmhcs,dump_info,errors", "\ndef assemble_complexes(pmhcs,tcrs,structure_dict,cutoff=10.):\n    '''\n    takes aligned and matched fragments for pmhcs and tcrs;\n    returns a list of complexes (incl. pure pmhcs and pure two-chain or single-chain tcrs) and a list of non-fatal errors\n    '''\n    n_pmhcs=len(pmhcs)\n    paired_tcrs=[x for x in tcrs if not(x[1] is None)]\n    n_paired_tcrs=len(paired_tcrs)\n    unpaired_tcrs=[x[0] for x in tcrs if (x[1] is None)]\n    n_proper_complexes_max=min(n_pmhcs,n_paired_tcrs)\n    complexes=[[None,None,None,x,None] for x in unpaired_tcrs]       #add unpaired tcrs\n    if n_pmhcs==0:\n        complexes+=[[None,None,None,x[0],x[1]] for x in paired_tcrs] #add paired tcrs if no pmhcs\n    if n_paired_tcrs==0:\n        complexes+=[[x[0],x[1][0],x[1][1],None,None] for x in pmhcs] #add pmhcs if no paired tcrs\n    if n_proper_complexes_max==0: #if no full complexes to assemble, exit here\n        return complexes,[],False \n    errors=[]               \n    complexes_assembled=[]\n    pmhcs_dropped=[] #keep ones for which no tcr was found\n    for x in pmhcs: \n        pep=_ungap_pair(x[0])\n        coord_x=_get_fragment_coords(pep,structure_dict,'   1 ','   9 ') #pep core        \n        ds=[]        \n        for i,y in enumerate(paired_tcrs):\n            yb_ungapped=_ungap_pair(y[1])\n            coord_y=_get_fragment_coords(yb_ungapped,structure_dict,' 105 ',' 117 ') #tcr-b cdr3            \n            ds.append(np.min(pdb_tools.distance2_matrix(coord_x,coord_y))**0.5)\n        ds=np.array(ds)           \n        ind=np.nonzero(ds<cutoff)[0]\n        if len(ind)==1:\n            i=ind[0]\n            complexes_assembled.append([x[0],x[1][0],x[1][1],*paired_tcrs[i]])\n            del paired_tcrs[i]\n        else:\n            pmhcs_dropped.append(x)\n            if len(ind)==0:\n                pass #do nothing; error added later            \n            else:                \n                errors.append(f'full complex assembly: multiple tcrs for same pmhc;')\n    #add whatever remains unpaired\n    for x in pmhcs_dropped:        \n        complexes.append([x[0],x[1][0],x[1][1],None,None])\n    for x in paired_tcrs:\n        complexes.append([None,None,None,x[0],x[1]])\n    n_assembled=len(complexes_assembled)\n    if n_assembled<n_proper_complexes_max:\n        errors.append(f'full complex assembly: assembled {n_assembled} complexes '\\\n                      f'from {n_pmhcs} pmhcs and {n_paired_tcrs} paired tcrs;')\n    if n_assembled==0 and n_proper_complexes_max>0:\n        try_again=True\n    else:\n        try_again=False\n    return complexes+complexes_assembled,errors,try_again", "\n### get and apply transformations ###\n\ndef _get_transformations(pdb_lines):   \n    '''\n    reads REMARK 350, gives a list of transformation dicts {chain:matrix} for each biomolecule;\n    only the first BIOMT used; (multiple BIOMT means software suggests to repeat the same chain);\n    each matrix is np.array([[r11,r12,r13,v1],...]), i.e. combines rotation r and shift v\n    '''\n    matrices=[]    \n    s_iter=iter(pdb_lines)\n    for line in s_iter:\n        if line.startswith('ATOM'): #past the REMARKS section\n            break\n        if line.startswith('REMARK 350 BIOMOLECULE:'):\n            matrices.append({})\n        if line.startswith('REMARK 350 APPLY THE FOLLOWING TO CHAINS:'):\n            matrix=[]\n            chains=line.split(':')[1].replace(' ','').split(',')\n            for j in range(3):  #only the first BIOMT used (\n                line=next(s_iter)\n                if line.startswith('REMARK 350                    AND CHAINS:'):\n                    chains+=line.split(':')[1].replace(' ','').split(',')\n                    line=next(s_iter)\n                assert line.startswith(f'REMARK 350   BIOMT{j+1}'), 'BIOMT not found'\n                matrix.append([float(a) for a in line.split()[4:]])\n            matrix=np.array(matrix)\n            for c in chains:\n                matrices[-1][c]=matrix        \n    return matrices", "def _transform(structure,transformation):\n    '''\n    applies transformation in place\n    '''\n    for chain in structure.get_chains():\n        matrix=transformation.get(chain.id)\n        if not (matrix is None):            \n            chain.transform(matrix[:,:-1].T,matrix[:,-1]) #need rot.x+transl, while Bio.PDB transform does x.rot+transl\n            \n### add hetero 3-letter data to protein ###\ndef _add_hetero(fragment,sequences,seqres_hetero):\n    ind=np.nonzero(np.isin(fragment[0].data['seq'],('x','X')))[0]\n    if len(ind)==0: #no hetero\n        fragment[0].info['hetero_res']={}\n        return fragment   \n    chain=fragment[1].info['chain']\n    hetero_res={}\n    for i in ind:\n        pdbnum=fragment[1].data[i]['pdbnum']\n        pdbnum_new=fragment[0].data[i]['pdbnum']\n        seq_het=sequences['hetero'].get(chain)\n        seq_unk=sequences['modified'].get(chain)        \n        if not (seq_het is None):\n            ind1=np.nonzero(seq_het.data['pdbnum']==pdbnum)[0]\n            if len(ind1)==1:                \n                hetero_res[pdbnum_new]=seq_het.data[ind1[0]]['seq']\n                continue\n        if not (seq_unk is None):\n            ind1=np.nonzero(seq_unk.data['pdbnum']==pdbnum)[0]\n            if len(ind1)==1:                \n                hetero_res[pdbnum_new]=seq_unk.data[ind1[0]]['seq']\n                continue\n        #for gaps, HETATM not present in sequences but present in seqres; aligning seqres to pdbnum is tedious,\n        #so just process cases when there is only one HETATM in seqres, and leave errors otherwise\n        if len(seqres_hetero[chain])==1:\n            hetero_res[pdbnum_new]=seqres_hetero[chain][0]\n            continue\n        raise ValueError(f'het for chain {chain} pdbnum {pdbnum} not found;')\n    fragment[0].info['hetero_res']=hetero_res\n    return fragment", "            \n### add hetero 3-letter data to protein ###\ndef _add_hetero(fragment,sequences,seqres_hetero):\n    ind=np.nonzero(np.isin(fragment[0].data['seq'],('x','X')))[0]\n    if len(ind)==0: #no hetero\n        fragment[0].info['hetero_res']={}\n        return fragment   \n    chain=fragment[1].info['chain']\n    hetero_res={}\n    for i in ind:\n        pdbnum=fragment[1].data[i]['pdbnum']\n        pdbnum_new=fragment[0].data[i]['pdbnum']\n        seq_het=sequences['hetero'].get(chain)\n        seq_unk=sequences['modified'].get(chain)        \n        if not (seq_het is None):\n            ind1=np.nonzero(seq_het.data['pdbnum']==pdbnum)[0]\n            if len(ind1)==1:                \n                hetero_res[pdbnum_new]=seq_het.data[ind1[0]]['seq']\n                continue\n        if not (seq_unk is None):\n            ind1=np.nonzero(seq_unk.data['pdbnum']==pdbnum)[0]\n            if len(ind1)==1:                \n                hetero_res[pdbnum_new]=seq_unk.data[ind1[0]]['seq']\n                continue\n        #for gaps, HETATM not present in sequences but present in seqres; aligning seqres to pdbnum is tedious,\n        #so just process cases when there is only one HETATM in seqres, and leave errors otherwise\n        if len(seqres_hetero[chain])==1:\n            hetero_res[pdbnum_new]=seqres_hetero[chain][0]\n            continue\n        raise ValueError(f'het for chain {chain} pdbnum {pdbnum} not found;')\n    fragment[0].info['hetero_res']=hetero_res\n    return fragment", "        \n#### process structure ####\n     \ndef process_structure(input_filename,output_dir):\n    '''\n    takes path to pdb file and path to output dir;\n    processes pdb file and saves new pdb files and other information\n    '''        \n    #make output directory tree\n    for x in ['/pdb','/proteins','/pdb_info','/debug']:\n        os.makedirs(output_dir+x,exist_ok=True)    \n    def dump(content,name): #for saving into '/info'\n        with open(output_dir+f'/debug/{pdb_id}/{name}.pckl','wb') as f:\n            pickle.dump(content,f) \n    errors=[]     #non-fatal errors    \n    pdb_id=input_filename.split('/')[-1].split('.')[0]\n    print(f'processing structure {pdb_id}...')\n    os.makedirs(output_dir+f'/debug/{pdb_id}',exist_ok=True)    \n    ###parse pdb       \n    #read pdb as text\n    with open(input_filename) as f:\n        pdb_lines=f.read().split('\\n') \n    #parse SEQRES lines to use in gap identification\n    seqres={}\n    seqres_hetero={} #keep 3-letter heteros here\n    for line in pdb_lines:\n        if line.startswith('SEQRES'):        \n            chain=line[11]\n            seqres.setdefault(chain,'')\n            seq=line[19:].split()\n            for x in seq:\n                if x not in pdb_tools.aa_dict:\n                    seqres_hetero.setdefault(chain,[]).append(x)\n            seq=''.join([pdb_tools.aa_dict.get(x) or 'X' for x in seq])                        \n            seqres[chain]+=seq\n    #read pdb into Bio.PDB objects\n    structure,header=pdb_tools.parse_pdb(input_filename,pdb_id)\n    pdb_info={}\n    pdb_info['deposition_date']=header.get('deposition_date') #yyyy-mm-dd (Bio.PDB parses into this format)\n    pdb_info['resolution']=header.get('resolution')\n    pdb_info['includes_mhc']=False #change to 'includes_antigen', set True if any nonTCR protein present (e.g. bacterial toxin)\n    pdb_info['includes_tcr']=False\n    sequences=pdb_tools.get_chain_sequences(structure)\n    mod_het={}    \n    mod_het['modified']=set(x for v in sequences['modified'].values() for x in v.data['seq']) or None\n    mod_het['het']=set(x for v in sequences['hetero'].values() for x in v.data['seq']) or None    \n    dump(mod_het,'mod_het')\n    ###fix gaps using SEQRES###\n    seqres_errors=[] #keep separately, filter in the end\n    for k,v in sequences['canonical'].items():        \n        seqres_seq=seqres.get(k)\n        if not (seqres_seq is None): #otherwise assume no gaps; probably just hetero atoms\n            v,c_error=_repair_chain_gaps(v,seqres_seq)\n            sequences['canonical'][k]=v\n            seqres_errors+=c_error            \n    ###blast and realign chains   \n    aligned_fragments=[]\n    pep_candidates_main=[]\n    pep_candidates_other=[]\n    chains_dump={}\n    for c,v in sequences['canonical'].items():            \n        if len(v.data)<20:   #probably peptide\n            v.info['linker']=[False,False]\n            pep_candidates_main.append(v)\n        elif len(v.data)<50: #maybe peptide, surely too short to blast/realign\n            v.info['linker']=[False,False]\n            pep_candidates_other.append(v)\n        else:\n            chain_aligned_fragments,chain_pep_candidates,includes_mhc,includes_tcr,chain_debug,chain_errors=process_chain(v)\n            aligned_fragments+=chain_aligned_fragments\n            pep_candidates_other+=chain_pep_candidates  \n            pdb_info['includes_mhc']|=includes_mhc\n            pdb_info['includes_tcr']|=includes_tcr\n            for k in chain_debug:                \n                chains_dump.setdefault(k,[])\n                chains_dump[k]+=chain_debug[k]\n            errors+=chain_errors \n    for k,v in chains_dump.items():\n        dump(v,k)        \n    if pep_candidates_main:\n        pep_candidates=pep_candidates_main\n    else:\n        pep_candidates=pep_candidates_other    \n    for p in pep_candidates: #mark pep candidates that have hetero atoms in their chains\n        if p.info['chain'] in sequences['hetero']:\n            p.info['hetero']=True\n        else:\n            p.info['hetero']=False\n    dump(aligned_fragments,'aligned_fragments')\n    dump(pep_candidates,'pep_candidates')\n    #stop if nothing realigned\n    if len(aligned_fragments)==0:\n        errors.append('no fragments realigned;')\n        with open(output_dir+f'/debug/{pdb_id}/errors.txt','w') as f:\n            f.write('\\n'.join(errors))\n        return None        \n    ###assemble parts\n    structure_dict=pdb_tools.get_structure_dict(structure,include_coords=True) #more convenient structure representation    \n    #sort fragments (MHC I, MHC II, TCR; assume no other kind appears)\n    mhcs_I=[]\n    mhcs_II=[]\n    tcrs=[]\n    for x in aligned_fragments:\n        cl=x[0].info.get('class')\n        if cl=='I':\n            mhcs_I.append(x)\n        elif cl=='II':\n            mhcs_II.append(x)\n        else:\n            tcrs.append(x)\n    #assemble mhcs\n    mhcs_II,c_errors=assemble_mhcs_II(mhcs_II,structure_dict)\n    errors+=c_errors\n    #for simplicity, make sure only one MHC class appears  \n    assert len(mhcs_I)*len(mhcs_II)==0, \"process_structure: cl I and II present;\"          \n    mhcs=[[x,None] for x in mhcs_I]+mhcs_II    \n    #assemble pmhcs    \n    pmhcs,dump_info,c_errors=assemble_pmhcs(mhcs,pep_candidates,structure)\n    for k,v in dump_info.items():\n        dump(v,k)\n    errors+=c_errors    \n    #assemble tcrs\n    tcrs,c_errors=assemble_tcrs(tcrs,structure_dict)\n    errors+=c_errors    \n    #assemble complexes\n    complexes,c_errors,try_again=assemble_complexes(pmhcs,tcrs,structure_dict)\n    apply_transform=False\n    if try_again: #apply transformations and see if any full complexes assembled\n        transformations=_get_transformations(pdb_lines)\n        for transformation in transformations:            \n            #keep the original structure untransformed, because transformations are relative to it\n            #doing transformation in place, because Bio.PDB.Entity.copy produces weird results:\n            #seems to sometimes apply a transformation? (Weird result when run as package, works normally\n            #when function run from notebook!) Maybe disordered atoms are the problem?\n            new_structure,_=pdb_tools.parse_pdb(input_filename,pdb_id) \n            _transform(new_structure,transformation) \n            new_structure_dict=pdb_tools.get_structure_dict(new_structure,include_coords=True)             \n            complexes,c_errors,try_again=assemble_complexes(pmhcs,tcrs,new_structure_dict)\n            if not try_again:                \n                apply_transform=True\n                break    \n    errors+=c_errors            \n    #check complex chains are in seqres\n    for c in complexes:\n        for cc in c:\n            if not (cc is None):\n                chain=cc[1].info['chain']\n                if chain not in seqres:\n                    errors.append(f'chain {chain} appears in complexes but not in seqres;')\n    #filter seqres errors: drop 'min error' errors for chains other than pep chain\n    pep_chains=[c[0][1].info['chain'] for c in complexes if c[0] is not None]    \n    for e in seqres_errors:\n        if (not e.startswith('_repair_chain_gaps: min error num')) or (e[-2] in pep_chains):\n            errors.append(e)            \n    #make renum dict and proteins, and add hetero 3-letter names on the way\n    proteins={}    \n    renum_dict={} #{Cnnnni(old):(structure_num,Cnnnni(new))}, assuming each res goes into <=1 structure    \n    new_chains=['P','M','N','A','B']\n    for i,x in enumerate(complexes):        \n        proteins[i]={}\n        for j,y in enumerate(x):\n            if not (y is None):    \n                try:\n                    y=_add_hetero(y,sequences,seqres_hetero)\n                except Exception as e:\n                    errors.append(f'adding hetero res: error {e};')\n                chain_old=y[1].info['chain']\n                chain_new=new_chains[j]\n                proteins[i][chain_new]=y[0].dump() #renumbered chain\n                pdbnum_old=y[1].data['pdbnum']                \n                pdbnum_new=y[0].data['pdbnum'] #no need to ungap!\n                assert len(pdbnum_old)==len(pdbnum_new),'pdbnum len mismatch'\n                for a,b in zip(pdbnum_old,pdbnum_new):\n                    renum_dict[chain_old+a]=(i,chain_new+b)\n    #make new pdbs; transform if necessary\n    new_pdbs={}\n    model_count=0\n    for line in pdb_lines:\n        if line.startswith('MODEL'): #only keep the first model (applies to 1bwm)            \n            if model_count>0:\n                break\n            model_count+=1\n        if line.startswith(('ATOM','HETATM','TER')): #add 'ANISOU'? but never need them\n            chain_pdbnum=line[21:27]\n            if chain_pdbnum in renum_dict:\n                complex_i,new_chain_pdbnum=renum_dict[chain_pdbnum]\n                new_line=line[:21]+new_chain_pdbnum+line[27:]\n                if apply_transform and not line.startswith('TER'):                   \n                    matrix=transformation.get(chain_pdbnum[0])\n                    if not (matrix is None):\n                        x=float(line[30:38])\n                        y=float(line[38:46])\n                        z=float(line[46:54])\n                        xyz=np.array([x,y,z])\n                        xyz_new=np.dot(matrix[:,:-1],xyz)+matrix[:,-1]\n                        xyz_new='{:8.3f}{:8.3f}{:8.3f}'.format(*xyz_new)\n                        new_line=new_line[:30]+xyz_new+new_line[54:]\n                new_pdbs.setdefault(complex_i,[]).append(new_line)\n    #save\n    for i,v in new_pdbs.items():\n        with open(output_dir+f'/pdb/{pdb_id}_{str(i)}.pdb','w') as f:\n            f.write('\\n'.join(v))\n    for i,v in proteins.items():\n        with open(output_dir+f'/proteins/{pdb_id}_{str(i)}.pckl','wb') as f:\n            pickle.dump(v,f)   \n    with open(output_dir+f'/pdb_info/{pdb_id}.pckl','wb') as f:\n            pickle.dump(pdb_info,f)   \n    with open(output_dir+f'/debug/{pdb_id}/errors.txt','w') as f:\n        f.write('\\n'.join(errors))", "                \n#### main ####\n\nif __name__=='__main__':               \n    from argparse import ArgumentParser\n    import csv\n    t0=time.time() \n    parser=ArgumentParser()\n    parser.add_argument('input_filename', type=str, help='path to input file')    \n    args=parser.parse_args()  \n    inputs=[]\n    with open(args.input_filename) as f:\n        f_csv=csv.reader(f,delimiter='\\t')\n        inputs=[x for x in f_csv]        \n    print(f'processing {len(inputs)} tasks...')\n    for x in inputs:\n        process_structure(*x)\n    print('finished {} tasks in {} s'.format(len(inputs),time.time()-t0))"]}
{"filename": "tfold/modeling/result_parse_tools.py", "chunked_list": ["#Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2019-2022\n\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport time\nimport re\n", "import re\n\n#from matplotlib import pyplot as plt\n#from sklearn.linear_model import LassoCV\n\nfrom tfold.utils import pdb_tools\nfrom tfold.config import data_dir\n\n#COLLECT RESULTS\ndef _get_pep_lddts(lddts,pdbnums):\n    '''\n    takes a np.array of predicted lddts and a list of pdbnums;\n    collects pdbnums for all pep residues\n    '''\n    return np.array([x for x in zip(pdbnums,lddts) if x[0][0]=='P'],dtype=[('pdbnum','<U6'),('lddt',float)])", "#COLLECT RESULTS\ndef _get_pep_lddts(lddts,pdbnums):\n    '''\n    takes a np.array of predicted lddts and a list of pdbnums;\n    collects pdbnums for all pep residues\n    '''\n    return np.array([x for x in zip(pdbnums,lddts) if x[0][0]=='P'],dtype=[('pdbnum','<U6'),('lddt',float)])\ndef _get_mhc_lddts(lddts,pdbnums):\n    '''\n    takes a np.array of predicted lddts and a list of pdbnums;\n    collects pdbnums for a set of MHC residues that typically contact the peptide;\n    (top 8 contacts for cl I, top 7 contacts for cl II);\n    '''\n    #detect cl\n    cl='I'\n    for x in pdbnums[::-1]:\n        if x[0]=='N':\n            cl='II'\n            break\n    reskeep={'I':['M   7 ','M  63 ','M  66 ','M  77 ','M1009 ','M1059 ','M1070 ','M1077 '],\n             'II':['M  61 ','M  70 ','M  77 ','N1059 ','N1073 ','N1076 ','N1077 ']}\n    return np.array([x for x in zip(pdbnums,lddts) if x[0] in reskeep[cl]],dtype=[('pdbnum','<U6'),('lddt',float)])", "                \ndef _seqnn_logkd_from_target_df(x,df_target):    \n    y=df_target[df_target['pmhc_id']==x['pmhc_id']].iloc[0]['seqnn_logkds_all']\n    logkd=np.log(5e4)\n    for y1 in y:\n        if y1['tail']==x['af_tails']:\n            logkd=y1['logkd']\n            break\n    return logkd\ndef parse_results(ctarget_dir):\n    #read inputs\n    t0=time.time()\n    df1={'pmhc_id':[],'model_id':[],'tpl_tails':[],'best_score':[],'best_mhc_score':[]}\n    for filename in os.listdir(ctarget_dir+'/inputs'):\n        with open(ctarget_dir+'/inputs/'+filename,'rb') as f:\n            c_input=pickle.load(f)\n        for x in c_input:\n            df1['pmhc_id'].append(x['target_id'])\n            df1['model_id'].append(x['current_id'])\n            df1['tpl_tails'].append(x['registers'][0]) #x['registers'] is a list of one element (assume no tiling)\n            df1['best_score'].append(x['best_score'])\n            df1['best_mhc_score'].append(x['best_mhc_score'])\n    df1=pd.DataFrame(df1)      \n    #read results        \n    df2={'pmhc_id':[],'model_id':[],'register_identified':[],'af_tails':[],'pep_lddt':[],'mhc_lddt':[]}\n    #check if rmsd is present\n    pmhc_id=os.listdir(ctarget_dir+'/outputs')[0]\n    pmhc_id=int(pmhc_id)\n    for result_filename in os.listdir(ctarget_dir+f'/outputs/{pmhc_id}'):      \n        if result_filename.endswith('.pkl'):\n            model_id=int(result_filename[:-4].split('_')[-1]) #assume use exactly one AF model\n            with open(ctarget_dir+f'/outputs/{pmhc_id}/{result_filename}','rb') as f:\n                c_result=pickle.load(f)\n            get_rmsd=('pep_CA' in c_result)\n            break\n    if get_rmsd:\n        df2.update({'pep_CA':[],'pep_all':[],'mhc_CA':[],'mhc_all':[]})\n    #collect\n    for pmhc_id in os.listdir(ctarget_dir+'/outputs'):     \n        pmhc_id=int(pmhc_id)\n        for result_filename in os.listdir(ctarget_dir+f'/outputs/{pmhc_id}'):      \n            if result_filename.endswith('.pkl'):\n                model_id=int(result_filename[:-4].split('_')[-1]) #assume use exactly one AF model\n                with open(ctarget_dir+f'/outputs/{pmhc_id}/{result_filename}','rb') as f:\n                    c_result=pickle.load(f)\n                df2['pmhc_id'].append(pmhc_id)\n                df2['model_id'].append(model_id)            \n                df2['register_identified'].append(c_result['pep_renumbered'])\n                df2['af_tails'].append(c_result['pep_tails'])\n                df2['pep_lddt'].append(_get_pep_lddts(c_result['plddt'],c_result['pdbnum_list']))\n                df2['mhc_lddt'].append(_get_mhc_lddts(c_result['plddt'],c_result['pdbnum_list']))\n                if get_rmsd:\n                    for k in ['pep_CA','pep_all','mhc_CA','mhc_all']:\n                        df2[k].append(c_result[k])\n    df2=pd.DataFrame(df2)    \n    #merge input and result dfs, add info from df_target\n    df_target=pd.read_pickle(ctarget_dir+'/target_df.pckl')\n    result_df=df1.merge(df2,left_on=['pmhc_id','model_id'],right_on=['pmhc_id','model_id'])\n    result_df=result_df.merge(df_target.drop(['templates','seqnn_logkds_all','seqnn_logkd','seqnn_tails'],axis=1),\n                              left_on='pmhc_id',right_on='pmhc_id')    \n    #add counts of registers in all models for given target\n    result_df=pd.merge(result_df,result_df.groupby('pmhc_id')['af_tails'].nunique(),left_on='pmhc_id',right_on='pmhc_id')\n    result_df=result_df.rename({'af_tails_x':'af_tails','af_tails_y':'af_n_reg'},axis=1)    \n    #add seqnn kd\n    result_df['seqnn_logkd']=result_df.apply(lambda x: _seqnn_logkd_from_target_df(x,df_target),axis=1) \n    #add 100-pLDDT score\n    result_df['score']=100-result_df['pep_lddt'].map(mean_pep_lddt)\n    #save\n    result_df.to_pickle(ctarget_dir+'/result_df.pckl')\n    print('{:4d} outputs collected in {:6.1f} s'.format(len(result_df),time.time()-t0))", "def parse_results(ctarget_dir):\n    #read inputs\n    t0=time.time()\n    df1={'pmhc_id':[],'model_id':[],'tpl_tails':[],'best_score':[],'best_mhc_score':[]}\n    for filename in os.listdir(ctarget_dir+'/inputs'):\n        with open(ctarget_dir+'/inputs/'+filename,'rb') as f:\n            c_input=pickle.load(f)\n        for x in c_input:\n            df1['pmhc_id'].append(x['target_id'])\n            df1['model_id'].append(x['current_id'])\n            df1['tpl_tails'].append(x['registers'][0]) #x['registers'] is a list of one element (assume no tiling)\n            df1['best_score'].append(x['best_score'])\n            df1['best_mhc_score'].append(x['best_mhc_score'])\n    df1=pd.DataFrame(df1)      \n    #read results        \n    df2={'pmhc_id':[],'model_id':[],'register_identified':[],'af_tails':[],'pep_lddt':[],'mhc_lddt':[]}\n    #check if rmsd is present\n    pmhc_id=os.listdir(ctarget_dir+'/outputs')[0]\n    pmhc_id=int(pmhc_id)\n    for result_filename in os.listdir(ctarget_dir+f'/outputs/{pmhc_id}'):      \n        if result_filename.endswith('.pkl'):\n            model_id=int(result_filename[:-4].split('_')[-1]) #assume use exactly one AF model\n            with open(ctarget_dir+f'/outputs/{pmhc_id}/{result_filename}','rb') as f:\n                c_result=pickle.load(f)\n            get_rmsd=('pep_CA' in c_result)\n            break\n    if get_rmsd:\n        df2.update({'pep_CA':[],'pep_all':[],'mhc_CA':[],'mhc_all':[]})\n    #collect\n    for pmhc_id in os.listdir(ctarget_dir+'/outputs'):     \n        pmhc_id=int(pmhc_id)\n        for result_filename in os.listdir(ctarget_dir+f'/outputs/{pmhc_id}'):      \n            if result_filename.endswith('.pkl'):\n                model_id=int(result_filename[:-4].split('_')[-1]) #assume use exactly one AF model\n                with open(ctarget_dir+f'/outputs/{pmhc_id}/{result_filename}','rb') as f:\n                    c_result=pickle.load(f)\n                df2['pmhc_id'].append(pmhc_id)\n                df2['model_id'].append(model_id)            \n                df2['register_identified'].append(c_result['pep_renumbered'])\n                df2['af_tails'].append(c_result['pep_tails'])\n                df2['pep_lddt'].append(_get_pep_lddts(c_result['plddt'],c_result['pdbnum_list']))\n                df2['mhc_lddt'].append(_get_mhc_lddts(c_result['plddt'],c_result['pdbnum_list']))\n                if get_rmsd:\n                    for k in ['pep_CA','pep_all','mhc_CA','mhc_all']:\n                        df2[k].append(c_result[k])\n    df2=pd.DataFrame(df2)    \n    #merge input and result dfs, add info from df_target\n    df_target=pd.read_pickle(ctarget_dir+'/target_df.pckl')\n    result_df=df1.merge(df2,left_on=['pmhc_id','model_id'],right_on=['pmhc_id','model_id'])\n    result_df=result_df.merge(df_target.drop(['templates','seqnn_logkds_all','seqnn_logkd','seqnn_tails'],axis=1),\n                              left_on='pmhc_id',right_on='pmhc_id')    \n    #add counts of registers in all models for given target\n    result_df=pd.merge(result_df,result_df.groupby('pmhc_id')['af_tails'].nunique(),left_on='pmhc_id',right_on='pmhc_id')\n    result_df=result_df.rename({'af_tails_x':'af_tails','af_tails_y':'af_n_reg'},axis=1)    \n    #add seqnn kd\n    result_df['seqnn_logkd']=result_df.apply(lambda x: _seqnn_logkd_from_target_df(x,df_target),axis=1) \n    #add 100-pLDDT score\n    result_df['score']=100-result_df['pep_lddt'].map(mean_pep_lddt)\n    #save\n    result_df.to_pickle(ctarget_dir+'/result_df.pckl')\n    print('{:4d} outputs collected in {:6.1f} s'.format(len(result_df),time.time()-t0))", "    \n#CONFIDENCE SCORES FROM LINEAR MODELS\ndef mean_pep_lddt(x):\n    '''mean over core'''\n    reskeep=['P{:4d} '.format(i) for i in range(1,10)]+['P   5{:1d}'.format(i) for i in range(1,10)]             \n    return np.mean(x['lddt'][np.isin(x['pdbnum'],reskeep)])\ndef lddt_score(x):\n    return np.log(100-mean_pep_lddt(x))\ndef _get_features1(x):\n    '''signs chosen so that higher score -> higher rmsd'''\n    cl=x['class']\n    features={}    \n    features['len']=len(x['pep'])                        #pep length\n    features['mhc_score']=x['best_mhc_score']\n    features['n_reg']=x['af_n_reg']                      #number of registers in AF output   \n    features['seqnn_logkd']=x['seqnn_logkd']             #seqnn logkd for af predicted register\n    features['register_identified']=-float(x['register_identified'])\n    features['mean_mhc_lddt']=100-np.mean(x['mhc_lddt']['lddt'])\n    #mean pep lddt\n    pep_lddt=x['pep_lddt']                                          \n    reskeep=['P{:4d} '.format(i) for i in range(1,10)]+['P   5{:1d}'.format(i) for i in range(1,10)]             \n    features['mean_pep_lddt']=100-np.mean(pep_lddt['lddt'][np.isin(pep_lddt['pdbnum'],reskeep)])                                          \n    #per-residue lddt for pep core\n    lddts_core=[]\n    #res 1    \n    if 'P   1 ' in pep_lddt['pdbnum']:\n        lddts_core+=list(pep_lddt['lddt'][pep_lddt['pdbnum']=='P   1 '])\n    else:\n        lddts_core+=list(pep_lddt['lddt'][pep_lddt['pdbnum']=='P   2 '])\n    #res 2-4\n    lddts_core+=list(pep_lddt['lddt'][('P   2 '<=pep_lddt['pdbnum'])&(pep_lddt['pdbnum']<='P   4 ')])\n    #res 5, including possible insertions\n    lddts_core.append(np.mean(pep_lddt['lddt'][('P   5 '<=pep_lddt['pdbnum'])&(pep_lddt['pdbnum']<='P   59')]))\n    #res 6 (missing in canonical 8mers)\n    if 'P   6 ' in pep_lddt['pdbnum']:\n        lddts_core+=list(pep_lddt['lddt'][pep_lddt['pdbnum']=='P   6 '])\n    else:\n        lddts_core+=list(pep_lddt['lddt'][pep_lddt['pdbnum']=='P   7 '])\n    #res 7-9\n    lddts_core+=list(pep_lddt['lddt'][('P   7 '<=pep_lddt['pdbnum'])&(pep_lddt['pdbnum']<='P   9 ')])\n    for i,l in enumerate(lddts_core):\n        features[f'P{i+1}']=100-l    \n    return features", "def _get_features1(x):\n    '''signs chosen so that higher score -> higher rmsd'''\n    cl=x['class']\n    features={}    \n    features['len']=len(x['pep'])                        #pep length\n    features['mhc_score']=x['best_mhc_score']\n    features['n_reg']=x['af_n_reg']                      #number of registers in AF output   \n    features['seqnn_logkd']=x['seqnn_logkd']             #seqnn logkd for af predicted register\n    features['register_identified']=-float(x['register_identified'])\n    features['mean_mhc_lddt']=100-np.mean(x['mhc_lddt']['lddt'])\n    #mean pep lddt\n    pep_lddt=x['pep_lddt']                                          \n    reskeep=['P{:4d} '.format(i) for i in range(1,10)]+['P   5{:1d}'.format(i) for i in range(1,10)]             \n    features['mean_pep_lddt']=100-np.mean(pep_lddt['lddt'][np.isin(pep_lddt['pdbnum'],reskeep)])                                          \n    #per-residue lddt for pep core\n    lddts_core=[]\n    #res 1    \n    if 'P   1 ' in pep_lddt['pdbnum']:\n        lddts_core+=list(pep_lddt['lddt'][pep_lddt['pdbnum']=='P   1 '])\n    else:\n        lddts_core+=list(pep_lddt['lddt'][pep_lddt['pdbnum']=='P   2 '])\n    #res 2-4\n    lddts_core+=list(pep_lddt['lddt'][('P   2 '<=pep_lddt['pdbnum'])&(pep_lddt['pdbnum']<='P   4 ')])\n    #res 5, including possible insertions\n    lddts_core.append(np.mean(pep_lddt['lddt'][('P   5 '<=pep_lddt['pdbnum'])&(pep_lddt['pdbnum']<='P   59')]))\n    #res 6 (missing in canonical 8mers)\n    if 'P   6 ' in pep_lddt['pdbnum']:\n        lddts_core+=list(pep_lddt['lddt'][pep_lddt['pdbnum']=='P   6 '])\n    else:\n        lddts_core+=list(pep_lddt['lddt'][pep_lddt['pdbnum']=='P   7 '])\n    #res 7-9\n    lddts_core+=list(pep_lddt['lddt'][('P   7 '<=pep_lddt['pdbnum'])&(pep_lddt['pdbnum']<='P   9 ')])\n    for i,l in enumerate(lddts_core):\n        features[f'P{i+1}']=100-l    \n    return features", "\n#REDUCE\ndef s_min(df,e_name,dell_e=False,how='min'):\n    '''\n    takes a dataframe and a name e_name of the column;\n    returns a reduced dataframe with one row -- the first row with minimal score in column e_name;\n    if dell_e (default False), remove the column e_name\n    '''    \n    if how=='min':\n        df=df[df[e_name]==df[e_name].min()].iloc[0]\n    elif how=='max':\n        df=df[df[e_name]==df[e_name].max()].iloc[0]\n    else:\n        raise ValueError('value for --how not recognized')\n    if dell_e:\n        del df[e_name]\n    return df", "\ndef reduce_to_best(df,aggr_by,t_key,how='min'):\n    '''\n    takes a dataframe to reduce;\n    takes a list aggr_by of columns by which to aggregate, e.g. ['m target','m template'];\n    takes column name t_key;\n    reduces to rows with minimal (default; how='max' for maximal) values in column t_key in each group\n    '''\n    x=df.copy()\n    f=lambda d: s_min(d,t_key,how=how)\n    x=x.groupby(aggr_by).apply(f)\n    return x", "def reduce_to_best_all(dfs,aggr_by,t_key,how='min'):\n    return apply_to_dict(reduce_to_best,dfs,[aggr_by,t_key,how])    \n\ndef add_weighted_score(df,weight,score_name='s'):\n    '''\n    takes a df and a weight (dict, missing weights imputed as zeros);\n    adds (in place) a column with the weighted score; \n    column name is score_name (default 's')\n    '''\n    full_weight=[]\n    for c in df.columns:\n        if c in weight:\n            full_weight.append(weight[c])\n        else:\n            full_weight.append(0.)\n    full_weight=np.array(full_weight)    \n    x=df.dot(full_weight)  \n    if 'intercept' in weight:\n        x+=weight['intercept']\n    df[score_name]=x", "def reduce_by_weighted_score(df,aggr_by,weight):\n    '''\n    takes a dataframe to reduce;\n    takes a list aggr_by of columns by which to aggregate, e.g. ['m target','m template'];    \n    takes a weight vector which should be a dict with keys being some columns of df; \n    (other weights assumed 0);\n    reduces in each group to one (!) row with min weighted score;\n    returns the reduced dataframe\n    '''\n    x=df.copy()\n    add_weighted_score(x,weight) \n    f=lambda d: s_min(d,'s',dell_e=True)    \n    x=x.groupby(aggr_by).apply(f)\n    #restore dtype to int for some columns\n    c_int=['m target','m template','m model_num']    \n    for k in c_int:\n        x[k]=x[k].astype(int) #float->int\n    return x   ", "def reduce_by_weighted_score_all(dfs,aggr_by,weight):\n    return apply_to_dict(reduce_by_weighted_score,dfs,[aggr_by,weight]) \n\n#SUMMARIZE AND PLOT\n\ndef summarize_columns(df,column_list,s0='',nl=6,nr=2):\n    '''\n    print summary for columns in format #column: mean std min max; column: ...#\n    takes s0 to print in the beginning of the line (default '')\n    nl and nr determine printing format as {:(nl+nr+1).nr}\n    '''\n    sf='{:'+str(nl+nr+1)+'.'+str(nr)+'f}'\n    s=s0    \n    for c in column_list:\n        s+=(' '.join([sf]*4)+';  ').format(df[c].mean(),df[c].std(),df[c].min(),df[c].max())        \n    print(s)", "def summarize_columns_all(dfs,column_list=['d pep_ca','d pep_all','d mhc_ca','d mhc_all'],\n                          q_list=None,nl=6,nr=2):\n    '''\n    runs summarize columns for a dict of dfs; splits by q_group;\n    if q_list provided, uses it, otherwise all q groups;\n    nl and nr determine printing format as {:(nl+nr+1).nr}\n    '''\n    def shorten(k): #shorten some alg names to fit in line\n        k1=k\n        k1=re.sub('fast_relax','fr',k1)\n        k1=re.sub('custom','c',k1)\n        return k1            \n    nmax=max([len(shorten(k)) for k in dfs])\n    if not q_list:\n        q_list=list(dfs.values())[0]['t q_group'].unique()\n        q_list.sort()\n    k_list=np.sort(list(dfs.keys()))\n    print('; '.join(column_list))\n    print('mean, std, min, max')\n    for q in q_list:\n        print(q)\n        for k in k_list:\n            df=dfs[k]\n            summarize_columns(df[df['t q_group']==q],column_list,('{:'+str(nmax)+'s}: ').format(shorten(k)),nl,nr)            \n        print()                                      ", "    \ndef plot_x_vs_all(df,xkey,n_columns=6,scale=3):\n    '''\n    make scatterplots of column xkey vs all other columns in a dataframe (except 'm ...');\n    optionally takes n_columns and scale for plot layout\n    '''\n    n=len(df.columns)-3-1    \n    n_rows=n//n_columns+int(n%n_columns)  \n    plt.figure(figsize=(scale*n_columns,scale*n_rows))\n    ii=1\n    for i,c in enumerate(df.columns):\n        if c[0]!='m' and c!=xkey:\n            plt.subplot(n_rows,n_columns,ii)\n            plt.scatter(df[c],df[xkey])\n            plt.title(c)\n            ii+=1\n    plt.show()", "    \ndef plot_correlations(df,size=7.):\n    '''\n    plots correlation matrix between columns (excluding 'm ...');\n    optionally, takes plot size (default 7)\n    #note: 'invalid value encountered in true_divide': division by zero variance, ignore;\n    '''\n    columns_keep=[c for c in df.columns if c[0]!='m']    \n    corr_matr=np.zeros((len(columns_keep),len(columns_keep)))\n    for i,x in enumerate(columns_keep):\n        for j,y in enumerate(columns_keep):\n            corr_matr[i,j]=np.corrcoef(df[x],df[y])[0,1]\n    plt.figure(figsize=(size,size))\n    plt.imshow(corr_matr,cmap='bwr',vmin=-1.,vmax=1.)\n    plt.xticks(range(len(columns_keep)),columns_keep,rotation=90)\n    plt.yticks(range(len(columns_keep)),columns_keep)\n    plt.colorbar()\n    plt.show()", "        \ncolors=['grey','blue','green','black','red','yellow','navy','lightgray','brown','purple']            \ndef plot_histograms_all(dfs,column_list=None,cumulative=0,n_columns=6,scale=7):\n    '''\n    takes a dict of dfs;\n    plots histograms for columns in column_list (if None, all columns except 'm ...');\n    (splits by q_group);\n    takes n_columns and scale for plot layout;\n    '''\n    titles=list(dfs.keys())\n    df0=dfs[titles[0]]\n    q_list=df0['t q_group'].unique()\n    q_list.sort()\n    if not column_list:\n        column_list=[c for c in df0.columns if c[0]!='m']\n    n=len(column_list)\n    n_columns=min(n_columns,n) #don't make more columns than necessary\n    n_rows=n//n_columns+int(n%n_columns)  \n    for q in q_list:\n        print('q group {}:'.format(q))        \n        plt.figure(figsize=(scale*n_columns,scale*n_rows))        \n        for i,c in enumerate(column_list):            \n            plt.subplot(n_rows,n_columns,i+1)            \n            for j,t in enumerate(titles):\n                df=dfs[t]\n                df=df[df['t q_group']==q] #restrict to q_group\n                plt.hist(df[c],histtype='step',density=True,color=colors[j],cumulative=cumulative)   \n            if len(titles)>1:\n                plt.legend(titles)#,loc='upper right')\n            plt.title(c)            \n            plt.grid()\n        plt.show()    ", "        \n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n"]}
{"filename": "tfold/modeling/mmcif_tools.py", "chunked_list": ["#Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2021-2022\n\n#tools for making mmcifs suitable for AF input out of pdbs\n\nfrom Bio.PDB.PDBParser import PDBParser \nfrom Bio.PDB import MMCIFIO\nimport io\n\ndef rearrange_pdb_file(s,pep_pdbnum):\n    '''\n    rearrange chains in the order PMNAB; change chain names to 'A' and pdbnums to sequential;\n    drop pep residues other than pep_pdbnum (to cut linkers)\n    '''\n    lines=s.split('\\n')    \n    lines_by_chain={}\n    pdbnums_by_chain={}\n    for line in lines:\n        if line.startswith(('ATOM','HETATM')): #drop 'TER'\n            chain=line[21]\n            pdbnum=line[21:27] #incl chain\n            if (chain=='P') and (pdbnum not in pep_pdbnum): #drop pep residues not in pep_pdbnum\n                continue\n            lines_by_chain.setdefault(chain,[]).append(line)\n            pdbnums_by_chain.setdefault(chain,[])                        \n            if pdbnum not in pdbnums_by_chain[chain]:  #each should appear once\n                pdbnums_by_chain[chain].append(pdbnum)\n    lines_new=[]\n    pdbnum_list=[]\n    for chain in ['P','M','N','A','B']:\n        lines_new+=lines_by_chain.get(chain,[])\n        pdbnum_list+=pdbnums_by_chain.get(chain,[])\n    lines_renumbered=[]\n    reslist=[] #for use in mmcif header\n    pdbnums_encountered=set()\n    for line in lines_new:\n        pdbnum='{:6s}'.format(line[21:27]) #format included in case line is shorter\n        index=pdbnum_list.index(pdbnum)\n        if pdbnum not in pdbnums_encountered:\n            reslist.append([index+1,line[17:20]])\n            pdbnums_encountered.add(pdbnum)\n        line=line[:21]+'A{:4d} '.format(index+1)+line[27:]\n        lines_renumbered.append(line)                             \n    return '\\n'.join(lines_renumbered),reslist", "\ndef rearrange_pdb_file(s,pep_pdbnum):\n    '''\n    rearrange chains in the order PMNAB; change chain names to 'A' and pdbnums to sequential;\n    drop pep residues other than pep_pdbnum (to cut linkers)\n    '''\n    lines=s.split('\\n')    \n    lines_by_chain={}\n    pdbnums_by_chain={}\n    for line in lines:\n        if line.startswith(('ATOM','HETATM')): #drop 'TER'\n            chain=line[21]\n            pdbnum=line[21:27] #incl chain\n            if (chain=='P') and (pdbnum not in pep_pdbnum): #drop pep residues not in pep_pdbnum\n                continue\n            lines_by_chain.setdefault(chain,[]).append(line)\n            pdbnums_by_chain.setdefault(chain,[])                        \n            if pdbnum not in pdbnums_by_chain[chain]:  #each should appear once\n                pdbnums_by_chain[chain].append(pdbnum)\n    lines_new=[]\n    pdbnum_list=[]\n    for chain in ['P','M','N','A','B']:\n        lines_new+=lines_by_chain.get(chain,[])\n        pdbnum_list+=pdbnums_by_chain.get(chain,[])\n    lines_renumbered=[]\n    reslist=[] #for use in mmcif header\n    pdbnums_encountered=set()\n    for line in lines_new:\n        pdbnum='{:6s}'.format(line[21:27]) #format included in case line is shorter\n        index=pdbnum_list.index(pdbnum)\n        if pdbnum not in pdbnums_encountered:\n            reslist.append([index+1,line[17:20]])\n            pdbnums_encountered.add(pdbnum)\n        line=line[:21]+'A{:4d} '.format(index+1)+line[27:]\n        lines_renumbered.append(line)                             \n    return '\\n'.join(lines_renumbered),reslist", "\ndef mmcif_loop(keys,values):        \n    #loop block:\n    ##\n    #loop_\n    #_x.a\n    #_x.b\n    #_x.c\n    #a1 b1 c1\n    #a2 b2 c2\n    #...\n    lines=[]\n    lines.append('#')\n    lines.append('loop_')\n    for k in keys:\n        lines.append(k)\n    for v in values:\n        lines.append(' '.join(['{}'.format(x) for x in v]))\n    return lines", "\nrestypes=['ALA','ARG','ASN','ASP','CYS','GLN','GLU','GLY','HIS','ILE','LEU','LYS','MET',\n           'PHE','PRO','SER','THR','TRP','TYR','VAL']\n\ndef pdb_to_mmcif(s,pep_pdbnum):\n    pdb_parser=PDBParser()\n    mmcifio=MMCIFIO()\n    \n    s,reslist=rearrange_pdb_file(s,pep_pdbnum)\n    \n    with io.StringIO(s) as f:        \n        x=pdb_parser.get_structure('some_pdb',f)\n    mmcifio.set_structure(x)\n    with io.StringIO() as f:\n        mmcifio.save(f)\n        f.seek(0)\n        s=f.read()\n        \n    #add mmcif data    \n    lines=[]\n    #header\n    d={'_exptl.method':'some_method',                              #fake experimental method\n       '_pdbx_audit_revision_history.revision_date':'1950-01-01',  #fake date\n       '_refine.ls_d_res_high':'1.0'}                              #fake resolution        \n    for k,v in d.items():\n        lines.append('#')\n        lines.append('{:50s} {:20s}'.format(k,v))\n    #entity\n    reslist=[['1',a[1],a[0]] for a in reslist]\n    lines+=mmcif_loop(['_entity_poly_seq.entity_id','_entity_poly_seq.mon_id','_entity_poly_seq.num'],reslist)\n    #chem: used by AF parser only to identify protein chains; don't need to care about HETATM\n    lines+=mmcif_loop(['_chem_comp.id','_chem_comp.type'],zip(restypes,['L-peptide_linking']*len(restypes)))\n    #struct\n    lines+=mmcif_loop(['_struct_asym.id','_struct_asym.entity_id'],[['A','1']])           \n    s=s.split('\\n')    \n    lines=[s[0]]+lines+s[1:]\n    s='\\n'.join(lines)\n    return s", "\n#used in mmcif parser:\n#'_exptl.method' #'some_method'\n#'_pdbx_audit_revision_history.revision_date' #'1980-01-01'\n#'_refine.ls_d_res_high' #'2.0'\n##'_entity_poly_seq.',\n#'_entity_poly_seq.entity_id', #'1'\n#'_entity_poly_seq.mon_id',    #residue, e.g. 'ALA'\n#'_entity_poly_seq.num',       #resnum, e.g. '25'\n##'_chem_comp.',", "#'_entity_poly_seq.num',       #resnum, e.g. '25'\n##'_chem_comp.',\n#'_chem_comp.id',    #20 residues, e.g. 'ALA'\n#'_chem_comp.type',  #'L-peptide_linking'  #NO SPACES!!!\n##'_struct_asym.',\n#'_struct_asym.id',         #'A'\n#'_struct_asym.entity_id']  #'1'"]}
{"filename": "tfold/modeling/__init__.py", "chunked_list": [""]}
{"filename": "tfold/modeling/template_tools.py", "chunked_list": ["#Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2021-2022\n\nimport os\nimport copy\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nfrom Bio import pairwise2", "\nfrom Bio import pairwise2\n\nimport tfold.utils.seq_tools as seq_tools\nimport tfold.nn.nn_utils as nn_utils\n\n##### load data #####\ndef load_data(source_dir):\n    global template_data,template_info,pep_pdbnums_template,mhc_pdbnums_template,summary\n    with open(source_dir+'/summary.pckl','rb') as f:\n        summary=pickle.load(f) \n    template_data,template_info={},{}\n    pep_pdbnums_template,mhc_pdbnums_template={},{}\n    for cl in ['I','II']:\n        template_data[cl]=pd.read_pickle(source_dir+f'/templates/template_data_{cl}.pckl')\n        template_info[cl]=pd.read_pickle(source_dir+f'/templates/template_info_{cl}.pckl')\n        with open(source_dir+f'/templates/pep_pdbnums_{cl}.pckl','rb') as f:\n            pep_pdbnums_template[cl]=pickle.load(f)\n        with open(source_dir+f'/templates/mhc_pdbnums_{cl}.pckl','rb') as f:\n            mhc_pdbnums_template[cl]=pickle.load(f)         ", "       \n##### edit distance, used for clustering #####\n\ndef edit_distance(seq1,seq2,return_all=False):\n    '''\n    takes two sequences, returns the number of mismatches;\n    (globalms alignment with gap penalties);    \n    if return_all, returns all alignments\n    '''\n    y=pairwise2.align.globalms(seq1,seq2,match=1,mismatch=-1,open=-1,extend=-1)\n    s_best=1000\n    i_best=-1\n    for i,x in enumerate(y):\n        s=int((x[4]-x[3]-x[2])/2) #integer anyway\n        if s<s_best:\n            s_best=s\n            i_best=i\n    if return_all:\n        return y,i_best\n    else:\n        return s_best    ", "def _pmhc_edit_distance(pmhc1,pmhc2):    \n    pep1=seq_tools.load_NUMSEQ(pmhc1['P']).get_fragment_by_pdbnum('   09',' 10 ').seq() #cut tails (incl. linkers)\n    pep2=seq_tools.load_NUMSEQ(pmhc2['P']).get_fragment_by_pdbnum('   09',' 10 ').seq()\n    pep_dist=edit_distance(pep1,pep2)\n    mhc_seq1=[''.join(pmhc1['M']['data']['seq'])]\n    mhc_seq2=[''.join(pmhc2['M']['data']['seq'])]    \n    if pmhc1['class']=='II':\n        mhc_seq1.append(''.join(pmhc1['N']['data']['seq']))\n    if pmhc2['class']=='II':\n        mhc_seq2.append(''.join(pmhc2['N']['data']['seq']))      \n    if pmhc1['class']!=pmhc2['class']: #join chains M and N for cl II\n        mhc_seq1=[''.join(mhc_seq1)]\n        mhc_seq2=[''.join(mhc_seq2)]        \n    mhc_dist=sum([edit_distance(*x) for x in zip(mhc_seq1,mhc_seq2)])\n    return pep_dist+mhc_dist          ", "def _tcr_edit_distance(tcr1,tcr2):        \n    tcr_seq1=''.join(tcr1['obj']['data']['seq'])\n    tcr_seq2=''.join(tcr2['obj']['data']['seq'])\n    return edit_distance(tcr_seq1,tcr_seq2)    \ndef protein_edit_distances_all(inputs,output_dir,proteins,pdb_dir):\n    #load pmhc/tcr records    \n    if proteins not in ['pmhcs','tcrs']:\n        raise ValueError(f'protein {proteins} not understood;')\n    with open(f'{pdb_dir}/{proteins}.pckl','rb') as f:\n        proteins_data=pickle.load(f)\n    print(f'protein records {proteins} of len {len(proteins_data)}')\n    if proteins=='pmhcs':\n        dist_func=_pmhc_edit_distance\n    elif proteins=='tcrs':\n        dist_func=_tcr_edit_distance\n    distances={}\n    for x in inputs:\n        a,b=int(x[0]),int(x[1])\n        distances[a,b]=dist_func(proteins_data[a],proteins_data[b])\n    with open(output_dir+f'/d_{a}_{b}.pckl','wb') as f:\n        pickle.dump(distances,f)     ", "        \n##### tools for template assignment #####       \n\n#binding registers\ncl_I_resnum_template_left=['   0{:1d}'.format(x) for x in range(1,10)]+['{:4d} '.format(x) for x in range(1,6)]\ncl_I_resnum_template_insert=['   5{:1d}'.format(x) for x in range(1,10)]\ncl_I_resnum_template_right=['{:4d} '.format(x) for x in range(6,10000)]\ncl_II_resnum_template_ext=['   0{}'.format(x) for x in 'abcdefghijklmnopqrstuvwxyz']\ncl_II_resnum_template=['   0{:1d}'.format(x) for x in range(1,10)]+['{:4d} '.format(x) for x in range(1,10000)]\ndef _make_pep_pdbnums_I(pep_len,left_tail,right_tail):\n    '''\n    pdbnums for a class I peptide\n    '''\n    assert -1<=left_tail<=9,    'cl I pep: left tail length should be between -1 and 9;'\n    assert 0<=right_tail<=9999, 'cl I pep: right tail length must be between 0 and 9999;'    \n    core_len=pep_len-left_tail-right_tail    #core length\n    assert core_len>=8, 'cl I pep: core length must be at least 8;'   \n    assert core_len<=18, 'cl I pep: core too long;' #cannot index cores longer than 18\n    left_part=cl_I_resnum_template_left[9-left_tail:] #e.g. [9:] for no tail, [10:] for tail=-1 (i.e. from res 2)\n    l_insert=max(0,core_len-9)\n    l_insert_right=l_insert//2\n    l_insert_left=l_insert-l_insert_right\n    center_part=cl_I_resnum_template_insert[:l_insert_left]+cl_I_resnum_template_insert[9-l_insert_right:]\n    right_part=cl_I_resnum_template_right[max(0,9-core_len):] #remove res 6 if core length 8\n    pdbnum=left_part+center_part+right_part        \n    return pdbnum[:pep_len]", "cl_II_resnum_template=['   0{:1d}'.format(x) for x in range(1,10)]+['{:4d} '.format(x) for x in range(1,10000)]\ndef _make_pep_pdbnums_I(pep_len,left_tail,right_tail):\n    '''\n    pdbnums for a class I peptide\n    '''\n    assert -1<=left_tail<=9,    'cl I pep: left tail length should be between -1 and 9;'\n    assert 0<=right_tail<=9999, 'cl I pep: right tail length must be between 0 and 9999;'    \n    core_len=pep_len-left_tail-right_tail    #core length\n    assert core_len>=8, 'cl I pep: core length must be at least 8;'   \n    assert core_len<=18, 'cl I pep: core too long;' #cannot index cores longer than 18\n    left_part=cl_I_resnum_template_left[9-left_tail:] #e.g. [9:] for no tail, [10:] for tail=-1 (i.e. from res 2)\n    l_insert=max(0,core_len-9)\n    l_insert_right=l_insert//2\n    l_insert_left=l_insert-l_insert_right\n    center_part=cl_I_resnum_template_insert[:l_insert_left]+cl_I_resnum_template_insert[9-l_insert_right:]\n    right_part=cl_I_resnum_template_right[max(0,9-core_len):] #remove res 6 if core length 8\n    pdbnum=left_part+center_part+right_part        \n    return pdbnum[:pep_len]", "def _make_pep_pdbnums_II(pep_len,left_tail):  \n    '''\n    pdbnums for a class II peptide\n    '''    \n    assert pep_len-left_tail>=9, 'cl II pep: core too short;'\n    left_tail_ext=max(left_tail-9,0) #part with letter insertion codes\n    pdbnum=cl_II_resnum_template_ext[len(cl_II_resnum_template_ext)-left_tail_ext:]+cl_II_resnum_template[max(0,9-left_tail):]\n    return pdbnum[:pep_len]\n\n#template search", "\n#template search\naa_array=np.array(list('ACDEFGHIKLMNPQRSTVWXY'))\ndef one_hot_encoding(seq):\n    l=len(aa_array)\n    return (np.repeat(list(seq),l).reshape(-1,l)==aa_array).astype(int)\ndef data_to_matrix(data,pdbnums):\n    '''\n    takes 'data' array of NUMSEQ and an array of pdbnums;\n    returns a one-hot encoded matrix for data spaced to pdbnums with 0-vectors;\n    positions in data that are not in pdbnums are dropped\n    '''\n    data=data[np.isin(data['pdbnum'],pdbnums)] #drop data with positions not in pdbnums\n    data=data[np.argsort(data['pdbnum'])]\n    ind=np.isin(pdbnums,data['pdbnum'])\n    data_enc=one_hot_encoding(data['seq'])\n    matrix=np.zeros((len(pdbnums),data_enc.shape[1]))\n    matrix[ind]=data_enc\n    return matrix        ", "def assign_templates(cl,pep_seq,pep_tails,mhc_A,mhc_B=None,templates_per_register=None,pep_gap_penalty=0,mhc_cutoff=None,shuffle=False,\n                     pdbs_exclude=None,date_cutoff=None,score_cutoff=None,pep_score_cutoff=None):\n    '''\n    takes class, pep sequence, mhc data array; assigns templates;\n    for each possible binding register, templates are sorted by pep_score+mhc_score+pep_gap_penalty*pep_gap_count (total mismatch number);\n    (for pep mismatch number, not all residues are considered: see pep_pdbnums_template[cl]);\n    templates with pdbs in pdbs_exclude are dropped; templates with date>date_cutoff are excluded; with total score<=score_cutoff excluded;\n    templates with mhc_score greater than mhc_cutoff+min(mhc_scores) are droppped;\n    with pep_score<=pep_score_cutoff dropped;\n    then no more than templates_per_register templates are assigned for each register;\n    each CA cluster is allowed no more than once per register\n    '''    \n    if cl=='I':\n        mhc_data=mhc_A\n    else:\n        mhc_data=np.concatenate((mhc_A,mhc_B))    \n    mhc_matrix=data_to_matrix(mhc_data,mhc_pdbnums_template[cl])\n    mhc_scores=np.sum(np.any(template_data[cl]['mhc_data']-mhc_matrix,axis=2).astype(int),axis=1)        \n    #exclude by date, pdb_id, mhc_score\n    ind_keep=np.array([True for i in range(len(mhc_scores))])\n    x=template_info[cl]\n    if date_cutoff:\n        ind_keep&=(x['date']<date_cutoff).values\n    if pdbs_exclude:           \n        ind_keep&=~x['pdb_id_short'].isin(pdbs_exclude)\n    if mhc_cutoff:\n        ind_keep&=((mhc_scores-np.min(mhc_scores[ind_keep]))<=mhc_cutoff)        \n    #pep pdbnums   \n    pep_len=len(pep_seq)    \n    if cl=='I':        \n        c_pep_pdbnums=[(x,_make_pep_pdbnums_I(pep_len,x[0],x[1])) for x in pep_tails]\n    else:        \n        c_pep_pdbnums=[(x,_make_pep_pdbnums_II(pep_len,x[0])) for x in pep_tails]    \n    templates_assigned={}\n    for tails,pdbnum in c_pep_pdbnums:\n        pep_data=seq_tools.NUMSEQ(seq=pep_seq,pdbnum=pdbnum).data\n        pep_matrix=data_to_matrix(pep_data,pep_pdbnums_template[cl])    \n        pep_scores=np.sum(np.any(template_data[cl]['pep_data']-pep_matrix,axis=2).astype(int),axis=1)\n        total_scores=mhc_scores+pep_scores+pep_gap_penalty*template_info[cl]['pep_gaps']\n        if score_cutoff:\n            ind_keep1=ind_keep&(total_scores>score_cutoff)\n        else:\n            ind_keep1=ind_keep \n        if pep_score_cutoff:\n            ind_keep1=ind_keep1&(pep_scores>pep_score_cutoff)        \n        ind=np.argsort(total_scores,kind='mergesort') #stable sort to preserve order for elements w. identical scores\n        cluster_CA_ids_used=set()        \n        templates_assigned[tails]=[]\n        for i in ind:\n            if ind_keep1[i]:                \n                x=template_info[cl].iloc[i]                \n                if x['cluster_CA'] not in cluster_CA_ids_used: #use one structure per CA cluster                    \n                    templates_assigned[tails].append({'pdb_id':x['pdb_id'],\n                                                      'pep_score':pep_scores[i],'mhc_score':mhc_scores[i],'pep_gaps':x['pep_gaps'],\n                                                      'score':total_scores[i]})\n                    cluster_CA_ids_used.add(x['cluster_CA'])\n                    if len(templates_assigned[tails])>=templates_per_register:\n                        break                   \n    templates_assigned=pd.DataFrame(templates_assigned) #same num templates per register\n    if shuffle:\n        templates_assigned=templates_assigned.sample(frac=1)\n    return templates_assigned", "    \n#turn templates into AF hits\ndef _interlace_lists(l1,l2):\n    '''interlaces two lists preserving order'''\n    l1_iter=iter(l1)\n    l2_iter=iter(l2)\n    a=next(l1_iter)\n    b=next(l2_iter)\n    result=[]\n    while True:\n        add_a=False\n        add_b=False\n        try:\n            if a==b:\n                result.append(a)\n                a=next(l1_iter)\n                add_a=True\n                b=next(l2_iter)\n                add_b=True\n            elif a<b:\n                result.append(a)\n                a=next(l1_iter)\n                add_a=True\n            else:\n                result.append(b)\n                b=next(l2_iter)\n                add_b=True\n        except StopIteration:\n            break\n    try:\n        if add_a:\n            while True:\n                result.append(a)\n                a=next(l1_iter)\n        elif add_b:\n            while True:\n                result.append(b)\n                b=next(l2_iter)\n    except StopIteration:\n        pass\n    return result", "def align_numseq(query,target):\n    '''takes query and target NUMSEQ objects;\n    returns dict with aligned query seq, target seq, indices query, indices target\n    '''\n    pdbnum_x=list(query.data['pdbnum'])\n    pdbnum_y=list(target.data['pdbnum'])\n    pdbnum_joined=_interlace_lists(pdbnum_x,pdbnum_y)\n    indices_query=[]\n    indices_target=[]\n    query_seq=''\n    target_seq=''\n    iq,it=0,0\n    for p in pdbnum_joined:\n        ind=np.nonzero(query.data['pdbnum']==p)[0]        \n        if len(ind)>0:\n            query_seq+=query.data['seq'][ind[0]]\n            indices_query.append(iq)\n            iq+=1\n        else:\n            query_seq+='-'\n            indices_query.append(-1)\n        ind=np.nonzero(target.data['pdbnum']==p)[0] \n        if len(ind)>0:\n            target_seq+=target.data['seq'][ind[0]]\n            indices_target.append(it)\n            it+=1\n        else:\n            target_seq+='-'\n            indices_target.append(-1)    \n    return {'query_seq':query_seq,'target_seq':target_seq,'indices_query':indices_query,'indices_target':indices_target}        ", "def join_fragment_alignments(fragments):\n    '''\n    join multiple alignments into one\n    '''\n    if len(fragments)==0:\n        raise ValueError('no fragments provided')\n    elif len(fragments)==1:\n        return fragments[0]    \n    alignment=copy.deepcopy(fragments[0])\n    for f in fragments[1:]:\n        max_ind_query=max(alignment['indices_query'])\n        max_ind_target=max(alignment['indices_target'])        \n        alignment['indices_query']+=[-1*(x==-1)+(x+max_ind_query+1)*(x!=-1) for x in f['indices_query']]\n        alignment['indices_target']+=[-1*(x==-1)+(x+max_ind_target+1)*(x!=-1) for x in f['indices_target']]\n        alignment['query_seq']+=f['query_seq']\n        alignment['target_seq']+=f['target_seq']\n    return alignment", "\ndef make_template_hit(cl,x,pep_query,mhc_A_query,mhc_B_query=None):\n    '''\n    takes cl, dict x {'pdbid':..,...}, NUMSEQ objects for pep and mhc queries;\n    returns a copy of dict x with added field 'template_hit' (AF formatted template hit)\n    '''    \n    fragment_alignments=[]    \n    pdb_id=x['pdb_id']\n    summary_record=summary[pdb_id]    \n    pep_target=seq_tools.load_NUMSEQ(summary_record['P'])\n    pep_target=pep_target.ungap_small()\n    fragment_alignments.append(align_numseq(pep_query,pep_target))    \n    mhc_A_target=seq_tools.load_NUMSEQ(summary_record['M'])\n    mhc_A_target=mhc_A_target.ungap_small()\n    fragment_alignments.append(align_numseq(mhc_A_query,mhc_A_target))\n    if cl=='II':\n        mhc_B_target=seq_tools.load_NUMSEQ(summary_record['N'])\n        mhc_B_target=mhc_B_target.ungap_small()\n        fragment_alignments.append(align_numseq(mhc_B_query,mhc_B_target))    \n    hit=join_fragment_alignments(fragment_alignments)\n    template_hit={}\n    template_hit['index']=None #to be added when run inputs are assembled\n    template_hit['name']=pdb_id                \n    template_hit['aligned_cols']=len(hit['query_seq'])-hit['query_seq'].count('-')-hit['target_seq'].count('-')\n    template_hit['sum_probs']=1000-x['score']\n    template_hit['query']=hit['query_seq']\n    template_hit['hit_sequence']=hit['target_seq']\n    template_hit['indices_query']=hit['indices_query']\n    template_hit['indices_hit']=hit['indices_target']         \n    return {'template_hit':template_hit,**x}", "            \ntask_names={'distances':protein_edit_distances_all}\nif __name__=='__main__': \n    import time    \n    from argparse import ArgumentParser\n    import csv\n    t0=time.time()    \n    parser=ArgumentParser()\n    parser.add_argument('input_filename', type=str, help='path to input file')    \n    parser.add_argument('task', type=str, help='task, e.g. \"distances_pmhcs\"')    \n    parser.add_argument('output_dir', type=str, help='path to output dir')   \n    parser.add_argument('pdb_dir',type=str)\n    args=parser.parse_args()      \n    os.makedirs(args.output_dir,exist_ok=True)\n    inputs=[]\n    with open(args.input_filename) as f:\n        f_csv=csv.reader(f,delimiter='\\t')\n        inputs=[x for x in f_csv]        \n    print(f'processing {len(inputs)} tasks {args.task}...')\n    _func=task_names[args.task.split('_')[0]]\n    _func(inputs,args.output_dir,*args.task.split('_')[1:],args.pdb_dir)                                          \n    print('finished {} tasks in {} s'.format(len(inputs),time.time()-t0))", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n"]}
{"filename": "tfold/modeling/make_inputs.py", "chunked_list": ["import pickle\nimport numpy as np\nimport pandas as pd\n\nimport tfold.config\nimport tfold.utils.seq_tools as seq_tools\nimport tfold.nn.nn_predict as nn_predict\nimport tfold.modeling.template_tools as template_tools\ntemplate_tools.load_data(tfold.config.template_source_dir)\n", "template_tools.load_data(tfold.config.template_source_dir)\n\ntemplates_per_run=4 #AF maximum\n\nwith open(tfold.config.data_dir+'/msas/mhc_msa_index.pckl','rb') as f: #index of precomputed MSAs for MHCs\n    mhc_msa_index=pickle.load(f)\nwith open(tfold.config.data_dir+'/msas/pmhc_msa_index.pckl','rb') as f: #index of pMHC assay MSAs\n    pmhc_msa_index=pickle.load(f)\n\ndef _mhcb(x):\n    if x['class']=='I':\n        return None\n    else:\n        return x['mhc_b'].data    ", "\ndef _mhcb(x):\n    if x['class']=='I':\n        return None\n    else:\n        return x['mhc_b'].data    \ndef _exclude_pdbs(x):\n    if 'exclude_pdbs' in x.index:\n        return set(x['exclude_pdbs'])\n    else:\n        return None", "\ndef _prefilter_registers(logkds,threshold):\n    return logkds['tail'][logkds['logkd']-np.min(logkds['logkd'])<np.log10(threshold)]\n    \ndef _get_mhc_msa_filenames(cl,chain,pdbnum):\n    '''\n    find MSA filenames for given MHC class, chain, pdbnum;\n    (returns a list of str filenames)\n    '''\n    pdbnum='|'.join(pdbnum)\n    try:\n        msa_id=mhc_msa_index[cl+'_'+chain][pdbnum][0]\n    except KeyError:\n        raise ValueError('MHC MSA not precomputed for given class, chain and pdbnum')\n    msas=[]\n    for x in ['bfd_uniclust_hits.a3m','mgnify_hits.a3m','uniref90_hits.a3m']:\n        msas.append(tfold.config.data_dir+f'/msas/MHC/{cl}_{chain}_{msa_id}/{x}')\n    return msas", "def _get_pmhc_msa_filenames(cl,chain,pdbnum):\n    '''\n    find MSA filename for pMHC assays for given class, chain (P,M,N), pdbnum;\n    (returns a str filename)\n    '''\n    pdbnum='|'.join(pdbnum)\n    try:\n        msa_id=pmhc_msa_index[cl+'_'+chain][pdbnum]\n    except KeyError:\n        raise ValueError('pMHC MSA not precomputed for given class, chain and pdbnum')\n    return tfold.config.data_dir+f'/msas/pMHC/{cl}_{chain}_{msa_id}.a3m'", "\ndef _make_af_inputs_for_one_entry(x,use_mhc_msa,use_paired_msa,tile_registers):  \n    '''\n    takes a Series x with entries: class, pep, mhc_a, (mhc_b), templates, pmhc_id, (pdb_id -- for true structure);\n    also options for msa and register tiling;\n    returns a list of AF inputs\n    '''\n    sequences=[x['pep']]                \n    msas_mhc=[]\n    #mhc_a\n    mhc_A_query=x['mhc_a']\n    sequences.append(mhc_A_query.seq())\n    renumber_list_mhc=['M'+a for a in mhc_A_query.data['pdbnum']]\n    if use_mhc_msa:\n        try:\n            msas_filenames=_get_mhc_msa_filenames(x['class'],'A',mhc_A_query.data['pdbnum'])\n            msas_mhc+=[{1:f} for f in msas_filenames]\n        except Exception as e:\n            print(f'MHC chain A MSA not available: {e}')\n    #mhc_b\n    if x['class']=='II':\n        mhc_B_query=x['mhc_b']\n        sequences.append(mhc_B_query.seq())\n        renumber_list_mhc+=['N'+a for a in mhc_B_query.data['pdbnum']]      \n        if use_mhc_msa:\n            try:\n                msas_filenames=_get_mhc_msa_filenames(x['class'],'B',mhc_B_query.data['pdbnum'])\n                msas_mhc+=[{2:f} for f in msas_filenames]\n            except Exception as e:\n                print(f'MHC chain B MSA not available: {e}')\n    else:\n        mhc_B_query=None\n    #prepare templates in AF input format\n    all_tails=list(x['templates'].columns) #for statistics\n    pep_len=len(x['pep'])\n    templates_processed={}\n    for tails in x['templates'].columns:\n        if x['class']=='I':\n            pdbnum=template_tools._make_pep_pdbnums_I(pep_len,*tails)\n        else:\n            pdbnum=template_tools._make_pep_pdbnums_II(pep_len,tails[0])\n        pep_query=seq_tools.NUMSEQ(seq=x['pep'],pdbnum=pdbnum)        \n        z=x['templates'][tails].map(lambda y: template_tools.make_template_hit(x['class'],y,pep_query,mhc_A_query,mhc_B_query))        \n        for w in z: #add tail info\n            w['tails']=tails        \n        templates_processed[tails]=z        \n    templates_processed=pd.DataFrame(templates_processed)        \n    #split templates by runs            \n    if tile_registers: #order as #[tail1_templ1,tail2_templ1,...], split into consecutive groups of <=templates_per_run\n        z=templates_processed.values\n        z=z.reshape(z.shape[0]*z.shape[1]) \n        lz=len(z)\n        templates_by_run=[z[templates_per_run*i:templates_per_run*(i+1)] \n                          for i in range(lz//templates_per_run+int(lz%templates_per_run!=0))] \n    else:             #for each tail, split into groups of <=templates_per_run; don't mix tails within the same run\n        templates_by_run=[]\n        for c in templates_processed.columns:\n            z=templates_processed[c].values\n            lz=len(z)\n            templates_by_run+=[z[templates_per_run*i:templates_per_run*(i+1)] \n                               for i in range(lz//templates_per_run+int(lz%templates_per_run!=0))]         \n    #make inputs for each run\n    inputs=[]\n    input_id=0\n    for run in templates_by_run:\n        #collect tails and scores\n        tails=set()\n        best_mhc_score=1000\n        best_score=1000\n        scores=[]\n        for r in run:\n            tails.add(r['tails'])\n            best_mhc_score=min(best_mhc_score,r['mhc_score'])\n            best_score=min(best_score,r['score'])\n        tails=list(tails)\n        #renumber list: use pdbnum from random template within run\n        t0=run[np.random.randint(len(run))]['tails']\n        if x['class']=='I':\n            pdbnum=template_tools._make_pep_pdbnums_I(pep_len,t0[0],t0[1]) \n        else:\n            pdbnum=template_tools._make_pep_pdbnums_II(pep_len,t0[0])\n        renumber_list=['P'+a for a in pdbnum]+renumber_list_mhc\n        #paired msa\n        msas_pmhc=[]\n        if use_paired_msa and (len(tails)==1): #only use when there is one register in the run\n            try:\n                pmhc_msa_parts=[_get_pmhc_msa_filenames(x['class'],'P',pdbnum),\n                                _get_pmhc_msa_filenames(x['class'],'M',mhc_A_query.data['pdbnum'])]                \n                if x['class']=='II':\n                    pmhc_msa_parts.append(_get_pmhc_msa_filenames(x['class'],'N',mhc_B_query.data['pdbnum']))                               \n                msas_pmhc+=[{i:f for i,f in enumerate(pmhc_msa_parts)}] #{0:pep,1:M,2:N}\n            except Exception as e:\n                print(f'paired MSA not available: {e}')\n        msas=msas_mhc+msas_pmhc\n        #make input\n        input_data={}\n        input_data['sequences']=sequences\n        input_data['msas']=msas\n        input_data['template_hits']=[r['template_hit'] for r in run]\n        input_data['renumber_list']=renumber_list\n        input_data['target_id']=x['pmhc_id']      #pmhc id of query: add from index if not present!            \n        input_data['current_id']=input_id\n        input_id+=1\n        #additional info (not used by AlphaFold)        \n        input_data['registers']=tails\n        input_data['best_mhc_score']=best_mhc_score\n        input_data['best_score']=best_score\n        if 'pdb_id' in x:\n            input_data['true_pdb']=x['pdb_id'] #pdb_id of true structure, if given\n        inputs.append(input_data)\n    return {'inputs':inputs,'class':x['class'],'tails':all_tails}", "\ndef run_seqnn(df,use_seqnnf=False): \n    '''\n    takes a dataframe with fields \"class\" (I or II), \n    '''\n    df1=df[df['class']=='I']\n    if len(df1)>0:\n        df1=nn_predict.predict(df1,'I',mhc_as_obj=True)\n    df2=df[df['class']=='II']\n    if len(df2)>0:\n        if use_seqnnf:       \n            df2=nn_predict.predict(df2,'II',mhc_as_obj=True,model_list=[(33,)])\n        else:\n            df2=nn_predict.predict(df2,'II',mhc_as_obj=True)        \n    return pd.concat([df1,df2])", "\ndef map_one_mhc_allele(a):\n    species,la=a.split('-')\n    if species in ['HLA','H2']:\n        species={'HLA':'9606','H2':'10090'}[species]\n    if '/' in la:\n        cl='II'\n        laA,laB=la.split('/')\n        lA,aA=laA.split('*')\n        lB,aB=laB.split('*')\n        mhc_a=seq_tools.mhcs.get((species,lA,aA))\n        mhc_b=seq_tools.mhcs.get((species,lB,aB))\n        if (mhc_a is None) or (mhc_b is None):\n            raise ValueError(f'Cannot find MHC {a}. Please check format.')\n    else:\n        cl='I'\n        lA,aA=la.split('*')\n        mhc_a=seq_tools.mhcs.get((species,lA,aA))\n        mhc_b=None\n        if (mhc_a is None):\n            raise ValueError(f'Cannot find MHC {a}. Please check format.')\n    return mhc_a,mhc_b,cl", "\ndef map_one_mhc_seq(s):\n    if '/' in s:\n        sA,sB=s.split('/')\n        cl='II'\n        try:\n            mhc_a=seq_tools.mhc_from_seq(sA)\n        except Exception:\n            raise ValueError(f'Cannot align alpha-chain MHC sequence {sA}.')\n        try:\n            mhc_b=seq_tools.mhc_from_seq(sB)\n        except Exception:\n            raise ValueError(f'Cannot align beta-chain MHC sequence {sB}.')        \n    else:\n        cl='I'\n        try:\n            mhc_a=seq_tools.mhc_from_seq(s)\n        except Exception:\n            raise ValueError(f'Cannot align MHC sequence {s}.')\n        mhc_b=None\n    return mhc_a,mhc_b,cl", "\ndef prepare_mhc_objects(df):\n    if 'MHC sequence' in df:\n        print('Aligning MHC sequences.')\n        f,k=map_one_mhc_seq,'MHC sequence'\n    elif 'MHC allele' in df:\n        print('Retrieving MHC objects from alleles.')\n        f,k=map_one_mhc_allele,'MHC allele'  \n    else:\n        raise ValueError('Cannot find columns \"MHC allele\" or \"MHC sequence\" in input data.')\n    df[['mhc_a','mhc_b','class']]=df[k].map(f).tolist()", "\ndef preprocess_df(df,mhc_as_obj=False,use_seqnnf=False):\n    '''\n    takes a df with columns \"pep\", \"MHC allele\" or \"MHC sequence\";\n    adds pmhc_id if not present;\n    adds MHC NUMSEQ objects in columns mhc_a and mhc_b (skip if mhc_as_obj=True);\n    runs seqnn (for cl II will use seqnn-f if use_seqnnf=True)\n    '''\n    df=df.copy()\n    if 'pmhc_id' not in df.columns: #assign ids\n        df['pmhc_id']=df.index.copy()\n    if not mhc_as_obj:\n        prepare_mhc_objects(df)\n    else:\n        print('MHC objects from columns mhc_a and mhc_b will be used.')\n    df=run_seqnn(df,use_seqnnf=use_seqnnf)\n    return df", "    \ndef make_inputs(df,params={},date_cutoff=None,print_stats=False):\n    '''\n    takes df with fields: class, pep (str for pep seq), mhc_a (mhc_b) (NUMSEQ objects), (pdb_id for true structure);\n    optionally params and date_cutoff (for templates), print_stats;\n    params for each of two classes should have entries: \n    templates_per_register, mhc_cutoff, score_cutoff, kd_threshold, use_mhc_msa, use_paired_msa, tile_registers;\n    returns a list of AF inputs; if print_stats, prints total runs, reg/target and runs/target histograms for cl 1 and 2\n    '''    \n    if not params:\n        params=tfold.config.af_input_params                 \n    #prefilter registers by predicted Kd\n    df['tails_prefiltered']=df.apply(lambda x: _prefilter_registers(x['seqnn_logkds_all'],params[x['class']]['kd_threshold']),axis=1)\n    #assign templates        \n    df['templates']=df.apply(lambda x: template_tools.assign_templates(\n                                                x['class'],x['pep'],pep_tails=x['tails_prefiltered'],\n                                                mhc_A=x['mhc_a'].data,mhc_B=_mhcb(x),\n                                                templates_per_register=params[x['class']]['templates_per_register'],\n                                                pep_gap_penalty=params[x['class']]['pep_gap_penalty'],\n                                                mhc_cutoff=params[x['class']]['mhc_cutoff'],\n                                                shuffle=params[x['class']]['shuffle'],\n                                                pdbs_exclude=_exclude_pdbs(x),date_cutoff=date_cutoff,\n                                                score_cutoff=params[x['class']]['score_cutoff'],\n                                                pep_score_cutoff=params[x['class']].get('pep_score_cutoff'))\n                              ,axis=1)       \n    #add pmhc_id if not present (used by AF for naming output files)\n    if 'pmhc_id' not in df:\n        df['pmhc_id']=df.index\n    #make AF inputs (incl. split templates)                                                                    \n    inputs_dicts=df.apply(lambda x:_make_af_inputs_for_one_entry(x,params[x['class']]['use_mhc_msa'],\n                                                                 params[x['class']]['use_paired_msa'],                                                                                                      params[x['class']]['tile_registers']),axis=1)    \n    inputs=[]\n    reg_counts={'I':[],'II':[]}\n    run_counts={'I':[],'II':[]}\n    for x in inputs_dicts.values:        \n        inputs+=x['inputs']\n        reg_counts[x['class']].append(len(x['tails']))\n        run_counts[x['class']].append(len(x['inputs']))    \n    if print_stats:\n        for cl in ['I','II']:\n            if reg_counts[cl]:\n                queries, runs=np.sum(df['class']==cl), sum(run_counts[cl])\n                registers=sum(reg_counts[cl])\n                print(f'class {cl}:')\n                print('pmhcs: {:3d}; runs: {:4d}, runs per pmhc: av {:3.1f}, max {:2d}; registers per pmhc: av {:3.1f}, max {:2d}'.format(\n                       queries,runs,runs/queries,max(run_counts[cl]),registers/queries,max(reg_counts[cl])))                 \n    return inputs", "    \n    \n    \n    \n    \n    \n    "]}
{"filename": "tfold/utils/__init__.py", "chunked_list": [""]}
{"filename": "tfold/utils/utils.py", "chunked_list": ["#Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2021-2022\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport csv\nimport stat\n\n#from matplotlib import pyplot as plt", "\n#from matplotlib import pyplot as plt\n\ndef print_hist(a,order=0,use_template=True,max_n=None):\n    '''\n    print histogram for list/array a; \n    order==0 (default): sort by alphabet;    \n    order>0: sort by counts, increasing;\n    order<0: sort by counts, decreasing\n    '''\n    a=np.array(a)\n    a_u=np.unique(a)    \n    a_n=np.array([np.sum(a==x) for x in a_u])\n    if order==0:\n        ind=np.argsort(a_u)\n    else:\n        ind=np.argsort(order*a_n)\n    max_len_a=max([len(str(x)) for x in a_u])\n    max_len_n=max([len(str(x)) for x in a_n])\n    if max_n:\n        ind=ind[:max_n]\n    if use_template:\n        template='{:'+str(max_len_a)+'s} {:'+str(max_len_n)+'n}'\n        for i in ind:\n            print(template.format(str(a_u[i]),a_n[i]))\n    else:\n        for i in ind:\n            print(a_u[i],a_n[i])", "\ndef cluster(a,distance,threshold=0):\n    '''\n    cluster elements in list 'a' according to the condition distance(x,y)<=threshold;\n    returns a list of lists of elements in clusters;\n    the dumbest alg possible\n    '''\n    clusters=[]\n    for x in a:\n        c_linked=[]\n        for i,c in enumerate(clusters):\n            #check if element linked to cluster\n            for y in c:\n                if distance(x,y)<=threshold: \n                    c_linked.append(i)\n                    break    \n        #merge clusters in c_links + new element into a new cluster    \n        merged=[el for i in c_linked for el in clusters[i]]\n        merged.append(x)    \n        #update clusters    \n        clusters=[c for i,c in enumerate(clusters) if i not in c_linked]\n        clusters.append(merged)\n    return clusters", "\ndef split_jobs(n,m):\n    '''\n    split n jobs over <=m actors;        \n    first minimize max number of jobs of one actor, then give p jobs to everybody and <=p to the last one;    \n    returns a list of lists of sizes n1,..,nk, k<=m, with indices of jobs assigned to actors (indices randomly permuted)    \n    '''    \n    #p is max number of jobs of one actor\n    p=n//m+int(n%m!=0)\n    ind=np.random.permutation(n)\n    output=[]\n    n_assigned=0\n    i=0\n    while n_assigned<n:\n        output.append(ind[i*p:(i+1)*p])\n        i+=1\n        n_assigned+=p\n    return output    ", "\ndef make_task(inputs,n_tasks,\n              sh_path,python_path,input_dir,input_dir_server=None,\n              qos='short',max_run_time=720,slow_nodes=None,gpu=False,exclusive=False,local=False,argstring='',\n              cd=None):\n    '''\n    takes a list of inputs; each input is a list of arguments to be passed to the processing algorithm;\n    (can be of len 1 if only one argument);\n    takes n_tasks -- number of separate tasks to cut the job into; \n    takes sh_path for the sh script, python_path for the processing alg;\n    takes input_dir; in input_dir/inputs will put inputs as tsv files, and will use input_dir/logs to pipe outputs and errors;\n    optionally takes input_dir_server (default input_dir) as path to input_dir as seen by the processing server;\n    takes slurm qos (default 'short');\n    takes max_run_time in minutes (default 720);\n    use slow_nodes to give a list of node numbers (e.g. [39,60]) to exclude;\n    set flag gpu to use GPUs;\n    if local (default False), makes separate jobs without slurm commands;\n    if argstring is given, will be added as argument after the input file name;\n    if cd (a path) is given, script will have cd $cd (e.g. to a dir that contains a module);\n    creates input_%i.tsv files and .sh script    \n    '''        \n    input_dir_server       =input_dir_server or input_dir\n    input_dir_proper       =input_dir+'/inputs'\n    input_dir_server_proper=input_dir_server+'/inputs'\n    log_dir                =input_dir+'/logs'\n    log_dir_server         =input_dir_server+'/logs'\n    #make dirs\n    for d in [input_dir_proper,log_dir]:\n        os.makedirs(d,exist_ok=True)    \n    #make input files\n    indices=split_jobs(len(inputs),n_tasks)\n    n_tasks=len(indices)\n    for i,job in enumerate(indices):\n        c_inputs=[inputs[j] for j in job]        \n        with open(input_dir_proper+'/input_'+str(i)+'.tsv','w',newline='') as f:\n            f_csv=csv.writer(f,delimiter='\\t')\n            for line in c_inputs:\n                f_csv.writerow(line)            \n    #make .sh        \n    if not local:\n        lines=[]\n        lines.append('#!/bin/bash')\n        lines.append('')        \n        for x in slow_nodes:\n            lines.append('#SBATCH --exclude=node{}'.format(x))     #exclude slow nodes        \n        lines.append(f'#SBATCH --array=0-{n_tasks-1}')             #run an array of n_tasks tasks\n        lines.append(f'#SBATCH --output={log_dir_server}/output_%a.txt')  #here %a will evaluate to array task id\n        lines.append(f'#SBATCH --error={log_dir_server}/error_%a.txt')\n        lines.append(f'#SBATCH --ntasks=1')                        #each array element is one task\n        if exclusive:\n            lines.append(f'#SBATCH --exclusive')                   #request exclusive use of node, e.g. for hhblits\n        if gpu:\n            lines.append(f'#SBATCH --gpus=1')                      #one gpu per task will use one core\n        elif not exclusive:\n            lines.append(f'#SBATCH --cpus-per-task=1')             #one cpu per task\n        lines.append(f'#SBATCH --qos={qos}')  \n        lines.append(f'#SBATCH --time={max_run_time}')             #maximal run time in minutes, same for each array task\n        lines.append('')\n        if cd:\n            lines.append(f'cd {cd}')\n        lines.append(f'input_file={input_dir_server_proper}/input_$SLURM_ARRAY_TASK_ID.tsv')        \n        lines.append(f'srun python {python_path} $input_file {argstring}')\n    else:        \n        lines=[]\n        lines.append('#!/bin/bash')\n        lines.append('')         \n        if cd:\n            lines.append(f'cd {cd}')\n        for i in range(n_tasks):\n            lines.append(f'input_file{i}={input_dir_server_proper}/input_{i}.tsv')\n            lines.append(f'python {python_path} $input_file{i} {argstring} > {log_dir}/output_{i}.txt 2> {log_dir}/error_{i}.txt')          \n    if not sh_path.startswith(('/','./')):\n        sh_path='./'+sh_path\n    with open(sh_path,'w') as f:\n        f.writelines('\\n'.join(lines))\n    os.chmod(sh_path, stat.S_IRWXU | stat.S_IXGRP | stat.S_IXOTH)      \n    return n_tasks", "\n#ROC and plotting\ndef ROC(labels,predictions): \n    '''\n    Compute data for a ROC.\n    Takes np.arrays for labels (0s for neg, 1s for pos) and classifier scores (bigger=>more pos, any range),\n    returns [fpr, tpr], AUC, where \n    fpr=array[(neg | score >= thr)/(all neg) for possible thr],\n    tpr=array[(pos | score >= thr)/(all pos) for possible thr], \n    '''\n    s_p=predictions[np.nonzero(labels==1)[0]] #predictions for true positives\n    s_n=predictions[np.nonzero(labels==0)[0]] #predictions for true negatives\n    n_p=len(s_p)\n    n_n=len(s_n) \n    TP=[0]\n    FP=[0]\n    auc=0\n    pts=np.sort(np.unique(predictions))[::-1]    \n    for x in pts:\n        TP.append(TP[-1]+np.sum(s_p==x))\n        FP.append(FP[-1]+np.sum(s_n==x))\n        auc+=(TP[-1]+TP[-2])*(FP[-1]-FP[-2])/2\n    TP=np.array(TP)\n    FP=np.array(FP)\n    return [FP/n_n,TP/n_p],auc/(n_p*n_n)", "\ncolors=['k','b','g','y','r','c','m','k','k','k','k','k']\ndef plot_ROC(scores,titles):\n    '''\n    Takes lists of scores (np.arrays) [[labels0,predictions0],..] and titles [l0, l1..].\n    Plots the ROC curves, AUCs,\n    and also the number of positive examples for each title.\n    '''\n    labels_auc=[]\n    for i,s in enumerate(scores):\n        n_pos=int(np.sum(s[0]))\n        if n_pos>0:\n            roc_i,auc_i=ROC(s[0],s[1])\n            labels_auc.append('{:6s}({:5d}): {:5.3f}'.format(titles[i],n_pos,auc_i))\n            plt.plot(roc_i[0],roc_i[1],color=colors[i])\n        else:\n            labels_auc.append('{:6s}({:5d})'.format(titles[i],0))\n            plt.plot([0],[0],color=colors[i])\n    plt.plot([0,1],[0,1],linestyle='--',color='black',label='_nolegend_')\n    plt.xlim(0,1)\n    plt.ylim(0,1)\n    plt.grid()\n    plt.legend(labels_auc) ", "    \n    \n#stats\ndef agresti_coull(n,N):\n    '''\n    Agresti-Coull binomial 95% confidence interval (z=2);\n    returns edge estimates\n    ''' \n    n1,N1=n+2,N+4\n    p1=n1/N1\n    sigma=(p1*(1-p1)/N1)**0.5\n    pm,pp=p1-2*sigma,p1+2*sigma\n    return max(0.,pm),min(1.,pp)", "def bootstrap(df,func_d,N=1000,plot=False):\n    '''\n    takes a df and a dict of functions, optionally N and whether to plot;\n    returns a df with bootstrapped distributions of values of the functions\n    '''\n    d={k:[] for k in func_d}\n    for i in range(N):\n        df1=df.sample(frac=1.,replace=True)\n        for k in func_d:            \n            d[k].append(func_d[k](df1))\n    d=pd.DataFrame(d)\n    if plot:\n        n=len(d.columns)\n        plt.figure(figsize=(5*n,5))\n        for i,c in enumerate(d.columns):\n            plt.subplot(1,n,i+1)\n            plt.hist(d[c].values,histtype='step',bins=100)\n            plt.title(c)\n        plt.show()\n    return d", "def get_CI(df): \n    '''get means and CIs for df columns (returns dict of tuples)'''\n    d={}\n    for k in df.columns:\n        d[k]=(df[k].mean(),df[k].quantile(0.025),df[k].quantile(0.975))\n    return d\ndef proportions_p_value(n1,N1,n2,N2,n_repeats=10000):\n    '''\n    significance of difference between proportions via simulation\n    (idea from Coursera; should be used when numbers are small so that the normal approx doesn't work)\n    fix total N and total failures, shuffle, see distribution of f1-f2;\n    takes numbers of successes and totals for two conditions, returns two-sided p-value\n    '''\n    n=n1+n2\n    N=N1+N2\n    x=np.concatenate([np.ones(n),np.zeros(N-n)])\n    deltas=[]\n    for i in range(n_repeats):\n        x=np.random.permutation(x)\n        n1_c=np.sum(x[:N1])\n        n2_c=n-n1_c\n        deltas.append(n1_c/N1-n2_c/N2)\n    p=np.sum(np.abs(deltas)>=abs(n1/N1-n2/N2))/len(deltas)\n    return p", "\n\n\n"]}
{"filename": "tfold/utils/netmhc_tools.py", "chunked_list": ["import os\nimport re\ncwd=os.getcwd()\n\nimport stat\nfrom shutil import rmtree\nimport pandas as pd\n\nimport tfold.config\nnetmhcpanI_dir=tfold.config.netmhcpanI_dir", "import tfold.config\nnetmhcpanI_dir=tfold.config.netmhcpanI_dir\nnetmhcpanII_dir=tfold.config.netmhcpanII_dir\nimport tfold.utils.seq_tools as seq_tools\nseq_tools.load_mhcs()\n\ns_ii_query='-xls -BA -xlsfile'\n\n#to use old version of netmhc ii\nuse_old_netmhc_II_flag=False\ndef use_old_netmhc_II():\n    '''use version 3.2 for class II predictions'''\n    global netmhcpanII_dir,use_old_netmhc_II_flag,alleles_II,s_ii_query\n    netmhcpanII_dir=tfold.config.netmhcpanII_old_dir\n    use_old_netmhc_II_flag=True\n    with open(netmhcpanII_dir+'/data/allele.list') as f: #change allele list\n        alleles_II=[line for line in f.read().split('\\n') if line]  \n    s_ii_query='>'", "#to use old version of netmhc ii\nuse_old_netmhc_II_flag=False\ndef use_old_netmhc_II():\n    '''use version 3.2 for class II predictions'''\n    global netmhcpanII_dir,use_old_netmhc_II_flag,alleles_II,s_ii_query\n    netmhcpanII_dir=tfold.config.netmhcpanII_old_dir\n    use_old_netmhc_II_flag=True\n    with open(netmhcpanII_dir+'/data/allele.list') as f: #change allele list\n        alleles_II=[line for line in f.read().split('\\n') if line]  \n    s_ii_query='>'", "\nalleles_I=[]\nwith open(netmhcpanI_dir+'/data/allelenames') as f:\n    for line in f.read().split('\\n'):\n        if line:\n            alleles_I.append(line.split()[0])      \nwith open(netmhcpanII_dir+'/data/allele.list') as f:\n    alleles_II=[line for line in f.read().split('\\n') if line]  \n    \ndef convert_mhc_name_I(x):\n    prefix_dict={'9913':'BoLA','9615':'DLA','9796':'Eqca','9593':'Gogo','9598':'Patr','9823':'SLA',}\n    if x[0]=='9606':\n        x=f'HLA-{x[1]}{x[2]}'\n    elif x[0] in prefix_dict:                 \n        x=prefix_dict[x[0]]+'-'+x[1]+x[2].replace(\":\",\"\")\n    elif x[0]=='9544':\n        x=f'Mamu-{x[1]}:{x[2].replace(\":\",\"\")}'\n    elif x[0]=='10090':\n        x=f'H-2-{x[1]}{x[2]}'    \n    if x in alleles_I:\n        return x\n    else:\n        return ''", "    \ndef convert_mhc_name_I(x):\n    prefix_dict={'9913':'BoLA','9615':'DLA','9796':'Eqca','9593':'Gogo','9598':'Patr','9823':'SLA',}\n    if x[0]=='9606':\n        x=f'HLA-{x[1]}{x[2]}'\n    elif x[0] in prefix_dict:                 \n        x=prefix_dict[x[0]]+'-'+x[1]+x[2].replace(\":\",\"\")\n    elif x[0]=='9544':\n        x=f'Mamu-{x[1]}:{x[2].replace(\":\",\"\")}'\n    elif x[0]=='10090':\n        x=f'H-2-{x[1]}{x[2]}'    \n    if x in alleles_I:\n        return x\n    else:\n        return ''", "def convert_mhc_name_II(xa,xb):\n    y=None\n    if xa[0]!=xb[0]:\n        raise ValueError('different species in two chains')\n    if xa[0]=='9606':\n        if xb[1].startswith('DRB'):\n            y=f'{xb[1]}_{xb[2].replace(\":\",\"\")}'\n        else:\n            y=f'HLA-{xa[1]}{xa[2].replace(\":\",\"\")}-{xb[1]}{xb[2].replace(\":\",\"\")}'\n    elif xa[0]=='10090':\n        if xa[1][:-1]==xb[1][:-1] and xa[2]==xb[2]: #same locus and allele letter\n            y=f'H-2-{xa[1][:-1]}{xa[2]}'    \n    if y in alleles_II:\n        return y\n    else:\n        return ''    ", "    \ndef _make_pep_files(df_group,input_dir):    \n    with open(input_dir+f'/{df_group.name}.pep','w') as f:\n        f.write('\\n'.join(df_group['pep'].values))      \n        \ndef _make_hla_fasta(x,name,target_dir,chain='A'):\n    with open(f'{target_dir}/{name}_{chain}.fasta','w') as f:\n        f.write('>HLA\\n'+seq_tools.mhcs[x].seq())\n\ndef _make_query_I_allele(name,allele,tmp_dir):    \n    input_fname=tmp_dir+f'/input/{name}.pep'\n    output_fname=tmp_dir+f'/output/{name}.out'\n    return f'{netmhcpanI_dir}/netMHCpan -p {input_fname} -xls -BA -a {allele} -xlsfile {output_fname}'", "\ndef _make_query_I_allele(name,allele,tmp_dir):    \n    input_fname=tmp_dir+f'/input/{name}.pep'\n    output_fname=tmp_dir+f'/output/{name}.out'\n    return f'{netmhcpanI_dir}/netMHCpan -p {input_fname} -xls -BA -a {allele} -xlsfile {output_fname}'\n    \ndef _make_query_I_seq(name,tmp_dir):    \n    input_fname=tmp_dir+f'/input/{name}.pep'\n    output_fname=tmp_dir+f'/output/{name}.out'\n    hla_fasta=tmp_dir+f'/hla_fasta/{name}_A.fasta'    \n    return f'{netmhcpanI_dir}/netMHCpan -p {input_fname} -xls -BA -hlaseq {hla_fasta} -xlsfile {output_fname}'", "    \ndef _make_query_II_allele(name,allele,tmp_dir):    \n    input_fname=tmp_dir+f'/input/{name}.pep'\n    output_fname=tmp_dir+f'/output/{name}.out'\n    return f'{netmhcpanII_dir}/netMHCIIpan -f {input_fname} -inptype 1 -a {allele} {s_ii_query} {output_fname}'\n\ndef _make_query_II_seq(name,tmp_dir):    \n    input_fname=tmp_dir+f'/input/{name}.pep'\n    output_fname=tmp_dir+f'/output/{name}.out'\n    A_fasta=tmp_dir+f'/hla_fasta/{name}_A.fasta'    \n    B_fasta=tmp_dir+f'/hla_fasta/{name}_B.fasta'    \n    return f'{netmhcpanII_dir}/netMHCIIpan -f {input_fname} -inptype 1 -hlaseqA {A_fasta} -hlaseq {B_fasta}'\\\n           f'{s_ii_query} {output_fname}'", "        \nclass NETMHC():\n    def __init__(self,tmp_dir):\n        self.tmp_dir=tmp_dir                            \n    def make_query(self,df,cl,use_mhc_seq=False):\n        self.use_mhc_seq=use_mhc_seq                \n        if True:\n            try:\n                rmtree(self.tmp_dir+'/input')\n            except FileNotFoundError:\n                pass\n            try:\n                rmtree(self.tmp_dir+'/output')  \n            except FileNotFoundError:\n                pass\n            try:\n                rmtree(self.tmp_dir+'/hla_fasta')\n            except FileNotFoundError:\n                pass\n            os.makedirs(self.tmp_dir+'/input')\n            os.mkdir(self.tmp_dir+'/output')\n            os.mkdir(self.tmp_dir+'/hla_fasta')\n        self.df=df.copy()\n        self.cl=cl                \n        ca='mhc_a' in self.df.columns\n        cb='mhc_b' in self.df.columns\n        if (self.cl=='I' and ca) or (self.cl=='II' and ca and cb):\n            pass\n        else:\n            raise ValueError('df columns must include \"mhc_a\" for cl I, \"mhc_a\" and \"mhc_b\" for cl II')                 \n        assert 'pep' in self.df.columns, 'df must include column \"pep\"'\n        #check that no duplicates are present: \n        #duplicates create unnecessary extra work and cause confusion in merging results to dataframe, hence disallow\n        if self.cl=='I':\n            _n_u=len(self.df[['pep','mhc_a']].apply(tuple,axis=1).unique())\n        else:\n            _n_u=len(self.df[['pep','mhc_a','mhc_b']].apply(tuple,axis=1).unique())\n        assert _n_u==len(self.df), 'please deduplicate df!'                    \n        #rename mhcs, add mhc indices\n        self._rename_mhc()\n        #make files with pep lists\n        self.df.groupby('_mhc_id').apply(lambda x: _make_pep_files(x,self.tmp_dir+'/input'))\n        #split into allele known, allele not known\n        if not self.use_mhc_seq:\n            df_allele=self.df[self.df['_mhc']!='']\n            df_seq   =self.df[self.df['_mhc']=='']           \n            print('pmhcs with alleles known to netMHC:',len(df_allele))\n            print('pmhcs with alleles not known to netMHC:',len(df_seq))\n            if self.cl=='I':\n                print('alleles not known:')\n                print(df_seq['mhc_a'].unique())\n            else:             \n                print('alleles not known:')\n                print(df_seq[['mhc_a','mhc_b']].apply(tuple,axis=1).unique())\n        else:\n            df_allele=self.df.head(0)\n            df_seq=self.df            \n        #make HLA fastas  \n        if self.use_mhc_seq==False:\n            if self.cl=='I':            \n                for x in df_seq[['mhc_a','_mhc_id']].apply(tuple,axis=1).unique():\n                    _make_hla_fasta(*x,self.tmp_dir+'/hla_fasta')\n            else:\n                for x in df_seq[['mhc_a','_mhc_id']].apply(tuple,axis=1).unique():\n                    _make_hla_fasta(*x,self.tmp_dir+'/hla_fasta',chain='A')\n                for x in df_seq[['mhc_b','_mhc_id']].apply(tuple,axis=1).unique():\n                    _make_hla_fasta(*x,self.tmp_dir+'/hla_fasta',chain='B')\n        else:\n            if self.cl=='I':            \n                for x in df_seq[['mhc_a','_mhc_id']].apply(tuple,axis=1).unique():\n                    with open(self.tmp_dir+f'/hla_fasta/{x[1]}_A.fasta','w') as f:\n                        f.write('>mhc\\n'+x[0])\n            else:\n                for x in df_seq[['mhc_a','_mhc_id']].apply(tuple,axis=1).unique():\n                    with open(self.tmp_dir+f'/hla_fasta/{x[1]}_A.fasta','w') as f:\n                        f.write('>mhc\\n'+x[0])\n                for x in df_seq[['mhc_b','_mhc_id']].apply(tuple,axis=1).unique():\n                    with open(self.tmp_dir+f'/hla_fasta/{x[1]}_B.fasta','w') as f:\n                        f.write('>mhc\\n'+x[0])            \n        #make queries\n        queries=[]\n        if self.cl=='I':\n            for x in df_allele[['_mhc_id','_mhc']].apply(tuple,axis=1).unique():\n                queries.append(_make_query_I_allele(*x,self.tmp_dir))\n            for x in df_seq['_mhc_id'].unique():\n                queries.append(_make_query_I_seq(x,self.tmp_dir))\n        else:\n            for x in df_allele[['_mhc_id','_mhc']].apply(tuple,axis=1).unique():\n                queries.append(_make_query_II_allele(*x,self.tmp_dir))\n            for x in df_seq['_mhc_id'].unique():\n                queries.append(_make_query_II_seq(x,self.tmp_dir))\n        sh_path=cwd+'/scripts/run_netmhc.sh'\n        with open(sh_path,'w') as f:\n            f.write('\\n'.join(queries))\n        os.chmod(sh_path, stat.S_IRWXU | stat.S_IXGRP | stat.S_IXOTH)   \n    def _rename_mhc(self):        \n        if self.cl=='I':\n            assert 'mhc_a' in self.df.columns, 'df must include column \"mhc_a\"'\n            if not self.use_mhc_seq:\n                self.df['_mhc']=self.df['mhc_a'].map(convert_mhc_name_I)\n            else:\n                self.df['_mhc']=self.df['mhc_a']\n            mhc_list=list(self.df['mhc_a'].unique())\n            self.df['_mhc_id']=self.df['mhc_a'].map(mhc_list.index)\n        else:    \n            assert ('mhc_a' in self.df.columns) and ('mhc_b' in self.df.columns), 'df must include columns \"mhc_a\", \"mhc_b\"'\n            if not self.use_mhc_seq:\n                self.df['_mhc']=self.df.apply(lambda x: convert_mhc_name_II(x['mhc_a'],x['mhc_b']),axis=1)\n            else:\n                self.df['_mhc']=self.df['mhc_a']+self.df['mhc_b']\n            mhc_pairs=self.df[['mhc_a','mhc_b']].apply(tuple,axis=1)\n            mhc_list=list(mhc_pairs.unique())\n            self.df['_mhc_id']=mhc_pairs.map(mhc_list.index)            \n    def parse(self,include_all_scores=False):\n        '''\n        if include_all_scores, outputs Kd, BA-rank, EL-score, EL-rank; otherwise only Kd\n        '''\n        #bascore=1-log(kd)/log(50000)\n        if self.cl=='I':\n            ipep,icore,ikd,ibarank,ielscore,ielrank=1,3,7,8,5,6 #5:elscore,6:elrank,7:bascore,8:barank\n        else:\n            if use_old_netmhc_II_flag:\n                ipep,icore,ikd,ibarank,ielscore,ielrank=2,5,8,9,8,9 #no elscore/elrank!\n            else:\n                ipep,icore,ikd,ibarank,ielscore,ielrank=1,4,8,9,5,6 #5:elscore,6:elrank,7:bascore,8:kd,9:barank\n        output_list=os.listdir(self.tmp_dir+'/output')\n        outputs=[]\n        for fname in output_list:\n            mhc_id=int(fname.split('.')[0])\n            with open(self.tmp_dir+'/output/'+fname) as f:\n                lines=f.read().split('\\n')\n            if use_old_netmhc_II_flag: #collect lines between dash separators\n                n_dash=0\n                lines1=[]\n                for l in lines:\n                    if l.startswith('-'*10):\n                        if n_dash<2:                            \n                            n_dash+=1\n                        else:\n                            break\n                    elif n_dash==2:\n                        l=re.sub('^[ ]+','',l)\n                        l=re.sub('[ ]+','\\t',l)\n                        lines1.append(l)\n                lines=lines1\n            else:\n                lines=lines[2:]\n            lines=[x.split('\\t') for x in lines if x]    \n            outputs+=[[x[ipep],mhc_id,x[icore],x[ikd],x[ibarank],x[ielscore],x[ielrank]] for x in lines] #pep, mhc_id, core, kd\n        outputs=pd.DataFrame(outputs,columns=['pep','_mhc_id','netmhc_core','netmhc_kd','netmhc_barank','netmhc_elscore','netmhc_elrank'])\n        outputs['netmhc_kd']=pd.to_numeric(outputs['netmhc_kd'])\n        outputs['netmhc_barank']=pd.to_numeric(outputs['netmhc_barank'],errors='coerce')\n        outputs['netmhc_elscore']=pd.to_numeric(outputs['netmhc_elscore'],errors='coerce')\n        outputs['netmhc_elrank']=pd.to_numeric(outputs['netmhc_elrank'],errors='coerce')\n        if self.cl=='I':\n            outputs['netmhc_kd']=50000**(1-outputs['netmhc_kd'])\n        print(f'pmhcs expected: {len(self.df)}; loaded: {len(outputs)}')\n        self.df_output=self.df.copy()\n        self.df_output=self.df_output.merge(outputs,on=['pep','_mhc_id'])\n        self.df_output=self.df_output.drop(['_mhc','_mhc_id'],axis=1)        \n        if not include_all_scores:\n            self.df_output=self.df_output.drop(['netmhc_barank','netmhc_elscore','netmhc_elrank'],axis=1)\n        return self.df_output", "    "]}
{"filename": "tfold/utils/seq_tools.py", "chunked_list": ["\n#Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2019-2022\n\nimport os\n#import warnings\nimport numpy as np\nimport json\nimport pickle\nimport pandas as pd", "import pickle\nimport pandas as pd\nimport time\nimport re\n\nfrom tfold import config\nfrom tfold.utils import utils\n\ndata_dir=config.seq_tools_data_dir #path to data\ntmp_dir=config.seq_tools_tmp_dir   #path to tmp dir to be used in BLAST searches\nif not os.path.exists(tmp_dir):\n    os.mkdir(tmp_dir)", "data_dir=config.seq_tools_data_dir #path to data\ntmp_dir=config.seq_tools_tmp_dir   #path to tmp dir to be used in BLAST searches\nif not os.path.exists(tmp_dir):\n    os.mkdir(tmp_dir)\n#in the future: do data loading with pkgutil\n#import pkgutil\n#data = pkgutil.get_data(__name__, \"templates/temp_file\")\n#use os.path.dirname(os.path.realpath(__file__))?\n\nfrom Bio.Blast.Applications import NcbiblastpCommandline", "\nfrom Bio.Blast.Applications import NcbiblastpCommandline\nfrom Bio.Blast import NCBIXML\nfrom Bio import pairwise2\n#from Bio.SubsMat import MatrixInfo as matlist\n#blosum62=matlist.blosum62\nfrom Bio.Align import substitution_matrices\nblosum62=substitution_matrices.load('BLOSUM62')\n\naa_set=set(list('ACDEFGHIKLMNPQRSTVWY'))", "\naa_set=set(list('ACDEFGHIKLMNPQRSTVWY'))\n\n######################### UTILS #########################\ndef _parse_fasta(s):\n    proteins={}\n    header=None\n    for line in s.split('\\n'):\n        if line.startswith('>'):\n            header=line[1:]\n            proteins[header]=''\n        elif line and header:\n            proteins[header]+=line\n    return proteins", "\n######################### NUMSEQ CLASS #########################\ndef _num_from_args(num,ins,pdbnum):           \n    if not (num is None):\n        if (ins is None) or (len(ins)==0):\n            ins=['']*len(num)\n        ins=['{:1s}'.format(x) for x in ins] #default ' '\n        pdbnum=['{:4d}{:1s}'.format(*x) for x in zip(num,ins)]\n    elif not (pdbnum is None):\n        num=[int(x[:-1]) for x in pdbnum]\n        ins=[x[-1] for x in pdbnum]\n    else:\n        raise ValueError('num or pdbnum must be provided')    \n    return num,ins,pdbnum", "#numseq_dtype\n#'pdbnum' formated as pdb(23-26,27), i.e. 'nnnni'\n#note: '<U3' for seq to accomodate pdb res codes, incl hetero atoms\nnumseq_dtype=[('seq','<U3'),('num',np.int16),('ins','U1'),('pdbnum','U5'),('ss','<U15'),('mutations','<U3')]\nclass NUMSEQ():\n    '''\n    CONTAINS:\n    - structured array self.data of dtype numseq_dtype; \n    (can add more fields optionally, but then not guaranteed that join_NUMSEQ and other functions will work);    \n    - a dict self.info to store e.g. species, locus, allele, etc; \n    includes at least 'gaps', which is a structured array of numseq_dtype;\n    INIT TAKES: kwd arguments; arg 'info' becomes self.info, other args go into data;\n    if arg 'data' given, goes into self.data, otherwise self.data built from args 'seq','num','ins'/'pdbnum'\n    and others\n    '''\n    def __init__(self,**args):         \n        if 'data' in args:\n            self.data=args['data']\n        elif ('seq' and 'num') or ('seq' and 'pdbnum') in args:\n            args['seq']=list(args['seq'])\n            args['num'],args['ins'],args['pdbnum']=_num_from_args(args.get('num'),args.get('ins'),args.get('pdbnum'))\n            args.setdefault('ss','')\n            if type(args['ss'])==str:\n                args['ss']=[args['ss']]*len(args['seq'])  \n            else:\n                pass #assume args['ss'] is a list            \n            args.setdefault('mutations',['']*len(args['seq']))            \n            l=len(args['seq'])\n            data_list_extra=[]\n            dtypes_list_extra=[]\n            numseq_dtype_names=[x[0] for x in numseq_dtype]\n            for k,v in args.items(): #check if fields other than those in numseq_dtype or 'info' are provided\n                if (k!='info') and (k not in numseq_dtype_names):\n                    v=np.array(v)\n                    assert len(v)==l                    \n                    data_list.append(v)\n                    dtypes_list.append((k,v.dtype))\n            data_list=[args[k] for k in numseq_dtype_names]+data_list_extra\n            dtypes_list=numseq_dtype+dtypes_list_extra\n            self.data=np.array(list(zip(*data_list)),dtype=dtypes_list)                    \n        else:\n            raise ValueError('provide data or seq and numbering')\n        self.info=args.get('info') or dict()     \n        self.info.setdefault('gaps',np.array([],dtype=numseq_dtype))        \n    def seq(self,hetero=False):\n        '''\n        return the sequence as a string;\n        if hetero (default False), include 3-letter codes for hetero atoms \n        as e.g. GILG(ABA)VFTL or GILG(aba)VFTL if gap\n        '''\n        if not hetero:\n            return ''.join(self.data['seq'])  \n        seq=''\n        for x in self.data:\n            if x['seq']=='X':\n                seq+='('+self.info['hetero_res'][x['pdbnum']]+')'\n            elif x['seq']=='x':\n                seq+='('+self.info['hetero_res'][x['pdbnum']].lower()+')'\n            else:\n                seq+=x['seq']\n        return seq\n    def get_fragment(self,num_l,num_r,complement=False):\n        '''\n        complement==False (default): returns structured array for num_l<=num<=num_r;\n        complement==True: returns structured array for complement of the above\n        '''                \n        ind=(num_l<=self.data['num'])*(self.data['num']<=num_r)\n        if not complement:\n            data1=self.data[ind]\n        else:\n            data1=self.data[~ind]\n        return NUMSEQ(data=data1,info=self.info.copy()) #need to keep info e.g. for V fragment in TCR grafting\n    def get_fragment_by_pdbnum(self,pdbnum_l,pdbnum_r,include_left_end=True,include_right_end=True):\n        '''\n        returns NUMSEQ obj cut by pdbnum [pdbnum_l:pdbnum_r];\n        flags include_left_end, include_right_end determine whether the ends are included\n        (default True, True)\n        '''                   \n        if include_left_end:\n            ind1=(self.data['pdbnum']>=pdbnum_l)\n        else:\n            ind1=(self.data['pdbnum']>pdbnum_l)\n        if include_right_end:\n            ind2=(self.data['pdbnum']<=pdbnum_r)\n        else:\n            ind2=(self.data['pdbnum']<pdbnum_r)\n        data1=self.data[ind1&ind2]\n        return NUMSEQ(data=data1,info=self.info.copy())\n    def get_fragment_by_i(self,i_l,i_r):\n        '''\n        returns NUMSEQ obj cut by python index [i_l:i_r] (end included)\n        '''                        \n        data1=self.data[i_l:i_r+1]        \n        return NUMSEQ(data=data1,info=self.info.copy())\n    def get_residues_by_nums(self,nums):\n        '''        \n        return a np array of residues with res numbers in the array (or list) nums;\n        (residues with all insertion codes included)       \n        (for missing res, no gap shown; motivation: without ins codes, can miss gaps e.g. res '1A' when res '1' present)\n        '''                \n        return self.data[np.isin(self.data['num'],nums)]['seq']\n    def get_residues_by_pdbnums(self,pdbnums,show_gaps=True):\n        '''        \n        return a np array of residues with pdbnums in the array (or list) pdbnums;\n        if show_gaps (default True), output '-'s for missing pdbnums\n        '''\n        if show_gaps:\n            s=[]\n            for x in pdbnums:\n                ind=np.nonzero(self.data['pdbnum']==x)[0]\n                if len(ind)==0:\n                    s.append('-')\n                else:                    \n                    s+=list(self.data[ind]['seq'])\n            s=np.array(s)\n        else:\n            s=self.data[np.isin(self.data['pdbnum'],pdbnums)]['seq']\n        return s\n    def mutations(self):\n        '''\n        returns the list of mutations and gaps in the format 'X|nnnni|Y', sorted by pdbnum\n        '''\n        pdbnums=[]\n        mutations=[]\n        for x in self.data[self.data['mutations']!='']:\n            pdbnums.append(x['pdbnum'])\n            mutations.append('{:1s}|{:5s}|{:1s}'.format(x['mutations'],x['pdbnum'],x['seq']))\n        for x in self.info['gaps']:\n            pdbnums.append(x['pdbnum'])\n            mutations.append('{:1s}|{:5s}|-'.format(x['seq'],x['pdbnum']))\n        ind=np.argsort(pdbnums)\n        mutations=np.array(mutations)\n        mutations=[mutations[i] for i in ind]\n        return mutations\n    def count_mutations(self):\n        '''\n        counts mutations, gaps, gaps_left, gaps_right\n        '''\n        counts={}\n        counts['mutations']=np.sum(self.data['mutations']!='')        \n        pdbnum_l=self.data['pdbnum'][0]\n        pdbnum_r=self.data['pdbnum'][-1]       \n        counts['gaps_left']=np.sum(self.info['gaps']['pdbnum']<pdbnum_l)\n        counts['gaps_right']=np.sum(self.info['gaps']['pdbnum']>pdbnum_r)\n        counts['gaps']=len(self.info['gaps'])\n        return counts        \n    def mutate(self,mutations,shift=None):\n        '''\n        takes mutations formated as a list [res|n|newres,...]; newres can be '-' for gap;        \n        if shift==None (default), assume 'n' are pdbnums;\n        if shift is int, assume 'n' refers to aa seq[n-shift-1];        \n        e.g. if n references seq 'GSHS..' in 1-based indexing, shift should be 0 if seq='GSHS...' and 1 if seq='SHS...';        \n        (note: when gaps present, safer to use pdbnums!)\n        returns a new object with mutations, and an error string;    \n        if any of the old residues do not match what is in the mutations, or pdbnums are missing or\n        occur multiple times, an error is appended with the sublist of mutations that are bad.\n        Error format: 'res|pdbnum|newres|errorcode;...', where errorcode=0 for wrong residue\n        and 1 for no pdb_num or multiple pdb_nums. \n        A mutation with an error is not implemented, all others are.\n        mutations added to output object's data['mutations'] (stores original residues, default '' for non-mutated)\n        and info['gaps'] (all columns incl. e.g. 'ss', if present)\n        '''\n        error=''\n        seq1=self.data['seq'].copy()\n        mutations1=self.data['mutations'].copy()\n        gaps_list=[]\n        for m in mutations:\n            res0,n,res1=m.split('|')\n            if not (shift is None):                \n                i0=int(n)-shift-1\n                if i0 not in range(len(seq1)):\n                    error+=(m+'|1;')\n                    continue\n            else:             \n                i0s=np.nonzero(self.data['pdbnum']==n)[0]\n                if len(i0s)!=1:\n                    error+=(m+'|1;')\n                    continue\n                i0=i0s[0]            \n            if seq1[i0]!=res0:\n                error+=(m+'|0;')\n                continue\n            if res1=='-':\n                gaps_list.append(self.data[i0])\n                seq1[i0]='x' #gap symbol for new gaps\n            else:\n                mutations1[i0]=res0 \n                seq1[i0]=res1            \n        data1=self.data.copy()\n        data1['seq']=seq1\n        data1['mutations']=mutations1        \n        if gaps_list:\n            gaps1=np.concatenate([self.info['gaps'],np.array(gaps_list)])            \n            gaps1.sort(order=('num','ins'))\n        else:\n            gaps1=self.info['gaps']\n        info1=self.info.copy()\n        info1['gaps']=gaps1\n        result=NUMSEQ(data=data1,info=info1)\n        if gaps_list:\n            result=result.ungap(gaplist=['x'])\n        return result, error\n    def ungap(self,gaplist=['.','-']):\n        '''\n        return a new object with gap symbols removed, and all renumbered accordingly;\n        gap symbols defined in gaplist (default '.', '-'); info copied;\n        info['gaps'] not updated, since gaps do not include previous res information\n        '''               \n        ind=np.isin(self.data['seq'],gaplist)\n        data1=self.data[~ind].copy()        \n        return NUMSEQ(data=data1,info=self.info.copy())  \n    def ungap_small(self):\n        '''assume small letters in seq are gaps; remove them and add to info['gaps'] (but large)'''\n        small=list('acdefghiklmnpqrstvwxy')\n        ind=np.isin(self.data['seq'],small)\n        data1=self.data[~ind].copy()\n        gaps=self.data[ind].copy()\n        for i in range(len(gaps)):\n            gaps[i]['seq']=gaps[i]['seq'].capitalize()\n        gaps=np.concatenate((self.info['gaps'],gaps))\n        gaps.sort(order=('num','ins'))\n        new_obj=NUMSEQ(data=data1,info=self.info.copy())        \n        new_obj.info['gaps']=gaps\n        return new_obj    \n    def repair_gaps(self,which='all',small=False):\n        '''\n        insert back residues from gaps; (positions determined according to pdbnum);\n        arg 'which' defines which gaps to restore; can be 'all' (default), 'left', 'right';\n        if small (default False), repaired gaps are in small letters;\n        also changes small-letter gaps in the sequence, unless small==True\n        '''\n        if which not in ['all','left','right']:\n            raise ValueError(f'value {which} of \"which\" not recognized;')\n        data=self.data.copy()\n        #repair small gaps in the sequence        \n        if not small:\n            seq=self.seq()\n            x=re.search('^[a-z]+',seq)\n            if x:\n                i_left=x.end()\n            else:\n                i_left=0        \n            x=re.search('[a-z]+$',seq)\n            if x:\n                i_right=x.start()\n            else:\n                i_right=len(seq)\n            for i in range(len(self.data)):\n                if (which=='right' and i>=n_right) or (which=='left' and i<n_left) or (which=='all'):\n                    data[i]['seq']=data[i]['seq'].upper()\n        #repair other gaps        \n        gaps=self.info['gaps'].copy()\n        if len(gaps)==0:\n            return NUMSEQ(data=data,info=self.info.copy()) \n        gaps.sort(order=('num','ins')) #just in case                            \n        n=len(data)\n        fragments=[]  \n        gaps_remaining=[]\n        for g in gaps:\n            #can't order by pdbnum when negative nums are present, and they are\n            i0=np.sum((data['num']<g['num'])|((data['num']==g['num'])&(data['ins']<g['ins'])))\n            if i0==0:\n                if (which=='all') or (which=='left' and len(data)==n) or (which=='right' and len(data)==0):\n                    if small:\n                        g['seq']=g['seq'].lower()\n                    fragments.append(np.array([g]))\n                else:\n                    gaps_remaining.append(g)\n            elif i0==len(data):\n                fragments.append(data)\n                if which!='left':\n                    if small:\n                        g['seq']=g['seq'].lower()\n                    fragments.append(np.array([g]))\n                else:\n                    gaps_remaining.append(g)\n            else:\n                fragments.append(data[:i0])\n                if which=='all':\n                    if small:\n                        g['seq']=g['seq'].lower()\n                    fragments.append(np.array([g]))\n                else:\n                    gaps_remaining.append(g)            \n            data=data[i0:]\n        fragments.append(data)       \n        data=np.concatenate(fragments)\n        info=self.info.copy()\n        info['gaps']=np.array(gaps_remaining,dtype=numseq_dtype)\n        return NUMSEQ(data=data,info=info)  \n    def dump(self):\n        '''returns {'data':data,'info':info}. For pickling''' \n        return {'data':self.data,'info':self.info}\n    def copy(self):\n        return NUMSEQ(data=self.data.copy(),info=self.info.copy())", "def load_NUMSEQ(data_info):\n    '''restore NUMSEQ object from {'data':data,'info':info}. For unpickling'''\n    return NUMSEQ(data=data_info['data'],info=data_info['info'])                          \ndef join_NUMSEQ(records):\n    '''\n    takes a list/array of NUMSEQ objects, returns the joined NUMSEQ object;\n    info from all objects collected, overwritten in the order left to right if keys repeat,\n    except info['gaps'], which are joined\n    '''            \n    data1=np.concatenate([x.data for x in records])        \n    gaps1=np.concatenate([x.info['gaps'] for x in records])\n    info1={}\n    for x in records:\n        info1.update(x.info) \n    info1['gaps']=gaps1\n    return NUMSEQ(data=data1,info=info1)", "\n######################### BLAST AND ALIGNMENT #########################\n\ndef _blast_parse_fasta_record(s):\n    '''\n    takes fasta record, e.g. 'TCR 9606 B V TRBV18 01', 'MHC 10090 1 A D b', 'B2M 9606';\n    returns protein,species,locus,allele, where \n    protein is e.g. 'TCR_A/D_V', 'TCR_B_J', 'MHC_2_B', 'MHC_1_A', 'B2M',\n    locus and allele are e.g. TRBV18,01 for TCR, D,b for MHC, '','' for B2M\n    '''\n    line=s.split()\n    if line[0]=='B2M':\n        protein='B2M'\n        species,locus,allele=line[1],'',''\n    elif line[0]=='TCR':\n        protein='_'.join(['TCR',line[2],line[3]])\n        species,locus,allele=line[1],line[4],line[5]\n    elif line[0]=='MHC':\n        protein='_'.join(['MHC',line[2],line[3]])\n        species,locus,allele=line[1],line[4],line[5]\n    else:\n        raise ValueError(f'fasta protein record not understood: {s}')\n    return protein,species,locus,allele            ", "\ndef blast_prot(seq,dbs=['B2M','MHC','TRV','TRJ'],species=None):\n    '''\n    takes protein sequence, optionally a list of databases (default: use all), optionally a species; \n    does blastp search, returns a list of all hits;\n    each hit is a dict with keys\n    'protein','species','locus','allele','score','identities','len_target','query_start','query_end';\n    where 'protein' is e.g. 'TCR_A/D_V', 'TCR_A_V', 'TCR_B_J', 'B2M', 'MHC_2_B', 'MHC_1_A'; \n    'identities' is the number of aa matches; 'query_start/end' are 0-based\n    '''    \n    #check seq for unconventional symbols    \n    seq_aa=''.join(set(list(seq))-aa_set)\n    if seq_aa:\n        #raise ValueError(f'sequence contains non-canonical symbols {seq_aa};') \n        pass #blast works with non-canonical symbols\n    #full path for dbs\n    db_dir=data_dir+'/db'\n    dbs_available=os.listdir(db_dir)\n    dbs_full=[]\n    for db in dbs:        \n        if species:\n            db+=('_'+species)\n        db+='.fasta'\n        if db not in dbs_available:\n            raise ValueError(f'db {db} not available;')\n        db=db_dir+'/'+db\n        dbs_full.append(db)\n    #make unique tmp_id\n    tmp_id=seq[:10]+''.join([str(x) for x in np.random.randint(10,size=10)])    \n    query_path=tmp_dir+f'/{tmp_id}.fasta'\n    output_path=tmp_dir+f'/{tmp_id}.xml'\n    #write query to fasta file\n    with open(query_path,'w',encoding='utf8',newline='') as f:\n        f.write('>seq\\n'+seq)        \n    #run blastp    \n    hits=[]\n    for db in dbs_full:\n        blastp_cline=NcbiblastpCommandline(query=query_path, db=db,outfmt=5, out=output_path)\n        stdout,stderr=blastp_cline()\n        #parse blastp output\n        with open(output_path,'r') as f:\n            blast_record=NCBIXML.read(f)        \n        for x in blast_record.alignments:\n            h={}\n            h['protein'],h['species'],h['locus'],h['allele']=_blast_parse_fasta_record(x.hit_def)\n            h['len_target']=x.length #length of full db protein\n            y=x.hsps[0]\n            h['score']=y.score      #alignment score\n            h['identities']=y.identities #identical aa's in alignment\n            #start and end indices in query seq, 0-based\n            h['query_start']=y.query_start-1\n            h['query_end']=y.query_end-1            \n            hits.append(h)\n    #remove tmp files\n    os.remove(query_path)\n    os.remove(output_path)\n    return hits", "    \ndef filter_blast_hits_to_multiple(hits,keep=1):\n    '''\n    per protein, hits are sorted by blossum score and <=keep (default 1) kept;\n    '''\n    hits_dict={}\n    for h in hits:\n        hits_dict.setdefault(h['protein'],[]).append(h)    \n    hits_reduced=[]\n    for protein,hits_list in hits_dict.items(): \n        ind=np.argsort([-h['score'] for h in hits_list])[:keep]        \n        hits_reduced+=[hits_list[i] for i in ind]    \n    return hits_reduced", "\ndef filter_blast_hits_to_single(hits,filter_func):\n    '''\n    hits first filtered by protein according to filter_func,\n    then sorted by blossum score, then locus name, then allele name, then by query_start left to right;\n    the top hit is returned; (may be None if no hits left after filtering)\n    #[protein,species,locus,allele,blossum_score,identities,len(target),query_start,query_end]\n    '''\n    #filter\n    hits_reduced=[h for h in hits if filter_func(h['protein'])]\n    if len(hits_reduced)==0:\n        return None\n    #sort\n    scores=np.array([(-h['score'],h['locus'],h['allele'],h['query_start']) for h in hits_reduced],\n                     dtype=[('score',float),('locus','U20'),('allele','U20'),('query_start',int)])\n    i0=np.argsort(scores,order=['score','locus','allele','query_start'])[0]\n    return hits_reduced[i0]    ", "\ndef _hit_distance(x,y):\n    '''\n    return minus overlap of two hits\n    '''\n    ilx,irx=x['query_start'],x['query_end']\n    ily,iry=y['query_start'],y['query_end']\n    lx=irx-ilx+1\n    ly=iry-ily+1\n    return -max(min(irx-ily+1, iry-ilx+1,lx,ly),0)       \ndef filter_blast_hits_for_chain(hits,threshold):\n    '''\n    cluster hits by pairwise overlap; in each cluster, keep the hit with the highest blossum score;\n    (note: can happen e.g. chain TRV_Y_TRJ with low score Y overlapping TRV and TRJ,\n    causing TRJ to be dropped. Unlikely with sufficient overlap threshold.)\n    '''\n    #cluster\n    hits_clusters=utils.cluster(hits,distance=_hit_distance,threshold=threshold)    \n    #filter\n    hits_keep=[]\n    for c in hits_clusters:\n        scores=np.array([(-h['score'],h['locus'],h['allele']) for h in c],\n                     dtype=[('score',float),('locus','U20'),('allele','U20')])\n        i0=np.argsort(scores,order=['score','locus','allele'])[0]\n        hits_keep.append(c[i0])\n    return hits_keep        ", "def filter_blast_hits_for_chain(hits,threshold):\n    '''\n    cluster hits by pairwise overlap; in each cluster, keep the hit with the highest blossum score;\n    (note: can happen e.g. chain TRV_Y_TRJ with low score Y overlapping TRV and TRJ,\n    causing TRJ to be dropped. Unlikely with sufficient overlap threshold.)\n    '''\n    #cluster\n    hits_clusters=utils.cluster(hits,distance=_hit_distance,threshold=threshold)    \n    #filter\n    hits_keep=[]\n    for c in hits_clusters:\n        scores=np.array([(-h['score'],h['locus'],h['allele']) for h in c],\n                     dtype=[('score',float),('locus','U20'),('allele','U20')])\n        i0=np.argsort(scores,order=['score','locus','allele'])[0]\n        hits_keep.append(c[i0])\n    return hits_keep        ", "        \ndef _find_mutations(seqA,seqB,pdbnum):\n    mutations=[]\n    for i,x in enumerate(zip(seqA,seqB)):\n        if x[0]!=x[1]:\n            mutations.append('|'.join([x[1],pdbnum[i],x[0]]))\n    return mutations  \n\ndef realign(seq,target,name=''):\n    '''\n    realign sequence and NUMSEQ object;\n    symbols X (also B,Z) accepted and have standard blosum62 scores\n    '''\n    #lower penalty for gap in query (assume missing res possible), high penalty for gap in target\n    y=pairwise2.align.globaldd(seq,target.seq(),blosum62,\n                               openA=-5,extendA=-5,openB=-15,extendB=-15,penalize_end_gaps=(False,False),\n                               one_alignment_only=True)[0]            \n    seqA,seqB=y.seqA,y.seqB    \n    if re.search('[A-Z]-+[A-Z]',seqB):\n        raise ValueError(f'internal gap in aligned target for {name}')   \n    #indices of proper target within alignment\n    x=re.search('^-+',seqB)\n    if x:\n        i1=x.end()\n    else:\n        i1=0        \n    x=re.search('-+$',seqB)\n    if x:\n        i2=x.start()\n    else:\n        i2=len(seqB)        \n    #find start and end positions of alignment within query [i_start,i_end)\n    i_start=i1\n    i_end=len(seq)-(len(seqB)-i2)            \n    #cut to aligned target\n    seqA,seqB=seqA[i1:i2],seqB[i1:i2]           \n    #identify and make mutations\n    mutations=_find_mutations(seqA,seqB,target.data['pdbnum'])    \n    result,error=target.mutate(mutations)\n    if error:\n        raise ValueError(f'mutation error {error} for {name}')\n    return result,i_start,i_end", "def realign(seq,target,name=''):\n    '''\n    realign sequence and NUMSEQ object;\n    symbols X (also B,Z) accepted and have standard blosum62 scores\n    '''\n    #lower penalty for gap in query (assume missing res possible), high penalty for gap in target\n    y=pairwise2.align.globaldd(seq,target.seq(),blosum62,\n                               openA=-5,extendA=-5,openB=-15,extendB=-15,penalize_end_gaps=(False,False),\n                               one_alignment_only=True)[0]            \n    seqA,seqB=y.seqA,y.seqB    \n    if re.search('[A-Z]-+[A-Z]',seqB):\n        raise ValueError(f'internal gap in aligned target for {name}')   \n    #indices of proper target within alignment\n    x=re.search('^-+',seqB)\n    if x:\n        i1=x.end()\n    else:\n        i1=0        \n    x=re.search('-+$',seqB)\n    if x:\n        i2=x.start()\n    else:\n        i2=len(seqB)        \n    #find start and end positions of alignment within query [i_start,i_end)\n    i_start=i1\n    i_end=len(seq)-(len(seqB)-i2)            \n    #cut to aligned target\n    seqA,seqB=seqA[i1:i2],seqB[i1:i2]           \n    #identify and make mutations\n    mutations=_find_mutations(seqA,seqB,target.data['pdbnum'])    \n    result,error=target.mutate(mutations)\n    if error:\n        raise ValueError(f'mutation error {error} for {name}')\n    return result,i_start,i_end", "\n######################### SPECIES INFO #########################\nwith open(data_dir+'/species_dict.pckl','rb') as f:\n    species=pickle.load(f)\n\n######################### MHC TOOLS #########################\nmhc_dir=data_dir+'/MHC'\n\n#numeration and secondary structure\nwith open(mhc_dir+'/num_ins_ss.pckl','rb') as f:\n    num_ins_ss=pickle.load(f)", "#numeration and secondary structure\nwith open(mhc_dir+'/num_ins_ss.pckl','rb') as f:\n    num_ins_ss=pickle.load(f)\n\n#load\ndef load_mhcs(species_list=None,use_pickle=True):\n    '''optionally, restricts to species in species_list;\n    use_pickle==True (default) means reads from .pckl (or creates it, if doesn't exist);\n    otherwise reads from fasta and overwrites .pckl'''\n    global mhcs\n    global mhcs_df\n    global mhc_rename_dict\n    t0=time.time()\n    with open(data_dir+'/MHC/mhc_rename.pckl','rb') as f: #read dict for renaming (species,locus,allele) triples for g-region\n        mhc_rename_dict=pickle.load(f)\n    if species_list is not None:\n        use_pickle=False\n    pckl_filename=data_dir+'/MHC/MHC.pckl'    \n    if os.path.isfile(pckl_filename) and use_pickle:\n        print('MHC loading from MHC.pckl. To update the pickle file, set use_pickle to False')\n        with open(pckl_filename,'rb') as f:\n            mhcs,mhcs_df=pickle.load(f)\n    else:\n        cl_dict={'1':'I','2':'II'}\n        mhcs={}\n        mhcs_df=[]    \n        with open(data_dir+'/MHC/MHC.fasta') as f:\n            s=f.read()\n        s=_parse_fasta(s)    \n        for k,seq in s.items(): #k is e.g. 'MHC 9606 1 A A 01:01'        \n            _,species,cl,chain,locus,allele=k.split()\n            if (species_list is None) or (species in species_list):\n                cl=cl_dict[cl]        \n                num,ins,ss=num_ins_ss[cl+chain]\n                info={'species':species,'class':cl,'chain':chain,'locus':locus,'allele':allele}            \n                mhcs[species,locus,allele]=NUMSEQ(seq=seq,num=num,ins=ins,ss=ss,info=info).ungap()\n                mhcs_df.append([species,cl,chain,locus,allele])\n        mhcs_df=pd.DataFrame(mhcs_df,columns=['species_id','cl','chain','locus','allele'])\n        if species_list is None:        \n            with open(pckl_filename,'wb') as f:\n                pickle.dump((mhcs,mhcs_df),f)\n    print('loaded {} MHC sequences in {:4.1f} s'.format(len(mhcs),time.time()-t0))", "\n#MHC reconstruction\ndef mhc_from_seq(seq,species=None,target=None,return_boundaries=False,rename_by_g_region=True):\n    '''\n    if species given, search restricted to the corresponding database;\n    if target is given (NUMSEQ object), no search is done;  \n    returns NUMSEQ object for seq, including mutation information relative to the identified allele;\n    no gaps in aligned target allowed, but gaps are allowed in aligned seq;\n    output info includes gap counts for left, right, internal;\n    if return_boundaries (default False), returns left and right indices of alignment within query (0-based, ends included);\n    if rename_by_g_region (default True), \n    uses the first by sorting (exception: human first) (species,locus,allele) triple with the same g-region sequence\n    '''\n    if target is None:                \n        hits=blast_prot(seq,['MHC'],species=species)\n        hit=filter_blast_hits_to_single(hits,lambda k: k.startswith('MHC'))\n        if hit is None:\n            raise ValueError('no MHC hits found for MHC sequence')\n        if rename_by_g_region:\n            species,locus,allele=mhc_rename_dict[hit['species'],hit['locus'],hit['allele']]\n        else:\n            species,locus,allele=hit['species'],hit['locus'],hit['allele']\n        target=mhcs[species,locus,allele]                           \n    result,il,ir=realign(seq,target,'MHC')    \n    if return_boundaries:\n        return result,il,ir-1\n    else:\n        return result", "  \n######################### TCR TOOLS #########################\n#numeration and secondary structure\ntcr_dir=data_dir+'/TCR'\nwith open(tcr_dir+'/V_num_ins.pckl','rb') as f:\n    v_num_ins=pickle.load(f)\nwith open(tcr_dir+'/CDR3_num_ins.pckl','rb') as f:\n    cdr3_num_ins=pickle.load(f)    \nwith open(tcr_dir+'/J_FGXG.pckl','rb') as f:\n    j_fgxg=pickle.load(f)\nwith open(tcr_dir+'/ss.pckl','rb') as f:\n    tcr_ss=pickle.load(f)", "with open(tcr_dir+'/J_FGXG.pckl','rb') as f:\n    j_fgxg=pickle.load(f)\nwith open(tcr_dir+'/ss.pckl','rb') as f:\n    tcr_ss=pickle.load(f)\n\n#load\ndef load_tcrs(species_list=None,use_pickle=True):\n    '''optionally, restricts to species in species_list'''\n    global tcrs\n    global tcrs_df\n    t0=time.time()  \n    if species_list is not None:\n        use_pickle=False\n    pckl_filename=tcr_dir+'/TCR.pckl'\n    if os.path.isfile(pckl_filename) and use_pickle:\n        print('TCR loading from TCR.pckl. To update the pickle file, set use_pickle to False')\n        with open(pckl_filename,'rb') as f:\n            tcrs,tcrs_df=pickle.load(f)\n    else:\n        with open(tcr_dir+'/TCR.fasta') as f:\n            s=f.read()\n        s=_parse_fasta(s)\n        tcrs={}\n        tcrs_df=[]\n        for k,seq in s.items(): #TCR 37293 A J TRAJ2 01\n            _,species,chain,reg,locus,allele=k.split()\n            if (species_list is None) or (species in species_list):\n                info={'species':species,'chain':chain,'reg':reg,'locus':locus,'allele':allele}\n                tcrs_df.append([species,chain,reg,locus,allele])\n                if reg=='V':\n                    if chain=='A/D': #use chain 'A' templates for 'A/D'\n                        num,ins=v_num_ins[species,'A']\n                    else:\n                        num,ins=v_num_ins[species,chain]\n                    ss=[tcr_ss[i-1] for i in num]   \n                    ls=len(seq)\n                    ln=len(num)\n                    if ls>ln: #happens e.g. for 37293 TRAV12S1 01. (Likely a recombined sequence)\n                        seq=seq[:ln]\n                    elif ls<ln:\n                        num=num[:ls]\n                        ins=ins[:ls]\n                        ss=ss[:ls]                \n                    tcr=NUMSEQ(seq=seq,num=num,ins=ins,ss=ss,info=info).ungap()\n                elif reg=='J':\n                    pattern=j_fgxg.get((species,locus,allele)) or 'FG.G'                   \n                    search_i0=re.search(pattern,seq)\n                    if search_i0:\n                        i0=search_i0.start()\n                    else:\n                        raise ValueError(f'pattern {pattern} not found in {species,locus,allele}')                    \n                    num=np.arange(118-i0,118-i0+len(seq))\n                    tcr=NUMSEQ(seq=seq,num=num,ins=None,ss='J',info=info)                 \n                else:\n                    raise ValueError(f'reg {reg} not recognized')            \n                tcrs[species,locus,allele]=tcr\n        tcrs_df=pd.DataFrame(tcrs_df,columns=['species_id','chain','reg','locus','allele'])\n        if species_list is None:\n            with open(pckl_filename,'wb') as f:\n                pickle.dump((tcrs,tcrs_df),f)\n    print('loaded {} TCR sequences in {:4.1f} s'.format(len(tcrs),time.time()-t0))", "\n#TCR reconstruction\ndef tcr_from_genes(V,J,cdr3ext,strict=True): \n    '''\n    takes NUMSEQ objects V and J, and extended cdr3 sequence (1-res overhangs on both sides relative to cdr3_imgt);\n    reconstructs the full sequence and returns the tcr NUMSEQ object;\n    also returns nv and nj: lengths of V and J contig tails that match cdr3ext;\n    raises error if cdr3ext length not in [4,33];\n    if strict==True (default), also raises error if nv==0 or nj==0, i.e. when res 104 or 118 do not match in cdr3ext and V/J;\n    info for TCR constructed as follows:\n    'species' taken from V, 'V' e.g. TRAV1*01, 'J' e.g. 'TRBJ2*02', 'cdr3ext': cdr3ext sequence    \n    '''\n    lcdr=len(cdr3ext)-2    \n    if lcdr in cdr3_num_ins:\n        cdr_num,cdr_ins=cdr3_num_ins[lcdr]\n    else:\n        raise ValueError(f'cdr3ext length {lcdr+2} out of bounds')    \n    cdr3=NUMSEQ(seq=cdr3ext[1:-1],num=cdr_num,ins=cdr_ins,ss='CDR3')\n    tcr=join_NUMSEQ([V.get_fragment(-1,104),cdr3,J.get_fragment(118,500)])\n    info={}\n    info['species']=V.info['species']\n    info['V']=V.info['locus']+'*'+V.info['allele']\n    info['J']=J.info['locus']+'*'+J.info['allele']        \n    tcr.info=info        \n    for i,x in enumerate(zip(cdr3ext,V.get_fragment(104,500).data['seq'])): #iterates to min(len1,len2)\n        if x[0]!=x[1]:\n            break\n    n_v=i\n    for i,x in enumerate(zip(cdr3ext[::-1],J.get_fragment(0,118).data['seq'][::-1])): #iterates to min(len1,len2)\n        if x[0]!=x[1]:\n            break\n    n_j=i\n    if strict and (n_v<1 or n_j<1):\n        raise ValueError(f'cdr3ext mismatch with V or J: n_v={n_v}, n_j={n_j}')\n    return tcr,n_v,n_j", "\ndef tcr_from_seq(seq,species=None,V=None,J=None,return_boundaries=False):\n    '''\n    reconstructs tcr NUMSEQ object from sequence; takes seq, optionally species to restrict search;\n    optionally V and/or J objects (then no search for V and/or J is done);    \n    returns NUMSEQ object for TCR, including mutation information relative to the identified allele;\n    no gaps in aligned target allowed, but gaps are allowed in aligned seq;\n    output info includes gap counts for left, right, internal;\n    if return_boundaries (default False), returns left and right indices of alignment within query\n    (0-based, ends included)\n    '''    \n    #find and realign V  \n    if V is None:                \n        hits=blast_prot(seq,['TRV'],species=species)\n        hit=filter_blast_hits_to_single(hits,lambda k: (k.startswith('TCR') and k.endswith('V')))\n        if hit is None:\n            raise ValueError('no TRV hits found for TCR sequence')\n        V=tcrs[hit['species'],hit['locus'],hit['allele']]                    \n    V=V.get_fragment(-1,104) #restrict to framework region\n    V,i_V_left,i_cdr3_start=realign(seq,V,'TRV') \n    V_mutation_info=V.count_mutations()    \n    n_gaps_right_V=V_mutation_info['gaps_right']    \n    #require no gaps on the right (otherwise res 104 missing) and res 104 being C\n    if n_gaps_right_V or V.data['seq'][-1]!='C':\n        raise ValueError(f'framework V realign error: gaps on the right ({n_gaps_right_V}) or wrong last res')                                 \n    species=V.info['species'] #impose species from V in J search\n    #find and realign J\n    if J is None:            \n        hits=blast_prot(seq,['TRJ'],species=species)\n        hit=filter_blast_hits_to_single(hits,lambda k: (k.startswith('TCR') and k.endswith('J')))\n        if hit is None:\n            raise ValueError('no TRJ hits found for TCR sequence')\n        J=tcrs[hit['species'],hit['locus'],hit['allele']]                         \n    #first realign, then cut, because J outside cdr3 can be very short\n    J,i_cdr3_end,i_J_right=realign(seq,J,'TRJ')\n    if not (' 118 ' in J.data['pdbnum']):\n        raise ValueError(f'res 118 missing in realigned J;')\n    #adjust i_cdr3_end    \n    i_cdr3_end+=np.sum(J.data['pdbnum']<' 118 ')\n    #restrict to framework region\n    J=J.get_fragment(118,500)\n    J.info['gaps']=J.info['gaps'][J.info['gaps']['pdbnum']>=' 118 ']                \n    #make cdr3 object\n    cdr3=seq[i_cdr3_start:i_cdr3_end]\n    l=len(cdr3)\n    if l in cdr3_num_ins:\n        cdr_num,cdr_ins=cdr3_num_ins[l]\n    else:\n        raise ValueError(f'cdr3 {cdr3} of improper length')\n    cdr3=NUMSEQ(seq=cdr3,num=cdr_num,ins=cdr_ins,ss='CDR3')    \n    #make TCR object    \n    tcr=join_NUMSEQ([V,cdr3,J])\n    info=tcr.info.copy()    \n    info.pop('reg')\n    info.pop('locus')\n    info.pop('allele')    \n    if J.info['chain'] not in V.info['chain']: #should be equal or (A in A/D) or (D in A/D)        \n        print('Warning! V and J chain mismatch')\n        #warnings.warn('V and J chain mismatch')\n    if V.info['chain']=='A/D':\n        info['chain']=J.info['chain']\n    else:\n        info['chain']=V.info['chain']\n    #if V.info['species']!=J.info['species']:          #deprecated: now impose V species for J search\n    #    print('Warning! V and J species mismatch')\n    #    #warnings.warn('V and J species mismatch')            \n    info['species']=V.info['species']    \n    info['V']=V.info['locus']+'*'+V.info['allele']\n    info['J']=J.info['locus']+'*'+J.info['allele']         \n    tcr.info=info    \n    if return_boundaries:\n        return tcr,i_V_left,i_J_right-1\n    else:\n        return tcr", "\n#TO BE ADDED: \n#cdr3 improvement function from v2-2.1\n"]}
{"filename": "tfold/utils/pdb_tools.py", "chunked_list": ["#Victor Mikhaylov, vmikhayl@ias.edu\n#Institute for Advanced Study, 2021-2022\n\n#lookup HETATM entities here: https://www.ebi.ac.uk/pdbe-srv/pdbechem/\n\nimport Bio.PDB as PDB\nimport numpy as np\nimport os\nimport pickle\nimport re", "import pickle\nimport re\n\nfrom tfold.utils import seq_tools\n\naa_dict={'ARG':'R','HIS':'H','LYS':'K','ASP':'D','GLU':'E','SER':'S','THR':'T','ASN':'N','GLN':'Q','CYS':'C',\n         'GLY':'G','PRO':'P','ALA':'A','VAL':'V','ILE':'I','LEU':'L','MET':'M','PHE':'F','TYR':'Y','TRP':'W'}\n\n#A cartography of the van der Waals territories\n#Santiago Alvareza   ", "#A cartography of the van der Waals territories\n#Santiago Alvareza   \n#Dalton Trans., 2013,42, 8617-8636 \n#https://pubs.rsc.org/en/content/articlelanding/2013/DT/c3dt50599e\nvdw_radii={'C':1.77,'O':1.50,'N':1.66,'S':1.89,'H':0.,'P':1.90,'I':2.04,'F':1.46}\n#original {'H':1.20} switched to 0. The reason: most pdbs don't have hydrogens, but those that do\n#would disproportionately contribute to contact counting. Set H radius to 0 as a way to drop hydrogens.\n\n#### parsing ####\n\ndef parse_pdb(filename,name=None):\n    if name is None: #'/../../X.pdb' -> 'X'\n        name=filename.split('/')[-1].split('.')[0]        \n    pdb_parser=PDB.PDBParser(PERMISSIVE=False,QUIET=True)\n    structure=pdb_parser.get_structure(name,filename)[0]\n    header=pdb_parser.get_header()    \n    return structure,header   ", "#### parsing ####\n\ndef parse_pdb(filename,name=None):\n    if name is None: #'/../../X.pdb' -> 'X'\n        name=filename.split('/')[-1].split('.')[0]        \n    pdb_parser=PDB.PDBParser(PERMISSIVE=False,QUIET=True)\n    structure=pdb_parser.get_structure(name,filename)[0]\n    header=pdb_parser.get_header()    \n    return structure,header   \ndef save_pdb(obj,filename):\n    '''\n    save Bio.PDB pdb object to filename as pdb\n    '''\n    io=PDB.PDBIO()\n    io.set_structure(obj)\n    io.save(filename)", "def save_pdb(obj,filename):\n    '''\n    save Bio.PDB pdb object to filename as pdb\n    '''\n    io=PDB.PDBIO()\n    io.set_structure(obj)\n    io.save(filename)\ndef _int_or_repl(x,replacement=-100):\n    '''convert to int or return replacement (default -100)'''\n    try:\n        return int(x)\n    except ValueError:\n        return replacement ", "def _atom_to_chain_pdbnum(a):\n    '''takes Bio.PDB atom, returns Cnnnni'''\n    chain,res_id,___=a.get_full_id()[-3:] #note: can be len 5 or len 4 (first entry is pdb_id, drops upon copying)\n    _,num,ins=res_id\n    num=_int_or_repl(num)\n    return '{:1s}{:4d}{:1s}'.format(chain,num,ins)\ndef get_structure_dict(structure,include_coords,keep_hetero=True):\n    '''\n    takes a Bio.PDB object with get_atoms method;\n    if include_coords: returns dict {chain:{pdbnum:{atom_name:array_xyz,..},..},..},\n    otherwise:         returns dict {chain:{pdbnum:[atom_name,..],..},..};    \n    if keep_hetero (default True), keeps hetero res/atoms, otherwise drops them; #SWITCHED TO DEFAULT TRUE!!!\n    (always drops waters)\n    '''\n    structure_dict={}\n    for a in structure.get_atoms():\n        chain,res_id,atom_id=a.get_full_id()[-3:]\n        het,num,ins=res_id\n        if not (het.strip()=='W'): #drop waters\n            atomname,_=atom_id\n            pdbnum='{:4d}{:1s}'.format(_int_or_repl(num),ins)\n            if keep_hetero or not het.strip():\n                if include_coords:\n                    structure_dict.setdefault(chain,{}).setdefault(pdbnum,{})[atomname]=a.get_coord()\n                else:                \n                    structure_dict.setdefault(chain,{}).setdefault(pdbnum,[]).append(atomname)\n    return structure_dict    ", "hetero_exclude=['W','GOL','NAG','SO4','EDO','NA','FUC','ACT','CL','BMA','MAN','PEG'] #most common hence likely uninformative\ndef get_chain_sequences(structure):\n    '''\n    takes a Bio.PDB object with .get_residues method,\n    returns a dict {'canonical':d1,'modified':d2,'hetero':d3}, \n    where each d is {chain:NUMSEQ};\n    canonical includes 'X' for hetero or modified! Drops waters;\n    note: when num from pdb not convertible to int, set to default e.g. -100, as a replacement for np.nan which is float;\n    NUMSEQ info includes pdb_id and chain\n    '''    \n    sequences={'canonical':{},'modified':{},'hetero':{}}\n    for x in structure.get_residues():\n        chain_id,res_id=x.get_full_id()[-2:] #'1ao7'(optional), 0, 'A', (' ', 2, ' '))                \n        het,num,ins=res_id\n        if not (het.strip() in hetero_exclude): #exclude most common HETATM that are likely not interesting\n            num=_int_or_repl(num)\n            aa=x.resname\n            aa1=aa_dict.get(aa) or 'X'\n            sequences['canonical'].setdefault(chain_id,[]).append([aa1,num,ins])\n            if (not het.strip()) and (aa not in aa_dict):\n                sequences['modified'].setdefault(chain_id,[]).append([aa,num,ins])\n            if het.strip():\n                sequences['hetero'].setdefault(chain_id,[]).append([aa,num,ins])               \n    sequences1={}\n    for k,s in sequences.items():\n        sequences1[k]={}\n        for c in s:\n            seq=[x[0] for x in s[c]]\n            num=[x[1] for x in s[c]]\n            ins=[x[2] for x in s[c]]\n            info={'chain':c}\n            sequences1[k][c]=seq_tools.NUMSEQ(seq=seq,num=num,ins=ins,info=info) \n    return sequences1", "\n#### maps ####\n\ndef chainmap_to_resmap(structure1,structure2,chainmap,verbose=False):\n    '''\n    takes two structures and a chainmap, e.g. [['M','N'],['M','M'], ['A','A'], ['P','P']]; \n    returns resmap which matches residues with identical pdbnums in each chain pair,\n    e.g. [['M1070 ','N1070 '],['P   5 ','P   5 ']]\n    '''    \n    structure1_dict=get_structure_dict(structure1,include_coords=False)\n    structure2_dict=get_structure_dict(structure2,include_coords=False)\n    resmap=[]\n    for x,y in chainmap:\n        res1=structure1_dict.get(x)\n        res2=structure2_dict.get(y)\n        if (res1 is not None) and (res2 is not None):\n            res1=set(res1.keys())\n            res2=set(res2.keys())\n            res_both=res1&res2\n            delta1=res1-res_both\n            delta2=res2-res_both\n            if verbose:\n                if delta1:\n                    print(f'res {delta1} present in structure 1 chain {x} but missing in structure 2 chain {y};')\n                if delta2:\n                    print(f'res {delta2} present in structure 2 chain {y} but missing in structure 1 chain {x};')\n            for r in res_both:\n                resmap.append([x+r,y+r])\n        elif verbose:\n            if res1 is None:\n                print(f'chain {x} missing in structure 1;')\n            if res2 is None:\n                print(f'chain {y} missing in structure 2;')    \n    return resmap", "def resmap_to_atommap(structure1,structure2,resmap,CA_only=False,allow_missing_res=False,verbose=False):\n    '''\n    if allow_missing_res==False, will raise error when residues from resmap are missing in structure,\n    otherwise will skip those residue pairs\n    '''\n    structure1_dict=get_structure_dict(structure1,include_coords=False)\n    structure2_dict=get_structure_dict(structure2,include_coords=False)\n    atoms1=[]\n    atoms2=[]    \n    for x,y in resmap:\n        chain1=x[0]\n        pdbnum1=x[1:]\n        chain2=y[0]\n        pdbnum2=y[1:]\n        #assume resnum was properly generated. If not, raise errors\n        if (chain1 not in structure1_dict) or (chain2 not in structure2_dict):\n            raise ValueError('defective resmap: chains missing in structure;')        \n        res1_dict=structure1_dict[chain1]\n        res2_dict=structure2_dict[chain2]\n        if (pdbnum1 not in res1_dict) or (pdbnum2 not in res2_dict):\n            if allow_missing_res:\n                continue\n            else:\n                raise ValueError('defective resmap: pdbnums missing in structure;')\n        atom_names1=res1_dict[pdbnum1]\n        atom_names2=res2_dict[pdbnum2]        \n        if CA_only:\n            if ('CA' in atom_names1) and ('CA' in atom_names2):\n                atoms1.append((x,'CA'))\n                atoms2.append((y,'CA'))\n            elif verbose:\n                if 'CA' not in atom_names1:\n                    print(f'CA missing in structure 1 residue {x}')\n                if 'CA' not in atom_names2:\n                    print(f'CA missing in structure 2 residue {y}')\n        else:            \n            atoms_both=set(atom_names1)&set(atom_names2)\n            for a in atoms_both:\n                atoms1.append((x,a))\n                atoms2.append((y,a)) \n    #make atommap with atom objects\n    atoms=[atoms1,atoms2]    \n    atommap=[[None,None] for x in atoms1]\n    for i,structure in enumerate([structure1,structure2]):\n        for a in structure.get_atoms():\n            x=_atom_to_chain_pdbnum(a)\n            if (x,a.name) in atoms[i]:\n                ind=atoms[i].index((x,a.name))\n                atommap[ind][i]=a    \n    return atommap", "\n#### distances and contacts ####\n\ndef distance2_matrix(x1,x2):\n    '''\n    takes two non-empty np arrays of coordinates.\n    Returns d_ij^2 matrix\n    '''    \n    delta_x=np.tile(x1[:,np.newaxis,:],[1,len(x2),1])-np.tile(x2[np.newaxis,:,:],[len(x1),1,1])\n    return np.sum(delta_x**2,axis=2)\ndef find_contact_atoms(a1,a2,eps=0,pqr=False):\n    '''\n    takes two lists of pqr or pdb Bio.PDB atom objects;\n    returns boolean contact matrix;\n    if eps given (default eps=0), detects contact for threshold (1+eps)*(r1+r2);\n    uses radii from pqr (pqr=True) or fixed radii (pqr=False, default);        \n    '''    \n    factor=(1+eps)**2 \n    x1=np.array([a.get_coord() for a in a1])\n    x2=np.array([a.get_coord() for a in a2])\n    dij2=distance2_matrix(x1,x2)\n    if pqr:\n        rij2=np.array([[(a.radius+b.radius)**2 for b in a2] for a in a1])\n    else:        \n        rij2=np.array([[(vdw_radii[re.sub('[0-9]','',a.name)[0]]+\n                         vdw_radii[re.sub('[0-9]','',b.name)[0]])**2 for b in a2] for a in a1])    \n    return dij2<=factor*rij2 #boolean contact matrix    ", "def find_contact_atoms(a1,a2,eps=0,pqr=False):\n    '''\n    takes two lists of pqr or pdb Bio.PDB atom objects;\n    returns boolean contact matrix;\n    if eps given (default eps=0), detects contact for threshold (1+eps)*(r1+r2);\n    uses radii from pqr (pqr=True) or fixed radii (pqr=False, default);        \n    '''    \n    factor=(1+eps)**2 \n    x1=np.array([a.get_coord() for a in a1])\n    x2=np.array([a.get_coord() for a in a2])\n    dij2=distance2_matrix(x1,x2)\n    if pqr:\n        rij2=np.array([[(a.radius+b.radius)**2 for b in a2] for a in a1])\n    else:        \n        rij2=np.array([[(vdw_radii[re.sub('[0-9]','',a.name)[0]]+\n                         vdw_radii[re.sub('[0-9]','',b.name)[0]])**2 for b in a2] for a in a1])    \n    return dij2<=factor*rij2 #boolean contact matrix    ", "def count_r_by_r_contacts(a_list1,a_list2,eps=0,pqr=False,drop_zeros=False):  \n    '''\n    takes two lists of Bio.PDB atom objects; returns a dict for res by res contact numbers;\n    dict keys are pairs of Cnnnni; zero contact entries kept by default, to drop them, set flag drop_zeros;\n    if eps given (default eps=0), detects contact for threshold (1+eps)*(r1+r2);\n    uses radii from pqr (pqr=True) or fixed radii (pqr=False, default);   \n    '''\n    resnames1=np.array([_atom_to_chain_pdbnum(a) for a in a_list1])\n    resnames2=np.array([_atom_to_chain_pdbnum(a) for a in a_list2])\n    resnames1_u=np.unique(resnames1)\n    resnames2_u=np.unique(resnames2)\n    matrix=find_contact_atoms(a_list1,a_list2,eps,pqr).astype(np.int16)    \n    contact_counts={}\n    for r1 in resnames1_u:\n        ind1=resnames1==r1        \n        for r2 in resnames2_u:\n            ind2=resnames2==r2            \n            m=matrix[ind1,:]\n            m=m[:,ind2]          \n            n=np.sum(m)\n            if n or not drop_zeros:\n                contact_counts[r1,r2]=n\n    return contact_counts      ", "def make_contact_maps(filename,output_dir):\n    '''\n    make and save interchain contact maps for a pdb file\n    '''\n    os.makedirs(output_dir,exist_ok=True)\n    pdb_id=filename.split('/')[-1].split('.')[0]\n    print(f'processing {pdb_id}...')\n    structure,header=parse_pdb(filename)\n    chains=list(structure.get_chains())\n    chain_ids=[c.id for c in chains]\n    contact_maps={}\n    for i,c1 in enumerate(chain_ids):\n        #(drop elements \n        atomlist1=[a for a in chains[i].get_atoms() if re.sub('[0-9]','',a.name)[0] in vdw_radii]\n        for j,c2 in enumerate(chain_ids[i+1:]):\n            atomlist2=[a for a in chains[j+i+1].get_atoms() if re.sub('[0-9]','',a.name)[0] in vdw_radii]\n            contact_maps[c1,c2]=count_r_by_r_contacts(atomlist1,atomlist2,eps=0.1)\n    with open(output_dir+'/'+pdb_id+'.pckl','wb') as f:\n        pickle.dump(contact_maps,f)", "\n#### superimposing ####\n\n#a quick fix for a bug in transforming disordered atoms\n#(from https://github.com/biopython/biopython/issues/455)\n#NOTE: only keeps position A of disordered atoms and drops the rest\ndef get_unpacked_list_patch(self):\n    '''\n    Returns all atoms from the residue;\n    in case of disordered, keep only first alt loc and remove the alt-loc tag\n    '''\n    atom_list = self.get_list()\n    undisordered_atom_list = []\n    for atom in atom_list:\n        if atom.is_disordered():\n            atom.altloc=\" \"\n            undisordered_atom_list.append(atom)\n        else:\n            undisordered_atom_list.append(atom)\n    return undisordered_atom_list", "PDB.Residue.Residue.get_unpacked_list=get_unpacked_list_patch\ndef superimpose_by_resmap(structure1,structure2,resmap,CA_only=True,allow_missing_res=False,verbose=False):\n    '''\n    superimpose structure1 onto structure2 according to given resmap;\n    resmap should be a list of pairs ['Cnnnni','Cnnnni'] of corresponding residues;\n    if CA_only=True (default), only uses CA atoms;    \n    if allow_missing_res (default False), does not raise error when residue in resmap are missing in structure;\n    transforms structure1 in place; returns rmsd\n    '''\n    atommap=resmap_to_atommap(structure1,structure2,resmap,CA_only,allow_missing_res,verbose)    \n    if verbose:\n        print(f'superimposing on {len(atommap)} atoms...')     \n    atoms1,atoms2=zip(*atommap)\n    sup=PDB.Superimposer()\n    sup.set_atoms(atoms2,atoms1)\n    sup.apply(structure1)\n    return sup.rms            ", "def superimpose_by_chainmap(structure1,structure2,chainmap,CA_only=True,verbose=False):\n    '''\n    superimpose structure1 onto structure2 according to a chainmap;\n    chainmap is e.g. [['M','N'],['M','M'], ['A','A'], ['P','P']]; matching pdbnums in each pair of chains used;    \n    if CA_only=True (default), only uses CA atoms;    \n    transforms structure1 in place; returns rmsd    \n    '''    \n    resmap=chainmap_to_resmap(structure1,structure2,chainmap,verbose)\n    rmsd=superimpose_by_resmap(structure1,structure2,resmap,CA_only,verbose)\n    return rmsd", "    \n#### rmsd ####\ndef rmsd_by_resmap(structure1,structure2,resmap,allow_missing_res=False,verbose=False):\n    '''\n    compute rmsds (CA and all-atom) according to resmap;\n    note: resmap should be list, not zip!\n    does not superimpose!\n    '''\n    result={}\n    for name in ['CA','all']:\n        CA_only=name=='CA'        \n        atommap=resmap_to_atommap(structure1,structure2,resmap,CA_only=CA_only,\n                                  allow_missing_res=allow_missing_res,verbose=verbose) \n        if verbose:\n            print(f'rmsd_{name} over {len(atommap)} atoms...')\n        d2s=[]\n        for a,b in atommap:\n            delta=a.get_coord()-b.get_coord()\n            d2s.append(np.dot(delta,delta))\n        result[name]=np.average(d2s)**0.5\n    return result        ", "def rmsd_by_chainmap(structure1,structure2,chainmap,verbose=False):\n    '''\n    compute rmsds (CA and all-atom) according to chainmap;\n    does not superimpose!\n    '''\n    resmap=chainmap_to_resmap(structure1,structure2,chainmap,verbose=verbose)\n    if verbose:\n        print(f'rmsd over {len(resmap)} residues...')\n    return rmsd_by_resmap(structure1,structure2,resmap,verbose)\n    ", "    \n### main ###\n\n_func_dict={'make_contact_maps':make_contact_maps}\nif __name__=='__main__': \n    import time    \n    from argparse import ArgumentParser\n    import csv\n    t0=time.time()    \n    parser=ArgumentParser()\n    parser.add_argument('input_filename', type=str, help='path to input file')    \n    args=parser.parse_args()  \n    inputs=[]\n    with open(args.input_filename) as f:\n        f_csv=csv.reader(f,delimiter='\\t')\n        inputs=[x for x in f_csv]        \n    print(f'processing {len(inputs)} tasks...')\n    for x in inputs:                                #input format: [function_name,*args]\n        processing_function=_func_dict[x[0]]\n        processing_function(*x[1:])        \n    print('finished {} tasks in {} s'.format(len(inputs),time.time()-t0))", "                           \n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n\n\n", "\n\n"]}
{"filename": "tfold/nn/nn_utils.py", "chunked_list": ["def generate_registers_I(l):\n    '''\n    generate all admissible pairs of tails for a cl II pep of length l;\n    assume only combinations (0,x) and (x,0) allowed, to avoid quadratic proliferaton of registers;\n    experimentally, there is only one structure with tails (-1,1) violating this rule\n    '''\n    if l<8:\n        raise ValueError('peplen<8 not allowed for cl I')\n    registers=[]\n    for i in range(-1,min(2,l-7)):\n        registers.append((i,0))\n    if l>8:\n        for i in range(1,l-7):\n            registers.append((0,i))\n    return registers", "def generate_registers_II(l):\n    '''\n    generate all admissible pairs of tails for a cl II pep of length l;\n    assume a flat binding core of len 9aa;\n    experimentally, there is one structure with peplen 10 and left tail -1 violating the rule,\n    and no structures with bulged or stretched binding core, except possibly for pig MHC or such\n    '''\n    if l<9:\n        raise ValueError('peplen<9 not allowed for cl II')    \n    registers=[]\n    for i in range(l-8):\n        registers.append((i,l-i-9))\n    return registers", "    "]}
{"filename": "tfold/nn/models.py", "chunked_list": ["import numpy as np\nimport pickle\n\nfrom tensorflow.keras.regularizers import l1,l2\nfrom tensorflow.keras.constraints import non_neg\nfrom tensorflow.math import reduce_sum, reduce_min, reduce_mean, log as tf_log, exp as tf_exp\nfrom tensorflow import reshape, expand_dims, stack, squeeze, tile, newaxis, cast, float32 as tf_float32, gather\n\nfrom tensorflow.nn import softmax\n", "from tensorflow.nn import softmax\n\nfrom tensorflow.keras.layers import Input, Activation\nfrom tensorflow.keras.layers import Dense, Flatten, Conv1D\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\n\nfrom tensorflow.keras import Model\n\nfrom tfold.config import seqnn_params\nfrom tfold.nn import nn_utils", "from tfold.config import seqnn_params\nfrom tfold.nn import nn_utils\n\nn_pep_dict={'I':seqnn_params['max_core_len_I']+2,'II':9}\nn_mhc_dict={'I':seqnn_params['n_mhc_I'],'II':seqnn_params['n_mhc_II']}\nmax_registers_dict={'I' :len(nn_utils.generate_registers_I(seqnn_params['max_pep_len_I'])),\n                    'II':len(nn_utils.generate_registers_II(seqnn_params['max_pep_len_II']))}\nn_tail_bits=seqnn_params['n_tail_bits']\n\ndef positional_encoding(n_positions,n_bits):\n    omega=lambda i: (1/n_positions)**(2*(i//2)/n_bits)\n    bits=np.arange(n_bits)[np.newaxis,:]\n    positions=np.arange(n_positions)[:,np.newaxis]\n    pos_enc=omega(bits)*positions\n    pos_enc[:,0::2]=np.sin(pos_enc[:,0::2])\n    pos_enc[:,1::2]=np.cos(pos_enc[:,1::2])\n    return cast(pos_enc, dtype=tf_float32)", "\ndef positional_encoding(n_positions,n_bits):\n    omega=lambda i: (1/n_positions)**(2*(i//2)/n_bits)\n    bits=np.arange(n_bits)[np.newaxis,:]\n    positions=np.arange(n_positions)[:,np.newaxis]\n    pos_enc=omega(bits)*positions\n    pos_enc[:,0::2]=np.sin(pos_enc[:,0::2])\n    pos_enc[:,1::2]=np.cos(pos_enc[:,1::2])\n    return cast(pos_enc, dtype=tf_float32)\n\ndef fully_connected(params):\n    \n    cl=params['cl']\n    n_pep=n_pep_dict[cl]\n    n_mhc=n_mhc_dict[cl]\n    max_registers=max_registers_dict[cl]    \n    \n    pep_mask=params.setdefault('pep_mask',None)    \n    n_hidden=params['n_hidden']\n    if type(n_hidden)==int:\n        n_hidden=[n_hidden]    \n    actn=params['actn']\n    reg_l2_weight=params.get('reg_l2') or 0.\n    if reg_l2_weight:\n        reg_l2=l2(reg_l2_weight)\n    else:\n        reg_l2=None   \n    #reg=params['reg']\n    batch_norm=params.setdefault('batch_norm',False)\n    dropout_rate=params['dropout_rate']\n    model_name=f'fully_connected'\n    use_tails=params.get('use_tails') or False   #whether input includes encoded tail lengths (to use in cl II)    \n    \n    input_pep=Input(shape=(max_registers,n_pep,21),name='pep')\n    input_mhc=Input(shape=(n_mhc,21),name='mhc')\n    if use_tails:        \n        input_tails=Input(shape=(max_registers,2,n_tail_bits),name='tails')\n    \n    if not (pep_mask is None):\n        x_pep=gather(input_pep,[i-1 for i in pep_mask],axis=2)\n        n_pep=len(pep_mask)\n    else:\n        x_pep=input_pep\n        \n    x_pep=reshape(x_pep,shape=(-1,max_registers,n_pep*21))    \n    x_mhc=Flatten()(input_mhc)    \n    dense_pep=Dense(n_hidden[0],kernel_regularizer=reg_l2)\n    dense_mhc=Dense(n_hidden[0],kernel_regularizer=reg_l2)\n    x_pep=dense_pep(x_pep)        \n    x_mhc=dense_mhc(x_mhc)\n    x_mhc=tile(x_mhc[:,newaxis,:],[1,max_registers,1])\n    x=x_pep+x_mhc\n    if use_tails:\n        x_tails=reshape(input_tails,shape=[-1,max_registers,2*n_tail_bits])\n        dense_tails=Dense(n_hidden[0],kernel_regularizer=reg_l2)\n        x_tails=dense_tails(x_tails)\n        x+=x_tails\n    x=Activation(actn)(x)\n    x=Dropout(dropout_rate)(x)\n    if batch_norm:\n        x=BatchNormalization()(x)\n    for i,n in enumerate(n_hidden[1:]):\n        dense_n=Dense(n,activation=actn,kernel_regularizer=reg_l2)\n        x=dense_n(x)\n        if i<len(n_hidden)-2: #don't do on the last one\n            x=Dropout(dropout_rate)(x)\n        if batch_norm:\n            x=BatchNormalization()(x)    \n    dense_out=Dense(1)\n    x=dense_out(x)    \n    x=x[:,:,0]  \n    if use_tails:\n        model=Model(inputs=[input_pep,input_mhc,input_tails],outputs=x,name=model_name)\n    else:\n        model=Model(inputs=[input_pep,input_mhc],outputs=x,name=model_name)\n    return model", "\ndef fully_connected(params):\n    \n    cl=params['cl']\n    n_pep=n_pep_dict[cl]\n    n_mhc=n_mhc_dict[cl]\n    max_registers=max_registers_dict[cl]    \n    \n    pep_mask=params.setdefault('pep_mask',None)    \n    n_hidden=params['n_hidden']\n    if type(n_hidden)==int:\n        n_hidden=[n_hidden]    \n    actn=params['actn']\n    reg_l2_weight=params.get('reg_l2') or 0.\n    if reg_l2_weight:\n        reg_l2=l2(reg_l2_weight)\n    else:\n        reg_l2=None   \n    #reg=params['reg']\n    batch_norm=params.setdefault('batch_norm',False)\n    dropout_rate=params['dropout_rate']\n    model_name=f'fully_connected'\n    use_tails=params.get('use_tails') or False   #whether input includes encoded tail lengths (to use in cl II)    \n    \n    input_pep=Input(shape=(max_registers,n_pep,21),name='pep')\n    input_mhc=Input(shape=(n_mhc,21),name='mhc')\n    if use_tails:        \n        input_tails=Input(shape=(max_registers,2,n_tail_bits),name='tails')\n    \n    if not (pep_mask is None):\n        x_pep=gather(input_pep,[i-1 for i in pep_mask],axis=2)\n        n_pep=len(pep_mask)\n    else:\n        x_pep=input_pep\n        \n    x_pep=reshape(x_pep,shape=(-1,max_registers,n_pep*21))    \n    x_mhc=Flatten()(input_mhc)    \n    dense_pep=Dense(n_hidden[0],kernel_regularizer=reg_l2)\n    dense_mhc=Dense(n_hidden[0],kernel_regularizer=reg_l2)\n    x_pep=dense_pep(x_pep)        \n    x_mhc=dense_mhc(x_mhc)\n    x_mhc=tile(x_mhc[:,newaxis,:],[1,max_registers,1])\n    x=x_pep+x_mhc\n    if use_tails:\n        x_tails=reshape(input_tails,shape=[-1,max_registers,2*n_tail_bits])\n        dense_tails=Dense(n_hidden[0],kernel_regularizer=reg_l2)\n        x_tails=dense_tails(x_tails)\n        x+=x_tails\n    x=Activation(actn)(x)\n    x=Dropout(dropout_rate)(x)\n    if batch_norm:\n        x=BatchNormalization()(x)\n    for i,n in enumerate(n_hidden[1:]):\n        dense_n=Dense(n,activation=actn,kernel_regularizer=reg_l2)\n        x=dense_n(x)\n        if i<len(n_hidden)-2: #don't do on the last one\n            x=Dropout(dropout_rate)(x)\n        if batch_norm:\n            x=BatchNormalization()(x)    \n    dense_out=Dense(1)\n    x=dense_out(x)    \n    x=x[:,:,0]  \n    if use_tails:\n        model=Model(inputs=[input_pep,input_mhc,input_tails],outputs=x,name=model_name)\n    else:\n        model=Model(inputs=[input_pep,input_mhc],outputs=x,name=model_name)\n    return model", "\ndef pairwise_energy_mini(params):\n    model_name=f'pairwise_energy_mini'        \n    \n    cl=params['cl']\n    n_pep=n_pep_dict[cl]\n    n_mhc=n_mhc_dict[cl]\n    max_registers=max_registers_dict[cl]    \n    \n    reg_l1_weight=params.get('reg_l1') or 0.\n    if reg_l1_weight:\n        reg_l1=l1(reg_l1_weight)\n    else:\n        reg_l1=None    \n    use_tails=params['use_tails']   #whether input includes encoded tail lengths (to use in cl II)   \n    symmetrize_aa_matrix=params['symmetrize_aa_matrix']                    \n    \n    input_pep=Input(shape=(max_registers,n_pep,21),name='pep')\n    input_mhc=Input(shape=(n_mhc,21),name='mhc')\n    if use_tails:        \n        input_tails=Input(shape=(max_registers,2,n_tail_bits),name='tails')\n    \n    x_pep=input_pep\n    x_pep=expand_dims(x_pep,axis=3)\n    x_pep=tile(x_pep,[1,1,1,n_mhc,1])   #-1, max_registers, n_pep, n_mhc, 21               \n    \n    x_mhc=input_mhc\n    x_mhc=expand_dims(input_mhc,axis=1)\n    x_mhc=expand_dims(x_mhc,axis=1)   \n    x_mhc=tile(x_mhc,[1,max_registers,n_pep,1,1]) #-1, max_registers, n_pep, n_mhc, 21 \n    \n    if params.get('allow_aa_bias'):\n        dense_aa=Dense(21,name='aa_matrix')        \n    else:\n        dense_aa=Dense(21,name='aa_matrix',use_bias=False)        \n    x=reduce_sum(dense_aa(x_pep)*x_mhc,axis=-1)    #-1, max_registers, n_pep, n_mhc\n    if symmetrize_aa_matrix:\n        x+=reduce_sum(dense_aa(x_mhc)*x_pep,axis=-1)                \n    x=reshape(x,[-1,max_registers,n_pep*n_mhc])\n            \n    dense_rr=Dense(1,kernel_regularizer=reg_l1,kernel_constraint=non_neg(),name='res_res_matrix')\n    x=dense_rr(x)\n    \n    if use_tails:\n        x_tails=reshape(input_tails,shape=[-1,max_registers,2*n_tail_bits])\n        dense_tails=Dense(1)\n        x+=dense_tails(x_tails)            \n    x=x[:,:,0]    \n    if use_tails:\n        model=Model(inputs=[input_pep,input_mhc,input_tails],outputs=x,name=model_name)\n    else:\n        model=Model(inputs=[input_pep,input_mhc],outputs=x,name=model_name)\n    return model", "\ndef pairwise_energy(params):\n    \n    cl=params['cl']\n    n_pep=n_pep_dict[cl]\n    n_mhc=n_mhc_dict[cl]\n    max_registers=max_registers_dict[cl]    \n    \n    n_hidden=params['n_hidden']    \n    if type(n_hidden)==int:\n        n_hidden=[n_hidden]\n    aa_pair_features=params['aa_pair_features']  \n    batch_norm=params.setdefault('batch_norm',False)    \n        \n    actn=params['actn']\n    #reg=params['reg']\n    dropout_rate=params['dropout_rate']\n    use_tails=params.get('use_tails') or False   #whether input includes encoded tail lengths (to use in cl II)   \n    model_name=f'pairwise_energy'        \n    \n    input_pep=Input(shape=(max_registers,n_pep,21),name='pep')\n    input_mhc=Input(shape=(n_mhc,21),name='mhc')\n    if use_tails:        \n        input_tails=Input(shape=(max_registers,2,n_tail_bits),name='tails')\n        \n    dense_aa=Dense(21*aa_pair_features)    \n    x_pep=dense_aa(input_pep)           #-1, max_registers, n_pep, 21*aa_pair_features                \n    x_pep=expand_dims(x_pep,axis=3)\n    x_pep=tile(x_pep,[1,1,1,n_mhc,1])   #-1, max_registers, n_pep, n_mhc, 21*aa_pair_features                \n    x_pep=reshape(x_pep,[-1,max_registers,n_pep,n_mhc,aa_pair_features,21])\n    \n    x_mhc=expand_dims(input_mhc,axis=1)\n    x_mhc=expand_dims(x_mhc,axis=1)\n    x_mhc=expand_dims(x_mhc,axis=4) #-1, 1, 1, n_mhc, 1, 21\n    x_mhc=tile(x_mhc,[1,max_registers,n_pep,1,aa_pair_features,1])     \n    \n    x=reduce_sum(x_pep*x_mhc,axis=-1)                \n    x=reshape(x,[-1,max_registers,n_pep*n_mhc*aa_pair_features])\n    \n    #hidden[0] separately, to add tails if needed\n    if batch_norm:\n        x=BatchNormalization()(x) \n    dense_0=Dense(n_hidden[0])\n    x=dense_0(x)\n    x=Dropout(dropout_rate)(x)               \n    if use_tails:\n        x_tails=reshape(input_tails,shape=[-1,max_registers,2*n_tail_bits])\n        dense_tails=Dense(n_hidden[0])\n        x_tails=dense_tails(x_tails)\n        x+=x_tails\n    x=Activation(actn)(x)\n           \n    for n in n_hidden[1:]:\n        if batch_norm:\n            x=BatchNormalization()(x) \n        dense_n=Dense(n,activation=actn)\n        x=dense_n(x)\n        x=Dropout(dropout_rate)(x)           \n            \n    dense_out=Dense(1)\n    x=dense_out(x)    \n    x=x[:,:,0]    \n    if use_tails:\n        model=Model(inputs=[input_pep,input_mhc,input_tails],outputs=x,name=model_name)\n    else:\n        model=Model(inputs=[input_pep,input_mhc],outputs=x,name=model_name)\n    return model", "\ndef attention_convolution(params):\n    \n    cl=params['cl']\n    n_pep=n_pep_dict[cl]\n    n_mhc=n_mhc_dict[cl]\n    max_registers=max_registers_dict[cl]    \n    \n    actn=params['actn']    \n    dropout_rate=params['dropout_rate']     \n    model_name=f'attention_convolution'      \n        \n    n_values=params['n_values']     \n    n_values_last=params['n_values_last']\n    kernel_size=params['kernel_size']\n    n_blocks=params['n_blocks']\n    reduce_x=params['reduce_x']\n    n_hidden=params['n_hidden']\n    if type(n_hidden)==int:\n        n_hidden=[n_hidden]\n    \n    input_pep=Input(shape=(max_registers,n_pep,21),name='pep')\n    input_mhc=Input(shape=(n_mhc,21),name='mhc')        \n    \n    #encode pep, add positional encoding\n    pep_embedding=Dense(n_values,name='pep_embedding')\n    x_pep=pep_embedding(input_pep) #[None,max_registers,n_pep,n_values]\n    x_pep+=positional_encoding(n_pep,n_values)[newaxis,newaxis,:,:]        \n    x_pep=BatchNormalization()(x_pep)\n    \n    #encode mhc, add positional encoding\n    mhc_embedding=Dense(n_values,name='mhc_embedding')\n    x_mhc=mhc_embedding(input_mhc) #[None,n_mhc,n_values]    \n    x_mhc+=positional_encoding(n_mhc,n_values)[newaxis,:,:]\n    x_mhc=tile(x_mhc[:,newaxis,newaxis,:,:],[1,max_registers,n_pep,1,1]) #[None,max_registers,n_pep,n_mhc,n_values]\n    x_mhc=BatchNormalization()(x_mhc)\n    \n    #attention   \n    for i in range(n_blocks):\n        att_mask_layer=Conv1D(n_mhc,1,strides=1,activation=None,name=f'att_mask_{i}')\n        att_gate_layer=Conv1D(1,1,strides=1,activation='sigmoid',name=f'gate_{i}')\n        att_mask=softmax(att_mask_layer(x_pep),axis=-1)             #[None,max_registers,n_pep,n_mhc]    \n        att_mask=tile(att_mask[:,:,:,:,newaxis],[1,1,1,1,n_values]) #[None,max_registers,n_pep,n_mhc,n_values] \n        att_gate=tile(att_gate_layer(x_pep),[1,1,1,n_values])       #[None,max_registers,n_pep,n_values]     \n        x_pep+=att_gate*reduce_sum(att_mask*x_mhc,axis=3)           #[None,max_registers,n_pep,n_values] \n        x_pep=BatchNormalization()(x_pep)\n        #convolution\n        pep_conv_layer=Conv1D(n_values,kernel_size,strides=1,padding='same',activation=actn,name=f'pep_conv_{i}')\n        x_pep=pep_conv_layer(x_pep)\n        x_pep=BatchNormalization()(x_pep)\n    \n    #output stack\n    if reduce_x:\n        x_pep=reduce_mean(x_pep,axis=2)     #[None,max_registers,n_values] \n    else:\n        pep_conv_reduce_layer=Conv1D(n_values_last,1,strides=1,padding='same',activation=actn,name='pep_conv_final')\n        x_pep=pep_conv_reduce_layer(x_pep)\n        x_pep=reshape(x_pep,[-1,max_registers,n_pep*n_values_last])\n    for i,n in enumerate(n_hidden):\n        dense_n=Dense(n,activation=actn,name=f'dense_{i}')\n        x_pep=Dropout(dropout_rate)(x_pep)   \n        x_pep=dense_n(x_pep)\n        x_pep=BatchNormalization()(x_pep)  \n    x=x_pep\n    dense_out=Dense(1,name='dense_output')\n    x=dense_out(x)    \n    x=x[:,:,0]    \n    return Model(inputs=[input_pep,input_mhc],outputs=x,name=model_name)", "\ndef attention_convolution1(params):\n    \n    cl=params['cl']\n    n_pep=n_pep_dict[cl]\n    n_mhc=n_mhc_dict[cl]\n    max_registers=max_registers_dict[cl]    \n    \n    actn=params['actn']    \n    dropout_rate=params['dropout_rate']     \n    model_name=f'attention_convolution'      \n        \n    n_values=params['n_values']     \n    n_values_last=params['n_values_last']\n    kernel_size=params['kernel_size']\n    n_blocks=params['n_blocks']\n    reduce_x=params['reduce_x']\n    n_hidden=params['n_hidden']\n    if type(n_hidden)==int:\n        n_hidden=[n_hidden]\n    \n    input_pep=Input(shape=(max_registers,n_pep,21),name='pep')\n    input_mhc=Input(shape=(n_mhc,21),name='mhc')        \n    \n    #encode pep, add positional encoding\n    pep_embedding=Dense(n_values_pep,name='pep_embedding')\n    x_pep=pep_embedding(input_pep) #[None,max_registers,n_pep,n_values]\n    x_pep+=positional_encoding(n_pep,n_values_pep)[newaxis,newaxis,:,:]          \n    x_pep=BatchNormalization()(x_pep)\n    \n    #encode mhc, add positional encoding\n    mhc_embedding=Dense(n_values_mhc,name='mhc_embedding')\n    x_mhc=mhc_embedding(input_mhc) #[None,n_mhc,n_values]    \n    x_mhc+=positional_encoding(n_mhc,n_values_mhc)[newaxis,:,:]\n    x_mhc=tile(x_mhc[:,newaxis,newaxis,:,:],[1,max_registers,n_pep,1,1]) #[None,max_registers,n_pep,n_mhc,n_values]\n    x_mhc=BatchNormalization()(x_mhc)\n    \n    #attention   \n    #pep query\n    pep_query_layer=Dense(n_pep_att,activation=actn)\n    x_pep_flat=reshape(x_pep,[-1,max_registers,n_pep*n_values_pep])\n    pep_query=pep_query_layer(x_pep_flat) #[None,max_registers,n_pep_att]\n    att_mask_layer=Dense(n_mhc*n_pep,activation=actn)\n    att_mask=reshape(att_mask_layer(pep_query),[])   #[None,max_registers,n_mhc,n_pep]\n    \n    \n    \n    for i in range(n_blocks):\n        att_mask_layer=Conv1D(n_mhc,1,strides=1,activation=None,name=f'att_mask_{i}')\n        att_gate_layer=Conv1D(1,1,strides=1,activation='sigmoid',name=f'gate_{i}')\n        att_mask=softmax(att_mask_layer(x_pep),axis=-1)             #[None,max_registers,n_pep,n_mhc]    \n        att_mask=tile(att_mask[:,:,:,:,newaxis],[1,1,1,1,n_values]) #[None,max_registers,n_pep,n_mhc,n_values] \n        att_gate=tile(att_gate_layer(x_pep),[1,1,1,n_values])       #[None,max_registers,n_pep,n_values]     \n        x_pep+=att_gate*reduce_sum(att_mask*x_mhc,axis=3)           #[None,max_registers,n_pep,n_values] \n        x_pep=BatchNormalization()(x_pep)\n        #convolution\n        pep_conv_layer=Conv1D(n_values,kernel_size,strides=1,padding='same',activation=actn,name=f'pep_conv_{i}')\n        x_pep=pep_conv_layer(x_pep)\n        x_pep=BatchNormalization()(x_pep)\n    \n    #output stack\n    if reduce_x:\n        x_pep=reduce_mean(x_pep,axis=2)     #[None,max_registers,n_values] \n    else:\n        pep_conv_reduce_layer=Conv1D(n_values_last,1,strides=1,padding='same',activation=actn,name='pep_conv_final')\n        x_pep=pep_conv_reduce_layer(x_pep)\n        x_pep=reshape(x_pep,[-1,max_registers,n_pep*n_values_last])\n    for i,n in enumerate(n_hidden):\n        dense_n=Dense(n,activation=actn,name=f'dense_{i}')\n        x_pep=Dropout(dropout_rate)(x_pep)   \n        x_pep=dense_n(x_pep)\n        x_pep=BatchNormalization()(x_pep)  \n    x=x_pep\n    dense_out=Dense(1,name='dense_output')\n    x=dense_out(x)    \n    x=x[:,:,0]    \n    return Model(inputs=[input_pep,input_mhc],outputs=x,name=model_name)", "\ndef reduce_model_min(model,params):\n    cl=params['cl']        \n    max_registers=max_registers_dict[cl]    \n    \n    inputs=model.inputs\n    input_regmask=Input(shape=(max_registers,),name='regmask')  \n    \n    if params.get('use_crossentropy'):\n        shift=100. #for logits, larger shift is necessary\n    else:\n        shift=np.log10(50000.)\n    \n    x=model(inputs)    \n    x+=(1-input_regmask)*shift #new runs: for run_n<=30, had *100 instead; problem: AF regmasks like (-1,7) would give huge error\n    x=reduce_min(x,axis=1)            \n    \n    return Model(inputs=inputs+[input_regmask],outputs=x)", "\ndef reduce_model_sum(model,params):\n    cl=params['cl']        \n    max_registers=max_registers_dict[cl]   \n    \n    c=np.log(10)\n    \n    inputs=model.inputs\n    input_regmask=Input(shape=(max_registers,),name='regmask') \n    \n    x=model(inputs)    \n    \n    x=tf_exp(-x*c)*input_regmask\n    x=reduce_sum(x,axis=1)\n    x=tf_log(x)/c\n             \n    return Model(inputs=inputs+[input_regmask],outputs=x)", "\n"]}
{"filename": "tfold/nn/pipeline.py", "chunked_list": ["import pickle\nimport numpy as np\nimport pandas as pd\n\nfrom tfold.config import data_dir, seqnn_params\nfrom tfold.nn import nn_utils\nfrom tfold.utils import seq_tools #used when mhc is given as allele rather than object\nseq_tools.load_mhcs()\n\ngap='-' #use same as appears in MHC gaps from seq_tools", "\ngap='-' #use same as appears in MHC gaps from seq_tools\n\n#one-hot\naa_ext=list('ACDEFGHIKLMNPQRSTVWY'+gap) #incl '-' for gap\ndef _one_hot(seq,alphabet=aa_ext):\n    '''\n    one-hot encoding\n    '''\n    aa_array=np.array(list(alphabet))\n    la=len(aa_array)\n    return (np.repeat(list(seq),la).reshape(-1,la)==aa_array).astype(int)", "\n#pep input\ndef _cut_extend(p,lmax):\n    nl=min(lmax,len(p))//2+1\n    nr=min(lmax,len(p))-nl    \n    return p[:nl]+gap*max(0,lmax-len(p))+p[-nr:]\ndef encode_pep_i(pep,\n                 max_core_len=seqnn_params['max_core_len_I'],\n                 max_tail_len=seqnn_params['max_pep_len_I']-9,\n                 n_tail_bits=seqnn_params['n_tail_bits'],\n                 p00=0.):    \n    '''\n    takes pep sequence, max_core_len, probability p00; returns encoded pep matched to all possible registers, as defined in nn_utils;\n    two flanking residues kept or padded with '-';\n    peptide middle is trimmed if core length > max_core_len, otherwise middle padded to max_core_len with '-';\n    for len 9, with prob p00 assigns only trivial register\n    '''    \n    assert len(pep)>=8, 'cl 1 peptide too short!'\n    if len(pep)==9 and np.random.rand()<p00:\n        registers=[(0,0)]\n    else:\n        registers=nn_utils.generate_registers_I(len(pep))\n    results_pep=[]\n    results_tails=[]\n    for r in registers:        \n        pep1=gap*(max(1-r[0],0))+pep[max(r[0]-1,0):len(pep)-r[1]+1]+gap*(max(1-r[1],0))\n        pep1=_cut_extend(pep1,max_core_len+2)        \n        pep1=_one_hot(pep1)                \n        results_pep.append(pep1)        \n        results_tails.append(_encode_tails([r[0],r[1]],max_tail_len,n_tail_bits))\n    return np.array(results_pep),np.array(results_tails)", "def _encode_tails(ls,max_len,n_bits):\n    return np.sin(np.pi/2*np.arange(1,n_bits+1)[np.newaxis,:]*np.array(ls)[:,np.newaxis]/max_len)    \ndef encode_pep_ii(pep,max_tail_len=seqnn_params['max_pep_len_II']-9,n_tail_bits=seqnn_params['n_tail_bits']):\n    '''\n    cut to cores of length 9, encode by one-hot encoding; return encoded pep, encoded tail lengths;\n    max_tail_len is used for normalization in tail length encoding\n    '''    \n    assert len(pep)>=9, 'cl 2 peptide too short!'\n    registers=nn_utils.generate_registers_II(len(pep))\n    results_pep=[]\n    results_tails=[]    \n    for r in registers:\n        results_pep.append(_one_hot(pep[r[0]:r[0]+9]))\n        results_tails.append(_encode_tails([r[0],r[1]],max_tail_len,n_tail_bits))\n    return np.array(results_pep),np.array(results_tails)", "\n#mhc input\nwith open(data_dir+'/obj/pmhc_contacts_av.pckl','rb') as f:\n    pmhc_contacts=pickle.load(f)\ndef encode_mhc_i(mhc,n):\n    contacts_i=pmhc_contacts['I'][:n]['res'].values\n    return _one_hot(mhc.get_residues_by_pdbnums(contacts_i))\ndef encode_mhc_ii(mhc_a,mhc_b,n):\n    contacts_iia=[x for x in pmhc_contacts['II'][:n]['res'].values if x<'1001 ']\n    contacts_iib=[x for x in pmhc_contacts['II'][:n]['res'].values if x>='1001 ']\n    return _one_hot(np.concatenate([mhc_a.get_residues_by_pdbnums(contacts_iia),mhc_b.get_residues_by_pdbnums(contacts_iib)]))", "def encode_mhc_i_allele(mhc,n):\n    return encode_mhc_i(seq_tools.mhcs[mhc],n)\ndef encode_mhc_ii_allele(mhc_a,mhc_b,n):\n    return encode_mhc_ii(seq_tools.mhcs[mhc_a],seq_tools.mhcs[mhc_b],n)\n\n#encoding pipelines\ndef _pad_registers(x,n_target):\n    '''\n    pad with random registers\n    '''\n    n=len(x[0])\n    assert n<=n_target, 'more than n_target registers'\n    if n<n_target:                \n        ind=[np.random.randint(n) for i in range(n_target-n)]\n        x=tuple([np.concatenate([y,np.array([y[i] for i in ind])]) for y in x])\n    return x", "def _regmask_from_regnum(x,max_registers):\n    return (np.tile(np.arange(max_registers)[np.newaxis,:],[len(x),1])<x[:,np.newaxis]).astype(int)\n\ndef pipeline_i(df,mhc_as_obj=True,p00=0.):\n    '''\n    df must have 'pep' (str) and 'mhc_a' (tuple or NUMSEQ) columns;\n    mhc_as_obj: mhc given as NUMSEQ obj; set to False if given as alleles;\n    p00: for a fraction p00 of 9mers, use canonical register only\n    '''\n    #set params    \n    n_mhc=seqnn_params['n_mhc_I']\n    max_registers=len(nn_utils.generate_registers_I(seqnn_params['max_pep_len_I']))        \n    inputs={}\n    #encode and pad pep\n    pep_tails=df['pep'].map(lambda x: encode_pep_i(x,p00=p00))\n    inputs['n_reg']=pep_tails.map(lambda x: len(x[0])).values  #actual number of registers\n    inputs['regmask']=_regmask_from_regnum(inputs['n_reg'],max_registers)\n    del inputs['n_reg']\n    pep_tails=pep_tails.map(lambda x: _pad_registers(x,max_registers))\n    inputs['pep']=[x[0] for x in pep_tails.values]\n    inputs['tails']=[x[1] for x in pep_tails.values]          \n    #encode mhc         \n    if mhc_as_obj:\n        inputs['mhc']=df['mhc_a'].map(lambda x: encode_mhc_i(x,n_mhc)).values        \n    else:\n        inputs['mhc']=df['mhc_a'].map(lambda x: encode_mhc_i_allele(x,n_mhc)).values    \n    for k in ['pep','mhc','tails']:\n        inputs[k]=np.stack(inputs[k]) #array of obj to array\n    return inputs", "    \ndef pipeline_ii(df,mhc_as_obj=True):    \n    #set params    \n    n_mhc=seqnn_params['n_mhc_II']                \n    max_registers=len(nn_utils.generate_registers_II(seqnn_params['max_pep_len_II']))          \n    inputs={}\n    #encode and pad pep    \n    pep_tails=df['pep'].map(lambda x: encode_pep_ii(x))              #(pep,tails) tuples\n    inputs['n_reg']=pep_tails.map(lambda x: len(x[0])).values        #save true reg numbers        \n    inputs['regmask']=_regmask_from_regnum(inputs['n_reg'],max_registers)\n    del inputs['n_reg']\n    pep_tails=pep_tails.map(lambda x: _pad_registers(x,max_registers))\n    inputs['pep']=[x[0] for x in pep_tails.values]\n    inputs['tails']=[x[1] for x in pep_tails.values]    \n    #encode mhc      \n    mhc_series=df[['mhc_a','mhc_b']].apply(tuple,axis=1)\n    if mhc_as_obj:\n        inputs['mhc']=mhc_series.map(lambda x: encode_mhc_ii(*x,n_mhc)).values    \n    else:\n        inputs['mhc']=mhc_series.map(lambda x: encode_mhc_ii_allele(*x,n_mhc)).values       \n    for k in ['pep','mhc','tails']:\n        inputs[k]=np.stack(inputs[k]) #array of obj to array\n    return inputs", "\n"]}
{"filename": "tfold/nn/nn_predict.py", "chunked_list": ["import os\nimport pickle\nimport numpy as np\nimport json\n\nfrom tfold.nn import pipeline as tfold_pipeline\nfrom tfold.nn import models as tfold_models\nfrom tfold.nn import nn_utils\nfrom tfold.config import seqnn_obj_dir\n\ndef _create_kd_arrays(cl,l):\n    if cl=='I':\n        tails=nn_utils.generate_registers_I(l)\n    else:\n        tails=nn_utils.generate_registers_II(l)    \n    return {'tails':tails,'logkds':[]}", "from tfold.config import seqnn_obj_dir\n\ndef _create_kd_arrays(cl,l):\n    if cl=='I':\n        tails=nn_utils.generate_registers_I(l)\n    else:\n        tails=nn_utils.generate_registers_II(l)    \n    return {'tails':tails,'logkds':[]}\n    \ndef predict(df,cl,mhc_as_obj=False,model_list=None,params_dir=None,weights_dir=None,keep_all_predictions=False):\n    df=df.copy()\n    #prepare data\n    if cl=='I':\n        pipeline=tfold_pipeline.pipeline_i\n        df['tails_all']=df['pep'].map(lambda x: nn_utils.generate_registers_I(len(x)))\n    else:\n        pipeline=tfold_pipeline.pipeline_ii\n        df['tails_all']=df['pep'].map(lambda x: nn_utils.generate_registers_II(len(x)))\n    df['logkd_all']=[[] for i in range(len(df))]\n    inputs=pipeline(df,mhc_as_obj=mhc_as_obj)         \n    #prepare params and such\n    params_dir=params_dir or (seqnn_obj_dir+'/params')\n    weights_dir=weights_dir or (seqnn_obj_dir+'/weights')    \n    if model_list:\n        pass\n    else:\n        with open(seqnn_obj_dir+f'/model_list_{cl}.pckl','rb') as f:\n            model_list=pickle.load(f)        \n    n_k=len(model_list[0]) #(run_n,model_n) or (run_n,model_n,split_n,copy_n)\n    params_all={}    \n    for filename in os.listdir(params_dir): \n        run_n=int(filename.split('.')[0].split('_')[1])        \n        with open(params_dir+'/'+filename) as f:\n            d=json.load(f) \n        for x in d:\n            k=(run_n,x['model_n'],x['split_n'],x['copy_n'])            \n            if k[:n_k] in model_list:\n                params_all[k]=x                \n    #do inference    \n    #use logkd, not kd in names!    \n    model_list_full=list(params_all.keys())\n    print(f'making Kd predictions for {len(df)} pmhcs...')\n    for k in model_list_full:\n        params=params_all[k]\n        model_func=getattr(tfold_models,params['model'])        \n        model=model_func(params)\n        weight_path=weights_dir+f'/run_{cl}_'+'_'.join([f'{kk}' for kk in k])\n        model.load_weights(weight_path)\n        outputs=model(inputs).numpy()\n        for x,y,z in zip(df['logkd_all'],df['tails_all'],outputs):\n            x.append(z[:len(y)])\n    df['logkd_all']=df['logkd_all'].map(np.array)    \n    x=df['logkd_all'].map(lambda x:np.average(x,axis=0))    \n    df['seqnn_logkds_all']=[np.array([tuple(c) for c in zip(b,a)],\n                            dtype=[('tail',object),('logkd',float)])\n                            for a,b in zip(x,df['tails_all'])]\n    df['seqnn_logkd']=x.map(np.min)\n    df['seqnn_tails']=x.map(np.argmin)\n    df['seqnn_tails']=df[['seqnn_tails','tails_all']].apply(lambda x: x['tails_all'][x['seqnn_tails']],axis=1)    \n    if not keep_all_predictions:\n        df=df.drop(['logkd_all','tails_all'],axis=1)\n        return df\n    else:\n        return df,model_list_full        ", "    \ndef predict(df,cl,mhc_as_obj=False,model_list=None,params_dir=None,weights_dir=None,keep_all_predictions=False):\n    df=df.copy()\n    #prepare data\n    if cl=='I':\n        pipeline=tfold_pipeline.pipeline_i\n        df['tails_all']=df['pep'].map(lambda x: nn_utils.generate_registers_I(len(x)))\n    else:\n        pipeline=tfold_pipeline.pipeline_ii\n        df['tails_all']=df['pep'].map(lambda x: nn_utils.generate_registers_II(len(x)))\n    df['logkd_all']=[[] for i in range(len(df))]\n    inputs=pipeline(df,mhc_as_obj=mhc_as_obj)         \n    #prepare params and such\n    params_dir=params_dir or (seqnn_obj_dir+'/params')\n    weights_dir=weights_dir or (seqnn_obj_dir+'/weights')    \n    if model_list:\n        pass\n    else:\n        with open(seqnn_obj_dir+f'/model_list_{cl}.pckl','rb') as f:\n            model_list=pickle.load(f)        \n    n_k=len(model_list[0]) #(run_n,model_n) or (run_n,model_n,split_n,copy_n)\n    params_all={}    \n    for filename in os.listdir(params_dir): \n        run_n=int(filename.split('.')[0].split('_')[1])        \n        with open(params_dir+'/'+filename) as f:\n            d=json.load(f) \n        for x in d:\n            k=(run_n,x['model_n'],x['split_n'],x['copy_n'])            \n            if k[:n_k] in model_list:\n                params_all[k]=x                \n    #do inference    \n    #use logkd, not kd in names!    \n    model_list_full=list(params_all.keys())\n    print(f'making Kd predictions for {len(df)} pmhcs...')\n    for k in model_list_full:\n        params=params_all[k]\n        model_func=getattr(tfold_models,params['model'])        \n        model=model_func(params)\n        weight_path=weights_dir+f'/run_{cl}_'+'_'.join([f'{kk}' for kk in k])\n        model.load_weights(weight_path)\n        outputs=model(inputs).numpy()\n        for x,y,z in zip(df['logkd_all'],df['tails_all'],outputs):\n            x.append(z[:len(y)])\n    df['logkd_all']=df['logkd_all'].map(np.array)    \n    x=df['logkd_all'].map(lambda x:np.average(x,axis=0))    \n    df['seqnn_logkds_all']=[np.array([tuple(c) for c in zip(b,a)],\n                            dtype=[('tail',object),('logkd',float)])\n                            for a,b in zip(x,df['tails_all'])]\n    df['seqnn_logkd']=x.map(np.min)\n    df['seqnn_tails']=x.map(np.argmin)\n    df['seqnn_tails']=df[['seqnn_tails','tails_all']].apply(lambda x: x['tails_all'][x['seqnn_tails']],axis=1)    \n    if not keep_all_predictions:\n        df=df.drop(['logkd_all','tails_all'],axis=1)\n        return df\n    else:\n        return df,model_list_full        ", "                 \n        \n        \n        \n        \n        \n        \n        \n        \n        ", "        \n        \n        \n        \n        \n        \n        \n        \n        \n        ", "        \n        "]}
{"filename": "tfold/nn/__init__.py", "chunked_list": [""]}
