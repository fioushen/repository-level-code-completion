{"filename": "utils/measure.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport numpy as np\nimport time\nimport sys\n\ntry:\n    import torch_tensorrt\n    from torch_tensorrt.logging import set_reportable_log_level, Level\nexcept:\n    print(\"TensorRT is not installed\")", "\n\n@torch.no_grad()\ndef get_time(\n    model, input_shape=(1, 3, 224, 224), name=\"\", rep=1000, warmup=2000, verb=False\n):\n    st = time.time()\n    device = torch.device(\"cuda\")\n    model.eval()\n    model.to(device)\n    dummy_input = torch.randn(input_shape).to(device)\n    # INIT LOGGERS\n    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(\n        enable_timing=True\n    )\n    timings = np.zeros((rep, 1))\n    # GPU-WARM-UP\n    for _ in range(warmup):\n        _ = model(dummy_input)\n    # MEASURE PERFORMANCE\n    with torch.no_grad():\n        for rep_ in range(rep):\n            starter.record()\n            _ = model(dummy_input)\n            ender.record()\n            # WAIT FOR GPU SYNC\n            torch.cuda.synchronize()\n            curr_time = starter.elapsed_time(ender)\n            timings[rep_] = curr_time\n    mean_syn = np.sum(timings) / rep\n    std_syn = np.std(timings)\n    batch_size = input_shape[0]\n    thpts = 1 / (timings * 0.001) * batch_size\n    mean_thpt = np.sum(thpts) / rep\n    std_thpt = np.std(thpts)\n    if verb:\n        print(f\"Batch size is {batch_size}\")\n        print(f\"Measure time {time.time() - st:.2f} seconds\")\n        print(f\"[{name:>20}] THPT : {mean_thpt:.2f} || STD : {std_thpt:.2f}\")\n    print(f\"[{name:>20}] MEAN : {mean_syn:.2f}ms || STD : {std_syn:.2f}\")\n    return mean_syn, std_syn", "\n\ndef unroll_merged(module):\n    result = nn.Sequential()\n    for ind, blk in enumerate(module.m_features):\n        result.add_module(f\"blk{ind}\", blk)\n\n    pre_last = [nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten()]\n    for ind, blk in enumerate(pre_last):\n        result.add_module(f\"pre_last{ind}\", blk)\n\n    for ind, blk in enumerate(module.classifier):\n        result.add_module(f\"last{ind}\", blk)\n    return result", "\n\ndef unroll_merged_vgg(module):\n    result = nn.Sequential()\n    for ind, blk in enumerate(module.m_features):\n        result.add_module(f\"blk{ind}\", blk)\n\n    pre_last = [nn.AdaptiveAvgPool2d((7, 7)), nn.Flatten()]\n    for ind, blk in enumerate(pre_last):\n        result.add_module(f\"pre_last{ind}\", blk)\n\n    for ind, blk in enumerate(module.classifier):\n        result.add_module(f\"last{ind}\", blk)\n    return result", "\n\n@torch.no_grad()\ndef compile_and_time(module, sz, txt, verb=False):\n    if not \"torch_tensorrt\" in sys.modules:\n        raise Exception(\"You should install TensorRT\")\n    set_reportable_log_level(Level.Error)\n    st = time.time()\n    module.eval()\n    model = torch_tensorrt.compile(\n        module,\n        inputs=[torch_tensorrt.Input(sz)],\n        enabled_precisions={torch_tensorrt.dtype.float},\n    )\n    print(f\"TensorRT compiling done ({time.time() - st:.2f} seconds)\")\n    result, std = get_time(\n        model, input_shape=sz, name=txt, rep=1000, warmup=2000, verb=verb\n    )\n    del module\n    return result, std", "\n\n@torch.no_grad()\ndef torch_time(module, sz, txt, verb=False, rep=1000, warmup=2000):\n    module.eval()\n    print(f\"Measuring time without TensorRT compiling...\")\n    result, std = get_time(\n        module, input_shape=sz, name=txt, rep=rep, warmup=warmup, verb=verb\n    )\n    del module\n    return result, std", "\n\n@torch.no_grad()\ndef get_cpu_time(\n    model, input_shape=(1, 3, 224, 224), name=\"\", rep=10, warmup=100, verb=False\n):\n    st = time.time()\n    device = torch.device(\"cpu\")\n    model.eval()\n    model.to(device)\n    dummy_input = torch.randn(input_shape).to(device)\n    # INIT LOGGERS\n    timings = np.zeros((rep, 1))\n    # CPU-WARM-UP\n    for i in range(warmup):\n        _ = model(dummy_input)\n        if i % 10 == 0:\n            print(f\"{i} / {warmup}\")\n    # MEASURE PERFORMANCE\n    with torch.no_grad():\n        for rep_ in range(rep):\n            start = time.time() * 1000\n            _ = model(dummy_input)\n            end = time.time() * 1000\n            curr_time = end - start\n            timings[rep_] = curr_time\n            if rep_ % 10 == 0:\n                print(f\"{rep_} / {rep}\")\n    mean_syn = np.sum(timings) / rep\n    std_syn = np.std(timings)\n    batch_size = input_shape[0]\n    thpts = 1 / (timings * 0.001) * batch_size\n    mean_thpt = np.sum(thpts) / rep\n    std_thpt = np.std(thpts)\n    if verb:\n        print(f\"Batch size is {batch_size}\")\n        print(f\"Measure time {time.time() - st:.2f} seconds\")\n        print(f\"[{name:>20}] THPT : {mean_thpt:.2f} || STD : {std_thpt:.2f}\")\n    print(f\"[{name:>20}] MEAN : {mean_syn:.2f}ms || STD : {std_syn:.2f}\")\n    return mean_syn, std_syn", "\n\n@torch.no_grad()\ndef torch_cpu_time(module, sz, txt, verb=False):\n    module.eval()\n    print(f\"Measuring time with cpu...\")\n    result, std = get_cpu_time(\n        module, input_shape=sz, name=txt, rep=100, warmup=10, verb=verb\n    )\n    del module\n    return result, std", ""]}
{"filename": "utils/dp.py", "chunked_list": ["import os\nimport sys\n\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\nimport socket\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np", "import pandas as pd\nimport numpy as np\n\nfrom datetime import datetime\nfrom typing import List, Tuple\nfrom itertools import product\nfrom models.model_op import get_conv_lst, simulate_list_merge, valid_blks\nfrom utils.measure import compile_and_time, torch_time\nfrom utils.logger import Logger\n", "from utils.logger import Logger\n\n\ndef val_df(df, st, end, st_act=None, end_act=None, col=\"\"):\n    \"\"\"\n    Helper function for pandas DataFrame\n    \"\"\"\n    mask = (df[\"st\"] == st) & (df[\"end\"] == end)\n    if st_act != None:\n        mask = mask & (df[\"st_act\"] == st_act)\n    if end_act != None:\n        mask = mask & (df[\"end_act\"] == end_act)\n    filtered = df.loc[mask, :]\n    if len(filtered) == 0:\n        return None\n    assert len(filtered) == 1\n    return filtered[col].values[0]", "\n\ndef generate_time_table(\n    model,\n    blks: List[Tuple[int, int]],\n    arch: str,\n    blk_type,\n    dir: str,\n    tag: str = \"\",\n    logger: Logger = None,\n    trt: bool = True,\n):\n    \"\"\"\n    This function computes the $T(\\cdot, \\cdot)$ table in the paper\n    \"\"\"\n    cnt = torch.cuda.device_count()\n    env = f\"{socket.gethostname()}_gpu{cnt}\"\n    out_shape = model.out_shape\n\n    blks = valid_blks(model)\n    blks_dict = {\"id\": [], \"st\": [], \"end\": [], \"time\": [], \"stdev\": []}\n    for ind, (st, end) in enumerate(blks):\n        if logger:\n            now = datetime.now().strftime(\"%m/%d %H:%M:%S\")\n            logger.comment(\n                f\"ind : {ind:>3} | st : {st:>2} | end : {end:>2}    |     {now}\",\n                verb=True,\n            )\n        else:\n            print(f\"ind : {ind:>3} | st : {st:>2} | end : {end:>2}\")\n        blks_dict[\"id\"].append(ind)\n        blks_dict[\"st\"].append(st)\n        blks_dict[\"end\"].append(end)\n\n        if arch == \"learn_mobilenet_v2\":\n            conv_lst = get_conv_lst(model.features[1:-1], blk_type, st, end)\n        elif arch == \"learn_vgg19\":\n            conv_lst = get_conv_lst(model.features, blk_type, st, end)\n        else:\n            raise NotImplementedError(\"Not supported architecture\")\n\n        if logger:\n            logger.comment(str(conv_lst), verb=True)\n        else:\n            print(conv_lst)\n        merged_conv = simulate_list_merge(conv_lst)\n        if logger:\n            logger.comment(str(merged_conv), verb=True)\n        else:\n            print(merged_conv)\n        merged_model = nn.Sequential(merged_conv, nn.ReLU(inplace=True))\n        if trt:\n            time, std = compile_and_time(\n                merged_model, out_shape[st], f\"st : {st:>2} | end : {end:>2}\"\n            )\n        else:\n            time, std = torch_time(\n                merged_model,\n                out_shape[st],\n                f\"st : {st:>2} | end : {end:>2}\",\n                True,\n                rep=200,\n                warmup=300,\n            )\n        blks_dict[\"time\"].append(time)\n        blks_dict[\"stdev\"].append(std)\n\n    blks = pd.DataFrame(blks_dict)\n    if tag:\n        filename = f\"time_{tag}.csv\"\n    else:\n        filename = f\"time_{env}.csv\"\n    csv_path = os.path.join(dir, filename)\n    blks.to_csv(csv_path, sep=\",\", index=False)", "\n\ndef generate_optimal_time_table(\n    blks: List[Tuple[int, int]],\n    dir: str,\n    tag: str,\n    time_path=\"\",\n):\n    \"\"\"\n    This function computes the $T_{opt}(\\cdot, \\cdot)$ table in the paper\n    \"\"\"\n    df = pd.read_csv(time_path)\n    # In order to speed up the `isin` operation\n    blks_set = set(blks.copy())\n\n    df_time_dict = dict()\n    df_time_stdev_dict = dict()\n    for st, end in blks:\n        df_time_dict[f\"{st}_{end}\"] = val_df(df, st=st, end=end, col=\"time\")\n        df_time_stdev_dict[f\"{st}_{end}\"] = val_df(df, st=st, end=end, col=\"stdev\")\n\n    opt_tab = dict()\n    argmin_lst = dict()\n    stdev_tab = dict()\n\n    for st, end in blks:\n        g = df_time_dict[f\"{st}_{end}\"]\n        stdev = df_time_stdev_dict[f\"{st}_{end}\"]\n        assert g != None\n        min_val = g\n        amin = end\n        for j in range(st + 1, end):\n            if (st, j) in blks_set and (j, end) in blks_set:\n                g_j = df_time_dict[f\"{j}_{end}\"]\n                std_j = df_time_stdev_dict[f\"{j}_{end}\"]\n                assert g_j != None\n                if opt_tab[(st, j)] + g_j < min_val:\n                    min_val = opt_tab[(st, j)] + g_j\n                    stdev = stdev_tab[(st, j)] + std_j\n                    amin = j\n        opt_tab[(st, end)] = min_val\n        stdev_tab[(st, end)] = stdev\n        if amin != end:\n            argmin_lst[(st, end)] = argmin_lst[(st, amin)] + [end]\n        else:\n            argmin_lst[(st, end)] = [st, end]\n\n    blks_dict = {\"id\": [], \"st\": [], \"end\": [], \"time\": [], \"stdev\": [], \"breaks\": []}\n    for ind, (st, end) in enumerate(blks):\n        blks_dict[\"id\"].append(ind)\n        blks_dict[\"st\"].append(st)\n        blks_dict[\"end\"].append(end)\n        blks_dict[\"time\"].append(opt_tab[(st, end)])\n        blks_dict[\"stdev\"].append(stdev_tab[(st, end)])\n        blks_dict[\"breaks\"].append(\",\".join([str(i) for i in argmin_lst[(st, end)]]))\n\n    blks = pd.DataFrame(blks_dict)\n    if tag:\n        filename = f\"opt_time_{tag}.csv\"\n    else:\n        filename = f\"opt_time.csv\"\n    csv_path = os.path.join(dir, filename)\n    blks.to_csv(csv_path, sep=\",\", index=False)", "\n\ndef generate_ext_imp_table(\n    ext_blks: List[Tuple[int, int, int, int]],\n    act_num: int,\n    dir: str,\n    imp_path=\"\",\n    score: str = \"val_acc\",\n    default: set = {},\n    norm: str = \"default\",\n    alpha: float = 1.0,\n):\n    \"\"\"\n    This function computes the $I_{ext}(\\cdot, \\cdot, \\cdot, \\cdot)$ table in the paper\n    \"\"\"\n    df_imp = pd.read_csv(imp_path)\n\n    if norm == \"default\":\n        # Normalize with default\n        mask1 = df_imp[\"end\"] - df_imp[\"st\"] == 1\n        mask2 = (df_imp[\"st_act\"] == 1) == (df_imp[\"st\"].isin(default))\n        mask3 = (df_imp[\"end_act\"] == 1) == (df_imp[\"end\"].isin(default))\n        mask4 = (df_imp[\"st\"] == 0) & (df_imp[\"end\"] == 1)\n        mask = (mask1 & mask2 & mask3) | mask4\n        assert sum(mask) == act_num\n    elif norm == \"single\":\n        mask = df_imp[\"end\"] - df_imp[\"st\"] == 1\n    elif norm == \"all\":\n        mask = df_imp[\"st_act\"].isin([0, 1])\n    else:\n        raise NotImplementedError()\n\n    mean = df_imp.loc[mask, :].mean()[score]\n    df_imp[score] -= alpha * mean\n    print(mean)\n    print(alpha * mean)\n\n    df_imp_dict = dict()\n    for m in range(1, act_num + 1):\n        for i in range(0, m):\n            for j in range(2):\n                for a in range(2):\n                    df_imp_dict[f\"{i}_{m}_{j}_{a}\"] = val_df(\n                        df_imp, st=i, end=m, st_act=j, end_act=a, col=score\n                    )\n\n    # In order to speed up the `isin` operation\n    ext_blks_set = set(ext_blks.copy())\n    ext_tab = dict()\n    argmax_lst = dict()\n\n    \"\"\"\n    I_{ext}(st, end, a) \\leftarrow \\max_j(I_{ext}(st, j, 0) + h(j, end, 0, a))\n    \"\"\"\n    for (st, end, st_act, end_act) in ext_blks:\n        h = df_imp_dict[f\"{st}_{end}_{st_act}_{end_act}\"]\n        assert h != None\n        max_val = h\n        amax = end\n        for j in range(st + 1, end):\n            if (st, j, st_act, 0) in ext_tab and (j, end, 0, end_act) in ext_blks_set:\n                h_j = df_imp_dict[f\"{j}_{end}_{0}_{end_act}\"]\n                assert h_j != None\n                if ext_tab[(st, j, st_act, 0)] + h_j > max_val:\n                    max_val = ext_tab[(st, j, st_act, 0)] + h_j\n                    amax = j\n        ext_tab[(st, end, st_act, end_act)] = max_val\n        if amax != end:\n            sub = argmax_lst[(st, amax, st_act, 0)]\n            argmax_lst[(st, end, st_act, end_act)] = sub + [end]\n        else:\n            argmax_lst[(st, end, st_act, end_act)] = [st, end]\n\n    blks_dict = {\n        \"id\": [],\n        \"st\": [],\n        \"end\": [],\n        \"st_act\": [],\n        \"end_act\": [],\n        \"imp\": [],\n        \"breaks\": [],\n    }\n    for ind, (st, end, st_act, end_act) in enumerate(ext_blks):\n        blks_dict[\"id\"].append(ind)\n        blks_dict[\"st\"].append(st)\n        blks_dict[\"st_act\"].append(st_act)\n        blks_dict[\"end\"].append(end)\n        blks_dict[\"end_act\"].append(end_act)\n        blks_dict[\"imp\"].append(ext_tab[(st, end, st_act, end_act)])\n        blks_dict[\"breaks\"].append(\n            \",\".join([str(i) for i in argmax_lst[(st, end, st_act, end_act)]])\n        )\n\n    blks = pd.DataFrame(blks_dict)\n\n    filename = f\"ext_importance\"\n    filename += f\"_s_{score}\"\n    if norm != \"default\":\n        filename += f\"_n_{norm}\"\n    if alpha != 1.0:\n        filename += f\"_a_{alpha}\"\n    filename += \".csv\"\n    csv_path = os.path.join(dir, filename)\n    blks.to_csv(csv_path, sep=\",\", index=False)", "\n\ndef optimal_patterns(\n    flt_time_limit: float,\n    act_num: int,\n    opt_time_path: str,\n    ext_imp_path: str,\n    prec: float = 100,\n    verbose: bool = False,\n    logger: Logger = None,\n):\n    df_opt_time = pd.read_csv(opt_time_path)\n    df_ext_imp = pd.read_csv(ext_imp_path)\n\n    df_opt_time_dict = dict()\n    df_opt_time_brks_dict = dict()\n    df_ext_imp_dict = dict()\n    for m in range(1, act_num + 1):\n        for i in range(0, m):\n            df_opt_time_dict[f\"{i}_{m}\"] = val_df(df_opt_time, st=i, end=m, col=\"time\")\n            df_opt_time_brks_dict[f\"{i}_{m}\"] = val_df(\n                df_opt_time, st=i, end=m, col=\"breaks\"\n            )\n            for j in range(2):\n                for a in range(2):\n                    df_ext_imp_dict[f\"{i}_{m}_{j}_{a}\"] = val_df(\n                        df_ext_imp, st=i, end=m, st_act=j, end_act=a, col=\"imp\"\n                    )\n\n    time_limit = int(flt_time_limit * prec)\n\n    # Sum of maximum importance\n    dp_tab = dict()\n    # Sum of optimal time\n    dp_time_tab = dict()\n    # Initialization\n    for t in range(0, time_limit):\n        dp_tab[(t, 0, 1)] = 0\n        dp_time_tab[(t, 0, 1)] = 0\n    max_tab = dict()\n    mpos_tab = dict()\n    for m in range(1, act_num + 1):\n        t_min = float(\"Inf\")\n        for i in range(0, m):\n            g_flt = df_opt_time_dict[f\"{i}_{m}\"]\n            if g_flt == None:\n                continue\n            cand = int(g_flt * prec)\n            if t_min > cand:\n                t_min = cand\n\n        # Impossible time limit\n        if t_min > time_limit:\n            if logger:\n                logger.comment(f\"Impossible time limit : {time_limit}\", verb=True)\n            else:\n                print(f\"Impossible time limit : {time_limit}\")\n            exit()\n\n        for t, a in product(range(t_min, time_limit + 1), range(2)):\n            if verbose and (t - t_min) % 1000 == 0:\n                now = datetime.now().strftime(\"%m/%d %H:%M:%S\")\n                prg = f\"m = {m:>2} & a = {a:>2} : {t - t_min:>5} / {time_limit - t_min:>5}       ||      {now}\"\n                if logger:\n                    logger.comment(prg, verb=True)\n                else:\n                    print(prg)\n\n            max_imp = float(\"-Inf\")\n            for i, j in product(range(m), range(2)):\n                g_flt = df_opt_time_dict[f\"{i}_{m}\"]\n                h = df_ext_imp_dict[f\"{i}_{m}_{j}_{a}\"]\n                if g_flt == None or h == None:\n                    assert h == None\n                    continue\n                g = int(g_flt * prec)\n                # Skip for impossible combination\n                if not (t - g, i, j) in dp_tab:\n                    continue\n                cand = dp_tab[(t - g, i, j)] + h\n                cand_time = dp_time_tab[(t - g, i, j)] + g\n\n                # For numerical stability of importance value\n                # (we round the values before comparison between float)\n                r_imp, r_cand = round(max_imp, 7), round(cand, 7)\n                # Choose maximum imp (on the tie we choose the one with faster time)\n                if r_imp < r_cand or (r_imp == r_cand and cand_time < opt_time):\n                    max_imp = cand\n                    opt_time = cand_time\n                    argmax_imp = (t - g, i, j)\n                    brkstr = df_opt_time_brks_dict[f\"{i}_{m}\"]\n                    brks = set([int(pos) for pos in brkstr.split(\",\")])\n                    if i == 0:\n                        mpos = brks\n                    else:\n                        mpos = set.union(brks, mpos_tab[(t - g, i, j)])\n\n            # Skip for impossible combination (or combination that we do not take into account)\n            if max_imp != float(\"-Inf\"):\n                dp_tab[(t, m, a)] = max_imp\n                dp_time_tab[(t, m, a)] = opt_time\n                max_tab[(t, m, a)] = argmax_imp\n                mpos_tab[(t, m, a)] = mpos\n\n    t, m = time_limit, act_num\n    if all(not (t, m, a) in dp_tab for a in range(0, 2)):\n        if logger:\n            logger.comment(f\"Impossible time limit : {t}\", verb=True)\n        else:\n            print(f\"Impossible time limit : {t}\")\n        exit()\n    ends = []\n    for a in range(2):\n        if (t, m, a) in dp_tab:\n            ends.append(dp_tab[(t, m, a)])\n        else:\n            ends.append(float(\"-Inf\"))\n    sol_a = np.argmax(ends)\n\n    a = sol_a\n    opt_m_pos = {m}\n    blk_ends = {m}\n    act_pos = {m} if bool(sol_a) else set()\n    while m > 0:\n        opt_m_pos = set.union(opt_m_pos, mpos_tab[(t, m, a)])\n        blk_ends = set.union(blk_ends, {m})\n        act_pos = set.union(act_pos, {m}) if bool(a) else act_pos\n        t, m, a = max_tab[(t, m, a)]\n    assert dp_tab[(t, m, a)] == 0\n    opt_m_pos -= {0}\n\n    st, st_act = 0, 1\n    imp_sum, int_time_sum, flt_time_sum = 0, 0, 0\n    for pos in sorted(list(blk_ends)):\n        a = int(pos in act_pos)\n        if pos == 0:\n            continue\n        imp = df_ext_imp_dict[f\"{st}_{pos}_{st_act}_{a}\"]\n        flt_time = df_opt_time_dict[f\"{st}_{pos}\"]\n        assert imp != None\n        imp_sum += imp\n        int_time_sum += int(flt_time * prec)\n        flt_time_sum += flt_time\n        st, st_act = pos, a\n    assert imp_sum == dp_tab[(time_limit, act_num, sol_a)]\n\n    return (act_pos, opt_m_pos, imp_sum, int_time_sum, flt_time_sum)", "\n\ndef optimal_merge_pattern(act_pos, act_num, time_path):\n    df = pd.read_csv(time_path)\n\n    dp_tab = {0: 0}\n    min_tab = dict()\n    for m in range(1, act_num + 1):\n        starts = set.union(act_pos, {0})\n        t_max = max([x for x in starts if x < m])\n        min_time = float(\"Inf\")\n        for t in range(t_max, m):\n            g = val_df(df, st=t, end=m, col=\"time\")\n            if g == None:\n                continue\n            cand = dp_tab[t] + g\n            if min_time > cand:\n                min_time = cand\n                argmin_time = t\n        dp_tab[m] = min_time\n        min_tab[m] = argmin_time\n\n    m = act_num\n    m_pos = {m}\n    while m > 0:\n        m_pos = set.union(m_pos, {m})\n        m = min_tab[m]\n\n    st, stdev = 0, 0\n    for pos in sorted(list(m_pos)):\n        if pos == 0:\n            continue\n        std = val_df(df, st=st, end=pos, col=\"stdev\")\n        assert std != None\n        stdev += std\n        st = pos\n\n    assert act_pos.issubset(m_pos)\n    return (m_pos, dp_tab[act_num], stdev)", ""]}
{"filename": "utils/train.py", "chunked_list": ["# Code started from  https://github.com/d-li14/mobilenetv2.pytorch.git\nimport os\nimport time\nfrom math import cos, pi\n\nimport torch\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed", "import torch.utils.data\nimport torch.utils.data.distributed\n\nfrom utils.logger import AverageMeter\nfrom utils.misc import KLLossSoft\n\n# progress bar\n# https://github.com/verigak/progress\nfrom progress.bar import Bar as Bar\n", "from progress.bar import Bar as Bar\n\n__all__ = [\"accuracy\", \"validate\", \"train\"]\n\n\ndef train(\n    train_loader,\n    model,\n    criterion,\n    optimizer,\n    epoch,\n    total_epochs,\n    lr_decay,\n    reg=None,\n    logger=None,\n    args=None,\n):\n    bar = Bar(\"Processing\", max=len(train_loader))\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    logger.comment(\"-----------------------------\")\n    for ind, (input, target) in enumerate(train_loader):\n        adjust_learning_rate(\n            optimizer,\n            epoch,\n            ind,\n            len(train_loader),\n            total_epochs,\n            args.lr,\n            lr_decay,\n            args.schedule,\n            args.gamma,\n        )\n\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        input = input.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n\n        # compute output\n        output = model(input)\n        if isinstance(criterion, KLLossSoft):\n            with torch.no_grad():\n                soft_logits = criterion.teacher(input)\n            loss = criterion(output, soft_logits, target)\n        else:\n            loss = criterion(output, target)\n        if reg != None:\n            loss += args.lamb * model.module.regularizer(reg)\n\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n        top1.update(prec1.item(), input.size(0))\n        top5.update(prec5.item(), input.size(0))\n        losses.update(loss.item(), input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        if args.aug:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        plot_progress = \"({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\".format(\n            batch=ind + 1,\n            size=len(train_loader),\n            data=data_time.avg,\n            bt=batch_time.avg,\n            total=bar.elapsed_td,\n            eta=bar.eta_td,\n            loss=losses.avg,\n            top1=top1.avg,\n            top5=top5.avg,\n        )\n        bar.suffix = plot_progress\n        bar.next()\n        # For logging, print newline\n        if (ind + 1) % args.print_freq == 0:\n            logger.comment(plot_progress)\n        # if i == len(train_loader.dataloader) - 1:\n        #     top1, _ = accuracy(output, target, topk=(1, 5))\n        if args.debug and ind > 5:\n            break\n\n    bar.finish()\n    logger.comment(\"-----------------------------\")\n    return (losses.avg, top1.avg)", "\n\ndef validate(val_loader, model, criterion, args):\n    bar = Bar(\"Processing\", max=len(val_loader))\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target) in enumerate(val_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        input = input.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n\n        with torch.no_grad():\n            # compute output\n            output = model(input)\n            if isinstance(criterion, KLLossSoft):\n                soft_logits = criterion.teacher(input)\n                loss = criterion(output, soft_logits, target)\n            else:\n                loss = criterion(output, target)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n        losses.update(loss.item(), input.size(0))\n        top1.update(prec1.item(), input.size(0))\n        top5.update(prec5.item(), input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix = \"({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\".format(\n            batch=i + 1,\n            size=len(val_loader),\n            data=data_time.avg,\n            bt=batch_time.avg,\n            total=bar.elapsed_td,\n            eta=bar.eta_td,\n            loss=losses.avg,\n            top1=top1.avg,\n            top5=top5.avg,\n        )\n        bar.next()\n        if args.debug and i > 5:\n            break\n    bar.finish()\n    return (losses.avg, top1.avg)", "\n\ndef adjust_learning_rate(\n    optimizer, epoch, iteration, num_iter, epochs, lr_init, lr_decay, schedule, gamma\n):\n    lr = optimizer.param_groups[0][\"lr\"]\n\n    # # This warmup parameter is obsolete\n    # wu_epoch = 5 if args.warmup else 0\n    # wu_iter = wu_epoch * num_iter\n    wu_epoch, wu_iter = 0, 0\n    cur_iter = iteration + epoch * num_iter\n    max_iter = epochs * num_iter\n\n    if lr_decay == \"step\":\n        lr = lr_init * (gamma ** ((cur_iter - wu_iter) / (max_iter - wu_iter)))\n    elif lr_decay == \"cos\":\n        lr = lr_init * (1 + cos(pi * (cur_iter - wu_iter) / (max_iter - wu_iter))) / 2\n    elif lr_decay == \"linear\":\n        lr = lr_init * (1 - (cur_iter - wu_iter) / (max_iter - wu_iter))\n    elif lr_decay == \"schedule\":\n        count = sum([1 for s in schedule if s <= epoch])\n        lr = lr_init * pow(gamma, count)\n    elif lr_decay == \"const\":\n        lr = lr_init\n    else:\n        raise ValueError(\"Unknown lr mode {}\".format(lr_decay))\n\n    if epoch < wu_epoch:\n        lr = lr_init * cur_iter / wu_iter\n\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr", "\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res", ""]}
{"filename": "utils/logger.py", "chunked_list": ["# A simple torch style logger\n# (C) Wei YANG 2017\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n__all__ = [\"Logger\", \"LoggerMonitor\", \"savefig\", \"AverageMeter\"]\n\n\ndef savefig(fname, dpi=None):\n    dpi = 150 if dpi == None else dpi\n    plt.savefig(fname, dpi=dpi)", "def savefig(fname, dpi=None):\n    dpi = 150 if dpi == None else dpi\n    plt.savefig(fname, dpi=dpi)\n\n\ndef plot_overlap(logger, names=None):\n    names = logger.names if names == None else names\n    numbers = logger.numbers\n    for _, name in enumerate(names):\n        x = np.arange(len(numbers[name]))\n        plt.plot(x, np.asarray(numbers[name]))\n    return [logger.title + \"(\" + name + \")\" for name in names]", "\n\nclass Logger(object):\n    \"\"\"Save training process to log file with simple plot function.\"\"\"\n\n    def __init__(self, fpath, title=None, resume=False):\n        self.file = None\n        self.resume = resume\n        self.title = \"\" if title == None else title\n        if fpath is not None:\n            if resume:\n                self.file = open(fpath, \"a\")\n            else:\n                self.file = open(fpath, \"w\")\n\n    def set_names(self, names):\n        # initialize numbers as empty list\n        self.numbers = {}\n        self.names = names\n        for _, name in enumerate(self.names):\n            self.file.write(name)\n            self.file.write(\"\\t\")\n            self.numbers[name] = []\n        self.file.write(\"\\n\")\n        self.file.flush()\n\n    def append(self, numbers):\n        assert len(self.names) == len(numbers), \"Numbers do not match names\"\n        for index, num in enumerate(numbers):\n            if self.names[index] == \"Epoch\":\n                self.file.write(\"{0:3}\".format(int(num)))\n            else:\n                self.file.write(\"{0:.6f}\".format(num))\n            self.file.write(\"\\t\")\n            self.numbers[self.names[index]].append(num)\n        self.file.write(\"\\n\")\n        self.file.flush()\n\n    def comment(self, string, verb=False):\n        self.file.write(string)\n        self.file.write(\"\\n\")\n        self.file.flush()\n        if verb:\n            print(string)\n\n    def plot(self, names=None):\n        names = self.names if names == None else names\n        numbers = self.numbers\n        for _, name in enumerate(names):\n            x = np.arange(len(numbers[name]))\n            plt.plot(x, np.asarray(numbers[name]))\n        plt.legend([self.title + \"(\" + name + \")\" for name in names])\n        plt.grid(True)\n\n    def close(self):\n        if self.file is not None:\n            self.file.close()", "\n\nclass LoggerMonitor(object):\n    \"\"\"Load and visualize multiple logs.\"\"\"\n\n    def __init__(self, paths):\n        \"\"\"paths is a distionary with {name:filepath} pair\"\"\"\n        self.loggers = []\n        for title, path in paths.items():\n            logger = Logger(path, title=title, resume=True)\n            self.loggers.append(logger)\n\n    def plot(self, names=None):\n        plt.figure()\n        plt.subplot(121)\n        legend_text = []\n        for logger in self.loggers:\n            legend_text += plot_overlap(logger, names)\n        plt.legend(legend_text, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n        plt.grid(True)", "\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\n    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count", ""]}
{"filename": "utils/__init__.py", "chunked_list": ["\"\"\"Useful utils\n\"\"\"\nfrom .logger import *\nfrom .train import *\n"]}
{"filename": "utils/misc.py", "chunked_list": ["import os\nimport shutil\n\nimport torch\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\n\n\ndef save_checkpoint(state, is_best, checkpoint=\"checkpoint\", filename=\"checkpoint.pth\"):\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    if is_best:\n        best_filepath = filepath[:-4]\n        best_filepath += \"_best.pth\"\n        shutil.copyfile(filepath, best_filepath)", "\n\ndef save_checkpoint(state, is_best, checkpoint=\"checkpoint\", filename=\"checkpoint.pth\"):\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    if is_best:\n        best_filepath = filepath[:-4]\n        best_filepath += \"_best.pth\"\n        shutil.copyfile(filepath, best_filepath)\n", "\n\ndef load_checkpoint(\n    model,\n    arch,\n    path,\n    act_path=None,\n    ds_pat=None,\n    logger=None,\n):\n\n    if os.path.isfile(path):\n        source_state = torch.load(path)\n        log_tool(f\"=> loading pretrained weight '{path}'\", logger)\n        if \"epoch\" in source_state:\n            log_tool(f\"=> (epoch {source_state['epoch']})\", logger)\n    else:\n        log_tool(f\"=> no weight found at '{path}'\", logger)\n        exit()\n\n    if act_path == None:\n        act_path = path\n\n    if arch in [\"mobilenet_v2\", \"vgg19\"]:\n        log_tool(f\"=> loading on the architecture '{arch}'\", logger)\n        act_state = dict()\n    elif os.path.isfile(act_path):\n        act_state = torch.load(act_path)\n        log_tool(f\"=> loading activations from '{act_path}'\", logger)\n        if \"act_pos\" in act_state:\n            log_tool(f\"Number of activations : {len(act_state['act_pos'])}\", logger)\n            log_tool(f\"{act_state['act_pos']}\", logger)\n        else:\n            log_tool(f\"=> no activations found at '{path}'\", logger)\n    else:\n        log_tool(f\"=> not a valid act_path '{act_path}'\", logger)\n        exit()\n\n    if ds_pat != None:\n        ds_pattern, compress_k = ds_pat\n        # fmt: off\n        pat2cmp = {\n            \"A\": 11, \"B\": 8, \"C\": 8, \"D\": 6, \"E\": 6, \"F\": 0, \"A10\": 12, \"B10\": 12, \"C10\": 9, \"D10\": 7, \"AR\": 11, \"BR\": 8, \"CR\": 6, \"AR10\": 12, \"BR10\": 9, \"CR10\": 7, \"AR_AUG\": 11, \"BR_AUG\": 8, \"CR_AUG\": 6, \"AR10_AUG\": 12, \"BR10_AUG\": 9, \"CR10_AUG\": 7\n        }\n        # fmt: on\n        assert ds_pattern != \"none\" and arch == \"dep_shrink_mobilenet_v2\"\n        assert compress_k == pat2cmp[ds_pattern]\n        log_tool(f\"=> training from ds_pattern '{ds_pattern}'\", logger)\n        source_state[\"compress_k\"] = compress_k\n        model.module.load_pattern(ds_pattern)\n\n    if arch == \"dep_shrink_mobilenet_v2\":\n        model.module.compress_k = source_state[\"compress_k\"]\n\n    # `act_pos` for finetuned/merged network weights\n    if \"act_pos\" in act_state:\n        a_pos = act_state[\"act_pos\"]\n        source_state[\"act_pos\"] = a_pos\n\n        # `merge_pos` for merged network weights\n        if \"merge_pos\" in act_state:\n            log_tool(f\"=> loading optimal merge pattern from '{act_path}'\", logger)\n            log_tool(f\"{act_state['merge_pos']}\", logger)\n            m_pos = act_state[\"merge_pos\"]\n            source_state[\"merge_pos\"] = m_pos\n        else:\n            m_pos = None\n\n        if arch in [\"learn_mobilenet_v2\", \"learn_vgg19\"]:\n            model.module.fix_act(a_pos, m_pos)\n        else:\n            model.module.fix_act(a_pos)\n\n        if \"merged\" in act_state:\n            model.to(\"cpu\")\n            if m_pos:\n                assert arch in [\"learn_mobilenet_v2\", \"learn_vgg19\"]\n                model.module.merge(act_pos=act_state[\"act_pos\"], merge_pos=m_pos)\n            else:\n                model.module.merge(act_pos=act_state[\"act_pos\"])\n            model.to(\"cuda\")\n\n    model.load_state_dict(source_state[\"state_dict\"], strict=False)\n\n    del source_state[\"state_dict\"]\n    return source_state", "\n\ndef cp_state(new_state, source_state, name):\n    if name in source_state:\n        new_state[name] = source_state[name]\n\n\ndef print_act_pos(module, source_state):\n    with torch.no_grad():\n        if \"act_pos\" in source_state:\n            act_pos = source_state[\"act_pos\"]\n            print(\"learned activation is below\")\n            print(sorted(list(act_pos)))\n            print(f\"{len(act_pos)} alive\")", "\n\ndef log_tool(string, logger, mode=\"print\"):\n    if mode == \"print\":\n        print(string)\n        if logger != None:\n            logger.comment(f\"{string}\")\n    elif mode == \"opt\":\n        logger.comment(\"\\n-----------------------------\\n\")\n        logger.comment(string)\n        logger.comment(\"\\n-----------------------------\\n\")\n    elif mode == \"ap\":\n        logger.comment(\"-----------------------------\")\n        logger.comment(\"learned activation is below\")\n        logger.comment(string)\n        logger.comment(\"-----------------------------\")", "\n\n# Implementation adapted from AlphaNet - https://github.com/facebookresearch/AlphaNet\nclass KLLossSoft(torch.nn.modules.loss._Loss):\n    \"\"\"inplace distillation for image classification\n    output: output logits of the student network\n    target: output logits of the teacher network\n    T: temperature\n    \"\"\"\n\n    def __init__(\n        self, alpha, teacher, size_average=None, reduce=None, reduction: str = \"mean\"\n    ) -> None:\n        super().__init__(size_average, reduce, reduction)\n        self.alpha = alpha\n        self.teacher = teacher\n\n    def forward(self, output, soft_logits, target, temperature=1.0):\n        output, soft_logits = output / temperature, soft_logits / temperature\n        soft_target_prob = torch.nn.functional.softmax(soft_logits, dim=1)\n        output_log_prob = torch.nn.functional.log_softmax(output, dim=1)\n        kd_loss = -torch.sum(soft_target_prob * output_log_prob, dim=1)\n        if target is not None:\n            target = torch.zeros_like(output).scatter(1, target.view(-1, 1), 1)\n            target = target.unsqueeze(1)\n            output_log_prob = output_log_prob.unsqueeze(2)\n            ce_loss = -torch.bmm(target, output_log_prob).squeeze()\n            loss = (\n                self.alpha * temperature * temperature * kd_loss\n                + (1.0 - self.alpha) * ce_loss\n            )\n        else:\n            loss = kd_loss\n\n        if self.reduction == \"mean\":\n            return loss.mean()\n        elif self.reduction == \"sum\":\n            return loss.sum()\n        return loss", ""]}
{"filename": "utils/loaders.py", "chunked_list": ["import os\n\nimport torch\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport utils.datasets as datasets\n", "import utils.datasets as datasets\n\nfrom timm.data import create_transform\n\n\ndef get_train_loader(\n    data_path,\n    batch_size,\n    workers=5,\n    _worker_init_fn=None,\n    input_size=224,\n    nclass=1000,\n    holdout=None,\n    timm_aug=False,\n):\n    traindir = os.path.join(data_path, \"train\")\n    normalize = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n    )\n\n    if holdout == \"val\":\n        aug = transforms.Compose(\n            [\n                transforms.Resize(256),\n                transforms.CenterCrop(input_size),\n                transforms.ToTensor(),\n                normalize,\n            ]\n        )\n    else:\n        if timm_aug:\n            aug = create_transform(\n                input_size=224,\n                is_training=True,\n                color_jitter=0.4,\n                auto_augment=\"rand-m5-mstd0.5-inc1\",\n                re_prob=0.25,\n                re_mode=\"pixel\",\n                re_count=1,\n                interpolation=\"bicubic\",\n            )\n        else:\n            aug = transforms.Compose(\n                [\n                    transforms.RandomResizedCrop(input_size),\n                    transforms.RandomHorizontalFlip(),\n                    transforms.ToTensor(),\n                    normalize,\n                ]\n            )\n\n    train_dataset = datasets.ImageFolder(traindir, aug, nclass=nclass, holdout=holdout)\n\n    if torch.distributed.is_initialized():\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n    else:\n        train_sampler = None\n\n    if holdout == \"val\":\n        shfl = False\n    else:\n        shfl = train_sampler is None\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        sampler=train_sampler,\n        batch_size=batch_size,\n        shuffle=shfl,\n        num_workers=workers,\n        worker_init_fn=_worker_init_fn,\n        pin_memory=True,\n    )\n\n    return train_loader, len(train_loader)", "\n\ndef get_val_loader(\n    data_path, batch_size, workers=5, _worker_init_fn=None, input_size=224, nclass=1000\n):\n    valdir = os.path.join(data_path, \"val\")\n    normalize = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n    )\n    val_dataset = datasets.ImageFolder(\n        valdir,\n        transforms.Compose(\n            [\n                transforms.Resize(256),\n                transforms.CenterCrop(input_size),\n                transforms.ToTensor(),\n                normalize,\n            ]\n        ),\n        nclass=nclass,\n    )\n\n    if torch.distributed.is_initialized():\n        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n    else:\n        val_sampler = None\n\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        sampler=val_sampler,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=workers,\n        worker_init_fn=_worker_init_fn,\n        pin_memory=True,\n    )\n\n    return val_loader, len(val_loader)", ""]}
{"filename": "utils/datasets.py", "chunked_list": ["import os\nimport os.path\nfrom typing import Any, Callable, cast, Dict, List, Optional, Tuple\nfrom typing import Union\n\nfrom PIL import Image\n\nfrom torchvision.datasets.vision import VisionDataset\n\n\ndef has_file_allowed_extension(\n    filename: str, extensions: Union[str, Tuple[str, ...]]\n) -> bool:\n    \"\"\"Checks if a file is an allowed extension.\n\n    Args:\n        filename (string): path to a file\n        extensions (tuple of strings): extensions to consider (lowercase)\n\n    Returns:\n        bool: True if the filename ends with one of given extensions\n    \"\"\"\n    return filename.lower().endswith(\n        extensions if isinstance(extensions, str) else tuple(extensions)\n    )", "\n\ndef has_file_allowed_extension(\n    filename: str, extensions: Union[str, Tuple[str, ...]]\n) -> bool:\n    \"\"\"Checks if a file is an allowed extension.\n\n    Args:\n        filename (string): path to a file\n        extensions (tuple of strings): extensions to consider (lowercase)\n\n    Returns:\n        bool: True if the filename ends with one of given extensions\n    \"\"\"\n    return filename.lower().endswith(\n        extensions if isinstance(extensions, str) else tuple(extensions)\n    )", "\n\ndef is_image_file(filename: str) -> bool:\n    \"\"\"Checks if a file is an allowed image extension.\n\n    Args:\n        filename (string): path to a file\n\n    Returns:\n        bool: True if the filename ends with a known image extension\n    \"\"\"\n    return has_file_allowed_extension(filename, IMG_EXTENSIONS)", "\n\ndef find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n    \"\"\"Finds the class folders in a dataset.\n\n    See :class:`DatasetFolder` for details.\n    \"\"\"\n    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n    if not classes:\n        raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n\n    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    return classes, class_to_idx", "\n\ndef get_holdout():\n    \"\"\"Returns a list of filename of a holdout validation data\n\n    Written in \"utils/txt/holdout_val.txt\" file\n    \"\"\"\n    with open(\"utils/txt/holdout_val.txt\") as f:\n        fnames = f.read().splitlines()\n    return set(fnames)", "\n\ndef make_dataset(\n    directory: str,\n    class_to_idx: Optional[Dict[str, int]] = None,\n    extensions: Optional[Union[str, Tuple[str, ...]]] = None,\n    is_valid_file: Optional[Callable[[str], bool]] = None,\n    holdout: Optional[str] = None,\n) -> List[Tuple[str, int]]:\n    \"\"\"Generates a list of samples of a form (path_to_sample, class).\n\n    See :class:`DatasetFolder` for details.\n\n    Note: The class_to_idx parameter is here optional and will use the logic of the ``find_classes`` function\n    by default.\n    \"\"\"\n    directory = os.path.expanduser(directory)\n\n    if class_to_idx is None:\n        _, class_to_idx = find_classes(directory)\n    elif not class_to_idx:\n        raise ValueError(\n            \"'class_to_index' must have at least one entry to collect any samples.\"\n        )\n\n    both_none = extensions is None and is_valid_file is None\n    both_something = extensions is not None and is_valid_file is not None\n    if both_none or both_something:\n        raise ValueError(\n            \"Both extensions and is_valid_file cannot be None or not None at the same time\"\n        )\n\n    if extensions is not None:\n\n        def is_valid_file(x: str) -> bool:\n            return has_file_allowed_extension(x, extensions)  # type: ignore[arg-type]\n\n    is_valid_file = cast(Callable[[str], bool], is_valid_file)\n\n    if holdout != None:\n        assert holdout in [\"train\", \"val\"]\n        holdout_fnames = get_holdout()\n\n    instances = []\n    available_classes = set()\n    for target_class in sorted(class_to_idx.keys()):\n        class_index = class_to_idx[target_class]\n        target_dir = os.path.join(directory, target_class)\n        if not os.path.isdir(target_dir):\n            continue\n        for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n            for fname in sorted(fnames):\n                path = os.path.join(root, fname)\n                if is_valid_file(path):\n                    item = path, class_index\n                    if holdout == \"train\" and fname in holdout_fnames:\n                        continue\n                    elif holdout == \"val\" and not fname in holdout_fnames:\n                        continue\n                    instances.append(item)\n\n                    if target_class not in available_classes:\n                        available_classes.add(target_class)\n\n    empty_classes = set(class_to_idx.keys()) - available_classes\n    if empty_classes:\n        msg = (\n            f\"Found no valid file for the classes {', '.join(sorted(empty_classes))}. \"\n        )\n        if extensions is not None:\n            msg += f\"Supported extensions are: {extensions if isinstance(extensions, str) else ', '.join(extensions)}\"\n        raise FileNotFoundError(msg)\n\n    return instances", "\n\nclass DatasetFolder(VisionDataset):\n    \"\"\"A generic data loader.\n\n    This default directory structure can be customized by overriding the\n    :meth:`find_classes` method.\n\n    Args:\n        root (string): Root directory path.\n        loader (callable): A function to load a sample given its path.\n        extensions (tuple[string]): A list of allowed extensions.\n            both extensions and is_valid_file should not be passed.\n        transform (callable, optional): A function/transform that takes in\n            a sample and returns a transformed version.\n            E.g, ``transforms.RandomCrop`` for images.\n        target_transform (callable, optional): A function/transform that takes\n            in the target and transforms it.\n        is_valid_file (callable, optional): A function that takes path of a file\n            and check if the file is a valid file (used to check of corrupt files)\n            both extensions and is_valid_file should not be passed.\n\n     Attributes:\n        classes (list): List of the class names sorted alphabetically.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        samples (list): List of (sample path, class_index) tuples\n        targets (list): The class_index value for each image in the dataset\n    \"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        loader: Callable[[str], Any],\n        extensions: Optional[Tuple[str, ...]] = None,\n        transform: Optional[Callable] = None,\n        target_transform: Optional[Callable] = None,\n        is_valid_file: Optional[Callable[[str], bool]] = None,\n        nclass: int = 1000,\n        holdout: Optional[str] = None,\n    ) -> None:\n        super().__init__(root, transform=transform, target_transform=target_transform)\n\n        if nclass < 1000:\n            self.classes, self.class_to_idx = self.find_subclasses(nclass=nclass)\n        else:\n            self.classes, self.class_to_idx = self.find_classes(self.root)\n\n        self.loader = loader\n        self.extensions = extensions\n\n        self.samples = self.make_dataset(\n            self.root, self.class_to_idx, self.extensions, is_valid_file, holdout\n        )\n        self.targets = [s[1] for s in self.samples]\n\n    @staticmethod\n    def make_dataset(\n        directory: str,\n        class_to_idx: Dict[str, int],\n        extensions: Optional[Tuple[str, ...]] = None,\n        is_valid_file: Optional[Callable[[str], bool]] = None,\n        holdout: Optional[str] = None,\n    ) -> List[Tuple[str, int]]:\n        \"\"\"Generates a list of samples of a form (path_to_sample, class).\n\n        This can be overridden to e.g. read files from a compressed zip file instead of from the disk.\n\n        Args:\n            directory (str): root dataset directory, corresponding to ``self.root``.\n            class_to_idx (Dict[str, int]): Dictionary mapping class name to class index.\n            extensions (optional): A list of allowed extensions.\n                Either extensions or is_valid_file should be passed. Defaults to None.\n            is_valid_file (optional): A function that takes path of a file\n                and checks if the file is a valid file\n                (used to check of corrupt files) both extensions and\n                is_valid_file should not be passed. Defaults to None.\n\n        Raises:\n            ValueError: In case ``class_to_idx`` is empty.\n            ValueError: In case ``extensions`` and ``is_valid_file`` are None or both are not None.\n            FileNotFoundError: In case no valid file was found for any class.\n\n        Returns:\n            List[Tuple[str, int]]: samples of a form (path_to_sample, class)\n        \"\"\"\n        if class_to_idx is None:\n            # prevent potential bug since make_dataset() would use the class_to_idx logic of the\n            # find_classes() function, instead of using that of the find_classes() method, which\n            # is potentially overridden and thus could have a different logic.\n            raise ValueError(\"The class_to_idx parameter cannot be None.\")\n        return make_dataset(\n            directory,\n            class_to_idx,\n            extensions=extensions,\n            is_valid_file=is_valid_file,\n            holdout=holdout,\n        )\n\n    def find_classes(self, directory: str) -> Tuple[List[str], Dict[str, int]]:\n        \"\"\"Find the class folders in a dataset structured as follows::\n\n            directory/\n            \u251c\u2500\u2500 class_x\n            \u2502   \u251c\u2500\u2500 xxx.ext\n            \u2502   \u251c\u2500\u2500 xxy.ext\n            \u2502   \u2514\u2500\u2500 ...\n            \u2502       \u2514\u2500\u2500 xxz.ext\n            \u2514\u2500\u2500 class_y\n                \u251c\u2500\u2500 123.ext\n                \u251c\u2500\u2500 nsdf3.ext\n                \u2514\u2500\u2500 ...\n                \u2514\u2500\u2500 asd932_.ext\n\n        This method can be overridden to only consider\n        a subset of classes, or to adapt to a different dataset directory structure.\n\n        Args:\n            directory(str): Root directory path, corresponding to ``self.root``\n\n        Raises:\n            FileNotFoundError: If ``dir`` has no class folders.\n\n        Returns:\n            (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\n        \"\"\"\n        return find_classes(directory)\n\n    def find_subclasses(self, nclass=100):\n        \"\"\"Finds the class folders in a dataset.\"\"\"\n\n        with open(\"utils/txt/class100.txt\", \"r\") as f:\n            classes = f.read().splitlines()\n\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n        assert len(classes) == nclass\n\n        return classes, class_to_idx\n\n    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        \"\"\"\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target\n\n    def __len__(self) -> int:\n        return len(self.samples)", "\n\nIMG_EXTENSIONS = (\n    \".jpg\",\n    \".jpeg\",\n    \".png\",\n    \".ppm\",\n    \".bmp\",\n    \".pgm\",\n    \".tif\",", "    \".pgm\",\n    \".tif\",\n    \".tiff\",\n    \".webp\",\n)\n\n\ndef pil_loader(path: str) -> Image.Image:\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \"rb\") as f:\n        img = Image.open(f)\n        return img.convert(\"RGB\")", "\n\n# TODO: specify the return type\ndef accimage_loader(path: str) -> Any:\n    import accimage\n\n    try:\n        return accimage.Image(path)\n    except OSError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)", "\n\ndef default_loader(path: str) -> Any:\n    from torchvision import get_image_backend\n\n    if get_image_backend() == \"accimage\":\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n", "\n\nclass ImageFolder(DatasetFolder):\n    \"\"\"A generic data loader where the images are arranged in this way by default: ::\n\n        root/dog/xxx.png\n        root/dog/xxy.png\n        root/dog/[...]/xxz.png\n\n        root/cat/123.png\n        root/cat/nsdf3.png\n        root/cat/[...]/asd932_.png\n\n    This class inherits from :class:`~torchvision.datasets.DatasetFolder` so\n    the same methods can be overridden to customize the dataset.\n\n    Args:\n        root (string): Root directory path.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an image given its path.\n        is_valid_file (callable, optional): A function that takes path of an Image file\n            and check if the file is a valid file (used to check of corrupt files)\n\n     Attributes:\n        classes (list): List of the class names sorted alphabetically.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    \"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        transform: Optional[Callable] = None,\n        target_transform: Optional[Callable] = None,\n        loader: Callable[[str], Any] = default_loader,\n        is_valid_file: Optional[Callable[[str], bool]] = None,\n        nclass: int = 1000,\n        holdout: Optional[str] = None,\n    ):\n        # ImageNet/ImageNet-100 is implemented\n        assert nclass in [100, 1000]\n        if holdout != None:\n            assert holdout in [\"train\", \"val\"]\n        super().__init__(\n            root,\n            loader,\n            IMG_EXTENSIONS if is_valid_file is None else None,\n            transform=transform,\n            target_transform=target_transform,\n            is_valid_file=is_valid_file,\n            nclass=nclass,\n            holdout=holdout,\n        )\n        self.imgs = self.samples", ""]}
{"filename": "config/arguments.py", "chunked_list": ["import os\nimport argparse\n\n\ndef str2bool(v):\n    \"\"\"Cast string to boolean\"\"\"\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\"Boolean value expected.\")", "\n\ndef make_log_file(mode, checkpoint, filename, **kwargs):\n    if mode in [\"train\", \"finetune\", \"dp_imp\"]:\n        if not os.path.isdir(checkpoint):\n            os.makedirs(checkpoint, exist_ok=True)\n    if mode == \"train\":\n        write_filename = filename\n        log_filename = \"log.txt\"\n        logs_name = \"logs\"\n    elif mode == \"finetune\":\n        if args.filename.endswith(\".pth\"):\n            write_filename = filename[:-4]\n        write_filename = write_filename + f\"_ft_lr{kwargs['lr']}.pth\"\n        log_filename = f\"log_{write_filename[:-4]}.txt\"\n        logs_name = f\"logs_{write_filename[:-4]}\"\n    elif mode == \"dp_imp\":\n        write_filename = \"\"\n        log_filename = f\"log_f{args.from_blk}_t{args.to_blk}.txt\"\n        logs_name = f\"log_f{args.from_blk}_t{args.to_blk}\"\n    else:\n        write_filename = \"\"\n        log_filename = \"\"\n        logs_name = \"\"\n    log_header = [\"Epoch\", \"LR\", \"Train Loss\", \"Valid Loss\", \"Train Acc.\", \"Valid Acc.\"]\n    return write_filename, log_filename, logs_name, log_header", "\n\nparser = argparse.ArgumentParser(description=\"PyTorch ImageNet Training\")\n\nparser.add_argument(\n    \"--aug\", default=False, type=str2bool, help=\"Add augmentation to training script\"\n)\nparser.add_argument(\"--distill\", default=0.0, type=float, help=\"distillation ratio\")\n# data and models\nparser.add_argument(", "# data and models\nparser.add_argument(\n    \"-d\", \"--data\", metavar=\"DIR\", default=\"/ssd_data/imagenet\", help=\"path to dataset\"\n)\nparser.add_argument(\n    \"--nclass\",\n    default=1000,\n    type=int,\n    choices=[10, 100, 1000],\n    help=\"number of class to use\",", "    choices=[10, 100, 1000],\n    help=\"number of class to use\",\n)\nparser.add_argument(\n    \"--dataset\",\n    default=\"imagenet\",\n    type=str,\n    choices=[\"imagenet\"],\n    help=\"Type of dataset to use\",\n)", "    help=\"Type of dataset to use\",\n)\nparser.add_argument(\n    \"-a\",\n    \"--arch\",\n    metavar=\"ARCH\",\n    default=\"mobilenet_v2\",\n    help=\"model architecture: \",\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"-j\",\n    \"--workers\",\n    default=4,\n    type=int,\n    metavar=\"N\",\n    help=\"number of data loading workers (default: 4)\",\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"--epochs\", default=150, type=int, metavar=\"N\", help=\"number of total epochs to run\"\n)\nparser.add_argument(\n    \"--start-epoch\",\n    default=0,\n    type=int,\n    metavar=\"N\",\n    help=\"manual epoch number (useful on restarts)\",", "    metavar=\"N\",\n    help=\"manual epoch number (useful on restarts)\",\n)\nparser.add_argument(\n    \"-b\",\n    \"--batch-size\",\n    default=256,\n    type=int,\n    metavar=\"N\",\n    help=\"mini-batch size (default: 256), this is the total \"", "    metavar=\"N\",\n    help=\"mini-batch size (default: 256), this is the total \"\n    \"batch size of all GPUs on the current node when \"\n    \"using Data Parallel or Distributed Data Parallel\",\n)\nparser.add_argument(\n    \"-c\",\n    \"--checkpoint\",\n    default=\"checkpoints\",\n    type=str,", "    default=\"checkpoints\",\n    type=str,\n    metavar=\"PATH\",\n    help=\"path to save checkpoint (default: checkpoints)\",\n)\nparser.add_argument(\n    \"-f\",\n    \"--filename\",\n    default=\"checkpoint.pth\",\n    type=str,", "    default=\"checkpoint.pth\",\n    type=str,\n    metavar=\"FILE\",\n    help=\"filename of the checkopint (default: checkpoint.pth)\",\n)\nparser.add_argument(\n    \"--width-mult\", type=float, default=1.0, help=\"MobileNet model width multiplier.\"\n)\nparser.add_argument(\n    \"--input-size\", type=int, default=224, help=\"MobileNet model input resolution\"", "parser.add_argument(\n    \"--input-size\", type=int, default=224, help=\"MobileNet model input resolution\"\n)\n\n# Optimizer\nparser.add_argument(\n    \"--lr\",\n    \"--learning-rate\",\n    default=0.1,\n    type=float,", "    default=0.1,\n    type=float,\n    metavar=\"LR\",\n    help=\"initial learning rate\",\n    dest=\"lr\",\n)\nparser.add_argument(\"--momentum\", default=0.9, type=float, metavar=\"M\", help=\"momentum\")\nparser.add_argument(\n    \"--wd\",\n    \"--weight-decay\",", "    \"--wd\",\n    \"--weight-decay\",\n    default=1e-5,\n    type=float,\n    metavar=\"W\",\n    help=\"weight decay (default: 4e-5)\",\n    dest=\"weight_decay\",\n)\nparser.add_argument(\"--nesterov\", default=True, type=str2bool, help=\"nesterov for sgd\")\nparser.add_argument(", "parser.add_argument(\"--nesterov\", default=True, type=str2bool, help=\"nesterov for sgd\")\nparser.add_argument(\n    \"--lr-decay\", type=str, default=\"cos\", help=\"mode for learning rate decay\"\n)\nparser.add_argument(\n    \"--schedule\",\n    type=int,\n    nargs=\"+\",\n    default=[45, 90, 135, 157],\n    help=\"decrease learning rate at these epochs.\",", "    default=[45, 90, 135, 157],\n    help=\"decrease learning rate at these epochs.\",\n)\nparser.add_argument(\n    \"--gamma\", type=float, default=0.1, help=\"LR is multiplied by gamma on schedule.\"\n)\n\n# Options\nparser.add_argument(\n    \"-p\",", "parser.add_argument(\n    \"-p\",\n    \"--print-freq\",\n    default=1000,\n    type=int,\n    metavar=\"N\",\n    help=\"print frequency (default: 1000)\",\n)\nparser.add_argument(\n    \"--resume\",", "parser.add_argument(\n    \"--resume\",\n    default=\"\",\n    type=str,\n    metavar=\"PATH\",\n    help=\"path to latest checkpoint (default: none)\",\n)\nparser.add_argument(\n    \"-m\",\n    \"--mode\",", "    \"-m\",\n    \"--mode\",\n    default=\"train\",\n    type=str,\n    choices=[\"train\", \"finetune\", \"eval\", \"merge\", \"dp_imp\"],\n    help=\"script mode : choose among (train/finetune/eval/merge/dp_imp)\",\n)\nparser.add_argument(\n    \"--seed\", default=None, type=int, help=\"seed for initializing training. \"\n)", "    \"--seed\", default=None, type=int, help=\"seed for initializing training. \"\n)\n\nparser.add_argument(\"--debug\", action=\"store_true\", help=\"only heavy ones\")\n\n# Hyperparams of main experiment/baseline\nparser.add_argument(\n    \"--pretrain\", default=\"\", type=str, help=\"path to the pretrained vanilla network\"\n)\nparser.add_argument(\"--act-path\", default=\"\", type=str, help=\"path to the act pos\")", ")\nparser.add_argument(\"--act-path\", default=\"\", type=str, help=\"path to the act pos\")\nparser.add_argument(\n    \"--ft-holdout\",\n    default=False,\n    type=str2bool,\n    help=\"Whether to finetune with holdout validation set\",\n)\n# Merge & Finetune\nparser.add_argument(", "# Merge & Finetune\nparser.add_argument(\n    \"--time-path\",\n    default=\"\",\n    type=str,\n    help=\"\"\"\n    Path to the time table. \n    Needed in finding optimal merging pattern using DP. \n    Will use act_pos for merging pattern if not specififed.\n    \"\"\",", "    Will use act_pos for merging pattern if not specififed.\n    \"\"\",\n)\n# DP\nparser.add_argument(\n    \"--imp-epoch\",\n    default=1,\n    type=int,\n    help=\"\"\"\n    epoch per measuring the importance", "    help=\"\"\"\n    epoch per measuring the importance\n    \"\"\",\n)\nparser.add_argument(\n    \"--imp-lr-decay\",\n    default=\"cos\",\n    type=str,\n    help=\"\"\"\n    lr_decay when measuring the importance", "    help=\"\"\"\n    lr_decay when measuring the importance\n    \"\"\",\n)\nparser.add_argument(\n    \"--from-blk\",\n    default=None,\n    type=int,\n    help=\"\"\"\n    ID of the block to measure from", "    help=\"\"\"\n    ID of the block to measure from\n    \"\"\",\n)\nparser.add_argument(\n    \"--to-blk\",\n    default=None,\n    type=int,\n    help=\"\"\"\n    ID of the block to measure until", "    help=\"\"\"\n    ID of the block to measure until\n    \"\"\",\n)\nparser.add_argument(\n    \"--exclude-zeros\",\n    default=False,\n    type=str2bool,\n    help=\"\"\"\n    Exclude blocks starting with 0 activation", "    help=\"\"\"\n    Exclude blocks starting with 0 activation\n    when both of the start and end indices are \n    end of the ResidualBlock\n    \"\"\",\n)\n# DepthShrinker\nparser.add_argument(\n    \"--lamb\",\n    default=1e-4,", "    \"--lamb\",\n    default=1e-4,\n    type=float,\n    help=\"lambda value that weighs sparsity regularizer\",\n)\nparser.add_argument(\n    \"--reg\",\n    default=\"none\",\n    choices=[\"none\", \"soft\", \"w1.0\", \"w1.4\"],\n    type=str,", "    choices=[\"none\", \"soft\", \"w1.0\", \"w1.4\"],\n    type=str,\n    help=\"Type of regularizer to use\",\n)\nparser.add_argument(\n    \"--compress-k\",\n    default=8,\n    type=int,\n    help=\"The number of InvertedResidualBlock to compress (min 0, max 17)\",\n)", "    help=\"The number of InvertedResidualBlock to compress (min 0, max 17)\",\n)\n# fmt: off\nparser.add_argument(\n    \"--ds-pattern\",\n    default=\"none\",\n    choices=[\"none\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"A10\", \"B10\", \"C10\", \"D10\", \"AR\", \"BR\", \"CR\", \"AR10\", \"BR10\", \"CR10\", \"AR_AUG\", \"BR_AUG\", \"CR_AUG\", \"AR10_AUG\", \"BR10_AUG\", \"CR10_AUG\"],\n    type=str,\n    help=\"pre-defined pattern from the paper\",\n)", "    help=\"pre-defined pattern from the paper\",\n)\n# fmt: on\nargs = parser.parse_args()\n\nif args.dataset == \"cifar\":\n    assert args.nclass == 10\nelif args.dataset == \"imagenet\":\n    assert args.nclass in [100, 1000]\n", "\n# Some post-processing on args for each mode\nif args.mode in [\"train\", \"dp_imp\"]:\n    if args.arch in [\"mobilenet_v2\", \"learn_mobilenet_v2\", \"dep_shrink_mobilenet_v2\"]:\n        args.checkpoint += f\"_w{args.width_mult}\"\n\n    # Architecture specific config\n    if args.mode in [\"dp_imp\"]:\n        args.checkpoint += f\"_ie{args.imp_epoch}\"\n        args.checkpoint += f\"_ild_{args.imp_lr_decay}\"\n        if args.exclude_zeros:\n            args.checkpoint += f\"_ex\"\n\n        args.checkpoint = os.path.join(args.checkpoint, f\"par\")\n\n    elif args.mode == \"train\":\n        args.checkpoint += f\"_e{args.epochs}\"\n        if args.arch == \"dep_shrink_mobilenet_v2\":\n            args.checkpoint += f\"_cmp{args.compress_k}\"", "\nif args.aug:\n    args.checkpoint += \"_aug\"\nif args.distill > 0:\n    args.checkpoint += f\"_dt{args.distill}\"\n\nif args.seed:\n    args.checkpoint += f\"_s{args.seed}\"\n\nif args.mode != \"train\" or args.arch == \"mobilenet_v2\" or args.reg == \"none\":\n    args.reg = None", "\nif args.mode != \"train\" or args.arch == \"mobilenet_v2\" or args.reg == \"none\":\n    args.reg = None\n\nif args.resume:\n    assert args.mode in [\"train\", \"finetune\"]\n    if not os.path.isfile(args.resume):\n        raise FileNotFoundError(\"=> no checkpoint found at '{}'\".format(args.resume))\n    args.checkpoint = os.path.dirname(args.resume)\n\nif args.ft_holdout:\n    assert args.mode == \"finetune\"", "\nif args.ft_holdout:\n    assert args.mode == \"finetune\"\n\nif args.time_path:\n    assert args.mode in [\"finetune\", \"merge\"]\n\n\nwrite_filename, log_filename, logs_name, log_header = make_log_file(\n    args.mode, args.checkpoint, args.filename, lr=args.lr", "write_filename, log_filename, logs_name, log_header = make_log_file(\n    args.mode, args.checkpoint, args.filename, lr=args.lr\n)\n\nlog_header = [\"Epoch\", \"LR\", \"Train Loss\", \"Valid Loss\", \"Train Acc.\", \"Valid Acc.\"]\n"]}
{"filename": "exps/aggregate_imp.py", "chunked_list": ["import os\nimport sys\n\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\nimport pandas as pd\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"Inference Time with TensorRT\")\nparser.add_argument(", "parser = argparse.ArgumentParser(description=\"Inference Time with TensorRT\")\nparser.add_argument(\n    \"-d\",\n    \"--dir\",\n    type=str,\n    help=\"directory name\",\n)\nparser.add_argument(\n    \"-n\",\n    \"--num\",", "    \"-n\",\n    \"--num\",\n    type=int,\n    help=\"the number of blks\",\n)\nimport re\n\n\ndef natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_)]", "def natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_)]\n\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    res = pd.DataFrame()\n    for currentpath, folders, files in os.walk(args.dir):\n        for f in sorted(files, key=natural_key):\n            if \".csv\" in f:\n                print(f)\n                tmp = pd.read_csv(os.path.join(currentpath, f))\n                res = pd.concat([res, tmp])\n    print(len(res))\n    assert len(res) == args.num\n    res.to_csv(os.path.join(args.dir, \"importance.csv\"))", ""]}
{"filename": "exps/main.py", "chunked_list": ["# Code started from  https://github.com/d-li14/mobilenetv2.pytorch.git\nimport os\nimport sys\n\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel", "import torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport pandas as pd\nimport copy\n\nfrom models.imagenet import models, blocks", "\nfrom models.imagenet import models, blocks\nfrom models.model_op import reset_layers, valid_blks\n\nfrom config.arguments import args, write_filename, log_filename, logs_name, log_header\nfrom utils import Logger, train, validate\nfrom utils.loaders import get_train_loader, get_val_loader\nfrom utils.misc import (\n    save_checkpoint,\n    load_checkpoint,", "    save_checkpoint,\n    load_checkpoint,\n    print_act_pos,\n    cp_state,\n    log_tool,\n    KLLossSoft,\n)\nfrom tensorboardX import SummaryWriter\nfrom itertools import product\nfrom timm.loss import LabelSmoothingCrossEntropy", "from itertools import product\nfrom timm.loss import LabelSmoothingCrossEntropy\n\n\ndef random_seed(args):\n    cudnn.benchmark = True\n    if args.seed is not None:\n        random.seed(args.seed)\n        torch.manual_seed(args.seed)\n        torch.cuda.manual_seed(args.seed)\n        cudnn.deterministic = True", "\n\ndef select_model(args, arch):\n    print(os.path.join(args.checkpoint, args.filename))\n    print(\"=> creating model '{}'\".format(arch))\n    if arch in [\"learn_mobilenet_v2\"]:\n        model = models[arch](\n            num_classes=args.nclass, width_mult=args.width_mult, add_relu=True\n        )\n    elif arch in [\"mobilenet_v2\", \"dep_shrink_mobilenet_v2\"]:\n        model = models[arch](num_classes=args.nclass, width_mult=args.width_mult)\n    elif arch in [\"vgg19\", \"learn_vgg19\"]:\n        model = models[arch](num_classes=args.nclass)\n    else:\n        raise NotImplementedError(f\"Architecture {arch} not supported\")\n\n    model = torch.nn.DataParallel(model).cuda()\n    return model", "\n\ndef load_resume(args, model, optimizer, logger):\n    print(\"=> loading checkpoint '{}'\".format(args.resume))\n    checkpoint = torch.load(args.resume)\n    args.start_epoch = checkpoint[\"epoch\"]\n    best_prec1 = checkpoint[\"best_prec1\"]\n    source_state = load_checkpoint(model, args.arch, args.resume, logger=logger)\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    print(f\"=> loaded checkpoint '{args.resume}' (epoch {args.start_epoch})\")\n    args.checkpoint = os.path.dirname(args.resume)\n    return best_prec1, source_state", "\n\ndef init_training(args, model, optimizer):\n    title = \"ImageNet-\" + args.arch\n    if args.resume:\n        fname = os.path.join(os.path.dirname(args.resume), log_filename)\n    else:\n        fname = os.path.join(args.checkpoint, log_filename)\n    logger = Logger(\n        fpath=fname,\n        title=title,\n        resume=bool(args.resume),\n    )\n\n    if args.resume:\n        logger.comment(\"\")\n        logger.comment(\"=================\")\n        logger.comment(\"\")\n        logger.comment(\"Resuming...\")\n        logger.comment(\"\")\n        best_prec1, source_state = load_resume(args, model, optimizer, logger)\n    else:\n        best_prec1, source_state = 0, dict()\n\n        if args.ds_pattern != \"none\":\n            ds_pat = (args.ds_pattern, args.compress_k)\n            assert args.pretrain\n        else:\n            ds_pat = None\n        path = args.pretrain if args.pretrain else None\n        act_path = args.act_path if args.act_path else path\n\n        if args.mode != \"train\":\n            source_state = load_checkpoint(\n                model, args.arch, path, act_path, ds_pat, logger\n            )\n\n    logger.set_names(log_header)\n\n    log_tool(str(optimizer), logger, \"opt\")\n    return logger, best_prec1, source_state", "\n\ndef set_loader(args, holdout=False):\n    assert args.dataset in [\"imagenet\"]\n    train_loader, _ = get_train_loader(\n        args.data,\n        args.batch_size,\n        workers=args.workers,\n        input_size=args.input_size,\n        nclass=args.nclass,\n        holdout=\"train\" if holdout else None,\n        timm_aug=args.aug,\n    )\n    if holdout:\n        holdout_loader, _ = get_train_loader(\n            args.data,\n            100,\n            workers=args.workers,\n            input_size=args.input_size,\n            nclass=args.nclass,\n            holdout=\"val\",\n        )\n    else:\n        holdout_loader = None\n    val_loader, _ = get_val_loader(\n        args.data,\n        100,\n        workers=args.workers,\n        input_size=args.input_size,\n        nclass=args.nclass,\n    )\n    return train_loader, holdout_loader, val_loader", "\n\ndef action(model, source_state, mode, args, val_loader, criterion):\n    if mode == \"eval\":\n        print(model)\n        print_act_pos(model.module, source_state)\n        val_loss, prec1 = validate(val_loader, model, criterion, args)\n        print(f\"Validation loss : {val_loss:.2f} | Top 1 Accuracy : {prec1}\")\n    elif mode == \"merge\":\n        model.to(\"cpu\")\n        module = model.module\n        save_state = {}\n        for name in [\"act_pos\", \"merge_pos\", \"compress_k\"]:\n            cp_state(save_state, source_state, name)\n\n        if args.filename.endswith(\".pth\"):\n            filename = args.filename[:-4]\n        filename = f\"{filename}_merged.pth\"\n\n        print(model)\n        if \"merge_pos\" in save_state:\n            m_pos = save_state[\"merge_pos\"]\n        else:\n            m_pos = save_state[\"act_pos\"]\n\n        if args.arch in [\n            \"learn_mobilenet_v2\",\n            \"learn_vgg19\",\n        ]:\n            module.merge(save_state[\"act_pos\"], m_pos)\n            print(model)\n            print()\n            print(f\"act_pos             : {save_state['act_pos']}\")\n            if \"merge_pos\" in save_state:\n                print(f\"merge_pos           : {m_pos}\")\n            print()\n        elif args.arch == \"dep_shrink_mobilenet_v2\":\n            module.merge(save_state[\"act_pos\"])\n            print(model)\n\n        save_state.update({\"state_dict\": model.state_dict(), \"merged\": True})\n        save_checkpoint(\n            save_state,\n            False,\n            checkpoint=args.checkpoint,\n            filename=filename,\n        )", "\n\ndef prepare_finetune(model, args, source_state):\n    model.to(\"cpu\")\n    module = model.module\n    save_state = dict()\n    cp_state(save_state, source_state, \"act_pos\")\n    cp_state(save_state, source_state, \"merge_pos\")\n    cp_state(save_state, source_state, \"compress_k\")\n\n    # For `learn_mobilenet_v2`, see `utils.misc.load_checkpoint` ftn\n    if args.arch == \"dep_shrink_mobilenet_v2\":\n        act_pos, _ = module.get_act_info()\n        module.fix_act(act_pos)\n        save_state[\"act_pos\"] = act_pos\n\n    model.to(\"cuda\")\n    return save_state", "\n\ndef prepare_train(model, args):\n    module = model.module\n    save_state = dict()\n    if args.arch == \"dep_shrink_mobilenet_v2\":\n        module.compress_k = args.compress_k\n        save_state[\"compress_k\"] = args.compress_k\n        module.set_act_hats()\n    return save_state", "\n\ndef run_one_epoch(\n    args,\n    epoch,\n    model,\n    logger,\n    writer,\n    train_loader,\n    val_loader,\n    criterion,\n    optimizer,\n    best_prec1,\n    save_state,\n):\n\n    print(\"\\nEpoch: [%d | %d]\" % (epoch + 1, args.epochs))\n    if args.mode == \"train\":\n        if args.arch == \"dep_shrink_mobilenet_v2\":\n            with torch.no_grad():\n                str_act_param = str(torch.cat(model.module.get_arch_parameters()))\n            log_tool(str_act_param, logger, \"ap\")\n\n    # train for one epoch\n    train_loss, train_acc = train(\n        train_loader,\n        model,\n        criterion,\n        optimizer,\n        epoch,\n        args.epochs,\n        args.lr_decay,\n        args.reg,\n        logger,\n        args,\n    )\n\n    # evaluate on validation set\n    val_loss, prec1 = validate(val_loader, model, criterion, args)\n    lr = optimizer.param_groups[0][\"lr\"]\n\n    # append logger file\n    logger.append([epoch + 1, lr, train_loss, val_loss, train_acc, prec1])\n\n    # tensorboardX\n    writer.add_scalar(\"learning_rate\", lr, epoch + 1)\n    writer.add_scalars(\n        \"loss\", {\"train loss\": train_loss, \"validation loss\": val_loss}, epoch + 1\n    )\n    writer.add_scalars(\n        \"accuracy\",\n        {\"train accuracy\": train_acc, \"validation accuracy\": prec1},\n        epoch + 1,\n    )\n\n    is_best = prec1 > best_prec1\n    best_prec1 = max(prec1, best_prec1)\n    save_state.update(\n        {\n            \"epoch\": epoch + 1,\n            \"arch\": args.arch,\n            \"state_dict\": model.state_dict(),\n            \"best_prec1\": best_prec1,\n            \"optimizer\": optimizer.state_dict(),\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n            \"train_acc\": train_acc,\n            \"val_acc\": prec1,\n        }\n    )\n\n    save_checkpoint(\n        save_state,\n        is_best,\n        checkpoint=args.checkpoint,\n        filename=write_filename,\n    )\n    return best_prec1", "\n\ndef train_finetune(model, criterion, optimizer):\n    # set logging file config\n    mode = args.mode\n    logger, best_prec1, source_state = init_training(args, model, optimizer)\n    if not args.ft_holdout:\n        train_loader, _, val_loader = set_loader(args)\n        vloader = val_loader\n    else:\n        train_loader, holdout_loader, test_loader = set_loader(args, holdout=True)\n        vloader = holdout_loader\n\n    # Mode dependant action of the script\n    if mode == \"train\":\n        save_state = prepare_train(model, args)\n    elif mode == \"finetune\":\n        save_state = prepare_finetune(model, args, source_state)\n        if \"act_pos\" in save_state:\n            logger.comment(str(save_state[\"act_pos\"]))\n\n    logger.comment(str(model))\n\n    # visualization\n    writer = SummaryWriter(os.path.join(args.checkpoint, logs_name))\n    for epoch in range(args.start_epoch, args.epochs):\n        best_prec1 = run_one_epoch(\n            args,\n            epoch,\n            model,\n            logger,\n            writer,\n            train_loader,\n            vloader,\n            criterion,\n            optimizer,\n            best_prec1,\n            save_state,\n        )\n        if args.ft_holdout:\n            test_loss, test_acc = validate(test_loader, model, criterion, args)\n            logger.comment(f\"Test Acc: {test_acc} | Test Loss : {test_loss}\")\n            logger.comment(\"-----------------------------\")\n\n    logger.close()\n    # logger.plot()\n    # savefig(os.path.join(args.checkpoint, \"log.eps\"))\n    writer.close()\n    print(\"Best accuracy:\")\n    print(best_prec1)", "\n\ndef eval_merge(model, criterion):\n    # set logging file config\n    mode = args.mode\n    path = os.path.join(args.checkpoint, args.filename)\n    if args.act_path:\n        act_path = args.act_path\n    else:\n        act_path = path\n    source_state = load_checkpoint(model, args.arch, path, act_path)\n    _, _, val_loader = set_loader(args)\n    # Mode dependant action of the script\n    action(model, source_state, mode, args, val_loader, criterion)", "\n\ndef measure_imp(model, criterion, optimizer):\n    logger, _, source_state = init_training(args, model, optimizer)\n    train_loader, holdout_loader, test_loader = set_loader(args, holdout=True)\n    module = model.module\n\n    act_num = module.get_act_info()[1]\n    epoch = 0\n\n    assert args.arch in [\"learn_mobilenet_v2\", \"learn_vgg19\"]\n    if args.arch in [\"learn_vgg19\"]:\n        default = set(range(1, act_num + 1))\n    elif args.arch == \"learn_mobilenet_v2\":\n        default = set(range(1, act_num + 1)) - set(range(2, act_num + 1, 3))\n    model.module.fix_act(default)\n\n    writer = SummaryWriter(os.path.join(args.checkpoint, logs_name))\n    if all([st in source_state for st in [\"holdout_train_loss\", \"holdout_train_acc\"]]):\n        train_loss, train_acc = (\n            source_state[\"holdout_train_loss\"],\n            source_state[\"holdout_train_acc\"],\n        )\n    else:\n        print(\"Measuring base performance in train-set\")\n        train_loss, train_acc = validate(train_loader, model, criterion, args)\n    if all(st in source_state for st in [\"holdout_val_loss\", \"holdout_val_acc\"]):\n        val_loss, val_acc = (\n            source_state[\"holdout_val_loss\"],\n            source_state[\"holdout_val_acc\"],\n        )\n    else:\n        print(\"Measuring base performance in holdout-val-set\")\n        val_loss, val_acc = validate(holdout_loader, model, criterion, args)\n\n    logger.append([epoch, args.lr, train_loss, val_loss, train_acc, val_acc])\n    logger.comment(\"\")\n\n    base_tl, base_ta, base_vl, base_va = train_loss, train_acc, val_loss, val_acc\n    blks = valid_blks(model.module)\n\n    blk_pos = list(model.module.get_blk_info()[0].values())\n    # Extended blocks have (st, end, st_act, end_act)\n    ext_blks = []\n    for st, end in blks:\n        # Exclude blk_pos blocks that ends with zero\n        # when both st and end is blk_pos\n        if args.arch == \"learn_mobilenet_v2\":\n            if args.exclude_zeros:\n                if st == 0:\n                    st_acts, end_acts = [1], [1]\n                elif st in blk_pos:\n                    st_acts, end_acts = [1, 0], [1]\n                elif end in blk_pos:\n                    st_acts, end_acts = [1], [1, 0]\n                else:\n                    st_acts, end_acts = [1], [1]\n            else:\n                st_acts = [1, 0] if st in blk_pos else [1]\n                end_acts = [1, 0] if end in blk_pos else [1]\n        elif args.arch == \"learn_vgg19\":\n            st_acts, end_acts = [1], [1]\n        else:\n            raise NotImplementedError()\n        for st_act, end_act in product(st_acts, end_acts):\n            ext_blks.append((st, end, st_act, end_act))\n\n    logger.comment(f\"Number of extended blocks are {len(ext_blks)}.\")\n    if args.from_blk != None and args.to_blk != None:\n        logger.comment(f\"Training blocks from {args.from_blk} to {args.to_blk}\")\n    logger.comment(\"\")\n\n    for ind, (st, end, st_act, end_act) in enumerate(ext_blks):\n        if args.from_blk != None and args.to_blk != None:\n            if ind < args.from_blk or args.to_blk <= ind:\n                continue\n        if os.path.exists(os.path.join(args.checkpoint, f\"importance{ind}.csv\")):\n            logger.comment(f\"Skipping block {ind:>3} since the csv file exists...\")\n            continue\n        tmp_model = copy.deepcopy(model)\n        logger.comment(\"\")\n        logger.comment(f\"blk  : {(st, end, st_act, end_act)}\")\n\n        if st + 1 == end:\n            if \"mobilenet_v2\" in args.arch:\n                reset_layers(\n                    tmp_model.module.features[1:-1], blocks[args.arch], st, end, logger\n                )\n            elif \"vgg19\" in args.arch:\n                reset_layers(\n                    tmp_model.module.features, blocks[args.arch], st, end, logger\n                )\n\n        new_acts = set()\n        for i in default:\n            if st < i and i < end:\n                continue\n            new_acts = set.union(new_acts, {i})\n        if st_act:\n            new_acts = set.union(new_acts, {st})\n        if end_act:\n            new_acts = set.union(new_acts, {end})\n\n        logger.comment(f\"acts : {new_acts}\")\n        tmp_model.module.fix_act(new_acts)\n\n        tmp_optimizer = torch.optim.SGD(\n            tmp_model.parameters(),\n            args.lr,\n            momentum=args.momentum,\n            weight_decay=args.weight_decay,\n            nesterov=args.nesterov,\n        )\n\n        n = args.imp_epoch\n        for i in range(n):\n            train_loss, train_acc = train(\n                train_loader,\n                tmp_model,\n                criterion,\n                tmp_optimizer,\n                i,\n                n,\n                args.imp_lr_decay,\n                args.reg,\n                logger,\n                args,\n            )\n            val_loss, val_acc = validate(holdout_loader, tmp_model, criterion, args)\n            lr = tmp_optimizer.param_groups[0][\"lr\"]\n            # append logger file\n            logger.append([epoch + 1, lr, train_loss, val_loss, train_acc, val_acc])\n\n        dtl, dta = train_loss - base_tl, train_acc - base_ta\n        dvl, dva = val_loss - base_vl, val_acc - base_va\n\n        del tmp_model\n        del tmp_optimizer\n\n        blks_dict = {\n            \"id\": [ind],\n            \"st\": [st],\n            \"end\": [end],\n            \"st_act\": [st_act],\n            \"end_act\": [end_act],\n            \"train_loss\": [dtl],\n            \"train_acc\": [dta],\n            \"val_loss\": [dvl],\n            \"val_acc\": [dva],\n        }\n        blks = pd.DataFrame(blks_dict)\n        csv_path = os.path.join(args.checkpoint, f\"importance{ind}.csv\")\n        blks.to_csv(csv_path, sep=\",\", index=False)", "\n\ndef main():\n    random_seed(args)\n    model = select_model(args, args.arch)\n    # define loss function (criterion) and optimizer\n    if args.distill > 0:\n        if args.arch in [\"learn_mobilenet_v2\", \"dep_shrink_mobilenet_v2\"]:\n            teacher_arch = \"mobilenet_v2\"\n        else:\n            raise NotImplementedError\n        teacher_model = models[teacher_arch](\n            num_classes=args.nclass, width_mult=args.width_mult\n        )\n        teacher_model = torch.nn.DataParallel(teacher_model).cuda()\n        pretrain_state = torch.load(args.pretrain)\n        teacher_model.load_state_dict(pretrain_state[\"state_dict\"])\n        del pretrain_state\n        criterion = KLLossSoft(alpha=args.distill, teacher=teacher_model)\n    elif args.aug:\n        criterion = LabelSmoothingCrossEntropy(smoothing=0.1).cuda()\n    else:\n        criterion = nn.CrossEntropyLoss().cuda()\n    optimizer = torch.optim.SGD(\n        model.parameters(),\n        args.lr,\n        momentum=args.momentum,\n        weight_decay=args.weight_decay,\n        nesterov=args.nesterov,\n    )\n\n    if args.mode in [\"train\", \"finetune\"]:\n        train_finetune(model, criterion, optimizer)\n    elif args.mode in [\"eval\", \"merge\"]:\n        eval_merge(model, criterion)\n    elif args.mode in [\"dp_imp\"]:\n        measure_imp(model, criterion, optimizer)\n    else:\n        raise NotImplementedError(\"Add mode\")", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "exps/inference_trt.py", "chunked_list": ["import os\nimport sys\n\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport time\nimport argparse", "import time\nimport argparse\nimport torch.backends.cudnn as cudnn\n\nfrom models.imagenet import (\n    MobileNetV2,\n    LearnMobileNetV2,\n    DepShrinkMobileNetV2,\n    VGG,\n    LearnVGG,", "    VGG,\n    LearnVGG,\n)\nfrom utils.misc import load_checkpoint\nfrom utils.logger import Logger\n\n\nvgg_cfgs = {\n    # (n, inp, oup, isize)\n    \"19\": [", "    # (n, inp, oup, isize)\n    \"19\": [\n        (2, 3, 64, 224),\n        (2, 64, 128, 112),\n        (4, 128, 256, 56),\n        (4, 256, 512, 28),\n        (4, 512, 512, 14),\n    ]\n}\n", "}\n\n\ndef str2bool(v):\n    \"\"\"Cast string to boolean\"\"\"\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\"Boolean value expected.\")", "\n\ndef load(path, arch, width, nclass, logger):\n    if arch == \"learn_mobilenet_v2\":\n        model = nn.DataParallel(\n            LearnMobileNetV2(num_classes=nclass, width_mult=width, add_relu=True)\n        )\n    elif arch == \"dep_shrink_mobilenet_v2\":\n        model = nn.DataParallel(\n            DepShrinkMobileNetV2(num_classes=nclass, width_mult=width)\n        )\n    elif arch == \"mobilenet_v2\":\n        model = nn.DataParallel(MobileNetV2(num_classes=nclass, width_mult=width))\n    # VGG\n    elif arch == \"vgg19\":\n        model = nn.DataParallel(VGG(cfg=vgg_cfgs[\"19\"], num_classes=nclass))\n    elif arch == \"learn_vgg19\":\n        model = nn.DataParallel(LearnVGG(cfg=vgg_cfgs[\"19\"], num_classes=nclass))\n    load_checkpoint(model, arch, path, logger=logger)\n    return model", "\n\ndef main():\n\n    print(torch.cuda.device_count())  # Check the num of gpu\n\n    cudnn.benchmark = True\n    parser = argparse.ArgumentParser(description=\"Inference Time with TensorRT\")\n    parser.add_argument(\n        \"-a\",\n        \"--arch\",\n        metavar=\"ARCH\",\n        default=\"mobilenet_v2\",\n        type=str,\n        choices=[\n            \"mobilenet_v2\",\n            \"learn_mobilenet_v2\",\n            \"dep_shrink_mobilenet_v2\",\n            \"vgg19\",\n            \"learn_vgg19\",\n        ],\n        help=\"model architecture\",\n    )\n    parser.add_argument(\n        \"-w\",\n        \"--width-mult\",\n        type=float,\n        default=1.0,\n        help=\"MobileNet model width multiplier.\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--checkpoint\",\n        type=str,\n        help=\"dir to the checkpoint (default: checkpoints)\",\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--filename\",\n        type=str,\n        metavar=\"FILE\",\n        help=\"filename of the checkopint (default: checkpoint.pth)\",\n    )\n    parser.add_argument(\n        \"--nclass\",\n        type=int,\n        default=1000,\n        choices=[10, 100, 1000],\n        help=\"number of classes\",\n    )\n    parser.add_argument(\n        \"-n\",\n        \"--name\",\n        type=str,\n        help=\"log name\",\n    )\n    parser.add_argument(\n        \"--trt\",\n        default=True,\n        type=str2bool,\n        help=\"Whether to measure in TensorRT or not\",\n    )\n    parser.add_argument(\n        \"--cpu\",\n        default=False,\n        type=str2bool,\n        help=\"Whether to measure with CPU\",\n    )\n\n    args = parser.parse_args()\n    if args.cpu:\n        assert not args.trt\n\n    logger = Logger(os.path.join(args.checkpoint, f\"time_{args.name}.log\"))\n    logger.comment(str(args))\n    logger.comment(\"\")\n\n    gpu_name = torch.cuda.get_device_name(0)\n    logger.comment(gpu_name)\n\n    ckpt = os.path.join(args.checkpoint, args.filename)\n    model = load(ckpt, args.arch, args.width_mult, args.nclass, logger)\n    model = model.to(\"cuda\")\n    model.eval()\n\n    # Fuse batchnorm layers...\n    if args.arch in [\n        \"mobilenet_v2\",\n        \"vgg19\",\n    ]:\n        model.module.merge()\n\n    print(model)\n\n    logger.comment(f\"TensorRT  :  {args.trt}\")\n    logger.comment(f\"CPU             :  {args.cpu}\")\n\n    if args.cpu:\n        time, std = model.module.cpu_time(txt=args.arch, verb=True)\n    else:\n        time, std = model.module.time(txt=args.arch, verb=True, trt=args.trt)\n    logger.comment(\"\")\n    logger.comment(\"TIME MEAN\")\n    logger.comment(str([time]))\n    logger.comment(\"STD\")\n    logger.comment(str([std]))", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "exps/solve_dp.py", "chunked_list": ["import os\nimport sys\n\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\nimport argparse\n\nfrom utils.logger import Logger\nfrom utils.dp import optimal_patterns, optimal_merge_pattern\nfrom utils.misc import save_checkpoint", "from utils.dp import optimal_patterns, optimal_merge_pattern\nfrom utils.misc import save_checkpoint\n\n\ndef make_log_file(checkpoint, filename, **kwargs):\n    if not os.path.isdir(checkpoint):\n        os.makedirs(checkpoint, exist_ok=True)\n    write_filename = filename\n    log_filename = \"log.txt\"\n    return write_filename, log_filename", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Script for solving DP\")\n    parser.add_argument(\n        \"-c\",\n        \"--checkpoint\",\n        default=\"checkpoints\",\n        type=str,\n        metavar=\"PATH\",\n        help=\"path to save checkpoint (default: checkpoints)\",\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--filename\",\n        default=\"checkpoint.pth\",\n        type=str,\n        metavar=\"FILE\",\n        help=\"filename of the checkopint (default: checkpoint.pth)\",\n    )\n    parser.add_argument(\n        \"--time-limit\",\n        type=float,\n        help=\"Time limit to optimize on\",\n    )\n    parser.add_argument(\n        \"--act-num\",\n        type=int,\n        help=\"Total number of the full activations\",\n    )\n    parser.add_argument(\n        \"--time-path\", type=str, help=\"Path to the inference time table of each block\"\n    )\n    parser.add_argument(\n        \"--imp-path\", type=str, help=\"Path to the importance table of each block\"\n    )\n    parser.add_argument(\n        \"--prec\",\n        type=float,\n        help=\"Precision when converting float constriant into int constraint\",\n    )\n    parser.add_argument(\n        \"--chk-time-path\",\n        default=\"\",\n        type=str,\n        help=\"Path to the inference time table of each block\",\n    )\n\n    args = parser.parse_args()\n\n    args.checkpoint = os.path.join(args.checkpoint, f\"p{args.prec}_tl{args.time_limit}\")\n\n    title = \"Solving DP\"\n    write_filename, log_filename = make_log_file(args.checkpoint, args.filename)\n    logger = Logger(os.path.join(args.checkpoint, log_filename), title=title)\n\n    logger.comment(f\"IMP PATH      : {args.imp_path}\")\n    logger.comment(f\"TIME PATH     : {args.time_path}\")\n    logger.comment(f\"TIME LIMIT    : {args.time_limit}\")\n    logger.comment(f\"PRECISION     : {args.prec}\")\n\n    logger.comment(\"\\n---------------------------\\n\")\n\n    act_pos, opt_m_pos, imp_sum, int_time_sum, flt_time_sum = optimal_patterns(\n        flt_time_limit=args.time_limit,\n        act_num=args.act_num,\n        opt_time_path=args.time_path,\n        ext_imp_path=args.imp_path,\n        prec=args.prec,\n        verbose=True,\n        logger=logger,\n    )\n\n    if args.chk_time_path:\n        m_pos, time, _ = optimal_merge_pattern(\n            act_pos=act_pos, act_num=args.act_num, time_path=args.chk_time_path\n        )\n        assert opt_m_pos == m_pos\n        logger.comment(\"\")\n        logger.comment(\"OPTIMAL MERGE POS IS CORRECT\")\n\n    logger.comment(\"\\n---------------------------\\n\")\n\n    logger.comment(f\"ACT POS       : {act_pos}\")\n    logger.comment(f\"MERGE POS     : {opt_m_pos}\")\n    logger.comment(f\"IMP SUM       : {imp_sum}\")\n    logger.comment(f\"INT TIME SUM  : {int_time_sum}\")\n    logger.comment(f\"FLT TIME SUM  : {flt_time_sum}\")\n\n    save_checkpoint(\n        {\n            \"act_pos\": act_pos,\n            \"merge_pos\": opt_m_pos,\n            \"imp_sum\": imp_sum,\n            \"int_time_sum\": int_time_sum,\n            \"flt_time_sum\": flt_time_sum,\n        },\n        False,\n        checkpoint=args.checkpoint,\n        filename=args.filename,\n    )", ""]}
{"filename": "exps/generate_tables.py", "chunked_list": ["import os\nimport sys\n\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\nimport argparse\nfrom itertools import product\n\nfrom models.imagenet import LearnMobileNetV2, LearnVGG, vgg_cfgs\nfrom models.imagenet import InvertedResidual, VGGBlock", "from models.imagenet import LearnMobileNetV2, LearnVGG, vgg_cfgs\nfrom models.imagenet import InvertedResidual, VGGBlock\nfrom models.model_op import valid_blks\nfrom utils.dp import (\n    generate_time_table,\n    generate_optimal_time_table,\n    generate_ext_imp_table,\n)\nfrom utils.logger import Logger\n", "from utils.logger import Logger\n\n\ndef str2bool(v):\n    \"\"\"Cast string to boolean\"\"\"\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\"Boolean value expected.\")", "\n\ndef construct_ext_blks(arch, blks, blk_pos, exclude_zeros=True):\n    ext_blks = []\n    for st, end in blks:\n        # Exclude blk_pos blocks that ends with zero\n        if arch == \"learn_mobilenet_v2\":\n            if exclude_zeros:\n                if st == 0:\n                    st_acts, end_acts = [1], [1]\n                elif st in blk_pos:\n                    st_acts, end_acts = [1, 0], [1]\n                elif end in blk_pos:\n                    st_acts, end_acts = [1], [1, 0]\n                else:\n                    st_acts, end_acts = [1], [1]\n            else:\n                st_acts = [1, 0] if st in blk_pos else [1]\n                end_acts = [1, 0] if end in blk_pos else [1]\n        elif arch == \"learn_vgg19\":\n            st_acts, end_acts = [1], [1]\n        else:\n            raise NotImplementedError()\n        for st_act, end_act in product(st_acts, end_acts):\n            ext_blks.append((st, end, st_act, end_act))\n    return ext_blks", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Script for Generating Time Table\")\n    parser.add_argument(\n        \"-d\",\n        \"--dir\",\n        type=str,\n        default=\"utils/table\",\n        help=\"directory name\",\n    )\n    parser.add_argument(\n        \"-a\",\n        \"--arch\",\n        type=str,\n        default=\"learn_mobilenet_v2\",\n        help=\"architecture of the network\",\n    )\n    parser.add_argument(\n        \"-w\",\n        \"--width-mult\",\n        type=float,\n        default=None,\n        help=\"width multiplier\",\n    )\n    parser.add_argument(\n        \"--nclass\",\n        type=int,\n        default=100,\n        choices=[10, 100, 1000],\n        help=\"number of classes\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--tag\",\n        type=str,\n        default=\"\",\n        help=\"tag\",\n    )\n    parser.add_argument(\"--mode\", type=str, choices=[\"time\", \"opt-time\", \"ext-imp\"])\n    parser.add_argument(\n        \"--time-path\", type=str, default=\"\", help=\"Path to the normal time table\"\n    )\n    parser.add_argument(\n        \"--trt\", type=str2bool, default=True, help=\"Path to the normal time table\"\n    )\n    parser.add_argument(\n        \"--imp-path\", type=str, default=\"\", help=\"Path to the normal importance table\"\n    )\n    parser.add_argument(\n        \"--score\", type=str, default=\"\", help=\"Score for the importance\"\n    )\n    parser.add_argument(\n        \"--norm\", type=str, default=\"default\", help=\"Normalization method of score\"\n    )\n    parser.add_argument(\"--alph\", type=float, default=1.0, help=\"Alpha in normalizing\")\n\n    args = parser.parse_args()\n\n    if args.mode == \"time\":\n        logger = Logger(os.path.join(args.dir, f\"time_table_{args.tag}.log\"))\n\n    assert args.arch in [\n        \"learn_mobilenet_v2\",\n        \"learn_vgg19\",\n    ]\n    if args.arch == \"learn_mobilenet_v2\":\n        model = LearnMobileNetV2(\n            num_classes=args.nclass, width_mult=args.width_mult, add_relu=True\n        )\n        blk_type = InvertedResidual\n        act_num = model.get_act_info()[1]\n        default = set(range(1, act_num + 1)) - set(range(2, act_num + 1, 3))\n    elif args.arch == \"learn_vgg19\":\n        model = LearnVGG(vgg_cfgs[\"19\"], num_classes=args.nclass)\n        act_num = model.get_act_info()[1]\n        blk_type = VGGBlock\n        default = set(range(1, act_num + 1))\n    blks = valid_blks(model)\n\n    if args.mode == \"time\":\n        generate_time_table(\n            model, blks, args.arch, blk_type, args.dir, args.tag, logger, args.trt\n        )\n    elif args.mode == \"opt-time\":\n        assert args.time_path\n        generate_optimal_time_table(blks, args.dir, args.tag, args.time_path)\n    elif args.mode == \"ext-imp\":\n        assert str(args.width_mult) in args.imp_path or args.width_mult == None\n        blk_pos = list(model.get_blk_info()[0].values())\n        ext_blks = construct_ext_blks(args.arch, blks, blk_pos)\n        assert args.imp_path and args.score in [\"train_acc\", \"val_acc\"]\n        generate_ext_imp_table(\n            ext_blks,\n            act_num,\n            args.dir,\n            args.imp_path,\n            args.score,\n            default,\n            args.norm,\n            args.alph,\n        )\n    else:\n        raise NotImplementedError()", ""]}
{"filename": "models/modules_trt.py", "chunked_list": ["from collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom collections import OrderedDict\n\n\nclass NaiveFeed(nn.Module):\n    def __init__(self, odict: OrderedDict) -> None:\n        super().__init__()\n        self.md = nn.Sequential(odict)\n\n    def forward(self, x):\n        return self.md(x)", "class NaiveFeed(nn.Module):\n    def __init__(self, odict: OrderedDict) -> None:\n        super().__init__()\n        self.md = nn.Sequential(odict)\n\n    def forward(self, x):\n        return self.md(x)\n\n\nclass SkipFeed(nn.Module):\n    def __init__(self, odict: OrderedDict, last=nn.Identity) -> None:\n        super().__init__()\n        self.md = nn.Sequential(odict)\n        self.last = last()\n\n    def forward(self, x):\n        return self.last(self.md(x) + x)", "\nclass SkipFeed(nn.Module):\n    def __init__(self, odict: OrderedDict, last=nn.Identity) -> None:\n        super().__init__()\n        self.md = nn.Sequential(odict)\n        self.last = last()\n\n    def forward(self, x):\n        return self.last(self.md(x) + x)\n", "\n\nclass Downsample(nn.Module):\n    def __init__(self, planes) -> None:\n        super().__init__()\n        self.planes = planes\n\n    def forward(self, x):\n        sz = x.shape[3] // 2\n        ch = x.shape[1] // 2\n        out = x\n        out = F.interpolate(out, size=(sz, sz))\n        zeros = out.mul(0)\n        out = torch.cat((zeros[:, :ch, :, :], out), 1)\n        out = torch.cat((out, zeros[:, ch:, :, :]), 1)\n        return out", "\n\nclass SkipFeedDown(nn.Module):\n    def __init__(\n        self, odict: OrderedDict, last=nn.Identity, downsample=nn.Identity()\n    ) -> None:\n        super().__init__()\n        self.md = nn.Sequential(odict)\n        self.last = last()\n        self.downsample = downsample\n\n    def forward(self, x):\n        return self.last(self.md(x) + self.downsample(x))", ""]}
{"filename": "models/model_op.py", "chunked_list": ["from torch import nn\nfrom einops import rearrange, repeat\nfrom typing import List\nfrom functools import reduce\nfrom colorama import Fore, Style\nfrom torchvision.ops.misc import Conv2dNormActivation\nfrom collections import OrderedDict\n\nimport warnings\nimport torch", "import warnings\nimport torch\nimport torch.nn.functional as F\nimport re\n\n\ndef unroll_bn_params(bn):\n    bw = bn.weight.data.clone().detach()\n    bb = bn.bias.data.clone().detach()\n    brm = bn.running_mean.data.clone().detach()\n    brv = bn.running_var.data.clone().detach()\n    return bw, bb, brm, brv", "\n\ndef unroll_conv_params(conv, dw=False):\n    # params of conv (if dw then to dense)\n    if dw:\n        cw = conv.weight.data.clone().detach()\n        cw = repeat(cw, \"c one h w -> c (rep one) h w\", rep=cw.shape[0])\n        cw = torch.diagonal(cw, dim1=0, dim2=1)\n        cw = torch.diag_embed(cw, dim1=0, dim2=1)\n    else:\n        cw = conv.weight.data.clone().detach()\n\n    if conv.bias == None:\n        bw = None\n    else:\n        bw = conv.bias.data.clone().detach()\n\n    return cw, bw", "\n\ndef unroll_bias_params(bias):\n    biasw = bias.weight.data.clone().detach()\n    return biasw\n\n\ndef adjust_with_bn(conv_weight, bias, bn):\n    # Address bn\n    bw, bb, brm, brv = unroll_bn_params(bn)\n    for i in range(bb.size(0)):\n        bias[i] *= bw[i] / torch.sqrt(brv[i] + 1e-5)\n        conv_weight[i] = conv_weight[i] / torch.sqrt(brv[i] + 1e-5) * bw[i]\n        bb[i] -= brm[i] / torch.sqrt(brv[i] + 1e-5) * bw[i]\n    for i in range(bb.size(0)):\n        bias[i] += bb[i]\n    return conv_weight, bias", "\n\ndef conv_with_new(old_conv, old_bias, new_conv, stride, padding):\n    old_conv = rearrange(old_conv, \"oup inp h w -> inp oup h w\")\n    f_new_conv = torch.flip(new_conv, [2, 3])\n    old_conv = F.conv2d(\n        input=old_conv, weight=f_new_conv, stride=1, padding=f_new_conv.shape[2] - 1\n    )\n    old_conv = rearrange(old_conv, \"inp oup h w -> oup inp h w\")\n    old_bias = torch.einsum(\"jikl,i->j\", new_conv, old_bias)\n    return old_conv, old_bias", "\n\n# `stack` is called by reference; pop/push reflects outside the function\ndef push_layer(stack, new_name, weight, mode, stride=1, padding=0, bias=None):\n    if mode == \"cw\":\n        layer = nn.Conv2d(\n            weight.size(1),\n            weight.size(0),\n            kernel_size=weight.size(2),\n            stride=stride,\n            padding=padding,\n            bias=(bias != None),\n        )\n        layer.weight.data = weight.clone().detach()\n        if bias != None:\n            layer.bias.data = bias.clone().detach()\n    elif mode == \"dw\":\n        assert weight.size(1) == weight.size(0)\n        layer = nn.Conv2d(\n            weight.size(1),\n            weight.size(0),\n            kernel_size=weight.size(2),\n            stride=stride,\n            padding=padding,\n            bias=(bias != None),\n            groups=weight.size(1),\n        )\n        weight = torch.diagonal(weight, dim1=0, dim2=1)\n        weight = rearrange(weight, \"h w (c one) -> c one h w\", one=1)\n        layer.weight.data = weight.clone().detach()\n        if bias != None:\n            layer.bias.data = bias.clone().detach()\n    elif mode == \"relu6\":\n        layer = nn.ReLU6(inplace=True)\n    elif mode == \"relu\":\n        layer = nn.ReLU(inplace=True)\n    else:\n        raise NotImplementedError(\"Not implemented mode\")\n\n    if isinstance(stack, list):\n        stack.append((new_name, layer))\n    elif isinstance(stack, nn.Sequential):\n        stack.add_module(new_name, layer)\n    else:\n        raise NotImplementedError(\"Not implemented type of stack\")", "\n\ndef get_mid(tensor: torch.Tensor, reduced_axes: List[int]):\n    sz = list(tensor.shape)\n    new_sz = sz.copy()\n    result = tensor\n    ind = [slice(None)] * len(sz)\n    for i in reduced_axes:\n        new_sz[i] = 1\n        ind[i] = sz[i] // 2\n        result = result[ind].view(new_sz)\n        ind[i] = slice(None)\n    return result", "\n\ndef push_merged_layers(m_layers, pos, weights, cparam, relu=False, act_type=\"relu6\"):\n    assert act_type in [\"relu6\", \"relu\"]\n    _, m_cw, m_p, m_b = weights\n    ctype, cstr = cparam\n    push_layer(m_layers, f\"merged_conv{pos}\", m_cw, ctype, cstr, m_p, m_b)\n    if relu:\n        push_layer(m_layers, f\"relu{pos}\", None, act_type)\n", "\n\ndef update_m_weights(isize, m_cw, m_p, m_b, conv, bn, merged):\n    cw, bw = unroll_conv_params(conv, conv.in_channels == conv.groups)\n    # feature size after conv\n    isize = (isize - cw.size(2) + 2 * conv.padding[0]) // conv.stride[0] + 1\n\n    # Convolve cw (w/ flip) and add bw\n    if merged:\n        m_p += conv.padding[0]\n        m_cw, m_b = conv_with_new(m_cw, m_b, cw, conv.stride[0], conv.padding[0])\n    else:\n        m_p = conv.padding[0]\n        m_cw = cw\n        m_b = torch.zeros(cw.size(0))\n\n    if bw != None:\n        m_b += bw\n\n    assert isinstance(bn, nn.BatchNorm2d) or bn == None\n    if isinstance(bn, nn.BatchNorm2d):\n        m_cw, m_b = adjust_with_bn(m_cw, m_b, bn)\n\n    return isize, m_cw, m_p, m_b", "\n\ndef merge_or_new(m_layers, pos, weights, lyrs, merged, cparam, pop_k=2):\n    \"\"\"\n    - if `merged` == True :\n        - Pop the last (conv, bn) in `m_layers` (have same params with `weights`).\n        - Merge the `weights` with `lyrs` and return the merged params.\n    - if `merged` == False :\n        - Return the parameters of `lyrs`.\n    \"\"\"\n    isize, m_cw, m_p, m_b = weights\n    ctype, cstr = cparam\n    conv, bn = lyrs\n\n    pos += 1\n    isize, m_cw, m_p, m_b = update_m_weights(isize, m_cw, m_p, m_b, conv, bn, merged)\n\n    new_type = \"dw\" if conv.in_channels == conv.groups else \"cw\"\n    new_str = conv.stride[0]\n\n    if merged:\n        # pop k layers from the back\n        del m_layers[-pop_k:]\n        pos -= 1\n        ctype = \"dw\" if all(tp == \"dw\" for tp in [new_type, ctype]) else \"cw\"\n        cstr = cstr * new_str\n    else:\n        ctype = new_type\n        cstr = new_str\n\n    return pos, (isize, m_cw, m_p, m_b), (ctype, cstr)", "\n\ndef fuse_skip(cw):\n    # Fuse identity addition\n    mid = cw.size(2) // 2\n    for i in range(cw.size(1)):\n        cw[i][i][mid][mid] += 1\n\n\ndef get_skip_info(blocks, arch=\"mbv2\"):\n    assert arch in [\"mbv2\"]\n\n    node_pos = 0\n    skip_s2t, skip_t2s = dict(), dict()\n    str_pos = set()\n    for block in blocks:\n        if arch == \"mbv2\" and block.use_res_connect:\n            src = node_pos\n        for layer in block.conv.modules():\n            if isinstance(layer, nn.Conv2d):\n                node_pos += 1\n                # merging strided conv is not implemented yet\n                if layer.stride != (1, 1):\n                    if arch == \"mbv2\":\n                        str_pos.add(node_pos + 1)\n        if arch == \"mbv2\" and block.use_res_connect:\n            skip_s2t[src], skip_t2s[node_pos] = node_pos, src\n    return skip_s2t, skip_t2s, str_pos", "\ndef get_skip_info(blocks, arch=\"mbv2\"):\n    assert arch in [\"mbv2\"]\n\n    node_pos = 0\n    skip_s2t, skip_t2s = dict(), dict()\n    str_pos = set()\n    for block in blocks:\n        if arch == \"mbv2\" and block.use_res_connect:\n            src = node_pos\n        for layer in block.conv.modules():\n            if isinstance(layer, nn.Conv2d):\n                node_pos += 1\n                # merging strided conv is not implemented yet\n                if layer.stride != (1, 1):\n                    if arch == \"mbv2\":\n                        str_pos.add(node_pos + 1)\n        if arch == \"mbv2\" and block.use_res_connect:\n            skip_s2t[src], skip_t2s[node_pos] = node_pos, src\n    return skip_s2t, skip_t2s, str_pos", "\n\ndef get_skip_bumps(act_pos, skip_s2t):\n    ind, acts = 0, sorted(list(act_pos))\n    bumps, bumps_s2t, l = set(), dict(), len(acts)\n    if l == 0:\n        return bumps, bumps_s2t\n    for src, tgt in skip_s2t.items():\n        while acts[ind] < tgt:\n            if acts[ind] > src:\n                bumps.update([src, tgt])\n                bumps_s2t[src] = tgt\n                break\n            ind += 1\n            if ind >= l:\n                return bumps, bumps_s2t\n    return bumps, bumps_s2t", "\n\ndef adjust_padding(blocks, bumps):\n    node_pos, pad, starting_layer = 0, 0, None\n    for block in blocks:\n        for layer in block.conv:\n            if isinstance(layer, nn.Conv2d):\n                if starting_layer == None:\n                    starting_layer = layer\n                if node_pos in bumps and not node_pos == 0:\n                    starting_layer.padding = (pad, pad)\n                    pad = 0\n                    starting_layer = layer\n                node_pos += 1\n                pad += layer.padding[0]\n                layer.padding = (0, 0)\n        starting_layer.padding = (pad, pad)", "\n\ndef adjust_isize(blocks):\n    for ind, block in enumerate(blocks):\n        if ind == 0:\n            cur_isize = block.isize\n        else:\n            block.isize = cur_isize\n        for layer in block.conv:\n            if isinstance(layer, nn.Conv2d):\n                k, pad, st = layer.kernel_size[0], layer.padding[0], layer.stride[0]\n                cur_isize = (cur_isize - k + 2 * pad) // st + 1", "\n\ndef add_nonlinear(blocks, add_pos):\n    node_pos = 0\n    for block in blocks:\n        for layer in block.conv:\n            if isinstance(layer, nn.Conv2d):\n                node_pos += 1\n        if node_pos in add_pos:\n            block.conv.add_module(\"relu3\", nn.ReLU6(inplace=True))", "\n\ndef fuse_bn(module):\n    module.to(\"cpu\")\n    # Input module is merged module\n    prev_lyr = None\n    remove_bns = []\n    for name, lyr in module.named_modules():\n        # Fuse batchnorm layers with previous conv layers\n        if \"md.bn\" in name:\n            assert isinstance(prev_lyr, nn.Conv2d) and isinstance(lyr, nn.BatchNorm2d)\n            cw, bw = unroll_conv_params(\n                prev_lyr, prev_lyr.in_channels == prev_lyr.groups\n            )\n            m_b = torch.zeros(cw.size(0))\n            if bw != None:\n                m_b += bw\n            m_cw, m_b = adjust_with_bn(cw, m_b, lyr)\n\n            new_conv = nn.Conv2d(\n                m_cw.size(1),\n                m_cw.size(0),\n                kernel_size=m_cw.size(2),\n                stride=prev_lyr.stride,\n                padding=prev_lyr.padding,\n                bias=True,\n                groups=prev_lyr.groups,\n            )\n            # depthwise conv\n            if prev_lyr.in_channels == prev_lyr.groups:\n                m_cw = torch.diagonal(m_cw, dim1=0, dim2=1)\n                m_cw = rearrange(m_cw, \"h w (c one) -> c one h w\", one=1)\n\n            new_conv.weight.data = m_cw.clone().detach()\n            new_conv.bias.data = m_b.clone().detach()\n\n            conv_names = prev_name.split(\".\")\n            seq = reduce(getattr, conv_names[:-1], module)\n            setattr(seq, conv_names[-1], new_conv)\n\n            remove_bns += [name]\n        prev_lyr = lyr\n        prev_name = name\n    module.to(\"cuda\")\n\n    for remove_bn in remove_bns:\n        bn_names = remove_bn.split(\".\")\n        seq = reduce(getattr, bn_names[:-1], module)\n        delattr(seq, bn_names[-1])", "\n\ndef simulate_merge(conv1: nn.Conv2d, conv2: nn.Conv2d):\n    assert all(isinstance(x, nn.Conv2d) for x in [conv1, conv2])\n    new_in_channels = conv1.in_channels\n    new_out_channels = conv2.out_channels\n    new_kernel = (conv1.kernel_size[0] // 2 + conv2.kernel_size[0] // 2) * 2 + 1\n    new_stride = conv1.stride[0] * conv2.stride[0]\n    new_pad = conv1.padding[0] + conv2.padding[0]\n    is_dw = all(x.groups == x.in_channels for x in [conv1, conv2])\n    new_conv = nn.Conv2d(\n        new_in_channels,\n        new_out_channels,\n        new_kernel,\n        new_stride,\n        new_pad,\n        groups=conv1.in_channels if is_dw else 1,\n    )\n    return new_conv", "\n\ndef simulate_list_merge(convs: List[nn.Conv2d]):\n    assert all(isinstance(x, nn.Conv2d) for x in convs)\n    new_in_channels = convs[0].in_channels\n    new_out_channels = convs[-1].out_channels\n    new_kernel = convs[0].kernel_size[0]\n    for conv in convs[1:]:\n        new_kernel = (new_kernel // 2 + conv.kernel_size[0] // 2) * 2 + 1\n    new_stride = convs[0].stride[0]\n    for conv in convs[1:]:\n        new_stride = new_stride * conv.stride[0]\n    new_pad = convs[0].padding[0]\n    for conv in convs[1:]:\n        new_pad = new_pad + conv.padding[0]\n    is_dw = all(x.groups == x.in_channels for x in convs)\n    new_conv = nn.Conv2d(\n        new_in_channels,\n        new_out_channels,\n        new_kernel,\n        new_stride,\n        new_pad,\n        groups=convs[0].in_channels if is_dw else 1,\n    )\n    return new_conv", "\n\ndef trace_feat_size(blks, inp):\n    pos, res = 0, dict()\n    out = inp\n    res[pos] = tuple(out.shape)\n    for blk in blks:\n        for lyr in blk.conv:\n            out = lyr(out)\n            if isinstance(lyr, nn.Conv2d):\n                pos += 1\n                res[pos] = tuple(out.shape)\n    return res", "\n\ndef get_act(blocks, block_type):\n    node_pos, act_pos = 0, set()\n    for block in blocks:\n        if isinstance(block, block_type):\n            for layer in block.conv:\n                if isinstance(layer, nn.Conv2d):\n                    node_pos += 1\n                elif isinstance(layer, DepShrinkReLU6):\n                    if layer.act_hat:\n                        act_pos.add(node_pos)\n                elif isinstance(layer, (nn.ReLU, nn.ReLU6)):\n                    act_pos.add(node_pos)\n    act_num = node_pos\n    return act_pos, act_num", "\n\n# Reset the layers between `st_pos` and `end_pos`\ndef reset_layers(blocks, block_type, st_pos, end_pos, logger=None):\n    node_pos = 0\n    for block in blocks:\n        if isinstance(block, block_type):\n            for layer in block.conv:\n                if isinstance(layer, (nn.Conv2d, nn.BatchNorm2d)):\n                    if isinstance(layer, nn.Conv2d):\n                        node_pos += 1\n                    if st_pos < node_pos and node_pos <= end_pos:\n                        if logger:\n                            logger.comment(str(layer))\n                        layer.reset_parameters()", "\n\ndef get_blk(blocks, block_type):\n    node_pos, blk_pos = 0, set()\n    for block in blocks:\n        if isinstance(block, block_type):\n            for _, (_, layer) in enumerate(block.conv._modules.items()):\n                if isinstance(layer, nn.Conv2d):\n                    node_pos += 1\n        blk_pos.add(node_pos)\n    return blk_pos", "\n\ndef get_conv_lst(blocks, block_type, st, end):\n    node_pos, lst = 0, []\n    assert st < end\n    for block in blocks:\n        if isinstance(block, block_type):\n            for layer in block.conv:\n                if isinstance(layer, nn.Conv2d):\n                    if node_pos >= st and node_pos < end:\n                        lst.append(layer)\n                    if node_pos + 1 == end:\n                        return lst\n                    node_pos += 1", "\n\ndef fix_act_lyrs(blocks, block_type, act_pos, act_type=\"relu6\"):\n    assert act_type in [\"relu6\", \"relu\"]\n    node_pos = 0\n    for block in blocks:\n        if isinstance(block, block_type):\n            for _, (name, layer) in enumerate(block.conv._modules.items()):\n                if isinstance(layer, nn.Conv2d):\n                    node_pos += 1\n                elif isinstance(layer, (nn.ReLU, nn.ReLU6, nn.Identity)):\n                    if node_pos in act_pos:\n                        if act_type == \"relu6\":\n                            setattr(block.conv, name, nn.ReLU6(inplace=True))\n                        elif act_type == \"relu\":\n                            setattr(block.conv, name, nn.ReLU(inplace=True))\n                        else:\n                            raise NotImplementedError(\"Not right activation\")\n                    else:\n                        setattr(block.conv, name, nn.Identity())", "\n\ndef valid_blks(model):\n    skip_s2t = model.skip_s2t\n    str_pos = sorted(list(model.str_pos))\n    act_num = model.get_act_info()[1]\n    breaks = str_pos + [act_num]\n    phase = 0\n    b_pos = breaks[phase]\n    skip_pos = b_pos\n    blks = []\n    for st in range(0, act_num):\n        if st == b_pos:\n            if b_pos == breaks[phase]:\n                phase += 1\n            b_pos = breaks[phase]\n        if st == skip_pos:\n            skip_pos = b_pos\n        end = st + 1\n        while end <= min(b_pos, skip_pos):\n            blks.append((st, end))\n            if end in skip_s2t:\n                end = skip_s2t[end]\n            else:\n                end += 1\n        if st in skip_s2t:\n            skip_pos = skip_s2t[st]\n    return blks", "\n\n# necessary for backwards compatibility\nclass _DeprecatedConvBNAct(Conv2dNormActivation):\n    def __init__(self, *args, **kwargs):\n        warnings.warn(\n            \"The ConvBNReLU/ConvBNActivation classes are deprecated since 0.12 and will be removed in 0.14. \"\n            \"Use torchvision.ops.misc.Conv2dNormActivation instead.\",\n            FutureWarning,\n        )\n        if kwargs.get(\"norm_layer\", None) is None:\n            kwargs[\"norm_layer\"] = nn.BatchNorm2d\n        if kwargs.get(\"activation_layer\", None) is None:\n            kwargs[\"activation_layer\"] = nn.ReLU6\n        super().__init__(*args, **kwargs)", "\n\nConvBNReLU = _DeprecatedConvBNAct\nConvBNActivation = _DeprecatedConvBNAct\n\n\nclass DepShrinkReLU6(nn.ReLU6):\n    # Inplace op not supported\n    def __init__(self, inplace=False) -> None:\n        super().__init__()\n        self.act = nn.Parameter(torch.tensor([1.0]), requires_grad=True)\n        # act_hat is either 1 or 0; indicates if activation is alive\n        self.act_hat = 1.0\n\n    def forward(self, x):\n        # straight-forward estimator\n        act = torch.clamp(self.act, 0, 1)\n        act = self.act_hat + act - act.detach()\n        return act * F.relu6(x) + (1 - act) * x\n\n    def __repr__(self):\n        if self.act_hat == 1.0:\n            string = (\n                f\"{Fore.GREEN}DepShrinkReLU6() Enabled {Style.RESET_ALL}\"\n                + f\"[Act : {self.act.data.item():.2e}]\"\n                + f\"[Act Hat : {self.act_hat}]\"\n            )\n        else:\n            string = (\n                f\"{Fore.RED}DepShrinkReLU6() Disabled {Style.RESET_ALL}\"\n                + f\"[Act : {self.act.data.item():.2e}] \"\n                + f\"[Act Hat : {self.act_hat}]\"\n            )\n        return string", ""]}
{"filename": "models/imagenet/mobilenetv2.py", "chunked_list": ["import os\nimport sys\n\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\nimport torch\nimport torch.nn.functional as F\nfrom functools import partial\nfrom typing import Any, Optional, List, Tuple\n", "from typing import Any, Optional, List, Tuple\n\nfrom torch import Tensor\nfrom torch import nn\nfrom collections import OrderedDict\n\nfrom torchvision.ops.misc import Conv2dNormActivation\nfrom torchvision.transforms._presets import ImageClassification\nfrom torchvision.utils import _log_api_usage_once\nfrom torchvision.models._api import WeightsEnum, Weights", "from torchvision.utils import _log_api_usage_once\nfrom torchvision.models._api import WeightsEnum, Weights\nfrom torchvision.models._meta import _IMAGENET_CATEGORIES\nfrom torchvision.models._utils import (\n    handle_legacy_interface,\n    _ovewrite_named_param,\n    _make_divisible,\n)\n\nfrom models.modules_trt import SkipFeed, NaiveFeed", "\nfrom models.modules_trt import SkipFeed, NaiveFeed\nfrom models.model_op import fuse_bn\nfrom utils.measure import compile_and_time, torch_time, unroll_merged, torch_cpu_time\nfrom fvcore.nn import FlopCountAnalysis\n\n__all__ = [\"InvertedResidual\", \"MobileNetV2\", \"MobileNet_V2_Weights\", \"mobilenet_v2\"]\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(\n        self,\n        inp: int,\n        oup: int,\n        stride: int,\n        expand_ratio: int,\n        norm_layer=nn.BatchNorm2d,\n        activation_layer=partial(nn.ReLU6, inplace=True),\n        last_relu=False,\n    ) -> None:\n        super().__init__()\n        self.stride = stride\n        self.activation_layer = activation_layer\n        act_layers = self.make_act_layers(3 if last_relu else 2)\n        if stride not in [1, 2]:\n            raise ValueError(f\"stride should be 1 or 2 insted of {stride}\")\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers: List[Tuple[str, torch.nn.Module]] = []\n\n        ind = 1\n        if expand_ratio != 1:\n            layers += [\n                (f\"conv{ind}\", nn.Conv2d(inp, hidden_dim, kernel_size=1, bias=False)),\n                (f\"bn{ind}\", norm_layer(hidden_dim)),\n                (f\"relu{ind}\", act_layers[0]),\n            ]\n            ind += 1\n\n        layers += [\n            (\n                f\"conv{ind}\",\n                nn.Conv2d(\n                    hidden_dim,\n                    hidden_dim,\n                    kernel_size=3,\n                    stride=self.stride,\n                    padding=1,\n                    groups=hidden_dim,\n                    bias=False,\n                ),\n            ),\n            (f\"bn{ind}\", norm_layer(hidden_dim)),\n            (f\"relu{ind}\", act_layers[1]),\n            (f\"conv{ind+1}\", nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False)),\n            (f\"bn{ind+1}\", norm_layer(oup)),\n        ]\n\n        if last_relu:\n            layers += [(f\"relu{ind+1}\", act_layers[2])]\n\n        # Each indicating if i-th act is disappeared\n        self.is_merged = False\n        self.m_layers: List[nn.Module] = []\n        self.m_seq: nn.Sequential = None\n        self.conv = nn.Sequential(OrderedDict(layers))\n        self.in_channels = inp\n        self.out_channels = oup\n        self._is_cn = stride > 1\n\n    def make_act_layers(self, num=2):\n        return [self.activation_layer() for _ in range(num)]\n\n    def forward(self, x: Tensor) -> Tensor:\n        if self.is_merged:\n            out = self.m_seq(x)\n        else:\n            pre_skip = len(self.conv)\n            if isinstance(self.conv[-1], nn.ReLU6):\n                pre_skip -= 1\n            out = self.conv[:pre_skip](x)\n\n            if self.use_res_connect:\n                # In fine-tuning stage, the size might not match\n                diff = (out.shape[-1] - x.shape[-1]) // 2\n                x = F.pad(x, [diff] * 4)\n                out = out + x\n\n            out = self.conv[pre_skip:](out)\n        return out", "\nclass InvertedResidual(nn.Module):\n    def __init__(\n        self,\n        inp: int,\n        oup: int,\n        stride: int,\n        expand_ratio: int,\n        norm_layer=nn.BatchNorm2d,\n        activation_layer=partial(nn.ReLU6, inplace=True),\n        last_relu=False,\n    ) -> None:\n        super().__init__()\n        self.stride = stride\n        self.activation_layer = activation_layer\n        act_layers = self.make_act_layers(3 if last_relu else 2)\n        if stride not in [1, 2]:\n            raise ValueError(f\"stride should be 1 or 2 insted of {stride}\")\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers: List[Tuple[str, torch.nn.Module]] = []\n\n        ind = 1\n        if expand_ratio != 1:\n            layers += [\n                (f\"conv{ind}\", nn.Conv2d(inp, hidden_dim, kernel_size=1, bias=False)),\n                (f\"bn{ind}\", norm_layer(hidden_dim)),\n                (f\"relu{ind}\", act_layers[0]),\n            ]\n            ind += 1\n\n        layers += [\n            (\n                f\"conv{ind}\",\n                nn.Conv2d(\n                    hidden_dim,\n                    hidden_dim,\n                    kernel_size=3,\n                    stride=self.stride,\n                    padding=1,\n                    groups=hidden_dim,\n                    bias=False,\n                ),\n            ),\n            (f\"bn{ind}\", norm_layer(hidden_dim)),\n            (f\"relu{ind}\", act_layers[1]),\n            (f\"conv{ind+1}\", nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False)),\n            (f\"bn{ind+1}\", norm_layer(oup)),\n        ]\n\n        if last_relu:\n            layers += [(f\"relu{ind+1}\", act_layers[2])]\n\n        # Each indicating if i-th act is disappeared\n        self.is_merged = False\n        self.m_layers: List[nn.Module] = []\n        self.m_seq: nn.Sequential = None\n        self.conv = nn.Sequential(OrderedDict(layers))\n        self.in_channels = inp\n        self.out_channels = oup\n        self._is_cn = stride > 1\n\n    def make_act_layers(self, num=2):\n        return [self.activation_layer() for _ in range(num)]\n\n    def forward(self, x: Tensor) -> Tensor:\n        if self.is_merged:\n            out = self.m_seq(x)\n        else:\n            pre_skip = len(self.conv)\n            if isinstance(self.conv[-1], nn.ReLU6):\n                pre_skip -= 1\n            out = self.conv[:pre_skip](x)\n\n            if self.use_res_connect:\n                # In fine-tuning stage, the size might not match\n                diff = (out.shape[-1] - x.shape[-1]) // 2\n                x = F.pad(x, [diff] * 4)\n                out = out + x\n\n            out = self.conv[pre_skip:](out)\n        return out", "\n\nclass MobileNetV2(nn.Module):\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        width_mult: float = 1.0,\n        inverted_residual_setting: Optional[List[List[int]]] = None,\n        round_nearest: int = 8,\n        block=InvertedResidual,\n        norm_layer=nn.BatchNorm2d,\n        dropout: float = 0.2,\n        add_relu: bool = False,\n    ) -> None:\n        \"\"\"\n        MobileNet V2 main class\n\n        Args:\n            num_classes (int): Number of classes\n            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n            inverted_residual_setting: Network structure\n            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n            Set to 1 to turn off rounding\n            block: Module specifying inverted residual building block for mobilenet\n            norm_layer: Module specifying the normalization layer to use\n            dropout (float): The droupout probability\n        \"\"\"\n        super().__init__()\n        _log_api_usage_once(self)\n\n        input_channel = 32\n        last_channel = 1280\n\n        if inverted_residual_setting is None:\n            inverted_residual_setting = [\n                # t, c, n, s, isize\n                [1, 16, 1, 1, 112],\n                [6, 24, 2, 2, 112],\n                [6, 32, 3, 2, 56],\n                [6, 64, 4, 2, 28],\n                [6, 96, 3, 1, 14],\n                [6, 160, 3, 2, 14],\n                [6, 320, 1, 1, 7],\n            ]\n\n        # only check the first element, assuming user knows t,c,n,s are required\n        if (\n            len(inverted_residual_setting) == 0\n            or len(inverted_residual_setting[0]) != 5\n        ):\n            raise ValueError(\n                f\"inverted_residual_setting should be non-empty or a 5-element list, got {inverted_residual_setting}\"\n            )\n        self.add_relu = add_relu\n        # building first layer\n        self.norm_layer = norm_layer\n        self.block = block\n        self.width_mult = width_mult\n        self.round_nearest = round_nearest\n        self.dropout = dropout\n        self.num_classes = num_classes\n\n        self.build(input_channel, last_channel, inverted_residual_setting)\n        self.initialize()\n\n        self.is_merged = False\n        self.is_fixed_act = False\n        self.m_blocks: List[nn.Module] = []\n        self.m_features = None  # nn.Sequential\n        self.compress_k: int = 0\n\n    def make_first_feature(self, input_channel):\n        return Conv2dNormActivation(\n            3,\n            input_channel,\n            stride=2,\n            norm_layer=self.norm_layer,\n            activation_layer=partial(nn.ReLU6, inplace=True),\n        )\n\n    def make_feature(self, input_channel, output_channel, stride, t):\n        return self.block(\n            input_channel,\n            output_channel,\n            stride,\n            expand_ratio=t,\n            norm_layer=self.norm_layer,\n            last_relu=self.add_relu,\n        )\n\n    def make_last_feature(self, input_channel):\n        return Conv2dNormActivation(\n            input_channel,\n            self.last_channel,\n            kernel_size=1,\n            norm_layer=self.norm_layer,\n            activation_layer=partial(nn.ReLU6, inplace=True),\n        )\n\n    def set_classifier(self):\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=self.dropout),\n            nn.Linear(self.last_channel, self.num_classes),\n        )\n\n    def initialize(self):\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def build(self, input_channel, last_channel, inverted_residual_setting):\n        input_channel = _make_divisible(\n            input_channel * self.width_mult, self.round_nearest\n        )\n        self.last_channel = _make_divisible(\n            last_channel * max(1.0, self.width_mult), self.round_nearest\n        )\n\n        features: List[nn.Module] = []\n        features.append(self.make_first_feature(input_channel))\n        # building inverted residual blocks\n        for t, c, n, s, isize in inverted_residual_setting:\n            output_channel = _make_divisible(c * self.width_mult, self.round_nearest)\n            self.cur_isize = isize\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(\n                    self.make_feature(input_channel, output_channel, stride, t)\n                )\n                self.cur_isize = self.cur_isize // stride\n                input_channel = output_channel\n        features.append(self.make_last_feature(input_channel))\n        self.features = nn.Sequential(*features)\n        self.set_classifier()\n\n    def set_act_hats(self):\n        pass\n\n    def _forward_impl(self, x: Tensor) -> Tensor:\n        if self.is_merged:\n            x = self.m_features(x)\n        else:\n            if not self.is_fixed_act:\n                self.set_act_hats()\n            x = self.features(x)\n        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self._forward_impl(x)\n\n    # Does not merge anything and fuse batchnorm layer\n    def merge(self):\n        self.is_merged = True\n        stack = []\n        for _, block in enumerate(self.features):\n            if isinstance(block, InvertedResidual):\n                if block.use_res_connect:\n                    stack.append(SkipFeed(block.conv._modules))\n                else:\n                    stack.append(NaiveFeed(block.conv._modules))\n            else:\n                stack.append(block)\n\n        self.m_features = nn.Sequential(*stack)\n        fuse_bn(self.m_features)\n        print(\"Fused batchnorm...\")\n        delattr(self, \"features\")\n\n    def time(self, txt=\"model\", verb=False, trt=True):\n        assert self.is_merged\n        unrolled_module = unroll_merged(self)\n        print(unrolled_module)\n\n        if trt:\n            print(\"Start compiling merged model...\")\n            result, std = compile_and_time(\n                unrolled_module, (128, 3, 224, 224), txt, verb\n            )\n        else:\n            result, std = torch_time(unrolled_module, (128, 3, 224, 224), txt, verb)\n        del unrolled_module\n        return result, std\n\n    def flops(self):\n        assert self.is_merged\n        unrolled_module = unroll_merged(self)\n        unrolled_module.eval()\n\n        print(unrolled_module)\n        inputs = torch.randn((1, 3, 224, 224)).cuda()\n        flops = FlopCountAnalysis(unrolled_module, inputs)\n        print(f\"number of MFLOPs: {flops.total() / 1e6}\")\n\n        del unrolled_module\n        return flops.total()\n\n    def mem(self):\n        assert self.is_merged\n        mem = torch.cuda.max_memory_allocated()\n        print(f\"Before : {mem / 1e6:>15} MB\")\n\n        unrolled_module = unroll_merged(self)\n        unrolled_module.eval()\n        params = sum(p.numel() for p in unrolled_module.parameters())\n        inputs = torch.randn((128, 3, 224, 224)).cuda()\n\n        print(unrolled_module)\n        for i in range(10):\n            torch.cuda.reset_peak_memory_stats()\n            unrolled_module(inputs)\n\n            mem = torch.cuda.max_memory_allocated()\n            print(f\"Iter {i}  : {mem / 1e6:>15} MB\")\n\n        return params, mem\n\n    def cpu_time(self, txt=\"model\", verb=False):\n        assert self.is_merged\n        unrolled_module = unroll_merged(self)\n\n        print(unrolled_module)\n\n        result, std = torch_cpu_time(unrolled_module, (128, 3, 224, 224), txt, verb)\n        del unrolled_module\n        return result, std", "\n\n_COMMON_META = {\n    \"num_params\": 3504872,\n    \"min_size\": (1, 1),\n    \"categories\": _IMAGENET_CATEGORIES,\n}\n\n\nclass MobileNet_V2_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\",\n        transforms=partial(ImageClassification, crop_size=224),\n        meta={\n            **_COMMON_META,\n            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv2\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 71.878,\n                    \"acc@5\": 90.286,\n                }\n            },\n            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n        },\n    )\n    IMAGENET1K_V2 = Weights(\n        url=\"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\",\n        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n        meta={\n            **_COMMON_META,\n            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 72.154,\n                    \"acc@5\": 90.822,\n                }\n            },\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using a modified version of TorchVision's\n                `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V2", "\nclass MobileNet_V2_Weights(WeightsEnum):\n    IMAGENET1K_V1 = Weights(\n        url=\"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\",\n        transforms=partial(ImageClassification, crop_size=224),\n        meta={\n            **_COMMON_META,\n            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#mobilenetv2\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 71.878,\n                    \"acc@5\": 90.286,\n                }\n            },\n            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n        },\n    )\n    IMAGENET1K_V2 = Weights(\n        url=\"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\",\n        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n        meta={\n            **_COMMON_META,\n            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#new-recipe-with-reg-tuning\",\n            \"_metrics\": {\n                \"ImageNet-1K\": {\n                    \"acc@1\": 72.154,\n                    \"acc@5\": 90.822,\n                }\n            },\n            \"_docs\": \"\"\"\n                These weights improve upon the results of the original paper by using a modified version of TorchVision's\n                `new training recipe\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n            \"\"\",\n        },\n    )\n    DEFAULT = IMAGENET1K_V2", "\n\n@handle_legacy_interface(weights=(\"pretrained\", MobileNet_V2_Weights.IMAGENET1K_V1))\ndef mobilenet_v2(\n    *,\n    weights: Optional[MobileNet_V2_Weights] = None,\n    progress: bool = True,\n    **kwargs: Any,\n) -> MobileNetV2:\n    \"\"\"MobileNetV2 architecture from the `MobileNetV2: Inverted Residuals and Linear\n    Bottlenecks <https://arxiv.org/abs/1801.04381>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.MobileNet_V2_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.MobileNet_V2_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.mobilenetv2.MobileNetV2``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.MobileNet_V2_Weights\n        :members:\n    \"\"\"\n    weights = MobileNet_V2_Weights.verify(weights)\n\n    if weights is not None:\n        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n\n    model = MobileNetV2(**kwargs)\n\n    if weights is not None:\n        model.load_state_dict(weights.get_state_dict(progress=progress))\n\n    return model", "\n\n# The dictionary below is internal implementation detail and will be removed in v0.15\nfrom torchvision.models._utils import _ModelURLs\n\nmodel_urls = _ModelURLs(\n    {\n        \"mobilenet_v2\": MobileNet_V2_Weights.IMAGENET1K_V1.url,\n    }\n)", "    }\n)\n"]}
{"filename": "models/imagenet/mobilenetv2_com.py", "chunked_list": ["import os\nimport sys\n\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\nfrom typing import Any, Optional, List\nfrom collections import OrderedDict\nfrom functools import partial\nfrom models.model_op import (\n    get_skip_info,", "from models.model_op import (\n    get_skip_info,\n    get_skip_bumps,\n    adjust_padding,\n    adjust_isize,\n    merge_or_new,\n    push_merged_layers,\n    fuse_skip,\n    trace_feat_size,\n    get_act,", "    trace_feat_size,\n    get_act,\n    get_blk,\n    fix_act_lyrs,\n)\nfrom models.imagenet.mobilenetv2 import InvertedResidual, MobileNetV2\nfrom models.modules_trt import NaiveFeed, SkipFeed\n\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision.models._meta import _IMAGENET_CATEGORIES\n\n__all__ = [\"LearnMobileNetV2\", \"learn_mobilenet_v2\"]\n\n\ndef get_merged_list(module):\n    merged = []\n    if isinstance(module, LearnInvertedResidual):\n        for _, act_ind in enumerate(range(2, len(module.conv), 3)):\n            act_lyr = module.conv[act_ind]\n            if isinstance(act_lyr, (nn.Identity, nn.ReLU6)):\n                merged += [isinstance(act_lyr, nn.Identity)]\n    else:\n        raise NotImplementedError()\n    return merged", "def get_merged_list(module):\n    merged = []\n    if isinstance(module, LearnInvertedResidual):\n        for _, act_ind in enumerate(range(2, len(module.conv), 3)):\n            act_lyr = module.conv[act_ind]\n            if isinstance(act_lyr, (nn.Identity, nn.ReLU6)):\n                merged += [isinstance(act_lyr, nn.Identity)]\n    else:\n        raise NotImplementedError()\n    return merged", "\n\nclass LearnInvertedResidual(InvertedResidual):\n    def __init__(\n        self,\n        inp: int,\n        oup: int,\n        stride: int,\n        expand_ratio: int,\n        isize: int,\n        norm_layer=nn.BatchNorm2d,\n        last_relu=False,\n    ) -> None:\n        super().__init__(\n            inp=inp,\n            oup=oup,\n            stride=stride,\n            expand_ratio=expand_ratio,\n            norm_layer=norm_layer,\n            activation_layer=nn.ReLU6,\n            last_relu=last_relu,\n        )\n        self.isize = isize\n        self.merged = [False] * (len(self.conv) // 3)\n\n    def merge(self):\n        if self.is_merged:\n            print(\"Model already Merged\")\n            return\n        self.is_merged = True\n\n        # weights = (isize, m_cw, m_p, m_b), cparam = (ctype, cstr)\n        weights, cparam = (self.isize, None, None, None), (\"dw\", 1)\n        pos = 0\n\n        # Bring merged list from act_lyr\n        self.merged = get_merged_list(self)\n\n        for ind in range(0, len(self.conv), 3):\n\n            is_merged = False if ind == 0 else self.merged[(ind - 2) // 3]\n            is_last = ind >= len(self.conv) - 3\n\n            relu = not (ind == len(self.conv) - 2)\n            if (ind == len(self.conv) - 3) and self.merged[-1]:\n                relu = False\n\n            conv, bn = self.conv[ind], self.conv[ind + 1]\n            lyrs = (conv, bn)\n\n            pos, weights, cparam = merge_or_new(\n                self.m_layers, pos, weights, lyrs, is_merged, cparam\n            )\n            if all(self.merged + [is_last, self.use_res_connect]):\n                fuse_skip(weights[1])\n                self.use_res_connect = False\n\n            push_merged_layers(self.m_layers, pos, weights, cparam, relu=relu)\n\n        delattr(self, \"conv\")\n\n        if not self.use_res_connect:\n            self.m_seq = NaiveFeed(OrderedDict(self.m_layers))\n        elif relu:\n            self.m_seq = SkipFeed(OrderedDict(self.m_layers[:-1]), nn.ReLU6)\n        elif not relu:\n            self.m_seq = SkipFeed(OrderedDict(self.m_layers))\n\n    def fix_act(self):\n        self.conv.conv1.padding = (1, 1)\n        self.conv.conv2.padding = (0, 0)\n        if hasattr(self.conv, \"conv3\"):\n            self.conv.conv3.padding = (0, 0)\n\n    def extra_repr(self):\n        r\"\"\"Set the extra representation of the module\n\n        To print customized extra information, you should re-implement\n        this method in your own modules. Both single-line and multi-line\n        strings are acceptable.\n        \"\"\"\n        if self.use_res_connect:\n            out = f\"[Skip : Enabled]  [isize : {self.isize}]\"\n        else:\n            out = f\"[Skip : Disabled] [isize : {self.isize}]\"\n        return out", "\n\nclass LearnMobileNetV2(MobileNetV2):\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        width_mult: float = 1.0,\n        inverted_residual_setting: Optional[List[List[int]]] = None,\n        round_nearest: int = 8,\n        block=LearnInvertedResidual,\n        norm_layer=nn.BatchNorm2d,\n        dropout: float = 0.2,\n        add_relu: bool = False,\n    ) -> None:\n\n        super().__init__(\n            num_classes,\n            width_mult,\n            inverted_residual_setting,\n            round_nearest,\n            block,\n            norm_layer,\n            dropout,\n            add_relu,\n        )\n\n        self.skip_s2t, self.skip_t2s, self.str_pos = get_skip_info(self.features[1:-1])\n        self.skip_bump, self.act_pos = set(), set()\n\n        inp = torch.randn(128, 3, 224, 224)\n        out = self.features[0](inp)\n        self.out_shape = trace_feat_size(self.features[1:-1], out)\n\n    def make_feature(self, input_channel, output_channel, stride, t):\n        return self.block(\n            input_channel,\n            output_channel,\n            stride,\n            expand_ratio=t,\n            isize=self.cur_isize,\n            norm_layer=self.norm_layer,\n            last_relu=self.add_relu,\n        )\n\n    def get_act_info(self):\n        act_pos, act_num = get_act(self.features[1:-1], InvertedResidual)\n        return act_pos, act_num\n\n    def get_blk_info(self):\n        blk_end = sorted(get_blk(self.features[1:-1], InvertedResidual))\n        blks = dict([(ind + 1, end) for ind, end in enumerate(blk_end)])\n        blk_num = len(blks)\n        return blks, blk_num\n\n    def fix_act(self, act_pos=None, merge_pos=None):\n        self.is_fixed_act = True\n\n        # position in nodes and bumps denote the number of come-acrossed conv\n        self.act_pos = act_pos if act_pos != None else self.get_act_info()[0]\n        self.merge_pos = merge_pos if merge_pos != None else self.act_pos\n        assert self.act_pos.issubset(self.merge_pos)\n\n        self.skip_bump, _ = get_skip_bumps(self.merge_pos, self.skip_s2t)\n        bumps = set.union(self.str_pos, self.merge_pos, self.skip_bump)\n\n        fix_act_lyrs(self.features[1:-1], InvertedResidual, self.act_pos)\n        # adjust padding and input size among the conv layers w.r.t. bumps\n        adjust_padding(self.features[1:-1], bumps)\n        adjust_isize(self.features[1:-1])\n\n    def merge(self, act_pos=None, merge_pos=None, keep_feat=False):\n        self.to(\"cpu\")\n        self.is_merged = True\n\n        self.act_pos = act_pos if act_pos != None else self.get_act_info()[0]\n        self.merge_pos = merge_pos if merge_pos != None else self.act_pos\n        assert self.act_pos.issubset(self.merge_pos)\n\n        self.skip_bump, skip_bump_s2t = get_skip_bumps(self.merge_pos, self.skip_s2t)\n        bumps = set.union(self.str_pos, self.merge_pos, self.skip_bump)\n\n        self.m_layers, stack = [], []\n        pos, bump_end, loc_end, last = 0, None, None, None\n\n        # weights = (isize, m_cw, m_p, m_b), cparam = (ctype, cstr)\n        weights, cparam = (112, None, None, None), (\"dw\", 1)\n\n        for block in self.features[1:-1]:\n            for ind in range(0, len(block.conv), 3):\n\n                if pos in skip_bump_s2t:\n                    if len(stack) > 0:\n                        self.m_layers += [NaiveFeed(OrderedDict(stack))]\n                        stack = []\n                    bump_end = skip_bump_s2t[pos]\n\n                relu = not (ind == len(block.conv) - 2)\n                if not pos + 1 in self.act_pos:\n                    relu = False\n\n                conv, bn = block.conv[ind], block.conv[ind + 1]\n                lyrs = (conv, bn)\n\n                is_merged = pos > 0 and not pos in bumps\n\n                if pos in self.skip_s2t and not pos in skip_bump_s2t:\n                    loc_end = self.skip_s2t[pos]\n                    saved = (stack, weights, cparam, last, is_merged)\n                    weights, cparam = (weights[0], None, None, None), (\"dw\", 1)\n                    stack, last, is_merged = [], None, False\n\n                if pos + 1 == loc_end:\n                    _, weights, cparam = merge_or_new(\n                        stack, pos, weights, lyrs, is_merged, cparam, last\n                    )\n                    fuse_skip(weights[1])\n                    push_merged_layers(stack, pos, weights, cparam, relu=relu)\n\n                    # Doesn't have bn (already fused), thus give `None` in the place of `bn`\n                    lyrs = stack[0][1], None\n                    relu = len(stack) > 1\n                    stack, weights, cparam, last, is_merged = saved\n\n                _, weights, cparam = merge_or_new(\n                    stack, pos, weights, lyrs, is_merged, cparam, last\n                )\n                push_merged_layers(stack, pos, weights, cparam, relu=relu)\n\n                if pos + 1 == bump_end:\n                    if relu:\n                        layer = partial(nn.ReLU6, inplace=True)\n                        skipfeed = SkipFeed(OrderedDict(stack[:-1]), layer)\n                    else:\n                        skipfeed = SkipFeed(OrderedDict(stack))\n                    self.m_layers += [skipfeed]\n                    stack = []\n\n                pos += 1\n                last = 2 if relu else 1\n\n        if len(stack) > 0:\n            self.m_layers += [NaiveFeed(OrderedDict(stack))]\n        self.m_layers = [self.features[0]] + self.m_layers + [self.features[-1]]\n        self.m_features = nn.Sequential(*self.m_layers)\n\n        if not keep_feat:\n            delattr(self, \"features\")\n        self.to(\"cuda\")\n\n    def unmerge(self):\n        assert hasattr(self, \"features\")\n        if self.is_merged:\n            self.is_merged = False\n            delattr(self, \"m_features\")", "\n\n_COMMON_META = {\n    \"num_params\": 3504872,\n    \"min_size\": (1, 1),\n    \"categories\": _IMAGENET_CATEGORIES,\n}\n\n\ndef learn_mobilenet_v2(\n    **kwargs: Any,\n) -> LearnMobileNetV2:\n    \"\"\"MobileNetV2 architecture from the `MobileNetV2: Inverted Residuals and Linear\n    Bottlenecks <https://arxiv.org/abs/1801.04381>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.MobileNet_V2_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.MobileNet_V2_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.mobilenetv2.MobileNetV2``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.MobileNet_V2_Weights\n        :members:\n    \"\"\"\n    model = LearnMobileNetV2(**kwargs)\n    return model", "\ndef learn_mobilenet_v2(\n    **kwargs: Any,\n) -> LearnMobileNetV2:\n    \"\"\"MobileNetV2 architecture from the `MobileNetV2: Inverted Residuals and Linear\n    Bottlenecks <https://arxiv.org/abs/1801.04381>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.MobileNet_V2_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.MobileNet_V2_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.mobilenetv2.MobileNetV2``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.MobileNet_V2_Weights\n        :members:\n    \"\"\"\n    model = LearnMobileNetV2(**kwargs)\n    return model", ""]}
{"filename": "models/imagenet/__init__.py", "chunked_list": ["from .mobilenetv2 import *\nfrom .mobilenetv2_com import *\nfrom .mobilenetv2_ds import *\nfrom .vgg import *\nfrom .vgg_com import *\n\nmodels = {\n    \"mobilenet_v2\": mobilenet_v2,\n    \"learn_mobilenet_v2\": learn_mobilenet_v2,\n    \"dep_shrink_mobilenet_v2\": dep_shrink_mobilenet_v2,", "    \"learn_mobilenet_v2\": learn_mobilenet_v2,\n    \"dep_shrink_mobilenet_v2\": dep_shrink_mobilenet_v2,\n    \"vgg19\": vgg19_bn,\n    \"learn_vgg19\": learn_vgg19_bn,\n}\n\nblocks = {\n    \"mobilenet_v2\": InvertedResidual,\n    \"learn_mobilenet_v2\": InvertedResidual,\n    \"dep_shrink_mobilenet_v2\": InvertedResidual,", "    \"learn_mobilenet_v2\": InvertedResidual,\n    \"dep_shrink_mobilenet_v2\": InvertedResidual,\n    \"vgg19\": VGGBlock,\n    \"learn_vgg19\": LearnVGGBlock,\n}\n"]}
{"filename": "models/imagenet/mobilenetv2_ds.py", "chunked_list": ["import os\nimport sys\n\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\nfrom functools import partial\nfrom collections import OrderedDict\nfrom models.imagenet.mobilenetv2 import InvertedResidual, MobileNetV2\n\nfrom typing import Any, Optional, List", "\nfrom typing import Any, Optional, List\nfrom models.model_op import (\n    get_act,\n    get_blk,\n    adjust_padding,\n    fix_act_lyrs,\n    add_nonlinear,\n    merge_or_new,\n    push_merged_layers,", "    merge_or_new,\n    push_merged_layers,\n    fuse_skip,\n    DepShrinkReLU6,\n)\nfrom models.modules_trt import SkipFeed, NaiveFeed\n\nimport torch\nfrom torch import nn\n", "from torch import nn\n\nfrom torchvision.models._meta import _IMAGENET_CATEGORIES\n\n__all__ = [\"DepShrinkMobileNetV2\", \"dep_shrink_mobilenet_v2\"]\n\n\nclass DepShrinkInvertedResidual(InvertedResidual):\n    def __init__(\n        self,\n        inp: int,\n        oup: int,\n        stride: int,\n        expand_ratio: int,\n        isize: int,\n        norm_layer=nn.BatchNorm2d,\n    ) -> None:\n        super().__init__(\n            inp=inp,\n            oup=oup,\n            stride=stride,\n            expand_ratio=expand_ratio,\n            norm_layer=norm_layer,\n            activation_layer=DepShrinkReLU6,\n        )\n        self.isize = isize\n\n    def make_act_layers(self, _):\n        self.act_layer = self.activation_layer()\n        return self.act_layer, self.act_layer\n\n    def set_act_hat(self, val):\n        self.act_layer.act_hat = val\n\n    def get_act_hat(self):\n        return self.act_layer.act_hat\n\n    def merge(self, merged):\n        if self.is_merged:\n            print(\"Model already Merged\")\n            return\n        self.is_merged = True\n\n        # weights = (isize, m_cw, m_p, m_b), cparam = (ctype, cstr)\n        weights, cparam = (self.isize, None, None, None), (\"dw\", 1)\n        pos = 0\n\n        last_relu = isinstance(self.conv[-1], nn.ReLU6)\n        for ind in range(0, len(self.conv), 3):\n\n            is_merged = False if ind == 0 else merged\n            is_last = ind >= len(self.conv) - 3\n\n            relu = not (ind == len(self.conv) - 2)\n\n            conv, bn = self.conv[ind], self.conv[ind + 1]\n            lyrs = (conv, bn)\n\n            pos, weights, cparam = merge_or_new(\n                self.m_layers, pos, weights, lyrs, is_merged, cparam\n            )\n            if all([merged, is_last, self.use_res_connect]):\n                fuse_skip(weights[1])\n                self.use_res_connect = False\n\n            push_merged_layers(self.m_layers, pos, weights, cparam, relu=relu)\n\n        delattr(self, \"conv\")\n\n        if self.use_res_connect and last_relu:\n            layer = partial(nn.ReLU6, inplace=True)\n            self.m_seq = SkipFeed(OrderedDict(self.m_layers[:-1]), layer)\n        elif self.use_res_connect and not last_relu:\n            self.m_seq = SkipFeed(OrderedDict(self.m_layers))\n        else:\n            self.m_seq = NaiveFeed(OrderedDict(self.m_layers))\n\n    def extra_repr(self):\n        r\"\"\"Set the extra representation of the module\n\n        To print customized extra information, you should re-implement\n        this method in your own modules. Both single-line and multi-line\n        strings are acceptable.\n        \"\"\"\n        if self.use_res_connect:\n            out = f\"[Skip : Enabled]\"\n        else:\n            out = f\"[Skip : Disabled]\"\n        return out", "\n\nclass DepShrinkMobileNetV2(MobileNetV2):\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        width_mult: float = 1.0,\n        inverted_residual_setting: Optional[List[List[int]]] = None,\n        round_nearest: int = 8,\n        block=DepShrinkInvertedResidual,\n        norm_layer=nn.BatchNorm2d,\n        dropout: float = 0.2,\n    ) -> None:\n        super().__init__(\n            num_classes,\n            width_mult,\n            inverted_residual_setting,\n            round_nearest,\n            block,\n            norm_layer,\n            dropout,\n        )\n\n        self.compress_k = -1\n        self.act_pos = set()\n\n    def make_feature(self, input_channel, output_channel, stride, t):\n        return self.block(\n            input_channel,\n            output_channel,\n            stride,\n            expand_ratio=t,\n            isize=self.cur_isize,\n            norm_layer=self.norm_layer,\n        )\n\n    def load_pattern(self, pat):\n        if pat == \"none\":\n            return\n        elif pat == \"A\":\n            lst = [0, 3, 4, 6, 8, 10, 11, 13, 14, 15, 16]\n        elif pat == \"B\":\n            lst = [3, 4, 10, 11, 13, 14, 15, 16]\n        elif pat == \"C\":\n            lst = [0, 3, 10, 11, 13, 14, 15, 16]\n        elif pat == \"D\":\n            lst = [3, 4, 13, 14, 15, 16]\n        elif pat == \"E\":\n            lst = [0, 3, 13, 14, 15, 16]\n        elif pat == \"F\":\n            lst = []\n        elif pat == \"A10\":\n            lst = [2, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16]\n        elif pat == \"B10\":\n            lst = [0, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 16]\n        elif pat == \"C10\":\n            lst = [0, 3, 4, 6, 10, 13, 14, 15, 16]\n        elif pat == \"D10\":\n            lst = [0, 3, 10, 11, 13, 15, 16]\n        elif pat == \"AR\":\n            lst = [5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n        elif pat == \"BR\":\n            lst = [4, 10, 11, 12, 13, 14, 15, 16]\n        elif pat == \"CR\":\n            lst = [8, 9, 10, 11, 12, 16]\n        elif pat == \"AR10\":\n            lst = [4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n        elif pat == \"BR10\":\n            lst = [4, 7, 8, 9, 12, 13, 14, 15, 16]\n        elif pat == \"CR10\":\n            lst = [8, 10, 12, 13, 14, 15, 16]\n        elif pat == \"AR_AUG\":\n            lst = [0, 3, 5, 7, 8, 9, 12, 13, 14, 15, 16]\n        elif pat == \"BR_AUG\":\n            lst = [5, 7, 8, 9, 12, 14, 15, 16]\n        elif pat == \"CR_AUG\":\n            lst = [9, 11, 12, 14, 15, 16]\n        elif pat == \"AR10_AUG\":\n            lst = [0, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n        elif pat == \"BR10_AUG\":\n            lst = [5, 7, 8, 9, 12, 13, 14, 15, 16]\n        elif pat == \"CR10_AUG\":\n            lst = [6, 7, 9, 13, 14, 15, 16]\n        else:\n            raise NotImplementedError()\n\n        for ind, block in enumerate(self.features[1:-1]):\n            if ind in lst:\n                block.act_layer.act.data = torch.tensor([1e5])\n                block.act_layer.act.requires_grad = False\n            else:\n                block.act_layer.act.data = torch.tensor([-1e5])\n                block.act_layer.act.requires_grad = False\n\n        print(f\"Loaded pattern {pat}\")\n\n    def set_act_hats(self):\n        assert self.compress_k >= 0\n        lst, k = [], self.compress_k\n        for block in self.features[1:-1]:\n            if isinstance(block, DepShrinkInvertedResidual):\n                lst.append(block.act_layer.act.data)\n            else:\n                lst.append(torch.tensor([float(\"-inf\")]))\n\n        acts = torch.cat(lst)\n        _, act_ind = torch.sort(acts, stable=True)\n\n        # set act_hats for top-k blocks\n        for ind, block in enumerate(self.features[1:-1]):\n            if isinstance(block, DepShrinkInvertedResidual):\n                if k == 0:\n                    block.set_act_hat(0.0)\n                elif ind in act_ind[-k:]:\n                    block.set_act_hat(1.0)\n                else:\n                    block.set_act_hat(0.0)\n\n    def get_act_info(self):\n        self.set_act_hats()\n        act_pos, act_num = get_act(self.features[1:-1], InvertedResidual)\n        # DepthShrinker appends activation to merged blocks\n        blk_pos = get_blk(self.features[1:-1], InvertedResidual)\n        add_pos, blk_empty = set(), True\n        for i in range(act_num + 1):\n            if i in act_pos:\n                blk_empty = False\n            if i in blk_pos:\n                if blk_empty:\n                    add_pos.add(i)\n                else:\n                    blk_empty = True\n        act_pos = set.union(act_pos, add_pos)\n        return act_pos, act_num\n\n    def get_arch_parameters(self):\n        return [w for name, w in self.named_parameters() if \"act\" in name]\n\n    def regularizer(self, mode=\"soft\"):\n        if mode == \"soft\":\n            arch_params = torch.cat(self.get_arch_parameters())\n            loss = torch.sum(arch_params)\n        elif mode == \"w1.4\":\n            # fmt: off\n            lat = torch.tensor([15.19, 133.34, 75.77, 43.8, 30.39, 30.39, 18.81, 15.19, 15.19, 15.19, 17.0, 26.23, 26.23, 13.23, 11.41, 11.41, 14.39])\n            # fmt: on\n            arch_params = torch.cat(self.get_arch_parameters())\n            assert len(lat) == len(arch_params)\n            loss = torch.sum(torch.mul(lat.to(\"cuda\"), arch_params))\n        elif mode == \"w1.0\":\n            # fmt: off\n            lat = torch.tensor([18.15, 85.52, 57.97, 35.78, 20.93, 20.93, 12.51, 11.45, 11.45, 11.45, 11.64, 17.85, 17.85, 9.49, 7.35, 7.35, 8.48])\n            # fmt: on\n            arch_params = torch.cat(self.get_arch_parameters())\n            assert len(lat) == len(arch_params)\n            loss = torch.sum(torch.mul(lat.to(\"cuda\"), arch_params))\n        else:\n            raise NotImplementedError()\n        return loss\n\n    def fix_act(self, act_pos=None):\n        if self.is_fixed_act:\n            print(\"Model already Fixed\")\n            return\n        self.is_fixed_act = True\n\n        # position in nodes and bumps denote the number of come-acrossed conv\n        self.act_pos = act_pos if act_pos != None else self.get_act_info()[0]\n        blk_end = sorted(get_blk(self.features[1:-1], InvertedResidual))\n\n        fix_act_lyrs(self.features[1:-1], InvertedResidual, self.act_pos)\n        add_nonlinear(self.features[1:-1], self.act_pos)\n        adjust_padding(self.features[1:-1], set.union(self.act_pos, blk_end))\n\n        for block in self.features[1:-1]:\n            delattr(block, \"act_layer\")\n\n    def merge(self, act_pos=None):\n        if self.is_merged:\n            print(\"Model already Merged\")\n            return\n        self.is_merged = True\n\n        # position in nodes and bumps denote the number of come-acrossed conv\n        self.act_pos = act_pos if act_pos != None else self.get_act_info()[0]\n\n        node_pos = 0\n        for _, block in enumerate(self.features[1:-1]):\n            for _, layer in block.conv._modules.items():\n                if isinstance(layer, nn.Conv2d):\n                    node_pos += 1\n            self.m_blocks.append(block)\n            is_merged = node_pos in self.act_pos\n            if isinstance(block, DepShrinkInvertedResidual):\n                block.merge(is_merged)\n\n        self.m_blocks = [self.features[0]] + self.m_blocks + [self.features[-1]]\n\n        stack = []\n        for _, block in enumerate(self.m_blocks):\n            if isinstance(block, DepShrinkInvertedResidual):\n                if block.use_res_connect:\n                    stack.append(\n                        SkipFeed(block.m_seq.md._modules, block.m_seq.last.__class__)\n                    )\n                else:\n                    stack.append(NaiveFeed(block.m_seq.md._modules))\n            else:\n                stack.append(block)\n\n        self.m_features = nn.Sequential(*stack)\n\n        delattr(self, \"features\")", "\n\n_COMMON_META = {\n    \"num_params\": 3504872,\n    \"min_size\": (1, 1),\n    \"categories\": _IMAGENET_CATEGORIES,\n}\n\n\ndef dep_shrink_mobilenet_v2(\n    **kwargs: Any,\n) -> DepShrinkMobileNetV2:\n    \"\"\"MobileNetV2 architecture from the `MobileNetV2: Inverted Residuals and Linear\n    Bottlenecks <https://arxiv.org/abs/1801.04381>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.MobileNet_V2_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.MobileNet_V2_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.mobilenetv2.MobileNetV2``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.MobileNet_V2_Weights\n        :members:\n    \"\"\"\n\n    model = DepShrinkMobileNetV2(**kwargs)\n\n    return model", "\ndef dep_shrink_mobilenet_v2(\n    **kwargs: Any,\n) -> DepShrinkMobileNetV2:\n    \"\"\"MobileNetV2 architecture from the `MobileNetV2: Inverted Residuals and Linear\n    Bottlenecks <https://arxiv.org/abs/1801.04381>`_ paper.\n\n    Args:\n        weights (:class:`~torchvision.models.MobileNet_V2_Weights`, optional): The\n            pretrained weights to use. See\n            :class:`~torchvision.models.MobileNet_V2_Weights` below for\n            more details, and possible values. By default, no pre-trained\n            weights are used.\n        progress (bool, optional): If True, displays a progress bar of the\n            download to stderr. Default is True.\n        **kwargs: parameters passed to the ``torchvision.models.mobilenetv2.MobileNetV2``\n            base class. Please refer to the `source code\n            <https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py>`_\n            for more details about this class.\n\n    .. autoclass:: torchvision.models.MobileNet_V2_Weights\n        :members:\n    \"\"\"\n\n    model = DepShrinkMobileNetV2(**kwargs)\n\n    return model", ""]}
{"filename": "models/imagenet/vgg.py", "chunked_list": ["from functools import partial\nfrom typing import Union, List, Dict, Any, Optional, cast\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\nfrom torchvision.transforms._presets import ImageClassification\nfrom torchvision.utils import _log_api_usage_once\nfrom torchvision.models._api import WeightsEnum, Weights", "from torchvision.utils import _log_api_usage_once\nfrom torchvision.models._api import WeightsEnum, Weights\nfrom torchvision.models._meta import _IMAGENET_CATEGORIES\nfrom torchvision.models._utils import handle_legacy_interface, _ovewrite_named_param\n\nfrom models.modules_trt import NaiveFeed\nfrom models.model_op import fuse_bn\nfrom utils.measure import compile_and_time, torch_time, unroll_merged_vgg\n\n__all__ = [", "\n__all__ = [\n    \"VGG\",\n    \"VGGBlock\",\n    \"vgg19_bn\",\n    \"vgg_cfgs\",\n]\n\n\nclass VGGBlock(nn.Module):\n    def __init__(self, inp: int, oup: int, num: int) -> None:\n        super().__init__()\n        layers = []\n        ind = 1\n        for _ in range(num):\n            if ind != 1:\n                inp = oup\n            layers += [\n                (f\"conv{ind}\", nn.Conv2d(inp, oup, kernel_size=3, padding=1)),\n                (f\"bn{ind}\", nn.BatchNorm2d(oup)),\n                (f\"relu{ind}\", nn.ReLU(inplace=True)),\n            ]\n            ind += 1\n        layers += [(f\"pool{ind}\", nn.MaxPool2d(kernel_size=2, stride=2))]\n\n        # Each indicating if i-th act is disappeared\n        self.conv = nn.Sequential(OrderedDict(layers))\n\n    def forward(self, x):\n        out = self.conv(x)\n        return out", "\nclass VGGBlock(nn.Module):\n    def __init__(self, inp: int, oup: int, num: int) -> None:\n        super().__init__()\n        layers = []\n        ind = 1\n        for _ in range(num):\n            if ind != 1:\n                inp = oup\n            layers += [\n                (f\"conv{ind}\", nn.Conv2d(inp, oup, kernel_size=3, padding=1)),\n                (f\"bn{ind}\", nn.BatchNorm2d(oup)),\n                (f\"relu{ind}\", nn.ReLU(inplace=True)),\n            ]\n            ind += 1\n        layers += [(f\"pool{ind}\", nn.MaxPool2d(kernel_size=2, stride=2))]\n\n        # Each indicating if i-th act is disappeared\n        self.conv = nn.Sequential(OrderedDict(layers))\n\n    def forward(self, x):\n        out = self.conv(x)\n        return out", "\n\nclass VGG(nn.Module):\n    def __init__(\n        self,\n        cfg,\n        num_classes: int = 1000,\n        dropout: float = 0.5,\n        block: nn.Module = VGGBlock,\n    ) -> None:\n        super().__init__()\n        _log_api_usage_once(self)\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.dropout = dropout\n        self.num_classes = num_classes\n        self.block = block\n\n        self.build(cfg)\n        self.initialize()\n\n        self.is_merged = False\n\n    def set_classifier(self):\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(p=self.dropout),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(p=self.dropout),\n            nn.Linear(4096, self.num_classes),\n        )\n\n    def make_feature(self, inp, oup, n):\n        return self.block(inp, oup, n)\n\n    def build(self, cfg):\n        features = []\n        for n, inp, oup, isize in cfg:\n            self.cur_isize = isize\n            features.append(self.make_feature(inp, oup, n))\n        self.features = nn.Sequential(*features)\n        self.set_classifier()\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.is_merged:\n            x = self.m_features(x)\n        else:\n            x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n    # Does not merge anything and fuse batchnorm layer\n    def merge(self):\n        self.is_merged = True\n        stack = []\n        for _, block in enumerate(self.features):\n            stack.append(NaiveFeed(block.conv._modules))\n\n        self.m_features = nn.Sequential(*stack)\n        fuse_bn(self.m_features)\n        print(\"Fused batchnorm...\")\n        delattr(self, \"features\")\n\n    def time(self, txt=\"model\", verb=False, trt=True):\n        assert self.is_merged\n        unrolled_module = unroll_merged_vgg(self)\n        print(unrolled_module)\n\n        if trt:\n            print(\"Start compiling merged model...\")\n            result, std = compile_and_time(\n                unrolled_module, (64, 3, 224, 224), txt, verb\n            )\n        else:\n            result, std = torch_time(\n                unrolled_module, (64, 3, 224, 224), txt, verb, rep=200, warmup=300\n            )\n        del unrolled_module\n        return result, std\n\n    def mem(self):\n        assert self.is_merged\n        mem = torch.cuda.max_memory_allocated()\n        print(f\"Before : {mem / 1e6:>15} MB\")\n\n        unrolled_module = unroll_merged_vgg(self)\n        unrolled_module.eval()\n        params = sum(p.numel() for p in unrolled_module.parameters())\n        inputs = torch.randn((64, 3, 224, 224)).cuda()\n\n        print(unrolled_module)\n        for i in range(10):\n            torch.cuda.reset_peak_memory_stats()\n            unrolled_module(inputs)\n\n            mem = torch.cuda.max_memory_allocated()\n            print(f\"Iter {i}  : {mem / 1e6:>15} MB\")\n\n        return params, mem", "\n\n# fmt: off\n# cfgs: Dict[str, List[Union[str, int]]] = {\n#     \"A\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n#     \"B\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n#     \"D\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n#     \"E\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"],\n# }\n# fmt: on", "# }\n# fmt: on\n\nvgg_cfgs: Dict[str, List[Union[str, int]]] = {\n    # (n, inp, oup, isize)\n    \"19\": [\n        (2, 3, 64, 224),\n        (2, 64, 128, 112),\n        (4, 128, 256, 56),\n        (4, 256, 512, 28),", "        (4, 128, 256, 56),\n        (4, 256, 512, 28),\n        (4, 512, 512, 14),\n    ]\n}\n\n\ndef _vgg(\n    cfg: str,\n    weights: Optional[WeightsEnum],\n    progress: bool,\n    **kwargs: Any,\n) -> VGG:\n    if weights is not None:\n        kwargs[\"init_weights\"] = False\n        if weights.meta[\"categories\"] is not None:\n            _ovewrite_named_param(\n                kwargs, \"num_classes\", len(weights.meta[\"categories\"])\n            )\n    model = VGG(vgg_cfgs[cfg], **kwargs)\n    if weights is not None:\n        model.load_state_dict(weights.get_state_dict(progress=progress))\n    return model", "\n\ndef vgg19_bn(*, weights=None, progress=True, **kwargs: Any) -> VGG:\n    return _vgg(\"19\", weights, progress, **kwargs)\n"]}
{"filename": "models/imagenet/vgg_com.py", "chunked_list": ["import os\nimport sys\n\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\"))\n\nfrom typing import Any, Optional, List\nfrom collections import OrderedDict\nfrom functools import partial\nfrom models.model_op import (\n    adjust_padding,", "from models.model_op import (\n    adjust_padding,\n    adjust_isize,\n    merge_or_new,\n    push_merged_layers,\n    trace_feat_size,\n    get_act,\n    get_blk,\n    fix_act_lyrs,\n)", "    fix_act_lyrs,\n)\nfrom models.imagenet.vgg import VGG, VGGBlock, vgg_cfgs\nfrom models.modules_trt import NaiveFeed, SkipFeed\nfrom utils.measure import compile_and_time\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision.models._meta import _IMAGENET_CATEGORIES", "from torch import nn\nfrom torchvision.models._meta import _IMAGENET_CATEGORIES\n\n__all__ = [\"LearnVGG\", \"LearnVGGBlock\", \"learn_vgg19_bn\"]\n\n\ndef push_mp_layer(stack, new_name):\n    layer = nn.MaxPool2d(kernel_size=2, stride=2)\n    if isinstance(stack, list):\n        stack.append((new_name, layer))\n    elif isinstance(stack, nn.Sequential):\n        stack.add_module(new_name, layer)\n    else:\n        raise NotImplementedError(\"Not implemented type of stack\")", "\n\nclass LearnVGGBlock(VGGBlock):\n    def __init__(\n        self,\n        inp: int,\n        oup: int,\n        num: int,\n        isize: int,\n    ) -> None:\n        super().__init__(inp, oup, num)\n        self.isize = isize\n\n    def extra_repr(self):\n        out = f\"[isize : {self.isize}]\"\n        return out", "\n\nclass LearnVGG(VGG):\n    def __init__(\n        self,\n        cfg,\n        num_classes: int = 1000,\n        dropout: float = 0.5,\n    ) -> None:\n        super().__init__(cfg, num_classes, dropout, LearnVGGBlock)\n\n        self.act_pos = set()\n        self.str_pos = {2, 4, 8, 12, 16}\n        # No skip-connections\n        self.skip_s2t = dict()\n\n        inp = torch.randn(64, 3, 224, 224)\n        self.out_shape = trace_feat_size(self.features, inp)\n\n    def get_act_info(self):\n        act_pos, act_num = get_act(self.features, VGGBlock)\n        return act_pos, act_num\n\n    def get_blk_info(self):\n        blk_end = sorted(get_blk(self.features, VGGBlock))\n        blks = dict([(ind + 1, end) for ind, end in enumerate(blk_end)])\n        blk_num = len(blks)\n        return blks, blk_num\n\n    def make_feature(self, inp, oup, n):\n        return self.block(inp, oup, n, self.cur_isize)\n\n    def fix_act(self, act_pos=None, merge_pos=None):\n        self.is_fixed_act = True\n\n        self.act_pos = act_pos if act_pos != None else self.get_act_info()[0]\n        self.merge_pos = merge_pos if merge_pos != None else self.act_pos\n        assert self.act_pos.issubset(self.merge_pos)\n\n        bumps = set.union(self.str_pos, self.merge_pos)\n\n        fix_act_lyrs(self.features, VGGBlock, self.act_pos, \"relu\")\n        adjust_padding(self.features, bumps)\n        adjust_isize(self.features)\n\n    def merge(self, act_pos=None, merge_pos=None, keep_feat=False):\n        self.to(\"cpu\")\n        self.is_merged = True\n\n        self.act_pos = act_pos if act_pos != None else self.get_act_info()[0]\n        self.merge_pos = merge_pos if merge_pos != None else self.act_pos\n        assert self.act_pos.issubset(self.merge_pos)\n\n        bumps = set.union(self.str_pos, self.merge_pos)\n\n        self.m_layers, stack = [], []\n        pos, bump_end, loc_end, last = 0, None, None, None\n\n        # weights = (isize, m_cw, m_p, m_b), cparam = (ctype, cstr)\n        weights, cparam = (112, None, None, None), (\"dw\", 1)\n\n        for block in self.features:\n            for ind in range(0, len(block.conv) - 1, 3):\n                relu = pos + 1 in self.act_pos\n                conv, bn = block.conv[ind], block.conv[ind + 1]\n                lyrs = (conv, bn)\n\n                is_merged = pos > 0 and not pos in bumps\n\n                _, weights, cparam = merge_or_new(\n                    stack, pos, weights, lyrs, is_merged, cparam, last\n                )\n                push_merged_layers(\n                    stack, pos, weights, cparam, relu=relu, act_type=\"relu\"\n                )\n                pos += 1\n                last = 2 if relu else 1\n            push_mp_layer(stack, f\"pool{pos-1}\")\n\n        if len(stack) > 0:\n            self.m_layers += [NaiveFeed(OrderedDict(stack))]\n        self.m_features = nn.Sequential(*self.m_layers)\n\n        if not keep_feat:\n            delattr(self, \"features\")\n        self.to(\"cuda\")", "\n\ndef learn_vgg19_bn(num_classes):\n    return LearnVGG(vgg_cfgs[\"19\"], num_classes=num_classes)\n"]}
