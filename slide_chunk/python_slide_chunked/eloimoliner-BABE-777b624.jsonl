{"filename": "train.py", "chunked_list": ["import os\nimport re\nimport json\nimport hydra\nimport torch\n#from utils.torch_utils import distributed as dist\nimport utils.setup as setup\nfrom training.trainer import Trainer\n\nimport copy", "\nimport copy\n\n\ndef _main(args):\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    #assert torch.cuda.is_available()\n    #device=\"cuda\"\n\n    global __file__\n    __file__ = hydra.utils.to_absolute_path(__file__)\n    dirname = os.path.dirname(__file__)\n    args.model_dir = os.path.join(dirname, str(args.model_dir))\n    if not os.path.exists(args.model_dir):\n            os.makedirs(args.model_dir)\n\n\n    args.exp.model_dir=args.model_dir\n\n    #dist.init()\n    dset=setup.setup_dataset(args)\n    diff_params=setup.setup_diff_parameters(args)\n    network=setup.setup_network(args, device)\n    optimizer=setup.setup_optimizer(args, network)\n    #try:\n    test_set=setup.setup_dataset_test(args)\n    #except:\n    #test_set=None\n    network_tester=copy.deepcopy(network)\n\n    tester=setup.setup_tester(args, network=network_tester, diff_params=diff_params, test_set=test_set, device=device) #this will be used for making demos during training\n    print(\"setting up trainer\")\n    trainer=setup.setup_trainer(args, dset=dset, network=network, optimizer=optimizer, diff_params=diff_params, tester=tester, device=device) #this will be used for making demos during training\n    print(\"trainer set up\")\n\n\n    # Print options.\n    print()\n    print('Training options:')\n    print()\n    print(f'Output directory:        {args.model_dir}')\n    print(f'Network architecture:    {args.network.callable}')\n    print(f'Dataset:    {args.dset.callable}')\n    print(f'Diffusion parameterization:  {args.diff_params.callable}')\n    print(f'Batch size:              {args.exp.batch}')\n    print(f'Mixed-precision:         {args.exp.use_fp16}')\n    print()\n\n    # Train.\n    #trainer=Trainer(args=args, dset=dset, network=network, optimizer=optimizer, diff_params=diff_params, tester=tester, device=device)\n    trainer.training_loop()", "\n@hydra.main(config_path=\"conf\", config_name=\"conf\")\ndef main(args):\n    _main(args)\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n", "#----------------------------------------------------------------------------\n"]}
{"filename": "test.py", "chunked_list": ["import os\nimport re\nimport json\nimport hydra\n#import click\nimport torch\n#from utils.torch_utils import distributed as dist\nimport utils.setup as setup\nimport urllib.request\n", "import urllib.request\n\n\ndef _main(args):\n\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    #assert torch.cuda.is_available()\n    #device=\"cuda\"\n\n    global __file__\n    __file__ = hydra.utils.to_absolute_path(__file__)\n    dirname = os.path.dirname(__file__)\n    args.model_dir = os.path.join(dirname, str(args.model_dir))\n    if not os.path.exists(args.model_dir):\n            os.makedirs(args.model_dir)\n\n    args.exp.model_dir=args.model_dir\n\n    torch.multiprocessing.set_start_method('spawn')\n\n    diff_params=setup.setup_diff_parameters(args)\n    network=setup.setup_network(args, device)\n\n    test_set=setup.setup_dataset_test(args)\n\n    tester=setup.setup_tester(args, network=network, diff_params=diff_params, test_set=test_set, device=device) #this will be used for making demos during training\n    # Print options.\n    print()\n    print('Training options:')\n    print()\n    print(f'Output directory:        {args.model_dir}')\n    print(f'Network architecture:    {args.network.callable}')\n    print(f'Diffusion parameterization:  {args.diff_params.callable}')\n    print(f'Tester:                  {args.tester.callable}')\n    print(f'Experiment:                  {args.exp.exp_name}')\n    print()\n\n    # Train.\n    print(\"loading checkpoint path:\", args.tester.checkpoint)\n    if args.tester.checkpoint != 'None':\n        ckpt_path=os.path.join(dirname, args.tester.checkpoint)\n        if not(os.path.exists(ckpt_path)):\n            print(\"download ckpt\")\n            path, basename= os.path.split(args.tester.checkpoint)\n            if not(os.path.exists(os.path.join(dirname, path))):\n                    os.makedirs(os.path.join(dirname,path))\n            HF_path=\"https://huggingface.co/Eloimoliner/babe/resolve/main/\"+os.path.basename(args.tester.checkpoint)\n            urllib.request.urlretrieve(HF_path, filename=ckpt_path)\n        #relative path\n        ckpt_path=os.path.join(dirname, args.tester.checkpoint)\n        tester.load_checkpoint(ckpt_path) \n        #jexcept:\n        #j    #absolute path\n        #j   tester.load_checkpoint(os.path.join(args.model_dir,args.tester.checkpoint)) \n    else:\n        print(\"trying to load latest checkpoint\")\n        tester.load_latest_checkpoint()\n\n    tester.dodajob()", "\n@hydra.main(config_path=\"conf\", config_name=\"conf\")\ndef main(args):\n    _main(args)\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n", "#----------------------------------------------------------------------------\n"]}
{"filename": "utils/setup.py", "chunked_list": ["import torch\nimport numpy as np\nimport utils.dnnlib as dnnlib\n\ndef worker_init_fn(worker_id):\n    st=np.random.get_state()[2]\n    np.random.seed( st+ worker_id)\n\n\ndef setup_dataset(args):\n    try:\n        overfit=args.dset.overfit\n    except:\n        overfit=False\n\n    #the dataloader loads audio at the original sampling rate, then in the training loop we resample it to the target sampling rate. The mismatch between sampling rates is indicated by the resample_factor\n    #if resample_factor=1, then the audio is not resampled, and everything is normal\n    #try:\n    if args.dset.type==\"operator\":\n        dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.callable, dset_args=args.dset, fs=args.exp.sample_rate*args.exp.resample_factor)\n           \n        dataset_iterator = iter(torch.utils.data.DataLoader(dataset=dataset_obj, batch_size=args.exp.batch,  num_workers=args.exp.num_workers, pin_memory=True, worker_init_fn=worker_init_fn))\n                \n        return dataset_iterator\n    else:\n        if args.dset.name==\"maestro_allyears\" or args.dset.name==\"maestro_fs\":\n            dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.callable, dset_args=args.dset, overfit=overfit)\n        else:\n            dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.callable, dset_args=args.dset, fs=args.exp.sample_rate*args.exp.resample_factor, seg_len=args.exp.audio_len*args.exp.resample_factor, overfit=overfit)\n            \n        \n        dataset_iterator = iter(torch.utils.data.DataLoader(dataset=dataset_obj, batch_size=args.exp.batch,  num_workers=args.exp.num_workers, pin_memory=True, worker_init_fn=worker_init_fn, timeout=0, prefetch_factor=20))\n        \n        return dataset_iterator", "\ndef setup_dataset(args):\n    try:\n        overfit=args.dset.overfit\n    except:\n        overfit=False\n\n    #the dataloader loads audio at the original sampling rate, then in the training loop we resample it to the target sampling rate. The mismatch between sampling rates is indicated by the resample_factor\n    #if resample_factor=1, then the audio is not resampled, and everything is normal\n    #try:\n    if args.dset.type==\"operator\":\n        dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.callable, dset_args=args.dset, fs=args.exp.sample_rate*args.exp.resample_factor)\n           \n        dataset_iterator = iter(torch.utils.data.DataLoader(dataset=dataset_obj, batch_size=args.exp.batch,  num_workers=args.exp.num_workers, pin_memory=True, worker_init_fn=worker_init_fn))\n                \n        return dataset_iterator\n    else:\n        if args.dset.name==\"maestro_allyears\" or args.dset.name==\"maestro_fs\":\n            dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.callable, dset_args=args.dset, overfit=overfit)\n        else:\n            dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.callable, dset_args=args.dset, fs=args.exp.sample_rate*args.exp.resample_factor, seg_len=args.exp.audio_len*args.exp.resample_factor, overfit=overfit)\n            \n        \n        dataset_iterator = iter(torch.utils.data.DataLoader(dataset=dataset_obj, batch_size=args.exp.batch,  num_workers=args.exp.num_workers, pin_memory=True, worker_init_fn=worker_init_fn, timeout=0, prefetch_factor=20))\n        \n        return dataset_iterator", "\ndef setup_dataset_test(args):\n\n    if args.dset.name==\"maestro_allyears\" or args.dset.name==\"maestro_fs\":\n        dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.test.callable, dset_args=args.dset, num_samples=args.dset.test.num_samples)\n    else:\n        dataset_obj=dnnlib.call_func_by_name(func_name=args.dset.test.callable, dset_args=args.dset, fs=args.exp.sample_rate*args.exp.resample_factor,seg_len=args.exp.audio_len*args.exp.resample_factor, num_samples=args.dset.test.num_samples)\n    \n    dataset = torch.utils.data.DataLoader(dataset=dataset_obj, batch_size=args.dset.test.batch_size,  num_workers=args.exp.num_workers, pin_memory=True, worker_init_fn=worker_init_fn)\n\n    return dataset", "\ndef setup_diff_parameters(args):\n\n    diff_params_obj=dnnlib.call_func_by_name(func_name=args.diff_params.callable, args=args)\n\n    return diff_params_obj\n\ndef setup_network(args, device, operator=False):\n        #try:\n        network_obj=dnnlib.call_func_by_name(func_name=args.network.callable, args=args, device=device)\n        #except Exception as e:\n        #        print(e)\n        #        network_obj=dnnlib.call_func_by_name(func_name=args.network.callable, args=args.network, device=device)\n        return network_obj.to(device)", "\ndef setup_denoiser(args, device):\n        #try:\n        network_obj=dnnlib.call_func_by_name(func_name=args.tester.denoiser.callable, unet_args=args.tester.denoiser)\n\n        #except Exception as e:\n        #        print(e)\n        #        network_obj=dnnlib.call_func_by_name(func_name=args.network.callable, args=args.network, device=device)\n        return network_obj.to(device)\n\ndef setup_optimizer(args, network):\n    # setuo optimizer for training\n    optimizer = torch.optim.Adam(network.parameters(), lr=args.exp.lr, betas=(args.exp.optimizer.beta1, args.exp.optimizer.beta2), eps=args.exp.optimizer.eps)\n    return optimizer", "\ndef setup_optimizer(args, network):\n    # setuo optimizer for training\n    optimizer = torch.optim.Adam(network.parameters(), lr=args.exp.lr, betas=(args.exp.optimizer.beta1, args.exp.optimizer.beta2), eps=args.exp.optimizer.eps)\n    return optimizer\n\ndef setup_tester(args, network=None, network_operator=None, diff_params=None, test_set=None, device=\"cpu\"):\n    assert network is not None\n    assert diff_params is not None\n    if args.tester.do_test:\n        # setuo sampler for making demos during training\n        print(\"netwew operater\",network_operator)\n        if network_operator!=None:\n                sampler = dnnlib.call_func_by_name(func_name=args.tester.callable, args=args, network=network, network_operator=network_operator, test_set=test_set, diff_params=diff_params, device=device)\n        else:\n                sampler = dnnlib.call_func_by_name(func_name=args.tester.callable, args=args, network=network, test_set=test_set, diff_params=diff_params, device=device)\n        return sampler\n    else:\n        return None\n    trainer=setup.setup_trainer #this will be used for making demos during training", "def setup_trainer(args, dset=None, network=None, optimizer=None, diff_params=None, tester=None, device=\"cpu\"):\n    assert network is not None\n    assert diff_params is not None\n    assert optimizer is not None\n    assert tester is not None\n    print(args.exp.trainer_callable)\n    trainer = dnnlib.call_func_by_name(func_name=args.exp.trainer_callable, args=args, dset=dset, network=network, optimizer=optimizer, diff_params=diff_params, tester=tester, device=device)\n    return trainer\n    \n", "    \n\n    \n\n\n\n\n"]}
{"filename": "utils/utils_notebook.py", "chunked_list": ["import soundfile as sf\nimport torch\nimport numpy as np\n\ndef load_audio(name=\"test_dir/0.wav\",Ls=65536):\n    x, fs=sf.read(name)\n    x=torch.Tensor(x)\n    #x=x[1*fs:2*fs]\n    Ls=65536\n    x=x[0:Ls]\n    return x, fs", "\ndef save_wav(x, fs=22050, filename=\"test.wav\"):\n    x=x.numpy()\n    sf.write(filename, x, fs)\n\ndef plot_stft(x):\n    NFFT=1024\n    #hamming window\n    window = torch.hann_window(NFFT)\n    #apply STFT to x\n    X = torch.stft(x, NFFT, hop_length=NFFT//2, win_length=NFFT, window=window, center=False, normalized=False, onesided=True)\n\n    freqs=np.fft.rfftfreq(NFFT, 1/fs)\n\n    X_abs=(X[...,0]**2+X[...,1]**2)**0.5\n    #plot absolute value of STFT using px\n    fig = px.imshow(20*np.log10(X_abs.numpy()+1e-8), labels=dict(x=\"Time\", y=\"Frequency\", color=\"Magnitude\"))\n\n    fig.show()"]}
{"filename": "utils/training_utils.py", "chunked_list": ["\nimport torch\nimport torchaudio\nimport numpy as np\nimport scipy.signal\nclass EMAWarmup:\n    \"\"\"Implements an EMA warmup using an inverse decay schedule.\n    If inv_gamma=1 and power=1, implements a simple average. inv_gamma=1, power=2/3 are\n    good values for models you plan to train for a million or more steps (reaches decay\n    factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n    you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n    215.4k steps).\n    Args:\n        inv_gamma (float): Inverse multiplicative factor of EMA warmup. Default: 1.\n        power (float): Exponential factor of EMA warmup. Default: 1.\n        min_value (float): The minimum EMA decay rate. Default: 0.\n        max_value (float): The maximum EMA decay rate. Default: 1.\n        start_at (int): The epoch to start averaging at. Default: 0.\n        last_epoch (int): The index of last epoch. Default: 0.\n    \"\"\"\n\n    def __init__(self, inv_gamma=1., power=1., min_value=0., max_value=1., start_at=0,\n                 last_epoch=0):\n        self.inv_gamma = inv_gamma\n        self.power = power\n        self.min_value = min_value\n        self.max_value = max_value\n        self.start_at = start_at\n        self.last_epoch = last_epoch\n\n    def state_dict(self):\n        \"\"\"Returns the state of the class as a :class:`dict`.\"\"\"\n        return dict(self.__dict__.items())\n\n    def load_state_dict(self, state_dict):\n        \"\"\"Loads the class's state.\n        Args:\n            state_dict (dict): scaler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \"\"\"\n        self.__dict__.update(state_dict)\n\n    def get_value(self):\n        \"\"\"Gets the current EMA decay rate.\"\"\"\n        epoch = max(0, self.last_epoch - self.start_at)\n        value = 1 - (1 + epoch / self.inv_gamma) ** -self.power\n        return 0. if epoch < 0 else min(self.max_value, max(self.min_value, value))\n\n    def step(self):\n        \"\"\"Updates the step count.\"\"\"\n        self.last_epoch += 1", "\n\n#from https://github.com/csteinmetz1/auraloss/blob/main/auraloss/perceptual.py\nclass FIRFilter(torch.nn.Module):\n    \"\"\"FIR pre-emphasis filtering module.\n    Args:\n        filter_type (str): Shape of the desired FIR filter (\"hp\", \"fd\", \"aw\"). Default: \"hp\"\n        coef (float): Coefficient value for the filter tap (only applicable for \"hp\" and \"fd\"). Default: 0.85\n        ntaps (int): Number of FIR filter taps for constructing A-weighting filters. Default: 101\n        plot (bool): Plot the magnitude respond of the filter. Default: False\n    Based upon the perceptual loss pre-empahsis filters proposed by\n    [Wright & V\u00e4lim\u00e4ki, 2019](https://arxiv.org/abs/1911.08922).\n    A-weighting filter - \"aw\"\n    First-order highpass - \"hp\"\n    Folded differentiator - \"fd\"\n    Note that the default coefficeint value of 0.85 is optimized for\n    a sampling rate of 44.1 kHz, considering adjusting this value at differnt sampling rates.\n    \"\"\"\n\n    def __init__(self, filter_type=\"hp\", coef=0.85, fs=44100, ntaps=101, plot=False): \n        \"\"\"Initilize FIR pre-emphasis filtering module.\"\"\"\n        super(FIRFilter, self).__init__()\n        self.filter_type = filter_type\n        self.coef = coef\n        self.fs = fs\n        self.ntaps = ntaps\n        self.plot = plot\n\n        if ntaps % 2 == 0:\n            raise ValueError(f\"ntaps must be odd (ntaps={ntaps}).\")\n\n        if filter_type == \"hp\":\n            self.fir = torch.nn.Conv1d(1, 1, kernel_size=3, bias=False, padding=1)\n            self.fir.weight.requires_grad = False\n            self.fir.weight.data = torch.tensor([1, -coef, 0]).view(1, 1, -1)\n        elif filter_type == \"fd\":\n            self.fir = torch.nn.Conv1d(1, 1, kernel_size=3, bias=False, padding=1)\n            self.fir.weight.requires_grad = False\n            self.fir.weight.data = torch.tensor([1, 0, -coef]).view(1, 1, -1)\n        elif filter_type == \"aw\":\n            # Definition of analog A-weighting filter according to IEC/CD 1672.\n            f1 = 20.598997\n            f2 = 107.65265\n            f3 = 737.86223\n            f4 = 12194.217\n            A1000 = 1.9997\n\n            NUMs = [(2 * np.pi * f4) ** 2 * (10 ** (A1000 / 20)), 0, 0, 0, 0]\n            DENs = np.polymul(\n                [1, 4 * np.pi * f4, (2 * np.pi * f4) ** 2],\n                [1, 4 * np.pi * f1, (2 * np.pi * f1) ** 2],\n            )\n            DENs = np.polymul(\n                np.polymul(DENs, [1, 2 * np.pi * f3]), [1, 2 * np.pi * f2]\n            )\n\n            # convert analog filter to digital filter\n            b, a = scipy.signal.bilinear(NUMs, DENs, fs=fs)\n\n            # compute the digital filter frequency response\n            w_iir, h_iir = scipy.signal.freqz(b, a, worN=512, fs=fs)\n\n            # then we fit to 101 tap FIR filter with least squares\n            taps = scipy.signal.firls(ntaps, w_iir, abs(h_iir), fs=fs)\n\n            # now implement this digital FIR filter as a Conv1d layer\n            self.fir = torch.nn.Conv1d(\n                1, 1, kernel_size=ntaps, bias=False, padding=ntaps // 2\n            )\n            self.fir.weight.requires_grad = False\n            self.fir.weight.data = torch.tensor(taps.astype(\"float32\")).view(1, 1, -1)\n\n    def forward(self, error):\n        \"\"\"Calculate forward propagation.\n        Args:\n            input (Tensor): Predicted signal (B, #channels, #samples).\n            target (Tensor): Groundtruth signal (B, #channels, #samples).\n        Returns:\n            Tensor: Filtered signal.\n        \"\"\"\n        self.fir.weight.data=self.fir.weight.data.to(error.device)\n        error=error.unsqueeze(1)\n        error = torch.nn.functional.conv1d(\n            error, self.fir.weight.data, padding=self.ntaps // 2\n        )\n        error=error.squeeze(1)\n        return error", "\ndef resample_batch(audio, fs, fs_target, length_target=None):\n\n        device=audio.device\n        dtype=audio.dtype\n        B=audio.shape[0]\n        #if possible resampe in a batched way\n        #check if all the fs are the same and equal to 44100\n        #print(fs_target)\n        if fs_target==22050:\n            if (fs==44100).all():\n                 audio=torchaudio.functional.resample(audio, 2,1)\n                 return audio[:, 0:length_target] #trow away the last samples\n            elif (fs==48000).all():\n                 #approcimate resamppleint\n                 audio=torchaudio.functional.resample(audio, 160*2,147)\n                 return audio[:, 0:length_target]\n            else:\n                #if revious is unsuccesful bccause we have examples at 441000 and 48000 in the same batch,, just iterate over the batch\n                proc_batch=torch.zeros((B,length_target), device=device)\n                for i, (a, f_s) in enumerate(zip(audio, fs)): #I hope this shit wll not slow down everythingh\n                    if f_s==44100:\n                        #resample by 2\n                        a=torchaudio.functional.resample(a, 2,1)\n                    elif f_s==48000:\n                        a=torchaudio.functional.resample(a, 160*2,147)\n                    else:\n                        print(\"WARNING, strange fs\", f_s)\n           \n                    proc_batch[i]=a[0:length_target]\n                return proc_batch\n        elif fs_target==44100:\n            if (fs==44100).all():\n                 return audio[:, 0:length_target] #trow away the last samples\n            elif (fs==48000).all():\n                 #approcimate resamppleint\n                 audio=torchaudio.functional.resample(audio, 160,147)\n                 return audio[:, 0:length_target]\n            else:\n                #if revious is unsuccesful bccause we have examples at 441000 and 48000 in the same batch,, just iterate over the batch\n                B,C,L=audio.shape\n                proc_batch=torch.zeros((B,C,L), device=device)\n                #print(\"debigging resample batch\")\n                #print(audio.shape,fs.shape)\n                for i, (a, f_s) in enumerate(zip(audio, fs.tolist())): #I hope this shit wll not slow down everythingh\n                    #print(i,a.shape,f_s)\n                    if f_s==44100:\n                        #resample by 2\n                        pass\n                    elif f_s==22050:\n                        a=torchaudio.functional.resample(a, 1,2)\n                    elif f_s==48000:\n                        a=torchaudio.functional.resample(a, 160,147)\n                    else:\n                        print(\"WARNING, strange fs\", f_s)\n           \n\n                    proc_batch[i]=a[...,0:length_target] \n                return proc_batch\n        else:\n            if (fs==44100).all():\n                 audio=torchaudio.functional.resample(audio, 44100, fs_target)\n                 return audio[...,0:length_target] #trow away the last samples\n            elif (fs==48000).all():\n                 print(\"resampling 48000 to 16000\", length_target, audio.shape)\n                 #approcimate resamppleint\n                 audio=torchaudio.functional.resample(audio, 48000,fs_target)\n                 print(audio.shape)\n                 return audio[..., 0:length_target]\n            else:\n                #if revious is unsuccesful bccause we have examples at 441000 and 48000 in the same batch,, just iterate over the batch\n                proc_batch=torch.zeros((B,length_target), device=device)\n                for i, (a, f_s) in enumerate(zip(audio, fs)): #I hope this shit wll not slow down everythingh\n                    if f_s==44100:\n                        #resample by 2\n                        a=torchaudio.functional.resample(a, 44100,fs_target)\n                    elif f_s==48000:\n                        a=torchaudio.functional.resample(a, 48000,fs_target)\n                    else:\n                        print(\"WARNING, strange fs\", f_s)\n           \n                    proc_batch[i]=a[...,0:length_target] \n                return proc_batch", "\ndef load_state_dict( state_dict, network=None, ema=None, optimizer=None, log=True):\n        '''\n        utility for loading state dicts for different models. This function sequentially tries different strategies\n        args:\n            state_dict: the state dict to load\n        returns:\n            True if the state dict was loaded, False otherwise\n        Assuming the operations are don in_place, this function will not create a copy of the network and optimizer (I hope)\n        '''\n        #print(state_dict)\n        if log: print(\"Loading state dict\")\n        if log:\n            print(state_dict.keys())\n        #if there\n        try:\n            if log: print(\"Attempt 1: trying with strict=True\")\n            if network is not None:\n                network.load_state_dict(state_dict['network'])\n            if optimizer is not None:\n                optimizer.load_state_dict(state_dict['optimizer'])\n            if ema is not None:\n                ema.load_state_dict(state_dict['ema'])\n            return True\n        except Exception as e:\n            if log:\n                print(\"Could not load state dict\")\n                print(e)\n        try:\n            if log: print(\"Attempt 2: trying with strict=False\")\n            if network is not None:\n                network.load_state_dict(state_dict['network'], strict=False)\n            #we cannot load the optimizer in this setting\n            #self.optimizer.load_state_dict(state_dict['optimizer'], strict=False)\n            if ema is not None:\n                ema.load_state_dict(state_dict['ema'], strict=False)\n            return True\n        except Exception as e:\n            if log:\n                print(\"Could not load state dict\")\n                print(e)\n                print(\"training from scratch\")\n        try:\n            if log: print(\"Attempt 3: trying with strict=False,but making sure that the shapes are fine\")\n            if ema is not None:\n                ema_state_dict = ema.state_dict()\n            if network is not None:\n                network_state_dict = network.state_dict()\n            i=0 \n            if network is not None:\n                for name, param in state_dict['network'].items():\n                    if log: print(\"checking\",name) \n                    if name in network_state_dict.keys():\n                        if network_state_dict[name].shape==param.shape:\n                                network_state_dict[name]=param\n                                if log:\n                                    print(\"assigning\",name)\n                                i+=1\n            network.load_state_dict(network_state_dict)\n            if ema is not None:\n                for name, param in state_dict['ema'].items():\n                        if log: print(\"checking\",name) \n                        if name in ema_state_dict.keys():\n                            if ema_state_dict[name].shape==param.shape:\n                                ema_state_dict[name]=param\n                                if log:\n                                    print(\"assigning\",name)\n                                i+=1\n     \n            ema.load_state_dict(ema_state_dict)\n     \n            if i==0:\n                if log: print(\"WARNING, no parameters were loaded\")\n                raise Exception(\"No parameters were loaded\")\n            elif i>0:\n                if log: print(\"loaded\", i, \"parameters\")\n                return True\n\n        except Exception as e:\n            print(e)\n            print(\"the second strict=False failed\")\n\n\n        try:\n            if log: print(\"Attempt 4: Assuming the naming is different, with the network and ema called 'state_dict'\")\n            if network is not None:\n                network.load_state_dict(state_dict['state_dict'])\n            if ema is not None:\n                ema.load_state_dict(state_dict['state_dict'])\n        except Exception as e:\n            if log:\n                print(\"Could not load state dict\")\n                print(e)\n                print(\"training from scratch\")\n                print(\"It failed 3 times!! but not giving up\")\n            #print the names of the parameters in self.network\n\n        try:\n            if log: print(\"Attempt 5: trying to load with different names, now model='model' and ema='ema_weights'\")\n            if ema is not None:\n                dic_ema = {}\n                for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n                    dic_ema[key] = tensor\n                    ema.load_state_dict(dic_ema)\n                return True\n        except Exception as e:\n            if log:\n                print(e)\n\n        try:\n            if log: print(\"Attempt 6: If there is something wrong with the name of the ema parameters, we can try to load them using the names of the parameters in the model\")\n            if ema is not None:\n                dic_ema = {}\n                i=0\n                for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n                    if tensor.requires_grad:\n                        dic_ema[key]=state_dict['ema_weights'][i]\n                        i=i+1\n                    else:\n                        dic_ema[key]=tensor     \n                ema.load_state_dict(dic_ema)\n                return True\n        except Exception as e:\n            if log:\n                print(e)\n\n\n        try:\n            #assign the parameters in state_dict to self.network using a for loop\n            print(\"Attempt 7: Trying to load the parameters one by one. This is for the dance diffusion model, looking for parameters starting with 'diffusion.' or 'diffusion_ema.'\")\n            if ema is not None:\n                ema_state_dict = ema.state_dict()\n            if network is not None:\n                network_state_dict = ema.state_dict()\n            i=0 \n            if network is not None:\n                for name, param in state_dict['state_dict'].items():\n                    print(\"checking\",name) \n                    if name.startswith(\"diffusion.\"):\n                        i+=1\n                        name=name.replace(\"diffusion.\",\"\")\n                        if network_state_dict[name].shape==param.shape:\n                            #print(param.shape, network.state_dict()[name].shape)\n                            network_state_dict[name]=param\n                            #print(\"assigning\",name)\n           \n                network.load_state_dict(network_state_dict, strict=False)\n           \n            if ema is not None:\n                for name, param in state_dict['state_dict'].items():\n                    if name.startswith(\"diffusion_ema.\"): \n                        i+=1\n                        name=name.replace(\"diffusion_ema.\",\"\")\n                        if ema_state_dict[name].shape==param.shape:\n                            if log:\n                                    print(param.shape, ema.state_dict()[name].shape)\n                            ema_state_dict[name]=param\n           \n                ema.load_state_dict(ema_state_dict, strict=False)\n           \n            if i==0:\n                print(\"WARNING, no parameters were loaded\")\n                raise Exception(\"No parameters were loaded\")\n            elif i>0:\n                print(\"loaded\", i, \"parameters\")\n                return True\n        except Exception as e:\n            if log:\n                print(e)\n        #try:\n        # this is for the dmae1d mddel, assuming there is only one network\n        if network is not None:\n            network.load_state_dict(state_dict, strict=True)\n        if ema is not None:\n            ema.load_state_dict(state_dict, strict=True)\n        return True\n\n        #except Exception as e:\n        #    if log:\n        #        print(e)\n\n        return False", "\n            \ndef unnormalize(x,stds, args):\n        #unnormalize the STN separated audio\n        new_std=args.exp.normalization.target_std\n        if new_std==\"sigma_data\":\n            new_std=args.diff_params.sigma_data\n        x=stds*x/(new_std+1e-8)\n        return x\ndef normalize( xS, xT, xN, args, return_std=False):\n        #normalize the STN separated audio\n        if args.exp.normalization.mode==\"None\":\n            pass\n        elif args.exp.normalization.mode==\"residual_noise\":\n            #normalize the residual noise\n\n            std=xN.std(dim=-1, keepdim=True).mean(dim=1, keepdim=True)\n            new_std=args.exp.normalization.target_std\n\n            if new_std==\"sigma_data\":\n                new_std=args.diff_params.sigma_data\n\n            #print(std, new_std)\n\n            xN=new_std*xN/(std+1e-8)\n            #print(xN.std(dim=-1, keepdim=True))\n            xS=new_std*xS/(std+1e-8)\n            xT=new_std*xT/(std+1e-8)\n        elif args.exp.normalization.mode==\"residual_noise_batch\":\n            #normalize the residual noise per batch\n            #get the std of the entire batch\n            std=xN.std(dim=(0,1,2),unbiased=True, keepdim=False)\n        \n            new_std=args.exp.normalization.target_std\n\n            if new_std==\"sigma_data\":\n                new_std=args.diff_params.sigma_data\n\n            #print(std, new_std)\n\n            xN=new_std*xN/(std+1e-8)\n            #print(xN.std(dim=-1, keepdim=True).mean(dim=1, keepdim=True))\n            xS=new_std*xS/(std+1e-8)\n            xT=new_std*xT/(std+1e-8)\n\n        elif args.exp.normalization.mode==\"all\":\n            std=(xN+xS+xT).std(dim=-1, keepdim=True).mean(dim=1, keepdim=True)\n            new_std=args.exp.normalization.target_std\n            if new_std==\"sigma_data\":\n                new_std=args.diff_params.sigma_data\n            xN=new_std*xN/(std+1e-8)\n            xS=new_std*xS/(std+1e-8)\n            xT=new_std*xT/(std+1e-8)\n            #print(\"std\",xN.std(dim=-1, keepdim=True).mean(dim=1, keepdim=True))\n        else:\n            print(\"normalization mode not recognized\")\n            pass\n\n        try:\n            if return_std:\n                return xS, xT, xN, std\n        except Exception as e:\n            print(e)\n            print(\"warning!, std cannot be returned\")\n            pass\n\n        return xS, xT, xN", "def normalize( xS, xT, xN, args, return_std=False):\n        #normalize the STN separated audio\n        if args.exp.normalization.mode==\"None\":\n            pass\n        elif args.exp.normalization.mode==\"residual_noise\":\n            #normalize the residual noise\n\n            std=xN.std(dim=-1, keepdim=True).mean(dim=1, keepdim=True)\n            new_std=args.exp.normalization.target_std\n\n            if new_std==\"sigma_data\":\n                new_std=args.diff_params.sigma_data\n\n            #print(std, new_std)\n\n            xN=new_std*xN/(std+1e-8)\n            #print(xN.std(dim=-1, keepdim=True))\n            xS=new_std*xS/(std+1e-8)\n            xT=new_std*xT/(std+1e-8)\n        elif args.exp.normalization.mode==\"residual_noise_batch\":\n            #normalize the residual noise per batch\n            #get the std of the entire batch\n            std=xN.std(dim=(0,1,2),unbiased=True, keepdim=False)\n        \n            new_std=args.exp.normalization.target_std\n\n            if new_std==\"sigma_data\":\n                new_std=args.diff_params.sigma_data\n\n            #print(std, new_std)\n\n            xN=new_std*xN/(std+1e-8)\n            #print(xN.std(dim=-1, keepdim=True).mean(dim=1, keepdim=True))\n            xS=new_std*xS/(std+1e-8)\n            xT=new_std*xT/(std+1e-8)\n\n        elif args.exp.normalization.mode==\"all\":\n            std=(xN+xS+xT).std(dim=-1, keepdim=True).mean(dim=1, keepdim=True)\n            new_std=args.exp.normalization.target_std\n            if new_std==\"sigma_data\":\n                new_std=args.diff_params.sigma_data\n            xN=new_std*xN/(std+1e-8)\n            xS=new_std*xS/(std+1e-8)\n            xT=new_std*xT/(std+1e-8)\n            #print(\"std\",xN.std(dim=-1, keepdim=True).mean(dim=1, keepdim=True))\n        else:\n            print(\"normalization mode not recognized\")\n            pass\n\n        try:\n            if return_std:\n                return xS, xT, xN, std\n        except Exception as e:\n            print(e)\n            print(\"warning!, std cannot be returned\")\n            pass\n\n        return xS, xT, xN", ""]}
{"filename": "utils/blind_bwe_utils.py", "chunked_list": ["\nimport torch\nimport plotly.express as px\nimport pandas as pd\n\ndef apply_filter(x, H, NFFT):\n    '''\n    '''\n    X=apply_stft(x, NFFT)\n    xrec=apply_filter_istft(X, H, NFFT)\n    xrec=xrec[:,:x.shape[-1]]\n\n    return xrec", "\ndef apply_stft(x, NFFT):\n    '''\n    '''\n    #hamming window\n    window = torch.hamming_window(window_length=NFFT)\n    window=window.to(x.device)\n\n    x=torch.cat((x, torch.zeros(*x.shape[:-1],NFFT).to(x.device)),1) #is padding necessary?\n    X = torch.stft(x, NFFT, hop_length=NFFT//2,  window=window,  center=False, onesided=True, return_complex=True)\n    X=torch.view_as_real(X)\n\n    return X", "\ndef apply_filter_istft(X, H, NFFT):\n    '''\n    '''\n    #hamming window\n    window = torch.hamming_window(window_length=NFFT)\n    window=window.to(X.device)\n\n    X=X*H.unsqueeze(-1).unsqueeze(-1).expand(X.shape)\n    X=torch.view_as_complex(X)\n    x=torch.istft(X, NFFT, hop_length=NFFT//2,  window=window, center=False, return_complex=False)\n\n    return x", "\ndef design_filter_G(fc, A, G, f):\n    \"\"\"\n    fc: cutoff frequency \n        if fc is a scalar, the filter has one slopw\n        if fc is a list of scalars, the filter has multiple slopes\n    A: attenuation in dB\n        if A is a scalar, the filter has one slopw\n        if A is a list of scalars, the filter has multiple slopes\n    \"\"\"\n    multiple_slopes=False\n    #check if fc and A are lists\n    if isinstance(fc, list) and isinstance(A, list):\n        multiple_slopes=True\n    #check if fc is a tensor and A is a tensor\n    try:\n        if fc.shape[0]>1:\n            multiple_slopes=True\n    except:\n        pass\n\n    if multiple_slopes:\n        H=torch.zeros(f.shape).to(f.device)\n        H[f<fc[0]]=1\n        H[f>=fc[0]]=10**(A[0]*torch.log2(f[f>=fc[0]]/fc[0])/20)\n        for i in range(1,len(fc)):\n            #find the index of the first frequency that is greater than fc[i]\n            #fix=torch.where(f>=fc[i])[0][0]\n            #print(fc[i],fix)\n            #H[f>=fc[i]]=H[fix]-10**(A[i]*torch.log2(f[f>=fc[i]]/fc[i])/20)\n            H[f>=fc[i]]=10**(A[i]*torch.log2(f[f>=fc[i]]/fc[i])/20)*H[f>=fc[i]][0]\n        #apply the gain (G is in dB)\n        H=H*10**(G/20)\n        #return H\n    else:\n        #if fc and A are scalars\n        H=torch.zeros(f.shape).to(f.device)\n        H[f<fc]=1\n        H[f>=fc]=10**(A*torch.log2(f[f>=fc]/fc)/20)\n        H=H*10**(G/20)\n    return H", "\ndef design_filter(fc, A, f):\n    \"\"\"\n    fc: cutoff frequency \n        if fc is a scalar, the filter has one slopw\n        if fc is a list of scalars, the filter has multiple slopes\n    A: attenuation in dB\n        if A is a scalar, the filter has one slopw\n        if A is a list of scalars, the filter has multiple slopes\n    \"\"\"\n    multiple_slopes=False\n    #check if fc and A are lists\n    if isinstance(fc, list) and isinstance(A, list):\n        multiple_slopes=True\n    #check if fc is a tensor and A is a tensor\n    try:\n        if fc.shape[0]>1:\n            multiple_slopes=True\n    except:\n        pass\n\n    if multiple_slopes:\n        H=torch.zeros(f.shape).to(f.device)\n        H[f<fc[0]]=1\n        H[f>=fc[0]]=10**(A[0]*torch.log2(f[f>=fc[0]]/fc[0])/20)\n        for i in range(1,len(fc)):\n            #find the index of the first frequency that is greater than fc[i]\n            #fix=torch.where(f>=fc[i])[0][0]\n            #print(fc[i],fix)\n            #H[f>=fc[i]]=H[fix]-10**(A[i]*torch.log2(f[f>=fc[i]]/fc[i])/20)\n            H[f>=fc[i]]=10**(A[i]*torch.log2(f[f>=fc[i]]/fc[i])/20)*H[f>=fc[i]][0]\n\n        #return H\n    else:\n        #if fc and A are scalars\n        H=torch.zeros(f.shape).to(f.device)\n        H[f<fc]=1\n        H[f>=fc]=10**(A*torch.log2(f[f>=fc]/fc)/20)\n    return H", "\n#def design_filter(fc, A, f):\n#    H=torch.zeros(f.shape).to(f.device)\n#    H[f<fc]=1\n#    H[f>=fc]=10**(A*torch.log2(f[f>=fc]/fc)/20)\n#    return H\n\n\n\n\ndef apply_filter_and_norm_STFTmag(X,Xref, H):\n    #X: (N,513, T) \"clean\" example\n    #Xref: (N,513, T)  observations\n    #H: (513,) filter\n\n    #get the absolute value of the STFT\n    X=torch.sqrt(X[...,0]**2+X[...,1]**2)\n    Xref=torch.sqrt(Xref[...,0]**2+Xref[...,1]**2)\n\n    X=X*H.unsqueeze(-1).expand(X.shape)\n    norm=torch.linalg.norm(X.reshape(-1)-Xref.reshape(-1),ord=2)\n    return norm", "\n\ndef apply_filter_and_norm_STFTmag(X,Xref, H):\n    #X: (N,513, T) \"clean\" example\n    #Xref: (N,513, T)  observations\n    #H: (513,) filter\n\n    #get the absolute value of the STFT\n    X=torch.sqrt(X[...,0]**2+X[...,1]**2)\n    Xref=torch.sqrt(Xref[...,0]**2+Xref[...,1]**2)\n\n    X=X*H.unsqueeze(-1).expand(X.shape)\n    norm=torch.linalg.norm(X.reshape(-1)-Xref.reshape(-1),ord=2)\n    return norm", "\ndef apply_norm_filter(H,H2):\n    norm=torch.linalg.norm(H.reshape(-1)-H2.reshape(-1),ord=2)\n\n    return norm\n\ndef apply_norm_STFT_fweighted(y,den_rec, freq_weight=\"linear\", NFFT=1024):\n    #X: (N,513, T) \"clean\" example\n    #Xref: (N,513, T)  observations\n    #H: (513,) filter\n    X=apply_stft(den_rec, NFFT)\n    Xref=apply_stft(y, NFFT)\n\n    #get the absolute value of the STFT\n    #X=torch.sqrt(X[...,0]**2+X[...,1]**2)\n    #Xref=torch.sqrt(Xref[...,0]**2+Xref[...,1]**2)\n\n    freqs=torch.linspace(0, 1, X.shape[1]).to(X.device).unsqueeze(-1)\n    #print(X.shape, Xref.shape, freqs.shape)\n        \n    #apply frequency weighting to the cost function\n    if freq_weight==\"linear\":\n        X=X*freqs.unsqueeze(-1).expand(X.shape)\n        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)\n    elif freq_weight==\"None\":\n        pass\n    elif freq_weight==\"log\":\n        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape))\n        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape))\n    elif freq_weight==\"sqrt\":\n        X=X*torch.sqrt(freqs.unsqueeze(-1).expand(X.shape))\n        Xref=Xref*torch.sqrt(freqs.unsqueeze(-1).expand(Xref.shape))\n    elif freq_weight==\"log2\":\n        X=X*torch.log2(freqs.unsqueeze(-1).expand(X.shape))\n        Xref=Xref*torch.log2(freqs.unsqueeze(-1).expand(Xref.shape))\n    elif freq_weight==\"log10\":\n        X=X*torch.log10(freqs.unsqueeze(-1).expand(X.shape))\n        Xref=Xref*torch.log10(freqs.unsqueeze(-1).expand(Xref.shape))\n    elif freq_weight==\"cubic\":\n        X=X*freqs.unsqueeze(-1).expand(X.shape)**3\n        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**3\n    elif freq_weight==\"quadratic\":\n        X=X*freqs.unsqueeze(-1).expand(X.shape)**2\n        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**2\n    elif freq_weight==\"logcubic\":\n        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape)**3)\n        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape)**3)\n    elif freq_weight==\"logquadratic\":\n        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape)**2)\n        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape)**2)\n    elif freq_weight==\"squared\":\n        X=X*freqs.unsqueeze(-1).expand(X.shape)**4\n        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**4\n\n    norm=torch.linalg.norm(X.reshape(-1)-Xref.reshape(-1),ord=2)\n    return norm", "def apply_norm_STFTmag_fweighted(y,den_rec, freq_weight=\"linear\", NFFT=1024, logmag=False):\n    #X: (N,513, T) \"clean\" example\n    #Xref: (N,513, T)  observations\n    #H: (513,) filter\n    X=apply_stft(den_rec, NFFT)\n    Xref=apply_stft(y, NFFT)\n\n    #get the absolute value of the STFT\n    X=torch.sqrt(X[...,0]**2+X[...,1]**2)\n    Xref=torch.sqrt(Xref[...,0]**2+Xref[...,1]**2)\n\n    freqs=torch.linspace(0, 1, X.shape[1]).to(X.device)\n    #apply frequency weighting to the cost function\n    if freq_weight==\"linear\":\n        X=X*freqs.unsqueeze(-1).expand(X.shape)\n        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)\n    elif freq_weight==\"None\":\n        pass\n    elif freq_weight==\"log\":\n        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape))\n        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape))\n    elif freq_weight==\"sqrt\":\n        X=X*torch.sqrt(freqs.unsqueeze(-1).expand(X.shape))\n        Xref=Xref*torch.sqrt(freqs.unsqueeze(-1).expand(Xref.shape))\n    elif freq_weight==\"log2\":\n        X=X*torch.log2(freqs.unsqueeze(-1).expand(X.shape))\n        Xref=Xref*torch.log2(freqs.unsqueeze(-1).expand(Xref.shape))\n    elif freq_weight==\"log10\":\n        X=X*torch.log10(freqs.unsqueeze(-1).expand(X.shape))\n        Xref=Xref*torch.log10(freqs.unsqueeze(-1).expand(Xref.shape))\n    elif freq_weight==\"cubic\":\n        X=X*freqs.unsqueeze(-1).expand(X.shape)**3\n        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**3\n    elif freq_weight==\"quadratic\":\n        X=X*freqs.unsqueeze(-1).expand(X.shape)**2\n        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**2\n    elif freq_weight==\"logcubic\":\n        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape)**3)\n        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape)**3)\n    elif freq_weight==\"logquadratic\":\n        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape)**2)\n        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape)**2)\n    elif freq_weight==\"squared\":\n        X=X*freqs.unsqueeze(-1).expand(X.shape)**4\n        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**4\n\n    if logmag==True:\n        norm=torch.linalg.norm(torch.log10(X.reshape(-1)+1e-8)-torch.log10(Xref.reshape(-1)+1e-8),ord=2)\n    else:\n        norm=torch.linalg.norm(X.reshape(-1)-Xref.reshape(-1),ord=2)\n    return norm", "\ndef apply_filter_and_norm_STFTmag_fweighted(X,Xref, H, freq_weight=\"linear\"):\n    #X: (N,513, T) \"clean\" example\n    #Xref: (N,513, T)  observations\n    #H: (513,) filter\n\n    #get the absolute value of the STFT\n    X=torch.sqrt(X[...,0]**2+X[...,1]**2)\n    Xref=torch.sqrt(Xref[...,0]**2+Xref[...,1]**2)\n\n    X=X*H.unsqueeze(-1).expand(X.shape)\n    freqs=torch.linspace(0, 1, X.shape[1]).to(X.device)\n    #apply frequency weighting to the cost function\n    if freq_weight==\"linear\":\n        X=X*freqs.unsqueeze(-1).expand(X.shape)\n        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)\n    elif freq_weight==\"None\":\n        pass\n    elif freq_weight==\"log\":\n        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape))\n        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape))\n    elif freq_weight==\"sqrt\":\n        X=X*torch.sqrt(freqs.unsqueeze(-1).expand(X.shape))\n        Xref=Xref*torch.sqrt(freqs.unsqueeze(-1).expand(Xref.shape))\n    elif freq_weight==\"log2\":\n        X=X*torch.log2(freqs.unsqueeze(-1).expand(X.shape))\n        Xref=Xref*torch.log2(freqs.unsqueeze(-1).expand(Xref.shape))\n    elif freq_weight==\"log10\":\n        X=X*torch.log10(freqs.unsqueeze(-1).expand(X.shape))\n        Xref=Xref*torch.log10(freqs.unsqueeze(-1).expand(Xref.shape))\n    elif freq_weight==\"cubic\":\n        X=X*freqs.unsqueeze(-1).expand(X.shape)**3\n        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**3\n    elif freq_weight==\"quadratic\":\n        X=X*freqs.unsqueeze(-1).expand(X.shape)**2\n        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**2\n    elif freq_weight==\"logcubic\":\n        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape)**3)\n        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape)**3)\n    elif freq_weight==\"logquadratic\":\n        X=X*torch.log2(1+freqs.unsqueeze(-1).expand(X.shape)**2)\n        Xref=Xref*torch.log2(1+freqs.unsqueeze(-1).expand(Xref.shape)**2)\n    elif freq_weight==\"squared\":\n        X=X*freqs.unsqueeze(-1).expand(X.shape)**4\n        Xref=Xref*freqs.unsqueeze(-1).expand(Xref.shape)**4\n\n    norm=torch.linalg.norm(X.reshape(-1)-Xref.reshape(-1),ord=2)\n    return norm", "\ndef plot_filter(ref_filter, est_filter, NFFT=1024, fs=44100):\n    f=torch.fft.rfftfreq(NFFT, d=1/fs).to(ref_filter.device)\n    Href=design_filter(ref_filter[0],ref_filter[1], f)\n    H=design_filter(est_filter[0],est_filter[1], f)\n    fig=px.line(x=f.cpu(),y=20*torch.log10(H.cpu().detach()), log_x=True , title='Frequency response of a low pass filter', labels={'x':'Frequency (Hz)', 'y':'Magnitude (dB)'})\n    #plot the reference frequency response\n    fig.add_scatter(x=f.cpu(),y=20*torch.log10(Href.cpu().detach()), mode='lines', name='Reference')\n    return fig\n", "\n\ndef animation_filter(path, data_filters ,t,NFFT=1024, fs=44100, name=\"animation_filter\",NT=15 ):\n    '''\n    plot an animation of the reverse diffusion process of filters\n    args:\n        path: path to save the animation\n        x: input audio (N,T)\n        t: timesteps (sigma)\n        name: name of the animation\n    '''\n    #print(noisy.shape)\n    f=torch.fft.rfftfreq(NFFT, d=1/fs)\n    Nsteps=data_filters.shape[0]\n    numsteps=min(Nsteps,NT) #hardcoded, I'll probably need more!\n    tt=torch.linspace(0, Nsteps-1, numsteps)\n    i_s=[]\n    allX=None\n    for i in tt:\n        i=int(torch.floor(i))\n        i_s.append(i)\n        X=design_filter(data_filters[i,0],data_filters[i,1], f) # (513,)\n        X=X.unsqueeze(0) #(1,513)\n        if allX==None:\n             allX=X\n        else:\n             allX=torch.cat((allX,X), 0)\n\n    #allX shape is ( 513, numsteps)\n    sigma=t[i_s]\n    #x=x.squeeze(1)# (100,19)\n    print(allX.shape, f.shape, sigma.shape)\n    f=f.unsqueeze(0).expand(allX.shape[0], -1).reshape(-1)\n    sigma=sigma.unsqueeze(-1).expand(-1, allX.shape[1]).reshape(-1)\n    allX=allX.reshape(-1)\n    print(allX.shape, f.shape, sigma.shape)\n    df=pd.DataFrame(\n        {\n            \"f\": f.cpu().numpy(),\n            \"h\": 20*torch.log10(allX.cpu()).numpy(),\n            \"sigma\": sigma.cpu().numpy()\n        }\n    )\n    fig=px.line(df, x=\"f\",y=\"h\", animation_frame=\"sigma\", log_x=True) #I need\n    path_to_plotly_html = path+\"/\"+name+\".html\"\n    \n    fig.write_html(path_to_plotly_html, auto_play = False)\n\n    return fig", "\n"]}
{"filename": "utils/bandwidth_extension.py", "chunked_list": ["import scipy.signal\nimport torch\nimport torchaudio\nimport math\n\n\ndef prepare_filter(args, sample_rate):\n    #design the filter\n    order=args.tester.bandwidth_extension.filter.order\n    fc=args.tester.bandwidth_extension.filter.fc\n\n    if args.tester.bandwidth_extension.filter.type==\"firwin\":\n        beta=args.tester.bandwidth_extension.filter.beta\n        filter=get_FIR_lowpass(order,fc, beta,sample_rate)\n    elif args.tester.bandwidth_extension.filter.type==\"firwin_hpf\":\n        beta=args.tester.bandwidth_extension.filter.beta\n        filter=get_FIR_high_pass(order,fc, beta,sample_rate)\n    elif args.tester.bandwidth_extension.filter.type==\"cheby1\":\n        ripple =args.tester.bandwidth_extension.filter.ripple\n        b,a=get_cheby1_ba(order, ripple, 2*fc/sample_rate) \n        filter=(b,a)\n    elif args.tester.bandwidth_extension.filter.type==\"biquad\":\n        Q =args.tester.bandwidth_extension.filter.biquad.Q\n        parameters=design_biquad_lpf(fc, sample_rate, Q) \n        filter=parameters\n    elif args.tester.bandwidth_extension.filter.type==\"resample\":\n        filter= sample_rate/args.tester.bandwidth_extension.filter.resample.fs\n    elif args.tester.bandwidth_extension.filter.type==\"decimate\":\n        filter= int(args.tester.bandwidth_extension.decimate.factor)\n        args.tester.bandwidth_extension.filter.resample.fs=int(sample_rate/filter)\n\n    elif args.tester.bandwidth_extension.filter.type==\"cheby1filtfilt\":\n        raise NotImplementedError\n    elif args.tester.bandwidth_extension.filter.type==\"butter_fir\":\n        raise NotImplementedError\n    elif args.tester.bandwidth_extension.filter.type==\"cheby1_fir\":\n        raise NotImplementedError\n    else:\n        raise NotImplementedError\n    return filter", "\ndef get_FIR_high_pass(order,fc, beta, sr):\n    \"\"\"\n        This function designs a  FIR high pass filter using the window method. It uses scipy.signal\n        Args:\n            order(int): order of the filter\n            fc (float): cutoff frequency\n            sr (float): sampling rate\n\n        Returns:\n            B (Tensor): shape(1,1,order-1) FIR filter coefficients\n    \"\"\"\n\n    B=scipy.signal.firwin(numtaps=order-1,cutoff=fc, width=beta,window=\"kaiser\", fs=sr, pass_zero=\"highpass\")\n    B=torch.FloatTensor(B)\n    B=B.unsqueeze(0)\n    B=B.unsqueeze(0)\n    return B", "def get_FIR_lowpass(order,fc, beta, sr):\n    \"\"\"\n        This function designs a FIR low pass filter using the window method. It uses scipy.signal\n        Args:\n            order(int): order of the filter\n            fc (float): cutoff frequency\n            sr (float): sampling rate\n        Returns:\n            B (Tensor): shape(1,1,order) FIR filter coefficients\n    \"\"\"\n\n    B=scipy.signal.firwin(numtaps=order,cutoff=fc, width=beta,window=\"kaiser\", fs=sr)\n    B=torch.FloatTensor(B)\n    B=B.unsqueeze(0)\n    B=B.unsqueeze(0)\n    return B", "\ndef apply_low_pass_firwin(y,filter):\n    \"\"\"\n        Utility for applying a FIR filter, usinf pytorch conv1d\n        Args;\n            y (Tensor): shape (B,T) signal to filter\n            filter (Tensor): shape (1,1,order) FIR filter coefficients\n        Returns:\n            y_lpf (Tensor): shape (B,T) filtered signal\n    \"\"\"\n\n    #ii=2\n    B=filter.to(y.device)\n    #B=filter\n    y=y.unsqueeze(1)\n    #weight=torch.nn.Parameter(B)\n    \n    y_lpf=torch.nn.functional.conv1d(y,B,padding=\"same\")\n    y_lpf=y_lpf.squeeze(1) #some redundancy here, but its ok\n    #y_lpf=y\n    return y_lpf", "\ndef apply_decimate(y,factor):\n    \"\"\"\n        Function for applying a naive decimation for downsampling\n        Args:\n            y (Tensor): shape (B,T)\n            factor (int): decimation factor\n        Returns\n            y (Tensor): shape (B,T//factor)\n    \"\"\"\n\n    factor=factor\n    return y[...,0:-1:factor]", "\ndef apply_resample(y,factor):\n    \"\"\"\n        Applies torch's resmpling function\n        Args:\n            y (Tensor): shape (B,T)\n            factor (float): resampling factor\n    \"\"\"\n    N=100 \n    return torchaudio.functional.resample(y,orig_freq=int(factor*N),new_freq=N )", "\ndef apply_low_pass_biquad(y,filter):\n    \"\"\"\n        Applies torchaudio's biquad filter\n        Args:\n            y (Tensor): shape (B,T)\n            filter (tuple): biquad filter coefficients\n        Returns:        \n            y_lpf (Tensor) : shape (B,T) filtered signal\n    \"\"\"\n    b0,b1,b2,a0,a1,a2=filter\n    b0=torch.Tensor(b0).to(y.device)\n    b1=torch.Tensor(b1).to(y.device)\n    b2=torch.Tensor(b2).to(y.device)\n    a0=torch.Tensor(a0).to(y.device)\n    a1=torch.Tensor(a1).to(y.device)\n    a2=torch.Tensor(a2).to(y.device)\n    y_lpf=torchaudio.functional.biquad(y, b0,b1,b2,a0,a1,a2)\n    return y_lpf", "def apply_low_pass_IIR(y,filter):\n    b,a=filter\n    b=torch.Tensor(b).to(y.device)\n    a=torch.Tensor(a).to(y.device)\n    y_lpf=torchaudio.functional.lfilter(y, a,b, clamp=False)\n    return y_lpf\n\ndef apply_low_pass(y,filter, type):\n    \"\"\"\n        Meta-function for applying a lowpass filter, maps y to another function depending on the type\n        Args:\n           y (Tensors): shape (B,T)\n           filter (whatever): filter coefficients, or whatever that specifies the filter\n           type (string): specifier of the type of filter\n        Returns\n           y_lpf (Tensor): shape (B,,T) foltered signal\n    \"\"\"\n\n    if type==\"firwin\":\n        return apply_low_pass_firwin(y,filter)\n    if type==\"firwin_hpf\":\n        return apply_low_pass_firwin(y,filter)\n    elif type==\"cheby1\":\n        return apply_low_pass_IIR(y,filter)\n    elif type==\"biquad\":\n        return apply_low_pass_biquad(y,filter)\n    elif type==\"resample\":\n        return apply_resample(y,filter)\n    elif type==\"decimate\":\n        return apply_decimate(y,filter)", "\ndef get_cheby1_ba(order, ripple,hi ):\n    \"\"\"\n        Utility for designing a chebyshev type I IIR lowpass filter\n        Args:\n           order, ripple, hi: (see scipy.signal.cheby1 documentation)\n        Returns:\n           b,a: filter coefficients\n    \"\"\"\n    b,a = scipy.signal.cheby1(order, ripple, hi, btype='lowpass', output='ba')\n    return b,a", "\ndef design_biquad_lpf(fc, fs, Q):\n    \"\"\"\n        utility for designing a biqad lowpass filter\n        Args:\n            fc (float): cutoff frequency\n            fs (float): sampling frequency\n            Q (float):  Q-factor\n    \"\"\"\n    w0 = 2 * math.pi * fc / fs\n    w0 = torch.as_tensor(w0, dtype=torch.float32)\n    alpha = torch.sin(w0) / 2 / Q\n\n    b0 = (1 - torch.cos(w0)) / 2\n    b1 = 1 - torch.cos(w0)\n    b2 = b0\n    a0 = 1 + alpha\n    a1 = -2 * torch.cos(w0)\n    a2 = 1 - alpha\n    return b0, b1, b2, a0, a1, a2", "\n"]}
{"filename": "utils/logging.py", "chunked_list": ["import os\nimport torch \nimport time\nimport numpy as np\n#import torchaudio\nimport plotly.express as px\nimport soundfile as sf\n#import plotly.graph_objects as go\nimport pandas as pd\nimport plotly", "import pandas as pd\nimport plotly\nimport scipy.signal as sig\nimport plotly.graph_objects as go\n\n\"\"\"\nLogging related functions that I wrote for my own use\nThis is quite a mess, but I'm too lazy to clean it up\n\"\"\"\n#from src.CQT_nsgt import CQT_cpx", "\"\"\"\n#from src.CQT_nsgt import CQT_cpx\n\ndef do_stft(noisy, clean=None, win_size=2048, hop_size=512, device=\"cpu\", DC=True):\n    \"\"\"\n        applies the stft, this an ugly old function, but I'm using it for logging and I'm to lazy to modify it\n    \"\"\"\n    \n    #window_fn = tf.signal.hamming_window\n\n    #win_size=args.stft.win_size\n    #hop_size=args.stft.hop_size\n    window=torch.hamming_window(window_length=win_size)\n    window=window.to(noisy.device)\n    noisy=torch.cat((noisy, torch.zeros(noisy.shape[0],win_size).to(noisy.device)),1)\n    stft_signal_noisy=torch.stft(noisy, win_size, hop_length=hop_size,window=window,center=False,return_complex=False)\n    stft_signal_noisy=stft_signal_noisy.permute(0,3,2,1)\n    #stft_signal_noisy=tf.signal.stft(noisy,frame_length=win_size, window_fn=window_fn, frame_step=hop_size)\n    #stft_noisy_stacked=tf.stack( values=[tf.math.real(stft_signal_noisy), tf.math.imag(stft_signal_noisy)], axis=-1)\n    \n    if clean!=None:\n\n       # stft_signal_clean=tf.signal.stft(clean,frame_length=win_size, window_fn=window_fn, frame_step=hop_size)\n        clean=torch.cat((clean, torch.zeros(clean.shape[0],win_size).to(device)),1)\n        stft_signal_clean=torch.stft(clean, win_size, hop_length=hop_size,window=window, center=False,return_complex=False)\n        stft_signal_clean=stft_signal_clean.permute(0,3,2,1)\n        #stft_clean_stacked=tf.stack( values=[tf.math.real(stft_signal_clean), tf.math.imag(stft_signal_clean)], axis=-1)\n\n\n        if DC:\n            return stft_signal_noisy, stft_signal_clean\n        else:\n            return stft_signal_noisy[...,1:], stft_signal_clean[...,1:]\n    else:\n\n        if DC:\n            return stft_signal_noisy\n        else:\n            return stft_signal_noisy[...,1:]", "\ndef plot_norms(path, normsscores, normsguides, t, name):\n    values=t.cpu().numpy()\n     \n    df=pd.DataFrame.from_dict(\n                {\"sigma\": values[0:-1], \"score\": normsscores.cpu().numpy(), \"guidance\": normsguides.cpu().numpy()\n                }\n                )\n\n    fig= px.line(df, x=\"sigma\", y=[\"score\", \"guidance\"],log_x=True,  log_y=True, markers=True)\n\n\n    path_to_plotly_html = path+\"/\"+name+\".html\"\n    \n    fig.write_html(path_to_plotly_html, auto_play = False)\n    return fig", "    \n\ndef plot_spectral_analysis_sampling( avgspecNF,avgspecDEN, ts, fs=22050, nfft=1024):\n    T,F=avgspecNF.shape\n    f=torch.arange(0, F)*fs/nfft\n    f=f.unsqueeze(1).repeat(1,T).view(-1)\n    avgspecNF=avgspecNF.permute(1,0).reshape(-1)\n    avgspecDEN=avgspecDEN.permute(1,0).reshape(-1)\n    ts=ts.cpu().numpy().tolist()\n    ts=513*ts\n    #ts=np.repeat(ts,513)\n    print(f.shape, avgspecDEN.shape, avgspecNF.shape, len(ts))\n    df=pd.DataFrame.from_dict(\n        {\"f\": f,  \"noisy\": avgspecNF, \"denoised\":  avgspecDEN, \"sigma\":ts\n        }\n        )\n\n    fig= px.line(df, x=\"f\", y=[\"noisy\", \"denoised\"],  animation_frame=\"sigma\", log_x=False,log_y=False,  markers=False)\n    return fig", "\ndef plot_spectral_analysis(avgspecY, avgspecNF,avgspecDEN, ts, fs=22050, nfft=1024):\n    T,F=avgspecNF.shape\n    f=torch.arange(0, F)*fs/nfft\n    f=f.unsqueeze(1).repeat(1,T).view(-1)\n    avgspecY=avgspecY.squeeze(0).unsqueeze(1).repeat(1,T).view(-1)\n    avgspecNF=avgspecNF.permute(1,0).reshape(-1)\n    avgspecDEN=avgspecDEN.permute(1,0).reshape(-1)\n    ts=ts.cpu().numpy().tolist()\n    ts=513*ts\n    #ts=np.repeat(ts,513)\n    print(f.shape, avgspecY.shape, avgspecNF.shape, len(ts))\n    df=pd.DataFrame.from_dict(\n        {\"f\": f, \"y\": avgspecY, \"noisy\": avgspecNF, \"denoised\":  avgspecDEN, \"sigma\":ts\n        }\n        )\n\n    fig= px.line(df, x=\"f\", y=[\"y\", \"noisy\", \"denoised\"],  animation_frame=\"sigma\", log_x=False,log_y=False,  markers=False)\n    return fig", "\ndef plot_loss_by_sigma_test_snr(average_snr, average_snr_out, t):\n    #write a fancy plot to log in wandb\n    values=np.array(t)\n    df=pd.DataFrame.from_dict(\n                {\"sigma\": values, \"SNR\": average_snr, \"SNR_denoised\": average_snr_out\n                }\n                )\n\n    fig= px.line(df, x=\"sigma\", y=[\"SNR\", \"SNR_denoised\"],log_x=True,  markers=True)\n    return fig", "    \n\n\n\n# Create and style traces\n\ndef plot_loss_by_sigma(sigma_means, sigma_stds, sigma_bins):\n    df=pd.DataFrame.from_dict(\n                {\"sigma\": sigma_bins, \"loss\": sigma_means, \"std\": sigma_stds\n                }\n                )\n\n    fig= error_line('bar', data_frame=df, x=\"sigma\", y=\"loss\", error_y=\"std\", log_x=True,  markers=True, range_y=[0, 2])\n    \n    return fig", "\ndef plot_loss_by_sigma_and_freq(sigma_freq_means, sigma_freq_stds, sigma_bins, freq_bins):\n    df=pd.DataFrame.from_dict(\n                {\"sigma\": np.tile(sigma_bins, len(freq_bins))}) \n    names=[]\n    means=[]\n    stds=[]\n    for i in range(len(freq_bins)):\n        name=[str(freq_bins[i])+\"Hz\" for j in range(len(sigma_bins))]\n        names.extend(name)\n        means.extend(sigma_freq_means[i])\n        stds.extend(sigma_freq_stds[i])\n    df['freq']=names\n    df['means']=means\n    df['stds']=stds\n    print(df)\n\n\n    #basically, we want to plot the loss as a function of sigma and freq. We do it by plotting a line for each freq.\n    fig= error_line('bar', data_frame=df, x=\"sigma\", y=\"means\", error_y=\"stds\", color=\"freq\", log_x=True,  markers=True, range_y=[0, 2])\n    \n    return fig", "\n\n\ndef plot_melspectrogram(X, refr=1):\n    X=X.squeeze(1) #??\n    X=X.cpu().numpy()\n    if refr==None:\n        refr=np.max(np.abs(X))+1e-8\n     \n    S_db = 10*np.log10(np.abs(X)/refr)\n\n    S_db=np.transpose(S_db, (0,2,1))\n    S_db=np.flip(S_db, axis=1)\n\n    for i in range(X.shape[0]): #iterate over batch size, shity way of ploting all the batched spectrograms\n        o=S_db[i]\n        if i==0:\n             res=o\n        else:\n             res=np.concatenate((res,o), axis=1)\n      \n    fig=px.imshow( res,  zmin=-40, zmax=20)\n\n    fig.update_layout(coloraxis_showscale=False)\n\n    return fig", "\ndef plot_cpxspectrogram(X):\n    X=X.squeeze(1)\n    X=X.cpu().numpy()\n    #Xre=X[...,0]\n    #Xim=X[...,1]\n    #X=np.sqrt(X[:,:,:,0]**2 + X[:,:,:,1]**2)\n    #if refr==None:\n    #    refr=np.max(np.abs(X))+1e-8\n     \n    #S_db = 10*np.log10(np.abs(X)/refr)\n\n    #S_db=np.transpose(S_db, (0,2,1))\n    #S_db=np.flip(S_db, axis=1)\n\n    #for i in range(X.shape[0]): #iterate over batch size, shity way of ploting all the batched spectrograms\n    #    o=S_db[i]\n    #    if i==0:\n    #         res=o\n    #    else:\n    #         res=np.concatenate((res,o), axis=1)\n      \n    fig=px.imshow(X, facet_col=3, animation_frame=0)\n\n    fig.update_layout(coloraxis_showscale=False)\n\n    return fig", "def print_cuda_memory():\n    t = torch.cuda.get_device_properties(0).total_memory\n    r = torch.cuda.memory_reserved(0)\n    a = torch.cuda.memory_allocated(0)\n    f = r-a  # free inside reservedk\n    print(\"memrylog\",t,r,a,f)\n\ndef plot_spectrogram(X, refr=None):\n    X=X.squeeze(1)\n    X=X.cpu().numpy()\n    X=np.sqrt(X[:,:,:,0]**2 + X[:,:,:,1]**2)\n    if refr==None:\n        refr=np.max(np.abs(X))+1e-8\n     \n    S_db = 10*np.log10(np.abs(X)/refr)\n\n    S_db=np.transpose(S_db, (0,2,1))\n    S_db=np.flip(S_db, axis=1)\n\n    for i in range(X.shape[0]): #iterate over batch size, shity way of ploting all the batched spectrograms\n        o=S_db[i]\n        if i==0:\n             res=o\n        else:\n             res=np.concatenate((res,o), axis=1)\n      \n    fig=px.imshow( res,  zmin=-40, zmax=20)\n\n    fig.update_layout(coloraxis_showscale=False)\n\n    return fig", "\n\n\ndef plot_mag_spectrogram(X, refr=None, path=None,name=\"spec\"):\n    #X=X.squeeze(1)\n    X=X.cpu().numpy()\n    #X=np.sqrt(X[:,:,:,0]**2 + X[:,:,:,1]**2)\n    if refr==None:\n        refr=np.max(np.abs(X))+1e-8\n     \n    S_db = 10*np.log10(np.abs(X)/refr)\n\n    #S_db=np.transpose(S_db, (0,2,1))\n    S_db=np.flip(S_db, axis=1)\n\n    for i in range(X.shape[0]): #iterate over batch size, shity way of ploting all the batched spectrograms\n        o=S_db[i]\n        if i==0:\n             res=o\n        else:\n             res=np.concatenate((res,o), axis=1)\n      \n    fig=px.imshow( res,  zmin=-40, zmax=20)\n\n    fig.update_layout(coloraxis_showscale=False)\n\n    path_to_plotly_png = path+\"/\"+name+\".png\"\n    plotly.io.write_image(fig, path_to_plotly_png)\n    \n    return fig", "def plot_spectrogram(X, refr=None):\n    X=X.squeeze(1)\n    X=X.cpu().numpy()\n    X=np.sqrt(X[:,:,:,0]**2 + X[:,:,:,1]**2)\n    if refr==None:\n        refr=np.max(np.abs(X))+1e-8\n     \n    S_db = 10*np.log10(np.abs(X)/refr)\n\n    S_db=np.transpose(S_db, (0,2,1))\n    S_db=np.flip(S_db, axis=1)\n\n    for i in range(X.shape[0]): #iterate over batch size, shity way of ploting all the batched spectrograms\n        o=S_db[i]\n        if i==0:\n             res=o\n        else:\n             res=np.concatenate((res,o), axis=1)\n      \n    fig=px.imshow( res,  zmin=-40, zmax=20)\n\n    fig.update_layout(coloraxis_showscale=False)\n\n    return fig", "\ndef write_audio_file(x, sr, string: str, path='tmp', stereo=False):\n    if not(os.path.exists(path)): \n        os.makedirs(path)\n      \n    path=os.path.join(path,string+\".wav\")\n    if stereo:\n        '''\n        x has shape (B,2,T)\n        '''\n        x=x.permute(0,2,1) #B,T,2\n        x=x.flatten(0,1) #B*T,2\n        x=x.cpu().numpy()\n        #if np.abs(np.max(x))>=1:\n        #    #normalize to avoid clipping\n        #    x=x/np.abs(np.max(x))\n    else:\n        x=x.flatten()\n        x=x.unsqueeze(1)\n        x=x.cpu().numpy()\n        #if np.abs(np.max(x))>=1:\n        #    #normalize to avoid clipping\n        #    x=x/np.abs(np.max(x))\n    sf.write(path,x,sr)\n    return path", "\ndef plot_trajectories(x,sr, sigma_data):\n    \"\"\"\n    x: (B,T) Batch of trajectories\n    sr: sampling rate\n    args: args object\n    \"\"\"\n    x=x.cpu()\n    times=np.arange(x.shape[1])/sr\n\n    fig=plot_batch_of_lines(x/sigma_data, times, log_x=False)\n    return fig", "\ndef plot_trajectories_fft(x, sr, sigma_data):\n    \"\"\"\n    x: (B,T) Batch of trajectories\n    sr: sampling rate\n    args: args object\n\n    Apply fft to the trajectories and plot the magnitude of the fft\n    \"\"\"\n    x=x.cpu()\n\n    times=np.arange(x.shape[1])/sr\n    x=x/sigma_data\n    x_fft=np.fft.rfft(x, axis=1)\n    x_fft=np.abs(x_fft)\n    freqs=np.fft.rfftfreq(x.shape[1], d=1/sr)\n    #now do it on db scale\n    x_fft_db=10*np.log10(x_fft)\n    fig=plot_batch_of_lines(x_fft_db, freqs, log_x=False)\n    return fig\n    return fig", "\n\ndef plot_cpxCQT_from_raw_audio(x, args, refr=None ):\n    #shape of input spectrogram:  (     ,T,F, )\n    fmax=args.sample_rate/2\n    fmin=fmax/(2**args.cqt.numocts)\n    fbins=int(args.cqt.binsoct*args.cqt.numocts) \n    device=x.device\n    CQTransform=CQT_cpx(fmin,fbins, args.sample_rate, args.audio_len, device=device, split_0_nyq=False)\n\n    x=x\n    X=CQTransform.fwd(x)\n    return plot_cpxspectrogram(X)", "\ndef plot_CQT_from_raw_audio(x, args, refr=None ):\n    #shape of input spectrogram:  (     ,T,F, )\n    fmax=args.sample_rate/2\n    fmin=fmax/(2**args.cqt.numocts)\n    fbins=int(args.cqt.binsoct*args.cqt.numocts) \n    device=x.device\n    CQTransform=CQT_cpx(fmin,fbins, args.sample_rate, args.audio_len, device=device, split_0_nyq=False)\n\n    refr=3\n    x=x\n    X=CQTransform.fwd(x)\n    return plot_spectrogram(X, refr)", "\ndef get_spectrogram_from_raw_audio(x, stft, refr=1):\n    X=do_stft(x, win_size=stft.win_size, hop_size=stft.hop_size)\n    X=X.permute(0,2,3,1)\n\n    X=X.squeeze(1)\n    #X=X.cpu().numpy()\n    X=torch.sqrt(X[:,:,:,0]**2 + X[:,:,:,1]**2)\n\n    #if refr==None:\n    #    refr=np.max(np.abs(X))+1e-8\n     \n    S_db = 10*torch.log10(torch.abs(X)/refr)\n\n    S_db=S_db.permute(0,2,1)\n    #np.transpose(S_db, (0,2,1))\n    S_db=torch.flip(S_db, [1])\n\n    for i in range(X.shape[0]): #iterate over batch size, shity way of ploting all the batched spectrograms\n        o=S_db[i]\n        if i==0:\n             res=o\n        else:\n             res=torch.cat((res,o), 1)\n    return res", "\ndef downsample2d(inputArray, kernelSize):\n    \"\"\"This function downsamples a 2d numpy array by convolving with a flat\n    kernel and then sub-sampling the resulting array.\n    A kernel size of 2 means convolution with a 2x2 array [[1, 1], [1, 1]] and\n    a resulting downsampling of 2-fold.\n    :param: inputArray: 2d numpy array\n    :param: kernelSize: integer\n    \"\"\"\n    average_kernel = np.ones((kernelSize,kernelSize))\n\n    blurred_array = sig.convolve2d(inputArray, average_kernel, mode='same')\n    downsampled_array = blurred_array[::kernelSize,::kernelSize]\n    return downsampled_array", "\n\ndef diffusion_CQT_animation(path, x ,t,  args, refr=1, name=\"animation_diffusion\", resample_factor=1 ):\n    \"\"\"\n        Utility for creating an animation of the cqt diffusion process\n    \"\"\"\n    #shape of input spectrograms:  (Nsteps,B,Time,Freq)\n    #print(noisy.shape)\n    Nsteps=x.shape[0]\n    numsteps=10\n    tt=torch.linspace(0, Nsteps-1, numsteps)\n    i_s=[]\n    allX=None\n    device=x.device\n\n    fmax=args.sample_rate/2\n    fmin=fmax/(2**args.cqt.numocts)\n    fbins=int(args.cqt.binsoct*args.cqt.numocts) \n    CQTransform=CQT_cpx(fmin,fbins, args.sample_rate, args.audio_len, device=device, split_0_nyq=False)\n\n    for i in tt:\n        i=int(torch.floor(i))\n        i_s.append(i)\n        xx=x[i]\n        X=CQTransform.fwd(xx)\n\n        X=torch.sqrt(X[...,0]**2 + X[...,1]**2)\n\n        S_db = 10*torch.log10(torch.abs(X)/refr)\n        S_db = S_db[:,1:-1,1:-1]\n       \n        S_db=S_db.permute(0,2,1)\n        #np.transpose(S_db, (0,2,1))\n        S_db=torch.flip(S_db, [1])\n        S_db=S_db.unsqueeze(0)\n        S_db=torch.nn.functional.interpolate(S_db, size= (S_db.shape[2]//resample_factor, S_db.shape[3]//resample_factor), mode=\"bilinear\")\n        S_db=S_db.squeeze(0)\n        S_db=S_db.cpu().numpy()\n        #S_db=S_db.squeeze(0)        \n\n        if i==0:\n             allX=S_db\n        else:\n             allX=np.concatenate((allX,S_db), 0)\n    \n    #fig=px.imshow(S_db, animation_frame=0, facet_col=3, binary_compression_level=0) #I need to incorporate t here!!!\n    fig=px.imshow(allX, animation_frame=0,  zmin=-40, zmax=10, binary_compression_level=0) #I need to incorporate t here!!!\n\n    fig.update_layout(coloraxis_showscale=False)\n    fig.update_xaxes(showticklabels=False)\n    fig.update_yaxes(showticklabels=False)\n    \n    t=t[i_s].cpu().numpy()\n\n    assert len(t)==len(fig.frames), print(len(t), len(fig.frames))\n    #print(fig)\n    for i, f in enumerate(fig.layout.sliders[0].steps):\n        #hacky way of changing the axis values\n        #f.args[0]=[str(t[i])]\n        f.label=str(t[i])\n\n    path_to_plotly_html = path+\"/\"+name+\".html\"\n    \n    fig.write_html(path_to_plotly_html, auto_play = False)\n\n\n    return fig", "\ndef diffusion_joint_filter_animation(path, score, grads , f, t,  refr=1, name=\"animation_diffusion\", NT=15 ):\n    '''\n    plot an animation of the reverse diffusion process of filters\n    args:\n        path: path to save the animation\n        x: filters (Nsteps, F)\n        f: frequencies (F)\n        t: timesteps (sigma)\n        name: name of the animation\n    '''\n    #shape of input spectrograms:  (Nsteps,B,Time,Freq)\n    #print(noisy.shape)\n    Nsteps=score.shape[0]\n    #numsteps=min(Nsteps,NT) #hardcoded, I'll probably need more!\n\n    #tt=torch.linspace(0, Nsteps-1, numsteps)\n    #i_s=[]\n    #for i in tt:\n    #    i=int(torch.floor(i))\n    #    i_s.append(i)\n      \n    print(f.shape, score.shape, grads.shape, t.shape)  #19, (100,19)\n    #fig=px.line(x=f.unsqueeze(0).numpy(),y=x, animation_frame=0) #I need to incorporate t here!!!\n    sigma=t\n    s=score.squeeze(1)# (100,19)\n    g=grads.squeeze(1)# (100,19)\n    f=f# (19,)\n    f=f.unsqueeze(0).expand(s.shape[0], -1).reshape(-1)\n    sigma=sigma.unsqueeze(-1).expand(-1, s.shape[1]).reshape(-1)\n    s=s.reshape(-1)\n    g=g.reshape(-1)\n    max_score=torch.max(s).item()\n    df=pd.DataFrame(\n        {\n            \"f\": f.cpu().numpy(),\n            \"score\": s.cpu().numpy(),\n            \"grads\": g.cpu().numpy(),\n            \"sigma\": sigma.cpu().numpy()\n        }\n    )\n    fig=px.line(df, x=\"f\",y=[\"score\",\"grads\"], animation_frame=\"sigma\", log_x=True) #I need to incorporate t here!!!\n    #set the range of th y axis\n    fig.update_yaxes(range=[-max_score,max_score])\n    #t=t[i_s].cpu().numpy()\n    #t=t.cpu().numpy()\n       \n    #assert len(t)==len(fig.frames), print(len(t), len(fig.frames))\n    #print(fig)\n    #for i, f in enumerate(fig.layout.sliders[0].steps):\n        #hacky way of changing the axis values\n        #f.args[0]=[str(t[i])]\n    #    f.label=str(t[i])\n\n    path_to_plotly_html = path+\"/\"+name+\".html\"\n    \n    fig.write_html(path_to_plotly_html, auto_play = False)\n\n    return fig", "\ndef diffusion_filter_animation(path, x , f, t,  refr=1, name=\"animation_diffusion\", NT=15 ):\n    '''\n    plot an animation of the reverse diffusion process of filters\n    args:\n        path: path to save the animation\n        x: filters (Nsteps, F)\n        f: frequencies (F)\n        t: timesteps (sigma)\n        name: name of the animation\n    '''\n    #shape of input spectrograms:  (Nsteps,B,Time,Freq)\n    #print(noisy.shape)\n    Nsteps=x.shape[0]\n    #numsteps=min(Nsteps,NT) #hardcoded, I'll probably need more!\n\n    #tt=torch.linspace(0, Nsteps-1, numsteps)\n    #i_s=[]\n    #for i in tt:\n    #    i=int(torch.floor(i))\n    #    i_s.append(i)\n      \n    print(f.shape, x.shape, t.shape)  #19, (100,19)\n    #fig=px.line(x=f.unsqueeze(0).numpy(),y=x, animation_frame=0) #I need to incorporate t here!!!\n    sigma=t\n    x=x.squeeze(1)# (100,19)\n    f=f# (19,)\n    f=f.unsqueeze(0).expand(x.shape[0], -1).reshape(-1)\n    sigma=sigma.unsqueeze(-1).expand(-1, x.shape[1]).reshape(-1)\n    x=x.reshape(-1)\n    df=pd.DataFrame(\n        {\n            \"f\": f.cpu().numpy(),\n            \"x\": x.cpu().numpy(),\n            \"sigma\": sigma.cpu().numpy()\n        }\n    )\n    fig=px.line(df, x=\"f\",y=\"x\", animation_frame=\"sigma\", log_x=True) #I need to incorporate t here!!!\n    #t=t[i_s].cpu().numpy()\n    #t=t.cpu().numpy()\n       \n    #assert len(t)==len(fig.frames), print(len(t), len(fig.frames))\n    #print(fig)\n    #for i, f in enumerate(fig.layout.sliders[0].steps):\n        #hacky way of changing the axis values\n        #f.args[0]=[str(t[i])]\n    #    f.label=str(t[i])\n\n    path_to_plotly_html = path+\"/\"+name+\".html\"\n    \n    fig.write_html(path_to_plotly_html, auto_play = False)\n\n    return fig", "\ndef diffusion_spec_animation(path, x ,t,  stft, refr=1, name=\"animation_diffusion\",NT=15 ):\n    '''\n    plot an animation of the reverse diffusion process of filters\n    args:\n        path: path to save the animation\n        x: input audio (N,T)\n        t: timesteps (sigma)\n        name: name of the animation\n    '''\n    #print(noisy.shape)\n    Nsteps=x.shape[0]\n    numsteps=min(Nsteps,NT) #hardcoded, I'll probably need more!\n    tt=torch.linspace(0, Nsteps-1, numsteps)\n    i_s=[]\n    allX=None\n    for i in tt:\n        i=int(torch.floor(i))\n        i_s.append(i)\n        X=get_spectrogram_from_raw_audio(x[i],stft, refr)\n        X=X.unsqueeze(0)\n        if allX==None:\n             allX=X\n        else:\n             allX=torch.cat((allX,X), 0)\n\n      \n    allX=allX.cpu().numpy()\n    fig=px.imshow(allX, animation_frame=0,  zmin=-40, zmax=20) #I need to incorporate t here!!!\n\n    fig.update_layout(coloraxis_showscale=False)\n    \n    t=t[i_s].cpu().numpy()\n       \n    assert len(t)==len(fig.frames), print(len(t), len(fig.frames))\n    #print(fig)\n    for i, f in enumerate(fig.layout.sliders[0].steps):\n        #hacky way of changing the axis values\n        #f.args[0]=[str(t[i])]\n        f.label=str(t[i])\n\n    path_to_plotly_html = path+\"/\"+name+\".html\"\n    \n    fig.write_html(path_to_plotly_html, auto_play = False)\n\n\n    return fig", "\ndef plot_spectrogram_from_raw_audio(x, stft, refr=None ):\n    #shape of input spectrogram:  (     ,T,F, )\n    refr=3\n    x=x\n    X=do_stft(x, win_size=stft.win_size, hop_size=stft.hop_size)\n    X=X.permute(0,2,3,1)\n    return plot_spectrogram(X, refr)\n\ndef plot_spectrogram_from_cpxspec(X, refr=None):\n    #shape of input spectrogram:  (     ,T,F, )\n    return plot_spectrogram(X, refr)", "\ndef plot_spectrogram_from_cpxspec(X, refr=None):\n    #shape of input spectrogram:  (     ,T,F, )\n    return plot_spectrogram(X, refr)\n\n\ndef error_line(error_y_mode='band', **kwargs):\n    \"\"\"Extension of `plotly.express.line` to use error bands.\"\"\"\n    ERROR_MODES = {'bar','band','bars','bands',None}\n    if error_y_mode not in ERROR_MODES:\n        raise ValueError(f\"'error_y_mode' must be one of {ERROR_MODES}, received {repr(error_y_mode)}.\")\n    if error_y_mode in {'bar','bars',None}:\n        fig = px.line(**kwargs)\n    elif error_y_mode in {'band','bands'}:\n        if 'error_y' not in kwargs:\n            raise ValueError(f\"If you provide argument 'error_y_mode' you must also provide 'error_y'.\")\n        figure_with_error_bars = px.line(**kwargs)\n        fig = px.line(**{arg: val for arg,val in kwargs.items() if arg != 'error_y'})\n        for data in figure_with_error_bars.data:\n            x = list(data['x'])\n            y_upper = list(data['y'] + data['error_y']['array'])\n            y_lower = list(data['y'] - data['error_y']['array'] if data['error_y']['arrayminus'] is None else data['y'] - data['error_y']['arrayminus'])\n            color = f\"rgba({tuple(int(data['line']['color'].lstrip('#')[i:i+2], 16) for i in (0, 2, 4))},.3)\".replace('((','(').replace('),',',').replace(' ','')\n            fig.add_trace(\n                go.Scatter(\n                    x = x+x[::-1],\n                    y = y_upper+y_lower[::-1],\n                    fill = 'toself',\n                    fillcolor = color,\n                    line = dict(\n                        color = 'rgba(255,255,255,0)'\n                    ),\n                    hoverinfo = \"skip\",\n                    showlegend = False,\n                    legendgroup = data['legendgroup'],\n                    xaxis = data['xaxis'],\n                    yaxis = data['yaxis'],\n                )\n            )\n        # Reorder data as said here: https://stackoverflow.com/a/66854398/8849755\n        reordered_data = []\n        for i in range(int(len(fig.data)/2)):\n            reordered_data.append(fig.data[i+int(len(fig.data)/2)])\n            reordered_data.append(fig.data[i])\n        fig.data = tuple(reordered_data)\n    return fig", "\n\ndef plot_filters( x, freqs):\n    '''\n    This function plots a batch of lines using plotly\n    args:\n        x: (B, F)\n    '''\n    fig = px.line(x=freqs, y=x[0,:], log_x=True)\n\n    for i in range(1,x.shape[0]):\n        fig.add_trace(go.Scatter(x=freqs, y=x[i,:]))\n    return fig", "\ndef plot_batch_of_lines( x, freqs, log_x=True):\n    '''\n    This function plots a batch of lines using plotly\n    args:\n        x: (B, F)\n    '''\n    fig = px.line(x=freqs, y=x[0,:], log_x=log_x)\n\n    for i in range(1,x.shape[0]):\n        fig.add_trace(go.Scatter(x=freqs, y=x[i,:]))\n    return fig", ""]}
{"filename": "utils/dnnlib/__init__.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\nfrom .util import EasyDict, make_cache_dir_path, call_func_by_name\n", ""]}
{"filename": "utils/dnnlib/util.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Miscellaneous utility classes and functions.\"\"\"\n\nimport ctypes", "\nimport ctypes\nimport fnmatch\nimport importlib\nimport inspect\nimport numpy as np\nimport os\nimport shutil\nimport sys\nimport types", "import sys\nimport types\nimport io\nimport pickle\nimport re\nimport requests\nimport html\nimport hashlib\nimport glob\nimport tempfile", "import glob\nimport tempfile\nimport urllib\nimport urllib.request\nimport uuid\n\nfrom distutils.util import strtobool\nfrom typing import Any, List, Tuple, Union, Optional\n\n", "\n\n# Util classes\n# ------------------------------------------------------------------------------------------\n\n\nclass EasyDict(dict):\n    \"\"\"Convenience class that behaves like a dict but allows access with the attribute syntax.\"\"\"\n\n    def __getattr__(self, name: str) -> Any:\n        try:\n            return self[name]\n        except KeyError:\n            raise AttributeError(name)\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        self[name] = value\n\n    def __delattr__(self, name: str) -> None:\n        del self[name]", "\n\nclass Logger(object):\n    \"\"\"Redirect stderr to stdout, optionally print stdout to a file, and optionally force flushing on both stdout and the file.\"\"\"\n\n    def __init__(self, file_name: Optional[str] = None, file_mode: str = \"w\", should_flush: bool = True):\n        self.file = None\n\n        if file_name is not None:\n            self.file = open(file_name, file_mode)\n\n        self.should_flush = should_flush\n        self.stdout = sys.stdout\n        self.stderr = sys.stderr\n\n        sys.stdout = self\n        sys.stderr = self\n\n    def __enter__(self) -> \"Logger\":\n        return self\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        self.close()\n\n    def write(self, text: Union[str, bytes]) -> None:\n        \"\"\"Write text to stdout (and a file) and optionally flush.\"\"\"\n        if isinstance(text, bytes):\n            text = text.decode()\n        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n            return\n\n        if self.file is not None:\n            self.file.write(text)\n\n        self.stdout.write(text)\n\n        if self.should_flush:\n            self.flush()\n\n    def flush(self) -> None:\n        \"\"\"Flush written text to both stdout and a file, if open.\"\"\"\n        if self.file is not None:\n            self.file.flush()\n\n        self.stdout.flush()\n\n    def close(self) -> None:\n        \"\"\"Flush, close possible files, and remove stdout/stderr mirroring.\"\"\"\n        self.flush()\n\n        # if using multiple loggers, prevent closing in wrong order\n        if sys.stdout is self:\n            sys.stdout = self.stdout\n        if sys.stderr is self:\n            sys.stderr = self.stderr\n\n        if self.file is not None:\n            self.file.close()\n            self.file = None", "\n\n# Cache directories\n# ------------------------------------------------------------------------------------------\n\n_dnnlib_cache_dir = None\n\ndef set_cache_dir(path: str) -> None:\n    global _dnnlib_cache_dir\n    _dnnlib_cache_dir = path", "\ndef make_cache_dir_path(*paths: str) -> str:\n    if _dnnlib_cache_dir is not None:\n        return os.path.join(_dnnlib_cache_dir, *paths)\n    if 'DNNLIB_CACHE_DIR' in os.environ:\n        return os.path.join(os.environ['DNNLIB_CACHE_DIR'], *paths)\n    if 'HOME' in os.environ:\n        return os.path.join(os.environ['HOME'], '.cache', 'dnnlib', *paths)\n    if 'USERPROFILE' in os.environ:\n        return os.path.join(os.environ['USERPROFILE'], '.cache', 'dnnlib', *paths)\n    return os.path.join(tempfile.gettempdir(), '.cache', 'dnnlib', *paths)", "\n# Small util functions\n# ------------------------------------------------------------------------------------------\n\n\ndef format_time(seconds: Union[int, float]) -> str:\n    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n    s = int(np.rint(seconds))\n\n    if s < 60:\n        return \"{0}s\".format(s)\n    elif s < 60 * 60:\n        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n    elif s < 24 * 60 * 60:\n        return \"{0}h {1:02}m {2:02}s\".format(s // (60 * 60), (s // 60) % 60, s % 60)\n    else:\n        return \"{0}d {1:02}h {2:02}m\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24, (s // 60) % 60)", "\n\ndef format_time_brief(seconds: Union[int, float]) -> str:\n    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n    s = int(np.rint(seconds))\n\n    if s < 60:\n        return \"{0}s\".format(s)\n    elif s < 60 * 60:\n        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n    elif s < 24 * 60 * 60:\n        return \"{0}h {1:02}m\".format(s // (60 * 60), (s // 60) % 60)\n    else:\n        return \"{0}d {1:02}h\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24)", "\n\ndef ask_yes_no(question: str) -> bool:\n    \"\"\"Ask the user the question until the user inputs a valid answer.\"\"\"\n    while True:\n        try:\n            print(\"{0} [y/n]\".format(question))\n            return strtobool(input().lower())\n        except ValueError:\n            pass", "\n\ndef tuple_product(t: Tuple) -> Any:\n    \"\"\"Calculate the product of the tuple elements.\"\"\"\n    result = 1\n\n    for v in t:\n        result *= v\n\n    return result", "\n\n_str_to_ctype = {\n    \"uint8\": ctypes.c_ubyte,\n    \"uint16\": ctypes.c_uint16,\n    \"uint32\": ctypes.c_uint32,\n    \"uint64\": ctypes.c_uint64,\n    \"int8\": ctypes.c_byte,\n    \"int16\": ctypes.c_int16,\n    \"int32\": ctypes.c_int32,", "    \"int16\": ctypes.c_int16,\n    \"int32\": ctypes.c_int32,\n    \"int64\": ctypes.c_int64,\n    \"float32\": ctypes.c_float,\n    \"float64\": ctypes.c_double\n}\n\n\ndef get_dtype_and_ctype(type_obj: Any) -> Tuple[np.dtype, Any]:\n    \"\"\"Given a type name string (or an object having a __name__ attribute), return matching Numpy and ctypes types that have the same size in bytes.\"\"\"\n    type_str = None\n\n    if isinstance(type_obj, str):\n        type_str = type_obj\n    elif hasattr(type_obj, \"__name__\"):\n        type_str = type_obj.__name__\n    elif hasattr(type_obj, \"name\"):\n        type_str = type_obj.name\n    else:\n        raise RuntimeError(\"Cannot infer type name from input\")\n\n    assert type_str in _str_to_ctype.keys()\n\n    my_dtype = np.dtype(type_str)\n    my_ctype = _str_to_ctype[type_str]\n\n    assert my_dtype.itemsize == ctypes.sizeof(my_ctype)\n\n    return my_dtype, my_ctype", "def get_dtype_and_ctype(type_obj: Any) -> Tuple[np.dtype, Any]:\n    \"\"\"Given a type name string (or an object having a __name__ attribute), return matching Numpy and ctypes types that have the same size in bytes.\"\"\"\n    type_str = None\n\n    if isinstance(type_obj, str):\n        type_str = type_obj\n    elif hasattr(type_obj, \"__name__\"):\n        type_str = type_obj.__name__\n    elif hasattr(type_obj, \"name\"):\n        type_str = type_obj.name\n    else:\n        raise RuntimeError(\"Cannot infer type name from input\")\n\n    assert type_str in _str_to_ctype.keys()\n\n    my_dtype = np.dtype(type_str)\n    my_ctype = _str_to_ctype[type_str]\n\n    assert my_dtype.itemsize == ctypes.sizeof(my_ctype)\n\n    return my_dtype, my_ctype", "\n\ndef is_pickleable(obj: Any) -> bool:\n    try:\n        with io.BytesIO() as stream:\n            pickle.dump(obj, stream)\n        return True\n    except:\n        return False\n", "\n\n# Functionality to import modules/objects by name, and call functions by name\n# ------------------------------------------------------------------------------------------\n\ndef get_module_from_obj_name(obj_name: str) -> Tuple[types.ModuleType, str]:\n    \"\"\"Searches for the underlying module behind the name to some python object.\n    Returns the module and the object name (original name with module part removed).\"\"\"\n\n    # allow convenience shorthands, substitute them by full names\n    obj_name = re.sub(\"^np.\", \"numpy.\", obj_name)\n    obj_name = re.sub(\"^tf.\", \"tensorflow.\", obj_name)\n\n    # list alternatives for (module_name, local_obj_name)\n    parts = obj_name.split(\".\")\n    name_pairs = [(\".\".join(parts[:i]), \".\".join(parts[i:])) for i in range(len(parts), 0, -1)]\n\n    # try each alternative in turn\n    for module_name, local_obj_name in name_pairs:\n        try:\n            module = importlib.import_module(module_name) # may raise ImportError\n            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n            return module, local_obj_name\n        except:\n            pass\n\n    # maybe some of the modules themselves contain errors?\n    for module_name, _local_obj_name in name_pairs:\n        try:\n            importlib.import_module(module_name) # may raise ImportError\n        except ImportError:\n            if not str(sys.exc_info()[1]).startswith(\"No module named '\" + module_name + \"'\"):\n                raise\n\n    # maybe the requested attribute is missing?\n    for module_name, local_obj_name in name_pairs:\n        try:\n            module = importlib.import_module(module_name) # may raise ImportError\n            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n        except ImportError:\n            pass\n\n    # we are out of luck, but we have no idea why\n    raise ImportError(obj_name)", "\n\ndef get_obj_from_module(module: types.ModuleType, obj_name: str) -> Any:\n    \"\"\"Traverses the object name and returns the last (rightmost) python object.\"\"\"\n    if obj_name == '':\n        return module\n    obj = module\n    for part in obj_name.split(\".\"):\n        obj = getattr(obj, part)\n    return obj", "\n\ndef get_obj_by_name(name: str) -> Any:\n    \"\"\"Finds the python object with the given name.\"\"\"\n    module, obj_name = get_module_from_obj_name(name)\n    return get_obj_from_module(module, obj_name)\n\n\ndef call_func_by_name(*args, func_name: str = None, **kwargs) -> Any:\n    \"\"\"Finds the python object with the given name and calls it as a function.\"\"\"\n    assert func_name is not None\n    func_obj = get_obj_by_name(func_name)\n    assert callable(func_obj)\n    return func_obj(*args, **kwargs)", "def call_func_by_name(*args, func_name: str = None, **kwargs) -> Any:\n    \"\"\"Finds the python object with the given name and calls it as a function.\"\"\"\n    assert func_name is not None\n    func_obj = get_obj_by_name(func_name)\n    assert callable(func_obj)\n    return func_obj(*args, **kwargs)\n\n\ndef construct_class_by_name(*args, class_name: str = None, **kwargs) -> Any:\n    \"\"\"Finds the python class with the given name and constructs it with the given arguments.\"\"\"\n    return call_func_by_name(*args, func_name=class_name, **kwargs)", "def construct_class_by_name(*args, class_name: str = None, **kwargs) -> Any:\n    \"\"\"Finds the python class with the given name and constructs it with the given arguments.\"\"\"\n    return call_func_by_name(*args, func_name=class_name, **kwargs)\n\n\ndef get_module_dir_by_obj_name(obj_name: str) -> str:\n    \"\"\"Get the directory path of the module containing the given object name.\"\"\"\n    module, _ = get_module_from_obj_name(obj_name)\n    return os.path.dirname(inspect.getfile(module))\n", "\n\ndef is_top_level_function(obj: Any) -> bool:\n    \"\"\"Determine whether the given object is a top-level function, i.e., defined at module scope using 'def'.\"\"\"\n    return callable(obj) and obj.__name__ in sys.modules[obj.__module__].__dict__\n\n\ndef get_top_level_function_name(obj: Any) -> str:\n    \"\"\"Return the fully-qualified name of a top-level function.\"\"\"\n    assert is_top_level_function(obj)\n    module = obj.__module__\n    if module == '__main__':\n        module = os.path.splitext(os.path.basename(sys.modules[module].__file__))[0]\n    return module + \".\" + obj.__name__", "\n\n# File system helpers\n# ------------------------------------------------------------------------------------------\n\ndef list_dir_recursively_with_ignore(dir_path: str, ignores: List[str] = None, add_base_to_relative: bool = False) -> List[Tuple[str, str]]:\n    \"\"\"List all files recursively in a given directory while ignoring given file and directory names.\n    Returns list of tuples containing both absolute and relative paths.\"\"\"\n    assert os.path.isdir(dir_path)\n    base_name = os.path.basename(os.path.normpath(dir_path))\n\n    if ignores is None:\n        ignores = []\n\n    result = []\n\n    for root, dirs, files in os.walk(dir_path, topdown=True):\n        for ignore_ in ignores:\n            dirs_to_remove = [d for d in dirs if fnmatch.fnmatch(d, ignore_)]\n\n            # dirs need to be edited in-place\n            for d in dirs_to_remove:\n                dirs.remove(d)\n\n            files = [f for f in files if not fnmatch.fnmatch(f, ignore_)]\n\n        absolute_paths = [os.path.join(root, f) for f in files]\n        relative_paths = [os.path.relpath(p, dir_path) for p in absolute_paths]\n\n        if add_base_to_relative:\n            relative_paths = [os.path.join(base_name, p) for p in relative_paths]\n\n        assert len(absolute_paths) == len(relative_paths)\n        result += zip(absolute_paths, relative_paths)\n\n    return result", "\n\ndef copy_files_and_create_dirs(files: List[Tuple[str, str]]) -> None:\n    \"\"\"Takes in a list of tuples of (src, dst) paths and copies files.\n    Will create all necessary directories.\"\"\"\n    for file in files:\n        target_dir_name = os.path.dirname(file[1])\n\n        # will create all intermediate-level directories\n        if not os.path.exists(target_dir_name):\n            os.makedirs(target_dir_name)\n\n        shutil.copyfile(file[0], file[1])", "\n\n# URL helpers\n# ------------------------------------------------------------------------------------------\n\ndef is_url(obj: Any, allow_file_urls: bool = False) -> bool:\n    \"\"\"Determine whether the given object is a valid URL string.\"\"\"\n    if not isinstance(obj, str) or not \"://\" in obj:\n        return False\n    if allow_file_urls and obj.startswith('file://'):\n        return True\n    try:\n        res = requests.compat.urlparse(obj)\n        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n            return False\n        res = requests.compat.urlparse(requests.compat.urljoin(obj, \"/\"))\n        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n            return False\n    except:\n        return False\n    return True", "\n\ndef open_url(url: str, cache_dir: str = None, num_attempts: int = 10, verbose: bool = True, return_filename: bool = False, cache: bool = True) -> Any:\n    \"\"\"Download the given URL and return a binary-mode file object to access the data.\"\"\"\n    assert num_attempts >= 1\n    assert not (return_filename and (not cache))\n\n    # Doesn't look like an URL scheme so interpret it as a local filename.\n    if not re.match('^[a-z]+://', url):\n        return url if return_filename else open(url, \"rb\")\n\n    # Handle file URLs.  This code handles unusual file:// patterns that\n    # arise on Windows:\n    #\n    # file:///c:/foo.txt\n    #\n    # which would translate to a local '/c:/foo.txt' filename that's\n    # invalid.  Drop the forward slash for such pathnames.\n    #\n    # If you touch this code path, you should test it on both Linux and\n    # Windows.\n    #\n    # Some internet resources suggest using urllib.request.url2pathname() but\n    # but that converts forward slashes to backslashes and this causes\n    # its own set of problems.\n    if url.startswith('file://'):\n        filename = urllib.parse.urlparse(url).path\n        if re.match(r'^/[a-zA-Z]:', filename):\n            filename = filename[1:]\n        return filename if return_filename else open(filename, \"rb\")\n\n    assert is_url(url)\n\n    # Lookup from cache.\n    if cache_dir is None:\n        cache_dir = make_cache_dir_path('downloads')\n\n    url_md5 = hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n    if cache:\n        cache_files = glob.glob(os.path.join(cache_dir, url_md5 + \"_*\"))\n        if len(cache_files) == 1:\n            filename = cache_files[0]\n            return filename if return_filename else open(filename, \"rb\")\n\n    # Download.\n    url_name = None\n    url_data = None\n    with requests.Session() as session:\n        if verbose:\n            print(\"Downloading %s ...\" % url, end=\"\", flush=True)\n        for attempts_left in reversed(range(num_attempts)):\n            try:\n                with session.get(url) as res:\n                    res.raise_for_status()\n                    if len(res.content) == 0:\n                        raise IOError(\"No data received\")\n\n                    if len(res.content) < 8192:\n                        content_str = res.content.decode(\"utf-8\")\n                        if \"download_warning\" in res.headers.get(\"Set-Cookie\", \"\"):\n                            links = [html.unescape(link) for link in content_str.split('\"') if \"export=download\" in link]\n                            if len(links) == 1:\n                                url = requests.compat.urljoin(url, links[0])\n                                raise IOError(\"Google Drive virus checker nag\")\n                        if \"Google Drive - Quota exceeded\" in content_str:\n                            raise IOError(\"Google Drive download quota exceeded -- please try again later\")\n\n                    match = re.search(r'filename=\"([^\"]*)\"', res.headers.get(\"Content-Disposition\", \"\"))\n                    url_name = match[1] if match else url\n                    url_data = res.content\n                    if verbose:\n                        print(\" done\")\n                    break\n            except KeyboardInterrupt:\n                raise\n            except:\n                if not attempts_left:\n                    if verbose:\n                        print(\" failed\")\n                    raise\n                if verbose:\n                    print(\".\", end=\"\", flush=True)\n\n    # Save to cache.\n    if cache:\n        safe_name = re.sub(r\"[^0-9a-zA-Z-._]\", \"_\", url_name)\n        safe_name = safe_name[:min(len(safe_name), 128)]\n        cache_file = os.path.join(cache_dir, url_md5 + \"_\" + safe_name)\n        temp_file = os.path.join(cache_dir, \"tmp_\" + uuid.uuid4().hex + \"_\" + url_md5 + \"_\" + safe_name)\n        os.makedirs(cache_dir, exist_ok=True)\n        with open(temp_file, \"wb\") as f:\n            f.write(url_data)\n        os.replace(temp_file, cache_file) # atomic\n        if return_filename:\n            return cache_file\n\n    # Return data as file object.\n    assert not return_filename\n    return io.BytesIO(url_data)", ""]}
{"filename": "training/__init__.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n# empty\n", ""]}
{"filename": "training/trainer.py", "chunked_list": ["# Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Main training loop.\"\"\"\n\nimport os", "\nimport os\nimport time\nimport copy\nimport numpy as np\nimport torch\nimport torchaudio\nfrom utils.torch_utils import training_stats\nfrom utils.torch_utils import misc\n", "from utils.torch_utils import misc\n\nimport librosa\nfrom glob import glob\nimport re\n\nimport wandb\n\nimport utils.logging as utils_logging\nfrom torch.profiler import tensorboard_trace_handler", "import utils.logging as utils_logging\nfrom torch.profiler import tensorboard_trace_handler\n\nimport omegaconf\n\nimport utils.training_utils as t_utils\n\nfrom surgeon_pytorch import Inspect, get_layers\n#----------------------------------------------------------------------------\n\nclass Trainer():\n    def __init__(self, args, dset, network, optimizer, diff_params, tester=None, device='cpu'):\n        self.args=args\n        self.dset=dset\n        #self.network=torch.compile(network)\n        self.network=network\n\n        self.optimizer=optimizer\n        self.diff_params=diff_params\n        self.device=device\n\n        #testing means generating demos by sampling from the model\n        self.tester=tester\n        if self.tester is None or not(self.args.tester.do_test):\n            self.do_test=False\n        else:\n            self.do_test=True\n\n        #these are settings set by karras. I am not sure what they do\n        #np.random.seed((seed * dist.get_world_size() + dist.get_rank()) % (1 << 31))\n        torch.manual_seed(np.random.randint(1 << 31))\n        torch.backends.cudnn.enabled = True\n        torch.backends.cudnn.benchmark = True\n\n        torch.backends.cudnn.allow_tf32 = True\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.deterministic = False\n\n        #torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n\n        #S=self.args.exp.resample_factor\n        #if S>2.1 and S<2.2:\n        #    #resampling 48k to 22.05k\n        #    self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n        #elif S!=1:\n        #    N=int(self.args.exp.audio_len*S)\n        #    self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\n        # Setup augmentation.\n        #augment_pipe = dnnlib.util.construct_class_by_name(**augment_kwargs) if augment_kwargs is not None else None # training.augment.AugmentPipe\n\n        #print model summary\n\n        self.total_params = sum(p.numel() for p in self.network.parameters() if p.requires_grad)\n        print(\"total_params: \",self.total_params/1e6, \"M\")\n\n        self.layer_list=get_layers(self.network) #used for logging feature statistics\n        self.network_wrapped=Inspect(self.network, self.layer_list) #used for logging feature statistics\n\n        self.ema = copy.deepcopy(self.network).eval().requires_grad_(False)\n        \n        #resume from checkpoint\n        self.latest_checkpoint=None\n        resuming=False\n        if self.args.exp.resume:\n            if self.args.exp.resume_checkpoint != \"None\":\n                resuming =self.resume_from_checkpoint(checkpoint_path=self.args.exp.resume_checkpoint)\n            else:\n                resuming =self.resume_from_checkpoint()\n            if not resuming:\n                print(\"Could not resume from checkpoint\")\n                print(\"training from scratch\")\n            else:\n                print(\"Resuming from iteration {}\".format(self.it))\n\n        if not resuming:\n            self.it=0\n            self.latest_checkpoint=None\n\n        if self.args.logging.print_model_summary:\n            #if dist.get_rank() == 0:\n             with torch.no_grad():\n                 audio=torch.zeros([args.exp.batch,args.exp.audio_len], device=device)\n                 sigma = torch.ones([args.exp.batch], device=device).unsqueeze(-1)\n                 misc.print_module_summary(self.network, [audio, sigma ], max_nesting=2)\n\n        \n        if self.args.logging.log:\n            #assert self.args.logging.heavy_log_interval % self.args.logging.save_interval == 0 #sorry for that, I just want to make sure that you are not wasting your time by logging too often, as the tester is only updated with the ema weights from a checkpoint\n            self.setup_wandb()\n            if self.do_test:\n               self.tester.setup_wandb_run(self.wandb_run)\n            self.setup_logging_variables()\n\n        self.profile=False\n        if self.args.logging.profiling.enabled:\n            try:\n                print(\"Profiling is being enabled\")\n                wait=self.args.logging.profiling.wait\n                warmup=self.args.logging.profiling.warmup\n                active=self.args.logging.profiling.active\n                repeat=self.args.logging.profiling.repeat\n\n                schedule =  torch.profiler.schedule(\n                wait=wait, warmup=warmup, active=active, repeat=repeat)\n                self.profiler = torch.profiler.profile(\n                schedule=schedule, on_trace_ready=tensorboard_trace_handler(\"wandb/latest-run/tbprofile\"), profile_memory=True, with_stack=False)\n                self.profile=True\n                self.profile_total_steps = (wait + warmup + active) * (1 + repeat)\n            except Exception as e:\n\n                print(\"Could not setup profiler\")\n                print(e)\n                self.profile=False\n                \n\n            \n\n    def setup_wandb(self):\n        \"\"\"\n        Configure wandb, open a new run and log the configuration.\n        \"\"\"\n        config=omegaconf.OmegaConf.to_container(\n            self.args, resolve=True, throw_on_missing=True\n        )\n        config[\"total_params\"]=self.total_params\n        self.wandb_run=wandb.init(project=self.args.exp.wandb.project, entity=self.args.exp.wandb.entity, config=config)\n        wandb.watch(self.network, log=\"all\", log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n\n    \n    def setup_logging_variables(self):\n\n        self.sigma_bins = np.logspace(np.log10(self.args.diff_params.sigma_min), np.log10(self.args.diff_params.sigma_max), num=self.args.logging.num_sigma_bins, base=10)\n\n        #logarithmically spaced bins for the frequency logging\n        self.freq_bins=np.logspace(np.log2(self.args.logging.cqt.fmin), np.log2(self.args.logging.cqt.fmin*2**(self.args.logging.cqt.num_octs)), num=self.args.logging.cqt.num_octs*self.args.logging.cqt.bins_per_oct, base=2)\n        self.freq_bins=self.freq_bins.astype(int)\n\n\n\n    def load_state_dict(self, state_dict):\n        #print(state_dict)\n        return t_utils.load_state_dict(state_dict, network=self.network, ema=self.ema, optimizer=self.optimizer)\n\n    def load_state_dict_legacy(self, state_dict):\n        #print(state_dict)\n        print(state_dict.keys())\n        try:\n            self.it = state_dict['it']\n        except:\n            self.it=150000 #large number to mean that we loaded somethin, but it is arbitrary\n        try:\n            self.network.load_state_dict(state_dict['network'])\n            self.optimizer.load_state_dict(state_dict['optimizer'])\n            self.ema.load_state_dict(state_dict['ema'])\n            return True\n        except Exception as e:\n            print(\"Could not load state dict\")\n            print(e)\n            print(\"trying with strict=False\")\n        try:\n            self.network.load_state_dict(state_dict['network'], strict=False)\n            #we cannot load the optimizer in this setting\n            #self.optimizer.load_state_dict(state_dict['optimizer'], strict=False)\n            self.ema.load_state_dict(state_dict['ema'], strict=False)\n            return True\n        except Exception as e:\n            print(\"Could not load state dict\")\n            print(e)\n            print(\"training from scratch\")\n\n        try:\n            self.network.load_state_dict(state_dict['state_dict'])\n            self.ema.load_state_dict(state_dict['state_dict'])\n        except Exception as e:\n            print(\"Could not load state dict\")\n            print(e)\n            print(\"training from scratch\")\n            print(\"It failed 3 times!! giving up..\")\n            return False\n            \n\n\n    def resume_from_checkpoint(self, checkpoint_path=None, checkpoint_id=None):\n        # Resume training from latest checkpoint available in the output director\n        if checkpoint_path is not None:\n            try:\n                checkpoint=torch.load(checkpoint_path, map_location=self.device)\n                print(checkpoint.keys())\n                #if it is possible, retrieve the iteration number from the checkpoint\n                try:\n                    self.it = checkpoint['it']\n                except:\n                    self.it=157007 #large number to mean that we loaded somethin, but it is arbitrary\n                return self.load_state_dict(checkpoint)\n            except Exception as e:\n                print(\"Could not resume from checkpoint\")\n                print(e)\n                print(\"training from scratch\")\n                self.it=0\n\n            try:\n                checkpoint=torch.load(os.path.join(self.args.model_dir,checkpoint_path), map_location=self.device)\n                print(checkpoint.keys())\n                #if it is possible, retrieve the iteration number from the checkpoint\n                try:\n                    self.it = checkpoint['it']\n                except:\n                    self.it=157007 #large number to mean that we loaded somethin, but it is arbitrary\n                self.network.load_state_dict(checkpoint['ema_model'])\n                return True\n            except Exception as e:\n                print(\"Could not resume from checkpoint\")\n                print(e)\n                print(\"training from scratch\")\n                self.it=0\n                return False\n        else:\n            try:\n                print(\"trying to load a project checkpoint\")\n                print(\"checkpoint_id\", checkpoint_id)\n                if checkpoint_id is None:\n                    # find latest checkpoint_id\n                    save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n                    save_name = f\"{self.args.model_dir}/{save_basename}\"\n                    print(save_name)\n                    list_weights = glob(save_name)\n                    id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n                    list_ids = [int(id_regex.search(weight_path).groups()[0])\n                                for weight_path in list_weights]\n                    checkpoint_id = max(list_ids)\n                    print(checkpoint_id)\n    \n                checkpoint = torch.load(\n                    f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n                #if it is possible, retrieve the iteration number from the checkpoint\n                try:\n                    self.it = checkpoint['it']\n                except:\n                    self.it=159000 #large number to mean that we loaded somethin, but it is arbitrary\n                self.load_state_dict(checkpoint)\n                return True\n            except Exception as e:\n                print(e)\n                return False\n\n\n    def state_dict(self):\n        return {\n            'it': self.it,\n            'network': self.network.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n            'ema': self.ema.state_dict(),\n            'args': self.args,\n        }\n\n    def save_checkpoint(self):\n        save_basename = f\"{self.args.exp.exp_name}-{self.it}.pt\"\n        save_name = f\"{self.args.model_dir}/{save_basename}\"\n        torch.save(self.state_dict(), save_name)\n        print(\"saving\",save_name)\n        if self.args.logging.remove_last_checkpoint:\n            try:\n                os.remove(self.latest_checkpoint)\n                print(\"removed last checkpoint\", self.latest_checkpoint)\n            except:\n                print(\"could not remove last checkpoint\", self.latest_checkpoint)\n        self.latest_checkpoint=save_name\n\n\n    def log_feature_stats(self):\n        print(\"logging feature stats\")\n        #this is specific for the edm diff_params, be careful if changing it\n        x=self.get_batch()\n        print(x.shape, x.std(-1))\n        sigma=self.diff_params.sample_ptrain_safe(x.shape[0]).unsqueeze(-1).to(x.device)\n\n        input, target, cnoise= self.diff_params.prepare_train_preconditioning(x, sigma)\n        estimate, feat_list=self.network_wrapped(input,cnoise) #is this too crazy memory wise?\n        \n        for name, a in zip(self.layer_list, feat_list):\n            #log an histogram for each layer in wandb\n            if a is not None:\n                \n                wandb.log({f\"features/{name}\": wandb.Histogram(a[0].detach().cpu().numpy())}, step=self.it)\n\n        del feat_list #I hope this frees the memory\n\n    def process_loss_for_logging(self, error: torch.Tensor, sigma: torch.Tensor):\n        \"\"\"\n        This function is used to process the loss for logging. It is used to group the losses by the values of sigma and report them using training_stats.\n        args:\n            error: the error tensor with shape [batch, audio_len]\n            sigma: the sigma tensor with shape [batch]\n        \"\"\"\n        #sigma values are ranged between self.args.diff_params.sigma_min and self.args.diff_params.sigma_max. We need to quantize the values of sigma into 10 logarithmically spaced bins between self.args.diff_params.sigma_min and self.args.diff_params.sigma_max\n        torch.nan_to_num(error) #not tested might crash\n        error=error.detach().cpu().numpy()\n\n        #Now I need to report the error respect to frequency. I would like to do this by using the CQT of the error and then report the error respect to both sigma and frequency\n        #I will use librosa to compute the CQT\n\n        #will this be too heavy? I am not sure. I will try it and see what happens\n        if self.it%self.args.logging.freq_cqt_logging==0: #do that only once every 50 iterations, for efficiency\n            cqt_res=[]\n            for i in range(error.shape[0]):\n                #running this cqt in gpu would be faster. \n                cqt = librosa.cqt(error[i], sr=self.args.exp.sample_rate, hop_length=self.args.logging.cqt.hop_length, fmin=self.args.logging.cqt.fmin, n_bins=self.args.logging.cqt.num_octs*self.args.logging.cqt.bins_per_oct, bins_per_octave=self.args.logging.cqt.bins_per_oct)\n                cqt_res.append(np.abs(cqt.mean(axis=1)))\n                #now I need to report the error respect to frequency\n        else:\n            cqt_res=None\n\n        for i in range(len(self.sigma_bins)):\n            if i == 0:\n                mask = sigma <= self.sigma_bins[i]\n            elif i == len(self.sigma_bins)-1:\n                mask = (sigma <= self.sigma_bins[i]) & (sigma > self.sigma_bins[i-1])\n\n            else:\n                mask = (sigma <= self.sigma_bins[i]) & (sigma > self.sigma_bins[i-1])\n            mask=mask.squeeze(-1).cpu()\n            if mask.sum() > 0:\n                #find the index of the first element of the mask\n                idx = np.where(mask==True)[0][0]\n\n                training_stats.report('error_sigma_'+str(self.sigma_bins[i]),error[idx].mean())\n\n                if cqt_res is not None:\n                    for j in range(cqt.shape[0]):\n                        training_stats.report('error_sigma_'+str(self.sigma_bins[i])+'_freq_'+str(self.freq_bins[j]),cqt_res[idx][j].mean())\n\n        if cqt_res is not None:\n            for j in range(cqt.shape[0]):\n                for i in range(len(cqt_res)):\n                    training_stats.report('error_freq_'+str(self.freq_bins[j]),cqt_res[i][j].mean())\n    def get_batch(self):\n        #load the data batch\n        if self.args.dset.name == \"maestro_allyears\":\n            audio, fs = next(self.dset)\n            audio=audio.to(self.device).to(torch.float32)\n            print(fs, audio.shape)\n            #do resampling if needed\n            #print(\"before resample\",audio.shape, self.args.exp.resample_factor)\n            return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n        else: \n            audio = next(self.dset)\n            audio=audio.to(self.device).to(torch.float32)\n            #do resampling if needed\n            #print(\"before resample\",audio.shape, self.args.exp.resample_factor)\n            if self.args.exp.resample_factor != 1:\n                #self.resample(audio)\n                audio=torchaudio.functional.resample(audio, self.args.exp.resample_factor, 1)\n            #TODO: add augmentation\n            return audio\n    def train_step(self):\n        # Train step\n        it_start_time = time.time()\n        #self.optimizer.zero_grad(set_to_none=True)\n        self.optimizer.zero_grad()\n        st_time=time.time()\n        for round_idx in range(self.args.exp.num_accumulation_rounds):\n            #with misc.ddp_sync(ddp, (round_idx == num_accumulation_rounds - 1)):\n            audio=self.get_batch()\n            print(audio.shape, self.args.exp.audio_len, audio.std(-1))\n\n            #print(audio.shape, self.args.exp.audio_len)\n            error, sigma = self.diff_params.loss_fn(self.network, audio)\n            loss=error.mean()\n            loss.backward() #TODO: take care of the loss scaling if using mixed precision\n            #do I want to call this at every round? It will slow down the training. I will try it and see what happens\n\n            #loss.sum().mul(self.args.exp.loss_scaling).backward()\n            #print(loss.item())\n\n\n        if self.it <= self.args.exp.lr_rampup_it:\n            for g in self.optimizer.param_groups:\n                #learning rate ramp up\n                g['lr'] = self.args.exp.lr * min(self.it / max(self.args.exp.lr_rampup_it, 1e-8), 1)\n\n        #for param in self.network.parameters():\n        #    #take care of none gradients. Is that needed? \n        #    if param.grad is not None:\n        #        torch.nan_to_num(param.grad, nan=0, posinf=1e5, neginf=-1e5, out=param.grad)#This is needed for the loss scaling. That is what causes the nan gradients. \n\n        if self.args.exp.use_grad_clip:\n            torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.args.exp.max_grad_norm)\n\n        # Update weights.\n        self.optimizer.step()\n\n        end_time=time.time()\n        if self.args.logging.log:\n            self.process_loss_for_logging(error, sigma)\n\n        it_end_time = time.time()\n        print(\"it :\",self.it, \"time:, \",end_time-st_time, \"total_time: \",training_stats.report('it_time',it_end_time-it_start_time) ,\"loss: \", training_stats.report('loss', loss.item())) #TODO: take care of the logging\n\n\n    def update_ema(self):\n        \"\"\"Update exponential moving average of self.network weights.\"\"\"\n\n        ema_rampup = self.args.exp.ema_rampup  #ema_rampup should be set to 10000 in the config file\n        ema_rate=self.args.exp.ema_rate #ema_rate should be set to 0.9999 in the config file\n        t = self.it * self.args.exp.batch\n        with torch.no_grad():\n            if t < ema_rampup:\n                s = np.clip(t / ema_rampup, 0.0, ema_rate)\n                for dst, src in zip(self.ema.parameters(), self.network.parameters()):\n                    dst.copy_(dst * s + src * (1-s))\n            else:\n                for dst, src in zip(self.ema.parameters(), self.network.parameters()):\n                    dst.copy_(dst * ema_rate + src * (1-ema_rate))\n\n    def easy_logging(self):\n        \"\"\"\n         Do the simplest logging here. This will be called every 1000 iterations or so\n        I will use the training_stats.report function for this, and aim to report the means and stds of the losses in wandb\n        \"\"\"\n        training_stats.default_collector.update()\n        #Is it a good idea to log the stds of the losses? I think it is not.\n        loss_mean=training_stats.default_collector.mean('loss')\n        self.wandb_run.log({'loss':loss_mean}, step=self.it)\n        loss_std=training_stats.default_collector.std('loss')\n        self.wandb_run.log({'loss_std':loss_std}, step=self.it)\n\n        it_time_mean=training_stats.default_collector.mean('it_time')\n        self.wandb_run.log({'it_time_mean':it_time_mean}, step=self.it)\n        it_time_std=training_stats.default_collector.std('it_time')\n        self.wandb_run.log({'it_time_std':it_time_std}, step=self.it)\n        \n        #here reporting the error respect to sigma. I should make a fancier plot too, with mean and std\n        sigma_means=[]\n        sigma_stds=[]\n        for i in range(len(self.sigma_bins)):\n            a=training_stats.default_collector.mean('error_sigma_'+str(self.sigma_bins[i]))\n            sigma_means.append(a)\n            self.wandb_run.log({'error_sigma_'+str(self.sigma_bins[i]):a}, step=self.it)\n            a=training_stats.default_collector.std('error_sigma_'+str(self.sigma_bins[i]))\n            sigma_stds.append(a)\n\n        \n        figure=utils_logging.plot_loss_by_sigma(sigma_means,sigma_stds, self.sigma_bins)\n        wandb.log({\"loss_dependent_on_sigma\": figure}, step=self.it, commit=True)\n\n\n        #TODO log here the losses at different noise levels. I don't know if these should be heavy\n        #TODO also log here the losses at different frequencies if we are reporting them. same as above\n\n    def heavy_logging(self):\n        \"\"\"\n        Do the heavy logging here. This will be called every 10000 iterations or so\n        \"\"\"\n        freq_means=[]\n        freq_stds=[]\n        freq_sigma_means=[]\n        freq_sigma_stds=[]\n        for j in range(len(self.freq_bins)):\n            a=training_stats.default_collector.mean('error_freq_'+str(self.freq_bins[j]))\n            freq_means.append(a)\n            self.wandb_run.log({'error_freq_'+str(self.freq_bins[j]):a}, step=self.it)\n\n            a=training_stats.default_collector.std('error_freq_'+str(self.freq_bins[j]))\n            freq_stds.append(a)\n            #I need to take care of this in some other way that is easier to visualize\n            omeans=[]\n            ostds=[]\n            for i in range(len(self.sigma_bins)):\n                a=training_stats.default_collector.mean('error_sigma_'+str(self.sigma_bins[i])+'_freq_'+str(self.freq_bins[j]))\n                omeans.append(a)\n                a=training_stats.default_collector.std('error_sigma_'+str(self.sigma_bins[i])+'_freq_'+str(self.freq_bins[j]))\n                ostds.append(a)\n                #wandb.log({'error_sigma_'+str(self.sigma_bins[i])+'_freq_'+str(self.freq_bins[j]):a}, step=self.it)\n                #this logging will not be so handy. I create a figure using plotly to nicely visualize the results\n            freq_sigma_means.append(omeans)\n            freq_sigma_stds.append(ostds)\n\n        figure=utils_logging.plot_loss_by_sigma(freq_means,freq_stds, self.freq_bins)\n        wandb.log({\"loss_dependent_on_freq\": figure}, step=self.it)\n\n\n        figure=utils_logging.plot_loss_by_sigma_and_freq(freq_sigma_means,freq_sigma_stds, self.sigma_bins, self.freq_bins)#TODO!!!\n        wandb.log({\"loss_dependent_on_freq_and_sigma\": figure}, step=self.it)\n\n        if self.do_test:\n\n            if self.latest_checkpoint is not None:\n                self.tester.load_checkpoint(self.latest_checkpoint)\n\n            preds=self.tester.sample_unconditional()\n            if \"inpainting\" in self.args.tester.modes:\n                preds=self.tester.test_inpainting()\n            if \"bwe\" in self.args.tester.modes:\n                preds=self.tester.test_bwe()\n            #self.log_audio(preds, \"unconditional_sampling\")\n\n        #TODO: call the unconditional generation function and log the audio samples\n    def log_audio(self,x, name):\n        string=name+\"_\"+self.args.tester.name\n        audio_path=utils_logging.write_audio_file(x,self.args.exp.sample_rate, string,path=self.args.model_dir)\n        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it)\n        #TODO: log spectrogram of the audio file to wandb\n        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(x, self.args.logging.stft)\n        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it)\n\n    def conditional_demos(self):\n        \"\"\"\n        Do the conditional demos here. This will be called every 10000 iterations or so\n        \"\"\"\n        #TODO: call the conditional generation function and log the audio samples\n        pass\n\n\n    def training_loop(self):\n        \n        # Initialize.\n\n        #ddp = torch.nn.parallel.DistributedDataParallel(net, device_ids=[device], broadcast_buffers=False)\n\n        while True:\n            # Accumulate gradients.\n\n            self.train_step()\n\n            self.update_ema()\n            \n            if self.profile and self.args.logging.log:\n                print(self.profile, self.profile_total_steps, self.it)\n                if self.it<self.profile_total_steps:\n                    self.profiler.step()\n                elif self.it==self.profile_total_steps +1:\n                    #log trace as an artifact in wandb\n                    profile_art = wandb.Artifact(f\"trace-{wandb.run.id}\", type=\"profile\")\n                    profile_art.add_file(glob(\"wandb/latest-run/tbprofile/*.pt.trace.json\")[0], \"trace.pt.trace.json\")\n                    wandb.log_artifact(profile_art)\n                    print(\"proiling done\")\n                elif self.it>self.profile_total_steps +1:\n                    self.profile=False\n\n\n\n            if self.it>0 and self.it%self.args.logging.save_interval==0 and self.args.logging.save_model:\n                #self.save_snapshot() #are the snapshots necessary? I think they are not.\n                self.save_checkpoint()\n\n            if self.it>0 and self.it%self.args.logging.log_feature_stats_interval==0 and self.args.logging.log_feature_stats:\n                self.log_feature_stats()\n\n            if self.it>0 and self.it%self.args.logging.heavy_log_interval==0 and self.args.logging.log:\n                self.heavy_logging()\n                #self.conditional_demos()\n\n            if self.it>0 and self.it%self.args.logging.log_interval==0 and self.args.logging.log:\n                self.easy_logging()\n\n\n            \n\n            # Update state.\n            self.it += 1", "#----------------------------------------------------------------------------\n\nclass Trainer():\n    def __init__(self, args, dset, network, optimizer, diff_params, tester=None, device='cpu'):\n        self.args=args\n        self.dset=dset\n        #self.network=torch.compile(network)\n        self.network=network\n\n        self.optimizer=optimizer\n        self.diff_params=diff_params\n        self.device=device\n\n        #testing means generating demos by sampling from the model\n        self.tester=tester\n        if self.tester is None or not(self.args.tester.do_test):\n            self.do_test=False\n        else:\n            self.do_test=True\n\n        #these are settings set by karras. I am not sure what they do\n        #np.random.seed((seed * dist.get_world_size() + dist.get_rank()) % (1 << 31))\n        torch.manual_seed(np.random.randint(1 << 31))\n        torch.backends.cudnn.enabled = True\n        torch.backends.cudnn.benchmark = True\n\n        torch.backends.cudnn.allow_tf32 = True\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.deterministic = False\n\n        #torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n\n        #S=self.args.exp.resample_factor\n        #if S>2.1 and S<2.2:\n        #    #resampling 48k to 22.05k\n        #    self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n        #elif S!=1:\n        #    N=int(self.args.exp.audio_len*S)\n        #    self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\n        # Setup augmentation.\n        #augment_pipe = dnnlib.util.construct_class_by_name(**augment_kwargs) if augment_kwargs is not None else None # training.augment.AugmentPipe\n\n        #print model summary\n\n        self.total_params = sum(p.numel() for p in self.network.parameters() if p.requires_grad)\n        print(\"total_params: \",self.total_params/1e6, \"M\")\n\n        self.layer_list=get_layers(self.network) #used for logging feature statistics\n        self.network_wrapped=Inspect(self.network, self.layer_list) #used for logging feature statistics\n\n        self.ema = copy.deepcopy(self.network).eval().requires_grad_(False)\n        \n        #resume from checkpoint\n        self.latest_checkpoint=None\n        resuming=False\n        if self.args.exp.resume:\n            if self.args.exp.resume_checkpoint != \"None\":\n                resuming =self.resume_from_checkpoint(checkpoint_path=self.args.exp.resume_checkpoint)\n            else:\n                resuming =self.resume_from_checkpoint()\n            if not resuming:\n                print(\"Could not resume from checkpoint\")\n                print(\"training from scratch\")\n            else:\n                print(\"Resuming from iteration {}\".format(self.it))\n\n        if not resuming:\n            self.it=0\n            self.latest_checkpoint=None\n\n        if self.args.logging.print_model_summary:\n            #if dist.get_rank() == 0:\n             with torch.no_grad():\n                 audio=torch.zeros([args.exp.batch,args.exp.audio_len], device=device)\n                 sigma = torch.ones([args.exp.batch], device=device).unsqueeze(-1)\n                 misc.print_module_summary(self.network, [audio, sigma ], max_nesting=2)\n\n        \n        if self.args.logging.log:\n            #assert self.args.logging.heavy_log_interval % self.args.logging.save_interval == 0 #sorry for that, I just want to make sure that you are not wasting your time by logging too often, as the tester is only updated with the ema weights from a checkpoint\n            self.setup_wandb()\n            if self.do_test:\n               self.tester.setup_wandb_run(self.wandb_run)\n            self.setup_logging_variables()\n\n        self.profile=False\n        if self.args.logging.profiling.enabled:\n            try:\n                print(\"Profiling is being enabled\")\n                wait=self.args.logging.profiling.wait\n                warmup=self.args.logging.profiling.warmup\n                active=self.args.logging.profiling.active\n                repeat=self.args.logging.profiling.repeat\n\n                schedule =  torch.profiler.schedule(\n                wait=wait, warmup=warmup, active=active, repeat=repeat)\n                self.profiler = torch.profiler.profile(\n                schedule=schedule, on_trace_ready=tensorboard_trace_handler(\"wandb/latest-run/tbprofile\"), profile_memory=True, with_stack=False)\n                self.profile=True\n                self.profile_total_steps = (wait + warmup + active) * (1 + repeat)\n            except Exception as e:\n\n                print(\"Could not setup profiler\")\n                print(e)\n                self.profile=False\n                \n\n            \n\n    def setup_wandb(self):\n        \"\"\"\n        Configure wandb, open a new run and log the configuration.\n        \"\"\"\n        config=omegaconf.OmegaConf.to_container(\n            self.args, resolve=True, throw_on_missing=True\n        )\n        config[\"total_params\"]=self.total_params\n        self.wandb_run=wandb.init(project=self.args.exp.wandb.project, entity=self.args.exp.wandb.entity, config=config)\n        wandb.watch(self.network, log=\"all\", log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n\n    \n    def setup_logging_variables(self):\n\n        self.sigma_bins = np.logspace(np.log10(self.args.diff_params.sigma_min), np.log10(self.args.diff_params.sigma_max), num=self.args.logging.num_sigma_bins, base=10)\n\n        #logarithmically spaced bins for the frequency logging\n        self.freq_bins=np.logspace(np.log2(self.args.logging.cqt.fmin), np.log2(self.args.logging.cqt.fmin*2**(self.args.logging.cqt.num_octs)), num=self.args.logging.cqt.num_octs*self.args.logging.cqt.bins_per_oct, base=2)\n        self.freq_bins=self.freq_bins.astype(int)\n\n\n\n    def load_state_dict(self, state_dict):\n        #print(state_dict)\n        return t_utils.load_state_dict(state_dict, network=self.network, ema=self.ema, optimizer=self.optimizer)\n\n    def load_state_dict_legacy(self, state_dict):\n        #print(state_dict)\n        print(state_dict.keys())\n        try:\n            self.it = state_dict['it']\n        except:\n            self.it=150000 #large number to mean that we loaded somethin, but it is arbitrary\n        try:\n            self.network.load_state_dict(state_dict['network'])\n            self.optimizer.load_state_dict(state_dict['optimizer'])\n            self.ema.load_state_dict(state_dict['ema'])\n            return True\n        except Exception as e:\n            print(\"Could not load state dict\")\n            print(e)\n            print(\"trying with strict=False\")\n        try:\n            self.network.load_state_dict(state_dict['network'], strict=False)\n            #we cannot load the optimizer in this setting\n            #self.optimizer.load_state_dict(state_dict['optimizer'], strict=False)\n            self.ema.load_state_dict(state_dict['ema'], strict=False)\n            return True\n        except Exception as e:\n            print(\"Could not load state dict\")\n            print(e)\n            print(\"training from scratch\")\n\n        try:\n            self.network.load_state_dict(state_dict['state_dict'])\n            self.ema.load_state_dict(state_dict['state_dict'])\n        except Exception as e:\n            print(\"Could not load state dict\")\n            print(e)\n            print(\"training from scratch\")\n            print(\"It failed 3 times!! giving up..\")\n            return False\n            \n\n\n    def resume_from_checkpoint(self, checkpoint_path=None, checkpoint_id=None):\n        # Resume training from latest checkpoint available in the output director\n        if checkpoint_path is not None:\n            try:\n                checkpoint=torch.load(checkpoint_path, map_location=self.device)\n                print(checkpoint.keys())\n                #if it is possible, retrieve the iteration number from the checkpoint\n                try:\n                    self.it = checkpoint['it']\n                except:\n                    self.it=157007 #large number to mean that we loaded somethin, but it is arbitrary\n                return self.load_state_dict(checkpoint)\n            except Exception as e:\n                print(\"Could not resume from checkpoint\")\n                print(e)\n                print(\"training from scratch\")\n                self.it=0\n\n            try:\n                checkpoint=torch.load(os.path.join(self.args.model_dir,checkpoint_path), map_location=self.device)\n                print(checkpoint.keys())\n                #if it is possible, retrieve the iteration number from the checkpoint\n                try:\n                    self.it = checkpoint['it']\n                except:\n                    self.it=157007 #large number to mean that we loaded somethin, but it is arbitrary\n                self.network.load_state_dict(checkpoint['ema_model'])\n                return True\n            except Exception as e:\n                print(\"Could not resume from checkpoint\")\n                print(e)\n                print(\"training from scratch\")\n                self.it=0\n                return False\n        else:\n            try:\n                print(\"trying to load a project checkpoint\")\n                print(\"checkpoint_id\", checkpoint_id)\n                if checkpoint_id is None:\n                    # find latest checkpoint_id\n                    save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n                    save_name = f\"{self.args.model_dir}/{save_basename}\"\n                    print(save_name)\n                    list_weights = glob(save_name)\n                    id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n                    list_ids = [int(id_regex.search(weight_path).groups()[0])\n                                for weight_path in list_weights]\n                    checkpoint_id = max(list_ids)\n                    print(checkpoint_id)\n    \n                checkpoint = torch.load(\n                    f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n                #if it is possible, retrieve the iteration number from the checkpoint\n                try:\n                    self.it = checkpoint['it']\n                except:\n                    self.it=159000 #large number to mean that we loaded somethin, but it is arbitrary\n                self.load_state_dict(checkpoint)\n                return True\n            except Exception as e:\n                print(e)\n                return False\n\n\n    def state_dict(self):\n        return {\n            'it': self.it,\n            'network': self.network.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n            'ema': self.ema.state_dict(),\n            'args': self.args,\n        }\n\n    def save_checkpoint(self):\n        save_basename = f\"{self.args.exp.exp_name}-{self.it}.pt\"\n        save_name = f\"{self.args.model_dir}/{save_basename}\"\n        torch.save(self.state_dict(), save_name)\n        print(\"saving\",save_name)\n        if self.args.logging.remove_last_checkpoint:\n            try:\n                os.remove(self.latest_checkpoint)\n                print(\"removed last checkpoint\", self.latest_checkpoint)\n            except:\n                print(\"could not remove last checkpoint\", self.latest_checkpoint)\n        self.latest_checkpoint=save_name\n\n\n    def log_feature_stats(self):\n        print(\"logging feature stats\")\n        #this is specific for the edm diff_params, be careful if changing it\n        x=self.get_batch()\n        print(x.shape, x.std(-1))\n        sigma=self.diff_params.sample_ptrain_safe(x.shape[0]).unsqueeze(-1).to(x.device)\n\n        input, target, cnoise= self.diff_params.prepare_train_preconditioning(x, sigma)\n        estimate, feat_list=self.network_wrapped(input,cnoise) #is this too crazy memory wise?\n        \n        for name, a in zip(self.layer_list, feat_list):\n            #log an histogram for each layer in wandb\n            if a is not None:\n                \n                wandb.log({f\"features/{name}\": wandb.Histogram(a[0].detach().cpu().numpy())}, step=self.it)\n\n        del feat_list #I hope this frees the memory\n\n    def process_loss_for_logging(self, error: torch.Tensor, sigma: torch.Tensor):\n        \"\"\"\n        This function is used to process the loss for logging. It is used to group the losses by the values of sigma and report them using training_stats.\n        args:\n            error: the error tensor with shape [batch, audio_len]\n            sigma: the sigma tensor with shape [batch]\n        \"\"\"\n        #sigma values are ranged between self.args.diff_params.sigma_min and self.args.diff_params.sigma_max. We need to quantize the values of sigma into 10 logarithmically spaced bins between self.args.diff_params.sigma_min and self.args.diff_params.sigma_max\n        torch.nan_to_num(error) #not tested might crash\n        error=error.detach().cpu().numpy()\n\n        #Now I need to report the error respect to frequency. I would like to do this by using the CQT of the error and then report the error respect to both sigma and frequency\n        #I will use librosa to compute the CQT\n\n        #will this be too heavy? I am not sure. I will try it and see what happens\n        if self.it%self.args.logging.freq_cqt_logging==0: #do that only once every 50 iterations, for efficiency\n            cqt_res=[]\n            for i in range(error.shape[0]):\n                #running this cqt in gpu would be faster. \n                cqt = librosa.cqt(error[i], sr=self.args.exp.sample_rate, hop_length=self.args.logging.cqt.hop_length, fmin=self.args.logging.cqt.fmin, n_bins=self.args.logging.cqt.num_octs*self.args.logging.cqt.bins_per_oct, bins_per_octave=self.args.logging.cqt.bins_per_oct)\n                cqt_res.append(np.abs(cqt.mean(axis=1)))\n                #now I need to report the error respect to frequency\n        else:\n            cqt_res=None\n\n        for i in range(len(self.sigma_bins)):\n            if i == 0:\n                mask = sigma <= self.sigma_bins[i]\n            elif i == len(self.sigma_bins)-1:\n                mask = (sigma <= self.sigma_bins[i]) & (sigma > self.sigma_bins[i-1])\n\n            else:\n                mask = (sigma <= self.sigma_bins[i]) & (sigma > self.sigma_bins[i-1])\n            mask=mask.squeeze(-1).cpu()\n            if mask.sum() > 0:\n                #find the index of the first element of the mask\n                idx = np.where(mask==True)[0][0]\n\n                training_stats.report('error_sigma_'+str(self.sigma_bins[i]),error[idx].mean())\n\n                if cqt_res is not None:\n                    for j in range(cqt.shape[0]):\n                        training_stats.report('error_sigma_'+str(self.sigma_bins[i])+'_freq_'+str(self.freq_bins[j]),cqt_res[idx][j].mean())\n\n        if cqt_res is not None:\n            for j in range(cqt.shape[0]):\n                for i in range(len(cqt_res)):\n                    training_stats.report('error_freq_'+str(self.freq_bins[j]),cqt_res[i][j].mean())\n    def get_batch(self):\n        #load the data batch\n        if self.args.dset.name == \"maestro_allyears\":\n            audio, fs = next(self.dset)\n            audio=audio.to(self.device).to(torch.float32)\n            print(fs, audio.shape)\n            #do resampling if needed\n            #print(\"before resample\",audio.shape, self.args.exp.resample_factor)\n            return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n        else: \n            audio = next(self.dset)\n            audio=audio.to(self.device).to(torch.float32)\n            #do resampling if needed\n            #print(\"before resample\",audio.shape, self.args.exp.resample_factor)\n            if self.args.exp.resample_factor != 1:\n                #self.resample(audio)\n                audio=torchaudio.functional.resample(audio, self.args.exp.resample_factor, 1)\n            #TODO: add augmentation\n            return audio\n    def train_step(self):\n        # Train step\n        it_start_time = time.time()\n        #self.optimizer.zero_grad(set_to_none=True)\n        self.optimizer.zero_grad()\n        st_time=time.time()\n        for round_idx in range(self.args.exp.num_accumulation_rounds):\n            #with misc.ddp_sync(ddp, (round_idx == num_accumulation_rounds - 1)):\n            audio=self.get_batch()\n            print(audio.shape, self.args.exp.audio_len, audio.std(-1))\n\n            #print(audio.shape, self.args.exp.audio_len)\n            error, sigma = self.diff_params.loss_fn(self.network, audio)\n            loss=error.mean()\n            loss.backward() #TODO: take care of the loss scaling if using mixed precision\n            #do I want to call this at every round? It will slow down the training. I will try it and see what happens\n\n            #loss.sum().mul(self.args.exp.loss_scaling).backward()\n            #print(loss.item())\n\n\n        if self.it <= self.args.exp.lr_rampup_it:\n            for g in self.optimizer.param_groups:\n                #learning rate ramp up\n                g['lr'] = self.args.exp.lr * min(self.it / max(self.args.exp.lr_rampup_it, 1e-8), 1)\n\n        #for param in self.network.parameters():\n        #    #take care of none gradients. Is that needed? \n        #    if param.grad is not None:\n        #        torch.nan_to_num(param.grad, nan=0, posinf=1e5, neginf=-1e5, out=param.grad)#This is needed for the loss scaling. That is what causes the nan gradients. \n\n        if self.args.exp.use_grad_clip:\n            torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.args.exp.max_grad_norm)\n\n        # Update weights.\n        self.optimizer.step()\n\n        end_time=time.time()\n        if self.args.logging.log:\n            self.process_loss_for_logging(error, sigma)\n\n        it_end_time = time.time()\n        print(\"it :\",self.it, \"time:, \",end_time-st_time, \"total_time: \",training_stats.report('it_time',it_end_time-it_start_time) ,\"loss: \", training_stats.report('loss', loss.item())) #TODO: take care of the logging\n\n\n    def update_ema(self):\n        \"\"\"Update exponential moving average of self.network weights.\"\"\"\n\n        ema_rampup = self.args.exp.ema_rampup  #ema_rampup should be set to 10000 in the config file\n        ema_rate=self.args.exp.ema_rate #ema_rate should be set to 0.9999 in the config file\n        t = self.it * self.args.exp.batch\n        with torch.no_grad():\n            if t < ema_rampup:\n                s = np.clip(t / ema_rampup, 0.0, ema_rate)\n                for dst, src in zip(self.ema.parameters(), self.network.parameters()):\n                    dst.copy_(dst * s + src * (1-s))\n            else:\n                for dst, src in zip(self.ema.parameters(), self.network.parameters()):\n                    dst.copy_(dst * ema_rate + src * (1-ema_rate))\n\n    def easy_logging(self):\n        \"\"\"\n         Do the simplest logging here. This will be called every 1000 iterations or so\n        I will use the training_stats.report function for this, and aim to report the means and stds of the losses in wandb\n        \"\"\"\n        training_stats.default_collector.update()\n        #Is it a good idea to log the stds of the losses? I think it is not.\n        loss_mean=training_stats.default_collector.mean('loss')\n        self.wandb_run.log({'loss':loss_mean}, step=self.it)\n        loss_std=training_stats.default_collector.std('loss')\n        self.wandb_run.log({'loss_std':loss_std}, step=self.it)\n\n        it_time_mean=training_stats.default_collector.mean('it_time')\n        self.wandb_run.log({'it_time_mean':it_time_mean}, step=self.it)\n        it_time_std=training_stats.default_collector.std('it_time')\n        self.wandb_run.log({'it_time_std':it_time_std}, step=self.it)\n        \n        #here reporting the error respect to sigma. I should make a fancier plot too, with mean and std\n        sigma_means=[]\n        sigma_stds=[]\n        for i in range(len(self.sigma_bins)):\n            a=training_stats.default_collector.mean('error_sigma_'+str(self.sigma_bins[i]))\n            sigma_means.append(a)\n            self.wandb_run.log({'error_sigma_'+str(self.sigma_bins[i]):a}, step=self.it)\n            a=training_stats.default_collector.std('error_sigma_'+str(self.sigma_bins[i]))\n            sigma_stds.append(a)\n\n        \n        figure=utils_logging.plot_loss_by_sigma(sigma_means,sigma_stds, self.sigma_bins)\n        wandb.log({\"loss_dependent_on_sigma\": figure}, step=self.it, commit=True)\n\n\n        #TODO log here the losses at different noise levels. I don't know if these should be heavy\n        #TODO also log here the losses at different frequencies if we are reporting them. same as above\n\n    def heavy_logging(self):\n        \"\"\"\n        Do the heavy logging here. This will be called every 10000 iterations or so\n        \"\"\"\n        freq_means=[]\n        freq_stds=[]\n        freq_sigma_means=[]\n        freq_sigma_stds=[]\n        for j in range(len(self.freq_bins)):\n            a=training_stats.default_collector.mean('error_freq_'+str(self.freq_bins[j]))\n            freq_means.append(a)\n            self.wandb_run.log({'error_freq_'+str(self.freq_bins[j]):a}, step=self.it)\n\n            a=training_stats.default_collector.std('error_freq_'+str(self.freq_bins[j]))\n            freq_stds.append(a)\n            #I need to take care of this in some other way that is easier to visualize\n            omeans=[]\n            ostds=[]\n            for i in range(len(self.sigma_bins)):\n                a=training_stats.default_collector.mean('error_sigma_'+str(self.sigma_bins[i])+'_freq_'+str(self.freq_bins[j]))\n                omeans.append(a)\n                a=training_stats.default_collector.std('error_sigma_'+str(self.sigma_bins[i])+'_freq_'+str(self.freq_bins[j]))\n                ostds.append(a)\n                #wandb.log({'error_sigma_'+str(self.sigma_bins[i])+'_freq_'+str(self.freq_bins[j]):a}, step=self.it)\n                #this logging will not be so handy. I create a figure using plotly to nicely visualize the results\n            freq_sigma_means.append(omeans)\n            freq_sigma_stds.append(ostds)\n\n        figure=utils_logging.plot_loss_by_sigma(freq_means,freq_stds, self.freq_bins)\n        wandb.log({\"loss_dependent_on_freq\": figure}, step=self.it)\n\n\n        figure=utils_logging.plot_loss_by_sigma_and_freq(freq_sigma_means,freq_sigma_stds, self.sigma_bins, self.freq_bins)#TODO!!!\n        wandb.log({\"loss_dependent_on_freq_and_sigma\": figure}, step=self.it)\n\n        if self.do_test:\n\n            if self.latest_checkpoint is not None:\n                self.tester.load_checkpoint(self.latest_checkpoint)\n\n            preds=self.tester.sample_unconditional()\n            if \"inpainting\" in self.args.tester.modes:\n                preds=self.tester.test_inpainting()\n            if \"bwe\" in self.args.tester.modes:\n                preds=self.tester.test_bwe()\n            #self.log_audio(preds, \"unconditional_sampling\")\n\n        #TODO: call the unconditional generation function and log the audio samples\n    def log_audio(self,x, name):\n        string=name+\"_\"+self.args.tester.name\n        audio_path=utils_logging.write_audio_file(x,self.args.exp.sample_rate, string,path=self.args.model_dir)\n        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it)\n        #TODO: log spectrogram of the audio file to wandb\n        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(x, self.args.logging.stft)\n        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it)\n\n    def conditional_demos(self):\n        \"\"\"\n        Do the conditional demos here. This will be called every 10000 iterations or so\n        \"\"\"\n        #TODO: call the conditional generation function and log the audio samples\n        pass\n\n\n    def training_loop(self):\n        \n        # Initialize.\n\n        #ddp = torch.nn.parallel.DistributedDataParallel(net, device_ids=[device], broadcast_buffers=False)\n\n        while True:\n            # Accumulate gradients.\n\n            self.train_step()\n\n            self.update_ema()\n            \n            if self.profile and self.args.logging.log:\n                print(self.profile, self.profile_total_steps, self.it)\n                if self.it<self.profile_total_steps:\n                    self.profiler.step()\n                elif self.it==self.profile_total_steps +1:\n                    #log trace as an artifact in wandb\n                    profile_art = wandb.Artifact(f\"trace-{wandb.run.id}\", type=\"profile\")\n                    profile_art.add_file(glob(\"wandb/latest-run/tbprofile/*.pt.trace.json\")[0], \"trace.pt.trace.json\")\n                    wandb.log_artifact(profile_art)\n                    print(\"proiling done\")\n                elif self.it>self.profile_total_steps +1:\n                    self.profile=False\n\n\n\n            if self.it>0 and self.it%self.args.logging.save_interval==0 and self.args.logging.save_model:\n                #self.save_snapshot() #are the snapshots necessary? I think they are not.\n                self.save_checkpoint()\n\n            if self.it>0 and self.it%self.args.logging.log_feature_stats_interval==0 and self.args.logging.log_feature_stats:\n                self.log_feature_stats()\n\n            if self.it>0 and self.it%self.args.logging.heavy_log_interval==0 and self.args.logging.log:\n                self.heavy_logging()\n                #self.conditional_demos()\n\n            if self.it>0 and self.it%self.args.logging.log_interval==0 and self.args.logging.log:\n                self.easy_logging()\n\n\n            \n\n            # Update state.\n            self.it += 1", "\n\n    #----------------------------------------------------------------------------\n"]}
{"filename": "networks/cqtdiff+.py", "chunked_list": ["import torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nimport math as m\nimport torch\n#import torchaudio\ntorch.pi = torch.acos(torch.zeros(1)).item() * 2 # which is 3.1415927410125732\n\nfrom cqt_nsgt_pytorch import CQT_nsgt\nimport torchaudio", "from cqt_nsgt_pytorch import CQT_nsgt\nimport torchaudio\nimport einops\nimport math\n\n\"\"\"\nAs similar as possible to the original CQTdiff architecture, but using the octave-base representation of the CQT\nThis should be more memory efficient, and also more efficient in terms of computation, specially when using higher sampling rates.\nI am expecting similar performance to the original CQTdiff architecture, but faster. \nPerhaps the fact that I am using powers of 2 for the time sizes is critical for transient reconstruction. I should thest CQT matrix model with powers of 2, this requires modifying the CQT_nsgt_pytorch.py file.", "I am expecting similar performance to the original CQTdiff architecture, but faster. \nPerhaps the fact that I am using powers of 2 for the time sizes is critical for transient reconstruction. I should thest CQT matrix model with powers of 2, this requires modifying the CQT_nsgt_pytorch.py file.\n\"\"\"\ndef weight_init(shape, mode, fan_in, fan_out):\n    if mode == 'xavier_uniform': return np.sqrt(6 / (fan_in + fan_out)) * (torch.rand(*shape) * 2 - 1)\n    if mode == 'xavier_normal':  return np.sqrt(2 / (fan_in + fan_out)) * torch.randn(*shape)\n    if mode == 'kaiming_uniform': return np.sqrt(3 / fan_in) * (torch.rand(*shape) * 2 - 1)\n    if mode == 'kaiming_normal':  return np.sqrt(1 / fan_in) * torch.randn(*shape)\n    raise ValueError(f'Invalid init mode \"{mode}\"')\n\nclass Linear(torch.nn.Module):\n    def __init__(self, in_features, out_features, bias=True, init_mode='kaiming_normal', init_weight=1, init_bias=0):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        init_kwargs = dict(mode=init_mode, fan_in=in_features, fan_out=out_features)\n        self.weight = torch.nn.Parameter(weight_init([out_features, in_features], **init_kwargs) * init_weight)\n        self.bias = torch.nn.Parameter(weight_init([out_features], **init_kwargs) * init_bias) if bias else None\n\n    def forward(self, x):\n        x = x @ self.weight.to(x.dtype).t()\n        if self.bias is not None:\n            x = x.add_(self.bias.to(x.dtype))\n        return x", "\nclass Linear(torch.nn.Module):\n    def __init__(self, in_features, out_features, bias=True, init_mode='kaiming_normal', init_weight=1, init_bias=0):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        init_kwargs = dict(mode=init_mode, fan_in=in_features, fan_out=out_features)\n        self.weight = torch.nn.Parameter(weight_init([out_features, in_features], **init_kwargs) * init_weight)\n        self.bias = torch.nn.Parameter(weight_init([out_features], **init_kwargs) * init_bias) if bias else None\n\n    def forward(self, x):\n        x = x @ self.weight.to(x.dtype).t()\n        if self.bias is not None:\n            x = x.add_(self.bias.to(x.dtype))\n        return x", "\nclass Conv1d(torch.nn.Module):\n    def __init__(self,\n        in_channels, out_channels, kernel=1, bias=False, dilation=1,\n        init_mode='kaiming_normal', init_weight=1, init_bias=0,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.dilation = dilation\n        init_kwargs = dict(mode=init_mode, fan_in=in_channels*kernel, fan_out=out_channels*kernel)\n        self.weight = torch.nn.Parameter(weight_init([out_channels, in_channels, kernel], **init_kwargs) * init_weight) \n        self.bias = torch.nn.Parameter(weight_init([out_channels], **init_kwargs) * init_bias) if bias else None\n\n    def forward(self, x):\n        w = self.weight.to(x.dtype) if self.weight is not None else None\n        b = self.bias.to(x.dtype) if self.bias is not None else None\n        w_pad = w.shape[-1] // 2 if w is not None else 0\n        #f_pad = (f.shape[-1] - 1) // 2 if f is not None else 0\n        #print(x.shape, w.shape)\n        if w is not None:\n                x = torch.nn.functional.conv1d(x, w, padding=\"same\", dilation=self.dilation)\n        if b is not None:\n            x = x.add_(b.reshape(1, -1, 1))\n        return x", "class Conv2d(torch.nn.Module):\n    def __init__(self,\n        in_channels, out_channels, kernel=(1,1), bias=False, dilation=1,\n        init_mode='kaiming_normal', init_weight=1, init_bias=0,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.dilation = dilation\n        init_kwargs = dict(mode=init_mode, fan_in=in_channels*kernel[0]*kernel[1], fan_out=out_channels*kernel[0]*kernel[1])\n        self.weight = torch.nn.Parameter(weight_init([out_channels, in_channels, kernel[0], kernel[1]], **init_kwargs) * init_weight) \n        self.bias = torch.nn.Parameter(weight_init([out_channels], **init_kwargs) * init_bias) if bias else None\n\n    def forward(self, x):\n        w = self.weight.to(x.dtype) if self.weight is not None else None\n        b = self.bias.to(x.dtype) if self.bias is not None else None\n        w_pad = w.shape[-1] // 2 if w is not None else 0\n        #f_pad = (f.shape[-1] - 1) // 2 if f is not None else 0\n        if w is not None:\n                x = torch.nn.functional.conv2d(x, w, padding=\"same\", dilation=self.dilation)\n        if b is not None:\n            x = x.add_(b.reshape(1, -1, 1, 1))\n        return x", "\nclass LayerScale(nn.Module):\n    \"\"\"Layer scale from [Touvron et al 2021] (https://arxiv.org/pdf/2103.17239.pdf).\n    This rescales diagonaly residual outputs close to 0 initially, then learnt.\n    \"\"\"\n\n    def __init__(self, channels: int, init: float = 1e-4, channel_last=True):\n        \"\"\"\n        channel_last = False corresponds to (B, C, T) tensors\n        channel_last = True corresponds to (T, B, C) tensors\n        \"\"\"\n        super().__init__()\n        self.channel_last = channel_last\n        self.scale = nn.Parameter(torch.zeros(channels, requires_grad=True))\n        self.scale.data[:] = init\n\n    def forward(self, x):\n        if self.channel_last:\n            return self.scale * x\n        else:\n            return self.scale[:, None] * x", "\nclass BiasFreeLayerNorm(nn.Module):\n\n    def __init__(self, num_features, eps=1e-7):\n        super(BiasFreeLayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(1,1,num_features))\n        #self.beta = nn.Parameter(torch.zeros(1,num_features,1,1))\n        #self.beta = torch.zeros(1,num_features,1,1)\n        self.eps = eps\n\n    def forward(self, x):\n        N, T, C = x.size()\n        #x = x.view(N, self.num_groups ,-1,H,W)\n        #x=einops.rearrange(x, 'n t c -> n (t c)')\n        #mean = x.mean(-1, keepdim=True)\n        #var = x.var(-1, keepdim=True)\n\n        std=x.std(-1, keepdim=True) #reduce over channels and time\n        #var = x.var(-1, keepdim=True)\n\n        ## normalize\n        x = (x) / (std+self.eps)\n        # normalize\n        #x=einops.rearrange(x, 'n (t c) -> n t c', t=T)\n        #x = x.view(N,C,H,W)\n        return x * self.gamma", "\nclass BiasFreeGroupNorm(nn.Module):\n\n    def __init__(self, num_features, num_groups=32, eps=1e-7):\n        super(BiasFreeGroupNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(1,num_features,1,1))\n        #self.beta = nn.Parameter(torch.zeros(1,num_features,1,1))\n        #self.beta = torch.zeros(1,num_features,1,1)\n        self.num_groups = num_groups\n        self.eps = eps\n\n    def forward(self, x):\n        N, C, F, T = x.size()\n        #x = x.view(N, self.num_groups ,-1,H,W)\n        gc=C//self.num_groups\n        x=einops.rearrange(x, 'n (g gc) f t -> n g (gc f t)', g=self.num_groups, gc=gc)\n        #mean = x.mean(-1, keepdim=True)\n        #var = x.var(-1, keepdim=True)\n\n        std=x.std(-1, keepdim=True) #reduce over channels and time\n        #var = x.var(-1, keepdim=True)\n\n        ## normalize\n        x = (x) / (std+self.eps)\n        # normalize\n        x=einops.rearrange(x, 'n g (gc f t) -> n (g gc) f t', g=self.num_groups, gc=gc, f=F, t=T)\n        #x = x.view(N,C,H,W)\n        return x * self.gamma", "\n\n\nclass RFF_MLP_Block(nn.Module):\n    \"\"\"\n        Encoder of the noise level embedding\n        Consists of:\n            -Random Fourier Feature embedding\n            -MLP\n    \"\"\"\n    def __init__(self, emb_dim=512, rff_dim=32, init=None):\n        super().__init__()\n        self.RFF_freq = nn.Parameter(\n            16 * torch.randn([1, rff_dim]), requires_grad=False)\n        self.MLP = nn.ModuleList([\n            Linear(2*rff_dim, 128, **init),\n            Linear(128, 256, **init),\n            Linear(256, emb_dim, **init),\n        ])\n\n    def forward(self, sigma):\n        \"\"\"\n        Arguments:\n          sigma:\n              (shape: [B, 1], dtype: float32)\n\n        Returns:\n          x: embedding of sigma\n              (shape: [B, 512], dtype: float32)\n        \"\"\"\n        x = self._build_RFF_embedding(sigma)\n        for layer in self.MLP:\n            x = F.relu(layer(x))\n        return x\n\n    def _build_RFF_embedding(self, sigma):\n        \"\"\"\n        Arguments:\n          sigma:\n              (shape: [B, 1], dtype: float32)\n        Returns:\n          table:\n              (shape: [B, 64], dtype: float32)\n        \"\"\"\n        freqs = self.RFF_freq\n        table = 2 * np.pi * sigma * freqs\n        table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)\n        return table", "\nclass AddFreqEncodingRFF(nn.Module):\n    '''\n    [B, T, F, 2] => [B, T, F, 12]  \n    Generates frequency positional embeddings and concatenates them as 10 extra channels\n    This function is optimized for F=1025\n    '''\n    def __init__(self, f_dim, N):\n        super(AddFreqEncodingRFF, self).__init__()\n        self.N=N\n        self.RFF_freq = nn.Parameter(\n            16 * torch.randn([1, N]), requires_grad=False)\n\n\n        self.f_dim=f_dim #f_dim is fixed\n        embeddings=self.build_RFF_embedding()\n        self.embeddings=nn.Parameter(embeddings, requires_grad=False) \n\n        \n    def build_RFF_embedding(self):\n        \"\"\"\n        Returns:\n          table:\n              (shape: [C,F], dtype: float32)\n        \"\"\"\n        freqs = self.RFF_freq\n        #freqs = freqs.to(device=torch.device(\"cuda\"))\n        freqs=freqs.unsqueeze(-1) # [1, 32, 1]\n\n        self.n=torch.arange(start=0,end=self.f_dim)\n        self.n=self.n.unsqueeze(0).unsqueeze(0)  #[1,1,F]\n\n        table = 2 * np.pi * self.n * freqs\n\n        #print(freqs.shape, x.shape, table.shape)\n        table = torch.cat([torch.sin(table), torch.cos(table)], dim=1) #[1,32,F]\n\n        return table\n    \n\n    def forward(self, input_tensor):\n\n        #print(input_tensor.shape)\n        batch_size_tensor = input_tensor.shape[0]  # get batch size\n        time_dim = input_tensor.shape[-1]  # get time dimension\n\n        fembeddings_2 = torch.broadcast_to(self.embeddings, [batch_size_tensor, time_dim,self.N*2, self.f_dim])\n        fembeddings_2=fembeddings_2.permute(0,2,3,1)\n    \n        \n        #print(input_tensor.shape, fembeddings_2.shape)\n        return torch.cat((input_tensor,fembeddings_2),1)  ", "\n\nclass RelativePositionBias(nn.Module):\n    def __init__(self, num_buckets: int, max_distance: int, num_heads: int):\n        super().__init__()\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.num_heads = num_heads\n        self.relative_attention_bias = nn.Embedding(num_buckets, num_heads)\n\n    @staticmethod\n    def _relative_position_bucket(\n        relative_position, num_buckets: int, max_distance: int\n    ):\n        num_buckets //= 2\n        ret = (relative_position >= 0).to(torch.long) * num_buckets\n        n = torch.abs(relative_position)\n\n        max_exact = num_buckets // 2\n        is_small = n < max_exact\n\n        val_if_large = (\n            max_exact\n            + (\n                torch.log(n.float() / max_exact)\n                / math.log(max_distance / max_exact)\n                * (num_buckets - max_exact)\n            ).long()\n        )\n        val_if_large = torch.min(\n            val_if_large, torch.full_like(val_if_large, num_buckets - 1)\n        )\n\n        ret += torch.where(is_small, n, val_if_large)\n        return ret\n\n    def forward(self, num_queries: int, num_keys: int):\n        i, j, device = num_queries, num_keys, self.relative_attention_bias.weight.device\n        q_pos = torch.arange(j - i, j, dtype=torch.long, device=device)\n        k_pos = torch.arange(j, dtype=torch.long, device=device)\n        rel_pos = einops.rearrange(k_pos, \"j -> 1 j\") - einops.rearrange(q_pos, \"i -> i 1\")\n\n        relative_position_bucket = self._relative_position_bucket(\n            rel_pos, num_buckets=self.num_buckets, max_distance=self.max_distance\n        )\n\n        bias = self.relative_attention_bias(relative_position_bucket)\n        bias = einops.rearrange(bias, \"m n h -> 1 h m n\")\n        return bias", "\nclass TimeAttentionBlock(nn.Module):\n    def __init__(self, Nin,attention_dict, init, init_zero, Fdim) -> None:\n        super().__init__()\n        #NA=attention_dict.N\n        self.attention_dict=attention_dict\n        self.Fdim=Fdim\n        N=attention_dict.num_heads*Fdim \n        self.qk = Conv1d(N, N*2, bias=self.attention_dict.bias_qkv, **init )\n        self.proj_in=Conv2d(Nin, attention_dict.num_heads, (1,1), bias=False, **init)\n        self.proj_out=Conv2d(attention_dict.num_heads, Nin, (1,1), bias=False, **init)\n        #not sure if a bias is a good idea here\n        #self.v = Conv2d(N, N*2, (1,1), bias=False,**init )\n        #I think that as long as the main signal path layers are bias free, we should be safe from artifacts\n        #self.proj = Conv1d(NA, NA, 1, bias=False, **init)\n\n        self.scale=(N/self.attention_dict.num_heads)**-0.5\n        self.use_rel_pos = self.attention_dict.use_rel_pos\n        if self.use_rel_pos:\n            self.rel_pos = RelativePositionBias(\n                num_buckets=attention_dict.rel_pos_num_buckets,\n                max_distance=attention_dict.rel_pos_max_distance,\n                num_heads=attention_dict.num_heads,\n            )\n\n    def forward(self, x):\n        #shape of x is [batch, C,F, T]\n\n        #we need shape: [batch, heads, T, D]\n        #with heands on different (original) channels\n        #print(x.shape, self.Fdim)\n\n        x=self.proj_in(x) #reduce the C dimensionality\n\n        #print(x.shape, self.Fdim)\n        #normalize everyting (easy)\n\n        #split into heads\n        x=einops.rearrange(x, \"b h f t -> b (h f) t\")\n\n        v=einops.rearrange(x,\"b (h f) t -> b h t f\", f=self.Fdim) #identity layer for the values\n\n        qk=self.qk(x) #linear layer\n        #for now, f are features (all merged) but still represents frequency\n\n        qk=einops.rearrange(qk, \"b (h d) t -> b h t d\", h=self.attention_dict.num_heads)\n        q,k=qk.chunk(2,dim=-1)\n\n        #print(\"qk\",q.shape, k.shape)\n        sim = torch.einsum(\"... n d, ... m d -> ... n m\", q, k)\n        #print(\"sim\",sim.shape)\n        sim = (sim + self.rel_pos(*sim.shape[-2:])) if self.use_rel_pos else sim\n        #print(\"sim\",sim.shape)\n        sim = sim * self.scale\n        # Get attention matrix with softmax\n        attn = sim.softmax(dim=-1)\n        # Compute values\n        #print(\"attn\",attn.shape, v.shape)\n        out = torch.einsum(\"... n m, ... m d -> ... n d\", attn, v)\n\n        #print(\"out\",out.shape)\n        out = einops.rearrange(out, \"b h t f -> b h f t\", f=self.Fdim)\n        #out = einops.rearrange(out, \"b (h f) t -> b h f t\", f=self.Fdim)\n\n        #reverse step\n        out=self.proj_out(out)\n\n        return out", "        \nclass ResnetBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        use_norm=True,\n        num_dils = 6,\n        bias=False,\n        kernel_size=(5,3),\n        emb_dim=512,\n        proj_place='before', #using 'after' in the decoder out blocks\n        init=None,\n        init_zero=None,\n        attention_dict=None,\n        Fdim=128, #number of frequency bins\n    ):\n        super().__init__()\n\n        self.bias=bias\n        self.use_norm=use_norm\n        self.num_dils=num_dils\n        self.proj_place=proj_place\n        self.Fdim=Fdim\n\n        if self.proj_place=='before':\n            #dim_out is the block dimension\n            N=dim_out\n        else:\n            #dim in is the block dimension\n            N=dim\n            self.proj_out = Conv2d(N, dim_out,   bias=bias, **init) if N!=dim_out else nn.Identity() #linear projection\n\n        self.res_conv = Conv2d(dim, dim_out, bias=bias, **init) if dim!= dim_out else nn.Identity() #linear projection\n        self.proj_in = Conv2d(dim, N,   bias=bias, **init) if dim!=N else nn.Identity()#linear projection\n\n\n\n        self.H=nn.ModuleList()\n        self.affine=nn.ModuleList()\n        self.gate=nn.ModuleList()\n        if self.use_norm:\n            self.norm=nn.ModuleList()\n\n        for i in range(self.num_dils):\n\n            if self.use_norm:\n                self.norm.append(BiasFreeGroupNorm(N,8))\n\n            self.affine.append(Linear(emb_dim, N, **init))\n            self.gate.append(Linear(emb_dim, N, **init_zero))\n            #self.H.append(Gated_residual_layer(dim_out, (5,3), (2**i,1), bias=bias)) #sometimes I changed this 1,5 to 3,5. be careful!!! (in exp 80 as far as I remember)\n            self.H.append(Conv2d(N,N,    \n                                    kernel=kernel_size,\n                                    dilation=(2**i,1),\n                                    bias=bias, **init)) #freq convolution (dilated) \n\n        self.attention_dict=attention_dict\n        if self.attention_dict is not None:\n            #NA=self.attention_dict.N\n            self.norm2=BiasFreeGroupNorm(N,8)\n            self.affine2=Linear(emb_dim, N, **init)\n            self.gate2=Linear(emb_dim, N, **init_zero)\n            #self.norm2 = BiasFreeGroupNorm(N,8)\n            #self.proj_attn_in = Conv1d(N*Fdim, NA,   bias=bias, **init) if (N*Fdim)!=NA else nn.Identity()#linear projection\n            #self.proj_attn_out = Conv1d(NA, N*Fdim,   bias=bias, **init_zero) if NA!=(N*Fdim) else nn.Identity() #linear projection\n            ##the attention is applied time-wise, since channels times frequency is too much, we need to reduce the dimensionality using a linear projection\n            self.attn_block=TimeAttentionBlock(N,self.attention_dict, init,init_zero, self.Fdim)\n\n\n\n    def forward(self, input_x, sigma):\n        \n        x=input_x\n\n        x=self.proj_in(x)\n\n        if self.attention_dict is not None:\n            i_x=x\n\n            gamma=self.affine2(sigma)\n            scale=self.gate2(sigma)\n\n            x=self.norm2(x)\n            x=x*(gamma.unsqueeze(2).unsqueeze(3)+1) #no bias\n\n            x=self.attn_block(x)*scale.unsqueeze(2).unsqueeze(3)\n\n            #x=(x+i_x)\n            x=(x+i_x)/(2**0.5)\n\n        for norm, affine, gate, conv in zip(self.norm, self.affine, self.gate, self.H):\n            x0=x\n            if self.use_norm:\n                x=norm(x)\n            gamma =affine(sigma)\n            scale=gate(sigma)\n\n            x=x*(gamma.unsqueeze(2).unsqueeze(3)+1) #no bias\n\n\n            x=(x0+conv(F.gelu(x))*scale.unsqueeze(2).unsqueeze(3))/(2**0.5) \n            #x=(x0+conv(F.gelu(x))*scale.unsqueeze(2).unsqueeze(3))\n        \n        #one residual connection here after the dilated convolutions\n\n\n        if self.proj_place=='after':\n            x=self.proj_out(x)\n\n        x=(x + self.res_conv(input_x))/(2**0.5)\n\n        return x", "\n\nclass AttentionOp(torch.autograd.Function):\n\n    def forward(ctx, q, k):\n        w = torch.einsum('ncq,nck->nqk', q.to(torch.float32), (k / np.sqrt(k.shape[1])).to(torch.float32)).softmax(dim=2).to(q.dtype)\n        ctx.save_for_backward(q, k, w)\n        return w\n\n    def backward(ctx, dw):\n        q, k, w = ctx.saved_tensors\n        db = torch._softmax_backward_data(grad_output=dw.to(torch.float32), output=w.to(torch.float32), dim=2, input_dtype=torch.float32)\n        dq = torch.einsum('nck,nqk->ncq', k.to(torch.float32), db).to(q.dtype) / np.sqrt(k.shape[1])\n        dk = torch.einsum('ncq,nqk->nck', q.to(torch.float32), db).to(k.dtype) / np.sqrt(k.shape[1])\n        return dq, dk", "\n_kernels = {\n    'linear':\n        [1 / 8, 3 / 8, 3 / 8, 1 / 8],\n    'cubic': \n        [-0.01171875, -0.03515625, 0.11328125, 0.43359375,\n        0.43359375, 0.11328125, -0.03515625, -0.01171875],\n    'lanczos3': \n        [0.003689131001010537, 0.015056144446134567, -0.03399861603975296,\n        -0.066637322306633, 0.13550527393817902, 0.44638532400131226,", "        [0.003689131001010537, 0.015056144446134567, -0.03399861603975296,\n        -0.066637322306633, 0.13550527393817902, 0.44638532400131226,\n        0.44638532400131226, 0.13550527393817902, -0.066637322306633,\n        -0.03399861603975296, 0.015056144446134567, 0.003689131001010537]\n}\nclass UpDownResample(nn.Module):\n    def __init__(self,\n        up=False, \n        down=False,\n        mode_resample=\"T\", #T for time, F for freq, TF for both\n        resample_filter='cubic', \n        pad_mode='reflect'\n        ):\n        super().__init__()\n        assert not (up and down) #you cannot upsample and downsample at the same time\n        assert up or down #you must upsample or downsample\n        self.down=down\n        self.up=up\n        if up or down:\n            #upsample block\n            self.pad_mode = pad_mode #I think reflect is a goof choice for padding\n            self.mode_resample=mode_resample\n            if mode_resample==\"T\":\n                kernel_1d = torch.tensor(_kernels[resample_filter], dtype=torch.float32)\n            elif mode_resample==\"F\":\n                #kerel shouuld be the same\n                kernel_1d = torch.tensor(_kernels[resample_filter], dtype=torch.float32)\n            else:\n                raise NotImplementedError(\"Only time upsampling is implemented\")\n                #TODO implement freq upsampling and downsampling\n            self.pad = kernel_1d.shape[0] // 2 - 1\n            self.register_buffer('kernel', kernel_1d)\n    def forward(self, x):\n        shapeorig=x.shape\n        #x=x.view(x.shape[0],-1,x.shape[-1])\n        x=x.view(-1,x.shape[-2],x.shape[-1]) #I have the feeling the reshape makes everything consume too much memory. There is no need to have the channel dimension different than 1. I leave it like this because otherwise it requires a contiguous() call, but I should check if the memory gain / speed, would be significant.\n        if self.mode_resample==\"F\":\n            x=x.permute(0,2,1)#call contiguous() here?\n\n        #print(\"after view\",x.shape)\n        if self.down:\n            x = F.pad(x, (self.pad,) * 2, self.pad_mode)\n        elif self.up:\n            x = F.pad(x, ((self.pad + 1) // 2,) * 2, self.pad_mode)\n\n        #print(\"after pad\",x.shape)\n\n        weight = x.new_zeros([x.shape[1], x.shape[1], self.kernel.shape[0]])\n        #print(\"weight\",weight.shape)\n        indices = torch.arange(x.shape[1], device=x.device)\n        #print(\"indices\",indices.shape)\n        #weight = self.kernel.to(x.device).unsqueeze(0).unsqueeze(0).expand(x.shape[1], x.shape[1], -1)\n        #print(\"weight\",weight.shape)\n        weight[indices, indices] = self.kernel.to(weight)\n        if self.down:\n            x_out= F.conv1d(x, weight, stride=2)\n        elif self.up:\n            x_out =F.conv_transpose1d(x, weight, stride=2, padding=self.pad * 2 + 1)\n\n        if self.mode_resample==\"F\":\n            x_out=x_out.permute(0,2,1).contiguous()\n            return x_out.view(shapeorig[0],-1,x_out.shape[-2], shapeorig[-1])\n        else:\n            return x_out.view(shapeorig[0],-1,shapeorig[2], x_out.shape[-1])", "\n\nclass Unet_CQT_oct_with_attention(nn.Module):\n    \"\"\"\n        Main U-Net model based on the CQT\n    \"\"\"\n    def __init__(self, args, device):\n        \"\"\"\n        Args:\n            args (dictionary): hydra dictionary\n            device: torch device (\"cuda\" or \"cpu\")\n        \"\"\"\n        super(Unet_CQT_oct_with_attention, self).__init__()\n        self.args=args\n        self.depth=args.network.cqt.num_octs\n        #self.depth=args.network.inner_depth+self.args.network.cqt.num_octs\n        #assert self.depth==args.network.depth, \"The depth of the network should be the sum of the inner depth and the number of octaves\" #make sure we are aware of the depth of the network\n\n        init = dict(init_mode='kaiming_uniform', init_weight=np.sqrt(1/3)) #same as ADM, according to edm implementation\n        init_zero = dict(init_mode='kaiming_uniform', init_weight=1e-7) #I think it is safer to initialize the last layer with a small weight, rather than zero. Breaking symmetry and all that. \n\n\n\n        self.emb_dim=args.network.emb_dim\n        self.embedding = RFF_MLP_Block(emb_dim=args.network.emb_dim, init=init)\n        self.use_norm=args.network.use_norm\n\n        #fmax=self.args.exp.sample_rate/2\n        #self.fmin=fmax/(2**self.args.cqt.numocts)\n        self.fbins=int(self.args.network.cqt.bins_per_oct*self.args.network.cqt.num_octs) \n        self.device=device\n        self.bins_per_oct=self.args.network.cqt.bins_per_oct\n        self.num_octs=self.args.network.cqt.num_octs\n        #self.CQTransform=CQT_nsgt(self.args.network.cqt.num_octs,self.args.network.cqt.bins_per_oct, \"oct\",  self.args.exp.sample_rate, self.args.exp.audio_len, device=self.device)\n        if self.args.network.cqt.window==\"kaiser\":\n            win=(\"kaiser\",self.args.network.cqt.beta)\n        else:\n            win=self.args.network.cqt.window\n\n        self.CQTransform=CQT_nsgt(self.args.network.cqt.num_octs, self.args.network.cqt.bins_per_oct, mode=\"oct\",window=win,fs=self.args.exp.sample_rate, audio_len=self.args.exp.audio_len, dtype=torch.float32, device=self.device)\n\n\n        self.f_dim=self.fbins #assuming we have thrown away the DC component and the Nyquist frequency\n\n        self.use_fencoding=self.args.network.use_fencoding\n        if self.use_fencoding:\n            N_freq_encoding=32\n    \n            self.freq_encodings=nn.ModuleList([])\n            for i in range(self.num_octs):\n                self.freq_encodings.append(AddFreqEncodingRFF(self.bins_per_oct,N_freq_encoding))\n            Nin=2*N_freq_encoding+2\n        else:\n            Nin=2\n\n        #Encoder\n        self.Ns= self.args.network.Ns\n        self.Ss= self.args.network.Ss\n\n        self.num_dils= self.args.network.num_dils #intuition: less dilations for the first layers and more for the deeper layers\n        #self.inner_num_dils=self.args.network.inner_num_dils\n        \n        self.attention_dict=self.args.network.attention_dict\n        #self.attention_Ns=self.args.network.attention_Ns\n\n        self.downsamplerT=UpDownResample(down=True, mode_resample=\"T\")\n        #self.downsamplerF=UpDownResample(down=True, mode_resample=\"F\")\n        self.upsamplerT=UpDownResample(up=True, mode_resample=\"T\")\n        #self.upsamplerF=UpDownResample(up=True, mode_resample=\"F\")\n\n        self.downs=nn.ModuleList([])\n        self.middle=nn.ModuleList([])\n        self.ups=nn.ModuleList([])\n\n        self.attention_layers=self.args.network.attention_layers\n        #sth like [0,0,0,0,0,0,1,1]\n        \n        for i in range(self.num_octs):\n            if i==0:\n                dim_in=self.Ns[i]\n                dim_out=self.Ns[i]\n            else:\n                dim_in=self.Ns[i-1]\n                dim_out=self.Ns[i]\n            if self.attention_layers[i]:\n                print(\"Attention layer at (down) octave {}\".format(i))\n                attn_dict=self.attention_dict\n                #attn_dict.N=self.attention_Ns[i]\n                #assert attn_dict.N > 0\n            else:\n                attn_dict=None\n\n            self.downs.append(\n                               nn.ModuleList([\n                                        ResnetBlock(Nin, dim_in, self.use_norm,num_dils=1, bias=False, kernel_size=(1,1), emb_dim=self.emb_dim, init=init, init_zero=init_zero),\n                                        Conv2d(2, dim_out, kernel=(5,3), bias=False, **init),\n                                        ResnetBlock(dim_in, dim_out, self.use_norm,num_dils=self.num_dils[i], bias=False , attention_dict=attn_dict, emb_dim=self.emb_dim, init=init, init_zero=init_zero, Fdim=(i+1)*self.bins_per_oct)\n                                        ]))\n\n        if self.args.network.bottleneck_type==\"res_dil_convs\":\n            for i in range(self.args.network.num_bottleneck_layers):\n                if self.attention_layers[-1]:\n                    attn_dict=self.attention_dict\n                    #attn_dict.N=self.attention_Ns[-1]\n                    #assert attn_dict.N > 0\n                else:\n                    attn_dict=None\n    \n                self.middle.append(nn.ModuleList([\n                                ResnetBlock(self.Ns[-1], 2, use_norm=self.use_norm,num_dils= 1,bias=False, kernel_size=(1,1), proj_place=\"after\", emb_dim=self.emb_dim, init=init, init_zero=init_zero),\n                                ResnetBlock(self.Ns[-1], self.Ns[-1], self.use_norm, num_dils=self.num_dils[-1], bias=False, emb_dim=self.emb_dim,attention_dict=attn_dict, init=init, init_zero=init_zero,\n                                Fdim=(self.num_octs)*self.bins_per_oct)]))\n        else:\n            raise NotImplementedError(\"bottleneck type not implemented\")\n                        \n\n\n        #self.pyr_up_proj_first=nn.Conv2d(dim_out, 2, (5,3), padding=\"same\", padding_mode=\"zeros\", bias=False)\n        \n        for i in range(self.num_octs-1,-1,-1):\n\n            if i==0:\n                dim_in=self.Ns[i]*2\n                dim_out=self.Ns[i]\n            else:\n                dim_in=self.Ns[i]*2\n                dim_out=self.Ns[i-1]\n\n            if self.attention_layers[i]:\n                print(\"Attention layer at (up) oct layer {}\".format(i))\n                attn_dict=self.attention_dict\n                #attn_dict.N=self.attention_Ns[i]\n                #assert attn_dict.N > 0\n            else:\n                attn_dict=None\n\n            self.ups.append(nn.ModuleList(\n                                        [\n                                        ResnetBlock(dim_out, 2, use_norm=self.use_norm,num_dils= 1,bias=False, kernel_size=(1,1), proj_place=\"after\", emb_dim=self.emb_dim, init=init, init_zero=init_zero),\n                                        ResnetBlock(dim_in, dim_out, use_norm=self.use_norm,num_dils= self.num_dils[i],attention_dict=attn_dict, bias=False, emb_dim=self.emb_dim, init=init, init_zero=init_zero, Fdim=(i+1)*self.bins_per_oct),\n                                        ]))\n\n\n\n        #self.cropconcat = CropConcatBlock()\n\n\n\n\n    def forward(self, inputs, sigma):\n        \"\"\"\n        Args: \n            inputs (Tensor):  Input signal in time-domsin, shape (B,T)\n            sigma (Tensor): noise levels,  shape (B,1)\n        Returns:\n            pred (Tensor): predicted signal in time-domain, shape (B,T)\n        \"\"\"\n        #apply RFF embedding+MLP of the noise level\n        sigma = self.embedding(sigma)\n\n        \n        #apply CQT to the inputs\n        X_list =self.CQTransform.fwd(inputs.unsqueeze(1))\n        X_list_out=X_list\n\n        hs=[]\n        for i,modules in enumerate(self.downs):\n            #print(\"downsampler\", i)\n            if i <=(self.num_octs-1):\n                C=X_list[-1-i]#get the corresponding CQT octave\n                C=C.squeeze(1)\n                C=torch.view_as_real(C)\n                C=C.permute(0,3,1,2).contiguous() # call contiguous() here?\n                if self.use_fencoding:\n                    #Cfreq=self.freq_encoding(C)\n                    C2=self.freq_encodings[i](C) #B, C + Nfreq*2, F,T\n                else:\n                    C2=C\n                    \n                init_block, pyr_down_proj, ResBlock=modules\n                C2=init_block(C2,sigma)\n            else:\n                pyr_down_proj, ResBlock=modules\n            \n            if i==0:\n                X=C2 #starting the main signal path\n                pyr=self.downsamplerT(C) #starting the auxiliary path\n            elif i<(self.num_octs-1):\n                pyr=torch.cat((self.downsamplerT(C),self.downsamplerT(pyr)),dim=2) #updating the auxiliary path\n                X=torch.cat((C2,X),dim=2) #updating the main signal path with the new octave\n            elif i==(self.num_octs-1):# last layer\n                #pyr=torch.cat((self.downsamplerF(C),self.downsamplerF(pyr)),dim=2) #updating the auxiliary path\n                pyr=torch.cat((C,pyr), dim=2) #no downsampling in the last layer\n                X=torch.cat((C2,X),dim=2) #updating the main signal path with the new octave\n            else: #last layer\n                pass\n                #pyr=pyr\n                #X=X\n\n            X=ResBlock(X, sigma)\n            hs.append(X)\n\n            #downsample the main signal path\n            #we do not need to downsample in the inner layer\n            if i<(self.num_octs-1): \n                X=self.downsamplerT(X)\n                #apply the residual connection\n                #X=(X+pyr_down_proj(pyr))/(2**0.5) #I'll my need to put that inside a combiner block??\n            else: #last layer\n                #no downsampling in the last layer\n                pass\n\n            #apply the residual connection\n            X=(X+pyr_down_proj(pyr))/(2**0.5) #I'll my need to put that inside a combiner block??\n            #print(\"encoder \", i, X.shape, X.mean().item(), X.std().item())\n                \n\n        #middle layers\n        #print(\"bttleneck\")\n        if self.args.network.bottleneck_type==\"res_dil_convs\":\n            for i in range(self.args.network.num_bottleneck_layers):\n                OutBlock, ResBlock =self.middle[i]\n                X=ResBlock(X, sigma)   \n                Xout=OutBlock(X,sigma)\n\n\n        for i,modules in enumerate(self.ups):\n            j=len(self.ups) -i-1\n            #print(\"upsampler\", j)\n\n            OutBlock,  ResBlock=modules\n\n            skip=hs.pop()\n            X=torch.cat((X,skip),dim=1)\n            X=ResBlock(X, sigma)\n            \n            Xout=(Xout+OutBlock(X,sigma))/(2**0.5)\n\n\n            if j<=(self.num_octs-1):\n                X= X[:,:,self.bins_per_oct::,:]\n                Out, Xout= Xout[:,:,0:self.bins_per_oct,:], Xout[:,:,self.bins_per_oct::,:]\n                #pyr_out, pyr= pyr[:,:,0:self.bins_per_oct,:], pyr[:,:,self.bins_per_oct::,:]\n                #X_out=(pyr_up_proj(X_out)+pyr_out)/(2**0.5)\n\n                Out=Out.permute(0,2,3,1).contiguous() #call contiguous() here?\n                Out=torch.view_as_complex(Out)\n\n                #save output\n                X_list_out[i]=Out.unsqueeze(1)\n\n            elif j>(self.num_octs-1):\n                print(\"We should not be here\")\n                pass\n\n            if j>0 and j<=(self.num_octs-1):\n                #pyr=self.upsampler(pyr) #call contiguous() here?\n                X=self.upsamplerT(X) #call contiguous() here?\n                Xout=self.upsamplerT(Xout) #call contiguous() here?\n\n        pred_time=self.CQTransform.bwd(X_list_out)\n        pred_time=pred_time.squeeze(1)\n        pred_time=pred_time[:,0:inputs.shape[-1]]\n        assert pred_time.shape==inputs.shape, \"bad shapes\"\n        return pred_time", "\n            \n\n\nclass CropAddBlock(nn.Module):\n\n    def forward(self,down_layer, x,  **kwargs):\n        x1_shape = down_layer.shape\n        x2_shape = x.shape\n\n        #print(x1_shape,x2_shape)\n        height_diff = (x1_shape[2] - x2_shape[2]) // 2\n        width_diff = (x1_shape[3] - x2_shape[3]) // 2\n\n\n        down_layer_cropped = down_layer[:,\n                                        :,\n                                        height_diff: (x2_shape[2] + height_diff),\n                                        width_diff: (x2_shape[3] + width_diff),:]\n        x = torch.add(down_layer_cropped, x)\n        return x", "\nclass CropConcatBlock(nn.Module):\n\n    def forward(self, down_layer, x, **kwargs):\n        x1_shape = down_layer.shape\n        x2_shape = x.shape\n\n        height_diff = (x1_shape[2] - x2_shape[2]) // 2\n        width_diff = (x1_shape[3] - x2_shape[3]) // 2\n        down_layer_cropped = down_layer[:,\n                                        :,\n                                        height_diff: (x2_shape[2] + height_diff),\n                                        width_diff: (x2_shape[3] + width_diff)]\n        x = torch.cat((down_layer_cropped, x),1)\n        return x", "\n"]}
{"filename": "networks/denoiser.py", "chunked_list": ["import torch.nn as nn\nimport math as m\nimport torch\n#import torchaudio\ntorch.pi = torch.acos(torch.zeros(1)).item() * 2 # which is 3.1415927410125732\n\n#def build_model_denoise(unet_args=None):\n#\n#    inputs=Input(shape=(None, None,2))\n#", "#    inputs=Input(shape=(None, None,2))\n#\n#    outputs_stage_2,outputs_stage_1=MultiStage_denoise(unet_args=unet_args)(inputs)\n#\n#    #Encapsulating MultiStage_denoise in a keras.Model object\n#    model= tf.keras.Model(inputs=inputs,outputs=[outputs_stage_2, outputs_stage_1])\n\n#    return model\n\nclass DenseBlock(nn.Module):\n    '''\n    [B, T, F, N] => [B, T, F, N] \n    DenseNet Block consisting of \"num_layers\" densely connected convolutional layers\n    '''\n    def __init__(self, num_layers,N0, N, ksize):\n        '''\n        num_layers:     number of densely connected conv. layers\n        N:              Number of filters (same in each layer) \n        ksize:          Kernel size (same in each layer) \n        '''\n        super(DenseBlock, self).__init__()\n\n        self.H=nn.ModuleList()\n        self.num_layers=num_layers\n\n        for i in range(num_layers):\n            if i==0:   \n                Nin=N0\n            else:\n                Nin=N0+i*N\n             \n            self.H.append(nn.Sequential(\n                                nn.Conv2d(Nin,N,\n                                      kernel_size=ksize,\n                                      stride=1,\n                                      padding='same',\n                                      padding_mode='reflect',\n                                      ),\n                                nn.ELU()        ))\n\n    def forward(self, x):\n        x_ = self.H[0](x)\n        if self.num_layers>1:\n            for h in self.H[1:]:\n                x = torch.cat((x_, x), 1)\n                #x_=tf.pad(x, self.padding_modes_1, mode='SYMMETRIC')\n                x_ = h(x)  \n                #add elu here\n\n        return x_", "\nclass DenseBlock(nn.Module):\n    '''\n    [B, T, F, N] => [B, T, F, N] \n    DenseNet Block consisting of \"num_layers\" densely connected convolutional layers\n    '''\n    def __init__(self, num_layers,N0, N, ksize):\n        '''\n        num_layers:     number of densely connected conv. layers\n        N:              Number of filters (same in each layer) \n        ksize:          Kernel size (same in each layer) \n        '''\n        super(DenseBlock, self).__init__()\n\n        self.H=nn.ModuleList()\n        self.num_layers=num_layers\n\n        for i in range(num_layers):\n            if i==0:   \n                Nin=N0\n            else:\n                Nin=N0+i*N\n             \n            self.H.append(nn.Sequential(\n                                nn.Conv2d(Nin,N,\n                                      kernel_size=ksize,\n                                      stride=1,\n                                      padding='same',\n                                      padding_mode='reflect',\n                                      ),\n                                nn.ELU()        ))\n\n    def forward(self, x):\n        x_ = self.H[0](x)\n        if self.num_layers>1:\n            for h in self.H[1:]:\n                x = torch.cat((x_, x), 1)\n                #x_=tf.pad(x, self.padding_modes_1, mode='SYMMETRIC')\n                x_ = h(x)  \n                #add elu here\n\n        return x_", "\n\nclass FinalBlock(nn.Module):\n    '''\n    [B, T, F, N] => [B, T, F, 2] \n    Final block. Basiforwardy, a 3x3 conv. layer to map the output features to the output complex spectrogram.\n\n    '''\n    def __init__(self, N0):\n        super(FinalBlock, self).__init__()\n        ksize=(3,3)\n        self.conv2=nn.Conv2d(N0,out_channels=2,\n                      kernel_size=ksize,\n                      stride=1, \n                      padding='same',\n                      padding_mode='reflect')\n\n\n    def forward(self, inputs ):\n\n        pred=self.conv2(inputs)\n\n        return pred", "\nclass SAM(nn.Module):\n    '''\n    [B, T, F, N] => [B, T, F, N] , [B, T, F, N]\n    Supervised Attention Module:\n    The purpose of SAM is to make the network only propagate the most relevant features to the second stage, discarding the less useful ones.\n    The estimated residual noise signal is generated from the U-Net output features by means of a 3x3 convolutional layer. \n    The first stage output is then calculated adding the original input spectrogram to the residual noise. \n    The attention-guided features are computed using the attention masks M, which are directly calculated from the first stage output with a 1x1 convolution and a sigmoid function. \n\n    '''\n    def __init__(self, n_feat):\n        super(SAM, self).__init__()\n\n        ksize=(3,3)\n        self.conv1 = nn.Conv2d(n_feat,out_channels=n_feat,\n                      kernel_size=ksize,\n                      stride=1, \n                      padding='same',\n                      padding_mode='reflect')\n        ksize=(3,3)\n        self.conv2=nn.Conv2d( n_feat,2,\n                      kernel_size=ksize,\n                      stride=1, \n                      padding='same',\n                      padding_mode='reflect')\n\n        ksize=(3,3)\n        self.conv3 = nn.Conv2d(2,n_feat,\n                      kernel_size=ksize,\n                      stride=1, \n                      padding='same',\n                      padding_mode='reflect')\n\n        #self.cropadd=CropAddBlock()\n\n    def forward(self, inputs, input_spectrogram):\n        x1 = self.conv1(inputs)\n        x=self.conv2(inputs)\n\n        #residual prediction\n        pred = torch.add(x, input_spectrogram) #features to next stage\n\n        M=self.conv3(pred)\n\n        M= torch.sigmoid(M)\n        x1=torch.multiply(x1, M)\n        x1 = torch.add(x1, inputs) #features to next stage\n\n        return x1, pred", "\n\nclass AddFreqEncoding(nn.Module):\n    '''\n    [B, T, F, 2] => [B, T, F, 12]  \n    Generates frequency positional embeddings and concatenates them as 10 extra channels\n    This function is optimized for F=1025\n    '''\n    def __init__(self, f_dim):\n        super(AddFreqEncoding, self).__init__()\n        pi=torch.pi\n        self.f_dim=f_dim #f_dim is fixed\n        n=torch.arange(start=0,end=f_dim)/(f_dim-1)\n        # n=n.type(torch.FloatTensor)\n        coss=torch.cos(pi*n)\n        f_channel = torch.unsqueeze(coss, -1) #(1025,1)\n        self.fembeddings= f_channel\n        \n        for k in range(1,10):   \n            coss=torch.cos(2**k*pi*n)\n            f_channel = torch.unsqueeze(coss, -1) #(1025,1)\n            self.fembeddings=torch.cat((self.fembeddings,f_channel),-1) #(1025,10)\n\n        self.fembeddings=nn.Parameter(self.fembeddings)\n        #self.register_buffer('fembeddings_const', self.fembeddings)\n\n    \n\n    def forward(self, input_tensor):\n\n        batch_size_tensor = input_tensor.shape[0]  # get batch size\n        time_dim = input_tensor.shape[2]  # get time dimension\n\n        fembeddings_2 = torch.broadcast_to(self.fembeddings, [batch_size_tensor, time_dim, self.f_dim, 10])\n        fembeddings_2=fembeddings_2.permute(0,3,1,2)\n    \n        \n        return torch.cat((input_tensor,fembeddings_2),1)  #(batch,12,427,1025)", "\n\nclass Decoder(nn.Module):\n    '''\n    [B, T, F, N] , skip connections => [B, T, F, N]  \n    Decoder side of the U-Net subnetwork.\n    '''\n    def __init__(self, Ns, Ss, unet_args):\n        super(Decoder, self).__init__()\n\n        self.Ns=Ns\n        self.Ss=Ss\n        self.depth=unet_args.depth\n\n        self.dblocks=nn.ModuleList()\n        for i in range(self.depth):\n            self.dblocks.append(D_Block(layer_idx=i,N0=self.Ns[i+1] ,N=self.Ns[i], S=self.Ss[i],num_tfc=unet_args.num_tfc))\n\n    def forward(self,inputs, contracting_layers):\n        x=inputs\n        for i in range(self.depth,0,-1):\n            x=self.dblocks[i-1](x, contracting_layers[i-1])\n        return x ", "\nclass Encoder(nn.Module):\n\n    '''\n    [B, T, F, N] => skip connections , [B, T, F, N_4]  \n    Encoder side of the U-Net subnetwork.\n    '''\n    def __init__(self,N0, Ns, Ss, unet_args):\n        super(Encoder, self).__init__()\n        self.Ns=Ns\n        self.Ss=Ss\n        self.depth=unet_args.depth\n\n        self.contracting_layers = {}\n\n        self.eblocks=nn.ModuleList()\n        for i in range(self.depth):\n            if i==0:\n                Nin=N0\n            else:\n                Nin=self.Ns[i]\n\n            self.eblocks.append(E_Block(layer_idx=i,N0=Nin,N01=self.Ns[i],N=self.Ns[i+1],S=self.Ss[i], num_tfc=unet_args.num_tfc))\n\n        self.i_block=I_Block(self.Ns[self.depth],self.Ns[self.depth],unet_args.num_tfc)\n\n    def forward(self, inputs):\n        x=inputs\n        for i in range(self.depth):\n\n            x, x_contract=self.eblocks[i](x)\n        \n            self.contracting_layers[i] = x_contract #if remove 0, correct this\n\n\n        x=self.i_block(x)\n\n        return x, self.contracting_layers", "\nclass MultiStage_denoise(nn.Module):\n\n    def __init__(self,  unet_args=None):\n        super(MultiStage_denoise, self).__init__()\n        self.depth=unet_args.depth\n        Nin=2\n        if unet_args.use_fencoding:\n            self.freq_encoding=AddFreqEncoding(unet_args.f_dim)\n            Nin=12 #hardcoded\n        self.use_sam=unet_args.use_SAM\n        self.use_fencoding=unet_args.use_fencoding\n        self.num_stages=unet_args.num_stages\n        #Encoder\n        self.Ns= [64,64,64,128,128,256,512] \n        self.Ss= [(2,2),(2,2),(2,2),(2,2),(2,2),(2,2)]\n        \n        #initial feature extractor\n        ksize=(7,7)\n\n        self.conv2d_1 = nn.Sequential(nn.Conv2d(Nin,self.Ns[0],\n                      kernel_size=ksize,\n                      padding='same',\n                      padding_mode='reflect'),\n                      nn.ELU())\n                        \n        self.encoder_s1=Encoder(self.Ns[0],self.Ns, self.Ss, unet_args)\n        self.decoder_s1=Decoder(self.Ns, self.Ss, unet_args)\n\n        self.cropconcat = CropConcatBlock()\n        #self.cropadd = CropAddBlock()\n\n        self.finalblock=FinalBlock(self.Ns[0])\n\n        if self.num_stages>1:\n            self.sam_1=SAM(self.Ns[0])\n\n            #initial feature extractor\n            ksize=(7,7)\n\n            self.conv2d_2 =nn.Sequential(\n                                 nn.Conv2d(Nin,self.Ns[0],\n                                 kernel_size=ksize,\n                                 stride=1, \n                                 padding='same',\n                                 padding_mode='reflect'),\n                                 nn.ELU())\n            \n\n            self.encoder_s2=Encoder(2*self.Ns[0],self.Ns, self.Ss, unet_args)\n            self.decoder_s2=Decoder(self.Ns, self.Ss, unet_args)\n\n    def forward(self, inputs):\n        if self.use_fencoding:\n            x_w_freq=self.freq_encoding(inputs)   #None, None, 1025, 12 \n        else:\n            x_w_freq=inputs\n\n        #intitial feature extractor\n        x=self.conv2d_1(x_w_freq) #None, None, 1025, 32\n\n        x, contracting_layers_s1= self.encoder_s1(x)\n        #decoder\n\n        feats_s1 =self.decoder_s1(x, contracting_layers_s1) #None, None, 1025, 32 features\n\n        if self.num_stages>1:        \n            #SAM module\n            Fout, pred_stage_1=self.sam_1(feats_s1,inputs)\n                \n            #intitial feature extractor\n            x=self.conv2d_2(x_w_freq)\n    \n            if self.use_sam:\n                x = torch.cat((x, Fout), 1)\n            else:\n                x = torch.cat((x,feats_s1), 1)\n\n\n            x, contracting_layers_s2= self.encoder_s2(x)\n\n\n            feats_s2=self.decoder_s2(x, contracting_layers_s2) #None, None, 1025, 32 features\n            \n            #consider implementing a third stage?\n\n            pred_stage_2=self.finalblock(feats_s2) \n            return pred_stage_2, pred_stage_1\n        else:             \n            pred_stage_1=self.finalblock(feats_s1) \n            return pred_stage_1", "            \nclass I_Block(nn.Module):\n    '''\n    [B, T, F, N] => [B, T, F, N] \n    Intermediate block:\n    Basiforwardy, a densenet block with a residual connection\n    '''\n    def __init__(self,N0,N, num_tfc, **kwargs):\n        super(I_Block, self).__init__(**kwargs)\n\n        ksize=(3,3)\n        self.tfc=DenseBlock(num_tfc,N0,N,ksize)\n\n        self.conv2d_res= nn.Conv2d(N0,N,\n                                      kernel_size=(1,1),\n                                      stride=1,\n                                      padding='same',\n                                      padding_mode='reflect')\n\n    def forward(self,inputs):\n        x=self.tfc(inputs)\n\n        inputs_proj=self.conv2d_res(inputs)\n        return torch.add(x,inputs_proj)", "\n\nclass E_Block(nn.Module):\n\n    def __init__(self, layer_idx,N0,N01, N,  S, num_tfc, **kwargs):\n        super(E_Block, self).__init__(**kwargs)\n        self.layer_idx=layer_idx\n        self.N0=N0\n        self.N=N\n        self.S=S\n        self.i_block=I_Block(N0,N01,num_tfc)\n\n        ksize=(S[0]+2,S[1]+2)\n        self.conv2d_2 = nn.Sequential(nn.Conv2d(N01,N,\n                                          kernel_size=(S[0]+2,S[1]+2),\n                                          padding=(2,2),\n                                          stride=S,\n                                          padding_mode='reflect'),\n                                      nn.ELU())\n\n\n    def forward(self, inputs, training=None, **kwargs):\n        x=self.i_block(inputs)\n        \n        x_down = self.conv2d_2(x)\n\n        return x_down, x", "\n\nclass D_Block(nn.Module):\n\n    def __init__(self, layer_idx,N0, N,  S,  num_tfc, **kwargs):\n        super(D_Block, self).__init__(**kwargs)\n        self.layer_idx=layer_idx\n        self.N=N\n        self.S=S\n        ksize=(S[0]+2, S[1]+2)\n        self.tconv_1= nn.Sequential(\n                                nn.ConvTranspose2d(N0,N,\n                                             kernel_size=(S[0]+2, S[1]+2),\n                                             stride=S,\n                                             padding_mode='zeros'),\n                                nn.ELU())\n\n        self.upsampling = nn.Upsample(scale_factor=S, mode=\"nearest\")\n\n        self.projection =nn.Conv2d(N0,N,\n                                      kernel_size=(1,1),\n                                      stride=1,\n                                      padding='same',\n                                      padding_mode='reflect')\n        self.cropadd=CropAddBlock()\n        self.cropconcat=CropConcatBlock()\n\n        self.i_block=I_Block(2*N,N,num_tfc)\n\n    def forward(self, inputs, bridge, **kwargs):\n        x = self.tconv_1(inputs)\n\n        x2= self.upsampling(inputs)\n\n        if x2.shape[-1]!=x.shape[-1]:\n            x2= self.projection(x2)\n\n        x= self.cropadd(x,x2)\n        \n        x=self.cropconcat(x,bridge)\n\n        x=self.i_block(x)\n        return x", "\n\nclass CropAddBlock(nn.Module):\n\n    def forward(self,down_layer, x,  **kwargs):\n        x1_shape = down_layer.shape\n        x2_shape = x.shape\n\n        #print(x1_shape,x2_shape)\n        height_diff = (x1_shape[2] - x2_shape[2]) // 2\n        width_diff = (x1_shape[3] - x2_shape[3]) // 2\n\n\n        down_layer_cropped = down_layer[:,\n                                        :,\n                                        height_diff: (x2_shape[2] + height_diff),\n                                        width_diff: (x2_shape[3] + width_diff)]\n        x = torch.add(down_layer_cropped, x)\n        return x", "\nclass CropConcatBlock(nn.Module):\n\n    def forward(self, down_layer, x, **kwargs):\n        x1_shape = down_layer.shape\n        x2_shape = x.shape\n\n        height_diff = (x1_shape[2] - x2_shape[2]) // 2\n        width_diff = (x1_shape[3] - x2_shape[3]) // 2\n        down_layer_cropped = down_layer[:,\n                                        :,\n                                        height_diff: (x2_shape[2] + height_diff),\n                                        width_diff: (x2_shape[3] + width_diff)]\n        x = torch.cat((down_layer_cropped, x),1)\n        return x", ""]}
{"filename": "diff_params/edm_PD.py", "chunked_list": ["import torch\nimport numpy as np\n\nimport utils.training_utils as utils\n\n\nclass EDM():\n    \"\"\"\n        Definition of most of the diffusion parameterization, following ( Karras et al., \"Elucidating...\", 2022)\n    \"\"\"\n\n    def __init__(self, args):\n        \"\"\"\n        Args:\n            args (dictionary): hydra arguments\n            sigma_data (float): \n        \"\"\"\n        self.args=args\n        self.sigma_min = args.diff_params.sigma_min\n        self.sigma_max =args.diff_params.sigma_max\n        self.P_mean=args.diff_params.P_mean\n        self.P_std=args.diff_params.P_std\n        self.ro=args.diff_params.ro\n        self.ro_train=args.diff_params.ro_train\n        self.sigma_data=args.diff_params.sigma_data #depends on the training data!! precalculated variance of the dataset\n        #parameters stochastic sampling\n        self.Schurn=args.diff_params.Schurn\n        self.Stmin=args.diff_params.Stmin\n        self.Stmax=args.diff_params.Stmax\n        self.Snoise=args.diff_params.Snoise\n\n        #perceptual filter\n        if self.args.diff_params.aweighting.use_aweighting:\n            self.AW=utils.FIRFilter(filter_type=\"aw\", fs=args.exp.sample_rate, ntaps=self.args.diff_params.aweighting.ntaps)\n\n        self.boundaries=self.create_schedule(self.args.diff_params.PD.boundaries.T)\n        #print(self.boundaries)\n        self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n        self.boundaries=self.boundaries.to(self.device)\n        #self.boundaries=torch.flip(self.boundaries,(0,)).to(self.device)\n        #print(\"real T\", len(self.boundaries))\n        #print(self.boundaries)\n       \n\n    def get_gamma(self, t): \n        \"\"\"\n        Get the parameter gamma that defines the stochasticity of the sampler\n        Args\n            t (Tensor): shape: (N_steps, ) Tensor of timesteps, from which we will compute gamma\n        \"\"\"\n        N=t.shape[0]\n        gamma=torch.zeros(t.shape).to(t.device)\n        \n        #If desired, only apply stochasticity between a certain range of noises Stmin is 0 by default and Stmax is a huge number by default. (Unless these parameters are specified, this does nothing)\n        indexes=torch.logical_and(t>self.Stmin , t<self.Stmax)\n         \n        #We use Schurn=5 as the default in our experiments\n        gamma[indexes]=gamma[indexes]+torch.min(torch.Tensor([self.Schurn/N, 2**(1/2) -1]))\n        \n        return gamma\n\n    def create_schedule(self,nb_steps):\n        \"\"\"\n        Define the schedule of timesteps\n        Args:\n           nb_steps (int): Number of discretized steps\n        \"\"\"\n        i=torch.arange(0,nb_steps+1)\n        t=(self.sigma_max**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - self.sigma_max**(1/self.ro)))**self.ro\n        t[-1]=0\n        return t\n\n    def create_schedule_from_initial_t(self,initial_t,nb_steps):\n        \"\"\"\n        Define the schedule of timesteps\n        Args:\n           nb_steps (int): Number of discretized steps\n        \"\"\"\n        i=torch.arange(0,nb_steps+1)\n        t=(initial_t**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - initial_t**(1/self.ro)))**self.ro\n        t[-1]=0\n        return t\n\n\n    def sample_ptrain(self,N):\n        \"\"\"\n        For training, getting t as a normal distribution, folowing Karras et al. \n        I'm not using this\n        Args:\n            N (int): batch size\n        \"\"\"\n        lnsigma=np.random.randn(N)*self.P_std +self.P_mean\n        return np.clip(np.exp(lnsigma),self.sigma_min, self.sigma_max) #not sure if clipping here is necessary, but makes sense to me\n    \n    def sample_ptrain_safe(self,N):\n        \"\"\"\n        For training, getting  t according to the same criteria as sampling\n        Args:\n            N (int): batch size\n        \"\"\"\n        a=torch.rand(N)\n        t=(self.sigma_max**(1/self.ro_train) +a *(self.sigma_min**(1/self.ro_train) - self.sigma_max**(1/self.ro_train)))**self.ro_train\n        return t\n\n    def sample_prior(self,shape,sigma):\n        \"\"\"\n        Just sample some gaussian noise, nothing more\n        Args:\n            shape (tuple): shape of the noise to sample, something like (B,T)\n            sigma (float): noise level of the noise\n        \"\"\"\n        n=torch.randn(shape).to(sigma.device)*sigma\n        return n\n\n    def cskip(self, sigma):\n        \"\"\"\n        Just one of the preconditioning parameters\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \n        \"\"\"\n        return self.sigma_data**2 *(sigma**2+self.sigma_data**2)**-1\n\n    def cout(self,sigma ):\n        \"\"\"\n        Just one of the preconditioning parameters\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        return sigma*self.sigma_data* (self.sigma_data**2+sigma**2)**(-0.5)\n\n    def cin(self, sigma):\n        \"\"\"\n        Just one of the preconditioning parameters\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        return (self.sigma_data**2+sigma**2)**(-0.5)\n\n    def cnoise(self,sigma ):\n        \"\"\"\n        preconditioning of the noise embedding\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        return (1/4)*torch.log(sigma)\n\n    def lambda_w(self,sigma):\n        return (sigma*self.sigma_data)**(-2) * (self.sigma_data**2+sigma**2)\n        \n    def denoiser(self, xn , net, sigma):\n        \"\"\"\n        This method does the whole denoising step, which implies applying the model and the preconditioning\n        Args:\n            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n            model (nn.Module): Model of the denoiser\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        if len(sigma.shape)==1:\n            sigma=sigma.unsqueeze(-1)\n        cskip=self.cskip(sigma)\n        cout=self.cout(sigma)\n        cin=self.cin(sigma)\n        cnoise=self.cnoise(sigma)\n\n        return cskip * xn +cout*net(cin*xn, cnoise)  #this will crash because of broadcasting problems, debug later!\n\n    def prepare_train_preconditioning(self, x, sigma):\n        #weight=self.lambda_w(sigma)\n        #Is calling the denoiser here a good idea? Maybe it would be better to apply directly the preconditioning as in the paper, even though Karras et al seem to do it this way in their code\n        #print(x.shape)\n        noise=self.sample_prior(x.shape,sigma)\n\n        cskip=self.cskip(sigma)\n        cout=self.cout(sigma)\n        cin=self.cin(sigma)\n        cnoise=self.cnoise(sigma)\n\n        target=(1/cout)*(x-cskip*(x+noise))\n\n        return cin*(x+noise), target, cnoise\n\n\n    def loss_fn(self, net, x):\n        \"\"\"\n        Loss function, which is the mean squared error between the denoised latent and the clean latent\n        Args:\n            net (nn.Module): Model of the denoiser\n            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        sigma=self.sample_ptrain_safe(x.shape[0]).unsqueeze(-1).to(x.device)\n\n\n        input, target, cnoise= self.prepare_train_preconditioning(x, sigma)\n        #print(\"inputs to net\", input.shape, cnoise.shape)\n        estimate=net(input,cnoise)\n        \n        error=(estimate-target)\n\n        try:\n            #this will only happen if the model is cqt-based, if it crashes it is normal\n            if self.args.net.use_cqt_DC_correction:\n                error=net.CQTransform.apply_hpf_DC(error) #apply the DC correction to the error as we dont want to propagate the DC component of the error as the network is discarding it. It also applies for the nyquit frequency, but this is less critical.\n        except:\n            pass \n\n        #APPLY A-WEIGHTING\n        if self.args.diff_params.aweighting.use_aweighting:\n            error=self.AW(error)\n\n        #here we have the chance to apply further emphasis to the error, as some kind of perceptual frequency weighting could be\n        return error**2, sigma\n\n    def ode_update(self, x, sigma_1, sigma_0, net_teacher):\n        x_0_hat=self.denoiser(x, net_teacher, sigma_0)\n        score=(x_0_hat-x)/sigma_0**2\n        return x-(sigma_1-sigma_0)*sigma_0*score\n\n    def loss_fn_PD(self, net, net_teacher, x, stage):\n        \"\"\"\n        Loss function, which is the mean squared error between the denoised latent and the clean latent\n        Args:\n            net (nn.Module): Model of the denoiser\n            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n\n        #sigma=self.sample_ptrain_safe(x.shape[0]).unsqueeze(-1).to(x.device)\n\n        #sample sigma using self.boundaries\n        if stage==0:\n            schedule=self.boundaries\n        else:\n            #if stage ==1 we are in the second stage, so we downsample the schedule by a factor of 2\n            schedule=self.boundaries[::(2**(stage))]\n\n        schedule=torch.flip(schedule,(0,))\n        print(schedule, \"stage\", stage, len(schedule), len(self.boundaries))\n\n        #print(\"stage\",stage,\"train schedule\", schedule, \"original boundaries\", self.boundaries)\n        if len(schedule)>3:\n            j = torch.randint(1, len(schedule)//2, (x.shape[0], 1), device=x.device)\n            i=j*2+1\n        else:\n            i=2\n\n        sigma_0=schedule[i]\n        sigma_1=schedule[i-1]\n        sigma_2=schedule[i-2]\n        #print(j,i, sigma_0, sigma_1, sigma_2)\n\n        #input_0, _ , cnoise_0= self.prepare_train_preconditioning(x, sigma_0)\n\n        noise=self.sample_prior(x.shape,sigma_0)\n        cskip_0=self.cskip(sigma_0)\n        cout_0=self.cout(sigma_0)\n        cin_0=self.cin(sigma_0)\n        #cin_1=self.cin(sigma_1)\n        cnoise_0=self.cnoise(sigma_0)\n\n        #input_1, _ , cnoise_1= self.prepare_train_preconditioning(x, sigma_1)\n\n\n        #2 ode steps with the teacher network\n        zn=x+noise\n        with torch.no_grad():\n            z_teacher=self.ode_update(zn, sigma_1,sigma_0, net_teacher)\n            z_teacher=self.ode_update(z_teacher, sigma_2, sigma_1, net_teacher)\n\n            #x_0_student=self.denoiser(cin_0*xn, net,sigma_0)\n            x_0_student=(z_teacher-(sigma_2/sigma_0)*zn)/(1-(sigma_2/sigma_0))\n        \n            target=(1/cout_0)*(x_0_student-cskip_0*zn)\n\n        estimate=net(cin_0*zn, cnoise_0)\n\n        error=(estimate-target.detach())\n\n        \n        try:\n            #this will only happen if the model is cqt-based, if it crashes it is normal\n            if self.args.net.use_cqt_DC_correction:\n                error=net.CQTransform.apply_hpf_DC(error) #apply the DC correction to the error as we dont want to propagate the DC component of the error as the network is discarding it. It also applies for the nyquit frequency, but this is less critical.\n        except:\n            pass \n\n        #APPLY A-WEIGHTING\n        if self.args.diff_params.aweighting.use_aweighting:\n            error=self.AW(error)\n\n        #here we have the chance to apply further emphasis to the error, as some kind of perceptual frequency weighting could be\n        return error**2, sigma_0\n\n    def PD_sample(self, N, L, net, device, stage):\n        \"\"\"\n           N: batch size\n           L: length\n           device\n        \"\"\"\n        #if stage ==1 we are in the second stage, so we downsample the schedule by a factor of 2\n        schedule=self.boundaries[::(2**(stage+1))]\n\n        schedule=torch.flip(schedule,(0,))\n        print(\"test schedule\", schedule)\n\n        z=torch.randn((N,L), device=device)*schedule[-1]\n        for i in range(len(schedule)-1):\n            sigma_0=schedule[-i-1]\n            sigma_1=schedule[-i-2]\n            print(i, sigma_0, sigma_1)\n            z=self.ode_update(z, sigma_1,sigma_0, net)\n        #x0=self.denoiser(z, net, sigma_1)\n        return z", "\n"]}
{"filename": "diff_params/edm.py", "chunked_list": ["import torch\nimport numpy as np\n\nimport utils.training_utils as utils\n\n\nclass EDM():\n    \"\"\"\n        Definition of most of the diffusion parameterization, following ( Karras et al., \"Elucidating...\", 2022)\n    \"\"\"\n\n    def __init__(self, args):\n        \"\"\"\n        Args:\n            args (dictionary): hydra arguments\n            sigma_data (float): \n        \"\"\"\n        self.args=args\n        self.sigma_min = args.diff_params.sigma_min\n        self.sigma_max =args.diff_params.sigma_max\n        self.P_mean=args.diff_params.P_mean\n        self.P_std=args.diff_params.P_std\n        self.ro=args.diff_params.ro\n        self.ro_train=args.diff_params.ro_train\n        self.sigma_data=args.diff_params.sigma_data #depends on the training data!! precalculated variance of the dataset\n        #parameters stochastic sampling\n        self.Schurn=args.diff_params.Schurn\n        self.Stmin=args.diff_params.Stmin\n        self.Stmax=args.diff_params.Stmax\n        self.Snoise=args.diff_params.Snoise\n\n        #perceptual filter\n        if self.args.diff_params.aweighting.use_aweighting:\n            self.AW=utils.FIRFilter(filter_type=\"aw\", fs=args.exp.sample_rate, ntaps=self.args.diff_params.aweighting.ntaps)\n\n       \n\n    def get_gamma(self, t): \n        \"\"\"\n        Get the parameter gamma that defines the stochasticity of the sampler\n        Args\n            t (Tensor): shape: (N_steps, ) Tensor of timesteps, from which we will compute gamma\n        \"\"\"\n        N=t.shape[0]\n        gamma=torch.zeros(t.shape).to(t.device)\n        \n        #If desired, only apply stochasticity between a certain range of noises Stmin is 0 by default and Stmax is a huge number by default. (Unless these parameters are specified, this does nothing)\n        indexes=torch.logical_and(t>self.Stmin , t<self.Stmax)\n         \n        #We use Schurn=5 as the default in our experiments\n        gamma[indexes]=gamma[indexes]+torch.min(torch.Tensor([self.Schurn/N, 2**(1/2) -1]))\n        \n        return gamma\n\n    def create_schedule(self,nb_steps):\n        \"\"\"\n        Define the schedule of timesteps\n        Args:\n           nb_steps (int): Number of discretized steps\n        \"\"\"\n        i=torch.arange(0,nb_steps+1)\n        t=(self.sigma_max**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - self.sigma_max**(1/self.ro)))**self.ro\n        t[-1]=0\n        return t\n\n    def create_schedule_from_initial_t(self,initial_t,nb_steps):\n        \"\"\"\n        Define the schedule of timesteps\n        Args:\n           nb_steps (int): Number of discretized steps\n        \"\"\"\n        i=torch.arange(0,nb_steps+1)\n        t=(initial_t**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - initial_t**(1/self.ro)))**self.ro\n        t[-1]=0\n        return t\n\n\n    def sample_ptrain(self,N):\n        \"\"\"\n        For training, getting t as a normal distribution, folowing Karras et al. \n        I'm not using this\n        Args:\n            N (int): batch size\n        \"\"\"\n        lnsigma=np.random.randn(N)*self.P_std +self.P_mean\n        return np.clip(np.exp(lnsigma),self.sigma_min, self.sigma_max) #not sure if clipping here is necessary, but makes sense to me\n    \n    def sample_ptrain_safe(self,N):\n        \"\"\"\n        For training, getting  t according to the same criteria as sampling\n        Args:\n            N (int): batch size\n        \"\"\"\n        a=torch.rand(N)\n        t=(self.sigma_max**(1/self.ro_train) +a *(self.sigma_min**(1/self.ro_train) - self.sigma_max**(1/self.ro_train)))**self.ro_train\n        return t\n\n    def sample_prior(self,shape,sigma):\n        \"\"\"\n        Just sample some gaussian noise, nothing more\n        Args:\n            shape (tuple): shape of the noise to sample, something like (B,T)\n            sigma (float): noise level of the noise\n        \"\"\"\n        n=torch.randn(shape).to(sigma.device)*sigma\n        return n\n\n    def cskip(self, sigma):\n        \"\"\"\n        Just one of the preconditioning parameters\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \n        \"\"\"\n        return self.sigma_data**2 *(sigma**2+self.sigma_data**2)**-1\n\n    def cout(self,sigma ):\n        \"\"\"\n        Just one of the preconditioning parameters\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        return sigma*self.sigma_data* (self.sigma_data**2+sigma**2)**(-0.5)\n\n    def cin(self, sigma):\n        \"\"\"\n        Just one of the preconditioning parameters\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        return (self.sigma_data**2+sigma**2)**(-0.5)\n\n    def cnoise(self,sigma ):\n        \"\"\"\n        preconditioning of the noise embedding\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        return (1/4)*torch.log(sigma)\n\n    def lambda_w(self,sigma):\n        return (sigma*self.sigma_data)**(-2) * (self.sigma_data**2+sigma**2)\n        \n    def denoiser(self, xn , net, sigma):\n        \"\"\"\n        This method does the whole denoising step, which implies applying the model and the preconditioning\n        Args:\n            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n            model (nn.Module): Model of the denoiser\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        if len(sigma.shape)==1:\n            sigma=sigma.unsqueeze(-1)\n        cskip=self.cskip(sigma)\n        cout=self.cout(sigma)\n        cin=self.cin(sigma)\n        cnoise=self.cnoise(sigma)\n\n        return cskip * xn +cout*net(cin*xn, cnoise)  #this will crash because of broadcasting problems, debug later!\n\n    def prepare_train_preconditioning(self, x, sigma):\n        #weight=self.lambda_w(sigma)\n        #Is calling the denoiser here a good idea? Maybe it would be better to apply directly the preconditioning as in the paper, even though Karras et al seem to do it this way in their code\n        print(x.shape)\n        noise=self.sample_prior(x.shape,sigma)\n\n        cskip=self.cskip(sigma)\n        cout=self.cout(sigma)\n        cin=self.cin(sigma)\n        cnoise=self.cnoise(sigma)\n\n        target=(1/cout)*(x-cskip*(x+noise))\n\n        return cin*(x+noise), target, cnoise\n\n\n    def loss_fn(self, net, x):\n        \"\"\"\n        Loss function, which is the mean squared error between the denoised latent and the clean latent\n        Args:\n            net (nn.Module): Model of the denoiser\n            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        sigma=self.sample_ptrain_safe(x.shape[0]).unsqueeze(-1).to(x.device)\n\n\n        input, target, cnoise= self.prepare_train_preconditioning(x, sigma)\n        print(\"inputs to net\", input.shape, cnoise.shape)\n        estimate=net(input,cnoise)\n        \n        error=(estimate-target)\n\n        try:\n            #this will only happen if the model is cqt-based, if it crashes it is normal\n            if self.args.net.use_cqt_DC_correction:\n                error=net.CQTransform.apply_hpf_DC(error) #apply the DC correction to the error as we dont want to propagate the DC component of the error as the network is discarding it. It also applies for the nyquit frequency, but this is less critical.\n        except:\n            pass \n\n        #APPLY A-WEIGHTING\n        if self.args.diff_params.aweighting.use_aweighting:\n            error=self.AW(error)\n\n        #here we have the chance to apply further emphasis to the error, as some kind of perceptual frequency weighting could be\n        return error**2, sigma", "\n"]}
{"filename": "diff_params/edm_eps.py", "chunked_list": ["import torch\nimport torch.nn.functional as F\n\nimport numpy as np\nfrom tqdm import tqdm\n\nimport utils.training_utils as utils\n\n\nclass EDM():\n    \"\"\"\n        Definition of most of the diffusion parameterization, following ( Karras et al., \"Elucidating...\", 2022)\n    \"\"\"\n\n    def __init__(self, args):\n        \"\"\"\n        Args:\n            args (dictionary): hydra arguments\n            sigma_data (float): \n        \"\"\"\n        self.args=args\n\n        #parameters from: https://github.com/yoyololicon/diffwave-sr/blob/main/ckpt/vctk_48k_udm/.hydra/config.yaml\n\n        self.T=self.args.diff_params.T\n\n        self.gamma1=torch.Tensor([self.args.diff_params.scheduler.gamma1])\n        self.gamma0=torch.Tensor([self.args.diff_params.scheduler.gamma0])\n        print(\"gamam0, 1\",self.gamma0, self.gamma1)\n       \n        t = torch.linspace(0, 1, self.T + 1)\n        self.gamma, t=self.LogSNRLinearScheduler(self.gamma1, self.gamma0, t)\n\n\n        self.sigma_min = args.diff_params.sigma_min\n        self.sigma_max =args.diff_params.sigma_max\n        self.P_mean=args.diff_params.P_mean\n        self.P_std=args.diff_params.P_std\n        self.ro=args.diff_params.ro\n        self.ro_train=args.diff_params.ro_train\n        self.sigma_data=args.diff_params.sigma_data #depends on the training data!! precalculated variance of the dataset\n        #parameters stochastic sampling\n        self.Schurn=args.diff_params.Schurn\n        self.Stmin=args.diff_params.Stmin\n        self.Stmax=args.diff_params.Stmax\n        self.Snoise=args.diff_params.Snoise\n\n        #perceptual filter\n        if self.args.diff_params.aweighting.use_aweighting:\n            self.AW=utils.FIRFilter(filter_type=\"aw\", fs=args.exp.sample_rate, ntaps=self.args.diff_params.aweighting.ntaps)\n\n    def LogSNRLinearScheduler(self, gamma1, gamma0, t):\n\n        t = t.clamp(0, 1)\n        gamma = gamma0 * (1 - t) + gamma1 * t\n\n        return gamma, t\n    def gamma_to_t(self, gamma):\n        \"\"\"\n        Convert the parameter gamma to the parameter t\n        Args:\n            gamma (Tensor): shape: (N_steps, ) Tensor of gamma values\n        \"\"\"\n        return (gamma - self.gamma0) / (self.gamma1 - self.gamma0)\n\n    def t_to_gamma(self, t):\n        \"\"\"\n        Convert the parameter t to the parameter gamma\"\"\"\n        return self.gamma0 + t * (self.gamma1 - self.gamma0)\n\n    def gamma_2_as(self, gamma):\n        \"\"\"\n        Convert the parameter gamma to the parameter alpha and s\n        Args:\n            gamma (Tensor): shape: (N_steps, ) Tensor of gamma values\n        \"\"\"\n        var = gamma.sigmoid()\n        return (1 - var).sqrt(), var.sqrt()\n    \n    def t_2_as(self, t):\n        \"\"\"\n        Convert the parameter t to the parameter alpha and s\n        Args:\n            t (Tensor): shape: (N_steps, ) Tensor of t values\n        \"\"\"\n        gamma=self.t_to_gamma(t)\n        return self.gamma_2_as(gamma)\n\n\n\n    def gamma_to_sigma(self, gamma):\n        \"\"\"\n        Convert the parameter gamma to the parameter sigma\n        Args:\n            gamma (Tensor): shape: (N_steps, ) Tensor of gamma values\n        \"\"\"\n        return torch.sqrt(1/torch.exp(-gamma))\n\n    def sigma_to_gamma(self, sigma):\n        \"\"\"\n        Convert the parameter sigma to the parameter gamma\n        Args:\n            sigma (Tensor): shape: (N_steps, ) Tensor of sigma values\n        \"\"\"\n        return torch.log(sigma**2)\n    \n    def sigma_to_t(self, sigma):\n        gamma=self.sigma_to_gamma(sigma)\n        print(\"gamma\",gamma)\n        return self.gamma_to_t(gamma)\n\n    def gamma2logas(self,g):\n        log_var = -F.softplus(-g)\n        return 0.5 * (-g + log_var), log_var\n       \n    def reverse_process_ddim(self,z_1,  model):\n\n        tt = torch.linspace(0, 1, self.T + 1)\n        gamma, steps =self.LogSNRLinearScheduler(self.gamma1, self.gamma0, tt)\n        #print(gamma, steps)\n        Pm1 = -torch.expm1((gamma[1:] - gamma[:-1]) * 0.5)\n        log_alpha, log_var = self.gamma2logas(gamma)\n        #print(\"log_alpha\", log_alpha, \"log_var\", log_var)\n        alpha_st = torch.exp(log_alpha[:-1] - log_alpha[1:])\n        #print(\"alpha_st\", alpha_st)\n        std = log_var.mul(0.5).exp()\n\n        T = gamma.numel() - 1\n        z_t = z_1\n        for t in tqdm(range(T, 0, -1)):\n            #print(\"z_t std\", z_t.std(-1))\n            s = t - 1\n            #print(\"steps\",steps[t:t+1],\"gamma\", gamma[t], \"alpha_st\", alpha_st[s], \"std\",std[s], \"pm1\", Pm1[s])\n            #print(z_t.shape, steps[t:t+1].shape)\n            noise_hat = model(z_t, steps[t:t+1])\n            noise_hat = noise_hat.float()\n            z_t.mul_(alpha_st[s]).add_(std[s] * Pm1[s] * noise_hat)\n\n        return z_t\n\n\n    def get_gamma(self, t): \n        \"\"\"\n        Get the parameter gamma that defines the stochasticity of the sampler, it is not the same as the parameter gamma in the scheduler\n        Args\n            t (Tensor): shape: (N_steps, ) Tensor of timesteps, from which we will compute gamma\n        \"\"\"\n        N=t.shape[0]\n        gamma=torch.zeros(t.shape).to(t.device)\n        \n        #If desired, only apply stochasticity between a certain range of noises Stmin is 0 by default and Stmax is a huge number by default. (Unless these parameters are specified, this does nothing)\n        indexes=torch.logical_and(t>self.Stmin , t<self.Stmax)\n         \n        #We use Schurn=5 as the default in our experiments\n        gamma[indexes]=gamma[indexes]+torch.min(torch.Tensor([self.Schurn/N, 2**(1/2) -1]))\n        \n        return gamma\n\n    def create_schedule(self,nb_steps):\n        \"\"\"\n        Define the schedule of timesteps\n        Args:\n           nb_steps (int): Number of discretized steps\n        \"\"\"\n        i=torch.arange(0,nb_steps+1)\n        t=(self.sigma_max**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - self.sigma_max**(1/self.ro)))**self.ro\n        t[-1]=0\n        return t\n\n    def create_schedule_from_initial_t(self,initial_t,nb_steps):\n        \"\"\"\n        Define the schedule of timesteps\n        Args:\n           nb_steps (int): Number of discretized steps\n        \"\"\"\n        i=torch.arange(0,nb_steps+1)\n        t=(initial_t**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - initial_t**(1/self.ro)))**self.ro\n        t[-1]=0\n        return t\n\n\n    def sample_ptrain(self,N):\n        \"\"\"\n        For training, getting t as a normal distribution, folowing Karras et al. \n        I'm not using this\n        Args:\n            N (int): batch size\n        \"\"\"\n        lnsigma=np.random.randn(N)*self.P_std +self.P_mean\n        return np.clip(np.exp(lnsigma),self.sigma_min, self.sigma_max) #not sure if clipping here is necessary, but makes sense to me\n    \n    def sample_ptrain_safe(self,N):\n        \"\"\"\n        For training, getting  t according to the same criteria as sampling\n        Args:\n            N (int): batch size\n        \"\"\"\n        a=torch.rand(N)\n        t=(self.sigma_max**(1/self.ro_train) +a *(self.sigma_min**(1/self.ro_train) - self.sigma_max**(1/self.ro_train)))**self.ro_train\n        return t\n\n    def sample_prior(self,shape,sigma):\n        \"\"\"\n        Just sample some gaussian noise, nothing more\n        Args:\n            shape (tuple): shape of the noise to sample, something like (B,T)\n            sigma (float): noise level of the noise\n        \"\"\"\n        n=torch.randn(shape).to(sigma.device)*sigma\n        return n\n\n    def cskip(self, sigma):\n        \"\"\"\n        Just one of the preconditioning parameters\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \n        \"\"\"\n        return self.sigma_data**2 *(sigma**2+self.sigma_data**2)**-1\n\n    def cout(self,sigma ):\n        \"\"\"\n        Just one of the preconditioning parameters\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        return sigma*self.sigma_data* (self.sigma_data**2+sigma**2)**(-0.5)\n\n    def cin(self, sigma):\n        \"\"\"\n        Just one of the preconditioning parameters\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        return (self.sigma_data**2+sigma**2)**(-0.5)\n\n    def cnoise(self,sigma ):\n        \"\"\"\n        preconditioning of the noise embedding\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        return (1/4)*torch.log(sigma)\n\n    def lambda_w(self,sigma):\n        return (sigma*self.sigma_data)**(-2) * (self.sigma_data**2+sigma**2)\n        \n    def denoiser(self, xn , net, sigma):\n        \"\"\"\n        This method does the whole denoising step, which implies applying the model and the preconditioning\n        Args:\n            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n            model (nn.Module): Model of the denoiser\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        if len(sigma.shape)==1:\n            sigma=sigma.unsqueeze(-1)\n        #cskip=self.cskip(sigma)\n        #cout=self.cout(sigma)\n        #cin=self.cin(sigma)\n        #cnoise=self.cnoise(sigma)\n        self.gamma0=self.gamma0.to(sigma.device)\n        self.gamma1=self.gamma1.to(sigma.device)\n\n\n        #print(sigma.device)\n        #t=self.sigma_to_t(sigma)\n\n        gamma=self.sigma_to_gamma(sigma)\n        #print(\"gamma\", gamma)\n        t=self.gamma_to_t(gamma)\n        a, s=self.gamma_2_as(gamma)\n        #print(a.shape, s.shape, sigma.shape, t.shape)\n        #print(\"sigma\", sigma, \"t\", t)\n        #print(\"a\", a,\"s\", s)\n\n        z_t=a*xn #this is equivalent to cin\n        #print(\"z_t std\",z_t.std(-1))\n\n        t=t.expand(z_t.shape[0],1).squeeze(-1) \n        #print(\"before net\", z_t.shape, t.shape)\n        eps_hat=net(z_t, t)\n        eps_hat=eps_hat\n\n        x0_hat=(-s*eps_hat+z_t)/a #equvalent to cout and cskip\n\n        return x0_hat#this will crash because of broadcasting problems, debug later!\n\n    def prepare_train_preconditioning(self, x, sigma):\n        #weight=self.lambda_w(sigma)\n        #Is calling the denoiser here a good idea? Maybe it would be better to apply directly the preconditioning as in the paper, even though Karras et al seem to do it this way in their code\n        print(x.shape)\n        noise=self.sample_prior(x.shape,sigma)\n\n        cskip=self.cskip(sigma)\n        cout=self.cout(sigma)\n        cin=self.cin(sigma)\n        cnoise=self.cnoise(sigma)\n\n        target=(1/cout)*(x-cskip*(x+noise))\n\n        return cin*(x+noise), target, cnoise\n\n\n    def loss_fn(self, net, x):\n        \"\"\"\n        Loss function, which is the mean squared error between the denoised latent and the clean latent\n        Args:\n            net (nn.Module): Model of the denoiser\n            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        sigma=self.sample_ptrain_safe(x.shape[0]).unsqueeze(-1).to(x.device)\n\n        input, target, cnoise= self.prepare_train_preconditioning(x, sigma)\n        estimate=net(input,cnoise)\n        \n        error=(estimate-target)\n\n        try:\n            #this will only happen if the model is cqt-based, if it crashes it is normal\n            if self.args.net.use_cqt_DC_correction:\n                error=net.CQTransform.apply_hpf_DC(error) #apply the DC correction to the error as we dont want to propagate the DC component of the error as the network is discarding it. It also applies for the nyquit frequency, but this is less critical.\n        except:\n            pass \n\n        #APPLY A-WEIGHTING\n        if self.args.diff_params.aweighting.use_aweighting:\n            error=self.AW(error)\n\n        #here we have the chance to apply further emphasis to the error, as some kind of perceptual frequency weighting could be\n        return error**2, sigma", "\nclass EDM():\n    \"\"\"\n        Definition of most of the diffusion parameterization, following ( Karras et al., \"Elucidating...\", 2022)\n    \"\"\"\n\n    def __init__(self, args):\n        \"\"\"\n        Args:\n            args (dictionary): hydra arguments\n            sigma_data (float): \n        \"\"\"\n        self.args=args\n\n        #parameters from: https://github.com/yoyololicon/diffwave-sr/blob/main/ckpt/vctk_48k_udm/.hydra/config.yaml\n\n        self.T=self.args.diff_params.T\n\n        self.gamma1=torch.Tensor([self.args.diff_params.scheduler.gamma1])\n        self.gamma0=torch.Tensor([self.args.diff_params.scheduler.gamma0])\n        print(\"gamam0, 1\",self.gamma0, self.gamma1)\n       \n        t = torch.linspace(0, 1, self.T + 1)\n        self.gamma, t=self.LogSNRLinearScheduler(self.gamma1, self.gamma0, t)\n\n\n        self.sigma_min = args.diff_params.sigma_min\n        self.sigma_max =args.diff_params.sigma_max\n        self.P_mean=args.diff_params.P_mean\n        self.P_std=args.diff_params.P_std\n        self.ro=args.diff_params.ro\n        self.ro_train=args.diff_params.ro_train\n        self.sigma_data=args.diff_params.sigma_data #depends on the training data!! precalculated variance of the dataset\n        #parameters stochastic sampling\n        self.Schurn=args.diff_params.Schurn\n        self.Stmin=args.diff_params.Stmin\n        self.Stmax=args.diff_params.Stmax\n        self.Snoise=args.diff_params.Snoise\n\n        #perceptual filter\n        if self.args.diff_params.aweighting.use_aweighting:\n            self.AW=utils.FIRFilter(filter_type=\"aw\", fs=args.exp.sample_rate, ntaps=self.args.diff_params.aweighting.ntaps)\n\n    def LogSNRLinearScheduler(self, gamma1, gamma0, t):\n\n        t = t.clamp(0, 1)\n        gamma = gamma0 * (1 - t) + gamma1 * t\n\n        return gamma, t\n    def gamma_to_t(self, gamma):\n        \"\"\"\n        Convert the parameter gamma to the parameter t\n        Args:\n            gamma (Tensor): shape: (N_steps, ) Tensor of gamma values\n        \"\"\"\n        return (gamma - self.gamma0) / (self.gamma1 - self.gamma0)\n\n    def t_to_gamma(self, t):\n        \"\"\"\n        Convert the parameter t to the parameter gamma\"\"\"\n        return self.gamma0 + t * (self.gamma1 - self.gamma0)\n\n    def gamma_2_as(self, gamma):\n        \"\"\"\n        Convert the parameter gamma to the parameter alpha and s\n        Args:\n            gamma (Tensor): shape: (N_steps, ) Tensor of gamma values\n        \"\"\"\n        var = gamma.sigmoid()\n        return (1 - var).sqrt(), var.sqrt()\n    \n    def t_2_as(self, t):\n        \"\"\"\n        Convert the parameter t to the parameter alpha and s\n        Args:\n            t (Tensor): shape: (N_steps, ) Tensor of t values\n        \"\"\"\n        gamma=self.t_to_gamma(t)\n        return self.gamma_2_as(gamma)\n\n\n\n    def gamma_to_sigma(self, gamma):\n        \"\"\"\n        Convert the parameter gamma to the parameter sigma\n        Args:\n            gamma (Tensor): shape: (N_steps, ) Tensor of gamma values\n        \"\"\"\n        return torch.sqrt(1/torch.exp(-gamma))\n\n    def sigma_to_gamma(self, sigma):\n        \"\"\"\n        Convert the parameter sigma to the parameter gamma\n        Args:\n            sigma (Tensor): shape: (N_steps, ) Tensor of sigma values\n        \"\"\"\n        return torch.log(sigma**2)\n    \n    def sigma_to_t(self, sigma):\n        gamma=self.sigma_to_gamma(sigma)\n        print(\"gamma\",gamma)\n        return self.gamma_to_t(gamma)\n\n    def gamma2logas(self,g):\n        log_var = -F.softplus(-g)\n        return 0.5 * (-g + log_var), log_var\n       \n    def reverse_process_ddim(self,z_1,  model):\n\n        tt = torch.linspace(0, 1, self.T + 1)\n        gamma, steps =self.LogSNRLinearScheduler(self.gamma1, self.gamma0, tt)\n        #print(gamma, steps)\n        Pm1 = -torch.expm1((gamma[1:] - gamma[:-1]) * 0.5)\n        log_alpha, log_var = self.gamma2logas(gamma)\n        #print(\"log_alpha\", log_alpha, \"log_var\", log_var)\n        alpha_st = torch.exp(log_alpha[:-1] - log_alpha[1:])\n        #print(\"alpha_st\", alpha_st)\n        std = log_var.mul(0.5).exp()\n\n        T = gamma.numel() - 1\n        z_t = z_1\n        for t in tqdm(range(T, 0, -1)):\n            #print(\"z_t std\", z_t.std(-1))\n            s = t - 1\n            #print(\"steps\",steps[t:t+1],\"gamma\", gamma[t], \"alpha_st\", alpha_st[s], \"std\",std[s], \"pm1\", Pm1[s])\n            #print(z_t.shape, steps[t:t+1].shape)\n            noise_hat = model(z_t, steps[t:t+1])\n            noise_hat = noise_hat.float()\n            z_t.mul_(alpha_st[s]).add_(std[s] * Pm1[s] * noise_hat)\n\n        return z_t\n\n\n    def get_gamma(self, t): \n        \"\"\"\n        Get the parameter gamma that defines the stochasticity of the sampler, it is not the same as the parameter gamma in the scheduler\n        Args\n            t (Tensor): shape: (N_steps, ) Tensor of timesteps, from which we will compute gamma\n        \"\"\"\n        N=t.shape[0]\n        gamma=torch.zeros(t.shape).to(t.device)\n        \n        #If desired, only apply stochasticity between a certain range of noises Stmin is 0 by default and Stmax is a huge number by default. (Unless these parameters are specified, this does nothing)\n        indexes=torch.logical_and(t>self.Stmin , t<self.Stmax)\n         \n        #We use Schurn=5 as the default in our experiments\n        gamma[indexes]=gamma[indexes]+torch.min(torch.Tensor([self.Schurn/N, 2**(1/2) -1]))\n        \n        return gamma\n\n    def create_schedule(self,nb_steps):\n        \"\"\"\n        Define the schedule of timesteps\n        Args:\n           nb_steps (int): Number of discretized steps\n        \"\"\"\n        i=torch.arange(0,nb_steps+1)\n        t=(self.sigma_max**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - self.sigma_max**(1/self.ro)))**self.ro\n        t[-1]=0\n        return t\n\n    def create_schedule_from_initial_t(self,initial_t,nb_steps):\n        \"\"\"\n        Define the schedule of timesteps\n        Args:\n           nb_steps (int): Number of discretized steps\n        \"\"\"\n        i=torch.arange(0,nb_steps+1)\n        t=(initial_t**(1/self.ro) +i/(nb_steps-1) *(self.sigma_min**(1/self.ro) - initial_t**(1/self.ro)))**self.ro\n        t[-1]=0\n        return t\n\n\n    def sample_ptrain(self,N):\n        \"\"\"\n        For training, getting t as a normal distribution, folowing Karras et al. \n        I'm not using this\n        Args:\n            N (int): batch size\n        \"\"\"\n        lnsigma=np.random.randn(N)*self.P_std +self.P_mean\n        return np.clip(np.exp(lnsigma),self.sigma_min, self.sigma_max) #not sure if clipping here is necessary, but makes sense to me\n    \n    def sample_ptrain_safe(self,N):\n        \"\"\"\n        For training, getting  t according to the same criteria as sampling\n        Args:\n            N (int): batch size\n        \"\"\"\n        a=torch.rand(N)\n        t=(self.sigma_max**(1/self.ro_train) +a *(self.sigma_min**(1/self.ro_train) - self.sigma_max**(1/self.ro_train)))**self.ro_train\n        return t\n\n    def sample_prior(self,shape,sigma):\n        \"\"\"\n        Just sample some gaussian noise, nothing more\n        Args:\n            shape (tuple): shape of the noise to sample, something like (B,T)\n            sigma (float): noise level of the noise\n        \"\"\"\n        n=torch.randn(shape).to(sigma.device)*sigma\n        return n\n\n    def cskip(self, sigma):\n        \"\"\"\n        Just one of the preconditioning parameters\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \n        \"\"\"\n        return self.sigma_data**2 *(sigma**2+self.sigma_data**2)**-1\n\n    def cout(self,sigma ):\n        \"\"\"\n        Just one of the preconditioning parameters\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        return sigma*self.sigma_data* (self.sigma_data**2+sigma**2)**(-0.5)\n\n    def cin(self, sigma):\n        \"\"\"\n        Just one of the preconditioning parameters\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        return (self.sigma_data**2+sigma**2)**(-0.5)\n\n    def cnoise(self,sigma ):\n        \"\"\"\n        preconditioning of the noise embedding\n        Args:\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        return (1/4)*torch.log(sigma)\n\n    def lambda_w(self,sigma):\n        return (sigma*self.sigma_data)**(-2) * (self.sigma_data**2+sigma**2)\n        \n    def denoiser(self, xn , net, sigma):\n        \"\"\"\n        This method does the whole denoising step, which implies applying the model and the preconditioning\n        Args:\n            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n            model (nn.Module): Model of the denoiser\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        if len(sigma.shape)==1:\n            sigma=sigma.unsqueeze(-1)\n        #cskip=self.cskip(sigma)\n        #cout=self.cout(sigma)\n        #cin=self.cin(sigma)\n        #cnoise=self.cnoise(sigma)\n        self.gamma0=self.gamma0.to(sigma.device)\n        self.gamma1=self.gamma1.to(sigma.device)\n\n\n        #print(sigma.device)\n        #t=self.sigma_to_t(sigma)\n\n        gamma=self.sigma_to_gamma(sigma)\n        #print(\"gamma\", gamma)\n        t=self.gamma_to_t(gamma)\n        a, s=self.gamma_2_as(gamma)\n        #print(a.shape, s.shape, sigma.shape, t.shape)\n        #print(\"sigma\", sigma, \"t\", t)\n        #print(\"a\", a,\"s\", s)\n\n        z_t=a*xn #this is equivalent to cin\n        #print(\"z_t std\",z_t.std(-1))\n\n        t=t.expand(z_t.shape[0],1).squeeze(-1) \n        #print(\"before net\", z_t.shape, t.shape)\n        eps_hat=net(z_t, t)\n        eps_hat=eps_hat\n\n        x0_hat=(-s*eps_hat+z_t)/a #equvalent to cout and cskip\n\n        return x0_hat#this will crash because of broadcasting problems, debug later!\n\n    def prepare_train_preconditioning(self, x, sigma):\n        #weight=self.lambda_w(sigma)\n        #Is calling the denoiser here a good idea? Maybe it would be better to apply directly the preconditioning as in the paper, even though Karras et al seem to do it this way in their code\n        print(x.shape)\n        noise=self.sample_prior(x.shape,sigma)\n\n        cskip=self.cskip(sigma)\n        cout=self.cout(sigma)\n        cin=self.cin(sigma)\n        cnoise=self.cnoise(sigma)\n\n        target=(1/cout)*(x-cskip*(x+noise))\n\n        return cin*(x+noise), target, cnoise\n\n\n    def loss_fn(self, net, x):\n        \"\"\"\n        Loss function, which is the mean squared error between the denoised latent and the clean latent\n        Args:\n            net (nn.Module): Model of the denoiser\n            x (Tensor): shape: (B,T) Intermediate noisy latent to denoise\n            sigma (float): noise level (equal to timestep is sigma=t, which is our default)\n        \"\"\"\n        sigma=self.sample_ptrain_safe(x.shape[0]).unsqueeze(-1).to(x.device)\n\n        input, target, cnoise= self.prepare_train_preconditioning(x, sigma)\n        estimate=net(input,cnoise)\n        \n        error=(estimate-target)\n\n        try:\n            #this will only happen if the model is cqt-based, if it crashes it is normal\n            if self.args.net.use_cqt_DC_correction:\n                error=net.CQTransform.apply_hpf_DC(error) #apply the DC correction to the error as we dont want to propagate the DC component of the error as the network is discarding it. It also applies for the nyquit frequency, but this is less critical.\n        except:\n            pass \n\n        #APPLY A-WEIGHTING\n        if self.args.diff_params.aweighting.use_aweighting:\n            error=self.AW(error)\n\n        #here we have the chance to apply further emphasis to the error, as some kind of perceptual frequency weighting could be\n        return error**2, sigma", "\n"]}
{"filename": "datasets/audiofolder_test.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Streaming images and labels from datasets created with dataset_tool.py.\"\"\"\n\nimport os", "\nimport os\nimport numpy as np\nimport zipfile\n#import PIL.Image\nimport json\nimport torch\nimport utils.dnnlib as dnnlib\nimport random\nimport pandas as pd", "import random\nimport pandas as pd\nimport glob\nimport soundfile as sf\n\n#try:\n#    import pyspng\n#except ImportError:\n#    pyspng = None\n", "#    pyspng = None\n\n#----------------------------------------------------------------------------\n# Dataset subclass that loads images recursively from the specified directory\n# or ZIP file.\nclass AudioFolderDatasetTest(torch.utils.data.Dataset):\n    def __init__(self,\n        dset_args,\n        fs=44100,\n        seg_len=131072,\n        num_samples=4,\n        seed=42 ):\n\n        super().__init__()\n        random.seed(seed)\n        np.random.seed(seed)\n        path=dset_args.test.path\n\n        filelist=glob.glob(os.path.join(path,\"*.wav\"))\n        assert len(filelist)>0 , \"error in dataloading: empty or nonexistent folder\"\n        self.train_samples=filelist\n        self.seg_len=int(seg_len)\n        self.fs=fs\n\n        self.test_samples=[]\n        self.filenames=[]\n        self._fs=[]\n        for i in range(num_samples):\n            file=self.train_samples[i]\n            self.filenames.append(os.path.basename(file))\n            data, samplerate = sf.read(file)\n            data=data.T\n            self._fs.append(samplerate)\n            if data.shape[-1]>=self.seg_len:\n                idx=np.random.randint(0,data.shape[-1]-self.seg_len)\n                data=data[...,idx:idx+self.seg_len]\n            else:\n                idx=0\n                data=np.tile(data,(self.seg_len//data.shape[-1]+1))[...,idx:idx+self.seg_len]\n\n            if not dset_args.test.stereo and len(data.shape)>1 :\n                data=np.mean(data,axis=1)\n\n            self.test_samples.append(data[...,0:self.seg_len]) #use only 50s\n\n\n    def __getitem__(self, idx):\n        #return self.test_samples[idx]\n        return self.test_samples[idx], self._fs[idx], self.filenames[idx]\n\n    def __len__(self):\n        return len(self.test_samples)", "\n"]}
{"filename": "datasets/maestro_dataset.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Streaming images and labels from datasets created with dataset_tool.py.\"\"\"\n\nimport os", "\nimport os\nimport numpy as np\nimport zipfile\n#import PIL.Image\nimport json\nimport torch\nimport utils.dnnlib as dnnlib\nimport random\nimport pandas as pd", "import random\nimport pandas as pd\nimport glob\nimport soundfile as sf\n\n#try:\n#    import pyspng\n#except ImportError:\n#    pyspng = None\n", "#    pyspng = None\n\n#----------------------------------------------------------------------------\n# Dataset subclass that loads images recursively from the specified directory\n# or ZIP file.\n\nclass MaestroDataset_fs(torch.utils.data.IterableDataset):\n    def __init__(self,\n        dset_args,\n        overfit=False, #set to True for overfitting dataset (lightweight tests to vccheck that the dataloading is not bottlenecking)\n        seed=42 ):\n\n        super().__init__()\n        self.overfit=overfit\n        random.seed(seed)\n        np.random.seed(seed)\n        path=dset_args.path\n        years=dset_args.years\n\n        metadata_file=os.path.join(path,\"maestro-v3.0.0.csv\")\n        metadata=pd.read_csv(metadata_file)\n\n        metadata=metadata[metadata[\"year\"].isin(years)]\n        metadata=metadata[metadata[\"split\"]==\"train\"]\n        filelist=metadata[\"audio_filename\"]\n\n        filelist=filelist.map(lambda x:  os.path.join(path,x)     , na_action='ignore')\n\n\n        self.train_samples=filelist.to_list()\n       \n        self.seg_len=int(dset_args.load_len)\n\n\n    def __iter__(self):\n        if self.overfit:\n           data_clean=self.overfit_sample\n        while True:\n            if not self.overfit:\n                num=random.randint(0,len(self.train_samples)-1)\n                #for file in self.train_samples:  \n                file=self.train_samples[num]\n                data, samplerate = sf.read(file)\n                #print(file,samplerate)\n\n                data_clean=data\n                #Stereo to mono\n                if len(data.shape)>1 :\n                    data_clean=np.mean(data_clean,axis=1)\n    \n            #normalize\n            #no normalization!!\n            #data_clean=data_clean/np.max(np.abs(data_clean))\n         \n            #framify data clean files\n\n            num_frames=np.floor(len(data_clean)/self.seg_len) \n            \n            if num_frames>4:\n                for i in range(8):\n                    #get 8 random batches to be a bit faster\n                    if not self.overfit:\n                        idx=np.random.randint(0,len(data_clean)-self.seg_len)\n                    else: \n                        idx=0\n                    segment=data_clean[idx:idx+self.seg_len]\n                    segment=segment.astype('float32')\n                    #b=np.mean(np.abs(segment))\n                    #segment= (10/(b*np.sqrt(2)))*segment #default rms  of 0.1. Is this scaling correct??\n                     \n                    #let's make this shit a bit robust to input scale\n                    #scale=np.random.uniform(1.75,2.25)\n                    #this way I estimage sigma_data (after pre_emph) to be around 1\n                   \n                    #segment=10.0**(scale) *segment\n                    yield  segment, samplerate\n            else:\n                pass", "class MaestroDataset(torch.utils.data.IterableDataset):\n    def __init__(self,\n        dset_args,\n        fs=44100,\n        seg_len=131072,\n        overfit=False, #set to True for overfitting dataset (lightweight tests to vccheck that the dataloading is not bottlenecking)\n        seed=42 ):\n\n        super().__init__()\n        self.overfit=overfit\n        random.seed(seed)\n        np.random.seed(seed)\n        path=dset_args.path\n        years=dset_args.years\n\n        metadata_file=os.path.join(path,\"maestro-v3.0.0.csv\")\n        metadata=pd.read_csv(metadata_file)\n\n        metadata=metadata[metadata[\"year\"].isin(years)]\n        metadata=metadata[metadata[\"split\"]==\"train\"]\n        filelist=metadata[\"audio_filename\"]\n\n        filelist=filelist.map(lambda x:  os.path.join(path,x)     , na_action='ignore')\n\n\n        self.train_samples=filelist.to_list()\n       \n        self.seg_len=int(seg_len)\n        self.fs=fs\n        if self.overfit:\n            file=self.train_samples[0]\n            data, samplerate = sf.read(file)\n            assert samplerate==self.fs, \"wrong sampling rate\"\n            if len(data.shape)>1 :\n                data=np.mean(data,axis=1)\n            self.overfit_sample=data[10*samplerate:60*samplerate] #use only 50s\n\n    def __iter__(self):\n        if self.overfit:\n           data_clean=self.overfit_sample\n        while True:\n            if not self.overfit:\n                num=random.randint(0,len(self.train_samples)-1)\n                #for file in self.train_samples:  \n                file=self.train_samples[num]\n                data, samplerate = sf.read(file)\n                assert(samplerate==self.fs, \"wrong sampling rate\")\n                data_clean=data\n                #Stereo to mono\n                if len(data.shape)>1 :\n                    data_clean=np.mean(data_clean,axis=1)\n    \n            #normalize\n            #no normalization!!\n            #data_clean=data_clean/np.max(np.abs(data_clean))\n         \n            #framify data clean files\n            num_frames=np.floor(len(data_clean)/self.seg_len) \n            \n            if num_frames>4:\n                for i in range(8):\n                    #get 8 random batches to be a bit faster\n                    if not self.overfit:\n                        idx=np.random.randint(0,len(data_clean)-self.seg_len)\n                    else: \n                        idx=0\n                    segment=data_clean[idx:idx+self.seg_len]\n                    segment=segment.astype('float32')\n                    #b=np.mean(np.abs(segment))\n                    #segment= (10/(b*np.sqrt(2)))*segment #default rms  of 0.1. Is this scaling correct??\n                     \n                    #let's make this shit a bit robust to input scale\n                    #scale=np.random.uniform(1.75,2.25)\n                    #this way I estimage sigma_data (after pre_emph) to be around 1\n                   \n                    #segment=10.0**(scale) *segment\n                    yield  segment\n            else:\n                pass", "\n"]}
{"filename": "datasets/audiofolder.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Streaming images and labels from datasets created with dataset_tool.py.\"\"\"\n\nimport os", "\nimport os\nimport numpy as np\nimport zipfile\n#import PIL.Image\nimport json\nimport torch\nimport utils.dnnlib as dnnlib\nimport random\nimport pandas as pd", "import random\nimport pandas as pd\nimport glob\nimport soundfile as sf\n\n#try:\n#    import pyspng\n#except ImportError:\n#    pyspng = None\n", "#    pyspng = None\n\n#----------------------------------------------------------------------------\n# Dataset subclass that loads images recursively from the specified directory\n# or ZIP file.\nclass AudioFolderDataset(torch.utils.data.IterableDataset):\n    def __init__(self,\n        dset_args,\n        fs=44100,\n        seg_len=131072,\n        overfit=False,\n        seed=42 ):\n        self.overfit=overfit\n\n        super().__init__()\n        random.seed(seed)\n        np.random.seed(seed)\n        path=dset_args.path\n\n        filelist=glob.glob(os.path.join(path,\"*.wav\"))\n        assert len(filelist)>0 , \"error in dataloading: empty or nonexistent folder\"\n\n        self.train_samples=filelist\n       \n        self.seg_len=int(seg_len)\n        self.fs=fs\n        if self.overfit:\n            file=self.train_samples[0]\n            data, samplerate = sf.read(file)\n            if len(data.shape)>1 :\n                data=np.mean(data,axis=1)\n            self.overfit_sample=data[10*samplerate:60*samplerate] #use only 50s\n\n    def __iter__(self):\n        if self.overfit:\n           data_clean=self.overfit_sample\n        while True:\n            if not self.overfit:\n                num=random.randint(0,len(self.train_samples)-1)\n                #for file in self.train_samples:  \n                file=self.train_samples[num]\n                data, samplerate = sf.read(file)\n                assert(samplerate==self.fs, \"wrong sampling rate\")\n                data_clean=data\n                #Stereo to mono\n                if len(data.shape)>1 :\n                    data_clean=np.mean(data_clean,axis=1)\n    \n            #normalize\n            #no normalization!!\n            #data_clean=data_clean/np.max(np.abs(data_clean))\n         \n            #framify data clean files\n            num_frames=np.floor(len(data_clean)/self.seg_len) \n            \n            #if num_frames>4:\n            for i in range(8):\n                #get 8 random batches to be a bit faster\n                if not self.overfit:\n                    idx=np.random.randint(0,len(data_clean)-self.seg_len)\n                else:\n                    idx=0\n                segment=data_clean[idx:idx+self.seg_len]\n                segment=segment.astype('float32')\n                #b=np.mean(np.abs(segment))\n                #segment= (10/(b*np.sqrt(2)))*segment #default rms  of 0.1. Is this scaling correct??\n                    \n                #let's make this shit a bit robust to input scale\n                #scale=np.random.uniform(1.75,2.25)\n                #this way I estimage sigma_data (after pre_emph) to be around 1\n                \n                #segment=10.0**(scale) *segment\n                yield  segment", "            #else:\n            #    pass\n\n\n"]}
{"filename": "datasets/cocochorales.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Streaming images and labels from datasets created with dataset_tool.py.\"\"\"\n\nimport os", "\nimport os\nimport numpy as np\nimport zipfile\n#import PIL.Image\nimport json\nimport torch\nimport utils.dnnlib as dnnlib\nimport random\nimport pandas as pd", "import random\nimport pandas as pd\nimport glob\nimport soundfile as sf\n\n#try:\n#    import pyspng\n#except ImportError:\n#    pyspng = None\n", "#    pyspng = None\n\n#----------------------------------------------------------------------------\n# Dataset subclass that loads images recursively from the specified directory\n# or ZIP file.\nclass AudioFolderDataset(torch.utils.data.IterableDataset):\n    def __init__(self,\n        dset_args,\n        fs=44100,\n        seg_len=131072,\n        overfit=False,\n        seed=42 ):\n        self.overfit=overfit\n\n        super().__init__()\n        random.seed(seed)\n        np.random.seed(seed)\n        path=dset_args.path\n        self.dset_args=dset_args\n\n        filelist=glob.glob(os.path.join(path,\"*/\"))\n        assert len(filelist)>0 , \"error in dataloading: empty or nonexistent folder\"\n\n        self.train_samples=filelist\n       \n        self.seg_len=int(seg_len)\n        self.fs=fs\n        if self.overfit:\n            raise NotImplementedError\n            file=self.train_samples[0]\n            data, samplerate = sf.read(file)\n            if len(data.shape)>1 :\n                data=np.mean(data,axis=1)\n            self.overfit_sample=data[10*samplerate:60*samplerate] #use only 50s\n    def load_audio_file(self,file):\n                data, samplerate = sf.read(file)\n                assert(samplerate==self.fs, \"wrong sampling rate\")\n                data_clean=data\n                #Stereo to mono\n                if len(data.shape)>1 :\n                    data_clean=np.mean(data_clean,axis=1)\n                return data_clean\n\n    def __iter__(self):\n        if self.overfit:\n           data_clean=self.overfit_sample\n        while True:\n            if not self.overfit:\n\n\n                num=random.randint(0,len(self.train_samples)-1)\n\n                #for file in self.train_samples:  \n                file=self.train_samples[num]\n\n                audio=np.zeros(self.seg_len)\n\n                #get random number between 0 and 1\n                rand_num=random.random()\n                if rand_num<self.dset_args.prob_quartet:\n\n                    #load the 4 stems\n                    print(\"load 4 stems\")\n                    stems=glob.glob(os.path.join(file,\"*.wav\"))\n                    if not( len(stems)==4):\n                         \"error in dataloading: wrong number of stems\"\n                    audio=[]\n                    print(stems)\n                    for s in stems:\n                        audio+=[self.load_audio_file(s)]\n\n                elif rand_num<self.dset_args.prob_quartet+self.dset_args.prob_trio:\n                    #load 3 stems\n                    print(\"load 3 stems\")\n                    stems=glob.glob(os.path.join(file,\"*.wav\"))\n                    #assert len(stems)==4, \"error in dataloading: wrong number of stems\"\n                    #remove one random stem\n                    stems.pop(random.randrange(len(stems)))\n                    #assert len(stems)==3, \"error in dataloading: wrong number of stems\"\n                    print(stems)\n                    audio=[]\n                    for s in stems:\n                        audio+=[self.load_audio_file(s)]\n\n                elif rand_num<self.dset_args.prob_quartet+self.dset_args.prob_trio+self.dset_args.prob_duo:\n                    #load 2 stems\n                    print(\"load 2 stems\")\n                    stems=glob.glob(os.path.join(file,\"*.wav\"))\n                    #assert len(stems)==4, \"error in dataloading: wrong number of stems\"\n                    #remove two random stems\n                    stems.pop(random.randrange(len(stems)))\n                    stems.pop(random.randrange(len(stems)))\n                    #assert len(stems)==2, \"error in dataloading: wrong number of stems\"\n                    audio=[]\n                    print(stems)\n                    for s in stems:\n                        audio+=[self.load_audio_file(s)]\n                else:\n                    #load 1 stem\n                    print(\"load 1 stem\")\n                    stems=glob.glob(os.path.join(file,\"*.wav\"))\n                    #assert len(stems)==4, \"error in dataloading: wrong number of stems\"\n                    #remove two random stems\n                    stems.pop(random.randrange(len(stems)))\n                    stems.pop(random.randrange(len(stems)))\n                    stems.pop(random.randrange(len(stems)))\n                    #assert len(stems)==1, \"error in dataloading: wrong number of stems\"\n                    audio=[]\n                    print(stems)\n                    for s in stems:\n                        audio+=[self.load_audio_file(s)]\n\n            #normalize\n            #no normalization!!\n            #data_clean=data_clean/np.max(np.abs(data_clean))\n         \n            #framify data clean files\n            num_frames=np.floor(len(audio[0])/self.seg_len) \n            \n            #if num_frames>4:\n            for i in range(8):\n                #get 8 random batches to be a bit faster\n                if not self.overfit:\n                    idx=np.random.randint(0,len(audio[0])-self.seg_len)\n                else:\n                    idx=0\n\n\n                segment=audio[0][idx:idx+self.seg_len]\n                if len(audio)>1:\n                    for d in audio[1:]:\n                        try:\n                            segment+=d[idx:idx+self.seg_len]\n                        except:\n                            pass\n                \n\n                segment=segment.astype('float32')\n                #b=np.mean(np.abs(segment))\n                #segment= (10/(b*np.sqrt(2)))*segment #default rms  of 0.1. Is this scaling correct??\n                    \n                #let's make this shit a bit robust to input scale\n                #scale=np.random.uniform(1.75,2.25)\n                #this way I estimage sigma_data (after pre_emph) to be around 1\n                \n                #segment=10.0**(scale) *segment\n                yield  segment", "            #else:\n            #    pass\n\n\n"]}
{"filename": "datasets/maestro_dataset_test.py", "chunked_list": ["# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# This work is licensed under a Creative Commons\n# Attribution-NonCommercial-ShareAlike 4.0 International License.\n# You should have received a copy of the license along with this\n# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n\n\"\"\"Streaming images and labels from datasets created with dataset_tool.py.\"\"\"\n\nimport os", "\nimport os\nimport numpy as np\nimport zipfile\n#import PIL.Image\nimport json\nimport torch\nimport utils.dnnlib as dnnlib\nimport random\nimport pandas as pd", "import random\nimport pandas as pd\nimport glob\nimport soundfile as sf\n\n#try:\n#    import pyspng\n#except ImportError:\n#    pyspng = None\n", "#    pyspng = None\n\n#----------------------------------------------------------------------------\n# Dataset subclass that loads images recursively from the specified directory\n# or ZIP file.\nclass MaestroDatasetTestChunks(torch.utils.data.Dataset):\n    def __init__(self,\n        dset_args,\n        num_samples=4,\n        seed=42 ):\n\n        super().__init__()\n        random.seed(seed)\n        np.random.seed(seed)\n        path=dset_args.path\n        years=dset_args.years\n\n        self.seg_len=int(dset_args.load_len)\n\n        metadata_file=os.path.join(path,\"maestro-v3.0.0.csv\")\n        metadata=pd.read_csv(metadata_file)\n\n        metadata=metadata[metadata[\"year\"].isin(years)]\n        metadata=metadata[metadata[\"split\"]==\"test\"]\n        filelist=metadata[\"audio_filename\"]\n\n        filelist=filelist.map(lambda x:  os.path.join(path,x)     , na_action='ignore')\n\n\n        self.filelist=filelist.to_list()\n\n        self.test_samples=[]\n        self.filenames=[]\n        self.f_s=[]\n        for i in range(num_samples):\n            file=self.filelist[i]\n            self.filenames.append(os.path.basename(file))\n            data, samplerate = sf.read(file)\n            if len(data.shape)>1 :\n                data=np.mean(data,axis=1)\n\n            self.test_samples.append(data[10*samplerate:10*samplerate+self.seg_len]) #use only 50s\n            self.f_s.append(samplerate)\n       \n\n    def __getitem__(self, idx):\n        return self.test_samples[idx], self.f_s[idx], self.filenames[idx]\n\n    def __len__(self):\n        return len(self.test_samples)", "\n\n"]}
{"filename": "testing/blind_bwe_tester_mushra.py", "chunked_list": ["from datetime import date\nimport pickle\nimport re\nimport torch\nimport torchaudio\n#from src.models.unet_cqt import Unet_CQT\n#from src.models.unet_stft import Unet_STFT\n#from src.models.unet_1d import Unet_1d\n#import src.utils.setup as utils_setup\n#from src.sde import  VE_Sde_Elucidating", "#import src.utils.setup as utils_setup\n#from src.sde import  VE_Sde_Elucidating\nimport numpy as np\nimport utils.dnnlib as dnnlib\nimport os\n\nimport utils.logging as utils_logging\nimport wandb\nimport copy\n", "import copy\n\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport utils.bandwidth_extension as utils_bwe\nimport omegaconf\n\n#import utils.filter_generation_utils as f_utils\nimport utils.blind_bwe_utils as blind_bwe_utils", "#import utils.filter_generation_utils as f_utils\nimport utils.blind_bwe_utils as blind_bwe_utils\nimport utils.training_utils as t_utils\n\nimport soundfile as sf\n\n#from utils.spectral_analysis import LTAS_processor\n\n\nclass BlindTester():\n    def __init__(\n        self, args=None, network=None, diff_params=None, test_set=None, device=None, it=None\n    ):\n        self.args=args\n        self.network=torch.compile(network)\n        #self.network=network\n        #prnt number of parameters\n        \n\n        self.diff_params=copy.copy(diff_params)\n        self.device=device\n        #choose gpu as the device if possible\n        if self.device is None:\n            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.network=network\n\n        torch.backends.cudnn.benchmark = True\n\n        today=date.today() \n        if it is None:\n            self.it=0\n\n        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n        if not os.path.exists(self.path_sampling):\n            os.makedirs(self.path_sampling)\n\n\n        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n        self.setup_sampler()\n\n        self.use_wandb=False #hardcoded for now\n\n        S=self.args.exp.resample_factor\n        if S>2.1 and S<2.2:\n            #resampling 48k to 22.05k\n            self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n        elif S!=1:\n            N=int(self.args.exp.audio_len*S)\n            self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\n        if test_set is not None:\n            self.test_set=test_set\n            self.do_inpainting=True\n            self.do_bwe=True\n            self.do_blind_bwe=True\n        else:\n            self.test_set=None\n            self.do_inpainting=False\n            self.do_bwe=False #these need to be set up in the config file\n            self.do_blind_bwe=False\n\n        self.paths={}\n        if self.do_inpainting and (\"inpainting\" in self.args.tester.modes):\n            self.do_inpainting=True\n            mode=\"inpainting\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"inpainting\",\"masked\",\"inpainted\")\n            #TODO add more information in the subirectory names\n        else: self.do_inpainting=False\n\n        if self.do_bwe and (\"bwe\" in self.args.tester.modes):\n            self.do_bwe=True\n            mode=\"bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"bwe\",\"lowpassed\",\"bwe\")\n            #TODO add more information in the subirectory names\n        else:\n            self.do_bwe=False\n\n        if self.do_blind_bwe and (\"blind_bwe\" in self.args.tester.modes):\n            self.do_blind_bwe=True\n            mode=\"blind_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"], self.paths[mode+\"degraded_estimate\"]=self.prepare_blind_experiment(\"blind_bwe\",\"masked\",\"blind_bwe\",\"degraded_estimate\")\n            #TODO add more information in the subirectory names\n        if \"real_blind_bwe\" in self.args.tester.modes:\n            self.do_blind_bwe=True\n            mode=\"real_blind_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"real_blind_bwe\",\"degraded\",\"reconstructed\")\n            #TODO add more information in the subirectory names\n\n        if \"formal_test_bwe\" in self.args.tester.modes:\n            self.do_formal_test_bwe=True\n            mode=\"formal_test_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"formal_test_bwe\",\"degraded\",\"reconstructed\")\n        \n        if (\"unconditional\" in self.args.tester.modes):\n            mode=\"unconditional\"\n            self.paths[mode]=self.prepare_unc_experiment(\"unconditional\")\n\n\n        if (\"filter_bwe\" in self.args.tester.modes):\n            mode=\"filter_bwe\"\n            self.paths[mode]=self.prepare_unc_experiment(\"filter_bwe\")\n\n        #self.LTAS_processor=LTAS_processor(self.args.tester.blind_bwe.LTAS.sample_rate,self.args.tester.blind_bwe.LTAS.audio_len)\n        #self.LTAS_processor.load_dataset_LTAS(self.args.tester.blind_bwe.LTAS.path)\n\n    def prepare_unc_experiment(self, str):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n            return path_exp\n\n    def prepare_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n\n            return path_exp, path_degraded, path_original, path_reconstructed\n\n    def resample_audio(self, audio, fs):\n        #this has been reused from the trainer.py\n        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\n    def prepare_blind_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\", str_degraded_estimate=\"degraded_estimate\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n            \n            n=str_degraded_estimate\n            path_degraded_estimate=os.path.join(path_exp, n) #path for the estimated degraded signal\n            #ensure the path exists\n            if not os.path.exists(path_degraded_estimate):\n                os.makedirs(path_degraded_estimate)\n\n            return path_exp, path_degraded, path_original, path_reconstructed, path_degraded_estimate\n\n    def setup_wandb(self):\n        \"\"\"\n        Configure wandb, open a new run and log the configuration.\n        \"\"\"\n        config=omegaconf.OmegaConf.to_container(\n            self.args, resolve=True, throw_on_missing=True\n        )\n        self.wandb_run=wandb.init(project=\"testing\"+self.args.tester.name, entity=self.args.exp.wandb.entity, config=config)\n        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n        self.use_wandb=True\n\n    def setup_wandb_run(self, run):\n        #get the wandb run object from outside (in trainer.py or somewhere else)\n        self.wandb_run=run\n        self.use_wandb=True\n\n    def setup_sampler(self):\n        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network,  diff_params=self.diff_params, args=self.args, rid=True) #rid is used to log some extra information\n\n    \n    def load_latest_checkpoint(self ):\n        #load the latest checkpoint from self.args.model_dir\n        try:\n            # find latest checkpoint_id\n            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n            save_name = f\"{self.args.model_dir}/{save_basename}\"\n            list_weights = glob(save_name)\n            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n            list_ids = [int(id_regex.search(weight_path).groups()[0])\n                        for weight_path in list_weights]\n            checkpoint_id = max(list_ids)\n\n            state_dict = torch.load(\n                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n            self.network.load_state_dict(state_dict['ema'])\n            print(f\"Loaded checkpoint {checkpoint_id}\")\n            return True\n        except (FileNotFoundError, ValueError):\n            raise ValueError(\"No checkpoint found\")\n\n\n    def load_checkpoint(self, path):\n        state_dict = torch.load(path, map_location=self.device)\n        if self.args.exp.exp_name==\"diffwave-sr\":\n            print(state_dict.keys())\n            print(\"noise_schedukar\",state_dict[\"noise_scheduler\"])\n            self.network.load_state_dict(state_dict['ema_model'])\n            self.network.eval()\n            print(\"ckpt loaded\")\n        else:\n            try:\n                print(\"load try 1\")\n                self.network.load_state_dict(state_dict['ema'])\n            except:\n                #self.network.load_state_dict(state_dict['model'])\n                try:\n                    print(\"load try 2\")\n                    dic_ema = {}\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n                        dic_ema[key] = tensor\n                    self.network.load_state_dict(dic_ema)\n                except:\n                    print(\"load try 3\")\n                    dic_ema = {}\n                    i=0\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n                        if tensor.requires_grad:\n                            dic_ema[key]=state_dict['ema_weights'][i]\n                            i=i+1\n                        else:\n                            dic_ema[key]=tensor     \n                    self.network.load_state_dict(dic_ema)\n        try:\n            self.it=state_dict['it']\n        except:\n            self.it=0\n\n    def log_filter(self,preds, f, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n\n        fig_filter=utils_logging.plot_batch_of_lines(preds, f)\n\n        self.wandb_run.log({\"filters_\"+str(string): fig_filter}, step=self.it, commit=True)\n\n    def log_audio(self,preds, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=self.args.model_dir)\n        print(audio_path)\n        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it, commit=False)\n        #TODO: log spectrogram of the audio file to wandb\n        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\n        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it, commit=True)\n\n    def sample_unconditional_diffwavesr(self):\n        #print some parameters of self.network\n        #print(\"self.network\", self.network.input_projection[0].weight)\n        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        rid=False\n        z_1=torch.randn(shape, device=self.device)\n        #print(\"sd\",z_1.std(-1))\n        outputs=self.sampler.diff_params.reverse_process_ddim(z_1, self.network)\n        preds=outputs\n\n        self.log_audio(preds.detach(), \"unconditional\")\n\n        return preds\n    def sample_unconditional(self):\n        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        rid=False\n        outputs=self.sampler.predict_unconditional(shape, self.device, rid=rid)\n        if rid:\n            preds, data_denoised, t=outputs\n            fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"unconditional_signal_generation\")\n        else:\n            preds=outputs\n\n        self.log_audio(preds, \"unconditional\")\n\n        return preds\n\n\n    def formal_test_bwe(self, typefilter=\"firwin\", test_filter_fit=False, compute_sweep=False, blind=False):\n        print(\"BLIND\", blind)\n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n        test_bwe_table_audio = wandb.Table(columns=columns)\n\n        if not self.do_formal_test_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        elif typefilter==\"3rdoct\":\n            type=\"3rdoct\"\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n            da_filter=da_filter.to(self.device)\n        else:\n            type=self.args.tester.bandwidth_extension.filter.type\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n            da_filter=da_filter.to(self.device)\n\n\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\n        path=self.args.tester.formal_test.path\n        filenames=glob(path+\"/*.wav\")\n\n        segL=self.args.exp.audio_len\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n\n        for filename in filenames:\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n            #open audio file\n            d,fs=sf.read(filename)\n            D=torch.Tensor(d).to(self.device).unsqueeze(0)\n            print(\"D\", D.shape, fs)\n\n\n            path_out=self.args.tester.formal_test.folder\n\n            print(\"skippint?\", os.path.join(path_out, basename))\n            if os.path.exists(os.path.join(path_out, basename)):\n                print(\"yes skippint\", os.path.join(path_out, basename))\n                continue\n            print(\"skippint?\", os.path.join(path_out, basename+\".wav\"))\n            if os.path.exists(os.path.join(path_out, basename+\".wav\")):\n                print(\"yes skippint\", os.path.join(path_out, basename+\".wav\"))\n                continue\n\n            if type==\"fc_A\":\n                degraded=self.apply_lowpass_fcA(D, da_filter)\n            else:\n                degraded=self.apply_low_pass(D, da_filter, type)\n            #path_degraded=utils_logging.write_audio_file(degraded, self.args.exp.sample_rate, basename+\".degraded.wav\", path=path_out)\n\n            print(\"filename\",filename)\n\n            #n=os.path.splitext(os.path.basename(filename))[0]+typefilter+str(self.args.tester.bandwidth_extension.filter.fc)\n            n=os.path.splitext(os.path.basename(filename))[0]\n\n            #degraded=degraded.float().to(self.device).unsqueeze(0)\n            print(n)\n            final_pred=torch.zeros_like(degraded)\n    \n            print(\"dsds FS\",fs)\n    \n            print(\"seg shape\",degraded.shape)\n            degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n            print(\"seg shape\",degraded.shape)\n    \n            std= degraded.std(-1)\n    \n            rid=False\n\n    \n            L=degraded.shape[-1]\n            #modify the sampler, so that it is computationally cheaper\n    \n            discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n            discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n    \n            #first segment\n            ix=0\n            seg=degraded[...,ix:ix+segL]\n            #pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n            filter_data=[]\n\n            rid=False\n            if blind:\n                outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                pred, estimated_filter =outputs\n                filter_data.append(((ix, ix+segL), estimated_filter))\n\n            else:\n                pred=self.sampler.predict_bwe(seg, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False)\n    \n            if self.args.tester.formal_test.use_AR:\n                assert not blind\n                previous_pred=pred[..., 0:segL-discard_end]\n                final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\n                ix+=segL-overlap-discard_end\n     \n                y_masked=torch.zeros_like(pred, device=self.device)\n                mask=torch.ones_like(seg, device=self.device)\n                mask[...,overlap::]=0\n            else:\n                print(\"noar\")\n                hann_window=torch.hann_window(self.args.tester.formal_test.OLA*2, device=self.device)\n                win_pred=pred[...,0:segL-discard_end]\n                win_pred[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n                print(\"ix\", ix, \"segL\", segL, \"discard_end\", discard_end, \"win pred shape\", win_pred.shape)\n                final_pred[...,ix:ix+segL-discard_end]=win_pred\n\n                ix+=segL-discard_end-self.args.tester.formal_test.OLA\n    \n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n\n            if blind:\n                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                        pickle.dump(filter_data, f)\n\n            while ix<L-segL-discard_end-discard_start:\n\n                seg=degraded[...,ix:ix+segL]\n                if self.args.tester.formal_test.use_AR:\n                    y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n                    pred=self.sampler.predict_bwe_AR(seg, y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n                else:\n                    if blind:\n                        outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                        pred, estimated_filter =outputs\n                        filter_data.append(((ix, ix+segL), estimated_filter))\n                    else:\n                        pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n    \n                previous_pred_win=pred[..., 0:segL-discard_end]\n                previous_pred_win[..., 0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n                previous_pred_win[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n    \n    \n                final_pred[...,ix:ix+segL-discard_end]+=previous_pred_win\n\n                #do a little bit of overlap and add with a hann window to avoid discontinuities\n                #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n                #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n    \n                path, basename=os.path.split(filename)\n                print(path, basename)\n    \n                path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n    \n                if self.args.tester.formal_test.use_AR:\n\n                    ix+=segL-overlap-discard_end\n                else:\n                    ix+=segL-discard_end-self.args.tester.formal_test.OLA\n\n                if blind:\n                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                        pickle.dump(filter_data, f)\n    \n            #skipping the last segment, which is not complete, I am lazy\n            seg=degraded[...,ix::]\n\n            if self.args.tester.formal_test.use_AR:\n                y_masked[...,0:overlap]=pred[...,-overlap::]\n    \n            if seg.shape[-1]<segL:\n                #cat zeros\n                seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n    \n                if self.args.tester.formal_test.use_AR:\n                    #the cat zeroes will also be part of the observed signal, so I need to mask them\n                    y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n                    mask[...,seg.shape[-1]:segL]=0\n    \n            else:\n                seg_zp=seg[...,0:segL]\n    \n    \n            if self.args.tester.formal_test.use_AR:\n                pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n            else:\n                if blind:\n                    outputs=self.sampler.predict_blind_bwe(seg_zp, rid=False)\n                    pred, estimated_filter =outputs\n                    filter_data.append(((ix, ix+segL), estimated_filter))\n                else:\n                    pred=self.sampler.predict_bwe(seg_zp, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n                \n            \n            if not self.args.tester.formal_test.use_AR:\n                win_pred=pred[...,0:seg.shape[-1]]\n                win_pred[...,0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n                final_pred[...,ix::]+=win_pred\n            else:\n                final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n    \n            #final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n            #final_pred=final_pred*10**(-scale/20)\n    \n            #extract path from filename\n    \n    \n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".wav\", path=path_out)\n            #save filter_data in a pickle file\n            with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                pickle.dump(filter_data, f)\n\n               \n\n\n    def test_bwe(self, typefilter=\"fc_A\", test_filter_fit=False, compute_sweep=False):\n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n        test_bwe_table_audio = wandb.Table(columns=columns)\n\n        if not self.do_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        elif typefilter==\"3rdoct\":\n            type=\"3rdoct\"\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n            da_filter=da_filter.to(self.device)\n        else:\n            type=self.args.tester.bandwidth_extension.filter.type\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n            da_filter=da_filter.to(self.device)\n\n\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n            n=os.path.splitext(filename[0])[0]\n            seg=original.float().to(self.device)\n\n            seg=self.resample_audio(seg, fs)\n\n\n            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n            #        print(\"gain boost\", self.args.tester.bandwidth_extension.gain_boost)\n            #        #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n            #        #add gain boost (in dB)\n            #        seg=seg*10**(self.args.tester.bandwidth_extension.gain_boost/20)\n\n            if type==\"fc_A\":\n                y=self.apply_lowpass_fcA(seg, da_filter)\n            else:\n                y=self.apply_low_pass(seg, da_filter, type)\n            #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type, typefilter) \n\n            #if self.args.tester.bandwidth_extension.sigma_observations != \"None\":\n            #    sigma=self.args.tester.bandwidth_extension.sigma_observations\n            #    y+=sigma*torch.randn(y.shape).to(y.device)\n\n            if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n\n            print(\"y\", y.shape)\n            if test_filter_fit:\n                if compute_sweep:\n                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True, compute_sweep=True)\n                    pred, data_denoised, data_score, t, data_filters, data_norms, data_grads =out\n                    #save the data_norms and data_grads as a .npy file\n                    np.save(self.paths[\"bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                    np.save(self.paths[\"bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                else:\n                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True)\n                    pred, data_denoised, data_score, t, data_filters =out\n            else:\n                rid=True\n                out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=False, compute_sweep=False)\n                \n                pred, data_denoised, data_score, t =out\n\n\n            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n            #    #compensate gain boost\n            #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n            #    #add gain boost (in dB)\n            #    pred=pred*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n            #    seg=seg*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n            #    y=y*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\n            res[i,:]=pred\n       \n            path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"original\"])\n\n            path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"degraded\"])\n\n            path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"reconstructed\"])\n\n            test_bwe_table_audio.add_data(i, \n                    wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                    wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                    wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\n            if rid:\n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n            if test_filter_fit:\n\n                #expecting to crash here\n                print(data_filters.shape)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\n        self.wandb_run.log({\"table_bwe_audio\": test_bwe_table_audio}, commit=True) \n\n        if self.use_wandb:\n            self.log_audio(res, \"bwe\")\n\n    def apply_low_pass(self, seg, filter, typefilter):\n        y=utils_bwe.apply_low_pass(seg, filter, self.args.tester.bandwidth_extension.filter.type) \n        return y\n\n    def apply_lowpass_fcA(self, seg, params):\n        freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n        H=blind_bwe_utils.design_filter(params[0], params[1], freqs)\n        xfilt=blind_bwe_utils.apply_filter(seg,H,self.args.tester.blind_bwe.NFFT)\n        return xfilt\n\n    def prepare_filter(self, sample_rate, typefilter):\n        filter=utils_bwe.prepare_filter(self.args, sample_rate )\n        return filter\n    \n    def test_real_blind_bwe_complete(self, typefilter=\"fc_A\", compute_sweep=False):\n        #raise NotImplementedError\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        \n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        filename=self.args.tester.complete_recording.path\n        d,fs=sf.read(filename)\n        degraded=torch.Tensor(d)\n\n        segL=self.args.exp.audio_len\n\n        ix_first=self.args.exp.sample_rate*self.args.tester.complete_recording.ix_start #index of the first segment to be processed, might have to depend on the sample rate\n\n        #for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\n        print(\"filename\",filename)\n        n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n        degraded=degraded.float().to(self.device).unsqueeze(0)\n        print(n)\n\n        print(\"dsds FS\",fs)\n\n        print(\"seg shape\",degraded.shape)\n        degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n        print(\"seg shape\",degraded.shape)\n\n        std= degraded.std(-1)\n        degraded=self.args.tester.complete_recording.std*degraded/std.unsqueeze(-1)\n        #add noise\n        if self.args.tester.complete_recording.SNR_extra_noise!=\"None\":\n            #contaminate a bit with white noise\n            SNR=10**(self.args.tester.complete_recording.SNR_extra_noise/10)\n            sigma2_s=torch.Tensor([self.args.tester.complete_recording.std**2]).to(degraded.device)\n            sigma=torch.sqrt(sigma2_s/SNR)\n            degraded+=sigma*torch.randn(degraded.shape).to(degraded.device)\n\n\n        if self.args.tester.complete_recording.n_segments_blindstep==1:\n            y=degraded[...,ix_first:ix_first+segL]\n        else:\n            #initialize y with the first segment and repeat it\n            y=degraded[...,ix_first:ix_first+segL].repeat(self.args.tester.complete_recording.n_segments_blindstep,1)\n            for j in range(0, self.args.tester.complete_recording.n_segments_blindstep):\n                #random index\n                ix=np.random.randint(0, degraded.shape[-1]-segL)\n                y[j,...]=degraded[...,ix:ix+segL]\n        \n        print(\"y shape\",y.shape)\n\n            \n\n        #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, fs)\n        #print(\"scale\",scale) #TODO I should calculate this with the whole track, not just the first segment\n\n        #y=y*10**(scale/20)\n        #degraded=degraded*10**(scale/20)\n\n\n\n        rid=False\n        outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n        pred, estimated_filter =outputs\n\n        #now I will just throw away the first segment and process the rest of the signal with the estimated filter. Later I should think of a better way to do it\n\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n        hop=segL-overlap\n\n        final_pred=torch.zeros_like(degraded)\n        final_pred[0, ix_first:ix_first+segL]=pred[0]\n\n        path, basename=os.path.split(filename)\n        print(path, basename)\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n        L=degraded.shape[-1]\n\n        #modify the sampler, so that it is computationally cheaper\n\n        discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n        discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\n        #first segment\n        ix=0\n        seg=degraded[...,ix:ix+segL]\n        pred=self.sampler.predict_bwe(seg, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n        previous_pred=pred[..., 0:segL-discard_end]\n\n        final_pred[...,ix:ix+segL-discard_end]=previous_pred\n        ix+=segL-overlap-discard_end\n\n        y_masked=torch.zeros_like(pred, device=self.device)\n        mask=torch.ones_like(seg, device=self.device)\n        mask[...,overlap::]=0\n\n        hann_window=torch.hann_window(overlap*2, device=self.device)\n\n        while ix<L-segL-discard_end-discard_start:\n            y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n            seg=degraded[...,ix:ix+segL]\n\n            pred=self.sampler.predict_bwe_AR(seg, y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n            previous_pred=pred[..., 0:segL-discard_end]\n\n\n            final_pred[...,ix:ix+segL-discard_end]=previous_pred\n            #do a little bit of overlap and add with a hann window to avoid discontinuities\n            #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n            #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n\n\n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n            ix+=segL-overlap-discard_end\n\n        #skipping the last segment, which is not complete, I am lazy\n        seg=degraded[...,ix::]\n        y_masked[...,0:overlap]=pred[...,-overlap::]\n\n        if seg.shape[-1]<segL:\n            #cat zeros\n            seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\n            #the cat zeroes will also be part of the observed signal, so I need to mask them\n            y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n            mask[...,seg.shape[-1]:segL]=0\n\n        else:\n            seg_zp=seg[...,0:segL]\n\n\n        pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n        final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\n        final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n        #final_pred=final_pred*10**(-scale/20)\n\n        #extract path from filename\n\n\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n               \n               \n               \n    def test_real_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\",\"degraded_audio\", \"reconstructed audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n        \n        \n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        path=self.args.tester.blind_bwe.real_recordings.path\n        audio_files=glob(path+\"/*.wav\")\n        print(audio_files, path)\n        test_set_data=[]\n        test_set_fs=[]\n        test_set_names=[]\n        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n            d,fs=sf.read(audio_files[i])\n            #print(d.shape, self.args.exp.audio_len)\n            #if d.shape[-1] >= self.args.exp.audio_len:\n            #    d=d[...,0:self.args.exp.audio_len]\n            test_set_data.append(torch.Tensor(d))\n            test_set_fs.append(fs)\n            print(\"fs\",fs)\n            print(\"len\",len(d))\n            test_set_names.append(audio_files[i])\n\n        for i, (degraded,  filename, fs) in enumerate(tqdm(zip(test_set_data,  test_set_names, test_set_fs))):\n                print(\"filename\",filename)\n                n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n                seg=degraded.float().to(self.device).unsqueeze(0)\n                print(n)\n\n                print(\"dsds FS\",fs)\n\n                print(\"seg shape\",seg.shape)\n                seg=torchaudio.functional.resample(seg, fs, self.args.exp.sample_rate)\n                print(\"seg shape\",seg.shape)\n                ix_start=self.args.tester.blind_bwe\n\n                seg=seg[...,self.args.exp.sample_rate*0:self.args.exp.sample_rate*0+self.args.exp.audio_len]\n                y=seg\n                print(\"y shape\",y.shape)\n                #normalize???\n                std= y.std(-1)\n                y=self.args.tester.blind_bwe.sigma_norm*y/std.unsqueeze(-1)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y,fs)\n                #print(\"scale\",scale)\n                #y=y*10**(scale/20)\n\n\n\n               \n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n                pred=pred*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                y=y*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"reconstructed\"])\n               \n               \n               \n                fig_est_filter=blind_bwe_utils.plot_filter(estimated_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"real_blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n               \n                if typefilter==\"fc_A\":\n                    test_blind_bwe_table_filters.add_data(i, \n                        wandb.Html(path_est_filter),\n                    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"real_blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"real_blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    def test_blind_bwe(self, typefilter=\"firwin\", compute_sweep=False, blind=True):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            fc=self.args.tester.blind_bwe.test_filter.fc\n            A=self.args.tester.blind_bwe.test_filter.A\n            da_filter=torch.Tensor([fc, A]).to(self.device)\n        else:\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate, typefilter) #standardly designed filter\n            da_filter=da_filter.to(self.device)\n        \n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\n        path=self.args.tester.blind_bwe.real_recordings.path\n        audio_files=glob(path+\"/*.wav\")\n        print(audio_files, path)\n        test_set_data=[]\n        test_set_fs=[]\n        test_set_names=[]\n        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n            d,fs=sf.read(audio_files[i])\n            #print(d.shape, self.args.exp.audio_len)\n            #if d.shape[-1] >= self.args.exp.audio_len:\n            #    d=d[...,0:self.args.exp.audio_len]\n            test_set_data.append(torch.Tensor(d))\n            test_set_fs.append(fs)\n            print(\"fs\",fs)\n            print(\"len\",len(d))\n            test_set_names.append(audio_files[i])\n\n        for i,( original,  filename, fs) in enumerate(tqdm(zip(test_set_data,  test_set_names, test_set_fs))):\n        #for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n                n=os.path.basename(filename)+typefilter\n                print(\"n\",n, filename)\n                seg=original.float().to(self.device).unsqueeze(0)\n                #seg=self.resample_audio(seg, fs)\n                seg=seg[...,self.args.exp.sample_rate*0:self.args.exp.sample_rate*0+self.args.exp.audio_len]\n\n                #sigma_norm=0.07 #hardcoded\n                #orig_std=seg.std(-1)\n                #seg=sigma_norm*seg/orig_std\n\n                #if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                #    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                #    orig_std=seg.std(-1)\n                #    seg=sigma_norm*seg/orig_std\n        \n                #elif self.args.tester.blind_bwe.gain_boost != 0:\n                #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                #    #add gain boost (in dB)\n                #    seg=seg*10**(self.args.tester.blind_bwe.gain_boost/20)\n\n\n                #apply lowpass filter\n                print(seg.shape)\n                if typefilter==\"fc_A\":\n                    y=self.apply_lowpass_fcA(seg, da_filter)\n                else:\n                    y=self.apply_low_pass(seg,da_filter, typefilter)\n\n                #add noise to the observations for regularization\n                if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, self.args.exp.sample_rate)\n                #print(\"applied scale\",scale)\n                #y=y*10**(scale/20)\n\n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                if blind:\n                    rid=True\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n                else:\n                    rid=False\n                    pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=rid, test_filter_fit=False, compute_sweep=False)\n               \n                if blind:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"blind_bwe\"]+\"data_t\"+str(i)+\".npy\", t.cpu().numpy())\n                        np.save(self.paths[\"blind_bwe\"]+\"data_denoised\"+str(i)+\".npy\", data_denoised)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_filters\"+str(i)+\".npy\", data_filters)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pass\n               \n               \n               \n                #y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\n                #if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                #    assert orig_std is not None\n                #seg=orig_std*seg/sigma_norm\n                #pred=orig_std*pred/sigma_norm\n                #elif self.args.tester.blind_bwe.gain_boost != 0:\n                #    #compensate gain boost #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.  #    #add gain boost (in dB) #    y_est=y_est*10**(-self.args.tester.blind_bwe.gain_boost/20) #    pred=pred*10**(-self.args.tester.blind_bwe.gain_boost/20) ##    seg=seg*10**(-self.args.tester.blind_bwe.gain_boost/20) #    y=y*10**(-self.args.tester.blind_bwe.gain_boost/20) \n                #y_est=y_est*10**(-scale/20)\n               # pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n               \n                \n                path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n               \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"reconstructed\"])\n               \n                #path_degrade_estimate=utils_logging.write_audio_file(y_est, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded_estimate\"])\n               \n               \n                #will probably crash here!\n                #fig_est_filter=blind_bwe_utils.plot_filter(da_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                #path_est_filter=os.path.join(self.paths[\"blind_bwe\"], str(i)+\"_raw_filter.html\")\n                #fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                #test_blind_bwe_table_audio.add_data(i, \n                #        wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                #        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                #        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate),\n                #        wandb.Audio(path_degrade_estimate, sample_rate=self.args.exp.sample_rate))\n               \n                #if typefilter==\"fc_A\":\n                #    test_blind_bwe_table_filters.add_data(i, \n                #        wandb.Html(path_est_filter),\n                #    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                #print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                #fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                #print(data_filters.shape)\n                #will crash here\n                #fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        #self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        #self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    \n        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n    def dodajob(self):\n        #self.setup_wandb()\n        for m in self.args.tester.modes:\n\n            if m==\"unconditional\":\n                print(\"testing unconditional\")\n                self.sample_unconditional()\n            if m==\"unconditional_diffwavesr\":\n                print(\"testing unconditional\")\n                self.sample_unconditional_diffwavesr()\n            self.it+=1\n            if m==\"blind_bwe\":\n                print(\"TESTING BLIND BWE\")\n                self.test_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep, typefilter=\"firwin\")\n            if m==\"real_blind_bwe\":\n                print(\"TESTING REAL BLIND BWE\")\n                self.test_real_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe_complete\":\n                #process the whole audio file\n                #Estimate the filter in the first chunk, and then apply it to the rest of the audio file (using a little bit of overlap or outpainting)\n                print(\"TESTING REAL BLIND BWE COMPLETE\")\n                self.test_real_blind_bwe_complete(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep)\n            if m==\"formal_test_bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.formal_test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"firwin\", blind=self.args.tester.formal_test.blind)\n        self.it+=1", "\nclass BlindTester():\n    def __init__(\n        self, args=None, network=None, diff_params=None, test_set=None, device=None, it=None\n    ):\n        self.args=args\n        self.network=torch.compile(network)\n        #self.network=network\n        #prnt number of parameters\n        \n\n        self.diff_params=copy.copy(diff_params)\n        self.device=device\n        #choose gpu as the device if possible\n        if self.device is None:\n            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.network=network\n\n        torch.backends.cudnn.benchmark = True\n\n        today=date.today() \n        if it is None:\n            self.it=0\n\n        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n        if not os.path.exists(self.path_sampling):\n            os.makedirs(self.path_sampling)\n\n\n        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n        self.setup_sampler()\n\n        self.use_wandb=False #hardcoded for now\n\n        S=self.args.exp.resample_factor\n        if S>2.1 and S<2.2:\n            #resampling 48k to 22.05k\n            self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n        elif S!=1:\n            N=int(self.args.exp.audio_len*S)\n            self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\n        if test_set is not None:\n            self.test_set=test_set\n            self.do_inpainting=True\n            self.do_bwe=True\n            self.do_blind_bwe=True\n        else:\n            self.test_set=None\n            self.do_inpainting=False\n            self.do_bwe=False #these need to be set up in the config file\n            self.do_blind_bwe=False\n\n        self.paths={}\n        if self.do_inpainting and (\"inpainting\" in self.args.tester.modes):\n            self.do_inpainting=True\n            mode=\"inpainting\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"inpainting\",\"masked\",\"inpainted\")\n            #TODO add more information in the subirectory names\n        else: self.do_inpainting=False\n\n        if self.do_bwe and (\"bwe\" in self.args.tester.modes):\n            self.do_bwe=True\n            mode=\"bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"bwe\",\"lowpassed\",\"bwe\")\n            #TODO add more information in the subirectory names\n        else:\n            self.do_bwe=False\n\n        if self.do_blind_bwe and (\"blind_bwe\" in self.args.tester.modes):\n            self.do_blind_bwe=True\n            mode=\"blind_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"], self.paths[mode+\"degraded_estimate\"]=self.prepare_blind_experiment(\"blind_bwe\",\"masked\",\"blind_bwe\",\"degraded_estimate\")\n            #TODO add more information in the subirectory names\n        if \"real_blind_bwe\" in self.args.tester.modes:\n            self.do_blind_bwe=True\n            mode=\"real_blind_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"real_blind_bwe\",\"degraded\",\"reconstructed\")\n            #TODO add more information in the subirectory names\n\n        if \"formal_test_bwe\" in self.args.tester.modes:\n            self.do_formal_test_bwe=True\n            mode=\"formal_test_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"formal_test_bwe\",\"degraded\",\"reconstructed\")\n        \n        if (\"unconditional\" in self.args.tester.modes):\n            mode=\"unconditional\"\n            self.paths[mode]=self.prepare_unc_experiment(\"unconditional\")\n\n\n        if (\"filter_bwe\" in self.args.tester.modes):\n            mode=\"filter_bwe\"\n            self.paths[mode]=self.prepare_unc_experiment(\"filter_bwe\")\n\n        #self.LTAS_processor=LTAS_processor(self.args.tester.blind_bwe.LTAS.sample_rate,self.args.tester.blind_bwe.LTAS.audio_len)\n        #self.LTAS_processor.load_dataset_LTAS(self.args.tester.blind_bwe.LTAS.path)\n\n    def prepare_unc_experiment(self, str):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n            return path_exp\n\n    def prepare_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n\n            return path_exp, path_degraded, path_original, path_reconstructed\n\n    def resample_audio(self, audio, fs):\n        #this has been reused from the trainer.py\n        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\n    def prepare_blind_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\", str_degraded_estimate=\"degraded_estimate\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n            \n            n=str_degraded_estimate\n            path_degraded_estimate=os.path.join(path_exp, n) #path for the estimated degraded signal\n            #ensure the path exists\n            if not os.path.exists(path_degraded_estimate):\n                os.makedirs(path_degraded_estimate)\n\n            return path_exp, path_degraded, path_original, path_reconstructed, path_degraded_estimate\n\n    def setup_wandb(self):\n        \"\"\"\n        Configure wandb, open a new run and log the configuration.\n        \"\"\"\n        config=omegaconf.OmegaConf.to_container(\n            self.args, resolve=True, throw_on_missing=True\n        )\n        self.wandb_run=wandb.init(project=\"testing\"+self.args.tester.name, entity=self.args.exp.wandb.entity, config=config)\n        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n        self.use_wandb=True\n\n    def setup_wandb_run(self, run):\n        #get the wandb run object from outside (in trainer.py or somewhere else)\n        self.wandb_run=run\n        self.use_wandb=True\n\n    def setup_sampler(self):\n        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network,  diff_params=self.diff_params, args=self.args, rid=True) #rid is used to log some extra information\n\n    \n    def load_latest_checkpoint(self ):\n        #load the latest checkpoint from self.args.model_dir\n        try:\n            # find latest checkpoint_id\n            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n            save_name = f\"{self.args.model_dir}/{save_basename}\"\n            list_weights = glob(save_name)\n            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n            list_ids = [int(id_regex.search(weight_path).groups()[0])\n                        for weight_path in list_weights]\n            checkpoint_id = max(list_ids)\n\n            state_dict = torch.load(\n                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n            self.network.load_state_dict(state_dict['ema'])\n            print(f\"Loaded checkpoint {checkpoint_id}\")\n            return True\n        except (FileNotFoundError, ValueError):\n            raise ValueError(\"No checkpoint found\")\n\n\n    def load_checkpoint(self, path):\n        state_dict = torch.load(path, map_location=self.device)\n        if self.args.exp.exp_name==\"diffwave-sr\":\n            print(state_dict.keys())\n            print(\"noise_schedukar\",state_dict[\"noise_scheduler\"])\n            self.network.load_state_dict(state_dict['ema_model'])\n            self.network.eval()\n            print(\"ckpt loaded\")\n        else:\n            try:\n                print(\"load try 1\")\n                self.network.load_state_dict(state_dict['ema'])\n            except:\n                #self.network.load_state_dict(state_dict['model'])\n                try:\n                    print(\"load try 2\")\n                    dic_ema = {}\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n                        dic_ema[key] = tensor\n                    self.network.load_state_dict(dic_ema)\n                except:\n                    print(\"load try 3\")\n                    dic_ema = {}\n                    i=0\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n                        if tensor.requires_grad:\n                            dic_ema[key]=state_dict['ema_weights'][i]\n                            i=i+1\n                        else:\n                            dic_ema[key]=tensor     \n                    self.network.load_state_dict(dic_ema)\n        try:\n            self.it=state_dict['it']\n        except:\n            self.it=0\n\n    def log_filter(self,preds, f, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n\n        fig_filter=utils_logging.plot_batch_of_lines(preds, f)\n\n        self.wandb_run.log({\"filters_\"+str(string): fig_filter}, step=self.it, commit=True)\n\n    def log_audio(self,preds, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=self.args.model_dir)\n        print(audio_path)\n        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it, commit=False)\n        #TODO: log spectrogram of the audio file to wandb\n        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\n        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it, commit=True)\n\n    def sample_unconditional_diffwavesr(self):\n        #print some parameters of self.network\n        #print(\"self.network\", self.network.input_projection[0].weight)\n        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        rid=False\n        z_1=torch.randn(shape, device=self.device)\n        #print(\"sd\",z_1.std(-1))\n        outputs=self.sampler.diff_params.reverse_process_ddim(z_1, self.network)\n        preds=outputs\n\n        self.log_audio(preds.detach(), \"unconditional\")\n\n        return preds\n    def sample_unconditional(self):\n        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        rid=False\n        outputs=self.sampler.predict_unconditional(shape, self.device, rid=rid)\n        if rid:\n            preds, data_denoised, t=outputs\n            fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"unconditional_signal_generation\")\n        else:\n            preds=outputs\n\n        self.log_audio(preds, \"unconditional\")\n\n        return preds\n\n\n    def formal_test_bwe(self, typefilter=\"firwin\", test_filter_fit=False, compute_sweep=False, blind=False):\n        print(\"BLIND\", blind)\n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n        test_bwe_table_audio = wandb.Table(columns=columns)\n\n        if not self.do_formal_test_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        elif typefilter==\"3rdoct\":\n            type=\"3rdoct\"\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n            da_filter=da_filter.to(self.device)\n        else:\n            type=self.args.tester.bandwidth_extension.filter.type\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n            da_filter=da_filter.to(self.device)\n\n\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\n        path=self.args.tester.formal_test.path\n        filenames=glob(path+\"/*.wav\")\n\n        segL=self.args.exp.audio_len\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n\n        for filename in filenames:\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n            #open audio file\n            d,fs=sf.read(filename)\n            D=torch.Tensor(d).to(self.device).unsqueeze(0)\n            print(\"D\", D.shape, fs)\n\n\n            path_out=self.args.tester.formal_test.folder\n\n            print(\"skippint?\", os.path.join(path_out, basename))\n            if os.path.exists(os.path.join(path_out, basename)):\n                print(\"yes skippint\", os.path.join(path_out, basename))\n                continue\n            print(\"skippint?\", os.path.join(path_out, basename+\".wav\"))\n            if os.path.exists(os.path.join(path_out, basename+\".wav\")):\n                print(\"yes skippint\", os.path.join(path_out, basename+\".wav\"))\n                continue\n\n            if type==\"fc_A\":\n                degraded=self.apply_lowpass_fcA(D, da_filter)\n            else:\n                degraded=self.apply_low_pass(D, da_filter, type)\n            #path_degraded=utils_logging.write_audio_file(degraded, self.args.exp.sample_rate, basename+\".degraded.wav\", path=path_out)\n\n            print(\"filename\",filename)\n\n            #n=os.path.splitext(os.path.basename(filename))[0]+typefilter+str(self.args.tester.bandwidth_extension.filter.fc)\n            n=os.path.splitext(os.path.basename(filename))[0]\n\n            #degraded=degraded.float().to(self.device).unsqueeze(0)\n            print(n)\n            final_pred=torch.zeros_like(degraded)\n    \n            print(\"dsds FS\",fs)\n    \n            print(\"seg shape\",degraded.shape)\n            degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n            print(\"seg shape\",degraded.shape)\n    \n            std= degraded.std(-1)\n    \n            rid=False\n\n    \n            L=degraded.shape[-1]\n            #modify the sampler, so that it is computationally cheaper\n    \n            discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n            discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n    \n            #first segment\n            ix=0\n            seg=degraded[...,ix:ix+segL]\n            #pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n            filter_data=[]\n\n            rid=False\n            if blind:\n                outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                pred, estimated_filter =outputs\n                filter_data.append(((ix, ix+segL), estimated_filter))\n\n            else:\n                pred=self.sampler.predict_bwe(seg, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False)\n    \n            if self.args.tester.formal_test.use_AR:\n                assert not blind\n                previous_pred=pred[..., 0:segL-discard_end]\n                final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\n                ix+=segL-overlap-discard_end\n     \n                y_masked=torch.zeros_like(pred, device=self.device)\n                mask=torch.ones_like(seg, device=self.device)\n                mask[...,overlap::]=0\n            else:\n                print(\"noar\")\n                hann_window=torch.hann_window(self.args.tester.formal_test.OLA*2, device=self.device)\n                win_pred=pred[...,0:segL-discard_end]\n                win_pred[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n                print(\"ix\", ix, \"segL\", segL, \"discard_end\", discard_end, \"win pred shape\", win_pred.shape)\n                final_pred[...,ix:ix+segL-discard_end]=win_pred\n\n                ix+=segL-discard_end-self.args.tester.formal_test.OLA\n    \n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n\n            if blind:\n                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                        pickle.dump(filter_data, f)\n\n            while ix<L-segL-discard_end-discard_start:\n\n                seg=degraded[...,ix:ix+segL]\n                if self.args.tester.formal_test.use_AR:\n                    y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n                    pred=self.sampler.predict_bwe_AR(seg, y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n                else:\n                    if blind:\n                        outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                        pred, estimated_filter =outputs\n                        filter_data.append(((ix, ix+segL), estimated_filter))\n                    else:\n                        pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n    \n                previous_pred_win=pred[..., 0:segL-discard_end]\n                previous_pred_win[..., 0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n                previous_pred_win[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n    \n    \n                final_pred[...,ix:ix+segL-discard_end]+=previous_pred_win\n\n                #do a little bit of overlap and add with a hann window to avoid discontinuities\n                #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n                #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n    \n                path, basename=os.path.split(filename)\n                print(path, basename)\n    \n                path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n    \n                if self.args.tester.formal_test.use_AR:\n\n                    ix+=segL-overlap-discard_end\n                else:\n                    ix+=segL-discard_end-self.args.tester.formal_test.OLA\n\n                if blind:\n                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                        pickle.dump(filter_data, f)\n    \n            #skipping the last segment, which is not complete, I am lazy\n            seg=degraded[...,ix::]\n\n            if self.args.tester.formal_test.use_AR:\n                y_masked[...,0:overlap]=pred[...,-overlap::]\n    \n            if seg.shape[-1]<segL:\n                #cat zeros\n                seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n    \n                if self.args.tester.formal_test.use_AR:\n                    #the cat zeroes will also be part of the observed signal, so I need to mask them\n                    y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n                    mask[...,seg.shape[-1]:segL]=0\n    \n            else:\n                seg_zp=seg[...,0:segL]\n    \n    \n            if self.args.tester.formal_test.use_AR:\n                pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n            else:\n                if blind:\n                    outputs=self.sampler.predict_blind_bwe(seg_zp, rid=False)\n                    pred, estimated_filter =outputs\n                    filter_data.append(((ix, ix+segL), estimated_filter))\n                else:\n                    pred=self.sampler.predict_bwe(seg_zp, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n                \n            \n            if not self.args.tester.formal_test.use_AR:\n                win_pred=pred[...,0:seg.shape[-1]]\n                win_pred[...,0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n                final_pred[...,ix::]+=win_pred\n            else:\n                final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n    \n            #final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n            #final_pred=final_pred*10**(-scale/20)\n    \n            #extract path from filename\n    \n    \n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".wav\", path=path_out)\n            #save filter_data in a pickle file\n            with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                pickle.dump(filter_data, f)\n\n               \n\n\n    def test_bwe(self, typefilter=\"fc_A\", test_filter_fit=False, compute_sweep=False):\n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n        test_bwe_table_audio = wandb.Table(columns=columns)\n\n        if not self.do_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        elif typefilter==\"3rdoct\":\n            type=\"3rdoct\"\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n            da_filter=da_filter.to(self.device)\n        else:\n            type=self.args.tester.bandwidth_extension.filter.type\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n            da_filter=da_filter.to(self.device)\n\n\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n            n=os.path.splitext(filename[0])[0]\n            seg=original.float().to(self.device)\n\n            seg=self.resample_audio(seg, fs)\n\n\n            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n            #        print(\"gain boost\", self.args.tester.bandwidth_extension.gain_boost)\n            #        #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n            #        #add gain boost (in dB)\n            #        seg=seg*10**(self.args.tester.bandwidth_extension.gain_boost/20)\n\n            if type==\"fc_A\":\n                y=self.apply_lowpass_fcA(seg, da_filter)\n            else:\n                y=self.apply_low_pass(seg, da_filter, type)\n            #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type, typefilter) \n\n            #if self.args.tester.bandwidth_extension.sigma_observations != \"None\":\n            #    sigma=self.args.tester.bandwidth_extension.sigma_observations\n            #    y+=sigma*torch.randn(y.shape).to(y.device)\n\n            if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n\n            print(\"y\", y.shape)\n            if test_filter_fit:\n                if compute_sweep:\n                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True, compute_sweep=True)\n                    pred, data_denoised, data_score, t, data_filters, data_norms, data_grads =out\n                    #save the data_norms and data_grads as a .npy file\n                    np.save(self.paths[\"bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                    np.save(self.paths[\"bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                else:\n                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True)\n                    pred, data_denoised, data_score, t, data_filters =out\n            else:\n                rid=True\n                out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=False, compute_sweep=False)\n                \n                pred, data_denoised, data_score, t =out\n\n\n            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n            #    #compensate gain boost\n            #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n            #    #add gain boost (in dB)\n            #    pred=pred*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n            #    seg=seg*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n            #    y=y*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\n            res[i,:]=pred\n       \n            path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"original\"])\n\n            path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"degraded\"])\n\n            path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"reconstructed\"])\n\n            test_bwe_table_audio.add_data(i, \n                    wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                    wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                    wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\n            if rid:\n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n            if test_filter_fit:\n\n                #expecting to crash here\n                print(data_filters.shape)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\n        self.wandb_run.log({\"table_bwe_audio\": test_bwe_table_audio}, commit=True) \n\n        if self.use_wandb:\n            self.log_audio(res, \"bwe\")\n\n    def apply_low_pass(self, seg, filter, typefilter):\n        y=utils_bwe.apply_low_pass(seg, filter, self.args.tester.bandwidth_extension.filter.type) \n        return y\n\n    def apply_lowpass_fcA(self, seg, params):\n        freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n        H=blind_bwe_utils.design_filter(params[0], params[1], freqs)\n        xfilt=blind_bwe_utils.apply_filter(seg,H,self.args.tester.blind_bwe.NFFT)\n        return xfilt\n\n    def prepare_filter(self, sample_rate, typefilter):\n        filter=utils_bwe.prepare_filter(self.args, sample_rate )\n        return filter\n    \n    def test_real_blind_bwe_complete(self, typefilter=\"fc_A\", compute_sweep=False):\n        #raise NotImplementedError\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        \n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        filename=self.args.tester.complete_recording.path\n        d,fs=sf.read(filename)\n        degraded=torch.Tensor(d)\n\n        segL=self.args.exp.audio_len\n\n        ix_first=self.args.exp.sample_rate*self.args.tester.complete_recording.ix_start #index of the first segment to be processed, might have to depend on the sample rate\n\n        #for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\n        print(\"filename\",filename)\n        n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n        degraded=degraded.float().to(self.device).unsqueeze(0)\n        print(n)\n\n        print(\"dsds FS\",fs)\n\n        print(\"seg shape\",degraded.shape)\n        degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n        print(\"seg shape\",degraded.shape)\n\n        std= degraded.std(-1)\n        degraded=self.args.tester.complete_recording.std*degraded/std.unsqueeze(-1)\n        #add noise\n        if self.args.tester.complete_recording.SNR_extra_noise!=\"None\":\n            #contaminate a bit with white noise\n            SNR=10**(self.args.tester.complete_recording.SNR_extra_noise/10)\n            sigma2_s=torch.Tensor([self.args.tester.complete_recording.std**2]).to(degraded.device)\n            sigma=torch.sqrt(sigma2_s/SNR)\n            degraded+=sigma*torch.randn(degraded.shape).to(degraded.device)\n\n\n        if self.args.tester.complete_recording.n_segments_blindstep==1:\n            y=degraded[...,ix_first:ix_first+segL]\n        else:\n            #initialize y with the first segment and repeat it\n            y=degraded[...,ix_first:ix_first+segL].repeat(self.args.tester.complete_recording.n_segments_blindstep,1)\n            for j in range(0, self.args.tester.complete_recording.n_segments_blindstep):\n                #random index\n                ix=np.random.randint(0, degraded.shape[-1]-segL)\n                y[j,...]=degraded[...,ix:ix+segL]\n        \n        print(\"y shape\",y.shape)\n\n            \n\n        #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, fs)\n        #print(\"scale\",scale) #TODO I should calculate this with the whole track, not just the first segment\n\n        #y=y*10**(scale/20)\n        #degraded=degraded*10**(scale/20)\n\n\n\n        rid=False\n        outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n        pred, estimated_filter =outputs\n\n        #now I will just throw away the first segment and process the rest of the signal with the estimated filter. Later I should think of a better way to do it\n\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n        hop=segL-overlap\n\n        final_pred=torch.zeros_like(degraded)\n        final_pred[0, ix_first:ix_first+segL]=pred[0]\n\n        path, basename=os.path.split(filename)\n        print(path, basename)\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n        L=degraded.shape[-1]\n\n        #modify the sampler, so that it is computationally cheaper\n\n        discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n        discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\n        #first segment\n        ix=0\n        seg=degraded[...,ix:ix+segL]\n        pred=self.sampler.predict_bwe(seg, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n        previous_pred=pred[..., 0:segL-discard_end]\n\n        final_pred[...,ix:ix+segL-discard_end]=previous_pred\n        ix+=segL-overlap-discard_end\n\n        y_masked=torch.zeros_like(pred, device=self.device)\n        mask=torch.ones_like(seg, device=self.device)\n        mask[...,overlap::]=0\n\n        hann_window=torch.hann_window(overlap*2, device=self.device)\n\n        while ix<L-segL-discard_end-discard_start:\n            y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n            seg=degraded[...,ix:ix+segL]\n\n            pred=self.sampler.predict_bwe_AR(seg, y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n            previous_pred=pred[..., 0:segL-discard_end]\n\n\n            final_pred[...,ix:ix+segL-discard_end]=previous_pred\n            #do a little bit of overlap and add with a hann window to avoid discontinuities\n            #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n            #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n\n\n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n            ix+=segL-overlap-discard_end\n\n        #skipping the last segment, which is not complete, I am lazy\n        seg=degraded[...,ix::]\n        y_masked[...,0:overlap]=pred[...,-overlap::]\n\n        if seg.shape[-1]<segL:\n            #cat zeros\n            seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\n            #the cat zeroes will also be part of the observed signal, so I need to mask them\n            y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n            mask[...,seg.shape[-1]:segL]=0\n\n        else:\n            seg_zp=seg[...,0:segL]\n\n\n        pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n        final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\n        final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n        #final_pred=final_pred*10**(-scale/20)\n\n        #extract path from filename\n\n\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n               \n               \n               \n    def test_real_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\",\"degraded_audio\", \"reconstructed audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n        \n        \n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        path=self.args.tester.blind_bwe.real_recordings.path\n        audio_files=glob(path+\"/*.wav\")\n        print(audio_files, path)\n        test_set_data=[]\n        test_set_fs=[]\n        test_set_names=[]\n        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n            d,fs=sf.read(audio_files[i])\n            #print(d.shape, self.args.exp.audio_len)\n            #if d.shape[-1] >= self.args.exp.audio_len:\n            #    d=d[...,0:self.args.exp.audio_len]\n            test_set_data.append(torch.Tensor(d))\n            test_set_fs.append(fs)\n            print(\"fs\",fs)\n            print(\"len\",len(d))\n            test_set_names.append(audio_files[i])\n\n        for i, (degraded,  filename, fs) in enumerate(tqdm(zip(test_set_data,  test_set_names, test_set_fs))):\n                print(\"filename\",filename)\n                n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n                seg=degraded.float().to(self.device).unsqueeze(0)\n                print(n)\n\n                print(\"dsds FS\",fs)\n\n                print(\"seg shape\",seg.shape)\n                seg=torchaudio.functional.resample(seg, fs, self.args.exp.sample_rate)\n                print(\"seg shape\",seg.shape)\n                ix_start=self.args.tester.blind_bwe\n\n                seg=seg[...,self.args.exp.sample_rate*0:self.args.exp.sample_rate*0+self.args.exp.audio_len]\n                y=seg\n                print(\"y shape\",y.shape)\n                #normalize???\n                std= y.std(-1)\n                y=self.args.tester.blind_bwe.sigma_norm*y/std.unsqueeze(-1)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y,fs)\n                #print(\"scale\",scale)\n                #y=y*10**(scale/20)\n\n\n\n               \n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n                pred=pred*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                y=y*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"reconstructed\"])\n               \n               \n               \n                fig_est_filter=blind_bwe_utils.plot_filter(estimated_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"real_blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n               \n                if typefilter==\"fc_A\":\n                    test_blind_bwe_table_filters.add_data(i, \n                        wandb.Html(path_est_filter),\n                    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"real_blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"real_blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    def test_blind_bwe(self, typefilter=\"firwin\", compute_sweep=False, blind=True):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            fc=self.args.tester.blind_bwe.test_filter.fc\n            A=self.args.tester.blind_bwe.test_filter.A\n            da_filter=torch.Tensor([fc, A]).to(self.device)\n        else:\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate, typefilter) #standardly designed filter\n            da_filter=da_filter.to(self.device)\n        \n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\n        path=self.args.tester.blind_bwe.real_recordings.path\n        audio_files=glob(path+\"/*.wav\")\n        print(audio_files, path)\n        test_set_data=[]\n        test_set_fs=[]\n        test_set_names=[]\n        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n            d,fs=sf.read(audio_files[i])\n            #print(d.shape, self.args.exp.audio_len)\n            #if d.shape[-1] >= self.args.exp.audio_len:\n            #    d=d[...,0:self.args.exp.audio_len]\n            test_set_data.append(torch.Tensor(d))\n            test_set_fs.append(fs)\n            print(\"fs\",fs)\n            print(\"len\",len(d))\n            test_set_names.append(audio_files[i])\n\n        for i,( original,  filename, fs) in enumerate(tqdm(zip(test_set_data,  test_set_names, test_set_fs))):\n        #for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n                n=os.path.basename(filename)+typefilter\n                print(\"n\",n, filename)\n                seg=original.float().to(self.device).unsqueeze(0)\n                #seg=self.resample_audio(seg, fs)\n                seg=seg[...,self.args.exp.sample_rate*0:self.args.exp.sample_rate*0+self.args.exp.audio_len]\n\n                #sigma_norm=0.07 #hardcoded\n                #orig_std=seg.std(-1)\n                #seg=sigma_norm*seg/orig_std\n\n                #if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                #    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                #    orig_std=seg.std(-1)\n                #    seg=sigma_norm*seg/orig_std\n        \n                #elif self.args.tester.blind_bwe.gain_boost != 0:\n                #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                #    #add gain boost (in dB)\n                #    seg=seg*10**(self.args.tester.blind_bwe.gain_boost/20)\n\n\n                #apply lowpass filter\n                print(seg.shape)\n                if typefilter==\"fc_A\":\n                    y=self.apply_lowpass_fcA(seg, da_filter)\n                else:\n                    y=self.apply_low_pass(seg,da_filter, typefilter)\n\n                #add noise to the observations for regularization\n                if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, self.args.exp.sample_rate)\n                #print(\"applied scale\",scale)\n                #y=y*10**(scale/20)\n\n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                if blind:\n                    rid=True\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n                else:\n                    rid=False\n                    pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=rid, test_filter_fit=False, compute_sweep=False)\n               \n                if blind:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"blind_bwe\"]+\"data_t\"+str(i)+\".npy\", t.cpu().numpy())\n                        np.save(self.paths[\"blind_bwe\"]+\"data_denoised\"+str(i)+\".npy\", data_denoised)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_filters\"+str(i)+\".npy\", data_filters)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pass\n               \n               \n               \n                #y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\n                #if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                #    assert orig_std is not None\n                #seg=orig_std*seg/sigma_norm\n                #pred=orig_std*pred/sigma_norm\n                #elif self.args.tester.blind_bwe.gain_boost != 0:\n                #    #compensate gain boost #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.  #    #add gain boost (in dB) #    y_est=y_est*10**(-self.args.tester.blind_bwe.gain_boost/20) #    pred=pred*10**(-self.args.tester.blind_bwe.gain_boost/20) ##    seg=seg*10**(-self.args.tester.blind_bwe.gain_boost/20) #    y=y*10**(-self.args.tester.blind_bwe.gain_boost/20) \n                #y_est=y_est*10**(-scale/20)\n               # pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n               \n                \n                path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n               \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"reconstructed\"])\n               \n                #path_degrade_estimate=utils_logging.write_audio_file(y_est, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded_estimate\"])\n               \n               \n                #will probably crash here!\n                #fig_est_filter=blind_bwe_utils.plot_filter(da_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                #path_est_filter=os.path.join(self.paths[\"blind_bwe\"], str(i)+\"_raw_filter.html\")\n                #fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                #test_blind_bwe_table_audio.add_data(i, \n                #        wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                #        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                #        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate),\n                #        wandb.Audio(path_degrade_estimate, sample_rate=self.args.exp.sample_rate))\n               \n                #if typefilter==\"fc_A\":\n                #    test_blind_bwe_table_filters.add_data(i, \n                #        wandb.Html(path_est_filter),\n                #    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                #print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                #fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                #print(data_filters.shape)\n                #will crash here\n                #fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        #self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        #self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    \n        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n    def dodajob(self):\n        #self.setup_wandb()\n        for m in self.args.tester.modes:\n\n            if m==\"unconditional\":\n                print(\"testing unconditional\")\n                self.sample_unconditional()\n            if m==\"unconditional_diffwavesr\":\n                print(\"testing unconditional\")\n                self.sample_unconditional_diffwavesr()\n            self.it+=1\n            if m==\"blind_bwe\":\n                print(\"TESTING BLIND BWE\")\n                self.test_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep, typefilter=\"firwin\")\n            if m==\"real_blind_bwe\":\n                print(\"TESTING REAL BLIND BWE\")\n                self.test_real_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe_complete\":\n                #process the whole audio file\n                #Estimate the filter in the first chunk, and then apply it to the rest of the audio file (using a little bit of overlap or outpainting)\n                print(\"TESTING REAL BLIND BWE COMPLETE\")\n                self.test_real_blind_bwe_complete(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep)\n            if m==\"formal_test_bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.formal_test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"firwin\", blind=self.args.tester.formal_test.blind)\n        self.it+=1", ""]}
{"filename": "testing/blind_bwe_tester.py", "chunked_list": ["from datetime import date\nimport pickle\nimport re\nimport torch\nimport torchaudio\n#from src.models.unet_cqt import Unet_CQT\n#from src.models.unet_stft import Unet_STFT\n#from src.models.unet_1d import Unet_1d\n#import src.utils.setup as utils_setup\n#from src.sde import  VE_Sde_Elucidating", "#import src.utils.setup as utils_setup\n#from src.sde import  VE_Sde_Elucidating\nimport numpy as np\nimport utils.dnnlib as dnnlib\nimport os\n\nimport utils.logging as utils_logging\nimport wandb\nimport copy\n", "import copy\n\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport utils.bandwidth_extension as utils_bwe\nimport omegaconf\n\n#import utils.filter_generation_utils as f_utils\nimport utils.blind_bwe_utils as blind_bwe_utils", "#import utils.filter_generation_utils as f_utils\nimport utils.blind_bwe_utils as blind_bwe_utils\nimport utils.training_utils as t_utils\n\nimport soundfile as sf\n\n#from utils.spectral_analysis import LTAS_processor\n\n\nclass BlindTester():\n    def __init__(\n        self, args=None, network=None, diff_params=None, test_set=None, device=None, it=None\n    ):\n        self.args=args\n        self.network=torch.compile(network)\n        #self.network=network\n        #prnt number of parameters\n        \n\n        self.diff_params=copy.copy(diff_params)\n        self.device=device\n        #choose gpu as the device if possible\n        if self.device is None:\n            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.network=network\n\n        torch.backends.cudnn.benchmark = True\n\n        today=date.today() \n        if it is None:\n            self.it=0\n\n        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n        if not os.path.exists(self.path_sampling):\n            os.makedirs(self.path_sampling)\n\n\n        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n        self.setup_sampler()\n\n        self.use_wandb=False #hardcoded for now\n\n        S=self.args.exp.resample_factor\n        if S>2.1 and S<2.2:\n            #resampling 48k to 22.05k\n            self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n        elif S!=1:\n            N=int(self.args.exp.audio_len*S)\n            self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\n        if test_set is not None:\n            self.test_set=test_set\n            self.do_inpainting=True\n            self.do_bwe=True\n            self.do_blind_bwe=True\n        else:\n            self.test_set=None\n            self.do_inpainting=False\n            self.do_bwe=False #these need to be set up in the config file\n            self.do_blind_bwe=False\n\n        self.paths={}\n        if self.do_inpainting and (\"inpainting\" in self.args.tester.modes):\n            self.do_inpainting=True\n            mode=\"inpainting\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"inpainting\",\"masked\",\"inpainted\")\n            #TODO add more information in the subirectory names\n        else: self.do_inpainting=False\n\n        if self.do_bwe and (\"bwe\" in self.args.tester.modes):\n            self.do_bwe=True\n            mode=\"bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"bwe\",\"lowpassed\",\"bwe\")\n            #TODO add more information in the subirectory names\n        else:\n            self.do_bwe=False\n\n        if self.do_blind_bwe and (\"blind_bwe\" in self.args.tester.modes):\n            self.do_blind_bwe=True\n            mode=\"blind_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"], self.paths[mode+\"degraded_estimate\"]=self.prepare_blind_experiment(\"blind_bwe\",\"masked\",\"blind_bwe\",\"degraded_estimate\")\n            #TODO add more information in the subirectory names\n        if \"real_blind_bwe\" in self.args.tester.modes:\n            self.do_blind_bwe=True\n            mode=\"real_blind_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"real_blind_bwe\",\"degraded\",\"reconstructed\")\n            #TODO add more information in the subirectory names\n\n        if \"formal_test_bwe\" in self.args.tester.modes:\n            self.do_formal_test_bwe=True\n            mode=\"formal_test_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"formal_test_bwe\",\"degraded\",\"reconstructed\")\n        \n        if (\"unconditional\" in self.args.tester.modes):\n            mode=\"unconditional\"\n            self.paths[mode]=self.prepare_unc_experiment(\"unconditional\")\n\n\n        if (\"filter_bwe\" in self.args.tester.modes):\n            mode=\"filter_bwe\"\n            self.paths[mode]=self.prepare_unc_experiment(\"filter_bwe\")\n\n        #self.LTAS_processor=LTAS_processor(self.args.tester.blind_bwe.LTAS.sample_rate,self.args.tester.blind_bwe.LTAS.audio_len)\n        #self.LTAS_processor.load_dataset_LTAS(self.args.tester.blind_bwe.LTAS.path)\n\n    def prepare_unc_experiment(self, str):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n            return path_exp\n\n    def prepare_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n\n            return path_exp, path_degraded, path_original, path_reconstructed\n\n    def resample_audio(self, audio, fs):\n        #this has been reused from the trainer.py\n        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\n    def prepare_blind_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\", str_degraded_estimate=\"degraded_estimate\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n            \n            n=str_degraded_estimate\n            path_degraded_estimate=os.path.join(path_exp, n) #path for the estimated degraded signal\n            #ensure the path exists\n            if not os.path.exists(path_degraded_estimate):\n                os.makedirs(path_degraded_estimate)\n\n            return path_exp, path_degraded, path_original, path_reconstructed, path_degraded_estimate\n\n    def setup_wandb(self):\n        \"\"\"\n        Configure wandb, open a new run and log the configuration.\n        \"\"\"\n        config=omegaconf.OmegaConf.to_container(\n            self.args, resolve=True, throw_on_missing=True\n        )\n        self.wandb_run=wandb.init(project=\"testing\"+self.args.tester.name, entity=self.args.exp.wandb.entity, config=config)\n        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n        self.use_wandb=True\n\n    def setup_wandb_run(self, run):\n        #get the wandb run object from outside (in trainer.py or somewhere else)\n        self.wandb_run=run\n        self.use_wandb=True\n\n    def setup_sampler(self):\n        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network,  diff_params=self.diff_params, args=self.args, rid=True) #rid is used to log some extra information\n\n    \n    def load_latest_checkpoint(self ):\n        #load the latest checkpoint from self.args.model_dir\n        try:\n            # find latest checkpoint_id\n            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n            save_name = f\"{self.args.model_dir}/{save_basename}\"\n            list_weights = glob(save_name)\n            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n            list_ids = [int(id_regex.search(weight_path).groups()[0])\n                        for weight_path in list_weights]\n            checkpoint_id = max(list_ids)\n\n            state_dict = torch.load(\n                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n            self.network.load_state_dict(state_dict['ema'])\n            print(f\"Loaded checkpoint {checkpoint_id}\")\n            return True\n        except (FileNotFoundError, ValueError):\n            raise ValueError(\"No checkpoint found\")\n\n\n    def load_checkpoint(self, path):\n        state_dict = torch.load(path, map_location=self.device)\n        if self.args.exp.exp_name==\"diffwave-sr\":\n            print(state_dict.keys())\n            print(\"noise_schedukar\",state_dict[\"noise_scheduler\"])\n            self.network.load_state_dict(state_dict['ema_model'])\n            self.network.eval()\n            print(\"ckpt loaded\")\n        else:\n            try:\n                print(\"load try 1\")\n                self.network.load_state_dict(state_dict['ema'])\n            except:\n                #self.network.load_state_dict(state_dict['model'])\n                try:\n                    print(\"load try 2\")\n                    dic_ema = {}\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n                        dic_ema[key] = tensor\n                    self.network.load_state_dict(dic_ema)\n                except:\n                    print(\"load try 3\")\n                    dic_ema = {}\n                    i=0\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n                        if tensor.requires_grad:\n                            dic_ema[key]=state_dict['ema_weights'][i]\n                            i=i+1\n                        else:\n                            dic_ema[key]=tensor     \n                    self.network.load_state_dict(dic_ema)\n        try:\n            self.it=state_dict['it']\n        except:\n            self.it=0\n\n    def log_filter(self,preds, f, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n\n        fig_filter=utils_logging.plot_batch_of_lines(preds, f)\n\n        self.wandb_run.log({\"filters_\"+str(string): fig_filter}, step=self.it, commit=True)\n\n    def log_audio(self,preds, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=self.args.model_dir)\n        print(audio_path)\n        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it, commit=False)\n        #TODO: log spectrogram of the audio file to wandb\n        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\n        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it, commit=True)\n\n    def sample_unconditional_diffwavesr(self):\n        #print some parameters of self.network\n        #print(\"self.network\", self.network.input_projection[0].weight)\n        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        rid=False\n        z_1=torch.randn(shape, device=self.device)\n        #print(\"sd\",z_1.std(-1))\n        outputs=self.sampler.diff_params.reverse_process_ddim(z_1, self.network)\n        preds=outputs\n\n        self.log_audio(preds.detach(), \"unconditional\")\n\n        return preds\n    def sample_unconditional(self):\n        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        rid=False\n        outputs=self.sampler.predict_unconditional(shape, self.device, rid=rid)\n        if rid:\n            preds, data_denoised, t=outputs\n            fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"unconditional_signal_generation\")\n        else:\n            preds=outputs\n\n        self.log_audio(preds, \"unconditional\")\n\n        return preds\n\n\n    def formal_test_bwe(self, typefilter=\"firwin\", test_filter_fit=False, compute_sweep=False, blind=False, robustness=False):\n        print(\"BLIND\", blind)\n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n        test_bwe_table_audio = wandb.Table(columns=columns)\n\n        if not self.do_formal_test_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        elif typefilter==\"3rdoct\":\n            type=\"3rdoct\"\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n            da_filter=da_filter.to(self.device)\n        else:\n            type=self.args.tester.bandwidth_extension.filter.type\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n            da_filter=da_filter.to(self.device)\n\n\n        if robustness:\n            order=self.args.tester.formal_test.robustness_filter.order\n            fc=self.args.tester.formal_test.robustness_filter.fc\n            beta=self.args.tester.formal_test.robustness_filter.beta\n            da_other_filter=utils_bwe.get_FIR_lowpass(order,fc, beta,self.args.exp.sample_rate)\n\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\n        path=self.args.tester.formal_test.path\n        filenames=glob(path+\"/*.wav\")\n\n        segL=self.args.exp.audio_len\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n\n        for filename in filenames:\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n            #open audio file\n            d,fs=sf.read(filename)\n            D=torch.Tensor(d).to(self.device).unsqueeze(0)\n            print(\"D\", D.shape, fs)\n\n\n            path_out=self.args.tester.formal_test.folder\n\n            print(\"skippint?\", os.path.join(path_out, basename))\n            if os.path.exists(os.path.join(path_out, basename)):\n                print(\"yes skippint\", os.path.join(path_out, basename))\n                continue\n            print(\"skippint?\", os.path.join(path_out, basename+\".wav\"))\n\n            if os.path.exists(os.path.join(path_out, basename+\".wav\")):\n                print(\"yes skippint\", os.path.join(path_out, basename+\".wav\"))\n                continue\n\n            if not(robustness):\n                if type==\"fc_A\":\n                    degraded=self.apply_lowpass_fcA(D, da_filter)\n                else:\n                    degraded=self.apply_low_pass(D, da_filter, type)\n            else:\n                print(\"test robustness, using a different filter\")\n                degraded=self.apply_low_pass(D, da_other_filter, type)\n\n            #path_degraded=utils_logging.write_audio_file(degraded, self.args.exp.sample_rate, basename+\".degraded.wav\", path=path_out)\n\n            print(\"filename\",filename)\n\n            #n=os.path.splitext(os.path.basename(filename))[0]+typefilter+str(self.args.tester.bandwidth_extension.filter.fc)\n            n=os.path.splitext(os.path.basename(filename))[0]\n\n            #degraded=degraded.float().to(self.device).unsqueeze(0)\n            print(n)\n            final_pred=torch.zeros_like(degraded)\n    \n            print(\"dsds FS\",fs)\n    \n            print(\"seg shape\",degraded.shape)\n            degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n            print(\"seg shape\",degraded.shape)\n    \n            std= degraded.std(-1)\n    \n            rid=False\n\n    \n            L=degraded.shape[-1]\n            #modify the sampler, so that it is computationally cheaper\n    \n            discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n            discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n    \n            #first segment\n            ix=0\n            seg=degraded[...,ix:ix+segL]\n            #pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n            filter_data=[]\n\n            rid=False\n            if blind:\n                outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                pred, estimated_filter =outputs\n                filter_data.append(((ix, ix+segL), estimated_filter))\n\n            else:\n                if robustness:\n                        pred=self.sampler.predict_bwe(seg, da_other_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n                else:\n                        pred=self.sampler.predict_bwe(seg, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n    \n            if self.args.tester.formal_test.use_AR:\n                assert not blind\n                previous_pred=pred[..., 0:segL-discard_end]\n                final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\n                ix+=segL-overlap-discard_end\n     \n                y_masked=torch.zeros_like(pred, device=self.device)\n                mask=torch.ones_like(seg, device=self.device)\n                mask[...,overlap::]=0\n            else:\n                print(\"noar\")\n                hann_window=torch.hann_window(self.args.tester.formal_test.OLA*2, device=self.device)\n                win_pred=pred[...,0:segL-discard_end]\n                win_pred[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n                print(\"ix\", ix, \"segL\", segL, \"discard_end\", discard_end, \"win pred shape\", win_pred.shape)\n                final_pred[...,ix:ix+segL-discard_end]=win_pred\n\n                ix+=segL-discard_end-self.args.tester.formal_test.OLA\n    \n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n\n            if blind:\n                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                        pickle.dump(filter_data, f)\n\n            while ix<L-segL-discard_end-discard_start:\n\n                seg=degraded[...,ix:ix+segL]\n                if self.args.tester.formal_test.use_AR:\n                    y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n                    pred=self.sampler.predict_bwe_AR(seg, y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n                else:\n                    if blind:\n                        outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                        pred, estimated_filter =outputs\n                        filter_data.append(((ix, ix+segL), estimated_filter))\n                    \n                    else:\n                        if robustness:\n                                pred=self.sampler.predict_bwe(seg, da_other_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n                        else:\n                                pred=self.sampler.predict_bwe(seg, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n                #if blind:\n                #        outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                #        pred, estimated_filter =outputs\n                #        filter_data.append(((ix, ix+segL), estimated_filter))\n                #    else:\n                #        pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n    \n                previous_pred_win=pred[..., 0:segL-discard_end]\n                previous_pred_win[..., 0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n                previous_pred_win[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n    \n    \n                final_pred[...,ix:ix+segL-discard_end]+=previous_pred_win\n\n                #do a little bit of overlap and add with a hann window to avoid discontinuities\n                #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n                #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n    \n                path, basename=os.path.split(filename)\n                print(path, basename)\n    \n                path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n    \n                if self.args.tester.formal_test.use_AR:\n\n                    ix+=segL-overlap-discard_end\n                else:\n                    ix+=segL-discard_end-self.args.tester.formal_test.OLA\n\n                if blind:\n                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                        pickle.dump(filter_data, f)\n    \n            #skipping the last segment, which is not complete, I am lazy\n            seg=degraded[...,ix::]\n\n            if self.args.tester.formal_test.use_AR:\n                y_masked[...,0:overlap]=pred[...,-overlap::]\n    \n            if seg.shape[-1]<segL:\n                #cat zeros\n                seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n    \n                if self.args.tester.formal_test.use_AR:\n                    #the cat zeroes will also be part of the observed signal, so I need to mask them\n                    y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n                    mask[...,seg.shape[-1]:segL]=0\n    \n            else:\n                seg_zp=seg[...,0:segL]\n    \n    \n            if self.args.tester.formal_test.use_AR:\n                pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n            else:\n                if blind:\n                        outputs=self.sampler.predict_blind_bwe(seg_zp, rid=False)\n                        pred, estimated_filter =outputs\n                        filter_data.append(((ix, ix+segL), estimated_filter))\n                    \n                else:\n                        if robustness:\n                                pred=self.sampler.predict_bwe(seg_zp, da_other_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n                        else:\n                                pred=self.sampler.predict_bwe(seg_zp, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n\n                #if blind:\n                #    outputs=self.sampler.predict_blind_bwe(seg_zp, rid=False)\n                #    pred, estimated_filter =outputs\n                #    filter_data.append(((ix, ix+segL), estimated_filter))\n                #else:\n                #    pred=self.sampler.predict_bwe(seg_zp, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n                \n            \n            if not self.args.tester.formal_test.use_AR:\n                win_pred=pred[...,0:seg.shape[-1]]\n                win_pred[...,0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n                final_pred[...,ix::]+=win_pred\n            else:\n                final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n    \n            #final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n            #final_pred=final_pred*10**(-scale/20)\n    \n            #extract path from filename\n    \n    \n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".wav\", path=path_out)\n            #save filter_data in a pickle file\n            with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                pickle.dump(filter_data, f)\n\n               \n\n\n    def test_bwe(self, typefilter=\"fc_A\", test_filter_fit=False, compute_sweep=False):\n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n        test_bwe_table_audio = wandb.Table(columns=columns)\n\n        if not self.do_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        elif typefilter==\"3rdoct\":\n            type=\"3rdoct\"\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n            da_filter=da_filter.to(self.device)\n        else:\n            type=self.args.tester.bandwidth_extension.filter.type\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n            da_filter=da_filter.to(self.device)\n\n\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n            n=os.path.splitext(filename[0])[0]\n            seg=original.float().to(self.device)\n\n            seg=self.resample_audio(seg, fs)\n\n\n            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n            #        print(\"gain boost\", self.args.tester.bandwidth_extension.gain_boost)\n            #        #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n            #        #add gain boost (in dB)\n            #        seg=seg*10**(self.args.tester.bandwidth_extension.gain_boost/20)\n\n            if type==\"fc_A\":\n                y=self.apply_lowpass_fcA(seg, da_filter)\n            else:\n                y=self.apply_low_pass(seg, da_filter, type)\n            #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type, typefilter) \n\n            #if self.args.tester.bandwidth_extension.sigma_observations != \"None\":\n            #    sigma=self.args.tester.bandwidth_extension.sigma_observations\n            #    y+=sigma*torch.randn(y.shape).to(y.device)\n\n            if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n\n            print(\"y\", y.shape)\n            if test_filter_fit:\n                if compute_sweep:\n                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True, compute_sweep=True)\n                    pred, data_denoised, data_score, t, data_filters, data_norms, data_grads =out\n                    #save the data_norms and data_grads as a .npy file\n                    np.save(self.paths[\"bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                    np.save(self.paths[\"bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                else:\n                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True)\n                    pred, data_denoised, data_score, t, data_filters =out\n            else:\n                rid=True\n                out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=False, compute_sweep=False)\n                \n                pred, data_denoised, data_score, t =out\n\n\n            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n            #    #compensate gain boost\n            #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n            #    #add gain boost (in dB)\n            #    pred=pred*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n            #    seg=seg*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n            #    y=y*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\n            res[i,:]=pred\n       \n            path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"original\"])\n\n            path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"degraded\"])\n\n            path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"reconstructed\"])\n\n            test_bwe_table_audio.add_data(i, \n                    wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                    wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                    wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\n            if rid:\n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n            if test_filter_fit:\n\n                #expecting to crash here\n                print(data_filters.shape)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\n        self.wandb_run.log({\"table_bwe_audio\": test_bwe_table_audio}, commit=True) \n\n        if self.use_wandb:\n            self.log_audio(res, \"bwe\")\n\n    def apply_low_pass(self, seg, filter, typefilter):\n        y=utils_bwe.apply_low_pass(seg, filter, self.args.tester.bandwidth_extension.filter.type) \n        return y\n\n    def apply_lowpass_fcA(self, seg, params):\n        freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n        H=blind_bwe_utils.design_filter(params[0], params[1], freqs)\n        xfilt=blind_bwe_utils.apply_filter(seg,H,self.args.tester.blind_bwe.NFFT)\n        return xfilt\n\n    def prepare_filter(self, sample_rate, typefilter):\n        filter=utils_bwe.prepare_filter(self.args, sample_rate )\n        return filter\n    \n    def test_real_blind_bwe_complete(self, typefilter=\"fc_A\", compute_sweep=False):\n        #raise NotImplementedError\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        \n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        filename=self.args.tester.complete_recording.path\n        d,fs=sf.read(filename)\n        degraded=torch.Tensor(d)\n\n        segL=self.args.exp.audio_len\n\n        ix_first=self.args.exp.sample_rate*self.args.tester.complete_recording.ix_start #index of the first segment to be processed, might have to depend on the sample rate\n\n        #for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\n        print(\"filename\",filename)\n        n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n        degraded=degraded.float().to(self.device).unsqueeze(0)\n        print(n)\n\n        print(\"dsds FS\",fs)\n\n        print(\"seg shape\",degraded.shape)\n        degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n        print(\"seg shape\",degraded.shape)\n\n        std= degraded.std(-1)\n        degraded=self.args.tester.complete_recording.std*degraded/std.unsqueeze(-1)\n        #add noise\n        if self.args.tester.complete_recording.SNR_extra_noise!=\"None\":\n            #contaminate a bit with white noise\n            SNR=10**(self.args.tester.complete_recording.SNR_extra_noise/10)\n            sigma2_s=torch.Tensor([self.args.tester.complete_recording.std**2]).to(degraded.device)\n            sigma=torch.sqrt(sigma2_s/SNR)\n            degraded+=sigma*torch.randn(degraded.shape).to(degraded.device)\n\n\n        if self.args.tester.complete_recording.n_segments_blindstep==1:\n            y=degraded[...,ix_first:ix_first+segL]\n        else:\n            #initialize y with the first segment and repeat it\n            y=degraded[...,ix_first:ix_first+segL].repeat(self.args.tester.complete_recording.n_segments_blindstep,1)\n            for j in range(0, self.args.tester.complete_recording.n_segments_blindstep):\n                #random index\n                ix=np.random.randint(0, degraded.shape[-1]-segL)\n                y[j,...]=degraded[...,ix:ix+segL]\n        \n        print(\"y shape\",y.shape)\n\n            \n\n        #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, fs)\n        #print(\"scale\",scale) #TODO I should calculate this with the whole track, not just the first segment\n\n        #y=y*10**(scale/20)\n        #degraded=degraded*10**(scale/20)\n\n\n\n        rid=False\n        outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n        pred, estimated_filter =outputs\n\n        #now I will just throw away the first segment and process the rest of the signal with the estimated filter. Later I should think of a better way to do it\n\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n        hop=segL-overlap\n\n        final_pred=torch.zeros_like(degraded)\n        final_pred[0, ix_first:ix_first+segL]=pred[0]\n\n        path, basename=os.path.split(filename)\n        print(path, basename)\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n        L=degraded.shape[-1]\n\n        #modify the sampler, so that it is computationally cheaper\n\n        discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n        discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\n        #first segment\n        ix=0\n        seg=degraded[...,ix:ix+segL]\n        pred=self.sampler.predict_bwe(seg, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n        previous_pred=pred[..., 0:segL-discard_end]\n\n        final_pred[...,ix:ix+segL-discard_end]=previous_pred\n        ix+=segL-overlap-discard_end\n\n        y_masked=torch.zeros_like(pred, device=self.device)\n        mask=torch.ones_like(seg, device=self.device)\n        mask[...,overlap::]=0\n\n        hann_window=torch.hann_window(overlap*2, device=self.device)\n\n        while ix<L-segL-discard_end-discard_start:\n            y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n            seg=degraded[...,ix:ix+segL]\n\n            pred=self.sampler.predict_bwe_AR(seg, y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n            previous_pred=pred[..., 0:segL-discard_end]\n\n\n            final_pred[...,ix:ix+segL-discard_end]=previous_pred\n            #do a little bit of overlap and add with a hann window to avoid discontinuities\n            #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n            #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n\n\n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n            ix+=segL-overlap-discard_end\n\n        #skipping the last segment, which is not complete, I am lazy\n        seg=degraded[...,ix::]\n        y_masked[...,0:overlap]=pred[...,-overlap::]\n\n        if seg.shape[-1]<segL:\n            #cat zeros\n            seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\n            #the cat zeroes will also be part of the observed signal, so I need to mask them\n            y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n            mask[...,seg.shape[-1]:segL]=0\n\n        else:\n            seg_zp=seg[...,0:segL]\n\n\n        pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n        final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\n        final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n        #final_pred=final_pred*10**(-scale/20)\n\n        #extract path from filename\n\n\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n               \n               \n               \n    def test_real_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\",\"degraded_audio\", \"reconstructed audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n        \n        \n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        path=self.args.tester.blind_bwe.real_recordings.path\n        audio_files=glob(path+\"/*.wav\")\n        print(audio_files, path)\n        test_set_data=[]\n        test_set_fs=[]\n        test_set_names=[]\n        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n            d,fs=sf.read(audio_files[i])\n            #print(d.shape, self.args.exp.audio_len)\n            #if d.shape[-1] >= self.args.exp.audio_len:\n            #    d=d[...,0:self.args.exp.audio_len]\n            test_set_data.append(torch.Tensor(d))\n            test_set_fs.append(fs)\n            print(\"fs\",fs)\n            print(\"len\",len(d))\n            test_set_names.append(audio_files[i])\n\n        for i, (degraded,  filename, fs) in enumerate(tqdm(zip(test_set_data,  test_set_names, test_set_fs))):\n                print(\"filename\",filename)\n                n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n                seg=degraded.float().to(self.device).unsqueeze(0)\n                print(n)\n\n                print(\"dsds FS\",fs)\n\n                print(\"seg shape\",seg.shape)\n                seg=torchaudio.functional.resample(seg, fs, self.args.exp.sample_rate)\n                print(\"seg shape\",seg.shape)\n                ix_start=self.args.tester.blind_bwe\n\n                seg=seg[...,self.args.exp.sample_rate*0:self.args.exp.sample_rate*0+self.args.exp.audio_len]\n                y=seg\n                print(\"y shape\",y.shape)\n                #normalize???\n                std= y.std(-1)\n                y=self.args.tester.blind_bwe.sigma_norm*y/std.unsqueeze(-1)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y,fs)\n                #print(\"scale\",scale)\n                #y=y*10**(scale/20)\n\n\n\n               \n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n                pred=pred*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                y=y*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"reconstructed\"])\n               \n               \n               \n                fig_est_filter=blind_bwe_utils.plot_filter(estimated_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"real_blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n               \n                if typefilter==\"fc_A\":\n                    test_blind_bwe_table_filters.add_data(i, \n                        wandb.Html(path_est_filter),\n                    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"real_blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"real_blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    def test_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            fc=self.args.tester.blind_bwe.test_filter.fc\n            A=self.args.tester.blind_bwe.test_filter.A\n            da_filter=torch.Tensor([fc, A]).to(self.device)\n        else:\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate, typefilter) #standardly designed filter\n            da_filter=da_filter.to(self.device)\n        \n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n                n=os.path.splitext(filename[0])[0]+typefilter\n                seg=original.float().to(self.device)\n                seg=self.resample_audio(seg, fs)\n\n                if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                    orig_std=seg.std(-1)\n                    seg=sigma_norm*seg/orig_std\n        \n                elif self.args.tester.blind_bwe.gain_boost != 0:\n                    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                    #add gain boost (in dB)\n                    seg=seg*10**(self.args.tester.blind_bwe.gain_boost/20)\n                \n\n                #apply lowpass filter\n                if typefilter==\"fc_A\":\n                    y=self.apply_lowpass_fcA(seg, da_filter)\n                else:\n                    y=self.apply_low_pass(seg,da_filter, typefilter)\n\n                #add noise to the observations for regularization\n                if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, self.args.exp.sample_rate)\n                #print(\"applied scale\",scale)\n                #y=y*10**(scale/20)\n\n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"blind_bwe\"]+\"data_t\"+str(i)+\".npy\", t.cpu().numpy())\n                        np.save(self.paths[\"blind_bwe\"]+\"data_denoised\"+str(i)+\".npy\", data_denoised)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_filters\"+str(i)+\".npy\", data_filters)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n               \n                y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\n                if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                    assert orig_std is not None\n                    seg=orig_std*seg/sigma_norm\n                elif self.args.tester.blind_bwe.gain_boost != 0:\n                    #compensate gain boost\n                    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                    #add gain boost (in dB)\n                    y_est=y_est*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                    pred=pred*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                    seg=seg*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                    y=y*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n               \n                \n                path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n               \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"reconstructed\"])\n               \n                path_degrade_estimate=utils_logging.write_audio_file(y_est, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded_estimate\"])\n               \n               \n                #will probably crash here!\n                fig_est_filter=blind_bwe_utils.plot_filter(da_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_degrade_estimate, sample_rate=self.args.exp.sample_rate))\n               \n                #if typefilter==\"fc_A\":\n                #    test_blind_bwe_table_filters.add_data(i, \n                #        wandb.Html(path_est_filter),\n                #    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters.shape)\n                #will crash here\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    \n        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n    def dodajob(self):\n        #self.setup_wandb()\n        for m in self.args.tester.modes:\n\n            if m==\"unconditional\":\n                print(\"testing unconditional\")\n                self.sample_unconditional()\n            if m==\"unconditional_diffwavesr\":\n                print(\"testing unconditional\")\n                self.sample_unconditional_diffwavesr()\n            self.it+=1\n            if m==\"blind_bwe\":\n                print(\"TESTING BLIND BWE\")\n                self.test_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe\":\n                print(\"TESTING REAL BLIND BWE\")\n                self.test_real_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe_complete\":\n                #process the whole audio file\n                #Estimate the filter in the first chunk, and then apply it to the rest of the audio file (using a little bit of overlap or outpainting)\n                print(\"TESTING REAL BLIND BWE COMPLETE\")\n                self.test_real_blind_bwe_complete(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep)\n            if m==\"formal_test_bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.formal_test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"firwin\", blind=self.args.tester.formal_test.blind, robustness=self.args.tester.formal_test.robustness)\n        self.it+=1", "\nclass BlindTester():\n    def __init__(\n        self, args=None, network=None, diff_params=None, test_set=None, device=None, it=None\n    ):\n        self.args=args\n        self.network=torch.compile(network)\n        #self.network=network\n        #prnt number of parameters\n        \n\n        self.diff_params=copy.copy(diff_params)\n        self.device=device\n        #choose gpu as the device if possible\n        if self.device is None:\n            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.network=network\n\n        torch.backends.cudnn.benchmark = True\n\n        today=date.today() \n        if it is None:\n            self.it=0\n\n        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n        if not os.path.exists(self.path_sampling):\n            os.makedirs(self.path_sampling)\n\n\n        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n        self.setup_sampler()\n\n        self.use_wandb=False #hardcoded for now\n\n        S=self.args.exp.resample_factor\n        if S>2.1 and S<2.2:\n            #resampling 48k to 22.05k\n            self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n        elif S!=1:\n            N=int(self.args.exp.audio_len*S)\n            self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\n        if test_set is not None:\n            self.test_set=test_set\n            self.do_inpainting=True\n            self.do_bwe=True\n            self.do_blind_bwe=True\n        else:\n            self.test_set=None\n            self.do_inpainting=False\n            self.do_bwe=False #these need to be set up in the config file\n            self.do_blind_bwe=False\n\n        self.paths={}\n        if self.do_inpainting and (\"inpainting\" in self.args.tester.modes):\n            self.do_inpainting=True\n            mode=\"inpainting\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"inpainting\",\"masked\",\"inpainted\")\n            #TODO add more information in the subirectory names\n        else: self.do_inpainting=False\n\n        if self.do_bwe and (\"bwe\" in self.args.tester.modes):\n            self.do_bwe=True\n            mode=\"bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"bwe\",\"lowpassed\",\"bwe\")\n            #TODO add more information in the subirectory names\n        else:\n            self.do_bwe=False\n\n        if self.do_blind_bwe and (\"blind_bwe\" in self.args.tester.modes):\n            self.do_blind_bwe=True\n            mode=\"blind_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"], self.paths[mode+\"degraded_estimate\"]=self.prepare_blind_experiment(\"blind_bwe\",\"masked\",\"blind_bwe\",\"degraded_estimate\")\n            #TODO add more information in the subirectory names\n        if \"real_blind_bwe\" in self.args.tester.modes:\n            self.do_blind_bwe=True\n            mode=\"real_blind_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"real_blind_bwe\",\"degraded\",\"reconstructed\")\n            #TODO add more information in the subirectory names\n\n        if \"formal_test_bwe\" in self.args.tester.modes:\n            self.do_formal_test_bwe=True\n            mode=\"formal_test_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"formal_test_bwe\",\"degraded\",\"reconstructed\")\n        \n        if (\"unconditional\" in self.args.tester.modes):\n            mode=\"unconditional\"\n            self.paths[mode]=self.prepare_unc_experiment(\"unconditional\")\n\n\n        if (\"filter_bwe\" in self.args.tester.modes):\n            mode=\"filter_bwe\"\n            self.paths[mode]=self.prepare_unc_experiment(\"filter_bwe\")\n\n        #self.LTAS_processor=LTAS_processor(self.args.tester.blind_bwe.LTAS.sample_rate,self.args.tester.blind_bwe.LTAS.audio_len)\n        #self.LTAS_processor.load_dataset_LTAS(self.args.tester.blind_bwe.LTAS.path)\n\n    def prepare_unc_experiment(self, str):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n            return path_exp\n\n    def prepare_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n\n            return path_exp, path_degraded, path_original, path_reconstructed\n\n    def resample_audio(self, audio, fs):\n        #this has been reused from the trainer.py\n        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\n    def prepare_blind_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\", str_degraded_estimate=\"degraded_estimate\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n            \n            n=str_degraded_estimate\n            path_degraded_estimate=os.path.join(path_exp, n) #path for the estimated degraded signal\n            #ensure the path exists\n            if not os.path.exists(path_degraded_estimate):\n                os.makedirs(path_degraded_estimate)\n\n            return path_exp, path_degraded, path_original, path_reconstructed, path_degraded_estimate\n\n    def setup_wandb(self):\n        \"\"\"\n        Configure wandb, open a new run and log the configuration.\n        \"\"\"\n        config=omegaconf.OmegaConf.to_container(\n            self.args, resolve=True, throw_on_missing=True\n        )\n        self.wandb_run=wandb.init(project=\"testing\"+self.args.tester.name, entity=self.args.exp.wandb.entity, config=config)\n        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n        self.use_wandb=True\n\n    def setup_wandb_run(self, run):\n        #get the wandb run object from outside (in trainer.py or somewhere else)\n        self.wandb_run=run\n        self.use_wandb=True\n\n    def setup_sampler(self):\n        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network,  diff_params=self.diff_params, args=self.args, rid=True) #rid is used to log some extra information\n\n    \n    def load_latest_checkpoint(self ):\n        #load the latest checkpoint from self.args.model_dir\n        try:\n            # find latest checkpoint_id\n            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n            save_name = f\"{self.args.model_dir}/{save_basename}\"\n            list_weights = glob(save_name)\n            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n            list_ids = [int(id_regex.search(weight_path).groups()[0])\n                        for weight_path in list_weights]\n            checkpoint_id = max(list_ids)\n\n            state_dict = torch.load(\n                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n            self.network.load_state_dict(state_dict['ema'])\n            print(f\"Loaded checkpoint {checkpoint_id}\")\n            return True\n        except (FileNotFoundError, ValueError):\n            raise ValueError(\"No checkpoint found\")\n\n\n    def load_checkpoint(self, path):\n        state_dict = torch.load(path, map_location=self.device)\n        if self.args.exp.exp_name==\"diffwave-sr\":\n            print(state_dict.keys())\n            print(\"noise_schedukar\",state_dict[\"noise_scheduler\"])\n            self.network.load_state_dict(state_dict['ema_model'])\n            self.network.eval()\n            print(\"ckpt loaded\")\n        else:\n            try:\n                print(\"load try 1\")\n                self.network.load_state_dict(state_dict['ema'])\n            except:\n                #self.network.load_state_dict(state_dict['model'])\n                try:\n                    print(\"load try 2\")\n                    dic_ema = {}\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n                        dic_ema[key] = tensor\n                    self.network.load_state_dict(dic_ema)\n                except:\n                    print(\"load try 3\")\n                    dic_ema = {}\n                    i=0\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n                        if tensor.requires_grad:\n                            dic_ema[key]=state_dict['ema_weights'][i]\n                            i=i+1\n                        else:\n                            dic_ema[key]=tensor     \n                    self.network.load_state_dict(dic_ema)\n        try:\n            self.it=state_dict['it']\n        except:\n            self.it=0\n\n    def log_filter(self,preds, f, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n\n        fig_filter=utils_logging.plot_batch_of_lines(preds, f)\n\n        self.wandb_run.log({\"filters_\"+str(string): fig_filter}, step=self.it, commit=True)\n\n    def log_audio(self,preds, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=self.args.model_dir)\n        print(audio_path)\n        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it, commit=False)\n        #TODO: log spectrogram of the audio file to wandb\n        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\n        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it, commit=True)\n\n    def sample_unconditional_diffwavesr(self):\n        #print some parameters of self.network\n        #print(\"self.network\", self.network.input_projection[0].weight)\n        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        rid=False\n        z_1=torch.randn(shape, device=self.device)\n        #print(\"sd\",z_1.std(-1))\n        outputs=self.sampler.diff_params.reverse_process_ddim(z_1, self.network)\n        preds=outputs\n\n        self.log_audio(preds.detach(), \"unconditional\")\n\n        return preds\n    def sample_unconditional(self):\n        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        rid=False\n        outputs=self.sampler.predict_unconditional(shape, self.device, rid=rid)\n        if rid:\n            preds, data_denoised, t=outputs\n            fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"unconditional_signal_generation\")\n        else:\n            preds=outputs\n\n        self.log_audio(preds, \"unconditional\")\n\n        return preds\n\n\n    def formal_test_bwe(self, typefilter=\"firwin\", test_filter_fit=False, compute_sweep=False, blind=False, robustness=False):\n        print(\"BLIND\", blind)\n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n        test_bwe_table_audio = wandb.Table(columns=columns)\n\n        if not self.do_formal_test_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        elif typefilter==\"3rdoct\":\n            type=\"3rdoct\"\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n            da_filter=da_filter.to(self.device)\n        else:\n            type=self.args.tester.bandwidth_extension.filter.type\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n            da_filter=da_filter.to(self.device)\n\n\n        if robustness:\n            order=self.args.tester.formal_test.robustness_filter.order\n            fc=self.args.tester.formal_test.robustness_filter.fc\n            beta=self.args.tester.formal_test.robustness_filter.beta\n            da_other_filter=utils_bwe.get_FIR_lowpass(order,fc, beta,self.args.exp.sample_rate)\n\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\n        path=self.args.tester.formal_test.path\n        filenames=glob(path+\"/*.wav\")\n\n        segL=self.args.exp.audio_len\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n\n        for filename in filenames:\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n            #open audio file\n            d,fs=sf.read(filename)\n            D=torch.Tensor(d).to(self.device).unsqueeze(0)\n            print(\"D\", D.shape, fs)\n\n\n            path_out=self.args.tester.formal_test.folder\n\n            print(\"skippint?\", os.path.join(path_out, basename))\n            if os.path.exists(os.path.join(path_out, basename)):\n                print(\"yes skippint\", os.path.join(path_out, basename))\n                continue\n            print(\"skippint?\", os.path.join(path_out, basename+\".wav\"))\n\n            if os.path.exists(os.path.join(path_out, basename+\".wav\")):\n                print(\"yes skippint\", os.path.join(path_out, basename+\".wav\"))\n                continue\n\n            if not(robustness):\n                if type==\"fc_A\":\n                    degraded=self.apply_lowpass_fcA(D, da_filter)\n                else:\n                    degraded=self.apply_low_pass(D, da_filter, type)\n            else:\n                print(\"test robustness, using a different filter\")\n                degraded=self.apply_low_pass(D, da_other_filter, type)\n\n            #path_degraded=utils_logging.write_audio_file(degraded, self.args.exp.sample_rate, basename+\".degraded.wav\", path=path_out)\n\n            print(\"filename\",filename)\n\n            #n=os.path.splitext(os.path.basename(filename))[0]+typefilter+str(self.args.tester.bandwidth_extension.filter.fc)\n            n=os.path.splitext(os.path.basename(filename))[0]\n\n            #degraded=degraded.float().to(self.device).unsqueeze(0)\n            print(n)\n            final_pred=torch.zeros_like(degraded)\n    \n            print(\"dsds FS\",fs)\n    \n            print(\"seg shape\",degraded.shape)\n            degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n            print(\"seg shape\",degraded.shape)\n    \n            std= degraded.std(-1)\n    \n            rid=False\n\n    \n            L=degraded.shape[-1]\n            #modify the sampler, so that it is computationally cheaper\n    \n            discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n            discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n    \n            #first segment\n            ix=0\n            seg=degraded[...,ix:ix+segL]\n            #pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n            filter_data=[]\n\n            rid=False\n            if blind:\n                outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                pred, estimated_filter =outputs\n                filter_data.append(((ix, ix+segL), estimated_filter))\n\n            else:\n                if robustness:\n                        pred=self.sampler.predict_bwe(seg, da_other_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n                else:\n                        pred=self.sampler.predict_bwe(seg, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n    \n            if self.args.tester.formal_test.use_AR:\n                assert not blind\n                previous_pred=pred[..., 0:segL-discard_end]\n                final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\n                ix+=segL-overlap-discard_end\n     \n                y_masked=torch.zeros_like(pred, device=self.device)\n                mask=torch.ones_like(seg, device=self.device)\n                mask[...,overlap::]=0\n            else:\n                print(\"noar\")\n                hann_window=torch.hann_window(self.args.tester.formal_test.OLA*2, device=self.device)\n                win_pred=pred[...,0:segL-discard_end]\n                win_pred[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n                print(\"ix\", ix, \"segL\", segL, \"discard_end\", discard_end, \"win pred shape\", win_pred.shape)\n                final_pred[...,ix:ix+segL-discard_end]=win_pred\n\n                ix+=segL-discard_end-self.args.tester.formal_test.OLA\n    \n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n\n            if blind:\n                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                        pickle.dump(filter_data, f)\n\n            while ix<L-segL-discard_end-discard_start:\n\n                seg=degraded[...,ix:ix+segL]\n                if self.args.tester.formal_test.use_AR:\n                    y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n                    pred=self.sampler.predict_bwe_AR(seg, y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n                else:\n                    if blind:\n                        outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                        pred, estimated_filter =outputs\n                        filter_data.append(((ix, ix+segL), estimated_filter))\n                    \n                    else:\n                        if robustness:\n                                pred=self.sampler.predict_bwe(seg, da_other_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n                        else:\n                                pred=self.sampler.predict_bwe(seg, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n                #if blind:\n                #        outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                #        pred, estimated_filter =outputs\n                #        filter_data.append(((ix, ix+segL), estimated_filter))\n                #    else:\n                #        pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n    \n                previous_pred_win=pred[..., 0:segL-discard_end]\n                previous_pred_win[..., 0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n                previous_pred_win[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n    \n    \n                final_pred[...,ix:ix+segL-discard_end]+=previous_pred_win\n\n                #do a little bit of overlap and add with a hann window to avoid discontinuities\n                #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n                #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n    \n                path, basename=os.path.split(filename)\n                print(path, basename)\n    \n                path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n    \n                if self.args.tester.formal_test.use_AR:\n\n                    ix+=segL-overlap-discard_end\n                else:\n                    ix+=segL-discard_end-self.args.tester.formal_test.OLA\n\n                if blind:\n                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                        pickle.dump(filter_data, f)\n    \n            #skipping the last segment, which is not complete, I am lazy\n            seg=degraded[...,ix::]\n\n            if self.args.tester.formal_test.use_AR:\n                y_masked[...,0:overlap]=pred[...,-overlap::]\n    \n            if seg.shape[-1]<segL:\n                #cat zeros\n                seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n    \n                if self.args.tester.formal_test.use_AR:\n                    #the cat zeroes will also be part of the observed signal, so I need to mask them\n                    y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n                    mask[...,seg.shape[-1]:segL]=0\n    \n            else:\n                seg_zp=seg[...,0:segL]\n    \n    \n            if self.args.tester.formal_test.use_AR:\n                pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n            else:\n                if blind:\n                        outputs=self.sampler.predict_blind_bwe(seg_zp, rid=False)\n                        pred, estimated_filter =outputs\n                        filter_data.append(((ix, ix+segL), estimated_filter))\n                    \n                else:\n                        if robustness:\n                                pred=self.sampler.predict_bwe(seg_zp, da_other_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n                        else:\n                                pred=self.sampler.predict_bwe(seg_zp, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False) \n\n                #if blind:\n                #    outputs=self.sampler.predict_blind_bwe(seg_zp, rid=False)\n                #    pred, estimated_filter =outputs\n                #    filter_data.append(((ix, ix+segL), estimated_filter))\n                #else:\n                #    pred=self.sampler.predict_bwe(seg_zp, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n                \n            \n            if not self.args.tester.formal_test.use_AR:\n                win_pred=pred[...,0:seg.shape[-1]]\n                win_pred[...,0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n                final_pred[...,ix::]+=win_pred\n            else:\n                final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n    \n            #final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n            #final_pred=final_pred*10**(-scale/20)\n    \n            #extract path from filename\n    \n    \n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".wav\", path=path_out)\n            #save filter_data in a pickle file\n            with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                pickle.dump(filter_data, f)\n\n               \n\n\n    def test_bwe(self, typefilter=\"fc_A\", test_filter_fit=False, compute_sweep=False):\n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n        test_bwe_table_audio = wandb.Table(columns=columns)\n\n        if not self.do_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        elif typefilter==\"3rdoct\":\n            type=\"3rdoct\"\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n            da_filter=da_filter.to(self.device)\n        else:\n            type=self.args.tester.bandwidth_extension.filter.type\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n            da_filter=da_filter.to(self.device)\n\n\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n            n=os.path.splitext(filename[0])[0]\n            seg=original.float().to(self.device)\n\n            seg=self.resample_audio(seg, fs)\n\n\n            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n            #        print(\"gain boost\", self.args.tester.bandwidth_extension.gain_boost)\n            #        #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n            #        #add gain boost (in dB)\n            #        seg=seg*10**(self.args.tester.bandwidth_extension.gain_boost/20)\n\n            if type==\"fc_A\":\n                y=self.apply_lowpass_fcA(seg, da_filter)\n            else:\n                y=self.apply_low_pass(seg, da_filter, type)\n            #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type, typefilter) \n\n            #if self.args.tester.bandwidth_extension.sigma_observations != \"None\":\n            #    sigma=self.args.tester.bandwidth_extension.sigma_observations\n            #    y+=sigma*torch.randn(y.shape).to(y.device)\n\n            if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n\n            print(\"y\", y.shape)\n            if test_filter_fit:\n                if compute_sweep:\n                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True, compute_sweep=True)\n                    pred, data_denoised, data_score, t, data_filters, data_norms, data_grads =out\n                    #save the data_norms and data_grads as a .npy file\n                    np.save(self.paths[\"bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                    np.save(self.paths[\"bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                else:\n                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True)\n                    pred, data_denoised, data_score, t, data_filters =out\n            else:\n                rid=True\n                out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=False, compute_sweep=False)\n                \n                pred, data_denoised, data_score, t =out\n\n\n            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n            #    #compensate gain boost\n            #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n            #    #add gain boost (in dB)\n            #    pred=pred*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n            #    seg=seg*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n            #    y=y*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\n            res[i,:]=pred\n       \n            path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"original\"])\n\n            path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"degraded\"])\n\n            path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"reconstructed\"])\n\n            test_bwe_table_audio.add_data(i, \n                    wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                    wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                    wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\n            if rid:\n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n            if test_filter_fit:\n\n                #expecting to crash here\n                print(data_filters.shape)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\n        self.wandb_run.log({\"table_bwe_audio\": test_bwe_table_audio}, commit=True) \n\n        if self.use_wandb:\n            self.log_audio(res, \"bwe\")\n\n    def apply_low_pass(self, seg, filter, typefilter):\n        y=utils_bwe.apply_low_pass(seg, filter, self.args.tester.bandwidth_extension.filter.type) \n        return y\n\n    def apply_lowpass_fcA(self, seg, params):\n        freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n        H=blind_bwe_utils.design_filter(params[0], params[1], freqs)\n        xfilt=blind_bwe_utils.apply_filter(seg,H,self.args.tester.blind_bwe.NFFT)\n        return xfilt\n\n    def prepare_filter(self, sample_rate, typefilter):\n        filter=utils_bwe.prepare_filter(self.args, sample_rate )\n        return filter\n    \n    def test_real_blind_bwe_complete(self, typefilter=\"fc_A\", compute_sweep=False):\n        #raise NotImplementedError\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        \n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        filename=self.args.tester.complete_recording.path\n        d,fs=sf.read(filename)\n        degraded=torch.Tensor(d)\n\n        segL=self.args.exp.audio_len\n\n        ix_first=self.args.exp.sample_rate*self.args.tester.complete_recording.ix_start #index of the first segment to be processed, might have to depend on the sample rate\n\n        #for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\n        print(\"filename\",filename)\n        n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n        degraded=degraded.float().to(self.device).unsqueeze(0)\n        print(n)\n\n        print(\"dsds FS\",fs)\n\n        print(\"seg shape\",degraded.shape)\n        degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n        print(\"seg shape\",degraded.shape)\n\n        std= degraded.std(-1)\n        degraded=self.args.tester.complete_recording.std*degraded/std.unsqueeze(-1)\n        #add noise\n        if self.args.tester.complete_recording.SNR_extra_noise!=\"None\":\n            #contaminate a bit with white noise\n            SNR=10**(self.args.tester.complete_recording.SNR_extra_noise/10)\n            sigma2_s=torch.Tensor([self.args.tester.complete_recording.std**2]).to(degraded.device)\n            sigma=torch.sqrt(sigma2_s/SNR)\n            degraded+=sigma*torch.randn(degraded.shape).to(degraded.device)\n\n\n        if self.args.tester.complete_recording.n_segments_blindstep==1:\n            y=degraded[...,ix_first:ix_first+segL]\n        else:\n            #initialize y with the first segment and repeat it\n            y=degraded[...,ix_first:ix_first+segL].repeat(self.args.tester.complete_recording.n_segments_blindstep,1)\n            for j in range(0, self.args.tester.complete_recording.n_segments_blindstep):\n                #random index\n                ix=np.random.randint(0, degraded.shape[-1]-segL)\n                y[j,...]=degraded[...,ix:ix+segL]\n        \n        print(\"y shape\",y.shape)\n\n            \n\n        #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, fs)\n        #print(\"scale\",scale) #TODO I should calculate this with the whole track, not just the first segment\n\n        #y=y*10**(scale/20)\n        #degraded=degraded*10**(scale/20)\n\n\n\n        rid=False\n        outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n        pred, estimated_filter =outputs\n\n        #now I will just throw away the first segment and process the rest of the signal with the estimated filter. Later I should think of a better way to do it\n\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n        hop=segL-overlap\n\n        final_pred=torch.zeros_like(degraded)\n        final_pred[0, ix_first:ix_first+segL]=pred[0]\n\n        path, basename=os.path.split(filename)\n        print(path, basename)\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n        L=degraded.shape[-1]\n\n        #modify the sampler, so that it is computationally cheaper\n\n        discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n        discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\n        #first segment\n        ix=0\n        seg=degraded[...,ix:ix+segL]\n        pred=self.sampler.predict_bwe(seg, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n        previous_pred=pred[..., 0:segL-discard_end]\n\n        final_pred[...,ix:ix+segL-discard_end]=previous_pred\n        ix+=segL-overlap-discard_end\n\n        y_masked=torch.zeros_like(pred, device=self.device)\n        mask=torch.ones_like(seg, device=self.device)\n        mask[...,overlap::]=0\n\n        hann_window=torch.hann_window(overlap*2, device=self.device)\n\n        while ix<L-segL-discard_end-discard_start:\n            y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n            seg=degraded[...,ix:ix+segL]\n\n            pred=self.sampler.predict_bwe_AR(seg, y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n            previous_pred=pred[..., 0:segL-discard_end]\n\n\n            final_pred[...,ix:ix+segL-discard_end]=previous_pred\n            #do a little bit of overlap and add with a hann window to avoid discontinuities\n            #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n            #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n\n\n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n            ix+=segL-overlap-discard_end\n\n        #skipping the last segment, which is not complete, I am lazy\n        seg=degraded[...,ix::]\n        y_masked[...,0:overlap]=pred[...,-overlap::]\n\n        if seg.shape[-1]<segL:\n            #cat zeros\n            seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\n            #the cat zeroes will also be part of the observed signal, so I need to mask them\n            y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n            mask[...,seg.shape[-1]:segL]=0\n\n        else:\n            seg_zp=seg[...,0:segL]\n\n\n        pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n        final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\n        final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n        #final_pred=final_pred*10**(-scale/20)\n\n        #extract path from filename\n\n\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n               \n               \n               \n    def test_real_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\",\"degraded_audio\", \"reconstructed audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n        \n        \n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        path=self.args.tester.blind_bwe.real_recordings.path\n        audio_files=glob(path+\"/*.wav\")\n        print(audio_files, path)\n        test_set_data=[]\n        test_set_fs=[]\n        test_set_names=[]\n        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n            d,fs=sf.read(audio_files[i])\n            #print(d.shape, self.args.exp.audio_len)\n            #if d.shape[-1] >= self.args.exp.audio_len:\n            #    d=d[...,0:self.args.exp.audio_len]\n            test_set_data.append(torch.Tensor(d))\n            test_set_fs.append(fs)\n            print(\"fs\",fs)\n            print(\"len\",len(d))\n            test_set_names.append(audio_files[i])\n\n        for i, (degraded,  filename, fs) in enumerate(tqdm(zip(test_set_data,  test_set_names, test_set_fs))):\n                print(\"filename\",filename)\n                n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n                seg=degraded.float().to(self.device).unsqueeze(0)\n                print(n)\n\n                print(\"dsds FS\",fs)\n\n                print(\"seg shape\",seg.shape)\n                seg=torchaudio.functional.resample(seg, fs, self.args.exp.sample_rate)\n                print(\"seg shape\",seg.shape)\n                ix_start=self.args.tester.blind_bwe\n\n                seg=seg[...,self.args.exp.sample_rate*0:self.args.exp.sample_rate*0+self.args.exp.audio_len]\n                y=seg\n                print(\"y shape\",y.shape)\n                #normalize???\n                std= y.std(-1)\n                y=self.args.tester.blind_bwe.sigma_norm*y/std.unsqueeze(-1)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y,fs)\n                #print(\"scale\",scale)\n                #y=y*10**(scale/20)\n\n\n\n               \n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n                pred=pred*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                y=y*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"reconstructed\"])\n               \n               \n               \n                fig_est_filter=blind_bwe_utils.plot_filter(estimated_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"real_blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n               \n                if typefilter==\"fc_A\":\n                    test_blind_bwe_table_filters.add_data(i, \n                        wandb.Html(path_est_filter),\n                    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"real_blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"real_blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    def test_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            fc=self.args.tester.blind_bwe.test_filter.fc\n            A=self.args.tester.blind_bwe.test_filter.A\n            da_filter=torch.Tensor([fc, A]).to(self.device)\n        else:\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate, typefilter) #standardly designed filter\n            da_filter=da_filter.to(self.device)\n        \n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n                n=os.path.splitext(filename[0])[0]+typefilter\n                seg=original.float().to(self.device)\n                seg=self.resample_audio(seg, fs)\n\n                if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                    orig_std=seg.std(-1)\n                    seg=sigma_norm*seg/orig_std\n        \n                elif self.args.tester.blind_bwe.gain_boost != 0:\n                    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                    #add gain boost (in dB)\n                    seg=seg*10**(self.args.tester.blind_bwe.gain_boost/20)\n                \n\n                #apply lowpass filter\n                if typefilter==\"fc_A\":\n                    y=self.apply_lowpass_fcA(seg, da_filter)\n                else:\n                    y=self.apply_low_pass(seg,da_filter, typefilter)\n\n                #add noise to the observations for regularization\n                if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, self.args.exp.sample_rate)\n                #print(\"applied scale\",scale)\n                #y=y*10**(scale/20)\n\n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"blind_bwe\"]+\"data_t\"+str(i)+\".npy\", t.cpu().numpy())\n                        np.save(self.paths[\"blind_bwe\"]+\"data_denoised\"+str(i)+\".npy\", data_denoised)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_filters\"+str(i)+\".npy\", data_filters)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n               \n                y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\n                if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                    assert orig_std is not None\n                    seg=orig_std*seg/sigma_norm\n                elif self.args.tester.blind_bwe.gain_boost != 0:\n                    #compensate gain boost\n                    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                    #add gain boost (in dB)\n                    y_est=y_est*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                    pred=pred*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                    seg=seg*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                    y=y*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n               \n                \n                path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n               \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"reconstructed\"])\n               \n                path_degrade_estimate=utils_logging.write_audio_file(y_est, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded_estimate\"])\n               \n               \n                #will probably crash here!\n                fig_est_filter=blind_bwe_utils.plot_filter(da_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_degrade_estimate, sample_rate=self.args.exp.sample_rate))\n               \n                #if typefilter==\"fc_A\":\n                #    test_blind_bwe_table_filters.add_data(i, \n                #        wandb.Html(path_est_filter),\n                #    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters.shape)\n                #will crash here\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    \n        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n    def dodajob(self):\n        #self.setup_wandb()\n        for m in self.args.tester.modes:\n\n            if m==\"unconditional\":\n                print(\"testing unconditional\")\n                self.sample_unconditional()\n            if m==\"unconditional_diffwavesr\":\n                print(\"testing unconditional\")\n                self.sample_unconditional_diffwavesr()\n            self.it+=1\n            if m==\"blind_bwe\":\n                print(\"TESTING BLIND BWE\")\n                self.test_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe\":\n                print(\"TESTING REAL BLIND BWE\")\n                self.test_real_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe_complete\":\n                #process the whole audio file\n                #Estimate the filter in the first chunk, and then apply it to the rest of the audio file (using a little bit of overlap or outpainting)\n                print(\"TESTING REAL BLIND BWE COMPLETE\")\n                self.test_real_blind_bwe_complete(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep)\n            if m==\"formal_test_bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.formal_test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"firwin\", blind=self.args.tester.formal_test.blind, robustness=self.args.tester.formal_test.robustness)\n        self.it+=1", ""]}
{"filename": "testing/edm_sampler.py", "chunked_list": ["from tqdm import tqdm\nimport torch\nimport torchaudio\n#import scipy.signal\n#import numpy as np\n\n\nclass Sampler():\n\n    def __init__(self, model, diff_params, args, rid=False):\n\n        self.model = model\n        self.diff_params = diff_params #same as training, useful if we need to apply a wrapper or something\n        self.args=args\n        if not(self.args.tester.diff_params.same_as_training):\n            self.update_diff_params()\n\n\n        self.order=self.args.tester.order\n        self.xi=self.args.tester.posterior_sampling.xi\n         #hyperparameter for the reconstruction guidance\n        self.data_consistency=self.args.tester.posterior_sampling.data_consistency #use reconstruction gudance without replacement\n        self.nb_steps=self.args.tester.T\n\n        #self.treshold_on_grads=args.tester.inference.max_thresh_grads\n        self.rid=rid #this is for logging, ignore for now\n\n        #try:\n        #    self.stereo=self.args.tester.stereo\n        #except:\n        #    self.stereo=False\n\n\n    def update_diff_params(self):\n        #the parameters for testing might not be necesarily the same as the ones used for training\n        self.diff_params.sigma_min=self.args.tester.diff_params.sigma_min\n        self.diff_params.sigma_max =self.args.tester.diff_params.sigma_max\n        self.diff_params.ro=self.args.tester.diff_params.ro\n        self.diff_params.sigma_data=self.args.tester.diff_params.sigma_data\n        #par.diff_params.meters stochastic sampling\n        self.diff_params.Schurn=self.args.tester.diff_params.Schurn\n        self.diff_params.Stmin=self.args.tester.diff_params.Stmin\n        self.diff_params.Stmax=self.args.tester.diff_params.Stmax\n        self.diff_params.Snoise=self.args.tester.diff_params.Snoise\n\n\n    def data_consistency_step(self, x_hat, y, degradation):\n        \"\"\"\n        Simple replacement method, used for inpainting and FIR bwe\n        \"\"\"\n        #get reconstruction estimate\n        den_rec= degradation(x_hat)     \n        #apply replacment (valid for linear degradations)\n        return y+x_hat-den_rec \n\n    def get_score_rec_guidance(self, x, y, t_i, degradation):\n\n        x.requires_grad_()\n        x_hat=self.diff_params.denoiser(x, self.model, t_i.unsqueeze(-1))\n\n        if self.args.tester.filter_out_cqt_DC_Nyq:\n            x_hat=self.model.CQTransform.apply_hpf_DC(x_hat)\n\n        den_rec= degradation(x_hat) \n\n        if len(y.shape)==3:\n            dim=(1,2)\n        elif len(y.shape)==2:\n            dim=1\n\n        norm=torch.linalg.norm(y-den_rec,dim=dim, ord=2)\n        \n        rec_grads=torch.autograd.grad(outputs=norm,\n                                      inputs=x)\n\n        rec_grads=rec_grads[0]\n        \n        normguide=torch.linalg.norm(rec_grads)/self.args.exp.audio_len**0.5\n        \n        #normalize scaling\n        s=self.xi/(normguide*t_i+1e-6)\n        \n        #optionally apply a treshold to the gradients\n        if False:\n            #pply tresholding to the gradients. It is a dirty trick but helps avoiding bad artifacts \n            rec_grads=torch.clip(rec_grads, min=-self.treshold_on_grads, max=self.treshold_on_grads)\n        \n\n        score=(x_hat.detach()-x)/t_i**2\n\n        #apply scaled guidance to the score\n        score=score-s*rec_grads\n\n        return score\n\n    def get_score(self,x, y, t_i, degradation):\n        if y==None:\n            assert degradation==None\n            #unconditional sampling\n            with torch.no_grad():\n                #print(\"In sampling\", x.shape, t_i.shape)\n                x_hat=self.diff_params.denoiser(x, self.model, t_i.unsqueeze(-1))\n                if self.args.tester.filter_out_cqt_DC_Nyq:\n                    x_hat=self.model.CQTransform.apply_hpf_DC(x_hat)\n                score=(x_hat-x)/t_i**2\n            return score\n        else:\n            if self.xi>0:\n                #apply rec. guidance\n                score=self.get_score_rec_guidance(x, y, t_i, degradation)\n    \n                #optionally apply replacement or consistency step\n                if self.data_consistency:\n                    #convert score to denoised estimate using Tweedie's formula\n                    x_hat=score*t_i**2+x\n    \n                    if self.args.inference.mode==\"phase_retrieval\":\n                        x_hat=self.data_consistency_step_phase_retrieval(x_hat,y)\n                    else:\n                        x_hat=self.data_consistency_step(x_hat,y, degradation)\n    \n                    #convert back to score\n                    score=(x_hat-x)/t_i**2\n    \n            else:\n                #denoised with replacement method\n                with torch.no_grad():\n                    x_hat=self.diff_params.denoiser(x, self.model, t_i.unsqueeze(-1))\n                        \n                    x_hat=self.data_consistency_step(x_hat,y, degradation)\n        \n                    score=(x_hat-x)/t_i**2\n    \n            return score\n\n    def predict_unconditional(\n        self,\n        shape,  #observations (lowpssed signal) Tensor with shape ??\n        device\n    ):\n        self.y=None\n        self.degradation=None\n        return self.predict(shape, device)\n\n    def predict_resample(\n        self,\n        y,  #observations (lowpssed signal) Tensor with shape ??\n        shape,\n        degradation, #lambda function\n    ):\n        self.degradation=degradation \n        self.y=y\n        print(shape)\n        return self.predict(shape, y.device)\n\n\n    def predict_conditional(\n        self,\n        y,  #observations (lowpssed signal) Tensor with shape ??\n        degradation, #lambda function\n    ):\n        self.degradation=degradation \n        self.y=y\n        return self.predict(y.shape, y.device)\n\n    def predict(\n        self,\n        shape,  #observations (lowpssed signal) Tensor with shape ??\n        device, #lambda function\n    ):\n\n        if self.rid:\n            data_denoised=torch.zeros((self.nb_steps,shape[0], shape[1]))\n\n        #get the noise schedule\n        t = self.diff_params.create_schedule(self.nb_steps).to(device)\n        #sample from gaussian distribution with sigma_max variance\n        x = self.diff_params.sample_prior(shape,t[0]).to(device)\n\n        #parameter for langevin stochasticity, if Schurn is 0, gamma will be 0 to, so the sampler will be deterministic\n        gamma=self.diff_params.get_gamma(t).to(device)\n\n\n        for i in tqdm(range(0, self.nb_steps, 1)):\n            #print(\"sampling step \",i,\" from \",self.nb_steps)\n\n            if gamma[i]==0:\n                #deterministic sampling, do nothing\n                t_hat=t[i] \n                x_hat=x\n            else:\n                #stochastic sampling\n                #move timestep\n                t_hat=t[i]+gamma[i]*t[i] \n                #sample noise, Snoise is 1 by default\n                epsilon=torch.randn(shape).to(device)*self.diff_params.Snoise\n                #add extra noise\n                x_hat=x+((t_hat**2 - t[i]**2)**(1/2))*epsilon \n\n            score=self.get_score(x_hat, self.y, t_hat, self.degradation)    \n\n            #d=-t_hat*((denoised-x_hat)/t_hat**2)\n            d=-t_hat*score\n            \n            #apply second order correction\n            h=t[i+1]-t_hat\n\n\n            if t[i+1]!=0 and self.order==2:  #always except last step\n                #second order correction2\n                #h=t[i+1]-t_hat\n                t_prime=t[i+1]\n                x_prime=x_hat+h*d\n                score=self.get_score(x_prime, self.y, t_prime, self.degradation)\n\n                d_prime=-t_prime*score\n\n                x=(x_hat+h*((1/2)*d +(1/2)*d_prime))\n\n            elif t[i+1]==0 or self.order==1: #first condition  is to avoid dividing by 0\n                #first order Euler step\n                x=x_hat+h*d\n\n            if self.rid: data_denoised[i]=x\n            \n        if self.rid:\n            return x.detach(), data_denoised.detach(), t.detach()\n        else:\n            return x.detach()\n\n    def apply_mask(self, x):\n        return self.mask*x\n\n    def predict_inpainting(\n        self,\n        y_masked,\n        mask\n        ):\n        self.mask=mask.to(y_masked.device)\n\n        degradation=lambda x: self.apply_mask(x)\n\n        return self.predict_conditional(y_masked, degradation)\n\n    def apply_FIR_filter(self,y):\n        y=y.unsqueeze(1)\n\n        #apply the filter with a convolution (it is an FIR)\n        y_lpf=torch.nn.functional.conv1d(y,self.filt,padding=\"same\")\n        y_lpf=y_lpf.squeeze(1) \n\n        return y_lpf\n    def apply_IIR_filter(self,y):\n        y_lpf=torchaudio.functional.lfilter(y, self.a,self.b, clamp=False)\n        return y_lpf\n    def apply_biquad(self,y):\n        y_lpf=torchaudio.functional.biquad(y, self.b0, self.b1, self.b2, self.a0, self.a1, self.a2)\n        return y_lpf\n    def decimate(self,x):\n        return x[...,0:-1:self.factor]\n\n    def resample(self,x):\n        N=100\n        return torchaudio.functional.resample(x,orig_freq=int(N*self.factor), new_freq=N)\n\n    def predict_bwe(\n        self,\n        ylpf,  #observations (lowpssed signal) Tensor with shape (L,)\n        filt, #filter Tensor with shape ??\n        filt_type\n        ):\n\n        #define the degradation model as a lambda\n        if filt_type==\"firwin\":\n            self.filt=filt.to(ylpf.device)\n            degradation=lambda x: self.apply_FIR_filter(x)\n        elif filt_type==\"firwin_hpf\":\n            self.filt=filt.to(ylpf.device)\n            degradation=lambda x: self.apply_FIR_filter(x)\n        elif filt_type==\"cheby1\":\n            b,a=filt\n            self.a=torch.Tensor(a).to(ylpf.device)\n            self.b=torch.Tensor(b).to(ylpf.device)\n            degradation=lambda x: self.apply_IIR_filter(x)\n        elif filt_type==\"biquad\":\n            b0, b1, b2, a0, a1, a2=filt\n            self.b0=torch.Tensor(b0).to(ylpf.device)\n            self.b1=torch.Tensor(b1).to(ylpf.device)\n            self.b2=torch.Tensor(b2).to(ylpf.device)\n            self.a0=torch.Tensor(a0).to(ylpf.device)\n            self.a1=torch.Tensor(a1).to(ylpf.device)\n            self.a2=torch.Tensor(a2).to(ylpf.device)\n            degradation=lambda x: self.apply_biquad(x)\n        elif filt_type==\"resample\":\n            self.factor =filt\n            degradation= lambda x: self.resample(x)\n            return self.predict_resample(ylpf,(ylpf.shape[0], self.args.exp.audio_len), degradation)\n        elif filt_type==\"decimate\":\n            self.factor =filt\n            degradation= lambda x: self.decimate(x)\n            return self.predict_resample(ylpf,(ylpf.shape[0], self.args.exp.audio_len), degradation)\n        else:\n           raise NotImplementedError\n\n        return self.predict_conditional(ylpf, degradation)", "\n\nclass SamplerPhaseRetrieval(Sampler):\n\n    def __init__(self, model, diff_params, args, xi=0, order=2, data_consistency=False, rid=False):\n        super().__init__(model, diff_params, args, xi, order, data_consistency, rid)\n        #assert data_consistency==False\n        assert xi>0\n\n    def apply_stft(self,x):\n\n        x2=torch.cat((x,self.zeropad ),-1)\n        X=torch.stft(x2, self.win_size, hop_length=self.hop_size,window=self.window,center=False,return_complex=False)\n        Y=torch.sqrt(X[...,0]**2 + X[...,1]**2)\n        return Y\n\n    def predict_pr(\n        self,\n        y\n        ):\n\n        self.win_size=self.args.inference.phase_retrieval.win_size\n        self.hop_size=self.args.inference.phase_retrieval.hop_size\n\n        print(y.shape)\n        self.zeropad=torch.zeros(y.shape[0],self.win_size ).to(y.device)\n        self.window=torch.hamming_window(window_length=self.win_size).to(y.device)\n\n        degradation=lambda x: self.apply_stft(x)\n\n        return self.predict_resample(y, (y.shape[0], self.args.exp.audio_len), degradation)", "class SamplerCompSens(Sampler):\n\n    def __init__(self, model, diff_params, args, xi=0, order=2, data_consistency=False, rid=False):\n        super().__init__(model, diff_params, args, xi, order, data_consistency, rid)\n        assert data_consistency==False\n        assert xi>0\n\n    def apply_mask(self, x):\n        return self.mask*x\n\n    def predict_compsens(\n        self,\n        y_masked,\n        mask\n        ):\n\n        self.mask=mask.to(y_masked.device)\n\n        degradation=lambda x: self.apply_mask(x)\n\n        return self.predict_conditional(y_masked, degradation)", "\nclass SamplerDeclipping(Sampler):\n\n    def __init__(self, model, diff_params, args, xi=0, order=2, data_consistency=False, rid=False):\n        super().__init__(model, diff_params, args, xi, order, data_consistency, rid)\n        assert data_consistency==False\n        assert xi>0\n\n    def apply_clip(self,x):\n        x_hat=torch.clip(x,min=-self.clip_value, max=self.clip_value)\n        return x_hat\n\n    def predict_declipping(\n        self,\n        y_clipped,\n        clip_value\n        ):\n        self.clip_value=clip_value\n\n        degradation=lambda x: self.apply_clip(x)\n\n        if self.rid:\n            res, denoised, t=self.predict_conditional(y_clipped, degradation)\n            return res, denoised, t\n        else: \n            res=self.predict_conditional(y_clipped, degradation)\n            return res", "\nclass SamplerAutoregressive(Sampler):\n\n    def __init__(self, model, diff_params, args, xi=0, order=2, data_consistency=False, rid=False):\n        super().__init__(model, diff_params, args, xi, order, data_consistency, rid)\n        assert rid==False\n        self.ov=self.args.inference.autoregressive.overlap\n\n    def apply_mask(self, x):\n        return self.mask*x\n\n    def predict_autoregressive(\n        self,\n        shape,\n        N,\n        device\n        ):\n\n        endmask=int(self.ov*shape[-1])\n        self.mask=torch.ones((1,self.args.exp.audio_len)).to(device) #assume between 5 and 6s of total length\n        self.mask[:,endmask::]=0\n\n        degradation=lambda x: self.apply_mask(x)\n\n        x= self.predict_unconditional(shape, device)\n        xcat=x\n        x_masked=torch.zeros((1,self.args.exp.audio_len)).to(device)\n\n        for i in range(N-1):\n            x_masked[:,0:endmask]=x[:,-endmask::]\n            x=self.predict_conditional(x_masked, degradation)\n            xcat=torch.cat((xcat,x[...,endmask::]),-1)\n\n        return xcat", "\n\n\n        \n\nclass SamplerInpainting(Sampler):\n\n    def __init__(self, model, diff_params, args, xi=0, order=2, data_consistency=False, rid=False):\n        super().__init__(model, diff_params, args, xi, order, data_consistency, rid)\n\n    def apply_mask(self, x):\n        return self.mask*x\n\n    def predict_inpainting(\n        self,\n        y_masked,\n        mask\n        ):\n        self.mask=mask.to(y_masked.device)\n\n        degradation=lambda x: self.apply_mask(x)\n\n        return self.predict_conditional(y_masked, degradation)", "\nclass SamplerBWE(Sampler):\n\n    def __init__(self, model, diff_params, args, xi=0, order=2, data_consistency=False, rid=False):\n        super().__init__(model, diff_params, args, xi, order, data_consistency, rid)\n\n    def apply_FIR_filter(self,y):\n        y=y.unsqueeze(1)\n\n        #apply the filter with a convolution (it is an FIR)\n        y_lpf=torch.nn.functional.conv1d(y,self.filt,padding=\"same\")\n        y_lpf=y_lpf.squeeze(1) \n\n        return y_lpf\n    def apply_IIR_filter(self,y):\n        y_lpf=torchaudio.functional.lfilter(y, self.a,self.b, clamp=False)\n        return y_lpf\n    def apply_biquad(self,y):\n        y_lpf=torchaudio.functional.biquad(y, self.b0, self.b1, self.b2, self.a0, self.a1, self.a2)\n        return y_lpf\n    def decimate(self,x):\n        return x[...,0:-1:self.factor]\n\n    def resample(self,x):\n        N=100\n        return torchaudio.functional.resample(x,orig_freq=int(N*self.factor), new_freq=N)\n\n    def predict_bwe(\n        self,\n        ylpf,  #observations (lowpssed signal) Tensor with shape (L,)\n        filt, #filter Tensor with shape ??\n        filt_type\n        ):\n\n        #define the degradation model as a lambda\n        if filt_type==\"firwin\":\n            self.filt=filt.to(ylpf.device)\n            degradation=lambda x: self.apply_FIR_filter(x)\n        elif filt_type==\"firwin_hpf\":\n            self.filt=filt.to(ylpf.device)\n            degradation=lambda x: self.apply_FIR_filter(x)\n        elif filt_type==\"cheby1\":\n            b,a=filt\n            self.a=torch.Tensor(a).to(ylpf.device)\n            self.b=torch.Tensor(b).to(ylpf.device)\n            degradation=lambda x: self.apply_IIR_filter(x)\n        elif filt_type==\"biquad\":\n            b0, b1, b2, a0, a1, a2=filt\n            self.b0=torch.Tensor(b0).to(ylpf.device)\n            self.b1=torch.Tensor(b1).to(ylpf.device)\n            self.b2=torch.Tensor(b2).to(ylpf.device)\n            self.a0=torch.Tensor(a0).to(ylpf.device)\n            self.a1=torch.Tensor(a1).to(ylpf.device)\n            self.a2=torch.Tensor(a2).to(ylpf.device)\n            degradation=lambda x: self.apply_biquad(x)\n        elif filt_type==\"resample\":\n            self.factor =filt\n            degradation= lambda x: self.resample(x)\n            return self.predict_resample(ylpf,(ylpf.shape[0], self.args.audio_len), degradation)\n        elif filt_type==\"decimate\":\n            self.factor =filt\n            degradation= lambda x: self.decimate(x)\n            return self.predict_resample(ylpf,(ylpf.shape[0], self.args.audio_len), degradation)\n        else:\n           raise NotImplementedError\n\n        return self.predict_conditional(ylpf, degradation)", "        \n                \n"]}
{"filename": "testing/blind_bwe_sampler.py", "chunked_list": ["from tqdm import tqdm\nimport torch\nimport torchaudio\n#import scipy.signal\nimport copy\n#import numpy as np\n#import utils.filter_generation_utils as f_utils\n\nimport utils.blind_bwe_utils as blind_bwe_utils\n", "import utils.blind_bwe_utils as blind_bwe_utils\n\n\nclass BlindSampler():\n\n    def __init__(self, model,  diff_params, args, rid=False):\n\n        self.model = model\n\n        self.diff_params = diff_params #same as training, useful if we need to apply a wrapper or something\n        self.args=args\n        if not(self.args.tester.diff_params.same_as_training):\n            self.update_diff_params()\n\n\n        self.order=self.args.tester.order\n\n        self.xi=self.args.tester.posterior_sampling.xi\n        #hyperparameter for the reconstruction guidance\n        self.data_consistency=self.args.tester.posterior_sampling.data_consistency #use reconstruction gudance without replacement\n        self.nb_steps=self.args.tester.T\n\n        #prepare optimization parameters\n        self.mu=torch.Tensor([self.args.tester.blind_bwe.optimization.mu[0], self.args.tester.blind_bwe.optimization.mu[1]])\n        #clamping parameters\n        self.fcmin=self.args.tester.blind_bwe.fcmin\n        if self.args.tester.blind_bwe.fcmax ==\"nyquist\":\n                self.fcmax=self.args.exp.sample_rate//2\n        else:\n                self.fcmax=self.args.tester.blind_bwe.fcmax\n        self.Amin=self.args.tester.blind_bwe.Amin\n        self.Amax=self.args.tester.blind_bwe.Amax\n        #used for congerence checking\n        self.tol=self.args.tester.blind_bwe.optimization.tol\n        \n        self.start_sigma=self.args.tester.posterior_sampling.start_sigma\n        if self.start_sigma ==\"None\":\n            self.start_sigma=None\n        print(\"start sigma\", self.start_sigma)\n\n\n    def update_diff_params(self):\n        #the parameters for testing might not be necesarily the same as the ones used for training\n        self.diff_params.sigma_min=self.args.tester.diff_params.sigma_min\n        self.diff_params.sigma_max =self.args.tester.diff_params.sigma_max\n        self.diff_params.ro=self.args.tester.diff_params.ro\n        self.diff_params.sigma_data=self.args.tester.diff_params.sigma_data\n        #par.diff_params.meters stochastic sampling\n        self.diff_params.Schurn=self.args.tester.diff_params.Schurn\n        self.diff_params.Stmin=self.args.tester.diff_params.Stmin\n        self.diff_params.Stmax=self.args.tester.diff_params.Stmax\n        self.diff_params.Snoise=self.args.tester.diff_params.Snoise\n\n\n    def data_consistency_step_classic(self, x_hat, y, degradation, filter_params=None):\n        \"\"\"\n        Simple replacement method, used for inpainting and FIR bwe\n        \"\"\"\n        #get reconstruction estimate\n        if filter_params is not None:\n            den_rec= degradation(x_hat, filter_params)     \n        else:\n            den_rec= degradation(x_hat)     \n        #apply replacment (valid for linear degradations)\n        return y+x_hat-den_rec \n    \n    def get_rec_grads(self, x_hat, y, x, t_i, degradation, filter_params=None):\n        \"\"\"\n        Compute the gradients of the reconstruction error with respect to the input\n        \"\"\" \n\n        if self.args.tester.posterior_sampling.SNR_observations !=\"None\":\n            snr=10**(self.args.tester.posterior_sampling.SNR_observations/10)\n            sigma2_s=torch.var(y, -1)\n            sigma=torch.sqrt(sigma2_s/snr).unsqueeze(-1)\n            #sigma=torch.tensor([self.args.tester.posterior_sampling.sigma_observations]).unsqueeze(-1).to(y.device)\n            #print(y.shape, sigma.shape)\n            y+=sigma*torch.randn(y.shape).to(y.device)\n\n        if filter_params is not None:\n            den_rec= degradation(x_hat, filter_params) \n        else:\n            den_rec= degradation(x_hat) \n\n        if len(y.shape)==3:\n            dim=(1,2)\n        elif len(y.shape)==2:\n            dim=1\n\n\n        if self.args.tester.posterior_sampling.norm==\"smoothl1\":\n            norm=torch.nn.functional.smooth_l1_loss(y, den_rec, reduction='sum', beta=self.args.tester.posterior_sampling.smoothl1_beta)\n        elif self.args.tester.posterior_sampling.norm==\"cosine\":\n            cos = torch.nn.CosineSimilarity(dim=dim, eps=1e-6)\n            norm = (1-cos(den_rec, y)).clamp(min=0)\n            print(\"norm\",norm)\n        elif self.args.tester.posterior_sampling.stft_distance.use:\n            if self.args.tester.posterior_sampling.stft_distance.use_multires:\n                print(\" applying multires \")\n                norm1, norm2=self.norm(y, den_rec)\n                norm=norm1+norm2\n            elif self.args.tester.posterior_sampling.stft_distance.mag:\n                print(\"logmag\", self.args.tester.posterior_sampling.stft_distance.logmag)\n                norm=blind_bwe_utils.apply_norm_STFTmag_fweighted(y, den_rec, self.args.tester.posterior_sampling.freq_weighting, self.args.tester.posterior_sampling.stft_distance.nfft, logmag=self.args.tester.posterior_sampling.stft_distance.logmag)\n                print(\"norm\", norm)\n            else:\n                norm=blind_bwe_utils.apply_norm_STFT_fweighted(y, den_rec, self.args.tester.posterior_sampling.freq_weighting, self.args.tester.posterior_sampling.stft_distance.nfft)\n        else:\n            norm=torch.linalg.norm(y-den_rec,dim=dim, ord=self.args.tester.posterior_sampling.norm)\n\n        \n        rec_grads=torch.autograd.grad(outputs=norm.sum(),\n                                      inputs=x)\n\n        rec_grads=rec_grads[0]\n        \n        normguide=torch.linalg.norm(rec_grads)/self.args.exp.audio_len**0.5\n        \n        #normalize scaling\n        s=self.xi/(normguide+1e-6)\n        \n        #optionally apply a treshold to the gradients\n        if False:\n            #pply tresholding to the gradients. It is a dirty trick but helps avoiding bad artifacts \n            rec_grads=torch.clip(rec_grads, min=-self.treshold_on_grads, max=self.treshold_on_grads)\n        \n        return s*rec_grads/t_i\n\n    def get_score_rec_guidance(self, x, y, t_i, degradation, filter_params=None):\n\n        x.requires_grad_()\n        x_hat=self.get_denoised_estimate(x, t_i)\n        #add noise to y\n\n        rec_grads=self.get_rec_grads(x_hat, y, x, t_i, degradation, filter_params)\n\n        score=self.denoised2score(x_hat, x, t_i)\n\n        #apply scaled guidance to the score\n        score=score-rec_grads\n\n        return score\n    \n    def get_denoised_estimate(self, x, t_i):\n        x_hat=self.diff_params.denoiser(x, self.model, t_i.unsqueeze(-1))\n\n        if self.args.tester.filter_out_cqt_DC_Nyq:\n            x_hat=self.model.CQTransform.apply_hpf_DC(x_hat)\n        return x_hat\n    \n\n    def get_score(self,x, y, t_i, degradation, filter_params=None):\n        if y==None:\n            assert degradation==None\n            #unconditional sampling\n            with torch.no_grad():\n                #print(\"In sampling\", x.shape, t_i.shape)\n                #print(\"before denoiser\", x.shape)\n                x_hat=self.diff_params.denoiser(x, self.model, t_i.unsqueeze(-1))\n                if self.args.tester.filter_out_cqt_DC_Nyq:\n                    x_hat=self.model.CQTransform.apply_hpf_DC(x_hat)\n                score=(x_hat-x)/t_i**2\n            return score\n        else:\n            if self.xi>0:\n                #apply rec. guidance\n                score=self.get_score_rec_guidance(x, y, t_i, degradation, filter_params=filter_params)\n    \n                #optionally apply replacement or consistency step\n                if self.data_consistency:\n                    #convert score to denoised estimate using Tweedie's formula\n                    x_hat=score*t_i**2+x\n    \n                    try:\n                        x_hat=self.data_consistency_step(x_hat)\n                    except:\n                        x_hat=self.data_consistency_step(x_hat,y, degradation)\n    \n                    #convert back to score\n                    score=(x_hat-x)/t_i**2\n    \n            else:\n                #raise NotImplementedError\n                #denoised with replacement method\n                with torch.no_grad():\n                    x_hat=self.diff_params.denoiser(x, self.model, t_i.unsqueeze(-1))\n                        \n                    #x_hat=self.data_consistency_step(x_hat,y, degradation)\n                    if self.data_consistency:\n                        try:\n                            x_hat=self.data_consistency_step(x_hat)\n                        except:\n                            try:\n                                x_hat=self.data_consistency_step(x_hat,y, degradation)\n                            except:\n                                x_hat=self.data_consistency_step(x_hat,y, degradation, filter_params)\n\n        \n                    score=(x_hat-x)/t_i**2\n    \n            return score\n\n    def apply_FIR_filter(self,y):\n        y=y.unsqueeze(1)\n\n        #apply the filter with a convolution (it is an FIR)\n        y_lpf=torch.nn.functional.conv1d(y,self.filt,padding=\"same\")\n        y_lpf=y_lpf.squeeze(1) \n\n        return y_lpf\n    def apply_IIR_filter(self,y):\n        y_lpf=torchaudio.functional.lfilter(y, self.a,self.b, clamp=False)\n        return y_lpf\n    def apply_biquad(self,y):\n        y_lpf=torchaudio.functional.biquad(y, self.b0, self.b1, self.b2, self.a0, self.a1, self.a2)\n        return y_lpf\n    def decimate(self,x):\n        return x[...,0:-1:self.factor]\n\n    def resample(self,x):\n        N=100\n        return torchaudio.functional.resample(x,orig_freq=int(N*self.factor), new_freq=N)\n\n    def prepare_smooth_mask(self, mask, size=10):\n        hann=torch.hann_window(size*2)\n        hann_left=hann[0:size]\n        hann_right=hann[size::]\n        B,N=mask.shape\n        mask=mask[0]\n        prev=1\n        new_mask=mask.clone()\n        #print(hann.shape)\n        for i in range(len(mask)):\n            if mask[i] != prev:\n                #print(i, mask.shape, mask[i], prev)\n                #transition\n                if mask[i]==0:\n                   print(\"apply right\")\n                   #gap encountered, apply hann right before\n                   new_mask[i-size:i]=hann_right\n                if mask[i]==1:\n                   print(\"apply left\")\n                   #gap encountered, apply hann left after\n                   new_mask[i:i+size]=hann_left\n                #print(mask[i-2*size:i+2*size])\n                #print(new_mask[i-2*size:i+2*size])\n                \n            prev=mask[i]\n        return new_mask.unsqueeze(0).expand(B,-1)\n\n    def predict_bwe_AR(\n        self,\n        ylpf,  #observations (lowpssed signal) Tensor with shape (L,)\n        y_masked,\n        filt, #filter Tensor with shape ??\n        filt_type,\n        rid=False,\n        test_filter_fit=False,\n        compute_sweep=False,\n        mask=None\n        ):\n        assert mask is not None\n\n        #define the degradation model as a lambda\n        if filt_type==\"fc_A\":\n            print(\"fc_A\")\n            self.freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(ylpf.device)\n            self.params=filt\n            print(self.params)\n\n\n            y=mask*y_masked+(1-mask)*ylpf\n\n            degradation=lambda x: mask*x +(1-mask)*self.apply_filter_fcA(x, self.params)\n        elif filt_type==\"firwin\":\n            self.filt=filt.to(ylpf.device)\n\n            y=mask*y_masked+(1-mask)*ylpf\n\n            degradation=lambda x: mask*x +(1-mask)*self.apply_FIR_filter(x)\n\n            #degradation=lambda x: self.apply_FIR_filter(x)\n        else:\n           raise NotImplementedError\n\n        if self.args.tester.complete_recording.inpaint_DC:\n            smooth_mask=self.prepare_smooth_mask(mask, 50)\n            y_smooth_masked=smooth_mask*y_masked\n\n            mask_degradation=lambda x: smooth_mask*x \n            self.data_consistency_step=lambda x_hat: self.data_consistency_step_classic(x_hat,y_smooth_masked, mask_degradation)\n            self.data_consistency=True\n\n\n        return self.predict_conditional(y, degradation, rid, test_filter_fit, compute_sweep)\n\n        \n    def predict_bwe(\n        self,\n        ylpf,  #observations (lowpssed signal) Tensor with shape (L,)\n        filt, #filter Tensor with shape ??\n        filt_type,\n        rid=False,\n        test_filter_fit=False,\n        compute_sweep=False\n        ):\n        print(\"test_filter_fit\", test_filter_fit)\n        print(\"compute_sweep\", compute_sweep)\n\n        #define the degradation model as a lambda\n        if filt_type==\"firwin\":\n            self.filt=filt.to(ylpf.device)\n            degradation=lambda x: self.apply_FIR_filter(x)\n        elif filt_type==\"firwin_hpf\":\n            self.filt=filt.to(ylpf.device)\n            degradation=lambda x: self.apply_FIR_filter(x)\n        elif filt_type==\"cheby1\":\n            b,a=filt\n            self.a=torch.Tensor(a).to(ylpf.device)\n            self.b=torch.Tensor(b).to(ylpf.device)\n            degradation=lambda x: self.apply_IIR_filter(x)\n        elif filt_type==\"biquad\":\n            b0, b1, b2, a0, a1, a2=filt\n            self.b0=torch.Tensor(b0).to(ylpf.device)\n            self.b1=torch.Tensor(b1).to(ylpf.device)\n            self.b2=torch.Tensor(b2).to(ylpf.device)\n            self.a0=torch.Tensor(a0).to(ylpf.device)\n            self.a1=torch.Tensor(a1).to(ylpf.device)\n            self.a2=torch.Tensor(a2).to(ylpf.device)\n            degradation=lambda x: self.apply_biquad(x)\n        elif filt_type==\"resample\":\n            self.factor =filt\n            degradation= lambda x: self.resample(x)\n            return self.predict_resample(ylpf,(ylpf.shape[0], self.args.exp.audio_len), degradation)\n        elif filt_type==\"decimate\":\n            self.factor =filt\n            degradation= lambda x: self.decimate(x)\n            return self.predict_resample(ylpf,(ylpf.shape[0], self.args.exp.audio_len), degradation)\n            #elif filt_type==\"3rdoct\":\n            #    freq_octs=torch.tensor(f_utils.get_third_octave_bands(self.args.exp.sample_rate, fmin=self.args.tester.blind_bwe.range.fmin, fmax=self.args.exp.sample_rate/2))\n            #    filt=f_utils.normalize_filter(filt)\n            #    degradation= lambda x: self.apply_3rdoct_filt(x, filt, freq_octs)\n        elif filt_type==\"fc_A\":\n            print(\"fc_A\")\n            self.freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(ylpf.device)\n            self.params=filt\n            print(self.params)\n            degradation=lambda x:  self.apply_filter_fcA(x, self.params)\n        else:\n           raise NotImplementedError\n        \n        if self.data_consistency:\n            #normal data consistency\n            self.data_consistency_step=lambda x,y,degradation: self.data_consistency_step_classic(x,y, degradation)\n\n        return self.predict_conditional(ylpf, degradation, rid, test_filter_fit, compute_sweep)\n\n    def predict_unconditional(\n        self,\n        shape,  #observations (lowpssed signal) Tensor with shape ??\n        device,\n        rid=False\n    ):\n        self.y=None\n        self.degradation=None\n        return self.predict(shape, device, rid)\n\n    def predict_resample(\n        self,\n        y,  #observations (lowpssed signal) Tensor with shape ??\n        shape,\n        degradation, #lambda function\n    ):\n        self.degradation=degradation \n        self.y=y\n        return self.predict(shape, y.device)\n\n\n    def predict_conditional(\n        self,\n        y,  #observations (lowpssed signal) Tensor with shape ??\n        degradation, #lambda function\n        rid=False,\n        test_filter_fit=False,\n        compute_sweep=False\n    ):\n        self.degradation=degradation \n\n        #if self.args.tester.posterior_sampling.SNR_observations is not None:\n        #    SNR=10**(self.args.tester.posterior_sampling.SNR_observations/10)\n        #    sigma2_s=torch.var(y, -1)\n        #    sigma=torch.sqrt(sigma2_s/SNR)\n        #    y+=sigma*torch.randn(y.shape).to(y.device)\n\n        self.y=y\n        return self.predict(y.shape, y.device, rid, test_filter_fit, compute_sweep)\n\n    def predict(\n        self,\n        shape,  #observations (lowpssed signal) Tensor with shape ??\n        device, #lambda function\n        rid=False,\n        test_filter_fit=False,\n        compute_sweep=False\n    ):\n\n        if rid:\n            data_denoised=torch.zeros((self.nb_steps,shape[0], shape[1]))\n            data_score=torch.zeros((self.nb_steps,shape[0], shape[1]))\n\n        if test_filter_fit:\n            filter_params=torch.Tensor([self.args.tester.blind_bwe.initial_conditions.fc, self.args.tester.blind_bwe.initial_conditions.A]).to(device)\n            if rid:\n                data_filters=torch.zeros((self.nb_steps, filter_params.shape[0]))\n\n        if self.start_sigma is None or self.y is None:\n            t=self.diff_params.create_schedule(self.nb_steps).to(device)\n            x=self.diff_params.sample_prior(shape, t[0]).to(device)\n        else:\n            #get the noise schedule\n            t = self.diff_params.create_schedule_from_initial_t(self.start_sigma,self.nb_steps).to(device)\n            #sample from gaussian distribution with sigma_max variance\n            x = self.y + self.diff_params.sample_prior(shape,t[0]).to(device)\n\n        #if self.args.tester.bandwidth_extension.sigma_observations>0 and self.y is not None:\n        #    self.y=self.y+self.args.tester.bandwidth_extension.sigma_observations*torch.randn_like(self.y)\n        #parameter for langevin stochasticity, if Schurn is 0, gamma will be 0 to, so the sampler will be deterministic\n        gamma=self.diff_params.get_gamma(t).to(device)\n\n        if compute_sweep:\n            self.fc_s=torch.logspace(2.5, 4, 15).to(device)\n            self.A_s=torch.linspace(-80, -5, 12).to(device)\n            if rid:\n                data_norms=torch.zeros((self.nb_steps,self.fc_s.shape[0], self.A_s.shape[0]))\n                data_grads=torch.zeros((self.nb_steps,self.fc_s.shape[0], self.A_s.shape[0], 2))\n\n        for i in tqdm(range(0, self.nb_steps, 1)):\n            #print(\"sampling step \",i,\" from \",self.nb_steps)\n            x_hat, t_hat=self.move_timestep(x, t[i], gamma[i],self.diff_params.Snoise)\n\n            score=self.get_score(x_hat, self.y, t_hat, self.degradation)    \n            if test_filter_fit:\n                denoised_estimate=self.score2denoised(score, x_hat, t_hat)\n                est_params=self.fit_params(denoised_estimate, self.y,  filter_params)\n                ##print(\"estimated params\",est_params.shape)\n\n            if compute_sweep:\n                denoised_estimate=self.score2denoised(score, x_hat, t_hat)\n                norms, grads=self.compute_sweep(denoised_estimate, self.y)\n\n            d=-t_hat*score\n\n            if rid: \n                data_denoised[i]=self.score2denoised(score, x_hat, t_hat)\n                data_score[i]=score\n                if test_filter_fit:\n                    data_filters[i]=est_params\n                if compute_sweep:\n                    data_norms[i]=norms\n                    data_grads[i]=grads\n            \n            #apply second order correction\n            h=t[i+1]-t_hat\n\n\n            if t[i+1]!=0 and self.order==2:  #always except last step\n                #second order correction2\n                #h=t[i+1]-t_hat\n                t_prime=t[i+1]\n                x_prime=x_hat+h*d\n                score=self.get_score(x_prime, self.y, t_prime, self.degradation)\n\n                d_prime=-t_prime*score\n\n                x=(x_hat+h*((1/2)*d +(1/2)*d_prime))\n\n            elif t[i+1]==0 or self.order==1: #first condition  is to avoid dividing by 0\n                #first order Euler step\n                x=x_hat+h*d\n\n            \n        if rid:\n            list_out=(x.detach(), data_denoised.detach(), data_score.detach(),t.detach())\n            if test_filter_fit:\n                list_out=list_out+(data_filters.detach(),)\n            if compute_sweep:\n                list_out=list_out+(data_norms.detach(), data_grads.detach())\n            return list_out\n        else:\n            return x.detach()\n\n\n\n  \n    def denoised2score(self,  x_d0, x, t):\n        #tweedie's score function\n        return (x_d0-x)/t**2\n    def score2denoised(self, score, x, t):\n        return score*t**2+x\n\n    def move_timestep(self, x, t, gamma, Snoise=1):\n        #if gamma_sig[i]==0 this is a deterministic step, make sure it doed not crash\n        t_hat=t+gamma*t\n        #sample noise, Snoise is 1 by default\n        epsilon=torch.randn(x.shape).to(x.device)*Snoise\n        #add extra noise\n        x_hat=x+((t_hat**2 - t**2)**(1/2))*epsilon\n        return x_hat, t_hat\n\n    def apply_filter_fcA(self, x, filter_params):\n        H=blind_bwe_utils.design_filter(filter_params[0], filter_params[1], self.freqs)\n        return blind_bwe_utils.apply_filter(x, H,self.args.tester.blind_bwe.NFFT)\n\n    def optimizer_func(self, Xden, Y, params):\n        \"\"\"\n        Xden: STFT of denoised estimate\n        y: observations\n        params: parameters of the degradation model (fc, A)\n        \"\"\"\n\n        #print(\"before design filter\", params)\n        H=blind_bwe_utils.design_filter(params[0],params[1], self.freqs)\n        return blind_bwe_utils.apply_filter_and_norm_STFTmag_fweighted(Xden, Y, H, self.args.tester.posterior_sampling.freq_weighting_filter)\n\n    def fit_params(self, denoised_estimate, y, filter_params):\n        #fit the parameters of the degradation model\n        #denoised_estimate: denoised estimate of the signal\n        #y: observations\n        #degradation: degradation function\n        #filter_params: initial estimate of parameters of the degradation model\n\n        #return: reestimated parameters of the degradation model\n\n        if self.args.tester.posterior_sampling.SNR_observations !=\"None\":\n            snr=10**(self.args.tester.posterior_sampling.SNR_observations/10)\n            sigma2_s=torch.var(y, -1)\n            sigma=torch.sqrt(sigma2_s/snr).unsqueeze(-1)\n            #sigma=torch.tensor([self.args.tester.posterior_sampling.sigma_observations]).unsqueeze(-1).to(y.device)\n            #print(y.shape, sigma.shape)\n            y+=sigma*torch.randn(y.shape).to(y.device)\n\n        #add noise to the denoised estimate for regularization\n        if self.args.tester.blind_bwe.sigma_den_estimate:\n            denoised_estimate=denoised_estimate+torch.randn(denoised_estimate.shape).to(denoised_estimate.device)*self.args.tester.blind_bwe.sigma_den_estimate\n        \n\n\n        Xden=blind_bwe_utils.apply_stft(denoised_estimate, self.args.tester.blind_bwe.NFFT)\n        Y=blind_bwe_utils.apply_stft(y, self.args.tester.blind_bwe.NFFT)\n\n        func=lambda  params: self.optimizer_func( Xden, Y, params)\n        self.mu=self.mu.to(y.device)\n        for i in tqdm(range(self.args.tester.blind_bwe.optimization.max_iter)):\n            filter_params.requires_grad=True\n                #fc.requires_grad=True\n            norm=func(filter_params)\n\n            grad=torch.autograd.grad(norm,filter_params,create_graph=True)\n            #update params with gradient descent, using backtracking line search\n            t=self.mu\n            newparams=filter_params-t.unsqueeze(1)*grad[0]\n\n            #update with the found step size\n            filter_params=newparams\n\n            filter_params.detach_()\n            #limit params to help stability\n            if self.args.tester.blind_bwe.optimization.clamp_fc:\n                    filter_params[0,0]=torch.clamp(filter_params[0,0],min=self.fcmin,max=self.fcmax)\n                    for k in range(1,len(filter_params[0])):\n                        filter_params[0,k]=torch.clamp(filter_params[0,k],min=filter_params[0,k-1]+1,max=self.fcmax)\n            if self.args.tester.blind_bwe.optimization.clamp_A:\n                    filter_params[1,0]=torch.clamp(filter_params[1,0],min=self.Amin,max=-1 if self.args.tester.blind_bwe.optimization.only_negative_A else self.Amax)\n                    for k in range(1,len(filter_params[0])):\n                        filter_params[1,k]=torch.clamp(filter_params[1,k],min=self.Amin,max=filter_params[1,k-1] if self.args.tester.blind_bwe.optimization.only_negative_A else self.Amax)\n    \n\n            if i>0:\n                if (torch.abs(filter_params[0]-prev_params[0]).mean()<self.tol[0]) and (torch.abs(filter_params[1]-prev_params[1]).mean()<self.tol[1]):\n                     break\n\n            prev_params=filter_params.clone().detach()\n\n        #print(\"fc: \",filter_params[0].item(),\" A: \", filter_params[1].item())\n        print(filter_params)\n        \n        return filter_params\n\n\n    def compute_sweep(self, denoised_estimate, y):\n\n        Xden=blind_bwe_utils.apply_stft(denoised_estimate, self.args.tester.blind_bwe.NFFT)\n        Y=blind_bwe_utils.apply_stft(y, self.args.tester.blind_bwe.NFFT)\n\n        func=lambda  params: self.optimizer_func( Xden, Y, params)\n\n        grads=torch.zeros(self.fc_s.shape[0], self.A_s.shape[0], 2)\n        norms=torch.zeros(self.fc_s.shape[0], self.A_s.shape[0])\n        #iterate over fc and A values\n\n        for fc in range(self.fc_s.shape[0]):\n            for A in range(self.A_s.shape[0]):\n                #print(\"fc: \",self.fc_s[fc].item(),\"A: \",self.A_s[A].item())\n                params=torch.Tensor([self.fc_s[fc], self.A_s[A]]).requires_grad_(True)\n                norm=func(params)\n                grads[fc,A,:]=torch.autograd.grad(norm,params,create_graph=True)[0]\n                norms[fc,A]=norm\n        return norms.detach(), grads.detach()\n\n\n    def predict_blind_bwe(\n        self,\n        y,  #observations (lowpssed signal) Tensor with shape (L,)\n        rid=False,\n        compute_sweep=False,\n        ):\n\n\n\n        \n        self.freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(y.device)\n        self.degradation=lambda x, filter_params: self.apply_filter_fcA(x, filter_params)\n\n        if self.data_consistency:\n            #normal data consistency\n            self.data_consistency_step=lambda x,y,degradation, filter_params: self.data_consistency_step_classic(x,y, degradation, filter_params)\n\n        #get shape and device from the observations tensor\n        shape=y.shape\n        device=y.device\n\n        #initialise filter parameters\n        filter_params=torch.Tensor([self.args.tester.blind_bwe.initial_conditions.fc, self.args.tester.blind_bwe.initial_conditions.A]).to(device)\n        if len(filter_params.shape)==1:\n            filter_params.unsqueeze_(1)\n        print(filter_params.shape)\n\n        shape_filter_params=filter_params.shape #fc and A\n        #retrieve the shape from the initial estimate of the parameters\n        \n        if compute_sweep:\n            self.fc_s=torch.logspace(2.5, 4, 15).to(device)\n            self.A_s=torch.linspace(-80, -5, 12).to(device)\n            if rid:\n                data_norms=torch.zeros((self.nb_steps,self.fc_s.shape[0], self.A_s.shape[0]))\n                data_grads=torch.zeros((self.nb_steps,self.fc_s.shape[0], self.A_s.shape[0], 2))\n\n        if rid:\n            data_denoised=torch.zeros((self.nb_steps,shape[0], shape[1]))\n            data_filters=torch.zeros((self.nb_steps,*shape_filter_params))\n            print(data_filters.shape)\n    \n\n\n        if self.start_sigma is None:\n            t=self.diff_params.create_schedule(self.nb_steps).to(device)\n            x=self.diff_params.sample_prior(shape, t[0]).to(device)\n        else:\n            #get the noise schedule\n            t = self.diff_params.create_schedule_from_initial_t(self.start_sigma,self.nb_steps).to(y.device)\n            #sample from gaussian distribution with sigma_max variance\n            x = y + self.diff_params.sample_prior(shape,t[0]).to(device)\n\n        #if self.args.tester.posterior_sampling.SNR_observations !=\"none\":\n        #    snr=10**(self.args.tester.posterior_sampling.SNR_observations/10)\n        #    sigma2_s=torch.var(y, -1)\n        #    sigma=torch.sqrt(sigma2_s/snr).unsqueeze(-1)\n        #    #sigma=torch.tensor([self.args.tester.posterior_sampling.sigma_observations]).unsqueeze(-1).to(y.device)\n        #    #print(y.shape, sigma.shape)\n        #    y+=sigma*torch.randn(y.shape).to(y.device)\n\n        #parameter for langevin stochasticity, if Schurn is 0, gamma will be 0 to, so the sampler will be deterministic\n        gamma=self.diff_params.get_gamma(t).to(device)\n\n\n\n        for i in tqdm(range(0, self.nb_steps, 1)):\n            #print(\"sampling step \",i,\" from \",self.nb_steps)\n            x_hat, t_hat=self.move_timestep(x, t[i], gamma[i])\n\n            x_hat.requires_grad_(True)\n\n            x_den=self.get_denoised_estimate(x_hat, t_hat)\n\n            x_den_2=x_den.clone().detach()\n\n            filter_params=self.fit_params(x_den_2, y,  filter_params)\n\n            rec_grads=self.get_rec_grads(x_den, y, x_hat, t_hat, self.degradation, filter_params)\n            \n            x_hat.detach_()\n\n            score=self.denoised2score(x_den_2, x_hat, t_hat)-rec_grads\n\n\n            if self.args.tester.posterior_sampling.data_consistency:\n                #apply data consistency here!\n                #it is a bit ugly, but I need to convert the score to denoied estimate again\n                x_den_3=self.score2denoised(score, x_hat, t_hat)\n                x_den_3=self.data_consistency_step(x_den_3, y, self.degradation, filter_params)\n                score=self.denoised2score(x_den_3, x_hat, t_hat)\n\n\n            if compute_sweep:\n                norms, grads=self.compute_sweep(x_den_2, y)\n\n            #d=-t_hat*((denoised-x_hat)/t_hat**2)\n            d=-t_hat*score\n\n            if rid: \n                data_denoised[i]=x_den_2\n                data_filters[i]=filter_params\n                if compute_sweep:\n                    data_norms[i]=norms\n                    data_grads[i]=grads\n            \n            #apply second order correction\n            h=t[i+1]-t_hat\n\n\n            if t[i+1]!=0 and self.order==2:  #always except last step\n                #second order correction2\n                #h=t[i+1]-t_hat\n                t_prime=t[i+1]\n                x_prime=x_hat+h*d\n                x_prime.requires_grad_(True)\n\n                x_den=self.get_denoised_estimate(x_prime, t_prime)\n\n                x_den_2=x_den.clone().detach()\n\n                filter_params=self.fit_params(x_den_2, y,  filter_params)\n\n                rec_grads=self.get_rec_grads(x_den, y, x_prime, t_prime, self.degradation, filter_params)\n\n                x_prime.detach_()\n\n                score=self.denoised2score(x_den_2, x_prime, t_prime)-rec_grads\n\n                if self.args.tester.posterior_sampling.data_consistency:\n                    #apply data consistency here!\n                    #it is a bit ugly, but I need to convert the score to denoied estimate again\n                    x_den_3=self.score2denoised(score, x_prime, t_prime)\n                    x_den_3=self.data_consistency_step(x_den_3, y, self.degradation, filter_params)\n                    score=self.denoised2score(x_den_3, x_prime, t_prime)\n\n                d_prime=-t_prime*score\n\n                x=(x_hat+h*((1/2)*d +(1/2)*d_prime))\n\n            elif t[i+1]==0 or self.order==1: #first condition  is to avoid dividing by 0\n                #first order Euler step\n                x=x_hat+h*d\n\n        if rid:\n            list_out=(x.detach(), filter_params.detach(), data_denoised.detach(),t.detach(), data_filters.detach())\n            if compute_sweep:\n                list_out=list_out+(data_norms.detach(), data_grads.detach())\n            return list_out\n        else:\n            return x.detach() , filter_params.detach()", "\n"]}
{"filename": "testing/blind_bwe_tester_small.py", "chunked_list": ["from datetime import date\nimport pickle\nimport re\nimport torch\nimport torchaudio\n#from src.models.unet_cqt import Unet_CQT\n#from src.models.unet_stft import Unet_STFT\n#from src.models.unet_1d import Unet_1d\n#import src.utils.setup as utils_setup\n#from src.sde import  VE_Sde_Elucidating", "#import src.utils.setup as utils_setup\n#from src.sde import  VE_Sde_Elucidating\nimport numpy as np\nimport utils.dnnlib as dnnlib\nimport os\n\nimport utils.logging as utils_logging\nimport wandb\nimport copy\n", "import copy\n\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport utils.bandwidth_extension as utils_bwe\nimport omegaconf\n\n#import utils.filter_generation_utils as f_utils\nimport utils.blind_bwe_utils as blind_bwe_utils", "#import utils.filter_generation_utils as f_utils\nimport utils.blind_bwe_utils as blind_bwe_utils\nimport utils.training_utils as t_utils\n\nimport soundfile as sf\n\n#from utils.spectral_analysis import LTAS_processor\n\n\nclass BlindTester():\n    def __init__(\n        self, args=None, network=None, diff_params=None, test_set=None, device=None, it=None\n    ):\n        self.args=args\n        self.network=torch.compile(network)\n        #self.network=network\n        #prnt number of parameters\n        \n\n        self.diff_params=copy.copy(diff_params)\n        self.device=device\n        #choose gpu as the device if possible\n        if self.device is None:\n            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.network=network\n\n        torch.backends.cudnn.benchmark = True\n\n        today=date.today() \n        if it is None:\n            self.it=0\n\n        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n        if not os.path.exists(self.path_sampling):\n            os.makedirs(self.path_sampling)\n\n\n        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n        self.setup_sampler()\n\n        self.use_wandb=False #hardcoded for now\n\n        S=self.args.exp.resample_factor\n        if S>2.1 and S<2.2:\n            #resampling 48k to 22.05k\n            self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n        elif S!=1:\n            N=int(self.args.exp.audio_len*S)\n            self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\n        if test_set is not None:\n            self.test_set=test_set\n            self.do_inpainting=True\n            self.do_bwe=True\n            self.do_blind_bwe=True\n        else:\n            self.test_set=None\n            self.do_inpainting=False\n            self.do_bwe=False #these need to be set up in the config file\n            self.do_blind_bwe=False\n\n        self.paths={}\n        if self.do_inpainting and (\"inpainting\" in self.args.tester.modes):\n            self.do_inpainting=True\n            mode=\"inpainting\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"inpainting\",\"masked\",\"inpainted\")\n            #TODO add more information in the subirectory names\n        else: self.do_inpainting=False\n\n        if self.do_bwe and (\"bwe\" in self.args.tester.modes):\n            self.do_bwe=True\n            mode=\"bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"bwe\",\"lowpassed\",\"bwe\")\n            #TODO add more information in the subirectory names\n        else:\n            self.do_bwe=False\n\n        if self.do_blind_bwe and (\"blind_bwe\" in self.args.tester.modes):\n            self.do_blind_bwe=True\n            mode=\"blind_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"], self.paths[mode+\"degraded_estimate\"]=self.prepare_blind_experiment(\"blind_bwe\",\"masked\",\"blind_bwe\",\"degraded_estimate\")\n            #TODO add more information in the subirectory names\n        if \"real_blind_bwe\" in self.args.tester.modes:\n            self.do_blind_bwe=True\n            mode=\"real_blind_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"real_blind_bwe\",\"degraded\",\"reconstructed\")\n            #TODO add more information in the subirectory names\n\n        if \"formal_test_bwe\" in self.args.tester.modes:\n            self.do_formal_test_bwe=True\n            mode=\"formal_test_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"formal_test_bwe\",\"degraded\",\"reconstructed\")\n        if \"formal_test_bwe_small\" in self.args.tester.modes:\n            self.do_formal_test_bwe=True\n            mode=\"formal_test_bwe_small\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"formal_test_bwe_small\",\"degraded\",\"reconstructed\")\n        \n        if (\"unconditional\" in self.args.tester.modes):\n            mode=\"unconditional\"\n            self.paths[mode]=self.prepare_unc_experiment(\"unconditional\")\n\n\n        if (\"filter_bwe\" in self.args.tester.modes):\n            mode=\"filter_bwe\"\n            self.paths[mode]=self.prepare_unc_experiment(\"filter_bwe\")\n\n        #self.LTAS_processor=LTAS_processor(self.args.tester.blind_bwe.LTAS.sample_rate,self.args.tester.blind_bwe.LTAS.audio_len)\n        #self.LTAS_processor.load_dataset_LTAS(self.args.tester.blind_bwe.LTAS.path)\n\n    def prepare_unc_experiment(self, str):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n            return path_exp\n\n    def prepare_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n\n            return path_exp, path_degraded, path_original, path_reconstructed\n\n    def resample_audio(self, audio, fs):\n        #this has been reused from the trainer.py\n        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\n    def prepare_blind_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\", str_degraded_estimate=\"degraded_estimate\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n            \n            n=str_degraded_estimate\n            path_degraded_estimate=os.path.join(path_exp, n) #path for the estimated degraded signal\n            #ensure the path exists\n            if not os.path.exists(path_degraded_estimate):\n                os.makedirs(path_degraded_estimate)\n\n            return path_exp, path_degraded, path_original, path_reconstructed, path_degraded_estimate\n\n    def setup_wandb(self):\n        \"\"\"\n        Configure wandb, open a new run and log the configuration.\n        \"\"\"\n        config=omegaconf.OmegaConf.to_container(\n            self.args, resolve=True, throw_on_missing=True\n        )\n        self.wandb_run=wandb.init(project=\"testing\"+self.args.tester.name, entity=self.args.exp.wandb.entity, config=config)\n        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n        self.use_wandb=True\n\n    def setup_wandb_run(self, run):\n        #get the wandb run object from outside (in trainer.py or somewhere else)\n        self.wandb_run=run\n        self.use_wandb=True\n\n    def setup_sampler(self):\n        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network,  diff_params=self.diff_params, args=self.args, rid=True) #rid is used to log some extra information\n\n    \n    def load_latest_checkpoint(self ):\n        #load the latest checkpoint from self.args.model_dir\n        try:\n            # find latest checkpoint_id\n            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n            save_name = f\"{self.args.model_dir}/{save_basename}\"\n            list_weights = glob(save_name)\n            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n            list_ids = [int(id_regex.search(weight_path).groups()[0])\n                        for weight_path in list_weights]\n            checkpoint_id = max(list_ids)\n\n            state_dict = torch.load(\n                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n            self.network.load_state_dict(state_dict['ema'])\n            print(f\"Loaded checkpoint {checkpoint_id}\")\n            return True\n        except (FileNotFoundError, ValueError):\n            raise ValueError(\"No checkpoint found\")\n\n\n    def load_checkpoint(self, path):\n        state_dict = torch.load(path, map_location=self.device)\n        if self.args.exp.exp_name==\"diffwave-sr\":\n            print(state_dict.keys())\n            print(\"noise_schedukar\",state_dict[\"noise_scheduler\"])\n            self.network.load_state_dict(state_dict['ema_model'])\n            self.network.eval()\n            print(\"ckpt loaded\")\n        else:\n            try:\n                print(\"load try 1\")\n                self.network.load_state_dict(state_dict['ema'])\n            except:\n                #self.network.load_state_dict(state_dict['model'])\n                try:\n                    print(\"load try 2\")\n                    dic_ema = {}\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n                        dic_ema[key] = tensor\n                    self.network.load_state_dict(dic_ema)\n                except:\n                    print(\"load try 3\")\n                    dic_ema = {}\n                    i=0\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n                        if tensor.requires_grad:\n                            dic_ema[key]=state_dict['ema_weights'][i]\n                            i=i+1\n                        else:\n                            dic_ema[key]=tensor     \n                    self.network.load_state_dict(dic_ema)\n        try:\n            self.it=state_dict['it']\n        except:\n            self.it=0\n\n    def log_filter(self,preds, f, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n\n        fig_filter=utils_logging.plot_batch_of_lines(preds, f)\n\n        self.wandb_run.log({\"filters_\"+str(string): fig_filter}, step=self.it, commit=True)\n\n    def log_audio(self,preds, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=self.args.model_dir)\n        print(audio_path)\n        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it, commit=False)\n        #TODO: log spectrogram of the audio file to wandb\n        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\n        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it, commit=True)\n\n    def sample_unconditional_diffwavesr(self):\n        #print some parameters of self.network\n        #print(\"self.network\", self.network.input_projection[0].weight)\n        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        rid=False\n        z_1=torch.randn(shape, device=self.device)\n        #print(\"sd\",z_1.std(-1))\n        outputs=self.sampler.diff_params.reverse_process_ddim(z_1, self.network)\n        preds=outputs\n\n        self.log_audio(preds.detach(), \"unconditional\")\n\n        return preds\n    def sample_unconditional(self):\n        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        rid=False\n        outputs=self.sampler.predict_unconditional(shape, self.device, rid=rid)\n        if rid:\n            preds, data_denoised, t=outputs\n            fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"unconditional_signal_generation\")\n        else:\n            preds=outputs\n\n        self.log_audio(preds, \"unconditional\")\n\n        return preds\n\n\n    def formal_test_bwe_small(self, typefilter=\"fc_A\", test_filter_fit=False, compute_sweep=False, blind=False):\n        print(\"BLIND\", blind)\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        else:\n            raise NotImplementedError\n        \n        path=self.args.tester.formal_test_small.path\n        path_out=self.args.tester.formal_test_small.path_out\n\n        filenames=glob(path+\"/*.wav\")\n        assert len(filenames)>0, \"No examples found in path \"+path\n\n        for filename in filenames:\n            path, basename=os.path.split(filename)\n            n=os.path.splitext(basename)[0]\n            print(path, basename)\n            #open audio file\n            d,fs=sf.read(filename)\n            seg=torch.Tensor(d).to(self.device).unsqueeze(0)\n            assert fs==self.args.exp.sample_rate, \"Sample rate of audio file is not consistent with the one specified in the config file\"\n            assert seg.shape[-1]==self.args.exp.audio_len, \"Audio length of audio file is not consistent with the one specified in the config file\"\n\n            print(\"skippint?\", os.path.join(path_out,\"reconstructed\", basename))\n            if os.path.exists(os.path.join(path_out, \"reconstructed\",basename)):\n                print(\"yes skippint\", os.path.join(path_out, basename))\n                continue\n            print(\"skippint?\", os.path.join(path_out,\"reconstructed\", basename+\".wav\"))\n            if os.path.exists(os.path.join(path_out, \"reconstructed\",basename+\".wav\")):\n                print(\"yes skippint\", os.path.join(path_out, basename+\".wav\"))\n                continue\n\n            if type==\"fc_A\":\n                y=self.apply_lowpass_fcA(seg, da_filter)\n            else:\n                raise NotImplementedError\n                y=self.apply_low_pass(D, da_filter, type)\n\n            rid=True\n            if blind:\n                outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n            else:\n                rid=False\n                outputs=self.sampler.predict_bwe(y, da_filter, type, rid=rid)\n           \n            if rid:\n                pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n                #the logged outputs are:\n                #   pred: the reconstructed audio\n                #   estimated_filter: the estimated filter ([fc, A])\n                #   t: the time step vector\n                #   data_denoised: a vector with the denoised audio for each time step\n                #   data_filters: a vector with the estimated filters for each time step\n            else:\n                if blind:\n                    pred, estimated_filter =outputs\n                else:\n                    pred= outputs\n           \n           \n            #y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\n            #path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n           \n            path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=os.path.join(path_out, \"degraded\"))\n           \n            path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=os.path.join(path_out, \"reconstructed\"))\n\n            if blind:\n                with open(os.path.join(path_out,\"filters\", n+\".filter_data.pkl\"), \"wb\") as f:\n                    pickle.dump(estimated_filter, f)\n\n                freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n                H_true=blind_bwe_utils.design_filter(da_filter[0], da_filter[1], freqs)\n                H_Pred=blind_bwe_utils.design_filter(estimated_filter[0], estimated_filter[1], freqs)\n\n                #compute dB MSE between the true and the estimated filter\n            \n                dB_MSE=torch.mean((20*torch.log10(H_true)-20*torch.log10(H_Pred))**2)\n                print(\"dB MSE\", dB_MSE)\n\n\n\n    def formal_test_bwe(self, typefilter=\"firwin\", test_filter_fit=False, compute_sweep=False, blind=False):\n        print(\"BLIND\", blind)\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n        #test_bwe_table_audio = wandb.Table(columns=columns)\n        if not self.do_formal_test_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        elif typefilter==\"3rdoct\":\n            type=\"3rdoct\"\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n            da_filter=da_filter.to(self.device)\n        else:\n            type=self.args.tester.bandwidth_extension.filter.type\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n            da_filter=da_filter.to(self.device)\n\n\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\n        path=self.args.tester.formal_test.path\n        filenames=glob(path+\"/*.wav\")\n\n        segL=self.args.exp.audio_len\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n\n        for filename in filenames:\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n            #open audio file\n            d,fs=sf.read(filename)\n            D=torch.Tensor(d).to(self.device).unsqueeze(0)\n            print(\"D\", D.shape, fs)\n\n\n            path_out=self.args.tester.formal_test.folder\n\n            print(\"skippint?\", os.path.join(path_out, basename))\n            if os.path.exists(os.path.join(path_out, basename)):\n                print(\"yes skippint\", os.path.join(path_out, basename))\n                continue\n            print(\"skippint?\", os.path.join(path_out, basename+\".wav\"))\n            if os.path.exists(os.path.join(path_out, basename+\".wav\")):\n                print(\"yes skippint\", os.path.join(path_out, basename+\".wav\"))\n                continue\n\n            if type==\"fc_A\":\n                degraded=self.apply_lowpass_fcA(D, da_filter)\n            else:\n                degraded=self.apply_low_pass(D, da_filter, type)\n            #path_degraded=utils_logging.write_audio_file(degraded, self.args.exp.sample_rate, basename+\".degraded.wav\", path=path_out)\n\n            print(\"filename\",filename)\n\n            #n=os.path.splitext(os.path.basename(filename))[0]+typefilter+str(self.args.tester.bandwidth_extension.filter.fc)\n            n=os.path.splitext(os.path.basename(filename))[0]\n\n            #degraded=degraded.float().to(self.device).unsqueeze(0)\n            print(n)\n            final_pred=torch.zeros_like(degraded)\n    \n            print(\"dsds FS\",fs)\n    \n            print(\"seg shape\",degraded.shape)\n            degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n            print(\"seg shape\",degraded.shape)\n    \n            std= degraded.std(-1)\n    \n            rid=False\n\n    \n            L=degraded.shape[-1]\n            #modify the sampler, so that it is computationally cheaper\n    \n            discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n            discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n    \n            #first segment\n            ix=0\n            seg=degraded[...,ix:ix+segL]\n            #pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n            filter_data=[]\n\n            rid=False\n            if blind:\n                outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                pred, estimated_filter =outputs\n                filter_data.append(((ix, ix+segL), estimated_filter))\n\n            else:\n                pred=self.sampler.predict_bwe(seg, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False)\n    \n            if self.args.tester.formal_test.use_AR:\n                assert not blind\n                previous_pred=pred[..., 0:segL-discard_end]\n                final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\n                ix+=segL-overlap-discard_end\n     \n                y_masked=torch.zeros_like(pred, device=self.device)\n                mask=torch.ones_like(seg, device=self.device)\n                mask[...,overlap::]=0\n            else:\n                print(\"noar\")\n                hann_window=torch.hann_window(self.args.tester.formal_test.OLA*2, device=self.device)\n                win_pred=pred[...,0:segL-discard_end]\n                win_pred[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n                print(\"ix\", ix, \"segL\", segL, \"discard_end\", discard_end, \"win pred shape\", win_pred.shape)\n                final_pred[...,ix:ix+segL-discard_end]=win_pred\n\n                ix+=segL-discard_end-self.args.tester.formal_test.OLA\n    \n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n\n            if blind:\n                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                        pickle.dump(filter_data, f)\n\n            while ix<L-segL-discard_end-discard_start:\n\n                seg=degraded[...,ix:ix+segL]\n                if self.args.tester.formal_test.use_AR:\n                    y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n                    pred=self.sampler.predict_bwe_AR(seg, y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n                else:\n                    if blind:\n                        outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                        pred, estimated_filter =outputs\n                        filter_data.append(((ix, ix+segL), estimated_filter))\n                    else:\n                        pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n    \n                previous_pred_win=pred[..., 0:segL-discard_end]\n                previous_pred_win[..., 0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n                previous_pred_win[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n    \n    \n                final_pred[...,ix:ix+segL-discard_end]+=previous_pred_win\n\n                #do a little bit of overlap and add with a hann window to avoid discontinuities\n                #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n                #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n    \n                path, basename=os.path.split(filename)\n                print(path, basename)\n    \n                path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n    \n                if self.args.tester.formal_test.use_AR:\n\n                    ix+=segL-overlap-discard_end\n                else:\n                    ix+=segL-discard_end-self.args.tester.formal_test.OLA\n\n                if blind:\n                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                        pickle.dump(filter_data, f)\n    \n            #skipping the last segment, which is not complete, I am lazy\n            seg=degraded[...,ix::]\n\n            if self.args.tester.formal_test.use_AR:\n                y_masked[...,0:overlap]=pred[...,-overlap::]\n    \n            if seg.shape[-1]<segL:\n                #cat zeros\n                seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n    \n                if self.args.tester.formal_test.use_AR:\n                    #the cat zeroes will also be part of the observed signal, so I need to mask them\n                    y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n                    mask[...,seg.shape[-1]:segL]=0\n    \n            else:\n                seg_zp=seg[...,0:segL]\n    \n    \n            if self.args.tester.formal_test.use_AR:\n                pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n            else:\n                if blind:\n                    outputs=self.sampler.predict_blind_bwe(seg_zp, rid=False)\n                    pred, estimated_filter =outputs\n                    filter_data.append(((ix, ix+segL), estimated_filter))\n                else:\n                    pred=self.sampler.predict_bwe(seg_zp, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n                \n            \n            if not self.args.tester.formal_test.use_AR:\n                win_pred=pred[...,0:seg.shape[-1]]\n                win_pred[...,0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n                final_pred[...,ix::]+=win_pred\n            else:\n                final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n    \n            #final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n            #final_pred=final_pred*10**(-scale/20)\n    \n            #extract path from filename\n    \n    \n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".wav\", path=path_out)\n            #save filter_data in a pickle file\n            with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                pickle.dump(filter_data, f)\n\n               \n\n\n    def test_bwe(self, typefilter=\"fc_A\", test_filter_fit=False, compute_sweep=False):\n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n        test_bwe_table_audio = wandb.Table(columns=columns)\n\n        if not self.do_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        elif typefilter==\"3rdoct\":\n            type=\"3rdoct\"\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n            da_filter=da_filter.to(self.device)\n        else:\n            type=self.args.tester.bandwidth_extension.filter.type\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n            da_filter=da_filter.to(self.device)\n\n\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n            n=os.path.splitext(filename[0])[0]\n            seg=original.float().to(self.device)\n\n            seg=self.resample_audio(seg, fs)\n\n\n            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n            #        print(\"gain boost\", self.args.tester.bandwidth_extension.gain_boost)\n            #        #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n            #        #add gain boost (in dB)\n            #        seg=seg*10**(self.args.tester.bandwidth_extension.gain_boost/20)\n\n            if type==\"fc_A\":\n                y=self.apply_lowpass_fcA(seg, da_filter)\n            else:\n                y=self.apply_low_pass(seg, da_filter, type)\n            #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type, typefilter) \n\n            #if self.args.tester.bandwidth_extension.sigma_observations != \"None\":\n            #    sigma=self.args.tester.bandwidth_extension.sigma_observations\n            #    y+=sigma*torch.randn(y.shape).to(y.device)\n\n            if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n\n            print(\"y\", y.shape)\n            if test_filter_fit:\n                if compute_sweep:\n                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True, compute_sweep=True)\n                    pred, data_denoised, data_score, t, data_filters, data_norms, data_grads =out\n                    #save the data_norms and data_grads as a .npy file\n                    np.save(self.paths[\"bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                    np.save(self.paths[\"bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                else:\n                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True)\n                    pred, data_denoised, data_score, t, data_filters =out\n            else:\n                rid=True\n                out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=False, compute_sweep=False)\n                \n                pred, data_denoised, data_score, t =out\n\n\n            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n            #    #compensate gain boost\n            #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n            #    #add gain boost (in dB)\n            #    pred=pred*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n            #    seg=seg*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n            #    y=y*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\n            res[i,:]=pred\n       \n            path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"original\"])\n\n            path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"degraded\"])\n\n            path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"reconstructed\"])\n\n            test_bwe_table_audio.add_data(i, \n                    wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                    wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                    wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\n            if rid:\n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n            if test_filter_fit:\n\n                #expecting to crash here\n                print(data_filters.shape)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\n        self.wandb_run.log({\"table_bwe_audio\": test_bwe_table_audio}, commit=True) \n\n        if self.use_wandb:\n            self.log_audio(res, \"bwe\")\n\n    def apply_low_pass(self, seg, filter, typefilter):\n        y=utils_bwe.apply_low_pass(seg, filter, self.args.tester.bandwidth_extension.filter.type) \n        return y\n\n    def apply_lowpass_fcA(self, seg, params):\n        freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n        H=blind_bwe_utils.design_filter(params[0], params[1], freqs)\n        xfilt=blind_bwe_utils.apply_filter(seg,H,self.args.tester.blind_bwe.NFFT)\n        return xfilt\n\n    def prepare_filter(self, sample_rate, typefilter):\n        filter=utils_bwe.prepare_filter(self.args, sample_rate )\n        return filter\n    \n    def test_real_blind_bwe_complete(self, typefilter=\"fc_A\", compute_sweep=False):\n        #raise NotImplementedError\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        \n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        filename=self.args.tester.complete_recording.path\n        d,fs=sf.read(filename)\n        degraded=torch.Tensor(d)\n\n        segL=self.args.exp.audio_len\n\n        ix_first=self.args.exp.sample_rate*self.args.tester.complete_recording.ix_start #index of the first segment to be processed, might have to depend on the sample rate\n\n        #for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\n        print(\"filename\",filename)\n        n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n        degraded=degraded.float().to(self.device).unsqueeze(0)\n        print(n)\n\n        print(\"dsds FS\",fs)\n\n        print(\"seg shape\",degraded.shape)\n        degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n        print(\"seg shape\",degraded.shape)\n\n        std= degraded.std(-1)\n        degraded=self.args.tester.complete_recording.std*degraded/std.unsqueeze(-1)\n        #add noise\n        if self.args.tester.complete_recording.SNR_extra_noise!=\"None\":\n            #contaminate a bit with white noise\n            SNR=10**(self.args.tester.complete_recording.SNR_extra_noise/10)\n            sigma2_s=torch.Tensor([self.args.tester.complete_recording.std**2]).to(degraded.device)\n            sigma=torch.sqrt(sigma2_s/SNR)\n            degraded+=sigma*torch.randn(degraded.shape).to(degraded.device)\n\n\n        if self.args.tester.complete_recording.n_segments_blindstep==1:\n            y=degraded[...,ix_first:ix_first+segL]\n        else:\n            #initialize y with the first segment and repeat it\n            y=degraded[...,ix_first:ix_first+segL].repeat(self.args.tester.complete_recording.n_segments_blindstep,1)\n            for j in range(0, self.args.tester.complete_recording.n_segments_blindstep):\n                #random index\n                ix=np.random.randint(0, degraded.shape[-1]-segL)\n                y[j,...]=degraded[...,ix:ix+segL]\n        \n        print(\"y shape\",y.shape)\n\n            \n\n        #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, fs)\n        #print(\"scale\",scale) #TODO I should calculate this with the whole track, not just the first segment\n\n        #y=y*10**(scale/20)\n        #degraded=degraded*10**(scale/20)\n\n\n\n        rid=False\n        outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n        pred, estimated_filter =outputs\n\n        #now I will just throw away the first segment and process the rest of the signal with the estimated filter. Later I should think of a better way to do it\n\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n        hop=segL-overlap\n\n        final_pred=torch.zeros_like(degraded)\n        final_pred[0, ix_first:ix_first+segL]=pred[0]\n\n        path, basename=os.path.split(filename)\n        print(path, basename)\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n        L=degraded.shape[-1]\n\n        #modify the sampler, so that it is computationally cheaper\n\n        discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n        discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\n        #first segment\n        ix=0\n        seg=degraded[...,ix:ix+segL]\n        pred=self.sampler.predict_bwe(seg, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n        previous_pred=pred[..., 0:segL-discard_end]\n\n        final_pred[...,ix:ix+segL-discard_end]=previous_pred\n        ix+=segL-overlap-discard_end\n\n        y_masked=torch.zeros_like(pred, device=self.device)\n        mask=torch.ones_like(seg, device=self.device)\n        mask[...,overlap::]=0\n\n        hann_window=torch.hann_window(overlap*2, device=self.device)\n\n        while ix<L-segL-discard_end-discard_start:\n            y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n            seg=degraded[...,ix:ix+segL]\n\n            pred=self.sampler.predict_bwe_AR(seg, y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n            previous_pred=pred[..., 0:segL-discard_end]\n\n\n            final_pred[...,ix:ix+segL-discard_end]=previous_pred\n            #do a little bit of overlap and add with a hann window to avoid discontinuities\n            #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n            #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n\n\n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n            ix+=segL-overlap-discard_end\n\n        #skipping the last segment, which is not complete, I am lazy\n        seg=degraded[...,ix::]\n        y_masked[...,0:overlap]=pred[...,-overlap::]\n\n        if seg.shape[-1]<segL:\n            #cat zeros\n            seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\n            #the cat zeroes will also be part of the observed signal, so I need to mask them\n            y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n            mask[...,seg.shape[-1]:segL]=0\n\n        else:\n            seg_zp=seg[...,0:segL]\n\n\n        pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n        final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\n        final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n        #final_pred=final_pred*10**(-scale/20)\n\n        #extract path from filename\n\n\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n               \n               \n               \n    def test_real_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\",\"degraded_audio\", \"reconstructed audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n        \n        \n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        path=self.args.tester.blind_bwe.real_recordings.path\n        audio_files=glob(path+\"/*.wav\")\n        test_set_data=[]\n        test_set_fs=[]\n        test_set_names=[]\n        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n            d,fs=sf.read(audio_files[i])\n            test_set_data.append(torch.Tensor(d))\n            print(\"fs\",fs)\n            print(\"len\",len(d))\n            test_set_names.append(audio_files[i])\n\n        for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n                print(\"filename\",filename)\n                n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n                seg=degraded.float().to(self.device).unsqueeze(0)\n                print(n)\n\n                print(\"dsds FS\",fs)\n\n                print(\"seg shape\",seg.shape)\n                seg=torchaudio.functional.resample(seg, fs, self.args.exp.sample_rate)\n                print(\"seg shape\",seg.shape)\n                ix_start=self.args.tester.blind_bwe\n\n                seg=seg[...,self.args.exp.sample_rate*5:self.args.exp.sample_rate*5+self.args.exp.audio_len]\n                y=seg\n                print(\"y shape\",y.shape)\n                #normalize???\n                std= y.std(-1)\n                y=self.args.tester.blind_bwe.sigma_norm*y/std.unsqueeze(-1)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y,fs)\n                #print(\"scale\",scale)\n                #y=y*10**(scale/20)\n\n\n\n               \n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n                pred=pred*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                y=y*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"reconstructed\"])\n               \n               \n               \n                fig_est_filter=blind_bwe_utils.plot_filter(estimated_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"real_blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n               \n                if typefilter==\"fc_A\":\n                    test_blind_bwe_table_filters.add_data(i, \n                        wandb.Html(path_est_filter),\n                    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"real_blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"real_blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    def test_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            fc=self.args.tester.blind_bwe.test_filter.fc\n            A=self.args.tester.blind_bwe.test_filter.A\n            da_filter=torch.Tensor([fc, A]).to(self.device)\n        else:\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate, typefilter) #standardly designed filter\n            da_filter=da_filter.to(self.device)\n        \n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n                n=os.path.splitext(filename[0])[0]+typefilter\n                seg=original.float().to(self.device)\n                seg=self.resample_audio(seg, fs)\n\n                #if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                #    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                #    orig_std=seg.std(-1)\n                #    seg=sigma_norm*seg/orig_std\n        \n                #elif self.args.tester.blind_bwe.gain_boost != 0:\n                #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                #    #add gain boost (in dB)\n                #    seg=seg*10**(self.args.tester.blind_bwe.gain_boost/20)\n                \n\n                #apply lowpass filter\n                if typefilter==\"fc_A\":\n                    y=self.apply_lowpass_fcA(seg, da_filter)\n                else:\n                    y=self.apply_low_pass(seg,da_filter, typefilter)\n\n                #add noise to the observations for regularization\n                if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, self.args.exp.sample_rate)\n                #print(\"applied scale\",scale)\n                #y=y*10**(scale/20)\n\n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n               \n                y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\n                #if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                #    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                #    assert orig_std is not None\n                #    seg=orig_std*seg/sigma_norm\n                #elif self.args.tester.blind_bwe.gain_boost != 0:\n                #    #compensate gain boost\n                #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                #    #add gain boost (in dB)\n                #    y_est=y_est*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                #    pred=pred*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                #    seg=seg*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                #    y=y*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n               \n                \n                path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n               \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"reconstructed\"])\n               \n                path_degrade_estimate=utils_logging.write_audio_file(y_est, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded_estimate\"])\n               \n               \n                #will probably crash here!\n                fig_est_filter=blind_bwe_utils.plot_filter(da_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_degrade_estimate, sample_rate=self.args.exp.sample_rate))\n               \n                #if typefilter==\"fc_A\":\n                #    test_blind_bwe_table_filters.add_data(i, \n                #        wandb.Html(path_est_filter),\n                #    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters.shape)\n                #will crash here\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    \n        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n    def dodajob(self):\n        self.setup_wandb()\n        for m in self.args.tester.modes:\n\n            if m==\"unconditional\":\n                print(\"testing unconditional\")\n                self.sample_unconditional()\n            if m==\"unconditional_diffwavesr\":\n                print(\"testing unconditional\")\n                self.sample_unconditional_diffwavesr()\n            self.it+=1\n            if m==\"blind_bwe\":\n                print(\"TESTING BLIND BWE\")\n                self.test_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe\":\n                print(\"TESTING REAL BLIND BWE\")\n                self.test_real_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe_complete\":\n                #process the whole audio file\n                #Estimate the filter in the first chunk, and then apply it to the rest of the audio file (using a little bit of overlap or outpainting)\n                print(\"TESTING REAL BLIND BWE COMPLETE\")\n                self.test_real_blind_bwe_complete(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep)\n            if m==\"formal_test_bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.formal_test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"firwin\", blind=self.args.tester.formal_test.blind)\n            if m==\"formal_test_bwe_small\": \n                print(\"TESTING NORMAL BWE\")\n                self.formal_test_bwe_small(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"fc_A\", blind=self.args.tester.formal_test.blind)\n        self.it+=1", "\nclass BlindTester():\n    def __init__(\n        self, args=None, network=None, diff_params=None, test_set=None, device=None, it=None\n    ):\n        self.args=args\n        self.network=torch.compile(network)\n        #self.network=network\n        #prnt number of parameters\n        \n\n        self.diff_params=copy.copy(diff_params)\n        self.device=device\n        #choose gpu as the device if possible\n        if self.device is None:\n            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.network=network\n\n        torch.backends.cudnn.benchmark = True\n\n        today=date.today() \n        if it is None:\n            self.it=0\n\n        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n        if not os.path.exists(self.path_sampling):\n            os.makedirs(self.path_sampling)\n\n\n        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n        self.setup_sampler()\n\n        self.use_wandb=False #hardcoded for now\n\n        S=self.args.exp.resample_factor\n        if S>2.1 and S<2.2:\n            #resampling 48k to 22.05k\n            self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n        elif S!=1:\n            N=int(self.args.exp.audio_len*S)\n            self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\n        if test_set is not None:\n            self.test_set=test_set\n            self.do_inpainting=True\n            self.do_bwe=True\n            self.do_blind_bwe=True\n        else:\n            self.test_set=None\n            self.do_inpainting=False\n            self.do_bwe=False #these need to be set up in the config file\n            self.do_blind_bwe=False\n\n        self.paths={}\n        if self.do_inpainting and (\"inpainting\" in self.args.tester.modes):\n            self.do_inpainting=True\n            mode=\"inpainting\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"inpainting\",\"masked\",\"inpainted\")\n            #TODO add more information in the subirectory names\n        else: self.do_inpainting=False\n\n        if self.do_bwe and (\"bwe\" in self.args.tester.modes):\n            self.do_bwe=True\n            mode=\"bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"bwe\",\"lowpassed\",\"bwe\")\n            #TODO add more information in the subirectory names\n        else:\n            self.do_bwe=False\n\n        if self.do_blind_bwe and (\"blind_bwe\" in self.args.tester.modes):\n            self.do_blind_bwe=True\n            mode=\"blind_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"], self.paths[mode+\"degraded_estimate\"]=self.prepare_blind_experiment(\"blind_bwe\",\"masked\",\"blind_bwe\",\"degraded_estimate\")\n            #TODO add more information in the subirectory names\n        if \"real_blind_bwe\" in self.args.tester.modes:\n            self.do_blind_bwe=True\n            mode=\"real_blind_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"real_blind_bwe\",\"degraded\",\"reconstructed\")\n            #TODO add more information in the subirectory names\n\n        if \"formal_test_bwe\" in self.args.tester.modes:\n            self.do_formal_test_bwe=True\n            mode=\"formal_test_bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"formal_test_bwe\",\"degraded\",\"reconstructed\")\n        if \"formal_test_bwe_small\" in self.args.tester.modes:\n            self.do_formal_test_bwe=True\n            mode=\"formal_test_bwe_small\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"formal_test_bwe_small\",\"degraded\",\"reconstructed\")\n        \n        if (\"unconditional\" in self.args.tester.modes):\n            mode=\"unconditional\"\n            self.paths[mode]=self.prepare_unc_experiment(\"unconditional\")\n\n\n        if (\"filter_bwe\" in self.args.tester.modes):\n            mode=\"filter_bwe\"\n            self.paths[mode]=self.prepare_unc_experiment(\"filter_bwe\")\n\n        #self.LTAS_processor=LTAS_processor(self.args.tester.blind_bwe.LTAS.sample_rate,self.args.tester.blind_bwe.LTAS.audio_len)\n        #self.LTAS_processor.load_dataset_LTAS(self.args.tester.blind_bwe.LTAS.path)\n\n    def prepare_unc_experiment(self, str):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n            return path_exp\n\n    def prepare_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n\n            return path_exp, path_degraded, path_original, path_reconstructed\n\n    def resample_audio(self, audio, fs):\n        #this has been reused from the trainer.py\n        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\n    def prepare_blind_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"reconstructed\", str_degraded_estimate=\"degraded_estimate\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n            \n            n=str_degraded_estimate\n            path_degraded_estimate=os.path.join(path_exp, n) #path for the estimated degraded signal\n            #ensure the path exists\n            if not os.path.exists(path_degraded_estimate):\n                os.makedirs(path_degraded_estimate)\n\n            return path_exp, path_degraded, path_original, path_reconstructed, path_degraded_estimate\n\n    def setup_wandb(self):\n        \"\"\"\n        Configure wandb, open a new run and log the configuration.\n        \"\"\"\n        config=omegaconf.OmegaConf.to_container(\n            self.args, resolve=True, throw_on_missing=True\n        )\n        self.wandb_run=wandb.init(project=\"testing\"+self.args.tester.name, entity=self.args.exp.wandb.entity, config=config)\n        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n        self.use_wandb=True\n\n    def setup_wandb_run(self, run):\n        #get the wandb run object from outside (in trainer.py or somewhere else)\n        self.wandb_run=run\n        self.use_wandb=True\n\n    def setup_sampler(self):\n        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network,  diff_params=self.diff_params, args=self.args, rid=True) #rid is used to log some extra information\n\n    \n    def load_latest_checkpoint(self ):\n        #load the latest checkpoint from self.args.model_dir\n        try:\n            # find latest checkpoint_id\n            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n            save_name = f\"{self.args.model_dir}/{save_basename}\"\n            list_weights = glob(save_name)\n            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n            list_ids = [int(id_regex.search(weight_path).groups()[0])\n                        for weight_path in list_weights]\n            checkpoint_id = max(list_ids)\n\n            state_dict = torch.load(\n                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n            self.network.load_state_dict(state_dict['ema'])\n            print(f\"Loaded checkpoint {checkpoint_id}\")\n            return True\n        except (FileNotFoundError, ValueError):\n            raise ValueError(\"No checkpoint found\")\n\n\n    def load_checkpoint(self, path):\n        state_dict = torch.load(path, map_location=self.device)\n        if self.args.exp.exp_name==\"diffwave-sr\":\n            print(state_dict.keys())\n            print(\"noise_schedukar\",state_dict[\"noise_scheduler\"])\n            self.network.load_state_dict(state_dict['ema_model'])\n            self.network.eval()\n            print(\"ckpt loaded\")\n        else:\n            try:\n                print(\"load try 1\")\n                self.network.load_state_dict(state_dict['ema'])\n            except:\n                #self.network.load_state_dict(state_dict['model'])\n                try:\n                    print(\"load try 2\")\n                    dic_ema = {}\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n                        dic_ema[key] = tensor\n                    self.network.load_state_dict(dic_ema)\n                except:\n                    print(\"load try 3\")\n                    dic_ema = {}\n                    i=0\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n                        if tensor.requires_grad:\n                            dic_ema[key]=state_dict['ema_weights'][i]\n                            i=i+1\n                        else:\n                            dic_ema[key]=tensor     \n                    self.network.load_state_dict(dic_ema)\n        try:\n            self.it=state_dict['it']\n        except:\n            self.it=0\n\n    def log_filter(self,preds, f, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n\n        fig_filter=utils_logging.plot_batch_of_lines(preds, f)\n\n        self.wandb_run.log({\"filters_\"+str(string): fig_filter}, step=self.it, commit=True)\n\n    def log_audio(self,preds, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=self.args.model_dir)\n        print(audio_path)\n        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it, commit=False)\n        #TODO: log spectrogram of the audio file to wandb\n        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\n        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it, commit=True)\n\n    def sample_unconditional_diffwavesr(self):\n        #print some parameters of self.network\n        #print(\"self.network\", self.network.input_projection[0].weight)\n        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        rid=False\n        z_1=torch.randn(shape, device=self.device)\n        #print(\"sd\",z_1.std(-1))\n        outputs=self.sampler.diff_params.reverse_process_ddim(z_1, self.network)\n        preds=outputs\n\n        self.log_audio(preds.detach(), \"unconditional\")\n\n        return preds\n    def sample_unconditional(self):\n        shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        rid=False\n        outputs=self.sampler.predict_unconditional(shape, self.device, rid=rid)\n        if rid:\n            preds, data_denoised, t=outputs\n            fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"unconditional_signal_generation\")\n        else:\n            preds=outputs\n\n        self.log_audio(preds, \"unconditional\")\n\n        return preds\n\n\n    def formal_test_bwe_small(self, typefilter=\"fc_A\", test_filter_fit=False, compute_sweep=False, blind=False):\n        print(\"BLIND\", blind)\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        else:\n            raise NotImplementedError\n        \n        path=self.args.tester.formal_test_small.path\n        path_out=self.args.tester.formal_test_small.path_out\n\n        filenames=glob(path+\"/*.wav\")\n        assert len(filenames)>0, \"No examples found in path \"+path\n\n        for filename in filenames:\n            path, basename=os.path.split(filename)\n            n=os.path.splitext(basename)[0]\n            print(path, basename)\n            #open audio file\n            d,fs=sf.read(filename)\n            seg=torch.Tensor(d).to(self.device).unsqueeze(0)\n            assert fs==self.args.exp.sample_rate, \"Sample rate of audio file is not consistent with the one specified in the config file\"\n            assert seg.shape[-1]==self.args.exp.audio_len, \"Audio length of audio file is not consistent with the one specified in the config file\"\n\n            print(\"skippint?\", os.path.join(path_out,\"reconstructed\", basename))\n            if os.path.exists(os.path.join(path_out, \"reconstructed\",basename)):\n                print(\"yes skippint\", os.path.join(path_out, basename))\n                continue\n            print(\"skippint?\", os.path.join(path_out,\"reconstructed\", basename+\".wav\"))\n            if os.path.exists(os.path.join(path_out, \"reconstructed\",basename+\".wav\")):\n                print(\"yes skippint\", os.path.join(path_out, basename+\".wav\"))\n                continue\n\n            if type==\"fc_A\":\n                y=self.apply_lowpass_fcA(seg, da_filter)\n            else:\n                raise NotImplementedError\n                y=self.apply_low_pass(D, da_filter, type)\n\n            rid=True\n            if blind:\n                outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n            else:\n                rid=False\n                outputs=self.sampler.predict_bwe(y, da_filter, type, rid=rid)\n           \n            if rid:\n                pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n                #the logged outputs are:\n                #   pred: the reconstructed audio\n                #   estimated_filter: the estimated filter ([fc, A])\n                #   t: the time step vector\n                #   data_denoised: a vector with the denoised audio for each time step\n                #   data_filters: a vector with the estimated filters for each time step\n            else:\n                if blind:\n                    pred, estimated_filter =outputs\n                else:\n                    pred= outputs\n           \n           \n            #y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\n            #path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n           \n            path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=os.path.join(path_out, \"degraded\"))\n           \n            path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=os.path.join(path_out, \"reconstructed\"))\n\n            if blind:\n                with open(os.path.join(path_out,\"filters\", n+\".filter_data.pkl\"), \"wb\") as f:\n                    pickle.dump(estimated_filter, f)\n\n                freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n                H_true=blind_bwe_utils.design_filter(da_filter[0], da_filter[1], freqs)\n                H_Pred=blind_bwe_utils.design_filter(estimated_filter[0], estimated_filter[1], freqs)\n\n                #compute dB MSE between the true and the estimated filter\n            \n                dB_MSE=torch.mean((20*torch.log10(H_true)-20*torch.log10(H_Pred))**2)\n                print(\"dB MSE\", dB_MSE)\n\n\n\n    def formal_test_bwe(self, typefilter=\"firwin\", test_filter_fit=False, compute_sweep=False, blind=False):\n        print(\"BLIND\", blind)\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n        #test_bwe_table_audio = wandb.Table(columns=columns)\n        if not self.do_formal_test_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        elif typefilter==\"3rdoct\":\n            type=\"3rdoct\"\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n            da_filter=da_filter.to(self.device)\n        else:\n            type=self.args.tester.bandwidth_extension.filter.type\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n            da_filter=da_filter.to(self.device)\n\n\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n\n        path=self.args.tester.formal_test.path\n        filenames=glob(path+\"/*.wav\")\n\n        segL=self.args.exp.audio_len\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n\n        for filename in filenames:\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n            #open audio file\n            d,fs=sf.read(filename)\n            D=torch.Tensor(d).to(self.device).unsqueeze(0)\n            print(\"D\", D.shape, fs)\n\n\n            path_out=self.args.tester.formal_test.folder\n\n            print(\"skippint?\", os.path.join(path_out, basename))\n            if os.path.exists(os.path.join(path_out, basename)):\n                print(\"yes skippint\", os.path.join(path_out, basename))\n                continue\n            print(\"skippint?\", os.path.join(path_out, basename+\".wav\"))\n            if os.path.exists(os.path.join(path_out, basename+\".wav\")):\n                print(\"yes skippint\", os.path.join(path_out, basename+\".wav\"))\n                continue\n\n            if type==\"fc_A\":\n                degraded=self.apply_lowpass_fcA(D, da_filter)\n            else:\n                degraded=self.apply_low_pass(D, da_filter, type)\n            #path_degraded=utils_logging.write_audio_file(degraded, self.args.exp.sample_rate, basename+\".degraded.wav\", path=path_out)\n\n            print(\"filename\",filename)\n\n            #n=os.path.splitext(os.path.basename(filename))[0]+typefilter+str(self.args.tester.bandwidth_extension.filter.fc)\n            n=os.path.splitext(os.path.basename(filename))[0]\n\n            #degraded=degraded.float().to(self.device).unsqueeze(0)\n            print(n)\n            final_pred=torch.zeros_like(degraded)\n    \n            print(\"dsds FS\",fs)\n    \n            print(\"seg shape\",degraded.shape)\n            degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n            print(\"seg shape\",degraded.shape)\n    \n            std= degraded.std(-1)\n    \n            rid=False\n\n    \n            L=degraded.shape[-1]\n            #modify the sampler, so that it is computationally cheaper\n    \n            discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n            discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n    \n            #first segment\n            ix=0\n            seg=degraded[...,ix:ix+segL]\n            #pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n            filter_data=[]\n\n            rid=False\n            if blind:\n                outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                pred, estimated_filter =outputs\n                filter_data.append(((ix, ix+segL), estimated_filter))\n\n            else:\n                pred=self.sampler.predict_bwe(seg, da_filter, type,rid=rid, test_filter_fit=False, compute_sweep=False)\n    \n            if self.args.tester.formal_test.use_AR:\n                assert not blind\n                previous_pred=pred[..., 0:segL-discard_end]\n                final_pred[...,ix:ix+segL-discard_end]=previous_pred\n\n                ix+=segL-overlap-discard_end\n     \n                y_masked=torch.zeros_like(pred, device=self.device)\n                mask=torch.ones_like(seg, device=self.device)\n                mask[...,overlap::]=0\n            else:\n                print(\"noar\")\n                hann_window=torch.hann_window(self.args.tester.formal_test.OLA*2, device=self.device)\n                win_pred=pred[...,0:segL-discard_end]\n                win_pred[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n                print(\"ix\", ix, \"segL\", segL, \"discard_end\", discard_end, \"win pred shape\", win_pred.shape)\n                final_pred[...,ix:ix+segL-discard_end]=win_pred\n\n                ix+=segL-discard_end-self.args.tester.formal_test.OLA\n    \n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n\n            if blind:\n                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                        pickle.dump(filter_data, f)\n\n            while ix<L-segL-discard_end-discard_start:\n\n                seg=degraded[...,ix:ix+segL]\n                if self.args.tester.formal_test.use_AR:\n                    y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n                    pred=self.sampler.predict_bwe_AR(seg, y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n                else:\n                    if blind:\n                        outputs=self.sampler.predict_blind_bwe(seg, rid=False)\n                        pred, estimated_filter =outputs\n                        filter_data.append(((ix, ix+segL), estimated_filter))\n                    else:\n                        pred=self.sampler.predict_bwe(seg, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n    \n                previous_pred_win=pred[..., 0:segL-discard_end]\n                previous_pred_win[..., 0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n                previous_pred_win[..., -self.args.tester.formal_test.OLA:]*=hann_window[self.args.tester.formal_test.OLA:]\n    \n    \n                final_pred[...,ix:ix+segL-discard_end]+=previous_pred_win\n\n                #do a little bit of overlap and add with a hann window to avoid discontinuities\n                #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n                #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n    \n                path, basename=os.path.split(filename)\n                print(path, basename)\n    \n                path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".partial.wav\", path=path_out)\n    \n                if self.args.tester.formal_test.use_AR:\n\n                    ix+=segL-overlap-discard_end\n                else:\n                    ix+=segL-discard_end-self.args.tester.formal_test.OLA\n\n                if blind:\n                    with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                        pickle.dump(filter_data, f)\n    \n            #skipping the last segment, which is not complete, I am lazy\n            seg=degraded[...,ix::]\n\n            if self.args.tester.formal_test.use_AR:\n                y_masked[...,0:overlap]=pred[...,-overlap::]\n    \n            if seg.shape[-1]<segL:\n                #cat zeros\n                seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n    \n                if self.args.tester.formal_test.use_AR:\n                    #the cat zeroes will also be part of the observed signal, so I need to mask them\n                    y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n                    mask[...,seg.shape[-1]:segL]=0\n    \n            else:\n                seg_zp=seg[...,0:segL]\n    \n    \n            if self.args.tester.formal_test.use_AR:\n                pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n            else:\n                if blind:\n                    outputs=self.sampler.predict_blind_bwe(seg_zp, rid=False)\n                    pred, estimated_filter =outputs\n                    filter_data.append(((ix, ix+segL), estimated_filter))\n                else:\n                    pred=self.sampler.predict_bwe(seg_zp, da_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n                \n            \n            if not self.args.tester.formal_test.use_AR:\n                win_pred=pred[...,0:seg.shape[-1]]\n                win_pred[...,0:self.args.tester.formal_test.OLA]*=hann_window[0:self.args.tester.formal_test.OLA]\n                final_pred[...,ix::]+=win_pred\n            else:\n                final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n    \n            #final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n            #final_pred=final_pred*10**(-scale/20)\n    \n            #extract path from filename\n    \n    \n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, n+\".wav\", path=path_out)\n            #save filter_data in a pickle file\n            with open(os.path.join(path_out, n+\".filter_data.pkl\"), \"wb\") as f:\n                pickle.dump(filter_data, f)\n\n               \n\n\n    def test_bwe(self, typefilter=\"fc_A\", test_filter_fit=False, compute_sweep=False):\n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\"] \n        test_bwe_table_audio = wandb.Table(columns=columns)\n\n        if not self.do_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            type=\"fc_A\"\n            da_filter=torch.Tensor([self.args.tester.blind_bwe.test_filter.fc,self.args.tester.blind_bwe.test_filter.A]).to(self.device)\n        elif typefilter==\"3rdoct\":\n            type=\"3rdoct\"\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,typefilter)\n            da_filter=da_filter.to(self.device)\n        else:\n            type=self.args.tester.bandwidth_extension.filter.type\n            da_filter=self.prepare_filter( self.args.exp.sample_rate,type)\n            da_filter=da_filter.to(self.device)\n\n\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n            n=os.path.splitext(filename[0])[0]\n            seg=original.float().to(self.device)\n\n            seg=self.resample_audio(seg, fs)\n\n\n            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n            #        print(\"gain boost\", self.args.tester.bandwidth_extension.gain_boost)\n            #        #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n            #        #add gain boost (in dB)\n            #        seg=seg*10**(self.args.tester.bandwidth_extension.gain_boost/20)\n\n            if type==\"fc_A\":\n                y=self.apply_lowpass_fcA(seg, da_filter)\n            else:\n                y=self.apply_low_pass(seg, da_filter, type)\n            #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type, typefilter) \n\n            #if self.args.tester.bandwidth_extension.sigma_observations != \"None\":\n            #    sigma=self.args.tester.bandwidth_extension.sigma_observations\n            #    y+=sigma*torch.randn(y.shape).to(y.device)\n\n            if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n\n            print(\"y\", y.shape)\n            if test_filter_fit:\n                if compute_sweep:\n                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True, compute_sweep=True)\n                    pred, data_denoised, data_score, t, data_filters, data_norms, data_grads =out\n                    #save the data_norms and data_grads as a .npy file\n                    np.save(self.paths[\"bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                    np.save(self.paths[\"bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                else:\n                    out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=True)\n                    pred, data_denoised, data_score, t, data_filters =out\n            else:\n                rid=True\n                out=self.sampler.predict_bwe(y, da_filter, type,rid=True, test_filter_fit=False, compute_sweep=False)\n                \n                pred, data_denoised, data_score, t =out\n\n\n            #if self.args.tester.bandwidth_extension.gain_boost != 0:\n            #    #compensate gain boost\n            #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n            #    #add gain boost (in dB)\n            #    pred=pred*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n            #    seg=seg*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n            #    y=y*10**(-self.args.tester.bandwidth_extension.gain_boost/20)\n\n            res[i,:]=pred\n       \n            path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"original\"])\n\n            path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"degraded\"])\n\n            path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"reconstructed\"])\n\n            test_bwe_table_audio.add_data(i, \n                    wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                    wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                    wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n\n            if rid:\n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n            if test_filter_fit:\n\n                #expecting to crash here\n                print(data_filters.shape)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n\n        self.wandb_run.log({\"table_bwe_audio\": test_bwe_table_audio}, commit=True) \n\n        if self.use_wandb:\n            self.log_audio(res, \"bwe\")\n\n    def apply_low_pass(self, seg, filter, typefilter):\n        y=utils_bwe.apply_low_pass(seg, filter, self.args.tester.bandwidth_extension.filter.type) \n        return y\n\n    def apply_lowpass_fcA(self, seg, params):\n        freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n        H=blind_bwe_utils.design_filter(params[0], params[1], freqs)\n        xfilt=blind_bwe_utils.apply_filter(seg,H,self.args.tester.blind_bwe.NFFT)\n        return xfilt\n\n    def prepare_filter(self, sample_rate, typefilter):\n        filter=utils_bwe.prepare_filter(self.args, sample_rate )\n        return filter\n    \n    def test_real_blind_bwe_complete(self, typefilter=\"fc_A\", compute_sweep=False):\n        #raise NotImplementedError\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        \n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        filename=self.args.tester.complete_recording.path\n        d,fs=sf.read(filename)\n        degraded=torch.Tensor(d)\n\n        segL=self.args.exp.audio_len\n\n        ix_first=self.args.exp.sample_rate*self.args.tester.complete_recording.ix_start #index of the first segment to be processed, might have to depend on the sample rate\n\n        #for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\n        print(\"filename\",filename)\n        n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n        degraded=degraded.float().to(self.device).unsqueeze(0)\n        print(n)\n\n        print(\"dsds FS\",fs)\n\n        print(\"seg shape\",degraded.shape)\n        degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n        print(\"seg shape\",degraded.shape)\n\n        std= degraded.std(-1)\n        degraded=self.args.tester.complete_recording.std*degraded/std.unsqueeze(-1)\n        #add noise\n        if self.args.tester.complete_recording.SNR_extra_noise!=\"None\":\n            #contaminate a bit with white noise\n            SNR=10**(self.args.tester.complete_recording.SNR_extra_noise/10)\n            sigma2_s=torch.Tensor([self.args.tester.complete_recording.std**2]).to(degraded.device)\n            sigma=torch.sqrt(sigma2_s/SNR)\n            degraded+=sigma*torch.randn(degraded.shape).to(degraded.device)\n\n\n        if self.args.tester.complete_recording.n_segments_blindstep==1:\n            y=degraded[...,ix_first:ix_first+segL]\n        else:\n            #initialize y with the first segment and repeat it\n            y=degraded[...,ix_first:ix_first+segL].repeat(self.args.tester.complete_recording.n_segments_blindstep,1)\n            for j in range(0, self.args.tester.complete_recording.n_segments_blindstep):\n                #random index\n                ix=np.random.randint(0, degraded.shape[-1]-segL)\n                y[j,...]=degraded[...,ix:ix+segL]\n        \n        print(\"y shape\",y.shape)\n\n            \n\n        #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, fs)\n        #print(\"scale\",scale) #TODO I should calculate this with the whole track, not just the first segment\n\n        #y=y*10**(scale/20)\n        #degraded=degraded*10**(scale/20)\n\n\n\n        rid=False\n        outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n        pred, estimated_filter =outputs\n\n        #now I will just throw away the first segment and process the rest of the signal with the estimated filter. Later I should think of a better way to do it\n\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n        hop=segL-overlap\n\n        final_pred=torch.zeros_like(degraded)\n        final_pred[0, ix_first:ix_first+segL]=pred[0]\n\n        path, basename=os.path.split(filename)\n        print(path, basename)\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n        L=degraded.shape[-1]\n\n        #modify the sampler, so that it is computationally cheaper\n\n        discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n        discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\n        #first segment\n        ix=0\n        seg=degraded[...,ix:ix+segL]\n        pred=self.sampler.predict_bwe(seg, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n        previous_pred=pred[..., 0:segL-discard_end]\n\n        final_pred[...,ix:ix+segL-discard_end]=previous_pred\n        ix+=segL-overlap-discard_end\n\n        y_masked=torch.zeros_like(pred, device=self.device)\n        mask=torch.ones_like(seg, device=self.device)\n        mask[...,overlap::]=0\n\n        hann_window=torch.hann_window(overlap*2, device=self.device)\n\n        while ix<L-segL-discard_end-discard_start:\n            y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n            seg=degraded[...,ix:ix+segL]\n\n            pred=self.sampler.predict_bwe_AR(seg, y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n            previous_pred=pred[..., 0:segL-discard_end]\n\n\n            final_pred[...,ix:ix+segL-discard_end]=previous_pred\n            #do a little bit of overlap and add with a hann window to avoid discontinuities\n            #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n            #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n\n\n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n            ix+=segL-overlap-discard_end\n\n        #skipping the last segment, which is not complete, I am lazy\n        seg=degraded[...,ix::]\n        y_masked[...,0:overlap]=pred[...,-overlap::]\n\n        if seg.shape[-1]<segL:\n            #cat zeros\n            seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\n            #the cat zeroes will also be part of the observed signal, so I need to mask them\n            y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n            mask[...,seg.shape[-1]:segL]=0\n\n        else:\n            seg_zp=seg[...,0:segL]\n\n\n        pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n        final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\n        final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n        #final_pred=final_pred*10**(-scale/20)\n\n        #extract path from filename\n\n\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n               \n               \n               \n    def test_real_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\",\"degraded_audio\", \"reconstructed audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n        \n        \n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        path=self.args.tester.blind_bwe.real_recordings.path\n        audio_files=glob(path+\"/*.wav\")\n        test_set_data=[]\n        test_set_fs=[]\n        test_set_names=[]\n        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n            d,fs=sf.read(audio_files[i])\n            test_set_data.append(torch.Tensor(d))\n            print(\"fs\",fs)\n            print(\"len\",len(d))\n            test_set_names.append(audio_files[i])\n\n        for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n                print(\"filename\",filename)\n                n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n                seg=degraded.float().to(self.device).unsqueeze(0)\n                print(n)\n\n                print(\"dsds FS\",fs)\n\n                print(\"seg shape\",seg.shape)\n                seg=torchaudio.functional.resample(seg, fs, self.args.exp.sample_rate)\n                print(\"seg shape\",seg.shape)\n                ix_start=self.args.tester.blind_bwe\n\n                seg=seg[...,self.args.exp.sample_rate*5:self.args.exp.sample_rate*5+self.args.exp.audio_len]\n                y=seg\n                print(\"y shape\",y.shape)\n                #normalize???\n                std= y.std(-1)\n                y=self.args.tester.blind_bwe.sigma_norm*y/std.unsqueeze(-1)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y,fs)\n                #print(\"scale\",scale)\n                #y=y*10**(scale/20)\n\n\n\n               \n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n                pred=pred*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                y=y*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"reconstructed\"])\n               \n               \n               \n                fig_est_filter=blind_bwe_utils.plot_filter(estimated_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"real_blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n               \n                if typefilter==\"fc_A\":\n                    test_blind_bwe_table_filters.add_data(i, \n                        wandb.Html(path_est_filter),\n                    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"real_blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"real_blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    def test_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            fc=self.args.tester.blind_bwe.test_filter.fc\n            A=self.args.tester.blind_bwe.test_filter.A\n            da_filter=torch.Tensor([fc, A]).to(self.device)\n        else:\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate, typefilter) #standardly designed filter\n            da_filter=da_filter.to(self.device)\n        \n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n                n=os.path.splitext(filename[0])[0]+typefilter\n                seg=original.float().to(self.device)\n                seg=self.resample_audio(seg, fs)\n\n                #if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                #    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                #    orig_std=seg.std(-1)\n                #    seg=sigma_norm*seg/orig_std\n        \n                #elif self.args.tester.blind_bwe.gain_boost != 0:\n                #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                #    #add gain boost (in dB)\n                #    seg=seg*10**(self.args.tester.blind_bwe.gain_boost/20)\n                \n\n                #apply lowpass filter\n                if typefilter==\"fc_A\":\n                    y=self.apply_lowpass_fcA(seg, da_filter)\n                else:\n                    y=self.apply_low_pass(seg,da_filter, typefilter)\n\n                #add noise to the observations for regularization\n                if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, self.args.exp.sample_rate)\n                #print(\"applied scale\",scale)\n                #y=y*10**(scale/20)\n\n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n               \n                y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\n                #if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                #    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                #    assert orig_std is not None\n                #    seg=orig_std*seg/sigma_norm\n                #elif self.args.tester.blind_bwe.gain_boost != 0:\n                #    #compensate gain boost\n                #    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                #    #add gain boost (in dB)\n                #    y_est=y_est*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                #    pred=pred*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                #    seg=seg*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                #    y=y*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n               \n                \n                path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n               \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"reconstructed\"])\n               \n                path_degrade_estimate=utils_logging.write_audio_file(y_est, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded_estimate\"])\n               \n               \n                #will probably crash here!\n                fig_est_filter=blind_bwe_utils.plot_filter(da_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_degrade_estimate, sample_rate=self.args.exp.sample_rate))\n               \n                #if typefilter==\"fc_A\":\n                #    test_blind_bwe_table_filters.add_data(i, \n                #        wandb.Html(path_est_filter),\n                #    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters.shape)\n                #will crash here\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    \n        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n    def dodajob(self):\n        self.setup_wandb()\n        for m in self.args.tester.modes:\n\n            if m==\"unconditional\":\n                print(\"testing unconditional\")\n                self.sample_unconditional()\n            if m==\"unconditional_diffwavesr\":\n                print(\"testing unconditional\")\n                self.sample_unconditional_diffwavesr()\n            self.it+=1\n            if m==\"blind_bwe\":\n                print(\"TESTING BLIND BWE\")\n                self.test_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe\":\n                print(\"TESTING REAL BLIND BWE\")\n                self.test_real_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe_complete\":\n                #process the whole audio file\n                #Estimate the filter in the first chunk, and then apply it to the rest of the audio file (using a little bit of overlap or outpainting)\n                print(\"TESTING REAL BLIND BWE COMPLETE\")\n                self.test_real_blind_bwe_complete(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep)\n            if m==\"formal_test_bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.formal_test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"firwin\", blind=self.args.tester.formal_test.blind)\n            if m==\"formal_test_bwe_small\": \n                print(\"TESTING NORMAL BWE\")\n                self.formal_test_bwe_small(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"fc_A\", blind=self.args.tester.formal_test.blind)\n        self.it+=1", ""]}
{"filename": "testing/tester.py", "chunked_list": ["from datetime import date\nimport re\nimport torch\nimport torchaudio\n#from src.models.unet_cqt import Unet_CQT\n#from src.models.unet_stft import Unet_STFT\n#from src.models.unet_1d import Unet_1d\n#import src.utils.setup as utils_setup\n#from src.sde import  VE_Sde_Elucidating\nimport utils.dnnlib as dnnlib", "#from src.sde import  VE_Sde_Elucidating\nimport utils.dnnlib as dnnlib\nimport os\n\nimport utils.logging as utils_logging\nimport wandb\nimport copy\n\nfrom glob import glob\nfrom tqdm import tqdm", "from glob import glob\nfrom tqdm import tqdm\n\nimport utils.bandwidth_extension as utils_bwe\nimport utils.training_utils as t_utils\nimport omegaconf\n\n\nclass Tester():\n    def __init__(\n        self, args, network, diff_params, test_set=None, device=None, it=None\n    ):\n        self.args=args\n        self.network=network\n        self.diff_params=copy.copy(diff_params)\n        self.device=device\n        #choose gpu as the device if possible\n        if self.device is None:\n            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.network=network\n\n        torch.backends.cudnn.benchmark = True\n\n        today=date.today() \n        if it is None:\n            self.it=0\n\n        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n        if not os.path.exists(self.path_sampling):\n            os.makedirs(self.path_sampling)\n\n\n        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n        self.setup_sampler()\n\n        self.use_wandb=False #hardcoded for now\n\n        #S=2\n        #if S>2.1 and S<2.2:\n        #    #resampling 48k to 22.05k\n        #    self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n        #elif S!=1:\n        #    N=int(self.args.exp.audio_len*S)\n        #    self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\n        if test_set is not None:\n            self.test_set=test_set\n            self.do_inpainting=True\n            self.do_bwe=True\n        else:\n            self.test_set=None\n            self.do_inpainting=False\n            self.do_bwe=False #these need to be set up in the config file\n\n        self.paths={}\n        if self.do_inpainting and (\"inpainting\" in self.args.tester.modes):\n            self.do_inpainting=True\n            mode=\"inpainting\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"inpainting\",\"masked\",\"inpainted\")\n            #TODO add more information in the subirectory names\n        else:\n            self.do_inpainting=False\n\n        if self.do_bwe and (\"bwe\" in self.args.tester.modes):\n            self.do_bwe=True\n            mode=\"bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"bwe\",\"lowpassed\",\"bwe\")\n            #TODO add more information in the subirectory names\n        else:\n            self.do_bwe=False\n\n        if (\"unconditional\" in self.args.tester.modes):\n            mode=\"unconditional\"\n            self.paths[mode]=self.prepare_unc_experiment(\"unconditional\")\n\n        try:\n            self.stereo=self.args.tester.stereo\n        except:\n            self.stereo=False\n\n    def prepare_unc_experiment(self, str):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n            return path_exp\n\n    def prepare_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"recosntucted\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n\n            return path_exp, path_degraded, path_original, path_reconstructed\n\n\n    def setup_wandb(self):\n        \"\"\"\n        Configure wandb, open a new run and log the configuration.\n        \"\"\"\n        config=omegaconf.OmegaConf.to_container(\n            self.args, resolve=True, throw_on_missing=True\n        )\n        self.wandb_run=wandb.init(project=\"testing\"+self.args.exp.wandb.project, entity=self.args.exp.wandb.entity, config=config)\n        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n        self.use_wandb=True\n\n    def setup_wandb_run(self, run):\n        #get the wandb run object from outside (in trainer.py or somewhere else)\n        self.wandb_run=run\n        self.use_wandb=True\n\n    def setup_sampler(self):\n        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network, diff_params=self.diff_params, args=self.args)\n    \n    def load_latest_checkpoint(self ):\n        #load the latest checkpoint from self.args.model_dir\n        try:\n            # find latest checkpoint_id\n            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n            save_name = f\"{self.args.model_dir}/{save_basename}\"\n            list_weights = glob(save_name)\n            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n            list_ids = [int(id_regex.search(weight_path).groups()[0])\n                        for weight_path in list_weights]\n            checkpoint_id = max(list_ids)\n\n            state_dict = torch.load(\n                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n            try:\n                self.network.load_state_dict(state_dict['ema'])\n            except Exception as e:\n                print(e)\n                print(\"Failed to load in strict mode, trying again without strict mode\")\n                self.network.load_state_dict(state_dict['model'], strict=False)\n\n            print(f\"Loaded checkpoint {checkpoint_id}\")\n            return True\n        except (FileNotFoundError, ValueError):\n            raise ValueError(\"No checkpoint found\")\n\n    def load_checkpoint(self, path):\n        state_dict = torch.load(path, map_location=self.device)\n        try:\n            self.it=state_dict['it']\n        except:\n            self.it=0\n        print(\"loading checkpoint\")\n        return t_utils.load_state_dict(state_dict, ema=self.network)\n\n    def load_checkpoint_legacy(self, path):\n        state_dict = torch.load(path, map_location=self.device)\n\n        try:\n            print(\"load try 1\")\n            self.network.load_state_dict(state_dict['ema'])\n        except:\n            #self.network.load_state_dict(state_dict['model'])\n            try:\n                print(\"load try 2\")\n                dic_ema = {}\n                for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n                    dic_ema[key] = tensor\n                self.network.load_state_dict(dic_ema)\n            except:\n                print(\"load try 3\")\n                dic_ema = {}\n                i=0\n                for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n                    if tensor.requires_grad:\n                        dic_ema[key]=state_dict['ema_weights'][i]\n                        i=i+1\n                    else:\n                        dic_ema[key]=tensor     \n                self.network.load_state_dict(dic_ema)\n        try:\n            self.it=state_dict['it']\n        except:\n            self.it=0\n\n    def log_audio(self,preds, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=os.path.join(self.args.model_dir, self.paths[mode]),stereo=self.stereo)\n        print(audio_path)\n        if self.use_wandb:\n            self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it)\n        #TODO: log spectrogram of the audio file to wandb\n        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n        if self.use_wandb:\n            self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it)\n\n    def sample_unconditional(self):\n        #the audio length is specified in the args.exp, doesnt depend on the tester\n        if self.stereo: \n            shape=[self.args.tester.unconditional.num_samples,2, self.args.exp.audio_len]\n        else:\n            shape=[self.args.tester.unconditional.num_samples, self.args.exp.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        preds=self.sampler.predict_unconditional(shape, self.device)\n        if self.use_wandb:\n            self.log_audio(preds, \"unconditional\")\n        else:\n            #TODO do something else if wandb is not used, like saving the audio file to the model directory\n            pass\n\n        return preds\n\n    def test_inpainting(self):\n        if not self.do_inpainting or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        self.inpainting_mask=torch.ones((1,self.args.exp.audio_len)).to(self.device) #assume between 5 and 6s of total length\n        gap=int(self.args.tester.inpainting.gap_length*self.args.exp.sample_rate/1000)      \n\n        if self.args.tester.inpainting.start_gap_idx ==\"None\": #we were crashing here!\n            #the gap is placed at the center\n            start_gap_index=int(self.args.exp.audio_len//2 - gap//2) \n        else:\n            start_gap_index=int(self.args.tester.inpainting.start_gap_idx*self.args.exp.sample_rate/1000)\n        self.inpainting_mask[...,start_gap_index:(start_gap_index+gap)]=0\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs, filename) in enumerate(tqdm(self.test_set)):\n            n=os.path.splitext(filename[0])[0]\n            original=original.float().to(self.device)\n            seg=self.resample_audio(original, fs)\n            #seg=torchaudio.functional.resample(seg, self.args.exp.resample_factor, 1)\n            utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"inpainting\"+\"original\"])\n            masked=seg*self.inpainting_mask\n            utils_logging.write_audio_file(masked, self.args.exp.sample_rate, n, path=self.paths[\"inpainting\"+\"degraded\"])\n            pred=self.sampler.predict_inpainting(masked, self.inpainting_mask)\n            utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"inpainting\"+\"reconstructed\"])\n            res[i,:]=pred\n\n        if self.use_wandb:\n            self.log_audio(res, \"inpainting\")\n        \n        #TODO save the files in the subdirectory inpainting of the model directory\n\n    def resample_audio(self, audio, fs):\n        #this has been reused from the trainer.py\n        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\n    def sample_inpainting(self, y, mask):\n\n        y_masked=y*mask\n        #shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        preds=self.sampler.predict_inpainting(y_masked, mask)\n\n        return preds\n    \n    def test_bwe(self, typefilter=\"whateverIignoreit\"):\n        if not self.do_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #prepare lowpass filters\n        self.filter=utils_bwe.prepare_filter(self.args, self.args.exp.sample_rate)\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs, filename) in enumerate(tqdm(self.test_set)):\n            n=os.path.splitext(filename[0])[0]\n            original=original.float().to(self.device)\n            seg=self.resample_audio(original, fs)\n\n            utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"original\"])\n\n            y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n\n            if self.args.tester.noise_in_observations_SNR != \"None\":\n                SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                sigma2_s=torch.var(y, -1)\n                sigma=torch.sqrt(sigma2_s/SNR)\n                y+=sigma*torch.randn(y.shape).to(y.device)\n\n            utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"degraded\"])\n\n            pred=self.sampler.predict_bwe(y, self.filter, self.args.tester.bandwidth_extension.filter.type)\n            utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"reconstructed\"])\n            res[i,:]=pred\n\n        if self.use_wandb:\n            self.log_audio(res, \"bwe\")\n\n            #preprocess the audio file if necessary\n\n\n    def dodajob(self):\n        self.setup_wandb()\n        if \"unconditional\" in self.args.tester.modes:\n            print(\"testing unconditional\")\n            self.sample_unconditional()\n        self.it+=1\n        if \"blind_bwe\" in self.args.tester.modes:\n            print(\"testing blind bwe\")\n            #tester.test_blind_bwe(typefilter=\"whatever\")\n            self.tester.test_blind_bwe(typefilter=\"3rdoct\")\n        self.it+=1\n        if \"filter_bwe\" in self.args.tester.modes:\n            print(\"testing filter bwe\")\n            self.test_filter_bwe(typefilter=\"3rdoct\")\n        self.it+=1\n        if \"unconditional_operator\" in self.args.tester.modes:\n            print(\"testing unconditional operator\")\n            self.sample_unconditional_operator()\n        self.it+=1\n        if \"bwe\" in self.args.tester.modes:\n            print(\"testing bwe\")\n            self.test_bwe(typefilter=\"3rdoct\")\n        self.it+=1\n        if \"inpainting\" in self.args.tester.modes:\n            self.test_inpainting()\n           \n        self.it+=1", "class Tester():\n    def __init__(\n        self, args, network, diff_params, test_set=None, device=None, it=None\n    ):\n        self.args=args\n        self.network=network\n        self.diff_params=copy.copy(diff_params)\n        self.device=device\n        #choose gpu as the device if possible\n        if self.device is None:\n            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.network=network\n\n        torch.backends.cudnn.benchmark = True\n\n        today=date.today() \n        if it is None:\n            self.it=0\n\n        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n        if not os.path.exists(self.path_sampling):\n            os.makedirs(self.path_sampling)\n\n\n        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n        self.setup_sampler()\n\n        self.use_wandb=False #hardcoded for now\n\n        #S=2\n        #if S>2.1 and S<2.2:\n        #    #resampling 48k to 22.05k\n        #    self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n        #elif S!=1:\n        #    N=int(self.args.exp.audio_len*S)\n        #    self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\n        if test_set is not None:\n            self.test_set=test_set\n            self.do_inpainting=True\n            self.do_bwe=True\n        else:\n            self.test_set=None\n            self.do_inpainting=False\n            self.do_bwe=False #these need to be set up in the config file\n\n        self.paths={}\n        if self.do_inpainting and (\"inpainting\" in self.args.tester.modes):\n            self.do_inpainting=True\n            mode=\"inpainting\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"inpainting\",\"masked\",\"inpainted\")\n            #TODO add more information in the subirectory names\n        else:\n            self.do_inpainting=False\n\n        if self.do_bwe and (\"bwe\" in self.args.tester.modes):\n            self.do_bwe=True\n            mode=\"bwe\"\n            self.paths[mode], self.paths[mode+\"degraded\"], self.paths[mode+\"original\"], self.paths[mode+\"reconstructed\"]=self.prepare_experiment(\"bwe\",\"lowpassed\",\"bwe\")\n            #TODO add more information in the subirectory names\n        else:\n            self.do_bwe=False\n\n        if (\"unconditional\" in self.args.tester.modes):\n            mode=\"unconditional\"\n            self.paths[mode]=self.prepare_unc_experiment(\"unconditional\")\n\n        try:\n            self.stereo=self.args.tester.stereo\n        except:\n            self.stereo=False\n\n    def prepare_unc_experiment(self, str):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n            return path_exp\n\n    def prepare_experiment(self, str, str_degraded=\"degraded\", str_reconstruced=\"recosntucted\"):\n            path_exp=os.path.join(self.path_sampling,str)\n            if not os.path.exists(path_exp):\n                os.makedirs(path_exp)\n\n            n=str_degraded\n            path_degraded=os.path.join(path_exp, n) #path for the lowpassed \n            #ensure the path exists\n            if not os.path.exists(path_degraded):\n                os.makedirs(path_degraded)\n            \n            path_original=os.path.join(path_exp, \"original\") #this will need a better organization\n            #ensure the path exists\n            if not os.path.exists(path_original):\n                os.makedirs(path_original)\n            \n            n=str_reconstruced\n            path_reconstructed=os.path.join(path_exp, n) #path for the clipped outputs\n            #ensure the path exists\n            if not os.path.exists(path_reconstructed):\n                os.makedirs(path_reconstructed)\n\n            return path_exp, path_degraded, path_original, path_reconstructed\n\n\n    def setup_wandb(self):\n        \"\"\"\n        Configure wandb, open a new run and log the configuration.\n        \"\"\"\n        config=omegaconf.OmegaConf.to_container(\n            self.args, resolve=True, throw_on_missing=True\n        )\n        self.wandb_run=wandb.init(project=\"testing\"+self.args.exp.wandb.project, entity=self.args.exp.wandb.entity, config=config)\n        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n        self.use_wandb=True\n\n    def setup_wandb_run(self, run):\n        #get the wandb run object from outside (in trainer.py or somewhere else)\n        self.wandb_run=run\n        self.use_wandb=True\n\n    def setup_sampler(self):\n        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network, diff_params=self.diff_params, args=self.args)\n    \n    def load_latest_checkpoint(self ):\n        #load the latest checkpoint from self.args.model_dir\n        try:\n            # find latest checkpoint_id\n            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n            save_name = f\"{self.args.model_dir}/{save_basename}\"\n            list_weights = glob(save_name)\n            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n            list_ids = [int(id_regex.search(weight_path).groups()[0])\n                        for weight_path in list_weights]\n            checkpoint_id = max(list_ids)\n\n            state_dict = torch.load(\n                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n            try:\n                self.network.load_state_dict(state_dict['ema'])\n            except Exception as e:\n                print(e)\n                print(\"Failed to load in strict mode, trying again without strict mode\")\n                self.network.load_state_dict(state_dict['model'], strict=False)\n\n            print(f\"Loaded checkpoint {checkpoint_id}\")\n            return True\n        except (FileNotFoundError, ValueError):\n            raise ValueError(\"No checkpoint found\")\n\n    def load_checkpoint(self, path):\n        state_dict = torch.load(path, map_location=self.device)\n        try:\n            self.it=state_dict['it']\n        except:\n            self.it=0\n        print(\"loading checkpoint\")\n        return t_utils.load_state_dict(state_dict, ema=self.network)\n\n    def load_checkpoint_legacy(self, path):\n        state_dict = torch.load(path, map_location=self.device)\n\n        try:\n            print(\"load try 1\")\n            self.network.load_state_dict(state_dict['ema'])\n        except:\n            #self.network.load_state_dict(state_dict['model'])\n            try:\n                print(\"load try 2\")\n                dic_ema = {}\n                for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n                    dic_ema[key] = tensor\n                self.network.load_state_dict(dic_ema)\n            except:\n                print(\"load try 3\")\n                dic_ema = {}\n                i=0\n                for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n                    if tensor.requires_grad:\n                        dic_ema[key]=state_dict['ema_weights'][i]\n                        i=i+1\n                    else:\n                        dic_ema[key]=tensor     \n                self.network.load_state_dict(dic_ema)\n        try:\n            self.it=state_dict['it']\n        except:\n            self.it=0\n\n    def log_audio(self,preds, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=os.path.join(self.args.model_dir, self.paths[mode]),stereo=self.stereo)\n        print(audio_path)\n        if self.use_wandb:\n            self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it)\n        #TODO: log spectrogram of the audio file to wandb\n        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n        if self.use_wandb:\n            self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it)\n\n    def sample_unconditional(self):\n        #the audio length is specified in the args.exp, doesnt depend on the tester\n        if self.stereo: \n            shape=[self.args.tester.unconditional.num_samples,2, self.args.exp.audio_len]\n        else:\n            shape=[self.args.tester.unconditional.num_samples, self.args.exp.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        preds=self.sampler.predict_unconditional(shape, self.device)\n        if self.use_wandb:\n            self.log_audio(preds, \"unconditional\")\n        else:\n            #TODO do something else if wandb is not used, like saving the audio file to the model directory\n            pass\n\n        return preds\n\n    def test_inpainting(self):\n        if not self.do_inpainting or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        self.inpainting_mask=torch.ones((1,self.args.exp.audio_len)).to(self.device) #assume between 5 and 6s of total length\n        gap=int(self.args.tester.inpainting.gap_length*self.args.exp.sample_rate/1000)      \n\n        if self.args.tester.inpainting.start_gap_idx ==\"None\": #we were crashing here!\n            #the gap is placed at the center\n            start_gap_index=int(self.args.exp.audio_len//2 - gap//2) \n        else:\n            start_gap_index=int(self.args.tester.inpainting.start_gap_idx*self.args.exp.sample_rate/1000)\n        self.inpainting_mask[...,start_gap_index:(start_gap_index+gap)]=0\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs, filename) in enumerate(tqdm(self.test_set)):\n            n=os.path.splitext(filename[0])[0]\n            original=original.float().to(self.device)\n            seg=self.resample_audio(original, fs)\n            #seg=torchaudio.functional.resample(seg, self.args.exp.resample_factor, 1)\n            utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"inpainting\"+\"original\"])\n            masked=seg*self.inpainting_mask\n            utils_logging.write_audio_file(masked, self.args.exp.sample_rate, n, path=self.paths[\"inpainting\"+\"degraded\"])\n            pred=self.sampler.predict_inpainting(masked, self.inpainting_mask)\n            utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"inpainting\"+\"reconstructed\"])\n            res[i,:]=pred\n\n        if self.use_wandb:\n            self.log_audio(res, \"inpainting\")\n        \n        #TODO save the files in the subdirectory inpainting of the model directory\n\n    def resample_audio(self, audio, fs):\n        #this has been reused from the trainer.py\n        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\n    def sample_inpainting(self, y, mask):\n\n        y_masked=y*mask\n        #shape=[self.args.tester.unconditional.num_samples, self.args.tester.unconditional.audio_len]\n        #TODO assert that the audio_len is consistent with the model\n        preds=self.sampler.predict_inpainting(y_masked, mask)\n\n        return preds\n    \n    def test_bwe(self, typefilter=\"whateverIignoreit\"):\n        if not self.do_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #prepare lowpass filters\n        self.filter=utils_bwe.prepare_filter(self.args, self.args.exp.sample_rate)\n        \n        res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs, filename) in enumerate(tqdm(self.test_set)):\n            n=os.path.splitext(filename[0])[0]\n            original=original.float().to(self.device)\n            seg=self.resample_audio(original, fs)\n\n            utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"original\"])\n\n            y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n\n            if self.args.tester.noise_in_observations_SNR != \"None\":\n                SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                sigma2_s=torch.var(y, -1)\n                sigma=torch.sqrt(sigma2_s/SNR)\n                y+=sigma*torch.randn(y.shape).to(y.device)\n\n            utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"degraded\"])\n\n            pred=self.sampler.predict_bwe(y, self.filter, self.args.tester.bandwidth_extension.filter.type)\n            utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"bwe\"+\"reconstructed\"])\n            res[i,:]=pred\n\n        if self.use_wandb:\n            self.log_audio(res, \"bwe\")\n\n            #preprocess the audio file if necessary\n\n\n    def dodajob(self):\n        self.setup_wandb()\n        if \"unconditional\" in self.args.tester.modes:\n            print(\"testing unconditional\")\n            self.sample_unconditional()\n        self.it+=1\n        if \"blind_bwe\" in self.args.tester.modes:\n            print(\"testing blind bwe\")\n            #tester.test_blind_bwe(typefilter=\"whatever\")\n            self.tester.test_blind_bwe(typefilter=\"3rdoct\")\n        self.it+=1\n        if \"filter_bwe\" in self.args.tester.modes:\n            print(\"testing filter bwe\")\n            self.test_filter_bwe(typefilter=\"3rdoct\")\n        self.it+=1\n        if \"unconditional_operator\" in self.args.tester.modes:\n            print(\"testing unconditional operator\")\n            self.sample_unconditional_operator()\n        self.it+=1\n        if \"bwe\" in self.args.tester.modes:\n            print(\"testing bwe\")\n            self.test_bwe(typefilter=\"3rdoct\")\n        self.it+=1\n        if \"inpainting\" in self.args.tester.modes:\n            self.test_inpainting()\n           \n        self.it+=1", "\n        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n\n\n"]}
{"filename": "testing/denoise_and_bwe_tester.py", "chunked_list": ["from datetime import date\nimport pickle\nimport re\nimport torch\nimport torchaudio\n#from src.models.unet_cqt import Unet_CQT\n#from src.models.unet_stft import Unet_STFT\n#from src.models.unet_1d import Unet_1d\n#import src.utils.setup as utils_setup\n#from src.sde import  VE_Sde_Elucidating", "#import src.utils.setup as utils_setup\n#from src.sde import  VE_Sde_Elucidating\nimport numpy as np\nimport utils.dnnlib as dnnlib\nimport os\n\nimport utils.logging as utils_logging\nimport wandb\nimport copy\n", "import copy\n\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport utils.bandwidth_extension as utils_bwe\nimport utils.setup as utils_setup\nimport omegaconf\n\n#import utils.filter_generation_utils as f_utils", "\n#import utils.filter_generation_utils as f_utils\nimport utils.blind_bwe_utils as blind_bwe_utils\nimport utils.training_utils as t_utils\n\nimport soundfile as sf\n\n#from utils.spectral_analysis import LTAS_processor\n\n\nclass BlindTester():\n    def __init__(\n        self, args=None, network=None, diff_params=None, test_set=None, device=None, it=None\n    ):\n        self.args=args\n        self.network=torch.compile(network)\n        #self.network=network\n        #prnt number of parameters\n        \n\n        self.diff_params=copy.copy(diff_params)\n        self.device=device\n        #choose gpu as the device if possible\n        if self.device is None:\n            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.network=network\n\n        torch.backends.cudnn.benchmark = True\n\n        today=date.today() \n        if it is None:\n            self.it=0\n\n        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n        if not os.path.exists(self.path_sampling):\n            os.makedirs(self.path_sampling)\n\n\n        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n        self.setup_sampler()\n\n        self.use_wandb=False #hardcoded for now\n\n        S=self.args.exp.resample_factor\n        if S>2.1 and S<2.2:\n            #resampling 48k to 22.05k\n            self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n        elif S!=1:\n            N=int(self.args.exp.audio_len*S)\n            self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\n        self.denoiser=utils_setup.setup_denoiser(self.args, self.device)\n        self.denoiser.load_state_dict(torch.load(self.args.tester.denoiser.checkpoint, map_location=self.device))\n        self.denoiser.to(self.device)\n\n\n    def resample_audio(self, audio, fs):\n        #this has been reused from the trainer.py\n        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\n\n    def setup_wandb(self):\n        \"\"\"\n        Configure wandb, open a new run and log the configuration.\n        \"\"\"\n        config=omegaconf.OmegaConf.to_container(\n            self.args, resolve=True, throw_on_missing=True\n        )\n        self.wandb_run=wandb.init(project=\"testing\"+self.args.tester.name, entity=self.args.exp.wandb.entity, config=config)\n        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n        self.use_wandb=True\n\n    def setup_wandb_run(self, run):\n        #get the wandb run object from outside (in trainer.py or somewhere else)\n        self.wandb_run=run\n        self.use_wandb=True\n\n    def setup_sampler(self):\n        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network,  diff_params=self.diff_params, args=self.args, rid=True) #rid is used to log some extra information\n\n\n    def apply_denoiser(self,x):\n        segment_size=self.args.tester.denoiser.sample_rate_denoiser*self.args.tester.denoiser.segment_size\n        length_data=x.shape[-1]\n        overlapsize=1024 #hardcoded for now\n        window=torch.hamming_window(window_length=2*overlapsize).to(self.device)\n        window_left=window[:overlapsize]\n        window_right=window[overlapsize:]\n        audio_finished=False\n        pointer=0\n        denoised_data=torch.zeros_like(x)\n        #numchunks=torch.ceil(torch.Tensor(lenght_data/segment_size)).int()\n        while (not(audio_finished)):\n            #for i in tqdm(range(numchunks)):\n            if pointer+segment_size<length_data:\n                segment=x[:,pointer:pointer+segment_size]\n                x_den=self.apply_denoiser_model(segment)\n                if pointer==0:\n                    x_den=torch.cat((x_den[:,0:int(segment_size-overlapsize)], x_den[:,int(segment_size-overlapsize):segment_size]*window_right), axis=-1)\n                else:\n                    x_den=torch.cat((x_den[...,0:int(overlapsize)]*window_left, x_den[:,int(overlapsize):int(segment_size-overlapsize)], x_den[:,int(segment_size-overlapsize):segment_size]*window_right), axis=-1)\n                denoised_data[:,pointer:pointer+segment_size]+=x_den\n                pointer+=(segment_size-overlapsize)\n            else:\n                segment=x[:,pointer:]\n                lensegment=segment.shape[-1]\n                segment=torch.cat((segment, torch.zeros(segment.shape[0],segment_size-lensegment).to(self.device)),-1)\n                audio_finished=True\n                x_den=self.apply_denoiser_model(segment)\n                if pointer!=0:\n                    x_den=torch.cat((x_den[...,0:int(overlapsize)]*window_left, x_den[:,int(overlapsize):int(segment_size-overlapsize)]), axis=-1)\n                denoised_data[:,pointer:]+=x_den[...,0:lensegment]\n\n        return denoised_data\n\n\n\n    def apply_denoiser_model(self, x):\n        win_size=self.args.tester.denoiser.stft_win_size\n        hop_size=self.args.tester.denoiser.stft_hop_size\n\n        window=torch.hamming_window(window_length=win_size).to(self.device)\n        x=torch.cat((x, torch.zeros(x.shape[0],win_size).to(self.device)),-1)\n        X=torch.stft(x, win_size, hop_length=hop_size,window=window,center=False,return_complex=False)\n        X=X.permute(0,3,2,1) #shape= (batch_size, R/I, time, freq)\n        \n        #segment_TF_ds=tf.data.Dataset.from_tensors(segment_TF)\n        with torch.no_grad():\n            #print( X.shape, X.dtype)\n            pred = self.denoiser(X)\n        if self.args.tester.denoiser.num_stages>1:\n            pred=pred[0]\n\n        pred=pred.permute(0,3,2,1).contiguous()\n        pred=torch.view_as_complex(pred)\n        pred_time=torch.istft(pred, win_size, hop_length=hop_size, window=window, center=False, return_complex=False)\n\n        return pred_time[...,0:x.shape[-1]]\n    \n    def load_latest_checkpoint(self ):\n        #load the latest checkpoint from self.args.model_dir\n        try:\n            # find latest checkpoint_id\n            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n            save_name = f\"{self.args.model_dir}/{save_basename}\"\n            list_weights = glob(save_name)\n            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n            list_ids = [int(id_regex.search(weight_path).groups()[0])\n                        for weight_path in list_weights]\n            checkpoint_id = max(list_ids)\n\n            state_dict = torch.load(\n                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n            self.network.load_state_dict(state_dict['ema'])\n            print(f\"Loaded checkpoint {checkpoint_id}\")\n            return True\n        except (FileNotFoundError, ValueError):\n            raise ValueError(\"No checkpoint found\")\n\n\n    def load_checkpoint(self, path):\n        state_dict = torch.load(path, map_location=self.device)\n        if self.args.exp.exp_name==\"diffwave-sr\":\n            print(state_dict.keys())\n            print(\"noise_schedukar\",state_dict[\"noise_scheduler\"])\n            self.network.load_state_dict(state_dict['ema_model'])\n            self.network.eval()\n            print(\"ckpt loaded\")\n        else:\n            try:\n                print(\"load try 1\")\n                self.network.load_state_dict(state_dict['ema'])\n            except:\n                #self.network.load_state_dict(state_dict['model'])\n                try:\n                    print(\"load try 2\")\n                    dic_ema = {}\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n                        dic_ema[key] = tensor\n                    self.network.load_state_dict(dic_ema)\n                except:\n                    print(\"load try 3\")\n                    dic_ema = {}\n                    i=0\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n                        if tensor.requires_grad:\n                            dic_ema[key]=state_dict['ema_weights'][i]\n                            i=i+1\n                        else:\n                            dic_ema[key]=tensor     \n                    self.network.load_state_dict(dic_ema)\n        try:\n            self.it=state_dict['it']\n        except:\n            self.it=0\n\n    def log_audio(self,preds, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=self.args.model_dir)\n        print(audio_path)\n        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it, commit=False)\n        #TODO: log spectrogram of the audio file to wandb\n        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\n        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it, commit=True)\n\n    def apply_low_pass(self, seg, filter, typefilter):\n        y=utils_bwe.apply_low_pass(seg, filter, self.args.tester.bandwidth_extension.filter.type) \n        return y\n\n    def apply_lowpass_fcA(self, seg, params):\n        freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n        H=blind_bwe_utils.design_filter(params[0], params[1], freqs)\n        xfilt=blind_bwe_utils.apply_filter(seg,H,self.args.tester.blind_bwe.NFFT)\n        return xfilt\n\n    def prepare_filter(self, sample_rate, typefilter):\n        filter=utils_bwe.prepare_filter(self.args, sample_rate )\n        return filter\n    \n    def test_real_blind_bwe_complete(self, typefilter=\"fc_A\", compute_sweep=False, use_denoiser=True):\n        #raise NotImplementedError\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        \n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        filename=self.args.tester.complete_recording.path\n        path, basename=os.path.split(filename)\n        print(path, basename)\n\n        d,fs=sf.read(filename)\n        #stereo to mono\n        print(\"d.shape\",d.shape)\n        if len(d.shape)>1:\n            d=d.mean(axis=1)\n\n\n        degraded=torch.Tensor(d)\n\n        segL=self.args.exp.audio_len\n\n        ix_first=self.args.exp.sample_rate*self.args.tester.complete_recording.ix_start #index of the first segment to be processed, might have to depend on the sample rate\n\n        #for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\n        print(\"filename\",filename)\n        n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n        degraded=degraded.float().to(self.device).unsqueeze(0)\n        print(n)\n\n        if self.args.tester.complete_recording.use_denoiser:\n                print(\"denoising\")\n                if fs!=self.args.tester.denoiser.sample_rate_denoiser:\n                    degraded=torchaudio.functional.resample(degraded, fs, self.args.tester.denoiser.sample_rate_denoiser)\n                degraded=self.apply_denoiser(degraded)\n                if fs!=self.args.tester.denoiser.sample_rate_denoiser:\n                    degraded=torchaudio.functional.resample(degraded,  self.args.tester.denoiser.sample_rate_denoiser, self.args.exp.sample_rate)\n                path_reconstructed=utils_logging.write_audio_file(degraded, self.args.exp.sample_rate, basename+\".denoised.wav\", path=path)\n        else:\n                print(\"no denoising\")\n                degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n\n        print(\"seg shape\",degraded.shape)\n\n        std= degraded.std(-1)\n        degraded=self.args.tester.complete_recording.std*degraded/std.unsqueeze(-1)\n        #add noise\n        if self.args.tester.complete_recording.SNR_extra_noise!=\"None\":\n            #contaminate a bit with white noise\n            SNR=10**(self.args.tester.complete_recording.SNR_extra_noise/10)\n            sigma2_s=torch.Tensor([self.args.tester.complete_recording.std**2]).to(degraded.device)\n            sigma=torch.sqrt(sigma2_s/SNR)\n            degraded+=sigma*torch.randn(degraded.shape).to(degraded.device)\n\n\n        if self.args.tester.complete_recording.n_segments_blindstep==1:\n            y=degraded[...,ix_first:ix_first+segL]\n        else:\n            #initialize y with the first segment and repeat it\n            y=degraded[...,ix_first:ix_first+segL].repeat(self.args.tester.complete_recording.n_segments_blindstep,1)\n            for j in range(0, self.args.tester.complete_recording.n_segments_blindstep):\n                #random index\n                ix=np.random.randint(0, degraded.shape[-1]-segL)\n                y[j,...]=degraded[...,ix:ix+segL]\n        \n        print(\"y shape\",y.shape)\n\n            \n\n        #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, fs)\n        #print(\"scale\",scale) #TODO I should calculate this with the whole track, not just the first segment\n\n        #y=y*10**(scale/20)\n        #degraded=degraded*10**(scale/20)\n\n\n\n        rid=False\n        outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n        pred, estimated_filter =outputs\n\n        #now I will just throw away the first segment and process the rest of the signal with the estimated filter. Later I should think of a better way to do it\n\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n        hop=segL-overlap\n\n        final_pred=torch.zeros_like(degraded)\n        final_pred[0, ix_first:ix_first+segL]=pred[0]\n\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n        L=degraded.shape[-1]\n\n        #modify the sampler, so that it is computationally cheaper\n\n        discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n        discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\n        #first segment\n        ix=0\n        seg=degraded[...,ix:ix+segL]\n        pred=self.sampler.predict_bwe(seg, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n        previous_pred=pred[..., 0:segL-discard_end]\n\n        final_pred[...,ix:ix+segL-discard_end]=previous_pred\n        ix+=segL-overlap-discard_end\n\n        y_masked=torch.zeros_like(pred, device=self.device)\n        mask=torch.ones_like(seg, device=self.device)\n        mask[...,overlap::]=0\n\n        hann_window=torch.hann_window(overlap*2, device=self.device)\n\n        while ix<L-segL-discard_end-discard_start:\n            y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n            seg=degraded[...,ix:ix+segL]\n\n            pred=self.sampler.predict_bwe_AR(seg, y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n            previous_pred=pred[..., 0:segL-discard_end]\n\n\n            final_pred[...,ix:ix+segL-discard_end]=previous_pred\n            #do a little bit of overlap and add with a hann window to avoid discontinuities\n            #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n            #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n\n\n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n            ix+=segL-overlap-discard_end\n\n        #skipping the last segment, which is not complete, I am lazy\n        seg=degraded[...,ix::]\n        y_masked[...,0:overlap]=pred[...,-overlap::]\n\n        if seg.shape[-1]<segL:\n            #cat zeros\n            seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\n            #the cat zeroes will also be part of the observed signal, so I need to mask them\n            y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n            mask[...,seg.shape[-1]:segL]=0\n\n        else:\n            seg_zp=seg[...,0:segL]\n\n\n        pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n        final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\n        final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n        #final_pred=final_pred*10**(-scale/20)\n\n        #extract path from filename\n\n\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n               \n               \n               \n    def test_real_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\",\"degraded_audio\", \"reconstructed audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n        \n        \n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        path=self.args.tester.blind_bwe.real_recordings.path\n        audio_files=glob(path+\"/*.wav\")\n        print(audio_files, path)\n        test_set_data=[]\n        test_set_fs=[]\n        test_set_names=[]\n        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n            d,fs=sf.read(audio_files[i])\n            #print(d.shape, self.args.exp.audio_len)\n            #if d.shape[-1] >= self.args.exp.audio_len:\n            #    d=d[...,0:self.args.exp.audio_len]\n            test_set_data.append(torch.Tensor(d))\n            test_set_fs.append(fs)\n            print(\"fs\",fs)\n            print(\"len\",len(d))\n            test_set_names.append(audio_files[i])\n\n        for i, (degraded,  filename, fs) in enumerate(tqdm(zip(test_set_data,  test_set_names, test_set_fs))):\n                print(\"filename\",filename)\n                n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n                seg=degraded.float().to(self.device).unsqueeze(0)\n                print(n)\n\n                print(\"dsds FS\",fs)\n\n                print(\"seg shape\",seg.shape)\n                seg=torchaudio.functional.resample(seg, fs, self.args.exp.sample_rate)\n                print(\"seg shape\",seg.shape)\n                ix_start=self.args.tester.blind_bwe\n\n                seg=seg[...,self.args.exp.sample_rate*0:self.args.exp.sample_rate*0+self.args.exp.audio_len]\n                y=seg\n                print(\"y shape\",y.shape)\n                #normalize???\n                std= y.std(-1)\n                y=self.args.tester.blind_bwe.sigma_norm*y/std.unsqueeze(-1)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y,fs)\n                #print(\"scale\",scale)\n                #y=y*10**(scale/20)\n\n\n\n               \n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n                pred=pred*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                y=y*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"reconstructed\"])\n               \n               \n               \n                fig_est_filter=blind_bwe_utils.plot_filter(estimated_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"real_blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n               \n                if typefilter==\"fc_A\":\n                    test_blind_bwe_table_filters.add_data(i, \n                        wandb.Html(path_est_filter),\n                    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"real_blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"real_blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    def test_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            fc=self.args.tester.blind_bwe.test_filter.fc\n            A=self.args.tester.blind_bwe.test_filter.A\n            da_filter=torch.Tensor([fc, A]).to(self.device)\n        else:\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate, typefilter) #standardly designed filter\n            da_filter=da_filter.to(self.device)\n        \n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n                n=os.path.splitext(filename[0])[0]+typefilter\n                seg=original.float().to(self.device)\n                seg=self.resample_audio(seg, fs)\n\n                if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                    orig_std=seg.std(-1)\n                    seg=sigma_norm*seg/orig_std\n        \n                elif self.args.tester.blind_bwe.gain_boost != 0:\n                    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                    #add gain boost (in dB)\n                    seg=seg*10**(self.args.tester.blind_bwe.gain_boost/20)\n                \n\n                #apply lowpass filter\n                if typefilter==\"fc_A\":\n                    y=self.apply_lowpass_fcA(seg, da_filter)\n                else:\n                    y=self.apply_low_pass(seg,da_filter, typefilter)\n\n                #add noise to the observations for regularization\n                if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, self.args.exp.sample_rate)\n                #print(\"applied scale\",scale)\n                #y=y*10**(scale/20)\n\n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"blind_bwe\"]+\"data_t\"+str(i)+\".npy\", t.cpu().numpy())\n                        np.save(self.paths[\"blind_bwe\"]+\"data_denoised\"+str(i)+\".npy\", data_denoised)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_filters\"+str(i)+\".npy\", data_filters)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n               \n                y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\n                if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                    assert orig_std is not None\n                    seg=orig_std*seg/sigma_norm\n                elif self.args.tester.blind_bwe.gain_boost != 0:\n                    #compensate gain boost\n                    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                    #add gain boost (in dB)\n                    y_est=y_est*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                    pred=pred*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                    seg=seg*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                    y=y*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n               \n                \n                path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n               \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"reconstructed\"])\n               \n                path_degrade_estimate=utils_logging.write_audio_file(y_est, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded_estimate\"])\n               \n               \n                #will probably crash here!\n                fig_est_filter=blind_bwe_utils.plot_filter(da_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_degrade_estimate, sample_rate=self.args.exp.sample_rate))\n               \n                #if typefilter==\"fc_A\":\n                #    test_blind_bwe_table_filters.add_data(i, \n                #        wandb.Html(path_est_filter),\n                #    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters.shape)\n                #will crash here\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    \n        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n    def dodajob(self):\n        self.setup_wandb()\n        for m in self.args.tester.modes:\n\n            if m==\"unconditional\":\n                print(\"testing unconditional\")\n                self.sample_unconditional()\n            if m==\"unconditional_diffwavesr\":\n                print(\"testing unconditional\")\n                self.sample_unconditional_diffwavesr()\n            self.it+=1\n            if m==\"blind_bwe\":\n                print(\"TESTING BLIND BWE\")\n                self.test_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe\":\n                print(\"TESTING REAL BLIND BWE\")\n                self.test_real_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe_complete\":\n                #process the whole audio file\n                #Estimate the filter in the first chunk, and then apply it to the rest of the audio file (using a little bit of overlap or outpainting)\n                print(\"TESTING REAL BLIND BWE COMPLETE\")\n                self.test_real_blind_bwe_complete(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep)\n            if m==\"formal_test_bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.formal_test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"firwin\", blind=self.args.tester.formal_test.blind)\n        self.it+=1", "\n\nclass BlindTester():\n    def __init__(\n        self, args=None, network=None, diff_params=None, test_set=None, device=None, it=None\n    ):\n        self.args=args\n        self.network=torch.compile(network)\n        #self.network=network\n        #prnt number of parameters\n        \n\n        self.diff_params=copy.copy(diff_params)\n        self.device=device\n        #choose gpu as the device if possible\n        if self.device is None:\n            self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.network=network\n\n        torch.backends.cudnn.benchmark = True\n\n        today=date.today() \n        if it is None:\n            self.it=0\n\n        mode='test' #this is hardcoded for now, I'll have to figure out how to deal with the subdirectories once I want to test conditional sampling\n        self.path_sampling=os.path.join(args.model_dir,mode+today.strftime(\"%d_%m_%Y\")+\"_\"+str(self.it))\n        if not os.path.exists(self.path_sampling):\n            os.makedirs(self.path_sampling)\n\n\n        #I have to rethink if I want to create the same sampler object to do conditional and unconditional sampling\n        self.setup_sampler()\n\n        self.use_wandb=False #hardcoded for now\n\n        S=self.args.exp.resample_factor\n        if S>2.1 and S<2.2:\n            #resampling 48k to 22.05k\n            self.resample=torchaudio.transforms.Resample(160*2,147).to(self.device)\n        elif S!=1:\n            N=int(self.args.exp.audio_len*S)\n            self.resample=torchaudio.transforms.Resample(N,self.args.exp.audio_len).to(self.device)\n\n        self.denoiser=utils_setup.setup_denoiser(self.args, self.device)\n        self.denoiser.load_state_dict(torch.load(self.args.tester.denoiser.checkpoint, map_location=self.device))\n        self.denoiser.to(self.device)\n\n\n    def resample_audio(self, audio, fs):\n        #this has been reused from the trainer.py\n        return t_utils.resample_batch(audio, fs, self.args.exp.sample_rate, self.args.exp.audio_len)\n\n\n    def setup_wandb(self):\n        \"\"\"\n        Configure wandb, open a new run and log the configuration.\n        \"\"\"\n        config=omegaconf.OmegaConf.to_container(\n            self.args, resolve=True, throw_on_missing=True\n        )\n        self.wandb_run=wandb.init(project=\"testing\"+self.args.tester.name, entity=self.args.exp.wandb.entity, config=config)\n        wandb.watch(self.network, log_freq=self.args.logging.heavy_log_interval) #wanb.watch is used to log the gradients and parameters of the model to wandb. And it is used to log the model architecture and the model summary and the model graph and the model weights and the model hyperparameters and the model performance metrics.\n        self.wandb_run.name=os.path.basename(self.args.model_dir)+\"_\"+self.args.exp.exp_name+\"_\"+self.wandb_run.id #adding the experiment number to the run name, bery important, I hope this does not crash\n        self.use_wandb=True\n\n    def setup_wandb_run(self, run):\n        #get the wandb run object from outside (in trainer.py or somewhere else)\n        self.wandb_run=run\n        self.use_wandb=True\n\n    def setup_sampler(self):\n        self.sampler=dnnlib.call_func_by_name(func_name=self.args.tester.sampler_callable, model=self.network,  diff_params=self.diff_params, args=self.args, rid=True) #rid is used to log some extra information\n\n\n    def apply_denoiser(self,x):\n        segment_size=self.args.tester.denoiser.sample_rate_denoiser*self.args.tester.denoiser.segment_size\n        length_data=x.shape[-1]\n        overlapsize=1024 #hardcoded for now\n        window=torch.hamming_window(window_length=2*overlapsize).to(self.device)\n        window_left=window[:overlapsize]\n        window_right=window[overlapsize:]\n        audio_finished=False\n        pointer=0\n        denoised_data=torch.zeros_like(x)\n        #numchunks=torch.ceil(torch.Tensor(lenght_data/segment_size)).int()\n        while (not(audio_finished)):\n            #for i in tqdm(range(numchunks)):\n            if pointer+segment_size<length_data:\n                segment=x[:,pointer:pointer+segment_size]\n                x_den=self.apply_denoiser_model(segment)\n                if pointer==0:\n                    x_den=torch.cat((x_den[:,0:int(segment_size-overlapsize)], x_den[:,int(segment_size-overlapsize):segment_size]*window_right), axis=-1)\n                else:\n                    x_den=torch.cat((x_den[...,0:int(overlapsize)]*window_left, x_den[:,int(overlapsize):int(segment_size-overlapsize)], x_den[:,int(segment_size-overlapsize):segment_size]*window_right), axis=-1)\n                denoised_data[:,pointer:pointer+segment_size]+=x_den\n                pointer+=(segment_size-overlapsize)\n            else:\n                segment=x[:,pointer:]\n                lensegment=segment.shape[-1]\n                segment=torch.cat((segment, torch.zeros(segment.shape[0],segment_size-lensegment).to(self.device)),-1)\n                audio_finished=True\n                x_den=self.apply_denoiser_model(segment)\n                if pointer!=0:\n                    x_den=torch.cat((x_den[...,0:int(overlapsize)]*window_left, x_den[:,int(overlapsize):int(segment_size-overlapsize)]), axis=-1)\n                denoised_data[:,pointer:]+=x_den[...,0:lensegment]\n\n        return denoised_data\n\n\n\n    def apply_denoiser_model(self, x):\n        win_size=self.args.tester.denoiser.stft_win_size\n        hop_size=self.args.tester.denoiser.stft_hop_size\n\n        window=torch.hamming_window(window_length=win_size).to(self.device)\n        x=torch.cat((x, torch.zeros(x.shape[0],win_size).to(self.device)),-1)\n        X=torch.stft(x, win_size, hop_length=hop_size,window=window,center=False,return_complex=False)\n        X=X.permute(0,3,2,1) #shape= (batch_size, R/I, time, freq)\n        \n        #segment_TF_ds=tf.data.Dataset.from_tensors(segment_TF)\n        with torch.no_grad():\n            #print( X.shape, X.dtype)\n            pred = self.denoiser(X)\n        if self.args.tester.denoiser.num_stages>1:\n            pred=pred[0]\n\n        pred=pred.permute(0,3,2,1).contiguous()\n        pred=torch.view_as_complex(pred)\n        pred_time=torch.istft(pred, win_size, hop_length=hop_size, window=window, center=False, return_complex=False)\n\n        return pred_time[...,0:x.shape[-1]]\n    \n    def load_latest_checkpoint(self ):\n        #load the latest checkpoint from self.args.model_dir\n        try:\n            # find latest checkpoint_id\n            save_basename = f\"{self.args.exp.exp_name}-*.pt\"\n            save_name = f\"{self.args.model_dir}/{save_basename}\"\n            list_weights = glob(save_name)\n            id_regex = re.compile(f\"{self.args.exp.exp_name}-(\\d*)\\.pt\")\n            list_ids = [int(id_regex.search(weight_path).groups()[0])\n                        for weight_path in list_weights]\n            checkpoint_id = max(list_ids)\n\n            state_dict = torch.load(\n                f\"{self.args.model_dir}/{self.args.exp.exp_name}-{checkpoint_id}.pt\", map_location=self.device)\n            self.network.load_state_dict(state_dict['ema'])\n            print(f\"Loaded checkpoint {checkpoint_id}\")\n            return True\n        except (FileNotFoundError, ValueError):\n            raise ValueError(\"No checkpoint found\")\n\n\n    def load_checkpoint(self, path):\n        state_dict = torch.load(path, map_location=self.device)\n        if self.args.exp.exp_name==\"diffwave-sr\":\n            print(state_dict.keys())\n            print(\"noise_schedukar\",state_dict[\"noise_scheduler\"])\n            self.network.load_state_dict(state_dict['ema_model'])\n            self.network.eval()\n            print(\"ckpt loaded\")\n        else:\n            try:\n                print(\"load try 1\")\n                self.network.load_state_dict(state_dict['ema'])\n            except:\n                #self.network.load_state_dict(state_dict['model'])\n                try:\n                    print(\"load try 2\")\n                    dic_ema = {}\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['ema_weights']):\n                        dic_ema[key] = tensor\n                    self.network.load_state_dict(dic_ema)\n                except:\n                    print(\"load try 3\")\n                    dic_ema = {}\n                    i=0\n                    for (key, tensor) in zip(state_dict['model'].keys(), state_dict['model'].values()):\n                        if tensor.requires_grad:\n                            dic_ema[key]=state_dict['ema_weights'][i]\n                            i=i+1\n                        else:\n                            dic_ema[key]=tensor     \n                    self.network.load_state_dict(dic_ema)\n        try:\n            self.it=state_dict['it']\n        except:\n            self.it=0\n\n    def log_audio(self,preds, mode:str):\n        string=mode+\"_\"+self.args.tester.name\n        audio_path=utils_logging.write_audio_file(preds,self.args.exp.sample_rate, string,path=self.args.model_dir)\n        print(audio_path)\n        self.wandb_run.log({\"audio_\"+str(string): wandb.Audio(audio_path, sample_rate=self.args.exp.sample_rate)},step=self.it, commit=False)\n        #TODO: log spectrogram of the audio file to wandb\n        spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n\n        self.wandb_run.log({\"spec_\"+str(string): spec_sample}, step=self.it, commit=True)\n\n    def apply_low_pass(self, seg, filter, typefilter):\n        y=utils_bwe.apply_low_pass(seg, filter, self.args.tester.bandwidth_extension.filter.type) \n        return y\n\n    def apply_lowpass_fcA(self, seg, params):\n        freqs=torch.fft.rfftfreq(self.args.tester.blind_bwe.NFFT, d=1/self.args.exp.sample_rate).to(seg.device)\n        H=blind_bwe_utils.design_filter(params[0], params[1], freqs)\n        xfilt=blind_bwe_utils.apply_filter(seg,H,self.args.tester.blind_bwe.NFFT)\n        return xfilt\n\n    def prepare_filter(self, sample_rate, typefilter):\n        filter=utils_bwe.prepare_filter(self.args, sample_rate )\n        return filter\n    \n    def test_real_blind_bwe_complete(self, typefilter=\"fc_A\", compute_sweep=False, use_denoiser=True):\n        #raise NotImplementedError\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        \n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        filename=self.args.tester.complete_recording.path\n        path, basename=os.path.split(filename)\n        print(path, basename)\n\n        d,fs=sf.read(filename)\n        #stereo to mono\n        print(\"d.shape\",d.shape)\n        if len(d.shape)>1:\n            d=d.mean(axis=1)\n\n\n        degraded=torch.Tensor(d)\n\n        segL=self.args.exp.audio_len\n\n        ix_first=self.args.exp.sample_rate*self.args.tester.complete_recording.ix_start #index of the first segment to be processed, might have to depend on the sample rate\n\n        #for i, (degraded,  filename) in enumerate(tqdm(zip(test_set_data,  test_set_names))):\n\n        print(\"filename\",filename)\n        n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n        degraded=degraded.float().to(self.device).unsqueeze(0)\n        print(n)\n\n        if self.args.tester.complete_recording.use_denoiser:\n                print(\"denoising\")\n                if fs!=self.args.tester.denoiser.sample_rate_denoiser:\n                    degraded=torchaudio.functional.resample(degraded, fs, self.args.tester.denoiser.sample_rate_denoiser)\n                degraded=self.apply_denoiser(degraded)\n                if fs!=self.args.tester.denoiser.sample_rate_denoiser:\n                    degraded=torchaudio.functional.resample(degraded,  self.args.tester.denoiser.sample_rate_denoiser, self.args.exp.sample_rate)\n                path_reconstructed=utils_logging.write_audio_file(degraded, self.args.exp.sample_rate, basename+\".denoised.wav\", path=path)\n        else:\n                print(\"no denoising\")\n                degraded=torchaudio.functional.resample(degraded, fs, self.args.exp.sample_rate)\n\n        print(\"seg shape\",degraded.shape)\n\n        std= degraded.std(-1)\n        degraded=self.args.tester.complete_recording.std*degraded/std.unsqueeze(-1)\n        #add noise\n        if self.args.tester.complete_recording.SNR_extra_noise!=\"None\":\n            #contaminate a bit with white noise\n            SNR=10**(self.args.tester.complete_recording.SNR_extra_noise/10)\n            sigma2_s=torch.Tensor([self.args.tester.complete_recording.std**2]).to(degraded.device)\n            sigma=torch.sqrt(sigma2_s/SNR)\n            degraded+=sigma*torch.randn(degraded.shape).to(degraded.device)\n\n\n        if self.args.tester.complete_recording.n_segments_blindstep==1:\n            y=degraded[...,ix_first:ix_first+segL]\n        else:\n            #initialize y with the first segment and repeat it\n            y=degraded[...,ix_first:ix_first+segL].repeat(self.args.tester.complete_recording.n_segments_blindstep,1)\n            for j in range(0, self.args.tester.complete_recording.n_segments_blindstep):\n                #random index\n                ix=np.random.randint(0, degraded.shape[-1]-segL)\n                y[j,...]=degraded[...,ix:ix+segL]\n        \n        print(\"y shape\",y.shape)\n\n            \n\n        #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, fs)\n        #print(\"scale\",scale) #TODO I should calculate this with the whole track, not just the first segment\n\n        #y=y*10**(scale/20)\n        #degraded=degraded*10**(scale/20)\n\n\n\n        rid=False\n        outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n        pred, estimated_filter =outputs\n\n        #now I will just throw away the first segment and process the rest of the signal with the estimated filter. Later I should think of a better way to do it\n\n        overlap=int(self.args.tester.complete_recording.overlap*self.args.exp.sample_rate)\n        hop=segL-overlap\n\n        final_pred=torch.zeros_like(degraded)\n        final_pred[0, ix_first:ix_first+segL]=pred[0]\n\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n        L=degraded.shape[-1]\n\n        #modify the sampler, so that it is computationally cheaper\n\n        discard_end=200 #discard the last 50 samples of the segment, because they are not used for the prediction\n        discard_start=0  #discard the first 50 samples of the segment, because they are not used for the prediction\n\n        #first segment\n        ix=0\n        seg=degraded[...,ix:ix+segL]\n        pred=self.sampler.predict_bwe(seg, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False)\n\n        previous_pred=pred[..., 0:segL-discard_end]\n\n        final_pred[...,ix:ix+segL-discard_end]=previous_pred\n        ix+=segL-overlap-discard_end\n\n        y_masked=torch.zeros_like(pred, device=self.device)\n        mask=torch.ones_like(seg, device=self.device)\n        mask[...,overlap::]=0\n\n        hann_window=torch.hann_window(overlap*2, device=self.device)\n\n        while ix<L-segL-discard_end-discard_start:\n            y_masked[...,0:overlap]=previous_pred[...,segL-overlap-discard_end:]\n            seg=degraded[...,ix:ix+segL]\n\n            pred=self.sampler.predict_bwe_AR(seg, y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n            previous_pred=pred[..., 0:segL-discard_end]\n\n\n            final_pred[...,ix:ix+segL-discard_end]=previous_pred\n            #do a little bit of overlap and add with a hann window to avoid discontinuities\n            #final_pred[...,ix:ix+overlap]=final_pred[...,ix:ix+overlap]*hann_window[overlap::]+pred[...,0:overlap]*hann_window[0:overlap]\n            #final_pred[...,ix+overlap:ix+segL]=pred[...,overlap::]\n\n            path, basename=os.path.split(filename)\n            print(path, basename)\n\n\n            path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n\n            ix+=segL-overlap-discard_end\n\n        #skipping the last segment, which is not complete, I am lazy\n        seg=degraded[...,ix::]\n        y_masked[...,0:overlap]=pred[...,-overlap::]\n\n        if seg.shape[-1]<segL:\n            #cat zeros\n            seg_zp=torch.cat((seg, torch.zeros((1,segL-seg.shape[-1]), device=self.device)), -1)\n\n            #the cat zeroes will also be part of the observed signal, so I need to mask them\n            y_masked[...,seg.shape[-1]:segL]=seg_zp[...,seg.shape[-1]:segL]\n            mask[...,seg.shape[-1]:segL]=0\n\n        else:\n            seg_zp=seg[...,0:segL]\n\n\n        pred=self.sampler.predict_bwe_AR(seg_zp,y_masked, estimated_filter, typefilter,rid=False, test_filter_fit=False, compute_sweep=False, mask=mask)\n\n        final_pred[...,ix::]=pred[...,0:seg.shape[-1]]\n\n        final_pred=final_pred*std.unsqueeze(-1)/self.args.tester.complete_recording.std\n        #final_pred=final_pred*10**(-scale/20)\n\n        #extract path from filename\n\n\n        path_reconstructed=utils_logging.write_audio_file(final_pred, self.args.exp.sample_rate, basename+\".reconstructed.wav\", path=path)\n               \n               \n               \n    def test_real_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\",\"degraded_audio\", \"reconstructed audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n        \n        \n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        path=self.args.tester.blind_bwe.real_recordings.path\n        audio_files=glob(path+\"/*.wav\")\n        print(audio_files, path)\n        test_set_data=[]\n        test_set_fs=[]\n        test_set_names=[]\n        for i in range(self.args.tester.blind_bwe.real_recordings.num_samples):\n            d,fs=sf.read(audio_files[i])\n            #print(d.shape, self.args.exp.audio_len)\n            #if d.shape[-1] >= self.args.exp.audio_len:\n            #    d=d[...,0:self.args.exp.audio_len]\n            test_set_data.append(torch.Tensor(d))\n            test_set_fs.append(fs)\n            print(\"fs\",fs)\n            print(\"len\",len(d))\n            test_set_names.append(audio_files[i])\n\n        for i, (degraded,  filename, fs) in enumerate(tqdm(zip(test_set_data,  test_set_names, test_set_fs))):\n                print(\"filename\",filename)\n                n=os.path.splitext(os.path.basename(filename))[0]+typefilter\n                seg=degraded.float().to(self.device).unsqueeze(0)\n                print(n)\n\n                print(\"dsds FS\",fs)\n\n                print(\"seg shape\",seg.shape)\n                seg=torchaudio.functional.resample(seg, fs, self.args.exp.sample_rate)\n                print(\"seg shape\",seg.shape)\n                ix_start=self.args.tester.blind_bwe\n\n                seg=seg[...,self.args.exp.sample_rate*0:self.args.exp.sample_rate*0+self.args.exp.audio_len]\n                y=seg\n                print(\"y shape\",y.shape)\n                #normalize???\n                std= y.std(-1)\n                y=self.args.tester.blind_bwe.sigma_norm*y/std.unsqueeze(-1)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y,fs)\n                #print(\"scale\",scale)\n                #y=y*10**(scale/20)\n\n\n\n               \n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"real_blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n                pred=pred*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                y=y*std.unsqueeze(-1)/self.args.tester.blind_bwe.sigma_norm\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"real_blind_bwe\"+\"reconstructed\"])\n               \n               \n               \n                fig_est_filter=blind_bwe_utils.plot_filter(estimated_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"real_blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate))\n               \n                if typefilter==\"fc_A\":\n                    test_blind_bwe_table_filters.add_data(i, \n                        wandb.Html(path_est_filter),\n                    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"real_blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters)\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"real_blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    def test_blind_bwe(self, typefilter=\"fc_A\", compute_sweep=False):\n\n        #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"estimate_filter\"] \n        columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\"] \n        test_blind_bwe_table_audio = wandb.Table(columns=columns)\n\n        if typefilter==\"3rdoct\":\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\", \"gt_filter\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        elif typefilter==\"fc_A\":\n            columns=[\"id\", \"estimate_filter\"]\n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n        else:\n            columns=[\"id\", \"raw_filter\", \"unnorm_filter\", \"estimate_filter_interpolated\"] \n            test_blind_bwe_table_filters = wandb.Table(columns=columns)\n\n        log_spec=False\n        if log_spec:\n            columns=[\"id\", \"original_spec\", \"degraded_spec\", \"reconstructed_spec\", \"degraded_estimate_spec\"] \n            test_blind_bwe_table_spec = wandb.Table(columns=columns)\n\n        if not self.do_blind_bwe or self.test_set is None:\n            print(\"No test set specified, skipping inpainting test\")\n            return\n\n        assert self.test_set is not None\n\n        if len(self.test_set) == 0:\n            print(\"No samples found in test set\")\n\n        if typefilter==\"fc_A\":\n            fc=self.args.tester.blind_bwe.test_filter.fc\n            A=self.args.tester.blind_bwe.test_filter.A\n            da_filter=torch.Tensor([fc, A]).to(self.device)\n        else:\n            #prepare lowpass filters\n            da_filter=self.prepare_filter( self.args.exp.sample_rate, typefilter) #standardly designed filter\n            da_filter=da_filter.to(self.device)\n        \n        #res=torch.zeros((len(self.test_set),self.args.exp.audio_len))\n        #the conditional sampling uses batch_size=1, so we need to loop over the test set. This is done for simplicity, but it is not the most efficient way to do it.\n        for i, (original, fs,  filename) in enumerate(tqdm(self.test_set)):\n                n=os.path.splitext(filename[0])[0]+typefilter\n                seg=original.float().to(self.device)\n                seg=self.resample_audio(seg, fs)\n\n                if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                    orig_std=seg.std(-1)\n                    seg=sigma_norm*seg/orig_std\n        \n                elif self.args.tester.blind_bwe.gain_boost != 0:\n                    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                    #add gain boost (in dB)\n                    seg=seg*10**(self.args.tester.blind_bwe.gain_boost/20)\n                \n\n                #apply lowpass filter\n                if typefilter==\"fc_A\":\n                    y=self.apply_lowpass_fcA(seg, da_filter)\n                else:\n                    y=self.apply_low_pass(seg,da_filter, typefilter)\n\n                #add noise to the observations for regularization\n                if self.args.tester.blind_bwe.SNR_observations!=\"None\":\n                    SNR=10**(self.args.tester.blind_bwe.SNR_observations/10)\n                    sigma2_s=torch.var(y, -1)\n                    sigma=torch.sqrt(sigma2_s/SNR)\n                    y+=sigma*torch.randn(y.shape).to(y.device)\n                    #y=y+self.args.tester.blind_bwe.sigma_observations*torch.randn_like(y)\n\n                #scale=self.LTAS_processor.rescale_audio_to_LTAS(y, self.args.exp.sample_rate)\n                #print(\"applied scale\",scale)\n                #y=y*10**(scale/20)\n\n                #y=utils_bwe.apply_low_pass(seg, self.filter, self.args.tester.bandwidth_extension.filter.type) \n               \n                #if self.args.tester.noise_in_observations_SNR != \"None\":\n                #    SNR=10**(self.args.tester.noise_in_observations_SNR/10)\n                #    sigma2_s=torch.var(y, -1)\n                #    sigma=torch.sqrt(sigma2_s/SNR)\n                #    y+=sigma*torch.randn(y.shape).to(y.device)\n               \n                rid=True\n                if compute_sweep:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid, compute_sweep=compute_sweep)\n                else:\n                    outputs=self.sampler.predict_blind_bwe(y, rid=rid)\n               \n                if rid:\n                    if compute_sweep:\n                        pred, estimated_filter, data_denoised,  t, data_filters, data_norms, data_grads  =outputs\n                        np.save(self.paths[\"blind_bwe\"]+\"data_t\"+str(i)+\".npy\", t.cpu().numpy())\n                        np.save(self.paths[\"blind_bwe\"]+\"data_denoised\"+str(i)+\".npy\", data_denoised)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_filters\"+str(i)+\".npy\", data_filters)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_norms\"+str(i)+\".npy\", data_norms)\n                        np.save(self.paths[\"blind_bwe\"]+\"data_grads\"+str(i)+\".npy\", data_grads)\n                    else:\n                        pred, estimated_filter, data_denoised,  t, data_filters  =outputs\n               \n                    #the logged outputs are:\n                    #   pred: the reconstructed audio\n                    #   estimated_filter: the estimated filter ([fc, A])\n                    #   t: the time step vector\n                    #   data_denoised: a vector with the denoised audio for each time step\n                    #   data_filters: a vector with the estimated filters for each time step\n               \n                else:\n                    pred, estimated_filter =outputs\n               \n               \n               \n                y_est=self.apply_lowpass_fcA(seg, estimated_filter)\n\n                if self.args.tester.blind_bwe.gain_boost ==\"None\":\n                    sigma_norm=self.args.tester.blind_bwe.sigma_norm\n                    assert orig_std is not None\n                    seg=orig_std*seg/sigma_norm\n                elif self.args.tester.blind_bwe.gain_boost != 0:\n                    #compensate gain boost\n                    #this is a bit of a hack, but the diffusion model is biased to generate more high-frequency content when the power is higher.\n                    #add gain boost (in dB)\n                    y_est=y_est*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                    pred=pred*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                    seg=seg*10**(-self.args.tester.blind_bwe.gain_boost/20)\n                    y=y*10**(-self.args.tester.blind_bwe.gain_boost/20)\n\n                #y_est=y_est*10**(-scale/20)\n                #pred=pred*10**(-scale/20)\n                #seg=seg*10**(-scale/20)\n                #y=y*10**(-scale/20)\n               \n                #if self.use_wandb:\n                #add to principal wandb table\n                #columns=[\"id\", \"original_audio\", \"degraded_audio\", \"reconstructed_audio\", \"degraded_estimate_audio\", \"raw_filter\", \"unnorm_filter\" \"estimate_filter_interpolated\"] \n               \n                #acum_orig[i,:]=seg\n                #acum_deg[i,:]=y\n                #acum_bwe[i,:]=pred\n                #acum_ded_est[i,:]=y_est\n               \n                \n                path_original=utils_logging.write_audio_file(seg, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"original\"])\n               \n                path_degraded=utils_logging.write_audio_file(y, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded\"])\n               \n                path_reconstructed=utils_logging.write_audio_file(pred, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"reconstructed\"])\n               \n                path_degrade_estimate=utils_logging.write_audio_file(y_est, self.args.exp.sample_rate, n, path=self.paths[\"blind_bwe\"+\"degraded_estimate\"])\n               \n               \n                #will probably crash here!\n                fig_est_filter=blind_bwe_utils.plot_filter(da_filter.cpu(),estimated_filter.cpu(), NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate)\n                path_est_filter=os.path.join(self.paths[\"blind_bwe\"], str(i)+\"_raw_filter.html\")\n                fig_est_filter.write_html(path_est_filter, auto_play = False)\n               \n               \n               \n                test_blind_bwe_table_audio.add_data(i, \n                        wandb.Audio(path_original, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_degraded, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_reconstructed, sample_rate=self.args.exp.sample_rate),\n                        wandb.Audio(path_degrade_estimate, sample_rate=self.args.exp.sample_rate))\n               \n                #if typefilter==\"fc_A\":\n                #    test_blind_bwe_table_filters.add_data(i, \n                #        wandb.Html(path_est_filter),\n                #    )\n               \n               \n                if log_spec:\n                    pass\n                    #spec_sample=utils_logging.plot_spectrogram_from_raw_audio(preds, self.args.logging.stft)\n                    #test_blind_bwe_table_spec.add_data(i, \n               \n                print(\"before fig_animation_sig\",data_denoised.shape, t.shape )\n                fig_animation_sig=utils_logging.diffusion_spec_animation(self.paths[\"blind_bwe\"],data_denoised, t[:-1], self.args.logging.stft,name=\"denoised_estimate_sig_animation\"+str(i) )\n               \n                print(data_filters.shape)\n                #will crash here\n                fig_animation_filter=blind_bwe_utils.animation_filter(self.paths[\"blind_bwe\"],data_filters,t[:-1], NFFT=self.args.tester.blind_bwe.NFFT, fs=self.args.exp.sample_rate, name=\"filter_animation\"+str(i) )\n               \n               \n                #fig_join_animation=utils_logging.diffusion_joint_animation()\n                #log the \n\n        self.wandb_run.log({\"table_blind_bwe_audio\": test_blind_bwe_table_audio}, commit=True) \n        self.wandb_run.log({\"table_blind_bwe_filters\": test_blind_bwe_table_filters}, commit=True) \n\n    \n        #do I want to save this audio file locally? I think I do, but I'll have to figure out how to do it\n    def dodajob(self):\n        self.setup_wandb()\n        for m in self.args.tester.modes:\n\n            if m==\"unconditional\":\n                print(\"testing unconditional\")\n                self.sample_unconditional()\n            if m==\"unconditional_diffwavesr\":\n                print(\"testing unconditional\")\n                self.sample_unconditional_diffwavesr()\n            self.it+=1\n            if m==\"blind_bwe\":\n                print(\"TESTING BLIND BWE\")\n                self.test_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe\":\n                print(\"TESTING REAL BLIND BWE\")\n                self.test_real_blind_bwe(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"real_blind_bwe_complete\":\n                #process the whole audio file\n                #Estimate the filter in the first chunk, and then apply it to the rest of the audio file (using a little bit of overlap or outpainting)\n                print(\"TESTING REAL BLIND BWE COMPLETE\")\n                self.test_real_blind_bwe_complete(compute_sweep=self.args.tester.blind_bwe.compute_sweep)\n            if m==\"bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep)\n            if m==\"formal_test_bwe\": \n                print(\"TESTING NORMAL BWE\")\n                self.formal_test_bwe(test_filter_fit=self.args.tester.bandwidth_extension.test_filter_fit, compute_sweep=self.args.tester.bandwidth_extension.compute_sweep, typefilter=\"firwin\", blind=self.args.tester.formal_test.blind)\n        self.it+=1", ""]}
