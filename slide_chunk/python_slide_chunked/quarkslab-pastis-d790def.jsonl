{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python3\n# coding: utf-8\n\"\"\"Installation script for PASTIS.\"\"\"\n\nfrom setuptools import setup\n\nwith open(\"README.md\") as f:\n    lines = f.readlines()\n    README = \"\\n\".join(lines[4:7]+lines[51:])\n", "\n\nsetup(\n    name=\"pastis-framework\",\n    version=\"1.0.4\",\n    description=\"PASTIS framework for collaborative fuzzing\",\n    long_description=README,\n    long_description_content_type='text/markdown',\n    packages=[\n        \"libpastis\",", "    packages=[\n        \"libpastis\",\n        \"libpastis.proto\",\n        \"pastisbroker\",\n        \"pastisbenchmark\",\n        \"pastisaflpp\",\n        \"aflppbroker\",\n        \"pastishf\",\n        \"hfbroker\",\n        \"pastisdse\",", "        \"hfbroker\",\n        \"pastisdse\",\n        \"pastisttbroker\"\n    ],\n    package_dir={\n        # AFL++\n        \"pastisaflpp\": \"engines/pastis-aflpp/pastisaflpp\",\n        \"aflppbroker\": \"engines/pastis-aflpp/broker-addon/aflppbroker\",\n        # Honggfuzz\n        \"pastishf\": \"engines/pastis-honggfuzz/pastishf\",", "        # Honggfuzz\n        \"pastishf\": \"engines/pastis-honggfuzz/pastishf\",\n        \"hfbroker\": \"engines/pastis-honggfuzz/broker-addon/hfbroker\",\n        # Triton\n        \"pastisdse\": \"engines/pastis-triton/pastisdse\",\n        \"pastisttbroker\": \"engines/pastis-triton/broker-addon/pastisttbroker\"\n    },\n    url=\"https://github.com/quarkslab/pastis\",\n    project_urls={\n        \"Documentation\": \"https://quarkslab.github.io/pastis/\",", "    project_urls={\n        \"Documentation\": \"https://quarkslab.github.io/pastis/\",\n        \"Bug Tracker\": \"https://github.com/quarkslab/pastis/issues\",\n        \"Source\": \"https://github.com/quarkslab/pastis\"\n    },\n    setup_requires=[],\n    install_requires=[\n        \"protobuf\",\n        \"pyzmq\",\n        \"psutil\",", "        \"pyzmq\",\n        \"psutil\",\n        \"aenum\",\n        \"lief\",\n        \"python-magic\",\n        \"click\",\n        \"coloredlogs\",\n        \"quokka-project\",\n        \"watchdog\",\n        \"pydantic\",", "        \"watchdog\",\n        \"pydantic\",\n        \"matplotlib\",\n        \"joblib\",\n        \"rich\",\n        \"tritondse\",\n    ],\n    tests_require=[],\n    license=\"AGPL-3.0\",\n    author=\"Quarkslab\",", "    license=\"AGPL-3.0\",\n    author=\"Quarkslab\",\n    classifiers=[\n        'Topic :: Security',\n        'Environment :: Console',\n        'Operating System :: OS Independent',\n    ],\n    test_suite=\"\",\n    scripts=[\n        'bin/pastis-broker',", "    scripts=[\n        'bin/pastis-broker',\n        'bin/pastis-benchmark',\n        'bin/pastisd',\n        'engines/pastis-honggfuzz/bin/pastis-honggfuzz',\n        'engines/pastis-triton/bin/pastis-triton',\n        'engines/pastis-aflpp/bin/pastis-aflpp'\n    ]\n)\n", ")\n"]}
{"filename": "engines/pastis-honggfuzz/broker-addon/hfbroker/__init__.py", "chunked_list": ["# built-in imports\nfrom pathlib import Path\nfrom typing import Union, Tuple, List, Optional\nimport json\n\n# third-party import\nimport lief\nfrom typing import Type\n\nfrom libpastis import FuzzingEngineDescriptor, EngineConfiguration", "\nfrom libpastis import FuzzingEngineDescriptor, EngineConfiguration\nfrom libpastis.types import ExecMode, CoverageMode, FuzzMode\n\n\nclass HonggfuzzConfigurationInterface(EngineConfiguration):\n    def __init__(self, args: List[str] = None):\n        self._argvs = [] if args is None else args # Argument to send on the command line\n\n    @staticmethod\n    def new() -> 'HonggfuzzConfigurationInterface':\n        return HonggfuzzConfigurationInterface()\n\n    @staticmethod\n    def from_file(filepath: Path) -> 'HonggfuzzConfigurationInterface':\n        with open(filepath, \"r\") as f:\n            return HonggfuzzConfigurationInterface(f.read().split())\n\n    @staticmethod\n    def from_str(s: str) -> 'HonggfuzzConfigurationInterface':\n        return HonggfuzzConfigurationInterface(s.split())\n\n    def to_str(self) -> str:\n        return \" \".join(self._argvs)\n\n    def get_coverage_mode(self) -> CoverageMode:\n        \"\"\"\n        Current coverage mode selected in the file.\n        Always EDGE for Honggfuzz\n        \"\"\"\n        return CoverageMode.AUTO\n\n    def set_target(self, target: int) -> None:\n        # Note: Giving a target to Honggfuzz does not\n        # do anything as Honggfuzz is not directed.\n        pass", "\n\nclass HonggfuzzEngineDescriptor(FuzzingEngineDescriptor):\n\n    NAME = \"HONGGFUZZ\"\n    SHORT_NAME = \"HF\"\n    VERSION = \"1.0.0\"  # Should be in sync with hfwrapper.__version__\n\n    HF_PERSISTENT_SIG = b\"\\x01_LIBHFUZZ_PERSISTENT_BINARY_SIGNATURE_\\x02\\xFF\"\n\n    config_class = HonggfuzzConfigurationInterface\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def accept_file(binary_file: Path) -> Tuple[bool, Optional[ExecMode], Optional[FuzzMode]]:\n        p = lief.parse(str(binary_file))\n        if not p:\n            return False, None, None\n\n        # Search for HF instrumentation\n        instrumented = False\n        for f in p.functions:\n            if \"hfuzz_\" in f.name:\n                instrumented = True\n                break\n        if not instrumented:\n            return True, ExecMode.PERSISTENT, FuzzMode.BINARY_ONLY\n\n        # Search for persistent magic\n        exmode = ExecMode.SINGLE_EXEC  # by default single_exec\n        sections = {x.name: x for x in p.sections}\n        if '.rodata' in sections:\n            rodata_content = bytearray(sections['.rodata'].content)\n            if HonggfuzzEngineDescriptor.HF_PERSISTENT_SIG in rodata_content:\n                exmode = ExecMode.PERSISTENT\n        else:\n            if 'HF_ITER' in (x.name for x in p.imported_functions):  # More dummy method\n                exmode = ExecMode.PERSISTENT\n        return True, exmode, FuzzMode.INSTRUMENTED\n\n    @staticmethod\n    def supported_coverage_strategies() -> List[CoverageMode]:\n        return [CoverageMode.AUTO]", ""]}
{"filename": "engines/pastis-honggfuzz/pastishf/replay.py", "chunked_list": ["import subprocess\nfrom io import BytesIO\nfrom typing import Optional, List, Tuple\nimport re\nimport logging\n\n\nEXAMPLES = '''\nREGEX_1:\n==373876==ERROR: AddressSanitizer: stack-buffer-overflow on address 0x7ffecfb907a4 at pc 0x00000043e9e7 bp 0x7ffecfb90660 sp 0x7ffecfb8fdf8", "REGEX_1:\n==373876==ERROR: AddressSanitizer: stack-buffer-overflow on address 0x7ffecfb907a4 at pc 0x00000043e9e7 bp 0x7ffecfb90660 sp 0x7ffecfb8fdf8\n\nREGEX_2:\n==372317==AddressSanitizer: WARNING: unexpected format specifier in printf interceptor: %\ufffd (reported once per process)\n==372317==AddressSanitizer CHECK failed: /build/llvm-toolchain-9-NoMHhU/llvm-toolchain-9-9.0.1/compiler-rt/lib/asan/../sanitizer_common/sani\n'''\n\n\nclass Replay(object):\n\n    INTRINSIC_REGEX = rb\".*REACHED ID (\\d+)\"\n    ASAN_REGEX_1 = rb\"^==\\d+==ERROR: AddressSanitizer: (\\S+) (.*)\"\n    ASAN_REGEX_2 = rb\"^==\\d+==AddressSanitizer:? ([^:]+): (.*)\"\n    HF_FETCH = b\"HonggfuzzFetchData()\"\n\n    def __init__(self):\n        self._process = None\n        self._alert_covered = []\n        self._alert_crash = None\n        self._is_hang = False\n        self._asan_line = \"\"\n        self._asan_bugtype = \"\"\n        self._is_hf_fetch = False\n\n        # For debugging\n        self.stdout, self.stderr = None, None\n\n    @staticmethod\n    def run(binary_path: str, args: List[str] = [], stdin_file=None, timeout=None, cwd=None):\n        replay = Replay()\n        replay._process = subprocess.Popen([binary_path]+args, stdin=open(stdin_file, 'rb'), stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n        try:\n            replay.stdout, replay.stderr = replay._process.communicate(timeout=timeout)\n            found = replay.__parse_output(replay.stdout)  # In case intrinsic are on output\n            found |= replay.__parse_output(replay.stderr)\n\n            if not found and replay.has_crashed():  # Crash that we were not able to link to an ASAN error\n                if replay._alert_covered: # Thus take the latest alert and consider it is the origin\n                    replay._alert_crash = replay._alert_covered[-1]\n        except subprocess.TimeoutExpired:\n            replay._is_hang = True\n\n        return replay\n\n    def is_asan_without_crash(self) -> bool:\n        \"\"\" Return True if an ASAN WARNING was shown without errors \"\"\"\n        return self._asan_bugtype and not self.has_crashed()\n\n    def is_hf_iter_crash(self) -> bool:\n        \"\"\"\n        Check we did not hit a call to HF_ITER that would lead\n        to such unexpected return code -1\n        \"\"\"\n        return self._is_hf_fetch and self.has_crashed()\n\n    @property\n    def returncode(self) -> int:\n        return self._process.returncode\n\n    @property\n    def alert_covered(self) -> List[int]:\n        \"\"\" Alert covered \"\"\"\n        return self._alert_covered\n\n    def has_hanged(self) -> bool:\n        \"\"\" Return true if the target hanged during its replay \"\"\"\n        return self._is_hang\n\n    def has_crashed(self) -> bool:\n        \"\"\" Return whether the execution has crashed or not \"\"\"\n        if self._process.returncode:\n            return self._process.returncode != 0\n        else:\n            return False\n\n    @property\n    def crashing_id(self) -> Optional[int]:\n        \"\"\" Return the alert identifier that made the program to crash (last one seen) \"\"\"\n        return self._alert_crash\n\n    def asan_info(self) -> Tuple[str, str]:\n        \"\"\" In case of crash return ASAN info gather by parsing \"\"\"\n        return self._asan_bugtype, self._asan_line\n\n\n    def __parse_output(self, raw_output: bytes) -> bool:\n        \"\"\" Return True if a vuln was matched \"\"\"\n        matched_vuln = False\n        for line in BytesIO(raw_output).readlines():\n            # Check if its a line of intrinsic output\n            m = re.match(self.INTRINSIC_REGEX, line)\n            if m:\n                id = int(m.groups()[0])\n                self._alert_covered.append(id)\n\n            # Check if its a line of ASAN\n            m1 = re.match(self.ASAN_REGEX_1, line)\n            m2 = re.match(self.ASAN_REGEX_2, line)\n            if m1 or m2:\n                if matched_vuln:\n                    logging.warning(f\"already matched ASAN with {self._asan_bugtype} now {m1.groups() if m1 else m2.groups()}\")\n                    continue\n                matched_vuln = True\n                if self._alert_covered: # Try getting last alert (and consider it to be the origin)\n                    self._alert_crash = self._alert_covered[-1]\n                # Else cannot link the crash to an ID\n\n                # Extract end of line and parameter\n                topic, details = m1.groups() if m1 else m2.groups()\n                self._asan_bugtype = topic.decode(errors=\"replace\")\n                self._asan_line = details.decode(errors=\"replace\")\n\n            if self.HF_FETCH in line:  # In case it is an issue with a HF_ITER read\n                self._is_hf_fetch = True\n\n        return matched_vuln", "\nclass Replay(object):\n\n    INTRINSIC_REGEX = rb\".*REACHED ID (\\d+)\"\n    ASAN_REGEX_1 = rb\"^==\\d+==ERROR: AddressSanitizer: (\\S+) (.*)\"\n    ASAN_REGEX_2 = rb\"^==\\d+==AddressSanitizer:? ([^:]+): (.*)\"\n    HF_FETCH = b\"HonggfuzzFetchData()\"\n\n    def __init__(self):\n        self._process = None\n        self._alert_covered = []\n        self._alert_crash = None\n        self._is_hang = False\n        self._asan_line = \"\"\n        self._asan_bugtype = \"\"\n        self._is_hf_fetch = False\n\n        # For debugging\n        self.stdout, self.stderr = None, None\n\n    @staticmethod\n    def run(binary_path: str, args: List[str] = [], stdin_file=None, timeout=None, cwd=None):\n        replay = Replay()\n        replay._process = subprocess.Popen([binary_path]+args, stdin=open(stdin_file, 'rb'), stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n        try:\n            replay.stdout, replay.stderr = replay._process.communicate(timeout=timeout)\n            found = replay.__parse_output(replay.stdout)  # In case intrinsic are on output\n            found |= replay.__parse_output(replay.stderr)\n\n            if not found and replay.has_crashed():  # Crash that we were not able to link to an ASAN error\n                if replay._alert_covered: # Thus take the latest alert and consider it is the origin\n                    replay._alert_crash = replay._alert_covered[-1]\n        except subprocess.TimeoutExpired:\n            replay._is_hang = True\n\n        return replay\n\n    def is_asan_without_crash(self) -> bool:\n        \"\"\" Return True if an ASAN WARNING was shown without errors \"\"\"\n        return self._asan_bugtype and not self.has_crashed()\n\n    def is_hf_iter_crash(self) -> bool:\n        \"\"\"\n        Check we did not hit a call to HF_ITER that would lead\n        to such unexpected return code -1\n        \"\"\"\n        return self._is_hf_fetch and self.has_crashed()\n\n    @property\n    def returncode(self) -> int:\n        return self._process.returncode\n\n    @property\n    def alert_covered(self) -> List[int]:\n        \"\"\" Alert covered \"\"\"\n        return self._alert_covered\n\n    def has_hanged(self) -> bool:\n        \"\"\" Return true if the target hanged during its replay \"\"\"\n        return self._is_hang\n\n    def has_crashed(self) -> bool:\n        \"\"\" Return whether the execution has crashed or not \"\"\"\n        if self._process.returncode:\n            return self._process.returncode != 0\n        else:\n            return False\n\n    @property\n    def crashing_id(self) -> Optional[int]:\n        \"\"\" Return the alert identifier that made the program to crash (last one seen) \"\"\"\n        return self._alert_crash\n\n    def asan_info(self) -> Tuple[str, str]:\n        \"\"\" In case of crash return ASAN info gather by parsing \"\"\"\n        return self._asan_bugtype, self._asan_line\n\n\n    def __parse_output(self, raw_output: bytes) -> bool:\n        \"\"\" Return True if a vuln was matched \"\"\"\n        matched_vuln = False\n        for line in BytesIO(raw_output).readlines():\n            # Check if its a line of intrinsic output\n            m = re.match(self.INTRINSIC_REGEX, line)\n            if m:\n                id = int(m.groups()[0])\n                self._alert_covered.append(id)\n\n            # Check if its a line of ASAN\n            m1 = re.match(self.ASAN_REGEX_1, line)\n            m2 = re.match(self.ASAN_REGEX_2, line)\n            if m1 or m2:\n                if matched_vuln:\n                    logging.warning(f\"already matched ASAN with {self._asan_bugtype} now {m1.groups() if m1 else m2.groups()}\")\n                    continue\n                matched_vuln = True\n                if self._alert_covered: # Try getting last alert (and consider it to be the origin)\n                    self._alert_crash = self._alert_covered[-1]\n                # Else cannot link the crash to an ID\n\n                # Extract end of line and parameter\n                topic, details = m1.groups() if m1 else m2.groups()\n                self._asan_bugtype = topic.decode(errors=\"replace\")\n                self._asan_line = details.decode(errors=\"replace\")\n\n            if self.HF_FETCH in line:  # In case it is an issue with a HF_ITER read\n                self._is_hf_fetch = True\n\n        return matched_vuln", ""]}
{"filename": "engines/pastis-honggfuzz/pastishf/workspace.py", "chunked_list": ["# builtin imports\nfrom typing import Callable\nimport time\nimport tempfile\nimport os\nimport logging\nfrom pathlib import Path\nfrom hashlib import md5\n\n# third-party imports", "\n# third-party imports\nfrom watchdog.events import FileSystemEventHandler\nfrom watchdog.observers import Observer\n\n\nclass Workspace(FileSystemEventHandler):\n\n    HFUZZ_WS_ENV_VAR = \"HFUZZ_WS\"\n    DEFAULT_WS_PATH = \"hfuzz_workspace\"\n    STATS_FILE = \"statsfile.log\"\n\n    def __init__(self):\n        self.observer = Observer()\n        self.modif_callbacks = {}  # Map fullpath -> callback\n        self.created_callbacks = {}\n        self.root_dir = None\n        self._setup_workspace()\n\n    def _setup_workspace(self):\n        ws = os.environ.get(self.HFUZZ_WS_ENV_VAR, None)\n        if ws is None:\n            self.root_dir = (Path(tempfile.gettempdir()) / self.DEFAULT_WS_PATH) / str(time.time()).replace(\".\", \"\")\n        else:\n            self.root_dir = Path(ws)  # Use the one provided\n\n        for d in [self.target_dir, self.input_dir, self.dynamic_input_dir, self.corpus_dir, self.crash_dir, self.stats_dir]:\n            d.mkdir(parents=True)\n\n    @property\n    def target_dir(self):\n        return self.root_dir / 'target'\n\n    @property\n    def input_dir(self):\n        return self.root_dir / 'inputs' / 'initial'\n\n    @property\n    def dynamic_input_dir(self):\n        return self.root_dir / 'inputs' / 'dynamic'\n\n    @property\n    def corpus_dir(self):\n        return self.root_dir / 'outputs' / 'coverage'\n\n    @property\n    def crash_dir(self):\n        return self.root_dir / 'outputs' / 'crashes'\n\n    @property\n    def stats_dir(self):\n        return self.root_dir / 'stats'\n\n    @property\n    def stats_file(self):\n        return self.stats_dir / self.STATS_FILE\n\n    def on_modified(self, event):\n        path = Path(event.src_path)\n        if path.is_dir():\n            return  # We don't care about directories\n        if path.parent in self.modif_callbacks:\n            self.modif_callbacks[path.parent](path)  # call the callback\n        else:\n            pass  # Do nothing at the moment\n\n    def on_created(self, event):\n        path = Path(event.src_path)\n        if path.is_dir():\n            return  # We don't care about directories\n        if path.parent in self.created_callbacks:\n            self.created_callbacks[path.parent](path)  # call the callback\n        else:\n            pass  # Do nothing at the moment\n\n    def add_file_modification_hook(self, path: str, callback: Callable):\n        self.observer.schedule(self, path=path, recursive=True)\n        self.modif_callbacks[path] = callback\n\n    def add_creation_hook(self, path: str, callback: Callable):\n        self.observer.schedule(self, path=path, recursive=True)\n        self.created_callbacks[path] = callback\n\n    def start(self):\n        self.observer.start()\n\n    def stop(self):\n        self.observer.stop()", ""]}
{"filename": "engines/pastis-honggfuzz/pastishf/driver.py", "chunked_list": ["# builtin imports\nimport logging\nfrom pathlib import Path\nimport stat\nimport threading\nimport time\nimport tempfile\nimport hashlib\nfrom typing import List, Union\n", "from typing import List, Union\n\n\n# Pastis & triton imports\nfrom libpastis import ClientAgent, BinaryPackage, SASTReport\nfrom libpastis.types import CheckMode, CoverageMode, ExecMode, FuzzingEngineInfo, SeedInjectLoc, SeedType, State, \\\n                            LogLevel, AlertData, FuzzMode\n\n# Local imports\nimport pastishf", "# Local imports\nimport pastishf\nfrom pastishf.replay import Replay\nfrom pastishf.honggfuzz import HonggfuzzProcess\nfrom pastishf.workspace import Workspace\n\n\n# Inotify logs are very talkative, set them to ERROR\nfor logger in (logging.getLogger(x) for x in [\"watchdog.observers.inotify_buffer\", 'watchdog.observers', \"watchdog\"]):\n    logger.setLevel(logging.ERROR)", "for logger in (logging.getLogger(x) for x in [\"watchdog.observers.inotify_buffer\", 'watchdog.observers', \"watchdog\"]):\n    logger.setLevel(logging.ERROR)\n\n\nclass HonggfuzzDriver:\n\n    def __init__(self, agent: ClientAgent, telemetry_frequency: int = 30):\n\n        # Internal objects\n        self._agent = agent\n        self.workspace = Workspace()\n        self.honggfuzz = HonggfuzzProcess()\n\n        # Parameters received through start_received\n        self.__exec_mode = None   # SINGLE_RUN, PERSISTENT\n        self.__check_mode = None  # CHECK_ALL, ALERT_ONLY\n        self.__seed_inj = None    # STDIN or ARGV\n        self.__report = None      # SAST report if provided\n\n        # Target data\n        self.__package = None\n        self.__target_args = None  # Kept for replay\n\n        self.__setup_agent()\n\n        # Configure hookds on workspace\n        self.workspace.add_creation_hook(self.workspace.corpus_dir, self.__send_seed)\n        self.workspace.add_creation_hook(self.workspace.crash_dir, self.__send_crash)\n        self.workspace.add_file_modification_hook(self.workspace.stats_dir, self.__send_telemetry)\n\n        # Telemetry frequency\n        self._tel_frequency = telemetry_frequency\n        self._tel_last = time.time()\n\n        # Runtime data\n        self._tot_seeds = 0\n        self._seed_recvs = set()  # Seed received to make sure NOT to send them back\n\n        # Variables for replay\n        self._replay_thread = None\n        self._queue_to_send = []\n        self._started = False\n\n    @staticmethod\n    def hash_seed(seed: bytes):\n        return hashlib.md5(seed).hexdigest()\n\n    def start(self, package: BinaryPackage, argv: List[str], exmode: ExecMode, fuzzmode: FuzzMode,\n              seed_inj: SeedInjectLoc, engine_args: str):\n        # Write target to disk.\n        self.__package = package\n        self.__target_args = argv\n\n        self.workspace.start()  # Start looking at directories\n\n        logging.info(\"Start process\")\n        if not self.honggfuzz.start(str(self.__package.executable_path.absolute()),\n                             argv,\n                             self.workspace,\n                             exmode,\n                             fuzzmode,\n                             seed_inj == SeedInjectLoc.STDIN,\n                             engine_args,\n                             str(package.dictionary.absolute()) if package.dictionary else None):\n            self._agent.send_log(LogLevel.ERROR, \"Cannot start target\")\n        self._started = True\n\n        # Start the replay worker (note that the queue might already have started to be filled by agent thread)\n        self._replay_thread = threading.Thread(target=self.replay_worker, daemon=True)\n        self._replay_thread.start()\n\n    def stop(self):\n        self.honggfuzz.stop()\n        self.workspace.stop()\n        self._started = False  # should stop the replay thread\n\n    def replay_worker(self):\n        while True:\n            if not self._started:\n                break  # Break when the fuzzer stops\n            if self._queue_to_send:\n                filename, res = self._queue_to_send.pop(0)\n                if not self.__check_seed_alert(filename, is_crash=res):\n                    break\n            time.sleep(0.05)\n\n    @property\n    def started(self):\n        return self._started\n\n    def add_seed(self, seed: bytes):\n        seed_path = self.workspace.dynamic_input_dir / f\"seed-{hashlib.md5(seed).hexdigest()}\"\n        seed_path.write_bytes(seed)\n\n    def init_agent(self, remote: str = \"localhost\", port: int = 5555):\n        self._agent.register_start_callback(self.start_received)  # Register start because launched manually (not by pastisd)\n        self._agent.connect(remote, port)\n        self._agent.start()\n        # Send initial HELLO message, whick will make the Broker send the START message.\n        self._agent.send_hello([FuzzingEngineInfo(\"HONGGFUZZ\", pastishf.__version__, \"hfbroker\")])\n\n    def run(self):\n        self.honggfuzz.wait()\n\n    def __setup_agent(self):\n        # Register callbacks.\n        self._agent.register_seed_callback(self.__seed_received)\n        self._agent.register_stop_callback(self.__stop_received)\n\n    def __send_seed(self, filename: Path):\n        self.__send(filename, SeedType.INPUT)\n\n    def __send_crash(self, filename: Path):\n        self.__send(filename, SeedType.CRASH)\n\n    def __send(self, filename: Path, typ: SeedType):\n        self._tot_seeds += 1\n        file = Path(filename)\n        raw = file.read_bytes()\n        h = self.hash_seed(raw)\n        logging.debug(f'[{typ.name}] Sending new: {h} [{self._tot_seeds}]')\n        if h not in self._seed_recvs:\n            self._agent.send_seed(typ, raw)\n        else:\n            logging.info(\"seed (previously sent) do not send it back\")\n        self._queue_to_send.append((filename, True if typ == SeedType.CRASH else False))\n\n    def __check_seed_alert(self, filename: Path, is_crash: bool) -> bool:\n        p = Path(filename)\n        # Only rerun the seed if in alert only mode and a SAST report was provided\n        if self.__check_mode == CheckMode.ALERT_ONLY and self.__report:\n\n            # Rerun the program with the seed\n            run = Replay.run(self.__package.executable_path.absolute(), self.__target_args, stdin_file=filename, timeout=5, cwd=str(self.workspace.target_dir))\n\n            if run.is_hf_iter_crash():\n                self.dual_log(LogLevel.ERROR, f\"Disable replay engine (because code uses HF_ITER)\")\n                return False\n\n            # Iterate all covered alerts\n            for id in run.alert_covered:\n                alert = self.__report.get_alert(id)\n                if not alert.covered:\n                    alert.covered = True\n                    logging.info(f\"New alert covered {alert} [{alert.id}]\")\n                    self._agent.send_alert_data(AlertData(alert.id, alert.covered, False, p.read_bytes()))\n\n            # Check if the target has crashed and if so tell the broker which one\n            if run.has_crashed() or run.is_asan_without_crash():  # Also consider ASAN warning as detection\n                if not run.crashing_id:\n                    self.dual_log(LogLevel.WARNING, f\"Crash on {filename.name} but can't link it to a SAST alert\")\n                else:\n                    alert = self.__report.get_alert(run.crashing_id)\n                    if not alert.validated:\n                        alert.validated = True\n                        bugt, aline = run.asan_info()\n                        self.dual_log(LogLevel.INFO, f\"Honggfuzz new alert validated {alert} [{alert.id}] ({aline})  (asan no crash: {run.is_asan_without_crash()})\")\n                        self._agent.send_alert_data(AlertData(alert.id, alert.covered, alert.validated, p.read_bytes()))\n            else:\n                if is_crash:\n                    self.dual_log(LogLevel.WARNING, f\"crash not reproducible by rerunning seed: {filename.name}\")\n\n            if run.has_hanged():  # Honggfuzz does not stores 'hangs' it will have been sent as corpus or crash\n                self.dual_log(LogLevel.WARNING, f\"Seed {filename} was hanging in replay\")\n        return True\n\n    def __send_telemetry(self, filename: Path):\n        now = time.time()\n        if now < (self._tel_last + self._tel_frequency):\n            return\n        self._tel_last = now\n\n        logging.debug(f'[TELEMETRY] Stats file updated: {filename}')\n\n        with open(filename, 'r') as stats_file:\n            try:\n                stats = stats_file.readlines()[-1]\n\n                if not stats or stats.startswith(\"#\"):\n                    return\n\n                stats = stats.split(',')\n\n                # Stats format:\n                #   unix_time, last_cov_update, total_exec, exec_per_sec,\n                #   crashes, unique_crashes, hangs, edge_cov, block_cov\n                state = State.RUNNING\n                last_cov_update = int(stats[1])\n                total_exec = int(stats[2])\n                exec_per_sec = int(stats[3])\n                timeout = int(stats[6])             # aka hangs.\n                coverage_edge = int(stats[7])       # aka edge_cov.\n                coverage_block = int(stats[8])      # aka block_cov.\n\n                # NOTE: `cycle` and `coverage_path` does not apply for Honggfuzz.\n                self._agent.send_telemetry(state=state,\n                                           exec_per_sec=exec_per_sec,\n                                           total_exec=total_exec,\n                                           timeout=timeout,\n                                           coverage_block=coverage_block,\n                                           coverage_edge=coverage_edge,\n                                           last_cov_update=last_cov_update)\n            except:\n                logging.error(f'Error retrieving stats!')\n\n    def start_received(self, fname: str, binary: bytes, engine: FuzzingEngineInfo, exmode: ExecMode, fuzzmode: FuzzMode, chkmode: CheckMode,\n                       _: CoverageMode, seed_inj: SeedInjectLoc, engine_args: str, argv: List[str], sast_report: str = None):\n        logging.info(f\"[START] bin:{fname} engine:{engine.name} exmode:{exmode.name} fuzzmode:{fuzzmode.name} seedloc:{seed_inj.name} chk:{chkmode.name}\")\n        if self.started:\n            self._agent.send_log(LogLevel.CRITICAL, \"Instance already started!\")\n            return\n\n        if engine.name != \"HONGGFUZZ\":\n            logging.error(f\"Wrong fuzzing engine received {engine.name} while I am Honggfuzz\")\n            self._agent.send_log(LogLevel.ERROR, f\"Invalid fuzzing engine received {engine.name} can't do anything\")\n            return\n        if engine.version != pastishf.__version__:\n            logging.error(f\"Wrong fuzzing engine version {engine.version} received\")\n            self._agent.send_log(LogLevel.ERROR, f\"Invalid fuzzing engine version {engine.version} do nothing\")\n            return\n\n        # Retrieve package out of the binary received\n        try:\n            package = BinaryPackage.from_binary(fname, binary, self.workspace.target_dir)\n        except FileNotFoundError:\n            logging.error(\"Invalid package received\")\n            return\n\n        if sast_report:\n            logging.info(\"Loading SAST report\")\n            self.__report = SASTReport.from_json(sast_report)\n\n        self.__check_mode = chkmode  # CHECK_ALL, ALERT_ONLY\n\n        self.start(package, argv, exmode, fuzzmode, seed_inj, engine_args)\n\n    def __seed_received(self, typ: SeedType, seed: bytes):\n        h = self.hash_seed(seed)\n        logging.info(f\"[SEED] received  {h} ({typ.name})\")\n        self._seed_recvs.add(h)\n\n        # HACK: Maybe doing it more nicely ?\n        if len(seed) > 1024*8: # HF_INPUT_DEFAULT_SIZE\n            logging.debug(f\"crop seed {h} received to fit 8Kb\")\n            seed = seed[:1024*8]\n\n        self.add_seed(seed)\n\n    def __stop_received(self):\n        logging.info(f\"[STOP]\")\n\n        self.stop()\n\n    def dual_log(self, level: LogLevel, message: str) -> None:\n        \"\"\"\n        Helper function to log message both in the local log system and also\n        to the broker.\n\n        :param level: LogLevel message type\n        :param message: string message to log\n        :return: None\n        \"\"\"\n        mapper = {LogLevel.DEBUG: \"debug\",\n                  LogLevel.INFO: \"info\",\n                  LogLevel.CRITICAL: \"critical\",\n                  LogLevel.WARNING: \"warning\",\n                  LogLevel.ERROR: \"error\"}\n        log_f = getattr(logging, mapper[level])\n        log_f(message)\n        self._agent.send_log(level, message)\n\n    def add_initial_seed(self, file: Union[str, Path]):\n        p = Path(file)\n        logging.info(f\"add initial seed {file.name}\")\n        # Write seed to disk.\n        seed_path = self.workspace.input_dir / p.name\n        seed_path.write_bytes(p.read_bytes())\n\n    @staticmethod\n    def honggfuzz_available() -> bool:\n        return HonggfuzzProcess.hfuzz_environ_check()", ""]}
{"filename": "engines/pastis-honggfuzz/pastishf/__init__.py", "chunked_list": ["import os\nfrom pathlib import Path\nfrom typing import Optional\nimport subprocess\n\nfrom .driver import HonggfuzzDriver\nfrom .replay import Replay\nfrom .honggfuzz import HonggfuzzProcess, HonggfuzzNotFound\nfrom .workspace import Workspace\n", "from .workspace import Workspace\n\n__version__ = \"1.0.0\"\n\n# Honggfuzz env variables\nHFUZZ_ENV_VAR = \"HFUZZ_WS\"\nHFUZZ_PATH_VAR = \"HFUZZ_PATH\"\nHFUZZ_THREADS = \"HFUZZ_THREADS\"\n\n\ndef spawn_online_honggfuzz(workspace: Optional[Path], hf_path: Optional[str], port: int = 5555, threads: int=0):\n    env = os.environ\n    env[HFUZZ_ENV_VAR] = str(workspace.absolute())\n    if hf_path:\n        env[HFUZZ_PATH_VAR] = str(hf_path)\n    if threads:\n        env[HFUZZ_THREADS] = str(threads)\n    cmd_line_honggfuzz = [\"pastis-honggfuzz\", \"online\", \"-p\", f\"{port}\"]\n    subprocess.Popen(cmd_line_honggfuzz, env=env, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)", "\n\ndef spawn_online_honggfuzz(workspace: Optional[Path], hf_path: Optional[str], port: int = 5555, threads: int=0):\n    env = os.environ\n    env[HFUZZ_ENV_VAR] = str(workspace.absolute())\n    if hf_path:\n        env[HFUZZ_PATH_VAR] = str(hf_path)\n    if threads:\n        env[HFUZZ_THREADS] = str(threads)\n    cmd_line_honggfuzz = [\"pastis-honggfuzz\", \"online\", \"-p\", f\"{port}\"]\n    subprocess.Popen(cmd_line_honggfuzz, env=env, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)", "\n"]}
{"filename": "engines/pastis-honggfuzz/pastishf/honggfuzz.py", "chunked_list": ["import logging\nimport os\nimport subprocess\nimport re\nimport signal\nimport time\nfrom pathlib import Path\nfrom typing import Optional\nfrom libpastis.types import ExecMode, FuzzMode\n", "from libpastis.types import ExecMode, FuzzMode\n\n# Local imports\nfrom .workspace import Workspace\n\n\nclass HonggfuzzNotFound(Exception):\n    \"\"\" Issue raised on \"\"\"\n    pass\n", "\n\nclass HonggfuzzProcess:\n\n    HFUZZ_ENV_VAR = \"HFUZZ_PATH\"\n    HFUZZ_THREADS_VAR = \"HFUZZ_THREADS\"\n    BINARY = \"honggfuzz\"\n    STAT_FILE = \"statsfile.log\"\n    VERSION = \"2.1\"\n\n    def __init__(self, path: str = None):\n        path = os.environ.get(self.HFUZZ_ENV_VAR) if path is None else path\n        if path is None:\n            raise Exception(\"Invalid Honggfuzz path provided\")\n\n        path = Path(path)\n        if not path.exists():\n            raise Exception('Invalid HFUZZ_PATH path!')\n        elif path.is_file() and path.name == self.BINARY:\n            self.__path = path\n        elif path.is_dir():\n            self.__path = Path(path) / self.BINARY\n            if not path.exists():\n                raise Exception(\"Can't find honggfuzz in HFUZZ_PATH path!\")\n\n        self._threads = os.environ.get(self.HFUZZ_THREADS_VAR)\n\n        self.__process = None\n\n    def start(self, target: str, target_arguments: list[str], workspace: Workspace, exmode: ExecMode, fuzzmode: FuzzMode,\n              stdin: bool, engine_args: str, dictionary: Optional[str] = None) -> bool:\n        if not stdin:\n            if \"@@\" in target_arguments:  # Change '@@' for ___FILE___\n                idx = target_arguments.index(\"@@\")\n                target_arguments[idx] = \"___FILE___\"\n            else:\n                if \"___FILE___\" not in target_arguments:\n                    logging.error(f\"seed provided via ARGV but can't find '@@'/___FILE___ on program argv\")\n                    return False\n\n        # Build target command line.\n        target_cmdline = f\"{target} {' '.join(target_arguments)}\"\n\n        HFQBDI_LIB_PATH = os.getenv('HFQBDI_LIB_PATH')\n\n        if fuzzmode == FuzzMode.BINARY_ONLY and HFQBDI_LIB_PATH is None:\n            logging.error(f\"target in BINARY_ONLY but can't find HFQBDI_LIB_PATH\")\n            return False\n\n        # Build fuzzer arguments.\n        hfuzz_arguments = ' '.join([\n            f\"--statsfile {workspace.stats_file}\",\n            f\"--stdin_input\" if stdin else \"\",\n            f\"--persistent\" if exmode == ExecMode.PERSISTENT or fuzzmode == FuzzMode.BINARY_ONLY else \"\",\n            f\"--env HFQBDI_FS=1\" if fuzzmode == FuzzMode.BINARY_ONLY else \"\",\n            f\"--env LD_LIBRARY_PATH={HFQBDI_LIB_PATH}\" if fuzzmode == FuzzMode.BINARY_ONLY else \"\",\n            f\"--env LD_PRELOAD={HFQBDI_LIB_PATH}/libHFQBDIpreload.so\" if fuzzmode == FuzzMode.BINARY_ONLY else \"\",\n            f\"--env LD_BIND_NOW=1\" if fuzzmode == FuzzMode.BINARY_ONLY else \"\",\n            re.sub(r\"\\s\", \" \", engine_args),  # Any arguments coming right from the broker (remove \\r\\n)\n            f\"--logfile logfile.log\",\n            f\"--input {workspace.input_dir}\",\n            f\"--dynamic_input {workspace.dynamic_input_dir}\",\n            f\"--output {workspace.corpus_dir}\",\n            f\"--crashdir {workspace.crash_dir}\",\n            f\"--workspace {workspace.root_dir}\",\n            f\"--threads {self._threads}\" if self._threads else \"\",\n            f\"--dict {dictionary}\" if dictionary is not None else \"\"\n        ])\n\n        # Build fuzzer command line.\n        hfuzz_cmdline = f'{self.__path} {hfuzz_arguments} -- {target_cmdline}'\n\n        logging.info(f\"Run Honggfuzz with: {hfuzz_cmdline}\")\n        logging.debug(f\"\\tWorkspace: {workspace.root_dir}\")\n\n        # Remove empty strings when converting the command to a list.\n        command = list(filter(None, hfuzz_cmdline.split(' ')))\n\n        # Create a new fuzzer process and set it apart into a new process group.\n        self.__process = subprocess.Popen(command, cwd=str(workspace.root_dir), preexec_fn=os.setsid)\n\n        logging.debug(f'Process pid: {self.__process.pid}')\n        return True\n\n    @property\n    def instanciated(self):\n        return self.__process is not None\n\n    def stop(self):\n        if self.__process:\n            logging.debug(f'Stopping process with pid: {self.__process.pid}')\n            os.killpg(os.getpgid(self.__process.pid), signal.SIGTERM)\n        else:\n            logging.debug(f\"Honggfuzz process seem's already killed\")\n\n    def wait(self):\n        while not self.instanciated:\n            time.sleep(0.1)\n        self.__process.wait()\n\n    @staticmethod\n    def hfuzz_environ_check() -> bool:\n        path = os.environ.get(HonggfuzzProcess.HFUZZ_ENV_VAR)\n        if path is None:\n            return False\n        else:\n            return (Path(path) / HonggfuzzProcess.BINARY).exists()", ""]}
{"filename": "engines/pastis-aflpp/pastisaflpp/aflpp.py", "chunked_list": ["# builtin imports\nimport logging\nimport os\nimport re\nimport signal\nimport subprocess\nimport time\nfrom typing import Optional, Union\nfrom pathlib import Path\n", "from pathlib import Path\n\n# third-party imports\nimport shutil\nfrom libpastis.types import ExecMode, FuzzMode\n\n# Local imports\nfrom .workspace import Workspace\n\n\nclass AFLPPNotFound(Exception):\n    \"\"\" Issue raised on \"\"\"\n    pass", "\n\nclass AFLPPNotFound(Exception):\n    \"\"\" Issue raised on \"\"\"\n    pass\n\n\nclass AFLPPProcess:\n\n    AFLPP_ENV_VAR = \"AFLPP_PATH\"\n    BINARY = \"afl-fuzz\"\n    STAT_FILE = \"fuzzer_stats\"\n    VERSION = \"master\"\n\n    def __init__(self, path: str = None):\n\n        self.__path = self.find_alfpp_binary(path)\n        if self.__path is None:\n            raise FileNotFoundError(\"Can't find AFL++ path (afl-fuzz)\")\n\n        self.__process = None\n        self.__logfile = None\n\n    @staticmethod\n    def find_alfpp_binary(root_dir: Union[Path, str]) -> Optional[Path]:\n        if root_dir:\n            bin_path = Path(root_dir) / AFLPPProcess.BINARY\n            return bin_path if bin_path.exists() else None\n        else:\n            aflpp_path = os.environ.get(AFLPPProcess.AFLPP_ENV_VAR)\n            return Path(aflpp_path) / 'afl-fuzz' if aflpp_path else shutil.which(AFLPPProcess.BINARY)\n\n    def start(self, target: str, target_arguments: list[str], workspace: Workspace, exmode: ExecMode, fuzzmode: FuzzMode, stdin: bool, engine_args: str, cmplog: Optional[str] = None, dictionary: Optional[str] = None):\n        # Check that we have '@@' if input provided via argv\n        if not stdin:\n            if \"@@\" not in target_arguments:\n                logging.error(f\"seed provided via ARGV but can't find '@@' on program argv\")\n                return\n        # Build target command line.\n        target_cmdline = f\"{target} {' '.join(target_arguments)}\"\n\n        # Build fuzzer arguments.\n        # NOTE: Assuming the target receives inputs from stdin.\n        aflpp_arguments = ' '.join([\n            re.sub(r\"\\s\", \" \", engine_args),  # Any arguments coming right from the broker (remove \\r\\n)\n            f\"-Q\" if fuzzmode == FuzzMode.BINARY_ONLY else \"\",\n            f\"-M main\", # Master MODE, seed distribution is ensured by the broker\n            f\"-i {workspace.input_dir}\",\n            f\"-F {workspace.dynamic_input_dir}\",\n            f\"-o {workspace.output_dir}\",\n            f\"-c {cmplog}\" if cmplog is not None else \"\",\n            f\"-x {dictionary}\" if dictionary is not None else \"\"\n        ])\n\n        # Export environmental variables.\n        os.environ[\"AFL_NO_UI\"] = \"1\"\n        os.environ[\"AFL_QUIET\"] = \"1\"\n        os.environ[\"AFL_IMPORT_FIRST\"] = \"1\"\n        os.environ[\"AFL_AUTORESUME\"] = \"1\"\n\n        # NOTE This prevents having to configure the system before running\n        #      AFL++.\n        # TODO Should we skip these steps?\n        os.environ[\"AFL_SKIP_CPUFREQ\"] = \"1\"\n        os.environ[\"AFL_I_DONT_CARE_ABOUT_MISSING_CRASHES\"] = \"1\"\n\n        # Build fuzzer command line.\n        aflpp_cmdline = f'{self.__path} {aflpp_arguments} -- {target_cmdline}'\n\n        logging.info(f\"Run AFL++: {aflpp_cmdline}\")\n        logging.debug(f\"\\tWorkspace: {workspace.root_dir}\")\n\n        # Remove empty strings when converting the command to a list.\n        command = list(filter(None, aflpp_cmdline.split(' ')))\n\n        # Open logfile (stdout will be redirected to this file).\n        self.__logfile = open(workspace.root_dir / 'logfile.log', 'w')\n\n        # Create a new fuzzer process and set it apart into a new process group.\n        self.__process = subprocess.Popen(command, cwd=str(workspace.root_dir), preexec_fn=os.setsid, stdout=self.__logfile)\n\n        logging.debug(f'Process pid: {self.__process.pid}')\n\n    @property\n    def instanciated(self):\n        return self.__process is not None\n\n    def stop(self):\n        if self.__process:\n            logging.debug(f'Stopping process with pid: {self.__process.pid}')\n            os.killpg(os.getpgid(self.__process.pid), signal.SIGTERM)\n        else:\n            logging.debug(f\"AFL++ process seems already killed\")\n\n        if self.__logfile:\n            self.__logfile.close()\n\n    def wait(self):\n        while not self.instanciated:\n            time.sleep(0.1)\n        self.__process.wait()\n        logging.info(f\"Fuzzer terminated with code : {self.__process.returncode}\")\n\n    @staticmethod\n    def aflpp_environ_check() -> bool:\n        return os.environ.get(AFLPPProcess.AFLPP_ENV_VAR) is not None", ""]}
{"filename": "engines/pastis-aflpp/pastisaflpp/replay.py", "chunked_list": ["import subprocess\nfrom io import BytesIO\nfrom typing import Optional, List, Tuple\nimport re\nimport logging\n\n\nEXAMPLES = '''\nREGEX_1:\n==373876==ERROR: AddressSanitizer: stack-buffer-overflow on address 0x7ffecfb907a4 at pc 0x00000043e9e7 bp 0x7ffecfb90660 sp 0x7ffecfb8fdf8", "REGEX_1:\n==373876==ERROR: AddressSanitizer: stack-buffer-overflow on address 0x7ffecfb907a4 at pc 0x00000043e9e7 bp 0x7ffecfb90660 sp 0x7ffecfb8fdf8\n\nREGEX_2:\n==372317==AddressSanitizer: WARNING: unexpected format specifier in printf interceptor: %\ufffd (reported once per process)\n==372317==AddressSanitizer CHECK failed: /build/llvm-toolchain-9-NoMHhU/llvm-toolchain-9-9.0.1/compiler-rt/lib/asan/../sanitizer_common/sani\n'''\n\n\nclass Replay(object):\n\n    INTRINSIC_REGEX = rb\".*REACHED ID (\\d+)\"\n    ASAN_REGEX_1 = rb\"^==\\d+==ERROR: AddressSanitizer: (\\S+) (.*)\"\n    ASAN_REGEX_2 = rb\"^==\\d+==AddressSanitizer:? ([^:]+): (.*)\"\n\n    def __init__(self):\n        self._process = None\n        self._alert_covered = []\n        self._alert_crash = None\n        self._is_hang = False\n        self._asan_line = \"\"\n        self._asan_bugtype = \"\"\n\n        # For debugging\n        self.stdout, self.stderr = None, None\n\n    @staticmethod\n    def run(binary_path: str, args: List[str] = [], stdin_file=None, timeout=None, cwd=None):\n        replay = Replay()\n        replay._process = subprocess.Popen([binary_path]+args, stdin=open(stdin_file, 'rb'), stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n        try:\n            replay.stdout, replay.stderr = replay._process.communicate(timeout=timeout)\n            found = replay.__parse_output(replay.stdout)  # In case intrinsic are on output\n            found |= replay.__parse_output(replay.stderr)\n\n            if not found and replay.has_crashed():  # Crash that we were not able to link to an ASAN error\n                if replay._alert_covered: # Thus take the latest alert and consider it is the origin\n                    replay._alert_crash = replay._alert_covered[-1]\n        except subprocess.TimeoutExpired:\n            replay._is_hang = True\n\n        return replay\n\n    def is_asan_without_crash(self) -> bool:\n        \"\"\" Return True if an ASAN WARNING was shown without errors \"\"\"\n        return self._asan_bugtype and not self.has_crashed()\n\n    @property\n    def returncode(self) -> int:\n        return self._process.returncode\n\n    @property\n    def alert_covered(self) -> List[int]:\n        \"\"\" Alert covered \"\"\"\n        return self._alert_covered\n\n    def has_hanged(self) -> bool:\n        \"\"\" Return true if the target hanged during its replay \"\"\"\n        return self._is_hang\n\n    def has_crashed(self) -> bool:\n        \"\"\" Return whether the execution has crashed or not \"\"\"\n        if self._process.returncode:\n            return self._process.returncode != 0\n        else:\n            return False\n\n    @property\n    def crashing_id(self) -> Optional[int]:\n        \"\"\" Return the alert identifier that made the program to crash (last one seen) \"\"\"\n        return self._alert_crash\n\n    def asan_info(self) -> Tuple[str, str]:\n        \"\"\" In case of crash return ASAN info gather by parsing \"\"\"\n        return self._asan_bugtype, self._asan_line\n\n    def __parse_output(self, raw_output: bytes) -> bool:\n        \"\"\" Return True if a vuln was matched \"\"\"\n        matched_vuln = False\n        for line in BytesIO(raw_output).readlines():\n            # Check if its a line of intrinsic output\n            m = re.match(self.INTRINSIC_REGEX, line)\n            if m:\n                id = int(m.groups()[0])\n                self._alert_covered.append(id)\n\n            # Check if its a line of ASAN\n            m1 = re.match(self.ASAN_REGEX_1, line)\n            m2 = re.match(self.ASAN_REGEX_2, line)\n            if m1 or m2:\n                if matched_vuln:\n                    logging.warning(f\"already matched ASAN with {self._asan_bugtype} now {m1.groups() if m1 else m2.groups()}\")\n                    continue\n                matched_vuln = True\n                if self._alert_covered: # Try getting last alert (and consider it to be the origin)\n                    self._alert_crash = self._alert_covered[-1]\n                # Else cannot link the crash to an ID\n\n                # Extract end of line and parameter\n                topic, details = m1.groups() if m1 else m2.groups()\n                self._asan_bugtype = topic.decode(errors=\"replace\")\n                self._asan_line = details.decode(errors=\"replace\")\n\n        return matched_vuln", "\nclass Replay(object):\n\n    INTRINSIC_REGEX = rb\".*REACHED ID (\\d+)\"\n    ASAN_REGEX_1 = rb\"^==\\d+==ERROR: AddressSanitizer: (\\S+) (.*)\"\n    ASAN_REGEX_2 = rb\"^==\\d+==AddressSanitizer:? ([^:]+): (.*)\"\n\n    def __init__(self):\n        self._process = None\n        self._alert_covered = []\n        self._alert_crash = None\n        self._is_hang = False\n        self._asan_line = \"\"\n        self._asan_bugtype = \"\"\n\n        # For debugging\n        self.stdout, self.stderr = None, None\n\n    @staticmethod\n    def run(binary_path: str, args: List[str] = [], stdin_file=None, timeout=None, cwd=None):\n        replay = Replay()\n        replay._process = subprocess.Popen([binary_path]+args, stdin=open(stdin_file, 'rb'), stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n        try:\n            replay.stdout, replay.stderr = replay._process.communicate(timeout=timeout)\n            found = replay.__parse_output(replay.stdout)  # In case intrinsic are on output\n            found |= replay.__parse_output(replay.stderr)\n\n            if not found and replay.has_crashed():  # Crash that we were not able to link to an ASAN error\n                if replay._alert_covered: # Thus take the latest alert and consider it is the origin\n                    replay._alert_crash = replay._alert_covered[-1]\n        except subprocess.TimeoutExpired:\n            replay._is_hang = True\n\n        return replay\n\n    def is_asan_without_crash(self) -> bool:\n        \"\"\" Return True if an ASAN WARNING was shown without errors \"\"\"\n        return self._asan_bugtype and not self.has_crashed()\n\n    @property\n    def returncode(self) -> int:\n        return self._process.returncode\n\n    @property\n    def alert_covered(self) -> List[int]:\n        \"\"\" Alert covered \"\"\"\n        return self._alert_covered\n\n    def has_hanged(self) -> bool:\n        \"\"\" Return true if the target hanged during its replay \"\"\"\n        return self._is_hang\n\n    def has_crashed(self) -> bool:\n        \"\"\" Return whether the execution has crashed or not \"\"\"\n        if self._process.returncode:\n            return self._process.returncode != 0\n        else:\n            return False\n\n    @property\n    def crashing_id(self) -> Optional[int]:\n        \"\"\" Return the alert identifier that made the program to crash (last one seen) \"\"\"\n        return self._alert_crash\n\n    def asan_info(self) -> Tuple[str, str]:\n        \"\"\" In case of crash return ASAN info gather by parsing \"\"\"\n        return self._asan_bugtype, self._asan_line\n\n    def __parse_output(self, raw_output: bytes) -> bool:\n        \"\"\" Return True if a vuln was matched \"\"\"\n        matched_vuln = False\n        for line in BytesIO(raw_output).readlines():\n            # Check if its a line of intrinsic output\n            m = re.match(self.INTRINSIC_REGEX, line)\n            if m:\n                id = int(m.groups()[0])\n                self._alert_covered.append(id)\n\n            # Check if its a line of ASAN\n            m1 = re.match(self.ASAN_REGEX_1, line)\n            m2 = re.match(self.ASAN_REGEX_2, line)\n            if m1 or m2:\n                if matched_vuln:\n                    logging.warning(f\"already matched ASAN with {self._asan_bugtype} now {m1.groups() if m1 else m2.groups()}\")\n                    continue\n                matched_vuln = True\n                if self._alert_covered: # Try getting last alert (and consider it to be the origin)\n                    self._alert_crash = self._alert_covered[-1]\n                # Else cannot link the crash to an ID\n\n                # Extract end of line and parameter\n                topic, details = m1.groups() if m1 else m2.groups()\n                self._asan_bugtype = topic.decode(errors=\"replace\")\n                self._asan_line = details.decode(errors=\"replace\")\n\n        return matched_vuln", ""]}
{"filename": "engines/pastis-aflpp/pastisaflpp/workspace.py", "chunked_list": ["# builtin imports\nfrom typing import Callable\nimport time\nimport tempfile\nimport os\nimport logging\nfrom pathlib import Path\nfrom hashlib import md5\n\n# third-party imports", "\n# third-party imports\nfrom watchdog.events import FileSystemEventHandler\nfrom watchdog.observers import Observer\n\n\nclass Workspace(FileSystemEventHandler):\n\n    AFLPP_WS_ENV_VAR = \"AFLPP_WS\"\n    DEFAULT_WS_PATH = \"aflpp_workspace\"\n    STATS_FILE = \"fuzzer_stats\"\n\n    def __init__(self):\n        self.observer = Observer()\n        self.modif_callbacks = {}  # Map fullpath -> callback\n        self.created_callbacks = {}\n        self.root_dir = None\n        self._setup_workspace()\n\n    def _setup_workspace(self):\n        ws = os.environ.get(self.AFLPP_WS_ENV_VAR, None)\n        if ws is None:\n            self.root_dir = (Path(tempfile.gettempdir()) / self.DEFAULT_WS_PATH) / str(time.time()).replace(\".\", \"\")\n        else:\n            self.root_dir = Path(ws)  # Use the one provided\n\n\n        for d in [self.target_dir, self.input_dir, self.dynamic_input_dir, self.corpus_dir, self.crash_dir]:\n            d.mkdir(parents=True)\n\n        # Create dummy input file.\n        # AFLPP requires that the initial seed directory is not empty.\n        # TODO Is there a better approach to this?\n        seed_path = self.input_dir / 'seed-dummy'\n        seed_path.write_bytes(b'A')\n\n    @property\n    def target_dir(self):\n        return self.root_dir / 'target'\n\n    @property\n    def input_dir(self):\n        return self.root_dir / 'inputs' / 'initial'\n\n    @property\n    def dynamic_input_dir(self):\n        return self.root_dir / 'inputs' / 'dynamic'\n\n    @property\n    def output_dir(self):\n        return self.root_dir / 'outputs'\n\n    @property\n    def corpus_dir(self):\n        return self.output_dir / 'main' / 'queue'\n\n    @property\n    def crash_dir(self):\n        return self.output_dir / 'main' / 'crashes'\n\n    @property\n    def stats_dir(self):\n        return self.output_dir / 'main'\n\n    @property\n    def stats_file(self):\n        return self.stats_dir / self.STATS_FILE\n\n    def on_modified(self, event):\n        path = Path(event.src_path)\n        if path.is_dir():\n            return  # We don't care about directories\n        if path.parent in self.modif_callbacks:\n            self.modif_callbacks[path.parent](path)  # call the callback\n        else:\n            pass  # Do nothing at the moment\n\n    def on_created(self, event):\n        path = Path(event.src_path)\n        if path.is_dir():\n            return  # We don't care about directories\n        if path.parent in self.created_callbacks:\n            self.created_callbacks[path.parent](path)  # call the callback\n        else:\n            pass  # Do nothing at the moment\n\n    def add_file_modification_hook(self, path: str, callback: Callable):\n        self.observer.schedule(self, path=path, recursive=True)\n        self.modif_callbacks[path] = callback\n\n    def add_creation_hook(self, path: str, callback: Callable):\n        self.observer.schedule(self, path=path, recursive=True)\n        self.created_callbacks[path] = callback\n\n    def start(self):\n        self.observer.start()\n\n    def stop(self):\n        self.observer.stop()", ""]}
{"filename": "engines/pastis-aflpp/pastisaflpp/driver.py", "chunked_list": ["# builtin imports\nimport hashlib\nimport logging\nimport stat\nimport threading\nimport time\n\nfrom pathlib import Path\nfrom typing import List, Union\n", "from typing import List, Union\n\n# Third party imports\nfrom libpastis import ClientAgent, BinaryPackage, SASTReport\nfrom libpastis.types import CheckMode, CoverageMode, ExecMode, FuzzingEngineInfo, SeedInjectLoc, SeedType, State, \\\n                            LogLevel, AlertData, FuzzMode\n\n\n# Local imports\nimport pastisaflpp", "# Local imports\nimport pastisaflpp\nfrom pastisaflpp.replay import Replay\nfrom pastisaflpp.aflpp import AFLPPProcess\nfrom pastisaflpp.workspace import Workspace\n\n\n# Inotify logs are very talkative, set them to ERROR\nfor logger in (logging.getLogger(x) for x in [\"watchdog.observers.inotify_buffer\", 'watchdog.observers', \"watchdog\"]):\n    logger.setLevel(logging.ERROR)", "for logger in (logging.getLogger(x) for x in [\"watchdog.observers.inotify_buffer\", 'watchdog.observers', \"watchdog\"]):\n    logger.setLevel(logging.ERROR)\n\n\nclass AFLPPDriver:\n\n    def __init__(self, agent: ClientAgent, telemetry_frequency: int = 30):\n\n        # Internal objects\n        self._agent = agent\n        self.workspace = Workspace()\n        self.aflpp = AFLPPProcess()\n\n        # Parameters received through start_received\n        self.__exec_mode = None   # SINGLE_RUN, PERSISTENT\n        self.__check_mode = None  # CHECK_ALL, ALERT_ONLY\n        self.__seed_inj = None    # STDIN or ARGV\n        self.__report = None      # Klocwork report if supported\n\n        # Target data\n        self.__package = None\n        self.__target_args = None  # Kept for replay\n\n        self.__setup_agent()\n\n        # Configure hookds on workspace\n        self.workspace.add_creation_hook(self.workspace.corpus_dir, self.__send_seed)\n        self.workspace.add_creation_hook(self.workspace.crash_dir, self.__send_crash)\n        self.workspace.add_file_modification_hook(self.workspace.stats_dir, self.__send_telemetry)\n\n        # Telemetry frequency\n        self._tel_frequency = telemetry_frequency\n        self._tel_last = time.time()\n\n        # Runtime data\n        self._tot_seeds = 0\n        self._seed_recvs = set()  # Seed received to make sure NOT to send them back\n\n        # Variables for replay\n        self._replay_thread = None\n        self._queue_to_send = []\n        self._started = False\n\n    @staticmethod\n    def hash_seed(seed: bytes):\n        return hashlib.md5(seed).hexdigest()\n\n    def start(self, package: BinaryPackage, argv: List[str], exmode: ExecMode, fuzzmode: FuzzMode, seed_inj: SeedInjectLoc, engine_args: str):\n        # Write target to disk.\n        self.__package = package\n        self.__target_args = argv\n\n        self.workspace.start()  # Start looking at directories\n\n        logging.info(f\"Start process (injectloc: {seed_inj.name})\")\n        self.aflpp.start(str(package.executable_path.absolute()),\n                         argv,\n                         self.workspace,\n                         exmode,\n                         fuzzmode,\n                         seed_inj == SeedInjectLoc.STDIN,\n                         engine_args,\n                         str(package.cmplog.absolute()) if package.cmplog else None,\n                         str(package.dictionary.absolute()) if package.dictionary else None)\n        self._started = True\n\n        # Start the replay worker (note that the queue might already have started to be filled by agent thread)\n        self._replay_thread = threading.Thread(target=self.replay_worker, daemon=True)\n        self._replay_thread.start()\n\n    def stop(self):\n        self.aflpp.stop()\n        self.workspace.stop()\n        self._started = False  # should stop the replay thread\n\n    def replay_worker(self):\n        while True:\n            if not self._started:\n                break  # Break when the fuzzer stops\n            if self._queue_to_send:\n                filename, res = self._queue_to_send.pop(0)\n                if not self.__check_seed_alert(filename, is_crash=res):\n                    break\n            time.sleep(0.05)\n\n    @property\n    def started(self):\n        return self._started\n\n    def add_seed(self, seed: bytes):\n        seed_path = self.workspace.dynamic_input_dir / f\"seed-{hashlib.md5(seed).hexdigest()}\"\n        seed_path.write_bytes(seed)\n\n    def init_agent(self, remote: str = \"localhost\", port: int = 5555):\n        self._agent.register_start_callback(self.start_received)  # Register start because launched manually (not by pastisd)\n        self._agent.connect(remote, port)\n        self._agent.start()\n        # Send initial HELLO message, whick will make the Broker send the START message.\n        self._agent.send_hello([FuzzingEngineInfo(\"AFLPP\", pastisaflpp.__version__, \"aflppbroker\")])\n\n    def run(self):\n        self.aflpp.wait()\n\n    def __setup_agent(self):\n        # Register callbacks.\n        self._agent.register_seed_callback(self.__seed_received)\n        self._agent.register_stop_callback(self.__stop_received)\n\n    def __send_seed(self, filename: Path):\n        self.__send(filename, SeedType.INPUT)\n\n    def __send_crash(self, filename: Path):\n        # Skip README file that AFL adds to the crash folder.\n        if filename.name != 'README.txt':\n            self.__send(filename, SeedType.CRASH)\n\n    def __send(self, filename: Path, typ: SeedType):\n        self._tot_seeds += 1\n        file = Path(filename)\n        raw = file.read_bytes()\n        h = self.hash_seed(raw)\n        logging.debug(f'[{typ.name}] Sending new: {h} [{self._tot_seeds}]')\n        if h not in self._seed_recvs:\n            self._agent.send_seed(typ, raw)\n        else:\n            logging.info(\"seed (previously sent) do not send it back\")\n        self._queue_to_send.append((filename, True if typ == SeedType.CRASH else False))\n\n    def __check_seed_alert(self, filename: Path, is_crash: bool) -> bool:\n        p = Path(filename)\n        # Only rerun the seed if in alert only mode and a SAST report was provided\n        if self.__check_mode == CheckMode.ALERT_ONLY and self.__report:\n\n            # Rerun the program with the seed\n            run = Replay.run(self.__package.executable_path.absolute(), self.__target_args, stdin_file=filename, timeout=5, cwd=str(self.workspace.target_dir))\n\n            # FIXME: Do same checks for AFL++ LOOP stuff for persistency mode\n            # if run.is_hf_iter_crash():\n            #     self.dual_log(LogLevel.ERROR, f\"Disable replay engine (because code uses HF_ITER)\")\n            #     return False\n\n            # Iterate all covered alerts\n            for id in run.alert_covered:\n                alert = self.__report.get_alert(id)\n                if not alert.covered:\n                    alert.covered = True\n                    logging.info(f\"New alert covered {alert} [{alert.id}]\")\n                    self._agent.send_alert_data(AlertData(alert.id, alert.covered, False, p.read_bytes()))\n\n            # Check if the target has crashed and if so tell the broker which one\n            if run.has_crashed() or run.is_asan_without_crash():  # Also consider ASAN warning as detection\n                if not run.crashing_id:\n                    self.dual_log(LogLevel.WARNING, f\"Crash on {filename.name} but can't link it to a Klocwork alert (maybe bonus !)\")\n                else:\n                    alert = self.__report.get_alert(run.crashing_id)\n                    if not alert.validated:\n                        alert.validated = True\n                        bugt, aline = run.asan_info()\n                        self.dual_log(LogLevel.INFO, f\"AFLPP new alert validated {alert} [{alert.id}] ({aline})  (asan no crash: {run.is_asan_without_crash()})\")\n                        self._agent.send_alert_data(AlertData(alert.id, alert.covered, alert.validated, p.read_bytes()))\n            else:\n                if is_crash:\n                    self.dual_log(LogLevel.WARNING, f\"crash not reproducible by rerunning seed: {filename.name}\")\n\n            if run.has_hanged():  # AFLPP does not stores 'hangs' it will have been sent as corpus or crash\n                self.dual_log(LogLevel.WARNING, f\"Seed {filename} was hanging in replay\")\n        return True\n\n    def __send_telemetry(self, filename: Path):\n        if filename.name != AFLPPProcess.STAT_FILE:\n            return\n\n        now = time.time()\n        if now < (self._tel_last + self._tel_frequency):\n            return\n        self._tel_last = now\n\n        logging.debug(f'[TELEMETRY] Stats file updated: {filename}')\n\n        with open(filename, 'r') as stats_file:\n            try:\n                stats = {}\n                for line in stats_file.readlines():\n                    k, v = line.strip('\\n').split(':')\n                    stats[k.strip()] = v.strip()\n\n                state = State.RUNNING\n                last_cov_update = int(stats['last_update'])\n                total_exec = int(stats['execs_done'])\n                exec_per_sec = int(float(stats['execs_per_sec']))\n                timeout = int(stats['unique_hangs']) if 'unique_hangs' in stats else None # N/A in AFL-QEMU.\n                coverage_edge = int(stats['total_edges'])\n                cycle = int(stats['cycles_done'])\n                coverage_path = int(stats['paths_total']) if 'paths_total' in stats else None # N/A in AFL-QEMU.\n\n                # NOTE: `coverage_block` does not apply for AFLPP.\n                self._agent.send_telemetry(state=state,\n                                           exec_per_sec=exec_per_sec,\n                                           total_exec=total_exec,\n                                           cycle=cycle,\n                                           timeout=timeout,\n                                           coverage_edge=coverage_edge,\n                                           coverage_path=coverage_path,\n                                           last_cov_update=last_cov_update)\n            except:\n                logging.error(f'Error retrieving stats!')\n\n    def start_received(self, fname: str, binary: bytes, engine: FuzzingEngineInfo, exmode: ExecMode, fuzzmode: FuzzMode, chkmode: CheckMode,\n                       _: CoverageMode, seed_inj: SeedInjectLoc, engine_args: str, argv: List[str], sast_report: str = None):\n        logging.info(f\"[START] bin:{fname} engine:{engine.name} exmode:{exmode.name} seedloc:{seed_inj.name} chk:{chkmode.name}\")\n        if self.started:\n            self._agent.send_log(LogLevel.CRITICAL, \"Instance already started!\")\n            return\n\n        if engine.name != \"AFLPP\":\n            logging.error(f\"Wrong fuzzing engine received {engine.name} while I am Honggfuzz\")\n            self._agent.send_log(LogLevel.ERROR, f\"Invalid fuzzing engine received {engine.name} can't do anything\")\n            return\n        if engine.version != pastisaflpp.__version__:\n            logging.error(f\"Wrong fuzzing engine version {engine.version} received\")\n            self._agent.send_log(LogLevel.ERROR, f\"Invalid fuzzing engine version {engine.version} do nothing\")\n            return\n\n        # Retrieve package out of the binary received\n        try:\n            package = BinaryPackage.from_binary(fname, binary, self.workspace.target_dir)\n        except FileNotFoundError:\n            logging.error(\"Invalid package received\")\n            return\n        except ValueError:\n            logging.error(\"Invalid package received\")\n            return\n\n        if sast_report:\n            logging.info(\"Loading SAST report\")\n            self.__report = SASTReport.from_json(sast_report)\n\n        self.__check_mode = chkmode  # CHECK_ALL, ALERT_ONLY\n\n        self.start(package, argv, exmode, fuzzmode, seed_inj, engine_args)\n\n    def __seed_received(self, typ: SeedType, seed: bytes):\n        h = self.hash_seed(seed)\n        logging.info(f\"[SEED] received  {h} ({typ.name})\")\n        self._seed_recvs.add(h)\n        self.add_seed(seed)\n\n    def __stop_received(self):\n        logging.info(f\"[STOP]\")\n\n        self.stop()\n\n    def dual_log(self, level: LogLevel, message: str) -> None:\n        \"\"\"\n        Helper function to log message both in the local log system and also\n        to the broker.\n\n        :param level: LogLevel message type\n        :param message: string message to log\n        :return: None\n        \"\"\"\n        mapper = {LogLevel.DEBUG: \"debug\",\n                  LogLevel.INFO: \"info\",\n                  LogLevel.CRITICAL: \"critical\",\n                  LogLevel.WARNING: \"warning\",\n                  LogLevel.ERROR: \"error\"}\n        log_f = getattr(logging, mapper[level])\n        log_f(message)\n        self._agent.send_log(level, message)\n\n    def add_initial_seed(self, file: Union[str, Path]):\n        p = Path(file)\n        logging.info(f\"add initial seed {file.name}\")\n        # Write seed to disk.\n        seed_path = self.workspace.input_dir / p.name\n        seed_path.write_bytes(p.read_bytes())\n\n    def aflpp_available(self):\n        return self.aflpp.aflpp_environ_check()", ""]}
{"filename": "engines/pastis-aflpp/pastisaflpp/__init__.py", "chunked_list": ["import os\nfrom pathlib import Path\nfrom typing import Optional\nimport subprocess\nimport logging\n\nfrom .driver import AFLPPDriver\nfrom .replay import Replay\nfrom .aflpp import AFLPPProcess, AFLPPNotFound\nfrom .workspace import Workspace", "from .aflpp import AFLPPProcess, AFLPPNotFound\nfrom .workspace import Workspace\n\n__version__ = \"1.0.0\"\n\n\n# AFL++ env variables\nAFLPP_ENV_VAR = \"AFLPP_WS\"\n\ndef spawn_online_aflpp(workspace: Optional[Path], port: int = 5555):\n    env = os.environ\n    env[AFLPP_ENV_VAR] = str(workspace.absolute())\n    logging.info(f\"aflpp workspace: {str(workspace.absolute())}\")\n    cmd_line_afl = [\"pastis-aflpp\", \"online\", \"-p\", f\"{port}\"]\n    logging.info(f\"run: {' '.join(cmd_line_afl)}\")\n    return subprocess.Popen(cmd_line_afl, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)", "\ndef spawn_online_aflpp(workspace: Optional[Path], port: int = 5555):\n    env = os.environ\n    env[AFLPP_ENV_VAR] = str(workspace.absolute())\n    logging.info(f\"aflpp workspace: {str(workspace.absolute())}\")\n    cmd_line_afl = [\"pastis-aflpp\", \"online\", \"-p\", f\"{port}\"]\n    logging.info(f\"run: {' '.join(cmd_line_afl)}\")\n    return subprocess.Popen(cmd_line_afl, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n\ndef check_scaling_frequency() -> bool:\n    data = Path(\"/sys/devices/system/cpu/cpu0/cpufreq/scaling_governor\").read_text()\n    return data == \"performance\\n\"", "\n\ndef check_scaling_frequency() -> bool:\n    data = Path(\"/sys/devices/system/cpu/cpu0/cpufreq/scaling_governor\").read_text()\n    return data == \"performance\\n\"\n"]}
{"filename": "engines/pastis-aflpp/broker-addon/aflppbroker/__init__.py", "chunked_list": ["# built-in imports\nfrom pathlib import Path\nfrom typing import Union, Tuple, List, Optional, Type\n\n# third-party import\nimport lief\n\nfrom libpastis import FuzzingEngineDescriptor, EngineConfiguration\nfrom libpastis.types import ExecMode, CoverageMode, FuzzMode\n", "from libpastis.types import ExecMode, CoverageMode, FuzzMode\n\n\nclass AFLConfigurationInterface(EngineConfiguration):\n    \"\"\"\n    Small wrapping function for AFL++ additional parameters\n    \"\"\"\n\n    def __init__(self, args: List[str] = None):\n        self._argvs = [] if args is None else args # Argument to send on the command line\n\n    @staticmethod\n    def new() -> 'AFLConfigurationInterface':\n        return AFLConfigurationInterface()\n\n    @staticmethod\n    def from_file(filepath: Path) -> 'AFLConfigurationInterface':\n        with open(filepath, \"r\") as f:\n            return AFLConfigurationInterface(f.read().split())\n\n    @staticmethod\n    def from_str(s: str) -> 'AFLConfigurationInterface':\n        return AFLConfigurationInterface(s.split())\n\n    def to_str(self) -> str:\n        return \" \".join(self._argvs)\n\n    def get_coverage_mode(self) -> CoverageMode:\n        \"\"\" Current coverage mode selected in the file \"\"\"\n        return CoverageMode.AUTO\n\n    def set_target(self, target: int) -> None:\n        # Note: Giving a target to Honggfuzz does not\n        # do anything as Honggfuzz is not directed.\n        pass", "\n\nclass AFLPPEngineDescriptor(FuzzingEngineDescriptor):\n\n    NAME = \"AFLPP\"\n    SHORT_NAME = \"AFLPP\"\n    VERSION = \"1.0.0\"  # Should be in sync with alfpp.__version__\n\n    config_class = AFLConfigurationInterface\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def accept_file(binary_file: Path) -> Tuple[bool, Optional[ExecMode], Optional[FuzzMode]]:\n        if str(binary_file).endswith(\".cmplog\"):\n            return False, None, None\n\n        p = lief.parse(str(binary_file))\n        if not p:\n            return False, None, None\n\n        # Search for HF instrumentation\n        instrumented = False\n\n        for s in p.symbols:\n            if \"__afl_\" in s.name:\n                instrumented = True\n                break\n\n        for f in p.functions:\n            if \"__afl_\" in f.name:\n                instrumented = True\n                break\n\n        if not instrumented:\n            # NOTE This can be improve. We usually use PERSISTENT mode when\n            # fuzzing a binary-only target because of performance reasons but\n            # it can also be SINGLE_EXEC. Therefore, ExecMode would not be the\n            # right place to add the BINARY_ONLY option (it was done this way\n            # to keep things simple).\n            return True, ExecMode.AUTO, FuzzMode.BINARY_ONLY\n\n        return True, ExecMode.AUTO, FuzzMode.INSTRUMENTED\n\n    @staticmethod\n    def supported_coverage_strategies() -> List[CoverageMode]:\n        return [CoverageMode.AUTO]", ""]}
{"filename": "engines/pastis-triton/broker-addon/pastisttbroker/__init__.py", "chunked_list": ["# built-in imports\nfrom pathlib import Path\nfrom typing import Union, Tuple, List, Optional\nimport json\n\n# third-party import\nimport lief\n\nfrom libpastis import FuzzingEngineDescriptor, EngineConfiguration\nfrom libpastis.types import ExecMode, CoverageMode, FuzzMode", "from libpastis import FuzzingEngineDescriptor, EngineConfiguration\nfrom libpastis.types import ExecMode, CoverageMode, FuzzMode\n\n# WARNING: This module is made in such a way that it does\n# not directly depend on tritondse (to facilitate installation)\n# coverage strategies thus have to be ported here !\nTRITON_DSE_COVS = ['BLOCK', 'EDGE', 'PATH']\n\n\nclass TritonConfigurationInterface(EngineConfiguration):\n    def __init__(self, data):\n        self.data = data\n\n    @staticmethod\n    def new() -> 'TritonConfigurationInterface':\n        return TritonConfigurationInterface({})\n\n    @staticmethod\n    def from_file(filepath: Path) -> 'TritonConfigurationInterface':\n        with open(filepath, \"r\") as f:\n            return TritonConfigurationInterface(json.load(f))\n\n    @staticmethod\n    def from_str(s: str) -> 'TritonConfigurationInterface':\n        return TritonConfigurationInterface(json.loads(s))\n\n    def to_str(self) -> str:\n        return json.dumps(self.data)\n\n    def get_coverage_mode(self) -> CoverageMode:\n        \"\"\" Current coverage mode selected in the file \"\"\"\n        v = self.data['coverage_strategy']\n        return CoverageMode(v)\n\n    def set_target(self, target: int) -> None:\n        self.data['custom'] = {}\n        self.data['custom']['target'] = target", "\nclass TritonConfigurationInterface(EngineConfiguration):\n    def __init__(self, data):\n        self.data = data\n\n    @staticmethod\n    def new() -> 'TritonConfigurationInterface':\n        return TritonConfigurationInterface({})\n\n    @staticmethod\n    def from_file(filepath: Path) -> 'TritonConfigurationInterface':\n        with open(filepath, \"r\") as f:\n            return TritonConfigurationInterface(json.load(f))\n\n    @staticmethod\n    def from_str(s: str) -> 'TritonConfigurationInterface':\n        return TritonConfigurationInterface(json.loads(s))\n\n    def to_str(self) -> str:\n        return json.dumps(self.data)\n\n    def get_coverage_mode(self) -> CoverageMode:\n        \"\"\" Current coverage mode selected in the file \"\"\"\n        v = self.data['coverage_strategy']\n        return CoverageMode(v)\n\n    def set_target(self, target: int) -> None:\n        self.data['custom'] = {}\n        self.data['custom']['target'] = target", "\n\nclass TritonEngineDescriptor(FuzzingEngineDescriptor):\n\n    NAME = \"TRITON\"\n    SHORT_NAME = \"TT\"\n    VERSION = \"1.0.0\"\n\n    FUNCTION_BLACKLIST_PREFIX = [\n        \"__sanitizer\",  # all fuzzer related sanitizers\n        \"__gcov_\",      # gcov functions\n        \"__asan_\",\n        \"__afl_\"\n    ]\n\n    SYMBOL_BLACKLIST_PREFIX = [\n        \"__sanitizer\",  # all fuzzer related sanitizers\n        \"__gcov_\",      # gcov functions\n        \"__asan_\",\n        \"__afl_\"\n    ]\n\n    IMPORT_BLACKLIST = [\n        \"HF_ITER\"       # honggfuzz functions\n    ]\n\n    config_class = TritonConfigurationInterface\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def accept_file(binary_file: Path) -> Tuple[bool, Optional[ExecMode], Optional[FuzzMode]]:\n        p = lief.parse(str(binary_file))\n        if not p:\n            return False, None, None\n\n        # Presumably good unless do find some instrumentation functions or imports\n        for f in p.functions:\n            for item in TritonEngineDescriptor.FUNCTION_BLACKLIST_PREFIX:\n                if f.name.startswith(item):\n                    return False, None, None\n\n        for f in p.imported_functions:\n            if f.name in TritonEngineDescriptor.IMPORT_BLACKLIST:\n                return False, None, None\n\n        for s in p.symbols:\n            for item in TritonEngineDescriptor.SYMBOL_BLACKLIST_PREFIX:\n                if s.name.startswith(item):\n                    return False, None, None\n\n        return True, ExecMode.SINGLE_EXEC, FuzzMode.BINARY_ONLY  # Only support single_exec, binary only (not instrumented)\n\n    @staticmethod\n    def supported_coverage_strategies() -> List[CoverageMode]:\n        return [CoverageMode(st) for st in TRITON_DSE_COVS]", ""]}
{"filename": "engines/pastis-triton/pastisdse/pastisdse.py", "chunked_list": ["# built-in imports\nfrom typing import List, Tuple\nimport os\nimport time\nimport logging\nfrom pathlib import Path\nimport threading\nimport platform\nimport json\nimport queue", "import json\nimport queue\n\n# third-party imports\nfrom triton               import MemoryAccess, CPUSIZE\n\n# Pastis & triton imports\nimport pastisdse\nfrom tritondse            import Config, Program, CleLoader, CoverageStrategy, SymbolicExplorator, \\\n                                 SymbolicExecutor, ProcessState, ExplorationStatus, SeedStatus, ProbeInterface, \\", "from tritondse            import Config, Program, CleLoader, CoverageStrategy, SymbolicExplorator, \\\n                                 SymbolicExecutor, ProcessState, ExplorationStatus, SeedStatus, ProbeInterface, \\\n                                 Workspace, Seed, CompositeData, SeedFormat, QuokkaProgram\nfrom tritondse.sanitizers import FormatStringSanitizer, NullDerefSanitizer, UAFSanitizer, IntegerOverflowSanitizer, mk_new_crashing_seed\nfrom tritondse.types      import Addr, Edge, SymExType, Architecture, Platform\nfrom libpastis import ClientAgent, BinaryPackage, SASTReport\nfrom libpastis.types      import SeedType, FuzzingEngineInfo, ExecMode, CoverageMode, SeedInjectLoc, CheckMode, LogLevel, AlertData, FuzzMode\nfrom tritondse.trace      import QBDITrace, TraceException\nfrom tritondse.worklist import FreshSeedPrioritizerWorklist, WorklistAddressToSet\n", "from tritondse.worklist import FreshSeedPrioritizerWorklist, WorklistAddressToSet\n\n\nclass PastisDSE(object):\n\n    INPUT_FILE_NAME = \"input_file\"\n    STAT_FILE = \"pastidse-stats.json\"\n\n    RAMDISK = \"/mnt/ramdisk\"\n    TMP_SEED = \"seed.seed\"\n    TMP_TRACE = \"result.trace\"\n\n    def __init__(self, agent: ClientAgent):\n        self.agent = agent\n        self._init_callbacks()  # register callbacks on the given agent\n\n        self.config = Config()\n        self.config.workspace = \"\"  # Reset workspace so that it will computed in start_received\n        self.dse        = None\n        self.program    = None\n        self._stop      = False\n        self.sast_report= None\n        self._last_id = None\n        self._last_id_pc = None\n        self._seed_received = set()\n        self._probes = []\n        self._chkmode = None\n        self._seedloc = None\n        self._program_slice = None\n        self._tracing_enabled = False\n\n        # local attributes for telemetry\n        self.nb_to, self.nb_crash = 0, 0\n        self._cur_cov_count = 0\n        self._last_cov_update = time.time()\n        self._seed_queue = queue.Queue()\n        self._sending_count = 0\n        self.seeds_merged = 0\n        self.seeds_rejected = 0\n\n        # Timing stats\n        self._start_time = 0\n        self._replay_acc = 0\n\n        self.replay_trace_file, self.replay_seed_file = self._initialize_tmp_files()\n\n    def _initialize_tmp_files(self) -> Tuple[Path, Path]:\n        ramdisk = Path(self.RAMDISK)\n        pid = os.getpid()\n        if ramdisk.exists():  # there is a ramdisk available\n            dir = ramdisk / f\"triton_{pid}\"\n            dir.mkdir()\n            logging.info(f\"tmp directory set to: {dir}\")\n            return dir / self.TMP_TRACE, dir / self.TMP_SEED\n        else:\n            logging.info(f\"tmp directory set to: /tmp\")\n            return Path(f\"/tmp/triton_{pid}.trace\"), Path(\"/tmp/triton_{pid}.seed\")\n\n    def add_probe(self, probe: ProbeInterface):\n        self._probes.append(probe)\n\n    def _init_callbacks(self):\n        self.agent.register_seed_callback(self.seed_received)\n        self.agent.register_stop_callback(self.stop_received)\n\n    def init_agent(self, remote: str = \"localhost\", port: int = 5555):\n        self.agent.register_start_callback(self.start_received) # register start because launched manually\n        self.agent.connect(remote, port)\n        self.agent.start()\n        self.agent.send_hello([FuzzingEngineInfo(\"TRITON\", pastisdse.__version__, \"pastisttbroker\")])\n\n    def start(self):\n        self._th = threading.Thread(target=self.run, daemon=True)\n        self._th.start()\n\n    def reset(self):\n        \"\"\" Reset the current DSE to be able to restart from fresh settings \"\"\"\n        self.dse = None  # remove DSE object\n        self.config = Config()\n        self.config.workspace = \"\"  # Reset workspace so that it will computed in start_received\n        self._last_id_pc = None\n        self._last_id = None\n        self.sast_report = None\n        self._program_slice = None\n        self._seed_received = set()\n        self.program = None\n        self._stop = False\n        self._chkmode = None\n        self._seedloc = None\n        self.nb_to, self.nb_crash = 0, 0\n        self._cur_cov_count = 0\n        self._last_cov_update = time.time()\n        self._tracing_enabled = False\n        self._sending_count = 0\n        self.seeds_merged = 0\n        self.seeds_rejected = 0\n        self._start_time = 0\n        self._replay_acc = 0\n        logging.info(\"DSE Ready\")\n\n    def run(self, online: bool, debug_pp: bool=False):\n\n        # Run while we are not instructed to stop\n        while not self._stop:\n\n            if online:  # in offline start_received, seed_received will already have been called\n                self.reset()\n\n            # Just wait until the broker says let's go\n            while self.dse is None:\n                time.sleep(0.10)\n\n            if debug_pp:\n                def cb_debug(se, _):\n                    se.debug_pp = True\n                self.dse.callback_manager.register_pre_execution_callback(cb_debug)\n\n            if not self.run_one(online):\n                break\n\n        self.agent.stop()\n\n    def run_one(self, online: bool):\n        # Run while we are not instructed to stop\n        while not self._stop:\n\n            # wait for seed event\n            self._wait_seed_event()\n            self._start_time = time.time()\n\n            st = self.dse.explore()\n\n            if not online:\n                return False  # in offline whatever the status we stop\n\n            else: # ONLINE\n                if st == ExplorationStatus.STOPPED:  # if the exploration stopped just return\n                    logging.info(\"exploration stopped\")\n                    return False\n                elif st == ExplorationStatus.TERMINATED:\n                    self.agent.send_stop_coverage_criteria()\n                    return True  # Reset and wait for further instruction from the broker\n                elif st == ExplorationStatus.IDLE:  # no seed\n                    if self._chkmode == CheckMode.ALERT_ONE:\n                        self.agent.send_stop_coverage_criteria()  # Warn: the broker we explored the whole search space and did not validated the target\n                        return True                               # Make ourself ready to receive a new one\n                    else: # wait for seed of peers\n                        logging.info(\"exploration idle (worklist empty)\")\n                        self.agent.send_log(LogLevel.INFO, \"exploration idle (worklist empty)\")\n                else:\n                    logging.error(f\"explorator not meant to be in state: {st}\")\n                    return False\n\n            # Finished an exploration batch\n            self.save_stats()  # Save stats\n\n    def _wait_seed_event(self):\n        logging.info(\"wait to receive seeds\")\n        while not self.dse.seeds_manager.seeds_available() and not self._stop:\n            self.try_process_seed_queue()\n            time.sleep(0.5)\n\n\n    def cb_post_execution(self, se: SymbolicExecutor, state: ProcessState):\n        \"\"\"\n        This function is called after each execution. In this function we verify\n        the ABV_GENERAL alert when a crash occurred during the last execution.\n\n        :param se: The current symbolic executor\n        :param state: The current processus state of the execution\n        :return: None\n        \"\"\"\n        # Send seed that have been executed\n        mapper = {SeedStatus.OK_DONE: SeedType.INPUT, SeedStatus.CRASH: SeedType.CRASH, SeedStatus.HANG: SeedType.HANG}\n        seed = se.seed\n        if seed.status == SeedStatus.NEW:\n            logging.warning(f\"seed is not meant to be NEW in post execution current:{seed.status.name}\")\n        elif seed.status in [SeedStatus.CRASH, SeedStatus.HANG]:  # The stats is new send it back again\n            if seed not in self._seed_received:  # Do not send back a seed that already came from broker\n                self.agent.send_seed(mapper[seed.status], seed.content.files[self.INPUT_FILE_NAME] if seed.is_composite() else seed.content)\n        else:  # INPUT\n            pass  # Do not send it back again\n\n        # Update some stats\n        if se.seed.status == SeedStatus.CRASH:\n            self.nb_crash += 1\n        elif se.seed.status == SeedStatus.HANG:\n            self.nb_to += 1\n\n        # Handle CRASH and ABV_GENERAL\n        if se.seed.status == SeedStatus.CRASH and self._last_id:\n            alert = self.sast_report.get_alert(self._last_id)\n            if alert.type == \"ABV_GENERAL\":\n                logging.info(f'A crash occured with an ABV_GENERAL encountered just before.')\n                self.dual_log(LogLevel.INFO, f\"Alert [{alert.id}] in {alert.file}:{alert.line}: {alert.type} validation [SUCCESS]\")\n                alert.validated = True\n                self.agent.send_alert_data(AlertData(alert.id, alert.covered, alert.validated, se.seed.content, self._last_id_pc))\n\n        # Process all the seed received\n        self.try_process_seed_queue()\n\n        # Print stats\n        if self.sast_report:\n            cov, va, tot = self.sast_report.get_stats()\n            logging.info(f\"SAST stats: defaults: [covered:{cov}/{tot}] [validated:{va}/{tot}]\")\n\n    def try_process_seed_queue(self):\n\n        while not self._seed_queue.empty() and not self._stop:\n            seed, typ = self._seed_queue.get()\n            self._process_seed_received(typ, seed)\n\n    def cb_telemetry(self, dse: SymbolicExplorator):\n        \"\"\"\n        Callback called after each execution to send telemetry to the broker\n\n        :param dse: SymbolicExplorator\n        :return: None\n        \"\"\"\n        new_count = dse.coverage.unique_covitem_covered\n\n        if new_count != self._cur_cov_count:         # Coverage has been updated\n            self._cur_cov_count = new_count          # update count\n            self._last_cov_update = time.time()  # update last coverage update to be now\n\n        if dse.coverage.strategy == CoverageStrategy.PREFIXED_EDGE:\n            new_count = len(set(x[1] for x in dse.coverage.covered_items.keys()))  # For prefixed-edge only count edge\n\n        self.agent.send_telemetry(exec_per_sec=int(dse.execution_count / (time.time()-dse.ts)),\n                                  total_exec=dse.execution_count,\n                                  timeout=self.nb_to,\n                                  coverage_block=dse.coverage.unique_instruction_covered,\n                                  coverage_edge=new_count if dse.coverage in [CoverageStrategy.EDGE, CoverageStrategy.PREFIXED_EDGE] else 0,\n                                  coverage_path=new_count if dse.coverage.strategy == CoverageStrategy.PATH else 0,\n                                  last_cov_update=int(self._last_cov_update))\n\n    def cb_on_solving(self, dse: SymbolicExplorator, pstate: ProcessState, edge: Edge, typ: SymExType) -> bool:\n        # Only consider conditional and dynamic jumps.\n        if typ in [SymExType.SYMBOLIC_READ, SymExType.SYMBOLIC_WRITE]:\n            return True\n\n        # Unpack edge.\n        src, dst = edge\n\n        # Find the function which holds the basic block of the destination.\n        dst_fn = self.program.find_function_from_addr(dst)\n        if dst_fn is None:\n            logging.warning(\"Solving edge ({src:#x} -> {dst:#x}) not in a function\")\n            return True\n        else:\n            if dst_fn.start in self._program_slice:\n                return True\n            else:\n                logging.info(\n                    f\"Slicer: reject edge ({src:#x} -> {dst:#x} ({dst_fn.name}) not in slice!\")\n                return False\n\n    def start_received(self, fname: str, binary: bytes, engine: FuzzingEngineInfo, exmode: ExecMode, fuzzmode: FuzzMode, chkmode: CheckMode,\n                       covmode: CoverageMode, seed_inj: SeedInjectLoc, engine_args: str, argv: List[str], sast_report: str=None):\n        \"\"\"\n        This function is called when the broker says to start the fuzzing session. Here, we receive all information about\n        the program to fuzz and the configuration.\n\n        :param fname: The name of the binary to explore\n        :param binary: The content of the binary to explore\n        :param engine: The kind of fuzzing engine (should be Triton for this script)\n        :param exmode: The mode of the exploration\n        :param fuzzmode: The fuzzing mode (instrumented or binary only)\n        :param chkmode: The mode of vulnerability check\n        :param covmode: The mode of coverage\n        :param seed_inj: The location where to inject input\n        :param engine_args: The engine arguments\n        :param argv: The program arguments\n        :param sast_report: The SAST report\n        :return: None\n        \"\"\"\n        logging.info(f\"[BROKER] [START] bin:{fname} engine:{engine.name} exmode:{exmode.name} cov:{covmode.name} chk:{chkmode.name}\")\n\n        if self.dse is not None:\n            logging.warning(\"DSE already instanciated (override it)\")\n\n        if engine.version != pastisdse.__version__:\n            logging.error(f\"Pastis-DSE mismatch with one from the server {engine.version} (local: {pastisdse.__version__})\")\n            return\n\n        self._seedloc = seed_inj\n\n        # ------- Create the TritonDSE configuration file ---------\n        if engine_args:\n            self.config = Config.from_json(engine_args)\n        else:\n            self.config = Config()  # Empty configuration\n            # Use argv ONLY if no configuration provided\n            self.config.program_argv = [f\"./{fname}\"]\n            if argv:\n                self.config.program_argv.extend(argv) # Preprend the binary to argv\n\n        \"\"\"\n        Actions taken depending on seed format & co:\n        Config    |  Inject  |  Result\n        RAW          STDIN      /\n        COMPOSITE    STDIN      / (but need 'stdin' in files)\n        RAW          ARGV       change to COMPOSITE to be able to inject on argv (and convert seeds on the fly)\n        COMPOSITE    ARGV       / (but need 'input_file' in files)\n        \"\"\"\n        if seed_inj == SeedInjectLoc.ARGV:  # Make sure we inject input on argv\n            if self.config.is_format_raw():\n                logging.warning(\"injection is ARGV thus switch config seed format to COMPOSITE\")\n                self.config.seed_format = SeedFormat.COMPOSITE\n            if \"@@\" in self.config.program_argv:\n                idx = self.config.program_argv.index(\"@@\")\n                self.config.program_argv[idx] = self.INPUT_FILE_NAME\n            else:\n                logging.warning(f\"seed inject {self._seedloc.name} but no '@@' found in argv (will likely not work!)\")\n        else:  # SeedInjectLoc.STDIN\n            if engine_args:\n                if self.config.is_format_composite():\n                    self.dual_log(LogLevel.WARNING, \"injecting on STDIN but seed format is COMPOSITE\")\n            else:  # no config was provided just override\n                self.config.seed_format = SeedFormat.RAW\n            pass  # nothing to do ?\n        # ----------------------------------------------------------\n\n        # If a workspace is given keep it other generate new unique one\n        if not self.config.workspace:\n            ws = f\"/tmp/triton_workspace/{int(time.time())}\"\n            logging.info(f\"Configure workspace to be: {ws}\")\n            self.config.workspace = ws\n\n        # Create the workspace object in advance (to directly save the binary inside\n        workspace = Workspace(self.config.workspace)\n        workspace.initialize(flush=False)\n\n        try:\n            pkg = BinaryPackage.from_binary(fname, binary, workspace.get_binary_directory())\n        except FileNotFoundError:\n            logging.error(\"Invalid package data\")\n            return\n\n        if pkg.is_quokka():\n            logging.info(f\"load QuokkaProgram: {pkg.quokka.name}\")\n            self.program = QuokkaProgram(pkg.quokka, pkg.executable_path)\n        else:\n            logging.info(f\"load Program: {pkg.executable_path.name} [{self._seedloc.name}]\")\n            program = Program(pkg.executable_path)\n\n            # Make sure the Program is compatible with the local platform\n            if self.is_compatible_with_local(program):\n                self.program = CleLoader(pkg.executable_path)\n            else:\n                self.program = program\n\n        if self.program is None:\n            self.dual_log(LogLevel.CRITICAL, f\"LIEF was not able to parse the binary file {fname}\")\n            self.agent.stop()\n            return\n\n        # Enable local tracing if the binary is compatible with local architecture\n        self._tracing_enabled = self.is_compatible_with_local(self.program)\n        logging.info(f\"Local arch and program arch matching: {self._tracing_enabled}\")\n\n        # Update the coverage strategy in the current config (it overrides the config file one)\n        try:\n            self.config.coverage_strategy = CoverageStrategy(covmode.value)  # names are meant to match\n        except Exception as e:\n            logging.info(f\"Invalid covmode. Not supported by the tritondse library {e}\")\n            self.agent.stop()\n            return\n\n        if sast_report:\n            self.sast_report = SASTReport.from_json(sast_report)\n            logging.info(f\"SAST report loaded: alerts:{len(list(self.sast_report.iter_alerts()))}\")\n\n        # Set seed scheduler based on whether tracing is enabled.\n        if self._tracing_enabled:\n            seed_scheduler_class = WorklistAddressToSet\n        else:\n            seed_scheduler_class = FreshSeedPrioritizerWorklist\n\n        dse = SymbolicExplorator(self.config, self.program, workspace=workspace, seed_scheduler_class=seed_scheduler_class)\n\n        # Register common callbacks\n        dse.callback_manager.register_new_input_callback(self.send_seed_to_broker) # must be the second cb\n        dse.callback_manager.register_post_execution_callback(self.cb_post_execution)\n        dse.callback_manager.register_exploration_step_callback(self.cb_telemetry)\n\n        for probe in self._probes:\n            dse.callback_manager.register_probe(probe)\n\n        # Save check mode\n        self._chkmode = chkmode\n\n        if chkmode == CheckMode.CHECK_ALL:\n           dse.callback_manager.register_probe(UAFSanitizer())\n           dse.callback_manager.register_probe(NullDerefSanitizer())\n           dse.callback_manager.register_probe(FormatStringSanitizer())\n           #dse.callback_manager.register_probe(IntegerOverflowSanitizer())\n           # TODO Buffer overflow\n\n        elif chkmode == CheckMode.ALERT_ONLY:\n           dse.callback_manager.register_function_callback('__sast_alert_placeholder', self.intrinsic_callback)\n\n        elif chkmode == CheckMode.ALERT_ONE:  # targeted approach\n            if not isinstance(self.program, QuokkaProgram):\n                logging.error(f\"Targeted mode [{chkmode.name}] requires a Quokka program\")\n                self.agent.stop()\n                return\n\n            target_addr = self.config.custom['target']  # retrieve the target address to reach\n            dse.callback_manager.register_post_addr_callback(target_addr, self.intrinsic_callback)\n\n            # NOTE Target address must be the starting address of a basic block.\n            slice_from = self.program.find_function_addr('main')\n            slice_to = target_addr\n\n            if slice_from and slice_to:\n                # Find the functions that correspond to the from and to addresses.\n                slice_from_fn = self.program.find_function_from_addr(slice_from)\n                slice_to_fn = self.program.find_function_from_addr(slice_to)\n                logging.info(f\"launching exploration in targeted mode on: 0x{target_addr:08x} in {slice_to_fn.name}\")\n\n                if slice_from_fn and slice_to_fn:\n                    # NOTE Generate call graph with backedges so when we do the\n                    #      slice it also includes functions that are called in\n                    #      the path from the source to the destination of the\n                    #      slice.\n                    call_graph = self.program.get_call_graph(backedge_on_ret=True)\n\n                    logging.info(f'Slicing program from {slice_from:#x} ({slice_from_fn.name}) to {slice_to:#x} ({slice_to_fn.name})')\n\n                    self._program_slice = QuokkaProgram.get_slice(call_graph, slice_from_fn.start, slice_to_fn.start)\n\n                    logging.info(f'Call graph (full): #nodes: {len(call_graph.nodes)}, #edges: {len(call_graph.edges)}')\n                    logging.info(f'Call graph (sliced): #nodes: {len(self._program_slice.nodes)}, #edges: {len(self._program_slice.edges)}')\n\n                    dse.callback_manager.register_on_solving_callback(self.cb_on_solving)\n                else:\n                    logging.warning(f'Invalid source or target function, skipping slicing!')\n            else:\n                logging.warning(f'Invalid source or target addresses, skipping slicing!')\n\n        # will trigger the dse to start has another thread is waiting for self.dse to be not None\n        self.dse = dse\n\n    def _get_seed(self, raw_seed: bytes) -> Seed:\n        # Convert seed to CompositeData\n        seed = Seed.from_bytes(raw_seed)\n\n        if self.config.is_format_composite() and seed.is_raw() and self._seedloc == SeedInjectLoc.ARGV:\n            logging.debug(\"convert raw seed to composite\")\n            return Seed(CompositeData(files={self.INPUT_FILE_NAME: seed.content}))\n\n        elif self.config.is_format_composite() and seed.is_raw() and self._seedloc == SeedInjectLoc.STDIN:\n            logging.warning(\"Seed is RAW but format is COMPOSITE with injection on STDIN\")\n            return Seed(CompositeData(files={\"stdin\": seed.content}))\n\n        else:\n            return seed\n\n    def seed_received(self, typ: SeedType, seed: bytes):\n        \"\"\"\n        This function is called when we receive a seed from the broker.\n\n        :param typ: The type of the seed\n        :param seed: The seed\n        :return: None\n        \"\"\"\n        seed = self._get_seed(seed)\n\n        if seed in self._seed_received:\n            logging.warning(f\"receiving seed already known: {seed.hash} (dropped)\")\n            return\n        else:\n            self._seed_queue.put((seed, typ))\n            logging.info(f\"seed received {seed.hash} (pool: {self._seed_queue.qsize()})\")\n\n\n    def _process_seed_received(self, typ: SeedType, seed: Seed):\n        \"\"\"\n        This function is called when we receive a seed from the broker.\n\n        :param typ: The type of the seed\n        :param seed: The seed\n        :return: None\n        \"\"\"\n        try:\n            if not self._tracing_enabled:\n                # Accept all seeds\n                self.dse.add_input_seed(seed)\n\n            else:  # Try running the seed to know whether to keep it\n                # NOTE: re-run the seed regardless of its status\n                coverage = None\n                logging.info(f\"process seed received {seed.hash} (pool: {self._seed_queue.qsize()})\")\n\n                data = seed.content.files[self.INPUT_FILE_NAME] if seed.is_composite() else seed.bytes()\n                self.replay_seed_file.write_bytes(data)\n                # Adjust injection location before calling QBDITrace\n                if self._seedloc == SeedInjectLoc.STDIN:\n                    stdin_file = str(self.replay_seed_file)\n                    argv = self.config.program_argv\n                else:  # SeedInjectLoc.ARGV\n                    stdin_file = None\n                    try:\n                        # Replace 'input_file' in argv with the temporary file name created\n                        argv = self.config.program_argv[:]\n                        idx = argv.index(self.INPUT_FILE_NAME)\n                        argv[idx] = str(self.replay_seed_file)\n                    except ValueError:\n                        logging.error(f\"seed injection {self._seedloc.name} but can't find 'input_file' on program argv\")\n                        return\n\n                try:\n                    # Run the seed and determine whether it improves our current coverage.\n                    t0 = time.time()\n                    if QBDITrace.run(self.config.coverage_strategy,\n                                          str(self.program.path.resolve()),\n                                          argv[1:] if len(argv) > 1 else [],\n                                          output_path=str(self.replay_trace_file),\n                                          stdin_file=stdin_file,\n                                          cwd=Path(self.program.path).parent,\n                                          timeout=60):\n                        coverage = QBDITrace.from_file(str(self.replay_trace_file)).coverage\n                    else:\n                        logging.warning(\"Cannot load the coverage file generated (maybe had crashed?)\")\n                        coverage = None\n                    self._replay_acc += time.time() - t0  # Save time spent replaying inputs\n                except FileNotFoundError:\n                    logging.warning(\"Cannot load the coverage file generated (maybe had crashed?)\")\n                except TraceException:\n                    logging.warning('There was an error while trying to re-run the seed')\n\n                if not coverage:\n                    logging.warning(f\"coverage not found after replaying: {seed.hash} [{typ.name}] (add it anyway)\")\n                    # Add the seed anyway, if it was not possible to re-run the seed.\n                    # TODO Set seed.coverage_objectives as \"empty\" (use ellipsis\n                    # object). Modify WorklistAddressToSet to support it.\n                    self.seeds_merged += 1\n                    self.dse.add_input_seed(seed)\n                else:\n                    # Check whether the seed improves the current coverage.\n                    if self.dse.coverage.improve_coverage(coverage):\n                        logging.info(f\"seed added {seed.hash} [{typ.name}] (coverage merged)\")\n                        self.seeds_merged += 1\n                        self.dse.coverage.merge(coverage)\n                        self.dse.seeds_manager.worklist.update_worklist(coverage)\n\n                        seed.coverage_objectives = self.dse.coverage.new_items_to_cover(coverage)\n                        self.dse.add_input_seed(seed)\n                    else:\n                        logging.info(f\"seed archived {seed.hash} [{typ.name}] (NOT merging coverage)\")\n                        self.seeds_rejected += 1\n                        #self.dse.seeds_manager.archive_seed(seed)\n                        # logging.info(f\"seed archived {seed.hash} [{typ.name}]\")\n\n            self._seed_received.add(seed)  # Remember seed received not to send them back\n        except FileNotFoundError as e:\n            # NOTE If reset() is call during the execution of this function,\n            #      self.dse will be set to None and an AttributeError will occur.\n            logging.warning(f\"receiving seeds while the DSE is not instantiated {e}\")\n\n        rcv = len(self._seed_received)\n        logging.info(f\"seeds recv: {rcv} | merged {self.seeds_merged} [{(self.seeds_merged/rcv) * 100:.2f}%]\"\n                     f\" rejected {self.seeds_rejected} [{(self.seeds_rejected/rcv) * 100:.2f}%]\")\n\n    def stop_received(self):\n        \"\"\"\n        This function is called when the broker says stop. (Called from the agent thread)\n        \"\"\"\n        logging.info(f\"[BROKER] [STOP]\")\n\n        if self.dse:\n            self.dse.stop_exploration()\n\n        self.save_stats()  # Save stats\n\n        self._stop = True\n        # self.agent.stop()  # Can't call it here as this function executed from within agent thread\n\n    def save_stats(self):\n        stat_file = self.dse.workspace.get_metadata_file_path(self.STAT_FILE)\n        data = {\n            \"total_time\": time.time() - self._start_time,\n            \"emulation_time\": self.dse.total_emulation_time,  # Note: includes replay time but not solving\n            \"solving_time\": self.dse.seeds_manager.total_solving_time,\n            \"replay_time\": self._replay_acc,\n            \"seed_accepted\": self.seeds_merged,\n            \"seed_rejected\": self.seeds_rejected,\n            \"seed_received\": self.seeds_merged + self.seeds_rejected\n        }\n        stat_file.write_text(json.dumps(data))\n\n    def dual_log(self, level: LogLevel, message: str) -> None:\n        \"\"\"\n        Helper function to log message both in the local log system and also\n        to the broker.\n\n        :param level: LogLevel message type\n        :param message: string message to log\n        :return: None\n        \"\"\"\n        mapper = {LogLevel.DEBUG: \"debug\",\n                  LogLevel.INFO: \"info\",\n                  LogLevel.CRITICAL: \"critical\",\n                  LogLevel.WARNING: \"warning\",\n                  LogLevel.ERROR: \"error\"}\n        log_f = getattr(logging, mapper[level])\n        log_f(message)\n        self.agent.send_log(level, message)\n\n    def send_seed_to_broker(self, se: SymbolicExecutor, state: ProcessState, seed: Seed):\n        if seed not in self._seed_received:  # Do not send back a seed that already came from broker\n            self._sending_count += 1\n            logging.info(f\"Sending new: {seed.hash} [{self._sending_count}]\")\n            bytes = seed.content.files[self.INPUT_FILE_NAME] if seed.is_composite() else seed.content\n            self.agent.send_seed(SeedType.INPUT, bytes)\n\n    def intrinsic_callback(self, se: SymbolicExecutor, state: ProcessState, addr: Addr):\n        \"\"\"\n        This function is called when an intrinsic call occurs in order to verify\n        defaults and vulnerabilities.\n\n        :param se: The current symbolic executor\n        :param state: The current processus state of the execution\n        :param addr: The instruction address of the intrinsic call\n        :return: None\n        \"\"\"\n        alert_id = state.get_argument_value(0)\n        self._last_id = alert_id\n        self._last_id_pc = se.previous_pc\n\n        def status_changed(a, cov, vld):\n            return a.covered != cov or a.validated != vld\n\n        if self.sast_report:\n            # Retrieve the SASTAlert object from the report\n            try:\n                alert = self.sast_report.get_alert(alert_id)\n                cov, vld = alert.covered, alert.validated\n            except IndexError:\n                logging.warning(f\"Intrinsic id {alert_id} not found in report (ignored)\")\n                return\n\n            if not alert.covered:\n                self.dual_log(LogLevel.INFO, f\"Alert [{alert.id}] in {alert.file}:{alert.line}: {alert.type} covered !\")\n                alert.covered = True  # that might also set validated to true!\n\n            if not alert.validated:  # If of type VULNERABILITY and not yet validated\n                res = self.check_alert_dispatcher(alert.code, se, state, addr)\n                if res:\n                    alert.validated = True\n                    self.dual_log(LogLevel.INFO, f\"Alert [{alert.id}] in {alert.file}:{alert.line}: {alert.type} validation [SUCCESS]\")\n                    if se.seed.is_status_set():\n                        logging.warning(f\"Status already set ({se.seed.status}) for seed {se.seed.hash} (override with CRASH)\")\n                    se.seed.status = SeedStatus.CRASH  # Mark the seed as crash, as it validates an alert\n                else:\n                    logging.info(f\"Alert [{alert.id}] in {alert.file}:{alert.line}: validation [FAIL]\")\n\n            if status_changed(alert, cov, vld):  # If either coverage or validation were improved print stats\n                # Send updates to the broker\n                self.agent.send_alert_data(AlertData(alert.id, alert.covered, alert.validated, se.seed.content, se.previous_pc))\n                cov, vals, tot = self.sast_report.get_stats()\n                logging.info(f\"SAST stats: defaults: [covered:{cov}/{tot}] [validated:{vals}/{tot}]\")\n\n                if self.sast_report.all_alerts_validated() or (self._chkmode == CheckMode.ALERT_ONE and alert.validated):\n                    self._do_stop_all_alerts_validated()\n\n        else:  # Kind of autonomous mode. Try to check it even it is not bound to a report\n            # Retrieve alert type from parameters\n            alert_type = se.pstate.get_string_argument(1)\n            try:\n                if self.check_alert_dispatcher(alert_type, se, state, addr):\n                    logging.info(f\"Alert {alert_id} of type {alert_type} [VALIDATED]\")\n                else:\n                    logging.info(f\"Alert {alert_id} of type {alert_type} [NOT VALIDATED]\")\n            except KeyError:\n                logging.error(f\"Alert type {alert_type} not recognized\")\n\n\n    def check_alert_dispatcher(self, type: str, se: SymbolicExecutor, state: ProcessState, addr: Addr) -> bool:\n        \"\"\"\n        This function is called by intrinsic_callback in order to verify defaults\n        and vulnerabilities.\n\n        :param type: Type of the alert as a string\n        :param se: The current symbolic executor\n        :param state: The current processus state of the execution\n        :param addr: The instruction address of the intrinsic call\n        :return: True if a vulnerability has been verified\n        \"\"\"\n        # BUFFER_OVERFLOW related alerts\n        if type == \"SV_STRBO_UNBOUND_COPY\":\n            size = se.pstate.get_argument_value(2)\n            ptr = se.pstate.get_argument_value(3)\n\n            # Runtime check\n            if len(se.pstate.get_memory_string(ptr)) >= size:\n                # FIXME: Do we have to define the seed as CRASH even if there is no crash?\n                # FIXME: Maybe we have to define a new TAG like BUG or VULN or whatever\n                return True\n\n            # Symbolic check\n            actx = se.pstate.actx\n            predicate = [se.pstate.tt_ctx.getPathPredicate()]\n\n            # For each memory cell, try to proof that they can be different from \\0\n            for i in range(size + 1): # +1 in order to proof that we can at least do an off-by-one\n                cell = se.pstate.tt_ctx.getMemoryAst(MemoryAccess(ptr + i, CPUSIZE.BYTE))\n                predicate.append(cell != 0)\n\n            # FIXME: Maybe we can generate models until unsat in order to find the bigger string\n\n            model = se.pstate.tt_ctx.getModel(actx.land(predicate))\n            if model:\n                crash_seed = mk_new_crashing_seed(se, model)\n                se.workspace.save_seed(crash_seed)\n                logging.info(f'Model found for a seed which may lead to a crash ({crash_seed.filename})')\n                return True\n\n            return False\n\n        ######################################################################\n\n        # BUFFER_OVERFLOW related alerts\n        elif type == \"SV_STRBO_BOUND_COPY_OVERFLOW\":\n            dst_size = se.pstate.get_argument_value(2)\n            ptr_inpt = se.pstate.get_argument_value(3)\n            max_size = se.pstate.get_argument_value(4)\n\n            # Runtime check\n            if max_size >= dst_size and len(se.pstate.get_memory_string(ptr_inpt)) >= dst_size:\n                # FIXME: Do we have to define the seed as CRASH even if there is no crash?\n                # FIXME: Maybe we have to define a new TAG like BUG or VULN or whatever\n                return True\n\n            # Symbolic check\n            actx = se.pstate.actx\n            max_size_s = se.pstate.get_argument_symbolic(4).getAst()\n            predicate = [se.pstate.tt_ctx.getPathPredicate(), max_size_s >= dst_size]\n\n            # For each memory cell, try to proof that they can be different from \\0\n            for i in range(dst_size + 1): # +1 in order to proof that we can at least do an off-by-one\n                cell = se.pstate.tt_ctx.getMemoryAst(MemoryAccess(ptr_inpt + i, CPUSIZE.BYTE))\n                predicate.append(cell != 0)\n\n            # FIXME: Maybe we can generate models until unsat in order to find the bigger string\n\n            model = se.pstate.tt_ctx.getModel(actx.land(predicate))\n            if model:\n                crash_seed = mk_new_crashing_seed(se, model)\n                se.workspace.save_seed(crash_seed)\n                logging.info(f'Model found for a seed which may lead to a crash ({crash_seed.filename})')\n                return True\n\n            return False\n\n        ######################################################################\n\n        # BUFFER_OVERFLOW related alerts\n        elif type == \"ABV_GENERAL\":\n            logging.warning(f'ABV_GENERAL encounter but can not check the issue. This issue will be handle if the program will crash.')\n            return False\n\n        ######################################################################\n\n        # All INTEGER_OVERFLOW related alerts\n        elif type == \"NUM_OVERFLOW\":\n            return IntegerOverflowSanitizer.check(se, state, state.current_instruction)\n\n        ######################################################################\n\n        # All USE_AFTER_FREE related alerts\n        elif type in [\"UFM_DEREF_MIGHT\", \"UFM_FFM_MUST\", \"UFM_FFM_MIGHT\"]:\n            ptr = se.pstate.get_argument_value(2)\n            return UAFSanitizer.check(se, state, ptr, f'UAF detected at {ptr:#x}')\n\n        ######################################################################\n\n        # All FORMAT_STRING related alerts\n        elif type in [\"SV_TAINTED_FMTSTR\", \"SV_FMTSTR_GENERIC\"]:\n            ptr = se.pstate.get_argument_value(2)\n            return FormatStringSanitizer.check(se, state, addr, ptr)\n\n        ######################################################################\n\n        # All INVALID_MEMORY related alerts\n        # FIXME: NPD_CHECK_MIGHT and NPD_CONST_CALL are not supported by klocwork-alert-inserter\n        elif type in [\"NPD_FUNC_MUST\", \"NPD_FUNC_MIGHT\", \"NPD_CHECK_MIGHT\", \"NPD_CONST_CALL\"]:\n            ptr = se.pstate.get_argument_value(2)\n            return NullDerefSanitizer.check(se, state, ptr, f'Invalid memory access at {ptr:#x}')\n\n        ######################################################################\n\n        elif type == \"MISRA_ETYPE_CATEGORY_DIFFERENT_2012\":\n            expr = se.pstate.get_argument_symbolic(2).getAst()\n\n            # Runtime check\n            if expr.isSigned():\n                # FIXME: Do we have to define the seed as CRASH even if there is no crash?\n                # FIXME: Maybe we have to define a new TAG like BUG or VULN or whatever\n                return True\n\n            # Symbolic check\n            actx = se.pstate.tt_ctx.getAstContext()\n            size = expr.getBitvectorSize() - 1\n            predicate = [se.pstate.tt_ctx.getPathPredicate(), actx.extract(size - 1, size - 1, expr) == 1]\n\n            model = se.pstate.tt_ctx.getModel(actx.land(predicate))\n            if model:\n                crash_seed = mk_new_crashing_seed(se, model)\n                se.workspace.save_seed(crash_seed)\n                logging.info(f'Model found for a seed which may lead to a crash ({crash_seed.filename})')\n                return True\n            return False\n\n        else:\n            logging.error(f\"Unsupported alert kind {type}\")\n\n\n    def _do_stop_all_alerts_validated(self) -> None:\n        \"\"\"\n        Function called if all alerts have been covered and validated. All data are meant to\n        have been transmitted to the broker, but writes down locally the CSV anyway\n        :return: None\n        \"\"\"\n        logging.info(\"All defaults and vulnerability have been covered !\")\n\n        # Write the final CSV in the workspace directory\n        out_file = self.dse.workspace.get_metadata_file_path(\"klocwork_coverage_results.csv\")\n        self.sast_report.write_csv(out_file)\n\n        # Stop the dse exploration\n        self.dse.terminate_exploration()\n\n\n    def is_compatible_with_local(self, program: Program) -> bool:\n        \"\"\"\n        Checks whether the given program is compatible with the current architecture\n        and platform.\n\n        :param program: Program\n        :return: True if the program can be run locally\n        \"\"\"\n        arch_m = {\"i386\": Architecture.X86, \"x86_64\": Architecture.X86_64, \"armv7l\": Architecture.ARM32, \"aarch64\": Architecture.AARCH64}\n        plfm_m = {\"Linux\": Platform.LINUX, \"Windows\": Platform.WINDOWS, \"MacOS\": Platform.MACOS, \"iOS\": Platform.IOS}\n        local_arch, local_plfm = arch_m[platform.machine()], plfm_m[platform.system()]\n        return program.architecture == local_arch and program.platform == local_plfm", ""]}
{"filename": "engines/pastis-triton/pastisdse/__init__.py", "chunked_list": ["from typing import Tuple\nimport subprocess\n\nfrom .pastisdse import PastisDSE\n\n# Expose triton version\nimport tritondse\n\n__version__ = \"1.0.0\"\n", "__version__ = \"1.0.0\"\n\nTRITON_VERSION = tritondse.TRITON_VERSION\n\n\ndef spawn_online_triton(port: int = 5555, probe: Tuple[str] = ()):\n    tt = [\"pastis-triton\", \"online\", \"-p\", f\"{port}\"]\n    if len(probe) > 0:\n        tt += [\"--probe\", f\"{probe}\"]\n    subprocess.Popen(tt, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)", ""]}
{"filename": "doc/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the", "# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport sys\nimport datetime\nimport sphinx_fontawesome\n\nsys.path.insert(0, os.path.abspath('../libpastis'))", "\nsys.path.insert(0, os.path.abspath('../libpastis'))\n\n# -- Project information -----------------------------------------------------\n\nproject = 'PASTIS'\ncopyright = '2023, Quarkslab'\nauthor = 'Quarkslab'\n\n# The full version, including alpha/beta/rc tags", "\n# The full version, including alpha/beta/rc tags\nrelease = '0.2'\n\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\nlanguage = 'en'\n\n# The name of the Pygments (syntax highlighting) style to use.", "\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'monokai'  # also monokai, friendly, colorful\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.", "# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.todo',\n    'sphinx.ext.viewcode',\n    'breathe',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.githubpages',\n    'sphinx_fontawesome',", "    'sphinx.ext.githubpages',\n    'sphinx_fontawesome',\n    \"nbsphinx\",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.", "# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store', '**.ipynb_checkpoints']\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.", "# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \"sphinx_rtd_theme\"\n\nhtml_theme_options = {\n    # If False, expand all TOC entries\n    'globaltoc_collapse': False,\n}\n", "}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['figs']\n\nautodoc_default_flags = ['members', 'inherited-members']\n\n# For internationalization", "\n# For internationalization\nlocale_dirs = ['locale/']   # path is example but recommended.\ngettext_compact = False     # optional.\n\n\n\nautoclass_content = \"both\"  # Comment class with both class docstring and __init__ docstring\n\nautodoc_typehints = \"signature\"", "\nautodoc_typehints = \"signature\"\n\nautodoc_type_aliases = {\n\n}\n\nintersphinx_mapping = {'python': ('https://docs.python.org/3', None),\n                       'lief': ('https://lief-project.github.io/doc/latest/', None)}\n", "                       'lief': ('https://lief-project.github.io/doc/latest/', None)}\n"]}
{"filename": "tests/test_broker.py", "chunked_list": ["#!/usr/bin/env python3\nimport logging\nfrom typing import Tuple, List\n\n\nfrom libpastis.agent import BrokerAgent\nfrom libpastis.types import SeedType, FuzzingEngineInfo, LogLevel, Arch, State\n\nlogging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s %(message)s\")\n", "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s %(message)s\")\n\n\ndef seed_received(cli_id: bytes, typ: SeedType, seed: bytes):\n    global agent\n    logging.info(f\"[{cli_id.hex()}] [SEED] {seed.hex()} ({typ.name})\")\n    agent.send_seed(cli_id, typ, seed[::-1])\n\n\ndef hello_received(cli_id: bytes, engines: List[FuzzingEngineInfo], arch: Arch, cpus: int, memory: int):\n    logging.info(f\"[{cli_id.hex()}] [HELLO] Arch:{arch.name} engines:{[x.name for x in engines]} (cpu:{cpus}, mem:{memory})\")", "\ndef hello_received(cli_id: bytes, engines: List[FuzzingEngineInfo], arch: Arch, cpus: int, memory: int):\n    logging.info(f\"[{cli_id.hex()}] [HELLO] Arch:{arch.name} engines:{[x.name for x in engines]} (cpu:{cpus}, mem:{memory})\")\n\n\ndef log_received(cli_id: bytes, level: LogLevel, message: str):\n    logging.info(f\"[{cli_id.hex()}] [LOG] [{level.name}] {message}\")\n\n\ndef telemetry_received(cli_id: bytes, *args):\n    # state: State = None, exec_per_sec: int = None, total_exec: int = None,\n    # cycle: int = None, timeout: int = None, coverage_block: int = None, coverage_edge: int = None,\n    # coverage_path: int = None, last_cov_update: int = None):\n    logging.info(f\"[{cli_id.hex()}] [TELEMETRY] [{args}\")", "\ndef telemetry_received(cli_id: bytes, *args):\n    # state: State = None, exec_per_sec: int = None, total_exec: int = None,\n    # cycle: int = None, timeout: int = None, coverage_block: int = None, coverage_edge: int = None,\n    # coverage_path: int = None, last_cov_update: int = None):\n    logging.info(f\"[{cli_id.hex()}] [TELEMETRY] [{args}\")\n\n\ndef stop_coverage_received(cli_id: bytes):\n    logging.info(f\"[{cli_id.hex()}] [STOP_COVERAGE]\")", "def stop_coverage_received(cli_id: bytes):\n    logging.info(f\"[{cli_id.hex()}] [STOP_COVERAGE]\")\n\n\nif __name__ == \"__main__\":\n\n    agent = BrokerAgent()\n    agent.bind()\n\n    agent.register_seed_callback(seed_received)\n    agent.register_hello_callback(hello_received)\n    agent.register_log_callback(log_received)\n    agent.register_telemetry_callback(telemetry_received)\n    agent.register_stop_coverage_callback(stop_coverage_received)\n\n    agent.run()", ""]}
{"filename": "tests/test_client.py", "chunked_list": ["#!/usr/bin/env python3\nimport random\nimport time\nimport logging\nfrom typing import List\n\nfrom libpastis.agent import ClientAgent\nfrom libpastis.types import SeedType, FuzzingEngineInfo, ExecMode, CoverageMode, SeedInjectLoc, CheckMode, LogLevel, State\n\nlogging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s %(message)s\")", "\nlogging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s %(message)s\")\n\n\ndef start_received(fname: str, binary: bytes, engine: FuzzingEngineInfo, exmode: ExecMode, chkmode: CheckMode,\n                   covmode: CoverageMode, seed_inj: SeedInjectLoc, engine_args: str, argv: List[str], kl_report: str=None):\n    logging.info(f\"[START] bin:{fname} engine:{engine.name} exmode:{exmode.name} cov:{covmode.name} chk:{chkmode.name}\")\n\n\ndef seed_received(typ: SeedType, seed: bytes):\n    logging.info(f\"[SEED] {seed.hex()} ({typ})\")", "\ndef seed_received(typ: SeedType, seed: bytes):\n    logging.info(f\"[SEED] {seed.hex()} ({typ})\")\n\n\ndef stop_received():\n    logging.info(f\"[STOP]\")\n\n\nif __name__ == \"__main__\":\n    agent = ClientAgent()\n    agent.connect()\n\n    agent.register_start_callback(start_received)\n    agent.register_seed_callback(seed_received)\n    agent.register_stop_callback(stop_received)\n\n    agent.start()\n    agent.send_hello([FuzzingEngineInfo(\"TRITON\", \"v0.8\", \"pastistriton\")])\n    logging.info(\"Hello sent!\")\n    # agent.run()\n    # exit(0)\n\n    while True:\n        #  Do some 'work'\n        time.sleep(3)\n        #continue\n\n        v = random.randint(0, 2)\n        if v == 0:\n            seed = bytes(random.getrandbits(8) for _ in range(16))\n            agent.send_seed(SeedType.INPUT, seed)\n        elif v == 1:\n            level = random.choice(list(LogLevel))\n            agent.send_log(level, f\"Message: {random.randint(0, 100)}\")\n            # can also call agent.debug(), agent.warning() ..\n        elif v == 2:\n            r1, r2, r3 = [random.randint(0, 100) for _ in range(3)]\n            agent.send_telemetry(State.RUNNING, exec_per_sec=r1, total_exec=r2, timeout=r3)", "\nif __name__ == \"__main__\":\n    agent = ClientAgent()\n    agent.connect()\n\n    agent.register_start_callback(start_received)\n    agent.register_seed_callback(seed_received)\n    agent.register_stop_callback(stop_received)\n\n    agent.start()\n    agent.send_hello([FuzzingEngineInfo(\"TRITON\", \"v0.8\", \"pastistriton\")])\n    logging.info(\"Hello sent!\")\n    # agent.run()\n    # exit(0)\n\n    while True:\n        #  Do some 'work'\n        time.sleep(3)\n        #continue\n\n        v = random.randint(0, 2)\n        if v == 0:\n            seed = bytes(random.getrandbits(8) for _ in range(16))\n            agent.send_seed(SeedType.INPUT, seed)\n        elif v == 1:\n            level = random.choice(list(LogLevel))\n            agent.send_log(level, f\"Message: {random.randint(0, 100)}\")\n            # can also call agent.debug(), agent.warning() ..\n        elif v == 2:\n            r1, r2, r3 = [random.randint(0, 100) for _ in range(3)]\n            agent.send_telemetry(State.RUNNING, exec_per_sec=r1, total_exec=r2, timeout=r3)", "        # elif v == 3:\n        #     agent.send_stop_coverage_criteria()\n"]}
{"filename": "tests/broker_fw_honggfuzz_seed.py", "chunked_list": ["#!/usr/bin/env python3\nimport logging\nfrom typing import Tuple, List\nimport sys\nfrom pathlib import Path\n\nimport inotify.adapters\n\nfrom libpastis.agent import BrokerAgent\nfrom libpastis.types import SeedType, FuzzingEngineInfo, LogLevel, Arch, State", "from libpastis.agent import BrokerAgent\nfrom libpastis.types import SeedType, FuzzingEngineInfo, LogLevel, Arch, State\n\nlogging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s %(message)s\")\n\nclients = set()\n\n\ndef seed_received(cli_id: bytes, typ: SeedType, seed: bytes):\n    logging.info(f\"[{cli_id.hex()}] [SEED] {seed.hex()} ({typ.name})\")", "def seed_received(cli_id: bytes, typ: SeedType, seed: bytes):\n    logging.info(f\"[{cli_id.hex()}] [SEED] {seed.hex()} ({typ.name})\")\n\n\ndef hello_received(cli_id: bytes, engines: List[FuzzingEngineInfo], arch: Arch, cpus: int, memory: int):\n    global clients\n    if cli_id not in clients:\n        logging.info(f\"[broker] new client: {cli_id.hex()}\")\n        clients.add(cli_id)\n    logging.info(f\"[{cli_id.hex()}] [HELLO] Arch:{arch.name} engines:{[x.name for x in engines]} (cpu:{cpus}, mem:{memory})\")", "\n\ndef log_received(cli_id: bytes, level: LogLevel, message: str):\n    logging.info(f\"[{cli_id.hex()}] [LOG] [{level.name}] {message}\")\n\n\ndef telemetry_received(cli_id: bytes, *args):\n    logging.info(f\"[{cli_id.hex()}] [TELEMETRY] [{args}\")\n\n\ndef stop_coverage_received(cli_id: bytes):\n    logging.info(f\"[{cli_id.hex()}] [STOP_COVERAGE]\")", "\n\ndef stop_coverage_received(cli_id: bytes):\n    logging.info(f\"[{cli_id.hex()}] [STOP_COVERAGE]\")\n\n\nif __name__ == \"__main__\":\n\n    agent = BrokerAgent()\n    agent.bind()\n\n    agent.register_seed_callback(seed_received)\n    agent.register_hello_callback(hello_received)\n    agent.register_log_callback(log_received)\n    agent.register_telemetry_callback(telemetry_received)\n    agent.register_stop_coverage_callback(stop_coverage_received)\n\n    agent.start()\n\n    # Now start listening on a seed folder given in parameter\n    # and send them to all clients that have at least sent\n    # once a message\n\n    i = inotify.adapters.Inotify()\n\n    i.add_watch(sys.argv[1])\n\n    for event in i.event_gen():\n        if event is not None:\n            (header, type_names, watch_path, filename) = event\n\n            if 'IN_CLOSE_WRITE' in type_names:\n                file = Path(watch_path) / filename\n                bts = file.read_bytes()\n                print(f\"send: {filename}\")\n                for cli in clients:\n                    agent.send_seed(cli, SeedType.INPUT, bts)", "\n'''\nPYTHONPATH=. python3  ./tests/broker_fw_honggfuzz_seed.py /tmp/toto \n'''"]}
{"filename": "pastisbenchmark/replayer.py", "chunked_list": ["# built-in imports\nfrom enum import Enum, auto\nfrom pathlib import Path\nfrom typing import Generator, Optional\nimport os\nimport subprocess\nimport logging\nimport time\nimport re\nfrom datetime import datetime, timedelta", "import re\nfrom datetime import datetime, timedelta\n\n# third-party\nfrom pastisbroker.workspace import Workspace, WorkspaceStatus\nfrom libpastis.types import SeedInjectLoc, SeedType\nfrom tritondse.trace import QBDITrace, TraceException\nfrom tritondse import CoverageStrategy\n\n\nclass ReplayType(Enum):\n    qbdi = auto()\n    llvm_profile = auto()", "\n\nclass ReplayType(Enum):\n    qbdi = auto()\n    llvm_profile = auto()\n\n\nclass Replayer(object):\n\n    QBDI_REPLAY_DIR = \"replays_qbdi\"\n    LLVMPROFILE_REPLAY_DIR = \"replays_llvmprof\"\n    REPLAY_FAILS_LOG = \"replay_fails.log\"\n\n    def __init__(self, program: Path, workspace: Path, type: ReplayType, injloc: SeedInjectLoc,\n                 stream: bool = False, full: bool = False, timeout: int = 15, *args):\n        self.workspace = Workspace(workspace)\n        self.type = type\n        self.stream = stream\n        self._full = full\n        self.program = Path(program)\n        self._inject_loc = injloc\n        self._timeout = timeout\n        self._args = list(args)\n        self._fails = []\n        self._tracing_times = []\n\n        # initiatialize directories\n        self._init_directories()\n\n    def _init_directories(self):\n        if not self.corpus_replay_dir.exists():\n            self.corpus_replay_dir.mkdir()\n\n    @property\n    def corpus_replay_dir(self) -> Path:\n        if self.type == ReplayType.qbdi:\n            return self.workspace.root / self.QBDI_REPLAY_DIR\n        else:\n            return self.workspace.root / self.LLVMPROFILE_REPLAY_DIR\n\n    def iter(self) -> Generator[Path, None, None]:\n        yield from self.workspace.iter_initial_corpus_directory()\n        yield from self.workspace.iter_corpus_directory(SeedType.INPUT)\n\n    def replay(self, input: Path) -> bool:\n        if self.type == ReplayType.qbdi:\n            return self._replay_qbdi(input)\n        else:\n            return self._replay_llvm_profile(input)\n\n    def _replay_qbdi(self, input: Path) -> bool:\n        out_file = self.corpus_replay_dir / (input.name + \".trace\")\n\n        if out_file.exists():\n            # The trace has already been generated\n            return True\n\n        args = self._args[:]\n\n        # If inject on argv try replacing the right argv\n        if self._inject_loc == SeedInjectLoc.ARGV:\n            if \"@@\" in args:\n                idx = args.index(\"@@\")\n                args[idx] = str(input.absolute())\n\n        try:\n            t0 = time.time()\n            res = QBDITrace.run(CoverageStrategy.EDGE,\n                                 str(self.program.absolute()),\n                                 args=args,\n                                 output_path=str(out_file.absolute()),\n                                 stdin_file=input if self._inject_loc == SeedInjectLoc.STDIN else None,\n                                 dump_trace=self._full,\n                                 cwd=self.program.parent,\n                                 timeout=self._timeout)\n            self._tracing_times.append(time.time()-t0)\n            return res\n        except TraceException as e:\n            self._fails.append(input)\n            return False\n\n    def _replay_llvm_profile(self, input: Path) -> bool:\n        pass\n\n\n    def start(self):\n        # TODO: Start monitoring folders (and status file)\n        pass\n\n    def save_fails(self):\n        with open(self.workspace.root / self.REPLAY_FAILS_LOG, \"w\") as f:\n            f.write(\"\\n\".join(str(x) for x in self._fails))\n\n    def print_stats(self):\n        def tt(secs):\n            return str(timedelta(seconds=int(secs)))\n        if not self._tracing_times:\n            print(\"nothing replayed\")\n            return\n        sum_tracing = sum(self._tracing_times)\n        mean_replay = sum_tracing / len(self._tracing_times)\n        print(f\"Tracing: {tt(sum_tracing)} (avg: {mean_replay}s)\")", ""]}
{"filename": "pastisbenchmark/models.py", "chunked_list": ["from typing import List\n\nfrom pydantic import BaseModel\n\nclass InputEntry(BaseModel):\n    engine: str\n    number: int\n    unique: int\n    useless: int\n    condition: int\n    symread: int\n    symwrite: int\n    symjump: int", "\nclass CoverageEntry(BaseModel):\n    engine: str\n    number: int\n    unique: int\n    first: int\n    total: int\n\nclass ExecEntry(BaseModel):\n    engine: str\n    dse: float\n    smt: float\n    replay: float\n    total: float\n    wait: float", "class ExecEntry(BaseModel):\n    engine: str\n    dse: float\n    smt: float\n    replay: float\n    total: float\n    wait: float\n\nclass SeedSharingEntry(BaseModel):\n    engine: str\n    accepted: int\n    rejected: int\n    total: int\n    ratio: float", "class SeedSharingEntry(BaseModel):\n    engine: str\n    accepted: int\n    rejected: int\n    total: int\n    ratio: float\n\nclass SmtEntry(BaseModel):\n    engine: str\n    sat: int\n    unsat: int\n    timeout: int\n    total: int\n    avg_query: float\n    cov_sat_ratio: float\n    branch_solved: int\n    branch_not_solved: int", "\n\nclass CampaignStats(BaseModel):\n    input_stats: List[InputEntry]\n    coverage_stats: List[CoverageEntry]\n    exec_stats: List[ExecEntry]\n    seed_sharing_stats: List[SeedSharingEntry]\n    smt_stats: List[SmtEntry]\n", ""]}
{"filename": "pastisbenchmark/__init__.py", "chunked_list": ["from .replayer import ReplayType, Replayer\nfrom .results import InputCovDelta, CampaignResult\nfrom .plotter import Plotter\n"]}
{"filename": "pastisbenchmark/plotter.py", "chunked_list": ["# built-in imports\nimport logging\nfrom pathlib import Path\nfrom typing import Union, List, Dict\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport json\nfrom datetime import timedelta\nfrom hashlib import md5\nimport base64", "from hashlib import md5\nimport base64\nfrom collections import Counter\n\n# third-party imports\nfrom rich.console import Console\nfrom rich.table import Table\n\n# local imports\nfrom pastisbenchmark.results import InputCovDelta, CampaignResult", "# local imports\nfrom pastisbenchmark.results import InputCovDelta, CampaignResult\nfrom pastisbenchmark.models import CampaignStats, InputEntry, CoverageEntry, ExecEntry, SeedSharingEntry, SmtEntry\nfrom tritondse import CoverageStrategy, SmtSolver, BranchSolvingStrategy\n\n\nclass Plotter(object):\n\n    LABEL_SZ = 18\n    TICK_SZ = 13\n    FONT_SZ = 18\n    LEGEND_SZ = 8\n\n    PLOT_DIR = \"plots\"\n\n    def __init__(self, name: str, timeout: int):\n        self.fig, (self.ax1, self.ax2) = plt.subplots(1, 2)\n        self.name = name\n        self._timeout = timeout\n\n        # self._configure_plot(self.ax1, ylabel=\"coverage (edge)\")\n        # self._configure_plot(self.ax2, ylabel=\"coverage (edge)\", is_log=True)\n\n    def _configure_plot(self, plot, ylabel: str, is_log: bool = False):\n        plot.tick_params(axis='both', which='major', labelsize=self.TICK_SZ)\n        plot.tick_params(axis='both', which='minor', labelsize=self.TICK_SZ)\n        plot.set_title(f\"{self.name} {'(logscale)' if is_log else ''}\", fontsize=self.FONT_SZ)\n        plot.set(xlabel='seconds', ylabel=ylabel)\n        plot.yaxis.label.set_size(self.LABEL_SZ)\n        plot.xaxis.label.set_size(self.LABEL_SZ)\n        plot.legend(prop={'size': self.LEGEND_SZ})\n        if is_log:\n            plot.set_xscale(\"log\")\n\n    def add_campaign_to_plot(self, campaign: CampaignResult, show_union: bool=True):\n        \"\"\" Iterate all stat_items and generate coverage plot.\"\"\"\n        for fuzzer, results in campaign.results:\n            is_all_fuzzer = bool(fuzzer == CampaignResult.ALL_FUZZER)\n            if fuzzer == CampaignResult.SEED_FUZZER:\n                continue\n            if is_all_fuzzer and campaign.is_half_duplex and not show_union:\n                continue\n            if campaign.is_full_duplex and not is_all_fuzzer:  # Only print the ALL fuzzer in fullduplex\n                continue\n            name = self.format_fuzzer_name(campaign, fuzzer)\n            # fmt = self.format_plot(fuzzer, campaign.is_full_duplex)\n\n            marker = \"--\" if campaign.is_half_duplex and is_all_fuzzer else \"-\"\n            color = self.format_plot(campaign, fuzzer)\n\n            self.add_to_plot(self.ax1, name, results, is_all_fuzzer, linestyle=marker, color=color)\n            self.add_to_plot(self.ax2, name, results, is_all_fuzzer, linestyle=marker, color=color)\n\n    def add_to_plot(self, plot, fuzzer: str, results: List[InputCovDelta], use_global: bool, **kwargs):\n        xaxe = [x.time_elapsed for x in results]\n        yaxe = [(x.overall_coverage_sum if use_global else x.fuzzer_coverage_sum) for x in results]\n\n        if not yaxe:\n            print(f\"no plot for {fuzzer}\")\n            return\n\n        # Add dummy value to make horizontal line\n        xaxe.append(self._timeout)\n        yaxe.append(yaxe[-1])\n\n        plot.plot(xaxe, yaxe, label=fuzzer, linewidth=2, **kwargs)\n\n    def format_fuzzer_name(self, campaign: CampaignResult, fuzzer: str, short: bool=False) -> str:\n        if fuzzer == CampaignResult.ALL_FUZZER:\n            return campaign.slug_name\n        elif fuzzer == CampaignResult.SEED_FUZZER:\n            return CampaignResult.SEED_FUZZER\n        elif \"TT\" in fuzzer:\n            if short:\n                return \"TritonDSE\"\n            config = campaign.fuzzers_config[fuzzer]\n            cov_name = {CoverageStrategy.BLOCK: \"B\", CoverageStrategy.EDGE: \"E\", CoverageStrategy.PREFIXED_EDGE: \"PE\",\n                        CoverageStrategy.PATH: \"P\"}[config.coverage_strategy]\n            param = [\n                \"R\" if BranchSolvingStrategy.COVER_SYM_READ in config.branch_solving_strategy else \"-\",\n                \"W\" if BranchSolvingStrategy.COVER_SYM_WRITE in config.branch_solving_strategy else \"-\",\n                \"J\" if BranchSolvingStrategy.COVER_SYM_DYNJUMP in config.branch_solving_strategy else \"-\"\n            ]\n            solver = {SmtSolver.Z3: \"Z3\", SmtSolver.BITWUZLA: \"BZLA\"}[config.smt_solver]\n            return f\"TritonDSE[{cov_name}][{''.join(param)}][{solver}]\"\n        elif \"AFLPP\" in fuzzer:\n            return \"AFL++\"\n        elif \"HF\" in fuzzer:\n            return \"Honggfuzz\"\n        else:\n            return fuzzer\n\n    def format_plot(self, campaign, fuzzer) -> str:\n        green = \"#30a230\"\n        grey = \"#1f77b4\"\n        brown = \"#944b0c\"\n        if fuzzer == CampaignResult.ALL_FUZZER:\n            if campaign.is_full_duplex:\n                return green if campaign.has_honggfuzz() else brown\n            else:\n                return grey\n        elif \"TT\" in fuzzer:\n            return \"#d62728\"\n        elif \"AFLPP\" in fuzzer:\n            return \"#ff8214\"\n        elif \"HF\" in fuzzer:\n            return \"#9467bd\"\n        else:\n            return \"\"\n\n\n    def add_triton_input(self, campaign: CampaignResult):\n        if campaign.is_full_duplex:\n            results = campaign.fuzzers_items[campaign.ALL_FUZZER]\n            X = [x.time_elapsed for x in results if \"TT\" in x.fuzzer]\n            Y = [x.overall_coverage_sum for x in results if \"TT\" in x.fuzzer]\n            self.ax1.plot(X, Y, 'b.', label=\"TT input\")\n            self.ax2.plot(X, Y, 'b.', label=\"TT input\")\n        else:\n            logging.warning(f\"campaign:{campaign.workspace.root} not full duplex do not show triton inputs\")\n\n    def show(self):\n        self._configure_plot(self.ax1, ylabel=\"coverage (edge)\")\n        self._configure_plot(self.ax2, ylabel=\"coverage (edge)\", is_log=True)\n        plt.show()\n\n    def save_to(self, dir: Union[str, Path]) -> None:\n        plt.savefig(dir / \"plot.pdf\", dpi=600)\n\n\n    def calculate_stats(self, campaign: CampaignResult) -> CampaignStats:\n        input_stats = self._calcul_input_stats(campaign)\n        coverage_stats = self._calcul_coverage_stats(campaign)\n        exec_stats = self._calcul_exec_stats(campaign)\n        seed_sharing_stats = self._calcul_seed_sharing_stats(campaign)\n        smt_stats = self._calcul_smt_stats(campaign, coverage_stats)\n        return CampaignStats(input_stats=input_stats, coverage_stats=coverage_stats, exec_stats=exec_stats,\n                             seed_sharing_stats=seed_sharing_stats, smt_stats=smt_stats)\n\n\n    def _calcul_triton_input_to_broker(self, campaign: CampaignResult) -> Dict[str, str]:\n        mapping = {}\n\n        def iter_input_dir(conf, dirname):\n            tt_workspace = campaign.workspace.root / conf.workspace\n            for file in (tt_workspace / dirname).iterdir():\n                try:\n                     raw = base64.b64decode(json.loads(file.read_text())['files']['input_file'])\n                except:\n                    raw = file.read_bytes()\n                mapping[md5(raw).hexdigest()] = str(file)\n\n        for fuzzer, items in campaign.results:\n            if campaign.is_triton(fuzzer):\n                conf = campaign.fuzzers_config[fuzzer]\n                iter_input_dir(conf, \"corpus\")\n                iter_input_dir(conf, \"worklist\")\n                iter_input_dir(conf, \"crashes\")\n\n        return mapping\n\n    def _calcul_input_stats(self, campaign: CampaignResult) -> List[InputEntry]:\n        entries = []\n\n        # FIXME: Compute uniquness\n        useless_ctrs = Counter()\n\n        for fuzzer, items in campaign.results:\n            num = len(items)\n            for item in items:\n                if not len(item.overall_new_items_covered):\n                    useless_ctrs[fuzzer] += 1\n            syms = {\"CC\": 0, \"SR\": 0, \"SW\": 0, \"DYN\": 0}\n            if campaign.is_triton(fuzzer):\n                conf = campaign.fuzzers_config[fuzzer]\n                tt_workspace = campaign.workspace.root / conf.workspace\n                for file in (tt_workspace / \"corpus\").iterdir():\n                    for s in syms:\n                        if s in str(file):\n                            syms[s] += 1\n                for file in (tt_workspace / \"worklist\").iterdir():\n                    for s in syms:\n                        if s in str(file):\n                            syms[s] += 1\n                for file in (tt_workspace / \"crashes\").iterdir():\n                    for s in syms:\n                        if s in str(file):\n                            syms[s] += 1\n\n            entry = InputEntry(engine=fuzzer, number=num, unique=-1, useless=useless_ctrs[fuzzer], condition=syms[\"CC\"],\n                               symread=syms[\"SR\"], symwrite=syms[\"SW\"], symjump=syms[\"DYN\"])\n            entries.append(entry)\n        return entries\n\n    def _calcul_coverage_stats(self, campaign: CampaignResult) -> List[CoverageEntry]:\n        # all_cov = campaign.fuzzers_items[campaign.ALL_FUZZER]\n        seed_cov = campaign.fuzzers_coverage[campaign.SEED_FUZZER]\n\n        entries = []\n\n        firsts = Counter()\n        for entry in campaign.delta_items:\n            firsts[entry.fuzzer] += len(entry.overall_new_items_covered)\n\n        for fuzzer, items in campaign.results:\n            cov = campaign.fuzzers_coverage[fuzzer]\n            num = len(cov.difference(seed_cov)) if fuzzer != campaign.SEED_FUZZER else cov.unique_covitem_covered\n\n            # FIXME: Compute unique & first\n            first = firsts[fuzzer]\n            entry = CoverageEntry(engine=fuzzer, number=num, unique=-1, first=first, total=cov.unique_covitem_covered)\n            entries.append(entry)\n        return entries\n\n    def _calcul_exec_stats(self, campaign: CampaignResult) -> List[ExecEntry]:\n        entries = []\n\n        for fuzzer, config in campaign.fuzzers_config.items():\n            try:\n                if campaign.is_triton(fuzzer):\n                    workdir = (campaign.workspace.root / \"clients_ws\") / Path(config.workspace).name\n                    pstats = json.loads((workdir / \"metadata/pastidse-stats.json\").read_text())\n\n                    # Timing stats\n                    tot, replay_time = pstats[\"total_time\"], pstats[\"replay_time\"]\n                    emu_time = pstats.get(\"emulation_time\", 0)\n\n                    solv_time = pstats.get(\"solving_time\")\n                    if solv_time is None:\n                        sstats = json.loads((workdir / \"metadata/solving_stats.json\").read_text())\n                        solv_time = sstats['total_solving_time']\n\n                    dse = emu_time - replay_time\n                    run_time = dse + replay_time + solv_time\n                    wait = tot - run_time\n\n                    entry = ExecEntry(engine=fuzzer, dse=dse, smt=solv_time, replay=replay_time, total=run_time, wait=wait)\n                    entries.append(entry)\n            except FileNotFoundError:\n                logging.error(f\"can't find Triton stats for {fuzzer}\")\n        return entries\n\n    def _calcul_seed_sharing_stats(self, campaign: CampaignResult) -> List[SeedSharingEntry]:\n        entries = []\n\n        for fuzzer, config in campaign.fuzzers_config.items():\n            try:\n                if campaign.is_triton(fuzzer):\n                    workdir = (campaign.workspace.root / \"clients_ws\") / Path(config.workspace).name\n                    pstats = json.loads((workdir / \"metadata/pastidse-stats.json\").read_text())\n                    tots, accs, rejs = pstats[\"seed_received\"], pstats[\"seed_accepted\"], pstats[\"seed_rejected\"]\n                    ratio = accs/tots if rejs else 1\n\n                    entry = SeedSharingEntry(engine=fuzzer, accepted=accs, rejected=rejs, total=tots, ratio=ratio)\n                    entries.append(entry)\n            except FileNotFoundError:\n                logging.error(f\"can't find Triton stats for {fuzzer}\")\n        return entries\n\n\n    def _calcul_smt_stats(self, campaign: CampaignResult, cov_results: List[CoverageEntry]) -> List[SmtEntry]:\n        cov_data = {cov.engine: cov for cov in cov_results}\n\n        entries = []\n\n        for fuzzer, config in campaign.fuzzers_config.items():\n            try:\n                if campaign.is_triton(fuzzer):\n                    cov_number = cov_data[fuzzer].number if fuzzer in cov_data else 0\n                    workdir = (campaign.workspace.root / \"clients_ws\") / Path(config.workspace).name\n                    sstats = json.loads((workdir / \"metadata/solving_stats.json\").read_text())\n\n                    # Solving stats\n                    sovt = sstats['total_solving_time']\n                    stot, sat, unsat, to = sstats[\"total_solving_attempt\"], sstats[\"SAT\"], sstats[\"UNSAT\"], sstats[\"TIMEOUT\"]\n                    coved, uncoved = len(sstats[\"branch_reverted\"]), len(sstats[\"branch_not_solved\"])\n                    ratio = cov_number / sat if sat else cov_number\n                    entry = SmtEntry(engine=fuzzer, sat=sat, unsat=unsat, timeout=to, total=stot, avg_query=sovt/stot,\n                                     cov_sat_ratio=ratio, branch_solved=coved, branch_not_solved=uncoved)\n                    entries.append(entry)\n            except FileNotFoundError:\n                logging.error(f\"can't find Triton stats for {fuzzer}\")\n        return entries\n\n\n    def print_stats(self, campaign: CampaignResult, stats: CampaignStats):\n        console = Console()\n\n        def sfmt(seconds, total, w = True) -> str:\n            m, s = divmod(seconds, 60)\n            h, m = divmod(m, 60)\n            s = int(s) if int(s) > 0 else f\"{s:.2f}\"\n            t = (f\"{int(h)}h\" if h else '') + f\"{int(m)}m{s}s\"\n            perc = f\"{seconds / total:.2%}\"\n            return f\"{t} ({perc})\" if w else t\n\n\n        # InputEntry\n        table = Table(show_header=True, title=\"INPUT\", header_style=\"bold magenta\")\n        for name in [\"engine\", \"number\", \"unique\", \"useless\", \"CC\", \"SR\", \"SW\", \"SDYN\"]:\n            table.add_column(name)\n        for it in stats.input_stats:\n            fname = self.format_fuzzer_name(campaign, it.engine, False)\n            useless = f\"{it.useless} ({it.useless / it.number:.2%})\"\n            table.add_row(fname, str(it.number), str(it.unique), useless, str(it.condition), str(it.symread), str(it.symwrite), str(it.symjump))\n        console.print(table)\n\n        # Coverage\n        table = Table(show_header=True, title=\"COVERAGE\", header_style=\"bold magenta\")\n        for name in [\"engine\", \"edge-cov\", \"unique\", \"first\", \"total\"]:\n            table.add_column(name)\n        for it in stats.coverage_stats:\n            fname = self.format_fuzzer_name(campaign, it.engine, False)\n            table.add_row(fname, str(it.number), str(it.unique), str(it.first), str(it.total))\n        console.print(table)\n\n        # ExecEntry\n        table = Table(show_header=True, title=\"EXECUTION\", header_style=\"bold magenta\")\n        for name in [\"engine\", \"DSE\", \"SMT\", \"replay\", \"total\", \"wait\"]:\n            table.add_column(name)\n        for it in stats.exec_stats:\n            fname = self.format_fuzzer_name(campaign, it.engine, False)\n            tot = it.total\n            table.add_row(fname, sfmt(it.dse, tot), sfmt(it.smt, tot), sfmt(it.replay, tot), sfmt(tot, tot, False), sfmt(it.wait, tot, False))\n        console.print(table)\n\n        # SeedSharingEntry\n        table = Table(show_header=True, title=\"SEED SHARING\", header_style=\"bold magenta\")\n        for name in [\"engine\", \"accepted\", \"rejected\", \"total\"]:\n            table.add_column(name)\n        for it in stats.seed_sharing_stats:\n            fname = self.format_fuzzer_name(campaign, it.engine, False)\n            acc = f\"{it.accepted} ({it.accepted / it.total:.2%})\"\n            rejs = f\"{it.rejected} ({it.rejected / it.total:.2%})\"\n            table.add_row(fname, acc, rejs, str(it.total))\n        console.print(table)\n\n        # SmtEntry\n        table = Table(show_header=True, title=\"SMT SOLVING\", header_style=\"bold magenta\")\n        for name in [\"engine\", \"SAT\", \"UNSAT\", \"TO\", \"Total\", \"mean query\", \"cov/input\", \"branches solved\", \"branches not solved\"]:\n            table.add_column(name)\n        for it in stats.smt_stats:\n            fname = self.format_fuzzer_name(campaign, it.engine, False)\n            table.add_row(fname, str(it.sat), str(it.unsat), str(it.timeout), str(it.total), f\"{it.avg_query:.2f}\", f\"{it.cov_sat_ratio:.2f}\", str(it.branch_solved), str(it.branch_not_solved))\n        console.print(table)\n\n\n        # for stat in (getattr(stats, x) for x in stats.schema()['properties']):\n        #     if not stat:\n        #         print(f\"Stat {stat} is empty\")\n        #         continue\n        #     table = Table(show_header=True, title=str(type(stat[0])), header_style=\"bold magenta\")\n        #     item = stat[0]\n        #\n        #     for name, column in {x: getattr(item, x) for x in item.schema()['properties']}.items():\n        #         table.add_column(name)\n        #     for item in stat:\n        #         table.add_row(*[str(getattr(item, x)) for x in item.schema()['properties']])\n        #     console.print(table)\n\n\n\n    def show_delta_history(self, campaign: CampaignResult) -> None:\n        console = Console()\n        def tt(secs):\n            return str(timedelta(seconds=int(secs)))\n        def pp_edge(e):\n            return f\"({e[0]:#08x}-{e[1]:#08x})\"\n\n        mapping = self._calcul_triton_input_to_broker(campaign)\n\n        def caract_input(name) -> str:\n            types = [\"_CC_\", \"_SR_\", \"_SW_\", \"_DYN_\"]\n            for t in types:\n                if t in name:\n                    return t[1:-1]\n            return \"N/C\"\n\n        table = Table(show_header=True, title=\"Delta History\", header_style=\"bold magenta\")\n        for col in [\"Elapsed\", \"Fuzzer\", \"Type\", \"New\", \"Tot Cov\", \"Items\"]:\n            table.add_column(col)\n        # FIXME: Ajouter une colonne pour le count\n\n        for delta in campaign.delta_items:\n\n            # Resolve broker input to triton ones (to what it was generated from)\n            typ = \"-\"\n            if campaign.is_triton(delta.fuzzer):\n                meta = campaign.parse_filename(delta.input_name)\n                if meta:\n                    hash = meta[3]\n                    triton_input_name = mapping.get(hash)\n                    if triton_input_name:\n                        typ = caract_input(triton_input_name)\n\n            table.add_row(tt(delta.time_elapsed),\n                          self.format_fuzzer_name(campaign, delta.fuzzer, short=True),\n                          typ,\n                          str(len(delta.overall_new_items_covered)),\n                          str(delta.overall_coverage_sum),\n                          \" \".join(pp_edge(e) for e in list(delta.overall_new_items_covered)[:4])\n            )\n\n        console.print(table)", ""]}
{"filename": "pastisbenchmark/results.py", "chunked_list": ["# built-in imports\nfrom abc import abstractmethod\nfrom enum import Enum, auto\nfrom pathlib import Path\nfrom typing import Generator, Optional, Union, List, Dict, Tuple, Set\nimport os\nimport json\nimport sys\nimport subprocess\nimport logging", "import subprocess\nimport logging\nimport re\nfrom datetime import datetime, timedelta\nfrom pydantic import BaseModel\nimport matplotlib.pyplot as plt\n\n# third-party\nfrom pastisbroker import BrokingMode\nfrom pastisbroker.workspace import Workspace, WorkspaceStatus", "from pastisbroker import BrokingMode\nfrom pastisbroker.workspace import Workspace, WorkspaceStatus\nfrom libpastis.types import SeedInjectLoc, SeedType\nfrom tritondse.trace import QBDITrace\nfrom tritondse import CoverageStrategy, GlobalCoverage, BranchSolvingStrategy, Config\nfrom tritondse.coverage import CovItem\n\n# local imports\nfrom pastisbenchmark.replayer import ReplayType\n\nclass InputCovDelta(BaseModel):\n    time_elapsed: float        # Time elapsed when input generated\n    input_name: str            # Input name\n    fuzzer: str                # The fuzzer that found that input\n\n    # Trace generic info\n    unique_items_covered_count: int            # The coverage of this one input (len(covered_items))\n    unique_insts_covered_count: int            # Instruction coverage of that input\n\n    # Local to the fuzzer\n    fuzzer_new_items_covered: Set[CovItem]  # New items covered by THIS fuzzer\n    fuzzer_coverage_sum: int            # Sum of covered items by THIS fuzzer at this point\n    fuzzer_inst_coverage_sum: int       # Sum of unique instructions covered by THIS FUZZER at this point\n    fuzzer_new_insts_covered: Set[int]  # Instr\n\n    # Global accros all fuzzers of the campaign\n    overall_new_items_covered: Set[CovItem]  # New items covered by this fuzzer agains ALL THE OTHERS\n    overall_coverage_sum: int            # The total coverage of the fuzz campaign at this point\n    overall_inst_coverage_sum: int       # Total instruction coverage at this point\n    overall_new_insts_covered: Set[int]  # Instr\n\n\n    def is_initial_input(self) -> bool:\n        return self.fuzzer == \"seeds\"\n\n    def is_triton_input(self) -> bool:\n        return \"TT\" in self.fuzzer", "from pastisbenchmark.replayer import ReplayType\n\nclass InputCovDelta(BaseModel):\n    time_elapsed: float        # Time elapsed when input generated\n    input_name: str            # Input name\n    fuzzer: str                # The fuzzer that found that input\n\n    # Trace generic info\n    unique_items_covered_count: int            # The coverage of this one input (len(covered_items))\n    unique_insts_covered_count: int            # Instruction coverage of that input\n\n    # Local to the fuzzer\n    fuzzer_new_items_covered: Set[CovItem]  # New items covered by THIS fuzzer\n    fuzzer_coverage_sum: int            # Sum of covered items by THIS fuzzer at this point\n    fuzzer_inst_coverage_sum: int       # Sum of unique instructions covered by THIS FUZZER at this point\n    fuzzer_new_insts_covered: Set[int]  # Instr\n\n    # Global accros all fuzzers of the campaign\n    overall_new_items_covered: Set[CovItem]  # New items covered by this fuzzer agains ALL THE OTHERS\n    overall_coverage_sum: int            # The total coverage of the fuzz campaign at this point\n    overall_inst_coverage_sum: int       # Total instruction coverage at this point\n    overall_new_insts_covered: Set[int]  # Instr\n\n\n    def is_initial_input(self) -> bool:\n        return self.fuzzer == \"seeds\"\n\n    def is_triton_input(self) -> bool:\n        return \"TT\" in self.fuzzer", "\n\nclass CampaignResult(object):\n\n    SEED_FUZZER = \"seeds\"\n    ALL_FUZZER = \"all\"\n\n    QBDI_REPLAY_DIR = \"replays_qbdi\"\n    LLVMPROFILE_REPLAY_DIR = \"replays_llvmprof\"\n    REPLAYS_DELTA = \"replays_delta\"\n    CLIENT_STATS = \"clients-stats.json\"\n    COVERAGE_DIR = \"coverages\"\n\n    def __init__(self, workspace: Union[Path, str]):\n        self.workspace = Workspace(Path(workspace))\n        # Stat items\n        self.fuzzers_items = {}     # fuzzer_name -> List[StatItem]\n        self.fuzzers_coverage = {}  # fuzzer_name -> Coverage\n        self.fuzzers_config = {}    # fuzzer_name -> Union[str, Config]\n        self._load_fuzzer_configs()\n        self._all_items = []\n\n        # initialize directories\n        self._init_directories()\n\n        # Load fuzzers configuration\n\n        self.mode = self.load_broking_mode(self.workspace)\n\n    def _load_fuzzer_configs(self):\n        f = self.workspace.root / self.CLIENT_STATS\n        data = json.loads(f.read_text())\n        for client in data:\n            id = client['strid']\n            conf = client['engine_args']\n            if self.is_triton(id):\n                self.fuzzers_config[id] = Config.from_json(conf)\n            else:\n                self.fuzzers_config[id] = conf\n\n    def has_honggfuzz(self):\n        for fuzz in self.fuzzers_items.keys():\n            if \"HF\" in fuzz:\n                return True\n        return False\n\n    @staticmethod\n    def is_triton(fuzzer: str) -> bool:\n        return \"TT\" in fuzzer\n\n    @property\n    def is_full_duplex(self) -> bool:\n        return bool(self.mode == BrokingMode.FULL)\n\n    @property\n    def is_half_duplex(self) -> bool:\n        return bool(self.mode == BrokingMode.NO_TRANSMIT)\n\n    @property\n    def slug_name(self) -> str:\n        data = [\"AFL\" if any((\"AFLPP\" in x for x in self.fuzzers_config)) else \"\",\n                \"HF\" if any((\"HF\" in x for x in self.fuzzers_config)) else \"\",\n                \"TT\" if any((\"TT\" in x for x in self.fuzzers_config)) else \"\"]\n        if self.is_full_duplex:\n            return f\"PASTIS[{'|'.join(x for x in data if x)}]\"\n        else:\n            return f\"U[{'|'.join(x for x in data if x)}]\"\n\n    @property\n    def results(self):\n        return self.fuzzers_items.items()\n\n    @property\n    def delta_items(self) -> Generator[InputCovDelta, None, None]:\n        yield from self._all_items\n\n    def _init_directories(self):\n        if not self.replay_delta_dir.exists():\n            self.replay_delta_dir.mkdir()\n\n    def _init_fuzzer_coverage(self, fuzzer_name: str) -> None:\n        if fuzzer_name in self.fuzzers_coverage:\n            return None\n\n        # consider seed inputs as common to all\n        if self.SEED_FUZZER in self.fuzzers_coverage:\n            cov = self.fuzzers_coverage[self.SEED_FUZZER].clone()\n        else: # else create an empty coverage file\n            cov = GlobalCoverage(CoverageStrategy.EDGE, BranchSolvingStrategy.ALL_NOT_COVERED)\n\n        self.fuzzers_coverage[fuzzer_name] = cov\n\n    @staticmethod\n    def load_broking_mode(workspace: Workspace) -> BrokingMode:\n        data = json.loads(workspace.config_file.read_text())\n        return BrokingMode[data['broker_mode']]\n\n    @property\n    def replay_delta_dir(self) -> Path:\n        return self.workspace.root / self.REPLAYS_DELTA\n\n    def has_delta_files(self) -> bool:\n        return len(list(self.replay_delta_dir.iterdir())) != 0\n\n    def replay_ok(self) -> bool:\n        return len(list(self.replay_delta_dir.iterdir())) != 0\n\n    @staticmethod\n    def parse_filename(filename) -> Optional[tuple]:\n        ref = datetime.strptime(\"0:00:00.00\", \"%H:%M:%S.%f\")\n        if re.match(\"^\\d{4}-\\d{2}-\\d{2}\", filename):  # start by the date\n            date, time, elapsed, fuzzer_id, hash = filename.split(\"_\")\n            date = datetime.strptime(f\"{date}_{time}\", \"%Y-%m-%d_%H:%M:%S\")\n            spl = elapsed.split(\":\")\n            if len(spl) == 4:\n                elapsed = int(spl[0][:-1])*(3600*24)\n                elapsed += (datetime.strptime(\":\".join(spl[1:]), \"%H:%M:%S.%f\") - ref).total_seconds()\n            else:\n                elapsed = (datetime.strptime(elapsed, \"%H:%M:%S.%f\") - ref).total_seconds()\n            hash = hash.split(\".\")[0]\n            return date, elapsed, fuzzer_id, hash\n        else:\n            return None\n\n    def _iter_sorted(self, path: Path):\n        files = {None: []}\n        for file in path.iterdir():\n            res = self.parse_filename(file.name)\n            if res is None:\n                files[None].append(file)\n            else:\n                date, elapsed, fuzzer, hash = res\n                if elapsed in files:\n                    logging.warning(f\"two files with same elapsed time: {files[elapsed]} | {file.name}\")\n                files[elapsed] = file\n\n        # First yield initial seeds\n        init_seeds = files.pop(None)\n        yield from init_seeds\n\n        # Then iterate file sorted by elapsed time\n        for k in sorted(files):\n            yield files[k]\n\n    def load(self, type: ReplayType = ReplayType.qbdi) -> None:\n        logging.info(f\"load in {type.name} [mode:{self.mode.name}]\")\n        if self.has_delta_files():\n            items = self.load_delta_directory()\n            self.load_delta(items)\n            self.load_coverage()  # If delta, coverage files shall also be present\n        elif type == ReplayType.qbdi:\n            items = self.load_qbdi()\n            self.load_delta(items)\n            self.save_coverage()  # Once loaded save coverage files\n        elif type == ReplayType.llvm_profile:\n            self.load_llvmprofile()\n        else:\n            assert False\n\n    def load_qbdi(self) -> List[Tuple[str, InputCovDelta]]:\n        logging.info(\"load qbdi trace files\")\n        folder = self.workspace.root / self.QBDI_REPLAY_DIR\n        total = sum(1 for _ in self._iter_sorted(folder))\n\n        items = []\n\n        # initialize a \"all\" fuzzer in all cases\n        self._init_fuzzer_coverage(self.ALL_FUZZER)\n\n        for i, file in enumerate(self._iter_sorted(folder)):\n            # parse name\n            print(f\"[{i+1}/{total}] {file}\\r\", file=sys.stderr, end=\"\")\n            meta = self.parse_filename(file.name)\n\n            # Get the fuzzer name (and coverage)\n            if meta is None:\n                fuzzer = self.SEED_FUZZER\n            else:\n                fuzzer = meta[2]\n\n            self._init_fuzzer_coverage(fuzzer)\n\n            cov = QBDITrace.from_file(file).coverage\n\n            # Compute differences to know what has been covered\n            if self.is_half_duplex:\n                fuzzer_coverage = self.fuzzers_coverage[fuzzer]   # get the fuzzer coverage at this point\n                local_new_items = cov - fuzzer_coverage\n                local_new_instrs = cov.covered_instructions.keys() - fuzzer_coverage.covered_instructions.keys()\n                fuzzer_coverage.merge(cov)   # Merge the new coverage\n                cov_sum = fuzzer_coverage.unique_covitem_covered\n                inst_sum = fuzzer_coverage.unique_instruction_covered\n            else:  # FULL DUPLEX\n                local_new_items, local_new_instrs, cov_sum, inst_sum = set(), set(), 0, 0\n\n\n            # Update coverage and info OVERALL\n            all_coverage = self.fuzzers_coverage[self.ALL_FUZZER]  # get the overall coverage\n            overall_new_items = cov - all_coverage                 # compute new items\n            overall_new_instrs = cov.covered_instructions.keys() - all_coverage.covered_instructions.keys()\n            all_coverage.merge(cov)                                # Merge all of them (all fuzzers all together)\n\n\n            # Create an InputCovDelta\n            statitem = InputCovDelta(\n                time_elapsed=0 if meta is None else meta[1],\n                input_name=file.name,\n                fuzzer=fuzzer,\n                # Trace generic info\n                unique_items_covered_count=cov.unique_covitem_covered,         # The coverage of this one input (len(covered_items))\n                unique_insts_covered_count=cov.unique_instruction_covered, # Instruction coverage of that input\n                # Local to the fuzzer\n                fuzzer_new_items_covered=local_new_items, # New items covered by THIS fuzzer\n                fuzzer_coverage_sum=cov_sum,              # Sum of covered items by THIS fuzzer at this point\n                fuzzer_inst_coverage_sum=inst_sum,        # Sum of unique instructions covered by THIS FUZZER at this point\n                fuzzer_new_insts_covered=local_new_instrs,# Instr\n                # Global accros all fuzzers of the campaign\n                overall_new_items_covered=overall_new_items,              # New items covered by this fuzzer agains ALL THE OTHERS\n                overall_coverage_sum=all_coverage.unique_covitem_covered, # The total coverage of the fuzz campaign at this point\n                overall_inst_coverage_sum=all_coverage.unique_instruction_covered, # Total instruction coverage at this point\n                overall_new_insts_covered=overall_new_instrs\n            )\n\n            items.append((fuzzer, statitem))\n\n            # Write the delta file\n            with open(self.replay_delta_dir / (file.name+\".json\"), 'w') as f:\n                f.write(statitem.json())\n        return items\n\n    def load_llvmprofile(self) -> None:\n        # TODO: to implement\n        pass\n\n    def load_delta_directory(self) -> List[Tuple[str, InputCovDelta]]:\n        logging.info(\"load delta directory\")\n\n        items = []\n\n        for file in self._iter_sorted(self.replay_delta_dir):\n            meta = self.parse_filename(file.name)\n\n            # Get the fuzzer name (and coverage)\n            if meta is None:\n                fuzzer = self.SEED_FUZZER\n            else:\n                fuzzer = meta[2]\n\n            delta = InputCovDelta.parse_file(file)\n            items.append((fuzzer, delta))\n        return items\n\n    def load_delta(self, items: List[Tuple[str, InputCovDelta]]) -> None:\n        self._all_items = [x[1] for x in items]\n        self.fuzzers_items[self.SEED_FUZZER] = []\n        self.fuzzers_items[self.ALL_FUZZER] = []\n\n        for fuzzer, item in items:\n            if fuzzer not in self.fuzzers_items:\n                self.fuzzers_items[fuzzer] = []\n\n            self.fuzzers_items[fuzzer].append(item)\n\n            if fuzzer != self.SEED_FUZZER:  # Add in ALL, all that are not seed ones\n                self.fuzzers_items[self.ALL_FUZZER].append(item)\n\n    def save_coverage(self):\n        cov_dir = self.workspace.root / self.COVERAGE_DIR\n        if not cov_dir.exists():\n            cov_dir.mkdir()\n\n        for fuzzer_name, cov in self.fuzzers_coverage.items():\n            covfile = cov_dir / (fuzzer_name + \".ttgcov\")\n            cov.to_file(covfile)\n\n            if cov.covered_instructions:  # if we have instructions covered save file in Lightouse format\n                covfile = cov_dir / (fuzzer_name + \".cov\")\n                with open(covfile, \"w\") as f:\n                    f.write(\"\\n\".join(f\"{x:#08x}\" for x in cov.covered_instructions.keys()))\n                    f.write(\"\\n\")\n\n    def load_coverage(self):\n        cov_dir = self.workspace.root / self.COVERAGE_DIR\n\n        for fuzzer_name in self.fuzzers_items.keys():\n            covfile = cov_dir / (fuzzer_name + \".ttgcov\")\n            if not covfile.exists():\n                logging.error(f\"can't find coverage of {fuzzer_name}\")\n            else:\n                self.fuzzers_coverage[fuzzer_name] = GlobalCoverage.from_file(covfile)\n\n    def print_stats(self):\n        overall_coverage = self.fuzzers_coverage[self.ALL_FUZZER]\n        for fuzzer, results in self.fuzzers_items.items():\n\n            tot_inps = sum(len(x) for x in self.fuzzers_items.values())\n            tot_edge = overall_coverage.unique_covitem_covered\n\n            print(\"-----------------------\")\n            print(f\"Fuzzer: {fuzzer}\")\n            cov = self.fuzzers_coverage[fuzzer]\n            print(f\"inputs: {len(results)}, {len(results)/tot_inps:.0%}\")\n            print(f\"edges: {cov.unique_covitem_covered} {cov.unique_covitem_covered/tot_edge:.0%)}\")\n            print(f\"instructions: {cov.unique_instruction_covered}\")\n            print(\"-----------------------\")\n            print(f\"Total inputs: {tot_inps}\")\n            print(f\"Total edges: {tot_edge}\")", ""]}
{"filename": "libpastis/enginedesc.py", "chunked_list": ["# builtin imports\nfrom pathlib import Path\nfrom typing import List, Tuple, Optional\n\n# Local imports\nfrom libpastis.types import CoverageMode, ExecMode, FuzzMode\n\n\nclass EngineConfiguration(object):\n    \"\"\"\n    Basic interface to represent an engine configuration file\n    on broker side. A fuzzing engine have to provide such object\n    so that the broker can load them and forwarding them to clients.\n    \"\"\"\n\n    @staticmethod\n    def new() -> 'EngineConfiguration':\n        \"\"\"\n        Static method that should return a fresh configuration object.\n\n        :return: Configuration object\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def from_file(filepath: Path) -> 'EngineConfiguration':\n        \"\"\"\n        Load a configuration object from file.\n\n        :param filepath: Path to the configuration\n        :return: Configuration object\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def from_str(s: str) -> 'EngineConfiguration':\n        \"\"\"\n        Parse a string to a configuration object.\n\n        :param s: configuration as string\n        :return: configuration object\n        \"\"\"\n        raise NotImplementedError\n\n    def to_str(self) -> str:\n        \"\"\"\n        Serialize configuration object to string.\n\n        :return: serialize configuration\n        \"\"\"\n        raise NotImplementedError\n\n    def get_coverage_mode(self) -> CoverageMode:\n        \"\"\"\n        Should return the coverage mode defined in the configuration.\n        For greybox fuzzer like AFL++, Honggfuzz one can return :py:obj:`CoverageMode.AUTO`.\n        If the engine support different coverage metric it should return\n        the one selected.\n\n        :return: coverage mode used\n        \"\"\"\n        raise NotImplementedError\n\n    def set_target(self, target: int) -> None:\n        \"\"\"\n        Set a specific target (address, index etc), that should be targeted by\n        the fuzzing engine. This will be used when running in a targeted way.\n\n        :param target: identifier of the target\n        \"\"\"\n        pass", "class EngineConfiguration(object):\n    \"\"\"\n    Basic interface to represent an engine configuration file\n    on broker side. A fuzzing engine have to provide such object\n    so that the broker can load them and forwarding them to clients.\n    \"\"\"\n\n    @staticmethod\n    def new() -> 'EngineConfiguration':\n        \"\"\"\n        Static method that should return a fresh configuration object.\n\n        :return: Configuration object\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def from_file(filepath: Path) -> 'EngineConfiguration':\n        \"\"\"\n        Load a configuration object from file.\n\n        :param filepath: Path to the configuration\n        :return: Configuration object\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def from_str(s: str) -> 'EngineConfiguration':\n        \"\"\"\n        Parse a string to a configuration object.\n\n        :param s: configuration as string\n        :return: configuration object\n        \"\"\"\n        raise NotImplementedError\n\n    def to_str(self) -> str:\n        \"\"\"\n        Serialize configuration object to string.\n\n        :return: serialize configuration\n        \"\"\"\n        raise NotImplementedError\n\n    def get_coverage_mode(self) -> CoverageMode:\n        \"\"\"\n        Should return the coverage mode defined in the configuration.\n        For greybox fuzzer like AFL++, Honggfuzz one can return :py:obj:`CoverageMode.AUTO`.\n        If the engine support different coverage metric it should return\n        the one selected.\n\n        :return: coverage mode used\n        \"\"\"\n        raise NotImplementedError\n\n    def set_target(self, target: int) -> None:\n        \"\"\"\n        Set a specific target (address, index etc), that should be targeted by\n        the fuzzing engine. This will be used when running in a targeted way.\n\n        :param target: identifier of the target\n        \"\"\"\n        pass", "\n\nclass FuzzingEngineDescriptor(object):\n    \"\"\"\n    Abstract class describing a fuzzer engine. This object is used on\n    broker side, to identify the name and version of a fuzzer and to\n    know whether or not it accept a specific executable file.\n    \"\"\"\n\n    NAME = \"abstract-engine\"\n    #: Name of the fuzzing Engine\n    SHORT_NAME = \"AE\"\n    #: Short name of the fuzzing engine\n    VERSION = \"1.0\"\n    #: Version of the engine\n\n    config_class = EngineConfiguration\n    #: Configuration class associated with the engine\n\n    @staticmethod\n    def accept_file(binary_file: Path) -> Tuple[bool, Optional[ExecMode], Optional[FuzzMode]]:\n        \"\"\"\n        Function called by the broker with all executable files detected in its directory.\n        As an fuzzer developer, you have to implement this function to indicate whether\n        a file is accepted as a target or not.\n\n        :param binary_file: file path to an executable file\n        :return: True if supported, and two optional attributes indicating the ExecMode and FuzzMode\n        \"\"\"\n        raise NotImplementedError()\n\n    @staticmethod\n    def supported_coverage_strategies() -> List[CoverageMode]:\n        \"\"\"\n        List of coverage metrics supported by the fuzzer. If it only\n        support a single one, it should be :py:obj:`CoverageMode.AUTO`.\n\n        :return: list of coverage modes\n        \"\"\"\n        raise NotImplementedError()", ""]}
{"filename": "libpastis/types.py", "chunked_list": ["import json\nfrom aenum import Enum, extend_enum\nfrom pathlib import Path\nfrom typing import Union\nimport base64\n\nPathLike = Union[str, Path]\n#: Union of a string or Path object\n\nclass State(Enum):\n    \"\"\"\n    Running type of a fuzzing engine. It\n    can be either running or idle.\n    \"\"\"\n    RUNNING = 0\n    IDLE = 1", "\nclass State(Enum):\n    \"\"\"\n    Running type of a fuzzing engine. It\n    can be either running or idle.\n    \"\"\"\n    RUNNING = 0\n    IDLE = 1\n\n\nclass Platform(Enum):\n    \"\"\"\n    Enum representing the platform.\n    \"\"\"\n    ANY = 0\n    LINUX = 1\n    WINDOWS = 2\n    MACOS = 3\n    ANDROID = 4\n    IOS = 5", "\n\nclass Platform(Enum):\n    \"\"\"\n    Enum representing the platform.\n    \"\"\"\n    ANY = 0\n    LINUX = 1\n    WINDOWS = 2\n    MACOS = 3\n    ANDROID = 4\n    IOS = 5", "\n\nclass SeedType(Enum):\n    \"\"\"\n    Type of an input. They can be plain input,\n    crash input or hanging input.\n    \"\"\"\n    INPUT = 0\n    CRASH = 1\n    HANG = 2", "\n\nclass ExecMode(Enum):\n    \"\"\"\n    Execution mode for fuzzing engine. With ``AUTO``\n    the fuzzer will automatically select, ``SINGLE_EXEC``\n    is the normal fuzzing mode where the process stops\n    at each iteration while ``PERSISTENT`` indicate the\n    fuzzer to run in persistent mode.\n    \"\"\"\n    AUTO = 0\n    SINGLE_EXEC = 1\n    PERSISTENT = 2", "\n\nclass FuzzMode(Enum):\n    \"\"\"\n    Fuzzing mode, indicates the fuzzer whether the target\n    is instrumented or not.\n    \"\"\"\n    AUTO = 0\n    INSTRUMENTED = 1\n    BINARY_ONLY = 2", "\n\nclass CheckMode(Enum):\n    \"\"\"\n    CheckMode is used to indicates a fuzzer how to run depdending on\n    the context. ``CHECK_ALL`` is the normal bug, vulnerability discovery\n    mode. Then ``ALERT_ONLY`` indicates the fuzzer to focus on SAST alerts.\n    Then ``ALERT_ONE`` indicates the fuzzer to focus on a single alert which\n    id should be provided through the configuration file.\n    \"\"\"\n    CHECK_ALL = 0\n    ALERT_ONLY = 1\n    ALERT_ONE = 2", "\nclass CoverageMode(str, Enum):\n    \"\"\"\n    Coverage metrics to use. Some fuzzing engines do support multiple coverage\n    metrics, thus the enum indicates the one to use.\n    \"\"\"\n    AUTO = \"auto\"\n    BLOCK = \"block\"\n    EDGE = \"edge\"\n    PATH = \"path\"\n    STATE = \"state\"\n\n    @classmethod\n    def _missing_(cls, val) -> 'CoverageMode':\n        \"\"\" Method used to dynmically creating an entry \"\"\"\n        enum_name = val.upper().replace(\" \", \"_\")\n        if enum_name in cls.__members__:\n            return cls.__members__[enum_name]\n        return extend_enum(cls, enum_name, val)", "\n\nclass SeedInjectLoc(Enum):\n    \"\"\"\n    Indicates the location where to inject inputs. It can either be\n    on STDIN or ARGV.\n    \"\"\"\n    STDIN = 0\n    ARGV = 1\n", "\n\nclass Arch(Enum):\n    \"\"\"\n    Architecture representation\n    \"\"\"\n    X86 = 0\n    X86_64 = 1\n    ARMV7 = 2\n    AARCH64 = 3", "\n\nclass LogLevel(Enum):\n    \"\"\"\n    Enum representing the Log level, for fuzzers to send message logs\n    to the broker.\n    \"\"\"\n    DEBUG = 10\n    INFO = 20\n    WARNING = 2\n    ERROR = 3\n    CRITICAL = 4", "\n\nclass AlertData(object):\n    \"\"\"\n    AlertData is data message that can be sent from fuzzing agents to\n    the broker to indicates that an alert has been covered or validated.\n    \"\"\"\n    def __init__(self, id: int, covered: bool, validated: bool, seed: bytes, address: int = 0):\n        self.id: int = id\n        #: Id of the alert\n        self.covered: bool = covered\n        #: True if the alert has been covered\n        self.validated: bool = validated\n        #: True if the alert has been validated\n        self.seed: bytes = seed\n        #: Input that reached or validated the alert\n        self.address: int = address\n        #: memory address of the alert\n\n\n    @staticmethod\n    def from_json(data: str) -> 'AlertData':\n        \"\"\"\n        Convert an AlertData in json to an instance.\n\n        :param data: json serialized alert\n        :return: AlertData object\n        \"\"\"\n        data = json.loads(data)\n\n        return AlertData(data['id'], data['covered'], data['validated'], base64.b64decode(data['seed']), data['address'])\n\n    def to_json(self) -> str:\n        \"\"\"\n        Serialize the alert to JSON.\n\n        :return: json serialized alert\n        \"\"\"\n        return json.dumps({'id': self.id,\n                           'covered': self.covered,\n                           'validated': self.validated,\n                           'seed': base64.b64encode(self.seed).decode(),\n                           'address': self.address})", "\n\nclass FuzzingEngineInfo(object):\n    \"\"\"\n    Class to represent a fuzzing engine metadata.\n    It contains its name, version and the Python module\n    where to load the descriptor and configuration object.\n    \"\"\"\n    def __init__(self, name: str, version: str, pymodule: str):\n        self.name: str = name\n        #: Name of the engine\n        self.version = version\n        #: Version of the engine\n        self.pymodule = pymodule\n        #: Name of the python module\n\n    @staticmethod\n    def from_pb(pb) -> 'FuzzingEngineInfo':\n        \"\"\"\n        Parse a protobuf object into a FuzzingEngineInfo object.\n\n        :param pb: protobuf object\n        :return: object\n        \"\"\"\n        return FuzzingEngineInfo(pb.name, pb.version, pb.pymodule)", ""]}
{"filename": "libpastis/agent.py", "chunked_list": ["# built-ins\nimport time\nfrom typing import Callable, Tuple, List, Union\nfrom enum import Enum\nimport logging\nimport threading\nfrom pathlib import Path\nimport socket\n\n# third-party libs", "\n# third-party libs\nimport zmq\nimport psutil\n\n# local imports\nfrom libpastis.proto import InputSeedMsg, StartMsg, StopMsg, HelloMsg, LogMsg, \\\n                            TelemetryMsg, StopCoverageCriteria, DataMsg, EnvelopeMsg\nfrom libpastis.types import SeedType, Arch, FuzzingEngineInfo, PathLike, ExecMode, CheckMode, CoverageMode, SeedInjectLoc, \\\n                            LogLevel, State, AlertData, Platform, FuzzMode", "from libpastis.types import SeedType, Arch, FuzzingEngineInfo, PathLike, ExecMode, CheckMode, CoverageMode, SeedInjectLoc, \\\n                            LogLevel, State, AlertData, Platform, FuzzMode\nfrom libpastis.utils import get_local_architecture, get_local_platform\n\nMessage = Union[InputSeedMsg, StartMsg, StopMsg, HelloMsg, LogMsg, TelemetryMsg, StopCoverageCriteria, DataMsg]\n\n\nclass MessageType(Enum):  # Topics in the ZMQ terminology\n    \"\"\"\n    Enum encoding the type of the message that can be received.\n    \"\"\"\n    HELLO = 'hello_msg'\n    # STATE = 1\n    START = 'start_msg'\n    INPUT_SEED = 'input_msg'\n    TELEMETRY = 'telemetry_msg'\n    LOG = 'log_msg'\n    STOP_COVERAGE_DONE = 'stop_crit_msg'\n    STOP = \"stop_msg\"\n    DATA = \"data_msg\"", "\n\nclass AgentMode(Enum):\n    \"\"\"\n    Internal enum identifying whether the agent is running as a broker\n    or a client.\n    \"\"\"\n    BROKER = 1\n    CLIENT = 2\n", "\n\nclass NetworkAgent(object):\n    \"\"\"\n    Base class for network-based PASTIS agents (both clients and servers)\n    \"\"\"\n    def __init__(self):\n        self.mode = None\n        self.ctx = zmq.Context()\n        self.socket = None\n        self._stop = False\n        self._th = None\n        self._cbs = {x: [] for x in MessageType}\n\n    def register_callback(self, typ: MessageType, callback: Callable) -> None:\n        \"\"\"\n        Register a callback function on a given message type.\n\n        :param typ: type of the message\n        :param callback: Callback function taking the protobuf object as parameter\n        :return: None\n        \"\"\"\n        self._cbs[typ].append(callback)\n\n    def bind(self, port: int = 5555, ip: str = \"*\") -> None:\n        \"\"\"\n        Bind on the given IP and port, to listen incoming messages.\n\n        :param port: listen port\n        :param ip: IP, can be \"*\" to listen on all interfaces\n        :return: None\n        \"\"\"\n        self.socket = self.ctx.socket(zmq.ROUTER)\n        self.socket.RCVTIMEO = 500  # 500 milliseconds\n        self.socket.bind(f\"tcp://{ip}:{port}\")\n        self.mode = AgentMode.BROKER\n\n    def connect(self, remote: str = \"localhost\", port: int = 5555) -> bool:\n        \"\"\"\n        Connect to a remote server on the given ``remote`` IP and ``port``.\n\n        :param remote: IP address or DNS\n        :param port: port to connect to\n        :return: Always true\n        \"\"\"\n        self.socket = self.ctx.socket(zmq.DEALER)\n        self.socket.RCVTIMEO = 500  # 500 milliseconds\n        self.socket.connect(f\"tcp://{remote}:{port}\")\n        self.mode = AgentMode.CLIENT\n        return True\n\n    def start(self) -> None:\n        \"\"\"\n        Start the listening thread.\n        \"\"\"\n        self._th = threading.Thread(name=\"[LIBPASTIS]\", target=self._recv_loop, daemon=True)\n        self._th.start()\n\n    def run(self) -> None:\n        \"\"\"\n        Run receiving loop in a blocking manner.\n        \"\"\"\n        self._recv_loop()\n\n    def stop(self) -> None:\n        \"\"\"\n        Stop the listening thread.\n        \"\"\"\n        self._stop = True\n        if self._th:\n            self._th.join()\n\n    def _recv_loop(self):\n        #flags = 0 if blocking else zmq.DONTWAIT\n        while 1:\n            if self._stop:\n                return\n            try:\n                if self.mode == AgentMode.BROKER:\n                    uid, data = self.socket.recv_multipart()\n                    self.__broker_transfer_to_callback(uid, data)\n                else:\n                    data = self.socket.recv()\n                    self.__client_transfer_to_callback(data)\n            except zmq.error.Again:\n                pass\n\n    def send_to(self, id: bytes, msg: Message, msg_type: MessageType = None) -> None:\n        \"\"\"\n        Send a message to a given client. Only meant to be used when\n        running as a server.\n\n        :param id: bytes id of the client\n        :param msg: protobuf :py:obj:`Message` object\n        :param msg_type: type of the message\n        \"\"\"\n        if self.mode == AgentMode.CLIENT:\n            logging.error(f\"cannot use sento_to() as {AgentMode.CLIENT.name}\")\n            return\n        if msg_type is None:\n            msg_type = self.msg_to_type(msg)\n        final_msg = EnvelopeMsg()\n        getattr(final_msg, msg_type.value).MergeFrom(msg)\n        self.socket.send_multipart([id, final_msg.SerializeToString()])\n\n    def send(self, msg: Message, msg_type: MessageType = None) -> None:\n        \"\"\"\n        Send a message on the socket (thus to the broker). Should\n        only be used as a client (fuzzing agent).\n\n        :param msg: Protobuf message to send\n        :param msg_type: Type of the message\n        \"\"\"\n        if self.mode == AgentMode.BROKER:\n            logging.error(f\"cannot use sento() as {AgentMode.BROKER.name}\")\n            return\n        if msg_type is None:\n            msg_type = self.msg_to_type(msg)\n        final_msg = EnvelopeMsg()\n        getattr(final_msg, msg_type.value).CopyFrom(msg)\n        self.socket.send(final_msg.SerializeToString())\n\n    @staticmethod\n    def msg_to_type(msg: Message) -> MessageType:\n        \"\"\"\n        Get the :py:obj:`MessageType` from a protobuf object.\n\n        :param msg: Protobuf message\n        :return: message type\n        \"\"\"\n        if isinstance(msg, InputSeedMsg):\n            return MessageType.INPUT_SEED\n        elif isinstance(msg, HelloMsg):\n            return MessageType.HELLO\n        elif isinstance(msg, TelemetryMsg):\n            return MessageType.TELEMETRY\n        elif isinstance(msg, LogMsg):\n            return MessageType.LOG\n        elif isinstance(msg, StopMsg):\n            return MessageType.STOP\n        elif isinstance(msg, StopCoverageCriteria):\n            return MessageType.STOP_COVERAGE_DONE\n        elif isinstance(msg, StartMsg):\n            return MessageType.START\n        elif isinstance(msg, DataMsg):\n            return MessageType.DATA\n        else:\n            logging.error(f\"invalid message type: {type(msg)} (cannot find associated topic)\")\n\n    def __broker_transfer_to_callback(self, id: bytes, message: bytes):\n        try:\n            msg = EnvelopeMsg()\n            msg.ParseFromString(message)\n        except:\n            logging.error(f\"can't parse message from {id} (len:{len(message)})\")\n            return\n        message, topic = self._unpack_message(msg)\n        if topic in [MessageType.START]:\n            logging.error(f\"Invalid message of type {topic.name} received\")\n        if not self._cbs[topic]:\n            logging.warning(f\"[broker] message of type {topic.name} (but no callback)\")\n        args = self._message_args(topic, message)\n        for cb in self._cbs[topic]:\n            cb(id, *args)\n\n    def __client_transfer_to_callback(self, message: bytes):\n        msg = EnvelopeMsg()\n        msg.ParseFromString(message)\n        message, topic = self._unpack_message(msg)\n        if topic in [MessageType.HELLO, MessageType.TELEMETRY, MessageType.LOG, MessageType.STOP_COVERAGE_DONE]:\n            logging.error(f\"Invalid message of type {topic.name} received\")\n        if not self._cbs[topic]:\n            logging.warning(f\"[agent] message of type {topic.name} (but no callback)\")\n        args = self._message_args(topic, message)\n        for cb in self._cbs[topic]:\n            cb(*args)\n\n    def _unpack_message(self, message: EnvelopeMsg) -> Tuple[MessageType, Message]:\n        typ = message.WhichOneof('msg')\n        return getattr(message, typ), MessageType(typ)\n\n    def _message_args(self, topic: MessageType, msg: Message):\n        if topic == MessageType.INPUT_SEED:\n            return [SeedType(msg.type), msg.seed]\n        elif topic == MessageType.LOG:\n            return [LogLevel(msg.level), msg.message]\n        elif topic == MessageType.TELEMETRY:\n            return [msg.state, msg.exec_per_sec, msg.total_exec, msg.cycle, msg.timeout, msg.coverage_block,\n                    msg.coverage_edge, msg.coverage_path, msg.last_cov_update]\n        elif topic == MessageType.HELLO:\n            engs = [(FuzzingEngineInfo.from_pb(x)) for x in msg.engines]\n            return [engs, Arch(msg.architecture), msg.cpus, msg.memory, msg.hostname, Platform(msg.platform)]\n        elif topic == MessageType.START:\n            return [msg.binary_filename, msg.binary, FuzzingEngineInfo.from_pb(msg.engine), ExecMode(msg.exec_mode), FuzzMode(msg.fuzz_mode),\n                    CheckMode(msg.check_mode), CoverageMode(msg.coverage_mode), SeedInjectLoc(msg.seed_location),\n                    msg.engine_args, [x for x in msg.program_argv], msg.sast_report]\n        elif topic == MessageType.DATA:\n            return [msg.data]\n        else:  # for stop and store_coverage_done nothing to unpack\n            return []", "\n\nclass BrokerAgent(NetworkAgent):\n\n    def send_seed(self, id: bytes, typ: SeedType, seed: bytes) -> None:\n        \"\"\"\n        Send the given input to the client `id`.\n\n        :param id: raw id of the client\n        :param typ: Type of the input\n        :param seed: Bytes the of input\n        \"\"\"\n        msg = InputSeedMsg()\n        msg.type = typ.value\n        msg.seed = seed\n        self.send_to(id, msg, msg_type=MessageType.INPUT_SEED)\n\n    def send_start(self, id: bytes, name: str, package: PathLike, argv: List[str], exmode: ExecMode, fuzzmode: FuzzMode,\n                   ckmode: CheckMode, covmode: CoverageMode, engine: FuzzingEngineInfo, engine_args: str,\n                   seed_loc: SeedInjectLoc, sast_report: bytes = None) -> None:\n        \"\"\"\n        Send a START message to a fuzzing agent with all the parameters it is meant to run with.\n\n        :param id: raw id of the client\n        :param name: name of the executable file or binary package\n        :param package: filepath of :py:obj:`BinaryPackage` or program executable to send\n        :param argv: argumnets to be provided on command line\n        :param exmode: execution mode\n        :param fuzzmode: fuzzing mode\n        :param ckmode: checking mode\n        :param covmode: coverage metric to use\n        :param engine: descriptor of the fuzzing engine\n        :param engine_args: engine's additional arguments or configuration file\n        :param seed_loc: location where to provide inputs (stdin or argv)\n        :param sast_report: SAST report if applicable\n        \"\"\"\n        msg = StartMsg()\n        if isinstance(package, str):\n            package = Path(package)\n        msg.binary_filename = name\n        msg.binary = package.read_bytes()\n        msg.engine.name = engine.name\n        msg.engine.version = engine.version\n        msg.exec_mode = exmode.value\n        msg.fuzz_mode = fuzzmode.value\n        msg.check_mode = ckmode.value\n        msg.coverage_mode = covmode.value\n        msg.seed_location = seed_loc.value\n        msg.engine_args = engine_args\n        if sast_report is not None:\n            msg.sast_report = sast_report\n        for arg in argv:\n            msg.program_argv.append(arg)\n        self.send_to(id, msg, msg_type=MessageType.START)\n\n    def send_stop(self, id: bytes) -> None:\n        \"\"\"\n        Send a stop message to the client.\n\n        :param id: raw id of the client\n        \"\"\"\n        msg = StopMsg()\n        self.send_to(id, msg, msg_type=MessageType.STOP)\n\n    def register_seed_callback(self, cb: Callable) -> None:\n        self.register_callback(MessageType.INPUT_SEED, cb)\n\n    def register_hello_callback(self, cb: Callable) -> None:\n        self.register_callback(MessageType.HELLO, cb)\n\n    def register_log_callback(self, cb: Callable) -> None:\n        self.register_callback(MessageType.LOG, cb)\n\n    def register_telemetry_callback(self, cb: Callable) -> None:\n        self.register_callback(MessageType.TELEMETRY, cb)\n\n    def register_stop_coverage_callback(self, cb: Callable) -> None:\n        self.register_callback(MessageType.STOP_COVERAGE_DONE, cb)\n\n    def register_data_callback(self, cb: Callable) -> None:\n        self.register_callback(MessageType.DATA, cb)", "\n\nclass ClientAgent(NetworkAgent):\n    \"\"\"\n    Subclass of NetworkAgent to connect to PASTIS as a fuzzing\n    agent. The class provides helper methods to interact with\n    the broker.\n    \"\"\"\n\n    def send_hello(self, engines: List[FuzzingEngineInfo], arch: Arch = None, platform: Platform = None) -> bool:\n        \"\"\"\n        Send the hello message to the broker. `engines` parameter is the list of fuzzing engines\n        that \"we\" as client support. E.g: Pastisd is meant to be an interface for all engines\n        locally, so it will advertise multiple engines.\n\n        :param engines: list of engines, the client is able to launch\n        :param arch: the architecture supported (if None, local one is used)\n        :param platform: the platform supported (if None local one used)\n        \"\"\"\n        msg = HelloMsg()\n        arch = get_local_architecture() if arch is None else arch\n        if arch is None:\n            logging.error(f\"current architecture: {platform.machine()} is not supported\")\n            return False\n        plfm = get_local_platform() if platform is None else platform\n        if plfm is None:\n            logging.error(f\"current platform is not supported\")\n            return False\n        msg.architecture = arch.value\n        msg.cpus = psutil.cpu_count()\n        msg.memory = psutil.virtual_memory().total\n        msg.hostname = socket.gethostname()\n        msg.platform = plfm.value\n        for eng in engines:\n            msg.engines.add(name=eng.name, version=eng.version, pymodule=eng.pymodule)\n        self.send(msg, msg_type=MessageType.HELLO)\n\n    def send_log(self, level: LogLevel, message: str) -> None:\n        \"\"\"\n        Log message to be sent and printed by the broker. All\n        logs received by the broker are logged in a client specific\n        logfile.\n\n        :param level: level of the log message\n        :param message: message as a string\n        \"\"\"\n        self.send(LogMsg(level=level.value, message=message), MessageType.LOG)\n\n    def debug(self, message: str) -> None:\n        \"\"\"\n        Send a debug message to the broker\n\n        :param message: message as a string\n        \"\"\"\n        self.send_log(LogLevel.DEBUG, message)\n\n    def info(self, message: str) -> None:\n        \"\"\"\n        Send an info (level) message to the broker\n\n        :param message: message to send\n        \"\"\"\n        self.send_log(LogLevel.INFO, message)\n\n    def warning(self, message: str) -> None:\n        \"\"\"\n        Send a warning (level) message to the broker.\n\n        :param message: message to send\n        \"\"\"\n        self.send_log(LogLevel.WARNING, message)\n\n    def error(self, message: str) -> None:\n        \"\"\"\n        Send an error (level) message to the broker.\n\n        :param message: message to send\n        \"\"\"\n        self.send_log(LogLevel.ERROR, message)\n\n    def critical(self, message: str) -> None:\n        \"\"\"\n        Send a critical (level) message to the broker.\n\n        :param message: message to send\n        \"\"\"\n        self.send_log(LogLevel.CRITICAL, message)\n\n    def send_telemetry(self,\n                       state: State = None,\n                       exec_per_sec: int = None,\n                       total_exec: int = None,\n                       cycle: int = None,\n                       timeout: int = None,\n                       coverage_block: int = None,\n                       coverage_edge: int = None,\n                       coverage_path: int = None,\n                       last_cov_update: int = None) -> None:\n        \"\"\"\n        Send a telemetry message to the broker. These data could be used on the\n        broker side to plot statistics.\n\n        :param state: current state of the fuzzer\n        :param exec_per_sec: number of execution per seconds\n        :param total_exec: total number of executions\n        :param cycle: number of cycles\n        :param timeout: timeout numbers\n        :param coverage_block: coverage count in blocks\n        :param coverage_edge: coverage count in edges\n        :param coverage_path: coverage count in paths\n        :param last_cov_update: last coverage update\n        \"\"\"\n        msg = TelemetryMsg()\n        msg.cpu_usage = psutil.cpu_percent()\n        msg.mem_usage = psutil.virtual_memory().percent\n        if state:\n            msg.state = state.value\n        if exec_per_sec:\n            msg.exec_per_sec = exec_per_sec\n        if total_exec:\n            msg.total_exec = total_exec\n        if cycle:\n            msg.cycle = cycle\n        if timeout:\n            msg.timeout = timeout\n        if coverage_block:\n            msg.coverage_block = coverage_block\n        if coverage_edge:\n            msg.coverage_edge = coverage_edge\n        if coverage_path:\n            msg.coverage_path = coverage_path\n        if last_cov_update:\n            msg.last_cov_update = last_cov_update\n        self.send(msg, msg_type=MessageType.TELEMETRY)\n\n    def send_stop_coverage_criteria(self) -> None:\n        \"\"\"\n        Send a message to the broker indicating, the program has been fully\n        covered in accordance to the coverage criteria (metric).\n        \"\"\"\n        self.send(StopCoverageCriteria(), MessageType.STOP_COVERAGE_DONE)\n\n    def send_seed(self, typ: SeedType, seed: bytes) -> None:\n        \"\"\"\n        Send an input seed to the broker. The ``typ`` indicates\n        the type of the seed, namely, input, crash or hang.\n\n        :param typ: type of the input\n        :param seed: bytes of the input\n        \"\"\"\n        msg = InputSeedMsg()\n        msg.type = typ.value\n        msg.seed = seed\n        self.send(msg, msg_type=MessageType.INPUT_SEED)\n\n    def send_alert_data(self, alert_data: AlertData) -> None:\n        \"\"\"\n        Send information related to the coverage or validation of a specific SAST\n        alert.\n\n        :param alert_data: alert object\n        \"\"\"\n        msg = DataMsg()\n        msg.data = alert_data.to_json()\n        self.send(msg, msg_type=MessageType.DATA)\n\n    def register_start_callback(self, cb: Callable) -> None:\n        \"\"\"\n        Register a callback that will be called when a start message will\n        be received. The callback should take 11 parameters.\n\n        :param cb: callback function.\n        \"\"\"\n        self.register_callback(MessageType.START, cb)\n\n    def register_stop_callback(self, cb: Callable) -> None:\n        \"\"\"\n        Register a callback called when the broker send a STOP\n        message. The fuzzing has to stop running and sending data.\n\n        :param cb: callback function\n        \"\"\"\n        self.register_callback(MessageType.STOP, cb)\n\n    def register_seed_callback(self, cb: Callable) -> None:\n        \"\"\"\n        Register a callback called when an input seed is received from the\n        broker. The callback function take 2 parameters seed type and content.\n\n        :param cb: callback function\n        \"\"\"\n        self.register_callback(MessageType.INPUT_SEED, cb)\n\n    def register_data_callback(self, cb: Callable) -> None:\n        \"\"\"\n        Register callback called when data is received. At the moment\n        data are necessarily AlertData messages.\n\n        :param cb: callback function\n        \"\"\"\n        self.register_callback(MessageType.DATA, cb)", "\n\n\nclass FileAgent(ClientAgent):\n    \"\"\"\n    Mock agent that will mimick all APIs function of a network agent\n    but which will never receive any incoming messages. All messages\n    sent are logged to a file\n    \"\"\"\n\n    def __init__(self, level=logging.INFO, log_file: str = None):\n        super(FileAgent, self).__init__()\n        del self.ctx    # Remove network related attributes\n        del self.socket\n        self.logger = logging.getLogger('FileAgent')\n        self.logger.parent = None  # Remove root handler to make sur it is not printed on output\n\n        # create file handler\n        if log_file is not None:\n            ch = logging.FileHandler(log_file)\n            ch.setLevel(level)\n            ch.setFormatter(logging.Formatter('%(asctime)s - [%(name)s] [%(levelname)s]: %(message)s'))\n            self.logger.addHandler(ch)\n\n    def bind(self, port: int = 5555, ip: str = \"*\"):\n        raise RuntimeError(\"FileAgent is not meant to be used as broker\")\n\n    def connect(self, remote: str = \"localhost\", port: int = 5555) -> bool:\n        return True  # Do nothing\n\n    def _recv_loop(self):\n        while 1:\n            if self._stop:\n                return\n            time.sleep(0.05)\n\n    def send_to(self, id: bytes, msg: Message, msg_type: MessageType = None):\n        raise RuntimeError(\"FileAgent is not meant to be used as broker\")\n\n    def send(self, msg: Message, msg_type: MessageType = None):\n        if self.mode == AgentMode.BROKER:\n            logging.error(f\"cannot use sento() as {AgentMode.BROKER.name}\")\n            return\n        if msg_type is None:\n            msg_type = self.msg_to_type(msg)\n\n        if isinstance(msg, InputSeedMsg):\n            msg = f\"{SeedType(msg.type).name}: {msg.seed[:20]}..\"\n        elif isinstance(msg, HelloMsg):\n            msg = f\"{msg.hostname}: {Platform(msg.platform)}({Arch(msg.architecture)}) CPU:{msg.cpus} engines:{[x.name for x in msg.engines]}\"\n        elif isinstance(msg, TelemetryMsg):\n            msg = f\"{State(msg.state).name} exec/s: {msg.exec_per_sec} total:{msg.total_exec}\"\n        elif isinstance(msg, LogMsg):\n            msg = f\"{LogLevel(msg.level).name}: {msg.message}\"\n        elif isinstance(msg, DataMsg):\n            msg = f\"Data: {msg.data}\"\n        elif isinstance(msg, StopCoverageCriteria):\n            msg = \"\"\n        else:\n            logging.error(f\"invalid message type: {type(msg)} as client\")\n            return\n\n        self.logger.info(f\"send {msg_type.name} {msg}\")", ""]}
{"filename": "libpastis/__init__.py", "chunked_list": ["from .agent import FileAgent, BrokerAgent, ClientAgent\nfrom .enginedesc import EngineConfiguration, FuzzingEngineDescriptor\nfrom .package import BinaryPackage\nfrom .sast import SASTAlert, SASTReport\n\n__version__ = \"1.0.0\"\n"]}
{"filename": "libpastis/utils.py", "chunked_list": ["# builtin imports\nimport platform\nfrom typing import Optional\n\n# Local imports\nfrom .types import Arch, Platform\n\n\ndef get_local_architecture() -> Optional[Arch]:\n    mapping = {\"i386\": Arch.X86, \"x86_64\": Arch.X86_64, \"armv7l\": Arch.ARMV7, \"aarch64\": Arch.AARCH64}\n    # FIXME: Make sure platform.machine() returns this string for architectures\n    return mapping.get(platform.machine())", "def get_local_architecture() -> Optional[Arch]:\n    mapping = {\"i386\": Arch.X86, \"x86_64\": Arch.X86_64, \"armv7l\": Arch.ARMV7, \"aarch64\": Arch.AARCH64}\n    # FIXME: Make sure platform.machine() returns this string for architectures\n    return mapping.get(platform.machine())\n\n\ndef get_local_platform() -> Optional[Platform]:\n    mapping = {\"Linux\": Platform.LINUX, \"Windows\": Platform.WINDOWS, \"MacOS\": Platform.MACOS, \"iOS\": Platform.IOS}\n    # FIXME: Make sure platform.system() returns this string for other platforms\n    return mapping.get(platform.system())", ""]}
{"filename": "libpastis/package.py", "chunked_list": ["# built-in imports\nfrom pathlib import Path\nimport zipfile\nimport tempfile\nimport logging\nfrom typing import Tuple, Optional, Union\n\n# third-party imports\nimport lief\nimport magic", "import lief\nimport magic\nimport shutil\nimport stat\n\n# local imports\nfrom libpastis.types import Arch, Platform\n\n\nclass BinaryPackage(object):\n    \"\"\"\n    Binary Package representing a given target to fuzz along with its shared\n    libraries and additional files required (cmplog, dictionnary etc.).\n    This object is received by fuzzing agents as part of the START message.\n    \"\"\"\n\n    EXTENSION_BLACKLIST = ['.gt', '.Quokka', '.quokka', '.cmplog']\n    #: specific extensions that will be ignored for the `other_files`\n\n    def __init__(self, main_binary: Path):\n        \"\"\"\n        :param main_binary: main executable file path\n        \"\"\"\n        self._main_bin = Path(main_binary)\n        self._quokka = None\n        self._callgraph = None\n        self._cmplog = None\n        self._dictionary = None\n        self.other_files = []\n        #: list of additional files contained in this package\n\n        self._package_file = None\n        self._arch = None\n        self._platform = None\n\n    @property\n    def executable_path(self) -> Path:\n        \"\"\"\n        Path to the main executable file to fuzz.\n\n        :return: filepath\n        \"\"\"\n        return self._main_bin\n\n    @property\n    def name(self) -> str:\n        \"\"\"\n        Name of the executable file\n\n        :return: name as a string\n        \"\"\"\n        return self._main_bin.name\n\n    @property\n    def quokka(self) -> Optional[Path]:\n        \"\"\"\n        Path to the quokka file if provided.\n\n        :return: path of the quokka file\n        \"\"\"\n        return self._quokka\n\n    @property\n    def callgraph(self) -> Optional[Path]:\n        \"\"\"\n        Path to the callgraph file if provided.\n\n        :return: path of the quokka file\n        \"\"\"\n        return self._callgraph\n\n    @property\n    def cmplog(self) -> Optional[Path]:\n        \"\"\"\n        Path to the complog executable file if provided.\n\n        :return: path to the complog file\n        \"\"\"\n        return self._cmplog\n\n    @property\n    def dictionary(self) -> Optional[Path]:\n        \"\"\"\n        Path the to dictionnary file if provided.\n\n        :return: path to the dictionnary file\n        \"\"\"\n        return self._dictionary\n\n    def is_cmplog(self) -> bool:\n        \"\"\"\n        Check if the package contains a cmplog file.\n\n        :return: True if contains cmplog\n        \"\"\"\n        return self._cmplog is not None\n\n    def is_quokka(self) -> bool:\n        \"\"\"\n        Check if the package contains a quokka file.\n\n        :return: True if contains a quokka file\n        \"\"\"\n        return self._quokka is not None\n\n    def is_dictionary(self) -> bool:\n        \"\"\"\n        Check if the package contains a dictionnary.\n\n        :return: True if contains a dictionnary\n        \"\"\"\n        return self._dictionary is not None\n\n    def is_standalone(self) -> bool:\n        \"\"\"\n        Indicates that this BinaryPackage only contains the program under test and no\n        additional files such as a Quokka database or a cmplog instrumented binary.\n        This is used in pastis-broker when sending the 'start' command to agents.\n        \"\"\"\n        return not (self.is_quokka() or\n                    self.is_cmplog() or\n                    self.is_dictionary() or\n                    bool(self.other_files))\n\n    @property\n    def arch(self) -> Arch:\n        \"\"\"\n        Return the architecture of the binary package (main executable target).\n\n        :return: architecture\n        \"\"\"\n        return self._arch\n\n    @property\n    def platform(self) -> Platform:\n        \"\"\"\n        Return the platform of the binary package (main exectuable target).\n\n        :return: platform\n        \"\"\"\n        return self._platform\n\n    @staticmethod\n    def auto(exe_file: Union[Path, str]) -> Optional['BinaryPackage']:\n        \"\"\"\n        Take a file and try creating a BinaryPackage with it. The `exe_file` is\n        the main executable file. From that the function will look for quokka,\n        cmplog, dictionary files (in the same directory).\n\n        :param exe_file: main target executable file\n        :return: a binary package if `exe_file` if applicable\n        \"\"\"\n        bin_f = Path(exe_file)\n\n        # Exclude file if have one of the\n        if bin_f.suffix in BinaryPackage.EXTENSION_BLACKLIST:\n            return None\n\n        # If do not exists\n        if not bin_f.exists():\n            return None\n\n        # Make sure its an executable\n        data = BinaryPackage._read_binary_infos(bin_f)\n        if not data:\n            return None\n\n        bin_f.chmod(stat.S_IRWXU)  # make sure the binary is executable\n\n        p = BinaryPackage(bin_f)\n        p._platform, p._arch = data\n\n        # Search for a Quokka file\n        qfile1, qfile2 = Path(str(bin_f)+\".Quokka\"), Path(str(bin_f)+\".quokka\")\n        if qfile1.exists():\n            p._quokka = qfile1\n        elif qfile2.exists():\n            p._quokka = qfile2\n\n        # Search for a graph file (containing callgraph)\n        cfile = Path(str(bin_f)+\".gt\")\n        if cfile.exists():\n            p._callgraph = cfile\n\n        # Search for a cmplog file if any\n        cfile = Path(str(bin_f)+\".cmplog\")\n        if cfile.exists():\n            p._cmplog = cfile\n            cfile.chmod(stat.S_IRWXU)  # make sure the cmplog binary is executable\n\n        # Search for a dictionary file if any\n        cfile = Path(str(bin_f)+\".dict\")\n        if cfile.exists():\n            p._dictionary = cfile\n\n        return p\n\n    @staticmethod\n    def auto_directory(exe_file: Union[str, Path]) -> Optional['BinaryPackage']:\n        \"\"\"\n        Create a BinaryPackage with all files it can find in the given\n        directory. The difference with :py:meth:`BinaryPackage.auto` is\n        that all additional files in the directory will be added to the\n        package.\n\n        :param exe_file: main executable in the directory\n        :return: BinaryPackage if applicable\n        \"\"\"\n        bin_f = Path(exe_file)\n\n        p = BinaryPackage.auto(bin_f)\n\n        if p is None:\n            return None\n\n        for file in bin_f.parent.iterdir():\n            if file not in [p._main_bin, p._callgraph, p._quokka, p._cmplog, p._dictionary]:\n                p.other_files.append(file)\n\n    def make_package(self) -> Path:\n        \"\"\"\n        Pack the BinaryPackage in a zip file.\n\n        :return: Path to a .zip file containing the whole package\n        \"\"\"\n        if self._package_file is not None:\n            if self._package_file.exists():\n                return self._package_file\n        # Recreate a package\n        fname = tempfile.mktemp(suffix=\".zip\")\n        zip = zipfile.ZipFile(fname, \"w\")\n        zip.write(self._main_bin, self._main_bin.name)\n        if self._quokka:\n            zip.write(self._quokka, self._quokka.name)\n        if self._callgraph:\n            zip.write(self._callgraph, self._callgraph.name)\n        if self._cmplog:\n            zip.write(self._cmplog, self._cmplog.name)\n        if self._dictionary:\n            zip.write(self._dictionary, self._dictionary.name)\n        for file in self.other_files:\n            zip.write(file)\n        zip.close()\n        return Path(fname)\n\n    @staticmethod\n    def _read_binary_infos(file: Path) -> Optional[Tuple[Platform, Arch]]:\n        p = lief.parse(str(file))\n        if not p:\n            return None\n        if not isinstance(p, lief.ELF.Binary):\n            logging.warning(f\"binary {file} not supported (only ELF at the moment)\")\n            return None\n\n        # Determine the architecture of the binary\n        mapping = {lief.ELF.ARCH.x86_64: Arch.X86_64,\n                   lief.ELF.ARCH.i386: Arch.X86,\n                   lief.ELF.ARCH.ARM: Arch.ARMV7,\n                   lief.ELF.ARCH.AARCH64: Arch.AARCH64}\n        arch = mapping.get(p.header.machine_type)\n\n        # Determine the platform from its format\n        mapping_elf = {lief.EXE_FORMATS.ELF: Platform.LINUX,\n                       lief.EXE_FORMATS.PE: Platform.WINDOWS,\n                       lief.EXE_FORMATS.MACHO: Platform.MACOS}\n        # FIXME: differentiating between ELF (Linux, Android ..) and MACHO (MacOS, iOS..)\n        fmt = mapping_elf.get(p.format)\n\n        if arch and fmt:\n            return fmt, arch\n        else:\n            return None\n\n    @staticmethod\n    def from_binary(name: str, binary: bytes, extract_dir: Path) -> 'BinaryPackage':\n        \"\"\"\n        Convert the binary blob received as a BinaryPackage object. If its an archive,\n        extract it and return the list of files. Files are extracted in /tmp. If\n        directly an executable save it to a file and return its path. Also ensure\n        the executable file is indeed executable in terms of permissions.\n\n        :param name: name of executable, or executable name in archive\n        :param binary: content\n        :param extract_dir: Path: directory where files should be extracted\n        :return: list of file paths\n\n        :raise FileNotFoundError: if the mime type of the binary is not recognized\n        \"\"\"\n        mime = magic.from_buffer(binary, mime=True)\n\n        if mime in ['application/x-tar', 'application/zip']:\n            map = {'application/x-tar': '.tar.gz', 'application/zip': '.zip'}\n            tmp_file = Path(tempfile.mktemp(suffix=map[mime]))\n            tmp_file.write_bytes(binary)          # write the archive in a file\n\n            # Extract the archive in the right directory\n            shutil.unpack_archive(tmp_file.as_posix(), extract_dir)  # unpack it in dst directory\n            # Create the package object\n            pkg = BinaryPackage.auto(Path(extract_dir) / name)\n            if pkg is None:\n                raise ValueError(f\"Cannot create a BinaryPackage with {name}\")\n            for file in extract_dir.iterdir():\n                if file not in [pkg.executable_path, pkg.callgraph, pkg.quokka, pkg.dictionary]:\n                    pkg.other_files.append(file)\n            return pkg\n        elif mime in ['application/x-pie-executable', 'application/x-dosexec', 'application/x-mach-binary', 'application/x-executable', 'application/x-sharedlib']:\n            program_path = extract_dir / name\n            program_path.write_bytes(binary)\n            program_path.chmod(stat.S_IRWXU)  # set the binary executable\n            return BinaryPackage(program_path)\n        else:\n            raise FileNotFoundError(f\"mimetype not recognized {mime}\")", "\nclass BinaryPackage(object):\n    \"\"\"\n    Binary Package representing a given target to fuzz along with its shared\n    libraries and additional files required (cmplog, dictionnary etc.).\n    This object is received by fuzzing agents as part of the START message.\n    \"\"\"\n\n    EXTENSION_BLACKLIST = ['.gt', '.Quokka', '.quokka', '.cmplog']\n    #: specific extensions that will be ignored for the `other_files`\n\n    def __init__(self, main_binary: Path):\n        \"\"\"\n        :param main_binary: main executable file path\n        \"\"\"\n        self._main_bin = Path(main_binary)\n        self._quokka = None\n        self._callgraph = None\n        self._cmplog = None\n        self._dictionary = None\n        self.other_files = []\n        #: list of additional files contained in this package\n\n        self._package_file = None\n        self._arch = None\n        self._platform = None\n\n    @property\n    def executable_path(self) -> Path:\n        \"\"\"\n        Path to the main executable file to fuzz.\n\n        :return: filepath\n        \"\"\"\n        return self._main_bin\n\n    @property\n    def name(self) -> str:\n        \"\"\"\n        Name of the executable file\n\n        :return: name as a string\n        \"\"\"\n        return self._main_bin.name\n\n    @property\n    def quokka(self) -> Optional[Path]:\n        \"\"\"\n        Path to the quokka file if provided.\n\n        :return: path of the quokka file\n        \"\"\"\n        return self._quokka\n\n    @property\n    def callgraph(self) -> Optional[Path]:\n        \"\"\"\n        Path to the callgraph file if provided.\n\n        :return: path of the quokka file\n        \"\"\"\n        return self._callgraph\n\n    @property\n    def cmplog(self) -> Optional[Path]:\n        \"\"\"\n        Path to the complog executable file if provided.\n\n        :return: path to the complog file\n        \"\"\"\n        return self._cmplog\n\n    @property\n    def dictionary(self) -> Optional[Path]:\n        \"\"\"\n        Path the to dictionnary file if provided.\n\n        :return: path to the dictionnary file\n        \"\"\"\n        return self._dictionary\n\n    def is_cmplog(self) -> bool:\n        \"\"\"\n        Check if the package contains a cmplog file.\n\n        :return: True if contains cmplog\n        \"\"\"\n        return self._cmplog is not None\n\n    def is_quokka(self) -> bool:\n        \"\"\"\n        Check if the package contains a quokka file.\n\n        :return: True if contains a quokka file\n        \"\"\"\n        return self._quokka is not None\n\n    def is_dictionary(self) -> bool:\n        \"\"\"\n        Check if the package contains a dictionnary.\n\n        :return: True if contains a dictionnary\n        \"\"\"\n        return self._dictionary is not None\n\n    def is_standalone(self) -> bool:\n        \"\"\"\n        Indicates that this BinaryPackage only contains the program under test and no\n        additional files such as a Quokka database or a cmplog instrumented binary.\n        This is used in pastis-broker when sending the 'start' command to agents.\n        \"\"\"\n        return not (self.is_quokka() or\n                    self.is_cmplog() or\n                    self.is_dictionary() or\n                    bool(self.other_files))\n\n    @property\n    def arch(self) -> Arch:\n        \"\"\"\n        Return the architecture of the binary package (main executable target).\n\n        :return: architecture\n        \"\"\"\n        return self._arch\n\n    @property\n    def platform(self) -> Platform:\n        \"\"\"\n        Return the platform of the binary package (main exectuable target).\n\n        :return: platform\n        \"\"\"\n        return self._platform\n\n    @staticmethod\n    def auto(exe_file: Union[Path, str]) -> Optional['BinaryPackage']:\n        \"\"\"\n        Take a file and try creating a BinaryPackage with it. The `exe_file` is\n        the main executable file. From that the function will look for quokka,\n        cmplog, dictionary files (in the same directory).\n\n        :param exe_file: main target executable file\n        :return: a binary package if `exe_file` if applicable\n        \"\"\"\n        bin_f = Path(exe_file)\n\n        # Exclude file if have one of the\n        if bin_f.suffix in BinaryPackage.EXTENSION_BLACKLIST:\n            return None\n\n        # If do not exists\n        if not bin_f.exists():\n            return None\n\n        # Make sure its an executable\n        data = BinaryPackage._read_binary_infos(bin_f)\n        if not data:\n            return None\n\n        bin_f.chmod(stat.S_IRWXU)  # make sure the binary is executable\n\n        p = BinaryPackage(bin_f)\n        p._platform, p._arch = data\n\n        # Search for a Quokka file\n        qfile1, qfile2 = Path(str(bin_f)+\".Quokka\"), Path(str(bin_f)+\".quokka\")\n        if qfile1.exists():\n            p._quokka = qfile1\n        elif qfile2.exists():\n            p._quokka = qfile2\n\n        # Search for a graph file (containing callgraph)\n        cfile = Path(str(bin_f)+\".gt\")\n        if cfile.exists():\n            p._callgraph = cfile\n\n        # Search for a cmplog file if any\n        cfile = Path(str(bin_f)+\".cmplog\")\n        if cfile.exists():\n            p._cmplog = cfile\n            cfile.chmod(stat.S_IRWXU)  # make sure the cmplog binary is executable\n\n        # Search for a dictionary file if any\n        cfile = Path(str(bin_f)+\".dict\")\n        if cfile.exists():\n            p._dictionary = cfile\n\n        return p\n\n    @staticmethod\n    def auto_directory(exe_file: Union[str, Path]) -> Optional['BinaryPackage']:\n        \"\"\"\n        Create a BinaryPackage with all files it can find in the given\n        directory. The difference with :py:meth:`BinaryPackage.auto` is\n        that all additional files in the directory will be added to the\n        package.\n\n        :param exe_file: main executable in the directory\n        :return: BinaryPackage if applicable\n        \"\"\"\n        bin_f = Path(exe_file)\n\n        p = BinaryPackage.auto(bin_f)\n\n        if p is None:\n            return None\n\n        for file in bin_f.parent.iterdir():\n            if file not in [p._main_bin, p._callgraph, p._quokka, p._cmplog, p._dictionary]:\n                p.other_files.append(file)\n\n    def make_package(self) -> Path:\n        \"\"\"\n        Pack the BinaryPackage in a zip file.\n\n        :return: Path to a .zip file containing the whole package\n        \"\"\"\n        if self._package_file is not None:\n            if self._package_file.exists():\n                return self._package_file\n        # Recreate a package\n        fname = tempfile.mktemp(suffix=\".zip\")\n        zip = zipfile.ZipFile(fname, \"w\")\n        zip.write(self._main_bin, self._main_bin.name)\n        if self._quokka:\n            zip.write(self._quokka, self._quokka.name)\n        if self._callgraph:\n            zip.write(self._callgraph, self._callgraph.name)\n        if self._cmplog:\n            zip.write(self._cmplog, self._cmplog.name)\n        if self._dictionary:\n            zip.write(self._dictionary, self._dictionary.name)\n        for file in self.other_files:\n            zip.write(file)\n        zip.close()\n        return Path(fname)\n\n    @staticmethod\n    def _read_binary_infos(file: Path) -> Optional[Tuple[Platform, Arch]]:\n        p = lief.parse(str(file))\n        if not p:\n            return None\n        if not isinstance(p, lief.ELF.Binary):\n            logging.warning(f\"binary {file} not supported (only ELF at the moment)\")\n            return None\n\n        # Determine the architecture of the binary\n        mapping = {lief.ELF.ARCH.x86_64: Arch.X86_64,\n                   lief.ELF.ARCH.i386: Arch.X86,\n                   lief.ELF.ARCH.ARM: Arch.ARMV7,\n                   lief.ELF.ARCH.AARCH64: Arch.AARCH64}\n        arch = mapping.get(p.header.machine_type)\n\n        # Determine the platform from its format\n        mapping_elf = {lief.EXE_FORMATS.ELF: Platform.LINUX,\n                       lief.EXE_FORMATS.PE: Platform.WINDOWS,\n                       lief.EXE_FORMATS.MACHO: Platform.MACOS}\n        # FIXME: differentiating between ELF (Linux, Android ..) and MACHO (MacOS, iOS..)\n        fmt = mapping_elf.get(p.format)\n\n        if arch and fmt:\n            return fmt, arch\n        else:\n            return None\n\n    @staticmethod\n    def from_binary(name: str, binary: bytes, extract_dir: Path) -> 'BinaryPackage':\n        \"\"\"\n        Convert the binary blob received as a BinaryPackage object. If its an archive,\n        extract it and return the list of files. Files are extracted in /tmp. If\n        directly an executable save it to a file and return its path. Also ensure\n        the executable file is indeed executable in terms of permissions.\n\n        :param name: name of executable, or executable name in archive\n        :param binary: content\n        :param extract_dir: Path: directory where files should be extracted\n        :return: list of file paths\n\n        :raise FileNotFoundError: if the mime type of the binary is not recognized\n        \"\"\"\n        mime = magic.from_buffer(binary, mime=True)\n\n        if mime in ['application/x-tar', 'application/zip']:\n            map = {'application/x-tar': '.tar.gz', 'application/zip': '.zip'}\n            tmp_file = Path(tempfile.mktemp(suffix=map[mime]))\n            tmp_file.write_bytes(binary)          # write the archive in a file\n\n            # Extract the archive in the right directory\n            shutil.unpack_archive(tmp_file.as_posix(), extract_dir)  # unpack it in dst directory\n            # Create the package object\n            pkg = BinaryPackage.auto(Path(extract_dir) / name)\n            if pkg is None:\n                raise ValueError(f\"Cannot create a BinaryPackage with {name}\")\n            for file in extract_dir.iterdir():\n                if file not in [pkg.executable_path, pkg.callgraph, pkg.quokka, pkg.dictionary]:\n                    pkg.other_files.append(file)\n            return pkg\n        elif mime in ['application/x-pie-executable', 'application/x-dosexec', 'application/x-mach-binary', 'application/x-executable', 'application/x-sharedlib']:\n            program_path = extract_dir / name\n            program_path.write_bytes(binary)\n            program_path.chmod(stat.S_IRWXU)  # set the binary executable\n            return BinaryPackage(program_path)\n        else:\n            raise FileNotFoundError(f\"mimetype not recognized {mime}\")", ""]}
{"filename": "libpastis/sast.py", "chunked_list": ["# built-in imports\nimport json\nimport csv\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Union\n\n\nclass SASTAlert:\n    \"\"\"\n    Class representing an alert in a somewhat abstract SAST tool. Its\n    used to perform alert driven testing.\n    \"\"\"\n    def __init__(self):\n        # Static alert data\n        self.id: int = -1\n        #: Unique ID of the alert\n        self.type: str = \"\"\n        #: Type of the alert BoF, UaF (in the convention of the SAST)\n        self.params: list = []\n        #: Additional parameters of the alert (list)\n        self.taxonomy: str = \"\"\n        #: Taxonomy of the alert (e.g: CWE, CVE, MISRA checker, ..)\n        self.severity: str = \"\"\n        #: Severity of the alert (e.g: Review, Error, Critical ..)\n        self.file: str = \"\"\n        #: Source file impacted\n        self.line: int = -1\n        #: line of code (in the file)\n        self.function: str = \"\"\n        #: Function impacted\n        self.raw_line: str = \"\"\n        #: Raw alert extract taken from the report (in its own format)\n\n        # Analysis results\n        self.covered = False\n        #: Coverage: True if the alert has been covered (path leading there)\n        self.validated = False\n        #: Validation: True if the alert has been validated (as a true positive by a checker)\n        self.uncoverable = False\n        #: Reachability: True if the alert cannot be reached by any paths\n\n    @staticmethod\n    def from_json(data: dict) -> 'SASTAlert':\n        \"\"\"\n        Create a SASTAlert object from the JSON data provided.\n\n        :param data: JSON data of the alert\n        :return: SASTAlert instance, initialized with the JSON\n        \"\"\"\n        alert = SASTAlert()\n        for name in [\"id\", \"type\", \"params\", \"taxonomy\", \"severity\", \"file\", \"line\", \"function\", \"raw_line\",\n                     \"covered\", \"validated\", \"uncoverable\"]:\n            val = data.get(name)\n            if val:\n                setattr(alert, name, val)\n        return alert\n\n\n    def to_dict(self) -> dict:\n        \"\"\"\n        Export the alert attribute to a valid JSON dictionnary\n        that can be written to file.\n\n        :return: JSON dict of the alert serialized\n        \"\"\"\n        return {x: getattr(self, x) for x in [\"id\", \"type\", \"params\", \"taxonomy\", \"severity\", \"file\", \"line\", \"function\",\n                                              \"raw_line\", \"covered\", \"validated\", \"uncoverable\"]}\n\n    def __repr__(self):\n        return f\"<Alert id:{self.id}: {self.type} {self.function}:{self.line} ({Path(self.file).name})>\"", "\n\n\nclass SASTReport:\n    \"\"\"\n    SAST report. Manages a list of SAST alerts taken from a report.\n    \"\"\"\n\n    def __init__(self):\n        self.alerts: Dict[int, SASTAlert] = {}\n        #: Dictionnary of alerts indexed by their ID\n\n\n    def iter_alerts(self) -> List[SASTAlert]:\n        \"\"\"\n        Iterate all the alerts of the report.\n        :return: list of alerts\n        \"\"\"\n        return list(self.alerts.values())\n\n\n    def all_alerts_validated(self) -> bool:\n        \"\"\"\n        Checks if all alerts have been validated (and thus covered)\n\n        :return: True if all alerts are covered and vulns validated\n        \"\"\"\n        for alert in self.iter_alerts():\n            if not alert.covered:\n                return False\n        return True\n\n\n    def add_alert(self, alert: SASTAlert) -> None:\n        \"\"\"\n        Add an alert in the report. This function is solely\n        meant to be used by the report parser\n\n        :param alert: Alert object to add in the report\n        \"\"\"\n        self.alerts[alert.id] = alert\n\n\n    @staticmethod\n    def from_file(file: Union[str, Path]) -> 'SASTReport':\n        \"\"\"\n        Parse the given file into a SAST report object.\n\n        :param file: path to report\n        :return: SASTReport object\n        \"\"\"\n        data = Path(file).read_bytes()\n        return SASTReport.from_json(data)\n\n\n    @staticmethod\n    def from_json(data: Union[str, bytes]) -> 'SASTReport':\n        \"\"\"\n        Parse the given string into a SAST report object.\n\n        :param data: serialized report in JSON\n        :return: SASTReport object\n        \"\"\"\n        data = json.loads(data)\n        report = SASTReport()\n        for it in data:\n            a = SASTAlert.from_json(it)\n            report.add_alert(a)\n        return report\n\n\n    def to_json(self) -> str:\n        \"\"\"\n        Export the current state of the alerts within a JSON dictionnary.\n\n        :return: JSON serialized report\n        \"\"\"\n        return json.dumps([x.to_dict() for x in self.alerts.values()], indent=2)\n\n\n    def write(self, out_file) -> None:\n        \"\"\"\n        Export the current state of the alerts within a JSON dictionary.\n\n        :param out_file: Output file path\n        \"\"\"\n        with open(out_file, \"w\") as f:\n            f.write(self.to_json())\n\n\n    def get_stats(self) -> Tuple[int, int, int]:\n        \"\"\"\n        Get stats about the report. The results is a triple\n        with the number of alerts covered, validated and total.\n\n        :return: triple of covered, validated, totoal number of alerts\n        \"\"\"\n        covered = 0\n        validated = 0\n        total = 0\n        for alert in self.alerts.values():\n            covered += int(alert.covered)\n            validated += int(alert.validated)\n            total += 1\n        return covered, validated, total\n\n    def write_csv(self, file: Path) -> None:\n        \"\"\"\n        Write the report as a csv into the given file.\n\n        :param file: CSV file to write\n        \"\"\"\n        with open(file, 'w', newline='') as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=['id', 'type', 'covered', 'validated'])\n            writer.writeheader()\n            for a in self.iter_alerts():\n                writer.writerow({'id': a.id,\n                                 'type': a.type,\n                                 'covered': a.covered,\n                                 'validated': a.validated})", ""]}
{"filename": "libpastis/proto/__init__.py", "chunked_list": ["from .message_pb2 import *\n\nSeedType = InputSeedMsg.SeedType\nArch = HelloMsg.Arch\n"]}
{"filename": "libpastis/proto/message_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: message.proto\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf.internal import builder as _builder\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n", "# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\rmessage.proto\\x12\\tlibpastis\\\"@\\n\\rFuzzingEngine\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07version\\x18\\x02 \\x01(\\t\\x12\\x10\\n\\x08pymodule\\x18\\x03 \\x01(\\t\\\"x\\n\\x0cInputSeedMsg\\x12\\x0c\\n\\x04seed\\x18\\x01 \\x01(\\x0c\\x12.\\n\\x04type\\x18\\x02 \\x01(\\x0e\\x32 .libpastis.InputSeedMsg.SeedType\\\"*\\n\\x08SeedType\\x12\\t\\n\\x05INPUT\\x10\\x00\\x12\\t\\n\\x05\\x43RASH\\x10\\x01\\x12\\x08\\n\\x04HANG\\x10\\x02\\\"\\x17\\n\\x07\\x44\\x61taMsg\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\t\\\"\\xde\\x04\\n\\x08StartMsg\\x12\\x17\\n\\x0f\\x62inary_filename\\x18\\x01 \\x01(\\t\\x12\\x0e\\n\\x06\\x62inary\\x18\\x02 \\x01(\\x0c\\x12\\x13\\n\\x0bsast_report\\x18\\x03 \\x01(\\x0c\\x12(\\n\\x06\\x65ngine\\x18\\x04 \\x01(\\x0b\\x32\\x18.libpastis.FuzzingEngine\\x12/\\n\\texec_mode\\x18\\x05 \\x01(\\x0e\\x32\\x1c.libpastis.StartMsg.ExecMode\\x12/\\n\\tfuzz_mode\\x18\\x06 \\x01(\\x0e\\x32\\x1c.libpastis.StartMsg.FuzzMode\\x12\\x31\\n\\ncheck_mode\\x18\\x07 \\x01(\\x0e\\x32\\x1d.libpastis.StartMsg.CheckMode\\x12\\x15\\n\\rcoverage_mode\\x18\\x08 \\x01(\\t\\x12\\x38\\n\\rseed_location\\x18\\t \\x01(\\x0e\\x32!.libpastis.StartMsg.SeedInjectLoc\\x12\\x13\\n\\x0b\\x65ngine_args\\x18\\n \\x01(\\t\\x12\\x14\\n\\x0cprogram_argv\\x18\\x0b \\x03(\\t\\\":\\n\\x08\\x45xecMode\\x12\\r\\n\\tAUTO_EXEC\\x10\\x00\\x12\\x0f\\n\\x0bSINGLE_EXEC\\x10\\x01\\x12\\x0e\\n\\nPERSISTENT\\x10\\x02\\\"<\\n\\x08\\x46uzzMode\\x12\\r\\n\\tAUTO_FUZZ\\x10\\x00\\x12\\x10\\n\\x0cINSTRUMENTED\\x10\\x01\\x12\\x0f\\n\\x0b\\x42INARY_ONLY\\x10\\x02\\\"9\\n\\tCheckMode\\x12\\r\\n\\tCHECK_ALL\\x10\\x00\\x12\\x0e\\n\\nALERT_ONLY\\x10\\x01\\x12\\r\\n\\tALERT_ONE\\x10\\x02\\\"$\\n\\rSeedInjectLoc\\x12\\t\\n\\x05STDIN\\x10\\x00\\x12\\x08\\n\\x04\\x41RGV\\x10\\x01\\\"\\t\\n\\x07StopMsg\\\"\\xf1\\x01\\n\\x08HelloMsg\\x12.\\n\\x0c\\x61rchitecture\\x18\\x01 \\x01(\\x0e\\x32\\x18.libpastis.HelloMsg.Arch\\x12\\x0c\\n\\x04\\x63pus\\x18\\x02 \\x01(\\r\\x12\\x0e\\n\\x06memory\\x18\\x03 \\x01(\\x04\\x12)\\n\\x07\\x65ngines\\x18\\x04 \\x03(\\x0b\\x32\\x18.libpastis.FuzzingEngine\\x12\\x10\\n\\x08hostname\\x18\\x05 \\x01(\\t\\x12%\\n\\x08platform\\x18\\x06 \\x01(\\x0e\\x32\\x13.libpastis.Platform\\\"3\\n\\x04\\x41rch\\x12\\x07\\n\\x03X86\\x10\\x00\\x12\\n\\n\\x06X86_64\\x10\\x01\\x12\\t\\n\\x05\\x41RMV7\\x10\\x02\\x12\\x0b\\n\\x07\\x41\\x41RCH64\\x10\\x03\\\"\\x8b\\x01\\n\\x06LogMsg\\x12\\x0f\\n\\x07message\\x18\\x01 \\x01(\\t\\x12)\\n\\x05level\\x18\\x02 \\x01(\\x0e\\x32\\x1a.libpastis.LogMsg.LogLevel\\\"E\\n\\x08LogLevel\\x12\\t\\n\\x05\\x44\\x45\\x42UG\\x10\\x00\\x12\\x08\\n\\x04INFO\\x10\\x01\\x12\\x0b\\n\\x07WARNING\\x10\\x02\\x12\\t\\n\\x05\\x45RROR\\x10\\x03\\x12\\x0c\\n\\x08\\x43RITICAL\\x10\\x04\\\"\\xfe\\x01\\n\\x0cTelemetryMsg\\x12\\x1f\\n\\x05state\\x18\\x01 \\x01(\\x0e\\x32\\x10.libpastis.State\\x12\\x14\\n\\x0c\\x65xec_per_sec\\x18\\x02 \\x01(\\r\\x12\\x12\\n\\ntotal_exec\\x18\\x03 \\x01(\\x04\\x12\\r\\n\\x05\\x63ycle\\x18\\x04 \\x01(\\r\\x12\\x0f\\n\\x07timeout\\x18\\x05 \\x01(\\r\\x12\\x16\\n\\x0e\\x63overage_block\\x18\\x06 \\x01(\\r\\x12\\x15\\n\\rcoverage_edge\\x18\\x07 \\x01(\\r\\x12\\x15\\n\\rcoverage_path\\x18\\x08 \\x01(\\r\\x12\\x17\\n\\x0flast_cov_update\\x18\\t \\x01(\\x04\\x12\\x11\\n\\tcpu_usage\\x18\\n \\x01(\\x02\\x12\\x11\\n\\tmem_usage\\x18\\x0b \\x01(\\x02\\\"\\x16\\n\\x14StopCoverageCriteria\\\"\\xf8\\x02\\n\\x0b\\x45nvelopeMsg\\x12,\\n\\tinput_msg\\x18\\x01 \\x01(\\x0b\\x32\\x17.libpastis.InputSeedMsgH\\x00\\x12&\\n\\x08\\x64\\x61ta_msg\\x18\\x02 \\x01(\\x0b\\x32\\x12.libpastis.DataMsgH\\x00\\x12(\\n\\tstart_msg\\x18\\x03 \\x01(\\x0b\\x32\\x13.libpastis.StartMsgH\\x00\\x12&\\n\\x08stop_msg\\x18\\x04 \\x01(\\x0b\\x32\\x12.libpastis.StopMsgH\\x00\\x12(\\n\\thello_msg\\x18\\x05 \\x01(\\x0b\\x32\\x13.libpastis.HelloMsgH\\x00\\x12$\\n\\x07log_msg\\x18\\x06 \\x01(\\x0b\\x32\\x11.libpastis.LogMsgH\\x00\\x12\\x30\\n\\rtelemetry_msg\\x18\\x07 \\x01(\\x0b\\x32\\x17.libpastis.TelemetryMsgH\\x00\\x12\\x38\\n\\rstop_crit_msg\\x18\\x08 \\x01(\\x0b\\x32\\x1f.libpastis.StopCoverageCriteriaH\\x00\\x42\\x05\\n\\x03msg*\\x1e\\n\\x05State\\x12\\x0b\\n\\x07RUNNING\\x10\\x00\\x12\\x08\\n\\x04IDLE\\x10\\x01*L\\n\\x08Platform\\x12\\x07\\n\\x03\\x41NY\\x10\\x00\\x12\\t\\n\\x05LINUX\\x10\\x01\\x12\\x0b\\n\\x07WINDOWS\\x10\\x02\\x12\\t\\n\\x05MACOS\\x10\\x03\\x12\\x0b\\n\\x07\\x41NDROID\\x10\\x04\\x12\\x07\\n\\x03IOS\\x10\\x05\\x62\\x06proto3')\n\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())", "\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'message_pb2', globals())\nif _descriptor._USE_C_DESCRIPTORS == False:\n\n  DESCRIPTOR._options = None\n  _STATE._serialized_start=1907\n  _STATE._serialized_end=1937\n  _PLATFORM._serialized_start=1939\n  _PLATFORM._serialized_end=2015\n  _FUZZINGENGINE._serialized_start=28\n  _FUZZINGENGINE._serialized_end=92\n  _INPUTSEEDMSG._serialized_start=94\n  _INPUTSEEDMSG._serialized_end=214\n  _INPUTSEEDMSG_SEEDTYPE._serialized_start=172\n  _INPUTSEEDMSG_SEEDTYPE._serialized_end=214\n  _DATAMSG._serialized_start=216\n  _DATAMSG._serialized_end=239\n  _STARTMSG._serialized_start=242\n  _STARTMSG._serialized_end=848\n  _STARTMSG_EXECMODE._serialized_start=631\n  _STARTMSG_EXECMODE._serialized_end=689\n  _STARTMSG_FUZZMODE._serialized_start=691\n  _STARTMSG_FUZZMODE._serialized_end=751\n  _STARTMSG_CHECKMODE._serialized_start=753\n  _STARTMSG_CHECKMODE._serialized_end=810\n  _STARTMSG_SEEDINJECTLOC._serialized_start=812\n  _STARTMSG_SEEDINJECTLOC._serialized_end=848\n  _STOPMSG._serialized_start=850\n  _STOPMSG._serialized_end=859\n  _HELLOMSG._serialized_start=862\n  _HELLOMSG._serialized_end=1103\n  _HELLOMSG_ARCH._serialized_start=1052\n  _HELLOMSG_ARCH._serialized_end=1103\n  _LOGMSG._serialized_start=1106\n  _LOGMSG._serialized_end=1245\n  _LOGMSG_LOGLEVEL._serialized_start=1176\n  _LOGMSG_LOGLEVEL._serialized_end=1245\n  _TELEMETRYMSG._serialized_start=1248\n  _TELEMETRYMSG._serialized_end=1502\n  _STOPCOVERAGECRITERIA._serialized_start=1504\n  _STOPCOVERAGECRITERIA._serialized_end=1526\n  _ENVELOPEMSG._serialized_start=1529\n  _ENVELOPEMSG._serialized_end=1905", "# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "pastisbroker/workspace.py", "chunked_list": ["import json\nfrom pathlib import Path\nfrom typing import Iterator, Generator\nimport shutil\nimport stat\nfrom enum import Enum, auto\n\nfrom libpastis.types import SeedType, PathLike\nfrom libpastis import SASTReport\n", "from libpastis import SASTReport\n\n\nclass WorkspaceStatus(Enum):\n    NOT_STARTED = auto()\n    RUNNING = auto()\n    FINISHED = auto()\n\n\nclass Workspace(object):\n    INPUT_DIR = \"corpus\"\n    HANGS_DIR = \"hangs\"\n    CRASH_DIR = \"crashes\"\n    LOG_DIR = \"logs\"\n    BINS_DIR = \"binaries\"\n    ALERTS_DIR = \"alerts_data\"\n    SEED_DIR = \"seeds\"\n\n    SAST_REPORT_COPY = \"sast-report.bin\"\n    CSV_FILE = \"results.csv\"\n    TELEMETRY_FILE = \"telemetry.csv\"\n    CLIENTS_STATS = \"clients-stats.json\"\n    LOG_FILE = \"broker.log\"\n    STATUS_FILE = \"STATUS\"\n    RUNTIME_CONFIG_FILE = \"config.json\"\n    COVERAGE_HISTORY = \"coverage-history.csv\"\n\n    def __init__(self, directory: Path, erase: bool = False):\n        self.root = directory\n\n        if erase:  # If want to erase the whole workspace\n            shutil.rmtree(self.root)\n\n        # Create the base directory structure\n        if not self.root.exists():\n            self.root.mkdir()\n        for s in [self.INPUT_DIR, self.CRASH_DIR, self.LOG_DIR, self.HANGS_DIR, self.BINS_DIR, self.SEED_DIR]:\n            p = self.root / s\n            if not p.exists():\n                p.mkdir()\n\n        # If no status file is found create one\n        status_file = Path(self.root / self.STATUS_FILE)\n        self._status = WorkspaceStatus.NOT_STARTED\n        if not status_file.exists():\n            status_file.write_text(self._status.name)\n        else:\n            self._status = WorkspaceStatus[status_file.read_text()]\n\n    def initialize_runtime(self, binaries_dir: PathLike, params: dict):\n        # First copy binary files in workspace if different directories\n        if self.root / self.BINS_DIR != binaries_dir:\n            shutil.copytree(binaries_dir, self.root / self.BINS_DIR, dirs_exist_ok=True)\n        # Save runtime configuration\n        config = self.root / self.RUNTIME_CONFIG_FILE\n        config.write_text(json.dumps(params))\n\n    def iter_corpus_directory(self, typ: SeedType) -> Generator[Path, None, None]:\n        dir_map = {SeedType.INPUT: self.INPUT_DIR, SeedType.CRASH: self.CRASH_DIR, SeedType.HANG: self.HANGS_DIR}\n        dir = self.root / dir_map[typ]\n        for file in dir.iterdir():\n            yield file\n\n    def iter_initial_corpus_directory(self) -> Generator[Path, None, None]:\n        for file in (self.root / self.SEED_DIR).iterdir():\n            yield file\n\n    def count_corpus_directory(self, typ: SeedType) -> int:\n        return sum(1 for _ in self.iter_corpus_directory(typ))\n\n    @property\n    def status(self) -> WorkspaceStatus:\n        return self._status\n\n    @status.setter\n    def status(self, value: WorkspaceStatus) -> None:\n        self._status = value\n        Path(self.root / self.STATUS_FILE).write_text(value.name)\n\n    @property\n    def telemetry_file(self) -> Path:\n        return self.root / self.TELEMETRY_FILE\n\n    @property\n    def clients_stat_file(self) -> Path:\n        return self.root / self.CLIENTS_STATS\n\n    @property\n    def sast_report_file(self) -> Path:\n        return self.root / self.SAST_REPORT_COPY\n\n    @property\n    def csv_result_file(self) -> Path:\n        return self.root / self.CSV_FILE\n\n    @property\n    def log_directory(self) -> Path:\n        return self.root / self.LOG_DIR\n\n    @property\n    def broker_log_file(self) -> Path:\n        return self.root / self.LOG_FILE\n\n    @property\n    def config_file(self) -> Path:\n        return self.root / self.RUNTIME_CONFIG_FILE\n\n    @property\n    def coverage_history(self) -> Path:\n        return self.root / self.COVERAGE_HISTORY\n\n    def add_binary(self, binary_path: Path) -> Path:\n        \"\"\"\n        Add a binary in the workspace directory structure.\n\n        :param binary_path: Path of the executable to copy\n        :return: the final executable file path\n        \"\"\"\n        dst_file = self.root / self.BINS_DIR / binary_path.name\n        if dst_file.absolute() != binary_path.absolute():  # If not already in the workspace copy them in workspace\n            dst_file.write_bytes(binary_path.read_bytes())\n            dst_file.chmod(stat.S_IRWXU)  # Change target mode to execute.\n        return dst_file\n\n    def add_binary_data(self, name: str, content: bytes) -> Path:\n        \"\"\"\n        Add a binary in the workspace directory structure.\n\n        :param name: Name of the executable file\n        :param content: Content of the executable\n        :return: the final executable file path\n        \"\"\"\n        dst_file = self.root / self.BINS_DIR / name\n        dst_file.write_bytes(content)\n        dst_file.chmod(stat.S_IRWXU)  # Change target mode to execute.\n        return dst_file\n\n    def add_sast_report(self, report: SASTReport) -> Path:\n        f = self.root / self.SAST_REPORT_COPY\n        report.write(f)\n        return f\n\n    @property\n    def binaries(self) -> Generator[Path, None, None]:\n        for file in (self.root / self.BINS_DIR).iterdir():\n            yield file\n\n    def initialize_alert_corpus(self, report: SASTReport) -> None:\n        \"\"\" Create a directory for each alert where to store coverage / vuln corpus \"\"\"\n        p = self.root / self.ALERTS_DIR\n        p.mkdir(exist_ok=True)\n        for alert in report.iter_alerts():\n            a_dir = p / str(alert.id)\n            a_dir.mkdir(exist_ok=True)\n\n    def save_alert_seed(self, id: int, name: str, data: bytes) -> None:\n        p = ((self.root / self.ALERTS_DIR) / str(id)) / name\n        p.write_bytes(data)\n\n    def save_seed_file(self, typ: SeedType, file: Path, initial: bool = False) -> None:\n        dir_map = {SeedType.INPUT: self.INPUT_DIR, SeedType.CRASH: self.CRASH_DIR, SeedType.HANG: self.HANGS_DIR}\n        if initial:\n            out = self.root / self.SEED_DIR / file.name\n        else:\n            out = self.root / dir_map[typ] / file.name\n        if str(file) != str(out):\n            shutil.copy(str(file), str(out))\n\n    def save_seed(self, typ: SeedType, name: str, data: bytes) -> None:\n        dir_map = {SeedType.INPUT: self.INPUT_DIR, SeedType.CRASH: self.CRASH_DIR, SeedType.HANG: self.HANGS_DIR}\n        out = self.root / dir_map[typ] / name\n        out.write_bytes(data)", "\nclass Workspace(object):\n    INPUT_DIR = \"corpus\"\n    HANGS_DIR = \"hangs\"\n    CRASH_DIR = \"crashes\"\n    LOG_DIR = \"logs\"\n    BINS_DIR = \"binaries\"\n    ALERTS_DIR = \"alerts_data\"\n    SEED_DIR = \"seeds\"\n\n    SAST_REPORT_COPY = \"sast-report.bin\"\n    CSV_FILE = \"results.csv\"\n    TELEMETRY_FILE = \"telemetry.csv\"\n    CLIENTS_STATS = \"clients-stats.json\"\n    LOG_FILE = \"broker.log\"\n    STATUS_FILE = \"STATUS\"\n    RUNTIME_CONFIG_FILE = \"config.json\"\n    COVERAGE_HISTORY = \"coverage-history.csv\"\n\n    def __init__(self, directory: Path, erase: bool = False):\n        self.root = directory\n\n        if erase:  # If want to erase the whole workspace\n            shutil.rmtree(self.root)\n\n        # Create the base directory structure\n        if not self.root.exists():\n            self.root.mkdir()\n        for s in [self.INPUT_DIR, self.CRASH_DIR, self.LOG_DIR, self.HANGS_DIR, self.BINS_DIR, self.SEED_DIR]:\n            p = self.root / s\n            if not p.exists():\n                p.mkdir()\n\n        # If no status file is found create one\n        status_file = Path(self.root / self.STATUS_FILE)\n        self._status = WorkspaceStatus.NOT_STARTED\n        if not status_file.exists():\n            status_file.write_text(self._status.name)\n        else:\n            self._status = WorkspaceStatus[status_file.read_text()]\n\n    def initialize_runtime(self, binaries_dir: PathLike, params: dict):\n        # First copy binary files in workspace if different directories\n        if self.root / self.BINS_DIR != binaries_dir:\n            shutil.copytree(binaries_dir, self.root / self.BINS_DIR, dirs_exist_ok=True)\n        # Save runtime configuration\n        config = self.root / self.RUNTIME_CONFIG_FILE\n        config.write_text(json.dumps(params))\n\n    def iter_corpus_directory(self, typ: SeedType) -> Generator[Path, None, None]:\n        dir_map = {SeedType.INPUT: self.INPUT_DIR, SeedType.CRASH: self.CRASH_DIR, SeedType.HANG: self.HANGS_DIR}\n        dir = self.root / dir_map[typ]\n        for file in dir.iterdir():\n            yield file\n\n    def iter_initial_corpus_directory(self) -> Generator[Path, None, None]:\n        for file in (self.root / self.SEED_DIR).iterdir():\n            yield file\n\n    def count_corpus_directory(self, typ: SeedType) -> int:\n        return sum(1 for _ in self.iter_corpus_directory(typ))\n\n    @property\n    def status(self) -> WorkspaceStatus:\n        return self._status\n\n    @status.setter\n    def status(self, value: WorkspaceStatus) -> None:\n        self._status = value\n        Path(self.root / self.STATUS_FILE).write_text(value.name)\n\n    @property\n    def telemetry_file(self) -> Path:\n        return self.root / self.TELEMETRY_FILE\n\n    @property\n    def clients_stat_file(self) -> Path:\n        return self.root / self.CLIENTS_STATS\n\n    @property\n    def sast_report_file(self) -> Path:\n        return self.root / self.SAST_REPORT_COPY\n\n    @property\n    def csv_result_file(self) -> Path:\n        return self.root / self.CSV_FILE\n\n    @property\n    def log_directory(self) -> Path:\n        return self.root / self.LOG_DIR\n\n    @property\n    def broker_log_file(self) -> Path:\n        return self.root / self.LOG_FILE\n\n    @property\n    def config_file(self) -> Path:\n        return self.root / self.RUNTIME_CONFIG_FILE\n\n    @property\n    def coverage_history(self) -> Path:\n        return self.root / self.COVERAGE_HISTORY\n\n    def add_binary(self, binary_path: Path) -> Path:\n        \"\"\"\n        Add a binary in the workspace directory structure.\n\n        :param binary_path: Path of the executable to copy\n        :return: the final executable file path\n        \"\"\"\n        dst_file = self.root / self.BINS_DIR / binary_path.name\n        if dst_file.absolute() != binary_path.absolute():  # If not already in the workspace copy them in workspace\n            dst_file.write_bytes(binary_path.read_bytes())\n            dst_file.chmod(stat.S_IRWXU)  # Change target mode to execute.\n        return dst_file\n\n    def add_binary_data(self, name: str, content: bytes) -> Path:\n        \"\"\"\n        Add a binary in the workspace directory structure.\n\n        :param name: Name of the executable file\n        :param content: Content of the executable\n        :return: the final executable file path\n        \"\"\"\n        dst_file = self.root / self.BINS_DIR / name\n        dst_file.write_bytes(content)\n        dst_file.chmod(stat.S_IRWXU)  # Change target mode to execute.\n        return dst_file\n\n    def add_sast_report(self, report: SASTReport) -> Path:\n        f = self.root / self.SAST_REPORT_COPY\n        report.write(f)\n        return f\n\n    @property\n    def binaries(self) -> Generator[Path, None, None]:\n        for file in (self.root / self.BINS_DIR).iterdir():\n            yield file\n\n    def initialize_alert_corpus(self, report: SASTReport) -> None:\n        \"\"\" Create a directory for each alert where to store coverage / vuln corpus \"\"\"\n        p = self.root / self.ALERTS_DIR\n        p.mkdir(exist_ok=True)\n        for alert in report.iter_alerts():\n            a_dir = p / str(alert.id)\n            a_dir.mkdir(exist_ok=True)\n\n    def save_alert_seed(self, id: int, name: str, data: bytes) -> None:\n        p = ((self.root / self.ALERTS_DIR) / str(id)) / name\n        p.write_bytes(data)\n\n    def save_seed_file(self, typ: SeedType, file: Path, initial: bool = False) -> None:\n        dir_map = {SeedType.INPUT: self.INPUT_DIR, SeedType.CRASH: self.CRASH_DIR, SeedType.HANG: self.HANGS_DIR}\n        if initial:\n            out = self.root / self.SEED_DIR / file.name\n        else:\n            out = self.root / dir_map[typ] / file.name\n        if str(file) != str(out):\n            shutil.copy(str(file), str(out))\n\n    def save_seed(self, typ: SeedType, name: str, data: bytes) -> None:\n        dir_map = {SeedType.INPUT: self.INPUT_DIR, SeedType.CRASH: self.CRASH_DIR, SeedType.HANG: self.HANGS_DIR}\n        out = self.root / dir_map[typ] / name\n        out.write_bytes(data)", ""]}
{"filename": "pastisbroker/client.py", "chunked_list": ["# Built-in imports\nfrom typing import Tuple, List, Dict\nfrom pathlib import Path\nimport logging\nimport time\nimport inspect\n\n# Third-party imports\nfrom libpastis.types import FuzzingEngineInfo, Arch, LogLevel, ExecMode, CheckMode, CoverageMode, SeedType, Platform\nfrom libpastis import FuzzingEngineDescriptor", "from libpastis.types import FuzzingEngineInfo, Arch, LogLevel, ExecMode, CheckMode, CoverageMode, SeedType, Platform\nfrom libpastis import FuzzingEngineDescriptor\n\n\nclass PastisClient(object):\n    \"\"\"\n    Utility class holding all information related to\n    a client connected to the broker.\n    \"\"\"\n\n    def __init__(self, id: int, netid: bytes, engines: List[FuzzingEngineInfo], arch: Arch, cpus: int, memory: int, hostname: str, platform: Platform):\n        # All this attributes are assigned once and for all\n        self.id = id\n        self.netid = netid\n        self.engines = engines\n        self.arch = arch\n        self.cpus = cpus\n        self.memory = memory\n        self.hostname = hostname\n        self.platform = platform\n\n        self.logger = None\n\n        # Runtime properties (reset at avery send_start)\n        self._program = None\n        self._running = False\n        self._engine = None  # FuzzingEngineDescriptor\n        self._engine_args = None\n        self._coverage_mode = None\n        self._exec_mode = None\n        self._check_mode = None\n        self._seeds_received = set()  # Seed sent to the client\n        self._seeds_submitted = set()  # Seed submitted by the client\n        self.target = None  # target in case of slicing\n        self.target_validated = False\n\n        # Runtime telemetry stats\n        self.exec_per_sec = None\n        self.total_exec = None\n        self.cycle = None\n        self.timeout = None\n        self.coverage_block = None\n        self.coverage_edge = None\n        self.coverage_path = None\n        self.last_cov_update = None\n\n        # seed stats\n        self.input_submitted_count = 0\n        self.crash_submitted_count = 0\n        self.timeout_submitted_count = 0\n        self.seed_first = 0\n\n        # SAST parameters\n        self.alert_covered = set()\n        self.alert_covered_first = 0\n        self.alert_validated = set()\n        self.alert_validated_first = 0\n\n        # time series\n        self._timeline_seeds = []  # List[Tuple[float, int, typ]]  # history of submission\n        # self._timeline_coverage = []\n\n    def configure_logger(self, log_dir, colorid: int):\n        if self.logger is None:\n            self.logger = logging.getLogger(f\"\\033[7m\\033[{colorid}m[{self.strid}]\\033[0m\")\n\n            # Add a file handler\n            hldr = logging.FileHandler(log_dir/f\"{self.strid}.log\")\n            hldr.setLevel(logging.DEBUG)\n            hldr.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s]: %(message)s\"))\n            self.logger.addHandler(hldr)\n\n    @property\n    def strid(self):\n        return f\"CLI-{self.id}-{self._engine.SHORT_NAME if self._engine else 'N/A'}\"\n\n    def is_new_seed(self, seed: bytes) -> bool:\n        \"\"\"\n        Return true if the seed has never been sent to a client\n\n        :param seed: seed bytes\n        :return: True if never sent to client\n        \"\"\"\n        return seed not in self._seeds_received and seed not in self._seeds_submitted\n\n    def add_peer_seed(self, seed: bytes) -> None:\n        self._seeds_received.add(seed)\n\n    def add_own_seed(self, seed: bytes) -> None:\n        self._seeds_submitted.add(seed)\n\n    def is_running(self) -> bool:\n        return self._running\n\n    def is_idle(self) -> bool:\n        return not self._running\n\n    def log(self, level: LogLevel, message: str):\n        # Get the function in the logger (warning, debug, info) and call it with message\n        if self.logger is None:  # Client has not yet been configured\n            getattr(logging, level.name.lower())(message)\n        else:  # Log in client logger\n            getattr(self.logger, level.name.lower())(message)\n\n    @property\n    def package_name(self) -> str:\n        return self._program\n\n    @property\n    def engine(self):\n        return self._engine\n\n    @property\n    def coverage_mode(self):\n        return self._coverage_mode\n\n    @property\n    def exec_mode(self):\n        return self._exec_mode\n\n    @property\n    def check_mode(self):\n        return self._check_mode\n\n    def set_stopped(self):\n        \"\"\" Flush runtime data (keep stats) \"\"\"\n        self._running = False\n        self._engine = None\n        self._coverage_mode = None\n        self._exec_mode = None\n        self._check_mode = None\n\n    def set_running(self, program: str, engine: FuzzingEngineDescriptor, covmode: CoverageMode, exmode: ExecMode,\n                    ckmode: CheckMode, engine_args: str = None):\n        self._program = program\n        self._running = True\n        self._engine = engine\n        self._coverage_mode = covmode\n        self._exec_mode = exmode\n        self._check_mode = ckmode\n        self._engine_args = engine_args\n        self._seeds_received = set()  # Seed sent to the client\n        self._seeds_submitted = set()  # Seed submitted by the client\n\n    def is_supported_engine(self, engine: FuzzingEngineDescriptor) -> bool:\n        for e in self.engines:\n            if e.name == engine.NAME:\n                return True\n        return False\n\n    def to_dict(self) -> Dict:\n        return {\n            \"id\": self.id,\n            \"strid\": self.strid,\n            \"engines\": [x.name for x in self.engines],\n            \"arch\": self.arch.name,\n            \"cpus\": self.cpus,\n            \"memory\": self.memory,\n            \"hostname\": self.hostname,\n            \"platform\": self.platform.name,\n            \"engine\": self._engine.NAME if self._engine else \"\",\n            \"engine_args\": self._engine_args,\n            \"coverage_mode\": self._coverage_mode.name,\n            \"exec_mode\": self._exec_mode.name,\n            \"check_mode\": self._check_mode.name,\n            \"seed_received_count\": len(self._seeds_received),\n            \"exec_per_sec\": self.exec_per_sec,\n            \"total_exec\": self.total_exec,\n            \"cycle\": self.cycle,\n            \"timeout\": self.timeout,\n            \"coverage_block\": self.coverage_block,\n            \"coverage_edge\": self.coverage_edge,\n            \"coverage_path\": self.coverage_path,\n            \"last_cov_update\": self.last_cov_update,\n            \"input_submitted_count\": self.input_submitted_count,\n            \"crash_submitted_count\": self.crash_submitted_count,\n            \"timeout_submitted_count\": self.timeout_submitted_count,\n            \"seed_first\": self.seed_first,\n            \"alert_covered\": list(self.alert_covered),\n            \"alert_covered_first\": self.alert_covered_first,\n            \"alert_validated\": list(self.alert_validated),\n            \"alert_validated_first\": self.alert_validated_first,\n        }\n\n    def add_covered_alert(self, a_id: int, cov: bool, cov_first: bool, val: bool, val_first: bool):\n        if cov:\n            self.alert_covered.add(a_id)\n        if cov_first:\n            self.alert_covered_first += 1\n        if val:\n            self.alert_validated.add(a_id)\n        if val_first:\n            self.alert_validated_first += 1", ""]}
{"filename": "pastisbroker/broker.py", "chunked_list": ["# built-in imports\nimport hashlib\nimport logging\nfrom typing import Generator, List, Optional, Union\nfrom pathlib import Path\nimport time\nfrom hashlib import md5\nfrom enum import Enum\nfrom collections import Counter\nimport datetime", "from collections import Counter\nimport datetime\nimport random\nimport queue\n\n# Third-party imports\nimport psutil\nfrom libpastis import BrokerAgent, FuzzingEngineDescriptor, EngineConfiguration, BinaryPackage, SASTReport, ClientAgent\nfrom libpastis.types import SeedType, FuzzingEngineInfo, LogLevel, Arch, State, SeedInjectLoc, CheckMode, CoverageMode, \\\n                            ExecMode, AlertData, PathLike, Platform, FuzzMode", "from libpastis.types import SeedType, FuzzingEngineInfo, LogLevel, Arch, State, SeedInjectLoc, CheckMode, CoverageMode, \\\n                            ExecMode, AlertData, PathLike, Platform, FuzzMode\nfrom libpastis.utils import get_local_architecture\nimport lief\nfrom tritondse import QuokkaProgram\n\n# Local imports\nfrom pastisbroker.client import PastisClient\nfrom pastisbroker.stat_manager import StatManager\nfrom pastisbroker.workspace import Workspace, WorkspaceStatus", "from pastisbroker.stat_manager import StatManager\nfrom pastisbroker.workspace import Workspace, WorkspaceStatus\nfrom pastisbroker.utils import load_engine_descriptor, Bcolors, COLORS\nfrom pastisbroker.coverage import CoverageManager, ClientInput\n\n\nlief.logging.disable()\n\n\nclass BrokingMode(Enum):\n    FULL = 1              # Transmit all seeds to all peers\n    NO_TRANSMIT = 2       # Does not transmit seed to peers (for comparing perfs of tools against each other)", "\nclass BrokingMode(Enum):\n    FULL = 1              # Transmit all seeds to all peers\n    NO_TRANSMIT = 2       # Does not transmit seed to peers (for comparing perfs of tools against each other)\n\n\n\n\nclass PastisBroker(BrokerAgent):\n\n    def __init__(self, workspace: PathLike,\n                 binaries_dir: PathLike,\n                 broker_mode: BrokingMode,\n                 check_mode: CheckMode = CheckMode.CHECK_ALL,\n                 inject_loc: SeedInjectLoc = SeedInjectLoc.STDIN,\n                 sast_report: PathLike = None,\n                 p_argv: List[str] = None,\n                 memory_threshold: int = 85,\n                 start_quorum: int = 0,\n                 filter_inputs: bool = False,\n                 stream: bool = False,\n                 replay_threads: int = 4):\n        super(PastisBroker, self).__init__()\n\n        # Initialize workspace\n        self.workspace = Workspace(Path(workspace))\n        params = {\"binaries_dir\": str(Path(binaries_dir).absolute()),\n                  \"broker_mode\": broker_mode.name,\n                  \"check_mode\": check_mode.name,\n                  \"inject_loc\": inject_loc.name,\n                  \"argvs\": p_argv}\n        self.workspace.initialize_runtime(binaries_dir, params)\n\n        self._configure_logging()\n\n        # Register all agent callbacks\n        self._register_all()\n\n        # Init internal state\n        self.broker_mode = broker_mode\n        self.ck_mode = check_mode\n        self.inject = inject_loc\n        self.argv = [] if p_argv is None else p_argv\n        self.engines_args = {}\n        self.engines = {}  # name->FuzzingEngineDescriptor\n\n        # for slicing mode (otherwise not used)\n        self._slicing_ongoing = {}  # Program -> {Addr -> [cli]}\n\n        # Initialize availables binaries\n        self.programs = {}  # Tuple[(Platform, Arch)] -> List[BinaryPackage]\n        self._find_binaries(binaries_dir)\n\n        # Klocwork informations\n        self.sast_report = None\n        if sast_report:\n            self.initialize_sast_report(sast_report)\n\n        # Client infos\n        self.clients = {}   # bytes -> Client\n        self._cur_id = 0\n\n        # Runtime infos\n        self._running = False\n        self._seed_pool = {}  # Seed bytes -> SeedType\n        self._init_seed_pool = {}  # Used for NO_TRANSMIT mode\n        self._start_time = None\n        self._stop = False\n\n        # Load the workspace seeds\n        self._load_workspace()\n\n        # Create the stat manager\n        self.statmanager = StatManager(self.workspace)\n\n        # Watchdog to monitor RAM usage\n        self.watchdog = None\n        self._threshold = memory_threshold # percent\n\n        # startup quorum\n        self._startup_quorum = start_quorum\n        self._current_quorum = 0\n\n        # Proxy feature\n        self._proxy = None\n        self._proxy_cli = None\n        self._proxy_start_signal = False\n        self._proxy_seed_queue = queue.Queue()\n\n        # Coverage + filtering feature\n        self._coverage_manager = None\n        self.filter_inputs: bool = filter_inputs\n        if filter_inputs or stream:\n            if (path := self.find_vanilla_binary()) is not None:  # Find an executable suitable for coverage\n                logging.info(f\"Coverage binary: {path}\")\n                stream_file = self.workspace.coverage_history if stream else \"\"\n                self._coverage_manager = CoverageManager(replay_threads, filter_inputs, path, self.argv, self.inject, stream_file)\n            else:\n                logging.warning(\"filtering or stream enabled but cannot find vanilla binary\")\n\n\n    def find_vanilla_binary(self) -> Optional[str]:\n        \"\"\"\n        Find a binary without instrumentation to be used for coverage\n        computation. It also has to match local architecture.\n        :return: Path to the progam\n        \"\"\"\n        local_arch = get_local_architecture()\n        for pkg in self.programs.get((Platform.LINUX, local_arch)):\n            path = str(pkg.executable_path.absolute())\n            p = lief.parse(str(path))\n            ok = True\n            for f in p.functions:\n                if \"hfuzz_\" in f.name or \"__afl_\" in f.name or \"__gcov_\" in f.name or \"__asan_\" in f.name:\n                    ok = False\n                    break\n            if ok:\n                return path\n        return None\n\n\n    def load_engine_addon(self, py_module: str) -> bool:\n        desc = load_engine_descriptor(py_module)\n        if desc is not None:\n            self.engines[desc.NAME] = desc\n            self.engines_args[desc.NAME] = []\n            return True\n        else:\n            return False\n\n    def initialize_sast_report(self, report: PathLike):\n        self.sast_report = SASTReport.from_file(report)\n        self.workspace.add_sast_report(self.sast_report)\n        self.workspace.initialize_alert_corpus(self.sast_report)\n\n    @property\n    def running(self) -> bool:\n        return self._running\n\n    def iter_other_clients(self, cli_id: bytes) -> Generator[PastisClient, None, None]:\n        \"\"\"\n        Generator of all clients but the one given in parameter\n\n        :param cli_id: netid of the client to ignore\n        :return: Generator of PastisClient object\n        \"\"\"\n        for c in self.clients.values():\n            if c.netid != cli_id:\n                yield c\n\n    def new_uid(self) -> int:\n        \"\"\"\n        Generate a new unique id for a client (int)\n\n        :return: int, unique (in an execution)\n        :return: int, unique (in an execution)\n        \"\"\"\n        v = self._cur_id\n        self._cur_id += 1\n        return v\n\n    def _register_all(self):\n        self.register_seed_callback(self.seed_received)\n        self.register_hello_callback(self.hello_received)\n        self.register_log_callback(self.log_received)\n        self.register_telemetry_callback(self.telemetry_received)\n        self.register_stop_coverage_callback(self.stop_coverage_received)\n        self.register_data_callback(self.data_received)\n\n    def get_client(self, cli_id: bytes) -> Optional[PastisClient]:\n        cli = self.clients.get(cli_id)\n        if not cli:\n            logging.warning(f\"client '{cli_id}' unknown (send stop)\")\n            self.send_stop(cli_id)\n        return cli\n\n    def kick_client(self, cli_id: bytes) -> None:\n        cli = self.clients.pop(cli_id)  # pop it from client list\n        logging.info(f\"kick client: {cli.strid}\")\n        self.send_stop(cli_id)\n\n    def seed_received(self, cli_id: bytes, typ: SeedType, seed: bytes):\n        cli = self.get_client(cli_id)\n        if not cli:\n            return\n        is_new = seed not in self._seed_pool\n        h = md5(seed).hexdigest()\n\n        # Show log message and save seed to file\n        self.statmanager.update_seed_stat(cli, typ)  # Add info only if new\n        cli.log(LogLevel.INFO, f\"seed {h} [{self._colored_seed_type(typ)}][{self._colored_seed_newness(is_new)}]\")\n        cli.add_own_seed(seed)  # Add seed in client's seed\n        fname = self.write_seed(typ, cli.strid, seed) # Write seed to file\n\n        if is_new:\n            self._seed_pool[seed] = typ  # Save it in the local pool\n\n            if self._coverage_manager:\n                sp = fname.split(\"_\")\n                covi = ClientInput(seed, \"\", f\"{sp[0]}_{sp[1]}\", sp[2], h, fname, typ, cli.netid, cli.strid, \"GRANTED\", \"\", -1, [])\n                self._coverage_manager.push_input(covi)\n\n            if not self.filter_inputs:  # If seed are not filtered send it right away\n                self.seed_granted(cli.netid, typ, seed)\n        else:\n            logging.debug(f\"receive duplicate seed {h} by {cli.strid}\")\n\n\n    def seed_granted(self, cli_id: bytes, typ: SeedType, seed: bytes):\n        # Iterate on all clients and send it to whomever never received it\n        if self.broker_mode == BrokingMode.FULL:\n            self.send_seed_to_all_others(cli_id, typ, seed)\n\n        if self.is_proxied:  # Forward it to the proxy\n            self._proxy.send_seed(typ, seed)\n\n    def send_seed_to_all_others(self, origin_id: bytes, typ: SeedType, seed: bytes) -> None:\n        for c in self.iter_other_clients(origin_id):\n            if c.is_new_seed(seed):\n                self.send_seed(c.netid, typ, seed)  # send the seed to the client\n                c.add_peer_seed(seed)  # Add it in its list of seed\n\n    def add_seed_file(self, file: PathLike, initial: bool = False) -> None:\n        p = Path(file)\n        logging.info(f\"Add seed {p.name} in pool\")\n        # Save seed in the workspace\n        self.workspace.save_seed_file(SeedType.INPUT, p, initial)\n\n        seed = p.read_bytes()\n        self._seed_pool[seed] = SeedType.INPUT\n        if initial:\n            self._init_seed_pool[seed] = SeedType.INPUT\n\n    def write_seed(self, typ: SeedType, cli_id: str, seed: bytes) -> str:\n        fname = self.mk_input_name(cli_id, seed)\n        self.workspace.save_seed(typ, fname, seed)\n        return fname\n\n    def mk_input_name(self, cli_id: str, seed: bytes) -> str:\n        t = time.strftime(\"%Y-%m-%d_%H:%M:%S\", time.localtime())\n        elapsed = str(datetime.timedelta(seconds=time.time() - self._start_time)).replace(\" day, \", \"d:\").replace(\" days, \", \"d:\")\n        return f\"{t}_{elapsed}_{cli_id}_{md5(seed).hexdigest()}.cov\"\n\n    def hello_received(self, cli_id: bytes, engines: List[FuzzingEngineInfo], arch: Arch, cpus: int, memory: int, hostname: str, platform: Platform):\n        uid = self.new_uid()\n        client = PastisClient(uid, cli_id, engines, arch, cpus, memory, hostname, platform)\n        logging.info(f\"[{client.strid}] [HELLO] Name:{hostname} Arch:{arch.name} engines:{[x.name for x in engines]} (cpu:{cpus}, mem:{memory})\")\n        self.clients[client.netid] = client\n\n        # Load engines if they are not (lazy loading)\n        for eng in engines:\n            if eng.name not in self.engines:\n                self.load_engine_addon(eng.pymodule)\n\n        if self.running:  # A client is coming in the middle of a session\n            if self._startup_quorum:\n                self._current_quorum += 1\n                if self._current_quorum >= self._startup_quorum:\n                    logging.info(\n                        f\"client number quorum ({self._current_quorum}/{self._startup_quorum}) reached, start all of them\")\n                    self.start_pending_clients()  # start all pending clients (including the one triggering this func)\n                else:\n                    # Put the client on-hold to wait for quorum\n                    logging.info(f\"client {client.strid} on-hold, wait quorum ({self._current_quorum}/{self._startup_quorum})\")\n            else:  # there is no quorum so start immediately\n                self.start_client_and_send_corpus(client)\n        else:\n            pass  # Client connection is kept in clients dict for later\n\n    def _transmit_pool(self, client, pool) -> None:\n        for seed, typ in pool.items():\n            self.send_seed(client.netid, typ, seed)  # necessarily a new seed\n            client.add_peer_seed(seed)  # Add it in its list of seed\n\n    def log_received(self, cli_id: bytes, level: LogLevel, message: str):\n        client = self.get_client(cli_id)\n        if not client:\n            return\n        client.log(level, message)\n\n    def telemetry_received(self, cli_id: bytes,\n                           _: State = None, exec_per_sec: int = None, total_exec: int = None,\n                           cycle: int = None, timeout: int = None, coverage_block: int = None, coverage_edge: int = None,\n                           coverage_path: int = None, last_cov_update: int = None):\n        client = self.get_client(cli_id)\n        if not client:\n            return\n        # NOTE: ignore state (shall we do something of it?)\n        args = [('-' if not x else x) for x in [exec_per_sec, total_exec, cycle, timeout, coverage_block, coverage_edge, coverage_path, last_cov_update]]\n        client.log(LogLevel.INFO, \"exec/s:{} tot_exec:{} cycle:{} To:{} CovB:{} CovE:{} CovP:{} last_up:{}\".format(*args))\n\n        # Update all stats in the stat manager (for later UI update)\n        self.statmanager.set_exec_per_sec(client, exec_per_sec)\n        self.statmanager.set_total_exec(client, total_exec)\n        self.statmanager.set_cycle(client, cycle)\n        self.statmanager.set_timeout(client, timeout)\n        self.statmanager.set_coverage_block(client, coverage_block)\n        self.statmanager.set_coverage_edge(client, coverage_edge)\n        self.statmanager.set_coverage_path(client, coverage_path)\n        self.statmanager.set_last_coverage_update(client, last_cov_update)\n        self.statmanager.update_telemetry_client(client)\n        # NOTE: Send an update signal for future UI ?\n\n    def stop_coverage_received(self, cli_id: bytes):\n        client = self.get_client(cli_id)\n        if not client:\n            return\n\n        if self.ck_mode == CheckMode.ALERT_ONE:  # Start it on another target\n            addr = client.target\n            s = \"validated \" if client.target_validated else \"is stuck for \"\n            logging.info(f\"[{client.strid}] [STOP_COVERAGE] client {s} 0x{addr:x} address (launch another target)\")\n            clis = self._slicing_ongoing[client.package_name].pop(addr)  # pop the address definitely\n            self.relaunch_clients(clis)  # relaunch all clients on a new target\n        else:\n            logging.info(f\"[{client.strid}] [STOP_COVERAGE]: restart client\")\n            self.relaunch_clients([client])  # restart the client\n\n    def data_received(self,  cli_id: bytes, data: str):\n        client = self.get_client(cli_id)\n        if not client:\n            return\n        first_cov, first_val = False, False\n\n        if not self.sast_report:\n            logging.warning(\"Data received while no SAST report is loaded (drop data)\")\n            return\n\n        alert_data = AlertData.from_json(data)\n        alert = self.sast_report.alerts[alert_data.id]\n\n        if not alert.validated and alert_data.validated:\n            logging.info(f\"[{client.strid}] First to validate {alert}\")\n            alert.validated = alert_data.validated\n            self.sast_report.write_csv(self.workspace.csv_result_file)  # Update CSV to keep it updated\n            first_val = True\n\n            if self.ck_mode == CheckMode.ALERT_ONE:\n                client.target_validated = True\n                # Note: Do not relaunch the client now, wait for him to send stop_coverage\n\n        # Need to be after validated because alert.covered = True (will also set validated)\n        if not alert.covered and alert_data.covered:\n            logging.info(f\"[{client.strid}] First to cover {alert}\")\n            alert.covered = alert_data.covered\n            self.sast_report.write_csv(self.workspace.csv_result_file)  # Update CSV to keep it updated\n            first_cov = True\n\n        # Update clients of and stats\n        client.add_covered_alert(alert.id, alert.covered, first_cov, alert.validated, first_val)\n\n        # Save systematically the AlertData received\n        self._save_alert_seed(client, alert_data)  # Also save seed in separate folder\n\n        if first_cov or first_val:\n            cov, val, tot = self.sast_report.get_stats()\n            logging.info(f\"SAST results updated: defaults: [covered:{cov}/{tot}] [validated:{val}/{tot}]\")\n\n        # If all alerts are validated send a stop to everyone\n        if self.sast_report.all_alerts_validated():\n            self.stop_broker()\n\n    def relaunch_clients(self, clients):\n        for cli in clients:\n            logging.info(\n                f\"Launch client {cli.strid} as its targeting an address that has just been validated\")\n            self.start_client_and_send_corpus(cli)\n\n    def stop_broker(self):\n        for client in self.clients.values():\n            logging.info(f\"Send stop to {client.strid}\")\n            self.send_stop(client.netid)\n        self._stop = True\n\n        # Stop coverage manager if any\n        if self._coverage_manager:\n            self._coverage_manager.stop()\n\n        # Call the statmanager to wrap-up values\n        self.statmanager.post_execution(list(self.clients.values()), self.workspace)\n\n        if self.is_proxied:\n            self._proxy.stop()\n\n        if self.sast_report:  # If a SAST report was loaded\n            # Write the final CSV in the workspace\n            self.sast_report.write_csv(self.workspace.csv_result_file)\n            # And also re-write the Klocwork report (that also contains resutls)\n            self.sast_report.write(self.workspace.sast_report_file)\n\n    def start_pending_clients(self) -> None:\n        \"\"\"\n        Start clients that connected but for which we did not yet send a response.\n        \"\"\"\n        # Send the start message to all clients (already connected)\n        for c in self.clients.values():\n            if not c.is_running():\n                self.start_client_and_send_corpus(c)\n\n    def start_client_and_send_corpus(self, client: PastisClient) -> None:\n        self.start_client(client)\n        # Iterate all the seed pool and send it to the client\n        if self.broker_mode == BrokingMode.FULL:\n            self._transmit_pool(client, self._seed_pool)\n        elif self.broker_mode == BrokingMode.NO_TRANSMIT:\n            self._transmit_pool(client, self._init_seed_pool)\n\n    def start_client(self, client: PastisClient):\n        engine = None\n        exmode = ExecMode.SINGLE_EXEC\n        fuzzmode = FuzzMode.INSTRUMENTED\n        engine_args = None\n        package = covmode = fuzzmod = None\n        engines = Counter({e: 0 for e in self.engines})\n        engines.update(c.engine.NAME for c in self.clients.values() if c.is_running())  # Count instances of each engine running\n        for eng, _ in engines.most_common()[::-1]:\n            eng_desc = self.engines[eng]\n            # If the engine is not supported by the client continue\n            if not client.is_supported_engine(eng_desc):\n                continue\n\n            # Try finding a suitable binary for the current engine and the client arch\n            programs: List[BinaryPackage] = self.programs.get((client.platform, client.arch))\n            package = None\n            exmode = None\n            fuzzmod = FuzzMode.AUTO\n            for p in programs:\n                res, xmod, fmod = eng_desc.accept_file(p.executable_path)  # Iterate all files on that engine descriptor to check it accept it\n                if res:\n                    if exmode:\n                        if exmode == ExecMode.SINGLE_EXEC and xmod == ExecMode.PERSISTENT:  # persistent supersede single_exec\n                            package, exmode, fuzzmod = p, xmod, fmod\n                        else:\n                            if fuzzmod == FuzzMode.BINARY_ONLY and fmod == FuzzMode.INSTRUMENTED:  # instrumented supersede binary only\n                                package, exmode, fuzzmod = p, xmod, fmod\n                            else:\n                                pass  # Program is suitable but we already had found a PERSISTENT one\n                    else:\n                        package, exmode, fuzzmod = p, xmod, fmod  # first suitable program found\n\n            if not package:  # If still no program was found continue iterating engines\n                continue\n\n            # Valid engine and suitable program found\n            engine = eng_desc\n\n            # Find a configuration to use for that engine\n            engine_args = self._find_configuration(engine)\n            covmode = self._find_coverage_mode(engine, engine_args)\n\n            # If got here a suitable program has been found just break loop\n            break\n\n        if engine is None or package is None:\n            logging.critical(f\"No suitable engine or program was found for client {client.strid} {client.engines}\")\n            return\n\n        if self.ck_mode == CheckMode.ALERT_ONE:\n            if package.is_quokka():\n                targets = self._slicing_ongoing[package.name]\n                sorted_targets = sorted(targets, key=lambda k: len(targets[k]), reverse=False)  # sort alert addresses by number of client instances working on it\n                if sorted_targets:\n                    addr = sorted_targets[0]\n                    targets[addr].append(client)\n                    client.target = addr   # keep the target on which the client is working on\n                    client.target_validated = False\n                    # Now twist the config to transmit it to the client\n                    engine_args = engine.config_class.new() if engine_args is None else engine_args\n                    engine_args.set_target(addr)\n                    logging.info(f\"will start client {client.strid} to target 0x{addr:x}\")\n                else:\n                    logging.error(f\"No alert target for binary package {package.name}\")\n            else:\n                logging.error(f\"In mode {self.ck_mode} but the binary package is not a QBinExport\")\n                return\n\n        # Update internal client info and send him the message\n        engine_args_str = engine_args.to_str() if engine_args else \"\"\n        logging.info(f\"send start client {client.id}: {package.name} [{engine.NAME}, {covmode.name}, {fuzzmod.name}, {exmode.name}]\")\n        client.set_running(package.name, engine, covmode, exmode, self.ck_mode, engine_args_str)\n        client.configure_logger(self.workspace.log_directory, random.choice(COLORS))  # Assign custom color client\n\n        self.send_start(client.netid,\n                        package.name,\n                        package.executable_path if package.is_standalone() else package.make_package(),\n                        self.argv,\n                        exmode,\n                        fuzzmod,\n                        self.ck_mode,\n                        covmode,\n                        FuzzingEngineInfo(engine.NAME, engine.VERSION, \"\"),\n                        engine_args_str,\n                        self.inject,\n                        self.sast_report.to_json() if self.sast_report else b\"\")\n\n    def _find_configuration(self, engine: FuzzingEngineDescriptor) -> Optional[EngineConfiguration]:\n        \"\"\"\n        Find a configuration for the engine. It will iterate all configuration\n        available or automatically balance de configuration if there is multiple of\n        them\n        :param engine:\n        :return:\n        \"\"\"\n        confs = self.engines_args[engine.NAME]\n        if confs:\n            if len(confs) == 1:\n                return confs[0]\n            else:\n                conf = confs.pop(0)\n                confs.append(conf)  # Rotate the configuration\n                return conf\n        else:\n            return None\n\n    def _find_coverage_mode(self, engine: FuzzingEngineDescriptor, conf: Union[EngineConfiguration]) -> CoverageMode:\n\n        # Get coverage modes supported by the engine\n        supported_mods = engine.supported_coverage_strategies()\n\n        # Now that program have been found select coverage strategy\n        if len(supported_mods) > 1:\n\n            if conf:  # In the case we wanted to launch engines with specific configuration\n                return conf.get_coverage_mode()    # return the appropriate coverage mode in that configuration\n\n            else:\n                # auto-balance the CoverageMode instances\n                covs = Counter({c: 0 for c in supported_mods})\n                # Count how many times each coverage strategies have been launched\n                covs.update(x.coverage_mode for x in self.clients.values() if x.is_running() and x.engine == engine)\n\n                if sum(covs.values()) == 0:  # No configuration has been triggered yet\n                    if CoverageMode.EDGE in supported_mods:  # launch preferably edge first\n                        return CoverageMode.EDGE\n                    else:\n                        return supported_mods[0]  # Return first supported mode\n\n                reverted = {v: k for k, v in covs.items()}  # Revert dictionnary to have freq -> covmodes\n                return reverted[min(reverted)]  # Select mode if least instances\n\n        else:\n            return supported_mods[0]\n\n    def start(self, running: bool = True):\n        super(PastisBroker, self).start()  # Start the listening thread\n        self._start_time = time.time()\n        self._running = running\n        self.workspace.status = WorkspaceStatus.RUNNING\n        logging.info(\"start broking\")\n\n        if self._coverage_manager:  # if it has been instanciated start it\n            self._coverage_manager.start()\n            for seed in self._init_seed_pool.keys():  # Push initial corpus to set baseline coverage\n                fname = self.mk_input_name(\"INITIAL\", seed)\n                sp = fname.split(\"_\")\n                covi = ClientInput(seed, \"\", f\"{sp[0]}_{sp[1]}\", sp[2], sp[4], fname, SeedType.INPUT, b\"INITIAL\", \"INITIAL\", \"GRANTED\", \"\", -1, [])\n                self._coverage_manager.push_input(covi)\n\n        if self.is_proxied and self._proxy_cli:\n            self._running = False  # disable running wait start broker\n            self._proxy.send_hello([self._proxy_cli])\n        else:\n            # Send the start message to all clients (already connected)\n            if self._running:  # If we want to run now (cmdline mode)\n                self.start_pending_clients()\n\n    def run(self, timeout: int = None):\n        self.start()\n        last_t = time.time()\n\n        # Start infinite loop\n        try:\n            while True:\n                time.sleep(0.05)\n                t = time.time()\n\n                # Check if the campaign have to be stopped\n                if timeout is not None:\n                    if t > (self._start_time + timeout):\n                        logging.info(\"Campaign timeout reached, stop campaign.\")\n                        self._stop = True\n\n                if t > (last_t + 60):  # only check every minute\n                    last_t = t\n                    if not self._check_memory_usage():\n                        # The machine starts being overloaded\n                        # For security kill triton instance\n                        for cli in list(self.clients.values()):\n                            if cli.engine.SHORT_NAME == \"TT\":  # is triton\n                                self.kick_client(cli.netid)\n\n                # if inputs are filtered. Get granted inputs and forward them to appropriate clients\n                if self.filter_inputs:\n                    for item in self._coverage_manager.iter_granted_inputs():\n                        self.seed_granted(item.fuzzer_id, item.seed_status, item.content)\n\n                # Check if we received the start signal from the proxy-master\n                if self._proxy_start_signal:\n                    self._proxy_start_signal = False\n                    self.start_pending_clients()\n\n                # Check if there are seed coming from the proxy-master to forward to clients\n                if not self._proxy_seed_queue.empty():\n                    try:\n                        while True:\n                            origin, typ, seed = self._proxy_seed_queue.get_nowait()\n                            self.send_seed_to_all_others(origin, typ, seed)\n                    except queue.Empty:\n                        pass\n\n                if self._stop:\n                    logging.info(\"broker terminate\")\n                    break\n        except KeyboardInterrupt:\n            logging.info(\"stop required (Ctrl+C)\")\n        self.workspace.status = WorkspaceStatus.FINISHED\n        self.stop_broker()\n\n    def _find_binaries(self, binaries_dir) -> None:\n        \"\"\"\n        Iterate the whole directory to find suitables binaries in the\n        various architecture, and compiled for the various engines.\n\n        :param binaries_dir: directory containing the various binaries\n        :return: None\n        \"\"\"\n        d = Path(binaries_dir)\n        for file in d.iterdir():\n            if file.is_file():\n                try:\n                    pkg = BinaryPackage.auto(file)  # try creating a package\n                except ValueError:  # if not an executable\n                    continue\n\n                if pkg is None:\n                    continue\n\n                # Check that we can find a Quokka file associated otherwise reject it\n                if self.ck_mode == CheckMode.ALERT_ONE:\n                    if pkg.is_quokka():\n                        try:\n                            # Instanciate the Quokka Program and monkey patch object\n                            quokka_prog = QuokkaProgram(pkg.quokka, pkg.executable_path)\n                            f = quokka_prog.get_function(\"__sast_alert_placeholder\")\n                            self._slicing_ongoing[file.name] = {x: [] for x in quokka_prog.get_caller_instructions(f)}\n                        except ValueError:\n                            logging.warning(f\"can't find placeholder file in {file.name}, thus ignores it.\")\n                            continue\n                    else:\n                        logging.warning(f\"{file.name} executable found but no QBinExport file associated (ignores it)\")\n                        continue\n\n                logging.info(f\"new binary detected [{pkg.platform.name}, {pkg.arch.name}]: {file}\")\n\n                # Add it in the internal structure\n                data = (pkg.platform, pkg.arch)\n                data2 = (Platform.ANY, pkg.arch)\n                if data not in self.programs:\n                    self.programs[data] = []\n                if data2 not in self.programs:\n                    self.programs[data2] = []\n                self.programs[data].append(pkg)\n                self.programs[data2].append(pkg)  # Also add an entry for any platform\n\n    def _load_workspace(self):\n        \"\"\" Load all the seeds in the workspace\"\"\"\n        for typ in list(SeedType):  # iter seed types: input, crash, hang..\n            for file in self.workspace.iter_corpus_directory(typ):\n                logging.debug(f\"Load seed in pool: {file.name}\")\n                content = file.read_bytes()\n                self._seed_pool[content] = typ\n        # TODO: Also dumping the current state to a file in case\n        # TODO: of exit. And being able to reload it. (not to resend all seeds to clients)\n\n    def add_engine_configuration(self, name: str, config_file: PathLike):\n        if name in self.engines_args:\n            engine = self.engines[name]\n            conf = engine.config_class.from_file(config_file)\n            self.engines_args[name].append(conf)\n        else:\n            logging.error(f\"can't find engine {name} (shall preload it to add a configuration)\")\n\n    def _configure_logging(self):\n        hldr = logging.FileHandler(self.workspace.broker_log_file)\n        hldr.setLevel(logging.root.level)\n        hldr.setFormatter(logging.Formatter(\"%(asctime)s [%(name)s] [%(levelname)s]: %(message)s\"))\n        logging.root.addHandler(hldr)\n\n    def _colored_seed_type(self, typ: SeedType) -> str:\n        mapper = {SeedType.INPUT: Bcolors.OKBLUE,\n                  SeedType.HANG: Bcolors.WARNING,\n                  SeedType.CRASH: Bcolors.FAIL}\n        return mapper[typ]+typ.name+Bcolors.ENDC\n\n    def _colored_seed_newness(self, is_new: bool) -> str:\n        col, text = {True: (Bcolors.OKGREEN, \"NEW\"),\n                     False: (Bcolors.WARNING, \"DUP\")}[is_new]\n        return col+text+Bcolors.ENDC\n\n    def _save_alert_seed(self, from_cli: PastisClient, alert: AlertData):\n        t = time.strftime(\"%Y-%m-%d_%H:%M:%S\", time.localtime())\n        elapsed = str(datetime.timedelta(seconds=time.time()-self._start_time)).replace(\" day, \", \"d:\").replace(\" days, \", \"d:\")\n        fname = f\"{t}_{elapsed}_{from_cli.strid}_{md5(alert.seed).hexdigest()}-{'CRASH' if alert.validated else 'COVERAGE'}.cov\"\n        logging.debug(f\"Save alert  [{alert.id}] file: {fname}\")\n        self.workspace.save_alert_seed(alert.id, fname, alert.seed)\n\n    def _check_memory_usage(self) -> bool:\n        mem = psutil.virtual_memory()\n        logging.info(f\"RAM usage: {mem.percent:.2f}%\")\n        if mem.percent >= self._threshold:\n            logging.warning(f\"Threshold reached: {mem.percent}%\")\n            return False\n        return True\n\n    @property\n    def is_proxied(self) -> bool:\n        \"\"\"\n        Get if the broker as a proxy to another broker.\n        \"\"\"\n        return self._proxy is not None\n\n    def set_proxy(self, ip: str, port: int, py_module: str) -> bool:\n        self._proxy = ClientAgent()\n\n        # Load the engine info to impersonate\n        desc = load_engine_descriptor(py_module)\n        if desc is None:\n            logging.error(\"Cannot load FuzzingEngineDescriptor\")\n            return False\n        else:\n            self._proxy_cli = FuzzingEngineInfo(desc.NAME, desc.VERSION, py_module)\n\n        self._proxy.register_start_callback(self._proxy_start_received)\n        self._proxy.register_seed_callback(self._proxy_seed_received)\n        self._proxy.register_stop_callback(self._proxy_stop_received)\n\n        logging.info(f\"connect to broker {ip}:{port}\")\n        self._proxy.connect(ip, port)\n        self._proxy.start()\n\n    def _proxy_start_received(self, fname: str, binary: bytes, engine: FuzzingEngineInfo, exmode: ExecMode,\n                              fuzzmode: FuzzMode, chkmode: CheckMode, covmode: CoverageMode, seed_inj: SeedInjectLoc,\n                              engine_args: str, argv: List[str], sast_report: str = None):\n        # FIXME: Use parameters received\n        logging.info(\"[PROXY] start received !\")\n        self._running = True\n        # if self._running:\n        #     self.start_pending_clients()\n\n    def _proxy_seed_received(self, typ: SeedType, seed: bytes):\n        # Forward the seed to underlying clients\n        logging.info(f\"[PROXY] seed {typ.name} received forward to agents\")\n\n        # Save the seed locally\n        self.write_seed(typ, \"PROXY\", seed)\n        self._seed_pool[seed] = typ  # add it to the pool\n        self._init_seed_pool[seed] = typ  # also consider it as initial corpus\n\n        # Forward it to all clients\n        self._proxy_seed_queue.put((b\"PROXY\", typ, seed))\n        # self.send_seed_to_all_others(b\"PROXY\", typ, seed)\n\n    def _proxy_stop_received(self):\n        logging.info(f\"[PROXY] stop received!\")\n        self._stop = True", "class PastisBroker(BrokerAgent):\n\n    def __init__(self, workspace: PathLike,\n                 binaries_dir: PathLike,\n                 broker_mode: BrokingMode,\n                 check_mode: CheckMode = CheckMode.CHECK_ALL,\n                 inject_loc: SeedInjectLoc = SeedInjectLoc.STDIN,\n                 sast_report: PathLike = None,\n                 p_argv: List[str] = None,\n                 memory_threshold: int = 85,\n                 start_quorum: int = 0,\n                 filter_inputs: bool = False,\n                 stream: bool = False,\n                 replay_threads: int = 4):\n        super(PastisBroker, self).__init__()\n\n        # Initialize workspace\n        self.workspace = Workspace(Path(workspace))\n        params = {\"binaries_dir\": str(Path(binaries_dir).absolute()),\n                  \"broker_mode\": broker_mode.name,\n                  \"check_mode\": check_mode.name,\n                  \"inject_loc\": inject_loc.name,\n                  \"argvs\": p_argv}\n        self.workspace.initialize_runtime(binaries_dir, params)\n\n        self._configure_logging()\n\n        # Register all agent callbacks\n        self._register_all()\n\n        # Init internal state\n        self.broker_mode = broker_mode\n        self.ck_mode = check_mode\n        self.inject = inject_loc\n        self.argv = [] if p_argv is None else p_argv\n        self.engines_args = {}\n        self.engines = {}  # name->FuzzingEngineDescriptor\n\n        # for slicing mode (otherwise not used)\n        self._slicing_ongoing = {}  # Program -> {Addr -> [cli]}\n\n        # Initialize availables binaries\n        self.programs = {}  # Tuple[(Platform, Arch)] -> List[BinaryPackage]\n        self._find_binaries(binaries_dir)\n\n        # Klocwork informations\n        self.sast_report = None\n        if sast_report:\n            self.initialize_sast_report(sast_report)\n\n        # Client infos\n        self.clients = {}   # bytes -> Client\n        self._cur_id = 0\n\n        # Runtime infos\n        self._running = False\n        self._seed_pool = {}  # Seed bytes -> SeedType\n        self._init_seed_pool = {}  # Used for NO_TRANSMIT mode\n        self._start_time = None\n        self._stop = False\n\n        # Load the workspace seeds\n        self._load_workspace()\n\n        # Create the stat manager\n        self.statmanager = StatManager(self.workspace)\n\n        # Watchdog to monitor RAM usage\n        self.watchdog = None\n        self._threshold = memory_threshold # percent\n\n        # startup quorum\n        self._startup_quorum = start_quorum\n        self._current_quorum = 0\n\n        # Proxy feature\n        self._proxy = None\n        self._proxy_cli = None\n        self._proxy_start_signal = False\n        self._proxy_seed_queue = queue.Queue()\n\n        # Coverage + filtering feature\n        self._coverage_manager = None\n        self.filter_inputs: bool = filter_inputs\n        if filter_inputs or stream:\n            if (path := self.find_vanilla_binary()) is not None:  # Find an executable suitable for coverage\n                logging.info(f\"Coverage binary: {path}\")\n                stream_file = self.workspace.coverage_history if stream else \"\"\n                self._coverage_manager = CoverageManager(replay_threads, filter_inputs, path, self.argv, self.inject, stream_file)\n            else:\n                logging.warning(\"filtering or stream enabled but cannot find vanilla binary\")\n\n\n    def find_vanilla_binary(self) -> Optional[str]:\n        \"\"\"\n        Find a binary without instrumentation to be used for coverage\n        computation. It also has to match local architecture.\n        :return: Path to the progam\n        \"\"\"\n        local_arch = get_local_architecture()\n        for pkg in self.programs.get((Platform.LINUX, local_arch)):\n            path = str(pkg.executable_path.absolute())\n            p = lief.parse(str(path))\n            ok = True\n            for f in p.functions:\n                if \"hfuzz_\" in f.name or \"__afl_\" in f.name or \"__gcov_\" in f.name or \"__asan_\" in f.name:\n                    ok = False\n                    break\n            if ok:\n                return path\n        return None\n\n\n    def load_engine_addon(self, py_module: str) -> bool:\n        desc = load_engine_descriptor(py_module)\n        if desc is not None:\n            self.engines[desc.NAME] = desc\n            self.engines_args[desc.NAME] = []\n            return True\n        else:\n            return False\n\n    def initialize_sast_report(self, report: PathLike):\n        self.sast_report = SASTReport.from_file(report)\n        self.workspace.add_sast_report(self.sast_report)\n        self.workspace.initialize_alert_corpus(self.sast_report)\n\n    @property\n    def running(self) -> bool:\n        return self._running\n\n    def iter_other_clients(self, cli_id: bytes) -> Generator[PastisClient, None, None]:\n        \"\"\"\n        Generator of all clients but the one given in parameter\n\n        :param cli_id: netid of the client to ignore\n        :return: Generator of PastisClient object\n        \"\"\"\n        for c in self.clients.values():\n            if c.netid != cli_id:\n                yield c\n\n    def new_uid(self) -> int:\n        \"\"\"\n        Generate a new unique id for a client (int)\n\n        :return: int, unique (in an execution)\n        :return: int, unique (in an execution)\n        \"\"\"\n        v = self._cur_id\n        self._cur_id += 1\n        return v\n\n    def _register_all(self):\n        self.register_seed_callback(self.seed_received)\n        self.register_hello_callback(self.hello_received)\n        self.register_log_callback(self.log_received)\n        self.register_telemetry_callback(self.telemetry_received)\n        self.register_stop_coverage_callback(self.stop_coverage_received)\n        self.register_data_callback(self.data_received)\n\n    def get_client(self, cli_id: bytes) -> Optional[PastisClient]:\n        cli = self.clients.get(cli_id)\n        if not cli:\n            logging.warning(f\"client '{cli_id}' unknown (send stop)\")\n            self.send_stop(cli_id)\n        return cli\n\n    def kick_client(self, cli_id: bytes) -> None:\n        cli = self.clients.pop(cli_id)  # pop it from client list\n        logging.info(f\"kick client: {cli.strid}\")\n        self.send_stop(cli_id)\n\n    def seed_received(self, cli_id: bytes, typ: SeedType, seed: bytes):\n        cli = self.get_client(cli_id)\n        if not cli:\n            return\n        is_new = seed not in self._seed_pool\n        h = md5(seed).hexdigest()\n\n        # Show log message and save seed to file\n        self.statmanager.update_seed_stat(cli, typ)  # Add info only if new\n        cli.log(LogLevel.INFO, f\"seed {h} [{self._colored_seed_type(typ)}][{self._colored_seed_newness(is_new)}]\")\n        cli.add_own_seed(seed)  # Add seed in client's seed\n        fname = self.write_seed(typ, cli.strid, seed) # Write seed to file\n\n        if is_new:\n            self._seed_pool[seed] = typ  # Save it in the local pool\n\n            if self._coverage_manager:\n                sp = fname.split(\"_\")\n                covi = ClientInput(seed, \"\", f\"{sp[0]}_{sp[1]}\", sp[2], h, fname, typ, cli.netid, cli.strid, \"GRANTED\", \"\", -1, [])\n                self._coverage_manager.push_input(covi)\n\n            if not self.filter_inputs:  # If seed are not filtered send it right away\n                self.seed_granted(cli.netid, typ, seed)\n        else:\n            logging.debug(f\"receive duplicate seed {h} by {cli.strid}\")\n\n\n    def seed_granted(self, cli_id: bytes, typ: SeedType, seed: bytes):\n        # Iterate on all clients and send it to whomever never received it\n        if self.broker_mode == BrokingMode.FULL:\n            self.send_seed_to_all_others(cli_id, typ, seed)\n\n        if self.is_proxied:  # Forward it to the proxy\n            self._proxy.send_seed(typ, seed)\n\n    def send_seed_to_all_others(self, origin_id: bytes, typ: SeedType, seed: bytes) -> None:\n        for c in self.iter_other_clients(origin_id):\n            if c.is_new_seed(seed):\n                self.send_seed(c.netid, typ, seed)  # send the seed to the client\n                c.add_peer_seed(seed)  # Add it in its list of seed\n\n    def add_seed_file(self, file: PathLike, initial: bool = False) -> None:\n        p = Path(file)\n        logging.info(f\"Add seed {p.name} in pool\")\n        # Save seed in the workspace\n        self.workspace.save_seed_file(SeedType.INPUT, p, initial)\n\n        seed = p.read_bytes()\n        self._seed_pool[seed] = SeedType.INPUT\n        if initial:\n            self._init_seed_pool[seed] = SeedType.INPUT\n\n    def write_seed(self, typ: SeedType, cli_id: str, seed: bytes) -> str:\n        fname = self.mk_input_name(cli_id, seed)\n        self.workspace.save_seed(typ, fname, seed)\n        return fname\n\n    def mk_input_name(self, cli_id: str, seed: bytes) -> str:\n        t = time.strftime(\"%Y-%m-%d_%H:%M:%S\", time.localtime())\n        elapsed = str(datetime.timedelta(seconds=time.time() - self._start_time)).replace(\" day, \", \"d:\").replace(\" days, \", \"d:\")\n        return f\"{t}_{elapsed}_{cli_id}_{md5(seed).hexdigest()}.cov\"\n\n    def hello_received(self, cli_id: bytes, engines: List[FuzzingEngineInfo], arch: Arch, cpus: int, memory: int, hostname: str, platform: Platform):\n        uid = self.new_uid()\n        client = PastisClient(uid, cli_id, engines, arch, cpus, memory, hostname, platform)\n        logging.info(f\"[{client.strid}] [HELLO] Name:{hostname} Arch:{arch.name} engines:{[x.name for x in engines]} (cpu:{cpus}, mem:{memory})\")\n        self.clients[client.netid] = client\n\n        # Load engines if they are not (lazy loading)\n        for eng in engines:\n            if eng.name not in self.engines:\n                self.load_engine_addon(eng.pymodule)\n\n        if self.running:  # A client is coming in the middle of a session\n            if self._startup_quorum:\n                self._current_quorum += 1\n                if self._current_quorum >= self._startup_quorum:\n                    logging.info(\n                        f\"client number quorum ({self._current_quorum}/{self._startup_quorum}) reached, start all of them\")\n                    self.start_pending_clients()  # start all pending clients (including the one triggering this func)\n                else:\n                    # Put the client on-hold to wait for quorum\n                    logging.info(f\"client {client.strid} on-hold, wait quorum ({self._current_quorum}/{self._startup_quorum})\")\n            else:  # there is no quorum so start immediately\n                self.start_client_and_send_corpus(client)\n        else:\n            pass  # Client connection is kept in clients dict for later\n\n    def _transmit_pool(self, client, pool) -> None:\n        for seed, typ in pool.items():\n            self.send_seed(client.netid, typ, seed)  # necessarily a new seed\n            client.add_peer_seed(seed)  # Add it in its list of seed\n\n    def log_received(self, cli_id: bytes, level: LogLevel, message: str):\n        client = self.get_client(cli_id)\n        if not client:\n            return\n        client.log(level, message)\n\n    def telemetry_received(self, cli_id: bytes,\n                           _: State = None, exec_per_sec: int = None, total_exec: int = None,\n                           cycle: int = None, timeout: int = None, coverage_block: int = None, coverage_edge: int = None,\n                           coverage_path: int = None, last_cov_update: int = None):\n        client = self.get_client(cli_id)\n        if not client:\n            return\n        # NOTE: ignore state (shall we do something of it?)\n        args = [('-' if not x else x) for x in [exec_per_sec, total_exec, cycle, timeout, coverage_block, coverage_edge, coverage_path, last_cov_update]]\n        client.log(LogLevel.INFO, \"exec/s:{} tot_exec:{} cycle:{} To:{} CovB:{} CovE:{} CovP:{} last_up:{}\".format(*args))\n\n        # Update all stats in the stat manager (for later UI update)\n        self.statmanager.set_exec_per_sec(client, exec_per_sec)\n        self.statmanager.set_total_exec(client, total_exec)\n        self.statmanager.set_cycle(client, cycle)\n        self.statmanager.set_timeout(client, timeout)\n        self.statmanager.set_coverage_block(client, coverage_block)\n        self.statmanager.set_coverage_edge(client, coverage_edge)\n        self.statmanager.set_coverage_path(client, coverage_path)\n        self.statmanager.set_last_coverage_update(client, last_cov_update)\n        self.statmanager.update_telemetry_client(client)\n        # NOTE: Send an update signal for future UI ?\n\n    def stop_coverage_received(self, cli_id: bytes):\n        client = self.get_client(cli_id)\n        if not client:\n            return\n\n        if self.ck_mode == CheckMode.ALERT_ONE:  # Start it on another target\n            addr = client.target\n            s = \"validated \" if client.target_validated else \"is stuck for \"\n            logging.info(f\"[{client.strid}] [STOP_COVERAGE] client {s} 0x{addr:x} address (launch another target)\")\n            clis = self._slicing_ongoing[client.package_name].pop(addr)  # pop the address definitely\n            self.relaunch_clients(clis)  # relaunch all clients on a new target\n        else:\n            logging.info(f\"[{client.strid}] [STOP_COVERAGE]: restart client\")\n            self.relaunch_clients([client])  # restart the client\n\n    def data_received(self,  cli_id: bytes, data: str):\n        client = self.get_client(cli_id)\n        if not client:\n            return\n        first_cov, first_val = False, False\n\n        if not self.sast_report:\n            logging.warning(\"Data received while no SAST report is loaded (drop data)\")\n            return\n\n        alert_data = AlertData.from_json(data)\n        alert = self.sast_report.alerts[alert_data.id]\n\n        if not alert.validated and alert_data.validated:\n            logging.info(f\"[{client.strid}] First to validate {alert}\")\n            alert.validated = alert_data.validated\n            self.sast_report.write_csv(self.workspace.csv_result_file)  # Update CSV to keep it updated\n            first_val = True\n\n            if self.ck_mode == CheckMode.ALERT_ONE:\n                client.target_validated = True\n                # Note: Do not relaunch the client now, wait for him to send stop_coverage\n\n        # Need to be after validated because alert.covered = True (will also set validated)\n        if not alert.covered and alert_data.covered:\n            logging.info(f\"[{client.strid}] First to cover {alert}\")\n            alert.covered = alert_data.covered\n            self.sast_report.write_csv(self.workspace.csv_result_file)  # Update CSV to keep it updated\n            first_cov = True\n\n        # Update clients of and stats\n        client.add_covered_alert(alert.id, alert.covered, first_cov, alert.validated, first_val)\n\n        # Save systematically the AlertData received\n        self._save_alert_seed(client, alert_data)  # Also save seed in separate folder\n\n        if first_cov or first_val:\n            cov, val, tot = self.sast_report.get_stats()\n            logging.info(f\"SAST results updated: defaults: [covered:{cov}/{tot}] [validated:{val}/{tot}]\")\n\n        # If all alerts are validated send a stop to everyone\n        if self.sast_report.all_alerts_validated():\n            self.stop_broker()\n\n    def relaunch_clients(self, clients):\n        for cli in clients:\n            logging.info(\n                f\"Launch client {cli.strid} as its targeting an address that has just been validated\")\n            self.start_client_and_send_corpus(cli)\n\n    def stop_broker(self):\n        for client in self.clients.values():\n            logging.info(f\"Send stop to {client.strid}\")\n            self.send_stop(client.netid)\n        self._stop = True\n\n        # Stop coverage manager if any\n        if self._coverage_manager:\n            self._coverage_manager.stop()\n\n        # Call the statmanager to wrap-up values\n        self.statmanager.post_execution(list(self.clients.values()), self.workspace)\n\n        if self.is_proxied:\n            self._proxy.stop()\n\n        if self.sast_report:  # If a SAST report was loaded\n            # Write the final CSV in the workspace\n            self.sast_report.write_csv(self.workspace.csv_result_file)\n            # And also re-write the Klocwork report (that also contains resutls)\n            self.sast_report.write(self.workspace.sast_report_file)\n\n    def start_pending_clients(self) -> None:\n        \"\"\"\n        Start clients that connected but for which we did not yet send a response.\n        \"\"\"\n        # Send the start message to all clients (already connected)\n        for c in self.clients.values():\n            if not c.is_running():\n                self.start_client_and_send_corpus(c)\n\n    def start_client_and_send_corpus(self, client: PastisClient) -> None:\n        self.start_client(client)\n        # Iterate all the seed pool and send it to the client\n        if self.broker_mode == BrokingMode.FULL:\n            self._transmit_pool(client, self._seed_pool)\n        elif self.broker_mode == BrokingMode.NO_TRANSMIT:\n            self._transmit_pool(client, self._init_seed_pool)\n\n    def start_client(self, client: PastisClient):\n        engine = None\n        exmode = ExecMode.SINGLE_EXEC\n        fuzzmode = FuzzMode.INSTRUMENTED\n        engine_args = None\n        package = covmode = fuzzmod = None\n        engines = Counter({e: 0 for e in self.engines})\n        engines.update(c.engine.NAME for c in self.clients.values() if c.is_running())  # Count instances of each engine running\n        for eng, _ in engines.most_common()[::-1]:\n            eng_desc = self.engines[eng]\n            # If the engine is not supported by the client continue\n            if not client.is_supported_engine(eng_desc):\n                continue\n\n            # Try finding a suitable binary for the current engine and the client arch\n            programs: List[BinaryPackage] = self.programs.get((client.platform, client.arch))\n            package = None\n            exmode = None\n            fuzzmod = FuzzMode.AUTO\n            for p in programs:\n                res, xmod, fmod = eng_desc.accept_file(p.executable_path)  # Iterate all files on that engine descriptor to check it accept it\n                if res:\n                    if exmode:\n                        if exmode == ExecMode.SINGLE_EXEC and xmod == ExecMode.PERSISTENT:  # persistent supersede single_exec\n                            package, exmode, fuzzmod = p, xmod, fmod\n                        else:\n                            if fuzzmod == FuzzMode.BINARY_ONLY and fmod == FuzzMode.INSTRUMENTED:  # instrumented supersede binary only\n                                package, exmode, fuzzmod = p, xmod, fmod\n                            else:\n                                pass  # Program is suitable but we already had found a PERSISTENT one\n                    else:\n                        package, exmode, fuzzmod = p, xmod, fmod  # first suitable program found\n\n            if not package:  # If still no program was found continue iterating engines\n                continue\n\n            # Valid engine and suitable program found\n            engine = eng_desc\n\n            # Find a configuration to use for that engine\n            engine_args = self._find_configuration(engine)\n            covmode = self._find_coverage_mode(engine, engine_args)\n\n            # If got here a suitable program has been found just break loop\n            break\n\n        if engine is None or package is None:\n            logging.critical(f\"No suitable engine or program was found for client {client.strid} {client.engines}\")\n            return\n\n        if self.ck_mode == CheckMode.ALERT_ONE:\n            if package.is_quokka():\n                targets = self._slicing_ongoing[package.name]\n                sorted_targets = sorted(targets, key=lambda k: len(targets[k]), reverse=False)  # sort alert addresses by number of client instances working on it\n                if sorted_targets:\n                    addr = sorted_targets[0]\n                    targets[addr].append(client)\n                    client.target = addr   # keep the target on which the client is working on\n                    client.target_validated = False\n                    # Now twist the config to transmit it to the client\n                    engine_args = engine.config_class.new() if engine_args is None else engine_args\n                    engine_args.set_target(addr)\n                    logging.info(f\"will start client {client.strid} to target 0x{addr:x}\")\n                else:\n                    logging.error(f\"No alert target for binary package {package.name}\")\n            else:\n                logging.error(f\"In mode {self.ck_mode} but the binary package is not a QBinExport\")\n                return\n\n        # Update internal client info and send him the message\n        engine_args_str = engine_args.to_str() if engine_args else \"\"\n        logging.info(f\"send start client {client.id}: {package.name} [{engine.NAME}, {covmode.name}, {fuzzmod.name}, {exmode.name}]\")\n        client.set_running(package.name, engine, covmode, exmode, self.ck_mode, engine_args_str)\n        client.configure_logger(self.workspace.log_directory, random.choice(COLORS))  # Assign custom color client\n\n        self.send_start(client.netid,\n                        package.name,\n                        package.executable_path if package.is_standalone() else package.make_package(),\n                        self.argv,\n                        exmode,\n                        fuzzmod,\n                        self.ck_mode,\n                        covmode,\n                        FuzzingEngineInfo(engine.NAME, engine.VERSION, \"\"),\n                        engine_args_str,\n                        self.inject,\n                        self.sast_report.to_json() if self.sast_report else b\"\")\n\n    def _find_configuration(self, engine: FuzzingEngineDescriptor) -> Optional[EngineConfiguration]:\n        \"\"\"\n        Find a configuration for the engine. It will iterate all configuration\n        available or automatically balance de configuration if there is multiple of\n        them\n        :param engine:\n        :return:\n        \"\"\"\n        confs = self.engines_args[engine.NAME]\n        if confs:\n            if len(confs) == 1:\n                return confs[0]\n            else:\n                conf = confs.pop(0)\n                confs.append(conf)  # Rotate the configuration\n                return conf\n        else:\n            return None\n\n    def _find_coverage_mode(self, engine: FuzzingEngineDescriptor, conf: Union[EngineConfiguration]) -> CoverageMode:\n\n        # Get coverage modes supported by the engine\n        supported_mods = engine.supported_coverage_strategies()\n\n        # Now that program have been found select coverage strategy\n        if len(supported_mods) > 1:\n\n            if conf:  # In the case we wanted to launch engines with specific configuration\n                return conf.get_coverage_mode()    # return the appropriate coverage mode in that configuration\n\n            else:\n                # auto-balance the CoverageMode instances\n                covs = Counter({c: 0 for c in supported_mods})\n                # Count how many times each coverage strategies have been launched\n                covs.update(x.coverage_mode for x in self.clients.values() if x.is_running() and x.engine == engine)\n\n                if sum(covs.values()) == 0:  # No configuration has been triggered yet\n                    if CoverageMode.EDGE in supported_mods:  # launch preferably edge first\n                        return CoverageMode.EDGE\n                    else:\n                        return supported_mods[0]  # Return first supported mode\n\n                reverted = {v: k for k, v in covs.items()}  # Revert dictionnary to have freq -> covmodes\n                return reverted[min(reverted)]  # Select mode if least instances\n\n        else:\n            return supported_mods[0]\n\n    def start(self, running: bool = True):\n        super(PastisBroker, self).start()  # Start the listening thread\n        self._start_time = time.time()\n        self._running = running\n        self.workspace.status = WorkspaceStatus.RUNNING\n        logging.info(\"start broking\")\n\n        if self._coverage_manager:  # if it has been instanciated start it\n            self._coverage_manager.start()\n            for seed in self._init_seed_pool.keys():  # Push initial corpus to set baseline coverage\n                fname = self.mk_input_name(\"INITIAL\", seed)\n                sp = fname.split(\"_\")\n                covi = ClientInput(seed, \"\", f\"{sp[0]}_{sp[1]}\", sp[2], sp[4], fname, SeedType.INPUT, b\"INITIAL\", \"INITIAL\", \"GRANTED\", \"\", -1, [])\n                self._coverage_manager.push_input(covi)\n\n        if self.is_proxied and self._proxy_cli:\n            self._running = False  # disable running wait start broker\n            self._proxy.send_hello([self._proxy_cli])\n        else:\n            # Send the start message to all clients (already connected)\n            if self._running:  # If we want to run now (cmdline mode)\n                self.start_pending_clients()\n\n    def run(self, timeout: int = None):\n        self.start()\n        last_t = time.time()\n\n        # Start infinite loop\n        try:\n            while True:\n                time.sleep(0.05)\n                t = time.time()\n\n                # Check if the campaign have to be stopped\n                if timeout is not None:\n                    if t > (self._start_time + timeout):\n                        logging.info(\"Campaign timeout reached, stop campaign.\")\n                        self._stop = True\n\n                if t > (last_t + 60):  # only check every minute\n                    last_t = t\n                    if not self._check_memory_usage():\n                        # The machine starts being overloaded\n                        # For security kill triton instance\n                        for cli in list(self.clients.values()):\n                            if cli.engine.SHORT_NAME == \"TT\":  # is triton\n                                self.kick_client(cli.netid)\n\n                # if inputs are filtered. Get granted inputs and forward them to appropriate clients\n                if self.filter_inputs:\n                    for item in self._coverage_manager.iter_granted_inputs():\n                        self.seed_granted(item.fuzzer_id, item.seed_status, item.content)\n\n                # Check if we received the start signal from the proxy-master\n                if self._proxy_start_signal:\n                    self._proxy_start_signal = False\n                    self.start_pending_clients()\n\n                # Check if there are seed coming from the proxy-master to forward to clients\n                if not self._proxy_seed_queue.empty():\n                    try:\n                        while True:\n                            origin, typ, seed = self._proxy_seed_queue.get_nowait()\n                            self.send_seed_to_all_others(origin, typ, seed)\n                    except queue.Empty:\n                        pass\n\n                if self._stop:\n                    logging.info(\"broker terminate\")\n                    break\n        except KeyboardInterrupt:\n            logging.info(\"stop required (Ctrl+C)\")\n        self.workspace.status = WorkspaceStatus.FINISHED\n        self.stop_broker()\n\n    def _find_binaries(self, binaries_dir) -> None:\n        \"\"\"\n        Iterate the whole directory to find suitables binaries in the\n        various architecture, and compiled for the various engines.\n\n        :param binaries_dir: directory containing the various binaries\n        :return: None\n        \"\"\"\n        d = Path(binaries_dir)\n        for file in d.iterdir():\n            if file.is_file():\n                try:\n                    pkg = BinaryPackage.auto(file)  # try creating a package\n                except ValueError:  # if not an executable\n                    continue\n\n                if pkg is None:\n                    continue\n\n                # Check that we can find a Quokka file associated otherwise reject it\n                if self.ck_mode == CheckMode.ALERT_ONE:\n                    if pkg.is_quokka():\n                        try:\n                            # Instanciate the Quokka Program and monkey patch object\n                            quokka_prog = QuokkaProgram(pkg.quokka, pkg.executable_path)\n                            f = quokka_prog.get_function(\"__sast_alert_placeholder\")\n                            self._slicing_ongoing[file.name] = {x: [] for x in quokka_prog.get_caller_instructions(f)}\n                        except ValueError:\n                            logging.warning(f\"can't find placeholder file in {file.name}, thus ignores it.\")\n                            continue\n                    else:\n                        logging.warning(f\"{file.name} executable found but no QBinExport file associated (ignores it)\")\n                        continue\n\n                logging.info(f\"new binary detected [{pkg.platform.name}, {pkg.arch.name}]: {file}\")\n\n                # Add it in the internal structure\n                data = (pkg.platform, pkg.arch)\n                data2 = (Platform.ANY, pkg.arch)\n                if data not in self.programs:\n                    self.programs[data] = []\n                if data2 not in self.programs:\n                    self.programs[data2] = []\n                self.programs[data].append(pkg)\n                self.programs[data2].append(pkg)  # Also add an entry for any platform\n\n    def _load_workspace(self):\n        \"\"\" Load all the seeds in the workspace\"\"\"\n        for typ in list(SeedType):  # iter seed types: input, crash, hang..\n            for file in self.workspace.iter_corpus_directory(typ):\n                logging.debug(f\"Load seed in pool: {file.name}\")\n                content = file.read_bytes()\n                self._seed_pool[content] = typ\n        # TODO: Also dumping the current state to a file in case\n        # TODO: of exit. And being able to reload it. (not to resend all seeds to clients)\n\n    def add_engine_configuration(self, name: str, config_file: PathLike):\n        if name in self.engines_args:\n            engine = self.engines[name]\n            conf = engine.config_class.from_file(config_file)\n            self.engines_args[name].append(conf)\n        else:\n            logging.error(f\"can't find engine {name} (shall preload it to add a configuration)\")\n\n    def _configure_logging(self):\n        hldr = logging.FileHandler(self.workspace.broker_log_file)\n        hldr.setLevel(logging.root.level)\n        hldr.setFormatter(logging.Formatter(\"%(asctime)s [%(name)s] [%(levelname)s]: %(message)s\"))\n        logging.root.addHandler(hldr)\n\n    def _colored_seed_type(self, typ: SeedType) -> str:\n        mapper = {SeedType.INPUT: Bcolors.OKBLUE,\n                  SeedType.HANG: Bcolors.WARNING,\n                  SeedType.CRASH: Bcolors.FAIL}\n        return mapper[typ]+typ.name+Bcolors.ENDC\n\n    def _colored_seed_newness(self, is_new: bool) -> str:\n        col, text = {True: (Bcolors.OKGREEN, \"NEW\"),\n                     False: (Bcolors.WARNING, \"DUP\")}[is_new]\n        return col+text+Bcolors.ENDC\n\n    def _save_alert_seed(self, from_cli: PastisClient, alert: AlertData):\n        t = time.strftime(\"%Y-%m-%d_%H:%M:%S\", time.localtime())\n        elapsed = str(datetime.timedelta(seconds=time.time()-self._start_time)).replace(\" day, \", \"d:\").replace(\" days, \", \"d:\")\n        fname = f\"{t}_{elapsed}_{from_cli.strid}_{md5(alert.seed).hexdigest()}-{'CRASH' if alert.validated else 'COVERAGE'}.cov\"\n        logging.debug(f\"Save alert  [{alert.id}] file: {fname}\")\n        self.workspace.save_alert_seed(alert.id, fname, alert.seed)\n\n    def _check_memory_usage(self) -> bool:\n        mem = psutil.virtual_memory()\n        logging.info(f\"RAM usage: {mem.percent:.2f}%\")\n        if mem.percent >= self._threshold:\n            logging.warning(f\"Threshold reached: {mem.percent}%\")\n            return False\n        return True\n\n    @property\n    def is_proxied(self) -> bool:\n        \"\"\"\n        Get if the broker as a proxy to another broker.\n        \"\"\"\n        return self._proxy is not None\n\n    def set_proxy(self, ip: str, port: int, py_module: str) -> bool:\n        self._proxy = ClientAgent()\n\n        # Load the engine info to impersonate\n        desc = load_engine_descriptor(py_module)\n        if desc is None:\n            logging.error(\"Cannot load FuzzingEngineDescriptor\")\n            return False\n        else:\n            self._proxy_cli = FuzzingEngineInfo(desc.NAME, desc.VERSION, py_module)\n\n        self._proxy.register_start_callback(self._proxy_start_received)\n        self._proxy.register_seed_callback(self._proxy_seed_received)\n        self._proxy.register_stop_callback(self._proxy_stop_received)\n\n        logging.info(f\"connect to broker {ip}:{port}\")\n        self._proxy.connect(ip, port)\n        self._proxy.start()\n\n    def _proxy_start_received(self, fname: str, binary: bytes, engine: FuzzingEngineInfo, exmode: ExecMode,\n                              fuzzmode: FuzzMode, chkmode: CheckMode, covmode: CoverageMode, seed_inj: SeedInjectLoc,\n                              engine_args: str, argv: List[str], sast_report: str = None):\n        # FIXME: Use parameters received\n        logging.info(\"[PROXY] start received !\")\n        self._running = True\n        # if self._running:\n        #     self.start_pending_clients()\n\n    def _proxy_seed_received(self, typ: SeedType, seed: bytes):\n        # Forward the seed to underlying clients\n        logging.info(f\"[PROXY] seed {typ.name} received forward to agents\")\n\n        # Save the seed locally\n        self.write_seed(typ, \"PROXY\", seed)\n        self._seed_pool[seed] = typ  # add it to the pool\n        self._init_seed_pool[seed] = typ  # also consider it as initial corpus\n\n        # Forward it to all clients\n        self._proxy_seed_queue.put((b\"PROXY\", typ, seed))\n        # self.send_seed_to_all_others(b\"PROXY\", typ, seed)\n\n    def _proxy_stop_received(self):\n        logging.info(f\"[PROXY] stop received!\")\n        self._stop = True", ""]}
{"filename": "pastisbroker/__init__.py", "chunked_list": ["from .broker import PastisBroker, BrokingMode\n\n__version__ = \"1.0.0\"\n"]}
{"filename": "pastisbroker/coverage.py", "chunked_list": ["#!/usr/bin/env python3\nimport time\nimport logging\nimport tempfile\nimport os\nimport subprocess\nfrom typing import Generator\nfrom pathlib import Path\nimport queue\nimport csv", "import queue\nimport csv\nfrom dataclasses import dataclass\nfrom threading import Thread\nfrom multiprocessing import Queue, Manager\nfrom multiprocessing.pool import Pool\n\nfrom libpastis.types import SeedType, SeedInjectLoc\n\n# tritondse imports", "\n# tritondse imports\nfrom tritondse import GlobalCoverage, CoverageSingleRun, CoverageStrategy, BranchSolvingStrategy\nfrom tritondse.trace import QBDITrace, TraceException\n\nfrom pastisbroker.utils import Bcolors, mk_color\n\n\n@dataclass\nclass ClientInput:\n    content: bytes          # Content of the input\n    log_time: str           # Time the log has been generated\n    recv_time: str          # Time the input has been received\n    elapsed: str            # Elapsed time since the begining\n    hash: str               # Input hash\n    path: str               # Input file path\n    seed_status: SeedType   # Status of the seed\n    fuzzer_id: bytes        # Fuzzer ID\n    fuzzer_name: str        # Fuzzer name\n    broker_status: str      # Status in: DUPLICATE, DROPPED, GRANTED\n    replay_status: str      # Status in: OK, TRACE_EXCEPTION, FAIL\n    replay_time: float      # Time taken for the replay\n    new_coverage: list[tuple[int, int]]  # New items covered", "@dataclass\nclass ClientInput:\n    content: bytes          # Content of the input\n    log_time: str           # Time the log has been generated\n    recv_time: str          # Time the input has been received\n    elapsed: str            # Elapsed time since the begining\n    hash: str               # Input hash\n    path: str               # Input file path\n    seed_status: SeedType   # Status of the seed\n    fuzzer_id: bytes        # Fuzzer ID\n    fuzzer_name: str        # Fuzzer name\n    broker_status: str      # Status in: DUPLICATE, DROPPED, GRANTED\n    replay_status: str      # Status in: OK, TRACE_EXCEPTION, FAIL\n    replay_time: float      # Time taken for the replay\n    new_coverage: list[tuple[int, int]]  # New items covered", "\n\nclass CoverageManager(object):\n\n    ARGV_PLACEHOLDER = \"@@\"\n    STRATEGY = CoverageStrategy.EDGE\n\n    def __init__(self, pool_size: int, filter: bool, program: str, args: list[str], inj_loc: SeedInjectLoc, stream_file: str = \"\"):\n        # Base info for replay\n        self.pool_size = pool_size\n        self.filter_enabled = filter\n        self.program = str(program)\n        self.args = args\n        self.inj_loc = inj_loc\n\n        # Coverage and messaging attributes\n        self._coverage = GlobalCoverage(self.STRATEGY, BranchSolvingStrategy.ALL_NOT_COVERED)\n        self._manager = Manager()\n        self.input_queue = self._manager.Queue()\n        self.cov_queue = self._manager.Queue()\n        self.granted_queue = self._manager.Queue()\n\n        # Pool of workers\n        self.pool = Pool(self.pool_size)\n        self._running = False\n        self.cov_worker = Thread(name=\"[coverage_worker]\", target=self.coverage_worker)\n\n        # stats\n        self.seeds_accepted, self.seeds_submitted = 0, 0\n        self.cli_stats = {}\n\n        # Streaming\n        if stream_file:\n            self.stream_file = open(stream_file, \"a\")\n            self.csv = csv.writer(self.stream_file)\n        else:\n            self.stream_file, self.csv = None, None\n\n\n    def start(self) -> None:\n        \"\"\"\n        Start all the workers\n        \"\"\"\n        # First start the coverage worker\n        self._running = True\n        self.cov_worker.start()\n        logging.info(\"Starting coverage manager\")\n\n        for work_id in range(self.pool_size):\n            self.pool.apply_async(self.worker, (self.input_queue, self.cov_queue, self.program, self.args, self.inj_loc))\n\n    def stop(self) -> None:\n        self._running = False\n        self.cov_worker.join()\n        self.pool.terminate()\n\n    def push_input(self, cli_input: ClientInput) -> None:\n        \"\"\" Push the input in the \"\"\"\n        cli_input.log_time = time.strftime(\"%Y-%m-%d_%H:%M:%S\", time.localtime())\n        # logging.info(f\"push input {str(cli_input)[:50]}\")\n\n        # Update stats\n        self.seeds_submitted += 1\n        if cli_input.fuzzer_id in self.cli_stats:\n            self.cli_stats[cli_input.fuzzer_id][0] += 1\n        else:\n            self.cli_stats[cli_input.fuzzer_id] = [1, 0]\n\n        self.input_queue.put(cli_input)\n\n    def iter_granted_inputs(self) -> Generator[ClientInput, None, None]:\n        try:\n            while True:\n                yield self.granted_queue.get_nowait()\n        except queue.Empty:\n            pass\n\n    @staticmethod\n    def worker_sleep(q, n) -> None:\n        \"\"\"\n        worker thread that unstack inputs and replay them.\n        \"\"\"\n        time.sleep(n)\n        q.put_nowait(n)\n        return n\n\n    def add_item_coverage_stream(self, item: ClientInput) -> None:\n        if self.stream_file:  # Stream enabled\n            self.csv.writerow([\n                item.log_time,\n                item.recv_time,\n                item.elapsed,\n                item.hash,\n                item.path,\n                item.seed_status.name,\n                item.fuzzer_name,\n                item.broker_status,\n                item.replay_status,\n                f\"{item.replay_time:.2f}\",\n                item.new_coverage\n            ])\n            self.stream_file.flush()\n\n    def coverage_worker(self):\n        while self._running:\n            try:\n                item, cov_file = self.cov_queue.get(timeout=0.5)\n                # logging.info(\"Coverage worker fetch item\")\n                new_items = []\n                try:\n                    coverage: CoverageSingleRun = QBDITrace.from_file(cov_file).coverage\n                    if self._coverage.improve_coverage(coverage):\n                        self.cli_stats[item.fuzzer_id][1] += 1  # input accepted\n                        self.seeds_accepted += 1\n\n                        # Get newly covered items (and put them in the stream queue\n\n                        new_items = coverage.difference(self._coverage)\n\n                        item.new_coverage = list(new_items)\n\n                        # logging.info(f\"seed {item.hash}  ({item.fuzzer_name}) [replay:{}][status:{}] ({len(new_items)} new edges)\")\n\n                        # Update the global coverage\n                        self._coverage.merge(coverage)\n\n                        if item.fuzzer_name != \"INITIAL\":  # if not initial corpus and granted\n                            self.granted_queue.put(item)\n\n                    else:\n                        item.broker_status = \"DROPPED\" if self.filter_enabled else \"GRANTED\"\n                        # logging.info(f\"seed {item.hash} ({item.seed_status.name}) of {item.fuzzer_name} rejected (do not improve coverage)\")\n\n                    # Remove the coverage file\n                    os.unlink(cov_file)\n                except FileNotFoundError:\n                    if item.seed_status == SeedType.INPUT:\n                        logging.warning(f\"seed {item.hash}({item.seed_status}) can't load coverage file (maybe had crashed?)\")\n                    else:\n                        logging.info(f\"seed {item.hash}({item.seed_status}) cannot get coverage (normal..)\")\n                    # Grant input\n                    self.seeds_accepted += 1\n                    self.granted_queue.put(item)\n\n                logging.info(f\"seed {item.hash} ({item.fuzzer_name}) [replay:{self.mk_rpl_status(item.replay_status)}][status:{self.mk_broker_status(item.broker_status, bool(new_items))}] ({len(new_items)} new edges)\")\n                # Regardless if it was a success or not log it\n                self.add_item_coverage_stream(item)\n            except queue.Empty:\n                pass\n            except KeyboardInterrupt:\n                self._running = False\n                logging.info(\"coverage worker stop\")\n                break\n\n    @staticmethod\n    def mk_rpl_status(status: str) -> str:\n        if status == \"SUCCESS\":\n            return mk_color(status, Bcolors.OKGREEN)\n        else:\n            return mk_color(status, Bcolors.FAIL)\n\n\n    @staticmethod\n    def mk_broker_status(status: str, new_items: bool) -> str:\n        if status == \"GRANTED\":\n            return mk_color(status, Bcolors.OKGREEN if new_items else Bcolors.WARNING)\n        elif status == \"DROPPED\":\n            return mk_color(status, Bcolors.WARNING)\n        else:\n            return mk_color(status, Bcolors.FAIL)\n\n    @staticmethod\n    def worker(input_queue: Queue, cov_queue: Queue, program: str, argv: list[str], seed_inj: SeedInjectLoc) -> None:\n        \"\"\"\n        worker thread that unstack inputs and replay them.\n        \"\"\"\n        tmpfile = Path(tempfile.mktemp(suffix=f\"{os.getpid()}.input\"))\n        pid = os.getpid()\n        try:\n            while True:\n                item: ClientInput = input_queue.get()\n                # logging.debug(f\"Worker {os.getpid()} fetch: {str(item)[:50]}\")\n                # Write inputs in our tempfile\n                tmpfile.write_bytes(item.content)\n\n                # Create to coverage file\n                cov_file = tempfile.mktemp(f\"_{item.hash}.cov\")\n\n                # Adjust injection location before calling QBDITrace\n                cur_argv = argv[:]\n                if seed_inj == SeedInjectLoc.ARGV:  # Try to replace the placeholder with filename\n                    try:\n                        # Replace 'input_file' in argv with the temporary file name created\n                        idx = cur_argv.index(CoverageManager.ARGV_PLACEHOLDER)\n                        cur_argv[idx] = str(tmpfile)\n                    except ValueError as e:\n                        logging.error(f\"seed injection {seed_inj.name} but can't find '@@' on program argv: {argv}: {e}\")\n                        continue\n\n                try:\n                    # Run the seed\n                    t0 = time.time()\n                    if QBDITrace.run(CoverageManager.STRATEGY,\n                                     program,\n                                     cur_argv,  # argv[1:] if len(argv) > 1 else [],\n                                     output_path=str(cov_file),\n                                     stdin_file=str(tmpfile) if seed_inj == SeedInjectLoc.STDIN else None,\n                                     cwd=Path(program).parent,\n                                     timeout=60):\n                        item.replay_time = time.time() - t0\n                        item.replay_status = \"SUCCESS\"\n                        # logging.info(f\"[worker-{pid}] replaying {item.hash} sucessful\")\n                    else:\n                        item.replay_status = \"FAIL_NO_COV\"\n                        logging.warning(\"Cannot load the coverage file generated (maybe had crashed?)\")\n                except TraceException:\n                    item.replay_status = \"FAIL_TIMEOUT\"\n                    logging.warning('Timeout hit, while trying to re-run the seed')\n\n                # Add it to the coverage queue (even if it failed\n                cov_queue.put((item, cov_file))\n        except KeyboardInterrupt:\n            pass", "            # logging.info(f\"replay worker {os.getpid()}, stops (keyboard interrupt)\")\n"]}
{"filename": "pastisbroker/utils.py", "chunked_list": ["#built-in imports\nimport logging\nfrom typing import Optional\nimport importlib\nimport inspect\n\nfrom libpastis.enginedesc import FuzzingEngineDescriptor\n\n\nHF_PERSISTENT_SIG = b\"\\x01_LIBHFUZZ_PERSISTENT_BINARY_SIGNATURE_\\x02\\xFF\"", "\nHF_PERSISTENT_SIG = b\"\\x01_LIBHFUZZ_PERSISTENT_BINARY_SIGNATURE_\\x02\\xFF\"\n\n\ndef load_engine_descriptor(py_module: str) -> Optional[FuzzingEngineDescriptor]:\n    try:\n        mod = importlib.import_module(py_module)\n        mems = inspect.getmembers(mod, lambda x: inspect.isclass(x) and issubclass(x, FuzzingEngineDescriptor) and x != FuzzingEngineDescriptor)\n        if not mems:\n            logging.error(f\"can't find FuzzingEngineDescriptor in module {py_module}\")\n            return None\n        else:\n            if len(mems) > 1:\n                logging.warning(f\"module {py_module} contain multiple subclass of {FuzzingEngineDescriptor} (take first)\")\n            return mems[0][1]\n    except ImportError:\n        logging.error(f\"cannot import py_module: {py_module}\")", "\n\n\nCOLORS = [32, 33, 34, 35, 36, 37, 39, 91, 93, 94, 95, 96]\n\n\nclass Bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'", "\ndef mk_color(text: str, color: str) -> str:\n    return color+text+Bcolors.ENDC\n"]}
{"filename": "pastisbroker/stat_manager.py", "chunked_list": ["# Built-in imports\nimport time\nfrom pathlib import Path\nimport csv\nimport json\nfrom typing import List\n\n# Third-party importq\nfrom libpastis.types import SeedType\n", "from libpastis.types import SeedType\n\n# Local imports\nfrom pastisbroker.client import PastisClient\nfrom pastisbroker.workspace import Workspace\n\n\nclass StatManager(object):\n    \"\"\"\n    Keeps temporal statistics to plot them.\n    \"\"\"\n    def __init__(self, workspace: Workspace):\n        # Configure CSV writer that will write stats\n        names = ['date', 'id', 'exec_per_sec', 'total_exec', 'cycle', 'timeout', 'block', 'edge', 'path', 'last_cov_update']\n        self._tel_file = open(workspace.telemetry_file, \"w\")\n        self.writer = csv.DictWriter(self._tel_file, fieldnames=names)\n        self.writer.writeheader()\n\n    def update_seed_stat(self, client: PastisClient, typ: SeedType) -> None:\n        t = time.localtime()\n        if typ == SeedType.INPUT:\n            client.input_submitted_count += 1\n            client._timeline_seeds.append((t, client.input_submitted_count, typ))\n        elif typ == SeedType.CRASH:\n            client.crash_submitted_count += 1\n            client._timeline_seeds.append((t, client.crash_submitted_count, typ))\n        elif typ == SeedType.HANG:\n            client.timeout_submitted_count += 1\n            client._timeline_seeds.append((t, client.timeout_submitted_count, typ))\n        else:\n            assert False\n\n        client.seed_first += 1\n\n    def set_exec_per_sec(self, client: PastisClient, exec_per_sec: int = None):\n        if exec_per_sec is not None:\n            client.exec_per_sec = exec_per_sec  # instantaneous value does not keep history\n\n    def set_total_exec(self, client: PastisClient, total_exec: int = None):\n        if total_exec is not None:\n            client.total_exec = total_exec  # instantaneous value does not keep history\n\n    def set_cycle(self, client: PastisClient, cycle: int = None):\n        if cycle is not None:\n            client.cycle = cycle  # instantaneous value does not keep history\n\n    def set_timeout(self, client: PastisClient, timeout: int = None):\n        if timeout is not None:\n            client.timeout = timeout  # instantaneous value does not keep history\n\n    def set_coverage_block(self, client: PastisClient, coverage: int = None):\n        if coverage is not None:\n            client.coverage_block = coverage\n\n    def set_coverage_edge(self, client: PastisClient, coverage: int = None):\n        if coverage is not None:\n            client.coverage_edge = coverage\n\n    def set_coverage_path(self, client: PastisClient, coverage: int = None):\n        if coverage is not None:\n            client.coverage_path = coverage\n\n    @staticmethod\n    def set_last_coverage_update(client: PastisClient, last_up: int = None):\n        if last_up is not None:\n            client.last_cov_update = last_up  # instantaneous value does not keep history\n\n    def update_telemetry_client(self, client: PastisClient):\n        self.writer.writerow({\n            'date': time.time(),\n            'id': client.strid,\n            'exec_per_sec': client.exec_per_sec,\n            'total_exec': client.total_exec,\n            'cycle': client.cycle,\n            'timeout': client.timeout,\n            'block': client.coverage_block,\n            'edge': client.coverage_edge,\n            'path': client.coverage_path,\n            'last_cov_update': client.last_cov_update\n        })\n\n\n    def post_execution(self, clients: List[PastisClient], workspace: Workspace) -> None:\n        \"\"\"\n        Called at the end of the execution. Export\n        :return: None\n        \"\"\"\n        self._tel_file.flush()  # Flush the csv if it has not been\n\n        with open(workspace.clients_stat_file, \"w\") as f:\n            json.dump([cli.to_dict() for cli in clients if cli.is_running()], f, indent=2)", ""]}
{"filename": "pastisbroker/dashboard.py", "chunked_list": [""]}
