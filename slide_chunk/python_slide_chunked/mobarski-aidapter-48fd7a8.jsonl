{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\nsetup(\n\tname='aidapter',\n\tversion='0.6.3',\n\tdescription='AI adapter / facade',\n\tauthor='Maciej Obarski',\n\tinstall_requires=[\n\t\t'retry',\n        'tqdm',", "\t\t'retry',\n        'tqdm',\n        'requests',\n        'diskcache',\n\t],\n\tpackages=find_packages()\n)\n"]}
{"filename": "aidapter/api_hf.py", "chunked_list": ["from . import base\n\nimport requests\nimport json\nimport os\n\n# TODO: handle error messages\n\ndef hf_api_query(payload, model_id, endpoint):\n    api_url = f\"https://api-inference.huggingface.co/{endpoint}/{model_id}\"\n    headers = {'Authorization': f'Bearer {os.environ[\"HF_API_TOKEN\"]}'} # TODO\n    #\n    data = json.dumps(payload)\n    raw_resp = requests.request(\"POST\", api_url, headers=headers, data=data)\n    resp = json.loads(raw_resp.content.decode(\"utf-8\"))\n    return resp", "def hf_api_query(payload, model_id, endpoint):\n    api_url = f\"https://api-inference.huggingface.co/{endpoint}/{model_id}\"\n    headers = {'Authorization': f'Bearer {os.environ[\"HF_API_TOKEN\"]}'} # TODO\n    #\n    data = json.dumps(payload)\n    raw_resp = requests.request(\"POST\", api_url, headers=headers, data=data)\n    resp = json.loads(raw_resp.content.decode(\"utf-8\"))\n    return resp\n\n# === TEXT ========================================================================================", "\n# === TEXT ========================================================================================\n\nclass TextModel(base.CompletionModel):\n\n    # TODO\n    def transform_one(self, prompt, **kw) -> dict:\n        #\n        resp = hf_api_query(prompt, self.name, 'models')\n        output_text = resp[0]['generated_text'] \n        #\n        out = {}\n        out['output'] = output_text\n        return out", "\n# === EMBEDDING ===================================================================================\n\n# REF: https://huggingface.co/blog/getting-started-with-embeddings\n# REF: https://huggingface.co/spaces/mteb/leaderboard\n\n# model_id = 'thenlper/gte-small' # model size: 0.07GB, width: 384\n# model_id = 'BAAI/bge-small-en'  # model size: 0.13GB, width: 384\n\nclass EmbeddingModel(base.EmbeddingModel):\n    def transform_one(self, text, **kw):\n        return self.embed_batch([text], **kw)[0]\n\n    def embed_batch(self, texts, **kw):\n        limit = kw.get('limit')\n        #\n        resp = hf_api_query(texts, self.name, 'pipeline/feature-extraction')\n        #\n        out = []\n        for x in resp:\n            out.append({\n                 'output': x[:limit],\n            })\n        return out", "\nclass EmbeddingModel(base.EmbeddingModel):\n    def transform_one(self, text, **kw):\n        return self.embed_batch([text], **kw)[0]\n\n    def embed_batch(self, texts, **kw):\n        limit = kw.get('limit')\n        #\n        resp = hf_api_query(texts, self.name, 'pipeline/feature-extraction')\n        #\n        out = []\n        for x in resp:\n            out.append({\n                 'output': x[:limit],\n            })\n        return out", "\n"]}
{"filename": "aidapter/base.py", "chunked_list": ["from tqdm import tqdm\nfrom retry.api import retry_call\n\nfrom multiprocessing.dummy import Pool\nfrom itertools import islice\nfrom datetime import date\nfrom time import time\nimport hashlib\n\n# TODO: callbacks: before / after, one vs many", "\n# TODO: callbacks: before / after, one vs many\n\n# COMPLETION\n\nclass BaseModel:\n    RENAME_KWARGS = {}\n    DEFAULT_KWARGS = {}\n\n    def __init__(self, name, api_kwargs, options=''):\n        self.kwargs = self.DEFAULT_KWARGS.copy()\n        self.kwargs.update(api_kwargs)\n        self.options = options\n        self.name = name\n        self.cache = DummyKV()\n        self.usage = DummyKV()\n        self.retry_tries = 5\n        self.retry_delay = 0.1\n        self.retry_backoff = 3\n        self.workers = 4\n        self.batch = 1\n        self.show_progress = False\n        self.id = '' # this will be set by the factory function\n\n    # INTERNAL\n\n    def get_api_kwargs(self, kw):\n        kwargs = self.kwargs.copy()\n        for k in self.DEFAULT_KWARGS:\n            if k in kw:\n                kwargs[k] = kw[k]\n        return kwargs\n    \n    def rename_kwargs(self, kwargs):\n        return {self.RENAME_KWARGS.get(k,k):v for k,v in kwargs.items()}\n\n    def register_usage(self, usage):\n        if usage:\n            keys = self.get_usage_keys()\n            for key in keys:\n                data = self.usage.get(key, {})\n                use_incr = hasattr(data, 'incr')\n                for k,v in usage.items():\n                    if use_incr:\n                        data.incr(k, v)\n                    else:\n                        data[k] = data.get(k,0) + v\n                self.usage[key] = data\n\n    def get_usage_keys(self):\n        day = str(date.today()) # iso format\n        keys = [\n            f'total:{self.name}',\n            f'day:{day}:{self.name}',\n        ]\n        return keys\n\n    # TRANSFORM\n\n    def transform(self, input, debug=False, cache='use', **kw):\n        if type(input)==str:\n            resp = self.transform_one_cached(input, cache=cache, **kw)\n            out = resp['output']\n        else:\n            resp = self.transform_many(input, cache=cache, **kw)\n            out = [x['output'] for x in resp]\n        self.usage.sync() # TODO: only if changed\n        self.cache.sync() # TODO: only if changed\n        if debug:\n            return resp\n        else:\n            return out\n\n    def transform_one_retry(self, input, **kw) -> dict:\n        return retry_call(\n            self.transform_one,\n            fargs=[input],\n            fkwargs=kw,\n            tries=self.retry_tries,\n            delay=self.retry_delay,\n            backoff=self.retry_backoff,\n        )\n\n    def transform_many(self, inputs, cache='use', **kw) -> list[dict]:\n        def worker(prompt):\n            return self.transform_one_cached(prompt, cache=cache, register=False, **kw)\n        data = inputs if self.batch==1 else batched(inputs, self.batch)\n        # TODO batched\n        with Pool(self.workers) as pool:\n            out = []\n            with tqdm(total=len(data), disable=not self.show_progress) as progress:\n                for x in pool.imap(worker, data):\n                    out.append(x)\n                    self.register_usage(x.get('usage',{}))\n                    progress.update()\n        return out\n\n    # TODO: cache=save\n    # TODO: dont save cache if empty\n    def transform_one_cached(self, input, cache='use', register=True, **kw) -> dict:\n        t0 = time()\n        kwargs = self.get_api_kwargs(kw)\n        cache_key = self.get_cache_key(input, kwargs, kw)\n        if self.skip_cache_condition(kwargs, kw, cache):\n            resp = self.transform_one_retry(input, **kw)\n            resp['usage'] = resp.get('usage', {})\n            resp['usage']['cache_skip'] = 1\n        else:\n            if cache_key in self.cache:\n                resp = self.cache[cache_key]\n                resp['usage'] = {f'cached_{k}':v for k,v in resp.get('usage',{}).items() if 'cache' not in k}\n                resp['usage']['cache_hit'] = 1\n            else:\n                resp = self.transform_one_retry(input, **kw)\n                resp['usage'] = resp.get('usage', {})\n                resp['usage']['cache_miss'] = 1\n                self.cache[cache_key] = resp\n        resp['usage']['time'] = time() - t0\n        if register:\n            self.register_usage(resp['usage'])\n        return resp\n\n    def skip_cache_condition(self, kwargs, kw, cache):\n        return cache in ('skip','save')\n\n    def get_cache_key(self, input, kwargs, kw):\n        all_kwargs = kwargs.copy()\n        all_kwargs.update(kw)\n        kwargs_str = str([(k,all_kwargs[k]) for k in sorted(all_kwargs)])\n        cache_key = self.name +':'+ md5(f'{input}:{kwargs_str}')\n        return cache_key", "\n\nclass CompletionModel(BaseModel):\n    DEFAULT_KWARGS = {'temperature':0, 'stop':[], 'limit':100}\n\n    def complete(self, prompt, debug=False, cache='use', **kw):\n        return self.transform(prompt, debug=debug, cache=cache, **kw)\n\n    def skip_cache_condition(self, kwargs, kw, cache):\n        return (kwargs.get('temperature',0)!=0 or cache=='skip') and cache!='force'\n\n    # MOCK\n\n    def transform_one(self, prompt, **kw) -> dict:\n        \"mock\"\n        kwargs = self.get_api_kwargs(kw)\n        # mock\n        system = kw.get('system','')\n        full_prompt = f'{system}\\n\\n{prompt}' if system else prompt\n        #\n        kwargs = self.rename_kwargs(kwargs)\n        from time import sleep\n        sleep(1)\n        resp = {}\n        #\n        resp['output'] = f'{full_prompt} DUMMY RESPONSE'\n        resp['usage'] = {'dummy_tokens': 3}\n        resp['kwargs'] = kwargs\n        return resp", "\n# EMBEDDING\n\nclass EmbeddingModel(BaseModel):\n\n    def embed(self, text, debug=False, cache='use', **kw):\n        return self.transform(text, debug=debug, cache=cache, **kw)\n\n    # MOCK\n\n    def transform_one(self, text, debug=False, cache='use', **kw):\n        limit = kw.get('limit')\n        return {\n            'output': [1,2,3,4,5][:limit],\n        }", "\n# HELPERS\n\ndef md5(text):\n    return hashlib.md5(text.encode()).hexdigest()\n\ndef batched(data, n, as_type=list):\n    it = iter(data)\n    while batch := as_type(islice(it, n)):\n        yield batch", "\nclass DummyKV(dict):\n    \"in-memory key-value store with shelve-like interface\"\n    def sync(self):\n        pass\n    def close(self):\n        pass\n"]}
{"filename": "aidapter/api_cohere.py", "chunked_list": ["# REF: https://cohere.ai/pricing\n# REF: https://dashboard.cohere.ai/api-keys\n# REF: https://docs.cohere.ai/reference/generate\n# REF: https://docs.cohere.ai/reference/embed\n# REF: https://docs.cohere.ai/reference/tokenize\n\nfrom . import base\nimport cohere\nimport sys\nimport os", "import sys\nimport os\n\ndef use_key(key):\n\tcohere.api_key = key\nif not getattr(cohere, 'api_key', None):\n\tuse_key(os.getenv('CO_API_KEY',''))\n\n\nclass TextModel(base.CompletionModel):\n    RENAME_KWARGS  = {'stop':'stop_sequences', 'limit':'max_tokens'}\n\n    def __init__(self, name, kwargs):\n        super().__init__(name, kwargs)\n        self.client = cohere.Client(cohere.api_key)\n\n    def transform_one(self, prompt, **kw) -> dict:\n        kwargs = self.get_api_kwargs(kw)\n        kwargs['stop'] = kwargs.get('stop') or [] # FIX empty value\n        kwargs['model'] = self.name\n        #\n        system = kw.get('system','')\n        start = kw.get('start','')\n        full_prompt = prompt if not system else f'{system.rstrip()}\\n\\n{prompt}'\n        full_prompt += start\n        kwargs['prompt'] = full_prompt\n        #\n        kwargs = self.rename_kwargs(kwargs)\n        resp = self.client.generate(**kwargs)\n        output_text = resp[0]\n        #\n        out = {}\n        out['output'] = start + output_text # TODO: detect and handle start duplication\n        out['usage'] = {} # TODO\n        out['kwargs'] = kwargs\n        out['resp'] = resp\n        # TODO usage\n        # TODO error\n        return out", "\nclass TextModel(base.CompletionModel):\n    RENAME_KWARGS  = {'stop':'stop_sequences', 'limit':'max_tokens'}\n\n    def __init__(self, name, kwargs):\n        super().__init__(name, kwargs)\n        self.client = cohere.Client(cohere.api_key)\n\n    def transform_one(self, prompt, **kw) -> dict:\n        kwargs = self.get_api_kwargs(kw)\n        kwargs['stop'] = kwargs.get('stop') or [] # FIX empty value\n        kwargs['model'] = self.name\n        #\n        system = kw.get('system','')\n        start = kw.get('start','')\n        full_prompt = prompt if not system else f'{system.rstrip()}\\n\\n{prompt}'\n        full_prompt += start\n        kwargs['prompt'] = full_prompt\n        #\n        kwargs = self.rename_kwargs(kwargs)\n        resp = self.client.generate(**kwargs)\n        output_text = resp[0]\n        #\n        out = {}\n        out['output'] = start + output_text # TODO: detect and handle start duplication\n        out['usage'] = {} # TODO\n        out['kwargs'] = kwargs\n        out['resp'] = resp\n        # TODO usage\n        # TODO error\n        return out", "\n\nclass EmbeddingModel(base.EmbeddingModel):\n\n    def __init__(self, name, kwargs):\n        super().__init__(name, kwargs)\n        self.client = cohere.Client(cohere.api_key)\n\n    def transform_one(self, text, **kw):\n        return self.embed_batch([text], **kw)[0]\n\n    def embed_batch(self, texts, **kw):\n        limit = kw.get('limit')\n        resp = self.client.embed(texts, model=self.name)\n        #\n        out = []\n        for x in resp.embeddings:\n            out.append({'output': x[:limit]})\n        return out", ""]}
{"filename": "aidapter/api_hf2.py", "chunked_list": ["from . import base2 as base\n\nimport requests\nimport json\nimport os\n\n\ndef hf_api_query(payload, model_id, endpoint):\n    api_url = f\"https://api-inference.huggingface.co/{endpoint}/{model_id}\"\n    headers = {'Authorization': f'Bearer {os.environ[\"HF_API_TOKEN\"]}'} # TODO\n    #\n    data = json.dumps(payload)\n    raw_resp = requests.request(\"POST\", api_url, headers=headers, data=data)\n    resp = json.loads(raw_resp.content.decode(\"utf-8\"))\n    # handle error messages\n    if isinstance(resp, dict) and 'error' in resp:\n        error = resp['error']\n        est_time = resp.get('estimated_time')\n        msg = error\n        if est_time:\n            msg += f' (estimated time: {est_time:0.1f}s)'\n        raise ValueError(msg)\n    return resp", "\n# === EMBEDDING ===================================================================================\n\n# REF: https://huggingface.co/blog/getting-started-with-embeddings\n# REF: https://huggingface.co/spaces/mteb/leaderboard\n\n# model_id = 'thenlper/gte-small' # model size: 0.07GB, width: 384\n# model_id = 'BAAI/bge-small-en'  # model size: 0.13GB, width: 384\n\n# TODO: different models -> different output shapes", "\n# TODO: different models -> different output shapes\n\nclass EmbeddingModel(base.BaseModelV2):\n    brand = 'huggingface'\n\n    def embed(self, inputs, **kwargs):\n        return self.transform(inputs, **kwargs)\n\n    def transform_batch(self, inputs, **kwargs):\n        limit = kwargs.get('limit')\n        resp = hf_api_query(inputs, self.name, 'pipeline/feature-extraction')\n        output = [x[:limit] for x in resp]\n        self.register_usage({'api-calls':1})\n        return output", "\n# === TEXT ========================================================================================\n\nclass TextModel(base.BaseModelV2):\n    brand = 'huggingface'\n\n    def complete(self, prompt, **kwargs):\n        return self.transform_many(prompt, **kwargs)\n\n    def transform_batch(self, prompts, **kwargs):\n        resp = hf_api_query(prompts, self.name, 'models')\n        output = [x['generated_text'] for x in resp]\n        return output", ""]}
{"filename": "aidapter/api_transformers.py", "chunked_list": ["from . import base\nimport transformers\nimport torch\nimport sys\nimport os\n\ndef use_key(key):\n\tpass\n\n\nclass TextModel(base.CompletionModel):\n    RENAME_KWARGS  = {'limit':'max_new_tokens'}\n\n    def __init__(self, name, kwargs, options):\n        super().__init__(name, kwargs)\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(name)\n        # OPTIONS\n        kw = {}\n        if '16bit' in options:\n            kw['torch_dtype'] = torch.float16\n        elif 'bloat16' in options:\n            kw['torch_dtype'] = torch.bfloat16\n        elif '8bit' in options:\n            kw['load_in_8bit'] = True\n        elif '4bit' in options:\n            kw['load_in_4bit'] = True\n        else:\n             kw['torch_dtype'] = 'auto'\n        if 'trust' in options:\n            kw['trust_remote_code'] = True\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(name, device_map=\"auto\", **kw)\n\n    def transform_one(self, prompt, **kw) -> dict:\n        kwargs = self.get_api_kwargs(kw)\n        kwargs['stop'] = kwargs.get('stop') or [] # FIX empty value\n        kwargs['model'] = self.name\n        # full_prompt\n        system = kw.get('system','')\n        start = kw.get('start','')\n        full_prompt = prompt if not system else f'{system.rstrip()}\\n\\n{prompt}'\n        full_prompt += start\n        kwargs['prompt'] = full_prompt\n        #\n        #kwargs = self.rename_kwargs(kwargs) # NOT USED - direct mapping below\n        final_kwargs = dict(\n            max_new_tokens = kwargs['limit'],\n            temperature = kwargs['temperature'],\n            pad_token_id = self.tokenizer.eos_token_id,\n        )\n        # stop early if stop criteria is met\n        if kwargs['stop']:\n            def stop_fun(ids, scores, **_):\n                output_text = self.tokenizer.decode(ids[0])[len(full_prompt):]\n                for s in kwargs['stop']:\n                    if s in output_text:\n                        return True\n            final_kwargs['stopping_criteria'] = [stop_fun]\n        #\n        prompt_tokens = self.tokenizer.encode(full_prompt, return_tensors='pt')\n        resp = self.model.generate(\n                prompt_tokens.to(\"cuda\"),\n                **final_kwargs\n            )\n        resp_text = self.tokenizer.decode(resp[0], skip_special_tokens=True)\n        output_text = resp_text[len(full_prompt):]\n        # remove stop criteria from output\n        if kwargs['stop']:\n            for s in sorted(kwargs['stop'], key=lambda x: len(x), reverse=True):\n                if s in output_text:\n                    output_text = output_text.split(s)[0]\n        #\n        out = {}\n        out['output'] = start + output_text\n        out['usage'] = {\n            'prompt_tokens': prompt_tokens.shape[1],\n            'resp_tokens': resp.shape[1],\n            'total_tokens': prompt_tokens.shape[1] + resp.shape[1],\n            'prompt_chars': len(full_prompt),\n            'resp_chars': len(out['output']),\n            'total_chars': len(full_prompt) + len(out['output']),\n        }\n        out['kwargs'] = final_kwargs\n        out['resp'] = {'resp':resp, 'prompt_tokens':prompt_tokens}\n        # TODO usage\n        # TODO error\n        return out\n\n    def raw_embed_one(self, text, **kw):\n        input = self.tokenizer.encode(text, return_tensors='pt')\n        outputs = self.model(input, output_hidden_states=True)\n        last_hidden_state = outputs.hidden_states[-1][0]\n        fun = lambda x: x[0].tolist()\n        return fun(last_hidden_state)", "\n\nclass TextModel(base.CompletionModel):\n    RENAME_KWARGS  = {'limit':'max_new_tokens'}\n\n    def __init__(self, name, kwargs, options):\n        super().__init__(name, kwargs)\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(name)\n        # OPTIONS\n        kw = {}\n        if '16bit' in options:\n            kw['torch_dtype'] = torch.float16\n        elif 'bloat16' in options:\n            kw['torch_dtype'] = torch.bfloat16\n        elif '8bit' in options:\n            kw['load_in_8bit'] = True\n        elif '4bit' in options:\n            kw['load_in_4bit'] = True\n        else:\n             kw['torch_dtype'] = 'auto'\n        if 'trust' in options:\n            kw['trust_remote_code'] = True\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(name, device_map=\"auto\", **kw)\n\n    def transform_one(self, prompt, **kw) -> dict:\n        kwargs = self.get_api_kwargs(kw)\n        kwargs['stop'] = kwargs.get('stop') or [] # FIX empty value\n        kwargs['model'] = self.name\n        # full_prompt\n        system = kw.get('system','')\n        start = kw.get('start','')\n        full_prompt = prompt if not system else f'{system.rstrip()}\\n\\n{prompt}'\n        full_prompt += start\n        kwargs['prompt'] = full_prompt\n        #\n        #kwargs = self.rename_kwargs(kwargs) # NOT USED - direct mapping below\n        final_kwargs = dict(\n            max_new_tokens = kwargs['limit'],\n            temperature = kwargs['temperature'],\n            pad_token_id = self.tokenizer.eos_token_id,\n        )\n        # stop early if stop criteria is met\n        if kwargs['stop']:\n            def stop_fun(ids, scores, **_):\n                output_text = self.tokenizer.decode(ids[0])[len(full_prompt):]\n                for s in kwargs['stop']:\n                    if s in output_text:\n                        return True\n            final_kwargs['stopping_criteria'] = [stop_fun]\n        #\n        prompt_tokens = self.tokenizer.encode(full_prompt, return_tensors='pt')\n        resp = self.model.generate(\n                prompt_tokens.to(\"cuda\"),\n                **final_kwargs\n            )\n        resp_text = self.tokenizer.decode(resp[0], skip_special_tokens=True)\n        output_text = resp_text[len(full_prompt):]\n        # remove stop criteria from output\n        if kwargs['stop']:\n            for s in sorted(kwargs['stop'], key=lambda x: len(x), reverse=True):\n                if s in output_text:\n                    output_text = output_text.split(s)[0]\n        #\n        out = {}\n        out['output'] = start + output_text\n        out['usage'] = {\n            'prompt_tokens': prompt_tokens.shape[1],\n            'resp_tokens': resp.shape[1],\n            'total_tokens': prompt_tokens.shape[1] + resp.shape[1],\n            'prompt_chars': len(full_prompt),\n            'resp_chars': len(out['output']),\n            'total_chars': len(full_prompt) + len(out['output']),\n        }\n        out['kwargs'] = final_kwargs\n        out['resp'] = {'resp':resp, 'prompt_tokens':prompt_tokens}\n        # TODO usage\n        # TODO error\n        return out\n\n    def raw_embed_one(self, text, **kw):\n        input = self.tokenizer.encode(text, return_tensors='pt')\n        outputs = self.model(input, output_hidden_states=True)\n        last_hidden_state = outputs.hidden_states[-1][0]\n        fun = lambda x: x[0].tolist()\n        return fun(last_hidden_state)", ""]}
{"filename": "aidapter/api_anthropic.py", "chunked_list": ["# REF: https://console.anthropic.com/docs/api/reference\n\nfrom . import base\nimport anthropic\nimport sys\nimport os\n\ndef use_key(key):\n\tanthropic.api_key = key\nif not getattr(anthropic, 'api_key', None):\n\tuse_key(os.getenv('ANTHROPIC_API_KEY',''))", "if not getattr(anthropic, 'api_key', None):\n\tuse_key(os.getenv('ANTHROPIC_API_KEY',''))\n\n\nclass ChatModel(base.CompletionModel):\n    RENAME_KWARGS  = {'stop':'stop_sequences', 'limit':'max_tokens_to_sample'}\n\n    def __init__(self, name, kwargs):\n        super().__init__(name, kwargs)\n        self.client = anthropic.Client(anthropic.api_key)\n\n    def transform_one(self, prompt, **kw) -> dict:\n        kwargs = self.get_api_kwargs(kw)\n        kwargs['stop'] = kwargs.get('stop') or [] # FIX empty value\n        kwargs['model'] = self.name\n        #\n        system = kw.get('system','')\n        start = kw.get('start','')\n        full_prompt = prompt if not system else f'{system.rstrip()}\\n\\n{prompt}'\n        full_prompt += start\n        kwargs['prompt'] = f\"{anthropic.HUMAN_PROMPT} {full_prompt}{anthropic.AI_PROMPT}\"\n        #\n        kwargs = self.rename_kwargs(kwargs)\n        resp = self.client.completion(**kwargs)\n        output_text = resp.get('completion','')\n        #\n        out = {}\n        out['output'] = start + output_text # TODO detect and handle start duplication\n        out['usage'] = {\n            'prompt_tokens': anthropic.count_tokens(kwargs['prompt']),\n            'resp_tokens': anthropic.count_tokens(output_text),\n            'total_tokens': anthropic.count_tokens(kwargs['prompt']) + anthropic.count_tokens(output_text),\n            'prompt_chars': len(kwargs['prompt']),\n            'resp_chars': len(output_text),\n            'total_chars': len(kwargs['prompt']) + len(output_text),\n        }\n        out['kwargs'] = kwargs\n        out['resp'] = resp\n        # TODO usage\n        # TODO resp['error']\n        return out", ""]}
{"filename": "aidapter/__init__.py", "chunked_list": ["\n# TODO: kind : brand : name ???\n\ndef model(model_id, **kwargs):\n\t\"model factory function\"\n\tbrand,_,name = model_id.partition(':')\n\tname,_,options = name.partition(':')\n\t#\n\tif brand=='anthropic':\n\t\tfrom . import api_anthropic\n\t\tmodel =  api_anthropic.ChatModel(name, kwargs)\n\telif brand=='openai':\n\t\tfrom . import api_openai\n\t\tfrom . import api_openai2\n\t\tif 'embed' in options or 'embedding' in name:\n\t\t\tmodel = api_openai2.EmbeddingModel(name, kwargs, options)\n\t\telif name.startswith('gpt-'):\n\t\t\tmodel = api_openai.ChatModel(name, kwargs)\n\t\telse:\n\t\t\tmodel = api_openai.TextModel(name, kwargs)\n\telif brand=='cohere':\n\t\tfrom . import api_cohere\n\t\tif 'embed' in name:\n\t\t\tmodel = api_cohere.EmbeddingModel(name, kwargs)\n\t\telse:\n\t\t\tmodel = api_cohere.TextModel(name, kwargs)\n\telif brand=='transformers':\n\t\tfrom . import api_transformers\n\t\tmodel = api_transformers.TextModel(name, kwargs, options)\n\telif brand=='vllm':\n\t\tfrom . import api_vllm\n\t\tmodel = api_vllm.TextModel(name, kwargs, options)\n\telif brand=='sentence-transformers':\n\t\tfrom . import api_sentence_transformers\n\t\tmodel = api_sentence_transformers.EmbeddingModel(name, kwargs, options)\n\telif brand=='hf':\n\t\tfrom . import api_hf\n\t\tif 'embed' in options:\n\t\t\tmodel = api_hf.EmbeddingModel(name, kwargs, options)\n\t\telse:\n\t\t\tmodel = api_hf.TextModel(name, kwargs, options)\n\telif brand=='hf2' or brand=='huggingface':\n\t\tfrom . import api_hf2\n\t\tif 'embed' in options:\n\t\t\tmodel = api_hf2.EmbeddingModel(name, kwargs, options)\n\t\telse:\n\t\t\tmodel = api_hf2.TextModel(name, kwargs, options)\n\telse:\n\t\traise ValueError(f'unknown brand: {brand}')\n\t#\n\tmodel.id = model_id\n\tmodel.id_safe = model_id.replace('/','--')\n\treturn model", ""]}
{"filename": "aidapter/api_sentence_transformers.py", "chunked_list": ["from . import base\nfrom sentence_transformers import SentenceTransformer\n\ndef use_key(key):\n\tpass\n\nclass EmbeddingModel(base.EmbeddingModel):\n    def __init__(self, name, kwargs, options):\n        super().__init__(name, kwargs)\n        self.model = SentenceTransformer(name)\n\n    def transform_one(self, text, **kw):\n\n        return self.embed_batch([text], **kw)[0]\n\n    def embed_batch(self, texts, **kw):\n        limit = kw.get('limit')\n        resp = self.model.encode(texts)\n        #\n        out = []\n        for x in resp:\n            out.append({'output': list(x)[:limit]})\n        return out", ""]}
{"filename": "aidapter/api_openai.py", "chunked_list": ["# REF: https://platform.openai.com/docs/api-reference/completions\n\nfrom . import base\nimport openai\nimport json\nimport sys\nimport os\n\ndef use_key(key):\n    openai.api_key = key\nif not openai.api_key:\n    use_key(os.getenv('OPENAI_API_KEY',''))", "def use_key(key):\n    openai.api_key = key\nif not openai.api_key:\n    use_key(os.getenv('OPENAI_API_KEY',''))\n\n\nclass ChatModel(base.CompletionModel):\n    RENAME_KWARGS = {'limit':'max_tokens'}\n\n\n    def transform_one(self, prompt, **kw) -> dict:\n        kwargs = self.get_api_kwargs(kw)\n        kwargs['stop'] = kwargs.get('stop') or None # FIX empty value\n        kwargs['model'] = self.name\n        #\n        system = kw.get('system','')\n        start = kw.get('start','')\n        messages = []\n        if system:\n              messages += [{'role':'system', 'content':system}]\n        messages += [{'role':'user', 'content':prompt+start}]\n        kwargs['messages'] = messages\n        #kwargs['max_tokens'] = limit # TODO\n        functions = kw.get('functions',[]) or kwargs.get('functions',[]) # NEW\n        kwargs['functions'] = [get_signature(f) for f in functions]\n        #\n        kwargs = self.rename_kwargs(kwargs)\n        resp = openai.ChatCompletion.create(**kwargs)\n        output_text = resp['choices'][0]['message'].get('content','')\n        fc = function_call = resp['choices'][0]['message'].get('function_call',{})\n        #\n        out = {}\n        if function_call:\n            out['output'] = {'function_name':fc['name'], 'arguments':json.loads(fc['arguments'])}\n        else:\n            out['output'] = start + output_text # TODO: detect and handle start duplication\n        out['usage'] = resp['usage']\n        out['kwargs'] = kwargs\n        out['resp'] = resp\n        return out", "\n\nclass TextModel(base.CompletionModel):\n    RENAME_KWARGS = {'limit':'max_tokens'}\n\n    def transform_one(self, prompt, **kw) -> dict:\n        kwargs = self.get_api_kwargs(kw)\n        kwargs['stop'] = kwargs.get('stop') or None # FIX empty value\n        kwargs['model'] = self.name\n        #\n        system = kw.get('system','')\n        start = kw.get('start','')\n        full_prompt = prompt if not system else f'{system.rstrip()}\\n\\n{prompt}'\n        full_prompt += start\n        kwargs['prompt'] = full_prompt\n        #\n        kwargs = self.rename_kwargs(kwargs)\n        resp = openai.Completion.create(**kwargs)\n        output_text = resp['choices'][0]['text']\n        #\n        out = {}\n        out['output'] = start + output_text\n        out['usage'] = resp['usage']\n        out['kwargs'] = kwargs\n        out['resp'] = resp\n        return out", "\n\nclass EmbeddingModel(base.EmbeddingModel):\n    def transform_one(self, text, **kw):\n        return self.embed_batch([text], **kw)[0]\n\n    def embed_batch(self, texts, **kw):\n        limit = kw.get('limit')\n        kwargs = {'input':texts, 'model':self.name}\n        resp = openai.Embedding.create(**kwargs)\n        #\n        out = []\n        for x in resp['data']:\n            out.append({\n                 'output': x['embedding'][:limit],\n            })\n        return out", "\n\nimport inspect\ndef get_signature(fun):\n    params = inspect.signature(fun).parameters\n    param_dict = {p:{} for p in params}\n    return {\n        'name':fun.__name__,\n        'description':fun.__doc__ or '',\n        'parameters':{'type':'object','properties':param_dict}\n    }", ""]}
{"filename": "aidapter/base2.py", "chunked_list": ["from multiprocessing.dummy import Pool\nfrom retry import retry\n\n# TODO: keys ???\n# TODO: kwargs vs options ???\n# DONE: iter vs list vs single\n# DONE: as_iter\n# DONE: kwargs\n# DONE: usage\n# DONE: retry", "# DONE: usage\n# DONE: retry\n# DONE: cache\n# DONE: progress\n# DONE: batch\n# DONE: workers\n\nclass BaseModelV2:\n\n    def __init__(self, name, kwargs, options):\n        \"Initialize a model.\"\n        self.name = name\n        self.brand = 'base-v2'\n        self.usage = {}\n        self.cache = {}\n        self.workers = 2\n        self.batch = 4\n        self.options = options\n        self.model_kwargs = kwargs\n        self.retry_kwargs = None\n        self.memoize_kwargs = {'name':f'{name}:default'}\n\n\n    def transform(self, inputs:str|list[str], as_iter=False, **kwargs):\n        \"Transform a single input or a list of inputs.\"\n        if isinstance(inputs, str):\n            return list(self.transform_many([inputs], **kwargs))[0]\n        elif as_iter:\n            return self.transform_many(inputs, **kwargs) # returns generator\n        else:\n            return list(self.transform_many(inputs, **kwargs))\n\n\n    def transform_many(self, inputs: list[str], **kwargs):\n        \"Transform a list of inputs in batches / parallel workers.\"\n        data = inputs if self.batch == 1 else batched(inputs, self.batch)\n        \n        # build worker\n        def worker(_inputs):\n            return self.transform_batch(_inputs, **kwargs)\n        if self.retry_kwargs:\n            worker = retry(**self.retry_kwargs)(worker)\n        if kwargs.get('cache',True):\n            worker = self.get_memoize()(worker)\n        \n        # run workers\n        with Pool(self.workers) as pool:\n            for resp in pool.imap(worker, data):\n                yield from resp\n    \n\n    def transform_batch(self, inputs, **kwargs):\n        \"Transform a batch of inputs (MOCK implementation)\"\n        from time import sleep\n        sleep(0.5)\n        resp = inputs\n        self.register_usage({'api-calls':1})\n        return resp\n    \n    # # #\n\n    def register_usage(self, kv: dict):\n        \"Register usage metrics.\"\n        for k,v in kv.items():\n            key = self.get_usage_key(k)\n            incr(self.usage, key, v)\n\n    # # #\n\n    def get_memoize(self):\n        \"Get a memoization function.\"\n        try:\n            return self.cache.memoize(**self.memoize_kwargs)\n        except:\n            return lambda x:x\n\n    def get_usage_key(self, k):\n        \"Get a key for a given metric that will be tracked in the usage key-value store.\"\n        return f'{k}:{self.brand}:{self.name}'", "\n# HELPERS\n\nfrom itertools import islice\n\ndef batched(data, n, as_type=list):\n    \"Batch an iterable into fixed-length chunks.\"\n    it = iter(data)\n    while batch := as_type(islice(it, n)):\n        yield batch", "\ndef incr(obj, key, val):\n    \"Increment a value in a dict or diskcache (for tracking usage)\"\n    if hasattr(obj, 'incr'):\n        obj.incr(key, val)\n    else:\n        obj[key] = obj.get(key,0) + val\n\n# === SANDBOX =====================================================================================\n\nif __name__=='__main__':\n    import diskcache as dc\n    from functools import partial\n    from tqdm import tqdm\n\n    m = BaseModelV2('test',{},{})\n    m.batch = 2\n    m.workers = 1\n    print(m.transform('XXX'))\n    print(m.transform(['YYY']))\n    print(m.transform(['AAA','BBB']))\n    for data in [\"11 22 33 44 55 66 77 88 99\".split(' '), ['XX'], 'YY']:\n        pg = tqdm(total=len(data))\n        out = m.transform(data, as_iter=True)\n        for x in out:\n            pg.update(1)\n            print(x)\n    print(m.usage)", "# === SANDBOX =====================================================================================\n\nif __name__=='__main__':\n    import diskcache as dc\n    from functools import partial\n    from tqdm import tqdm\n\n    m = BaseModelV2('test',{},{})\n    m.batch = 2\n    m.workers = 1\n    print(m.transform('XXX'))\n    print(m.transform(['YYY']))\n    print(m.transform(['AAA','BBB']))\n    for data in [\"11 22 33 44 55 66 77 88 99\".split(' '), ['XX'], 'YY']:\n        pg = tqdm(total=len(data))\n        out = m.transform(data, as_iter=True)\n        for x in out:\n            pg.update(1)\n            print(x)\n    print(m.usage)", ""]}
{"filename": "tests/test_cohere.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\nimport aidapter\n\nfor model_id in ['cohere:command-light']:\n    print(f'=== {model_id} ===')\n    model = aidapter.model(model_id)\n    model.retry_tries = 1\n    print(model.complete('2+2='))\n    print(model.complete(['2+2=','7*6=']))\n    print(model.complete('2+2=', system=\"answer with words only (don't use numbers)\"))\n    print(model.complete('2+2=', debug=True))\n    print(model.usage)\n    print()", ""]}
{"filename": "tests/test_stop.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\nimport aidapter\n\nmodel_id = 'transformers:RWKV/rwkv-raven-3b'\n#model_id = 'openai:text-davinci-003'\n#model_id = 'openai:gpt-3.5-turbo'\n#model_id = 'anthropic:claude-v1'\n#model_id = 'cohere:command-light'\n\nmodel = aidapter.model(model_id)", "\nmodel = aidapter.model(model_id)\n\nprint('===')\nprint(model.complete('Alice: Hello, my name is Alice.\\nBob:'))\n\nprint('===')\nprint(model.complete('Alice: Hello, my name is Alice.\\n', start='Bob:'))\n\nprint('===')", "\nprint('===')\nprint(model.complete('Alice: Hello, my name is Alice.\\n', start='Bob:', stop=['Alice:','.']))\n"]}
{"filename": "tests/test_hf_embed.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\nfrom pprint import pprint\n\nimport aidapter\n\nmodel = aidapter.model('hf:thenlper/gte-small:embed')\n\nvector = model.embed('mighty indeed', limit=5)\npprint(vector)\n", "pprint(vector)\n\nvectors = model.embed(['this is the way', 'so say we all'], limit=5)\npprint(vectors)\n\npprint(model.usage)\n\n\nmodel = aidapter.model('hf:gpt2')\nprint(model.complete(['2+2=']))", "model = aidapter.model('hf:gpt2')\nprint(model.complete(['2+2=']))\nprint(model.complete(['2+2=','7*6=']))\n"]}
{"filename": "tests/test_openai_functions.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\n### \n\nimport aidapter\n\ndef get_weather(city):\n    \"get weather info in a city; city must be all caps after ISO country code and a : separator (e.g. FR:PARIS)\"\n    pass\n", "\nmodel = aidapter.model('openai:gpt-3.5-turbo-0613')\nx=model.complete('Whats the weather in the capital of Poland?', functions=[get_weather])\nprint(x)\n# {'function_name': 'get_weather', 'arguments': {'city': 'PL:WARSAW'}}\n"]}
{"filename": "tests/test_hf2_embed.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\nfrom pprint import pprint\nimport diskcache as dc\n\nimport aidapter\n\nfor model_id in ['huggingface:thenlper/gte-small:embed']:\n    model = aidapter.model(model_id)\n    model.cache = dc.Cache('/tmp/aidapter/hf')\n\n    pprint( model.embed(['mighty indeed'], limit=5) )\n    pprint( model.embed( 'mighty indeed',  limit=5) )\n\n    vectors = model.embed(['this is the way', 'so say we all'], limit=5, cache=False)\n    pprint(vectors)\n\n    print('USAGE', model.usage)", ""]}
{"filename": "tests/test_openai.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\nimport aidapter\n\nfor model_id in ['openai:gpt-3.5-turbo','openai:text-davinci-003']:\n    print(f'=== {model_id} ===')\n    model = aidapter.model(model_id)\n    print(model.complete('2+2='))\n    print(model.complete(['2+2=','7*6=']))\n    print(model.complete('2+2=', system=\"answer with words only (don't use numbers)\"))\n    print(model.complete('2+2=', debug=True))\n    print(model.usage)\n    print()", ""]}
{"filename": "tests/test_anthropic.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\nimport aidapter\n\nfor model_id in ['anthropic:claude-instant-v1.1']:\n    print(f'=== {model_id} ===')\n    model = aidapter.model(model_id, temperature=0.1)\n    print(model.complete('2+2='))\n    print(model.complete(['2+2=','7*6=']))\n    print(model.complete('2+2=', system=\"answer with words only (don't use numbers)\"))\n    print(model.complete('2+2=', debug=True))\n    print(model.usage)\n    print()", ""]}
{"filename": "tests/test_base.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n\nfrom aidapter.base import BaseModel\nimport shelve\n\nm = BaseModel('base',{'x':123})\nm.cache = shelve.open('/tmp/aidapter-cache.shelve')\nm.usage = shelve.open('/tmp/aidapter-usage.shelve')\nprint(m.complete('hello', stop=['aa','bb'], debug=True))\nprint(m.complete(range(13)))", "print(m.complete('hello', stop=['aa','bb'], debug=True))\nprint(m.complete(range(13)))\nprint('-'*80)\nprint(m.complete(range(13)))\nprint('-'*80)\nprint(m.complete(range(6), temperature=1.0))\nprint('-'*80)\nprint(m.cache)\nprint('-'*80)\nprint(dict(m.usage))", "print('-'*80)\nprint(dict(m.usage))\n"]}
{"filename": "tests/test_transformers.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\n#import os\n#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128' # PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128 \n\nimport aidapter\nfrom time import time\n\n#model_id = 'openai:text-ada-001'\n\n# model_id = 'transformers:roneneldan/TinyStories-33M'", "\n# model_id = 'transformers:roneneldan/TinyStories-33M'\n#model_id = 'transformers:RWKV/rwkv-raven-1b5'\n#model_id = 'transformers:tiiuae/falcon-rw-1b:trust'\n#model_id = 'transformers:RWKV/rwkv-raven-3b'\n# model_id = 'transformers:TheBloke/guanaco-7B-HF:4bit'\n# model_id = 'transformers:project-baize/baize-v2-7b:4bit'\n# model_id = 'transformers:tiiuae/falcon-7b:trust'\n# model_id = 'transformers:ehartford/Wizard-Vicuna-13B-Uncensored:4bit'\n# model_id = 'transformers:ehartford/WizardLM-30B-Uncensored:4bit'", "# model_id = 'transformers:ehartford/Wizard-Vicuna-13B-Uncensored:4bit'\n# model_id = 'transformers:ehartford/WizardLM-30B-Uncensored:4bit'\n# model_id = 'transformers:timdettmers/guanaco-33b-merged:4bit'\n#model_id = 'transformers:WizardLM/WizardCoder-15B-V1.0:4bit'\n#model_id = 'transformers:cerebras/btlm-3b-8k-base:trust'\n#model_id = 'transformers:conceptofmind/LLongMA-2-7b:trust'\n#model_id = 'transformers:mosaicml/mpt-7b-8k-instruct:trust,16bit'\n#model_id = 'transformers:WizardLM/WizardLM-13B-V1.2:4bit'\n\nmodel_id = 'transformers:NousResearch/Nous-Hermes-Llama2-13b:4bit' # BEST", "\nmodel_id = 'transformers:NousResearch/Nous-Hermes-Llama2-13b:4bit' # BEST\n\nfrom pprint import pprint\nprint(f'=== {model_id} ===')\nmodel = aidapter.model(model_id)\nmodel.retry_tries = 1\n#print(model.complete('there was a little girl who', debug=False))\n#print()\nmath_prompts = ['2+2=','7*6=','3-7=','2^8=']", "#print()\nmath_prompts = ['2+2=','7*6=','3-7=','2^8=']\npprint(model.complete(math_prompts, debug=False, limit=5, stop=[]))\npprint(model.complete(math_prompts, debug=False, limit=5, stop=[\"\\n\",\"2\"]))\nprint(model.id)\n\n# How would you compare fitd and pbta?\n\nwhile True:\n    prompt = input('> ')\n    if prompt == '': break", "while True:\n    prompt = input('> ')\n    if prompt == '': break\n    t0 = time()\n    resp = model.complete(prompt)\n    print(resp)\n    dt = time()-t0\n    print(f\"DONE in {dt:.2f} seconds ({len(resp)/dt:.2f} chars/sec)\")\n\n# NOPE:", "\n# NOPE:\n# model_id = 'transformers:RWKV/rwkv-raven-14b:8bit' # NOPE\n# model_id = 'transformers:RWKV/rwkv-raven-14b:4bit' # NOPE\n# model_id = 'transformers:openaccess-ai-collective/mpt-7b-replit-update:4bit,trust' # NOPE\n# model_id = 'transformers:tiiuae/falcon-40b:4bit,trust' # NOPE\n"]}
{"filename": "tests/test_transformers_embed.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\nfrom pprint import pprint\n\nimport aidapter\n\nmodel = aidapter.model('sentence-transformers:multi-qa-mpnet-base-dot-v1')\n\nvector = model.embed('mighty indeed', limit=5)\npprint(vector)\n", "pprint(vector)\n\nvectors = model.embed(['this is the way', 'so say we all'], limit=5)\npprint(vectors)\n\npprint(model.usage)\n"]}
{"filename": "tests/test_openai_embed.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\nfrom pprint import pprint\n\nimport aidapter\n\nmodel = aidapter.model('openai:text-embedding-ada-002')\n\nvector = model.embed('mighty indeed', limit=5)\npprint(vector)\n", "pprint(vector)\n\nvectors = model.embed(['this is the way', 'so say we all'], limit=5)\npprint(vectors)\n\npprint(model.usage)\n"]}
{"filename": "tests/test_cohere_embed.py", "chunked_list": ["import sys; sys.path[0:0] = ['.','..']\nfrom pprint import pprint\n\nimport aidapter\n\nmodel = aidapter.model('cohere:embed-english-light-v2.0')\n\nvector = model.embed('mighty indeed', limit=5)\npprint(vector)\n", "pprint(vector)\n\nvectors = model.embed(['this is the way', 'so say we all'], limit=5)\npprint(vectors)\n\npprint(model.usage)\n"]}
