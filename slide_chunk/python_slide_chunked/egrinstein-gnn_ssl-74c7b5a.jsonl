{"filename": "train.py", "chunked_list": ["import hydra\nimport pytorch_lightning as pl\nimport torch.cuda as torch_gpu\n# import torch.backends.mps as torch_mps\n\nfrom hydra.utils import get_class\nfrom omegaconf import DictConfig\nfrom pytorch_lightning import loggers as pl_loggers\nfrom pytorch_lightning.callbacks import (\n    ModelCheckpoint, RichProgressBar, TQDMProgressBar,", "from pytorch_lightning.callbacks import (\n    ModelCheckpoint, RichProgressBar, TQDMProgressBar,\n    EarlyStopping\n)\n\nfrom gnn_ssl.models.base.utils import load_checkpoint\n\nSAVE_DIR = \"logs/\"\n\n\nclass Trainer(pl.Trainer):\n    def __init__(self, config):\n\n        # 1. Load method (trainer type) specified in the config.yaml file\n        method = get_class(config[\"class\"])(config)\n\n        accelerator = \"cpu\"\n        if torch_gpu.is_available():\n            accelerator = \"cuda\"\n        # elif torch_mps.is_available():\n        #     accelerator = \"mps\"\n        # Currently disabled, as mps doesn't support fft\n        \n        training_config = config[\"training\"]\n        strategy = training_config[\"multi_gpu_strategy\"]\n        n_devices = torch_gpu.device_count()\n        if (not training_config[\"multi_gpu\"]) or (n_devices <= 1):\n            # Strategy is only necessary when using multiple GPUs\n            strategy = None\n            n_devices = 1\n\n        if training_config[\"progress_bar_type\"] == \"rich\":\n            progress_bar = RichProgressBar(\n                refresh_rate=training_config[\"progress_bar_refresh_rate\"])\n        elif training_config[\"progress_bar_type\"] == \"tqdm\":\n            progress_bar = TQDMProgressBar(\n                refresh_rate=training_config[\"progress_bar_refresh_rate\"])\n\n        # Create callbacks (Progress bar, early stopping and weight saving)\n        callbacks=[\n            progress_bar,\n            ModelCheckpoint(monitor=\"validation_loss\",\n                            save_last=True,\n                            filename='weights-{epoch:02d}-{validation_loss:.2f}'\n            )\n        ]\n        early_stopping_config = training_config[\"early_stopping\"]\n        if early_stopping_config[\"enabled\"]:\n            callbacks.append(\n                EarlyStopping(early_stopping_config[\"key_to_monitor\"],\n                              early_stopping_config[\"min_delta\"],\n                              early_stopping_config[\"patience_in_epochs\"]\n                )\n            )\n\n        super().__init__(\n            max_epochs=training_config[\"n_epochs\"],\n            callbacks=callbacks,\n            logger=[pl_loggers.TensorBoardLogger(save_dir=SAVE_DIR)],\n            accelerator=accelerator,\n            strategy=strategy,\n            log_every_n_steps=25,\n            check_val_every_n_epoch=training_config[\"check_val_every_n_epoch\"],\n            devices=n_devices,\n        )\n\n        ckpt_path = config[\"inputs_train\"][\"checkpoint_path\"]\n        if ckpt_path is not None:\n            load_checkpoint(method.model, ckpt_path)\n        \n        self._method = method\n        self.config = config\n\n    def fit(self, train_dataloaders, val_dataloaders=None):\n        super().fit(self._method, train_dataloaders,\n                    val_dataloaders=val_dataloaders)\n\n    def fit_multiple(self, train_dataloaders, val_dataloaders=None):\n        \"\"\"Fit the model sequentially for multiple datasets\"\"\"\n\n        for i, train_dataloader in enumerate(train_dataloaders):\n            val_dataloader = None\n            if val_dataloaders is not None:\n                val_dataloader = val_dataloaders[i]\n            super().fit(self._method, train_dataloader,\n                        val_dataloaders=val_dataloader)\n\n\n    def test(self, test_dataloaders):\n        super().test(self._method, test_dataloaders, ckpt_path=\"best\")", "\n\nclass Trainer(pl.Trainer):\n    def __init__(self, config):\n\n        # 1. Load method (trainer type) specified in the config.yaml file\n        method = get_class(config[\"class\"])(config)\n\n        accelerator = \"cpu\"\n        if torch_gpu.is_available():\n            accelerator = \"cuda\"\n        # elif torch_mps.is_available():\n        #     accelerator = \"mps\"\n        # Currently disabled, as mps doesn't support fft\n        \n        training_config = config[\"training\"]\n        strategy = training_config[\"multi_gpu_strategy\"]\n        n_devices = torch_gpu.device_count()\n        if (not training_config[\"multi_gpu\"]) or (n_devices <= 1):\n            # Strategy is only necessary when using multiple GPUs\n            strategy = None\n            n_devices = 1\n\n        if training_config[\"progress_bar_type\"] == \"rich\":\n            progress_bar = RichProgressBar(\n                refresh_rate=training_config[\"progress_bar_refresh_rate\"])\n        elif training_config[\"progress_bar_type\"] == \"tqdm\":\n            progress_bar = TQDMProgressBar(\n                refresh_rate=training_config[\"progress_bar_refresh_rate\"])\n\n        # Create callbacks (Progress bar, early stopping and weight saving)\n        callbacks=[\n            progress_bar,\n            ModelCheckpoint(monitor=\"validation_loss\",\n                            save_last=True,\n                            filename='weights-{epoch:02d}-{validation_loss:.2f}'\n            )\n        ]\n        early_stopping_config = training_config[\"early_stopping\"]\n        if early_stopping_config[\"enabled\"]:\n            callbacks.append(\n                EarlyStopping(early_stopping_config[\"key_to_monitor\"],\n                              early_stopping_config[\"min_delta\"],\n                              early_stopping_config[\"patience_in_epochs\"]\n                )\n            )\n\n        super().__init__(\n            max_epochs=training_config[\"n_epochs\"],\n            callbacks=callbacks,\n            logger=[pl_loggers.TensorBoardLogger(save_dir=SAVE_DIR)],\n            accelerator=accelerator,\n            strategy=strategy,\n            log_every_n_steps=25,\n            check_val_every_n_epoch=training_config[\"check_val_every_n_epoch\"],\n            devices=n_devices,\n        )\n\n        ckpt_path = config[\"inputs_train\"][\"checkpoint_path\"]\n        if ckpt_path is not None:\n            load_checkpoint(method.model, ckpt_path)\n        \n        self._method = method\n        self.config = config\n\n    def fit(self, train_dataloaders, val_dataloaders=None):\n        super().fit(self._method, train_dataloaders,\n                    val_dataloaders=val_dataloaders)\n\n    def fit_multiple(self, train_dataloaders, val_dataloaders=None):\n        \"\"\"Fit the model sequentially for multiple datasets\"\"\"\n\n        for i, train_dataloader in enumerate(train_dataloaders):\n            val_dataloader = None\n            if val_dataloaders is not None:\n                val_dataloader = val_dataloaders[i]\n            super().fit(self._method, train_dataloader,\n                        val_dataloaders=val_dataloader)\n\n\n    def test(self, test_dataloaders):\n        super().test(self._method, test_dataloaders, ckpt_path=\"best\")", "\n\ndef _create_torch_dataloaders(config):\n    dataset_paths = config[\"inputs_train\"][\"dataset_paths\"]\n    dataloader = get_class(config[\"dataset\"][\"class\"])\n\n    batch_size = config[\"training\"][\"batch_size\"]\n    n_workers = config[\"training\"][\"n_workers\"]\n    training_dataloader = dataloader(config, dataset_paths[\"training\"],\n                                     batch_size=batch_size, n_workers=n_workers, shuffle=True)\n    \n    val_dataloader = None\n    test_dataloader = None\n\n    if dataset_paths[\"validation\"]:\n        val_dataloader = dataloader(config, dataset_paths[\"validation\"],\n                                    batch_size=batch_size, n_workers=n_workers)\n    \n    if dataset_paths[\"test\"]:\n        test_dataloader = dataloader(config, dataset_paths[\"test\"],\n                                     batch_size=batch_size, n_workers=n_workers)\n\n    return (\n        training_dataloader,\n        val_dataloader,\n        test_dataloader\n    )", "\n\n@hydra.main(config_path=\"config\", config_name=\"config\", version_base=\"1.1\")\ndef main(config: DictConfig):\n    \"\"\"Runs the training procedure using Pytorch lightning\n    And tests the model with the best validation score against the test dataset. \n\n    Args:\n        config (DictConfig): Configuration automatically loaded by Hydra.\n                                        See the config/ directory for the configuration\n    \"\"\"\n\n    dataset_train, dataset_val, dataset_test = _create_torch_dataloaders(config)\n\n    trainer = Trainer(config)\n\n    if config[\"training\"][\"transfer_learning\"]:\n        trainer.fit_multiple(dataset_train, val_dataloaders=dataset_val)\n    else:\n        trainer.fit(dataset_train, val_dataloaders=dataset_val)\n\n    trainer.test(dataset_test)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "evaluate.py", "chunked_list": ["import matplotlib.pyplot as plt\nimport hydra\nimport pandas as pd\nimport seaborn as sns\nimport torch\n\nfrom hydra.utils import get_class\nfrom omegaconf import DictConfig\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm", "from torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom gnn_ssl.models.base.utils import load_checkpoint\n\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef _compute_batch_metrics(batch, batch_idx, models, metrics):\n    x, y = batch[0]\n\n    x = _dict_to_device(x, DEVICE)\n    y = _dict_to_device(y, DEVICE)\n    \n    results = {}\n\n    for model_name, model in models.items():\n        y_hat = model(x)\n        results[model_name] = {}\n        for metric_name, metric in metrics.items():\n            values = metric(y_hat, y)\n            if isinstance(values, dict): # It computed multiple metrics\n                results[model_name].update(values)\n            else:\n                results[model_name][metric_name] = values.detach().cpu()\n    return results", "def _compute_batch_metrics(batch, batch_idx, models, metrics):\n    x, y = batch[0]\n\n    x = _dict_to_device(x, DEVICE)\n    y = _dict_to_device(y, DEVICE)\n    \n    results = {}\n\n    for model_name, model in models.items():\n        y_hat = model(x)\n        results[model_name] = {}\n        for metric_name, metric in metrics.items():\n            values = metric(y_hat, y)\n            if isinstance(values, dict): # It computed multiple metrics\n                results[model_name].update(values)\n            else:\n                results[model_name][metric_name] = values.detach().cpu()\n    return results", "\n\ndef _evaluate_dataset(dataset_name: str, config: DictConfig, models: dict, metrics: dict, batch_size: int):\n    print(f\"Evaluating models on dataset '{dataset_name}'\")\n    # 1. Load evaluation dataset\n    dataset_config = config[\"dataset\"]\n    inputs = config[\"inputs_eval\"]\n\n    dataloader = get_class(dataset_config[\"class\"])\n    dataset = dataloader(config, inputs[\"dataset_paths\"][dataset_name], batch_size=batch_size)\n\n    # Only evaluate on compatible models\n    models = {\n        model_name: model[\"model\"] for model_name, model in models.items()\n        if dataset_name in model[\"evaluate_on\"]\n    }\n\n    metrics = _compute_metrics(dataset_name, dataset, models, metrics)\n\n    return metrics", "\n\ndef load_models(config):\n    evaluation_config = config[\"evaluation\"]\n    inputs = config[\"inputs_eval\"]\n\n    # 1. Load models\n    models = {}\n\n    for model_name, model_config in evaluation_config[\"models\"].items():\n        evaluate_on = model_config[\"evaluate_on\"]\n        model_config = model_config[\"model\"]\n        model = get_class(model_config[\"class\"])(model_config)\n\n        models[model_name] = {\n            \"model\": model.eval(),\n            \"evaluate_on\": evaluate_on\n        }\n    \n    # 2. Load model checkpoints for trained methods (i.e. not classical signal processing ones)\n    for model_name, checkpoint_path in inputs[\"checkpoint_paths\"].items():\n        if model_name in models:\n            load_checkpoint(models[model_name][\"model\"], checkpoint_path)\n\n    return models", "\n\ndef _load_metrics(config):\n    metrics = {}\n    for metric_name, metric_config in config[\"evaluation\"][\"metrics\"].items():\n        metric_class = metric_config[\"class\"]\n        metric_config = metric_config[\"config\"] if \"config\" in metric_config else {}\n        metric = get_class(metric_class)(**metric_config)\n        metrics[metric_name] = metric\n\n    return metrics", "\n\ndef _compute_metrics(dataset_name: str, dataloader: DataLoader, models: dict, metrics: dict):\n    if not models:\n        return pd.DataFrame()\n\n    # 1. Compute metrics for all batches\n    batch_metrics = []\n    for i, batch in enumerate(tqdm(dataloader)):\n        batch_metrics.append(\n            _compute_batch_metrics(batch, i, models, metrics)\n        )\n\n    # 2. Group metrics for all batches\n    metrics = _group_batch_metrics(batch_metrics, dataset_name)\n\n    # 3. Compute aggregate metrics\n    _plot_histograms(metrics, dataset_name)\n    \n    return metrics", "\n\ndef _group_batch_metrics(batch_metrics: dict, dataset_name: str):\n    \"Group batch metrics into a Pandas dataframe\"\n    metrics = {}\n    df_metrics = []\n\n    model_names = list(batch_metrics[0].keys())\n    metric_names = list(batch_metrics[0][model_names[0]].keys())\n\n    for model_name in batch_metrics[0].keys():\n        metrics[model_name] = {}\n        metric_names = batch_metrics[0][model_name].keys()\n        for metric_name in metric_names:\n            errors = torch.cat([result[model_name][metric_name] for result in batch_metrics])\n            metrics[model_name][metric_name] = errors\n\n            df = pd.DataFrame.from_dict({\n                \"value\": errors.tolist(),\n                \"model\": [model_name]*len(errors),\n                \"metric\": [metric_name]*len(errors),\n                \"dataset\": [dataset_name]*len(errors)} ,orient='index').transpose()\n                \n            df_metrics.append(df)\n\n    return pd.concat(df_metrics)", "\n\ndef _plot_histograms(df: pd.DataFrame, dataset_name: str):\n    \"\"\"Plot one histogram per metric, comparing all models for a given dataset\n    \"\"\"\n\n    # the first key in the dict is the model, the second is the metric\n    \n    metric_names = df[\"metric\"].unique()\n\n    # Create a plot for each metric\n    for metric_name in metric_names:\n        fig, ax = plt.subplots()\n        ax.set_title(f\"{metric_name}\")\n        df_metric = df[df[\"metric\"] == metric_name]\n        try:\n            sns.histplot(\n                df_metric, x=\"value\",\n                hue=\"model\", multiple=\"layer\",\n                palette=\"Blues\",\n                ax=ax\n            )\n        except TypeError:\n            continue\n\n        ax.set_xlabel(\"Error (m)\")\n        ax.set_ylabel(\"Num. test cases\")\n        plt.savefig(f\"outputs/{dataset_name}_{metric_name}.pdf\")", "\n\n@hydra.main(config_path=\"config\", config_name=\"config\", version_base=None)\ndef main(config: DictConfig):\n    \"\"\"Evaluate the model and produce histograms\n    Args:\n        config (DictConfig): Configuration automatically loaded by Hydra.\n                                        See the config/ directory for the configuration\n    \"\"\"\n\n    # 1. Load models and checkpoints\n    models = load_models(config)\n\n    # 2. Load metrics\n    metrics = _load_metrics(config)\n\n    # 3. Evaluate metrics for every model, for each dataset\n    inputs = config[\"inputs_eval\"]\n    dataset_names = list(inputs[\"dataset_paths\"].keys())\n    batch_size = config[\"evaluation\"][\"batch_size\"]\n\n    results = []\n    for dataset_name in dataset_names:\n        dataset_results = _evaluate_dataset(dataset_name, config, models, metrics, batch_size)\n        results.append(dataset_results)\n\n    results = pd.concat(results, ignore_index=True)\n    \n    results.to_csv(\"results.csv\")", "\n\ndef _dict_to_device(d, device):\n    new_d = {}\n    for key, value in d.items():\n        if isinstance(value, torch.Tensor):\n            new_d[key] = value.to(device)\n        elif isinstance(value, dict):\n            new_d[key] = _dict_to_device(value, device)\n    return d", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "plots.py", "chunked_list": ["# Plots for the interspeech2023 submission\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\ndef make_plots(results_csv_path):\n    results = pd.read_csv(results_csv_path)\n\n    # TODO: Make lots of \"group by\" instead enumerating specifics.\n    # Create a \"dataset_type\" parameter and a \"model_type\" parameter\n    # Which would allow grouping of datasets of the same class (4 mics, for example)\n    # and methods of the same class (CRNN for example).\n\n    dataset_names = list(results[\"dataset\"].unique())\n    model_names = list(results[\"model\"].unique())\n    \n    # 1. Results for individual datasets\n    for dataset_name in dataset_names:\n        df = results[results[\"dataset\"] == dataset_name]\n        _plot_histogram(df, dataset_name, \"model\")\n\n    # 2. Results for number of mics\n    for dataset_group in [\"4 mics\", \"6 mics\"]:\n        df = results[results[\"dataset\"].str.contains(dataset_group)]\n        _plot_histogram(df, dataset_group, \"model\")\n\n    # 3. Results for reverb/recorded\n    for dataset_group in [\"reverb\", \"recorded\"]:\n        df = results[results[\"dataset\"].str.contains(dataset_group)]\n        _plot_histogram(df, dataset_group, \"model\")\n    \n    # 4. Results for individual methods\n    for model_name in model_names:\n        df = results[results[\"model\"] == model_name]\n        _plot_histogram(df, model_name, \"dataset\")", "def make_plots(results_csv_path):\n    results = pd.read_csv(results_csv_path)\n\n    # TODO: Make lots of \"group by\" instead enumerating specifics.\n    # Create a \"dataset_type\" parameter and a \"model_type\" parameter\n    # Which would allow grouping of datasets of the same class (4 mics, for example)\n    # and methods of the same class (CRNN for example).\n\n    dataset_names = list(results[\"dataset\"].unique())\n    model_names = list(results[\"model\"].unique())\n    \n    # 1. Results for individual datasets\n    for dataset_name in dataset_names:\n        df = results[results[\"dataset\"] == dataset_name]\n        _plot_histogram(df, dataset_name, \"model\")\n\n    # 2. Results for number of mics\n    for dataset_group in [\"4 mics\", \"6 mics\"]:\n        df = results[results[\"dataset\"].str.contains(dataset_group)]\n        _plot_histogram(df, dataset_group, \"model\")\n\n    # 3. Results for reverb/recorded\n    for dataset_group in [\"reverb\", \"recorded\"]:\n        df = results[results[\"dataset\"].str.contains(dataset_group)]\n        _plot_histogram(df, dataset_group, \"model\")\n    \n    # 4. Results for individual methods\n    for model_name in model_names:\n        df = results[results[\"model\"] == model_name]\n        _plot_histogram(df, model_name, \"dataset\")", "\n\n# def _plot_histogram(df, dataset_name, hue, ax=None):\n#     if not ax:\n#         fig, ax = plt.subplots()\n#     sns.histplot(\n#         df, x=\"value\",\n#         hue=hue, multiple=\"dodge\",\n#         palette=\"Blues\",\n#         ax=ax,", "#         palette=\"Blues\",\n#         ax=ax,\n#         bins=15\n#     )\n\n#     ax.set_title(dataset_name)\n#     ax.set_xlabel(\"Error (m)\")\n#     ax.set_ylabel(\"Num. test cases\")\n#     plt.savefig(f\"outputs/{dataset_name}.pdf\")\n", "#     plt.savefig(f\"outputs/{dataset_name}.pdf\")\n\n\ndef _plot_histogram(df, dataset_name, group_by, ax=None):\n    fig, ax = plt.subplots()\n    cmap = mpl.cm.get_cmap('Set2')\n    ax.set_title(f\"{group_by}\")\n\n    groups = df[group_by].unique()\n    n_groups = len(groups)\n\n    for i, key in enumerate(groups):\n        color = cmap((i+1)/n_groups)\n        values = df[df[group_by] == key][\"value\"]\n        counts, bins = np.histogram(values, bins=30)\n        ax.stairs(counts, bins, label=key, color=color)\n        try:\n            mean = values.mean()\n            ax.axvline(x=mean, color=color, alpha=0.2, linestyle=\"--\",\n                        label=\"mean={:.2f} m\".format(mean))\n        except RuntimeError:\n            pass\n    ax.legend()\n    ax.set_xlabel(\"Error (m)\")\n    ax.set_ylabel(\"Num. test cases\")\n    plt.savefig(f\"outputs/{dataset_name}.pdf\")", "\ndef make_tables(results_csv_path):\n    results = pd.read_csv(results_csv_path)\n    df_mean = []\n\n    dataset_names = results[\"dataset\"].unique()\n    model_names = results[\"model\"].unique()\n\n    for dataset_name in dataset_names:\n        df_dataset = results[results[\"dataset\"] == dataset_name]\n        for model_name in model_names:\n            df_model = df_dataset[df_dataset[\"model\"] == model_name]\n            df_mean.append({\n                \"Dataset\": dataset_name,\n                \"Model\": model_name,\n                \"Value\": df_model[\"value\"].mean(),\n                \"Std.\": df_model[\"value\"].std()\n            })\n\n    df_mean = pd.DataFrame(df_mean)\n    df_mean.to_csv(\"means.csv\")", "\n\nif __name__ == \"__main__\":\n    make_plots(\"results.csv\")\n    make_tables(\"results.csv\")\n"]}
{"filename": "visualize_outputs.py", "chunked_list": ["import hydra\n\nfrom hydra.utils import get_class\nfrom omegaconf import DictConfig\n\nfrom gnn_ssl.visualization import create_visualizations\nfrom evaluate import load_models\n\n\n@hydra.main(config_path=\"config\", config_name=\"config\", version_base=None)\ndef main(config: DictConfig):\n    \"\"\"Produce visualization of the model output\n    Args:\n        config (DictConfig): Configuration automatically loaded by Hydra.\n                                        See the config/ directory for the configuration\n    \"\"\"\n\n    # 1. Load evaluation dataset\n    dataloader = get_class(config[\"dataset\"][\"class\"])\n    inputs = config[\"inputs_eval\"]\n\n    dataset_names = list(inputs[\"dataset_paths\"].keys()) \n    dataset_path = inputs[\"dataset_paths\"][dataset_names[0]] # Only select first dataset right now, change this in future\n    dataset_test = dataloader(config, dataset_path, shuffle=False)\n\n    # 2. Load models\n    models = load_models(config)\n\n    # 3. Load loss function\n    loss = get_class(config[\"targets\"][\"loss_class\"])(config[\"targets\"])\n    \n    # Compute outputs for a single batch\n    x, y = batch = next(iter(dataset_test))[0]\n\n    model_outputs = {\n        model_name: model[\"model\"](x)\n        for model_name, model in models.items()\n    }\n\n    create_visualizations(model_outputs, y, loss, plot_target=False)", "\n@hydra.main(config_path=\"config\", config_name=\"config\", version_base=None)\ndef main(config: DictConfig):\n    \"\"\"Produce visualization of the model output\n    Args:\n        config (DictConfig): Configuration automatically loaded by Hydra.\n                                        See the config/ directory for the configuration\n    \"\"\"\n\n    # 1. Load evaluation dataset\n    dataloader = get_class(config[\"dataset\"][\"class\"])\n    inputs = config[\"inputs_eval\"]\n\n    dataset_names = list(inputs[\"dataset_paths\"].keys()) \n    dataset_path = inputs[\"dataset_paths\"][dataset_names[0]] # Only select first dataset right now, change this in future\n    dataset_test = dataloader(config, dataset_path, shuffle=False)\n\n    # 2. Load models\n    models = load_models(config)\n\n    # 3. Load loss function\n    loss = get_class(config[\"targets\"][\"loss_class\"])(config[\"targets\"])\n    \n    # Compute outputs for a single batch\n    x, y = batch = next(iter(dataset_test))[0]\n\n    model_outputs = {\n        model_name: model[\"model\"](x)\n        for model_name, model in models.items()\n    }\n\n    create_visualizations(model_outputs, y, loss, plot_target=False)", "    \n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "gnn_ssl/metrics.py", "chunked_list": ["from turtle import forward\nimport torch\n\nfrom torch.nn import Module\n\nimport pysoundloc.pysoundloc.metrics as ssl_metrics\n\nfrom pysoundloc.pysoundloc.target_grids import (\n    create_target_gaussian_grids, create_target_hyperbolic_grids\n)", "    create_target_gaussian_grids, create_target_hyperbolic_grids\n)\n\n\nclass Loss(Module):\n    def __init__(self, config, dim=None):\n        super().__init__()\n\n        self.config = config\n\n        self.loss = NormLoss(config[\"loss\"], dim=dim)\n\n        self.weighted = config[\"weighted_loss\"]\n\n        self.network_output_type = config[\"type\"]\n        if self.network_output_type == \"grid\":\n            self.grid_generator = LikelihoodGrid(\n                config[\"sigma\"],\n                config[\"n_points_per_axis\"],\n                config[\"hyperbolic_likelihood_weight\"],\n                config[\"gaussian_likelihood_weight\"],\n                normalize=config[\"normalize\"],\n                squared=config[\"squared\"]\n            )\n\n    def forward(self, model_output, targets, mean_reduce=True):\n\n        # 1. Prepare targets\n        if self.network_output_type == \"grid\":\n            targets = self.grid_generator(\n                targets[\"room_dims\"][..., :2], # :2 is because it's a 2-D grid\n                targets[\"source_coordinates\"][..., :2],\n                targets[\"mic_coordinates\"][..., :2],\n            )\n        elif self.network_output_type == \"source_coordinates\":\n            targets = targets[\"source_coordinates\"]\n\n        model_output = model_output[self.network_output_type]\n\n        # 2. Assert they have the same shape as output\n        if model_output.shape != targets.shape:\n            raise ValueError(\n                \"Model output's shape is {}, target's is {}\".format(\n                    model_output.shape, targets.shape\n            ))\n\n        # Compute loss\n        loss = self.loss(model_output, targets)\n        if self.weighted:\n            loss *= targets\n        \n        if mean_reduce:\n            if self.weighted:\n                loss = loss.sum()/loss.shape[0]\n            else:\n                loss = loss.mean()\n        \n        return loss", "    \n\nclass LikelihoodGrid(Module):\n    def __init__(self, sigma=1, n_points_per_axis=25,\n                 hyperbolic_weight=0.5, gaussian_weight=0.5, normalize=True,\n                 squared=False):\n        super().__init__()\n\n        self.sigma = sigma\n        self.n_points_per_axis = n_points_per_axis\n    \n        self.hyperbolic_weight = hyperbolic_weight\n        self.gaussian_weight = gaussian_weight\n        self.normalize = normalize\n        self.squared = squared\n\n    def forward(self, room_dims, source_coordinates, mic_coordinates=None):\n        batch_size = room_dims.shape[0]\n\n        if mic_coordinates.shape[1] == 2:\n            # Mixed grid is only used for two microphones\n            gaussian_weight = self.gaussian_weight\n            hyperbolic_weight = self.hyperbolic_weight\n        else:\n            # Compute gaussian grid only\n            gaussian_weight = 1\n            hyperbolic_weight = 0\n\n        grids = []\n        if gaussian_weight > 0:\n            grids.append(\n                create_target_gaussian_grids(\n                        source_coordinates, room_dims,\n                        self.n_points_per_axis,\n                        self.sigma,\n                        squared=self.squared)*gaussian_weight\n            )\n        if hyperbolic_weight > 0:\n            if mic_coordinates is None:\n                raise ValueError(\n                    \"mic_coordinates must be provided when computing hyperbolic grid\")\n            \n            grids.append(\n                create_target_hyperbolic_grids(\n                    source_coordinates,\n                    mic_coordinates[:, 0], mic_coordinates[:, 1],\n                    room_dims, self.n_points_per_axis,\n                    self.sigma,\n                    squared=self.squared)*hyperbolic_weight\n            )\n        grids = torch.stack(grids)\n\n        grids = grids.sum(dim=0)\n        \n        if self.normalize:\n            max_grids = grids.abs().flatten(start_dim=1).max(dim=1)[0]\n            min_grids = grids.abs().flatten(start_dim=1).min(dim=1)[0]\n\n            max_grids = max_grids.unsqueeze(dim=1).unsqueeze(dim=2)\n            min_grids = min_grids.unsqueeze(dim=1).unsqueeze(dim=2)\n            \n            # perform min-max normalization\n            grids = (grids - min_grids)/(max_grids - min_grids)\n\n        return grids", "\n\nclass NormLoss(Module):\n    def __init__(self, norm_type=\"l1\", dim=1, key=None):\n        super().__init__()\n\n        if norm_type not in [\"l1\", \"l2\", \"squared\"]:\n            raise ValueError(\"Supported norms are 'l1', 'l2', and 'squared'\")\n        self.norm_type = norm_type\n        self.dim = dim\n        self.key = key\n\n    def forward(self, model_output, targets, mean_reduce=False):\n        # targets = targets.to(model_output.device)\n        if self.key:\n            model_output = model_output[self.key]\n            targets = targets[self.key]\n        error = model_output - targets\n        if self.norm_type == \"l1\":\n            normed_error = error.abs()\n        elif self.norm_type == \"l2\" or self.norm_type == \"squared\":\n            normed_error = error**2\n        \n        if self.dim is not None: # Sum along the vector dimension\n            normed_error = torch.sum(normed_error, dim=self.dim)\n\n        if self.norm_type == \"l2\":\n            normed_error = torch.sqrt(normed_error)\n\n        if mean_reduce:\n            normed_error = normed_error.mean()\n\n        return normed_error", "\n\nclass SourceDistance(NormLoss):\n    def __init__(self):\n        super().__init__(\"l2\", 1, \"source_coordinates\")\n    \n    def forward(self, model_output, targets):\n        if len(targets[\"source_coordinates\"].shape) == 3: \n            targets[\"source_coordinates\"] = targets[\"source_coordinates\"][:, 0, :2]\n        return super().forward(model_output, targets)", "\n\nclass SslMetrics(ssl_metrics.SslMetrics):\n    def forward(self, model_output, targets):\n        model_output = model_output[\"source_coordinates\"]\n        room_dims = targets[\"room_dims\"]\n        targets = targets[\"source_coordinates\"][:, :2]\n        return super().forward(model_output, targets, room_dims)\n\n\nclass ExampleLoss(torch.nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = config\n\n    def forward(self, model_output, targets, mean_reduce=True):\n        x = model_output[\"example_output\"]\n\n        # Don't do anything with targets, just return the output\n        if mean_reduce:\n            x = x.mean()\n        \n        return x", "\n\nclass ExampleLoss(torch.nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = config\n\n    def forward(self, model_output, targets, mean_reduce=True):\n        x = model_output[\"example_output\"]\n\n        # Don't do anything with targets, just return the output\n        if mean_reduce:\n            x = x.mean()\n        \n        return x", ""]}
{"filename": "gnn_ssl/__init__.py", "chunked_list": [""]}
{"filename": "gnn_ssl/visualization.py", "chunked_list": ["import matplotlib.pyplot as plt\n\nfrom pysoundloc.pysoundloc.visualization import plot_grid\n\n\ndef create_visualizations(model_outputs, y, loss, plot_target=False):\n    # This function is specific for the NeuralSrp method\n\n    model_names = list(model_outputs.keys())\n    n_models = len(model_names)\n    batch_size = model_outputs[model_names[0]][\"grid\"].shape[0]\n\n    mic_coords = y[\"mic_coordinates\"][:, :, :2]\n    room_dims = y[\"room_dims\"][..., :2]\n    source_coords = y[\"source_coordinates\"][..., :2]\n    \n    target_grids = loss.grid_generator(\n            room_dims,\n            source_coords,\n            mic_coords,\n    )\n\n    n_plots = n_models\n    if plot_target:\n        n_plots +=1\n\n    for i in range(batch_size):\n        fig, axs = plt.subplots(nrows=n_plots, figsize=(5, 5))\n\n        # Plot model outputs\n        for j, model_name in enumerate(model_names):\n            plot_grid(model_outputs[model_name][\"grid\"][i].detach(),\n                      room_dims[i], source_coords=source_coords[i],\n                      microphone_coords=mic_coords[i], log=False, ax=axs[j])\n            axs[j].set_title(model_name)\n            if j < n_plots - 1:\n                axs[j].get_xaxis().set_visible(False)\n\n        # Plot target\n        if plot_target:\n            plot_grid(target_grids[i], room_dims[i], source_coords=source_coords[i],\n                    microphone_coords=mic_coords[i], log=False, ax=axs[n_models])\n            axs[n_models].set_title(\"Target grid\")\n\n        plt.tight_layout()\n        plt.savefig(f\"{i}.pdf\")", ""]}
{"filename": "gnn_ssl/feature_extractors/metadata.py", "chunked_list": ["import torch\nfrom copy import deepcopy\n\n\ndef format_metadata(metadata, use_rt60_as_metadata=False):\n    mic_coords = metadata[\"mic_coordinates\"]\n\n    if len(mic_coords.shape) == 3: # Multi device\n        mic_coords = mic_coords[0]\n\n    room_dims = metadata[\"room_dims\"]\n\n    m = {\n        \"local\": {\n            \"mic_coordinates\": mic_coords\n        },\n        \"global\": {\n            \"room_dims\": room_dims,\n        }\n    }\n    if use_rt60_as_metadata:\n        m[\"global\"][\"rt60\"] = torch.Tensor([metadata[\"rt60\"]])\n\n    return m", "\n\ndef flatten_metadata(metadata, metadata_dim=3):\n    \"\"\"Transform the metadata dictionary where the values are batches of metadata\n        into a batch of 1D torch vectors.\n    Args:\n        metadata (dict): Dictionary containing the keys [\"local\"][\"mic_coords\"]\n                         [\"global\"][\"room_dims\"] and [\"global\"][\"rt60\"] (optional)\n        metadata_dim (int): Whether to give 3D or 2D mic coordinates and room dims \n\n    Returns:\n        torch.Tensor: a matrix with flattened metadata for each batch\n    \"\"\"\n\n    mic_coords = metadata[\"local\"][\"mic_coordinates\"][:, :, :metadata_dim]\n    room_dims = metadata[\"global\"][\"room_dims\"][:, :metadata_dim]\n\n\n    flattened_metadata = torch.cat([\n        mic_coords.flatten(start_dim=1),\n        room_dims\n    ], dim=1)\n    if \"rt60\" in metadata[\"global\"].keys():\n        flattened_metadata = torch.cat([\n            flattened_metadata,\n            metadata[\"global\"][\"rt60\"]], dim=1\n        )\n    \n    return flattened_metadata", "\n\ndef filter_local_metadata(metadata, idxs):\n    result = deepcopy(metadata)\n    result[\"local\"][\"mic_coordinates\"] = result[\"local\"][\"mic_coordinates\"][:, idxs] \n\n    return result\n"]}
{"filename": "gnn_ssl/feature_extractors/__init__.py", "chunked_list": ["import math\nimport torch\nimport torch.nn as nn\n\nfrom omegaconf import OmegaConf\n\nfrom .stft import CrossSpectra, StftArray\n\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = OmegaConf.to_object(config)\n\n        self.n_input_seconds = config[\"dataset\"][\"n_input_seconds\"]\n        self.config = config\n\n        feature_name = config[\"type\"]\n\n        self.n_output_channels = 2\n        if feature_name.startswith(\"stft\"):\n            if feature_name == \"stft\":\n                self.n_output_channels = 4 # 2 mics, each with a real and imaginary channel\n                mag_only = phase_only = real_only = False\n            elif feature_name == \"stft_phase\":\n                mag_only = real_only = False\n                phase_only = True\n            elif feature_name == \"stft_mag\":\n                phase_only = real_only = False\n                mag_only = True\n            elif feature_name == \"stft_real\":\n                phase_only = mag_only = False\n                real_only = True\n            else:\n                raise ValueError(f\"{feature_name} is not a valid feature extractor\")\n            self.model = StftArray(\n                is_complex=False, complex_as_channels=True,\n                real_only=real_only, mag_only=mag_only, phase_only=phase_only,\n                n_dft=config[\"n_dft\"], hop_size=config[\"hop_size\"]\n            )\n        elif feature_name == \"cross_spectral_phase\":\n            self.model = CrossSpectra(phase_only=True,\n                            n_dft=config[\"n_dft\"], hop_size=config[\"hop_size\"])\n            self.n_output_channels = 1\n    \n        self.n_output = self._get_output_shape()\n\n    def forward(self, x):\n        return self.model(x[\"signal\"])\n\n    def _get_output_shape(self):\n        n_input_samples = self.n_input_seconds*self.config[\"dataset\"][\"sr\"]\n\n        out_width = math.ceil((n_input_samples)/self.config[\"hop_size\"])\n        out_height = self.config[\"n_dft\"]//2 # /2 as we use \"onesided\" dft\n        \n        return (out_width, out_height)", "\nclass FeatureExtractor(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = OmegaConf.to_object(config)\n\n        self.n_input_seconds = config[\"dataset\"][\"n_input_seconds\"]\n        self.config = config\n\n        feature_name = config[\"type\"]\n\n        self.n_output_channels = 2\n        if feature_name.startswith(\"stft\"):\n            if feature_name == \"stft\":\n                self.n_output_channels = 4 # 2 mics, each with a real and imaginary channel\n                mag_only = phase_only = real_only = False\n            elif feature_name == \"stft_phase\":\n                mag_only = real_only = False\n                phase_only = True\n            elif feature_name == \"stft_mag\":\n                phase_only = real_only = False\n                mag_only = True\n            elif feature_name == \"stft_real\":\n                phase_only = mag_only = False\n                real_only = True\n            else:\n                raise ValueError(f\"{feature_name} is not a valid feature extractor\")\n            self.model = StftArray(\n                is_complex=False, complex_as_channels=True,\n                real_only=real_only, mag_only=mag_only, phase_only=phase_only,\n                n_dft=config[\"n_dft\"], hop_size=config[\"hop_size\"]\n            )\n        elif feature_name == \"cross_spectral_phase\":\n            self.model = CrossSpectra(phase_only=True,\n                            n_dft=config[\"n_dft\"], hop_size=config[\"hop_size\"])\n            self.n_output_channels = 1\n    \n        self.n_output = self._get_output_shape()\n\n    def forward(self, x):\n        return self.model(x[\"signal\"])\n\n    def _get_output_shape(self):\n        n_input_samples = self.n_input_seconds*self.config[\"dataset\"][\"sr\"]\n\n        out_width = math.ceil((n_input_samples)/self.config[\"hop_size\"])\n        out_height = self.config[\"n_dft\"]//2 # /2 as we use \"onesided\" dft\n        \n        return (out_width, out_height)", "\n\ndef get_stft_output_shape(feature_config):\n    n_input_samples = feature_config[\"dataset\"][\"n_input_seconds\"]*feature_config[\"dataset\"][\"sr\"]\n\n    out_width = math.ceil((n_input_samples)/feature_config[\"hop_size\"])\n    out_height = feature_config[\"n_dft\"]//2 # /2 as we use \"onesided\" dft\n    \n    return torch.Tensor((out_width, out_height)).int()\n", ""]}
{"filename": "gnn_ssl/feature_extractors/pairwise_feature_extractors.py", "chunked_list": ["import torch\n\nfrom torch.nn import Module\n\nfrom pysoundloc.pysoundloc.gcc import gcc_phat_batch\nfrom pysoundloc.pysoundloc.srp import compute_pairwise_srp_grids\n\n\n# Every pairwise feature extractor inherits from the torch.nn.Module class\n# and has a n_output variable.", "# Every pairwise feature extractor inherits from the torch.nn.Module class\n# and has a n_output variable.\n# Furthermore, it implements a \"forward\" function with three parameters:\n# A tensor x and two indices i and j\n\n\nclass GccPhat(Module):\n    def __init__(self, sr, n_bins, normalize=True):\n        self.sr = sr\n        self.n_output = n_bins\n        self.normalize = normalize\n        super().__init__()\n    \n    def forward(self, x, i, j):\n        x_i = x[\"signal\"][:, i]\n        x_j = x[\"signal\"][:, j]\n\n        batch_size, n_signal = x_i.shape\n        results = []\n\n        results = gcc_phat_batch(x_i, x_j, self.sr)[0]\n        # Only select central self.n_output\n        results = results[:, (n_signal - self.n_output)//2:(n_signal + self.n_output)//2]\n\n        if self.normalize:\n            results /= results.max(dim=1, keepdims=True)[0]\n        return results", "\n\nclass SpatialLikelihoodGrid(Module):\n    def __init__(self, sr, n_grid_points, flatten=True, normalize=False):\n        super().__init__()\n        self.sr = sr\n        self.n_grid_points = n_grid_points\n\n        self.flatten = flatten\n        self.normalize = normalize\n\n        if flatten:\n            self.n_output = n_grid_points**2\n        else:\n            self.n_output = (n_grid_points, n_grid_points)\n    \n    def forward(self, x, i, j):\n        x_i = x[\"signal\"][:, i]\n        x_j = x[\"signal\"][:, j]\n\n        mic_i_coords = x[\"local\"][\"mic_coordinates\"][:, i]\n        mic_j_coords = x[\"local\"][\"mic_coordinates\"][:, j]\n        room_dims = x[\"global\"][\"room_dims\"]\n\n        grids = compute_pairwise_srp_grids(\n            x_i, x_j, self.sr,\n            mic_i_coords, mic_j_coords,\n            room_dims,\n            n_grid_points=self.n_grid_points\n        )\n\n        if self.flatten:\n            grids = grids.flatten(start_dim=1)\n            if self.normalize:\n                grids /= grids.max(dim=-1)[0].unsqueeze(1)\n\n        return grids", "\n   \nclass MetadataAwarePairwiseFeatureExtractor(Module):\n    def __init__(self, n_input, pairwise_feature_extractor=None,\n                 is_metadata_aware=True, use_rt60_as_metadata=True):\n        super().__init__()\n\n        self.pairwise_feature_extractor = pairwise_feature_extractor\n        \n        if pairwise_feature_extractor is None:\n            self.n_output = 2*n_input # 2 channels will be concatenated\n        else:\n            self.n_output = pairwise_feature_extractor.n_output\n\n        self.is_metadata_aware = is_metadata_aware\n        self.use_rt60_as_metadata = use_rt60_as_metadata\n\n        if is_metadata_aware:\n            self.n_output += 2*2*2 + 2 # 2 coordinates times 2 microphones per array + 2 room_dims\n\n            if use_rt60_as_metadata:\n                self.n_output += 1\n\n    def forward(self, x, i, j):\n        # 1. Apply pairwise feature extractor, if provided. Else, simply concat the signals\n        if self.pairwise_feature_extractor is not None:\n            x_ij = self.pairwise_feature_extractor(x, i, j)\n        else:\n            x_ij = torch.cat([x[\"signal\"][:, i], x[\"signal\"][:, j]], dim=1)\n\n        if not self.is_metadata_aware:\n            return x_ij\n        \n        # 2. Concatenate local metadata\n        mic_coords = x[\"local\"][\"mic_coordinates\"]\n        room_dims = x[\"global\"][\"room_dims\"]\n        x_ij = torch.cat([\n            x_ij, mic_coords[:, i].flatten(start_dim=1), mic_coords[:, j].flatten(start_dim=1),\n            room_dims], dim=1)\n\n        if not self.use_rt60_as_metadata:\n            return x_ij\n\n        # 3. Concatenate global_metadata        \n        rt60 = x[\"global\"][\"rt60\"]\n        x_ij = torch.cat([x_ij, room_dims, rt60], dim=1)\n\n        return x_ij", "\n\nclass ArrayWiseSpatialLikelihoodGrid(Module):\n    \"\"\"\n    Compute the cumulative Spatial Likelihood Function (SLF)\n    using the signals of a microphone array, their respective microphone coordinates\n    and the room dimensions.\n\n    If microphone_wise == True, one cumulative SLF will be produced for each microphone,\n    using the correlations between it and the other ones.\n    \"\"\"\n\n    def __init__(self, sr, n_grid_points, flatten=True, thickness=10):\n        super().__init__()\n        self.sr = sr\n        self.n_grid_points = n_grid_points\n        self.thickness = thickness\n\n        self.flatten = flatten\n        if flatten:\n            self.n_output = n_grid_points**2\n        else:\n            self.n_output = (n_grid_points, n_grid_points)\n\n    def forward(self, x):\n        x_signal = x[\"signal\"]\n        mic_coords = x[\"metadata\"][\"local\"][\"mic_coordinates\"]\n        room_dims = x[\"metadata\"][\"global\"][\"room_dims\"]\n        if x_signal.ndim == 3:\n            # Single array\n            x_signal = x_signal.unsqueeze(1)\n        batch_size, n_arrays, n_array, n_signal = x_signal.shape\n\n        grids = torch.zeros((\n            batch_size, n_arrays,\n            self.n_grid_points, self.n_grid_points\n        ))\n\n        for k in range(n_arrays):\n            for i in range(n_array):\n                for j in range(i + 1, n_array):\n                    grid_ij = compute_pairwise_srp_grids(\n                            x_signal[:, k, i], x_signal[:, k, j], self.sr,\n                            mic_coords[:, k, i], mic_coords[:, k, j],\n                            room_dims, n_grid_points=self.n_grid_points,\n                            n_correlation_neighbours=self.thickness\n                    )\n                    grids[:, k] += grid_ij\n\n        if self.flatten:\n            grids = grids.flatten(start_dim=2)\n            \n        # Normalize grids\n        # Only works on flattened!\n        max_grids = grids.max(dim=2)[0].unsqueeze(2)\n        grids /= max_grids\n\n        x[\"signal\"] = grids\n        return x", ""]}
{"filename": "gnn_ssl/feature_extractors/stft.py", "chunked_list": ["from calendar import c\nimport torch\n\nfrom torch.nn import Module\n\n\nclass StftArray(Module):\n    def __init__(self, n_dft=1024, hop_size=512, window_length=None,\n                 onesided=True, is_complex=True, complex_as_channels=False,\n                 mag_only=False, phase_only=False, real_only=False):\n\n        super().__init__()\n\n        self.n_dft = n_dft\n        self.hop_size = hop_size\n        self.onesided = onesided\n        self.is_complex = is_complex\n        self.complex_as_channels = complex_as_channels\n\n        self.mag_only = mag_only\n        self.phase_only = phase_only\n        self.real_only = real_only\n        self.window_length = n_dft if window_length is None else window_length\n        \n    def forward(self, x: torch.Tensor):\n        \"Expected input has shape (batch_size, n_channels, time_steps)\"\n\n        input_shape = x.shape\n\n        if len(input_shape) == 3:\n            # (batch_size, n_channels, time_steps) => Microphone array\n            # Collapse channels into batch\n            x = x.flatten(end_dim=1)\n\n\n        window = torch.hann_window(self.window_length, device=x.device)\n        y = torch.stft(x, self.n_dft, hop_length=self.hop_size, \n                       onesided=self.onesided, return_complex=True,\n                       win_length=self.window_length, window=window)\n        y = y[:, 1:] # Remove DC component (f=0hz)\n\n        y = y.transpose(1, 2)\n        # y.shape == (batch_size*channels, time, freqs)\n\n        if len(input_shape) == 3:\n            batch_size, num_channels, _ = input_shape\n            # De-collapse first dim into batch and channels\n            y = y.unflatten(0, (batch_size, num_channels))\n\n        if self.mag_only:\n            return y.abs()\n        if self.phase_only:\n            return y.angle()\n        if self.real_only:\n            return y.real\n\n        if not self.is_complex:\n            y = _complex_to_real(y, self.complex_as_channels)\n\n        return y", "\n\nclass StftPhaseArray(StftArray):\n    def __init__(self, features):\n        super().__init__(features[\"n_dft\"],\n                         features[\"hop_size\"],\n                         features[\"onesided\"],\n                         phase_only=True)\n\n        self.n_output_channels = 2", "\n\nclass CrossSpectra(StftArray):\n    def __init__(self, n_dft=1024, hop_size=512,\n                 onesided=True, is_complex=True, complex_as_channels=False,\n                 phase_only=False):        \n        super().__init__(n_dft, hop_size, onesided=onesided)\n\n        self._is_complex = is_complex # _ is added not to conflict with StftArray\n        self.complex_as_channels = complex_as_channels\n        self.phase_only = phase_only\n\n    def forward(self, X):\n        \"Expected input has shape (batch_size, n_channels, time_steps)\"\n        batch_size, n_channels, time_steps = X.shape\n\n        stfts = super().forward(X)\n        # (batch_size, n_channels, n_time_bins, n_freq_bins)\n        y = []\n\n        # Compute the cross-spectrum between each pair of channels\n        for i in range(n_channels):\n            for j in range(i + 1, n_channels):\n                y_ij = stfts[:, i]*stfts[:, j].conj()\n                y.append(y_ij)\n        \n        y = torch.stack(y, dim=1)\n\n        if self.phase_only:\n            return y.angle()\n\n        if not self._is_complex:\n            _complex_to_real(y, self.complex_as_channels)\n\n        return y", "\n\ndef _complex_to_real(x, as_channels=False): \n    y = torch.view_as_real(x)\n    if as_channels:\n        # Merge channels and real and imaginary parts (last dim) as separate input channels\n        y = y.transpose(2, -1).flatten(start_dim=1, end_dim=2)\n\n    return y\n", ""]}
{"filename": "gnn_ssl/models/di_nn.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom hydra.utils import get_class\nfrom omegaconf import OmegaConf\n\nfrom gnn_ssl.feature_extractors import get_stft_output_shape\nfrom pysoundloc.pysoundloc.utils.math import grid_argmax\n\nfrom .base.rnn import RNN, init_gru", "\nfrom .base.rnn import RNN, init_gru\nfrom .base.mlp import MLP\nfrom .base.cnn import ConvBlock\nfrom ..feature_extractors.metadata import flatten_metadata\n\n\nclass DINN(nn.Module):\n    \"\"\"\n    CRNN model that accepts a secondary input\n    consisting of metadata (microphone positions, room dimensions, rt60)\n    \"\"\"\n    def __init__(self, config):\n        \n        super().__init__()\n\n        feature_config = config[\"features\"]\n        dataset_config = feature_config[\"dataset\"]\n        targets_config = config[\"targets\"]\n\n\n        self.feature_extractor = get_class(\n            config[\"features\"][\"class\"])(config[\"features\"])\n        self.config = config = OmegaConf.to_object(config)\n\n        # 1. Store configuration\n        self.n_input_channels = config[\"n_mics\"]\n        self.pool_type = config[\"pool_type\"]\n        self.pool_size = config[\"pool_size\"]\n        self.kernel_size = config[\"kernel_size\"]\n\n        self.input_shape = get_stft_output_shape(feature_config)\n\n        self.is_metadata_aware = config[\"is_metadata_aware\"]\n        self.metadata_dim = dataset_config[\"metadata_dim\"]\n\n        self.grid_size = targets_config[\"n_points_per_axis\"]\n        self.output_type = targets_config[\"type\"] # grid or regression vector\n        if self.output_type == \"source_coordinates\":\n            config[\"encoder_mlp_config\"][\"n_output_channels\"] = targets_config[\"n_output_coordinates\"]\n\n        # 3. Create encoder\n        self.encoder = Encoder(self.n_input_channels,\n                               self.input_shape,\n                               config[\"conv_layers_config\"],\n                               config[\"rnn_config\"],\n                               config[\"encoder_mlp_config\"],\n                               config[\"init_layers\"],\n                               config[\"pool_type\"],\n                               config[\"pool_size\"],\n                               config[\"kernel_size\"],\n                               flatten_dims=config[\"flatten_dims\"],\n                               batch_norm=config[\"batch_norm\"],\n                               is_metadata_aware=self.is_metadata_aware,\n                               metadata_dim=dataset_config[\"metadata_dim\"])\n\n        if config[\"output_activation\"] == \"relu\":\n            self.output_activation = nn.ReLU()\n        elif config[\"output_activation\"] == \"sigmoid\":\n            self.output_activation = nn.Sigmoid()\n        else:\n            self.output_activation = None\n\n    def forward(self, x):\n        room_dims = x[\"metadata\"][\"global\"][\"room_dims\"][:, :2]\n\n        x = {\n            \"signal\": self.feature_extractor(x),\n            \"metadata\": x[\"metadata\"]\n        }        \n\n        x = self.encoder(x)\n\n        if self.output_activation is not None:\n            x = self.output_activation(x)\n\n        if self.output_type == \"grid\":\n            batch_size = x.shape[0]\n            x = x.reshape((batch_size, self.grid_size, self.grid_size))\n            return {\n                \"source_coordinates\": grid_argmax(x, room_dims),\n                \"grid\": x\n            }\n        else:\n            return {\n                \"source_coordinates\": x\n            }", "\n\nclass Encoder(nn.Module):\n    def __init__(self, n_input_channels,\n                       input_shape,\n                       conv_layers_config,\n                       rnn_config,\n                       mlp_config,\n                       init_layers,\n                       pool_type,\n                       pool_size,\n                       kernel_size,\n                       flatten_dims=False,\n                       batch_norm=False,\n                       is_metadata_aware=True,\n                       metadata_dim=3):\n    \n        super().__init__()\n\n        # 1. Store configuration\n        self.n_input_channels = n_input_channels\n        self.input_shape = input_shape\n        self.pool_type = pool_type\n        self.pool_size = pool_size\n        self.kernel_size = kernel_size\n        self.n_conv_output_output = conv_layers_config[-1][\"n_channels\"]\n        self.n_rnn_output_channels = rnn_config[\"n_output_channels\"]\n        self.flatten_dims = flatten_dims\n        self.batch_norm = batch_norm\n        self.metadata_dim = metadata_dim # 3 for 3D localization, 2 for 2 for 2D localization\n        self.is_metadata_aware = is_metadata_aware\n\n        # 2. Create convolutional blocks\n        self.conv_blocks, self.conv_output_shape = self._create_conv_blocks(\n            conv_layers_config, batch_norm=batch_norm\n        )\n\n        # 3. Create recurrent block\n        self.n_rnn_input = self.n_conv_output_output\n        if flatten_dims:\n            # The input for the RNN will be\n            # The frequency_bins x conv_output_channels\n            self.n_rnn_input = int(self.n_rnn_input*self.conv_output_shape[-1])\n\n        self.rnn = RNN(self.n_rnn_input,\n                       self.n_rnn_output_channels,\n                       rnn_config[\"bidirectional\"],\n                       rnn_config[\"output_mode\"],\n                       n_layers=rnn_config[\"n_layers\"]\n        )\n\n        # 4. Create mlp block\n        n_input_mlp = self.n_rnn_output_channels\n        if is_metadata_aware:\n            n_metadata = metadata_dim*(self.n_input_channels + 1) # 1 => room dims \n            n_input_mlp += n_metadata\n\n        self.mlp = MLP(n_input_mlp, mlp_config[\"n_output_channels\"],\n                        n_hidden_features=mlp_config[\"n_hidden_channels\"],\n                        n_layers=mlp_config[\"n_layers\"],\n                        batch_norm=False, # Batch normalization on the MLP slowed training down.\n                        dropout_rate=mlp_config[\"dropout_rate\"]) \n        if init_layers:\n            init_gru(self.rnn.rnn)\n\n    def forward(self, x):\n\n        if self.is_metadata_aware:\n            metadata = flatten_metadata(x[\"metadata\"])\n\n        x = x[\"signal\"]\n        # (batch_size, num_channels, time_steps, freqs)\n\n        # 1. Extract features using convolutional layers\n        for conv_block in self.conv_blocks:\n            x = conv_block(x)\n        \n        # (batch_size, feature_maps, time_steps', freqs')\n\n        # 2. Average across all frequency bins,\n        # or flatten the frequency and channels\n\n        if self.flatten_dims:\n            x = x.transpose(2, 3)\n            x = x.flatten(start_dim=1, end_dim=2)\n        else:\n            if self.pool_type == \"avg\":\n                x = torch.mean(x, dim=3)\n            elif self.pool_type == \"max\":\n                x = torch.max(x, dim=3)[0]\n\n        # (batch_size, feature_maps, time_steps)\n\n        # Preprocessing for RNN\n        x = x.transpose(1,2)\n        # (batch_size, time_steps, feature_maps):\n\n        # 3. Apply RNN\n        x = self.rnn(x)\n\n\n        if self.is_metadata_aware:\n            # Concatenate metadata before sending to fully connected layer,\n            x = torch.cat([x, metadata], dim=1)\n            # (batch_size, n_metadata_unaware_features + n_metadata)\n        \n        # 5. Fully connected layer\n        x = self.mlp(x)\n        # (batch_size, class_num)\n\n        return x\n\n    def _create_conv_blocks(self, conv_layers_config, batch_norm):\n        conv_blocks = [\n            ConvBlock(self.n_input_channels, conv_layers_config[0][\"n_channels\"],\n                      block_type=conv_layers_config[0][\"type\"],\n                      pool_size=self.pool_size,\n                      pool_type=self.pool_type,\n                      kernel_size=self.kernel_size,\n                      batch_norm=batch_norm)\n        ]\n        current_output_shape = conv_blocks[-1].get_output_shape(self.input_shape)\n\n        for i, config in enumerate(conv_layers_config[1:]):\n            last_layer = conv_blocks[-1]\n            in_channels = last_layer.out_channels\n            conv_blocks.append(\n                ConvBlock(in_channels, config[\"n_channels\"],\n                          block_type=config[\"type\"],\n                          pool_size=self.pool_size,\n                          pool_type=self.pool_type,\n                          kernel_size=self.kernel_size,\n                          batch_norm=batch_norm)\n            )\n            current_output_shape = conv_blocks[-1].get_output_shape(current_output_shape)\n        return nn.ModuleList(conv_blocks), current_output_shape", ""]}
{"filename": "gnn_ssl/models/example.py", "chunked_list": ["import torch.nn as nn\n\nfrom hydra.utils import get_class\nfrom omegaconf import OmegaConf\n\nfrom gnn_ssl.feature_extractors import get_stft_output_shape\n\nfrom .base.mlp import MLP\n\n\nclass ExampleNet(nn.Module):\n    def __init__(self, config):\n        \n        super().__init__()\n\n        feature_config, targets_config = config[\"features\"], config[\"targets\"]\n        self.feature_extractor = get_class(\n            config[\"features\"][\"class\"])(config[\"features\"])\n        self.config = config = OmegaConf.to_object(config)\n\n        n_frames, n_frame = get_stft_output_shape(feature_config)\n        \n        self.mlp = MLP(\n            n_frame, config[\"n_output_features\"], config[\"n_hidden_features\"],\n            config[\"n_layers\"], config[\"activation\"], config[\"output_activation\"],\n            config[\"batch_norm\"], config[\"dropout_rate\"]\n        )\n\n    def forward(self, x):\n        x = {\n            \"signal\": self.feature_extractor(x),\n            \"metadata\": x[\"metadata\"]\n        }\n\n        batch_size, n_mics, n_frames, n_frame = x[\"signal\"].shape\n\n        x = self.mlp(x[\"signal\"])\n        # x.shape == (batch_size, n_mics, n_frames, n_output_features)\n\n        return {\n            \"example_output\": x\n        }", "\n\nclass ExampleNet(nn.Module):\n    def __init__(self, config):\n        \n        super().__init__()\n\n        feature_config, targets_config = config[\"features\"], config[\"targets\"]\n        self.feature_extractor = get_class(\n            config[\"features\"][\"class\"])(config[\"features\"])\n        self.config = config = OmegaConf.to_object(config)\n\n        n_frames, n_frame = get_stft_output_shape(feature_config)\n        \n        self.mlp = MLP(\n            n_frame, config[\"n_output_features\"], config[\"n_hidden_features\"],\n            config[\"n_layers\"], config[\"activation\"], config[\"output_activation\"],\n            config[\"batch_norm\"], config[\"dropout_rate\"]\n        )\n\n    def forward(self, x):\n        x = {\n            \"signal\": self.feature_extractor(x),\n            \"metadata\": x[\"metadata\"]\n        }\n\n        batch_size, n_mics, n_frames, n_frame = x[\"signal\"].shape\n\n        x = self.mlp(x[\"signal\"])\n        # x.shape == (batch_size, n_mics, n_frames, n_output_features)\n\n        return {\n            \"example_output\": x\n        }", ""]}
{"filename": "gnn_ssl/models/__init__.py", "chunked_list": [""]}
{"filename": "gnn_ssl/models/neural_srp.py", "chunked_list": ["import torch\n\nfrom pysoundloc.pysoundloc.utils.math import grid_argmax\n\nfrom .pairwise_neural_srp import PairwiseNeuralSrp\nfrom ..feature_extractors.metadata import filter_local_metadata\n\n\nclass NeuralSrp(PairwiseNeuralSrp):\n    \"\"\"Multi-microphone version of PairwiseNeuralSrp,\n    which works for a single pair of microphones.\n\n    NeuralSrp computes a PairwiseNeuralSrp grid for each microphone pair,\n    then sums them together.\n\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(config)\n        \n    def forward(self, x, estimate_coords=True, mean=True):\n        # x = self.feature_extractor(x) TODO: Extract features before for efficiency\n        # batch_size, n_channels, n_time_bins, n_freq_bins = x[\"signal\"].shape\n        batch_size, n_channels, n_time_samples = x[\"signal\"].shape\n\n        room_dims = x[\"metadata\"][\"global\"][\"room_dims\"][:, :2]\n\n        y = []\n        n_pairs = n_channels*(n_channels - 1)/2\n        for i in range(n_channels):\n            for j in range(i + 1, n_channels):\n                x_ij = {\n                    \"signal\": x[\"signal\"][:, [i, j]],\n                    \"metadata\": filter_local_metadata(x[\"metadata\"], [i, j])\n                }\n                \n                y.append(super().forward(x_ij)[\"grid\"])\n                #count += 1\n\n        y = torch.stack(y, dim=1).sum(dim=1)\n\n        if mean:\n            y /= n_pairs\n\n        if estimate_coords:\n            estimated_coords = grid_argmax(y, room_dims)\n\n            return {\n                \"source_coordinates\": estimated_coords,\n                \"grid\": y    \n            }\n\n        return y", "class NeuralSrp(PairwiseNeuralSrp):\n    \"\"\"Multi-microphone version of PairwiseNeuralSrp,\n    which works for a single pair of microphones.\n\n    NeuralSrp computes a PairwiseNeuralSrp grid for each microphone pair,\n    then sums them together.\n\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(config)\n        \n    def forward(self, x, estimate_coords=True, mean=True):\n        # x = self.feature_extractor(x) TODO: Extract features before for efficiency\n        # batch_size, n_channels, n_time_bins, n_freq_bins = x[\"signal\"].shape\n        batch_size, n_channels, n_time_samples = x[\"signal\"].shape\n\n        room_dims = x[\"metadata\"][\"global\"][\"room_dims\"][:, :2]\n\n        y = []\n        n_pairs = n_channels*(n_channels - 1)/2\n        for i in range(n_channels):\n            for j in range(i + 1, n_channels):\n                x_ij = {\n                    \"signal\": x[\"signal\"][:, [i, j]],\n                    \"metadata\": filter_local_metadata(x[\"metadata\"], [i, j])\n                }\n                \n                y.append(super().forward(x_ij)[\"grid\"])\n                #count += 1\n\n        y = torch.stack(y, dim=1).sum(dim=1)\n\n        if mean:\n            y /= n_pairs\n\n        if estimate_coords:\n            estimated_coords = grid_argmax(y, room_dims)\n\n            return {\n                \"source_coordinates\": estimated_coords,\n                \"grid\": y    \n            }\n\n        return y", ""]}
{"filename": "gnn_ssl/models/pairwise_neural_srp.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom hydra.utils import get_class\nfrom omegaconf import OmegaConf\n\nfrom gnn_ssl.feature_extractors import get_stft_output_shape\n\nfrom .base.rnn import RNN, init_gru\nfrom .base.mlp import MLP", "from .base.rnn import RNN, init_gru\nfrom .base.mlp import MLP\nfrom .base.cnn import ConvBlock\nfrom ..feature_extractors.metadata import flatten_metadata\n\n\nclass PairwiseNeuralSrp(nn.Module):\n    def __init__(self, config):\n        \n        super().__init__()\n\n        feature_config, targets_config = config[\"features\"], config[\"targets\"]\n        self.feature_extractor = get_class(\n            config[\"features\"][\"class\"])(config[\"features\"])\n        self.config = config = OmegaConf.to_object(config)\n\n        # 1. Store configuration\n        self.n_input_channels = 2 # Neural SRP is pairwise\n        self.pool_type = config[\"pool_type\"]\n        self.pool_size = config[\"pool_size\"]\n        self.kernel_size = config[\"kernel_size\"]\n        self.normalize_output = config[\"normalize_output\"]\n\n        self.input_shape = get_stft_output_shape(feature_config)\n        self.grid_size = targets_config[\"n_points_per_axis\"]\n\n        self.is_metadata_aware = config[\"is_metadata_aware\"]\n        self.metadata_dim = feature_config[\"dataset\"][\"metadata_dim\"]\n\n\n        # 3. Create encoder\n        self.encoder = Encoder(self.n_input_channels,\n                               self.input_shape,\n                               config[\"conv_layers_config\"],\n                               config[\"rnn_config\"],\n                               config[\"encoder_mlp_config\"],\n                               config[\"init_layers\"],\n                               config[\"pool_type\"],\n                               config[\"pool_size\"],\n                               config[\"kernel_size\"],\n                               flatten_dims=config[\"flatten_dims\"],\n                               batch_norm=config[\"batch_norm\"],\n                               is_metadata_aware=config[\"is_metadata_aware\"],\n                               metadata_dim=feature_config[\"dataset\"][\"metadata_dim\"])\n\n        if config[\"output_activation\"] == \"relu\":\n            self.output_activation = nn.ReLU()\n        elif config[\"output_activation\"] == \"sigmoid\":\n            self.output_activation = nn.Sigmoid()\n        else:\n            self.output_activation = None\n\n    def forward(self, x):\n        x = {\n            \"signal\": self.feature_extractor(x),\n            \"metadata\": x[\"metadata\"]\n        }\n        \n        x = self.encoder(x)\n        batch_size = x.shape[0]\n\n        x = x.reshape((batch_size, self.grid_size, self.grid_size))\n\n        if self.output_activation is not None:\n            x = self.output_activation(x)\n\n        if self.normalize_output:\n            x_max = x.abs().flatten(start_dim=1).max(dim=1)[0].unsqueeze(dim=1).unsqueeze(dim=2)\n            x = x/x_max\n\n        return {\n            \"grid\": x\n        }", "\n\nclass Encoder(nn.Module):\n    def __init__(self, n_input_channels,\n                       input_shape,\n                       conv_layers_config,\n                       rnn_config,\n                       mlp_config,\n                       init_layers,\n                       pool_type,\n                       pool_size,\n                       kernel_size,\n                       flatten_dims=False,\n                       batch_norm=False,\n                       is_metadata_aware=True,\n                       metadata_dim=3):\n    \n        super().__init__()\n\n        # 1. Store configuration\n        self.n_input_channels = n_input_channels\n        self.input_shape = input_shape\n        self.pool_type = pool_type\n        self.pool_size = pool_size\n        self.kernel_size = kernel_size\n        self.n_conv_output_output = conv_layers_config[-1][\"n_channels\"]\n        self.n_rnn_output_channels = rnn_config[\"n_output_channels\"]\n        self.flatten_dims = flatten_dims\n        self.batch_norm = batch_norm\n        self.metadata_dim = metadata_dim\n        self.is_metadata_aware = is_metadata_aware\n\n        # 2. Create convolutional blocks\n        self.conv_blocks, self.conv_output_shape = self._create_conv_blocks(\n            conv_layers_config, batch_norm=batch_norm\n        )\n\n        # 3. Create recurrent block\n        self.n_rnn_input = self.n_conv_output_output\n        if flatten_dims:\n            # The input for the RNN will be\n            # The frequency_bins x conv_output_channels\n            self.n_rnn_input = int(self.n_rnn_input*self.conv_output_shape[-1])\n\n        self.rnn = RNN(self.n_rnn_input,\n                       self.n_rnn_output_channels,\n                       rnn_config[\"bidirectional\"],\n                       rnn_config[\"output_mode\"],\n                       n_layers=rnn_config[\"n_layers\"]\n        )\n\n        # 4. Create mlp block\n        n_input_mlp = self.n_rnn_output_channels\n        if is_metadata_aware:\n            n_metadata = metadata_dim*(n_input_channels + 1) # 3 = 2 mic coords, + room dims \n            n_input_mlp += n_metadata\n\n        self.mlp = MLP(n_input_mlp, mlp_config[\"n_output_channels\"],\n                           n_hidden_features=mlp_config[\"n_hidden_channels\"],\n                           n_layers=mlp_config[\"n_layers\"],\n                           batch_norm=False) # Batch normalization on the MLP slowed training down. \n        if init_layers:\n            init_gru(self.rnn.rnn)\n\n    def forward(self, x):\n\n        if self.is_metadata_aware > 0:\n            metadata = flatten_metadata(x[\"metadata\"])\n\n        x = x[\"signal\"]\n        # (batch_size, num_channels, time_steps, freqs)\n\n        # 1. Extract features using convolutional layers\n        for conv_block in self.conv_blocks:\n            x = conv_block(x)\n        \n        # (batch_size, feature_maps, time_steps', freqs')\n\n        # 2. Average across all frequency bins,\n        # or flatten the frequency and channels\n\n        if self.flatten_dims:\n            x = x.transpose(2, 3)\n            x = x.flatten(start_dim=1, end_dim=2)\n        else:\n            if self.pool_type == \"avg\":\n                x = torch.mean(x, dim=3)\n            elif self.pool_type == \"max\":\n                x = torch.max(x, dim=3)[0]\n\n        # (batch_size, feature_maps, time_steps)\n\n        # Preprocessing for RNN\n        x = x.transpose(1,2)\n        # (batch_size, time_steps, feature_maps):\n\n        # 3. Apply RNN\n        x = self.rnn(x)\n\n\n        if self.is_metadata_aware:\n            # Concatenate metadata before sending to fully connected layer,\n            x = torch.cat([x, metadata], dim=1)\n            # (batch_size, n_metadata_unaware_features + n_metadata)\n        \n        # 5. Fully connected layer\n        x = self.mlp(x)\n        # (batch_size, class_num)\n\n        # (batch_size, feature_maps)\n\n        return x\n\n    def _create_conv_blocks(self, conv_layers_config, batch_norm):\n        conv_blocks = [\n            ConvBlock(self.n_input_channels, conv_layers_config[0][\"n_channels\"],\n                      block_type=conv_layers_config[0][\"type\"],\n                      pool_size=self.pool_size,\n                      pool_type=self.pool_type,\n                      kernel_size=self.kernel_size,\n                      batch_norm=batch_norm)\n        ]\n        current_output_shape = conv_blocks[-1].get_output_shape(self.input_shape)\n\n        for i, config in enumerate(conv_layers_config[1:]):\n            last_layer = conv_blocks[-1]\n            in_channels = last_layer.out_channels\n            conv_blocks.append(\n                ConvBlock(in_channels, config[\"n_channels\"],\n                          block_type=config[\"type\"],\n                          pool_size=self.pool_size,\n                          pool_type=self.pool_type,\n                          kernel_size=self.kernel_size,\n                          batch_norm=batch_norm)\n            )\n            current_output_shape = conv_blocks[-1].get_output_shape(current_output_shape)\n        return nn.ModuleList(conv_blocks), current_output_shape", ""]}
{"filename": "gnn_ssl/models/gnn_ssl.py", "chunked_list": ["from hydra.utils import get_class\nfrom omegaconf import OmegaConf\n\nfrom gnn_ssl.feature_extractors.pairwise_feature_extractors import ArrayWiseSpatialLikelihoodGrid\nfrom gnn_ssl.feature_extractors.pairwise_feature_extractors import (\n    GccPhat, MetadataAwarePairwiseFeatureExtractor, SpatialLikelihoodGrid\n)\n\nfrom pysoundloc.pysoundloc.utils.math import grid_argmax\n", "from pysoundloc.pysoundloc.utils.math import grid_argmax\n\nfrom .base.mlp import MLP\nfrom .base.base_relation_network import BaseRelationNetwork\n\nSIGNAL_KEY = \"signal\"\n\n\nclass GnnSslNet(BaseRelationNetwork):\n    def __init__(self,\n                 config,\n                 **kwargs):\n\n        self.config = config = OmegaConf.to_object(config)\n\n        feature_config, target_config = config[\"features\"], config[\"targets\"]\n        dataset_config = feature_config[\"dataset\"]\n\n        # 1. Store configuration\n        self.is_metadata_aware = config[\"is_metadata_aware\"]\n        self.use_rt60_as_metadata = dataset_config[\"use_rt60_as_metadata\"]\n        self.n_likelihood_grid_points_per_axis = target_config[\"n_points_per_axis\"]\n        self.sr = dataset_config[\"sr\"]\n        n_input_features = int(dataset_config[\"n_input_seconds\"]*self.sr)\n        # Set output size\n        self.output_target = target_config[\"type\"]\n        if self.output_target == \"source_coordinates\":\n            n_output_features = 2 # x, y coords of the microphones\n        else:\n            n_output_features = self.n_likelihood_grid_points_per_axis**2\n\n        # 2. Create local feature extractor\n        if config[\"local_feature_extractor\"] and config[\"pairwise_feature_extractor\"]:\n            raise ValueError(\n                \"\"\"Simultaneously using local and pairwise\n                feature extractors is not yet supported.\"\"\")\n\n        if config[\"local_feature_extractor\"] == \"mlp\":\n            config[\"local_feature_extractor\"] = MLP(n_input_features,\n                                          config[\"n_pairwise_features\"],\n                                          config[\"n_pairwise_features\"],\n                                          config[\"activation\"],\n                                          None,\n                                          config[\"batch_norm\"],\n                                          config[\"dropout_rate\"],\n                                          config[\"n_layers\"])\n        elif config[\"local_feature_extractor\"] == \"slf\":\n            config[\"local_feature_extractor\"] = ArrayWiseSpatialLikelihoodGrid(\n                self.sr, self.n_likelihood_grid_points_per_axis,\n                thickness=feature_config[\"srp_thickness\"]\n            )\n        if config[\"local_feature_extractor\"] is not None:\n            n_input_features = config[\"local_feature_extractor\"].n_output\n\n\n        super().__init__(n_input_features, n_output_features, config[\"n_pairwise_features\"], config[\"local_feature_extractor\"],\n                    config[\"pairwise_feature_extractor\"], config[\"pairwise_network_only\"],\n                    config[\"activation\"], config[\"output_activation\"], config[\"init_layers\"], config[\"batch_norm\"],\n                    config[\"dropout_rate\"], config[\"n_layers\"], SIGNAL_KEY)\n\n        # 3. Create pairwise feature extractor\n        self.use_pairwise_feature_extractor = config[\"pairwise_feature_extractor\"] is not None\n        if self.use_pairwise_feature_extractor:\n            if config[\"pairwise_feature_extractor\"] == \"gcc_phat\":\n                config[\"pairwise_feature_extractor\"] = GccPhat(self.sr, feature_config[\"n_dft\"])\n            elif config[\"pairwise_feature_extractor\"] == \"spatial_likelihood_grid\":\n                config[\"pairwise_feature_extractor\"] = SpatialLikelihoodGrid(self.sr, self.n_likelihood_grid_points_per_axis)\n            else:\n                raise ValueError(\"pairwise_feature_extractor must be 'gcc_phat' or 'spatial_likelihood_grid'\")\n\n            n_input_features = pairwise_feature_extractor.n_output\n    \n        pairwise_feature_extractor = MetadataAwarePairwiseFeatureExtractor(\n            n_input_features,\n            config[\"pairwise_feature_extractor\"],\n            self.is_metadata_aware,\n            self.use_rt60_as_metadata\n        )\n\n    def forward(self, x, estimate_coords=False):\n        y = super().forward(x)\n\n        if estimate_coords and self.output_target == \"likelihood_grid\":\n            batch_size = y.shape[0]\n            estimated_coords = grid_argmax(\n                y.reshape(batch_size, self.n_likelihood_grid_points_per_axis, self.n_likelihood_grid_points_per_axis),\n                x[\"global\"][\"room_dims\"])\n\n            return estimated_coords, y\n        else:\n            return y", "class GnnSslNet(BaseRelationNetwork):\n    def __init__(self,\n                 config,\n                 **kwargs):\n\n        self.config = config = OmegaConf.to_object(config)\n\n        feature_config, target_config = config[\"features\"], config[\"targets\"]\n        dataset_config = feature_config[\"dataset\"]\n\n        # 1. Store configuration\n        self.is_metadata_aware = config[\"is_metadata_aware\"]\n        self.use_rt60_as_metadata = dataset_config[\"use_rt60_as_metadata\"]\n        self.n_likelihood_grid_points_per_axis = target_config[\"n_points_per_axis\"]\n        self.sr = dataset_config[\"sr\"]\n        n_input_features = int(dataset_config[\"n_input_seconds\"]*self.sr)\n        # Set output size\n        self.output_target = target_config[\"type\"]\n        if self.output_target == \"source_coordinates\":\n            n_output_features = 2 # x, y coords of the microphones\n        else:\n            n_output_features = self.n_likelihood_grid_points_per_axis**2\n\n        # 2. Create local feature extractor\n        if config[\"local_feature_extractor\"] and config[\"pairwise_feature_extractor\"]:\n            raise ValueError(\n                \"\"\"Simultaneously using local and pairwise\n                feature extractors is not yet supported.\"\"\")\n\n        if config[\"local_feature_extractor\"] == \"mlp\":\n            config[\"local_feature_extractor\"] = MLP(n_input_features,\n                                          config[\"n_pairwise_features\"],\n                                          config[\"n_pairwise_features\"],\n                                          config[\"activation\"],\n                                          None,\n                                          config[\"batch_norm\"],\n                                          config[\"dropout_rate\"],\n                                          config[\"n_layers\"])\n        elif config[\"local_feature_extractor\"] == \"slf\":\n            config[\"local_feature_extractor\"] = ArrayWiseSpatialLikelihoodGrid(\n                self.sr, self.n_likelihood_grid_points_per_axis,\n                thickness=feature_config[\"srp_thickness\"]\n            )\n        if config[\"local_feature_extractor\"] is not None:\n            n_input_features = config[\"local_feature_extractor\"].n_output\n\n\n        super().__init__(n_input_features, n_output_features, config[\"n_pairwise_features\"], config[\"local_feature_extractor\"],\n                    config[\"pairwise_feature_extractor\"], config[\"pairwise_network_only\"],\n                    config[\"activation\"], config[\"output_activation\"], config[\"init_layers\"], config[\"batch_norm\"],\n                    config[\"dropout_rate\"], config[\"n_layers\"], SIGNAL_KEY)\n\n        # 3. Create pairwise feature extractor\n        self.use_pairwise_feature_extractor = config[\"pairwise_feature_extractor\"] is not None\n        if self.use_pairwise_feature_extractor:\n            if config[\"pairwise_feature_extractor\"] == \"gcc_phat\":\n                config[\"pairwise_feature_extractor\"] = GccPhat(self.sr, feature_config[\"n_dft\"])\n            elif config[\"pairwise_feature_extractor\"] == \"spatial_likelihood_grid\":\n                config[\"pairwise_feature_extractor\"] = SpatialLikelihoodGrid(self.sr, self.n_likelihood_grid_points_per_axis)\n            else:\n                raise ValueError(\"pairwise_feature_extractor must be 'gcc_phat' or 'spatial_likelihood_grid'\")\n\n            n_input_features = pairwise_feature_extractor.n_output\n    \n        pairwise_feature_extractor = MetadataAwarePairwiseFeatureExtractor(\n            n_input_features,\n            config[\"pairwise_feature_extractor\"],\n            self.is_metadata_aware,\n            self.use_rt60_as_metadata\n        )\n\n    def forward(self, x, estimate_coords=False):\n        y = super().forward(x)\n\n        if estimate_coords and self.output_target == \"likelihood_grid\":\n            batch_size = y.shape[0]\n            estimated_coords = grid_argmax(\n                y.reshape(batch_size, self.n_likelihood_grid_points_per_axis, self.n_likelihood_grid_points_per_axis),\n                x[\"global\"][\"room_dims\"])\n\n            return estimated_coords, y\n        else:\n            return y", ""]}
{"filename": "gnn_ssl/models/base/mlp.py", "chunked_list": ["import torch.nn as nn\n\n\nACTIVATIONS = {\n    \"relu\": nn.ReLU,\n    \"prelu\": nn.PReLU,\n    \"sigmoid\": nn.Sigmoid,\n    None: None\n}\n", "}\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_input_features, n_output_features,\n                n_hidden_features, n_layers, activation=\"relu\",\n                output_activation=None, batch_norm=False, dropout_rate=0):\n\n        super().__init__()\n\n        activation = ACTIVATIONS[activation]\n        output_activation = ACTIVATIONS[output_activation]\n\n        layers = [\n            fc_block(n_input_features, n_hidden_features, activation, batch_norm, dropout_rate)\n        ]\n\n        for _ in range(n_layers - 2): # -2 = skipping input and output layers\n            layers.append(\n                fc_block(n_hidden_features,\n                          n_hidden_features,\n                          activation,\n                          batch_norm=batch_norm\n                )\n            )\n\n        layers.append(fc_block(\n            n_hidden_features, n_output_features,\n            activation=output_activation, batch_norm=False)\n        )\n\n        self.layers = nn.Sequential(*layers)\n        self.n_output = n_output_features\n\n    def forward(self, x):\n        return self.layers(x)", "\n\ndef fc_block(n_input, n_output, activation, batch_norm=False, dropout_rate=0):\n    layers = [nn.Linear(n_input, n_output)]\n    if activation:\n        layers.append(activation())\n    if batch_norm:\n        layers.append(nn.BatchNorm1d(n_output))\n    if dropout_rate > 0:\n        layers.append(nn.Dropout(dropout_rate))\n\n    return nn.Sequential(*layers)", ""]}
{"filename": "gnn_ssl/models/base/rnn.py", "chunked_list": ["import math\nimport torch\nimport torch.nn as nn\n\n\nclass RNN(nn.Module):\n    def __init__(self, n_input, n_hidden,\n                 bidirectional=False, output_mode=\"last_time\", n_layers=1):\n        self.n_input = n_input\n        self.n_hidden = n_hidden\n        self.bidirectional = bidirectional\n        self.output_mode = output_mode\n\n        if bidirectional:\n            n_hidden //=2\n\n        super().__init__()\n\n        self.rnn = nn.GRU(input_size=n_input,\n                          hidden_size=n_hidden,\n                          batch_first=True,\n                          bidirectional=bidirectional,\n                          num_layers=n_layers)\n\n    def forward(self, x):\n        (x, _) = self.rnn(x)\n        # (batch_size, time_steps, feature_maps):\n        # Select last time step\n        if self.output_mode == \"last_time\":\n            return x[:, -1]\n        elif self.output_mode == \"avg_time\":\n            return x.mean(dim=1)\n        elif self.output_mode == \"avg_channels\":\n            return x.mean(dim=2)", "\n\ndef init_gru(rnn):\n    \"\"\"Initialize a GRU layer. \"\"\"\n    \n    def _concat_init(tensor, init_funcs):\n        (length, fan_out) = tensor.shape\n        fan_in = length // len(init_funcs)\n    \n        for (i, init_func) in enumerate(init_funcs):\n            init_func(tensor[i * fan_in : (i + 1) * fan_in, :])\n        \n    def _inner_uniform(tensor):\n        fan_in = nn.init._calculate_correct_fan(tensor, 'fan_in')\n        nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n    \n    for i in range(rnn.num_layers):\n        _concat_init(\n            getattr(rnn, 'weight_ih_l{}'.format(i)),\n            [_inner_uniform, _inner_uniform, _inner_uniform]\n        )\n        torch.nn.init.constant_(getattr(rnn, 'bias_ih_l{}'.format(i)), 0)\n\n        _concat_init(\n            getattr(rnn, 'weight_hh_l{}'.format(i)),\n            [_inner_uniform, _inner_uniform, nn.init.orthogonal_]\n        )\n        torch.nn.init.constant_(getattr(rnn, 'bias_hh_l{}'.format(i)), 0)", ""]}
{"filename": "gnn_ssl/models/base/utils.py", "chunked_list": ["import math\nimport torch\nimport torch.nn as nn\n\nACTIVATIONS = {\n    \"relu\": nn.ReLU,\n    \"prelu\": nn.PReLU,\n    \"sigmoid\": nn.Sigmoid,\n    None: None\n}", "    None: None\n}\n\n\nclass MaxLayer(nn.Module):\n    def __init__(self, dim=1):\n        super().__init__()\n        self.dim = dim\n    def forward(self, x):\n        return x.max(dim=self.dim)", "\n\nclass AvgLayer(nn.Module):\n    def __init__(self, dim=1):\n        super().__init__()\n        self.dim = dim\n    def forward(self, x):\n        return x.mean(dim=self.dim)\n\n\ndef init_layer(layer, nonlinearity='relu'):\n    \"\"\"Initialize a convolutional or linear layer\n    Credits to Yin Cao et al:\n    https://github.com/yinkalario/Two-Stage-Polyphonic-Sound-Event-Detection-and-Localization/blob/master/models/model_utilities.py\n    \"\"\"\n\n    classname = layer.__class__.__name__\n    if (classname.find('Conv') != -1) or (classname.find('Linear') != -1):\n        nn.init.kaiming_uniform_(layer.weight, nonlinearity=nonlinearity)\n        #nn.init.normal_(layer.weight, 1.0, 0.02)\n        if hasattr(layer, 'bias'):\n            if layer.bias is not None:\n                nn.init.constant_(layer.bias, 0.0)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(layer.weight, 1.0, 0.02)\n        nn.init.constant_(layer.bias, 0.0)", "\n\ndef init_layer(layer, nonlinearity='relu'):\n    \"\"\"Initialize a convolutional or linear layer\n    Credits to Yin Cao et al:\n    https://github.com/yinkalario/Two-Stage-Polyphonic-Sound-Event-Detection-and-Localization/blob/master/models/model_utilities.py\n    \"\"\"\n\n    classname = layer.__class__.__name__\n    if (classname.find('Conv') != -1) or (classname.find('Linear') != -1):\n        nn.init.kaiming_uniform_(layer.weight, nonlinearity=nonlinearity)\n        #nn.init.normal_(layer.weight, 1.0, 0.02)\n        if hasattr(layer, 'bias'):\n            if layer.bias is not None:\n                nn.init.constant_(layer.bias, 0.0)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(layer.weight, 1.0, 0.02)\n        nn.init.constant_(layer.bias, 0.0)", "\n\ndef load_checkpoint(model, checkpoint_path):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    \n    state_dict = {}\n\n    for k, v in checkpoint[\"state_dict\"].items():\n        k = _remove_prefix(k, \"model.\")\n        state_dict[k] = v\n\n    model.load_state_dict(state_dict)", "\n\ndef _remove_prefix(s, prefix):\n    return s[len(prefix):] if s.startswith(prefix) else s\n"]}
{"filename": "gnn_ssl/models/base/base_relation_network.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom .mlp import MLP\nfrom .utils import ACTIVATIONS\n\n\n# Base generic class to be inherited by the SSL specific network\nclass BaseRelationNetwork(nn.Module):\n    def __init__(self, n_input_features, n_output_features,\n                 n_pairwise_features,\n                 on_input_hook=None, # None or nn.Module\n                 on_pairwise_relation_network_start_hook=None, # None or nn.Module\n                 pairwise_network_only=False,\n                 activation=\"relu\",\n                 output_activation=\"sigmoid\",\n                 init_layers=True,\n                 batch_norm=False,\n                 dropout_rate=0,\n                 n_layers=3,\n                 signal_key=None):\n        \n        super().__init__()\n\n        # 1. Store configuration\n        self.n_input_features = n_input_features\n        self.n_output_features = n_output_features\n        self.n_pairwise_features = n_pairwise_features\n        self.pairwise_network_only = pairwise_network_only\n        self.n_input_relation_fusion_network = n_pairwise_features\n        self.activation = ACTIVATIONS[activation]\n        self.signal_key = signal_key\n\n        # 2. Create local feature extractor, if provided\n        self.on_input_hook = on_input_hook\n        if on_input_hook is None:\n            self.on_input_hook = DefaultOnInputHook(n_input_features)\n\n        # 2. Create pairwise relation network\n        self.pairwise_relation_network = PairwiseRelationNetwork(\n            self.on_input_hook.n_output, self.n_pairwise_features,\n            on_pairwise_relation_network_start_hook,\n            activation, None, init_layers,\n            batch_norm, dropout_rate, n_layers=n_layers,\n            signal_key=signal_key, standalone=pairwise_network_only\n        )\n        \n        # 3. Create relation fusion network\n        self.relation_fusion_network = RelationFusionNetwork(\n            self.n_input_relation_fusion_network, n_output_features,\n            activation, output_activation, init_layers, batch_norm,\n            dropout_rate, n_layers=n_layers, signal_key=signal_key\n        )\n\n    def forward(self, x):\n        # x.shape == (batch_size, num_channels, feature_size)\n        x = self.on_input_hook(x)\n        # x.shape == (batch_size, num_channels, self.on_input_hook.n_output)\n        x = self.pairwise_relation_network(x)\n        # x.shape == (batch_size, self.n_pairwise_features)\n        if self.pairwise_network_only:\n            return x\n\n        x = self.relation_fusion_network(x)\n        # x.shape == (batch_size, self.n_output_features)\n        return x", "class BaseRelationNetwork(nn.Module):\n    def __init__(self, n_input_features, n_output_features,\n                 n_pairwise_features,\n                 on_input_hook=None, # None or nn.Module\n                 on_pairwise_relation_network_start_hook=None, # None or nn.Module\n                 pairwise_network_only=False,\n                 activation=\"relu\",\n                 output_activation=\"sigmoid\",\n                 init_layers=True,\n                 batch_norm=False,\n                 dropout_rate=0,\n                 n_layers=3,\n                 signal_key=None):\n        \n        super().__init__()\n\n        # 1. Store configuration\n        self.n_input_features = n_input_features\n        self.n_output_features = n_output_features\n        self.n_pairwise_features = n_pairwise_features\n        self.pairwise_network_only = pairwise_network_only\n        self.n_input_relation_fusion_network = n_pairwise_features\n        self.activation = ACTIVATIONS[activation]\n        self.signal_key = signal_key\n\n        # 2. Create local feature extractor, if provided\n        self.on_input_hook = on_input_hook\n        if on_input_hook is None:\n            self.on_input_hook = DefaultOnInputHook(n_input_features)\n\n        # 2. Create pairwise relation network\n        self.pairwise_relation_network = PairwiseRelationNetwork(\n            self.on_input_hook.n_output, self.n_pairwise_features,\n            on_pairwise_relation_network_start_hook,\n            activation, None, init_layers,\n            batch_norm, dropout_rate, n_layers=n_layers,\n            signal_key=signal_key, standalone=pairwise_network_only\n        )\n        \n        # 3. Create relation fusion network\n        self.relation_fusion_network = RelationFusionNetwork(\n            self.n_input_relation_fusion_network, n_output_features,\n            activation, output_activation, init_layers, batch_norm,\n            dropout_rate, n_layers=n_layers, signal_key=signal_key\n        )\n\n    def forward(self, x):\n        # x.shape == (batch_size, num_channels, feature_size)\n        x = self.on_input_hook(x)\n        # x.shape == (batch_size, num_channels, self.on_input_hook.n_output)\n        x = self.pairwise_relation_network(x)\n        # x.shape == (batch_size, self.n_pairwise_features)\n        if self.pairwise_network_only:\n            return x\n\n        x = self.relation_fusion_network(x)\n        # x.shape == (batch_size, self.n_output_features)\n        return x", "\n\nclass PairwiseRelationNetwork(nn.Module):\n    def __init__(self, n_input_features,\n                       n_output_features,\n                       on_start_hook=None,\n                       activation=\"relu\",\n                       output_activation=None,\n                       init_layers=True,\n                       batch_norm=False,\n                       dropout_rate=0,\n                       n_layers=3,\n                       signal_key=None,\n                       standalone=False,\n                       mask=True):\n\n        super().__init__()\n\n        self.signal_key = signal_key\n        self.standalone = standalone\n        self.output_activation = ACTIVATIONS[output_activation]\n        self.mask = mask\n\n        # 1. Create hook to be executed before network (Like extracting GCC-PHAT, or simply concatenating the pair)\n        self.on_start_hook = on_start_hook\n        if on_start_hook is None:\n            self.on_start_hook = DefaultPairwiseRelationNetworkStartHook(n_input_features)\n            # If pairwise feature extractor is provided, n_input_features refers to its output.\n            # Else, it refers to the input of each object, therefore we double it.\n        n_input_features = self.on_start_hook.n_output\n\n        # 2. Create neural network\n        self.pairwise_relation_network = MLP(n_input_features,\n                                                n_output_features,\n                                                n_output_features,\n                                                n_layers,\n                                                activation,\n                                                activation,\n                                                batch_norm,\n                                                dropout_rate)\n\n        if init_layers:\n            for layer in self.pairwise_relation_network.layers:\n                torch.nn.init.ones_(layer[0].weight)\n            #init_layer(self.pairwise_relation_network)\n\n    def forward(self, x):\n        if self.signal_key is None:\n            x_signal = x\n        else:\n            x_signal = x[self.signal_key]\n\n        batch_size, n_channels, n_input_features = x_signal.shape\n\n        # TODO: parallelize this?\n        pairwise_relations = []\n        for i in range(n_channels):\n            for j in range(i + 1, n_channels):\n                x_ij = self.on_start_hook(x, i, j)\n                pairwise_relation = self.pairwise_relation_network(x_ij)#*x_ij[:, :625]\n                # if self.mask:\n                #     pairwise_relation *= x_ij[:, :pairwise_relation.shape[1]]\n                pairwise_relations.append(pairwise_relation)\n        \n        pairwise_relations = torch.stack(pairwise_relations, dim=1)\n        \n        if self.standalone:\n            x = pairwise_relations.mean(dim=1)\n            if self.output_activation:\n                x = self.output_activation(x)\n            return x\n\n        if self.signal_key is not None:\n            x[\"signal\"] = pairwise_relations\n            return x\n        else:\n            pairwise_relations", "\n\nclass RelationFusionNetwork(nn.Module):\n    def __init__(self, n_input_features,\n                       n_output_features,\n                       activation=\"relu\",\n                       output_activation=\"sigmoid\",\n                       init_layers=True,\n                       batch_norm=False,\n                       dropout_rate=0,\n                       n_layers=3,\n                       signal_key=None):\n\n        super().__init__()\n\n        self.relation_fusion_network = MLP(n_input_features,\n                                           n_output_features,\n                                           n_input_features,\n                                           n_layers,\n                                           activation,\n                                           output_activation,\n                                           batch_norm,\n                                           dropout_rate)\n        self.signal_key = signal_key\n\n        if init_layers:\n            for layer in self.relation_fusion_network.layers:\n                torch.nn.init.eye_(layer[0].weight)\n            #init_layer(self.relation_fusion_network)\n\n    def forward(self, x):\n        if self.signal_key is None:\n            x_signal = x\n        else:\n            x_signal = x[self.signal_key]\n\n        batch_size, n_channels, n_pairwise_features = x_signal.shape\n        x_signal = x_signal.mean(dim=1)\n        # x.shape == (batch_size, n_pairwise_features) \n        x_signal = self.relation_fusion_network(x_signal)\n        # x.shape == (batch_size, self.n_output_features) \n\n        return x_signal", "\n\nclass DefaultPairwiseRelationNetworkStartHook(nn.Module):\n    def __init__(self, n_input_features):\n        super().__init__()\n\n        self.n_output = 2*n_input_features\n\n    def forward(self, x, i, j):\n        x_i = x[:, i]\n        x_j = x[:, j]\n        \n        x_ij = torch.cat([x_i, x_j], axis=-1)\n        \n        return x_ij", "\n\nclass DefaultOnInputHook(nn.Module):\n    \"\"\"This is a placeholder for a torch.nn.Module which may be\n    provider by a user who'd like for some preprocessing to occur on their input\n    before sending it to the pairwise feature extractor.\n\n    An example of an on_input_hook function may be a feature extractor such as the \n    Discrete Fourier transform to be applied individually to each channel.\n    \"\"\"\n    def __init__(self, n_input_features):\n        super().__init__()\n\n        self.n_output = n_input_features\n\n    def forward(self, x):\n        return x"]}
{"filename": "gnn_ssl/models/base/cnn.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom .mlp import fc_block\nfrom .utils import ACTIVATIONS, AvgLayer, init_layer\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, \n                kernel_size=(3,3), stride=(1,1),\n                padding=(1,1), dilation=(1, 1),\n                pool_size=(2, 2), pool_type=\"avg\",\n                block_type=\"double\",\n                dropout_rate=0, batch_norm=False):\n        \n        super().__init__()\n\n        # Dump parameters\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.pool_size = pool_size\n        self.block_type = block_type\n        self.dropout_rate = dropout_rate\n        self.batch_norm = batch_norm\n\n        blocks = [\n            nn.Conv2d(in_channels=in_channels, \n                      out_channels=out_channels,\n                      kernel_size=kernel_size, stride=stride,\n                      padding=padding, dilation=dilation),\n        ]\n        \n        if batch_norm:\n            blocks.append(nn.BatchNorm2d(out_channels))\n\n        blocks.append(nn.ReLU())\n\n        if block_type == \"double\": \n            blocks.append(nn.Conv2d(in_channels=out_channels, \n                                   out_channels=out_channels,\n                                   kernel_size=kernel_size, stride=stride,\n                                   padding=padding, dilation=dilation))\n            if batch_norm:\n                blocks.append(nn.BatchNorm2d(out_channels))\n            blocks.append(nn.ReLU())\n        \n\n        # Create activation, dropout and pooling blocks\n        if pool_type == \"avg\":\n            blocks.append(nn.AvgPool2d(pool_size))\n        elif pool_type == \"max\":\n            blocks.append(nn.MaxPool2d(pool_size))\n        else:\n            raise ValueError(f\"pool_type '{pool_type}' is invalid. Must be 'max' or 'avg'\")\n\n        if dropout_rate > 0:\n            blocks.append(nn.Dropout(dropout_rate))\n\n        self.model = nn.Sequential(*blocks)\n        \n    def forward(self, x):\n        x = self.model(x)\n        \n        if self.dropout_rate > 0:\n            x = self.dropout(x)\n        return x\n\n    def get_output_shape(self, input_shape):\n        \"\"\"\n        Args:\n            input_shape (tuple): (in_width, in_height)\n\n        Returns:\n            tuple: (out_width, out_height)\n        \"\"\"\n\n        output_shape = get_conv2d_output_shape(\n            input_shape, self.kernel_size, self.stride, self.dilation, self.padding)\n\n        if self.block_type == \"double\":\n            output_shape = get_conv2d_output_shape(\n                output_shape, self.kernel_size, self.stride, self.dilation, self.padding)\n        output_shape = get_pool2d_output_shape(output_shape, self.pool_size)\n\n        return output_shape", "\n\nclass Cnn1d(nn.Module):\n    def __init__(self, n_input_channels, n_output_features, n_hidden_channels,\n                 activation, output_activation, batch_norm, n_layers, n_metadata):\n        super().__init__()\n\n        activation = ACTIVATIONS[activation]\n        output_activation = ACTIVATIONS[output_activation]\n\n        layers = [\n            _conv_1d_block(n_input_channels, n_hidden_channels, activation, batch_norm=batch_norm)\n        ]\n\n        for _ in range(n_layers - 1): # -1 = skipping input layer\n            layers.append(\n                _conv_1d_block(n_hidden_channels,\n                            n_hidden_channels,\n                            activation,\n                            batch_norm=batch_norm\n                )\n            )\n\n        layers.append(AvgLayer(dim=2)) # dim0 = batch, dim1 = channels, dim2 = time_steps\n\n        self.conv_layers = nn.Sequential(*layers)\n        self.fc_layer = fc_block(\n            n_hidden_channels + 2*n_metadata, n_output_features,\n            activation=output_activation, batch_norm=False\n        )\n        self.n_metadata = n_metadata\n    \n    def forward(self, x):\n        batch_size, n_channels, n_time_steps = x.shape\n\n        if self.n_metadata > 0:\n            x = x[:, :, :-self.n_metadata]\n            metadata = x[:, :, -self.n_metadata:]\n            metadata = metadata.reshape((batch_size, n_channels*self.n_metadata))\n        \n        x = self.conv_layers(x)\n        \n        if self.n_metadata > 0:\n            x = torch.cat([x, metadata], dim=1)\n\n        x = self.fc_layer(x)\n\n        return x", "\n\ndef _conv_1d_block(n_input_channels, n_output_channels,\n                   activation, kernel_size=3, stride=1,\n                   padding=1, pool_size=1, pool_type=\"avg\",\n                   batch_norm=False):\n        \n    layers = [\n        nn.Conv1d(n_input_channels, n_output_channels,\n                    kernel_size, stride, padding)\n    ]\n    \n    if activation:\n        layers.append(activation())\n    if batch_norm:\n        layers.append(nn.BatchNorm1d(n_output_channels))\n    if pool_size > 1:\n        if pool_type == \"avg\":\n            layers.append(nn.AvgPool1d(pool_size))\n        elif pool_type == \"max\":\n            layers.append(nn.MaxPool1d(pool_size))\n        else:\n            raise ValueError(\"Pooling layer must be 'max' or 'avg'\")\n    \n    return nn.Sequential(*layers)", "\n\ndef get_conv2d_output_shape(input_shape: tuple,\n                            kernel_size: tuple,\n                            stride=(1, 1),\n                            dilation=(1, 1),\n                            padding=(0, 0)):\n    \"\"\"Compute the output of a convolutional layer.\n    See https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n    for more information.\n    \"\"\"\n\n    input_shape = torch.Tensor(input_shape)\n    kernel_size = torch.Tensor(kernel_size)\n    stride = torch.Tensor(stride)\n    dilation = torch.Tensor(dilation)\n    padding = torch.Tensor(padding)\n\n    n_output_shape = (input_shape + 2*padding - kernel_size - (dilation - 1)*(kernel_size - 1))/stride + 1\n    #print(n_output_shape)\n    n_output_shape = torch.floor(n_output_shape)\n\n    return n_output_shape", "\n\ndef get_pool2d_output_shape(n_input_shape, pool_size):\n    return get_conv2d_output_shape(n_input_shape,\n                                   pool_size,\n                                   pool_size)\n\n\ndef get_conv_pool_output_shape(n_input_shape: tuple,\n                               kernel_size: tuple,\n                               pool_size: tuple,\n                               stride=(1, 1),\n                               dilation=(1, 1),\n                               padding=(0, 0)):\n\n    conv_output = get_conv2d_output_shape(\n        n_input_shape, kernel_size, stride,\n        dilation, padding\n    )\n    pool_output = get_pool2d_output_shape(conv_output, pool_size)\n\n    return pool_output", "def get_conv_pool_output_shape(n_input_shape: tuple,\n                               kernel_size: tuple,\n                               pool_size: tuple,\n                               stride=(1, 1),\n                               dilation=(1, 1),\n                               padding=(0, 0)):\n\n    conv_output = get_conv2d_output_shape(\n        n_input_shape, kernel_size, stride,\n        dilation, padding\n    )\n    pool_output = get_pool2d_output_shape(conv_output, pool_size)\n\n    return pool_output", ""]}
{"filename": "gnn_ssl/models/baselines/srp_phat.py", "chunked_list": ["import torch.nn as nn\n\nfrom pysoundloc.pysoundloc.models import srp_phat\n\nclass SrpPhat(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n    \n        self.feature_config, self.targets_config = config[\"features\"], config[\"targets\"]\n        self.n_points_per_axis = self.targets_config[\"n_points_per_axis\"]\n        self.sr = config[\"features\"][\"dataset\"][\"sr\"]\n        self.thickness = config[\"features\"][\"srp_thickness\"]\n\n    def forward(self, x):\n        mic_signals = x[\"signal\"]\n        mic_coords = x[\"metadata\"][\"local\"][\"mic_coordinates\"][..., :2]\n        room_dims = x[\"metadata\"][\"global\"][\"room_dims\"][..., :2]\n        return srp_phat(mic_signals, mic_coords, room_dims,\n                        self.sr, self.n_points_per_axis,\n                        thickness=self.thickness)", ""]}
{"filename": "gnn_ssl/trainers/base.py", "chunked_list": ["import pytorch_lightning as pl\nimport torch\n\nfrom hydra.utils import get_class\nfrom torch.optim.lr_scheduler import MultiStepLR\n\n\nclass BaseTrainer(pl.LightningModule):\n    \"\"\"Class which abstracts interactions with Hydra\n    and basic training/testing/validation conventions\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n\n        model, training = config[\"model\"], config[\"training\"]\n        features, targets = config[\"features\"], config[\"targets\"]\n\n        self.model = get_class(model[\"class\"])(model)\n        # self.feature_extractor = get_class(features[\"class\"])(features)\n        self.loss = get_class(targets[\"loss_class\"])(targets)\n\n        self.model_config =  model\n        self.features_config = features\n        self.targets_config = targets\n        self.training_config = training\n\n        self.log_step = self.training_config[\"log_step\"]\n\n    def forward(self, x):\n        return self.model(x)\n\n    def _step(self, batch, batch_idx, epoch_type):\n\n        x, y = batch[0] # 0 is to ignore the microphone array index\n        \n        # 1. Compute model output and loss\n        x = self.forward(x)\n        loss = self.loss(x, y, mean_reduce=True)\n\n        self.log_dict({f\"{epoch_type}_loss_step\": loss})\n        \n        return {\n            \"loss\": loss\n        }\n\n    def training_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, \"train\")\n  \n    def validation_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, \"validation\")\n    \n    def test_step(self, batch, batch_idx):\n        return self._step(batch, batch_idx, \"test\")\n    \n    def _epoch_end(self, outputs, epoch_type=\"train\"):\n        # 1. Compute epoch metrics\n        outputs = _merge_list_of_dicts(outputs)\n        tensorboard_logs = {\n            f\"{epoch_type}_loss\": outputs[\"loss\"].mean(),\n            # f\"{epoch_type}_std\": outputs[\"loss\"].std(),\n            \"step\": self.current_epoch\n        }\n\n        print(tensorboard_logs)\n        self.log_dict(tensorboard_logs)\n    \n    def training_epoch_end(self, outputs):\n        self._epoch_end(outputs)\n\n    def validation_epoch_end(self, outputs):\n        self._epoch_end(outputs, epoch_type=\"validation\")\n\n    def test_epoch_end(self, outputs):\n        self._epoch_end(outputs, epoch_type=\"test\")\n\n    def forward(self, x):\n        return self.model(x)\n        \n    def fit(self, dataset_train, dataset_val):\n        super().fit(self.model, dataset_train, val_dataloaders=dataset_val)\n\n    def test(self, dataset_test, ckpt_path=\"best\"):\n        super().test(self.model, dataset_test, ckpt_path=ckpt_path)\n\n    def configure_optimizers(self):\n        lr = self.training_config[\"learning_rate\"]\n        decay_step = self.training_config[\"learning_rate_decay_steps\"]\n        decay_value = self.training_config[\"learning_rate_decay_values\"]\n\n        if self.training_config[\"optimizer\"] == \"sgd\":\n            optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n        elif self.training_config[\"optimizer\"] == \"adam\":\n            optimizer = torch.optim.Adam(self.parameters(), lr=lr,\n                            betas=self.training_config[\"betas\"])\n        scheduler = MultiStepLR(optimizer, decay_step, decay_value)\n\n        return [optimizer], [scheduler]", "\n\ndef _merge_list_of_dicts(list_of_dicts):\n    \"\"\"Function used at the end of an epoch.\n    It is used to merge together the many vectors generated at each step\n    \"\"\"\n\n    result = {}\n\n    def _add_to_dict(key, value):\n        if len(value.shape) == 0: # 0-dimensional tensor\n            value = value.unsqueeze(0)\n\n        if key not in result:\n            result[key] = value\n        else:\n            result[key] = torch.cat([\n                result[key], value\n            ])\n    \n    for d in list_of_dicts:\n        for key, value in d.items():\n            _add_to_dict(key, value)\n\n    return result"]}
{"filename": "gnn_ssl/trainers/neural_srp.py", "chunked_list": ["from gnn_ssl.metrics import NormLoss\nfrom .base import BaseTrainer\n\n\nclass NeuralSrpTrainer(BaseTrainer):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.norm = NormLoss(\"l2\")\n\n    def _step(self, batch, batch_idx, epoch_type):\n        x, y = batch[0] # 0 is because the current dataloader is multi-microphone\n        if self.targets_config[\"type\"] == \"source_coordinates\":\n            n_output_coords = self.targets_config[\"n_output_coordinates\"]\n            y[\"source_coordinates\"] = y[\"source_coordinates\"][:, 0, :n_output_coords] # Only select first source\n\n        return super()._step((x, y), batch_idx, epoch_type)", ""]}
{"filename": "gnn_ssl/trainers/gnn_ssl.py", "chunked_list": ["from omegaconf import OmegaConf\nimport torch\n\nfrom torch.optim.lr_scheduler import MultiStepLR\n\nfrom ..models.gnn_ssl import GnnSslNet\n\nfrom ..metrics import Loss\nfrom .base import BaseTrainer\n", "from .base import BaseTrainer\n\n\nclass GnnSslNetTrainer(BaseTrainer):\n    \"\"\"This class abstracts the\n       training/validation/testing procedures\n       used for training a GnnSslNet\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.rmse = Loss(config[\"targets\"], mode=\"l2\", dim=1)\n\n    def _step(self, batch, batch_idx, log_model_output=False, log_labels=False):\n        \"\"\"This is the base step done on every training, validation and testing batch iteration.\n        This had to be overriden since we are using multiple datasets\n        \"\"\"\n\n        loss_vectors = []\n        outputs = []\n        targets = []\n        n_mics = []\n        \n        # Loop for every microphone number\n        for (x, y) in batch:\n            n = x[\"signal\"].shape[1]\n\n            # 1. Compute model output and loss\n            output = self.model(x)\n            loss = self.loss(output, y, mean_reduce=False)\n\n            outputs.append(output)\n            targets.append(y)\n            loss_vectors.append(loss)\n            n_mics.append(n)\n\n            # 2. RMSE\n            rmse_error = self.rmse(output, y, mean_reduce=True)\n            self.log(f\"rmse_{n}_mics\", rmse_error, on_step=True, prog_bar=False, on_epoch=False)\n\n        output_dict = {}\n\n        for i, n in enumerate(n_mics):\n            loss_vector = loss_vectors[i]\n            mean_loss = loss_vector.mean()\n            output_dict[f\"loss_vector_{n}\"] = loss_vector.detach().cpu()\n            output_dict[f\"mean_loss_{n}\"] = mean_loss.detach().cpu()\n\n            # TODO: Add these to a callback\n            # 2. Log model output\n            if log_model_output:\n                output_dict[f\"model_outputs_{n}\"] = outputs[i]\n            \n\n        output_dict[\"loss\"] = torch.stack(loss_vectors).mean()\n\n        return output_dict", ""]}
{"filename": "gnn_ssl/datasets/__init__.py", "chunked_list": [""]}
{"filename": "gnn_ssl/datasets/distributed_ssl.py", "chunked_list": ["import torch\n\nfrom torch.utils.data import DataLoader\n\nfrom sydra.sydra.dataset import SydraDataset\nfrom ..feature_extractors.metadata import format_metadata\n\n\nclass DistributedSslDataset(SydraDataset):\n    def __init__(self, dataset_dir, config,\n                 trim_signals_mode=\"random\", flatten_metadata=True):\n        super().__init__(dataset_dir, config[\"sr\"],\n                         config[\"n_input_seconds\"], trim_signals_mode)\n\n        self.use_rt60_as_metadata = config[\"use_rt60_as_metadata\"]\n        self.flatten_metadata = flatten_metadata\n\n    def __getitem__(self, index):\n\n        (x, y) = super().__getitem__(index)\n        # 1. Add metadata to input\n\n        # Collapse channels and arrays\n        x = x.flatten(start_dim=0, end_dim=1)\n        x = {\n            \"signal\": x,\n            \"metadata\": format_metadata(y, self.use_rt60_as_metadata)\n        }\n\n        if len(y[\"mic_coordinates\"].shape) == 3:\n            # Array\n            y[\"mic_coordinates\"] = y[\"mic_coordinates\"][0]\n        \n        if len(y[\"source_coordinates\"].shape) == 1:\n            # Expand to multi-source\n            y[\"source_coordinates\"] = y[\"source_coordinates\"].unsqueeze(0)\n\n        y = {\n            \"source_coordinates\": y[\"source_coordinates\"],\n            \"mic_coordinates\": y[\"mic_coordinates\"],\n            \"room_dims\": y[\"room_dims\"],\n        }\n\n        return (x, y)", "class DistributedSslDataset(SydraDataset):\n    def __init__(self, dataset_dir, config,\n                 trim_signals_mode=\"random\", flatten_metadata=True):\n        super().__init__(dataset_dir, config[\"sr\"],\n                         config[\"n_input_seconds\"], trim_signals_mode)\n\n        self.use_rt60_as_metadata = config[\"use_rt60_as_metadata\"]\n        self.flatten_metadata = flatten_metadata\n\n    def __getitem__(self, index):\n\n        (x, y) = super().__getitem__(index)\n        # 1. Add metadata to input\n\n        # Collapse channels and arrays\n        x = x.flatten(start_dim=0, end_dim=1)\n        x = {\n            \"signal\": x,\n            \"metadata\": format_metadata(y, self.use_rt60_as_metadata)\n        }\n\n        if len(y[\"mic_coordinates\"].shape) == 3:\n            # Array\n            y[\"mic_coordinates\"] = y[\"mic_coordinates\"][0]\n        \n        if len(y[\"source_coordinates\"].shape) == 1:\n            # Expand to multi-source\n            y[\"source_coordinates\"] = y[\"source_coordinates\"].unsqueeze(0)\n\n        y = {\n            \"source_coordinates\": y[\"source_coordinates\"],\n            \"mic_coordinates\": y[\"mic_coordinates\"],\n            \"room_dims\": y[\"room_dims\"],\n        }\n\n        return (x, y)", "\n\nclass DistributedSslDataLoader(DataLoader):\n    def __init__(self, config, dataset_path, shuffle=False,\n                 batch_size=16, n_workers=1, **kwargs):\n\n        self.config = config\n        self.dataset_path = dataset_path\n        if type(dataset_path) == str:\n            dataset_path = [dataset_path] # Convert to a 1-element list\n\n        dataset_config = config[\"dataset\"]\n        datasets = [\n            DistributedSslDataset(d, dataset_config)\n            for d in dataset_path\n        ]\n\n        dataset = ConcatDataset(datasets)\n\n        super().__init__(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            pin_memory=True,\n            drop_last=False,\n            num_workers=n_workers,\n        )", "\n\n# Source: https://forums.pytorchlightning.ai/t/how-to-use-multiple-train-dataloaders-with-different-lengths/214/2\nclass ConcatDataset(torch.utils.data.Dataset):\n    def __init__(self, datasets):\n        self.datasets = datasets\n\n    def __getitem__(self, i):\n        return tuple(d[i] for d in self.datasets)\n\n    def __len__(self):\n        return min(len(d) for d in self.datasets)", ""]}
