{"filename": "setup.py", "chunked_list": ["from setuptools import find_packages\nfrom setuptools import setup\n\nwith open(\"README.md\") as f:\n    long_description = f.read()\n\nsetup(\n    name=\"vampnet\",\n    version=\"0.0.1\",\n    classifiers=[", "    version=\"0.0.1\",\n    classifiers=[\n        \"Intended Audience :: Developers\",\n        \"Natural Language :: English\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Topic :: Artistic Software\",\n        \"Topic :: Multimedia\",\n        \"Topic :: Multimedia :: Sound/Audio\",\n        \"Topic :: Multimedia :: Sound/Audio :: Editors\",\n        \"Topic :: Software Development :: Libraries\",", "        \"Topic :: Multimedia :: Sound/Audio :: Editors\",\n        \"Topic :: Software Development :: Libraries\",\n    ],\n    description=\"Generative Music Modeling.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    author=\"Hugo Flores Garc\u00eda, Prem Seetharaman\",\n    author_email=\"hfgacrcia@descript.com\",\n    url=\"https://github.com/hugofloresgarcia/vampnet\",\n    license=\"MIT\",", "    url=\"https://github.com/hugofloresgarcia/vampnet\",\n    license=\"MIT\",\n    packages=find_packages(),\n    install_requires=[\n        \"torch\",\n        \"argbind>=0.3.2\",\n        \"numpy==1.23\",\n        \"wavebeat @ git+https://github.com/hugofloresgarcia/wavebeat\",\n        \"lac @ git+https://github.com/hugofloresgarcia/lac.git\",\n        \"descript-audiotools @ git+https://github.com/descriptinc/audiotools.git@0.7.2\",", "        \"lac @ git+https://github.com/hugofloresgarcia/lac.git\",\n        \"descript-audiotools @ git+https://github.com/descriptinc/audiotools.git@0.7.2\",\n        \"gradio\", \n        \"loralib\",\n        \"torch_pitch_shift\",\n        \"madmom\",\n    ],\n)\n", ""]}
{"filename": "app.py", "chunked_list": ["from pathlib import Path\nfrom typing import Tuple\nimport yaml\nimport tempfile\nimport uuid\nfrom dataclasses import dataclass, asdict\n\nimport numpy as np\nimport audiotools as at\nimport argbind", "import audiotools as at\nimport argbind\n\nimport gradio as gr\nfrom vampnet.interface import Interface\nfrom vampnet import mask as pmask\n\nInterface = argbind.bind(Interface)\n# AudioLoader = argbind.bind(at.data.datasets.AudioLoader)\n", "# AudioLoader = argbind.bind(at.data.datasets.AudioLoader)\n\nconf = argbind.parse_args()\n\n\nfrom torch_pitch_shift import pitch_shift, get_fast_shifts\ndef shift_pitch(signal, interval: int):\n    signal.samples = pitch_shift(\n        signal.samples, \n        shift=interval, \n        sample_rate=signal.sample_rate\n    )\n    return signal", "\ndef load_interface():\n    with argbind.scope(conf):\n        interface = Interface()\n        # loader = AudioLoader()\n        print(f\"interface device is {interface.device}\")\n        return interface\n\n\n", "\n\n\ninterface = load_interface()\n\n\n\n\n# dataset = at.data.datasets.AudioDataset(\n#     loader,", "# dataset = at.data.datasets.AudioDataset(\n#     loader,\n#     sample_rate=interface.codec.sample_rate,\n#     duration=interface.coarse.chunk_size_s,\n#     n_examples=5000,\n#     without_replacement=True,\n# )\n\nOUT_DIR = Path(\"gradio-outputs\")\nOUT_DIR.mkdir(exist_ok=True, parents=True)", "OUT_DIR = Path(\"gradio-outputs\")\nOUT_DIR.mkdir(exist_ok=True, parents=True)\n\n\ndef load_audio(file):\n    print(file)\n    filepath = file.name\n    sig = at.AudioSignal.salient_excerpt(\n        filepath, \n        duration=interface.coarse.chunk_size_s\n    )\n    sig = interface.preprocess(sig)\n\n    out_dir = OUT_DIR / \"tmp\" / str(uuid.uuid4())\n    out_dir.mkdir(parents=True, exist_ok=True)\n    sig.write(out_dir / \"input.wav\")\n    return sig.path_to_file", "\n\ndef load_example_audio():\n    return \"./assets/example.wav\"\n\n\ndef _vamp(data, return_mask=False):\n\n    out_dir = OUT_DIR / str(uuid.uuid4())\n    out_dir.mkdir()\n    sig = at.AudioSignal(data[input_audio])\n    sig = interface.preprocess(sig)\n\n    if data[pitch_shift_amt] != 0:\n        sig = shift_pitch(sig, data[pitch_shift_amt])\n\n    z = interface.encode(sig)\n\n    ncc = data[n_conditioning_codebooks]\n\n    # build the mask\n    mask = pmask.linear_random(z, data[rand_mask_intensity])\n    mask = pmask.mask_and(\n        mask, pmask.inpaint(\n            z,\n            interface.s2t(data[prefix_s]),\n            interface.s2t(data[suffix_s])\n        )\n    )\n    mask = pmask.mask_and(\n        mask, pmask.periodic_mask(\n            z,\n            data[periodic_p],\n            data[periodic_w],\n            random_roll=True\n        )\n    )\n    if data[onset_mask_width] > 0:\n        mask = pmask.mask_or(\n            mask, pmask.onset_mask(sig, z, interface, width=data[onset_mask_width])\n        )\n    if data[beat_mask_width] > 0:\n        beat_mask = interface.make_beat_mask(\n            sig,\n            after_beat_s=(data[beat_mask_width]/1000), \n            mask_upbeats=not data[beat_mask_downbeats],\n        )\n        mask = pmask.mask_and(mask, beat_mask)\n\n    # these should be the last two mask ops\n    mask = pmask.dropout(mask, data[dropout])\n    mask = pmask.codebook_unmask(mask, ncc)\n\n\n    print(f\"dropout {data[dropout]}\")\n    print(f\"masktemp {data[masktemp]}\")\n    print(f\"sampletemp {data[sampletemp]}\")\n    print(f\"top_p {data[top_p]}\")\n    print(f\"prefix_s {data[prefix_s]}\")\n    print(f\"suffix_s {data[suffix_s]}\")\n    print(f\"rand_mask_intensity {data[rand_mask_intensity]}\")\n    print(f\"num_steps {data[num_steps]}\")\n    print(f\"periodic_p {data[periodic_p]}\")\n    print(f\"periodic_w {data[periodic_w]}\")\n    print(f\"n_conditioning_codebooks {data[n_conditioning_codebooks]}\")\n    print(f\"use_coarse2fine {data[use_coarse2fine]}\")\n    print(f\"onset_mask_width {data[onset_mask_width]}\")\n    print(f\"beat_mask_width {data[beat_mask_width]}\")\n    print(f\"beat_mask_downbeats {data[beat_mask_downbeats]}\")\n    print(f\"stretch_factor {data[stretch_factor]}\")\n    print(f\"seed {data[seed]}\")\n    print(f\"pitch_shift_amt {data[pitch_shift_amt]}\")\n    print(f\"sample_cutoff {data[sample_cutoff]}\")\n    \n    \n    _top_p = data[top_p] if data[top_p] > 0 else None\n    # save the mask as a txt file\n    np.savetxt(out_dir / \"mask.txt\", mask[:,0,:].long().cpu().numpy())\n\n    _seed = data[seed] if data[seed] > 0 else None\n    zv, mask_z = interface.coarse_vamp(\n        z, \n        mask=mask,\n        sampling_steps=data[num_steps],\n        mask_temperature=data[masktemp]*10,\n        sampling_temperature=data[sampletemp],\n        return_mask=True, \n        typical_filtering=data[typical_filtering], \n        typical_mass=data[typical_mass], \n        typical_min_tokens=data[typical_min_tokens], \n        top_p=_top_p,\n        gen_fn=interface.coarse.generate,\n        seed=_seed,\n        sample_cutoff=data[sample_cutoff],\n    )\n\n    if use_coarse2fine: \n        zv = interface.coarse_to_fine(\n            zv, \n            mask_temperature=data[masktemp]*10, \n            sampling_temperature=data[sampletemp],\n            mask=mask,\n            sampling_steps=data[num_steps],\n            sample_cutoff=data[sample_cutoff], \n            seed=_seed,\n        )\n\n    sig = interface.to_signal(zv).cpu()\n    print(\"done\")\n\n    \n\n    sig.write(out_dir / \"output.wav\")\n\n    if return_mask:\n        mask = interface.to_signal(mask_z).cpu()\n        mask.write(out_dir / \"mask.wav\")\n        return sig.path_to_file, mask.path_to_file\n    else:\n        return sig.path_to_file", "\ndef vamp(data):\n    return _vamp(data, return_mask=True)\n\ndef api_vamp(data):\n    return _vamp(data, return_mask=False)\n        \ndef save_vamp(data):\n    out_dir = OUT_DIR / \"saved\" / str(uuid.uuid4())\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    sig_in = at.AudioSignal(data[input_audio])\n    sig_out = at.AudioSignal(data[output_audio])\n\n    sig_in.write(out_dir / \"input.wav\")\n    sig_out.write(out_dir / \"output.wav\")\n    \n    _data = {\n        \"masktemp\": data[masktemp],\n        \"sampletemp\": data[sampletemp],\n        \"top_p\": data[top_p],\n        \"prefix_s\": data[prefix_s],\n        \"suffix_s\": data[suffix_s],\n        \"rand_mask_intensity\": data[rand_mask_intensity],\n        \"num_steps\": data[num_steps],\n        \"notes\": data[notes_text],\n        \"periodic_period\": data[periodic_p],\n        \"periodic_width\": data[periodic_w],\n        \"n_conditioning_codebooks\": data[n_conditioning_codebooks], \n        \"use_coarse2fine\": data[use_coarse2fine],\n        \"stretch_factor\": data[stretch_factor],\n        \"seed\": data[seed],\n        \"samplecutoff\": data[sample_cutoff],\n    }\n\n    # save with yaml\n    with open(out_dir / \"data.yaml\", \"w\") as f:\n        yaml.dump(_data, f)\n\n    import zipfile\n    zip_path = out_dir.with_suffix(\".zip\")\n    with zipfile.ZipFile(zip_path, \"w\") as zf:\n        for file in out_dir.iterdir():\n            zf.write(file, file.name)\n\n    return f\"saved! your save code is {out_dir.stem}\", zip_path", "\n\n\nwith gr.Blocks() as demo:\n\n    with gr.Row():\n        with gr.Column():\n            gr.Markdown(\"# VampNet Audio Vamping\")\n            gr.Markdown(\"\"\"## Description:\n            This is a demo of the VampNet, a generative audio model that transforms the input audio based on the chosen settings. \n            You can control the extent and nature of variation with a set of manual controls and presets. \n            Use this interface to experiment with different mask settings and explore the audio outputs.\n            \"\"\")\n\n            gr.Markdown(\"\"\"\n            ## Instructions:\n            1. You can start by uploading some audio, or by loading the example audio. \n            2. Choose a preset for the vamp operation, or manually adjust the controls to customize the mask settings. \n            3. Click the \"generate (vamp)!!!\" button to apply the vamp operation. Listen to the output audio.\n            4. Optionally, you can add some notes and save the result. \n            5. You can also use the output as the new input and continue experimenting!\n            \"\"\")\n    with gr.Row():\n        with gr.Column():\n\n\n            manual_audio_upload = gr.File(\n                label=f\"upload some audio (will be randomly trimmed to max of {interface.coarse.chunk_size_s:.2f}s)\",\n                file_types=[\"audio\"]\n            )\n            load_example_audio_button = gr.Button(\"or load example audio\")\n\n            input_audio = gr.Audio(\n                label=\"input audio\",\n                interactive=False, \n                type=\"filepath\",\n            )\n\n            audio_mask = gr.Audio(\n                label=\"audio mask (listen to this to hear the mask hints)\",\n                interactive=False, \n                type=\"filepath\",\n            )\n\n            # connect widgets\n            load_example_audio_button.click(\n                fn=load_example_audio,\n                inputs=[],\n                outputs=[ input_audio]\n            )\n\n            manual_audio_upload.change(\n                fn=load_audio,\n                inputs=[manual_audio_upload],\n                outputs=[ input_audio]\n            )\n                \n        # mask settings\n        with gr.Column():\n\n\n            presets = {\n                    \"unconditional\": {\n                        \"periodic_p\": 0,\n                        \"onset_mask_width\": 0,\n                        \"beat_mask_width\": 0,\n                        \"beat_mask_downbeats\": False,\n                    }, \n                    \"slight periodic variation\": {\n                        \"periodic_p\": 5,\n                        \"onset_mask_width\": 5,\n                        \"beat_mask_width\": 0,\n                        \"beat_mask_downbeats\": False,\n                    },\n                    \"moderate periodic variation\": {\n                        \"periodic_p\": 13,\n                        \"onset_mask_width\": 5,\n                        \"beat_mask_width\": 0,\n                        \"beat_mask_downbeats\": False,\n                    },\n                    \"strong periodic variation\": {\n                        \"periodic_p\": 17,\n                        \"onset_mask_width\": 5,\n                        \"beat_mask_width\": 0,\n                        \"beat_mask_downbeats\": False,\n                    },\n                    \"very strong periodic variation\": {\n                        \"periodic_p\": 21,\n                        \"onset_mask_width\": 5,\n                        \"beat_mask_width\": 0,\n                        \"beat_mask_downbeats\": False,\n                    },\n                    \"beat-driven variation\": {\n                        \"periodic_p\": 0,\n                        \"onset_mask_width\": 0,\n                        \"beat_mask_width\": 50,\n                        \"beat_mask_downbeats\": False,\n                    },\n                    \"beat-driven variation (downbeats only)\": {\n                        \"periodic_p\": 0,\n                        \"onset_mask_width\": 0,\n                        \"beat_mask_width\": 50,\n                        \"beat_mask_downbeats\": True,\n                    },\n                    \"beat-driven variation (downbeats only, strong)\": {\n                        \"periodic_p\": 0,\n                        \"onset_mask_width\": 0,\n                        \"beat_mask_width\": 20,\n                        \"beat_mask_downbeats\": True,\n                    },\n                }\n\n            preset = gr.Dropdown(\n                label=\"preset\", \n                choices=list(presets.keys()),\n                value=\"strong periodic variation\",\n            )\n            load_preset_button = gr.Button(\"load_preset\")\n\n            with gr.Accordion(\"manual controls\", open=True):\n                periodic_p = gr.Slider(\n                    label=\"periodic prompt  (0 - unconditional, 2 - lots of hints, 8 - a couple of hints, 16 - occasional hint, 32 - very occasional hint, etc)\",\n                    minimum=0,\n                    maximum=128, \n                    step=1,\n                    value=3, \n                )\n\n\n                onset_mask_width = gr.Slider(\n                    label=\"onset mask width (multiplies with the periodic mask, 1 step ~= 10milliseconds) \",\n                    minimum=0,\n                    maximum=100,\n                    step=1,\n                    value=5,\n                )\n\n                beat_mask_width = gr.Slider(\n                    label=\"beat mask width (in milliseconds)\",\n                    minimum=0,\n                    maximum=200,\n                    value=0,\n                )\n                beat_mask_downbeats = gr.Checkbox(\n                    label=\"beat mask downbeats only?\", \n                    value=False\n                )\n\n\n                with gr.Accordion(\"extras \", open=False):\n                    pitch_shift_amt = gr.Slider(\n                        label=\"pitch shift amount (semitones)\",\n                        minimum=-12,\n                        maximum=12,\n                        step=1,\n                        value=0,\n                    )\n\n                    rand_mask_intensity = gr.Slider(\n                        label=\"random mask intensity. (If this is less than 1, scatters prompts throughout the audio, should be between 0.9 and 1.0)\",\n                        minimum=0.0,\n                        maximum=1.0,\n                        value=1.0\n                    )\n\n                    periodic_w = gr.Slider(\n                        label=\"periodic prompt width (steps, 1 step ~= 10milliseconds)\",\n                        minimum=1,\n                        maximum=20,\n                        step=1,\n                        value=1,\n                    )\n                    n_conditioning_codebooks = gr.Number(\n                        label=\"number of conditioning codebooks. probably 0\", \n                        value=0,\n                        precision=0,\n                    )\n\n                    stretch_factor = gr.Slider(\n                        label=\"time stretch factor\",\n                        minimum=0,\n                        maximum=64, \n                        step=1,\n                        value=1, \n                    )\n\n            preset_outputs = {\n                periodic_p, \n                onset_mask_width, \n                beat_mask_width,\n                beat_mask_downbeats,\n            }\n\n            def load_preset(_preset):\n                return tuple(presets[_preset].values())\n\n            load_preset_button.click(\n                fn=load_preset,\n                inputs=[preset],\n                outputs=preset_outputs\n            )\n\n\n            with gr.Accordion(\"prefix/suffix prompts\", open=False):\n                prefix_s = gr.Slider(\n                    label=\"prefix hint length (seconds)\",\n                    minimum=0.0,\n                    maximum=10.0,\n                    value=0.0\n                )\n                suffix_s = gr.Slider(\n                    label=\"suffix hint length (seconds)\",\n                    minimum=0.0,\n                    maximum=10.0,\n                    value=0.0\n                )\n\n            masktemp = gr.Slider(\n                label=\"mask temperature\",\n                minimum=0.0,\n                maximum=100.0,\n                value=1.5\n            )\n            sampletemp = gr.Slider(\n                label=\"sample temperature\",\n                minimum=0.1,\n                maximum=10.0,\n                value=1.0, \n                step=0.001\n            )\n        \n\n\n            with gr.Accordion(\"sampling settings\", open=False):\n                top_p = gr.Slider(\n                    label=\"top p (0.0 = off)\",\n                    minimum=0.0,\n                    maximum=1.0,\n                    value=0.0\n                )\n                typical_filtering = gr.Checkbox(\n                    label=\"typical filtering \",\n                    value=False\n                )\n                typical_mass = gr.Slider( \n                    label=\"typical mass (should probably stay between 0.1 and 0.5)\",\n                    minimum=0.01,\n                    maximum=0.99,\n                    value=0.15\n                )\n                typical_min_tokens = gr.Slider(\n                    label=\"typical min tokens (should probably stay between 1 and 256)\",\n                    minimum=1,\n                    maximum=256,\n                    step=1,\n                    value=64\n                )\n                sample_cutoff = gr.Slider(\n                    label=\"sample cutoff\",\n                    minimum=0.0,\n                    maximum=1.0,\n                    value=0.5, \n                    step=0.01\n                )\n\n            use_coarse2fine = gr.Checkbox(\n                label=\"use coarse2fine\",\n                value=True, \n                visible=False\n            )\n\n            num_steps = gr.Slider(\n                label=\"number of steps (should normally be between 12 and 36)\",\n                minimum=1,\n                maximum=128,\n                step=1,\n                value=36\n            )\n\n            dropout = gr.Slider(\n                label=\"mask dropout\",\n                minimum=0.0,\n                maximum=1.0,\n                step=0.01,\n                value=0.0\n            )\n\n\n            seed = gr.Number(\n                label=\"seed (0 for random)\",\n                value=0,\n                precision=0,\n            )\n\n\n\n        # mask settings\n        with gr.Column():\n\n            # lora_choice = gr.Dropdown(\n            #     label=\"lora choice\", \n            #     choices=list(loras.keys()),\n            #     value=LORA_NONE, \n            #     visible=False\n            # )\n\n            vamp_button = gr.Button(\"generate (vamp)!!!\")\n            output_audio = gr.Audio(\n                label=\"output audio\",\n                interactive=False,\n                type=\"filepath\"\n            )\n\n            notes_text = gr.Textbox(\n                label=\"type any notes about the generated audio here\", \n                value=\"\",\n                interactive=True\n            )\n            save_button = gr.Button(\"save vamp\")\n            download_file = gr.File(\n                label=\"vamp to download will appear here\",\n                interactive=False\n            )\n            use_as_input_button = gr.Button(\"use output as input\")\n            \n            thank_you = gr.Markdown(\"\")\n\n\n    _inputs = {\n            input_audio, \n            num_steps,\n            masktemp,\n            sampletemp,\n            top_p,\n            prefix_s, suffix_s, \n            rand_mask_intensity, \n            periodic_p, periodic_w,\n            n_conditioning_codebooks, \n            dropout,\n            use_coarse2fine, \n            stretch_factor, \n            onset_mask_width, \n            typical_filtering,\n            typical_mass,\n            typical_min_tokens,\n            beat_mask_width,\n            beat_mask_downbeats,\n            seed, \n            # lora_choice,\n            pitch_shift_amt, \n            sample_cutoff\n        }\n  \n    # connect widgets\n    vamp_button.click(\n        fn=vamp,\n        inputs=_inputs,\n        outputs=[output_audio, audio_mask], \n    )\n\n    api_vamp_button = gr.Button(\"api vamp\", visible=False)\n    api_vamp_button.click(\n        fn=api_vamp,\n        inputs=_inputs, \n        outputs=[output_audio], \n        api_name=\"vamp\"\n    )\n\n    use_as_input_button.click(\n        fn=lambda x: x,\n        inputs=[output_audio],\n        outputs=[input_audio]\n    )\n\n    save_button.click(\n        fn=save_vamp,\n        inputs=_inputs | {notes_text, output_audio},\n        outputs=[thank_you, download_file]\n    )", "\ndemo.launch(share=True, enable_queue=True, debug=True)\n"]}
{"filename": "scripts/exp/experiment.py", "chunked_list": ["from pathlib import Path\nimport random\nfrom typing import List\nimport tempfile\nimport subprocess\n\nimport argbind\nfrom tqdm import tqdm\nimport torch\n", "import torch\n\nfrom vampnet.interface import Interface\nfrom vampnet import mask as pmask\nimport audiotools as at\n\nInterface: Interface = argbind.bind(Interface)\n\n\n\ndef calculate_bitrate(\n        interface, num_codebooks, \n        downsample_factor\n    ):\n    bit_width = 10\n    sr = interface.codec.sample_rate\n    hop = interface.codec.hop_size\n    rate = (sr / hop) * ((bit_width * num_codebooks) / downsample_factor)\n    return rate", "\n\ndef calculate_bitrate(\n        interface, num_codebooks, \n        downsample_factor\n    ):\n    bit_width = 10\n    sr = interface.codec.sample_rate\n    hop = interface.codec.hop_size\n    rate = (sr / hop) * ((bit_width * num_codebooks) / downsample_factor)\n    return rate", "\ndef baseline(sig, interface):\n    return interface.preprocess(sig)\n\ndef reconstructed(sig, interface):\n    return interface.to_signal(\n        interface.encode(sig)\n    )\n\ndef coarse2fine(sig, interface):\n    z = interface.encode(sig)\n    z = z[:, :interface.c2f.n_conditioning_codebooks, :]\n\n    z = interface.coarse_to_fine(z)\n    return interface.to_signal(z)", "\ndef coarse2fine(sig, interface):\n    z = interface.encode(sig)\n    z = z[:, :interface.c2f.n_conditioning_codebooks, :]\n\n    z = interface.coarse_to_fine(z)\n    return interface.to_signal(z)\n\nclass CoarseCond:\n\n    def __init__(self, num_conditioning_codebooks, downsample_factor):\n        self.num_conditioning_codebooks = num_conditioning_codebooks\n        self.downsample_factor = downsample_factor\n\n    def __call__(self, sig, interface):\n        z = interface.encode(sig)\n        mask = pmask.full_mask(z)\n        mask = pmask.codebook_unmask(mask, self.num_conditioning_codebooks)\n        mask = pmask.periodic_mask(mask, self.downsample_factor)\n\n        zv = interface.coarse_vamp(z, mask)\n        zv = interface.coarse_to_fine(zv)\n        return interface.to_signal(zv)", "class CoarseCond:\n\n    def __init__(self, num_conditioning_codebooks, downsample_factor):\n        self.num_conditioning_codebooks = num_conditioning_codebooks\n        self.downsample_factor = downsample_factor\n\n    def __call__(self, sig, interface):\n        z = interface.encode(sig)\n        mask = pmask.full_mask(z)\n        mask = pmask.codebook_unmask(mask, self.num_conditioning_codebooks)\n        mask = pmask.periodic_mask(mask, self.downsample_factor)\n\n        zv = interface.coarse_vamp(z, mask)\n        zv = interface.coarse_to_fine(zv)\n        return interface.to_signal(zv)", "\ndef opus(sig, interface, bitrate=128):\n    sig = interface.preprocess(sig)\n    \n    with tempfile.NamedTemporaryFile(suffix=\".wav\") as f:\n        sig.write(f.name)\n\n        opus_name = Path(f.name).with_suffix(\".opus\")\n        # convert to opus\n        cmd = [\n            \"ffmpeg\", \"-y\", \"-i\", f.name, \n            \"-c:a\", \"libopus\", \n            \"-b:a\", f\"{bitrate}\", \n           opus_name\n        ]\n        subprocess.run(cmd, check=True)\n\n        # convert back to wav\n        output_name = Path(f\"{f.name}-opus\").with_suffix(\".wav\")\n        cmd = [\n            \"ffmpeg\", \"-y\", \"-i\", opus_name, \n            output_name\n        ]\n\n        subprocess.run(cmd, check=True)\n\n        sig = at.AudioSignal(\n            output_name, \n            sample_rate=sig.sample_rate\n        )\n    return sig", "\ndef mask_ratio_1_step(ratio=1.0):\n    def wrapper(sig, interface):\n        z = interface.encode(sig)\n        mask = pmask.linear_random(z, ratio)\n        zv = interface.coarse_vamp(\n            z, \n            mask,\n            sampling_steps=1, \n        )\n\n        return interface.to_signal(zv)\n    return wrapper", "\ndef num_sampling_steps(num_steps=1):\n    def wrapper(sig, interface: Interface):\n        z = interface.encode(sig)\n        mask = pmask.periodic_mask(z, 16)\n        zv = interface.coarse_vamp(\n            z, \n            mask,\n            sampling_steps=num_steps, \n        )\n\n        zv = interface.coarse_to_fine(zv)\n        return interface.to_signal(zv)\n    return wrapper", "\ndef beat_mask(ctx_time):\n    def wrapper(sig, interface):\n        beat_mask = interface.make_beat_mask(\n            sig,\n            before_beat_s=ctx_time/2,\n            after_beat_s=ctx_time/2,\n            invert=True\n        )\n\n        z = interface.encode(sig)\n\n        zv = interface.coarse_vamp(\n            z, beat_mask\n        )\n\n        zv = interface.coarse_to_fine(zv)\n        return interface.to_signal(zv)\n    return wrapper", "\ndef inpaint(ctx_time):\n    def wrapper(sig, interface: Interface):\n        z = interface.encode(sig)\n        mask = pmask.inpaint(z, interface.s2t(ctx_time), interface.s2t(ctx_time))\n\n        zv = interface.coarse_vamp(z, mask)\n        zv = interface.coarse_to_fine(zv)\n        \n        return interface.to_signal(zv)\n    return wrapper", "\ndef token_noise(noise_amt):\n    def wrapper(sig, interface: Interface):\n        z = interface.encode(sig)\n        mask = pmask.random(z, noise_amt)\n        z = torch.where(\n            mask, \n            torch.randint_like(z, 0, interface.coarse.vocab_size), \n            z\n        )\n        return interface.to_signal(z)\n    return wrapper", "\nEXP_REGISTRY = {}\n\nEXP_REGISTRY[\"gen-compression\"] = {\n    \"baseline\": baseline,\n    \"reconstructed\": reconstructed,\n    \"coarse2fine\": coarse2fine,\n    **{\n        f\"{n}_codebooks_downsampled_{x}x\": CoarseCond(num_conditioning_codebooks=n, downsample_factor=x)\n            for (n, x) in (", "        f\"{n}_codebooks_downsampled_{x}x\": CoarseCond(num_conditioning_codebooks=n, downsample_factor=x)\n            for (n, x) in (\n                (1, 1), # 1 codebook, no downsampling\n                (4, 4), # 4 codebooks, downsampled 4x\n                (4, 16), # 4 codebooks, downsampled 16x\n                (4, 32), # 4 codebooks, downsampled 16x\n            )\n    }, \n    **{\n        f\"token_noise_{x}\": mask_ratio_1_step(ratio=x)", "    **{\n        f\"token_noise_{x}\": mask_ratio_1_step(ratio=x)\n            for x in [0.25, 0.5, 0.75]\n    },\n\n}\n\n\nEXP_REGISTRY[\"sampling-steps\"] = {\n    # \"codec\": reconstructed,", "EXP_REGISTRY[\"sampling-steps\"] = {\n    # \"codec\": reconstructed,\n    **{f\"steps_{n}\": num_sampling_steps(n)  for n in [1, 4, 12, 36, 64, 72]},\n}\n\n\nEXP_REGISTRY[\"musical-sampling\"] = {\n    **{f\"beat_mask_{t}\": beat_mask(t) for t in [0.075]}, \n    **{f\"inpaint_{t}\": inpaint(t) for t in [0.5, 1.0,]}, # multiply these by 2 (they go left and right)\n}", "    **{f\"inpaint_{t}\": inpaint(t) for t in [0.5, 1.0,]}, # multiply these by 2 (they go left and right)\n}\n\n@argbind.bind(without_prefix=True)\ndef main(\n        sources=[\n            \"/media/CHONK/hugo/spotdl/val\",\n        ], \n        output_dir: str = \"./samples\",\n        max_excerpts: int = 2000,\n        exp_type: str = \"gen-compression\", \n        seed: int = 0,\n        ext: str = [\".mp3\"],\n    ):\n    at.util.seed(seed)\n    interface = Interface()\n\n    output_dir = Path(output_dir) \n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    from audiotools.data.datasets import AudioLoader, AudioDataset\n\n    loader = AudioLoader(sources=sources, shuffle_state=seed, ext=ext)\n    dataset = AudioDataset(loader, \n        sample_rate=interface.codec.sample_rate, \n        duration=interface.coarse.chunk_size_s, \n        n_examples=max_excerpts, \n        without_replacement=True,\n    )\n\n    if exp_type in EXP_REGISTRY:\n        SAMPLE_CONDS = EXP_REGISTRY[exp_type]\n    else:\n        raise ValueError(f\"Unknown exp_type {exp_type}\")\n\n\n    indices = list(range(max_excerpts))\n    random.shuffle(indices)\n    for i in tqdm(indices):\n        # if all our files are already there, skip\n        done = []\n        for name in SAMPLE_CONDS:\n            o_dir = Path(output_dir) / name\n            done.append((o_dir / f\"{i}.wav\").exists())\n        if all(done):\n            continue\n\n        sig = dataset[i][\"signal\"]\n        results = {\n            name: cond(sig, interface).cpu()\n            for name, cond in SAMPLE_CONDS.items()\n        }\n\n        for name, sig in results.items():\n            o_dir = Path(output_dir) / name\n            o_dir.mkdir(exist_ok=True, parents=True)\n\n            sig.write(o_dir / f\"{i}.wav\")", "\nif __name__ == \"__main__\":\n    args = argbind.parse_args()\n\n    with argbind.scope(args):\n        main()\n"]}
{"filename": "scripts/exp/fine_tune.py", "chunked_list": ["import argbind\nfrom pathlib import Path\nimport yaml\nfrom typing import List\n\n\n\n\n\"\"\"example output: (yaml)\n", "\"\"\"example output: (yaml)\n\n\"\"\"\n\n@argbind.bind(without_prefix=True, positional=True)\ndef fine_tune(audio_files_or_folders: List[str], name: str):\n\n    conf_dir = Path(\"conf\")\n    assert conf_dir.exists(), \"conf directory not found. are you in the vampnet directory?\"\n\n    conf_dir = conf_dir / \"generated\"\n    conf_dir.mkdir(exist_ok=True)\n\n    finetune_dir = conf_dir / name\n    finetune_dir.mkdir(exist_ok=True)\n\n    finetune_c2f_conf = {\n        \"$include\": [\"conf/lora/lora.yml\"],\n        \"fine_tune\": True,\n        \"train/AudioLoader.sources\": audio_files_or_folders,\n        \"val/AudioLoader.sources\": audio_files_or_folders,\n        \"VampNet.n_codebooks\": 14,\n        \"VampNet.n_conditioning_codebooks\": 4,\n        \"VampNet.embedding_dim\": 1280,\n        \"VampNet.n_layers\": 16,\n        \"VampNet.n_heads\": 20,\n        \"AudioDataset.duration\": 3.0,\n        \"AudioDataset.loudness_cutoff\": -40.0,\n        \"save_path\": f\"./runs/{name}/c2f\",\n        \"fine_tune_checkpoint\": \"./models/vampnet/c2f.pth\"\n    }\n\n    finetune_coarse_conf = {\n        \"$include\": [\"conf/lora/lora.yml\"],\n        \"fine_tune\": True,\n        \"train/AudioLoader.sources\": audio_files_or_folders,\n        \"val/AudioLoader.sources\": audio_files_or_folders,\n        \"save_path\": f\"./runs/{name}/coarse\",\n        \"fine_tune_checkpoint\": \"./models/vampnet/coarse.pth\"\n    }\n\n    interface_conf = {\n        \"Interface.coarse_ckpt\": f\"./runs/{name}/coarse/latest/vampnet/weights.pth\",\n\n        \"Interface.coarse2fine_ckpt\": f\"./runs/{name}/c2f/latest/vampnet/weights.pth\",\n        \"Interface.wavebeat_ckpt\": \"./models/wavebeat.pth\",\n\n        \"Interface.codec_ckpt\": \"./models/vampnet/codec.pth\",\n        \"AudioLoader.sources\": [audio_files_or_folders],\n    }\n\n    # save the confs\n    with open(finetune_dir / \"c2f.yml\", \"w\") as f:\n        yaml.dump(finetune_c2f_conf, f)\n\n    with open(finetune_dir / \"coarse.yml\", \"w\") as f:\n        yaml.dump(finetune_coarse_conf, f)\n    \n    with open(finetune_dir / \"interface.yml\", \"w\") as f: \n        yaml.dump(interface_conf, f)\n\n\n    print(f\"generated confs in {finetune_dir}. run training jobs with `python scripts/exp/train.py --args.load {finetune_dir}/<c2f/coarse>.yml` \")", "\nif __name__ == \"__main__\":\n    args = argbind.parse_args()\n\n    with argbind.scope(args):\n        fine_tune()\n\n\n\n    ", "\n    "]}
{"filename": "scripts/exp/train.py", "chunked_list": ["import os\nimport sys\nimport warnings\nfrom pathlib import Path\nfrom typing import Optional\nfrom dataclasses import dataclass\n\nimport argbind\nimport audiotools as at\nimport torch", "import audiotools as at\nimport torch\nimport torch.nn as nn\nfrom audiotools import AudioSignal\nfrom audiotools.data import transforms\nfrom einops import rearrange\nfrom rich import pretty\nfrom rich.traceback import install\nfrom torch.utils.tensorboard import SummaryWriter\n", "from torch.utils.tensorboard import SummaryWriter\n\nimport vampnet\nfrom vampnet.modules.transformer import VampNet\nfrom vampnet.util import codebook_unflatten, codebook_flatten\nfrom vampnet import mask as pmask\n# from dac.model.dac import DAC\nfrom lac.model.lac import LAC as DAC\n\nfrom audiotools.ml.decorators import (", "\nfrom audiotools.ml.decorators import (\n    timer, Tracker, when\n)\n\nimport loralib as lora\n\nimport torch._dynamo\ntorch._dynamo.config.verbose=True\n", "torch._dynamo.config.verbose=True\n\n\n# Enable cudnn autotuner to speed up training\n# (can be altered by the funcs.seed function)\ntorch.backends.cudnn.benchmark = bool(int(os.getenv(\"CUDNN_BENCHMARK\", 1)))\n# Uncomment to trade memory for speed.\n\n# Install to make things look nice\nwarnings.filterwarnings(\"ignore\", category=UserWarning)", "# Install to make things look nice\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\npretty.install()\ninstall()\n\n# optim\nAccelerator = argbind.bind(at.ml.Accelerator, without_prefix=True)\nCrossEntropyLoss = argbind.bind(nn.CrossEntropyLoss)\nAdamW = argbind.bind(torch.optim.AdamW)\nNoamScheduler = argbind.bind(vampnet.scheduler.NoamScheduler)", "AdamW = argbind.bind(torch.optim.AdamW)\nNoamScheduler = argbind.bind(vampnet.scheduler.NoamScheduler)\n\n# transforms\nfilter_fn = lambda fn: hasattr(fn, \"transform\") and fn.__qualname__ not in [\n    \"BaseTransform\",\n    \"Compose\",\n    \"Choose\",\n]\ntfm = argbind.bind_module(transforms, \"train\", \"val\", filter_fn=filter_fn)", "]\ntfm = argbind.bind_module(transforms, \"train\", \"val\", filter_fn=filter_fn)\n\n# model\nVampNet = argbind.bind(VampNet)\n\n\n# data\nAudioLoader = argbind.bind(at.datasets.AudioLoader)\nAudioDataset = argbind.bind(at.datasets.AudioDataset, \"train\", \"val\")", "AudioLoader = argbind.bind(at.datasets.AudioLoader)\nAudioDataset = argbind.bind(at.datasets.AudioDataset, \"train\", \"val\")\n\nIGNORE_INDEX = -100\n\n\n@argbind.bind(\"train\", \"val\", without_prefix=True)\ndef build_transform():\n    transform = transforms.Compose(\n        tfm.VolumeNorm((\"const\", -24)),\n        # tfm.PitchShift(),\n        tfm.RescaleAudio(),\n    )\n    return transform", "\n\n@torch.no_grad()\ndef apply_transform(transform_fn, batch):\n    sig: AudioSignal = batch[\"signal\"]\n    kwargs = batch[\"transform_args\"]\n\n    sig: AudioSignal = transform_fn(sig.clone(), **kwargs)\n    return sig\n", "\n\ndef build_datasets(args, sample_rate: int):\n    with argbind.scope(args, \"train\"):\n        train_data = AudioDataset(\n            AudioLoader(), sample_rate, transform=build_transform()\n        )\n    with argbind.scope(args, \"val\"):\n        val_data = AudioDataset(AudioLoader(), sample_rate, transform=build_transform())\n    return train_data, val_data", "\n\ndef rand_float(shape, low, high, rng):\n    return rng.draw(shape)[:, 0] * (high - low) + low\n\n\ndef flip_coin(shape, p, rng):\n    return rng.draw(shape)[:, 0] < p\n\n\ndef num_params_hook(o, p):\n    return o + f\" {p/1e6:<.3f}M params.\"", "\n\ndef num_params_hook(o, p):\n    return o + f\" {p/1e6:<.3f}M params.\"\n\n\ndef add_num_params_repr_hook(model):\n    import numpy as np\n    from functools import partial\n\n    for n, m in model.named_modules():\n        o = m.extra_repr()\n        p = sum([np.prod(p.size()) for p in m.parameters()])\n\n        setattr(m, \"extra_repr\", partial(num_params_hook, o=o, p=p))", "\n\ndef accuracy(\n    preds: torch.Tensor,\n    target: torch.Tensor,\n    top_k: int = 1,\n    ignore_index: Optional[int] = None,\n) -> torch.Tensor:\n    # Flatten the predictions and targets to be of shape (batch_size * sequence_length, n_class)\n    preds = rearrange(preds, \"b p s -> (b s) p\")\n    target = rearrange(target, \"b s -> (b s)\")\n\n    # return torchmetrics.functional.accuracy(preds, target, task='multiclass', top_k=topk, num_classes=preds.shape[-1], ignore_index=ignore_index)\n    if ignore_index is not None:\n        # Create a mask for the ignored index\n        mask = target != ignore_index\n        # Apply the mask to the target and predictions\n        preds = preds[mask]\n        target = target[mask]\n\n    # Get the top-k predicted classes and their indices\n    _, pred_indices = torch.topk(preds, k=top_k, dim=-1)\n\n    # Determine if the true target is in the top-k predicted classes\n    correct = torch.sum(torch.eq(pred_indices, target.unsqueeze(1)), dim=1)\n\n    # Calculate the accuracy\n    accuracy = torch.mean(correct.float())\n\n    return accuracy", "\ndef _metrics(z_hat, r, target, flat_mask, output):\n    for r_range in [(0, 0.5), (0.5, 1.0)]:\n        unmasked_target = target.masked_fill(flat_mask.bool(), IGNORE_INDEX)\n        masked_target = target.masked_fill(~flat_mask.bool(), IGNORE_INDEX)\n\n        assert target.shape[0] == r.shape[0]\n        # grab the indices of the r values that are in the range\n        r_idx = (r >= r_range[0]) & (r < r_range[1])\n\n        # grab the target and z_hat values that are in the range\n        r_unmasked_target = unmasked_target[r_idx]\n        r_masked_target = masked_target[r_idx]\n        r_z_hat = z_hat[r_idx]\n\n        for topk in (1, 25):\n            s, e = r_range\n            tag = f\"accuracy-{s}-{e}/top{topk}\"\n\n            output[f\"{tag}/unmasked\"] = accuracy(\n                preds=r_z_hat,\n                target=r_unmasked_target,\n                ignore_index=IGNORE_INDEX,\n                top_k=topk,\n            )\n            output[f\"{tag}/masked\"] = accuracy(\n                preds=r_z_hat,\n                target=r_masked_target,\n                ignore_index=IGNORE_INDEX,\n                top_k=topk,\n            )", "\n\n@dataclass\nclass State:\n    model: VampNet\n    codec: DAC\n\n    optimizer: AdamW\n    scheduler: NoamScheduler\n    criterion: CrossEntropyLoss\n    grad_clip_val: float\n\n    rng: torch.quasirandom.SobolEngine\n\n    train_data: AudioDataset\n    val_data: AudioDataset\n\n    tracker: Tracker", "\n\n@timer()\ndef train_loop(state: State, batch: dict, accel: Accelerator):\n    state.model.train()\n    batch = at.util.prepare_batch(batch, accel.device)\n    signal = apply_transform(state.train_data.transform, batch)\n\n    output = {}\n    vn = accel.unwrap(state.model)\n    with accel.autocast():\n        with torch.inference_mode():\n            state.codec.to(accel.device)\n            z = state.codec.encode(signal.samples, signal.sample_rate)[\"codes\"]\n            z = z[:, : vn.n_codebooks, :]\n\n        n_batch = z.shape[0]\n        r = state.rng.draw(n_batch)[:, 0].to(accel.device)\n\n        mask = pmask.random(z, r)\n        mask = pmask.codebook_unmask(mask, vn.n_conditioning_codebooks)\n        z_mask, mask = pmask.apply_mask(z, mask, vn.mask_token)\n        \n        z_mask_latent = vn.embedding.from_codes(z_mask, state.codec)\n\n        dtype = torch.bfloat16 if accel.amp else None\n        with accel.autocast(dtype=dtype):\n            z_hat = state.model(z_mask_latent)\n\n        target = codebook_flatten(\n            z[:, vn.n_conditioning_codebooks :, :],\n        )\n\n        flat_mask = codebook_flatten(\n            mask[:, vn.n_conditioning_codebooks :, :],\n        )\n\n        # replace target with ignore index for masked tokens\n        t_masked = target.masked_fill(~flat_mask.bool(), IGNORE_INDEX)\n        output[\"loss\"] = state.criterion(z_hat, t_masked)\n\n        _metrics(\n            r=r,\n            z_hat=z_hat,\n            target=target,\n            flat_mask=flat_mask,\n            output=output,\n        )\n\n    \n    accel.backward(output[\"loss\"])\n\n    output[\"other/learning_rate\"] = state.optimizer.param_groups[0][\"lr\"]\n    output[\"other/batch_size\"] = z.shape[0]\n\n\n    accel.scaler.unscale_(state.optimizer)\n    output[\"other/grad_norm\"] = torch.nn.utils.clip_grad_norm_(\n        state.model.parameters(), state.grad_clip_val\n    )\n\n    accel.step(state.optimizer)\n    state.optimizer.zero_grad()\n\n    state.scheduler.step()\n    accel.update()\n\n\n    return {k: v for k, v in sorted(output.items())}", "\n\n@timer()\n@torch.no_grad()\ndef val_loop(state: State, batch: dict, accel: Accelerator):\n    state.model.eval()\n    state.codec.eval()\n    batch = at.util.prepare_batch(batch, accel.device)\n    signal = apply_transform(state.val_data.transform, batch)\n\n    vn = accel.unwrap(state.model)\n    z = state.codec.encode(signal.samples, signal.sample_rate)[\"codes\"]\n    z = z[:, : vn.n_codebooks, :]\n\n    n_batch = z.shape[0]\n    r = state.rng.draw(n_batch)[:, 0].to(accel.device)\n\n    mask = pmask.random(z, r)\n    mask = pmask.codebook_unmask(mask, vn.n_conditioning_codebooks)\n    z_mask, mask = pmask.apply_mask(z, mask, vn.mask_token)\n\n    z_mask_latent = vn.embedding.from_codes(z_mask, state.codec)\n\n    z_hat = state.model(z_mask_latent)\n\n    target = codebook_flatten(\n        z[:, vn.n_conditioning_codebooks :, :],\n    )\n\n    flat_mask = codebook_flatten(\n        mask[:, vn.n_conditioning_codebooks :, :]\n    )\n\n    output = {}\n    # replace target with ignore index for masked tokens\n    t_masked = target.masked_fill(~flat_mask.bool(), IGNORE_INDEX)\n    output[\"loss\"] = state.criterion(z_hat, t_masked)\n\n    _metrics(\n        r=r,\n        z_hat=z_hat,\n        target=target,\n        flat_mask=flat_mask,\n        output=output,\n    )\n\n    return output", "\n\ndef validate(state, val_dataloader, accel):\n    for batch in val_dataloader:\n        output = val_loop(state, batch, accel)\n    # Consolidate state dicts if using ZeroRedundancyOptimizer\n    if hasattr(state.optimizer, \"consolidate_state_dict\"):\n        state.optimizer.consolidate_state_dict()\n    return output\n", "\n\ndef checkpoint(state, save_iters, save_path, fine_tune):\n    if accel.local_rank != 0:\n        state.tracker.print(f\"ERROR:Skipping checkpoint on rank {accel.local_rank}\")\n        return\n\n    metadata = {\"logs\": dict(state.tracker.history)}\n\n    tags = [\"latest\"]\n    state.tracker.print(f\"Saving to {str(Path('.').absolute())}\")\n\n    if state.tracker.step in save_iters:\n        tags.append(f\"{state.tracker.step // 1000}k\")\n\n    if state.tracker.is_best(\"val\", \"loss\"):\n        state.tracker.print(f\"Best model so far\")\n        tags.append(\"best\")\n\n    if fine_tune:\n        for tag in tags: \n            # save the lora model \n            (Path(save_path) / tag).mkdir(parents=True, exist_ok=True)\n            torch.save(\n                lora.lora_state_dict(accel.unwrap(state.model)), \n                f\"{save_path}/{tag}/lora.pth\"\n            )\n\n    for tag in tags:\n        model_extra = {\n            \"optimizer.pth\": state.optimizer.state_dict(),\n            \"scheduler.pth\": state.scheduler.state_dict(),\n            \"tracker.pth\": state.tracker.state_dict(),\n            \"metadata.pth\": metadata,\n        }\n\n        accel.unwrap(state.model).metadata = metadata\n        accel.unwrap(state.model).save_to_folder(\n            f\"{save_path}/{tag}\", model_extra, package=False\n        )", "\n\ndef save_sampled(state, z, writer):\n    num_samples = z.shape[0]\n\n    for i in range(num_samples):\n        sampled = accel.unwrap(state.model).generate(\n            codec=state.codec,\n            time_steps=z.shape[-1],\n            start_tokens=z[i : i + 1],\n        )\n        sampled.cpu().write_audio_to_tb(\n            f\"sampled/{i}\",\n            writer,\n            step=state.tracker.step,\n            plot_fn=None,\n        )", "\n\ndef save_imputation(state, z, val_idx, writer):\n    n_prefix = int(z.shape[-1] * 0.25)\n    n_suffix = int(z.shape[-1] *  0.25)\n\n    vn = accel.unwrap(state.model)\n\n    mask = pmask.inpaint(z, n_prefix, n_suffix)\n    mask = pmask.codebook_unmask(mask, vn.n_conditioning_codebooks)\n    z_mask, mask = pmask.apply_mask(z, mask, vn.mask_token)\n\n    imputed_noisy = vn.to_signal(z_mask, state.codec)\n    imputed_true = vn.to_signal(z, state.codec)\n\n    imputed = []\n    for i in range(len(z)):\n        imputed.append(\n            vn.generate(\n                codec=state.codec,\n                time_steps=z.shape[-1],\n                start_tokens=z[i][None, ...],\n                mask=mask[i][None, ...],\n            )   \n        )   \n    imputed = AudioSignal.batch(imputed)\n\n    for i in range(len(val_idx)):\n        imputed_noisy[i].cpu().write_audio_to_tb(\n            f\"imputed_noisy/{i}\",\n            writer,\n            step=state.tracker.step,\n            plot_fn=None,\n        )\n        imputed[i].cpu().write_audio_to_tb(\n            f\"imputed/{i}\",\n            writer,\n            step=state.tracker.step,\n            plot_fn=None,\n        )\n        imputed_true[i].cpu().write_audio_to_tb(\n            f\"imputed_true/{i}\",\n            writer,\n            step=state.tracker.step,\n            plot_fn=None,\n        )", "\n\n@torch.no_grad()\ndef save_samples(state: State, val_idx: int, writer: SummaryWriter):\n    state.model.eval()\n    state.codec.eval()\n    vn = accel.unwrap(state.model)\n\n    batch = [state.val_data[i] for i in val_idx]\n    batch = at.util.prepare_batch(state.val_data.collate(batch), accel.device)\n\n    signal = apply_transform(state.val_data.transform, batch)\n\n    z = state.codec.encode(signal.samples, signal.sample_rate)[\"codes\"]\n    z = z[:, : vn.n_codebooks, :]\n\n    r = torch.linspace(0.1, 0.95, len(val_idx)).to(accel.device)\n\n\n    mask = pmask.random(z, r)\n    mask = pmask.codebook_unmask(mask, vn.n_conditioning_codebooks)\n    z_mask, mask = pmask.apply_mask(z, mask, vn.mask_token)\n\n    z_mask_latent = vn.embedding.from_codes(z_mask, state.codec)\n\n    z_hat = state.model(z_mask_latent)\n\n    z_pred = torch.softmax(z_hat, dim=1).argmax(dim=1)\n    z_pred = codebook_unflatten(z_pred, n_c=vn.n_predict_codebooks)\n    z_pred = torch.cat([z[:, : vn.n_conditioning_codebooks, :], z_pred], dim=1)\n\n    generated = vn.to_signal(z_pred, state.codec)\n    reconstructed = vn.to_signal(z, state.codec)\n    masked = vn.to_signal(z_mask.squeeze(1), state.codec)\n\n    for i in range(generated.batch_size):\n        audio_dict = {\n            \"original\": signal[i],\n            \"masked\": masked[i],\n            \"generated\": generated[i],\n            \"reconstructed\": reconstructed[i],\n        }\n        for k, v in audio_dict.items():\n            v.cpu().write_audio_to_tb(\n                f\"samples/_{i}.r={r[i]:0.2f}/{k}\",\n                writer,\n                step=state.tracker.step,\n                plot_fn=None,\n            )\n\n    save_sampled(state=state, z=z, writer=writer)\n    save_imputation(state=state, z=z, val_idx=val_idx, writer=writer)", "\n\n\n@argbind.bind(without_prefix=True)\ndef load(\n    args,\n    accel: at.ml.Accelerator,\n    tracker: Tracker,\n    save_path: str,\n    resume: bool = False,\n    tag: str = \"latest\",\n    fine_tune_checkpoint: Optional[str] = None,\n    grad_clip_val: float = 5.0,\n) -> State:\n    codec = DAC.load(args[\"codec_ckpt\"], map_location=\"cpu\")\n    codec.eval()\n\n    model, v_extra = None, {}\n\n    if resume:\n        kwargs = {\n            \"folder\": f\"{save_path}/{tag}\",\n            \"map_location\": \"cpu\",\n            \"package\": False,\n        }\n        tracker.print(f\"Loading checkpoint from {kwargs['folder']}\")\n        if (Path(kwargs[\"folder\"]) / \"vampnet\").exists():\n            model, v_extra = VampNet.load_from_folder(**kwargs)\n        else:\n            raise ValueError(\n                f\"Could not find a VampNet checkpoint in {kwargs['folder']}\"\n            )\n\n\n    if args[\"fine_tune\"]:\n        assert fine_tune_checkpoint is not None, \"Must provide a fine-tune checkpoint\"\n        model = torch.compile(\n            VampNet.load(location=Path(fine_tune_checkpoint), \n                         map_location=\"cpu\", \n            )\n        )\n\n\n    model = torch.compile(VampNet()) if model is None else model\n    model = accel.prepare_model(model)\n\n    # assert accel.unwrap(model).n_codebooks == codec.quantizer.n_codebooks\n    assert (\n        accel.unwrap(model).vocab_size == codec.quantizer.quantizers[0].codebook_size\n    )\n\n    optimizer = AdamW(model.parameters(), use_zero=accel.use_ddp)\n    scheduler = NoamScheduler(optimizer, d_model=accel.unwrap(model).embedding_dim)\n    scheduler.step()\n\n    if \"optimizer.pth\" in v_extra:\n        optimizer.load_state_dict(v_extra[\"optimizer.pth\"])\n        scheduler.load_state_dict(v_extra[\"scheduler.pth\"])\n    if \"tracker.pth\" in v_extra:\n        tracker.load_state_dict(v_extra[\"tracker.pth\"])\n    \n    criterion = CrossEntropyLoss()\n\n    sample_rate = codec.sample_rate\n\n    # a better rng for sampling from our schedule\n    rng = torch.quasirandom.SobolEngine(1, scramble=True, seed=args[\"seed\"])  \n\n    # log a model summary w/ num params\n    if accel.local_rank == 0:\n        add_num_params_repr_hook(accel.unwrap(model))\n        with open(f\"{save_path}/model.txt\", \"w\") as f:\n            f.write(repr(accel.unwrap(model)))\n\n    # load the datasets\n    train_data, val_data = build_datasets(args, sample_rate)\n\n    return State(\n        tracker=tracker,\n        model=model,\n        codec=codec,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        criterion=criterion,\n        rng=rng,\n        train_data=train_data,\n        val_data=val_data,\n        grad_clip_val=grad_clip_val,\n    )", "\n\n@argbind.bind(without_prefix=True)\ndef train(\n    args,\n    accel: at.ml.Accelerator,\n    seed: int = 0,\n    codec_ckpt: str = None,\n    save_path: str = \"ckpt\",\n    num_iters: int = int(1000e6),\n    save_iters: list = [10000, 50000, 100000, 300000, 500000,],\n    sample_freq: int = 10000, \n    val_freq: int = 1000,\n    batch_size: int = 12,\n    val_idx: list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n    num_workers: int = 10,\n    fine_tune: bool = False, \n):\n    assert codec_ckpt is not None, \"codec_ckpt is required\"\n\n    seed = seed + accel.local_rank\n    at.util.seed(seed)\n    writer = None\n\n    if accel.local_rank == 0:\n        writer = SummaryWriter(log_dir=f\"{save_path}/logs/\")\n        argbind.dump_args(args, f\"{save_path}/args.yml\")\n\n        tracker = Tracker(\n            writer=writer, log_file=f\"{save_path}/log.txt\", rank=accel.local_rank\n        )\n\n    # load the codec model\n    state: State = load(\n        args=args, \n        accel=accel, \n        tracker=tracker, \n        save_path=save_path)\n    print(\"initialized state.\")\n\n    train_dataloader = accel.prepare_dataloader(\n        state.train_data,\n        start_idx=state.tracker.step * batch_size,\n        num_workers=num_workers,\n        batch_size=batch_size,\n        collate_fn=state.train_data.collate,\n    )\n    val_dataloader = accel.prepare_dataloader(\n        state.val_data,\n        start_idx=0,\n        num_workers=num_workers,\n        batch_size=batch_size,\n        collate_fn=state.val_data.collate,\n        persistent_workers=num_workers > 0,\n    )\n    print(\"initialized dataloader.\")\n\n    \n\n    if fine_tune:\n        lora.mark_only_lora_as_trainable(state.model)\n        print(\"marked only lora as trainable.\")\n\n    # Wrap the functions so that they neatly track in TensorBoard + progress bars\n    # and only run when specific conditions are met.\n    global train_loop, val_loop, validate, save_samples, checkpoint\n\n    train_loop = tracker.log(\"train\", \"value\", history=False)(\n        tracker.track(\"train\", num_iters, completed=state.tracker.step)(train_loop)\n    )\n    val_loop = tracker.track(\"val\", len(val_dataloader))(val_loop)\n    validate = tracker.log(\"val\", \"mean\")(validate)\n\n    save_samples = when(lambda: accel.local_rank == 0)(save_samples)\n    checkpoint = when(lambda: accel.local_rank == 0)(checkpoint)\n\n    print(\"starting training loop.\")\n    with tracker.live:\n        for tracker.step, batch in enumerate(train_dataloader, start=tracker.step):\n            train_loop(state, batch, accel)\n\n            last_iter = (\n                tracker.step == num_iters - 1 if num_iters is not None else False\n            )\n\n            if tracker.step % sample_freq == 0 or last_iter:\n                save_samples(state, val_idx, writer)\n\n            if tracker.step % val_freq == 0 or last_iter:\n                validate(state, val_dataloader, accel)\n                checkpoint(\n                    state=state, \n                    save_iters=save_iters, \n                    save_path=save_path, \n                    fine_tune=fine_tune)\n\n                # Reset validation progress bar, print summary since last validation.\n                tracker.done(\"val\", f\"Iteration {tracker.step}\")\n\n            if last_iter:\n                break", "\n\nif __name__ == \"__main__\":\n    args = argbind.parse_args()\n    args[\"args.debug\"] = int(os.getenv(\"LOCAL_RANK\", 0)) == 0\n    with argbind.scope(args):\n        with Accelerator() as accel:\n            if accel.local_rank != 0:\n                sys.tracebacklimit = 0\n            train(args, accel)", ""]}
{"filename": "scripts/exp/eval.py", "chunked_list": ["from pathlib import Path\nimport os\nfrom functools import partial\n\nfrom frechet_audio_distance import FrechetAudioDistance\nimport pandas\nimport argbind\nimport torch\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nimport audiotools\nfrom audiotools import AudioSignal\n\n@argbind.bind(without_prefix=True)\ndef eval(\n    exp_dir: str = None,\n    baseline_key: str = \"baseline\", \n    audio_ext: str = \".wav\",\n):\n    assert exp_dir is not None\n    exp_dir = Path(exp_dir)\n    assert exp_dir.exists(), f\"exp_dir {exp_dir} does not exist\"\n\n    # set up our metrics\n    # sisdr_loss = audiotools.metrics.distance.SISDRLoss()\n    # stft_loss = audiotools.metrics.spectral.MultiScaleSTFTLoss()\n    mel_loss = audiotools.metrics.spectral.MelSpectrogramLoss()\n    frechet = FrechetAudioDistance(\n        use_pca=False, \n        use_activation=False,\n        verbose=True, \n        audio_load_worker=4,\n    )\n    frechet.model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # figure out what conditions we have\n    conditions = [d.name for d in exp_dir.iterdir() if d.is_dir()]\n\n    assert baseline_key in conditions, f\"baseline_key {baseline_key} not found in {exp_dir}\"\n    conditions.remove(baseline_key)\n\n    print(f\"Found {len(conditions)} conditions in {exp_dir}\")\n    print(f\"conditions: {conditions}\")\n\n    baseline_dir = exp_dir / baseline_key \n    baseline_files = sorted(list(baseline_dir.glob(f\"*{audio_ext}\")), key=lambda x: int(x.stem))\n\n    metrics = []\n    for condition in tqdm(conditions):\n        cond_dir = exp_dir / condition\n        cond_files = sorted(list(cond_dir.glob(f\"*{audio_ext}\")), key=lambda x: int(x.stem))\n\n        print(f\"computing fad for {baseline_dir} and {cond_dir}\")\n        frechet_score = frechet.score(baseline_dir, cond_dir)\n\n        # make sure we have the same number of files\n        num_files = min(len(baseline_files), len(cond_files))\n        baseline_files = baseline_files[:num_files]\n        cond_files = cond_files[:num_files]\n        assert len(list(baseline_files)) == len(list(cond_files)), f\"number of files in {baseline_dir} and {cond_dir} do not match. {len(list(baseline_files))} vs {len(list(cond_files))}\"\n\n        def process(baseline_file, cond_file):\n            # make sure the files match (same name)\n            assert baseline_file.stem == cond_file.stem, f\"baseline file {baseline_file} and cond file {cond_file} do not match\"\n\n            # load the files\n            baseline_sig = AudioSignal(str(baseline_file))\n            cond_sig = AudioSignal(str(cond_file))\n\n            cond_sig.resample(baseline_sig.sample_rate)\n            cond_sig.truncate_samples(baseline_sig.length)\n\n            # if our condition is inpainting, we need to trim the conditioning off\n            if \"inpaint\" in condition:\n                ctx_amt = float(condition.split(\"_\")[-1])\n                ctx_samples = int(ctx_amt * baseline_sig.sample_rate)\n                print(f\"found inpainting condition. trimming off {ctx_samples} samples from {cond_file} and {baseline_file}\")\n                cond_sig.trim(ctx_samples, ctx_samples)\n                baseline_sig.trim(ctx_samples, ctx_samples)\n\n            return {\n                # \"sisdr\": -sisdr_loss(baseline_sig, cond_sig).item(),\n                # \"stft\": stft_loss(baseline_sig, cond_sig).item(),\n                \"mel\": mel_loss(baseline_sig, cond_sig).item(),\n                \"frechet\": frechet_score,\n                # \"visqol\": vsq,\n                \"condition\": condition,\n                \"file\": baseline_file.stem,\n            }\n\n        print(f\"processing {len(baseline_files)} files in {baseline_dir} and {cond_dir}\")\n        metrics.extend(tqdm(map(process, baseline_files, cond_files), total=len(baseline_files)))\n\n    metric_keys = [k for k in metrics[0].keys() if k not in (\"condition\", \"file\")]\n\n\n    for mk in metric_keys:\n        stat = pandas.DataFrame(metrics)\n        stat = stat.groupby(['condition'])[mk].agg(['mean', 'count', 'std'])\n        stat.to_csv(exp_dir / f\"stats-{mk}.csv\")\n\n    df = pandas.DataFrame(metrics)\n    df.to_csv(exp_dir / \"metrics-all.csv\", index=False)", "\n\nif __name__ == \"__main__\":\n    args = argbind.parse_args()\n\n    with argbind.scope(args):\n        eval()"]}
{"filename": "scripts/utils/remove_quiet_files.py", "chunked_list": ["# removes files with loudness below 24db\n\nfrom pathlib import Path \nimport shutil\nimport audiotools as at\nimport argbind\n\n@argbind.bind(without_prefix=True)\ndef remove_quiet_files(\n    src_dir: Path = None,\n    dest_dir: Path = None,\n    min_loudness: float = -30,\n):\n    # copy src to dest\n    dest_dir.mkdir(parents=True, exist_ok=True)\n    shutil.copytree(src_dir, dest_dir, dirs_exist_ok=True)\n    \n    audio_files = at.util.find_audio(dest_dir)\n    for audio_file in audio_files:\n        sig = at.AudioSignal(audio_file)\n        if sig.loudness() < min_loudness:\n            audio_file.unlink()\n            print(f\"removed {audio_file}\")", "def remove_quiet_files(\n    src_dir: Path = None,\n    dest_dir: Path = None,\n    min_loudness: float = -30,\n):\n    # copy src to dest\n    dest_dir.mkdir(parents=True, exist_ok=True)\n    shutil.copytree(src_dir, dest_dir, dirs_exist_ok=True)\n    \n    audio_files = at.util.find_audio(dest_dir)\n    for audio_file in audio_files:\n        sig = at.AudioSignal(audio_file)\n        if sig.loudness() < min_loudness:\n            audio_file.unlink()\n            print(f\"removed {audio_file}\")", "\nif __name__ == \"__main__\":\n    args = argbind.parse_args()\n\n    with argbind.scope(args):\n        remove_quiet_files()"]}
{"filename": "scripts/utils/stage.py", "chunked_list": ["import os\nimport subprocess\nfrom pathlib import Path\n\nimport argbind\nimport rich\nfrom audiotools.ml import Experiment\n\n\n@argbind.bind(without_prefix=True)\ndef run(\n    run_dir: str = os.getenv(\"PATH_TO_RUNS\", \"runs\"),\n    name: str = None,\n    recent: bool = False,\n):\n    if recent:\n        paths = sorted(Path(run_dir).iterdir(), key=os.path.getmtime)\n        paths = [p.name for p in paths if p.is_dir()]\n        if paths:\n            name = paths[-1]\n\n    with Experiment(run_dir, name) as exp:\n        exp.snapshot()\n        rich.print(f\"Created a snapshot of {exp.parent_directory} at {exp.exp_dir}\")", "\n@argbind.bind(without_prefix=True)\ndef run(\n    run_dir: str = os.getenv(\"PATH_TO_RUNS\", \"runs\"),\n    name: str = None,\n    recent: bool = False,\n):\n    if recent:\n        paths = sorted(Path(run_dir).iterdir(), key=os.path.getmtime)\n        paths = [p.name for p in paths if p.is_dir()]\n        if paths:\n            name = paths[-1]\n\n    with Experiment(run_dir, name) as exp:\n        exp.snapshot()\n        rich.print(f\"Created a snapshot of {exp.parent_directory} at {exp.exp_dir}\")", "\n\nif __name__ == \"__main__\":\n    args = argbind.parse_args()\n    with argbind.scope(args):\n        run()\n"]}
{"filename": "scripts/utils/xeno-canto-dl.py", "chunked_list": ["from xenopy import Query\n\n\nSPECIES = [\n    \"American Robin\",\n    \"Northern Cardinal\",\n    \"Mourning Dove\",\n    \"American Crow\",\n    \"Baltimore Oriole\",\n    \"Blue Jay\",", "    \"Baltimore Oriole\",\n    \"Blue Jay\",\n    \"Eastern Bluebird\",\n    \"House Finch\",\n    \"American Goldfinch\",\n    \"House Sparrow\",\n    \"Song Sparrow\",\n    \"Tufted Titmouse\",\n    \"White-breasted Nuthatch\",\n    \"European Starling\",", "    \"White-breasted Nuthatch\",\n    \"European Starling\",\n    \"American Redstart\",\n    \"Red-winged Blackbird\",\n    \"Brown-headed Cowbird\",\n    \"Common Grackle\",\n    \"Boat-tailed Grackle\",\n    \"Common Yellowthroat\",\n    \"Northern Mockingbird\",\n    \"Carolina Wren\",", "    \"Northern Mockingbird\",\n    \"Carolina Wren\",\n    \"Eastern Meadowlark\",\n    \"Chipping Sparrow\",\n    \"Tree Swallow\",\n    \"Barn Swallow\",\n    \"Cliff Swallow\",\n    \"Pine Siskin\",\n    \"Indigo Bunting\",\n    \"Eastern Towhee\",", "    \"Indigo Bunting\",\n    \"Eastern Towhee\",\n    \"Carolina Chickadee\",\n    \"Great Crested Flycatcher\",\n    \"Eastern Wood-Pewee\",\n    \"Ovenbird\",\n    \"Northern Flicker\",\n    \"Red-eyed Vireo\",\n    \"American Woodcock\",\n    \"Eastern Phoebe\",", "    \"American Woodcock\",\n    \"Eastern Phoebe\",\n    \"Downy Woodpecker\",\n    \"Scarlet Tanager\",\n    \"Yellow Warbler\",\n    \"White-eyed Vireo\",\n    \"Common Loon\",\n    \"White-throated Sparrow\",\n    \"Yellow-throated Vireo\",\n    \"Great Blue Heron\",", "    \"Yellow-throated Vireo\",\n    \"Great Blue Heron\",\n    \"Belted Kingfisher\",\n    \"Pied-billed Grebe\",\n    \"Wild Turkey\",\n    \"Wood Thrush\",\n    \"Rose-breasted Grosbeak\",\n    \"Field Sparrow\",\n    \"Hooded Warbler\",\n    \"Northern Parula\",", "    \"Hooded Warbler\",\n    \"Northern Parula\",\n    \"Chestnut-sided Warbler\",\n    \"Blue-winged Warbler\",\n    \"Red-bellied Woodpecker\",\n    \"Yellow-billed Cuckoo\",\n    \"Gray Catbird\",\n    \"Northern Saw-whet Owl\",\n    \"Osprey\",\n    \"Common Nighthawk\",", "    \"Osprey\",\n    \"Common Nighthawk\",\n    \"Broad-winged Hawk\",\n    \"Black-throated Green Warbler\",\n    \"Great Horned Owl\",\n    \"Common Raven\",\n    \"Barred Owl\",\n    \"Canada Warbler\",\n    \"Magnolia Warbler\",\n    \"Black-and-white Warbler\",", "    \"Magnolia Warbler\",\n    \"Black-and-white Warbler\",\n    \"Eastern Kingbird\",\n    \"Swainson's Thrush\",\n    \"Worm-eating Warbler\",\n    \"Prairie Warbler\",\n    \"Baltimore Oriole\",\n    \"Black-throated Blue Warbler\",\n    \"Louisiana Waterthrush\",\n    \"Blackburnian Warbler\",", "    \"Louisiana Waterthrush\",\n    \"Blackburnian Warbler\",\n    \"Black-capped Chickadee\",\n    \"Cerulean Warbler\",\n    \"Red-shouldered Hawk\",\n    \"Cooper's Hawk\",\n    \"Yellow-throated Warbler\",\n    \"Blue-headed Vireo\",\n    \"Blackpoll Warbler\",\n    \"Ruffed Grouse\",", "    \"Blackpoll Warbler\",\n    \"Ruffed Grouse\",\n    \"Kentucky Warbler\",\n    \"Hermit Thrush\",\n    \"Cedar Waxwing\",\n    \"Eastern Screech-Owl\",\n    \"Northern Goshawk\",\n    \"Green Heron\",\n    \"Red-tailed Hawk\",\n    \"Black Vulture\",", "    \"Red-tailed Hawk\",\n    \"Black Vulture\",\n    \"Hairy Woodpecker\",\n    \"Golden-crowned Kinglet\",\n    \"Ruby-crowned Kinglet\",\n    \"Bicknell's Thrush\",\n    \"Blue-gray Gnatcatcher\",\n    \"Veery\",\n    \"Pileated Woodpecker\",\n    \"Purple Finch\",", "    \"Pileated Woodpecker\",\n    \"Purple Finch\",\n    \"White-crowned Sparrow\",\n    \"Snow Bunting\",\n    \"Pine Grosbeak\",\n    \"American Tree Sparrow\",\n    \"Dark-eyed Junco\",\n    \"Snowy Owl\",\n    \"White-winged Crossbill\",\n    \"Red Crossbill\",", "    \"White-winged Crossbill\",\n    \"Red Crossbill\",\n    \"Common Redpoll\",\n    \"Northern Shrike\",\n    \"Northern Harrier\",\n    \"Rough-legged Hawk\",\n    \"Long-eared Owl\",\n    \"Evening Grosbeak\",\n    \"Northern Pintail\",\n    \"American Black Duck\",", "    \"Northern Pintail\",\n    \"American Black Duck\",\n    \"Mallard\",\n    \"Canvasback\",\n    \"Redhead\",\n    \"Ring-necked Duck\",\n    \"Greater Scaup\",\n    \"Lesser Scaup\",\n    \"Bufflehead\",\n    \"Common Goldeneye\",", "    \"Bufflehead\",\n    \"Common Goldeneye\",\n    \"Hooded Merganser\",\n    \"Common Merganser\",\n    \"Red-breasted Merganser\",\n    \"Ruddy Duck\",\n    \"Wood Duck\",\n    \"Gadwall\",\n    \"American Wigeon\",\n    \"Northern Shoveler\",", "    \"American Wigeon\",\n    \"Northern Shoveler\",\n    \"Green-winged Teal\",\n    \"Blue-winged Teal\",\n    \"Cinnamon Teal\",\n    \"Ringed Teal\",\n    \"Cape Teal\",\n    \"Northern Fulmar\",\n    \"Yellow-billed Loon\",\n    \"Red-throated Loon\",", "    \"Yellow-billed Loon\",\n    \"Red-throated Loon\",\n    \"Arctic Loon\",\n    \"Pacific Loon\",\n    \"Horned Grebe\",\n    \"Red-necked Grebe\",\n    \"Eared Grebe\",\n    \"Western Grebe\",\n    \"Clark's Grebe\",\n    \"Double-crested Cormorant\",", "    \"Clark's Grebe\",\n    \"Double-crested Cormorant\",\n    \"Pelagic Cormorant\",\n    \"Great Cormorant\",\n    \"American White Pelican\",\n    \"Brown Pelican\",\n    \"Brandt's Cormorant\",\n    \"Least Bittern\",\n    \"Great Egret\",\n    \"Snowy Egret\",", "    \"Great Egret\",\n    \"Snowy Egret\",\n    \"Little Blue Heron\",\n    \"Tricolored Heron\",\n    \"Reddish Egret\",\n    \"Black-crowned Night-Heron\",\n    \"Yellow-crowned Night-Heron\",\n    \"White Ibis\",\n    \"Glossy Ibis\",\n    \"Roseate Spoonbill\",", "    \"Glossy Ibis\",\n    \"Roseate Spoonbill\",\n    \"Wood Stork\",\n    \"Black-bellied Whistling-Duck\",\n    \"Fulvous Whistling-Duck\",\n    \"Greater White-fronted Goose\",\n    \"Snow Goose\",\n    \"Ross's Goose\",\n    \"Canada Goose\",\n    \"Brant\",", "    \"Canada Goose\",\n    \"Brant\",\n    \"Mute Swan\",\n    \"Tundra Swan\",\n    \"Whooper Swan\",\n    \"Sandhill Crane\",\n    \"Black-necked Stilt\",\n    \"American Avocet\",\n    \"Northern Jacana\",\n    \"Greater Yellowlegs\",", "    \"Northern Jacana\",\n    \"Greater Yellowlegs\",\n    \"Lesser Yellowlegs\",\n    \"Willet\",\n    \"Spotted Sandpiper\",\n    \"Upland Sandpiper\",\n    \"Whimbrel\",\n    \"Long-billed Curlew\",\n    \"Marbled Godwit\",\n    \"Ruddy Turnstone\",", "    \"Marbled Godwit\",\n    \"Ruddy Turnstone\",\n    \"Red Knot\",\n    \"Sanderling\",\n    \"Semipalmated Sandpiper\",\n    \"Western Sandpiper\",\n    \"Least Sandpiper\",\n    \"White-rumped Sandpiper\",\n    \"Baird's Sandpiper\",\n    \"Pectoral Sandpiper\",", "    \"Baird's Sandpiper\",\n    \"Pectoral Sandpiper\",\n    \"Dunlin\",\n    \"Buff-breasted Sandpiper\",\n    \"Short-billed Dowitcher\",\n    \"Long-billed Dowitcher\",\n    \"Common Snipe\",\n    \"American Woodcock\",\n    \"Wilson's Phalarope\",\n    \"Red-necked Phalarope\",", "    \"Wilson's Phalarope\",\n    \"Red-necked Phalarope\",\n    \"Red Phalarope\"\n]\n\nfrom pathlib import Path\n\ndef remove_spaces(s):\n    return s.replace(\" \", \"\")\n\nfor species in SPECIES: \n    if Path(\"/media/CHONK/hugo/xeno-canto-full/\" + remove_spaces(species)).exists():\n        continue\n    try:\n        q = Query(\n            name=species, q=\"A\", length=\"10-30\", \n            )\n\n        # retrieve metadata\n        metafiles = q.retrieve_meta(verbose=True)\n        # retrieve recordings\n        q.retrieve_recordings(multiprocess=True, nproc=10, attempts=10, outdir=\"/media/CHONK/hugo/xeno-canto-full/\")\n\n    except:\n        print(\"Failed to download \" + species)\n        continue", "\nfor species in SPECIES: \n    if Path(\"/media/CHONK/hugo/xeno-canto-full/\" + remove_spaces(species)).exists():\n        continue\n    try:\n        q = Query(\n            name=species, q=\"A\", length=\"10-30\", \n            )\n\n        # retrieve metadata\n        metafiles = q.retrieve_meta(verbose=True)\n        # retrieve recordings\n        q.retrieve_recordings(multiprocess=True, nproc=10, attempts=10, outdir=\"/media/CHONK/hugo/xeno-canto-full/\")\n\n    except:\n        print(\"Failed to download \" + species)\n        continue"]}
{"filename": "scripts/utils/split.py", "chunked_list": ["from pathlib import Path\nimport random\nimport shutil\nimport os\nimport json \n\nimport argbind\nfrom tqdm import tqdm\nfrom tqdm.contrib.concurrent import thread_map\n", "from tqdm.contrib.concurrent import thread_map\n\nfrom audiotools.core import util\n\n\n@argbind.bind(without_prefix=True)\ndef train_test_split(\n    audio_folder: str = \".\", \n    test_size: float = 0.2,\n    seed: int = 42,\n    pattern: str = \"**/*.mp3\",\n):\n    print(f\"finding audio\")\n\n    audio_folder = Path(audio_folder)\n    audio_files = list(tqdm(audio_folder.glob(pattern)))\n    print(f\"found {len(audio_files)} audio files\")\n    \n    # split according to test_size\n    n_test = int(len(audio_files) * test_size)\n    n_train = len(audio_files) - n_test\n\n    # shuffle\n    random.seed(seed)\n    random.shuffle(audio_files)\n\n    train_files = audio_files[:n_train]\n    test_files = audio_files[n_train:]\n\n\n    print(f\"Train files: {len(train_files)}\")\n    print(f\"Test files: {len(test_files)}\")\n    continue_ = input(\"Continue [yn]? \") or \"n\"\n\n    if continue_ != \"y\":\n        return\n    \n    for split, files in (\n        (\"train\", train_files), (\"test\", test_files)\n    ):\n        for file in tqdm(files):\n            out_file = audio_folder.parent / f\"{audio_folder.name}-{split}\" / Path(file).name\n            out_file.parent.mkdir(exist_ok=True, parents=True)\n            os.symlink(file, out_file)\n\n        # save split as json\n        with open(Path(audio_folder) / f\"{split}.json\", \"w\") as f:\n            json.dump([str(f) for f in files], f)", "    \n\n    \nif __name__ == \"__main__\":\n    args  = argbind.parse_args()\n\n    with argbind.scope(args):\n        train_test_split()"]}
{"filename": "scripts/utils/plots.py", "chunked_list": ["import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.api.types import CategoricalDtype\n\ndef plot_metrics(metrics, condition_to_latex, title, color_palette):\n    # Add a new column to your dataframe with the latex representation\n    metrics['condition_latex'] = metrics['condition'].map(condition_to_latex)\n\n    # Order condition_latex as per the condition_to_latex dictionary\n    cat_type = CategoricalDtype(categories=condition_to_latex.values(), ordered=True)\n    metrics['condition_latex'] = metrics['condition_latex'].astype(cat_type)\n\n    # Compute mean and std for each condition for each metric\n    grouped = metrics.groupby('condition_latex')[['mel', 'frechet']].agg(['mean', 'std'])\n\n    fig, axs = plt.subplots(2, 1, figsize=(7, 5.25))\n\n    # Set the main title for the figure\n    fig.suptitle(title, fontsize=16)\n\n    # Get color for each bar in the plot\n    bar_colors = [color_palette[condition] for condition in grouped.index]\n\n    # Plot mel\n    sns.boxplot(x='condition_latex', y='mel', data=metrics, ax=axs[0], palette=color_palette, showfliers=False)\n    axs[0].set_ylabel('Mel Spectrogram Loss \\u2190')\n    axs[0].set_xlabel('') # Remove x-axis label\n    axs[0].set_xticklabels(grouped.index, rotation=0, ha='center')\n\n    # Plot frechet\n    axs[1].bar(grouped.index, grouped['frechet']['mean'], yerr=grouped['frechet']['std'], color=bar_colors)\n    axs[1].set_ylabel('FAD \\u2190')\n    axs[1].set_xlabel('') # Remove x-axis label\n    axs[1].set_xticklabels(grouped.index, rotation=0, ha='center')\n\n    # Adjust the space between plots\n    plt.subplots_adjust(hspace=0.1)\n\n    # Remove any unnecessary space around the plot\n    plt.tight_layout(rect=[0, 0, 1, 0.96])\n\n    # Reduce the space between suptitle and the plot\n    plt.subplots_adjust(top=0.92)"]}
{"filename": "scripts/utils/augment.py", "chunked_list": ["from pathlib import Path\n\nimport audiotools as at\nfrom audiotools import AudioSignal\n\nimport argbind\nimport tqdm\nimport torch\n\n", "\n\nfrom torch_pitch_shift import pitch_shift, get_fast_shifts\nfrom torch_time_stretch import time_stretch, get_fast_stretches\n\nfrom audiotools.core.util import sample_from_dist\n\n\n@argbind.bind(without_prefix=True)\ndef augment(\n    audio_folder: Path = None,\n    dest_folder: Path = None,\n    n_augmentations: int = 10,\n):\n    \"\"\" \n        Augment a folder of audio files by applying audiotools and pedalboard transforms. \n\n        The dest foler will contain a folder for each of the clean dataset's files. \n        Under each of these folders, there will be a clean file and many augmented files.\n    \"\"\"\n    assert audio_folder is not None\n    assert dest_folder is not None\n    audio_files = at.util.find_audio(audio_folder)\n\n    for audio_file in tqdm.tqdm(audio_files):\n        subtree = dest_folder / audio_file.relative_to(audio_folder).parent\n        subdir = subtree / audio_file.stem\n        subdir.mkdir(parents=True, exist_ok=True)\n\n        src = AudioSignal(audio_file).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        \n        for i, chunk in tqdm.tqdm(enumerate(src.windows(10, 10))):\n            # apply pedalboard transforms\n            for j in range(n_augmentations):\n                # pitch shift between -7 and 7 semitones\n                import random\n                dst = chunk.clone()\n                dst.samples = pitch_shift(\n                    dst.samples, \n                    shift=random.choice(get_fast_shifts(src.sample_rate, \n                            condition=lambda x: x >= 0.25 and x <= 1.0)), \n                    sample_rate=src.sample_rate\n                )\n                dst.samples = time_stretch(\n                    dst.samples,\n                    stretch=random.choice(get_fast_stretches(src.sample_rate, \n                                          condition=lambda x: x >= 0.667 and x <= 1.5, )),\n                    sample_rate=src.sample_rate, \n                )\n\n                dst.cpu().write(subdir / f\"{i}-{j}.wav\")", "@argbind.bind(without_prefix=True)\ndef augment(\n    audio_folder: Path = None,\n    dest_folder: Path = None,\n    n_augmentations: int = 10,\n):\n    \"\"\" \n        Augment a folder of audio files by applying audiotools and pedalboard transforms. \n\n        The dest foler will contain a folder for each of the clean dataset's files. \n        Under each of these folders, there will be a clean file and many augmented files.\n    \"\"\"\n    assert audio_folder is not None\n    assert dest_folder is not None\n    audio_files = at.util.find_audio(audio_folder)\n\n    for audio_file in tqdm.tqdm(audio_files):\n        subtree = dest_folder / audio_file.relative_to(audio_folder).parent\n        subdir = subtree / audio_file.stem\n        subdir.mkdir(parents=True, exist_ok=True)\n\n        src = AudioSignal(audio_file).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        \n        for i, chunk in tqdm.tqdm(enumerate(src.windows(10, 10))):\n            # apply pedalboard transforms\n            for j in range(n_augmentations):\n                # pitch shift between -7 and 7 semitones\n                import random\n                dst = chunk.clone()\n                dst.samples = pitch_shift(\n                    dst.samples, \n                    shift=random.choice(get_fast_shifts(src.sample_rate, \n                            condition=lambda x: x >= 0.25 and x <= 1.0)), \n                    sample_rate=src.sample_rate\n                )\n                dst.samples = time_stretch(\n                    dst.samples,\n                    stretch=random.choice(get_fast_stretches(src.sample_rate, \n                                          condition=lambda x: x >= 0.667 and x <= 1.5, )),\n                    sample_rate=src.sample_rate, \n                )\n\n                dst.cpu().write(subdir / f\"{i}-{j}.wav\")", "\n\nif __name__ == \"__main__\":\n    args = argbind.parse_args()\n\n    with argbind.scope(args):\n        augment()"]}
{"filename": "scripts/utils/split_long_audio_file.py", "chunked_list": ["from pathlib import Path\nimport argbind\n\nimport audiotools as at\nimport tqdm\n\n\n@argbind.bind(without_prefix=True)\ndef split_long_audio_file(\n    file: str = None, \n    max_chunk_size_s: int = 60*10\n):\n    file = Path(file)\n    output_dir = file.parent / file.stem\n    output_dir.mkdir()\n    \n    sig = at.AudioSignal(file)\n\n    # split into chunks\n    for i, sig in tqdm.tqdm(enumerate(sig.windows(\n        window_duration=max_chunk_size_s, hop_duration=max_chunk_size_s/2, \n        preprocess=True))\n    ):\n        sig.write(output_dir / f\"{i}.wav\")\n\n    print(f\"wrote {len(list(output_dir.glob('*.wav')))} files to {output_dir}\")\n    \n    return output_dir", "def split_long_audio_file(\n    file: str = None, \n    max_chunk_size_s: int = 60*10\n):\n    file = Path(file)\n    output_dir = file.parent / file.stem\n    output_dir.mkdir()\n    \n    sig = at.AudioSignal(file)\n\n    # split into chunks\n    for i, sig in tqdm.tqdm(enumerate(sig.windows(\n        window_duration=max_chunk_size_s, hop_duration=max_chunk_size_s/2, \n        preprocess=True))\n    ):\n        sig.write(output_dir / f\"{i}.wav\")\n\n    print(f\"wrote {len(list(output_dir.glob('*.wav')))} files to {output_dir}\")\n    \n    return output_dir", "\nif __name__ == \"__main__\":\n    args = argbind.parse_args()\n\n    with argbind.scope(args):\n        split_long_audio_file()"]}
{"filename": "scripts/utils/maestro-reorg.py", "chunked_list": ["from pathlib import Path\nimport json\nimport os\n\nmaestro_path = Path(\"/media/CHONK/hugo/maestro-v3.0.0\")\noutput_path = Path(\"/media/CHONK/hugo/maestro-v3.0.0-split\")\n\n# split\nwith open(maestro_path / \"maestro-v3.0.0.json\") as f:\n    maestro = json.load(f)", "with open(maestro_path / \"maestro-v3.0.0.json\") as f:\n    maestro = json.load(f)\n\nbreakpoint()\ntrain = []\nvalidation = []\ntest = []\nfor key, split in maestro[\"split\"].items():\n    audio_filename = maestro['audio_filename'][key]\n    if split == \"train\":\n        train.append(audio_filename)\n    elif split == \"test\":\n        test.append(audio_filename)\n    elif split == \"validation\":\n        validation.append(audio_filename)\n    else:\n        raise ValueError(f\"Unknown split {split}\")", "\n# symlink all files\nfor audio_filename in train:\n    p = output_path / \"train\" / audio_filename\n    p.parent.mkdir(parents=True, exist_ok=True)\n    os.symlink(maestro_path / audio_filename, p)\nfor audio_filename in validation:\n    p = output_path / \"validation\" / audio_filename\n    p.parent.mkdir(parents=True, exist_ok=True)\n    os.symlink(maestro_path / audio_filename, p)\nfor audio_filename in test:\n    p = output_path / \"test\" / audio_filename\n    p.parent.mkdir(parents=True, exist_ok=True)\n    os.symlink(maestro_path / audio_filename, p)", "for audio_filename in test:\n    p = output_path / \"test\" / audio_filename\n    p.parent.mkdir(parents=True, exist_ok=True)\n    os.symlink(maestro_path / audio_filename, p)"]}
{"filename": "vampnet/scheduler.py", "chunked_list": ["import copy\nfrom typing import List\n\nimport torch\n\nclass NoamScheduler:\n    \"\"\"OG scheduler from transformer paper: https://arxiv.org/pdf/1706.03762.pdf\n    Implementation from Annotated Transformer: https://nlp.seas.harvard.edu/2018/04/03/attention.html\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        d_model: int = 512,\n        factor: float = 1.0,\n        warmup: int = 4000,\n    ):\n        # Store hparams\n        self.warmup = warmup\n        self.factor = factor\n        self.d_model = d_model\n\n        # Initialize variables `lr` and `steps`\n        self.lr = None\n        self.steps = 0\n\n        # Store the optimizer\n        self.optimizer = optimizer\n\n    def state_dict(self):\n        return {\n            key: value for key, value in self.__dict__.items() if key != \"optimizer\"\n        }\n\n    def load_state_dict(self, state_dict):\n        self.__dict__.update(state_dict)\n\n    def step(self):\n        self.steps += 1\n        self.lr = self.factor * (\n            self.d_model ** (-0.5)\n            * min(self.steps ** (-0.5), self.steps * self.warmup ** (-1.5))\n        )\n\n        for p in self.optimizer.param_groups:\n            p[\"lr\"] = self.lr", "\n"]}
{"filename": "vampnet/__init__.py", "chunked_list": ["\nfrom . import modules\nfrom . import scheduler\nfrom .interface import Interface\n\n__version__ = \"0.0.1\"\n"]}
{"filename": "vampnet/mask.py", "chunked_list": ["from typing import Optional\n\nimport torch\nfrom audiotools import AudioSignal\n\nfrom .util import scalar_to_batch_tensor\n\ndef _gamma(r):\n    return (r * torch.pi / 2).cos().clamp(1e-10, 1.0)\n\ndef _invgamma(y):\n    if not torch.is_tensor(y):\n        y = torch.tensor(y)[None]\n    return 2 * y.acos() / torch.pi", "\ndef _invgamma(y):\n    if not torch.is_tensor(y):\n        y = torch.tensor(y)[None]\n    return 2 * y.acos() / torch.pi\n\ndef full_mask(x: torch.Tensor):\n    assert x.ndim == 3, \"x must be (batch, n_codebooks, seq)\"\n    return torch.ones_like(x).long()\n\ndef empty_mask(x: torch.Tensor):\n    assert x.ndim == 3, \"x must be (batch, n_codebooks, seq)\"\n    return torch.zeros_like(x).long()", "\ndef empty_mask(x: torch.Tensor):\n    assert x.ndim == 3, \"x must be (batch, n_codebooks, seq)\"\n    return torch.zeros_like(x).long()\n\ndef apply_mask(\n        x: torch.Tensor, \n        mask: torch.Tensor, \n        mask_token: int\n    ):\n    assert mask.ndim == 3, \"mask must be (batch, n_codebooks, seq), but got {mask.ndim}\"\n    assert mask.shape == x.shape, f\"mask must be same shape as x, but got {mask.shape} and {x.shape}\" \n    assert mask.dtype == torch.long, \"mask must be long dtype, but got {mask.dtype}\"\n    assert ~torch.any(mask > 1), \"mask must be binary\"\n    assert ~torch.any(mask < 0), \"mask must be binary\"\n\n    fill_x = torch.full_like(x, mask_token)\n    x = x * (1 - mask) + fill_x * mask\n\n    return x, mask", "\ndef random(\n    x: torch.Tensor,\n    r: torch.Tensor\n):\n    assert x.ndim == 3, \"x must be (batch, n_codebooks, seq)\"\n    if not isinstance(r, torch.Tensor):\n        r = scalar_to_batch_tensor(r, x.shape[0]).to(x.device)\n\n    r = _gamma(r)[:, None, None]\n    probs = torch.ones_like(x) * r\n\n    mask = torch.bernoulli(probs)\n    mask = mask.round().long()\n\n    return mask", "\ndef linear_random(\n    x: torch.Tensor,\n    r: torch.Tensor,\n):\n    assert x.ndim == 3, \"x must be (batch, n_codebooks, seq)\"\n    if not isinstance(r, torch.Tensor):\n        r = scalar_to_batch_tensor(r, x.shape[0]).to(x.device).float()\n\n    probs = torch.ones_like(x).to(x.device).float()\n    # expand to batch and codebook dims\n    probs = probs.expand(x.shape[0], x.shape[1], -1)\n    probs = probs * r\n\n    mask = torch.bernoulli(probs)\n    mask = mask.round().long()\n\n    return mask", "\ndef inpaint(x: torch.Tensor, \n    n_prefix,\n    n_suffix,\n):\n    assert n_prefix is not None\n    assert n_suffix is not None\n    \n    mask = full_mask(x)\n\n    # if we have a prefix or suffix, set their mask prob to 0\n    if n_prefix > 0:\n        if not isinstance(n_prefix, torch.Tensor):\n            n_prefix = scalar_to_batch_tensor(n_prefix, x.shape[0]).to(x.device) \n        for i, n in enumerate(n_prefix):\n            if n > 0:\n                mask[i, :, :n] = 0.0\n    if n_suffix > 0:\n        if not isinstance(n_suffix, torch.Tensor):\n            n_suffix = scalar_to_batch_tensor(n_suffix, x.shape[0]).to(x.device)\n        for i, n in enumerate(n_suffix):\n            if n > 0:\n                mask[i, :, -n:] = 0.0\n\n    \n    return mask", "\ndef periodic_mask(x: torch.Tensor, \n                period: int, width: int = 1, \n                random_roll=False,\n    ):\n    mask = full_mask(x)\n    if period == 0:\n        return mask\n\n    if not isinstance(period, torch.Tensor):\n        period = scalar_to_batch_tensor(period, x.shape[0])\n    for i, factor in enumerate(period):\n        if factor == 0:\n            continue\n        for j in range(mask.shape[-1]):\n            if j % factor == 0:\n                # figure out how wide the mask should be\n                j_start = max(0, j - width // 2  )\n                j_end = min(mask.shape[-1] - 1, j + width // 2 ) + 1 \n                # flip a coin for each position in the mask\n                j_mask = torch.bernoulli(torch.ones(j_end - j_start))\n                assert torch.all(j_mask == 1)\n                j_fill = torch.ones_like(j_mask) * (1 - j_mask)\n                assert torch.all(j_fill == 0)\n                # fill\n                mask[i, :, j_start:j_end] = j_fill\n    if random_roll:\n        # add a random offset to the mask\n        offset = torch.randint(0, period[0], (1,))\n        mask = torch.roll(mask, offset.item(), dims=-1)\n\n    return mask", "\ndef codebook_unmask(\n    mask: torch.Tensor, \n    n_conditioning_codebooks: int\n):\n    if n_conditioning_codebooks == None:\n        return mask\n    # if we have any conditioning codebooks, set their mask  to 0\n    mask = mask.clone()\n    mask[:, :n_conditioning_codebooks, :] = 0\n    return mask", "\ndef mask_and(\n    mask1: torch.Tensor, \n    mask2: torch.Tensor\n):\n    assert mask1.shape == mask2.shape, \"masks must be same shape\"\n    return torch.min(mask1, mask2)\n\ndef dropout(\n    mask: torch.Tensor,\n    p: float,\n):\n    assert 0 <= p <= 1, \"p must be between 0 and 1\"\n    assert mask.max() <= 1, \"mask must be binary\"\n    assert mask.min() >= 0, \"mask must be binary\"\n    mask = (~mask.bool()).float()\n    mask = torch.bernoulli(mask * (1 - p))\n    mask = ~mask.round().bool()\n    return mask.long()", "def dropout(\n    mask: torch.Tensor,\n    p: float,\n):\n    assert 0 <= p <= 1, \"p must be between 0 and 1\"\n    assert mask.max() <= 1, \"mask must be binary\"\n    assert mask.min() >= 0, \"mask must be binary\"\n    mask = (~mask.bool()).float()\n    mask = torch.bernoulli(mask * (1 - p))\n    mask = ~mask.round().bool()\n    return mask.long()", "\ndef mask_or(\n    mask1: torch.Tensor, \n    mask2: torch.Tensor\n):\n    assert mask1.shape == mask2.shape, f\"masks must be same shape, but got {mask1.shape} and {mask2.shape}\"\n    assert mask1.max() <= 1, \"mask1 must be binary\"\n    assert mask2.max() <= 1, \"mask2 must be binary\"\n    assert mask1.min() >= 0, \"mask1 must be binary\"\n    assert mask2.min() >= 0, \"mask2 must be binary\"\n    return (mask1 + mask2).clamp(0, 1)", "\ndef time_stretch_mask(\n    x: torch.Tensor, \n    stretch_factor: int,\n):\n    assert stretch_factor >= 1, \"stretch factor must be >= 1\"\n    c_seq_len = x.shape[-1]\n    x = x.repeat_interleave(stretch_factor, dim=-1)\n\n    # trim cz to the original length\n    x = x[:, :, :c_seq_len]\n\n    mask = periodic_mask(x, stretch_factor, width=1)\n    return mask", "\ndef onset_mask(\n    sig: AudioSignal, \n    z: torch.Tensor,\n    interface,\n    width: int = 1\n):\n    import librosa\n    import madmom\n    from madmom.features.onsets import RNNOnsetProcessor, OnsetPeakPickingProcessor\n    import tempfile\n    import numpy as np \n\n    with tempfile.NamedTemporaryFile(suffix='.wav') as f:\n        sig = sig.clone()\n        sig.write(f.name)\n\n        proc = RNNOnsetProcessor(online=False)\n        onsetproc = OnsetPeakPickingProcessor(threshold=0.3,\n                                              fps=sig.sample_rate/interface.codec.hop_length)\n        \n        act = proc(f.name)\n        onset_times = onsetproc(act)\n\n        # convert to indices for z array\n        onset_indices = librosa.time_to_frames(onset_times, sr=sig.sample_rate, hop_length=interface.codec.hop_length)\n\n        if onset_indices.shape[0] == 0:\n            mask = empty_mask(z)   \n            print(f\"no onsets found, returning empty mask\")\n        else: \n            torch.set_printoptions(threshold=1000)\n            print(\"onset indices: \", onset_indices)\n            print(\"onset times: \", onset_times)\n\n            # create a mask, set onset \n            mask = torch.ones_like(z)\n            n_timesteps = z.shape[-1]\n\n            for onset_index in onset_indices:\n                onset_index = min(onset_index, n_timesteps - 1)\n                onset_index = max(onset_index, 0)\n                mask[:, :, onset_index - width:onset_index + width] = 0.0\n\n            print(mask)\n    \n    return mask", "\n\n\nif __name__ == \"__main__\":\n    pass\n"]}
{"filename": "vampnet/beats.py", "chunked_list": ["import json\nimport logging\nimport warnings\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Union\n", "from typing import Union\n\nimport librosa\nimport torch\nimport numpy as np\nfrom audiotools import AudioSignal\n\n\nlogging.basicConfig(level=logging.INFO)\n", "logging.basicConfig(level=logging.INFO)\n\n###################\n# beat sync utils #\n###################\n\nAGGREGATOR_REGISTRY = {\n    \"mean\": np.mean,\n    \"median\": np.median,\n    \"max\": np.max,", "    \"median\": np.median,\n    \"max\": np.max,\n    \"min\": np.min,\n}\n\n\ndef list_aggregators() -> list:\n    return list(AGGREGATOR_REGISTRY.keys())\n\n", "\n\n@dataclass\nclass TimeSegment:\n    start: float\n    end: float\n\n    @property\n    def duration(self):\n        return self.end - self.start\n\n    def __str__(self) -> str:\n        return f\"{self.start} - {self.end}\"\n\n    def find_overlapping_segment(\n        self, segments: List[\"TimeSegment\"]\n    ) -> Union[\"TimeSegment\", None]:\n        \"\"\"Find the first segment that overlaps with this segment, or None if no segment overlaps\"\"\"\n        for s in segments:\n            if s.start <= self.start and s.end >= self.end:\n                return s\n        return None", "\n\ndef mkdir(path: Union[Path, str]) -> Path:\n    p = Path(path)\n    p.mkdir(parents=True, exist_ok=True)\n    return p\n\n\n\n###################", "\n###################\n#    beat data    #\n###################\n@dataclass\nclass BeatSegment(TimeSegment):\n    downbeat: bool = False  # if there's a downbeat on the start_time\n\n\nclass Beats:\n    def __init__(self, beat_times, downbeat_times):\n        if isinstance(beat_times, np.ndarray):\n            beat_times = beat_times.tolist()\n        if isinstance(downbeat_times, np.ndarray):\n            downbeat_times = downbeat_times.tolist()\n        self._beat_times = beat_times\n        self._downbeat_times = downbeat_times\n        self._use_downbeats = False\n\n    def use_downbeats(self, use_downbeats: bool = True):\n        \"\"\"use downbeats instead of beats when calling beat_times\"\"\"\n        self._use_downbeats = use_downbeats\n\n    def beat_segments(self, signal: AudioSignal) -> List[BeatSegment]:\n        \"\"\"\n        segments a song into time segments corresponding to beats.\n        the first segment starts at 0 and ends at the first beat time.\n        the last segment starts at the last beat time and ends at the end of the song.\n        \"\"\"\n        beat_times = self._beat_times.copy()\n        downbeat_times = self._downbeat_times\n        beat_times.insert(0, 0)\n        beat_times.append(signal.signal_duration)\n\n        downbeat_ids = np.intersect1d(beat_times, downbeat_times, return_indices=True)[\n            1\n        ]\n        is_downbeat = [\n            True if i in downbeat_ids else False for i in range(len(beat_times))\n        ]\n        segments = [\n            BeatSegment(start_time, end_time, downbeat)\n            for start_time, end_time, downbeat in zip(\n                beat_times[:-1], beat_times[1:], is_downbeat\n            )\n        ]\n        return segments\n\n    def get_beats(self) -> np.ndarray:\n        \"\"\"returns an array of beat times, in seconds\n        if downbeats is True, returns an array of downbeat times, in seconds\n        \"\"\"\n        return np.array(\n            self._downbeat_times if self._use_downbeats else self._beat_times\n        )\n\n    @property\n    def beat_times(self) -> np.ndarray:\n        \"\"\"return beat times\"\"\"\n        return np.array(self._beat_times)\n\n    @property\n    def downbeat_times(self) -> np.ndarray:\n        \"\"\"return downbeat times\"\"\"\n        return np.array(self._downbeat_times)\n\n    def beat_times_to_feature_frames(\n        self, signal: AudioSignal, features: np.ndarray\n    ) -> np.ndarray:\n        \"\"\"convert beat times to frames, given an array of time-varying features\"\"\"\n        beat_times = self.get_beats()\n        beat_frames = (\n            beat_times * signal.sample_rate / signal.signal_length * features.shape[-1]\n        ).astype(np.int64)\n        return beat_frames\n\n    def sync_features(\n        self, feature_frames: np.ndarray, features: np.ndarray, aggregate=\"median\"\n    ) -> np.ndarray:\n        \"\"\"sync features to beats\"\"\"\n        if aggregate not in AGGREGATOR_REGISTRY:\n            raise ValueError(f\"unknown aggregation method {aggregate}\")\n\n        return librosa.util.sync(\n            features, feature_frames, aggregate=AGGREGATOR_REGISTRY[aggregate]\n        )\n\n    def to_json(self) -> dict:\n        \"\"\"return beats and downbeats as json\"\"\"\n        return {\n            \"beats\": self._beat_times,\n            \"downbeats\": self._downbeat_times,\n            \"use_downbeats\": self._use_downbeats,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict):\n        \"\"\"load beats and downbeats from json\"\"\"\n        inst = cls(data[\"beats\"], data[\"downbeats\"])\n        inst.use_downbeats(data[\"use_downbeats\"])\n        return inst\n\n    def save(self, output_dir: Path):\n        \"\"\"save beats and downbeats to json\"\"\"\n        mkdir(output_dir)\n        with open(output_dir / \"beats.json\", \"w\") as f:\n            json.dump(self.to_json(), f)\n\n    @classmethod\n    def load(cls, input_dir: Path):\n        \"\"\"load beats and downbeats from json\"\"\"\n        beats_file = Path(input_dir) / \"beats.json\"\n        with open(beats_file, \"r\") as f:\n            data = json.load(f)\n        return cls.from_dict(data)", "\nclass Beats:\n    def __init__(self, beat_times, downbeat_times):\n        if isinstance(beat_times, np.ndarray):\n            beat_times = beat_times.tolist()\n        if isinstance(downbeat_times, np.ndarray):\n            downbeat_times = downbeat_times.tolist()\n        self._beat_times = beat_times\n        self._downbeat_times = downbeat_times\n        self._use_downbeats = False\n\n    def use_downbeats(self, use_downbeats: bool = True):\n        \"\"\"use downbeats instead of beats when calling beat_times\"\"\"\n        self._use_downbeats = use_downbeats\n\n    def beat_segments(self, signal: AudioSignal) -> List[BeatSegment]:\n        \"\"\"\n        segments a song into time segments corresponding to beats.\n        the first segment starts at 0 and ends at the first beat time.\n        the last segment starts at the last beat time and ends at the end of the song.\n        \"\"\"\n        beat_times = self._beat_times.copy()\n        downbeat_times = self._downbeat_times\n        beat_times.insert(0, 0)\n        beat_times.append(signal.signal_duration)\n\n        downbeat_ids = np.intersect1d(beat_times, downbeat_times, return_indices=True)[\n            1\n        ]\n        is_downbeat = [\n            True if i in downbeat_ids else False for i in range(len(beat_times))\n        ]\n        segments = [\n            BeatSegment(start_time, end_time, downbeat)\n            for start_time, end_time, downbeat in zip(\n                beat_times[:-1], beat_times[1:], is_downbeat\n            )\n        ]\n        return segments\n\n    def get_beats(self) -> np.ndarray:\n        \"\"\"returns an array of beat times, in seconds\n        if downbeats is True, returns an array of downbeat times, in seconds\n        \"\"\"\n        return np.array(\n            self._downbeat_times if self._use_downbeats else self._beat_times\n        )\n\n    @property\n    def beat_times(self) -> np.ndarray:\n        \"\"\"return beat times\"\"\"\n        return np.array(self._beat_times)\n\n    @property\n    def downbeat_times(self) -> np.ndarray:\n        \"\"\"return downbeat times\"\"\"\n        return np.array(self._downbeat_times)\n\n    def beat_times_to_feature_frames(\n        self, signal: AudioSignal, features: np.ndarray\n    ) -> np.ndarray:\n        \"\"\"convert beat times to frames, given an array of time-varying features\"\"\"\n        beat_times = self.get_beats()\n        beat_frames = (\n            beat_times * signal.sample_rate / signal.signal_length * features.shape[-1]\n        ).astype(np.int64)\n        return beat_frames\n\n    def sync_features(\n        self, feature_frames: np.ndarray, features: np.ndarray, aggregate=\"median\"\n    ) -> np.ndarray:\n        \"\"\"sync features to beats\"\"\"\n        if aggregate not in AGGREGATOR_REGISTRY:\n            raise ValueError(f\"unknown aggregation method {aggregate}\")\n\n        return librosa.util.sync(\n            features, feature_frames, aggregate=AGGREGATOR_REGISTRY[aggregate]\n        )\n\n    def to_json(self) -> dict:\n        \"\"\"return beats and downbeats as json\"\"\"\n        return {\n            \"beats\": self._beat_times,\n            \"downbeats\": self._downbeat_times,\n            \"use_downbeats\": self._use_downbeats,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict):\n        \"\"\"load beats and downbeats from json\"\"\"\n        inst = cls(data[\"beats\"], data[\"downbeats\"])\n        inst.use_downbeats(data[\"use_downbeats\"])\n        return inst\n\n    def save(self, output_dir: Path):\n        \"\"\"save beats and downbeats to json\"\"\"\n        mkdir(output_dir)\n        with open(output_dir / \"beats.json\", \"w\") as f:\n            json.dump(self.to_json(), f)\n\n    @classmethod\n    def load(cls, input_dir: Path):\n        \"\"\"load beats and downbeats from json\"\"\"\n        beats_file = Path(input_dir) / \"beats.json\"\n        with open(beats_file, \"r\") as f:\n            data = json.load(f)\n        return cls.from_dict(data)", "\n\n###################\n#  beat tracking  #\n###################\n\n\nclass BeatTracker:\n    def extract_beats(self, signal: AudioSignal) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"extract beats from an audio signal\"\"\"\n        raise NotImplementedError\n\n    def __call__(self, signal: AudioSignal) -> Beats:\n        \"\"\"extract beats from an audio signal\n        NOTE: if the first beat (and/or downbeat) is detected within the first 100ms of the audio,\n        it is discarded. This is to avoid empty bins with no beat synced features in the first beat.\n        Args:\n            signal (AudioSignal): signal to beat track\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: beats and downbeats\n        \"\"\"\n        beats, downbeats = self.extract_beats(signal)\n        return Beats(beats, downbeats)", "\n\nclass WaveBeat(BeatTracker):\n    def __init__(self, ckpt_path: str = \"checkpoints/wavebeat\", device: str = \"cpu\"):\n        from wavebeat.dstcn import dsTCNModel\n\n        model = dsTCNModel.load_from_checkpoint(ckpt_path, map_location=torch.device(device))\n        model.eval()\n\n        self.device = device\n        self.model = model\n\n    def extract_beats(self, signal: AudioSignal) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"returns beat and downbeat times, in  seconds\"\"\"\n        # extract beats\n        beats, downbeats = self.model.predict_beats_from_array(\n            audio=signal.audio_data.squeeze(0),\n            sr=signal.sample_rate,\n            use_gpu=self.device != \"cpu\",\n        )\n\n        return beats, downbeats", "\n\nclass MadmomBeats(BeatTracker):\n    def __init__(self):\n        raise NotImplementedError\n\n    def extract_beats(self, signal: AudioSignal) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"returns beat and downbeat times, in  seconds\"\"\"\n        pass\n", "\n\nBEAT_TRACKER_REGISTRY = {\n    \"wavebeat\": WaveBeat,\n    \"madmom\": MadmomBeats,\n}\n\n\ndef list_beat_trackers() -> list:\n    return list(BEAT_TRACKER_REGISTRY.keys())", "def list_beat_trackers() -> list:\n    return list(BEAT_TRACKER_REGISTRY.keys())\n\n\ndef load_beat_tracker(beat_tracker: str, **kwargs) -> BeatTracker:\n    if beat_tracker not in BEAT_TRACKER_REGISTRY:\n        raise ValueError(\n            f\"Unknown beat tracker {beat_tracker}. Available: {list_beat_trackers()}\"\n        )\n\n    return BEAT_TRACKER_REGISTRY[beat_tracker](**kwargs)"]}
{"filename": "vampnet/util.py", "chunked_list": ["import tqdm\n\nimport torch\nfrom einops import rearrange\n\ndef scalar_to_batch_tensor(x, batch_size):\n    return torch.tensor(x).repeat(batch_size)\n\n\ndef parallelize(\n        fn, \n        *iterables,\n        parallel: str = \"thread_map\",\n        **kwargs\n    ):\n    if parallel == \"thread_map\":\n        from tqdm.contrib.concurrent import thread_map\n        return thread_map(\n            fn, \n            *iterables, \n            **kwargs\n        )\n    elif parallel == \"process_map\":\n        from tqdm.contrib.concurrent import process_map\n        return process_map(\n            fn, \n            *iterables, \n            **kwargs\n        )\n    elif parallel == \"single\":\n        return [fn(x) for x in tqdm.tqdm(*iterables)]\n    else:\n        raise ValueError(f\"parallel must be one of 'thread_map', 'process_map', 'single', but got {parallel}\")", "\ndef parallelize(\n        fn, \n        *iterables,\n        parallel: str = \"thread_map\",\n        **kwargs\n    ):\n    if parallel == \"thread_map\":\n        from tqdm.contrib.concurrent import thread_map\n        return thread_map(\n            fn, \n            *iterables, \n            **kwargs\n        )\n    elif parallel == \"process_map\":\n        from tqdm.contrib.concurrent import process_map\n        return process_map(\n            fn, \n            *iterables, \n            **kwargs\n        )\n    elif parallel == \"single\":\n        return [fn(x) for x in tqdm.tqdm(*iterables)]\n    else:\n        raise ValueError(f\"parallel must be one of 'thread_map', 'process_map', 'single', but got {parallel}\")", "    \ndef codebook_flatten(tokens: torch.Tensor):\n    \"\"\" \n    flatten a sequence of tokens from (batch, codebook, time) to (batch, codebook * time)\n    \"\"\"\n    return rearrange(tokens, \"b c t -> b (t c)\")\n\ndef codebook_unflatten(flat_tokens: torch.Tensor, n_c: int = None):\n    \"\"\"\n    unflatten a sequence of tokens from (batch, codebook * time) to (batch, codebook, time)\n    \"\"\"\n    tokens = rearrange(flat_tokens, \"b (t c) -> b c t\", c=n_c)\n    return tokens", ""]}
{"filename": "vampnet/interface.py", "chunked_list": ["import os\nfrom pathlib import Path\nimport math\n\nimport torch\nimport numpy as np\nfrom audiotools import AudioSignal\nimport tqdm\n\nfrom .modules.transformer import VampNet", "\nfrom .modules.transformer import VampNet\nfrom .beats import WaveBeat\nfrom .mask import *\n\n# from dac.model.dac import DAC\nfrom lac.model.lac import LAC as DAC\n\n\ndef signal_concat(\n    audio_signals: list,\n):\n    audio_data = torch.cat([x.audio_data for x in audio_signals], dim=-1)\n\n    return AudioSignal(audio_data, sample_rate=audio_signals[0].sample_rate)", "\ndef signal_concat(\n    audio_signals: list,\n):\n    audio_data = torch.cat([x.audio_data for x in audio_signals], dim=-1)\n\n    return AudioSignal(audio_data, sample_rate=audio_signals[0].sample_rate)\n\n\ndef _load_model(\n    ckpt: str, \n    lora_ckpt: str = None,\n    device: str = \"cpu\",\n    chunk_size_s: int = 10,\n):\n    # we need to set strict to False if the model has lora weights to add later\n    model = VampNet.load(location=Path(ckpt), map_location=\"cpu\", strict=False)\n\n    # load lora weights if needed\n    if lora_ckpt is not None:\n        if not Path(lora_ckpt).exists():\n            should_cont = input(\n                f\"lora checkpoint {lora_ckpt} does not exist. continue? (y/n) \"\n            )\n            if should_cont != \"y\":\n                raise Exception(\"aborting\")\n        else:\n            model.load_state_dict(torch.load(lora_ckpt, map_location=\"cpu\"), strict=False)\n\n    model.to(device)\n    model.eval()\n    model.chunk_size_s = chunk_size_s\n    return model", "\ndef _load_model(\n    ckpt: str, \n    lora_ckpt: str = None,\n    device: str = \"cpu\",\n    chunk_size_s: int = 10,\n):\n    # we need to set strict to False if the model has lora weights to add later\n    model = VampNet.load(location=Path(ckpt), map_location=\"cpu\", strict=False)\n\n    # load lora weights if needed\n    if lora_ckpt is not None:\n        if not Path(lora_ckpt).exists():\n            should_cont = input(\n                f\"lora checkpoint {lora_ckpt} does not exist. continue? (y/n) \"\n            )\n            if should_cont != \"y\":\n                raise Exception(\"aborting\")\n        else:\n            model.load_state_dict(torch.load(lora_ckpt, map_location=\"cpu\"), strict=False)\n\n    model.to(device)\n    model.eval()\n    model.chunk_size_s = chunk_size_s\n    return model", "\n\n\nclass Interface(torch.nn.Module):\n    def __init__(\n        self,\n        coarse_ckpt: str = None,\n        coarse_lora_ckpt: str = None,\n        coarse2fine_ckpt: str = None,\n        coarse2fine_lora_ckpt: str = None,\n        codec_ckpt: str = None,\n        wavebeat_ckpt: str = None,\n        device: str = \"cpu\",\n        coarse_chunk_size_s: int =  10, \n        coarse2fine_chunk_size_s: int =  3,\n    ):\n        super().__init__()\n        assert codec_ckpt is not None, \"must provide a codec checkpoint\"\n        self.codec = DAC.load(Path(codec_ckpt))\n        self.codec.eval()\n        self.codec.to(device)\n\n        assert coarse_ckpt is not None, \"must provide a coarse checkpoint\"\n        self.coarse = _load_model(\n            ckpt=coarse_ckpt,\n            lora_ckpt=coarse_lora_ckpt,\n            device=device,\n            chunk_size_s=coarse_chunk_size_s,\n        )\n\n        # check if we have a coarse2fine ckpt\n        if coarse2fine_ckpt is not None:\n            self.c2f = _load_model(\n                ckpt=coarse2fine_ckpt,\n                lora_ckpt=coarse2fine_lora_ckpt,\n                device=device,\n                chunk_size_s=coarse2fine_chunk_size_s,\n            )\n        else:\n            self.c2f = None\n\n        if wavebeat_ckpt is not None:\n            print(f\"loading wavebeat from {wavebeat_ckpt}\")\n            self.beat_tracker = WaveBeat(wavebeat_ckpt)\n            self.beat_tracker.model.to(device)\n        else:\n            self.beat_tracker = None\n\n        self.device = device\n\n    def lora_load(\n        self, \n        coarse_ckpt: str = None,\n        c2f_ckpt: str = None,\n        full_ckpts: bool = False,\n    ):\n        if full_ckpts:\n            if coarse_ckpt is not None:\n                self.coarse = _load_model(\n                    ckpt=coarse_ckpt,  \n                    device=self.device,\n                    chunk_size_s=self.coarse.chunk_size_s,\n                )\n            if c2f_ckpt is not None:\n                self.c2f = _load_model(\n                    ckpt=c2f_ckpt,\n                    device=self.device,\n                    chunk_size_s=self.c2f.chunk_size_s,\n                )\n        else:\n            if coarse_ckpt is not None:\n                self.coarse.to(\"cpu\")\n                state_dict = torch.load(coarse_ckpt, map_location=\"cpu\")\n                print(f\"loading coarse from {coarse_ckpt}\")\n                self.coarse.load_state_dict(state_dict, strict=False)\n                self.coarse.to(self.device)\n            if c2f_ckpt is not None:\n                self.c2f.to(\"cpu\")\n                state_dict = torch.load(c2f_ckpt, map_location=\"cpu\")\n                print(f\"loading c2f from {c2f_ckpt}\")\n                self.c2f.load_state_dict(state_dict, strict=False)\n                self.c2f.to(self.device)\n        \n    def s2t(self, seconds: float):\n        \"\"\"seconds to tokens\"\"\"\n        if isinstance(seconds, np.ndarray):\n            return np.ceil(seconds * self.codec.sample_rate / self.codec.hop_length)\n        else:\n            return math.ceil(seconds * self.codec.sample_rate / self.codec.hop_length)\n\n    def s2t2s(self, seconds: float):\n        \"\"\"seconds to tokens to seconds\"\"\"\n        return self.t2s(self.s2t(seconds))\n    \n    def t2s(self, tokens: int):\n        \"\"\"tokens to seconds\"\"\"\n        return tokens * self.codec.hop_length / self.codec.sample_rate\n\n    def to(self, device):\n        self.device = device\n        self.coarse.to(device)\n        self.codec.to(device)\n\n        if self.c2f is not None:\n            self.c2f.to(device)\n\n        if self.beat_tracker is not None:\n            self.beat_tracker.model.to(device)\n        return self\n\n    def to_signal(self, z: torch.Tensor):\n        return self.coarse.to_signal(z, self.codec)\n    \n    def preprocess(self, signal: AudioSignal):\n        signal = (\n            signal.clone()\n            .resample(self.codec.sample_rate)\n            .to_mono()\n            .normalize(-24)\n            .ensure_max_of_audio(1.0)\n        )\n        return signal\n    \n    @torch.inference_mode()\n    def encode(self, signal: AudioSignal):\n        signal = self.preprocess(signal).to(self.device)\n        z = self.codec.encode(signal.samples, signal.sample_rate)[\"codes\"]\n        return z\n\n    def snap_to_beats(\n        self, \n        signal: AudioSignal\n    ):\n        assert hasattr(self, \"beat_tracker\"), \"No beat tracker loaded\"\n        beats, downbeats = self.beat_tracker.extract_beats(signal)\n        \n        # trim the signa around the first beat time\n        samples_begin = int(beats[0] * signal.sample_rate )\n        samples_end = int(beats[-1] * signal.sample_rate)\n        print(beats[0])\n        signal = signal.clone().trim(samples_begin, signal.length - samples_end)\n\n        return signal\n\n    def make_beat_mask(self, \n            signal: AudioSignal, \n            before_beat_s: float = 0.0,\n            after_beat_s: float = 0.02,\n            mask_downbeats: bool = True,\n            mask_upbeats: bool = True,\n            downbeat_downsample_factor: int = None,\n            beat_downsample_factor: int = None,\n            dropout: float = 0.0,\n            invert: bool = True,\n    ):\n        \"\"\"make a beat synced mask. that is, make a mask that \n        places 1s at and around the beat, and 0s everywhere else. \n        \"\"\"\n        assert self.beat_tracker is not None, \"No beat tracker loaded\"\n\n        # get the beat times\n        beats, downbeats = self.beat_tracker.extract_beats(signal)\n\n        # get the beat indices in z\n        beats_z, downbeats_z = self.s2t(beats), self.s2t(downbeats)\n\n        # remove downbeats from beats\n        beats_z = torch.tensor(beats_z)[~torch.isin(torch.tensor(beats_z), torch.tensor(downbeats_z))]\n        beats_z = beats_z.tolist()\n        downbeats_z = downbeats_z.tolist()\n\n        # make the mask \n        seq_len = self.s2t(signal.duration)\n        mask = torch.zeros(seq_len, device=self.device)\n        \n        mask_b4 = self.s2t(before_beat_s)\n        mask_after = self.s2t(after_beat_s)\n\n        if beat_downsample_factor is not None:\n            if beat_downsample_factor < 1:\n                raise ValueError(\"mask_beat_downsample_factor must be >= 1 or None\")\n        else:\n            beat_downsample_factor = 1\n\n        if downbeat_downsample_factor is not None:\n            if downbeat_downsample_factor < 1:\n                raise ValueError(\"mask_beat_downsample_factor must be >= 1 or None\")\n        else:\n            downbeat_downsample_factor = 1\n\n        beats_z = beats_z[::beat_downsample_factor]\n        downbeats_z = downbeats_z[::downbeat_downsample_factor]\n        print(f\"beats_z: {len(beats_z)}\")\n        print(f\"downbeats_z: {len(downbeats_z)}\")\n    \n        if mask_upbeats:\n            for beat_idx in beats_z:\n                _slice = int(beat_idx - mask_b4), int(beat_idx + mask_after)\n                num_steps = mask[_slice[0]:_slice[1]].shape[0]\n                _m = torch.ones(num_steps, device=self.device)\n                _m_mask = torch.bernoulli(_m * (1 - dropout))\n                _m = _m * _m_mask.long()\n                \n                mask[_slice[0]:_slice[1]] = _m\n\n        if mask_downbeats:\n            for downbeat_idx in downbeats_z:\n                _slice = int(downbeat_idx - mask_b4), int(downbeat_idx + mask_after)\n                num_steps = mask[_slice[0]:_slice[1]].shape[0]\n                _m = torch.ones(num_steps, device=self.device)\n                _m_mask = torch.bernoulli(_m * (1 - dropout))\n                _m = _m * _m_mask.long()\n                \n                mask[_slice[0]:_slice[1]] = _m\n        \n        mask = mask.clamp(0, 1)\n        if invert:\n            mask = 1 - mask\n        \n        mask = mask[None, None, :].bool().long()\n        if self.c2f is not None:\n            mask = mask.repeat(1, self.c2f.n_codebooks, 1)\n        else:\n            mask = mask.repeat(1, self.coarse.n_codebooks, 1)\n        return mask\n        \n    def coarse_to_fine(\n        self, \n        z: torch.Tensor,\n        mask: torch.Tensor = None,\n        **kwargs\n    ):\n        assert self.c2f is not None, \"No coarse2fine model loaded\"\n        length = z.shape[-1]\n        chunk_len = self.s2t(self.c2f.chunk_size_s)\n        n_chunks = math.ceil(z.shape[-1] / chunk_len)\n\n        # zero pad to chunk_len\n        if length % chunk_len != 0:\n            pad_len = chunk_len - (length % chunk_len)\n            z = torch.nn.functional.pad(z, (0, pad_len))\n            mask = torch.nn.functional.pad(mask, (0, pad_len)) if mask is not None else None\n\n        n_codebooks_to_append = self.c2f.n_codebooks - z.shape[1]\n        if n_codebooks_to_append > 0:\n            z = torch.cat([\n                z,\n                torch.zeros(z.shape[0], n_codebooks_to_append, z.shape[-1]).long().to(self.device)\n            ], dim=1)\n\n        # set the mask to 0 for all conditioning codebooks\n        if mask is not None:\n            mask = mask.clone()\n            mask[:, :self.c2f.n_conditioning_codebooks, :] = 0\n\n        fine_z = []\n        for i in range(n_chunks):\n            chunk = z[:, :, i * chunk_len : (i + 1) * chunk_len]\n            mask_chunk = mask[:, :, i * chunk_len : (i + 1) * chunk_len] if mask is not None else None\n            \n            chunk = self.c2f.generate(\n                codec=self.codec,\n                time_steps=chunk_len,\n                start_tokens=chunk,\n                return_signal=False,\n                mask=mask_chunk,\n                **kwargs\n            )\n            fine_z.append(chunk)\n\n        fine_z = torch.cat(fine_z, dim=-1)\n        return fine_z[:, :, :length].clone()\n    \n    def coarse_vamp(\n        self, \n        z, \n        mask,\n        return_mask=False,\n        gen_fn=None,\n        **kwargs\n    ):\n        # coarse z\n        cz = z[:, : self.coarse.n_codebooks, :].clone()\n        assert cz.shape[-1] <= self.s2t(self.coarse.chunk_size_s), f\"the sequence of tokens provided must match the one specified in the coarse chunk size, but got {cz.shape[-1]} and {self.s2t(self.coarse.chunk_size_s)}\"\n\n        mask = mask[:, : self.coarse.n_codebooks, :]\n\n        cz_masked, mask = apply_mask(cz, mask, self.coarse.mask_token)\n        cz_masked = cz_masked[:, : self.coarse.n_codebooks, :]\n\n        gen_fn = gen_fn or self.coarse.generate\n        c_vamp = gen_fn(\n            codec=self.codec,\n            time_steps=cz.shape[-1],\n            start_tokens=cz,\n            mask=mask, \n            return_signal=False,\n            **kwargs\n        )\n\n        # add the fine codes back in\n        c_vamp = torch.cat(\n            [c_vamp, z[:, self.coarse.n_codebooks :, :]], \n            dim=1\n        )\n\n        if return_mask:\n            return c_vamp, cz_masked\n        \n        return c_vamp", "\n\nif __name__ == \"__main__\":\n    import audiotools as at\n    import logging\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    torch.set_printoptions(threshold=10000)\n    at.util.seed(42)\n\n    interface = Interface(\n        coarse_ckpt=\"./models/vampnet/coarse.pth\", \n        coarse2fine_ckpt=\"./models/vampnet/c2f.pth\", \n        codec_ckpt=\"./models/vampnet/codec.pth\",\n        device=\"cuda\", \n        wavebeat_ckpt=\"./models/wavebeat.pth\"\n    )\n\n\n    sig = at.AudioSignal('assets/example.wav')\n\n    z = interface.encode(sig)\n    breakpoint()\n\n    # mask = linear_random(z, 1.0)\n    # mask = mask_and(\n    #     mask, periodic_mask(\n    #         z,\n    #         32,\n    #         1,\n    #         random_roll=True\n    #     )\n    # )\n\n    # mask = interface.make_beat_mask(\n    #     sig, 0.0, 0.075\n    # )\n    # mask = dropout(mask, 0.0)\n    # mask = codebook_unmask(mask, 0)\n\n    mask = inpaint(z, n_prefix=100, n_suffix=100)\n    \n    zv, mask_z = interface.coarse_vamp(\n        z, \n        mask=mask,\n        sampling_steps=36,\n        temperature=8.0,\n        return_mask=True, \n        gen_fn=interface.coarse.generate\n    )\n    \n\n    use_coarse2fine = True\n    if use_coarse2fine: \n        zv = interface.coarse_to_fine(zv, temperature=0.8, mask=mask)\n        breakpoint()\n\n    mask = interface.to_signal(mask_z).cpu()\n\n    sig = interface.to_signal(zv).cpu()\n    print(\"done\")", "\n        "]}
{"filename": "vampnet/modules/layers.py", "chunked_list": ["import time\nfrom typing import Optional\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\nfrom torch.nn.utils import weight_norm\n", "from torch.nn.utils import weight_norm\n\n# Scripting this brings model speed up 1.4x\n@torch.jit.script\ndef snake(x, alpha):\n    shape = x.shape\n    x = x.reshape(shape[0], shape[1], -1)\n    x = x + (alpha + 1e-9).reciprocal() * torch.sin(alpha * x).pow(2)\n    x = x.reshape(shape)\n    return x", "\n\nclass Snake1d(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.alpha = nn.Parameter(torch.ones(1, channels, 1))\n\n    def forward(self, x):\n        return snake(x, self.alpha)\n", "\n\ndef num_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef recurse_children(module, fn):\n    for child in module.children():\n        if isinstance(child, nn.ModuleList):\n            for c in child:\n                yield recurse_children(c, fn)\n        if isinstance(child, nn.ModuleDict):\n            for c in child.values():\n                yield recurse_children(c, fn)\n\n        yield recurse_children(child, fn)\n        yield fn(child)", "\n\ndef WNConv1d(*args, **kwargs):\n    return weight_norm(nn.Conv1d(*args, **kwargs))\n\n\ndef WNConvTranspose1d(*args, **kwargs):\n    return weight_norm(nn.ConvTranspose1d(*args, **kwargs))\n\n\nclass SequentialWithFiLM(nn.Module):\n    \"\"\"\n    handy wrapper for nn.Sequential that allows FiLM layers to be\n    inserted in between other layers.\n    \"\"\"\n\n    def __init__(self, *layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n\n    @staticmethod\n    def has_film(module):\n        mod_has_film = any(\n            [res for res in recurse_children(module, lambda c: isinstance(c, FiLM))]\n        )\n        return mod_has_film\n\n    def forward(self, x, cond):\n        for layer in self.layers:\n            if self.has_film(layer):\n                x = layer(x, cond)\n            else:\n                x = layer(x)\n        return x", "\n\nclass SequentialWithFiLM(nn.Module):\n    \"\"\"\n    handy wrapper for nn.Sequential that allows FiLM layers to be\n    inserted in between other layers.\n    \"\"\"\n\n    def __init__(self, *layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n\n    @staticmethod\n    def has_film(module):\n        mod_has_film = any(\n            [res for res in recurse_children(module, lambda c: isinstance(c, FiLM))]\n        )\n        return mod_has_film\n\n    def forward(self, x, cond):\n        for layer in self.layers:\n            if self.has_film(layer):\n                x = layer(x, cond)\n            else:\n                x = layer(x)\n        return x", "\n\nclass FiLM(nn.Module):\n    def __init__(self, input_dim: int, output_dim: int):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        if input_dim > 0:\n            self.beta = nn.Linear(input_dim, output_dim)\n            self.gamma = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x, r):\n        if self.input_dim == 0:\n            return x\n        else:\n            beta, gamma = self.beta(r), self.gamma(r)\n            beta, gamma = (\n                beta.view(x.size(0), self.output_dim, 1),\n                gamma.view(x.size(0), self.output_dim, 1),\n            )\n            x = x * (gamma + 1) + beta\n        return x", "\n\nclass CodebookEmbedding(nn.Module):\n    def __init__(\n        self,\n        vocab_size: int,\n        latent_dim: int,\n        n_codebooks: int,\n        emb_dim: int,\n        special_tokens: Optional[Tuple[str]] = None,\n    ):\n        super().__init__()\n        self.n_codebooks = n_codebooks\n        self.emb_dim = emb_dim\n        self.latent_dim = latent_dim\n        self.vocab_size = vocab_size\n\n        if special_tokens is not None:\n            for tkn in special_tokens:\n                self.special = nn.ParameterDict(\n                    {\n                        tkn: nn.Parameter(torch.randn(n_codebooks, self.latent_dim))\n                        for tkn in special_tokens\n                    }\n                )\n                self.special_idxs = {\n                    tkn: i + vocab_size for i, tkn in enumerate(special_tokens)\n                }\n\n        self.out_proj = nn.Conv1d(n_codebooks * self.latent_dim, self.emb_dim, 1)\n\n    def from_codes(self, codes: torch.Tensor, codec):\n        \"\"\" \n        get a sequence of continuous embeddings from a sequence of discrete codes. \n        unlike it's counterpart in the original VQ-VAE, this function adds for any special tokens\n        necessary for the language model, like <MASK>. \n        \"\"\"\n        n_codebooks = codes.shape[1]\n        latent = []\n        for i in range(n_codebooks):\n            c = codes[:, i, :]\n\n            lookup_table = codec.quantizer.quantizers[i].codebook.weight\n            if hasattr(self, \"special\"):\n                special_lookup = torch.cat(\n                    [self.special[tkn][i : i + 1] for tkn in self.special], dim=0\n                )\n                lookup_table = torch.cat([lookup_table, special_lookup], dim=0)\n\n            l = F.embedding(c, lookup_table).transpose(1, 2)\n            latent.append(l)\n\n        latent = torch.cat(latent, dim=1)\n        return latent\n\n    def forward(self, latents: torch.Tensor):\n        \"\"\"\n        project a sequence of latents to a sequence of embeddings\n        \"\"\"\n        x = self.out_proj(latents)\n        return x", "\n"]}
{"filename": "vampnet/modules/transformer.py", "chunked_list": ["import math\nimport logging\nfrom typing import Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\nimport loralib as lora", "from einops import rearrange\nimport loralib as lora\nimport audiotools as at\n\nfrom .activations import get_activation\nfrom .layers import CodebookEmbedding\nfrom .layers import FiLM\nfrom .layers import SequentialWithFiLM\nfrom .layers import WNConv1d\nfrom ..util import scalar_to_batch_tensor, codebook_flatten, codebook_unflatten", "from .layers import WNConv1d\nfrom ..util import scalar_to_batch_tensor, codebook_flatten, codebook_unflatten\nfrom ..mask import _gamma\n\nLORA_R = 8\n\n# def log(t, eps=1e-20):\n#     return torch.log(t + eps)\n\n\ndef gumbel_noise_like(t):\n    noise = torch.zeros_like(t).uniform_(1e-20, 1)\n    return -torch.log(-torch.log(noise))", "\n\ndef gumbel_noise_like(t):\n    noise = torch.zeros_like(t).uniform_(1e-20, 1)\n    return -torch.log(-torch.log(noise))\n\n\ndef gumbel_sample(t, temperature=1.0, dim=-1):\n    return ((t / max(temperature, 1e-10)) + gumbel_noise_like(t)).argmax(dim=dim)\n", "\n\nclass RMSNorm(nn.Module):\n    def __init__(self, hidden_size: int, eps=1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.var_eps = eps\n\n    def forward(self, x):\n        \"\"\"Returns root mean square normalized version of input `x`\n        # T5 uses a layer_norm which only scales and doesn't shift, which is also known\n        # as Root Mean Square Layer Normalization https://arxiv.org/abs/1910.07467\n        # thus varience is calculated w/o mean and there is no bias\n        Parameters\n        ----------\n        x : Tensor[B x T x D]\n        Returns\n        -------\n        Tensor[B x T x D]\n        \"\"\"\n        var = x.pow(2).mean(-1, keepdim=True)\n        x = x * torch.rsqrt(var + self.var_eps)\n\n        return self.weight * x", "\n\nclass FeedForward(nn.Module):\n    def __init__(\n        self, d_model: int = 512, dropout: float = 0.1, activation: str = \"geglu\"\n    ):\n        super().__init__()\n        factor = 2 if activation == \"geglu\" else 1\n        self.w_1 = lora.Linear(d_model, d_model * 4, bias=False, r=LORA_R)\n        self.w_2 = lora.Linear(d_model * 4 // factor, d_model, bias=False, r=LORA_R)\n        self.drop = nn.Dropout(dropout)\n        self.act = get_activation(activation)()\n\n    def forward(self, x):\n        \"\"\"Computes position-wise feed-forward layer\n        Parameters\n        ----------\n        x : Tensor[B x T x D]\n        Returns\n        -------\n        Tensor[B x T x D]\n        \"\"\"\n        x = self.w_1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.w_2(x)\n        return x", "\n\nclass MultiHeadRelativeAttention(nn.Module):\n    def __init__(\n        self,\n        n_head: int = 8,\n        d_model: int = 512,\n        dropout: float = 0.1,\n        bidirectional: bool = True,\n        has_relative_attention_bias: bool = True,\n        attention_num_buckets: int = 32,\n        attention_max_distance: int = 128,\n    ):\n        super().__init__()\n        d_head = d_model // n_head\n        self.n_head = n_head\n        self.d_head = d_head\n        self.bidirectional = bidirectional\n        self.has_relative_attention_bias = has_relative_attention_bias\n        self.attention_num_buckets = attention_num_buckets\n        self.attention_max_distance = attention_max_distance\n\n        # Create linear query, key, value projections\n        self.w_qs = lora.Linear(d_model, d_model, bias=False, r=LORA_R)\n        self.w_ks = nn.Linear(d_model, d_model, bias=False)\n        self.w_vs = lora.Linear(d_model, d_model, bias=False, r=LORA_R)\n\n        # Create linear final output projection\n        self.fc = lora.Linear(d_model, d_model, bias=False, r=LORA_R)\n\n        # Dropout for attention output weights\n        self.dropout = nn.Dropout(dropout)\n\n        # Create relative positional embeddings (if turned on)\n        if has_relative_attention_bias:\n            self.relative_attention_bias = nn.Embedding(attention_num_buckets, n_head)\n\n    def _relative_position_bucket(self, relative_position):\n        \"\"\"Converts unbounded relative position into bounded set of buckets\n        with half \"exact\" buckets (1 position = 1 bucket) and half \"log-spaced\"\n        buckets\n        Parameters\n        ----------\n        relative_position : Tensor[T_q x T_kv]\n            Relative positions between queries and key_value items\n        Returns\n        -------\n        Tensor[T_q x T_kv]\n            Input relative positions converted into buckets\n        \"\"\"\n        relative_buckets = 0\n        num_buckets = self.attention_num_buckets\n        max_distance = self.attention_max_distance\n\n        # Convert relative position for (-inf, inf) to [0, inf]\n        # Negative relative positions correspond to past\n        # Positive relative positions correspond to future\n        if self.bidirectional:\n            # use half buckets for each side (past / future)\n            num_buckets //= 2\n\n            # Shift the position positions by `num_buckets` to wrap around\n            # negative positions\n            relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n            relative_position = torch.abs(relative_position)\n        else:\n            # If not bidirectional, ignore positive positions and wrap\n            # negative positions to positive\n            relative_position = -torch.min(\n                relative_position, torch.zeros_like(relative_position)\n            )\n\n        # Allocate half of the buckets are for exact increments in positions\n        max_exact = num_buckets // 2\n        is_small = relative_position < max_exact\n\n        # The other half of the buckets are for logarithmically bigger bins in\n        # positions up to `max_distance`\n        relative_postion_if_large = max_exact + (\n            torch.log(relative_position.float() / max_exact)\n            / math.log(max_distance / max_exact)\n            * (num_buckets - max_exact)\n        ).to(torch.long)\n\n        # Clip the max relative position to `num_buckets - 1`\n        relative_postion_if_large = torch.min(\n            relative_postion_if_large,\n            torch.full_like(relative_postion_if_large, num_buckets - 1),\n        )\n\n        # Choose relative buckets based on small or large positions\n        relative_buckets += torch.where(\n            is_small, relative_position, relative_postion_if_large\n        )\n\n        return relative_buckets\n\n    def compute_bias(self, query_length, key_length):\n        \"\"\"Computes a position bias scalar for each index in query_length x key_length\n        Parameters\n        ----------\n        query_length : int\n        key_length : int\n        Returns\n        -------\n        Tensor[heads x 1 x T_q x T_kv]\n            Position bias to be applied on attention logits\n        \"\"\"\n\n        query_position = torch.arange(query_length, dtype=torch.long)[:, None]\n        key_position = torch.arange(key_length, dtype=torch.long)[None, :]\n        relative_position = key_position - query_position\n\n        # Convert relative position to buckets\n        relative_position_bucket = self._relative_position_bucket(relative_position)\n        relative_position_bucket = relative_position_bucket.to(\n            self.relative_attention_bias.weight.device\n        )\n\n        # Index attention bias values\n        values = self.relative_attention_bias(relative_position_bucket)\n        values = rearrange(values, \"q k h -> h 1 q k\")\n\n        return values\n\n    def forward(self, q, k, v, mask=None, position_bias=None):\n        \"\"\"Computes attention over (keys, values) for every timestep in query\n        Parameters\n        ----------\n        q : Tensor[B x T_q x d_model]\n            Query vectors\n        k : Tensor[B x T_kv x d_model]\n            Key vectors to compute attention over\n        v : Tensor[B x T_kv x d_model]\n            Value vectors corresponding to the keys\n        mask : Tensor[B x T_q x T_kv], optional\n        position_bias: Tensor[head x 1 x T_q x T_kv]\n        Returns\n        -------\n        Tensor[B x T_q x d_model]\n            Outputs after attending (key, value) using queries\n        \"\"\"\n        # Compute query, key, value projections\n        q = rearrange(self.w_qs(q), \"b l (head k) -> head b l k\", head=self.n_head)\n        k = rearrange(self.w_ks(k), \"b t (head k) -> head b t k\", head=self.n_head)\n        v = rearrange(self.w_vs(v), \"b t (head k) -> head b t k\", head=self.n_head)\n\n        # Compute attention matrix\n        attn = torch.einsum(\"hblk,hbtk->hblt\", [q, k]) / np.sqrt(q.shape[-1])\n\n        # Add relative position bias to attention scores\n        if position_bias is None:\n            if self.has_relative_attention_bias:\n                position_bias = self.compute_bias(q.size(-2), k.size(-2))\n            else:\n                position_bias = torch.zeros_like(attn)\n        attn += position_bias\n\n        # Apply mask to attention scores to prevent looking up invalid locations\n        if mask is not None:\n            attn = attn.masked_fill(mask[None] == 0, -1e9)\n\n        # Normalize attention scores and add dropout\n        attn = torch.softmax(attn, dim=3)\n        attn = self.dropout(attn)\n\n        # Compute attended outputs (product of attention matrix and values)\n        output = torch.einsum(\"hblt,hbtv->hblv\", [attn, v])\n        output = rearrange(output, \"head b l v -> b l (head v)\")\n        output = self.fc(output)\n\n        return output, position_bias", "\n\nclass TransformerLayer(nn.Module):\n    def __init__(\n        self,\n        d_model: int = 512,\n        d_cond: int = 64,\n        n_heads: int = 8,\n        bidirectional: bool = True,\n        is_decoder: bool = False,\n        has_relative_attention_bias: bool = False,\n        flash_attn: bool = False,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        # Store args\n        self.is_decoder = is_decoder\n\n        # Create self-attention layer\n        self.norm_1 = RMSNorm(d_model)\n        self.film_1 = FiLM(d_cond, d_model)\n        self.flash_attn = flash_attn\n\n        if flash_attn:\n            from flash_attn.flash_attention import FlashMHA\n            self.self_attn = FlashMHA(\n                embed_dim=d_model,\n                num_heads=n_heads,\n                attention_dropout=dropout,\n                causal=False,\n            )\n        else:\n            self.self_attn = MultiHeadRelativeAttention(\n                n_heads, d_model, dropout, bidirectional, has_relative_attention_bias\n            )\n\n        # (Optional) Create cross-attention layer\n        if is_decoder:\n            self.norm_2 = RMSNorm(d_model)\n            self.film_2 = FiLM(d_cond, d_model)\n            self.cross_attn = MultiHeadRelativeAttention(\n                n_heads,\n                d_model,\n                dropout,\n                bidirectional=True,\n                has_relative_attention_bias=False,\n            )\n\n        # Create last feed-forward layer\n        self.norm_3 = RMSNorm(d_model)\n        self.film_3 = FiLM(d_cond, d_model)\n        self.feed_forward = FeedForward(d_model=d_model, dropout=dropout)\n\n        # Create dropout\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(\n        self,\n        x,\n        x_mask,\n        cond,\n        src=None,\n        src_mask=None,\n        position_bias=None,\n        encoder_decoder_position_bias=None,\n    ):\n        \"\"\"Computes one transformer layer consisting of self attention, (op) cross attention\n        and feedforward layer\n        Parameters\n        ----------\n        x : Tensor[B x T_q x D]\n        x_mask : Tensor[B x T_q]\n        src : Tensor[B x T_kv x D], optional\n        src_mask : Tensor[B x T_kv x D], optional\n        position_bias : Tensor[heads x B x T_q x T_q], optional\n            Relative position bias for self attention layer\n        encoder_decoder_position_bias : Tensor[heads x B x T_q x T_kv], optional\n            Relative position bias for cross attention layer\n        Returns\n        -------\n        Tensor[B x T_q x D]\n        \"\"\"\n        y = self.norm_1(x)\n        y = self.film_1(y.permute(0, 2, 1), cond).permute(0, 2, 1)\n        if self.flash_attn:\n            with torch.autocast(y.device.type, dtype=torch.bfloat16):\n                y = self.self_attn(y)[0]\n        else:\n            y, position_bias = self.self_attn(y, y, y, x_mask, position_bias)\n        x = x + self.dropout(y)\n\n        if self.is_decoder:\n            y = self.norm_2(x)\n            y = self.film_2(y.permute(0, 2, 1), cond).permute(0, 2, 1)\n            y, encoder_decoder_position_bias = self.cross_attn(\n                y, src, src, src_mask, encoder_decoder_position_bias\n            )\n            x = x + self.dropout(y)\n\n        y = self.norm_3(x)\n        y = self.film_3(\n            y.permute(\n                0,\n                2,\n                1,\n            ),\n            cond,\n        ).permute(0, 2, 1)\n        y = self.feed_forward(y)\n        x = x + self.dropout(y)\n\n        return x, position_bias, encoder_decoder_position_bias", "\n\nclass TransformerStack(nn.Module):\n    def __init__(\n        self,\n        d_model: int = 512,\n        d_cond: int = 64,\n        n_heads: int = 8,\n        n_layers: int = 8,\n        last_layer: bool = True,\n        bidirectional: bool = True,\n        flash_attn: bool = False,\n        is_decoder: bool = False,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        # Store args\n        self.bidirectional = bidirectional\n        self.is_decoder = is_decoder\n\n        # Create transformer layers\n        # In T5, relative attention bias is shared by all layers in the stack\n        self.layers = nn.ModuleList(\n            [\n                TransformerLayer(\n                    d_model,\n                    d_cond,\n                    n_heads,\n                    bidirectional,\n                    is_decoder,\n                    has_relative_attention_bias=True if (i == 0) else False,\n                    flash_attn=flash_attn,\n                    dropout=dropout,\n                )\n                for i in range(n_layers)\n            ]\n        )\n\n        # Perform last normalization\n        self.norm = RMSNorm(d_model) if last_layer else None\n\n    def subsequent_mask(self, size):\n        return torch.ones(1, size, size).tril().bool()\n\n    def forward(self, x, x_mask, cond=None, src=None, src_mask=None):\n        \"\"\"Computes a full transformer stack\n        Parameters\n        ----------\n        x : Tensor[B x T_q x D]\n        x_mask : Tensor[B x T_q]\n        src : Tensor[B x T_kv x D], optional\n        src_mask : Tensor[B x T_kv], optional\n        Returns\n        -------\n        Tensor[B x T_q x D]\n        \"\"\"\n\n        # Convert `src_mask` to (B x T_q x T_kv) shape for cross attention masking\n        if self.is_decoder:\n            src_mask = x_mask.unsqueeze(-1) * src_mask.unsqueeze(-2)\n\n        # Convert `x_mask` to (B x T_q x T_q) shape for self attention masking\n        x_mask = x_mask.unsqueeze(-2)\n        if not self.bidirectional:\n            x_mask = x_mask * self.subsequent_mask(x.size(1)).to(x_mask.device)\n\n        # Initialize position biases\n        position_bias = None\n        encoder_decoder_position_bias = None\n\n        # Compute transformer layers\n        for layer in self.layers:\n            x, position_bias, encoder_decoder_position_bias = layer(\n                x=x,\n                x_mask=x_mask,\n                cond=cond,\n                src=src,\n                src_mask=src_mask,\n                position_bias=position_bias,\n                encoder_decoder_position_bias=encoder_decoder_position_bias,\n            )\n\n        return self.norm(x) if self.norm is not None else x", "\n\nclass VampNet(at.ml.BaseModel):\n    def __init__(\n        self,\n        n_heads: int = 20,\n        n_layers: int = 16,\n        r_cond_dim: int = 0,\n        n_codebooks: int = 9,\n        n_conditioning_codebooks: int = 0,\n        latent_dim: int = 8,\n        embedding_dim: int = 1280,\n        vocab_size: int = 1024,\n        flash_attn: bool = True,\n        noise_mode: str = \"mask\",\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        assert r_cond_dim == 0, f\"r_cond_dim must be 0 (not supported), but got {r_cond_dim}\"\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.r_cond_dim = r_cond_dim\n        self.n_codebooks = n_codebooks\n        self.n_conditioning_codebooks = n_conditioning_codebooks\n        self.embedding_dim = embedding_dim\n        self.vocab_size = vocab_size\n        self.latent_dim = latent_dim\n        self.flash_attn = flash_attn\n        self.noise_mode = noise_mode\n\n        assert self.noise_mode == \"mask\", \"deprecated\"\n\n        self.embedding = CodebookEmbedding(\n            latent_dim=latent_dim,\n            n_codebooks=n_codebooks,\n            vocab_size=vocab_size,\n            emb_dim=embedding_dim,\n            special_tokens=[\"MASK\"],\n        )\n        self.mask_token = self.embedding.special_idxs[\"MASK\"]\n\n        self.transformer = TransformerStack(\n            d_model=embedding_dim,\n            d_cond=r_cond_dim,\n            n_heads=n_heads,\n            n_layers=n_layers,\n            last_layer=True,\n            bidirectional=True,\n            flash_attn=flash_attn,\n            is_decoder=False,\n            dropout=dropout,\n        )\n\n        # Add final conv layer\n        self.n_predict_codebooks = n_codebooks - n_conditioning_codebooks\n        self.classifier = SequentialWithFiLM(\n            WNConv1d(\n                embedding_dim,\n                vocab_size * self.n_predict_codebooks,\n                kernel_size=1,\n                padding=\"same\",\n                # groups=self.n_predict_codebooks,\n            ),\n        )\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x_mask = torch.ones_like(x, dtype=torch.bool)[:, :1, :].squeeze(1)\n\n        x = rearrange(x, \"b d n -> b n d\")\n        out = self.transformer(x=x, x_mask=x_mask)\n        out = rearrange(out, \"b n d -> b d n\")\n\n        out = self.classifier(out, None) # no cond here!\n\n        out = rearrange(out, \"b (p c) t -> b p (t c)\", c=self.n_predict_codebooks)\n\n        return out\n    \n    def r_embed(self, r, max_positions=10000):\n        if self.r_cond_dim > 0:\n            dtype = r.dtype\n\n            r = _gamma(r) * max_positions\n            half_dim = self.r_cond_dim // 2\n\n            emb = math.log(max_positions) / (half_dim - 1)\n            emb = torch.arange(half_dim, device=r.device).float().mul(-emb).exp()\n\n            emb = r[:, None] * emb[None, :]\n            emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n\n            if self.r_cond_dim % 2 == 1:  # zero pad\n                emb = nn.functional.pad(emb, (0, 1), mode=\"constant\")\n\n            return emb.to(dtype)\n        else:\n            return r\n    \n    @torch.no_grad()\n    def to_signal(self, z, codec):\n        \"\"\"\n        convert a sequence of latents to a signal. \n        \"\"\"\n        assert z.ndim == 3\n\n        signal = at.AudioSignal(\n            codec.decode(\n                codec.quantizer.from_latents(self.embedding.from_codes(z, codec))[0]\n            )[\"audio\"],\n            codec.sample_rate,\n        )\n\n        # find where the mask token is and replace it with silence in the audio\n        for tstep in range(z.shape[-1]):\n            if torch.any(z[:, :, tstep] == self.mask_token):\n                sample_idx_0 = tstep * codec.hop_length\n                sample_idx_1 = sample_idx_0 + codec.hop_length\n                signal.samples[:, :, sample_idx_0:sample_idx_1] = 0.0\n\n        return signal\n\n\n    @torch.no_grad()\n    def generate(\n        self,\n        codec,\n        time_steps: int = 300,\n        sampling_steps: int = 36,\n        start_tokens: Optional[torch.Tensor] = None,\n        sampling_temperature: float = 1.0,\n        mask: Optional[torch.Tensor] = None,\n        mask_temperature: float = 10.5,\n        typical_filtering=False,\n        typical_mass=0.2,\n        typical_min_tokens=1,\n        top_p=None,\n        return_signal=True,\n        seed: int = None, \n        sample_cutoff: float = 0.5,\n    ):\n        if seed is not None:\n            at.util.seed(seed)\n        logging.debug(f\"beginning generation with {sampling_steps} steps\")\n\n\n\n        ##################### \n        # resolve initial z #\n        #####################\n        z = start_tokens\n\n        if z is None:\n            z = torch.full((1, self.n_codebooks, time_steps), self.mask_token).to(\n                self.device\n            )\n\n        logging.debug(f\"created z with shape {z.shape}\")\n\n\n        #################\n        # resolve mask #\n        #################\n\n        if mask is None:\n            mask = torch.ones_like(z).to(self.device).int()\n            mask[:, : self.n_conditioning_codebooks, :] = 0.0\n        if mask.ndim == 2:\n            mask = mask[:, None, :].repeat(1, z.shape[1], 1)\n        # init_mask = mask.clone()\n        \n        logging.debug(f\"created mask with shape {mask.shape}\")\n\n\n        ###########\n        # set up #\n        ##########\n        # apply the mask to z\n        z_masked = z.masked_fill(mask.bool(), self.mask_token)\n        # logging.debug(f\"z_masked: {z_masked}\")\n\n        # how many mask tokens to begin with?\n        num_mask_tokens_at_start = (z_masked == self.mask_token).sum()\n        logging.debug(f\"num mask tokens at start: {num_mask_tokens_at_start}\")\n\n        # how many codebooks are we inferring vs conditioning on?\n        n_infer_codebooks = self.n_codebooks - self.n_conditioning_codebooks\n        logging.debug(f\"n infer codebooks: {n_infer_codebooks}\")\n\n        #################\n        # begin sampling #\n        #################\n\n        for i in range(sampling_steps):\n            logging.debug(f\"step {i} of {sampling_steps}\")\n\n            # our current schedule step\n            r = scalar_to_batch_tensor(\n                (i + 1) / sampling_steps, \n                z.shape[0]\n            ).to(z.device)\n            logging.debug(f\"r: {r}\")\n\n            # get latents\n            latents = self.embedding.from_codes(z_masked, codec)\n            logging.debug(f\"computed latents with shape: {latents.shape}\")\n\n\n            # infer from latents\n            # NOTE: this collapses the codebook dimension into the sequence dimension\n            logits = self.forward(latents) # b, prob, seq\n            logits = logits.permute(0, 2, 1)  # b, seq, prob\n            b = logits.shape[0]\n\n            logging.debug(f\"permuted logits with shape: {logits.shape}\")\n\n            sampled_z, selected_probs = sample_from_logits(\n                logits, sample=(\n                   (i / sampling_steps) <= sample_cutoff\n                ), \n                temperature=sampling_temperature,\n                typical_filtering=typical_filtering, typical_mass=typical_mass,\n                typical_min_tokens=typical_min_tokens,\n                top_k=None, top_p=top_p, return_probs=True,\n            )\n\n            logging.debug(f\"sampled z with shape: {sampled_z.shape}\")\n\n            # flatten z_masked and mask, so we can deal with the sampling logic\n            # we'll unflatten them at the end of the loop for the next forward pass\n            # remove conditioning codebooks, we'll add them back at the end\n            z_masked = codebook_flatten(z_masked[:, self.n_conditioning_codebooks:, :])           \n\n            mask = (z_masked == self.mask_token).int()\n            \n            # update the mask, remove conditioning codebooks from the mask\n            logging.debug(f\"updated mask with shape: {mask.shape}\")\n            # add z back into sampled z where the mask was false\n            sampled_z = torch.where(\n                mask.bool(), sampled_z, z_masked\n            )\n            logging.debug(f\"added z back into sampled z with shape: {sampled_z.shape}\")\n\n            # ignore any tokens that weren't masked\n            selected_probs = torch.where(\n                mask.bool(), selected_probs, torch.inf\n            )\n\n            # get the num tokens to mask, according to the schedule\n            num_to_mask = torch.floor(_gamma(r) * num_mask_tokens_at_start).unsqueeze(1).long()\n            logging.debug(f\"num to mask: {num_to_mask}\")\n\n            if i != (sampling_steps - 1):\n                num_to_mask = torch.maximum(\n                    torch.tensor(1),\n                    torch.minimum(\n                        mask.sum(dim=-1, keepdim=True) - 1,\n                        num_to_mask\n                    )\n                )\n\n\n            # get our new mask\n            mask = mask_by_random_topk(\n                num_to_mask, selected_probs, mask_temperature * (1-r)\n            )  \n\n            # update the mask\n            z_masked = torch.where(\n                mask.bool(), self.mask_token, sampled_z\n            )\n            logging.debug(f\"updated z_masked with shape: {z_masked.shape}\")\n\n            z_masked = codebook_unflatten(z_masked, n_infer_codebooks)\n            mask = codebook_unflatten(mask, n_infer_codebooks)\n            logging.debug(f\"unflattened z_masked with shape: {z_masked.shape}\")\n\n            # add conditioning codebooks back to z_masked\n            z_masked = torch.cat(\n                (z[:, :self.n_conditioning_codebooks, :], z_masked), dim=1\n            )\n            logging.debug(f\"added conditioning codebooks back to z_masked with shape: {z_masked.shape}\")\n\n\n        # add conditioning codebooks back to sampled_z\n        sampled_z = codebook_unflatten(sampled_z, n_infer_codebooks)\n        sampled_z = torch.cat(\n            (z[:, :self.n_conditioning_codebooks, :], sampled_z), dim=1\n        )\n\n        logging.debug(f\"finished sampling\")\n\n        if return_signal:\n            return self.to_signal(sampled_z, codec)\n        else:\n            return sampled_z", "\ndef sample_from_logits(\n        logits, \n        sample: bool = True,\n        temperature: float = 1.0,\n        top_k: int = None,\n        top_p: float = None,\n        typical_filtering: bool = False,\n        typical_mass: float = 0.2,\n        typical_min_tokens: int = 1,\n        return_probs: bool = False\n    ):\n    \"\"\"Convenience function to sample from a categorial distribution with input as\n    unnormalized logits.\n\n    Parameters\n    ----------\n    logits : Tensor[..., vocab_size]\n    config: SamplingConfig\n        The set of hyperparameters to be used for sampling\n        sample : bool, optional\n            Whether to perform multinomial sampling, by default True\n        temperature : float, optional\n            Scaling parameter when multinomial samping, by default 1.0\n        top_k : int, optional\n            Restricts sampling to only `top_k` values acc. to probability,\n            by default None\n        top_p : float, optional\n            Restricts sampling to only those values with cumulative\n            probability = `top_p`, by default None\n\n    Returns\n    -------\n    Tensor[...]\n        Sampled tokens\n    \"\"\"\n    shp = logits.shape[:-1]\n\n    if typical_filtering:\n        typical_filter(logits, \n                        typical_mass=typical_mass, \n                        typical_min_tokens=typical_min_tokens\n        )\n\n    # Apply top_k sampling\n    if top_k is not None:\n        v, _ = logits.topk(top_k)\n        logits[logits < v[..., [-1]]] = -float(\"inf\")\n\n    # Apply top_p (nucleus) sampling\n    if top_p is not None and top_p < 1.0:\n        v, sorted_indices = logits.sort(descending=True)\n        cumulative_probs = v.softmax(dim=-1).cumsum(dim=-1)\n\n        sorted_indices_to_remove = cumulative_probs > top_p\n        # Right shift indices_to_remove to keep 1st token over threshold\n        sorted_indices_to_remove = F.pad(sorted_indices_to_remove, (1, 0), value=False)[\n            ..., :-1\n        ]\n\n        # Compute indices_to_remove in unsorted array\n        indices_to_remove = sorted_indices_to_remove.scatter(\n            -1, sorted_indices, sorted_indices_to_remove\n        )\n\n        logits[indices_to_remove] = -float(\"inf\")\n\n    # Perform multinomial sampling after normalizing logits\n    probs = (\n        F.softmax(logits / temperature, dim=-1)\n        if temperature > 0\n        else logits.softmax(dim=-1)\n    )\n    token = (\n        probs.view(-1, probs.size(-1)).multinomial(1).squeeze(1).view(*shp)\n        if sample\n        else logits.argmax(-1)\n    )\n\n    if return_probs:\n        token_probs = probs.take_along_dim(token.unsqueeze(-1), dim=-1).squeeze(-1)\n        return token, token_probs\n    else:\n        return token", "    \n\n\ndef mask_by_random_topk(\n        num_to_mask: int, \n        probs: torch.Tensor, \n        temperature: float = 1.0, \n    ):\n    \"\"\"\n    Args:\n        num_to_mask (int): number of tokens to mask\n        probs (torch.Tensor): probabilities for each sampled event, shape (batch, seq)\n        temperature (float, optional): temperature. Defaults to 1.0.\n    \"\"\"\n    logging.debug(f\"masking by random topk\")\n    logging.debug(f\"num to mask: {num_to_mask}\")\n    logging.debug(f\"probs shape: {probs.shape}\")\n    logging.debug(f\"temperature: {temperature}\")\n    logging.debug(\"\")\n\n    noise = gumbel_noise_like(probs)\n    confidence = torch.log(probs) + temperature * noise\n    logging.debug(f\"confidence shape: {confidence.shape}\")\n\n    sorted_confidence, sorted_idx = confidence.sort(dim=-1)\n    logging.debug(f\"sorted confidence shape: {sorted_confidence.shape}\")\n    logging.debug(f\"sorted idx shape: {sorted_idx.shape}\")\n\n    # get the cut off threshold, given the mask length\n    cut_off = torch.take_along_dim(\n        sorted_confidence, num_to_mask, axis=-1\n    )\n    logging.debug(f\"cut off shape: {cut_off.shape}\")\n\n    # mask out the tokens\n    mask = confidence < cut_off\n    logging.debug(f\"mask shape: {mask.shape}\")\n\n    return mask", "\ndef typical_filter(\n        logits, \n        typical_mass: float = 0.95,\n        typical_min_tokens: int = 1,):\n    nb, nt, _ = logits.shape\n    x_flat = rearrange(logits, \"b t l -> (b t ) l\")\n    x_flat_norm = torch.nn.functional.log_softmax(x_flat, dim=-1)\n    x_flat_norm_p = torch.exp(x_flat_norm)\n    entropy = -(x_flat_norm * x_flat_norm_p).nansum(-1, keepdim=True)\n\n    c_flat_shifted = torch.abs((-x_flat_norm) - entropy)\n    c_flat_sorted, x_flat_indices = torch.sort(c_flat_shifted, descending=False)\n    x_flat_cumsum = (\n        x_flat.gather(-1, x_flat_indices).softmax(dim=-1).cumsum(dim=-1)\n    )\n\n    last_ind = (x_flat_cumsum < typical_mass).sum(dim=-1)\n    sorted_indices_to_remove = c_flat_sorted > c_flat_sorted.gather(\n        1, last_ind.view(-1, 1)\n    )\n    if typical_min_tokens > 1:\n        sorted_indices_to_remove[..., :typical_min_tokens] = 0\n    indices_to_remove = sorted_indices_to_remove.scatter(\n        1, x_flat_indices, sorted_indices_to_remove\n    )\n    x_flat = x_flat.masked_fill(indices_to_remove, -float(\"Inf\"))\n    logits = rearrange(x_flat, \"(b t) l -> b t l\", t=nt)\n    return logits", "\n\nif __name__ == \"__main__\":\n    # import argbind\n    from .layers import num_params\n\n    VampNet = argbind.bind(VampNet)\n\n    @argbind.bind(without_prefix=True)\n    def try_model(device: str = \"cuda\", batch_size: int = 2, seq_len_s: float = 10.0):\n        seq_len = int(32000 / 512 * seq_len_s)\n\n        model = VampNet().to(device)\n\n        z = torch.randint(\n            0, model.vocab_size, size=(batch_size, model.n_codebooks, seq_len)\n        ).to(device)\n\n        r = torch.zeros(batch_size).to(device)\n        \n        z_mask_latent = torch.rand(\n            batch_size, model.latent_dim * model.n_codebooks, seq_len\n        ).to(device)\n        z_hat = model(z_mask_latent)\n\n        pred = z_hat.argmax(dim=1)\n        pred = model.embedding.unflatten(pred, n_codebooks=model.n_predict_codebooks)\n\n        print(f\"model has {num_params(model)/1e6:<.3f}M parameters\")\n        print(f\"prediction has shape {pred.shape}\")\n        breakpoint()\n\n    args = argbind.parse_args()\n    with argbind.scope(args):\n        try_model()", "\n\n"]}
{"filename": "vampnet/modules/__init__.py", "chunked_list": ["import audiotools\n\naudiotools.ml.BaseModel.INTERN += [\"vampnet.modules.**\"]\naudiotools.ml.BaseModel.EXTERN += [\"einops\", \"flash_attn.flash_attention\", \"loralib\"]\n\nfrom .transformer import VampNet"]}
{"filename": "vampnet/modules/activations.py", "chunked_list": ["import math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\n\nclass NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo\n    (identical to OpenAI GPT). Also see the Gaussian Error Linear Units\n    paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n\n    def forward(self, x):\n        return (\n            0.5\n            * x\n            * (\n                1.0\n                + torch.tanh(\n                    math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))\n                )\n            )\n        )", "class NewGELU(nn.Module):\n    \"\"\"\n    Implementation of the GELU activation function currently in Google BERT repo\n    (identical to OpenAI GPT). Also see the Gaussian Error Linear Units\n    paper: https://arxiv.org/abs/1606.08415\n    \"\"\"\n\n    def forward(self, x):\n        return (\n            0.5\n            * x\n            * (\n                1.0\n                + torch.tanh(\n                    math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))\n                )\n            )\n        )", "\nclass GatedGELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.gelu = NewGELU()\n\n    def forward(self, x, dim: int = -1):\n        p1, p2 = x.chunk(2, dim=dim)\n        return p1 * self.gelu(p2)\n\nclass Snake1d(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.alpha = nn.Parameter(torch.ones(channels))\n\n    def forward(self, x):\n        return x + (self.alpha + 1e-9).reciprocal() * torch.sin(self.alpha * x).pow(2)", "\nclass Snake1d(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.alpha = nn.Parameter(torch.ones(channels))\n\n    def forward(self, x):\n        return x + (self.alpha + 1e-9).reciprocal() * torch.sin(self.alpha * x).pow(2)\n\ndef get_activation(name: str = \"relu\"):\n    if name == \"relu\":\n        return nn.ReLU\n    elif name == \"gelu\":\n        return NewGELU\n    elif name == \"geglu\":\n        return GatedGELU\n    elif name == \"snake\":\n        return Snake1d\n    else:\n        raise ValueError(f\"Unrecognized activation {name}\")", "\ndef get_activation(name: str = \"relu\"):\n    if name == \"relu\":\n        return nn.ReLU\n    elif name == \"gelu\":\n        return NewGELU\n    elif name == \"geglu\":\n        return GatedGELU\n    elif name == \"snake\":\n        return Snake1d\n    else:\n        raise ValueError(f\"Unrecognized activation {name}\")"]}
