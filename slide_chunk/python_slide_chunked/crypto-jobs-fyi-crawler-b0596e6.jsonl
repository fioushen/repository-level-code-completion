{"filename": "write_jobs_age.py", "chunked_list": ["import json\nfrom datetime import datetime\n\nwith open('jobs.json', 'r') as f:\n    jobs_json = json.load(f)\n    jobs_data = jobs_json.get('data', [])\n    print(f'Number of jobs: {len(jobs_data)}')\n\nwith open('jobs_age.json', 'r') as age_file:\n    age_data = json.load(age_file)\n    print(f'Number of jobs ages stored: {len(age_data)}')", "with open('jobs_age.json', 'r') as age_file:\n    age_data = json.load(age_file)\n    print(f'Number of jobs ages stored: {len(age_data)}')\n\ncurrent_jobs = {\"Total Jobs\": len(jobs_data)}\nfor j in jobs_data:\n    company_name = j.get('company')\n    current_jobs[company_name] = current_jobs.get(company_name, 0) + 1\n\nwith open(\"current.json\", \"w\") as file:\n    json.dump(current_jobs, file, indent=4)", "\nwith open(\"current.json\", \"w\") as file:\n    json.dump(current_jobs, file, indent=4)\n\nfor job in jobs_data:\n    link = job.get('link')\n    num = age_data.get(link, 0)\n    if num != 0:\n        age_data[link] = num + 1\n    else:\n        age_data[link] = 1", "\nprint(f'Number of jobs processed: {len(age_data)}')\nwith open('jobs_age.json', 'w') as file:\n    json.dump(age_data, file, indent=4)\n\n\ndef write_history(current_jobs_json: dict):\n    now = f'{datetime.date(datetime.now())}'\n    with open('history.json', 'r') as f:\n        jobs_data_json = json.load(f)\n    for c_job in current_jobs_json:\n        it = jobs_data_json.get(c_job, {})\n        it[now] = current_jobs_json[c_job]\n        jobs_data_json[c_job] = it\n    with open('history.json', 'w') as history_file:\n        json.dump(jobs_data_json, history_file, indent=4)", "\n\nwith open('current.json', 'r') as current_json_file:\n    c_data = json.load(current_json_file)\n    write_history(dict(c_data))\nwith open('companies.json', 'r') as companies_file:\n    companies_file_data = json.load(companies_file)\nprint('--- No jobs found for following companies: ---')\nfor c in companies_file_data:\n    c_name = c.get('company_name')\n    if c_data.get(c_name, 0) == 0:\n        print(f'Company: {c_name} Jobs board: {c.get(\"jobs_url\")}')", "for c in companies_file_data:\n    c_name = c.get('company_name')\n    if c_data.get(c_name, 0) == 0:\n        print(f'Company: {c_name} Jobs board: {c.get(\"jobs_url\")}')\n\nprint('^^^ No jobs found for the above companies: ^^^')\n"]}
{"filename": "crawler.py", "chunked_list": ["import time\nfrom datetime import datetime\n\nfrom selenium import webdriver\n\nfrom src.company_list import get_company_list\nfrom src.company_list import write_companies\n\ncompany_list = get_company_list()\nprint(f'[CRAWLER] Number of companies: {len(company_list)}')", "company_list = get_company_list()\nprint(f'[CRAWLER] Number of companies: {len(company_list)}')\nwrite_companies('companies.json')\n\nwith open('jobs.json', 'w') as f:\n    f.write('{}')\n\n# setup headless webdriver\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('--no-sandbox')", "chrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--disable-dev-shm-usage')\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--disable-extensions')\ndriver = webdriver.Chrome(options=chrome_options)\n\nn = 1\n\nfor company in company_list:\n    now = datetime.date(datetime.now())\n    st = time.time()\n    print(f'[CRAWLER] scrape {n} of {len(company_list)}')\n    n = n + 1\n    jobs_data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n    print(f'[CRAWLER] Company {company.company_name} has {len(jobs_data)} open positions on {now}')\n    print('[CRAWLER] Execution time:', (time.time() - st), 'seconds')", "\nfor company in company_list:\n    now = datetime.date(datetime.now())\n    st = time.time()\n    print(f'[CRAWLER] scrape {n} of {len(company_list)}')\n    n = n + 1\n    jobs_data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n    print(f'[CRAWLER] Company {company.company_name} has {len(jobs_data)} open positions on {now}')\n    print('[CRAWLER] Execution time:', (time.time() - st), 'seconds')\n", "\ndriver.close()\n"]}
{"filename": "crawler_empty.py", "chunked_list": ["import time\n\nfrom selenium import webdriver\n\nfrom src.company_list_empty import get_company_list\nfrom src.company_list_empty import write_companies\n\ncompany_list = get_company_list()\nprint(f'[CRAWLER] Number of companies: {len(company_list)}')\nwrite_companies('companies_empty.json')", "print(f'[CRAWLER] Number of companies: {len(company_list)}')\nwrite_companies('companies_empty.json')\n\nwith open('jobs_empty.json', 'w') as f:\n    f.write('{}')\n\n# setup headless webdriver\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--disable-dev-shm-usage')", "chrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--disable-dev-shm-usage')\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--disable-extensions')\ndriver = webdriver.Chrome(options=chrome_options)\n\ncurrent_jobs = {}\nn = 1\nfor company in company_list:\n    st = time.time()\n    print(f'[CRAWLER] scrape {n} of {len(company_list)}')\n    n = n + 1\n    jobs_data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n    print('[CRAWLER] Execution time:', (time.time() - st), 'seconds')", "for company in company_list:\n    st = time.time()\n    print(f'[CRAWLER] scrape {n} of {len(company_list)}')\n    n = n + 1\n    jobs_data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n    print('[CRAWLER] Execution time:', (time.time() - st), 'seconds')\n\ndriver.close()\n", ""]}
{"filename": "crawler_async.py", "chunked_list": ["from src.company_item import CompanyItem\nfrom src.scrape_ashbyhq import ScrapeAshbyhqAsync\nfrom src.scrape_lever import ScrapeLeverAsync\nfrom src.scrape_greenhouse import ScrapeGreenhouseAsync\nimport time\nfrom caqui import synchronous\nimport asyncio\n\nMAX_CONCURRENCY = 5  # number of WebDriver instances\nsem = asyncio.Semaphore(MAX_CONCURRENCY)", "MAX_CONCURRENCY = 5  # number of WebDriver instances\nsem = asyncio.Semaphore(MAX_CONCURRENCY)\n\n\nasync def __schedule_tasks(companies):\n    tasks = [asyncio.ensure_future(__collect_data(company)) for company in companies]\n    await asyncio.gather(*tasks)\n\n\nasync def __collect_data(company):", "\nasync def __collect_data(company):\n    async with sem:\n        driver_url = \"http://127.0.0.1:9515\"\n        capabilities = {\n            \"desiredCapabilities\": {\n                \"name\": \"webdriver\",\n                \"browserName\": \"firefox\",\n                \"marionette\": True,\n                \"acceptInsecureCerts\": True,", "                \"marionette\": True,\n                \"acceptInsecureCerts\": True,\n                # uncomment to set headless\n                \"goog:chromeOptions\": {\"extensions\": [], \"args\": [\"--headless\"]},\n            }\n        }\n        session = synchronous.get_session(driver_url, capabilities)\n        driver = [driver_url, session]\n\n        data = await company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n        for entry in data:\n            print(entry)", "\n        data = await company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n        for entry in data:\n            print(entry)\n\n        synchronous.close_session(*driver)\n\n\nasync def __schedule_tasks():\n    companies = [", "async def __schedule_tasks():\n    companies = [\n        CompanyItem('evmos', 'https://boards.eu.greenhouse.io/evmos', ScrapeGreenhouseAsync, 'https://evmos.org',\n                    'Cross-Chain Connectivity'),\n        CompanyItem('orderlynetwork', 'https://boards.greenhouse.io/orderlynetwork', ScrapeGreenhouseAsync,\n                    'https://orderly.network', 'Exchange'),\n        CompanyItem(\"0x\", \"https://boards.greenhouse.io/0x\", ScrapeGreenhouseAsync, \"https://0x.org\",\n                    \"Blockchain\"),\n        CompanyItem('econetwork', 'https://boards.greenhouse.io/econetwork', ScrapeGreenhouseAsync,\n                    'https://eco.com', 'Web3 wallet'),", "        CompanyItem('econetwork', 'https://boards.greenhouse.io/econetwork', ScrapeGreenhouseAsync,\n                    'https://eco.com', 'Web3 wallet'),\n        CompanyItem(\"bitcoin\", \"https://www.bitcoin.com/jobs/#joblist\", ScrapeGreenhouseAsync,\n                    \"https://www.bitcoin.com\", 'Exchange'),\n        CompanyItem('rain', 'https://jobs.ashbyhq.com/rain', ScrapeAshbyhqAsync, 'https://www.raincards.xyz',\n                    'Web3 cards'),\n        CompanyItem('exponential', 'https://jobs.ashbyhq.com/exponential', ScrapeAshbyhqAsync, 'https://exponential.fi',\n                    'DeFi'),\n        CompanyItem('conduit', 'https://jobs.ashbyhq.com/Conduit', ScrapeAshbyhqAsync, 'https://conduit.xyz',\n                    'Infrastructure'),", "        CompanyItem('conduit', 'https://jobs.ashbyhq.com/Conduit', ScrapeAshbyhqAsync, 'https://conduit.xyz',\n                    'Infrastructure'),\n        CompanyItem('kiln', 'https://jobs.ashbyhq.com/kiln.fi', ScrapeAshbyhqAsync, 'https://www.kiln.fi',\n                    'Staking & Infra'),\n        CompanyItem(\"flashbots\", \"https://jobs.ashbyhq.com/flashbots.net\", ScrapeAshbyhqAsync,\n                    \"https://www.flashbots.net\", \"ETH MEV\"),\n        CompanyItem('paradigm.xyz', 'https://jobs.ashbyhq.com/paradigm', ScrapeAshbyhqAsync, 'https://www.paradigm.xyz',\n                    'Web3 financing'),\n        CompanyItem('dune', 'https://jobs.ashbyhq.com/dune', ScrapeAshbyhqAsync, 'https://dune.com',\n                    'Web3 data'),", "        CompanyItem('dune', 'https://jobs.ashbyhq.com/dune', ScrapeAshbyhqAsync, 'https://dune.com',\n                    'Web3 data'),\n        CompanyItem(\"solanafoundation\", \"https://jobs.ashbyhq.com/Solana%20Foundation\", ScrapeAshbyhqAsync,\n                    \"https://solana.org\", \"Blockchain\"),\n        CompanyItem('syndica', 'https://jobs.ashbyhq.com/syndica', ScrapeAshbyhqAsync, 'https://syndica.io',\n                    'Infrastructure'),\n        CompanyItem('ellipsislabs', 'https://jobs.ashbyhq.com/ellipsislabs', ScrapeAshbyhqAsync,\n                    'https://ellipsislabs.xyz', 'Trading Protocol'),\n        CompanyItem(\"kraken\", \"https://jobs.lever.co/kraken\", ScrapeLeverAsync, \"https://kraken.com\", \"Exchange\"),\n        CompanyItem('arbitrumfoundation', 'https://jobs.lever.co/arbitrumfoundation', ScrapeLeverAsync,", "        CompanyItem(\"kraken\", \"https://jobs.lever.co/kraken\", ScrapeLeverAsync, \"https://kraken.com\", \"Exchange\"),\n        CompanyItem('arbitrumfoundation', 'https://jobs.lever.co/arbitrumfoundation', ScrapeLeverAsync,\n                    'https://arbitrum.foundation', 'Layer 2'),\n        CompanyItem(\"chainlink\", \"https://jobs.lever.co/chainlink\", ScrapeLeverAsync, \"https://chain.link\",\n                    \"Blockchain\"),\n        CompanyItem('ethglobal', 'https://jobs.lever.co/ETHGlobal', ScrapeLeverAsync, 'https://ethglobal.com',\n                    'Community'),\n        CompanyItem('multiversx', 'https://jobs.lever.co/multiversx', ScrapeLeverAsync, 'https://multiversx.com',\n                    'Blockchain'),\n        CompanyItem('sprucesystems', 'https://jobs.lever.co/sprucesystems', ScrapeLeverAsync, 'https://spruceid.com',", "                    'Blockchain'),\n        CompanyItem('sprucesystems', 'https://jobs.lever.co/sprucesystems', ScrapeLeverAsync, 'https://spruceid.com',\n                    'Web3 ID'),\n        CompanyItem('BlockSwap', 'https://jobs.lever.co/BlockSwap', ScrapeLeverAsync, 'https://www.blockswap.network',\n                    'Infra'),\n        CompanyItem('Metatheory', 'https://jobs.lever.co/Metatheory', ScrapeLeverAsync,\n                    'https://www.duskbreakers.gg', 'Web3 game'),\n        CompanyItem('axiomzen', 'https://jobs.lever.co/axiomzen', ScrapeLeverAsync, 'https://www.axiomzen.com', 'Web3'),\n        CompanyItem('fuellabs', 'https://jobs.lever.co/fuellabs', ScrapeLeverAsync, 'https://www.fuel.network',\n                    'Blockchain'),", "        CompanyItem('fuellabs', 'https://jobs.lever.co/fuellabs', ScrapeLeverAsync, 'https://www.fuel.network',\n                    'Blockchain'),\n        CompanyItem('harmony', 'https://jobs.lever.co/harmony', ScrapeLeverAsync, 'https://www.harmony.one',\n                    'Blockchain'),\n        CompanyItem('wintermute', 'https://jobs.lever.co/wintermute-trading', ScrapeLeverAsync,\n                    'https://www.wintermute.com',\n                    'Trading'),\n        CompanyItem(\"kaiko\", \"https://jobs.eu.lever.co/kaiko\", ScrapeLeverAsync, \"https://www.kaiko.com\", \"Data\"),\n        CompanyItem('bebop', 'https://jobs.lever.co/Bebop', ScrapeLeverAsync, 'https://bebop.xyz', 'DeFi Exchange'),\n        CompanyItem(\"Coinshift\", \"https://jobs.lever.co/Coinshift\", ScrapeLeverAsync, \"https://coinshift.xyz\",", "        CompanyItem('bebop', 'https://jobs.lever.co/Bebop', ScrapeLeverAsync, 'https://bebop.xyz', 'DeFi Exchange'),\n        CompanyItem(\"Coinshift\", \"https://jobs.lever.co/Coinshift\", ScrapeLeverAsync, \"https://coinshift.xyz\",\n                    \"Custody software\"),\n        CompanyItem(\"swissborg\", \"https://jobs.lever.co/swissborg\", ScrapeLeverAsync, \"https://swissborg.com\",\n                    \"Exchange\"),\n        CompanyItem(\"OpenSea\", \"https://jobs.lever.co/OpenSea\", ScrapeLeverAsync, \"https://opensea.io\", \"NFT\"),\n        CompanyItem(\"storyprotocol\", \"https://jobs.lever.co/storyprotocol\", ScrapeLeverAsync,\n                    \"https://www.storyprotocol.xyz\", \"Protocol\"),\n        CompanyItem(\"ethereumfoundation\", \"https://jobs.lever.co/ethereumfoundation\", ScrapeLeverAsync,\n                    \"https://ethereum.org\", \"Blockchain\"),", "        CompanyItem(\"ethereumfoundation\", \"https://jobs.lever.co/ethereumfoundation\", ScrapeLeverAsync,\n                    \"https://ethereum.org\", \"Blockchain\"),\n        CompanyItem(\"aave\", \"https://jobs.eu.lever.co/aave\", ScrapeLeverAsync, \"https://aave.com\", \"Protocol\"),\n        CompanyItem(\"crypto\", \"https://jobs.lever.co/crypto\", ScrapeLeverAsync, \"https://crypto.com\", \"Exchange\"),\n        CompanyItem(\"Luxor\", \"https://jobs.lever.co/LuxorTechnology\", ScrapeLeverAsync, \"https://www.luxor.tech\",\n                    \"Mining\"),\n        CompanyItem(\"anchorage\", \"https://jobs.lever.co/anchorage\", ScrapeLeverAsync, \"https://www.anchorage.com\",\n                    \"Trading\"),\n        CompanyItem(\"biconomy\", \"https://jobs.lever.co/biconomy\", ScrapeLeverAsync, \"https://www.biconomy.io\",\n                    \"Infra\"),", "        CompanyItem(\"biconomy\", \"https://jobs.lever.co/biconomy\", ScrapeLeverAsync, \"https://www.biconomy.io\",\n                    \"Infra\"),\n        CompanyItem('enso', 'https://jobs.lever.co/Enso', ScrapeLeverAsync, 'https://www.enso.finance', 'DeFi'),\n        CompanyItem(\"Polygon\", \"https://jobs.lever.co/Polygon\", ScrapeLeverAsync, \"https://polygon.technology\",\n                    \"Blockchain\"),\n        CompanyItem(\"tokenmetrics\", \"https://jobs.lever.co/tokenmetrics\", ScrapeLeverAsync,\n                    \"https://www.tokenmetrics.com\", \"Information\"),\n        CompanyItem(\"offchainlabs\", \"https://jobs.lever.co/offchainlabs\", ScrapeLeverAsync,\n                    \"https://offchainlabs.com\", \"Protocol\"),\n        CompanyItem(\"subspacelabs\", \"https://jobs.lever.co/subspacelabs\", ScrapeLeverAsync,", "                    \"https://offchainlabs.com\", \"Protocol\"),\n        CompanyItem(\"subspacelabs\", \"https://jobs.lever.co/subspacelabs\", ScrapeLeverAsync,\n                    \"https://subspace.network\", \"Blockchain Infra\"),\n        CompanyItem('3boxlabs', 'https://jobs.lever.co/3box', ScrapeLeverAsync, 'https://3boxlabs.com',\n                    'Infra'),\n        CompanyItem(\"ramp.network\", \"https://jobs.lever.co/careers.ramp.network\", ScrapeLeverAsync,\n                    \"https://ramp.network\", \"Payments\"),\n        CompanyItem('risklabs', 'https://jobs.lever.co/risklabs', ScrapeLeverAsync, 'https://risklabs.foundation',\n                    'Protocol'),\n        CompanyItem('celestia', 'https://jobs.lever.co/celestia', ScrapeLeverAsync, 'https://celestia.org',", "                    'Protocol'),\n        CompanyItem('celestia', 'https://jobs.lever.co/celestia', ScrapeLeverAsync, 'https://celestia.org',\n                    'Modular Blockchain'),\n        CompanyItem('polymerlabs', 'https://jobs.lever.co/polymerlabs', ScrapeLeverAsync, 'https://www.polymerlabs.org',\n                    'Modular Blockchain'),\n        CompanyItem('royal', 'https://jobs.lever.co/Royal', ScrapeLeverAsync, 'https://royal.io', 'Web3 + Music'),\n        CompanyItem('gauntlet', 'https://jobs.lever.co/gauntlet', ScrapeLeverAsync, 'https://gauntlet.network',\n                    'Web3 + Financial Modelling'),\n        CompanyItem(\"ledger\", \"https://jobs.lever.co/ledger\", ScrapeLeverAsync, \"https://www.ledger.com\", \"Wallet\"),\n        CompanyItem(\"request\", \"https://jobs.lever.co/request\", ScrapeLeverAsync, \"https://request.network\",", "        CompanyItem(\"ledger\", \"https://jobs.lever.co/ledger\", ScrapeLeverAsync, \"https://www.ledger.com\", \"Wallet\"),\n        CompanyItem(\"request\", \"https://jobs.lever.co/request\", ScrapeLeverAsync, \"https://request.network\",\n                    \"Payments\"),\n        CompanyItem(\"immutable\", \"https://jobs.lever.co/immutable\", ScrapeLeverAsync, \"https://www.immutable.com\",\n                    \"NFT\"),\n        CompanyItem(\"web3auth\", \"https://jobs.lever.co/TorusLabs\", ScrapeLeverAsync, \"https://web3auth.io\", \"Auth\"),\n        CompanyItem(\"cere-network\", \"https://jobs.lever.co/cere-network\", ScrapeLeverAsync, \"https://cere.network\",\n                    \"Infra\"),\n        CompanyItem('matterlabs', 'https://jobs.eu.lever.co/matterlabs', ScrapeLeverAsync, 'https://matter-labs.io',\n                    'Protocol'),", "        CompanyItem('matterlabs', 'https://jobs.eu.lever.co/matterlabs', ScrapeLeverAsync, 'https://matter-labs.io',\n                    'Protocol'),\n        CompanyItem(\"hiro\", \"https://jobs.lever.co/hiro\", ScrapeLeverAsync, \"https://www.hiro.so\", \"Infra\"),\n        CompanyItem('AQX', 'https://jobs.lever.co/presto', ScrapeLeverAsync, 'https://aqx.com', 'Exchange and Web3'),\n        CompanyItem('ultra', 'https://jobs.lever.co/ultra', ScrapeLeverAsync,\n                    'https://ultra.io', 'Web3 Gaming'),\n        CompanyItem('bitwise', 'https://jobs.lever.co/bitwiseinvestments', ScrapeLeverAsync,\n                    'https://bitwiseinvestments.com', 'Asset Management'),\n    ]\n    tasks = [asyncio.ensure_future(__collect_data(company)) for company in companies]", "    ]\n    tasks = [asyncio.ensure_future(__collect_data(company)) for company in companies]\n    await asyncio.gather(*tasks)\n\n\nif __name__ == \"__main__\":\n    start = time.time()\n    loop = asyncio.get_event_loop()\n    try:\n        loop.run_until_complete(__schedule_tasks())\n    finally:\n        loop.run_until_complete(loop.shutdown_asyncgens())\n        loop.close()\n        end = time.time()\n        print(f\"Time: {end - start:.2f} sec\")", "        # 66.46 sec for 56 company names\n"]}
{"filename": "test/test_consensys_scraper.py", "chunked_list": ["from selenium import webdriver\nfrom src.scrape_consensys import ScrapeConsensys\n\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)\njobs = ScrapeConsensys().getJobs(driver, \"https://consensys.net/open-roles\")\nfor job in jobs:\n    print(job)", "for job in jobs:\n    print(job)\n\ndriver.close()\n"]}
{"filename": "test/test_ashbyhq_scraper.py", "chunked_list": ["from selenium import webdriver\nfrom src.company_item import CompanyItem\nfrom src.scrape_ashbyhq import ScrapeAshbyhq\n\nimport time \nstart = time.time()\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)", "options.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)\ncompanies = [\n    CompanyItem('flashbots', 'https://jobs.ashbyhq.com/flashbots.net', ScrapeAshbyhq, '', ''),\n    CompanyItem('rain', 'https://jobs.ashbyhq.com/rain', ScrapeAshbyhq, 'https://www.raincards.xyz', 'Web3 cards'),\n    CompanyItem('exponential', 'https://jobs.ashbyhq.com/exponential', ScrapeAshbyhq, 'https://exponential.fi', 'DeFi'),\n    CompanyItem('kiln', 'https://jobs.ashbyhq.com/kiln.fi', ScrapeAshbyhq, 'https://www.kiln.fi', 'Staking'),\n    CompanyItem('dune', 'https://jobs.ashbyhq.com/dune', ScrapeAshbyhq, 'https://dune.com',\n                'Web3 data'),\n    CompanyItem('conduit', 'https://jobs.ashbyhq.com/Conduit', ScrapeAshbyhq, 'https://conduit.xyz',", "                'Web3 data'),\n    CompanyItem('conduit', 'https://jobs.ashbyhq.com/Conduit', ScrapeAshbyhq, 'https://conduit.xyz',\n                'Infrastructure'),\n    CompanyItem('paradigm.xyz', 'https://jobs.ashbyhq.com/paradigm', ScrapeAshbyhq, 'https://www.paradigm.xyz',\n                'Web3 data'),\n    CompanyItem('syndica', 'https://jobs.ashbyhq.com/syndica', ScrapeAshbyhq, 'https://www.sygnum.com',\n                'Crypto bank'),\n    CompanyItem('solana-foundation', 'https://jobs.ashbyhq.com/Solana%20Foundation', ScrapeAshbyhq,\n                'https://www.sygnum.com',\n                'Crypto bank'),", "                'https://www.sygnum.com',\n                'Crypto bank'),\n    CompanyItem('ellipsislabs', 'https://jobs.ashbyhq.com/ellipsislabs', ScrapeAshbyhq,\n                'https://ellipsislabs.xyz', 'Trading Protocol')\n]\n\nfor company in companies:\n    data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n    for entry in data:\n        print(entry)", "\ndriver.close()\n\nend = time.time()\nprint(f\"Time: {end-start:.2f} sec\")"]}
{"filename": "test/test_any_scraper.py", "chunked_list": ["import unittest\n\nfrom selenium import webdriver\nfrom src.company_item import CompanyItem\nfrom src.scrape_ashbyhq import ScrapeAshbyhq\nfrom src.scrape_greenhouse import ScrapeGreenhouse\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)", "options.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)\n\ncompany_list = [\n    CompanyItem('Blockworks', 'https://jobs.ashbyhq.com/Blockworks', ScrapeAshbyhq,\n                'https://blockworks.co', 'Web3 News'),\n    CompanyItem('21co', 'https://boards.greenhouse.io/21co', ScrapeGreenhouse,\n                'https://www.21.co', 'Web3 DeFi ETP'),\n    CompanyItem('xapo', 'https://boards.greenhouse.io/xapo61', ScrapeGreenhouse,\n                'https://www.xapobank.com', 'Web3 bank')", "    CompanyItem('xapo', 'https://boards.greenhouse.io/xapo61', ScrapeGreenhouse,\n                'https://www.xapobank.com', 'Web3 bank')\n]\n\nfor company in company_list:\n    jobs_data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n    for entry in jobs_data:\n        print(entry)\n\ndriver.close()", "\ndriver.close()\n\n\nclass TestScraper(unittest.TestCase):\n    def test_upper(self):\n        self.assertGreater(len(jobs_data), 1)\n\n\nif __name__ == '__main__':\n    unittest.main()", "\nif __name__ == '__main__':\n    unittest.main()\n"]}
{"filename": "test/test_company_list.py", "chunked_list": ["import json\nfrom src.company_list import get_company_list\nfrom src.company_list import get_company\n\ncompany_list = get_company_list()\nprint(f'Number of companies: {len(company_list)}')\nresult_list = []\nfor company in company_list:\n    company_item = {\n        \"company_name\": company.company_name,\n        \"company_url\": company.company_url,\n        \"jobs_url\": company.jobs_url,\n    }\n    result_list.append(company_item)", "print(f'Number of companies in JSON: {len(result_list)}')\nwith open('companies.json', 'w') as f:\n    json.dump(result_list, f, indent=4)\n\nprint(get_company('kraken'))\n"]}
{"filename": "test/test_greenhouse_scraper.py", "chunked_list": ["from selenium import webdriver\nfrom src.company_item import CompanyItem\nfrom src.scrape_greenhouse import ScrapeGreenhouse\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)\n\ncompany_list = [\n    CompanyItem('grayscaleinvestments', 'https://boards.greenhouse.io/grayscaleinvestments', ScrapeGreenhouse,", "company_list = [\n    CompanyItem('grayscaleinvestments', 'https://boards.greenhouse.io/grayscaleinvestments', ScrapeGreenhouse,\n                'https://grayscale.com', 'Web3 Asset Manager'),\n    CompanyItem('dragonflycapital', 'https://boards.greenhouse.io/dragonflycapital', ScrapeGreenhouse,\n                'https://www.dragonfly.xyz', 'Web3 funding'),\n    CompanyItem('penumbralabs', 'https://boards.greenhouse.io/penumbralabs', ScrapeGreenhouse,\n                'https://eco.com', 'Web3 trading'),\n    CompanyItem('econetwork', 'https://boards.greenhouse.io/econetwork', ScrapeGreenhouse,\n                'https://eco.com', 'Web3 wallet'),\n    CompanyItem('outlierventures', 'https://boards.eu.greenhouse.io/outlierventures', ScrapeGreenhouse,", "                'https://eco.com', 'Web3 wallet'),\n    CompanyItem('outlierventures', 'https://boards.eu.greenhouse.io/outlierventures', ScrapeGreenhouse,\n                'https://outlierventures.io', 'Web3 Ventures'),\n    CompanyItem('evmos', 'https://boards.eu.greenhouse.io/evmos', ScrapeGreenhouse, 'https://evmos.org',\n                'Cross-Chain Connectivity'),\n    CompanyItem('magic', 'https://boards.greenhouse.io/magic', ScrapeGreenhouse, 'https://magic.link', 'Web3 Wallets'),\n    CompanyItem('trmlabs', 'https://www.trmlabs.com/careers-list', ScrapeGreenhouse,\n                'https://www.trmlabs.com', 'Web3 Information'),\n    CompanyItem('foundrydigital', 'https://boards.greenhouse.io/foundrydigital', ScrapeGreenhouse,\n                'https://foundrydigital.com', 'Web3'),", "    CompanyItem('foundrydigital', 'https://boards.greenhouse.io/foundrydigital', ScrapeGreenhouse,\n                'https://foundrydigital.com', 'Web3'),\n    CompanyItem('o1labs', 'https://boards.greenhouse.io/o1labs', ScrapeGreenhouse, 'https://o1labs.org',\n                'Web3'),\n    CompanyItem('orderlynetwork', 'https://boards.greenhouse.io/orderlynetwork', ScrapeGreenhouse,\n                'https://orderly.network', 'Exchange'),\n    CompanyItem('paradigm.co', 'https://boards.greenhouse.io/paradigm62', ScrapeGreenhouse, 'https://www.paradigm.co',\n                'Liquidity'),\n    CompanyItem('immunefi', 'https://boards.greenhouse.io/immunefi', ScrapeGreenhouse, 'https://immunefi.com',\n                'Bug bounty platform'),", "    CompanyItem('immunefi', 'https://boards.greenhouse.io/immunefi', ScrapeGreenhouse, 'https://immunefi.com',\n                'Bug bounty platform'),\n    CompanyItem('protocollabs', 'https://boards.greenhouse.io/protocollabs', ScrapeGreenhouse, 'https://protocol.ai/about',\n                'Web3 IPFS research platform'),\n    CompanyItem('taxbit', 'https://boards.greenhouse.io/taxbit', ScrapeGreenhouse, 'https://taxbit.com', 'Accounting'),\n    CompanyItem('osmosisdex', 'https://boards.greenhouse.io/osmosisdex', ScrapeGreenhouse, 'https://osmosis.zone',\n                'Exchange'),\n    CompanyItem('stellar', 'https://boards.greenhouse.io/stellar', ScrapeGreenhouse, 'https://stellar.org',\n                'Blockchain'),\n    CompanyItem('bitfury', 'https://boards.greenhouse.io/bitfury', ScrapeGreenhouse, 'https://bitfury.com',", "                'Blockchain'),\n    CompanyItem('bitfury', 'https://boards.greenhouse.io/bitfury', ScrapeGreenhouse, 'https://bitfury.com',\n                'Web3'),\n    CompanyItem('mobilecoin', 'https://boards.greenhouse.io/mobilecoin', ScrapeGreenhouse,\n                'https://mobilecoin.com', 'Blockchain'),\n    CompanyItem('chia', 'https://www.chia.net/careers', ScrapeGreenhouse,\n                'https://www.chia.net', 'Blockchain'),\n    CompanyItem('okcoin', 'https://boards.greenhouse.io/okcoin', ScrapeGreenhouse, 'https://www.okcoin.com',\n                'Exchange'),\n    CompanyItem(\"solanafoundation\", \"https://boards.greenhouse.io/solanafoundation\", ScrapeGreenhouse,", "                'Exchange'),\n    CompanyItem(\"solanafoundation\", \"https://boards.greenhouse.io/solanafoundation\", ScrapeGreenhouse,\n                \"https://solana.org\", \"Blockchain\"),\n    CompanyItem(\"worldcoinorg\", \"https://boards.greenhouse.io/worldcoinorg\", ScrapeGreenhouse,\n                \"https://worldcoin.org\", \"Blockchain\"),\n    CompanyItem(\"edgeandnode\", \"https://boards.greenhouse.io/edgeandnode\", ScrapeGreenhouse,\n                \"https://edgeandnode.com\", \"Infra\"),\n    CompanyItem(\"clearmatics\", \"https://boards.greenhouse.io/clearmatics\", ScrapeGreenhouse,\n                \"https://www.clearmatics.com\", \"Protocol\"),\n    CompanyItem(\"aztec\", \"https://boards.eu.greenhouse.io/aztec\", ScrapeGreenhouse, \"https://aztec.network\",", "                \"https://www.clearmatics.com\", \"Protocol\"),\n    CompanyItem(\"aztec\", \"https://boards.eu.greenhouse.io/aztec\", ScrapeGreenhouse, \"https://aztec.network\",\n                \"Protocol\"),\n    CompanyItem(\"avalabs\", \"https://boards.greenhouse.io/avalabs\", ScrapeGreenhouse,\n                \"https://www.avalabs.org\", \"Blockchain\"),\n    CompanyItem(\"galaxydigitalservices\", \"https://boards.greenhouse.io/galaxydigitalservices\",\n                ScrapeGreenhouse, \"https://www.galaxy.com\", 'Trading'),\n    CompanyItem(\"bittrex\", \"https://boards.greenhouse.io/bittrex\", ScrapeGreenhouse,\n                \"https://global.bittrex.com\", 'Exchange'),\n    CompanyItem(\"bitcoin\", \"https://www.bitcoin.com/jobs/#joblist\", ScrapeGreenhouse,", "                \"https://global.bittrex.com\", 'Exchange'),\n    CompanyItem(\"bitcoin\", \"https://www.bitcoin.com/jobs/#joblist\", ScrapeGreenhouse,\n                \"https://www.bitcoin.com\", 'Exchange'),\n    CompanyItem(\"EigenLabs\", \"https://boards.greenhouse.io/layrlabs\", ScrapeGreenhouse,\n                \"https://www.v1.eigenlayer.xyz\", \"Infra\"),\n    CompanyItem(\"kadena\", \"https://boards.greenhouse.io/kadenallc\", ScrapeGreenhouse, \"https://kadena.io\",\n                \"PoW chain\"),\n    CompanyItem(\"poap\", \"https://boards.greenhouse.io/poaptheproofofattendanceprotocol\", ScrapeGreenhouse,\n                \"https://poap.xyz\", \"Protocol\"),\n    CompanyItem(\"chainsafesystems\", \"https://boards.greenhouse.io/chainsafesystems\", ScrapeGreenhouse,", "                \"https://poap.xyz\", \"Protocol\"),\n    CompanyItem(\"chainsafesystems\", \"https://boards.greenhouse.io/chainsafesystems\", ScrapeGreenhouse,\n                \"https://chainsafe.io\", \"Infra\"),\n    CompanyItem(\"status\", \"https://jobs.status.im\", ScrapeGreenhouse, \"https://status.im\", \"Messanger\"),\n    CompanyItem(\"digitalasset\", \"https://boards.greenhouse.io/digitalasset\", ScrapeGreenhouse,\n                \"https://www.digitalasset.com\", \"Custody\"),\n    CompanyItem(\"copperco\", \"https://boards.eu.greenhouse.io/copperco\", ScrapeGreenhouse,\n                \"https://copper.co\", \"Custody\"),\n    CompanyItem(\"messari\", \"https://boards.greenhouse.io/messari\", ScrapeGreenhouse, \"https://messari.io\",\n                \"Information\"),", "    CompanyItem(\"messari\", \"https://boards.greenhouse.io/messari\", ScrapeGreenhouse, \"https://messari.io\",\n                \"Information\"),\n    CompanyItem(\"layerzerolabs\", \"https://boards.greenhouse.io/layerzerolabs\", ScrapeGreenhouse,\n                \"https://layerzero.network\", \"Infra\"),\n    CompanyItem(\"jumpcrypto\", \"https://boards.greenhouse.io/jumpcrypto\", ScrapeGreenhouse,\n                \"https://jumpcrypto.com\", \"Infra\"),\n    CompanyItem(\"oasisnetwork\", \"https://boards.greenhouse.io/oasisnetwork\", ScrapeGreenhouse,\n                \"https://oasisprotocol.org\", \"Protocol\")]\n\nfor company in company_list:\n    jobs_data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n    for entry in jobs_data:\n        print(entry)", "\nfor company in company_list:\n    jobs_data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n    for entry in jobs_data:\n        print(entry)\n\ndriver.close()\n"]}
{"filename": "test/test_bamboohr_scraper.py", "chunked_list": ["from selenium import webdriver\nfrom src.company_item import CompanyItem\nfrom src.scrape_bamboohr import ScrapeBamboohr\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)\ncompany_list = [\n    CompanyItem('wirex', 'https://wirex.bamboohr.com/careers', ScrapeBamboohr, 'https://wirexapp.com', 'Web3 card'),\n    CompanyItem('sygnum', 'https://sygnum.bamboohr.com/careers', ScrapeBamboohr, 'https://www.sygnum.com',", "    CompanyItem('wirex', 'https://wirex.bamboohr.com/careers', ScrapeBamboohr, 'https://wirexapp.com', 'Web3 card'),\n    CompanyItem('sygnum', 'https://sygnum.bamboohr.com/careers', ScrapeBamboohr, 'https://www.sygnum.com',\n                'Crypto bank'),\n    CompanyItem('iofinnet', 'https://iofinnethr.bamboohr.com/jobs/?source=bamboohr', ScrapeBamboohr,\n                'https://www.iofinnet.com', 'Custody'),\n    CompanyItem('web3', 'https://web3.bamboohr.com/jobs', ScrapeBamboohr, 'https://web3.foundation',\n                'web3'),\n    CompanyItem('dappradar', 'https://dappradar.bamboohr.com/careers', ScrapeBamboohr,\n                'https://dappradar.com', 'Exchange & NFT'),\n    CompanyItem(\"cexio\", \"https://cexio.bamboohr.com/jobs\", ScrapeBamboohr, \"https://cex.io\", \"Exchange\"),", "                'https://dappradar.com', 'Exchange & NFT'),\n    CompanyItem(\"cexio\", \"https://cexio.bamboohr.com/jobs\", ScrapeBamboohr, \"https://cex.io\", \"Exchange\"),\n    CompanyItem(\"chainstack\", \"https://chainstack.bamboohr.com/jobs\", ScrapeBamboohr,\n                \"https://chainstack.com\", \"Infra\")\n]\n\nfor company in company_list:\n    print(company.jobs_url)\n    data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n    for entry in data:\n        print(entry)", "\ndriver.close()\n"]}
{"filename": "test/test_lever_scraper.py", "chunked_list": ["from selenium import webdriver\nfrom src.company_item import CompanyItem\nfrom src.scrape_lever import ScrapeLever\n\ncompany_list = [\n    CompanyItem('ethenalabs', 'https://jobs.lever.co/ethenalabs', ScrapeLever,\n                'https://www.ethena.fi', 'Web3 bonds'),\n    CompanyItem('arbitrumfoundation', 'https://jobs.lever.co/arbitrumfoundation', ScrapeLever,\n                'https://arbitrum.foundation', 'Layer 2'),\n    CompanyItem('3boxlabs', 'https://jobs.lever.co/3box', ScrapeLever, 'https://3boxlabs.com',", "                'https://arbitrum.foundation', 'Layer 2'),\n    CompanyItem('3boxlabs', 'https://jobs.lever.co/3box', ScrapeLever, 'https://3boxlabs.com',\n                'Infra'),\n    CompanyItem('BlockSwap', 'https://jobs.lever.co/BlockSwap', ScrapeLever, 'https://www.blockswap.network',\n                'Infra'),\n    CompanyItem('wintermute', 'https://jobs.lever.co/wintermute-trading', ScrapeLever, 'https://www.wintermute.com',\n                'Trading'),\n    CompanyItem('sprucesystems', 'https://jobs.lever.co/sprucesystems', ScrapeLever, 'https://spruceid.com',\n                'Web3 ID'),\n    CompanyItem('royal', 'https://jobs.lever.co/Royal', ScrapeLever, 'https://royal.io', 'Web3 + Music'),", "                'Web3 ID'),\n    CompanyItem('royal', 'https://jobs.lever.co/Royal', ScrapeLever, 'https://royal.io', 'Web3 + Music'),\n    CompanyItem('enso', 'https://jobs.lever.co/Enso', ScrapeLever, 'https://www.enso.finance', 'DeFi'),\n    CompanyItem('gauntlet', 'https://jobs.lever.co/gauntlet', ScrapeLever, 'https://gauntlet.network',\n                'Web3 + Financial Modelling'),\n    CompanyItem('AQX', 'https://jobs.lever.co/presto', ScrapeLever, 'https://aqx.com', 'Exchange and Web3'),\n    CompanyItem('multiversx', 'https://jobs.lever.co/multiversx', ScrapeLever, 'https://multiversx.com', 'Blockchain'),\n    CompanyItem('matterlabs', 'https://jobs.eu.lever.co/matterlabs', ScrapeLever, 'https://matter-labs.io', 'Protocol'),\n    CompanyItem('fuellabs', 'https://jobs.lever.co/fuellabs', ScrapeLever, 'https://www.fuel.network', 'Blockchain'),\n    CompanyItem(\"Luxor\", \"https://jobs.lever.co/LuxorTechnology\", ScrapeLever, \"https://www.luxor.tech\", \"Mining\"),", "    CompanyItem('fuellabs', 'https://jobs.lever.co/fuellabs', ScrapeLever, 'https://www.fuel.network', 'Blockchain'),\n    CompanyItem(\"Luxor\", \"https://jobs.lever.co/LuxorTechnology\", ScrapeLever, \"https://www.luxor.tech\", \"Mining\"),\n    CompanyItem(\"anchorage\", \"https://jobs.lever.co/anchorage\", ScrapeLever, \"https://www.anchorage.com\", \"Trading\"),\n    CompanyItem(\"biconomy\", \"https://jobs.lever.co/biconomy\", ScrapeLever, \"https://www.biconomy.io\", \"Infra\"),\n    CompanyItem(\"kraken\", \"https://jobs.lever.co/kraken\", ScrapeLever, \"https://kraken.com\", \"Exchange\"),\n    CompanyItem(\"chainlink\", \"https://jobs.lever.co/chainlink\", ScrapeLever, \"https://chain.link\", \"Blockchain\"),\n    CompanyItem(\"hiro\", \"https://jobs.lever.co/hiro\", ScrapeLever, \"https://www.hiro.so\", \"Infra\"),\n    CompanyItem(\"kaiko\", \"https://jobs.eu.lever.co/kaiko\", ScrapeLever, \"https://www.kaiko.com\", \"Data\"),\n    CompanyItem(\"tessera\", \"https://jobs.lever.co/ftc\", ScrapeLever, \"https://tessera.co\", \"NFT\"),\n    CompanyItem(\"cere-network\", \"https://jobs.lever.co/cere-network\", ScrapeLever, \"https://cere.network\", \"Infra\"),", "    CompanyItem(\"tessera\", \"https://jobs.lever.co/ftc\", ScrapeLever, \"https://tessera.co\", \"NFT\"),\n    CompanyItem(\"cere-network\", \"https://jobs.lever.co/cere-network\", ScrapeLever, \"https://cere.network\", \"Infra\"),\n    CompanyItem(\"ramp.network\", \"https://jobs.lever.co/careers.ramp.network\", ScrapeLever, \"https://ramp.network\",\n                \"Payments\"),\n    CompanyItem(\"ledger\", \"https://jobs.lever.co/ledger\", ScrapeLever, \"https://www.ledger.com\", \"Wallet\"),\n    CompanyItem(\"request\", \"https://jobs.lever.co/request\", ScrapeLever, \"https://request.network\", \"Payments\"),\n    CompanyItem(\"immutable\", \"https://jobs.lever.co/immutable\", ScrapeLever, \"https://www.immutable.com\", \"NFT\"),\n    CompanyItem(\"web3auth\", \"https://jobs.lever.co/TorusLabs\", ScrapeLever, \"https://web3auth.io\", \"Auth\")]\n\noptions = webdriver.ChromeOptions()", "\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)\n\nfor company in company_list:\n    data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n    for entry in data:\n        print(entry)\n", "\ndriver.close()\n"]}
{"filename": "test/test_recruitee_scraper.py", "chunked_list": ["from selenium import webdriver\nfrom src.company_item import CompanyItem\nfrom src.scrape_recruitee import ScrapeRecruitee\n\n\ncompanies = [\n    CompanyItem(\"ramp.network\", \"https://metrika.recruitee.com\", ScrapeRecruitee, \"https://ramp.network\", \"Payments\"),\n    CompanyItem(\"tether\", \"https://tether.recruitee.com\", ScrapeRecruitee, \"https://tether.to/en\", \"Stable Coin\")\n]\n", "]\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)\n\nfor company in companies:\n    print(company.jobs_url)\n    data = company.scraper_type().getJobs(driver, company.jobs_url)\n    for entry in data:\n        print(entry)", "\ndriver.close()\n"]}
{"filename": "test/test_paxos_scraper.py", "chunked_list": ["from selenium import webdriver\nfrom src.scrape_paxos import ScrapePaxos\n\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)\nurl1 = \"https://paxos.com/job-posts/?_sft_department=compliance,engineering,finance-accounting,hr-talent,information-technology,legal,operations,product&_sft_office=us\"\nurl2 = url1 + \"&sf_paged=2\"\njobs = ScrapePaxos().getJobs(driver, url1)\nfor job in jobs:\n    print(job)", "url2 = url1 + \"&sf_paged=2\"\njobs = ScrapePaxos().getJobs(driver, url1)\nfor job in jobs:\n    print(job)\n\ndriver.close()\n"]}
{"filename": "test/test_jobs_reader.py", "chunked_list": ["import json\n\nwith open('jobs.json', 'r') as f:\n    jobs_json = json.load(f)\n    jobs_data = jobs_json.get('data', [])\n    print(len(jobs_data))\n\nprint(jobs_data[0])\n\ndata = list(filter(lambda jd: jd.get('company') == 'wintermute', jobs_data))", "\ndata = list(filter(lambda jd: jd.get('company') == 'wintermute', jobs_data))\nprint(data)\n\n# write age\nwith open('age.json', 'r') as age_file:\n    age_data = json.load(age_file)\nprint(age_data)\n\nfor job in jobs_data:\n    link = job.get('link')\n    print(link)\n    num = age_data.get(link, 0)\n    if num != 0:\n        age_data[link] = num+1\n    else:\n        age_data[link] = 1", "\nfor job in jobs_data:\n    link = job.get('link')\n    print(link)\n    num = age_data.get(link, 0)\n    if num != 0:\n        age_data[link] = num+1\n    else:\n        age_data[link] = 1\n\nwith open('age.json', 'w') as file:\n    json.dump(age_data, file, indent=4)", "\nwith open('age.json', 'w') as file:\n    json.dump(age_data, file, indent=4)\n"]}
{"filename": "test/test_binance_scraper.py", "chunked_list": ["from selenium import webdriver\nfrom src.scrape_binance import ScrapeBinance\n\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)\njobs = ScrapeBinance().getJobs(driver, \"https://www.binance.com/en/careers/job-openings\")\nfor job in jobs:\n    print(job)", "for job in jobs:\n    print(job)\n\ndriver.close()\n"]}
{"filename": "test/test_smartrecruiters_scraper.py", "chunked_list": ["from selenium import webdriver\nfrom src.scrape_smartrecruiters import ScrapeSmartrecruiters\n\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)\njobs = ScrapeSmartrecruiters().getJobs(driver, \"https://careers.smartrecruiters.com/B6/coinmarketcap\", 'xxx')\nfor job in jobs:\n    print(job)", "for job in jobs:\n    print(job)\n\ndriver.close()\n"]}
{"filename": "test/test_ripple_scraper.py", "chunked_list": ["from selenium import webdriver\nfrom src.scrape_ripple import ScrapeRipple\n\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)\njobs = ScrapeRipple().getJobs(driver, \"https://ripple.com/careers/all-jobs\")\nfor job in jobs:\n    print(job)", "for job in jobs:\n    print(job)\n\ndriver.close()\n"]}
{"filename": "test/test_workable_scraper.py", "chunked_list": ["from selenium import webdriver\nfrom src.company_item import CompanyItem\nfrom src.scrape_workable import ScrapeWorkable\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\ndriver = webdriver.Chrome(options=options)\ncompany_list = [\n    CompanyItem('dydxopsdao', 'https://apply.workable.com/dydx-operations-trust', ScrapeWorkable,\n                'https://dydxopsdao.com', 'Web3 DAO'),", "    CompanyItem('dydxopsdao', 'https://apply.workable.com/dydx-operations-trust', ScrapeWorkable,\n                'https://dydxopsdao.com', 'Web3 DAO'),\n    CompanyItem('almanak', 'https://apply.workable.com/almanak-blockchain-labs-ag', ScrapeWorkable,\n                'https://almanak.co', 'Web3 Simulator'),\n    CompanyItem('walletconnect', 'https://apply.workable.com/walletconnect', ScrapeWorkable,\n                'https://walletconnect.com', 'Web3 Wallet Infra'),\n    CompanyItem('cryptofinance', 'https://apply.workable.com/cryptofinance', ScrapeWorkable,\n                'https://www.crypto-finance.com', 'Exchange'),\n    CompanyItem('bitstamp', 'https://apply.workable.com/bitstamp/#jobs', ScrapeWorkable,\n                'https://www.bitstamp.net', 'Exchange'),", "    CompanyItem('bitstamp', 'https://apply.workable.com/bitstamp/#jobs', ScrapeWorkable,\n                'https://www.bitstamp.net', 'Exchange'),\n    CompanyItem('smart-token-labs', 'https://apply.workable.com/smart-token-labs', ScrapeWorkable,\n                'https://smarttokenlabs.com', 'Web3 bridge'),\n    CompanyItem('avantgarde', 'https://apply.workable.com/avantgarde', ScrapeWorkable,\n                'https://avantgarde.finance', 'Asset Management'),\n    CompanyItem('stably', 'https://apply.workable.com/stably', ScrapeWorkable, 'https://stably.io',\n                'Stable Coin')\n]\n# company_list.append(CompanyItem('bitget', 'https://apply.workable.com/bitget', ScrapeWorkable, 'https://www.bitget.com/en', 'Exchange'))", "]\n# company_list.append(CompanyItem('bitget', 'https://apply.workable.com/bitget', ScrapeWorkable, 'https://www.bitget.com/en', 'Exchange'))\n\nfor company in company_list:\n    print(company.jobs_url)\n    data = company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n    for entry in data:\n        print(entry)\n\ndriver.close()", "\ndriver.close()\n"]}
{"filename": "test/test_ashbyhq_scraper_async.py", "chunked_list": ["from selenium import webdriver\nfrom src.company_item import CompanyItem\nfrom src.scrape_ashbyhq import ScrapeAshbyhqAsync\nimport time\nfrom caqui import synchronous\nfrom pytest import mark\n\n@mark.asyncio\nasync def test_ashbyhq_async():\n    start = time.time()", "async def test_ashbyhq_async():\n    start = time.time()\n    # options = webdriver.ChromeOptions()\n    # options.add_argument('--headless')\n    # driver = webdriver.Chrome(options=options)\n    driver_url = \"http://127.0.0.1:9999\"\n    capabilities = {\n        \"desiredCapabilities\": {\n            \"name\": \"webdriver\",\n            \"browserName\": \"firefox\",", "            \"name\": \"webdriver\",\n            \"browserName\": \"firefox\",\n            \"marionette\": True,\n            \"acceptInsecureCerts\": True,\n            # uncomment to set headless\n            \"goog:chromeOptions\": {\"extensions\": [], \"args\": [\"--headless\"]},\n        }\n    }\n    session = synchronous.get_session(driver_url, capabilities)\n    driver = [driver_url, session]", "    session = synchronous.get_session(driver_url, capabilities)\n    driver = [driver_url, session]\n\n    companies = [\n        CompanyItem('kiln', 'https://jobs.ashbyhq.com/kiln.fi', ScrapeAshbyhqAsync, 'https://www.kiln.fi', 'Staking'),\n        CompanyItem('dune', 'https://jobs.ashbyhq.com/dune', ScrapeAshbyhqAsync, 'https://dune.com',\n                    'Web3 data'),\n        CompanyItem('conduit', 'https://jobs.ashbyhq.com/Conduit', ScrapeAshbyhqAsync, 'https://conduit.xyz',\n                    'Infrastructure'),\n        CompanyItem('paradigm.xyz', 'https://jobs.ashbyhq.com/paradigm', ScrapeAshbyhqAsync, 'https://www.paradigm.xyz',", "                    'Infrastructure'),\n        CompanyItem('paradigm.xyz', 'https://jobs.ashbyhq.com/paradigm', ScrapeAshbyhqAsync, 'https://www.paradigm.xyz',\n                    'Web3 data'),\n        CompanyItem('syndica', 'https://jobs.ashbyhq.com/syndica', ScrapeAshbyhqAsync, 'https://www.sygnum.com',\n                    'Crypto bank'),\n        CompanyItem('solana-foundation', 'https://jobs.ashbyhq.com/Solana%20Foundation', ScrapeAshbyhqAsync,\n                    'https://www.sygnum.com',\n                    'Crypto bank'),\n        CompanyItem('ellipsislabs', 'https://jobs.ashbyhq.com/ellipsislabs', ScrapeAshbyhqAsync,\n                    'https://ellipsislabs.xyz', 'Trading Protocol')", "        CompanyItem('ellipsislabs', 'https://jobs.ashbyhq.com/ellipsislabs', ScrapeAshbyhqAsync,\n                    'https://ellipsislabs.xyz', 'Trading Protocol')\n    ]\n\n    for company in companies:\n        data = await company.scraper_type().getJobs(driver, company.jobs_url, company.company_name)\n        for entry in data:\n            print(entry)\n\n    synchronous.close_session(*driver)", "\n    synchronous.close_session(*driver)\n\n    end = time.time()\n    print(f\"Time: {end-start:.2f} sec\")\n"]}
{"filename": "src/scrape_binance.py", "chunked_list": ["from selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.wait import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom src.scrape_it import ScrapeIt, write_jobs\n\n\ndef clean_location(location: str):\n    location = location.replace(\"/ Full-time\", \"\")\n    if 'global' in location.lower() or 'remote : remote' in location.lower():\n        return \"REMOTE\"\n    set_of_locations = set(([x.strip() for x in location.split(',')]))\n    result_locations = set()\n    for loc in set_of_locations:\n        if loc.endswith('/'):\n            result_locations.add(loc[0:-1].strip())\n            break\n        result_locations.add(loc)\n    return ' '.join(result_locations)", "\n\nclass ScrapeBinance(ScrapeIt):\n    def getJobs(self, driver, web_page, company='binance') -> []:\n        print(f'[BINANCE] Scrap page: {web_page}')\n        driver.get(web_page)\n        wait = WebDriverWait(driver, 120)\n        apply_buttons = wait.until(EC.presence_of_all_elements_located((By.XPATH, '//a[.=\"Apply\"]')))\n        group_elements = driver.find_elements(By.XPATH, '//div[contains(@class,\"posting\")]')\n        print(f'[BINANCE] Found {len(apply_buttons)} jobs on {web_page}')\n        result = []\n        for elem in group_elements:\n            link_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n            location_elem = elem.find_element(By.CSS_SELECTOR, 'div[data-bn-type=\"text\"]')\n            job_url = link_elem.get_attribute('href')\n            location = clean_location(location_elem.text)\n            job_name = link_elem.text\n            job = {\n                \"company\": company,\n                \"title\": job_name,\n                \"location\": location,\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[BINANCE] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", ""]}
{"filename": "src/scrape_recruitee.py", "chunked_list": ["from selenium.webdriver.common.by import By\nfrom src.scrape_it import ScrapeIt, write_jobs\n\n\ndef clean_location(location):\n    if 'remote' in location.lower() or 'global' in location.lower():\n        return \"REMOTE\"\n    joined = ' '.join(set(([x.strip() for x in location.split(',')])))\n    return joined\n", "\n\nclass ScrapeRecruitee(ScrapeIt):\n    def getJobs(self, driver, web_page, company) -> []:\n        print(f'[RECRUITEE] Scrap page: {web_page}')\n        driver.get(web_page)\n        group_elements = driver.find_elements(By.CSS_SELECTOR, 'div [class=\"job\"]')\n        print(f'[RECRUITEE] Found {len(group_elements)} jobs.')\n        result = []\n        for elem in group_elements:\n            link_elem = elem.find_element(By.CSS_SELECTOR, '[class=\"job-title\"] a')\n            location_elem = elem.find_element(By.CSS_SELECTOR, '[class=\"job-location\"]')\n            job_url = link_elem.get_attribute('href')\n            location = clean_location(location_elem.text)\n            job_name = link_elem.text\n            job = {\n                \"company\": company,\n                \"title\": job_name,\n                \"location\": location,\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[RECRUITEE] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", ""]}
{"filename": "src/scrape_lever.py", "chunked_list": ["from caqui import asynchronous\nfrom selenium.webdriver.common.by import By\nfrom src.scrape_it import ScrapeIt, write_jobs\n\nCSS_SELECTOR = \"css\"  # for ChromeDriver\n\n\ndef clean_location(location):\n    locations = set(filter(None, ([x.strip() for x in location.split(',')])))\n    if len(locations) == 1:\n        return next(iter(locations)).strip().lstrip('-').title()\n    joined = ' '.join(locations).lower()\n    if joined.count('remote') > 1:\n        return joined.replace('remote', '', 1).strip().lstrip('-').title()\n    return joined.strip().lstrip('-').title()", "\n\nclass ScrapeLever(ScrapeIt):\n    def getJobs(self, driver, web_page, company) -> []:\n        print(f'[LEVER] Scrap page: {web_page}')\n        driver.get(web_page)\n        group_elements = driver.find_elements(By.CSS_SELECTOR, 'a[class=\"posting-title\"]')\n        print(f'[LEVER] Found {len(group_elements)} jobs.')\n        result = []\n        for elem in group_elements:\n            link_elem = elem.find_element(By.CSS_SELECTOR, '[data-qa=\"posting-name\"]')\n            location_elem = elem.find_elements(By.CSS_SELECTOR, '[class*=\"location\"]')\n            workplace_elem = elem.find_elements(By.CSS_SELECTOR, '[class*=\"workplaceTypes\"]')\n            job_url = elem.get_attribute('href')\n            if len(location_elem) > 0:\n                location = location_elem[0].text\n            else:\n                location = ''\n            if len(workplace_elem) > 0:\n                workplace = workplace_elem[0].text\n                merge_location = f'{location},{workplace}'\n            else:\n                merge_location = location\n            job = {\n                \"company\": company,\n                \"title\": link_elem.text,\n                \"location\": clean_location(merge_location),\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[LEVER] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", "\n\nclass ScrapeLeverAsync(ScrapeIt):\n    name = 'Lever'\n\n    async def getJobs(self, driver, web_page, company) -> []:\n        print(f'[{self.name}] Scrap page: {web_page}')\n        await asynchronous.go_to_page(*driver, web_page)\n        await asynchronous.set_timeouts(*driver, 5000)\n        group_elements = await asynchronous.find_elements(*driver, CSS_SELECTOR, 'a[class=\"posting-title\"]')\n        print(f'[LEVER] Found {len(group_elements)} jobs.')\n        result = []\n        for elem in group_elements:\n            link_elem = await asynchronous.find_child_element(*driver, elem, CSS_SELECTOR, '[data-qa=\"posting-name\"]')\n            location_elem = await asynchronous.find_children_elements(*driver, elem, CSS_SELECTOR, '[class*=\"location\"]')\n            workplace_elem = await asynchronous.find_children_elements(*driver, elem, CSS_SELECTOR, '[class*=\"workplaceTypes\"]')\n            job_url = await asynchronous.get_attribute(*driver, elem, \"href\")\n            title_text = await asynchronous.get_text(*driver, link_elem)\n            if len(location_elem) > 0:\n                location = await asynchronous.get_text(*driver, location_elem[0])\n            else:\n                location = ''\n            if len(workplace_elem) > 0:\n                workplace = await asynchronous.get_text(*driver, workplace_elem[0])\n                merge_location = f'{location},{workplace}'\n            else:\n                merge_location = location\n            job = {\n                \"company\": company,\n                \"title\": title_text,\n                \"location\": clean_location(merge_location),\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[LEVER] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", ""]}
{"filename": "src/scrape_smartrecruiters.py", "chunked_list": ["from selenium.webdriver.common.by import By\nfrom src.scrape_it import ScrapeIt, write_jobs\nimport time\n\n\ndef clean_location(location):\n    if 'remote' in location.lower() or 'global' in location.lower():\n        return \"REMOTE\"\n    locations = set(([x.strip() for x in location.split(',')]))\n    joined = ' '.join(locations)\n    return joined", "\n\ndef show_more(driver, locator):\n    print(f'[SmartRecruiters] Show more jobs..')\n    show_more_button = driver.find_elements(By.XPATH, locator)\n    if len(show_more_button) > 0:\n        driver.execute_script(\"arguments[0].scrollIntoView(true);\", show_more_button[0])\n        time.sleep(5)\n        driver.execute_script(\"arguments[0].click();\", show_more_button[0])\n        time.sleep(5)\n        show_more(driver, locator)", "\n\nclass ScrapeSmartrecruiters(ScrapeIt):\n    def getJobs(self, driver, web_page, company) -> []:\n        print(f'[SmartRecruiters] Scrap page: {web_page}')\n        driver.get(web_page)\n        more_links = '//a[.=\"Show more jobs\"]'\n        show_more(driver, more_links)\n        elements_locator = '//li[contains(@class,\"opening-job\") and not(contains(@class,\"js-more-container\"))]'\n        group_elements = driver.find_elements(By.XPATH, elements_locator)\n        print(f'[SmartRecruiters] Found jobs: {len(group_elements)}')\n        result = []\n        for elem in group_elements:\n            link_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n            job_title = elem.find_element(By.CSS_SELECTOR, 'h4').text\n            location_elem = elem.find_element(By.CSS_SELECTOR, 'span')\n            job_url = link_elem.get_attribute('href')\n            location = clean_location(location_elem.text)\n            job = {\n                \"company\": company,\n                \"title\": job_title,\n                \"location\": location,\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[SmartRecruiters] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", ""]}
{"filename": "src/scrape_bamboohr.py", "chunked_list": ["from selenium.webdriver.common.by import By\nfrom src.scrape_it import ScrapeIt, write_jobs\n\n\ndef clean_location(location):\n    locations = set(filter(None, ([x.strip() for x in location.split(',')])))\n    if len(locations) == 1:\n        return next(iter(locations))\n    joined = ' '.join(locations)\n    if joined.count('remote') > 1:\n        return joined.replace('remote', '', 1).strip().strip('-').title()\n    return joined.strip().strip('-').title()", "\n\nclass ScrapeBamboohr(ScrapeIt):\n    name = 'bamboohr'\n\n    def getJobs(self, driver, web_page, company) -> []:\n        print(f'[{self.name}] Scrap page: {web_page}')\n        driver.get(web_page)\n        driver.implicitly_wait(5)\n        group_elements = driver.find_elements(By.CSS_SELECTOR, 'div[itemscope].row')\n        job_location_locator = 'div[itemprop=\"jobLocation\"]'\n        if len(group_elements) == 0:\n            group_elements = driver.find_elements(By.XPATH, '//li/div/a/..')\n            job_location_locator = 'div[class=\"jss-e78\"]'\n        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n        result = []\n        for elem in group_elements:\n            link_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n            job_name_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n            location_elem = elem.find_element(By.CSS_SELECTOR, job_location_locator)\n            job_url = link_elem.get_attribute('href')\n            job_name = job_name_elem.text\n            location = location_elem.text\n            cleaned_location = location.replace('\\n', ', ')\n            job = {\n                \"company\": company,\n                \"title\": job_name,\n                \"location\": clean_location(cleaned_location),\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", ""]}
{"filename": "src/scrape_workable.py", "chunked_list": ["import time\n\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.wait import WebDriverWait\n\nfrom src.scrape_it import ScrapeIt, write_jobs\n\n\ndef show_more(driver, locator):\n    print(f'[workable] Show more jobs..')\n    show_more_button = driver.find_elements(By.CSS_SELECTOR, locator)\n    if len(show_more_button) > 0:\n        driver.execute_script(\"arguments[0].scrollIntoView(true);\", show_more_button[0])\n        time.sleep(5)\n        driver.execute_script(\"arguments[0].click();\", show_more_button[0])\n        time.sleep(5)\n        show_more(driver, locator)", "\ndef show_more(driver, locator):\n    print(f'[workable] Show more jobs..')\n    show_more_button = driver.find_elements(By.CSS_SELECTOR, locator)\n    if len(show_more_button) > 0:\n        driver.execute_script(\"arguments[0].scrollIntoView(true);\", show_more_button[0])\n        time.sleep(5)\n        driver.execute_script(\"arguments[0].click();\", show_more_button[0])\n        time.sleep(5)\n        show_more(driver, locator)", "\n\ndef clean_location(location):\n    return ' '.join(set(([x.strip() for x in location.split(',')])))\n\n\nclass ScrapeWorkable(ScrapeIt):\n    name = 'workable'.upper()\n\n    def getJobs(self, driver, web_page, company) -> []:\n        print(f'[{self.name}] Scrap page: {web_page}')\n        driver.get(web_page)\n        driver.implicitly_wait(20)\n        wait = WebDriverWait(driver, 20)\n        result = []\n        show_more_locator = 'button[data-ui=\"load-more-button\"]'\n        job_root_locator = 'li[data-ui^=\"job\"]'\n        show_more_buttons = driver.find_elements(By.CSS_SELECTOR, show_more_locator)\n        if len(show_more_buttons) > 0:\n            print(f'[{self.name}] Show more Jobs button found..')\n            show_more(driver, show_more_locator)\n        # just try to find elements and exit if none\n        temp_elements = driver.find_elements(By.CSS_SELECTOR, job_root_locator)\n        if len(temp_elements) == 0:\n            print(f'[{self.name}] Found 0 jobs on {web_page}')\n            return result\n        group_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, job_root_locator)))\n        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n        driver.implicitly_wait(5)\n        for elem in group_elements:\n            link_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n            remote_elem = elem.find_elements(By.CSS_SELECTOR, '[data-ui=\"job-remote\"]')\n            job_name_elem = elem.find_element(By.CSS_SELECTOR, '[data-ui=\"job-title\"],[data-id=\"job-item\"]')\n            location_elem = elem.find_element(By.CSS_SELECTOR, 'span[data-ui=\"job-location\"]')\n            job_url = link_elem.get_attribute('href')\n            job_name = job_name_elem.text\n            location = location_elem.text\n            if len(remote_elem) > 0:\n                location = location + ' REMOTE'\n            job = {\n                \"company\": company,\n                \"title\": job_name,\n                \"location\": clean_location(location),\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", ""]}
{"filename": "src/company_list.py", "chunked_list": ["import json\n\nfrom src.company_item import CompanyItem\nfrom src.scrape_lever import ScrapeLever\nfrom src.scrape_greenhouse import ScrapeGreenhouse\nfrom src.scrape_smartrecruiters import ScrapeSmartrecruiters\nfrom src.scrape_recruitee import ScrapeRecruitee\nfrom src.scrape_binance import ScrapeBinance\nfrom src.scrape_bamboohr import ScrapeBamboohr\nfrom src.scrape_consensys import ScrapeConsensys", "from src.scrape_bamboohr import ScrapeBamboohr\nfrom src.scrape_consensys import ScrapeConsensys\nfrom src.scrape_ripple import ScrapeRipple\nfrom src.scrape_workable import ScrapeWorkable\nfrom src.scrape_ashbyhq import ScrapeAshbyhq\nfrom src.scrape_paxos import ScrapePaxos\n\n\ndef get_company_list() -> []:\n    return [CompanyItem(\"kraken\", \"https://jobs.lever.co/kraken\", ScrapeLever, \"https://kraken.com\", \"Exchange\"),\n            CompanyItem('arbitrumfoundation', 'https://jobs.lever.co/arbitrumfoundation', ScrapeLever,\n                        'https://arbitrum.foundation', 'Layer 2'),\n            CompanyItem(\"chainlink\", \"https://jobs.lever.co/chainlink\", ScrapeLever, \"https://chain.link\",\n                        \"Blockchain\"),\n            CompanyItem('ethglobal', 'https://jobs.lever.co/ETHGlobal', ScrapeLever, 'https://ethglobal.com',\n                        'Community'),\n            CompanyItem('multiversx', 'https://jobs.lever.co/multiversx', ScrapeLever, 'https://multiversx.com',\n                        'Blockchain'),\n            CompanyItem('sprucesystems', 'https://jobs.lever.co/sprucesystems', ScrapeLever, 'https://spruceid.com',\n                        'Web3 ID'),\n            CompanyItem('BlockSwap', 'https://jobs.lever.co/BlockSwap', ScrapeLever, 'https://www.blockswap.network',\n                        'Infra'),\n            CompanyItem('Metatheory', 'https://jobs.lever.co/Metatheory', ScrapeLever,\n                        'https://www.duskbreakers.gg', 'Web3 game'),\n            CompanyItem('axiomzen', 'https://jobs.lever.co/axiomzen', ScrapeLever, 'https://www.axiomzen.com', 'Web3'),\n            CompanyItem('fuellabs', 'https://jobs.lever.co/fuellabs', ScrapeLever, 'https://www.fuel.network',\n                        'Blockchain'),\n            CompanyItem('harmony', 'https://jobs.lever.co/harmony', ScrapeLever, 'https://www.harmony.one',\n                        'Blockchain'),\n            CompanyItem('wintermute', 'https://jobs.lever.co/wintermute-trading', ScrapeLever,\n                        'https://www.wintermute.com',\n                        'Trading'),\n            CompanyItem(\"kaiko\", \"https://jobs.eu.lever.co/kaiko\", ScrapeLever, \"https://www.kaiko.com\", \"Data\"),\n            CompanyItem('bebop', 'https://jobs.lever.co/Bebop', ScrapeLever, 'https://bebop.xyz', 'DeFi Exchange'),\n            CompanyItem(\"Coinshift\", \"https://jobs.lever.co/Coinshift\", ScrapeLever, \"https://coinshift.xyz\",\n                        \"Custody software\"),\n            CompanyItem(\"swissborg\", \"https://jobs.lever.co/swissborg\", ScrapeLever, \"https://swissborg.com\",\n                        \"Exchange\"),\n            CompanyItem(\"OpenSea\", \"https://jobs.lever.co/OpenSea\", ScrapeLever, \"https://opensea.io\", \"NFT\"),\n            CompanyItem(\"storyprotocol\", \"https://jobs.lever.co/storyprotocol\", ScrapeLever,\n                        \"https://www.storyprotocol.xyz\", \"Protocol\"),\n            CompanyItem(\"ethereumfoundation\", \"https://jobs.lever.co/ethereumfoundation\", ScrapeLever,\n                        \"https://ethereum.org\", \"Blockchain\"),\n            CompanyItem(\"aave\", \"https://jobs.eu.lever.co/aave\", ScrapeLever, \"https://aave.com\", \"Protocol\"),\n            CompanyItem(\"crypto\", \"https://jobs.lever.co/crypto\", ScrapeLever, \"https://crypto.com\", \"Exchange\"),\n            CompanyItem(\"Luxor\", \"https://jobs.lever.co/LuxorTechnology\", ScrapeLever, \"https://www.luxor.tech\",\n                        \"Mining\"),\n            CompanyItem(\"anchorage\", \"https://jobs.lever.co/anchorage\", ScrapeLever, \"https://www.anchorage.com\",\n                        \"Trading\"),\n            CompanyItem(\"biconomy\", \"https://jobs.lever.co/biconomy\", ScrapeLever, \"https://www.biconomy.io\",\n                        \"Infra\"),\n            CompanyItem('enso', 'https://jobs.lever.co/Enso', ScrapeLever, 'https://www.enso.finance', 'DeFi'),\n            CompanyItem(\"Polygon\", \"https://jobs.lever.co/Polygon\", ScrapeLever, \"https://polygon.technology\",\n                        \"Blockchain\"),\n            CompanyItem(\"tokenmetrics\", \"https://jobs.lever.co/tokenmetrics\", ScrapeLever,\n                        \"https://www.tokenmetrics.com\", \"Information\"),\n            CompanyItem(\"offchainlabs\", \"https://jobs.lever.co/offchainlabs\", ScrapeLever,\n                        \"https://offchainlabs.com\", \"Protocol\"),\n            CompanyItem(\"subspacelabs\", \"https://jobs.lever.co/subspacelabs\", ScrapeLever,\n                        \"https://subspace.network\", \"Blockchain Infra\"),\n            CompanyItem('3boxlabs', 'https://jobs.lever.co/3box', ScrapeLever, 'https://3boxlabs.com',\n                        'Infra'),\n            CompanyItem(\"ramp.network\", \"https://jobs.lever.co/careers.ramp.network\", ScrapeLever,\n                        \"https://ramp.network\", \"Payments\"),\n            CompanyItem('risklabs', 'https://jobs.lever.co/risklabs', ScrapeLever, 'https://risklabs.foundation',\n                        'Protocol'),\n            CompanyItem('celestia', 'https://jobs.lever.co/celestia', ScrapeLever, 'https://celestia.org',\n                        'Modular Blockchain'),\n            CompanyItem('polymerlabs', 'https://jobs.lever.co/polymerlabs', ScrapeLever, 'https://www.polymerlabs.org',\n                        'Modular Blockchain'),\n            CompanyItem('royal', 'https://jobs.lever.co/Royal', ScrapeLever, 'https://royal.io', 'Web3 + Music'),\n            CompanyItem('gauntlet', 'https://jobs.lever.co/gauntlet', ScrapeLever, 'https://gauntlet.network',\n                        'Web3 + Financial Modelling'),\n            CompanyItem(\"ledger\", \"https://jobs.lever.co/ledger\", ScrapeLever, \"https://www.ledger.com\", \"Wallet\"),\n            CompanyItem(\"request\", \"https://jobs.lever.co/request\", ScrapeLever, \"https://request.network\",\n                        \"Payments\"),\n            CompanyItem(\"immutable\", \"https://jobs.lever.co/immutable\", ScrapeLever, \"https://www.immutable.com\",\n                        \"NFT\"),\n            CompanyItem(\"web3auth\", \"https://jobs.lever.co/TorusLabs\", ScrapeLever, \"https://web3auth.io\", \"Auth\"),\n            CompanyItem(\"cere-network\", \"https://jobs.lever.co/cere-network\", ScrapeLever, \"https://cere.network\",\n                        \"Infra\"),\n            CompanyItem('matterlabs', 'https://jobs.eu.lever.co/matterlabs', ScrapeLever, 'https://matter-labs.io',\n                        'Protocol'),\n            CompanyItem(\"hiro\", \"https://jobs.lever.co/hiro\", ScrapeLever, \"https://www.hiro.so\", \"Infra\"),\n            CompanyItem('AQX', 'https://jobs.lever.co/presto', ScrapeLever, 'https://aqx.com', 'Exchange and Web3'),\n            CompanyItem('ultra', 'https://jobs.lever.co/ultra', ScrapeLever,\n                        'https://ultra.io', 'Web3 Gaming'),\n            CompanyItem('ethenalabs', 'https://jobs.lever.co/ethenalabs', ScrapeLever,\n                        'https://www.ethena.fi', 'Web3 bonds'),\n            CompanyItem('bitwise', 'https://jobs.lever.co/bitwiseinvestments', ScrapeLever,\n                        'https://bitwiseinvestments.com', 'Asset Management'),\n            CompanyItem('grayscaleinvestments', 'https://boards.greenhouse.io/grayscaleinvestments', ScrapeGreenhouse,\n                        'https://grayscale.com', 'Web3 Asset Manager'),\n            CompanyItem(\"0x\", \"https://boards.greenhouse.io/0x\", ScrapeGreenhouse, \"https://0x.org\",\n                        \"Blockchain\"),\n            CompanyItem('econetwork', 'https://boards.greenhouse.io/econetwork', ScrapeGreenhouse,\n                        'https://eco.com', 'Web3 wallet'),\n            CompanyItem(\"bitcoin\", \"https://www.bitcoin.com/jobs/#joblist\", ScrapeGreenhouse,\n                        \"https://www.bitcoin.com\", 'Exchange'),\n            CompanyItem('magic', 'https://boards.greenhouse.io/magic', ScrapeGreenhouse, 'https://magic.link',\n                        'Web3 Wallets'),\n            CompanyItem(\"chainstack\", \"https://chainstack.bamboohr.com/careers\", ScrapeBamboohr,\n                        \"https://chainstack.com\", \"Infra\"),\n            CompanyItem(\"coinmarketcap\", \"https://careers.smartrecruiters.com/B6/coinmarketcap\",\n                        ScrapeSmartrecruiters, \"https://coinmarketcap.com\", \"Information\"),\n            CompanyItem('evmos', 'https://boards.eu.greenhouse.io/evmos', ScrapeGreenhouse, 'https://evmos.org',\n                        'Cross-Chain Connectivity'),\n            CompanyItem('orderlynetwork', 'https://boards.greenhouse.io/orderlynetwork', ScrapeGreenhouse,\n                        'https://orderly.network', 'Exchange'),\n            CompanyItem(\"paxos\",\n                        \"https://paxos.com/job-posts/?_sft_department=engineering-data,finance-accounting,hr-talent,information-technology,legal,operations,product-management,risk-compliance&_sft_office=us\",\n                        ScrapePaxos, \"https://paxos.com\",\n                        \"Stable Coin\"),\n            CompanyItem('dydxopsdao', 'https://apply.workable.com/dydx-operations-trust', ScrapeWorkable,\n                        'https://dydxopsdao.com', 'Web3 DAO'),\n            CompanyItem('almanak', 'https://apply.workable.com/almanak-blockchain-labs-ag', ScrapeWorkable,\n                        'https://almanak.co', 'Web3 Simulator'),\n            CompanyItem(\"zora\", \"https://boards.greenhouse.io/zora\", ScrapeGreenhouse, \"https://zora.co\", \"NFT\"),\n            CompanyItem('bitfury', 'https://boards.greenhouse.io/bitfury', ScrapeGreenhouse, 'https://bitfury.com',\n                        'Web3'),\n            CompanyItem(\"cexio\", \"https://cexio.bamboohr.com/jobs\", ScrapeBamboohr, \"https://cex.io\", \"Exchange\"),\n            CompanyItem(\"circle\", \"https://boards.greenhouse.io/circle\", ScrapeGreenhouse, \"https://circle.com\",\n                        \"Stable Coin\"),\n            CompanyItem(\"status\", \"https://jobs.status.im\", ScrapeGreenhouse, \"https://status.im\", \"Messanger\"),\n            CompanyItem(\"OKX\", \"https://boards.greenhouse.io/OKX\", ScrapeGreenhouse, \"https://okx.com\",\n                        \"Exchange\"),\n            CompanyItem(\"bittrex\", \"https://boards.greenhouse.io/bittrex\", ScrapeGreenhouse,\n                        \"https://global.bittrex.com\", 'Exchange'),\n            CompanyItem(\"bitmex\", \"https://boards.greenhouse.io/bitmex\", ScrapeGreenhouse, \"https://bitmex.com\",\n                        \"Exchange\"),\n            CompanyItem(\"bitgo\", \"https://boards.greenhouse.io/bitgo\", ScrapeGreenhouse, \"https://bitgo.com\",\n                        \"Exchange\"),\n            CompanyItem(\"bitpanda\", \"https://boards.eu.greenhouse.io/bitpanda\", ScrapeGreenhouse,\n                        \"https://bitpanda.com\", \"Exchange\"),\n            CompanyItem(\"uniswaplabs\", \"https://boards.greenhouse.io/uniswaplabs\", ScrapeGreenhouse,\n                        \"https://uniswap.org\", \"Exchange Protocol\"),\n            CompanyItem('osmosisdex', 'https://boards.greenhouse.io/osmosisdex', ScrapeGreenhouse,\n                        'https://osmosis.zone',\n                        'Exchange'),\n            CompanyItem(\"moonpay\", \"https://boards.greenhouse.io/moonpay\", ScrapeGreenhouse,\n                        \"https://www.moonpay.com\", \"Payments\"),\n            CompanyItem('penumbralabs', 'https://boards.greenhouse.io/penumbralabs', ScrapeGreenhouse,\n                        'https://eco.com', 'Web3 trading'),\n            CompanyItem(\"blockdaemon\", \"https://boards.greenhouse.io/blockdaemon\", ScrapeGreenhouse,\n                        \"https://www.blockdaemon.com\", \"Staking & Infra\"),\n            CompanyItem(\"figment\", \"https://boards.greenhouse.io/figment\", ScrapeGreenhouse,\n                        \"https://www.figment.io\", \"Staking & Infra\"),\n            CompanyItem('rain', 'https://jobs.ashbyhq.com/rain', ScrapeAshbyhq, 'https://www.raincards.xyz',\n                        'Web3 cards'),\n            CompanyItem('exponential', 'https://jobs.ashbyhq.com/exponential', ScrapeAshbyhq, 'https://exponential.fi',\n                        'DeFi'),\n            CompanyItem('conduit', 'https://jobs.ashbyhq.com/Conduit', ScrapeAshbyhq, 'https://conduit.xyz',\n                        'Infrastructure'),\n            CompanyItem('kiln', 'https://jobs.ashbyhq.com/kiln.fi', ScrapeAshbyhq, 'https://www.kiln.fi',\n                        'Staking & Infra'),\n            CompanyItem(\"flashbots\", \"https://jobs.ashbyhq.com/flashbots.net\", ScrapeAshbyhq,\n                        \"https://www.flashbots.net\", \"ETH MEV\"),\n            CompanyItem('paradigm.xyz', 'https://jobs.ashbyhq.com/paradigm', ScrapeAshbyhq, 'https://www.paradigm.xyz',\n                        'Web3 financing'),\n            CompanyItem('dune', 'https://jobs.ashbyhq.com/dune', ScrapeAshbyhq, 'https://dune.com',\n                        'Web3 data'),\n            CompanyItem(\"solanafoundation\", \"https://jobs.ashbyhq.com/Solana%20Foundation\", ScrapeAshbyhq,\n                        \"https://solana.org\", \"Blockchain\"),\n            CompanyItem('syndica', 'https://jobs.ashbyhq.com/syndica', ScrapeAshbyhq, 'https://syndica.io',\n                        'Infrastructure'),\n            CompanyItem('Blockworks', 'https://jobs.ashbyhq.com/Blockworks', ScrapeAshbyhq,\n                        'https://blockworks.co', 'Web3 News'),\n            CompanyItem('ellipsislabs', 'https://jobs.ashbyhq.com/ellipsislabs', ScrapeAshbyhq,\n                        'https://ellipsislabs.xyz', 'Trading Protocol'),\n            CompanyItem(\"quiknodeinc\", \"https://boards.greenhouse.io/quiknodeinc\", ScrapeGreenhouse,\n                        \"https://www.quicknode.com\", \"Staking & Infra\"),\n            CompanyItem('21co', 'https://boards.greenhouse.io/21co', ScrapeGreenhouse,\n                        'https://www.21.co', 'Web3 DeFi ETP'),\n            CompanyItem('xapo', 'https://boards.greenhouse.io/xapo61', ScrapeGreenhouse,\n                        'https://www.xapobank.com', 'Web3 bank'),\n            CompanyItem('dragonflycapital', 'https://boards.greenhouse.io/dragonflycapital', ScrapeGreenhouse,\n                        'https://www.dragonfly.xyz', 'Web3 funding'),\n            CompanyItem(\"exodus54\", \"https://boards.greenhouse.io/exodus54\", ScrapeGreenhouse,\n                        \"https://www.exodus.com\", \"Wallet\"),\n            CompanyItem(\"alchemy\", \"https://boards.greenhouse.io/alchemy\", ScrapeGreenhouse,\n                        \"https://www.alchemy.com\", \"Dev & Infra\"),\n            CompanyItem(\"chainalysis\", \"https://boards.greenhouse.io/chainalysis\", ScrapeGreenhouse,\n                        \"https://www.chainalysis.com\", \"Crypto Research\"),\n            CompanyItem(\"magiceden\", \"https://boards.greenhouse.io/magiceden\", ScrapeGreenhouse,\n                        \"https://www.magiceden.io\", \"NFT\"),\n            CompanyItem(\"aztec\", \"https://boards.eu.greenhouse.io/aztec\", ScrapeGreenhouse,\n                        \"https://aztec.network\", \"Protocol\"),\n            CompanyItem(\"nethermind\", \"https://boards.eu.greenhouse.io/nethermind\", ScrapeGreenhouse,\n                        \"https://nethermind.io\", \"Crypto software\"),\n            CompanyItem(\"dfinity\", \"https://boards.greenhouse.io/dfinity\", ScrapeGreenhouse, \"https://dfinity.org\",\n                        \"Blockchain\"),\n            CompanyItem('stellar', 'https://boards.greenhouse.io/stellar', ScrapeGreenhouse,\n                        'https://stellar.org', 'Blockchain'),\n            CompanyItem(\"parity\", \"https://boards.greenhouse.io/parity\", ScrapeGreenhouse, \"https://www.parity.io\",\n                        \"Infra\"),\n            CompanyItem(\"optimism\", \"https://boards.greenhouse.io/optimism\", ScrapeGreenhouse,\n                        \"https://www.optimism.io\", \"L2 protocol\"),\n            CompanyItem('coinmetrics', 'https://boards.greenhouse.io/coinmetrics', ScrapeGreenhouse,\n                        'https://coinmetrics.io', 'Web3 Data'),\n            CompanyItem(\"oplabs\", \"https://boards.greenhouse.io/oplabs\", ScrapeGreenhouse, \"https://www.oplabs.co\",\n                        \"L2 protocol\"),\n            CompanyItem('goldsky', 'https://boards.greenhouse.io/goldsky', ScrapeGreenhouse,\n                        'https://goldsky.com', 'Web3 Data'),\n            CompanyItem('outlierventures', 'https://boards.eu.greenhouse.io/outlierventures', ScrapeGreenhouse,\n                        'https://outlierventures.io', 'Web3 Ventures'),\n            CompanyItem('walletconnect', 'https://apply.workable.com/walletconnect', ScrapeWorkable,\n                        'https://walletconnect.com', 'Web3 Wallet Infra'),\n            CompanyItem(\"bitfinex\", \"https://bitfinex.recruitee.com\", ScrapeRecruitee, \"https://www.bitfinex.com\",\n                        \"Exchange\"),\n            CompanyItem('o1labs', 'https://boards.greenhouse.io/o1labs', ScrapeGreenhouse, 'https://o1labs.org',\n                        'Web3'),\n            CompanyItem('paradigm.co', 'https://boards.greenhouse.io/paradigm62', ScrapeGreenhouse,\n                        'https://www.paradigm.co',\n                        'Liquidity'),\n            CompanyItem(\"binance\", \"https://www.binance.com/en/careers/job-openings\", ScrapeBinance,\n                        \"https://www.binance.com\", \"Exchange\"),\n            CompanyItem(\"trustwallet\", \"https://careers.smartrecruiters.com/B6/trustwallet\", ScrapeSmartrecruiters,\n                        \"https://trustwallet.com\", \"Wallet\"),\n            CompanyItem(\"Swissquote\", \"https://careers.smartrecruiters.com/Swissquote\", ScrapeSmartrecruiters,\n                        \"https://en.swissquote.com\", \"Exchange\"),\n            CompanyItem('taxbit', 'https://boards.greenhouse.io/taxbit', ScrapeGreenhouse, 'https://taxbit.com',\n                        'Accounting'),\n            CompanyItem(\"avalabs\", \"https://boards.greenhouse.io/avalabs\", ScrapeGreenhouse,\n                        \"https://www.avalabs.org\", \"Blockchain\"),\n            CompanyItem(\"aptoslabs\", \"https://boards.greenhouse.io/aptoslabs\", ScrapeGreenhouse,\n                        \"https://aptoslabs.com\",\n                        \"Blockchain\"),\n            CompanyItem(\"filecoinfoundation\", \"https://boards.greenhouse.io/filecoinfoundation\", ScrapeGreenhouse,\n                        \"https://fil.org\", \"Blockchain\"),\n            CompanyItem('foundrydigital', 'https://boards.greenhouse.io/foundrydigital', ScrapeGreenhouse,\n                        'https://foundrydigital.com', 'Web3 Infra'),\n            CompanyItem('immunefi', 'https://boards.greenhouse.io/immunefi', ScrapeGreenhouse, 'https://immunefi.com',\n                        'Bug bounty platform'),\n            CompanyItem('wirex', 'https://wirex.bamboohr.com/careers', ScrapeBamboohr, 'https://wirexapp.com',\n                        'Web3 card'),\n            CompanyItem('protocollabs', 'https://boards.greenhouse.io/protocollabs', ScrapeGreenhouse,\n                        'https://protocol.ai/about',\n                        'Web3 IPFS research platform'),\n            CompanyItem('trmlabs', 'https://www.trmlabs.com/careers-list', ScrapeGreenhouse,\n                        'https://www.trmlabs.com', 'Web3 Information'),\n            CompanyItem(\"messari\", \"https://boards.greenhouse.io/messari\", ScrapeGreenhouse, \"https://messari.io\",\n                        \"Web3 Information\"),\n            CompanyItem(\"serotonin\", \"https://boards.greenhouse.io/serotonin\", ScrapeGreenhouse, \"https://serotonin.co\",\n                        \"Information\"),\n            CompanyItem(\"copperco\", \"https://boards.eu.greenhouse.io/copperco\", ScrapeGreenhouse,\n                        \"https://copper.co\", \"Custody\"),\n            CompanyItem(\"digitalasset\", \"https://boards.greenhouse.io/digitalasset\", ScrapeGreenhouse,\n                        \"https://www.digitalasset.com\", \"Custody\"),\n            CompanyItem(\"layerzerolabs\", \"https://boards.greenhouse.io/layerzerolabs\", ScrapeGreenhouse,\n                        \"https://layerzero.network\", \"Infra\"),\n            CompanyItem('okcoin', 'https://boards.greenhouse.io/okcoin', ScrapeGreenhouse,\n                        'https://www.okcoin.com', 'Exchange'),\n            CompanyItem(\"oasisnetwork\", \"https://boards.greenhouse.io/oasisnetwork\", ScrapeGreenhouse,\n                        \"https://oasisprotocol.org\", \"Protocol\"),\n            CompanyItem(\"consensys\", \"https://consensys.net/open-roles\", ScrapeConsensys, \"https://consensys.net\",\n                        \"Infra\"),\n            CompanyItem(\"ankr\", \"https://boards.greenhouse.io/ankrnetwork\", ScrapeGreenhouse,\n                        \"https://www.ankr.com\", \"Web3 Staking Protocol\"),\n            CompanyItem(\"chainsafesystems\", \"https://boards.greenhouse.io/chainsafesystems\", ScrapeGreenhouse,\n                        \"https://chainsafe.io\", \"Infra\"),\n            CompanyItem(\"ripple\", \"https://ripple.com/careers/all-jobs\", ScrapeRipple, \"https://ripple.com\",\n                        \"Blockchain\"),\n            CompanyItem(\"kadena\", \"https://boards.greenhouse.io/kadenallc\", ScrapeGreenhouse, \"https://kadena.io\",\n                        \"Blockchain\"),\n            CompanyItem(\"eigenlabs\", \"https://boards.greenhouse.io/eigenlabs\", ScrapeGreenhouse,\n                        \"https://www.eigenlayer.xyz\", \"Infra\"),\n            CompanyItem('sygnum', 'https://sygnum.bamboohr.com/careers', ScrapeBamboohr, 'https://www.sygnum.com',\n                        'Crypto bank'),\n            CompanyItem(\"galaxydigitalservices\", \"https://boards.greenhouse.io/galaxydigitalservices\",\n                        ScrapeGreenhouse, \"https://www.galaxy.com\", 'Trading'),\n            CompanyItem('web3', 'https://web3.bamboohr.com/jobs', ScrapeBamboohr, 'https://web3.foundation',\n                        'web3'),\n            CompanyItem(\"solana\", \"https://boards.greenhouse.io/solana\", ScrapeGreenhouse,\n                        \"https://solana.com\", \"Blockchain\"),\n            CompanyItem('mobilecoin', 'https://boards.greenhouse.io/mobilecoin', ScrapeGreenhouse,\n                        'https://mobilecoin.com', 'Blockchain'),\n            CompanyItem('chia', 'https://www.chia.net/careers', ScrapeGreenhouse,\n                        'https://www.chia.net', 'Blockchain'),\n            CompanyItem(\"worldcoin\", \"https://boards.greenhouse.io/worldcoinorg\", ScrapeGreenhouse,\n                        \"https://worldcoin.org\", \"Blockchain\"),\n            CompanyItem(\"edgeandnode\", \"https://boards.greenhouse.io/edgeandnode\", ScrapeGreenhouse,\n                        \"https://edgeandnode.com\", \"Infra\"),\n            CompanyItem(\"clearmatics\", \"https://boards.greenhouse.io/clearmatics\", ScrapeGreenhouse,\n                        \"https://www.clearmatics.com\", \"Protocol\"),\n            CompanyItem('bitstamp', 'https://apply.workable.com/bitstamp/#jobs', ScrapeWorkable,\n                        'https://www.bitstamp.net', 'Exchange'),\n            CompanyItem('yugalabs', 'https://boards.greenhouse.io/yugalabs', ScrapeGreenhouse,\n                        'https://yuga.com', 'NFT'),\n            CompanyItem('cryptofinance', 'https://apply.workable.com/crypto-finance', ScrapeWorkable,\n                        'https://www.crypto-finance.com', 'Exchange'),\n            CompanyItem('bitget', 'https://apply.workable.com/bitget', ScrapeWorkable, 'https://www.bitget.com/en',\n                        'Exchange')]", "def get_company_list() -> []:\n    return [CompanyItem(\"kraken\", \"https://jobs.lever.co/kraken\", ScrapeLever, \"https://kraken.com\", \"Exchange\"),\n            CompanyItem('arbitrumfoundation', 'https://jobs.lever.co/arbitrumfoundation', ScrapeLever,\n                        'https://arbitrum.foundation', 'Layer 2'),\n            CompanyItem(\"chainlink\", \"https://jobs.lever.co/chainlink\", ScrapeLever, \"https://chain.link\",\n                        \"Blockchain\"),\n            CompanyItem('ethglobal', 'https://jobs.lever.co/ETHGlobal', ScrapeLever, 'https://ethglobal.com',\n                        'Community'),\n            CompanyItem('multiversx', 'https://jobs.lever.co/multiversx', ScrapeLever, 'https://multiversx.com',\n                        'Blockchain'),\n            CompanyItem('sprucesystems', 'https://jobs.lever.co/sprucesystems', ScrapeLever, 'https://spruceid.com',\n                        'Web3 ID'),\n            CompanyItem('BlockSwap', 'https://jobs.lever.co/BlockSwap', ScrapeLever, 'https://www.blockswap.network',\n                        'Infra'),\n            CompanyItem('Metatheory', 'https://jobs.lever.co/Metatheory', ScrapeLever,\n                        'https://www.duskbreakers.gg', 'Web3 game'),\n            CompanyItem('axiomzen', 'https://jobs.lever.co/axiomzen', ScrapeLever, 'https://www.axiomzen.com', 'Web3'),\n            CompanyItem('fuellabs', 'https://jobs.lever.co/fuellabs', ScrapeLever, 'https://www.fuel.network',\n                        'Blockchain'),\n            CompanyItem('harmony', 'https://jobs.lever.co/harmony', ScrapeLever, 'https://www.harmony.one',\n                        'Blockchain'),\n            CompanyItem('wintermute', 'https://jobs.lever.co/wintermute-trading', ScrapeLever,\n                        'https://www.wintermute.com',\n                        'Trading'),\n            CompanyItem(\"kaiko\", \"https://jobs.eu.lever.co/kaiko\", ScrapeLever, \"https://www.kaiko.com\", \"Data\"),\n            CompanyItem('bebop', 'https://jobs.lever.co/Bebop', ScrapeLever, 'https://bebop.xyz', 'DeFi Exchange'),\n            CompanyItem(\"Coinshift\", \"https://jobs.lever.co/Coinshift\", ScrapeLever, \"https://coinshift.xyz\",\n                        \"Custody software\"),\n            CompanyItem(\"swissborg\", \"https://jobs.lever.co/swissborg\", ScrapeLever, \"https://swissborg.com\",\n                        \"Exchange\"),\n            CompanyItem(\"OpenSea\", \"https://jobs.lever.co/OpenSea\", ScrapeLever, \"https://opensea.io\", \"NFT\"),\n            CompanyItem(\"storyprotocol\", \"https://jobs.lever.co/storyprotocol\", ScrapeLever,\n                        \"https://www.storyprotocol.xyz\", \"Protocol\"),\n            CompanyItem(\"ethereumfoundation\", \"https://jobs.lever.co/ethereumfoundation\", ScrapeLever,\n                        \"https://ethereum.org\", \"Blockchain\"),\n            CompanyItem(\"aave\", \"https://jobs.eu.lever.co/aave\", ScrapeLever, \"https://aave.com\", \"Protocol\"),\n            CompanyItem(\"crypto\", \"https://jobs.lever.co/crypto\", ScrapeLever, \"https://crypto.com\", \"Exchange\"),\n            CompanyItem(\"Luxor\", \"https://jobs.lever.co/LuxorTechnology\", ScrapeLever, \"https://www.luxor.tech\",\n                        \"Mining\"),\n            CompanyItem(\"anchorage\", \"https://jobs.lever.co/anchorage\", ScrapeLever, \"https://www.anchorage.com\",\n                        \"Trading\"),\n            CompanyItem(\"biconomy\", \"https://jobs.lever.co/biconomy\", ScrapeLever, \"https://www.biconomy.io\",\n                        \"Infra\"),\n            CompanyItem('enso', 'https://jobs.lever.co/Enso', ScrapeLever, 'https://www.enso.finance', 'DeFi'),\n            CompanyItem(\"Polygon\", \"https://jobs.lever.co/Polygon\", ScrapeLever, \"https://polygon.technology\",\n                        \"Blockchain\"),\n            CompanyItem(\"tokenmetrics\", \"https://jobs.lever.co/tokenmetrics\", ScrapeLever,\n                        \"https://www.tokenmetrics.com\", \"Information\"),\n            CompanyItem(\"offchainlabs\", \"https://jobs.lever.co/offchainlabs\", ScrapeLever,\n                        \"https://offchainlabs.com\", \"Protocol\"),\n            CompanyItem(\"subspacelabs\", \"https://jobs.lever.co/subspacelabs\", ScrapeLever,\n                        \"https://subspace.network\", \"Blockchain Infra\"),\n            CompanyItem('3boxlabs', 'https://jobs.lever.co/3box', ScrapeLever, 'https://3boxlabs.com',\n                        'Infra'),\n            CompanyItem(\"ramp.network\", \"https://jobs.lever.co/careers.ramp.network\", ScrapeLever,\n                        \"https://ramp.network\", \"Payments\"),\n            CompanyItem('risklabs', 'https://jobs.lever.co/risklabs', ScrapeLever, 'https://risklabs.foundation',\n                        'Protocol'),\n            CompanyItem('celestia', 'https://jobs.lever.co/celestia', ScrapeLever, 'https://celestia.org',\n                        'Modular Blockchain'),\n            CompanyItem('polymerlabs', 'https://jobs.lever.co/polymerlabs', ScrapeLever, 'https://www.polymerlabs.org',\n                        'Modular Blockchain'),\n            CompanyItem('royal', 'https://jobs.lever.co/Royal', ScrapeLever, 'https://royal.io', 'Web3 + Music'),\n            CompanyItem('gauntlet', 'https://jobs.lever.co/gauntlet', ScrapeLever, 'https://gauntlet.network',\n                        'Web3 + Financial Modelling'),\n            CompanyItem(\"ledger\", \"https://jobs.lever.co/ledger\", ScrapeLever, \"https://www.ledger.com\", \"Wallet\"),\n            CompanyItem(\"request\", \"https://jobs.lever.co/request\", ScrapeLever, \"https://request.network\",\n                        \"Payments\"),\n            CompanyItem(\"immutable\", \"https://jobs.lever.co/immutable\", ScrapeLever, \"https://www.immutable.com\",\n                        \"NFT\"),\n            CompanyItem(\"web3auth\", \"https://jobs.lever.co/TorusLabs\", ScrapeLever, \"https://web3auth.io\", \"Auth\"),\n            CompanyItem(\"cere-network\", \"https://jobs.lever.co/cere-network\", ScrapeLever, \"https://cere.network\",\n                        \"Infra\"),\n            CompanyItem('matterlabs', 'https://jobs.eu.lever.co/matterlabs', ScrapeLever, 'https://matter-labs.io',\n                        'Protocol'),\n            CompanyItem(\"hiro\", \"https://jobs.lever.co/hiro\", ScrapeLever, \"https://www.hiro.so\", \"Infra\"),\n            CompanyItem('AQX', 'https://jobs.lever.co/presto', ScrapeLever, 'https://aqx.com', 'Exchange and Web3'),\n            CompanyItem('ultra', 'https://jobs.lever.co/ultra', ScrapeLever,\n                        'https://ultra.io', 'Web3 Gaming'),\n            CompanyItem('ethenalabs', 'https://jobs.lever.co/ethenalabs', ScrapeLever,\n                        'https://www.ethena.fi', 'Web3 bonds'),\n            CompanyItem('bitwise', 'https://jobs.lever.co/bitwiseinvestments', ScrapeLever,\n                        'https://bitwiseinvestments.com', 'Asset Management'),\n            CompanyItem('grayscaleinvestments', 'https://boards.greenhouse.io/grayscaleinvestments', ScrapeGreenhouse,\n                        'https://grayscale.com', 'Web3 Asset Manager'),\n            CompanyItem(\"0x\", \"https://boards.greenhouse.io/0x\", ScrapeGreenhouse, \"https://0x.org\",\n                        \"Blockchain\"),\n            CompanyItem('econetwork', 'https://boards.greenhouse.io/econetwork', ScrapeGreenhouse,\n                        'https://eco.com', 'Web3 wallet'),\n            CompanyItem(\"bitcoin\", \"https://www.bitcoin.com/jobs/#joblist\", ScrapeGreenhouse,\n                        \"https://www.bitcoin.com\", 'Exchange'),\n            CompanyItem('magic', 'https://boards.greenhouse.io/magic', ScrapeGreenhouse, 'https://magic.link',\n                        'Web3 Wallets'),\n            CompanyItem(\"chainstack\", \"https://chainstack.bamboohr.com/careers\", ScrapeBamboohr,\n                        \"https://chainstack.com\", \"Infra\"),\n            CompanyItem(\"coinmarketcap\", \"https://careers.smartrecruiters.com/B6/coinmarketcap\",\n                        ScrapeSmartrecruiters, \"https://coinmarketcap.com\", \"Information\"),\n            CompanyItem('evmos', 'https://boards.eu.greenhouse.io/evmos', ScrapeGreenhouse, 'https://evmos.org',\n                        'Cross-Chain Connectivity'),\n            CompanyItem('orderlynetwork', 'https://boards.greenhouse.io/orderlynetwork', ScrapeGreenhouse,\n                        'https://orderly.network', 'Exchange'),\n            CompanyItem(\"paxos\",\n                        \"https://paxos.com/job-posts/?_sft_department=engineering-data,finance-accounting,hr-talent,information-technology,legal,operations,product-management,risk-compliance&_sft_office=us\",\n                        ScrapePaxos, \"https://paxos.com\",\n                        \"Stable Coin\"),\n            CompanyItem('dydxopsdao', 'https://apply.workable.com/dydx-operations-trust', ScrapeWorkable,\n                        'https://dydxopsdao.com', 'Web3 DAO'),\n            CompanyItem('almanak', 'https://apply.workable.com/almanak-blockchain-labs-ag', ScrapeWorkable,\n                        'https://almanak.co', 'Web3 Simulator'),\n            CompanyItem(\"zora\", \"https://boards.greenhouse.io/zora\", ScrapeGreenhouse, \"https://zora.co\", \"NFT\"),\n            CompanyItem('bitfury', 'https://boards.greenhouse.io/bitfury', ScrapeGreenhouse, 'https://bitfury.com',\n                        'Web3'),\n            CompanyItem(\"cexio\", \"https://cexio.bamboohr.com/jobs\", ScrapeBamboohr, \"https://cex.io\", \"Exchange\"),\n            CompanyItem(\"circle\", \"https://boards.greenhouse.io/circle\", ScrapeGreenhouse, \"https://circle.com\",\n                        \"Stable Coin\"),\n            CompanyItem(\"status\", \"https://jobs.status.im\", ScrapeGreenhouse, \"https://status.im\", \"Messanger\"),\n            CompanyItem(\"OKX\", \"https://boards.greenhouse.io/OKX\", ScrapeGreenhouse, \"https://okx.com\",\n                        \"Exchange\"),\n            CompanyItem(\"bittrex\", \"https://boards.greenhouse.io/bittrex\", ScrapeGreenhouse,\n                        \"https://global.bittrex.com\", 'Exchange'),\n            CompanyItem(\"bitmex\", \"https://boards.greenhouse.io/bitmex\", ScrapeGreenhouse, \"https://bitmex.com\",\n                        \"Exchange\"),\n            CompanyItem(\"bitgo\", \"https://boards.greenhouse.io/bitgo\", ScrapeGreenhouse, \"https://bitgo.com\",\n                        \"Exchange\"),\n            CompanyItem(\"bitpanda\", \"https://boards.eu.greenhouse.io/bitpanda\", ScrapeGreenhouse,\n                        \"https://bitpanda.com\", \"Exchange\"),\n            CompanyItem(\"uniswaplabs\", \"https://boards.greenhouse.io/uniswaplabs\", ScrapeGreenhouse,\n                        \"https://uniswap.org\", \"Exchange Protocol\"),\n            CompanyItem('osmosisdex', 'https://boards.greenhouse.io/osmosisdex', ScrapeGreenhouse,\n                        'https://osmosis.zone',\n                        'Exchange'),\n            CompanyItem(\"moonpay\", \"https://boards.greenhouse.io/moonpay\", ScrapeGreenhouse,\n                        \"https://www.moonpay.com\", \"Payments\"),\n            CompanyItem('penumbralabs', 'https://boards.greenhouse.io/penumbralabs', ScrapeGreenhouse,\n                        'https://eco.com', 'Web3 trading'),\n            CompanyItem(\"blockdaemon\", \"https://boards.greenhouse.io/blockdaemon\", ScrapeGreenhouse,\n                        \"https://www.blockdaemon.com\", \"Staking & Infra\"),\n            CompanyItem(\"figment\", \"https://boards.greenhouse.io/figment\", ScrapeGreenhouse,\n                        \"https://www.figment.io\", \"Staking & Infra\"),\n            CompanyItem('rain', 'https://jobs.ashbyhq.com/rain', ScrapeAshbyhq, 'https://www.raincards.xyz',\n                        'Web3 cards'),\n            CompanyItem('exponential', 'https://jobs.ashbyhq.com/exponential', ScrapeAshbyhq, 'https://exponential.fi',\n                        'DeFi'),\n            CompanyItem('conduit', 'https://jobs.ashbyhq.com/Conduit', ScrapeAshbyhq, 'https://conduit.xyz',\n                        'Infrastructure'),\n            CompanyItem('kiln', 'https://jobs.ashbyhq.com/kiln.fi', ScrapeAshbyhq, 'https://www.kiln.fi',\n                        'Staking & Infra'),\n            CompanyItem(\"flashbots\", \"https://jobs.ashbyhq.com/flashbots.net\", ScrapeAshbyhq,\n                        \"https://www.flashbots.net\", \"ETH MEV\"),\n            CompanyItem('paradigm.xyz', 'https://jobs.ashbyhq.com/paradigm', ScrapeAshbyhq, 'https://www.paradigm.xyz',\n                        'Web3 financing'),\n            CompanyItem('dune', 'https://jobs.ashbyhq.com/dune', ScrapeAshbyhq, 'https://dune.com',\n                        'Web3 data'),\n            CompanyItem(\"solanafoundation\", \"https://jobs.ashbyhq.com/Solana%20Foundation\", ScrapeAshbyhq,\n                        \"https://solana.org\", \"Blockchain\"),\n            CompanyItem('syndica', 'https://jobs.ashbyhq.com/syndica', ScrapeAshbyhq, 'https://syndica.io',\n                        'Infrastructure'),\n            CompanyItem('Blockworks', 'https://jobs.ashbyhq.com/Blockworks', ScrapeAshbyhq,\n                        'https://blockworks.co', 'Web3 News'),\n            CompanyItem('ellipsislabs', 'https://jobs.ashbyhq.com/ellipsislabs', ScrapeAshbyhq,\n                        'https://ellipsislabs.xyz', 'Trading Protocol'),\n            CompanyItem(\"quiknodeinc\", \"https://boards.greenhouse.io/quiknodeinc\", ScrapeGreenhouse,\n                        \"https://www.quicknode.com\", \"Staking & Infra\"),\n            CompanyItem('21co', 'https://boards.greenhouse.io/21co', ScrapeGreenhouse,\n                        'https://www.21.co', 'Web3 DeFi ETP'),\n            CompanyItem('xapo', 'https://boards.greenhouse.io/xapo61', ScrapeGreenhouse,\n                        'https://www.xapobank.com', 'Web3 bank'),\n            CompanyItem('dragonflycapital', 'https://boards.greenhouse.io/dragonflycapital', ScrapeGreenhouse,\n                        'https://www.dragonfly.xyz', 'Web3 funding'),\n            CompanyItem(\"exodus54\", \"https://boards.greenhouse.io/exodus54\", ScrapeGreenhouse,\n                        \"https://www.exodus.com\", \"Wallet\"),\n            CompanyItem(\"alchemy\", \"https://boards.greenhouse.io/alchemy\", ScrapeGreenhouse,\n                        \"https://www.alchemy.com\", \"Dev & Infra\"),\n            CompanyItem(\"chainalysis\", \"https://boards.greenhouse.io/chainalysis\", ScrapeGreenhouse,\n                        \"https://www.chainalysis.com\", \"Crypto Research\"),\n            CompanyItem(\"magiceden\", \"https://boards.greenhouse.io/magiceden\", ScrapeGreenhouse,\n                        \"https://www.magiceden.io\", \"NFT\"),\n            CompanyItem(\"aztec\", \"https://boards.eu.greenhouse.io/aztec\", ScrapeGreenhouse,\n                        \"https://aztec.network\", \"Protocol\"),\n            CompanyItem(\"nethermind\", \"https://boards.eu.greenhouse.io/nethermind\", ScrapeGreenhouse,\n                        \"https://nethermind.io\", \"Crypto software\"),\n            CompanyItem(\"dfinity\", \"https://boards.greenhouse.io/dfinity\", ScrapeGreenhouse, \"https://dfinity.org\",\n                        \"Blockchain\"),\n            CompanyItem('stellar', 'https://boards.greenhouse.io/stellar', ScrapeGreenhouse,\n                        'https://stellar.org', 'Blockchain'),\n            CompanyItem(\"parity\", \"https://boards.greenhouse.io/parity\", ScrapeGreenhouse, \"https://www.parity.io\",\n                        \"Infra\"),\n            CompanyItem(\"optimism\", \"https://boards.greenhouse.io/optimism\", ScrapeGreenhouse,\n                        \"https://www.optimism.io\", \"L2 protocol\"),\n            CompanyItem('coinmetrics', 'https://boards.greenhouse.io/coinmetrics', ScrapeGreenhouse,\n                        'https://coinmetrics.io', 'Web3 Data'),\n            CompanyItem(\"oplabs\", \"https://boards.greenhouse.io/oplabs\", ScrapeGreenhouse, \"https://www.oplabs.co\",\n                        \"L2 protocol\"),\n            CompanyItem('goldsky', 'https://boards.greenhouse.io/goldsky', ScrapeGreenhouse,\n                        'https://goldsky.com', 'Web3 Data'),\n            CompanyItem('outlierventures', 'https://boards.eu.greenhouse.io/outlierventures', ScrapeGreenhouse,\n                        'https://outlierventures.io', 'Web3 Ventures'),\n            CompanyItem('walletconnect', 'https://apply.workable.com/walletconnect', ScrapeWorkable,\n                        'https://walletconnect.com', 'Web3 Wallet Infra'),\n            CompanyItem(\"bitfinex\", \"https://bitfinex.recruitee.com\", ScrapeRecruitee, \"https://www.bitfinex.com\",\n                        \"Exchange\"),\n            CompanyItem('o1labs', 'https://boards.greenhouse.io/o1labs', ScrapeGreenhouse, 'https://o1labs.org',\n                        'Web3'),\n            CompanyItem('paradigm.co', 'https://boards.greenhouse.io/paradigm62', ScrapeGreenhouse,\n                        'https://www.paradigm.co',\n                        'Liquidity'),\n            CompanyItem(\"binance\", \"https://www.binance.com/en/careers/job-openings\", ScrapeBinance,\n                        \"https://www.binance.com\", \"Exchange\"),\n            CompanyItem(\"trustwallet\", \"https://careers.smartrecruiters.com/B6/trustwallet\", ScrapeSmartrecruiters,\n                        \"https://trustwallet.com\", \"Wallet\"),\n            CompanyItem(\"Swissquote\", \"https://careers.smartrecruiters.com/Swissquote\", ScrapeSmartrecruiters,\n                        \"https://en.swissquote.com\", \"Exchange\"),\n            CompanyItem('taxbit', 'https://boards.greenhouse.io/taxbit', ScrapeGreenhouse, 'https://taxbit.com',\n                        'Accounting'),\n            CompanyItem(\"avalabs\", \"https://boards.greenhouse.io/avalabs\", ScrapeGreenhouse,\n                        \"https://www.avalabs.org\", \"Blockchain\"),\n            CompanyItem(\"aptoslabs\", \"https://boards.greenhouse.io/aptoslabs\", ScrapeGreenhouse,\n                        \"https://aptoslabs.com\",\n                        \"Blockchain\"),\n            CompanyItem(\"filecoinfoundation\", \"https://boards.greenhouse.io/filecoinfoundation\", ScrapeGreenhouse,\n                        \"https://fil.org\", \"Blockchain\"),\n            CompanyItem('foundrydigital', 'https://boards.greenhouse.io/foundrydigital', ScrapeGreenhouse,\n                        'https://foundrydigital.com', 'Web3 Infra'),\n            CompanyItem('immunefi', 'https://boards.greenhouse.io/immunefi', ScrapeGreenhouse, 'https://immunefi.com',\n                        'Bug bounty platform'),\n            CompanyItem('wirex', 'https://wirex.bamboohr.com/careers', ScrapeBamboohr, 'https://wirexapp.com',\n                        'Web3 card'),\n            CompanyItem('protocollabs', 'https://boards.greenhouse.io/protocollabs', ScrapeGreenhouse,\n                        'https://protocol.ai/about',\n                        'Web3 IPFS research platform'),\n            CompanyItem('trmlabs', 'https://www.trmlabs.com/careers-list', ScrapeGreenhouse,\n                        'https://www.trmlabs.com', 'Web3 Information'),\n            CompanyItem(\"messari\", \"https://boards.greenhouse.io/messari\", ScrapeGreenhouse, \"https://messari.io\",\n                        \"Web3 Information\"),\n            CompanyItem(\"serotonin\", \"https://boards.greenhouse.io/serotonin\", ScrapeGreenhouse, \"https://serotonin.co\",\n                        \"Information\"),\n            CompanyItem(\"copperco\", \"https://boards.eu.greenhouse.io/copperco\", ScrapeGreenhouse,\n                        \"https://copper.co\", \"Custody\"),\n            CompanyItem(\"digitalasset\", \"https://boards.greenhouse.io/digitalasset\", ScrapeGreenhouse,\n                        \"https://www.digitalasset.com\", \"Custody\"),\n            CompanyItem(\"layerzerolabs\", \"https://boards.greenhouse.io/layerzerolabs\", ScrapeGreenhouse,\n                        \"https://layerzero.network\", \"Infra\"),\n            CompanyItem('okcoin', 'https://boards.greenhouse.io/okcoin', ScrapeGreenhouse,\n                        'https://www.okcoin.com', 'Exchange'),\n            CompanyItem(\"oasisnetwork\", \"https://boards.greenhouse.io/oasisnetwork\", ScrapeGreenhouse,\n                        \"https://oasisprotocol.org\", \"Protocol\"),\n            CompanyItem(\"consensys\", \"https://consensys.net/open-roles\", ScrapeConsensys, \"https://consensys.net\",\n                        \"Infra\"),\n            CompanyItem(\"ankr\", \"https://boards.greenhouse.io/ankrnetwork\", ScrapeGreenhouse,\n                        \"https://www.ankr.com\", \"Web3 Staking Protocol\"),\n            CompanyItem(\"chainsafesystems\", \"https://boards.greenhouse.io/chainsafesystems\", ScrapeGreenhouse,\n                        \"https://chainsafe.io\", \"Infra\"),\n            CompanyItem(\"ripple\", \"https://ripple.com/careers/all-jobs\", ScrapeRipple, \"https://ripple.com\",\n                        \"Blockchain\"),\n            CompanyItem(\"kadena\", \"https://boards.greenhouse.io/kadenallc\", ScrapeGreenhouse, \"https://kadena.io\",\n                        \"Blockchain\"),\n            CompanyItem(\"eigenlabs\", \"https://boards.greenhouse.io/eigenlabs\", ScrapeGreenhouse,\n                        \"https://www.eigenlayer.xyz\", \"Infra\"),\n            CompanyItem('sygnum', 'https://sygnum.bamboohr.com/careers', ScrapeBamboohr, 'https://www.sygnum.com',\n                        'Crypto bank'),\n            CompanyItem(\"galaxydigitalservices\", \"https://boards.greenhouse.io/galaxydigitalservices\",\n                        ScrapeGreenhouse, \"https://www.galaxy.com\", 'Trading'),\n            CompanyItem('web3', 'https://web3.bamboohr.com/jobs', ScrapeBamboohr, 'https://web3.foundation',\n                        'web3'),\n            CompanyItem(\"solana\", \"https://boards.greenhouse.io/solana\", ScrapeGreenhouse,\n                        \"https://solana.com\", \"Blockchain\"),\n            CompanyItem('mobilecoin', 'https://boards.greenhouse.io/mobilecoin', ScrapeGreenhouse,\n                        'https://mobilecoin.com', 'Blockchain'),\n            CompanyItem('chia', 'https://www.chia.net/careers', ScrapeGreenhouse,\n                        'https://www.chia.net', 'Blockchain'),\n            CompanyItem(\"worldcoin\", \"https://boards.greenhouse.io/worldcoinorg\", ScrapeGreenhouse,\n                        \"https://worldcoin.org\", \"Blockchain\"),\n            CompanyItem(\"edgeandnode\", \"https://boards.greenhouse.io/edgeandnode\", ScrapeGreenhouse,\n                        \"https://edgeandnode.com\", \"Infra\"),\n            CompanyItem(\"clearmatics\", \"https://boards.greenhouse.io/clearmatics\", ScrapeGreenhouse,\n                        \"https://www.clearmatics.com\", \"Protocol\"),\n            CompanyItem('bitstamp', 'https://apply.workable.com/bitstamp/#jobs', ScrapeWorkable,\n                        'https://www.bitstamp.net', 'Exchange'),\n            CompanyItem('yugalabs', 'https://boards.greenhouse.io/yugalabs', ScrapeGreenhouse,\n                        'https://yuga.com', 'NFT'),\n            CompanyItem('cryptofinance', 'https://apply.workable.com/crypto-finance', ScrapeWorkable,\n                        'https://www.crypto-finance.com', 'Exchange'),\n            CompanyItem('bitget', 'https://apply.workable.com/bitget', ScrapeWorkable, 'https://www.bitget.com/en',\n                        'Exchange')]", "\n\ndef get_company(name) -> CompanyItem:\n    company_list = get_company_list()\n    companies = list(filter(lambda jd: jd.company_name == name, company_list))\n    if len(companies) > 1:\n        raise NameError(f'Duplicated company name: {name}')\n    return companies[0]\n\n\ndef write_companies(file_name):\n    result_list = []\n    for com in get_company_list():\n        company_item = {\n            \"company_name\": com.company_name,\n            \"company_url\": com.company_url,\n            \"jobs_url\": com.jobs_url,\n        }\n        result_list.append(company_item)\n    print(f'[COMPANY_LIST] Number of Companies writen {len(result_list)}')\n    with open(file_name, 'w') as companies_file:\n        json.dump(result_list, companies_file, indent=4)", "\n\ndef write_companies(file_name):\n    result_list = []\n    for com in get_company_list():\n        company_item = {\n            \"company_name\": com.company_name,\n            \"company_url\": com.company_url,\n            \"jobs_url\": com.jobs_url,\n        }\n        result_list.append(company_item)\n    print(f'[COMPANY_LIST] Number of Companies writen {len(result_list)}')\n    with open(file_name, 'w') as companies_file:\n        json.dump(result_list, companies_file, indent=4)", ""]}
{"filename": "src/company_item.py", "chunked_list": ["from dataclasses import dataclass\nfrom src.scrape_it import ScrapeIt\n\n\n@dataclass(init=True)\nclass CompanyItem:\n    company_name: str\n    jobs_url: str\n    scraper_type: ScrapeIt\n    company_url: str\n    company_type: str", ""]}
{"filename": "src/company_list_empty.py", "chunked_list": ["import json\n\nfrom src.company_item import CompanyItem\nfrom src.scrape_bamboohr import ScrapeBamboohr\nfrom src.scrape_greenhouse import ScrapeGreenhouse\nfrom src.scrape_lever import ScrapeLever\nfrom src.scrape_workable import ScrapeWorkable\n\n\ndef get_company_list() -> []:\n    return [\n        CompanyItem(\"archblock\", \"https://jobs.lever.co/archblock\", ScrapeLever, \"https://www.archblock.com\",\n                    \"Stable Coin\"),\n        CompanyItem(\"tessera\", \"https://jobs.lever.co/ftc\", ScrapeLever, \"https://tessera.co\", \"NFT\"),\n        CompanyItem(\"moonwalk\", \"https://boards.greenhouse.io/moonwalk\", ScrapeGreenhouse,\n                    \"https://www.moonwalk.com\", \"Platform\"),\n        CompanyItem(\"tron\", \"https://boards.greenhouse.io/rainberry\", ScrapeGreenhouse, \"https://tron.network\",\n                    \"Blockchain\"),\n        CompanyItem(\"jumpcrypto\", \"https://boards.greenhouse.io/jumpcrypto\", ScrapeGreenhouse,\n                    \"https://jumpcrypto.com\", \"Infra\"),\n        CompanyItem(\"poap\", \"https://boards.greenhouse.io/poaptheproofofattendanceprotocol\", ScrapeGreenhouse,\n                    \"https://poap.xyz\", \"Protocol\"),\n        CompanyItem('smart-token-labs', 'https://apply.workable.com/smart-token-labs', ScrapeWorkable,\n                    'https://smarttokenlabs.com', 'Web3 bridge'),\n        CompanyItem('avantgarde', 'https://apply.workable.com/avantgarde', ScrapeWorkable,\n                    'https://avantgarde.finance', 'Asset Management'),\n        CompanyItem('stably', 'https://apply.workable.com/stably', ScrapeWorkable, 'https://stably.io',\n                    'Stable Coin'),\n        CompanyItem('dappradar', 'https://dappradar.bamboohr.com/careers', ScrapeBamboohr,\n                    'https://dappradar.com', 'Exchange & NFT'),\n        CompanyItem('iofinnet', 'https://iofinnethr.bamboohr.com/jobs/?source=bamboohr', ScrapeBamboohr,\n                    'https://www.iofinnet.com', 'Custody'),\n        CompanyItem(\"amun\", \"https://boards.greenhouse.io/amun\", ScrapeGreenhouse, \"https://amun.com\", \"DeFi\"),\n    ]", "\ndef get_company_list() -> []:\n    return [\n        CompanyItem(\"archblock\", \"https://jobs.lever.co/archblock\", ScrapeLever, \"https://www.archblock.com\",\n                    \"Stable Coin\"),\n        CompanyItem(\"tessera\", \"https://jobs.lever.co/ftc\", ScrapeLever, \"https://tessera.co\", \"NFT\"),\n        CompanyItem(\"moonwalk\", \"https://boards.greenhouse.io/moonwalk\", ScrapeGreenhouse,\n                    \"https://www.moonwalk.com\", \"Platform\"),\n        CompanyItem(\"tron\", \"https://boards.greenhouse.io/rainberry\", ScrapeGreenhouse, \"https://tron.network\",\n                    \"Blockchain\"),\n        CompanyItem(\"jumpcrypto\", \"https://boards.greenhouse.io/jumpcrypto\", ScrapeGreenhouse,\n                    \"https://jumpcrypto.com\", \"Infra\"),\n        CompanyItem(\"poap\", \"https://boards.greenhouse.io/poaptheproofofattendanceprotocol\", ScrapeGreenhouse,\n                    \"https://poap.xyz\", \"Protocol\"),\n        CompanyItem('smart-token-labs', 'https://apply.workable.com/smart-token-labs', ScrapeWorkable,\n                    'https://smarttokenlabs.com', 'Web3 bridge'),\n        CompanyItem('avantgarde', 'https://apply.workable.com/avantgarde', ScrapeWorkable,\n                    'https://avantgarde.finance', 'Asset Management'),\n        CompanyItem('stably', 'https://apply.workable.com/stably', ScrapeWorkable, 'https://stably.io',\n                    'Stable Coin'),\n        CompanyItem('dappradar', 'https://dappradar.bamboohr.com/careers', ScrapeBamboohr,\n                    'https://dappradar.com', 'Exchange & NFT'),\n        CompanyItem('iofinnet', 'https://iofinnethr.bamboohr.com/jobs/?source=bamboohr', ScrapeBamboohr,\n                    'https://www.iofinnet.com', 'Custody'),\n        CompanyItem(\"amun\", \"https://boards.greenhouse.io/amun\", ScrapeGreenhouse, \"https://amun.com\", \"DeFi\"),\n    ]", "\n\ndef get_company(name) -> CompanyItem:\n    company_list = get_company_list()\n    companies = list(filter(lambda jd: jd.company_name == name, company_list))\n    return companies[0]\n\n\ndef write_companies(file_name):\n    result_list = []\n    for com in get_company_list():\n        company_item = {\n            \"company_name\": com.company_name,\n            \"company_url\": com.company_url,\n            \"jobs_url\": com.jobs_url,\n        }\n        result_list.append(company_item)\n    print(f'[COMPANY_LIST] Number of Companies writen {len(result_list)}')\n    with open(file_name, 'w') as companies_file:\n        json.dump(result_list, companies_file, indent=4)", "def write_companies(file_name):\n    result_list = []\n    for com in get_company_list():\n        company_item = {\n            \"company_name\": com.company_name,\n            \"company_url\": com.company_url,\n            \"jobs_url\": com.jobs_url,\n        }\n        result_list.append(company_item)\n    print(f'[COMPANY_LIST] Number of Companies writen {len(result_list)}')\n    with open(file_name, 'w') as companies_file:\n        json.dump(result_list, companies_file, indent=4)", ""]}
{"filename": "src/scrape_ashbyhq.py", "chunked_list": ["from selenium.webdriver.common.by import By\nfrom src.scrape_it import ScrapeIt, write_jobs\nfrom caqui import synchronous, asynchronous\n\nCSS_SELECTOR = \"css\"  # for ChromeDriver\n\n\ndef clean_location(location):\n    locations = list(filter(None, ([x.strip() for x in location.split('\u2022')])))\n    result = locations[1]\n    return result.strip().title()", "\n\nclass ScrapeAshbyhqAsync(ScrapeIt):\n    name = 'ashbyhq'\n\n    async def getJobs(self, driver, web_page, company) -> []:\n        print(f'[{self.name}] Scrap page: {web_page}')\n        # driver.get(web_page)\n        await asynchronous.go_to_page(*driver, web_page)\n        # driver.implicitly_wait(5)\n        await asynchronous.set_timeouts(*driver, 5000)\n        # group_elements = driver.find_elements(By.CSS_SELECTOR, 'a[class*=\"container_\"]')\n        group_elements = await asynchronous.find_elements(*driver, CSS_SELECTOR, 'a[class*=\"container_\"]')\n        job_location_locator = 'div p'\n        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n        result = []\n        for elem in group_elements:\n            link_elem = elem\n            # job_name_elem = elem.find_element(By.CSS_SELECTOR, 'h3')\n            job_name_elem = await asynchronous.find_child_element(*driver, elem, CSS_SELECTOR, \"h3\")\n            # location_elem = elem.find_element(By.CSS_SELECTOR, job_location_locator)\n            location_elem = await asynchronous.find_child_element(*driver, elem, CSS_SELECTOR, job_location_locator)\n            # job_url = link_elem.get_attribute('href')\n            job_url = await asynchronous.get_attribute(*driver, link_elem, \"href\")\n            # job_name = job_name_elem.text\n            job_name = await asynchronous.get_text(*driver, job_name_elem)\n            # location = location_elem.text\n            location = await asynchronous.get_text(*driver, location_elem)\n            cleaned_location = location.replace('\\n', ', ')\n            job = {\n                \"company\": company,\n                \"title\": job_name,\n                \"location\": clean_location(cleaned_location),\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", "\n\nclass ScrapeAshbyhq(ScrapeIt):\n    name = 'ashbyhq'\n\n    def getJobs(self, driver, web_page, company) -> []:\n        print(f'[{self.name}] Scrap page: {web_page}')\n        driver.get(web_page)\n        driver.implicitly_wait(5)\n        group_elements = driver.find_elements(By.CSS_SELECTOR, 'a[class*=\"container_\"]')\n        job_location_locator = 'div p'\n        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n        result = []\n        for elem in group_elements:\n            link_elem = elem\n            job_name_elem = elem.find_element(By.CSS_SELECTOR, 'h3')\n            location_elem = elem.find_element(By.CSS_SELECTOR, job_location_locator)\n            job_url = link_elem.get_attribute('href')\n            job_name = job_name_elem.text\n            location = location_elem.text\n            cleaned_location = location.replace('\\n', ', ')\n            job = {\n                \"company\": company,\n                \"title\": job_name,\n                \"location\": clean_location(cleaned_location),\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", ""]}
{"filename": "src/scrape_paxos.py", "chunked_list": ["from selenium.webdriver.common.by import By\nfrom src.scrape_it import ScrapeIt, write_jobs\n\n\ndef to_records(group_elements, company) -> []:\n    result = []\n    for elem in group_elements:\n        job_url = elem.get_attribute('href')\n        job = {\n            \"company\": company,\n            \"title\": elem.text,\n            \"location\": 'US',\n            \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n        }\n        result.append(job)\n    return result", "\n\nclass ScrapePaxos(ScrapeIt):\n    def getJobs(self, driver, web_page, company='paxos') -> []:\n        print(f'[PAXOS] Scrap page: {web_page}')\n        driver.get(web_page)\n        next_link = 'a[class=\"page-numbers\"]'\n        next_links = driver.find_elements(By.CSS_SELECTOR, next_link)\n        group_elements = driver.find_elements(By.CSS_SELECTOR, 'h3 a[href]')\n        result = to_records(group_elements, company)\n        i = 1\n        for nxt in next_links:\n            i += 1\n            driver.get(f'{web_page}&sf_paged={i}')\n            group_elements = driver.find_elements(By.CSS_SELECTOR, 'h3 a[href]')\n            result += to_records(group_elements, company)\n\n        print(f'[PAXOS] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", ""]}
{"filename": "src/scrape_it.py", "chunked_list": ["import json\n\n\ndef write_jobs(jobs):\n    with open('jobs.json', 'r') as f:\n        jobs_json = json.load(f)\n    jobs_data = jobs_json.get('data', [])\n    for job in jobs:\n        jobs_data.append(job)\n    jobs_json['data'] = jobs_data\n    with open('jobs.json', 'w') as file:\n        json.dump(jobs_json, file, indent=4)", "\n\nclass ScrapeIt:\n    def getJobs(self, driver, web_page, company):\n        pass\n"]}
{"filename": "src/scrape_consensys.py", "chunked_list": ["from selenium.webdriver.common.by import By\nfrom src.scrape_it import ScrapeIt, write_jobs\n\n\nclass ScrapeConsensys(ScrapeIt):\n    name = 'CONSENSYS'\n\n    def getJobs(self, driver, web_page, company='consensys') -> []:\n        print(f'[{self.name}] Scrap page: {web_page}')\n        driver.get(web_page)\n        driver.get(web_page)\n        group_elements = driver.find_elements(By.XPATH, '//div[@id=\"careers\"]//div[contains(@class, \"careersSectionItem_itemOuter\")]')\n        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n        result = []\n        for elem in group_elements:\n            link_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n            job_name_elem = elem.find_element(By.CSS_SELECTOR, 'h5')\n            location_elem = elem.find_element(By.XPATH, '//div[contains(@class, \"careersSectionItem_location\")]')\n            job_url = link_elem.get_attribute('href')\n            job_name = job_name_elem.text\n            location = location_elem.text\n            job = {\n                \"company\": company,\n                \"title\": job_name,\n                \"location\": location,\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", ""]}
{"filename": "src/scrape_greenhouse.py", "chunked_list": ["from selenium.webdriver.common.by import By\nfrom src.scrape_it import ScrapeIt, write_jobs\nfrom caqui import synchronous, asynchronous\nimport time\nCSS_SELECTOR = \"css\"  # for ChromeDriver\n\n\ndef clean_location(location):\n    locations = set(filter(None, ([x.strip() for x in location.split(',')])))\n    if len(locations) == 1:\n        return next(iter(locations))\n    joined = ' '.join(locations).lower()\n    if joined.count('remote') > 1:\n        return joined.replace('remote', '', 1).title()\n    return joined.strip().strip('-').title()", "\n\nclass ScrapeGreenhouse(ScrapeIt):\n    name = 'GREENHOUSE'\n\n    def getJobs(self, driver, web_page, company) -> []:\n        print(f'[{self.name}] Scrap page: {web_page}')\n        driver.get(web_page)\n        iframe = driver.find_elements(By.TAG_NAME, 'iframe')\n        if len(iframe) > 0:\n            print(f'[{self.name}] iFrame detected..')\n            time.sleep(3)\n            driver.switch_to.frame(iframe[0])\n            time.sleep(5)\n        group_elements = driver.find_elements(By.CSS_SELECTOR, 'div [class=\"opening\"]')\n        print(f'[{self.name}] Found {len(group_elements)} jobs.')\n        result = []\n        for elem in group_elements:\n            link_elem = elem.find_element(By.CSS_SELECTOR, 'a')\n            location_elem = elem.find_element(By.CSS_SELECTOR, 'span')\n            job_url = link_elem.get_attribute('href')\n            location = location_elem.text\n            job_name = link_elem.text\n            job = {\n                \"company\": company,\n                \"title\": job_name,\n                \"location\": clean_location(location),\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", "\n\nclass ScrapeGreenhouseAsync(ScrapeIt):\n    name = 'GREENHOUSE'\n\n    async def getJobs(self, driver, web_page, company) -> []:\n        print(f'[{self.name}] Scrap page: {web_page}')\n        await asynchronous.go_to_page(*driver, web_page)\n        await asynchronous.set_timeouts(*driver, 5000)\n        iframe = await asynchronous.find_elements(*driver, By.TAG_NAME, 'iframe')\n        if len(iframe) > 0:\n            print(f'[{self.name}] iFrame detected..')\n            time.sleep(3)\n            await asynchronous.switch_to_frame(*driver, iframe[0])\n            time.sleep(5)\n        group_elements = await asynchronous.find_elements(*driver, CSS_SELECTOR, 'div [class=\"opening\"]')\n        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n        result = []\n        for elem in group_elements:\n            link_elem = await asynchronous.find_child_element(*driver, elem, CSS_SELECTOR, 'a')\n            location_elem = await asynchronous.find_child_element(*driver, elem, CSS_SELECTOR, 'span')\n            job_url = await asynchronous.get_attribute(*driver, link_elem, \"href\")\n            job_name = await asynchronous.get_text(*driver, link_elem)\n            location = await asynchronous.get_text(*driver, location_elem)\n            cleaned_location = location.replace('\\n', ', ')\n            job = {\n                \"company\": company,\n                \"title\": job_name,\n                \"location\": clean_location(cleaned_location),\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", ""]}
{"filename": "src/scrape_ripple.py", "chunked_list": ["from selenium.webdriver.common.by import By\nfrom src.scrape_it import ScrapeIt, write_jobs\n\n\nclass ScrapeRipple(ScrapeIt):\n    name = 'RIPPLE'\n\n    def getJobs(self, driver, web_page, company='ripple') -> []:\n        print(f'[{self.name}] Scrap page: {web_page}')\n        driver.get(web_page)\n        # use reverse strategy from a link to a title\n        group_elements = driver.find_elements(By.XPATH, '//div/a[contains(@class, \"body3\")]')\n        print(f'[{self.name}] Found {len(group_elements)} jobs on {web_page}')\n        result = []\n        for elem in group_elements:\n            link_elem = elem\n            job_name_elem = elem.find_element(By.XPATH, './../../../div[contains(@class, \"heading3\")]')\n            location_elem = elem\n            job_url = link_elem.get_attribute('href')\n            job_name = job_name_elem.text\n            location = location_elem.text\n            job = {\n                \"company\": company,\n                \"title\": job_name,\n                \"location\": location,\n                \"link\": f\"<a href='{job_url}' target='_blank' >Apply</a>\"\n            }\n            result.append(job)\n        print(f'[{self.name}] Scraped {len(result)} jobs from {web_page}')\n        write_jobs(result)\n        return result", ""]}
