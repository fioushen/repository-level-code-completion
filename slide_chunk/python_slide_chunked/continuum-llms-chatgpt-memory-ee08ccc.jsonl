{"filename": "ui.py", "chunked_list": ["\"\"\"\nAdapted from https://github.com/avrabyt/MemoryBot\n\"\"\"\n\nimport requests\n\n# Import necessary libraries\nimport streamlit as st\n\nfrom chatgpt_memory.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT", "\nfrom chatgpt_memory.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\n\n# Set Streamlit page configuration\nst.set_page_config(page_title=\"\ud83e\udde0MemoryBot\ud83e\udd16\", layout=\"wide\")\n# Initialize session states\nif \"generated\" not in st.session_state:\n    st.session_state[\"generated\"] = []\nif \"past\" not in st.session_state:\n    st.session_state[\"past\"] = []\nif \"input\" not in st.session_state:\n    st.session_state[\"input\"] = \"\"", "if \"past\" not in st.session_state:\n    st.session_state[\"past\"] = []\nif \"input\" not in st.session_state:\n    st.session_state[\"input\"] = \"\"\nif \"stored_session\" not in st.session_state:\n    st.session_state[\"stored_session\"] = []\nif \"conversation_id\" not in st.session_state:\n    st.session_state[\"conversation_id\"] = None\n\n", "\n\n# Define function to get user input\ndef get_text():\n    \"\"\"\n    Get the user input text.\n\n    Returns:\n        (str): The text entered by the user\n    \"\"\"\n    input_text = st.text_input(\n        \"You: \",\n        st.session_state[\"input\"],\n        key=\"input\",\n        placeholder=\"Your AI assistant here! Ask me anything ...\",\n        label_visibility=\"hidden\",\n        on_change=send_text,\n    )\n\n    return input_text", "\n\ndef send_text():\n    user_input = st.session_state[\"input\"]\n    if user_input:\n        # Use the ChatGPTClient object to generate a response\n        url = \"http://localhost:8000/converse\"\n        payload = {\"message\": user_input, \"conversation_id\": st.session_state.conversation_id}\n\n        response = requests.post(url, json=payload).json()\n        # Update the conversation_id with the conversation_id from the response\n        if not st.session_state.conversation_id:\n            st.session_state.conversation_id = response[\"conversation_id\"]\n        st.session_state.past.insert(0, user_input)\n        st.session_state.generated.insert(0, response[\"chat_gpt_answer\"])\n        st.session_state[\"input\"] = \"\"", "\n\n# Define function to start a new chat\ndef new_chat():\n    \"\"\"\n    Clears session state and starts a new chat.\n    \"\"\"\n    save = []\n    for i in range(len(st.session_state[\"generated\"]) - 1, -1, -1):\n        save.append(\"Human:\" + st.session_state[\"past\"][i])\n        save.append(\"Assistant:\" + st.session_state[\"generated\"][i])\n    st.session_state[\"stored_session\"].append(save)\n    st.session_state[\"generated\"] = []\n    st.session_state[\"past\"] = []\n    st.session_state[\"input\"] = \"\"\n    st.session_state[\"conversation_id\"] = None", "\n\n# Set up the Streamlit app layout\nst.title(\"\ud83e\udd16 Chat Bot with \ud83e\udde0\")\nst.subheader(\" Powered by ChatGPT Memory + Redis Search\")\n\n\n# Session state storage would be ideal\nif not OPENAI_API_KEY:\n    st.sidebar.warning(\"API key required to try this app. The API key is not stored in any form.\")\nelif not (REDIS_HOST and REDIS_PASSWORD and REDIS_PORT):\n    st.sidebar.warning(\n        \"Redis `REDIS_HOST`, `REDIS_PASSWORD`, `REDIS_PORT` are required to try this app. Please set them as env variables properly.\"\n    )", "if not OPENAI_API_KEY:\n    st.sidebar.warning(\"API key required to try this app. The API key is not stored in any form.\")\nelif not (REDIS_HOST and REDIS_PASSWORD and REDIS_PORT):\n    st.sidebar.warning(\n        \"Redis `REDIS_HOST`, `REDIS_PASSWORD`, `REDIS_PORT` are required to try this app. Please set them as env variables properly.\"\n    )\n\n\n# Add a button to start a new chat\nst.sidebar.button(\"New Chat\", on_click=new_chat, type=\"primary\")", "# Add a button to start a new chat\nst.sidebar.button(\"New Chat\", on_click=new_chat, type=\"primary\")\n\n# Get the user input\nuser_input = get_text()\n\n# Allow to download as well\ndownload_str = []\n# Display the conversation history using an expander, and allow the user to download it\nwith st.expander(\"Conversation\", expanded=True):\n    for i in range(len(st.session_state[\"generated\"]) - 1, -1, -1):\n        st.info(st.session_state[\"past\"][i], icon=\"\ud83e\uddd0\")\n        st.success(st.session_state[\"generated\"][i], icon=\"\ud83e\udd16\")\n        download_str.append(st.session_state[\"past\"][i])\n        download_str.append(st.session_state[\"generated\"][i])\n\n    # Can throw error - requires fix\n    download_str = [\"\\n\".join(download_str)]\n    if download_str:\n        st.download_button(\"Download\", download_str[0])", "# Display the conversation history using an expander, and allow the user to download it\nwith st.expander(\"Conversation\", expanded=True):\n    for i in range(len(st.session_state[\"generated\"]) - 1, -1, -1):\n        st.info(st.session_state[\"past\"][i], icon=\"\ud83e\uddd0\")\n        st.success(st.session_state[\"generated\"][i], icon=\"\ud83e\udd16\")\n        download_str.append(st.session_state[\"past\"][i])\n        download_str.append(st.session_state[\"generated\"][i])\n\n    # Can throw error - requires fix\n    download_str = [\"\\n\".join(download_str)]\n    if download_str:\n        st.download_button(\"Download\", download_str[0])", "\n# Display stored conversation sessions in the sidebar\nfor i, sublist in enumerate(st.session_state.stored_session):\n    with st.sidebar.expander(label=f\"Conversation-Session:{i}\"):\n        st.write(sublist)\n\n# Allow the user to clear all stored conversation sessions\nif st.session_state.stored_session:\n    if st.sidebar.checkbox(\"Clear-all\"):\n        del st.session_state.stored_session", ""]}
{"filename": "rest_api.py", "chunked_list": ["from typing import Optional\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nfrom chatgpt_memory.datastore import RedisDataStore, RedisDataStoreConfig\nfrom chatgpt_memory.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\nfrom chatgpt_memory.llm_client import ChatGPTClient, ChatGPTConfig, ChatGPTResponse, EmbeddingClient, EmbeddingConfig\nfrom chatgpt_memory.memory import MemoryManager\n", "from chatgpt_memory.memory import MemoryManager\n\n# Instantiate an EmbeddingConfig object with the OpenAI API key\nembedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\n# Instantiate an EmbeddingClient object with the EmbeddingConfig object\nembed_client = EmbeddingClient(config=embedding_config)\n\n# Instantiate a RedisDataStoreConfig object with the Redis connection details\nredis_datastore_config = RedisDataStoreConfig(", "# Instantiate a RedisDataStoreConfig object with the Redis connection details\nredis_datastore_config = RedisDataStoreConfig(\n    host=REDIS_HOST,\n    port=REDIS_PORT,\n    password=REDIS_PASSWORD,\n)\n\n# Instantiate a RedisDataStore object with the RedisDataStoreConfig object\nredis_datastore = RedisDataStore(config=redis_datastore_config)\n", "redis_datastore = RedisDataStore(config=redis_datastore_config)\n\n# Instantiate a MemoryManager object with the RedisDataStore object and EmbeddingClient object\nmemory_manager = MemoryManager(datastore=redis_datastore, embed_client=embed_client, topk=1)\n\n# Instantiate a ChatGPTConfig object with the OpenAI API key and verbose set to True\nchat_gpt_config = ChatGPTConfig(api_key=OPENAI_API_KEY, verbose=False)\n\n# Instantiate a ChatGPTClient object with the ChatGPTConfig object and MemoryManager object\nchat_gpt_client = ChatGPTClient(config=chat_gpt_config, memory_manager=memory_manager)", "# Instantiate a ChatGPTClient object with the ChatGPTConfig object and MemoryManager object\nchat_gpt_client = ChatGPTClient(config=chat_gpt_config, memory_manager=memory_manager)\n\n\nclass MessagePayload(BaseModel):\n    conversation_id: Optional[str]\n    message: str\n\n\napp = FastAPI()", "\napp = FastAPI()\n\n\n@app.post(\"/converse/\")\nasync def converse(message_payload: MessagePayload) -> ChatGPTResponse:\n    response = chat_gpt_client.converse(**message_payload.dict())\n    return response\n", ""]}
{"filename": "tests/test_memory_manager.py", "chunked_list": ["from chatgpt_memory.datastore.config import RedisDataStoreConfig\nfrom chatgpt_memory.datastore.redis import RedisDataStore\nfrom chatgpt_memory.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\nfrom chatgpt_memory.llm_client.openai.embedding.config import EmbeddingConfig\nfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingClient\nfrom chatgpt_memory.memory.manager import MemoryManager\nfrom chatgpt_memory.memory.memory import Memory\n\n\nclass TestMemoryManager:\n    def setup(self):\n        # create a redis datastore\n        redis_datastore_config = RedisDataStoreConfig(\n            host=REDIS_HOST,\n            port=REDIS_PORT,\n            password=REDIS_PASSWORD,\n        )\n        self.datastore = RedisDataStore(redis_datastore_config, do_flush_data=True)\n\n        # create an openai embedding client\n        embedding_client_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n        self.embedding_client = EmbeddingClient(embedding_client_config)\n\n    def test_conversation_insertion_and_deletion(self):\n        # create a memory manager\n        memory_manager = MemoryManager(datastore=self.datastore, embed_client=self.embedding_client)\n\n        # assert that the memory manager is initially empty\n        assert len(memory_manager.conversations) == 0\n\n        # add a conversation to the memory manager\n        memory_manager.add_conversation(Memory(conversation_id=\"1\"))\n\n        # assert that the memory manager has 1 conversation\n        assert len(memory_manager.conversations) == 1\n\n        # remove the conversation from the memory manager\n        memory_manager.remove_conversation(Memory(conversation_id=\"1\"))\n\n        # assert that the memory manager is empty\n        assert len(memory_manager.conversations) == 0\n\n    def test_adding_messages_to_conversation(self):\n        # create a memory manager\n        memory_manager = MemoryManager(datastore=self.datastore, embed_client=self.embedding_client)\n\n        # add a conversation to the memory manager\n        memory_manager.add_conversation(Memory(conversation_id=\"1\"))\n\n        # assert that the memory manager has 1 conversation\n        assert len(memory_manager.conversations) == 1\n\n        # add a message to the conversation\n        memory_manager.add_message(conversation_id=\"1\", human=\"Hello\", assistant=\"Hello. How are you?\")\n\n        # get messages for that conversation\n        messages = memory_manager.get_messages(conversation_id=\"1\", query=\"Hello\")\n\n        # assert that the message was added\n        assert len(messages) == 1\n\n        # assert that the message is correct\n        assert messages[0].text == \"Human: Hello\\nAssistant: Hello. How are you?\"\n        assert messages[0].conversation_id == \"1\"", "\nclass TestMemoryManager:\n    def setup(self):\n        # create a redis datastore\n        redis_datastore_config = RedisDataStoreConfig(\n            host=REDIS_HOST,\n            port=REDIS_PORT,\n            password=REDIS_PASSWORD,\n        )\n        self.datastore = RedisDataStore(redis_datastore_config, do_flush_data=True)\n\n        # create an openai embedding client\n        embedding_client_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n        self.embedding_client = EmbeddingClient(embedding_client_config)\n\n    def test_conversation_insertion_and_deletion(self):\n        # create a memory manager\n        memory_manager = MemoryManager(datastore=self.datastore, embed_client=self.embedding_client)\n\n        # assert that the memory manager is initially empty\n        assert len(memory_manager.conversations) == 0\n\n        # add a conversation to the memory manager\n        memory_manager.add_conversation(Memory(conversation_id=\"1\"))\n\n        # assert that the memory manager has 1 conversation\n        assert len(memory_manager.conversations) == 1\n\n        # remove the conversation from the memory manager\n        memory_manager.remove_conversation(Memory(conversation_id=\"1\"))\n\n        # assert that the memory manager is empty\n        assert len(memory_manager.conversations) == 0\n\n    def test_adding_messages_to_conversation(self):\n        # create a memory manager\n        memory_manager = MemoryManager(datastore=self.datastore, embed_client=self.embedding_client)\n\n        # add a conversation to the memory manager\n        memory_manager.add_conversation(Memory(conversation_id=\"1\"))\n\n        # assert that the memory manager has 1 conversation\n        assert len(memory_manager.conversations) == 1\n\n        # add a message to the conversation\n        memory_manager.add_message(conversation_id=\"1\", human=\"Hello\", assistant=\"Hello. How are you?\")\n\n        # get messages for that conversation\n        messages = memory_manager.get_messages(conversation_id=\"1\", query=\"Hello\")\n\n        # assert that the message was added\n        assert len(messages) == 1\n\n        # assert that the message is correct\n        assert messages[0].text == \"Human: Hello\\nAssistant: Hello. How are you?\"\n        assert messages[0].conversation_id == \"1\"", ""]}
{"filename": "tests/test_redis_datastore.py", "chunked_list": ["import numpy as np\n\nfrom chatgpt_memory.datastore.redis import RedisDataStore\nfrom chatgpt_memory.environment import OPENAI_API_KEY\nfrom chatgpt_memory.llm_client.openai.embedding.config import EmbeddingConfig\nfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\nSAMPLE_QUERIES = [\"Where is Berlin?\"]\nSAMPLE_DOCUMENTS = [\n    {\"text\": \"Berlin is located in Germany.\", \"conversation_id\": \"1\"},", "SAMPLE_DOCUMENTS = [\n    {\"text\": \"Berlin is located in Germany.\", \"conversation_id\": \"1\"},\n    {\"text\": \"Vienna is in Austria.\", \"conversation_id\": \"1\"},\n    {\"text\": \"Salzburg is in Austria.\", \"conversation_id\": \"2\"},\n]\n\n\ndef test_redis_datastore(redis_datastore: RedisDataStore):\n    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n    openai_embedding_client = EmbeddingClient(config=embedding_config)\n    assert (\n        redis_datastore.redis_connection.ping()\n    ), \"Redis connection failed,\\\n          double check your connection parameters\"\n\n    document_embeddings: np.ndarray = openai_embedding_client.embed_documents(SAMPLE_DOCUMENTS)\n    for idx, embedding in enumerate(document_embeddings):\n        SAMPLE_DOCUMENTS[idx][\"embedding\"] = embedding.astype(np.float32).tobytes()\n    redis_datastore.index_documents(documents=SAMPLE_DOCUMENTS)\n\n    query_embeddings: np.ndarray = openai_embedding_client.embed_queries(SAMPLE_QUERIES)\n    query_vector = query_embeddings[0].astype(np.float32).tobytes()\n    search_results = redis_datastore.search_documents(query_vector=query_vector, conversation_id=\"1\", topk=1)\n    assert len(search_results), \"No documents returned, expected 1 document.\"\n\n    assert search_results[0].text == \"Berlin is located in Germany.\", \"Incorrect document returned as search result.\"\n\n    redis_datastore.delete_documents(conversation_id=\"1\")\n    assert redis_datastore.get_all_conversation_ids() == [\n        \"2\"\n    ], \"Document deletion failed, inconsistent documents in redis index\"", ""]}
{"filename": "tests/test_llm_embedding_client.py", "chunked_list": ["from chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\nSAMPLE_QUERIES = [\"Where is Berlin?\"]\nSAMPLE_DOCUMENTS = [{\"text\": \"Berlin is located in Germany.\"}]\n\nEXPECTED_EMBEDDING_DIMENSIONS = (1, 1024)\n\n\ndef test_openai_embedding_client(openai_embedding_client: EmbeddingClient):\n    assert (\n        openai_embedding_client.embed_queries(SAMPLE_QUERIES).shape == EXPECTED_EMBEDDING_DIMENSIONS\n    ), \"Generated query embedding is of inconsistent dimension\"\n\n    assert (\n        openai_embedding_client.embed_documents(SAMPLE_DOCUMENTS).shape == EXPECTED_EMBEDDING_DIMENSIONS\n    ), \"Generated document(s) embedding is of inconsistent dimension\"", "def test_openai_embedding_client(openai_embedding_client: EmbeddingClient):\n    assert (\n        openai_embedding_client.embed_queries(SAMPLE_QUERIES).shape == EXPECTED_EMBEDDING_DIMENSIONS\n    ), \"Generated query embedding is of inconsistent dimension\"\n\n    assert (\n        openai_embedding_client.embed_documents(SAMPLE_DOCUMENTS).shape == EXPECTED_EMBEDDING_DIMENSIONS\n    ), \"Generated document(s) embedding is of inconsistent dimension\"\n", ""]}
{"filename": "tests/conftest.py", "chunked_list": ["import pytest\n\nfrom chatgpt_memory.datastore.config import RedisDataStoreConfig\nfrom chatgpt_memory.datastore.redis import RedisDataStore\nfrom chatgpt_memory.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\nfrom chatgpt_memory.llm_client.openai.embedding.config import EmbeddingConfig\nfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\n\n@pytest.fixture(scope=\"session\")\ndef openai_embedding_client():\n    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n    return EmbeddingClient(config=embedding_config)", "\n@pytest.fixture(scope=\"session\")\ndef openai_embedding_client():\n    embedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n    return EmbeddingClient(config=embedding_config)\n\n\n@pytest.fixture(scope=\"session\")\ndef redis_datastore():\n    redis_datastore_config = RedisDataStoreConfig(\n        host=REDIS_HOST,\n        port=REDIS_PORT,\n        password=REDIS_PASSWORD,\n    )\n    redis_datastore = RedisDataStore(config=redis_datastore_config, do_flush_data=True)\n\n    return redis_datastore", "def redis_datastore():\n    redis_datastore_config = RedisDataStoreConfig(\n        host=REDIS_HOST,\n        port=REDIS_PORT,\n        password=REDIS_PASSWORD,\n    )\n    redis_datastore = RedisDataStore(config=redis_datastore_config, do_flush_data=True)\n\n    return redis_datastore\n", ""]}
{"filename": "chatgpt_memory/environment.py", "chunked_list": ["import os\n\nimport dotenv\n\n# Load environment variables from .env file\n_TESTING = os.getenv(\"CHATGPT_MEMORY_TESTING\", False)\nif _TESTING:\n    # for testing we use the .env.example file instead\n    dotenv.load_dotenv(dotenv.find_dotenv(\".env.example\"))\nelse:\n    dotenv.load_dotenv()", "\n# Any remote API (OpenAI, Cohere etc.)\nOPENAI_TIMEOUT = float(os.getenv(\"REMOTE_API_TIMEOUT_SEC\", 30))\nOPENAI_BACKOFF = float(os.getenv(\"REMOTE_API_BACKOFF_SEC\", 10))\nOPENAI_MAX_RETRIES = int(os.getenv(\"REMOTE_API_MAX_RETRIES\", 5))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Cloud data store (Redis, Pinecone etc.)\nREDIS_HOST = os.getenv(\"REDIS_HOST\")\nREDIS_PORT = int(os.getenv(\"REDIS_PORT\"))", "REDIS_HOST = os.getenv(\"REDIS_HOST\")\nREDIS_PORT = int(os.getenv(\"REDIS_PORT\"))\nREDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\")\n"]}
{"filename": "chatgpt_memory/errors.py", "chunked_list": ["\"\"\"Custom Errors for ChatGptMemory\"\"\"\n\nfrom typing import Optional\n\n\nclass ChatGPTMemoryError(Exception):\n    \"\"\"\n    Any error generated by ChatGptMemory.\n\n    This error wraps its source transparently in such a way that its attributes\n    can be accessed directly: for example, if the original error has a `message`\n    attribute.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: Optional[str] = None,\n    ):\n        super().__init__()\n        if message:\n            self.message = message\n\n    def __getattr__(self, attr):\n        # If self.__cause__ is None, it will raise the expected AttributeError\n        getattr(self.__cause__, attr)\n\n    def __repr__(self):\n        return str(self)", "\n\nclass OpenAIError(ChatGPTMemoryError):\n    \"\"\"Exception for issues that occur in the OpenAI APIs\"\"\"\n\n    def __init__(\n        self,\n        message: Optional[str] = None,\n        status_code: Optional[int] = None,\n    ):\n        super().__init__(message=message)\n        self.status_code = status_code", "\n\nclass OpenAIRateLimitError(OpenAIError):\n    \"\"\"\n    Rate limit error for OpenAI API (status code 429), See below:\n    https://help.openai.com/en/articles/5955604-how-can-i-solve-429-too-many-requests-errors\n    https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\n    \"\"\"\n\n    def __init__(self, message: Optional[str] = None):\n        super().__init__(message=message, status_code=429)\n\n    def __repr__(self):\n        return f\"message= {self.message}, status_code={self.status_code}\"", ""]}
{"filename": "chatgpt_memory/__init__.py", "chunked_list": [""]}
{"filename": "chatgpt_memory/constants.py", "chunked_list": ["# LLM Config related\n\"\"\"\nif OpenAI embedding model type is \"*-001\" the set max sequence length to `2046`,\notherwise for type \"*-002\" set `8191`\n\"\"\"\nMAX_ALLOWED_SEQ_LEN_001 = 2046\nMAX_ALLOWED_SEQ_LEN_002 = 8191\n"]}
{"filename": "chatgpt_memory/utils/reflection.py", "chunked_list": ["import inspect\nimport logging\nimport time\nfrom random import random\nfrom typing import Any, Callable, Dict, Tuple\n\nfrom chatgpt_memory.errors import OpenAIRateLimitError\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\n\ndef args_to_kwargs(args: Tuple, func: Callable) -> Dict[str, Any]:\n    sig = inspect.signature(func)\n    arg_names = list(sig.parameters.keys())\n    # skip self and cls args for instance and class methods\n    if any(arg_names) and arg_names[0] in [\"self\", \"cls\"]:\n        arg_names = arg_names[1 : 1 + len(args)]\n    args_as_kwargs = {arg_name: arg for arg, arg_name in zip(args, arg_names)}\n    return args_as_kwargs", "\n\ndef retry_with_exponential_backoff(\n    backoff_in_seconds: float = 1,\n    max_retries: int = 10,\n    errors: tuple = (OpenAIRateLimitError,),\n):\n    \"\"\"\n    Decorator to retry a function with exponential backoff.\n    :param backoff_in_seconds: The initial backoff in seconds.\n    :param max_retries: The maximum number of retries.\n    :param errors: The errors to catch retry on.\n    \"\"\"\n\n    def decorator(function):\n        def wrapper(*args, **kwargs):\n            # Initialize variables\n            num_retries = 0\n\n            # Loop until a successful response or max_retries is hit or an\n            # exception is raised\n            while True:\n                try:\n                    return function(*args, **kwargs)\n\n                # Retry on specified errors\n                except errors as e:\n                    # Check if max retries has been reached\n                    if num_retries > max_retries:\n                        raise Exception(f\"Maximum number of retries ({max_retries}) exceeded.\")\n\n                    # Increment the delay\n                    sleep_time = backoff_in_seconds * 2**num_retries + random()\n\n                    # Sleep for the delay\n                    logger.warning(\n                        \"%s - %s, retry %s in %s seconds...\",\n                        e.__class__.__name__,\n                        e,\n                        function.__name__,\n                        \"{0:.2f}\".format(sleep_time),\n                    )\n                    time.sleep(sleep_time)\n\n                    # Increment retries\n                    num_retries += 1\n\n        return wrapper\n\n    return decorator", ""]}
{"filename": "chatgpt_memory/utils/openai_utils.py", "chunked_list": ["\"\"\"Utils for using OpenAI API\"\"\"\nimport json\nimport logging\nfrom typing import Any, Dict, Tuple, Union\n\nimport requests\nfrom transformers import GPT2TokenizerFast\n\nfrom chatgpt_memory.environment import OPENAI_BACKOFF, OPENAI_MAX_RETRIES, OPENAI_TIMEOUT\nfrom chatgpt_memory.errors import OpenAIError, OpenAIRateLimitError", "from chatgpt_memory.environment import OPENAI_BACKOFF, OPENAI_MAX_RETRIES, OPENAI_TIMEOUT\nfrom chatgpt_memory.errors import OpenAIError, OpenAIRateLimitError\nfrom chatgpt_memory.utils.reflection import retry_with_exponential_backoff\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_openai_tokenizer(tokenizer_name: str, use_tiktoken: bool) -> Any:\n    \"\"\"\n    Load either the tokenizer from tiktoken (if the library is available) or\n    fallback to the GPT2TokenizerFast from the transformers library.\n\n    Args:\n        tokenizer_name (str): The name of the tokenizer to load.\n        use_tiktoken (bool): Use tiktoken tokenizer or not.\n\n    Raises:\n        ImportError: When `tiktoken` package is missing.\n        To use tiktoken tokenizer install it as follows:\n        `pip install tiktoken`\n\n    Returns:\n        tokenizer: Tokenizer of either GPT2 kind or tiktoken based.\n    \"\"\"\n    tokenizer = None\n    if use_tiktoken:\n        try:\n            import tiktoken  # pylint: disable=import-error\n\n            logger.debug(\"Using tiktoken %s tokenizer\", tokenizer_name)\n            tokenizer = tiktoken.get_encoding(tokenizer_name)\n        except ImportError:\n            raise ImportError(\n                \"The `tiktoken` package not found.\",\n                \"To install it use the following:\",\n                \"`pip install tiktoken`\",\n            )\n    else:\n        logger.warning(\n            \"OpenAI tiktoken module is not available for Python < 3.8,Linux ARM64 and \"\n            \"AARCH64. Falling back to GPT2TokenizerFast.\"\n        )\n\n        logger.debug(\"Using GPT2TokenizerFast tokenizer\")\n        tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_name)\n    return tokenizer", "\n\ndef count_openai_tokens(text: str, tokenizer: Any, use_tiktoken: bool) -> int:\n    \"\"\"\n    Count the number of tokens in `text` based on the provided OpenAI `tokenizer`.\n\n    Args:\n        text (str):  A string to be tokenized.\n        tokenizer (Any): An OpenAI tokenizer.\n        use_tiktoken (bool): Use tiktoken tokenizer or not.\n\n    Returns:\n        int: Number of tokens in the text.\n    \"\"\"\n\n    if use_tiktoken:\n        return len(tokenizer.encode(text))\n    else:\n        return len(tokenizer.tokenize(text))", "\n\n@retry_with_exponential_backoff(\n    backoff_in_seconds=OPENAI_BACKOFF,\n    max_retries=OPENAI_MAX_RETRIES,\n    errors=(OpenAIRateLimitError, OpenAIError),\n)\ndef openai_request(\n    url: str,\n    headers: Dict,\n    payload: Dict,\n    timeout: Union[float, Tuple[float, float]] = OPENAI_TIMEOUT,\n) -> Dict:\n    \"\"\"\n    Make a request to the OpenAI API given a `url`, `headers`, `payload`, and\n    `timeout`.\n\n    Args:\n        url (str): The URL of the OpenAI API.\n        headers (Dict): Dictionary of HTTP Headers to send with the :class:`Request`.\n        payload (Dict): The payload to send with the request.\n        timeout (Union[float, Tuple[float, float]], optional): The timeout length of the request. The default is 30s.\n        Defaults to OPENAI_TIMEOUT.\n\n    Raises:\n        openai_error: If the request fails.\n\n    Returns:\n        Dict: OpenAI Embedding API response.\n    \"\"\"\n\n    response = requests.request(\"POST\", url, headers=headers, data=json.dumps(payload), timeout=timeout)\n    res = json.loads(response.text)\n\n    # if request is unsucessful and `status_code = 429` then,\n    # raise rate limiting error else the OpenAIError\n    if response.status_code != 200:\n        openai_error: OpenAIError\n        if response.status_code == 429:\n            openai_error = OpenAIRateLimitError(f\"API rate limit exceeded: {response.text}\")\n        else:\n            openai_error = OpenAIError(\n                f\"OpenAI returned an error.\\n\"\n                f\"Status code: {response.status_code}\\n\"\n                f\"Response body: {response.text}\",\n                status_code=response.status_code,\n            )\n        raise openai_error\n\n    return res", "\n\ndef get_prompt(message: str, history: str) -> str:\n    \"\"\"\n    Generates the prompt based on the current history and message.\n\n    Args:\n        message (str): Current message from user.\n        history (str): Retrieved history for the current message.\n        History follows the following format for example:\n        ```\n        Human: hello\n        Assistant: hello, how are you?\n        Human: good, you?\n        Assistant: I am doing good as well. How may I help you?\n        ```\n    Returns:\n        prompt: Curated prompt for the ChatGPT API based on current params.\n    \"\"\"\n    prompt = f\"\"\"Assistant is a large language model trained by OpenAI.\n\n    Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\n    Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n\n    Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n\n    {history}\n    Human: {message}\n    Assistant:\"\"\"\n\n    return prompt", ""]}
{"filename": "chatgpt_memory/datastore/config.py", "chunked_list": ["from enum import Enum\n\nfrom pydantic import BaseModel\n\n\nclass RedisIndexType(Enum):\n    hnsw = \"HNSW\"\n    flat = \"FLAT\"\n\n\nclass DataStoreConfig(BaseModel):\n    host: str\n    port: int\n    password: str", "\n\nclass DataStoreConfig(BaseModel):\n    host: str\n    port: int\n    password: str\n\n\nclass RedisDataStoreConfig(DataStoreConfig):\n    index_type: str = RedisIndexType.hnsw.value\n    vector_field_name: str = \"embedding\"\n    vector_dimensions: int = 1024\n    distance_metric: str = \"L2\"\n    number_of_vectors: int = 686\n    M: int = 40\n    EF: int = 200", "class RedisDataStoreConfig(DataStoreConfig):\n    index_type: str = RedisIndexType.hnsw.value\n    vector_field_name: str = \"embedding\"\n    vector_dimensions: int = 1024\n    distance_metric: str = \"L2\"\n    number_of_vectors: int = 686\n    M: int = 40\n    EF: int = 200\n", ""]}
{"filename": "chatgpt_memory/datastore/__init__.py", "chunked_list": ["from chatgpt_memory.datastore.config import DataStoreConfig, RedisDataStoreConfig, RedisIndexType  # noqa: F401\nfrom chatgpt_memory.datastore.redis import RedisDataStore  # noqa: F401\n"]}
{"filename": "chatgpt_memory/datastore/redis.py", "chunked_list": ["import logging\nfrom typing import Any, Dict, List\nfrom uuid import uuid4\n\nimport redis\nfrom redis.commands.search.field import TagField, TextField, VectorField\nfrom redis.commands.search.query import Query\n\nfrom chatgpt_memory.datastore.config import RedisDataStoreConfig\nfrom chatgpt_memory.datastore.datastore import DataStore", "from chatgpt_memory.datastore.config import RedisDataStoreConfig\nfrom chatgpt_memory.datastore.datastore import DataStore\n\nlogger = logging.getLogger(__name__)\n\n\nclass RedisDataStore(DataStore):\n    def __init__(self, config: RedisDataStoreConfig, do_flush_data: bool = False):\n        super().__init__(config=config)\n        self.config = config\n        self.do_flush_data = do_flush_data\n\n        self.connect()\n        self.create_index()\n\n    def connect(self):\n        \"\"\"\n        Connect to the Redis server.\n        \"\"\"\n        connection_pool = redis.ConnectionPool(\n            host=self.config.host, port=self.config.port, password=self.config.password\n        )\n        self.redis_connection = redis.Redis(connection_pool=connection_pool)\n\n        # flush data only once after establishing connection\n        if self.do_flush_data:\n            self.flush_all_documents()\n            self.do_flush_data = False\n\n    def flush_all_documents(self):\n        \"\"\"\n        Removes all documents from the redis index.\n        \"\"\"\n        self.redis_connection.flushall()\n\n    def create_index(self):\n        \"\"\"\n        Creates a Redis index with a dense vector field.\n        \"\"\"\n        try:\n            self.redis_connection.ft().create_index(\n                [\n                    VectorField(\n                        self.config.vector_field_name,\n                        self.config.index_type,\n                        {\n                            \"TYPE\": \"FLOAT32\",\n                            \"DIM\": self.config.vector_dimensions,\n                            \"DISTANCE_METRIC\": self.config.distance_metric,\n                            \"INITIAL_CAP\": self.config.number_of_vectors,\n                            \"M\": self.config.M,\n                            \"EF_CONSTRUCTION\": self.config.EF,\n                        },\n                    ),\n                    TextField(\"text\"),  # contains the original message\n                    TagField(\"conversation_id\"),  # `conversation_id` for each session\n                ]\n            )\n            logger.info(\"Created a new Redis index for storing chat history\")\n        except redis.exceptions.ResponseError as redis_error:\n            logger.info(f\"Working with existing Redis index.\\nDetails: {redis_error}\")\n\n    def index_documents(self, documents: List[Dict]):\n        \"\"\"\n        Indexes the set of documents.\n\n        Args:\n            documents (List[Dict]): List of documents to be indexed.\n        \"\"\"\n        redis_pipeline = self.redis_connection.pipeline(transaction=False)\n        for document in documents:\n            assert (\n                \"text\" in document and \"conversation_id\" in document\n            ), \"Document must include the fields `text`, and `conversation_id`\"\n            redis_pipeline.hset(uuid4().hex, mapping=document)\n        redis_pipeline.execute()\n\n    def search_documents(\n        self,\n        query_vector: bytes,\n        conversation_id: str,\n        topk: int = 5,\n    ) -> List[Any]:\n        \"\"\"\n        Searches the redis index using the query vector.\n\n        Args:\n            query_vector (np.ndarray): Embedded query vector.\n            topk (int, optional): Number of results. Defaults to 5.\n            result_fields (int, optional): Name of the fields that you want to be\n                                           returned from the search result documents\n\n        Returns:\n            List[Any]: Search result documents.\n        \"\"\"\n        query = (\n            Query(\n                f\"\"\"(@conversation_id:{{{conversation_id}}})=>[KNN {topk} \\\n                    @{self.config.vector_field_name} $vec_param AS vector_score]\"\"\"\n            )\n            .sort_by(\"vector_score\")\n            .paging(0, topk)\n            .return_fields(\n                # parse `result_fields` as strings separated by comma to pass as params\n                \"conversation_id\",\n                \"vector_score\",\n                \"text\",\n            )\n            .dialect(2)\n        )\n        params_dict = {\"vec_param\": query_vector}\n        result_documents = self.redis_connection.ft().search(query, query_params=params_dict).docs\n\n        return result_documents\n\n    def get_all_conversation_ids(self) -> List[str]:\n        \"\"\"\n        Returns conversation ids of all conversations.\n\n        Returns:\n            List[str]: List of conversation ids stored in redis.\n        \"\"\"\n        query = Query(\"*\").return_fields(\"conversation_id\")\n        result_documents = self.redis_connection.ft().search(query).docs\n\n        conversation_ids: List[str] = []\n        conversation_ids = list(\n            set([getattr(result_document, \"conversation_id\") for result_document in result_documents])\n        )\n\n        return conversation_ids\n\n    def delete_documents(self, conversation_id: str):\n        \"\"\"\n        Deletes all documents for a given conversation id.\n\n        Args:\n            conversation_id (str): Id of the conversation to be deleted.\n        \"\"\"\n        query = (\n            Query(f\"\"\"(@conversation_id:{{{conversation_id}}})\"\"\")\n            .return_fields(\n                \"id\",\n            )\n            .dialect(2)\n        )\n        for document in self.redis_connection.ft().search(query).docs:\n            document_id = getattr(document, \"id\")\n            deletion_status = self.redis_connection.ft().delete_document(document_id, delete_actual_document=True)\n\n            assert deletion_status, f\"Deletion of the document with id {document_id} failed!\"", ""]}
{"filename": "chatgpt_memory/datastore/datastore.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom typing import Any, Dict, List\n\nfrom chatgpt_memory.datastore.config import DataStoreConfig\n\n\nclass DataStore(ABC):\n    \"\"\"\n    Abstract class for datastores.\n    \"\"\"\n\n    def __init__(self, config: DataStoreConfig):\n        self.config = config\n\n    @abstractmethod\n    def connect(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def create_index(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def index_documents(self, documents: List[Dict]):\n        raise NotImplementedError\n\n    @abstractmethod\n    def search_documents(self, query_vector: Any, conversation_id: str, topk: int) -> List[Any]:\n        raise NotImplementedError", ""]}
{"filename": "chatgpt_memory/llm_client/config.py", "chunked_list": ["from pydantic import BaseModel\n\n\nclass LLMClientConfig(BaseModel):\n    api_key: str\n    time_out: float = 30\n"]}
{"filename": "chatgpt_memory/llm_client/__init__.py", "chunked_list": ["from chatgpt_memory.llm_client.openai.conversation.chatgpt_client import (  # noqa: F401\n    ChatGPTClient,\n    ChatGPTConfig,\n    ChatGPTResponse,\n)\nfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingClient  # noqa: F401\nfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingConfig  # noqa: F401\nfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingModels  # noqa: F401\n", ""]}
{"filename": "chatgpt_memory/llm_client/llm_client.py", "chunked_list": ["from abc import ABC\n\nfrom chatgpt_memory.llm_client.config import LLMClientConfig\n\n\nclass LLMClient(ABC):\n    \"\"\"\n    Wrapper for the HTTP APIs for LLMs acting as data container for API configurations.\n    \"\"\"\n\n    def __init__(self, config: LLMClientConfig):\n        self._api_key = config.api_key\n        self._time_out = config.time_out\n\n    @property\n    def api_key(self):\n        return self._api_key\n\n    @property\n    def time_out(self):\n        return self._time_out", ""]}
{"filename": "chatgpt_memory/llm_client/openai/__init__.py", "chunked_list": [""]}
{"filename": "chatgpt_memory/llm_client/openai/embedding/embedding_client.py", "chunked_list": ["import logging\nfrom typing import Any, Dict, List, Union\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom chatgpt_memory.constants import MAX_ALLOWED_SEQ_LEN_001, MAX_ALLOWED_SEQ_LEN_002\nfrom chatgpt_memory.llm_client.llm_client import LLMClient\nfrom chatgpt_memory.llm_client.openai.embedding.config import EmbeddingConfig, EmbeddingModels\nfrom chatgpt_memory.utils.openai_utils import count_openai_tokens, load_openai_tokenizer, openai_request", "from chatgpt_memory.llm_client.openai.embedding.config import EmbeddingConfig, EmbeddingModels\nfrom chatgpt_memory.utils.openai_utils import count_openai_tokens, load_openai_tokenizer, openai_request\n\nlogger = logging.getLogger(__name__)\n\n\nclass EmbeddingClient(LLMClient):\n    def __init__(self, config: EmbeddingConfig):\n        super().__init__(config=config)\n\n        self.openai_embedding_config = config\n        model_class: str = EmbeddingModels(self.openai_embedding_config.model).name\n\n        tokenizer = self._setup_encoding_models(\n            model_class,\n            self.openai_embedding_config.model,\n            self.openai_embedding_config.max_seq_len,\n        )\n        self._tokenizer = load_openai_tokenizer(\n            tokenizer_name=tokenizer,\n            use_tiktoken=self.openai_embedding_config.use_tiktoken,\n        )\n\n    def _setup_encoding_models(self, model_class: str, model_name: str, max_seq_len: int):\n        \"\"\"\n        Setup the encoding models for the retriever.\n\n        Raises:\n            ImportError: When `tiktoken` package is missing.\n            To use tiktoken tokenizer install it as follows:\n            `pip install tiktoken`\n        \"\"\"\n\n        tokenizer_name = \"gpt2\"\n        # new generation of embedding models (December 2022), specify the full name\n        if model_name.endswith(\"-002\"):\n            self.query_encoder_model = model_name\n            self.doc_encoder_model = model_name\n            self.max_seq_len = min(MAX_ALLOWED_SEQ_LEN_002, max_seq_len)\n            if self.openai_embedding_config.use_tiktoken:\n                try:\n                    from tiktoken.model import MODEL_TO_ENCODING\n\n                    tokenizer_name = MODEL_TO_ENCODING.get(model_name, \"cl100k_base\")\n                except ImportError:\n                    raise ImportError(\n                        \"The `tiktoken` package not found.\",\n                        \"To install it use the following:\",\n                        \"`pip install tiktoken`\",\n                    )\n        else:\n            self.query_encoder_model = f\"text-search-{model_class}-query-001\"\n            self.doc_encoder_model = f\"text-search-{model_class}-doc-001\"\n            self.max_seq_len = min(MAX_ALLOWED_SEQ_LEN_001, max_seq_len)\n\n        return tokenizer_name\n\n    def _ensure_text_limit(self, text: str) -> str:\n        \"\"\"\n         Ensure that length of the text is within the maximum length of the model.\n        OpenAI v1 embedding models have a limit of 2046 tokens, and v2 models have\n        a limit of 8191 tokens.\n\n        Args:\n            text (str):  Text to be checked if it exceeds the max token limit\n\n        Returns:\n            text (str): Trimmed text if exceeds the max token limit\n        \"\"\"\n        n_tokens = count_openai_tokens(text, self._tokenizer, self.openai_embedding_config.use_tiktoken)\n        if n_tokens <= self.max_seq_len:\n            return text\n\n        logger.warning(\n            \"The prompt has been truncated from %s tokens to %s tokens to fit\" \"within the max token limit.\",\n            \"Reduce the length of the prompt to prevent it from being cut off.\",\n            n_tokens,\n            self.max_seq_len,\n        )\n\n        if self.openai_embedding_config.use_tiktoken:\n            tokenized_payload = self._tokenizer.encode(text)\n            decoded_string = self._tokenizer.decode(tokenized_payload[: self.max_seq_len])\n        else:\n            tokenized_payload = self._tokenizer.tokenize(text)\n            decoded_string = self._tokenizer.convert_tokens_to_string(tokenized_payload[: self.max_seq_len])\n\n        return decoded_string\n\n    def embed(self, model: str, text: List[str]) -> np.ndarray:\n        \"\"\"\n        Embeds the batch of texts using the specified LLM.\n\n        Args:\n            model (str): LLM model name for embeddings.\n            text (List[str]): List of documents to be embedded.\n\n        Raises:\n            ValueError: When the OpenAI API key is missing.\n\n        Returns:\n            np.ndarray: embeddings for the input documents.\n        \"\"\"\n        if self.api_key is None:\n            raise ValueError(\n                \"OpenAI API key is not set. You can set it via the \" \"`api_key` parameter of the `LLMClient`.\"\n            )\n\n        generated_embeddings: List[Any] = []\n\n        headers: Dict[str, str] = {\"Content-Type\": \"application/json\"}\n        payload: Dict[str, Union[List[str], str]] = {\"model\": model, \"input\": text}\n        headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        res = openai_request(\n            url=self.openai_embedding_config.url,\n            headers=headers,\n            payload=payload,\n            timeout=self.time_out,\n        )\n\n        unordered_embeddings = [(ans[\"index\"], ans[\"embedding\"]) for ans in res[\"data\"]]\n        ordered_embeddings = sorted(unordered_embeddings, key=lambda x: x[0])\n\n        generated_embeddings = [emb[1] for emb in ordered_embeddings]\n\n        return np.array(generated_embeddings)\n\n    def embed_batch(self, model: str, text: List[str]) -> np.ndarray:\n        all_embeddings = []\n        for i in tqdm(\n            range(0, len(text), self.openai_embedding_config.batch_size),\n            disable=not self.openai_embedding_config.progress_bar,\n            desc=\"Calculating embeddings\",\n        ):\n            batch = text[i : i + self.openai_embedding_config.batch_size]\n            batch_limited = [self._ensure_text_limit(content) for content in batch]\n            generated_embeddings = self.embed(model, batch_limited)\n            all_embeddings.append(generated_embeddings)\n\n        return np.concatenate(all_embeddings)\n\n    def embed_queries(self, queries: List[str]) -> np.ndarray:\n        return self.embed_batch(self.query_encoder_model, queries)\n\n    def embed_documents(self, docs: List[Dict]) -> np.ndarray:\n        return self.embed_batch(self.doc_encoder_model, [d[\"text\"] for d in docs])", ""]}
{"filename": "chatgpt_memory/llm_client/openai/embedding/config.py", "chunked_list": ["from enum import Enum\n\nfrom chatgpt_memory.llm_client.config import LLMClientConfig\n\n\nclass EmbeddingModels(Enum):\n    ada = \"*-ada-*-001\"\n    babbage = \"*-babbage-*-001\"\n    curie = \"*-curie-*-001\"\n    davinci = \"*-davinci-*-001\"", "\n\nclass EmbeddingConfig(LLMClientConfig):\n    url: str = \"https://api.openai.com/v1/embeddings\"\n    batch_size: int = 64\n    progress_bar: bool = False\n    model: str = EmbeddingModels.ada.value\n    max_seq_len: int = 8191\n    use_tiktoken: bool = False\n", ""]}
{"filename": "chatgpt_memory/llm_client/openai/embedding/__init__.py", "chunked_list": [""]}
{"filename": "chatgpt_memory/llm_client/openai/conversation/config.py", "chunked_list": ["from chatgpt_memory.llm_client.config import LLMClientConfig\n\n\nclass ChatGPTConfig(LLMClientConfig):\n    temperature: float = 0\n    model_name: str = \"gpt-3.5-turbo\"\n    max_retries: int = 6\n    max_tokens: int = 256\n    verbose: bool = False\n", ""]}
{"filename": "chatgpt_memory/llm_client/openai/conversation/__init__.py", "chunked_list": [""]}
{"filename": "chatgpt_memory/llm_client/openai/conversation/chatgpt_client.py", "chunked_list": ["import logging\nimport uuid\n\nfrom langchain import LLMChain, OpenAI, PromptTemplate\nfrom pydantic import BaseModel\n\nfrom chatgpt_memory.llm_client.llm_client import LLMClient\nfrom chatgpt_memory.llm_client.openai.conversation.config import ChatGPTConfig\nfrom chatgpt_memory.memory.manager import MemoryManager\nfrom chatgpt_memory.utils.openai_utils import get_prompt", "from chatgpt_memory.memory.manager import MemoryManager\nfrom chatgpt_memory.utils.openai_utils import get_prompt\n\nlogger = logging.getLogger(__name__)\n\n\nclass ChatGPTResponse(BaseModel):\n    conversation_id: str\n    message: str\n    chat_gpt_answer: str", "\n\nclass ChatGPTClient(LLMClient):\n    \"\"\"\n    ChatGPT client allows to interact with the ChatGPT model alonside having infinite contextual and adaptive memory.\n\n    \"\"\"\n\n    def __init__(self, config: ChatGPTConfig, memory_manager: MemoryManager):\n        super().__init__(config=config)\n        prompt = PromptTemplate(input_variables=[\"prompt\"], template=\"{prompt}\")\n        self.chatgpt_chain = LLMChain(\n            llm=OpenAI(\n                temperature=config.temperature,\n                openai_api_key=self.api_key,\n                model_name=config.model_name,\n                max_retries=config.max_retries,\n                max_tokens=config.max_tokens,\n            ),\n            prompt=prompt,\n            verbose=config.verbose,\n        )\n        self.memory_manager = memory_manager\n\n    def converse(self, message: str, conversation_id: str = None) -> ChatGPTResponse:\n        \"\"\"\n        Allows user to chat with user by leveraging the infinite contextual memor for fetching and\n        adding historical messages to the prompt to the ChatGPT model.\n\n        Args:\n            message (str): Message by the human user.\n            conversation_id (str, optional): Id of the conversation, if session already exists. Defaults to None.\n\n        Returns:\n            ChatGPTResponse: Response includes answer from th ChatGPT, conversation_id, and human message.\n        \"\"\"\n        if not conversation_id:\n            conversation_id = uuid.uuid4().hex\n\n        history = \"\"\n        try:\n            past_messages = self.memory_manager.get_messages(conversation_id=conversation_id, query=message)\n            history = \"\\n\".join([past_message.text for past_message in past_messages if getattr(past_message, \"text\")])\n        except ValueError as history_not_found_error:\n            logger.warning(\n                f\"No previous chat history found for conversation_id: {conversation_id}.\\nDetails: {history_not_found_error}\"\n            )\n        prompt = get_prompt(message=message, history=history)\n        chat_gpt_answer = self.chatgpt_chain.predict(prompt=prompt)\n\n        if len(message.strip()) and len(chat_gpt_answer.strip()):\n            self.memory_manager.add_message(conversation_id=conversation_id, human=message, assistant=chat_gpt_answer)\n\n        return ChatGPTResponse(message=message, chat_gpt_answer=chat_gpt_answer, conversation_id=conversation_id)", ""]}
{"filename": "chatgpt_memory/memory/manager.py", "chunked_list": ["from typing import Any, Dict, List\n\nimport numpy as np\n\nfrom chatgpt_memory.datastore.redis import RedisDataStore\nfrom chatgpt_memory.llm_client.openai.embedding.embedding_client import EmbeddingClient\n\nfrom .memory import Memory\n\n\nclass MemoryManager:\n    \"\"\"\n    Manages the memory of conversations.\n\n    Attributes:\n        datastore (DataStore): Datastore to use for storing and retrieving memories.\n        embed_client (EmbeddingClient): Embedding client to call for embedding conversations.\n        conversations (List[Memory]): List of conversation IDs to memories to be managed.\n    \"\"\"\n\n    def __init__(self, datastore: RedisDataStore, embed_client: EmbeddingClient, topk: int = 5) -> None:\n        \"\"\"\n        Initializes the memory manager.\n\n        Args:\n            datastore (DataStore): Datastore to be used. Assumed to be connected.\n            embed_client (EmbeddingClient): Embedding client to be used.\n            topk (int): Number of past message to be retrieved as context for current message.\n        \"\"\"\n        self.datastore = datastore\n        self.embed_client = embed_client\n        self.topk = topk\n        self.conversations: List[Memory] = [\n            Memory(conversation_id=conversation_id) for conversation_id in datastore.get_all_conversation_ids()\n        ]\n\n    def __del__(self) -> None:\n        \"\"\"Clear the memory manager when manager is deleted.\"\"\"\n        self.clear()\n\n    def add_conversation(self, conversation: Memory) -> None:\n        \"\"\"\n        Adds a conversation to the memory manager to be stored and manage.\n\n        Args:\n            conversation (Memory): Conversation to be added.\n        \"\"\"\n        if conversation not in self.conversations:\n            self.conversations.append(conversation)\n\n    def remove_conversation(self, conversation: Memory) -> None:\n        \"\"\"\n        Removes a conversation from the memory manager.\n\n        Args:\n            conversation (Memory): Conversation to be removed containing `conversation_id`.\n        \"\"\"\n        if conversation not in self.conversations:\n            return\n\n        conversation_idx = self.conversations.index(conversation)\n        if conversation_idx >= 0:\n            del self.conversations[conversation_idx]\n            self.datastore.delete_documents(conversation_id=conversation.conversation_id)\n\n    def clear(self) -> None:\n        \"\"\"\n        Clears the memory manager.\n        \"\"\"\n        self.datastore.flush_all_documents()\n        self.conversations = []\n\n    def add_message(self, conversation_id: str, human: str, assistant: str) -> None:\n        \"\"\"\n        Adds a message to a conversation.\n\n        Args:\n            conversation_id (str): ID of the conversation to add the message to.\n            human (str): User message.\n            assistant (str): Assistant message.\n        \"\"\"\n        document: Dict = {\"text\": f\"Human: {human}\\nAssistant: {assistant}\", \"conversation_id\": conversation_id}\n        document[\"embedding\"] = self.embed_client.embed_documents(docs=[document])[0].astype(np.float32).tobytes()\n        self.datastore.index_documents(documents=[document])\n\n        # optionally check if it is a new conversation\n        self.add_conversation(Memory(conversation_id=conversation_id))\n\n    def get_messages(self, conversation_id: str, query: str) -> List[Any]:\n        \"\"\"\n        Gets the messages of a conversation using the query message.\n\n        Args:\n            conversation_id (str): ID of the conversation to get the messages of.\n            query (str): Current user message you want to pull history for to use in the prompt.\n            topk (int): Number of messages to be returned. Defaults to 5.\n\n        Returns:\n            List[Any]: List of messages of the conversation.\n        \"\"\"\n        if Memory(conversation_id=conversation_id) not in self.conversations:\n            raise ValueError(f\"Conversation id: {conversation_id} is not present in past conversations.\")\n\n        query_vector = self.embed_client.embed_queries([query])[0].astype(np.float32).tobytes()\n        messages = self.datastore.search_documents(\n            query_vector=query_vector, conversation_id=conversation_id, topk=self.topk\n        )\n        return messages", "\n\nclass MemoryManager:\n    \"\"\"\n    Manages the memory of conversations.\n\n    Attributes:\n        datastore (DataStore): Datastore to use for storing and retrieving memories.\n        embed_client (EmbeddingClient): Embedding client to call for embedding conversations.\n        conversations (List[Memory]): List of conversation IDs to memories to be managed.\n    \"\"\"\n\n    def __init__(self, datastore: RedisDataStore, embed_client: EmbeddingClient, topk: int = 5) -> None:\n        \"\"\"\n        Initializes the memory manager.\n\n        Args:\n            datastore (DataStore): Datastore to be used. Assumed to be connected.\n            embed_client (EmbeddingClient): Embedding client to be used.\n            topk (int): Number of past message to be retrieved as context for current message.\n        \"\"\"\n        self.datastore = datastore\n        self.embed_client = embed_client\n        self.topk = topk\n        self.conversations: List[Memory] = [\n            Memory(conversation_id=conversation_id) for conversation_id in datastore.get_all_conversation_ids()\n        ]\n\n    def __del__(self) -> None:\n        \"\"\"Clear the memory manager when manager is deleted.\"\"\"\n        self.clear()\n\n    def add_conversation(self, conversation: Memory) -> None:\n        \"\"\"\n        Adds a conversation to the memory manager to be stored and manage.\n\n        Args:\n            conversation (Memory): Conversation to be added.\n        \"\"\"\n        if conversation not in self.conversations:\n            self.conversations.append(conversation)\n\n    def remove_conversation(self, conversation: Memory) -> None:\n        \"\"\"\n        Removes a conversation from the memory manager.\n\n        Args:\n            conversation (Memory): Conversation to be removed containing `conversation_id`.\n        \"\"\"\n        if conversation not in self.conversations:\n            return\n\n        conversation_idx = self.conversations.index(conversation)\n        if conversation_idx >= 0:\n            del self.conversations[conversation_idx]\n            self.datastore.delete_documents(conversation_id=conversation.conversation_id)\n\n    def clear(self) -> None:\n        \"\"\"\n        Clears the memory manager.\n        \"\"\"\n        self.datastore.flush_all_documents()\n        self.conversations = []\n\n    def add_message(self, conversation_id: str, human: str, assistant: str) -> None:\n        \"\"\"\n        Adds a message to a conversation.\n\n        Args:\n            conversation_id (str): ID of the conversation to add the message to.\n            human (str): User message.\n            assistant (str): Assistant message.\n        \"\"\"\n        document: Dict = {\"text\": f\"Human: {human}\\nAssistant: {assistant}\", \"conversation_id\": conversation_id}\n        document[\"embedding\"] = self.embed_client.embed_documents(docs=[document])[0].astype(np.float32).tobytes()\n        self.datastore.index_documents(documents=[document])\n\n        # optionally check if it is a new conversation\n        self.add_conversation(Memory(conversation_id=conversation_id))\n\n    def get_messages(self, conversation_id: str, query: str) -> List[Any]:\n        \"\"\"\n        Gets the messages of a conversation using the query message.\n\n        Args:\n            conversation_id (str): ID of the conversation to get the messages of.\n            query (str): Current user message you want to pull history for to use in the prompt.\n            topk (int): Number of messages to be returned. Defaults to 5.\n\n        Returns:\n            List[Any]: List of messages of the conversation.\n        \"\"\"\n        if Memory(conversation_id=conversation_id) not in self.conversations:\n            raise ValueError(f\"Conversation id: {conversation_id} is not present in past conversations.\")\n\n        query_vector = self.embed_client.embed_queries([query])[0].astype(np.float32).tobytes()\n        messages = self.datastore.search_documents(\n            query_vector=query_vector, conversation_id=conversation_id, topk=self.topk\n        )\n        return messages", ""]}
{"filename": "chatgpt_memory/memory/__init__.py", "chunked_list": ["from chatgpt_memory.memory.manager import MemoryManager  # noqa: F401\nfrom chatgpt_memory.memory.memory import Memory  # noqa: F401\n"]}
{"filename": "chatgpt_memory/memory/memory.py", "chunked_list": ["\"\"\"\nContains a memory dataclass.\n\"\"\"\nfrom pydantic import BaseModel\n\n\nclass Memory(BaseModel):\n    \"\"\"\n    A memory dataclass.\n    \"\"\"\n\n    conversation_id: str\n    \"\"\"ID of the conversation.\"\"\"", ""]}
{"filename": "examples/simple_usage.py", "chunked_list": ["#!/bin/env python3\n\"\"\"\nThis script describes a simple usage of the library.\nYou can see a breakdown of the individual steps in the README.md file.\n\"\"\"\nfrom chatgpt_memory.datastore import RedisDataStore, RedisDataStoreConfig\n\n## set the following ENVIRONMENT Variables before running this script\n# Import necessary modules\nfrom chatgpt_memory.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT", "# Import necessary modules\nfrom chatgpt_memory.environment import OPENAI_API_KEY, REDIS_HOST, REDIS_PASSWORD, REDIS_PORT\nfrom chatgpt_memory.llm_client import ChatGPTClient, ChatGPTConfig, EmbeddingClient, EmbeddingConfig\nfrom chatgpt_memory.memory import MemoryManager\n\n# Instantiate an EmbeddingConfig object with the OpenAI API key\nembedding_config = EmbeddingConfig(api_key=OPENAI_API_KEY)\n\n# Instantiate an EmbeddingClient object with the EmbeddingConfig object\nembed_client = EmbeddingClient(config=embedding_config)", "# Instantiate an EmbeddingClient object with the EmbeddingConfig object\nembed_client = EmbeddingClient(config=embedding_config)\n\n# Instantiate a RedisDataStoreConfig object with the Redis connection details\nredis_datastore_config = RedisDataStoreConfig(\n    host=REDIS_HOST,\n    port=REDIS_PORT,\n    password=REDIS_PASSWORD,\n)\n", ")\n\n# Instantiate a RedisDataStore object with the RedisDataStoreConfig object\nredis_datastore = RedisDataStore(config=redis_datastore_config)\n\n# Instantiate a MemoryManager object with the RedisDataStore object and EmbeddingClient object\nmemory_manager = MemoryManager(datastore=redis_datastore, embed_client=embed_client, topk=1)\n\n# Instantiate a ChatGPTConfig object with the OpenAI API key and verbose set to True\nchat_gpt_config = ChatGPTConfig(api_key=OPENAI_API_KEY, verbose=False)", "# Instantiate a ChatGPTConfig object with the OpenAI API key and verbose set to True\nchat_gpt_config = ChatGPTConfig(api_key=OPENAI_API_KEY, verbose=False)\n\n# Instantiate a ChatGPTClient object with the ChatGPTConfig object and MemoryManager object\nchat_gpt_client = ChatGPTClient(config=chat_gpt_config, memory_manager=memory_manager)\n\n\n# Initialize conversation_id to None\nconversation_id = None\n", "conversation_id = None\n\n# Start the chatbot loop\nwhile True:\n    # Prompt the user for input\n    user_message = input(\"\\n \\033[92m Please enter your message: \")\n\n    # Use the ChatGPTClient object to generate a response\n    response = chat_gpt_client.converse(message=user_message, conversation_id=None)\n", "    response = chat_gpt_client.converse(message=user_message, conversation_id=None)\n\n    # Update the conversation_id with the conversation_id from the response\n    conversation_id = response.conversation_id\n    print(\"\\n \\033[96m Assisstant: \" + response.chat_gpt_answer)\n    # Print the response generated by the chatbot\n    # print(response.chat_gpt_answer)\n"]}
