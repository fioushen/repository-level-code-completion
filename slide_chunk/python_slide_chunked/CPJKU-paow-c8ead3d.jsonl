{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport io\nimport os\nfrom setuptools import find_packages, setup\n\n\n# Package meta-data.\nNAME = 'paow'", "# Package meta-data.\nNAME = 'paow'\nDESCRIPTION = 'Symbolic music generation'\nKEYWORDS = 'generation midi score'\nURL = \"https://github.com/cpjku/paow\"\nAUTHOR = 'Silvan Peter'\nREQUIRES_PYTHON = '>=3.7'\nVERSION = '0.0.1'\n\n# What packages are required for this module to be executed?", "\n# What packages are required for this module to be executed?\nREQUIRED = [\n    'numpy',\n    'partitura>=1.1.0'\n    ]\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\ntry:\n    with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f:\n        long_description = '\\n' + f.read()\nexcept FileNotFoundError:\n    long_description = DESCRIPTION", "\ntry:\n    with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f:\n        long_description = '\\n' + f.read()\nexcept FileNotFoundError:\n    long_description = DESCRIPTION\n\n# Load the package's __version__.py module as a dictionary.\nabout = {}\nif not VERSION:\n    with open(os.path.join(here, NAME, '__version__.py')) as f:\n        exec(f.read(), about)\nelse:\n    about['__version__'] = VERSION", "about = {}\nif not VERSION:\n    with open(os.path.join(here, NAME, '__version__.py')) as f:\n        exec(f.read(), about)\nelse:\n    about['__version__'] = VERSION\n\n# Where the magic happens:\nsetup(\n    name=NAME,", "setup(\n    name=NAME,\n    version=about['__version__'],\n    description=DESCRIPTION,\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    keywords=KEYWORDS,\n    author=AUTHOR,\n    url=URL,\n    python_requires=REQUIRES_PYTHON,", "    url=URL,\n    python_requires=REQUIRES_PYTHON,\n    packages=find_packages(exclude=('tests',)),\n        package_data={\n        \"paow\": [\n            \"assets/dummy.txt\",\n        ]\n        },\n    install_requires=REQUIRED,\n    extras_require={},", "    install_requires=REQUIRED,\n    extras_require={},\n    include_package_data=True,\n    license=\"Apache 2.0\",\n    classifiers=[\n        # Trove classifiers\n        # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n    ],", "        \"Programming Language :: Python :: 3\",\n    ],\n)\n"]}
{"filename": "paow/__init__.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\"\"\"\nThe top level of the package.\n\"\"\"\nimport pkg_resources\nEXAMPLE = pkg_resources.resource_filename(\"paow\", \n                                          \"assets/dummy.txt\")\n\n", "\n\nfrom .utils import Sequencer, MidiRouter\nfrom .evolutionary import Optimizer\n\n__all__ = [\n    \"Sequencer\",\n    \"MidiRouter\",\n    \"Optimizer\",\n]", "    \"Optimizer\",\n]\n"]}
{"filename": "paow/23_05_31/test_pedal_thirds.py", "chunked_list": ["#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nThis script generates a MIDI file with octave-doubled arpeggios made of stacked major and minor thirds.\nWhether the next third is major or minor as well as the root pitch is randomly sampled.\nThe tempo starts at ~0.25 notes/sec and over the course of one hour is increased to 16 notes/sec.\nThe sustain pedal is controlled by sinusoid curves with varying frequency constants.\n\nEnjoy!\n\"\"\"", "Enjoy!\n\"\"\"\nfrom mido import Message, MidiFile, MidiTrack\nimport numpy as np\n\nmid = MidiFile()\ntrack = MidiTrack()\nmid.tracks.append(track)\n\n# some sinusoids with different frequency constants for pedal controls", "\n# some sinusoids with different frequency constants for pedal controls\nmidi_pedal_maps = []\nfactors = np.arange(1,11)\nfor n in range(1,11):\n    midi_pedal_maps.append(\n    np.round((np.sin(2*np.pi*np.arange(0,128)/(128/n*4))+1)/2*127).astype(int)\n    )\n\n# tempos: 100 tempos from whole notes at 60 bpm to sixteenth notes at 240 bpm", "\n# tempos: 100 tempos from whole notes at 60 bpm to sixteenth notes at 240 bpm\n# => 0.25 notes/sec - 16 notes/sec\n# MIDI default parts per quarter is 480, MIDI default tempo is 120 bpm -> 960 ticks/sec\ntempos_in_notes_per_sec = np.linspace(0.25,16,100)\ntempos_in_ticks_per_note = 960 / tempos_in_notes_per_sec\n# 128 notes per tempo gives a duration of ~1h\ntotal_duration_in_hours = (1/tempos_in_notes_per_sec * 128).sum()/3600 # 1.0089\n# get nice integer times divisible by two\ntempos_in_ticks_per_note_int = np.round(tempos_in_ticks_per_note / 2).astype(int) ", "# get nice integer times divisible by two\ntempos_in_ticks_per_note_int = np.round(tempos_in_ticks_per_note / 2).astype(int) \n\n# creating the notes\nfor l in range(100):\n    # lowest MIDI pitch: 21\n    root = np.random.randint(21, 40)\n    pitch = root\n    local_duration_halfed = tempos_in_ticks_per_note_int[l] \n    for k in range(128):\n        #track.append(Message('program_change', program=12, time=0))      \n        track.append(Message('note_on', note=pitch, velocity=44, time=local_duration_halfed))\n        track.append(Message('note_on', note=pitch+12, velocity=44, time=0))\n        track.append(Message('note_off', note=pitch, velocity=0, time=local_duration_halfed))\n        track.append(Message('note_off', note=pitch+12, velocity=0, time=0))\n        track.append(Message(\"control_change\",\n                            control=64,\n                            value=midi_pedal_maps[l%10][k]))\n        # highest MIDI pitch: 108\n        pitch = (pitch + np.random.randint(3,5) - root )%58 + root", "\nmid.save('test_pedal_thirds_hour.mid')"]}
{"filename": "paow/23_07_12/sample_continous.py", "chunked_list": ["#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nThis experiment outpaints the super mario theme for an hour, \ngradually forgetting where it started.\n\nEnjoy!\n\"\"\"\nimport time\n", "import time\n\nimport numpy as np\nfrom note_seq import note_sequence_to_midi_file, midi_file_to_note_sequence\n\nfrom hparams import get_sampler_hparams\nfrom utils import get_sampler, load_model\nfrom utils.sampler_utils import get_samples, np_to_ns, ns_to_np\n\n\ndef sample_cont(sampler, H):\n    x_T = None\n    batch = 1\n    if H.piece:\n        ns = midi_file_to_note_sequence(H.piece)\n        bars = min(64, int(max([n.end_time for n in ns.notes]) // 2))\n        npy = ns_to_np(ns, bars, 'melody').outputs[0]\n\n        x_T = np.zeros((batch, H.NOTES, 3), dtype=int)\n\n        x_T[:, :] = H.codebook_size\n        x_T[:, :npy.shape[0], 0] = npy[:, 0]\n\n        if npy.shape[1] == 3:\n            x_T[:, :npy.shape[0], 1] = npy[:, 1]\n            x_T[:, :npy.shape[0], 2] = npy[:, 2]\n\n    n_samples = 0\n    sampler.sampling_batch_size = batch\n    piece = None\n    while n_samples < H.n_samples:\n        sa = get_samples(sampler, H.sample_steps, x_T)\n\n        if piece is None:\n            piece = sa\n        else:\n            piece = np.append(piece, sa[:, sa.shape[1] // 2:], axis=1)\n\n        x_T = np.zeros((batch, H.NOTES, 3), dtype=int)\n        x_T[:, :] = H.codebook_size\n        x_T[:, :sa.shape[1] // 2] = sa[:, sa.shape[1] // 2:]\n        ns = np_to_ns(piece)\n\n        for _ in ns:\n            n_samples += 1\n        print(f'{n_samples}/{H.n_samples}')\n    for n in ns:\n        note_sequence_to_midi_file(n, f'data/out/conti{time.time()}.mid')", "\n\ndef sample_cont(sampler, H):\n    x_T = None\n    batch = 1\n    if H.piece:\n        ns = midi_file_to_note_sequence(H.piece)\n        bars = min(64, int(max([n.end_time for n in ns.notes]) // 2))\n        npy = ns_to_np(ns, bars, 'melody').outputs[0]\n\n        x_T = np.zeros((batch, H.NOTES, 3), dtype=int)\n\n        x_T[:, :] = H.codebook_size\n        x_T[:, :npy.shape[0], 0] = npy[:, 0]\n\n        if npy.shape[1] == 3:\n            x_T[:, :npy.shape[0], 1] = npy[:, 1]\n            x_T[:, :npy.shape[0], 2] = npy[:, 2]\n\n    n_samples = 0\n    sampler.sampling_batch_size = batch\n    piece = None\n    while n_samples < H.n_samples:\n        sa = get_samples(sampler, H.sample_steps, x_T)\n\n        if piece is None:\n            piece = sa\n        else:\n            piece = np.append(piece, sa[:, sa.shape[1] // 2:], axis=1)\n\n        x_T = np.zeros((batch, H.NOTES, 3), dtype=int)\n        x_T[:, :] = H.codebook_size\n        x_T[:, :sa.shape[1] // 2] = sa[:, sa.shape[1] // 2:]\n        ns = np_to_ns(piece)\n\n        for _ in ns:\n            n_samples += 1\n        print(f'{n_samples}/{H.n_samples}')\n    for n in ns:\n        note_sequence_to_midi_file(n, f'data/out/conti{time.time()}.mid')", "\n\nif __name__ == '__main__':\n    H = get_sampler_hparams('sample')\n    H.sample_schedule = \"rand\"\n    sampler = get_sampler(H).cuda()\n    sampler = load_model(\n                sampler, f'{H.sampler}_ema', H.load_step, H.load_dir)\n\n    sample_cont(sampler, H)"]}
{"filename": "paow/23_07_05/run.py", "chunked_list": ["import numpy as np\nimport mido\nimport random\nimport time\nimport os\nimport audiofile\nfrom audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, AirAbsorption, ApplyImpulseResponse, TimeMask, GainTransition\nimport zipfile\nimport wave\nimport pyaudio", "import wave\nimport pyaudio\nimport threading\nfrom scipy import signal\nfrom scipy.interpolate import interp1d\nfrom pedalboard import Reverb, Gain, Chorus, Convolution, Compressor, Phaser, LadderFilter, Pedalboard\n\n\ngrammar = {\n    \"Pattern\" : [[\"P1\", \"A1\", \"D1\", \"W1\"], [\"P2\", \"A2\", \"D2\", \"W2\"], [\"P3\", \"A3\", \"D3\", \"W3\"], [\"P4\", \"A4\", \"D4\", \"W4\"]],", "grammar = {\n    \"Pattern\" : [[\"P1\", \"A1\", \"D1\", \"W1\"], [\"P2\", \"A2\", \"D2\", \"W2\"], [\"P3\", \"A3\", \"D3\", \"W3\"], [\"P4\", \"A4\", \"D4\", \"W4\"]],\n    \"P1\": np.array([67, 75, 71, 78, 77, 88]),\n    \"P2\": np.array([27, 98, 62, 55, 71, 63]),\n    \"P4\": np.array([27, 98, 62, 55, 71, 63]),\n    \"P3\": np.array([55, 62, 69, 76, 81, 82, 88]),\n    \"P4\": np.array([27, 100]),\n    \"A1\": np.array([30, 30, 30, 30, 30, 30]),\n    \"A2\": np.array([20, 20, 20, 20, 20, 20]),\n    \"A3\": np.array([20, 20, 20, 20, 20, 20, 20]),", "    \"A2\": np.array([20, 20, 20, 20, 20, 20]),\n    \"A3\": np.array([20, 20, 20, 20, 20, 20, 20]),\n    \"A4\": np.array([20, 20]),\n    \"D1\": np.array([300, 300, 300, 300, 300, 300]),\n    \"D2\": np.array([4000, 4000, 3000, 3000, 3000, 3000]),\n    \"D3\": np.array([3000, 3000, 3000, 3000, 3000, 3000, 3000]),\n    \"D4\": np.array([3000, 3000]),\n    \"W1\": np.array([800, 800, 800, 800, 800, 800]),\n    \"W2\": np.array([0, 5000, 0, 0, 0, 3000]),\n    \"W3\": np.array([100, 100, 100, 100, 100, 100, 3000]),", "    \"W2\": np.array([0, 5000, 0, 0, 0, 3000]),\n    \"W3\": np.array([100, 100, 100, 100, 100, 100, 3000]),\n    \"W4\": np.array([0, 3000])\n\n}\n\ndef audio_grammar(main_path):\n    path1 = os.path.join(main_path, \"T1\")\n    path2 = os.path.join(main_path, \"T2\")\n    path3 = os.path.join(main_path, \"T3\")\n    one = list(map(lambda x: os.path.join(path1, x), os.listdir(path1)))\n    two = list(map(lambda x: os.path.join(path2, x), os.listdir(path2)))\n    three = list(map(lambda x: os.path.join(path3, x), os.listdir(path3)))\n    grammar = {\n        \"Type\": [\"T1\", \"T2\", \"T3\"],\n        \"T1\": one,\n        \"T2\": two,\n        \"T3\": three\n    }\n    return grammar", "\n\n\ndef perturbation(p, a, d, w):\n    new_p = np.array(p) + np.random.randint(-1, 2, len(p))\n    new_a = np.array(a) + np.random.randint(-10, 10, len(a))\n    add_mask = np.zeros(len(a))\n    ind_a = random.choice(np.arange(len(a)))\n    add_mask[ind_a] = 10\n    new_a = new_a + add_mask\n    new_d = np.array(d) + np.random.randint(-1000, 1000, len(d))\n    new_w = np.array(w) + np.random.randint(-300, 300, len(d))\n    # Filter out negative values\n    new_d[new_d < 0] = 0\n    new_w[new_w < 0] = 0\n    return new_p, new_a, new_d, new_w", "\n\ndef audio_param_perturbation(parameters):\n    \"\"\"Perturb the parameters of the audio file\"\"\"\n    new_parameters = {}\n    for key, value in parameters.items():\n        if key == \"min_amplitude\":\n            # sample from a gaussian distribution\n            new_min_amplitude = np.random.normal(value, 0.1)\n            new_parameters[key] = new_min_amplitude if new_min_amplitude > 0 else 0.001\n            new_parameters[\"max_amplitude\"] = new_parameters[\"min_amplitude\"] + 0.1\n        if key == \"min_rate\":\n            new_min_rate = np.random.normal(value, 1.)\n            new_parameters[key] = new_min_rate if new_min_rate > 0.1 else 0.5\n            max_rate = np.random.normal(value, 1.)\n            max_rate = max_rate if max_rate > 0 else 0\n            new_parameters[\"max_rate\"] = new_parameters[\"min_rate\"] + max_rate\n        if key == \"min_semitones\":\n            new_parameters[key] = -np.random.randint(0, 12)\n        if key == \"max_semitones\":\n            new_parameters[key] = np.random.randint(0, 12)\n        if key == \"min_fraction\":\n            new_parameters[key] = - np.random.uniform(0., 1.)\n        if key == \"max_fraction\":\n            new_parameters[key] = np.random.uniform(0., 1.)\n        if key == \"p\":\n            new_parameters[key] = np.random.uniform(0., 1.)\n        if key == \"volume\":\n            new_parameters[key] = np.random.uniform(0.5, 1.)\n    return new_parameters", "\n\ndef generate_grammar(grammar, memory, mem_length=10):\n    \"\"\"Generate a pattern from the grammar\"\"\"\n\n    if len(memory) == 0:\n        # first pattern\n        pattern = random.choice(grammar[\"Pattern\"])\n    else:\n        # weight more recent patterns\n        weights = np.arange(len(memory) + len(grammar[\"Pattern\"]))\n        pattern = random.choices(memory+grammar[\"Pattern\"], weights=weights)[0]\n\n    p, a, d, w = pattern\n    # Replace Letters\n    ########\n    p, a, d, w = grammar[p], grammar[a], grammar[d], grammar[w]\n\n    # add to memory\n    memory.append(pattern)\n    # last element remove from memory if it exceeds size\n    if len(memory) > mem_length:\n        memory.pop(-1)\n    return perturbation(p, a, d, w), memory", "\n\ndef generate_audio_grammar(grammar, effects, memory=[], mem_length=10, mode=1):\n    \"\"\"Generate a pattern from the grammar\"\"\"\n    if len(memory) == 0:\n        # first pattern\n        pattern = random.choice(grammar[\"Type\"])\n    else:\n        # weight more recent patterns\n        weights = np.arange(len(memory) + len(grammar[\"Type\"]))\n        pattern = random.choices(memory+grammar[\"Type\"], weights=weights)[0]\n    # add to memory\n    memory.append(pattern)\n    # last element remove from memory if it exceeds size\n    if len(memory) > mem_length:\n        memory.pop(-1)\n\n    audio_clip = random.choice(grammar[pattern])\n    parameters = {\n        \"min_amplitude\": 0.001,\n        \"max_amplitude\": 0.015,\n        \"min_rate\": 0.8,\n        \"max_rate\": 1.2,\n        \"min_semitones\": -4,\n        \"max_semitones\": 4,\n        \"min_fraction\": -0.5,\n        \"max_fraction\": 0.5,\n        \"p\": 0.5,\n        \"volume\": 1.0\n    }\n    # return audio_clip\n    parameters = audio_param_perturbation(parameters)\n    path = effects.apply_effect(audio_clip, parameters, mode)\n    return path, memory", "\n\ndef generate_and_send_midi(music_grammar, port_name, generation_length=3600, mem_length=10, test=False):\n    \"\"\"Generate a midi message and send it to the port\n\n    Parameters\n    ----------\n    music_grammar : dict\n        The grammar for the music\n    port_name : mido port name\n        The port to send the midi message\n    generation_length : int\n        The length of the generation in seconds\n    mem_length : int\n        The length of the memory\n    \"\"\"\n    # Initialize port\n    port = mido.open_output(port_name) if not test else None\n    # Hold down the pedal\n    msg = mido.Message('control_change', control=64, value=127)\n    port.send(msg)\n    start_time = time.time()\n    memory = list()\n    while time.time() - start_time < generation_length:\n        try:\n            (p, a, d, w), memory = generate_grammar(music_grammar, memory, mem_length)\n            # Generate midi message\n            ########\n            # Send midi message\n            for i in range(len(p)):\n                if test:\n                    print(\"Note: {}, Velocity: {}, Duration: {}\".format(p[i], a[i], d[i]))\n                    continue\n                msg = mido.Message('note_on', note=int(p[i]), velocity=int(a[i]), time=0)\n                port.send(msg)\n                print(msg)\n                msg = mido.Message('note_off', note=int(p[i]), velocity=int(a[i]), time=int(d[i]))\n                port.send(msg)\n\n                # Wait for the next note\n                time.sleep(w[i] / 1000)\n            # Release the pedal\n            if random.random() < 0.1:\n                msg = mido.Message('control_change', control=64, value=127)\n                port.send(msg)\n            # Wait for the next generation\n            ########\n            time.sleep(random.randint(1, 50) * 0.1)\n        except KeyboardInterrupt:\n            if not test:\n                # Release the pedal\n                msg = mido.Message('control_change', control=64, value=0)\n                port.send(msg)\n                port.close()\n            raise ValueError(\"Interrupted by user\")\n\n    # Close port\n    if not test:\n        # Release the pedal\n        msg = mido.Message('control_change', control=64, value=0)\n        port.send(msg)\n        port.close()", "\n\nclass EffectClass:\n\n    def __init__(self, sample_rate=44100):\n        self.sample_rate = sample_rate\n\n    def apply_effect(self, audio_path, params, mode):\n        signal, sampling_rate = audiofile.read(audio_path)\n        # reverse signal\n        signal = np.flip(signal) if random.gauss(mu=0, sigma=1) > 0.6 else signal\n        # normalize signal\n        # signal = signal/signal.max()\n        effect = self.select_effect(params)\n        out = effect(signal, sampling_rate)\n        # adjust volume\n        out = out*params[\"volume\"]\n        # do a fade in and fade out\n        fade_in = np.linspace(0, 1, 10000)\n        fade_out = np.linspace(1, 0, 10000)\n        out[:10000] = out[:10000]*fade_in\n        out[-10000:] = out[-10000:]*fade_out\n\n        fp = os.path.join(os.path.dirname(__file__), \"temp-{}.wav\".format(mode))\n        audiofile.write(fp, out, sampling_rate)\n        return fp\n\n    def select_effect(self, params):\n        effect = Compose([\n            # AddGaussianNoise(min_amplitude=params[\"min_amplitude\"], max_amplitude=params[\"max_amplitude\"], p=params[\"p\"]),\n            TimeStretch(min_rate=params[\"min_rate\"], max_rate=params[\"max_rate\"], p=params[\"p\"]),\n            PitchShift(min_semitones=params[\"min_semitones\"], max_semitones=params[\"max_semitones\"], p=params[\"p\"]),\n            Shift(min_fraction=params[\"min_fraction\"], max_fraction=params[\"max_fraction\"], p=params[\"p\"])\n        ])\n        return effect", "\ndef random_wave(length, sr=48000, base_freq=60, change_every=1, step=50):\n    change_every = length // change_every\n\n    x_samples = np.arange(0, change_every*step, step)\n    freq_samples = (np.random.random(x_samples.shape)*2*step - step) + base_freq\n\n    x = np.arange(length * sr) / sr\n\n    dx = 2*np.pi*x\n\n    interpolation = interp1d(x_samples, freq_samples, kind='quadratic')\n    freq = interpolation(x)\n\n    x_plot = freq*dx  # Cumsum freq * change in x\n\n    y = np.sin(x_plot)\n    return y", "\n\nclass GrammarGeneration:\n    def __init__(self, output_midi_port=\"iM/ONE 1\", generation_length=3600, mem_length=10):\n        self.generation_length = generation_length\n        self.mem_length = mem_length\n        self.output_midi_port = output_midi_port\n        # Download audio files\n        save_path = self.download_files()\n        # Initialize Background Audio\n        self.background_audio_path = self.generate_background_audio(save_path)\n\n\n        # Initialize Audio grammar\n        self.audio_grammar = audio_grammar(save_path)\n\n        # Initialize thread for midi generation\n        self.midi_thread = threading.Thread(target=generate_and_send_midi, args=(grammar, output_midi_port, generation_length, 10, False))\n        # Initialize effects\n        self.effects = EffectClass()\n\n        # Start midi generation\n        self.midi_thread.start()\n\n        # Start audio generation\n        self.start_audio_generation()\n\n    def download_files(self):\n        \"\"\"\n        Download audio files from dropbox\n        Returns:\n            save_path: path to the downloaded files\n        \"\"\"\n        url = \"https://www.dropbox.com/sh/xqrkq4ka8a5tvxa/AAA7j8JYvyECOfsPRdNc4Fsea?dl=1\"\n        if not os.path.exists(os.path.join(os.path.dirname(__file__), \"audio_files\")):\n            import requests\n            save_path = os.path.join(os.path.dirname(__file__), \"audio_files\", \"audio_files.zip\")\n            # create directory if it doesn't exist'\n            if not os.path.exists(os.path.dirname(save_path)):\n                os.makedirs(os.path.dirname(save_path))\n            r = requests.get(url, stream=True)\n            with open(save_path, 'wb') as fd:\n                for chunk in r.iter_content(chunk_size=128):\n                    fd.write(chunk)\n            # unzip files\n            with zipfile.ZipFile(save_path, 'r') as zip_ref:\n                zip_ref.extractall(os.path.join(os.path.dirname(__file__), \"audio_files\"))\n        return os.path.join(os.path.dirname(__file__), \"audio_files\")\n\n    def generate_background_audio(self, save_path, sr=48000):\n        # Fixed background audio time in seconds (5 minutes)\n        fixed_backround_audio_time = 3*60\n        fixed_backround_audio_time = fixed_backround_audio_time if fixed_backround_audio_time < self.generation_length else self.generation_length\n\n        # Initialize numpy array with gaussian noise\n        x = np.random.normal(0, 1, fixed_backround_audio_time * sr)\n        # Filter out values lower than 0.9 to create random impulse responses\n        x[x < 2.5] = 0\n\n        # create a wave of random frequency that changes every second\n        waves = np.array([\n            random_wave(fixed_backround_audio_time, sr=sr, base_freq=i, step=i//5)\n            for i in np.random.randint(40, 1500, 5)\n        ])\n\n        # Create filter of sawtooth impulse response\n        b = np.linspace(1, 0, sr//20, endpoint=False)\n        s = signal.lfilter(b, [1], x)\n        # add all together\n        waves = np.sum(waves, axis=0)\n        # normalize\n        waves = waves / np.max(np.abs(waves))\n        s = waves * s\n        # Normalize s between -1 and 1\n        s = s / np.max(np.abs(s))\n\n        # Apply other audio effects\n        effects = Compose([\n            # AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.01, p=0.5),\n            # ApplyImpulseResponse(, p=0.5),\n            # AirAbsorption(min_temperature=10, max_temperature=20, p=0.2),\n            GainTransition(min_gain_in_db=-20, max_gain_in_db=0, p=0.5),\n            TimeMask(min_band_part=0.01, max_band_part=0.1, fade=True, p=0.8),\n        ])\n        # Apply effects\n        s = effects(s, sr)\n        # Normalize s between -1 and 1\n        s = s / np.max(np.abs(s))\n        # Do a Fade-in of 1s\n        ramp = np.linspace(0., 1., sr)\n        s[:sr] = s[:sr] * ramp\n        # Scale audio to 0.5\n        board = Pedalboard([\n            # Compressor(threshold_db=-50, ratio=25),\n            # Gain(gain_db=30),\n            Chorus(),\n            Phaser(),\n            Reverb(room_size=0.5),\n        ])\n        s = board(s, sr)\n        s = s / np.max(np.abs(s))\n\n        s = s * 0.01\n        # Save audio file\n        audiofile.write(os.path.join(save_path, \"background.wav\"), s, sr)\n        ApplyImpulseResponse(os.path.join(save_path, \"background.wav\"), p=0.5)\n        return os.path.join(save_path, \"background.wav\")\n\n    def start_audio_generation(self):\n        CHUNK = 1024\n        wf = wave.open(self.background_audio_path, 'rb')\n        p = pyaudio.PyAudio()\n\n\n        background_stream = p.open(\n            format=p.get_format_from_width(wf.getsampwidth()),\n            channels=wf.getnchannels(),\n            rate=wf.getframerate(),\n            output=True,\n        )\n        audio_stream = p.open(\n            format=p.get_format_from_width(wf.getsampwidth()), channels=1, rate=48000, output=True)\n\n        audio_stream_p = p.open(\n            format=p.get_format_from_width(wf.getsampwidth()), channels=1, rate=48000, output=True\n        )\n\n        background_data = wf.readframes(CHUNK)\n        running_audio_data = b''\n        running_audio_data_p = b''\n        start_time = time.time()\n        audio_memory = []\n\n        while time.time() - start_time < self.generation_length:\n            # If background audio is finished, rewind\n            # Background audio is 5 minutes long to reduce memory and cpu usage\n            if background_data == b'':\n                wf.rewind()\n                background_data = wf.readframes(CHUNK)\n\n            background_stream.write(background_data)\n            background_data = wf.readframes(CHUNK)\n            if running_audio_data == b'':\n                audio_path, audio_memory = generate_audio_grammar(self.audio_grammar, self.effects, audio_memory, self.mem_length, mode=1)\n                wf_audio = wave.open(audio_path, 'rb')\n                if random.randint(0, 100) < 80:\n                    print(\"Playing audio clip.\")\n                    running_audio_data = wf_audio.readframes(CHUNK)\n            else:\n                audio_stream.write(running_audio_data)\n                running_audio_data = wf_audio.readframes(CHUNK)\n\n            if running_audio_data_p == b'':\n                audio_path, audio_memory = generate_audio_grammar(self.audio_grammar, self.effects, audio_memory, self.mem_length, mode=2)\n                wf_audio_p = wave.open(audio_path, 'rb')\n                if random.randint(0, 100) < 80:\n                    print(\"Playing audio clip.\")\n                    running_audio_data_p = wf_audio_p.readframes(CHUNK)\n            else:\n                audio_stream.write(running_audio_data_p)\n                running_audio_data_p = wf_audio_p.readframes(CHUNK)\n\n\n        background_stream.stop_stream()\n        audio_stream.stop_stream()\n        audio_stream_p.stop_stream()\n\n        background_stream.close()\n        audio_stream.close()\n        audio_stream_p.close()\n\n        p.terminate()", "\n\nif __name__ == \"__main__\":\n    import argparse\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\"--generation_length\", type=int, default=2200, help=\"Length of the generated audio in seconds\")\n    args.add_argument(\"--mem_length\", type=int, default=10, help=\"Length of the memory of the audio and midi generation\")\n    args.add_argument(\"--output_midi_port\", type=str, default=\"iM/ONE 1\", help=\"Name of the midi port to send the generated midi to\")\n\n    args = args.parse_args()\n\n    GrammarGeneration(generation_length=args.generation_length, mem_length=args.mem_length,\n                      output_midi_port=args.output_midi_port)", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n"]}
{"filename": "paow/23_08_30/merge_midis.py", "chunked_list": ["import os\nimport partitura as pt\nimport numpy as np\n\nMidi_path = r\"PATH0\"\nOut_path = r\"PATH1\"\ncur_time = 0\nnote_array = []\n\nfor l in range(10):\n\n    mid = pt.load_performance_midi(os.path.join(Midi_path, 'other_cutsample_{}_basic_pitch.mid'.format(l)))\n    print(l, mid.note_array()[0],mid.note_array()[-1] )\n    for note in mid[0].notes:\n        note['note_on'] += cur_time\n        note['note_off'] += cur_time\n        note['sound_off'] += cur_time\n        note_array.append(tuple([note['midi_pitch'], note['note_on'], note['note_off'] - note['note_on'], note['velocity']]))\n\n    cur_time += 60*5", "\nfor l in range(10):\n\n    mid = pt.load_performance_midi(os.path.join(Midi_path, 'other_cutsample_{}_basic_pitch.mid'.format(l)))\n    print(l, mid.note_array()[0],mid.note_array()[-1] )\n    for note in mid[0].notes:\n        note['note_on'] += cur_time\n        note['note_off'] += cur_time\n        note['sound_off'] += cur_time\n        note_array.append(tuple([note['midi_pitch'], note['note_on'], note['note_off'] - note['note_on'], note['velocity']]))\n\n    cur_time += 60*5", "\nperformed_part = pt.performance.PerformedPart.from_note_array(np.array(note_array,dtype=[(\"pitch\", \"i4\"),\n           (\"onset_sec\", \"f4\"),\n           (\"duration_sec\", \"f4\"),\n           (\"velocity\", \"i4\")\n           ]))\n\npt.save_performance_midi(performed_part, os.path.join(Out_path, \"other.mid\"))"]}
{"filename": "paow/23_08_30/cut_audio.py", "chunked_list": ["import numpy as np\nimport os\nimport librosa\nimport soundfile as sf\n\nsegment_length=60*5\nin_path = r\"PATH0\"\nout_path = r\"PATH1\"\n\n# LOAD", "\n# LOAD\nsignal = librosa.load(in_path)\n\nsr = signal[1]\nsig_len = signal[0].shape[0]\n\nsnippets = int(np.ceil(sig_len / (sr*segment_length)))\nfor i in range(snippets):\n    sf.write(os.path.join(out_path, 'other_cutsample_{}.wav'.format(i)),\n             signal[0][(i)*sr*segment_length:(i+1)*sr*segment_length] , sr)", "for i in range(snippets):\n    sf.write(os.path.join(out_path, 'other_cutsample_{}.wav'.format(i)),\n             signal[0][(i)*sr*segment_length:(i+1)*sr*segment_length] , sr)"]}
{"filename": "paow/23_06_28/main.py", "chunked_list": ["from model import PopMusicTransformer\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nimport random \n\ndef main():\n    # declare model\n    model = PopMusicTransformer(\n        checkpoint='checkpoint',\n        is_training=False)\n    \n    # generate from scratch\n    for i in range(500):\n        #randomly choose a tempreture between 1 and 2\n        temp = random.uniform(1, 2)\n        model.generate(\n            n_target_bar=16,\n            temperature=temp,\n            topk=5,\n            output_path='./2nd_round/gen_from_26_3_0_temp{}_{}.midi'.format(temp, i),\n            prompt=None)\n    \n    # close model\n    model.close()", "\nif __name__ == '__main__':\n    main()"]}
{"filename": "paow/23_06_28/merge_midis.py", "chunked_list": ["import os\nimport partitura as pt\nimport numpy as np\n\nMidi_address = 'MIDIs/'\ndir = os.listdir(Midi_address)\n\ncur_time = 0\nnote_array = []\n\nfor file in dir:\n\n    mid = pt.load_performance_midi(Midi_address+file)\n\n    for note in mid[0].notes:\n        note['note_on'] += cur_time\n        note['note_off'] += cur_time\n        note['sound_off'] += cur_time\n        note_array.append(tuple([note['midi_pitch'], note['note_on'], note['note_off'] - note['note_on'], note['velocity']]))\n\n    cur_time = mid[0].notes[-1]['note_off'] + 3", "note_array = []\n\nfor file in dir:\n\n    mid = pt.load_performance_midi(Midi_address+file)\n\n    for note in mid[0].notes:\n        note['note_on'] += cur_time\n        note['note_off'] += cur_time\n        note['sound_off'] += cur_time\n        note_array.append(tuple([note['midi_pitch'], note['note_on'], note['note_off'] - note['note_on'], note['velocity']]))\n\n    cur_time = mid[0].notes[-1]['note_off'] + 3", "\nperformed_part = pt.performance.PerformedPart.from_note_array(np.array(note_array,dtype=[(\"pitch\", \"i4\"),\n           (\"onset_sec\", \"f4\"),\n           (\"duration_sec\", \"f4\"),\n           (\"velocity\", \"i4\")\n           ]))\n\npt.save_performance_midi(performed_part, 'wannabe_mozert.mid')"]}
{"filename": "paow/utils/partitura_utils.py", "chunked_list": ["import partitura as pt\nimport numpy as np\n\ndef addnote(midipitch, part, voice, start, end, idx):\n    \"\"\"\n    adds a single note by midiptich to a part\n    \"\"\"\n    offset = midipitch%12\n    octave = int(midipitch-offset)/12\n    name = [(\"C\",0),\n            (\"C\",1),\n            (\"D\",0),\n            (\"D\",1),\n            (\"E\",0),\n            (\"F\",0),\n            (\"F\",1),\n            (\"G\",0),\n            (\"G\",1),\n            (\"A\",0),\n            (\"A\",1),\n            (\"B\",0)]\n    # print( id, start, end, offset)\n    step, alter = name[int(offset)]\n    part.add(pt.score.Note(id='n{}'.format(idx), step=step, \n                        octave=int(octave), alter=alter, voice=voice), \n                        start=start, end=end)", "\n\ndef partFromProgression(prog, \n                        quarter_duration = 4,\n                        rhythm = None):\n    part = pt.score.Part('P0', 'part from progression', quarter_duration=quarter_duration)\n    if rhythm is None:\n        rhythm = [(i*quarter_duration, (i+1)*quarter_duration) for i in range(len(prog.chords))]\n    for i, c in enumerate(prog.chords):\n        for j, pitch in enumerate(c.pitches):\n            addnote(pitch, part, j, rhythm[i][0], rhythm[i][1], str(j)+str(i))\n    \n    return part", "\n\ndef parttimefromrekorder(na, \n                         quarter_duration = 4,\n                         quarters = 8,\n                         num_frames = 8,\n                         rhythm = None):\n    positions = quarter_duration * quarters\n    sec_per_div = 10/quarters/quarter_duration\n    if rhythm is None:\n        interval = positions//num_frames\n        rhythm = [(i*interval, (i+1)*interval) for i in range(num_frames)]\n\n    frames = list()\n    for k in range(num_frames):\n        frames.append(na[\"pitch\"][np.logical_and(\n            na[\"onset_sec\"] < rhythm[k][1]*sec_per_div, \n            na[\"onset_sec\"] >= rhythm[k][0]*sec_per_div)])\n    # time_per_div = (num_frames/10) / quarter_duration\n    na[\"onset_sec\"] = np.round(na[\"onset_sec\"]/sec_per_div)\n    na[\"duration_sec\"] = np.round(na[\"duration_sec\"]/sec_per_div)\n    na[\"duration_sec\"][na[\"duration_sec\"] < 1] = 1   \n    return na, frames", "\n\ndef addmelody2part(part, na, quarter_duration = 4):\n    for j, note in enumerate(na):\n            addnote(note[\"pitch\"], part, j, \n                    note[\"onset_sec\"], \n                    note[\"duration_sec\"]+note[\"onset_sec\"], \n                    str(j)+\"_melody\")\n            \n\ndef progression_and_melody_to_part(progression, \n                                   melody,\n                                   quarter_duration = 4,\n                                   rhythm = None):\n    \"\"\"\n    converts a progression and a melody to a partitura part\n    \"\"\"\n    part = partFromProgression(progression,\n                               quarter_duration = quarter_duration,\n                               rhythm = rhythm\n                               )\n    na, _ = parttimefromrekorder(melody,\n                                 quarter_duration = quarter_duration,\n                                 num_frames = len(progression.chords)\n                                )\n    addmelody2part(part, na)\n    return part", "            \n\ndef progression_and_melody_to_part(progression, \n                                   melody,\n                                   quarter_duration = 4,\n                                   rhythm = None):\n    \"\"\"\n    converts a progression and a melody to a partitura part\n    \"\"\"\n    part = partFromProgression(progression,\n                               quarter_duration = quarter_duration,\n                               rhythm = rhythm\n                               )\n    na, _ = parttimefromrekorder(melody,\n                                 quarter_duration = quarter_duration,\n                                 num_frames = len(progression.chords)\n                                )\n    addmelody2part(part, na)\n    return part"]}
{"filename": "paow/utils/rhythm.py", "chunked_list": ["import numpy as np\n\n\ndef euclidean(cycle = 16, pulses = 4, offset= 0):\n    \"\"\"\n    compute an Euclidean rhythm\n    \"\"\"\n    rhythm = []\n    # pulses = how_many_notes-1\n    bucket = cycle-pulses\n    timepoint = 0\n    for step in range(cycle):\n        # print(bucket, rhythm) \n        bucket = bucket + pulses\n\n        if (bucket >= cycle):\n            bucket = bucket - cycle\n            rhythm.append(timepoint)\n        \n        timepoint += 1\n\n    rhythm_array = np.array(rhythm)\n    rhythm_array = (rhythm_array + offset) % cycle\n    rhythm_array = np.sort( rhythm_array)\n    return rhythm_array", "\nif __name__ == \"__main__\":\n    a = euclidean()"]}
{"filename": "paow/utils/pitch.py", "chunked_list": ["import numpy as np\nfrom itertools import product\nimport string\nimport random\n\n\ndef randomword(length):\n    \"\"\"\n    a random character generator\n    \"\"\"\n    letters = string.ascii_lowercase\n    return ''.join(random.choice(letters) for i in range(length))", "\ndef cycle_distance(u,v):\n    \"\"\"\n    pitch class distance between two values u and v\n    \"\"\"\n    a = np.sqrt(((u%12-v%12)%12)**2)\n    b = np.sqrt(((v%12-u%12)%12)**2)\n    return np.min([a, b])\n\n", "\n\n\ndef chordDistance(c1, c2):\n    \"\"\"\n    compute the minimal pitch distance between to chords\n    when every note of the second chord\n    can be transposed by up to -1 / +1 octaves.\n    assumes input chords as pitch-ordered np.arrays. \n\n    Parameters\n    ----------\n    c1 : np.array\n        first chord\n    c2 : np.array\n        second chord\n\n    Returns \n    -------\n    best_adds : np.array\n        the best transposition for each note of the second chord\n    best_total : int\n        the total distance between the two chords\n        \n    \"\"\"\n    l = min(c1.shape[0],c2.shape[0])\n    c1 = c1[:l]\n    c2 = c2[:l]\n    local_adds = dict()\n    for comb in product(np.arange(-1,1), repeat =  l):\n        c=np.array(comb)\n        new_c2 = c2 + 12* c\n        total = np.sum(np.abs(new_c2-c1))\n        local_adds[total] = c\n    \n    best_total = np.min(list(local_adds.keys()))\n    best_adds = local_adds[best_total]\n    \n    return  best_adds, best_total", "\n\nclass Chord:\n    \"\"\"\n    the Chord class representing a chord made of 3-4 notes of a scale.\n    For diatonic scales the intervals between notes are thirds.\n\n    Parameters\n    ----------\n    offset : int\n        the MIDI pitch of scale degree 0\n    scale : np.array\n        the scale from which the chord is built\n    how_many : int\n        the number of notes in the chord\n    root_id : int\n        the index of the root in the scale\n\n    \"\"\"\n    def __init__(self, \n                 offset = 48, #np.random.randint(48,60)-12, \n                 scale = np.array([0,2,4,5,7,9,11]),\n                 how_many = None,\n                 root_id = None\n                ):\n        self.offset = offset\n        self.scale = scale\n        if root_id is None:\n            self.root_id = np.random.randint(self.scale.shape[0])\n        else:\n            self.root_id = root_id\n        self.root = self.offset + self.scale[self.root_id]\n        if how_many is None:\n            self.how_many = np.random.randint(3,5)\n        else:\n            self.how_many = np.clip(how_many,3,4)\n        if self.how_many == 3:\n            self.jumps = [2,2,3]\n        elif self.how_many == 4:\n            self.jumps = [2,2,2,1]\n        self.jumps_cs = np.cumsum([0]+ self.jumps) \n        self.inversion = 0\n        self.inversion_jumps = None\n        self.inversion_jumps_cs = None\n        self.all_ids = None\n        self.pitches = None\n        self.pitch_classes = None\n        self.pitch_classes_relative = None\n        self.repitch = [0,0]\n        self.compute_pitch()\n        self.name = self.get_name()\n        \n    def get_name(self):\n        \"\"\"\n        rudimentary naming convention for chords\n        no difference between maj7 and dominant 7.\n        suffix for inversions and chromatic alterations.\n        \"\"\"\n        root_names = [\"C\",\"C#\",\"D\",\"D#\",\"E\",\"F\",\"F#\",\"G\",\"G#\",\"A\",\"A#\",\"B\"]\n        root_name = root_names[self.root%12]\n        first_jump =  cycle_distance(self.scale[(self.root_id+self.jumps[0])%self.scale.shape[0]],\n                                        self.scale[self.root_id])\n        if first_jump == 3:\n            mod = \"m\"\n        elif first_jump == 4:\n            mod = \"M\"\n        else:\n            mod = \"?\"\n        \n        if self.how_many == 3:\n            typ = \"\"\n        elif self.how_many == 4:\n            typ = \"7\"\n            \n        repitch_names = {1:\"#\", -1:\"b\"}\n        if self.repitch != [0,0]:\n            chromatic = \"_\" + str(self.repitch[0]) +repitch_names[self.repitch[1]]\n        else:\n            chromatic = \"\"\n        name = root_name + mod + typ + \"_\" + str(self.inversion) + chromatic\n        return name\n        \n    def compute_pitch(self):\n        self.inversion_jumps = self.jumps[self.inversion:]+self.jumps[:self.inversion]\n        self.inversion_jumps_cs = np.cumsum([0]+ self.inversion_jumps) \n        self.all_ids = np.array([(self.root_id+self.jumps_cs[self.inversion])%self.scale.shape[0]\n                                 +self.inversion_jumps_cs[n] for n in range(self.how_many)])\n        self.pitches = self.offset + np.concatenate((self.scale, self.scale+12))[self.all_ids]\n        self.pitches[self.repitch[0]] += self.repitch[1]\n        self.pitch_classes = self.pitches%12\n        self.pitch_classes_relative = (self.pitches-self.offset)%12\n        self.name = self.get_name()\n        \n    \n    def invert(self, n):\n        n %= self.how_many\n        self.inversion = n\n        self.compute_pitch()\n        \n    def add_repitch(self, idx, mod):\n        mod = np.clip(mod,-1,1)\n        idx %= self.how_many\n        self.repitch = [idx, mod]\n        self.compute_pitch()", "\nclass Progression:\n    \"\"\"\n    the Progression class representing a sequence of chords\n    \"\"\"\n    def __init__(self,\n                 number_of_chords = 8, \n                 chords = None):\n        if chords is not None:\n            self.chords = chords\n        else:\n            self.chords = [Chord() for c in range(number_of_chords)]\n        self.number_of_chords = number_of_chords\n        self.id = randomword(10)\n    \n    def copy(self):\n        return Progression(\n                number_of_chords = self.number_of_chords,\n                chords= self.chords\n                )\n    \n    def join(self, \n             another,\n             idx = None):\n        \"\"\"\n        create two new progressions by joining two existing ones\n        where the split of chords is defined by an index array\n        \"\"\"\n        if another.number_of_chords != self.number_of_chords:\n            raise ValueError(\"progressions must have the same number of chords\")\n\n        if idx is None:\n            idx = np.unique(np.random.randint(0,self.number_of_chords,self.number_of_chords//2))\n\n        new_progression = Progression(number_of_chords = self.number_of_chords)\n        new_another_progression = Progression(number_of_chords = self.number_of_chords)\n        for k in range(self.number_of_chords):\n            if k in idx:\n                new_progression.chords[k] = self.chords[k]\n                new_another_progression.chords[k] = another.chords[k]\n            else:\n                new_progression.chords[k] = another.chords[k]\n                new_another_progression.chords[k] = self.chords[k]\n        return new_progression, new_another_progression", "                \n\nif __name__ == \"__main__\":\n\n    a = Chord(how_many = 3, root_id = 0)\n\n"]}
{"filename": "paow/utils/__init__.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\"\"\"\nThis module contains utility functions for the paow package.\n\n\"\"\"\n\n\nfrom .midi import (MidiRouter, Sequencer, MidiInputThread)\nfrom .rhythm import (euclidean)", "from .midi import (MidiRouter, Sequencer, MidiInputThread)\nfrom .rhythm import (euclidean)\nfrom .pitch import (Chord, Progression, cycle_distance, chordDistance)\nfrom .partitura_utils import (progression_and_melody_to_part, parttimefromrekorder, partFromProgression)"]}
{"filename": "paow/utils/midi.py", "chunked_list": ["import partitura as pt  \nimport mido\nimport numpy as np\nimport time\nimport threading\nimport multiprocessing\nfrom collections import defaultdict\nimport partitura as pt\nimport winsound\n", "import winsound\n\n\nclass MidiInputThread(threading.Thread):\n    def __init__(\n        self,\n        port,\n        queue = list(),\n        init_time=None,\n    ):\n        threading.Thread.__init__(self)\n        self.midi_in = port\n        self.init_time = init_time\n        self.duration = 10\n        self.listen = False\n        self.queue = queue\n        self.beep_interval = self.duration / 8\n\n    def run(self):\n        self.start_listening()\n        self.queue = list()\n        print(self.current_time)\n        for msg in self.midi_in.iter_pending():\n            continue\n        c_time = self.current_time\n        while c_time < 2.49:\n            if self.beep_interval is not None:\n                if c_time % self.beep_interval < 0.01 and c_time < 2.0:\n                    winsound.Beep(1000, 50)\n            c_time = self.current_time\n        while c_time < self.duration + 2.5:\n            msg = self.midi_in.poll()\n            if msg is not None:\n                self.queue.append((msg, c_time-2.5))\n                print(msg, c_time-2.5)\n            if self.beep_interval is not None:\n                if c_time % self.beep_interval < 0.01:\n                    winsound.Beep(262, 50)\n            c_time = self.current_time\n        \n        \n        self.stop_listening()\n        return self.queue\n            \n\n    @property\n    def current_time(self):\n        \"\"\"\n\n        Get current time since starting to listen\n        \"\"\"\n        return time.perf_counter() - self.init_time\n\n    def start_listening(self):\n        \"\"\"\n        Start listening to midi input (open input port and\n        get starting time)\n        \"\"\"\n        print(\"start listening\")\n        self.listen = True\n        if self.init_time is None:\n            self.init_time = time.perf_counter()\n\n    def stop_listening(self):\n        \"\"\"\n        Stop listening to MIDI input\n        \"\"\"\n        print(\"stop listening\")\n        # break while loop in self.run\n        self.listen = False\n        # reset init time\n        self.init_time = None", "\n\ndef midi_msg_to_pt_note(msg):\n    return msg\n\ndef pt_note_to_midi_msg(pt_note):\n    return pt_note\n\n\nclass MidiRouter(object):\n    \"\"\"\n    This is a class handling MIDI I/O.\n    It takes (partial) strings for port names as inputs\n    and searches for a fitting port.\n    The reason this is set up in this way is that\n    different OS tend to name/index MIDI ports differently.\n\n    Use an instance if this class (and *only* this instance)\n    to handle everything related to port opening, closing,\n    finding, and panic. Expecially Windows is very finicky\n    about MIDI ports and it'll likely break if ports are\n    handled separately.\n\n    This class can be used to:\n    - create a midirouter = MidiRouter(**kwargs) with\n    a number of (partial) port names or fluidsynths\n    - poll a specific port: e.g.\n    midirouter.input_port.poll()\n    - send on a specific port: e.g.\n    midirouter.output_port.send(msg)\n    - open all set ports: midirouter.open_ports()\n    - close all set ports: midirouter.close_ports()\n    - panic reset all ports: midirouter.panic()\n    - get the full name of the used midi ports: e.g.\n    midirouter.input_port_name\n    (DON'T use this name to open, close, etc with it,\n    use the midirouter functions instead)\n\n    Args:\n        inport_name (string):\n            a (partial) string for the input name.\n        outport_name (string):\n            a (partial) string for the output name.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        inport_name=None,\n        outport_name=None\n    ):\n        self.available_input_ports = mido.get_input_names()\n        print(\"Available inputs MIDI for mido\", self.available_input_ports)\n        self.available_output_ports = mido.get_output_names()\n        print(\"Available outputs MIDI for mido\", self.available_output_ports)\n\n        self.input_port_names = {}\n        self.output_port_names = {}\n        self.open_ports_list = []\n\n        # the MIDI port name the accompanion listens at (port name)\n        self.input_port_name = self.proper_port_name(\n            inport_name, True\n        )\n        # the MIDI port names / Instrument the accompanion is sent\n        self.output_port_name = self.proper_port_name(\n            outport_name, False\n        )\n\n        self.open_ports()\n\n        self.input_port = self.assign_ports_by_name(\n            self.input_port_name, input=True\n        )\n        self.output_port = self.assign_ports_by_name(\n            self.output_port_name, input=False\n        )\n\n    def proper_port_name(self, try_name, input=True):\n        if isinstance(try_name, str):\n            if input:\n                possible_names = [\n                    (i, name)\n                    for i, name in enumerate(self.available_input_ports)\n                    if try_name in name\n                ]\n            else:\n                possible_names = [\n                    (i, name)\n                    for i, name in enumerate(self.available_output_ports)\n                    if try_name in name\n                ]\n\n            if len(possible_names) == 1:\n                print(\n                    \"port name found for trial name: \",\n                    try_name,\n                    \"the port is set to: \",\n                    possible_names[0],\n                )\n                if input:\n                    self.input_port_names[possible_names[0][1]] = None\n                else:\n                    self.output_port_names[possible_names[0][1]] = None\n                return possible_names[0]\n\n            elif len(possible_names) < 1:\n                print(\"no port names found for trial name: \", try_name)\n                return None\n            elif len(possible_names) > 1:\n                print(\" many port names found for trial name: \", try_name)\n                if input:\n                    self.input_port_names[possible_names[0][1]] = None\n                else:\n                    self.output_port_names[possible_names[0][1]] = None\n                return possible_names[0]\n                # return None\n        elif isinstance(try_name, int):\n            if input:\n                try:\n                    possible_name = (try_name, self.available_input_ports[try_name])\n                    self.input_port_names[possible_name[1]] = None\n                    return possible_name\n                except ValueError:\n                    raise ValueError(f\"no input port found for index: {try_name}\")\n            else:\n                try:\n                    possible_name = (try_name, self.available_output_ports[try_name])\n                    self.output_port_names[possible_name[1]] = None\n                    return possible_name\n                except ValueError:\n                    raise ValueError(f\"no output port found for index: {try_name}\")\n\n\n        else:\n            return None\n\n    def open_ports_by_name(self, try_name, input=True):\n        if try_name is not None:\n            if input:\n                port = mido.open_input(try_name)\n            else:\n                port = mido.open_output(try_name)\n                # Adding eventual key release.\n                port.reset()\n\n            self.open_ports_list.append(port)\n            return port\n\n        else:\n            return try_name\n\n    def open_ports(self):\n        for port_name in self.input_port_names.keys():\n            if self.input_port_names[port_name] is None:\n                port = self.open_ports_by_name(port_name, input=True)\n                self.input_port_names[port_name] = port\n        for port_name in self.output_port_names.keys():\n            if self.output_port_names[port_name] is None:\n                port = self.open_ports_by_name(port_name, input=False)\n                self.output_port_names[port_name] = port\n\n    def close_ports(self):\n        for port in self.open_ports_list:\n            port.close()\n        self.open_ports_list = []\n\n        for port_name in self.output_port_names.keys():\n            self.output_port_names[port_name] = None\n        for port_name in self.input_port_names.keys():\n            self.input_port_names[port_name] = None\n\n    def panic(self):\n        for port in self.open_ports_list:\n            port.panic()\n\n    def assign_ports_by_name(self, try_name, input=True):\n        if try_name is not None:\n            if input:\n                return self.input_port_names[try_name[1]]\n            else:\n                return self.output_port_names[try_name[1]]\n        else:\n            return None", "\nclass MidiRouter(object):\n    \"\"\"\n    This is a class handling MIDI I/O.\n    It takes (partial) strings for port names as inputs\n    and searches for a fitting port.\n    The reason this is set up in this way is that\n    different OS tend to name/index MIDI ports differently.\n\n    Use an instance if this class (and *only* this instance)\n    to handle everything related to port opening, closing,\n    finding, and panic. Expecially Windows is very finicky\n    about MIDI ports and it'll likely break if ports are\n    handled separately.\n\n    This class can be used to:\n    - create a midirouter = MidiRouter(**kwargs) with\n    a number of (partial) port names or fluidsynths\n    - poll a specific port: e.g.\n    midirouter.input_port.poll()\n    - send on a specific port: e.g.\n    midirouter.output_port.send(msg)\n    - open all set ports: midirouter.open_ports()\n    - close all set ports: midirouter.close_ports()\n    - panic reset all ports: midirouter.panic()\n    - get the full name of the used midi ports: e.g.\n    midirouter.input_port_name\n    (DON'T use this name to open, close, etc with it,\n    use the midirouter functions instead)\n\n    Args:\n        inport_name (string):\n            a (partial) string for the input name.\n        outport_name (string):\n            a (partial) string for the output name.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        inport_name=None,\n        outport_name=None\n    ):\n        self.available_input_ports = mido.get_input_names()\n        print(\"Available inputs MIDI for mido\", self.available_input_ports)\n        self.available_output_ports = mido.get_output_names()\n        print(\"Available outputs MIDI for mido\", self.available_output_ports)\n\n        self.input_port_names = {}\n        self.output_port_names = {}\n        self.open_ports_list = []\n\n        # the MIDI port name the accompanion listens at (port name)\n        self.input_port_name = self.proper_port_name(\n            inport_name, True\n        )\n        # the MIDI port names / Instrument the accompanion is sent\n        self.output_port_name = self.proper_port_name(\n            outport_name, False\n        )\n\n        self.open_ports()\n\n        self.input_port = self.assign_ports_by_name(\n            self.input_port_name, input=True\n        )\n        self.output_port = self.assign_ports_by_name(\n            self.output_port_name, input=False\n        )\n\n    def proper_port_name(self, try_name, input=True):\n        if isinstance(try_name, str):\n            if input:\n                possible_names = [\n                    (i, name)\n                    for i, name in enumerate(self.available_input_ports)\n                    if try_name in name\n                ]\n            else:\n                possible_names = [\n                    (i, name)\n                    for i, name in enumerate(self.available_output_ports)\n                    if try_name in name\n                ]\n\n            if len(possible_names) == 1:\n                print(\n                    \"port name found for trial name: \",\n                    try_name,\n                    \"the port is set to: \",\n                    possible_names[0],\n                )\n                if input:\n                    self.input_port_names[possible_names[0][1]] = None\n                else:\n                    self.output_port_names[possible_names[0][1]] = None\n                return possible_names[0]\n\n            elif len(possible_names) < 1:\n                print(\"no port names found for trial name: \", try_name)\n                return None\n            elif len(possible_names) > 1:\n                print(\" many port names found for trial name: \", try_name)\n                if input:\n                    self.input_port_names[possible_names[0][1]] = None\n                else:\n                    self.output_port_names[possible_names[0][1]] = None\n                return possible_names[0]\n                # return None\n        elif isinstance(try_name, int):\n            if input:\n                try:\n                    possible_name = (try_name, self.available_input_ports[try_name])\n                    self.input_port_names[possible_name[1]] = None\n                    return possible_name\n                except ValueError:\n                    raise ValueError(f\"no input port found for index: {try_name}\")\n            else:\n                try:\n                    possible_name = (try_name, self.available_output_ports[try_name])\n                    self.output_port_names[possible_name[1]] = None\n                    return possible_name\n                except ValueError:\n                    raise ValueError(f\"no output port found for index: {try_name}\")\n\n\n        else:\n            return None\n\n    def open_ports_by_name(self, try_name, input=True):\n        if try_name is not None:\n            if input:\n                port = mido.open_input(try_name)\n            else:\n                port = mido.open_output(try_name)\n                # Adding eventual key release.\n                port.reset()\n\n            self.open_ports_list.append(port)\n            return port\n\n        else:\n            return try_name\n\n    def open_ports(self):\n        for port_name in self.input_port_names.keys():\n            if self.input_port_names[port_name] is None:\n                port = self.open_ports_by_name(port_name, input=True)\n                self.input_port_names[port_name] = port\n        for port_name in self.output_port_names.keys():\n            if self.output_port_names[port_name] is None:\n                port = self.open_ports_by_name(port_name, input=False)\n                self.output_port_names[port_name] = port\n\n    def close_ports(self):\n        for port in self.open_ports_list:\n            port.close()\n        self.open_ports_list = []\n\n        for port_name in self.output_port_names.keys():\n            self.output_port_names[port_name] = None\n        for port_name in self.input_port_names.keys():\n            self.input_port_names[port_name] = None\n\n    def panic(self):\n        for port in self.open_ports_list:\n            port.panic()\n\n    def assign_ports_by_name(self, try_name, input=True):\n        if try_name is not None:\n            if input:\n                return self.input_port_names[try_name[1]]\n            else:\n                return self.output_port_names[try_name[1]]\n        else:\n            return None", "\n\ndef play_midi_from_score(score=None, \n                         midirouter=None, \n                         quarter_duration=1, \n                         default_velocity=60,\n                         print_messages=False):\n    \n    \"\"\"\n    Play a score using a midirouter\n    \"\"\"\n    if midirouter is None:\n        print(\"No midirouter provided\")\n        return\n    if score is None:\n        print(\"No score provided\")\n        return\n    note_array = score.note_array()\n    onset_sec = note_array[\"onset_quarter\"] * quarter_duration\n    offset_sec = note_array[\"duration_quarter\"] * quarter_duration + onset_sec\n    noteon_messages = np.array([(\"note_on\", p, onset_sec[i]) for i, p in enumerate(note_array[\"pitch\"])], dtype=[(\"msg\", \"<U10\"), (\"pitch\", int), (\"time\", float)])\n    noteoff_messages = np.array([(\"note_off\", p, offset_sec[i]) for i, p in enumerate(note_array[\"pitch\"])], dtype=[(\"msg\", \"<U10\"), (\"pitch\", int), (\"time\", float)])\n    messages = np.concatenate([noteon_messages, noteoff_messages])\n    messages = np.sort(messages, order=\"time\")\n    timediff = np.diff(messages[\"time\"])\n    output_port = midirouter.output_port\n\n    for i, msg in enumerate(messages):\n        if i == 0:\n            pass\n        else:\n            time.sleep(timediff[i-1])\n        m = mido.Message(msg[\"msg\"], note=msg[\"pitch\"], velocity=default_velocity)\n        output_port.send(m)\n        if print_messages:\n            print(m)", "\nclass Sequencer(multiprocessing.Process):\n    def __init__(\n        self,\n        outport_name=\"seq\",\n        queue=None,\n        *args, \n        **kwargs):\n        super(Sequencer, self).__init__(*args, **kwargs)\n        self.queue = queue\n        self.outport_name = outport_name\n        self.router = None\n\n        # sequencer variables\n        self.playhead = 0\n        self.playing = False\n        self.start_time = 0\n        self.tempo = 120#120\n        self.quarter_per_ns = self.tempo / ( 60 * 1e9)\n        self.next_time = 0\n\n        # music variables\n        self.default_velocity = 60\n        self.loop_start_quarter = 0\n        self.loop_end_quarter = 8\n        self.looped_notes = None\n        self.onset_quarter = None\n        self.offset_quarter = None\n        self.messages = None\n        self.message_times = None\n\n    def up(self, args):\n        self.queue.put(args)\n\n    def update_part(self, part):\n        pass\n        note_array = part.note_array()\n        mask_lower = note_array[\"onset_quarter\"] >= self.loop_start_quarter           \n        mask_upper = note_array[\"onset_quarter\"] < self.loop_end_quarter\n        mask = np.all((mask_lower, mask_upper), axis=0)\n        self.looped_notes = note_array[mask]\n        self.onset_quarter = note_array[mask][\"onset_quarter\"]\n        self.offset_quarter = np.clip(note_array[mask][\"duration_quarter\"] + self.onset_quarter, \n                                      self.loop_start_quarter, \n                                      self.loop_end_quarter-0.1)\n        \n        self.messages = defaultdict(list)\n        self.message_times = np.array([])\n\n        for i, note in enumerate(self.looped_notes):\n            on = mido.Message('note_on', \n                         note=note[\"pitch\"], \n                         velocity=self.default_velocity, \n                         time=0)\n            off = mido.Message('note_off',\n                            note=note[\"pitch\"],\n                            velocity=0,\n                            time=0)\n            self.messages[self.onset_quarter[i]].append(on)\n            self.messages[self.offset_quarter[i]].append(off)\n\n        self.message_times = np.sort(np.array(list(self.messages.keys())))\n        self.next_time = self.message_times[0]\n        print(self.message_times)\n        \n    def run(self):\n        self.start_time = time.perf_counter_ns()\n        self.router = MidiRouter(outport_name = self.outport_name)\n        self.playing = True\n        print(\"Sequencer started\")        \n        while self.playing:\n            try: \n                args = self.queue.get_nowait()\n                # print(args.note_array())\n                self.update_part(args)\n            except:\n                pass\n\n            current_time = time.perf_counter_ns()\n            elapsed_time = current_time - self.start_time\n            elapsed_quarter = elapsed_time * self.quarter_per_ns\n            self.playhead = elapsed_quarter % (self.loop_end_quarter - self.loop_start_quarter)\n            if self.playhead >= self.next_time - 0.02 and \\\n                self.playhead < self.next_time + 0.1 and \\\n                self.messages is not None:\n\n                for msg in self.messages[self.next_time]:\n                    self.router.output_port.send(msg)\n                    \n\n                # this_time = self.next_time\n                idx, = np.where(self.message_times == self.next_time)\n                i = idx[0]\n                self.next_time = self.message_times[(i+1)%len(self.message_times)]\n                # print(\"sent\", msg, i, self.next_time, self.playhead, self.message_times)\n                # time.sleep((self.next_time - this_time) / 2* self.quarter_per_ns * 1e9)\n\n            else:\n                time.sleep(0.02)", "\n    # def stop(self):\n    #     self.playing = False\n    #     self.join()\n\ndef addnote(midipitch, part, \n            voice = 1, \n            start = 0, \n            end = 1, \n            idx = 0):\n    \"\"\"\n    adds a single note by midipitch to a part\n    \"\"\"\n    step, alter, octave = pt.utils.music.midi_pitch_to_pitch_spelling(midipitch)\n\n    part.add(pt.score.Note(id='n{}'.format(idx), step=step, \n                        octave=int(octave), alter=alter, voice=voice, staff=int(voice)),\n                        start=start, end=end)", "\nif __name__ == \"__main__\":\n    part = pt.load_musicxml(pt.EXAMPLE_MUSICXML)[0]\n    queue =multiprocessing.Queue()\n    s = Sequencer(queue=queue,\n                  outport_name=\"iM/ONE\")\n    s.start()\n    time.sleep(2)\n    s.up(part)\n", "\n    # addnote(97, part, start=3, end=4, idx=100)\n    # s.up(part)\n\n    # time.sleep(4)\n    # s.terminate()\n    # s.join()\n"]}
{"filename": "paow/23_06_21/transcribe.py", "chunked_list": ["#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nThis experiment replays automatically transcribed solo piano performances.\nThis script contains the transcription functionality.\nIt is *not* standalone, but copied from the ATEPP repository:\nhttps://github.com/BetsyTang/ATEPP/\nand specifically the inference part of the transcription model:\nhttps://github.com/BetsyTang/ATEPP/blob/master/piano_transcription-master/pytorch/inference.py\nClone this repository to create your own transcriptions.", "https://github.com/BetsyTang/ATEPP/blob/master/piano_transcription-master/pytorch/inference.py\nClone this repository to create your own transcriptions.\n\nEnjoy!\n\"\"\"\n\nimport os\nimport sys\nsys.path.insert(1, os.path.join(sys.path[0], '../utils'))\nimport glob", "sys.path.insert(1, os.path.join(sys.path[0], '../utils'))\nimport glob\nimport numpy as np\nimport argparse\nimport h5py\nimport math\nimport time\nimport librosa\nimport logging\nimport matplotlib.pyplot as plt", "import logging\nimport matplotlib.pyplot as plt\n\nimport torch\n \nfrom utilities import (create_folder, get_filename, RegressionPostProcessor, \n    OnsetsFramesPostProcessor, write_events_to_midi, load_audio)\nfrom models import Note_pedal, Regress_onset_offset_frame_velocity_CRNN\nfrom pytorch_utils import move_data_to_device, forward\nimport config", "from pytorch_utils import move_data_to_device, forward\nimport config\n\n\nclass PianoTranscription(object):\n    def __init__(self, model_type, checkpoint_path=None, \n        segment_samples=16000*10, device=torch.device('cuda'), \n        post_processor_type='regression'):\n        \"\"\"Class for transcribing piano solo recording.\n\n        Args:\n          model_type: str\n          checkpoint_path: str\n          segment_samples: int\n          device: 'cuda' | 'cpu'\n        \"\"\"\n\n        if 'cuda' in str(device) and torch.cuda.is_available():\n            self.device = 'cuda'\n        else:\n            self.device = 'cpu'\n\n        self.segment_samples = segment_samples\n        self.post_processor_type = post_processor_type\n        self.frames_per_second = config.frames_per_second\n        self.classes_num = config.classes_num\n        self.onset_threshold = 0.3\n        self.offset_threshod = 0.3\n        self.frame_threshold = 0.1\n        self.pedal_offset_threshold = 0.2\n\n        # Build model\n        Model = eval(model_type)\n        self.model = Model(frames_per_second=self.frames_per_second, \n            classes_num=self.classes_num)\n\n        # Load model\n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model'], strict=False)\n\n        # Parallel\n        if 'cuda' in str(self.device):\n            self.model.to(self.device)\n            print('GPU number: {}'.format(torch.cuda.device_count()))\n            self.model = torch.nn.DataParallel(self.model)\n        else:\n            print('Using CPU.')\n\n    def transcribe(self, audio, midi_path):\n        \"\"\"Transcribe an audio recording.\n\n        Args:\n          audio: (audio_samples,)\n          midi_path: str, path to write out the transcribed MIDI.\n\n        Returns:\n          transcribed_dict, dict: {'output_dict':, ..., 'est_note_events': ..., \n            'est_pedal_events': ...}\n        \"\"\"\n\n        audio = audio[None, :]  # (1, audio_samples)\n\n        # Pad audio to be evenly divided by segment_samples\n        audio_len = audio.shape[1]\n        pad_len = int(np.ceil(audio_len / self.segment_samples)) \\\n            * self.segment_samples - audio_len\n\n        audio = np.concatenate((audio, np.zeros((1, pad_len))), axis=1)\n\n        # Enframe to segments\n        segments = self.enframe(audio, self.segment_samples)\n        \"\"\"(N, segment_samples)\"\"\"\n\n        # Forward\n        output_dict = forward(self.model, segments, batch_size=1)\n        \"\"\"{'reg_onset_output': (N, segment_frames, classes_num), ...}\"\"\"\n\n        # Deframe to original length\n        for key in output_dict.keys():\n            output_dict[key] = self.deframe(output_dict[key])[0 : audio_len]\n        \"\"\"output_dict: {\n          'reg_onset_output': (segment_frames, classes_num), \n          'reg_offset_output': (segment_frames, classes_num), \n          'frame_output': (segment_frames, classes_num), \n          'velocity_output': (segment_frames, classes_num), \n          'reg_pedal_onset_output': (segment_frames, 1), \n          'reg_pedal_offset_output': (segment_frames, 1), \n          'pedal_frame_output': (segment_frames, 1)}\"\"\"\n\n        # Post processor\n        if self.post_processor_type == 'regression':\n            \"\"\"Proposed high-resolution regression post processing algorithm.\"\"\"\n            post_processor = RegressionPostProcessor(self.frames_per_second, \n                classes_num=self.classes_num, onset_threshold=self.onset_threshold, \n                offset_threshold=self.offset_threshod, \n                frame_threshold=self.frame_threshold, \n                pedal_offset_threshold=self.pedal_offset_threshold)\n\n        elif self.post_processor_type == 'onsets_frames':\n            \"\"\"Google's onsets and frames post processing algorithm. Only used \n            for comparison.\"\"\"\n            post_processor = OnsetsFramesPostProcessor(self.frames_per_second, \n                self.classes_num)\n\n        # Post process output_dict to MIDI events\n        (est_note_events, est_pedal_events) = \\\n            post_processor.output_dict_to_midi_events(output_dict)\n\n        # Write MIDI events to file\n        if midi_path:\n            write_events_to_midi(start_time=0, note_events=est_note_events, \n                pedal_events=est_pedal_events, midi_path=midi_path)\n            print('Write out to {}'.format(midi_path))\n\n        transcribed_dict = {\n            'output_dict': output_dict, \n            'est_note_events': est_note_events,\n            'est_pedal_events': est_pedal_events}\n\n        return transcribed_dict\n\n    def enframe(self, x, segment_samples):\n        \"\"\"Enframe long sequence to short segments.\n\n        Args:\n          x: (1, audio_samples)\n          segment_samples: int\n\n        Returns:\n          batch: (N, segment_samples)\n        \"\"\"\n        assert x.shape[1] % segment_samples == 0\n        batch = []\n\n        pointer = 0\n        while pointer + segment_samples <= x.shape[1]:\n            batch.append(x[:, pointer : pointer + segment_samples])\n            pointer += segment_samples // 2\n\n        batch = np.concatenate(batch, axis=0)\n        return batch\n\n    def deframe(self, x):\n        \"\"\"Deframe predicted segments to original sequence.\n\n        Args:\n          x: (N, segment_frames, classes_num)\n\n        Returns:\n          y: (audio_frames, classes_num)\n        \"\"\"\n        if x.shape[0] == 1:\n            return x[0]\n\n        else:\n            x = x[:, 0 : -1, :]\n            \"\"\"Remove an extra frame in the end of each segment caused by the\n            'center=True' argument when calculating spectrogram.\"\"\"\n            (N, segment_samples, classes_num) = x.shape\n            assert segment_samples % 4 == 0\n\n            y = []\n            y.append(x[0, 0 : int(segment_samples * 0.75)])\n            for i in range(1, N - 1):\n                y.append(x[i, int(segment_samples * 0.25) : int(segment_samples * 0.75)])\n            y.append(x[-1, int(segment_samples * 0.25) :])\n            y = np.concatenate(y, axis=0)\n            return y", "\n\ndef inference(args):\n    \"\"\"Inference template.\n\n    Args:\n      model_type: str\n      checkpoint_path: str\n      post_processor_type: 'regression' | 'onsets_frames'. High-resolution \n        system should use 'regression'. 'onsets_frames' is only used to compare\n        with Googl's onsets and frames system.\n      audio_path: str\n      audio_paths: list\n      cuda: bool\n    \"\"\"\n\n    # Arugments & parameters\n    model_type = args.model_type\n    checkpoint_path = args.checkpoint_path\n    post_processor_type = args.post_processor_type\n    device = 'cuda' if args.cuda and torch.cuda.is_available() else 'cpu'\n    audio_path = args.audio_path\n    \n    sample_rate = config.sample_rate\n    segment_samples = sample_rate * 10  \n    \"\"\"Split audio to multiple 10-second segments for inference\"\"\"\n\n    # Paths\n    midi_path = 'results/{}.mid'.format(get_filename(audio_path))\n    create_folder(os.path.dirname(midi_path))\n\n    # Transcriptor\n    transcriptor = PianoTranscription(model_type, device=device, \n        checkpoint_path=checkpoint_path, segment_samples=segment_samples, \n        post_processor_type=post_processor_type)\n\n    # Load audio\n    (audio, _) = load_audio(audio_path, sr=sample_rate, mono=True)\n\n    # Transcribe and write out to MIDI file\n    transcribe_time = time.time()\n    transcribed_dict = transcriptor.transcribe(audio, midi_path)\n    print('Transcribe time: {:.3f} s'.format(time.time() - transcribe_time))\n\n    # Visualize for debug\n    plot = False\n    if plot:\n        output_dict = transcribed_dict['output_dict']\n        fig, axs = plt.subplots(5, 1, figsize=(15, 8), sharex=True)\n        mel = librosa.feature.melspectrogram(audio, sr=16000, n_fft=2048, hop_length=160, n_mels=229, fmin=30, fmax=8000)\n        axs[0].matshow(np.log(mel), origin='lower', aspect='auto', cmap='jet')\n        axs[1].matshow(output_dict['frame_output'].T, origin='lower', aspect='auto', cmap='jet')\n        axs[2].matshow(output_dict['reg_onset_output'].T, origin='lower', aspect='auto', cmap='jet')\n        axs[3].matshow(output_dict['reg_offset_output'].T, origin='lower', aspect='auto', cmap='jet')\n        axs[4].plot(output_dict['pedal_frame_output'])\n        axs[0].set_xlim(0, len(output_dict['frame_output']))\n        axs[4].set_xlabel('Frames')\n        axs[0].set_title('Log mel spectrogram')\n        axs[1].set_title('frame_output')\n        axs[2].set_title('reg_onset_output')\n        axs[3].set_title('reg_offset_output')\n        axs[4].set_title('pedal_frame_output')\n        plt.tight_layout(0, .05, 0)\n        fig_path = '_zz.pdf'.format(get_filename(audio_path))\n        plt.savefig(fig_path)\n        print('Plot to {}'.format(fig_path))", "    \n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(description='')\n    parser.add_argument('--model_type', type=str, required=True)\n    parser.add_argument('--checkpoint_path', type=str, required=True)\n    parser.add_argument('--post_processor_type', type=str, default='regression', choices=['onsets_frames', 'regression'])\n    parser.add_argument('--audio_path', type=str, required=True)\n    parser.add_argument('--cuda', action='store_true', default=False)\n\n    args = parser.parse_args()\n    inference(args)"]}
{"filename": "paow/23_07_26/run.py", "chunked_list": ["import numpy as np\nimport mido\nimport random\nimport time\n\n\ndef perturbation(p, a, d, w):\n    new_p = np.array(p) + np.random.randint(-1, 2, len(p))\n    new_a = np.array(a) + np.random.randint(-10, 10, len(a))\n    add_mask = np.zeros(len(a))\n    ind_a = random.choice(np.arange(len(a)))\n    add_mask[ind_a] = 10\n    new_a = new_a + add_mask\n    new_d = np.array(d) + np.random.randint(-1000, 1000, len(d))\n    new_w = np.array(w) + np.random.randint(-300, 300, len(d))\n    # Filter out negative values\n    new_d[new_d < 0] = 0\n    new_w[new_w < 0] = 0\n    return new_p, new_a, new_d, new_w", "\n\ndef generate_grammar(grammar, memory, mem_length=10):\n    \"\"\"Generate a pattern from the grammar\"\"\"\n\n    if len(memory) == 0:\n        # first pattern\n        pattern = random.choice(grammar[\"Pattern\"])\n    else:\n        # weight more recent patterns\n        weights = np.arange(len(memory) + len(grammar[\"Pattern\"]))\n        pattern = random.choices(memory+grammar[\"Pattern\"], weights=weights)[0]\n\n    p, a, d, w = pattern\n    # Replace Letters\n    ########\n    p, a, d, w = grammar[p], grammar[a], grammar[d], grammar[w]\n\n    # add to memory\n    memory.append(pattern)\n    # last element remove from memory if it exceeds size\n    if len(memory) > mem_length:\n        memory.pop(-1)\n    return perturbation(p, a, d, w), memory", "\n\ndef generate_and_send_midi(music_grammar, port_name, generation_length=3600, mem_length=10, test=False):\n    \"\"\"Generate a midi message and send it to the port\n\n    Parameters\n    ----------\n    music_grammar : dict\n        The grammar for the music\n    port_name : mido port name\n        The port to send the midi message\n    generation_length : int\n        The length of the generation in seconds\n    mem_length : int\n        The length of the memory\n    \"\"\"\n    # Initialize port\n    port = mido.open_output(port_name) if not test else None\n    # Hold down the pedal\n    msg = mido.Message('control_change', control=64, value=127)\n    port.send(msg)\n    start_time = time.time()\n    memory = list()\n    while time.time() - start_time < generation_length:\n        try:\n            (p, a, d, w), memory = generate_grammar(music_grammar, memory, mem_length)\n            # Generate midi message\n            ########\n            # With a small probability, release the pedal\n            if random.random() < 0.05:\n                # Hold down the pedal (sometimes it releases if no message is sent)\n                msg = mido.Message('control_change', control=64, value=0)\n                port.send(msg)\n            # Send midi message\n            for i in range(len(p)):\n                if test:\n                    print(\"Note: {}, Velocity: {}, Duration: {}\".format(p[i], a[i], d[i]))\n                    continue\n                msg = mido.Message('note_on', note=int(p[i]), velocity=int(a[i]), time=0)\n                port.send(msg)\n                print(msg)\n                msg = mido.Message('note_off', note=int(p[i]), velocity=int(a[i]), time=int(d[i]))\n                port.send(msg)\n\n                # Wait for the next note\n                time.sleep(w[i] / 1000)\n            if random.random() < 0.2:\n                # Hold down the pedal (sometimes it releases if no message is sent)\n                msg = mido.Message('control_change', control=64, value=127)\n                port.send(msg)\n            # Wait for the next generation\n            ########\n            time.sleep(random.randint(1, 50) * 0.2)\n        except KeyboardInterrupt:\n            if not test:\n                # Release the pedal\n                msg = mido.Message('control_change', control=64, value=0)\n                port.send(msg)\n                port.close()\n            raise ValueError(\"Interrupted by user\")\n\n    # Close port\n    if not test:\n        # Release the pedal\n        msg = mido.Message('control_change', control=64, value=0)\n        port.send(msg)\n        port.close()", "\n\nclass GrammarGeneration:\n    def __init__(self, output_midi_port=\"iM/ONE 1\", generation_length=3600, mem_length=10):\n        self.generation_length = generation_length\n        self.mem_length = mem_length\n        self.output_midi_port = output_midi_port\n        # Define grammar\n        self.grammar = self.define_grammar()\n        # Initialize thread for midi generation\n        self.start_midi_generation()\n\n    def start_midi_generation(self):\n        generate_and_send_midi(\n            music_grammar=self.grammar,\n            port_name=self.output_midi_port,\n            generation_length=self.generation_length,\n            mem_length=self.mem_length,\n        )\n\n    def define_grammar(self):\n        return {\n            \"Pattern\": [[f\"P{i}\", f\"A{i}\", f\"D{i}\", f\"W{i}\"] for i in range(7)],\n            \"P0\": np.array([60, 64, 67, 71, 74, 77, 80]),\n            \"P1\": np.array([72, 42, 73, 41, 76, 36, 75, 37]),\n            \"P2\": np.array([27, 107, 62, 55, 71, 63]),\n            \"P3\": np.array([35 + i*7 for i in range(10)]),\n            \"P4\": np.array([25, 108]),\n            \"P5\": np.array([27, 104, 60, 60, 60, 60]),\n            \"P6\": np.array([100 - i*5 for i in range(12)]),\n            \"A0\": np.array([random.randint(10, 45) for i in range(7)]),\n            \"A1\": np.array([25, 20, 25, 20, 35, 30, 25, 20]),\n            \"A2\": np.array([20, 30, 20, 20, 20, 20]),\n            \"A3\": np.array([random.randint(10, 40) for i in range(10)]),\n            \"A4\": np.array([20, 40]),\n            \"A5\": np.array([40, 40, 15, 15, 15, 15]),\n            \"A6\": np.array([random.randint(10, 40) for i in range(12)]),\n            \"D0\": np.array([2000 for i in range(7)]),\n            \"D1\": np.array([500, 500, 500, 500, 1000, 1000, 2000, 1000]),\n            \"D2\": np.array([4000, 4000, 3000, 3000, 3000, 3000]),\n            \"D3\": np.array([250 for i in range(10)]),\n            \"D4\": np.array([3000, 3000]),\n            \"D5\": np.array([4000, 4000, 100, 100, 100, 100]),\n            \"D6\": np.array([250 for i in range(12)]),\n            \"W0\": np.array([0 for i in range(6)] + [1000]),\n            \"W1\": np.array([0, 500, 0, 500, 0, 1000, 0, 2000]),\n            \"W2\": np.array([0, 5000, 0, 0, 0, 3000]),\n            \"W3\": np.array([300 for i in range(9)] + [1000]),\n            \"W4\": np.array([0, 3000]),\n            \"W5\": np.array([0, 0, 250, 80, 300, 0]),\n            \"W6\": np.array([300 for i in range(11)] + [1000]),\n        }", "\n\nif __name__ == \"__main__\":\n    import argparse\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\"--generation_length\", type=int, default=3600, help=\"Length of the generated audio in seconds\")\n    args.add_argument(\"--mem_length\", type=int, default=5, help=\"Length of the memory of the audio and midi generation\")\n    args.add_argument(\"--output_midi_port\", type=str, default=\"iM/ONE 1\", help=\"Name of the midi port to send the generated midi to\")\n\n    args = args.parse_args()\n\n    GrammarGeneration(generation_length=args.generation_length, mem_length=args.mem_length,\n                      output_midi_port=args.output_midi_port)", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n"]}
{"filename": "paow/23_08_02/all_notes.py", "chunked_list": ["#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nThis script generates a MIDI file with increasing numbers of simultaneous notes at increasing tempos.\nSetting the \"MIDI IN Delay\" of the Disklavier to on make the grand piano shut down when too many simultaneous notes arrive.\nWe turn it off for this experiment, which means the notes are handled as fast as possible and for many simultaneous notes, strange rhythms emerge. \nThe MIDI heard in the stream is a concatenation of two calls to this script with different settings.\n\nEnjoy!\n\"\"\"", "Enjoy!\n\"\"\"\nfrom mido import Message, MidiFile, MidiTrack\nimport numpy as np\n\nmid = MidiFile()\ntrack = MidiTrack()\nmid.tracks.append(track)\n\n# some sinusoids with different frequency constants for pedal controls", "\n# some sinusoids with different frequency constants for pedal controls\nmidi_pedal_maps = []\nfactors = np.arange(1,11)\nfor n in range(1,11):\n    midi_pedal_maps.append(\n    np.round((np.sin(2*np.pi*np.arange(0,128)/(128/n*4))+1)/2*127).astype(int)\n    )\n\n# tempos: 100 tempos from whole notes at 60 bpm to sixteenth notes at 240 bpm", "\n# tempos: 100 tempos from whole notes at 60 bpm to sixteenth notes at 240 bpm\n# => 0.25 notes/sec - 16 notes/sec\n# MIDI default parts per quarter is 480, MIDI default tempo is 120 bpm -> 960 ticks/sec\ntempos_in_notes_per_sec = np.linspace(0.25,16,100)\ntempos_in_ticks_per_note = 960 / tempos_in_notes_per_sec\n# 128 notes per tempo gives a duration of ~1h\ntotal_duration_in_hours = (1/tempos_in_notes_per_sec * 128).sum()/3600 # 1.0089\n# get nice integer times divisible by two\ntempos_in_ticks_per_note_int = np.round(tempos_in_ticks_per_note / 2).astype(int) ", "# get nice integer times divisible by two\ntempos_in_ticks_per_note_int = np.round(tempos_in_ticks_per_note / 2).astype(int) \n\n# creating the notes\nfor l in range(100):\n    # lowest MIDI pitch: 21\n    order = np.random.permutation(np.arange(21,21+88))\n    local_duration_halfed = tempos_in_ticks_per_note_int[l] \n    for k in range(44): # 44 -> at most half of the keys will be pressed \n        track.append(Message('note_on', note=order[0], velocity=np.random.randint(20,30), time=local_duration_halfed))\n        for pitch in order[1:k+1]:  \n            track.append(Message('note_on', note=pitch, velocity=np.random.randint(20,30), time=0))\n        track.append(Message('note_off', note=order[0], velocity=0, time=local_duration_halfed))\n        for pitch in order[1:k+1]:\n            track.append(Message('note_off', note=pitch, velocity=0, time=0))", "        # use sinusoidal pedal messages\n        # track.append(Message(\"control_change\",\n        #                     control=64,\n        #                     value=midi_pedal_maps[l%10][k]))\n\nmid.save('half_all_notes_no_pedal.mid')"]}
{"filename": "paow/23_06_07/carve_black_midi_blocks.py", "chunked_list": ["#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nThis experiment carves three well-known pieces from blocks of black midi.\n\nEnjoy!\n\"\"\"\nimport partitura as pt\nimport numpy as np\nimport numpy.lib.recfunctions as rfn", "import numpy as np\nimport numpy.lib.recfunctions as rfn\nfrom partitura.musicanalysis.performance_codec import to_matched_score\n\n# get an original performance\nperformance, alignment, score = pt.load_match(\"MYSTERY.match\", create_score=True)\nm_score, _ = to_matched_score(score, performance, alignment)\nptime_to_stime_map, stime_to_ptime_map = pt.musicanalysis.performance_codec.get_time_maps_from_alignment(performance, score, alignment)\n\n# parameters", "\n# parameters\ndtypes=[('onset_sec', '<f4'), ('duration_sec', '<f4'), ('pitch', '<i4'), ('velocity', '<i4')]\nwindow_width = 10\ntime_bins = 100\ndur_bins = 100\ndur_max = 1.0\ntemperature = 1 \n\n# custom softmax with temperature and clipping\ndef softmax(array, temp=1):\n    return np.exp(np.clip(array/temp, 0,100))/np.exp(np.clip(array/temp, 0,100)).sum()", "\n# custom softmax with temperature and clipping\ndef softmax(array, temp=1):\n    return np.exp(np.clip(array/temp, 0,100))/np.exp(np.clip(array/temp, 0,100)).sum()\n\n# sample ids from a histogram-based distribution\ndef sample(histogram, number_of_samples, temp):\n    return np.random.choice(np.arange(len(histogram)),\n                            size = number_of_samples, \n                            replace = True,\n                            p = softmax(histogram, temp = temp))", "\n# sample from specific values, quantized to bins\ndef sample_from_values(values, v_min, v_max, bins, number_of_samples, temp):\n    values_normalized = np.clip((values - v_min)/(v_max-v_min), 0 , 1)*bins\n    histogram = np.zeros(bins+1)\n    for val in values_normalized.astype(int):\n        histogram[val] += 1\n    new_value_idx = sample(histogram, number_of_samples, temp)\n    new_values = v_min + new_value_idx/bins * ((v_max-v_min))\n    return new_values", "\n# save the performance with note noise\ndef save_current_stack(nas, filename, performance):\n    full_note_array = rfn.stack_arrays(nas).data\n    pitch_sort_idx = np.argsort(full_note_array[\"pitch\"])\n    full_note_array = full_note_array[pitch_sort_idx]\n    onset_sort_idx = np.argsort(full_note_array[\"onset_sec\"], kind=\"mergesort\")\n    full_note_array = full_note_array[onset_sort_idx]\n    ppart = pt.performance.PerformedPart.from_note_array(full_note_array)\n    ppart.controls = performance[0].controls\n    pt.save_performance_midi(ppart, filename)", "\ndur_score = (m_score[\"onset\"]+m_score[\"duration\"]).max() - m_score[\"onset\"].min()\ndur_perf = (m_score[\"p_onset\"]+m_score[\"p_duration\"]).max() - m_score[\"p_onset\"].min()\nwindows = np.ceil(dur_score/window_width).astype(int)\nnas = [performance.note_array()[['onset_sec','duration_sec',\"pitch\",\"velocity\"]][:-1],\n       performance.note_array()[['onset_sec','duration_sec',\"pitch\",\"velocity\"]][-1:]]\nsave_current_stack(nas, \"orig.mid\", performance)\n\n# factors for number of notes and temperature parameters\nnotes_factor = [np.linspace(1.0,0.75, windows),", "# factors for number of notes and temperature parameters\nnotes_factor = [np.linspace(1.0,0.75, windows),\n                np.linspace(2.0,1.25, windows),\n                np.linspace(4.0,2.0, windows)]\ntemperature_factor = [np.linspace(1.0,0.05, windows),\n                      np.linspace(4.0,1.0, windows),\n                      np.linspace(5.0,1.0, windows)]\n\n# create three versions with increasing noise\nfor k in range(3):\n    for no, ws in enumerate(np.arange(m_score[\"onset\"].min(),(m_score[\"onset\"]+m_score[\"duration\"]).max(), window_width)):\n        mask = np.all((m_score[\"onset\"] >= ws, m_score[\"onset\"] < ws+ window_width), axis=0)\n        number_of_notes = int(mask.sum() * notes_factor[k][no])\n        temperature = temperature_factor[k][no]\n        # sample onsets\n        if number_of_notes > 0:\n            p_min = stime_to_ptime_map(ws)\n            p_max = stime_to_ptime_map(ws+window_width)\n            new_onsets = sample_from_values(m_score[\"p_onset\"][mask], p_min, p_max, time_bins, number_of_notes, temperature)\n            # sample durations\n            new_durations = sample_from_values(m_score[\"p_duration\"][mask], 0.001, dur_max, dur_bins, number_of_notes, temperature)\n            # sample pitches\n            # piano keyboard distribution 88 notes from 21 to 108\n            new_pitches = sample_from_values(m_score[\"pitch\"][mask], 21, 108, 87, number_of_notes, temperature).astype(int)\n            # sample velocities\n            new_velocities = sample_from_values(m_score[\"velocity\"][mask], 0, 127, 127, number_of_notes, temperature).astype(int)\n            new_note_array = np.array([(o,d,p,v) for o,d,p,v in zip(new_onsets, new_durations, new_pitches, new_velocities)], dtype = dtypes)\n            nas.append(new_note_array)\n\n    save_current_stack(nas, \"noise{0}.mid\".format(k), performance)", "# create three versions with increasing noise\nfor k in range(3):\n    for no, ws in enumerate(np.arange(m_score[\"onset\"].min(),(m_score[\"onset\"]+m_score[\"duration\"]).max(), window_width)):\n        mask = np.all((m_score[\"onset\"] >= ws, m_score[\"onset\"] < ws+ window_width), axis=0)\n        number_of_notes = int(mask.sum() * notes_factor[k][no])\n        temperature = temperature_factor[k][no]\n        # sample onsets\n        if number_of_notes > 0:\n            p_min = stime_to_ptime_map(ws)\n            p_max = stime_to_ptime_map(ws+window_width)\n            new_onsets = sample_from_values(m_score[\"p_onset\"][mask], p_min, p_max, time_bins, number_of_notes, temperature)\n            # sample durations\n            new_durations = sample_from_values(m_score[\"p_duration\"][mask], 0.001, dur_max, dur_bins, number_of_notes, temperature)\n            # sample pitches\n            # piano keyboard distribution 88 notes from 21 to 108\n            new_pitches = sample_from_values(m_score[\"pitch\"][mask], 21, 108, 87, number_of_notes, temperature).astype(int)\n            # sample velocities\n            new_velocities = sample_from_values(m_score[\"velocity\"][mask], 0, 127, 127, number_of_notes, temperature).astype(int)\n            new_note_array = np.array([(o,d,p,v) for o,d,p,v in zip(new_onsets, new_durations, new_pitches, new_velocities)], dtype = dtypes)\n            nas.append(new_note_array)\n\n    save_current_stack(nas, \"noise{0}.mid\".format(k), performance)"]}
{"filename": "paow/23_08_09/the_limits_of_the_age_of_reproduction.py", "chunked_list": ["#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nThis script contains a technical exercise investigating the limits of\nreproduction of the Disklavier.\n\nFirst, each note is played at different MIDI velocities. The audio for\nthis part will be analyzed to create a dictionary of sounds of the Disklavier\nfor later experiments.\n", "for later experiments.\n\nAfterwards, we explore the speed limits of the hammer actuators of the\nDisklavier, by playing repeated notes with increasing speed and velocity.\nThis experiment will be useful to assess how fast can the piano play, and\nwhether there are reproduction differences at different MIDI velocities.\n\"\"\"\nimport numpy as np\nimport partitura as pt\n", "import partitura as pt\n\nfrom partitura.performance import PerformedPart, Performance\n\n# MIDI velocities to test\nMIDI_VELOCITIES = np.clip(np.arange(0, 140, 10), a_min=1, a_max=127)\n\n# MIDI note numbers of the piano keys\nPIANO_KEYS = np.arange(21, 109)\n", "PIANO_KEYS = np.arange(21, 109)\n\n# Number of repetitions per second\nREPETITIONS = np.arange(1, 16)\n\n\ndef test_note_velocities(\n    midi_velocities: np.ndarray = MIDI_VELOCITIES,\n    piano_keys: np.ndarray = PIANO_KEYS,\n) -> np.ndarray:\n    \"\"\"\n    Generate a note array to test the MIDI velocity of the Disklavier.\n\n    Parameters\n    ----------\n    midi_velocities: np.ndarray\n        An array specifying the MIDI velocities to test.\n    piano_keys: np.ndarray\n       MIDI pitch of the piano keys to include in the test.\n\n    Returns\n    -------\n    note_array: np.ndarray\n        A structured array with note information. This array follows the\n        structure of the note arrays of `PerformedPart` objects.\n    \"\"\"\n\n    # Number of notes\n    n_onsets = len(piano_keys) * len(midi_velocities)\n\n    # Initialize array\n    note_array = np.zeros(\n        n_onsets,\n        dtype=[\n            (\"pitch\", \"i4\"),\n            (\"onset_sec\", \"f4\"),\n            (\"duration_sec\", \"f4\"),\n            (\"velocity\", \"i4\"),\n        ],\n    )\n\n    note_array[\"pitch\"] = np.repeat(piano_keys, len(midi_velocities))\n    note_array[\"velocity\"] = np.tile(midi_velocities, len(piano_keys))\n\n    # onsets will be every second\n    note_array[\"onset_sec\"] = np.arange(n_onsets)\n\n    # durations will be 0.5 seconds\n    note_array[\"duration_sec\"] = np.ones(n_onsets) * 0.5\n\n    return note_array", "\n\ndef test_repetitions(\n    repetitions: np.ndarray = REPETITIONS,\n    pitch: int = 60,\n    velocity: int = 60,\n) -> np.ndarray:\n    \"\"\"\n    Generate a note array to test the repetition speed of the Disklavier.\n\n    Parameters\n    ----------\n    repetitions: np.ndarray\n        An array specifying the number of repetitions per second of a note.\n    pitch: int\n        MIDI pitch of the note.\n    velocity: int\n        MIDI velocity of the note\n\n    Returns\n    -------\n    note_array: np.ndarray\n        A structured array with note information. This array follows the\n        structure of the note arrays of `PerformedPart` objects.\n    \"\"\"\n\n    n_onsets = sum(repetitions)\n\n    # Initialize array\n    note_array = np.zeros(\n        n_onsets,\n        dtype=[\n            (\"pitch\", \"i4\"),\n            (\"onset_sec\", \"f4\"),\n            (\"duration_sec\", \"f4\"),\n            (\"velocity\", \"i4\"),\n        ],\n    )\n\n    start = 0\n    for i, r in enumerate(repetitions):\n\n        sl = slice(start, start + r)\n\n        # set pitch\n        note_array[\"pitch\"][sl] = pitch\n        onsets = np.linspace(0, 1, r, False)\n        # set onsets\n        note_array[\"onset_sec\"][sl] = (\n            onsets + note_array[\"onset_sec\"][start - 1] + (2 * (i != 0))\n        )\n        # set duration (as half of the inter onset interval\n        note_array[\"duration_sec\"][sl] = np.ones(r) * 0.5 / r\n\n        # set velocity\n        note_array[\"velocity\"][sl] = np.ones(r) * velocity\n\n        start += r\n\n    return note_array", "\n\ndef get_duration(note_arrays: list) -> float:\n    \"\"\"\n    Get duration of the last array note array in the list.\n\n    Parameters\n    ----------\n    note_arrays: list\n        A list of structured note arrays with note information.\n\n    Returns\n    -------\n    end_time : float\n        Duration of the music in the note array in seconds.\n    \"\"\"\n\n    end_time = (\n        note_arrays[-1][\"onset_sec\"] + note_arrays[-1][\"duration_sec\"]\n    ).max() - note_arrays[-1][\"onset_sec\"].min()\n\n    return end_time", "\n\ndef main() -> None:\n    \"\"\"\n    Generate a MIDI file that explores the reproduction limits of\n    the Disklavier.\n    \"\"\"\n    # Initialize note arrays with test of MIDI velocities\n    note_arrays = [test_note_velocities()]\n\n    na_start = get_duration(note_arrays) + 3\n\n    diminished_chord = np.array([0, 3, 6, 9])\n\n    pitch = np.hstack(\n        ([21] + [diminished_chord + (i * 12) + 24 for i in range(7)] + [108])\n    )\n\n    for p in pitch:\n        for vel in np.arange(10, 130, 25):\n            note_arrays.append(test_repetitions(pitch=p, velocity=vel))\n            note_arrays[-1][\"onset_sec\"] += na_start\n            na_start += get_duration(note_arrays) + 3\n\n    # Concatenate all Note arrays\n    note_array = np.hstack(note_arrays)\n\n    # Create a PerformedPart\n    ppart = PerformedPart.from_note_array(note_array)\n\n    # Create a Performance\n    performance = Performance(ppart)\n\n    # Export the Performance to a MIDI file\n    pt.save_performance_midi(\n        performance,\n        out=\"limits_of_the_age_of_reproduction.mid\",\n    )", "\n\nif __name__ == \"__main__\":\n\n    main()\n"]}
{"filename": "paow/evolutionary/__init__.py", "chunked_list": ["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\"\"\"\nThis module contains evolutionary algorithms for music generation.\n\"\"\"\n\nfrom .chord_for_melody import Optimizer, Optimizer2"]}
{"filename": "paow/evolutionary/chord_for_melody.py", "chunked_list": ["\nimport numpy as np\n\nfrom paow.utils import Chord, Progression, euclidean\nfrom paow.utils import cycle_distance, chordDistance, parttimefromrekorder\n\nclass Optimizer:\n    def __init__(self):\n        pass    \n\n    def modify(self, population):\n        # add an accidental\n        subpop3 = np.random.choice(population, 30)\n        for element in subpop3:\n            cidx = np.random.randint(len(element.chords))\n            nidx = np.random.randint(4)\n            mod = np.random.choice([-1,1])\n            new_element = element.copy()\n            new_element.chords[cidx].add_repitch(nidx,mod)\n            population.append(new_element)\n        \n        # invert a chord\n        subpop4 = np.random.choice(population, 30)\n        for element in subpop4:\n            cidx = np.random.randint(len(element.chords))\n            nidx = np.random.randint(4)\n            new_element =  element.copy()\n            new_element.chords[cidx].invert(nidx)\n            population.append(new_element)\n            \n        # join some elements\n        subpop1 = np.random.choice(population, 30)\n        subpop2 = np.random.choice(population, 30)\n        for element0, element1 in zip(subpop1, subpop2):\n            elnew1, elnew2 = element0.join(element1)\n        \n            population.append(elnew1)\n            population.append(elnew2)\n        \n        return population\n    \n    def fitness(self, progression, melody_windows):\n        # the lower the fitness score the better\n        fit = 0\n        for c0,c1 in zip(progression.chords[:-1], progression.chords[1:]):\n            _, dist = chordDistance(c0.pitches, c1.pitches)\n            # penalize big leaps between pitches of adjacent chords\n            fit += dist \n            # penalize big leaps between scale tonic of adjacent chords\n            fit += abs(c0.offset - c1.offset)\n            # penalize small leaps between root of adjacent chords\n            fit += abs(5.0 - cycle_distance(c0.root,c1.root))\n            \n            #penalize sticking on a root\n            if c0.root_id == c1.root_id:\n                fit += 30\n            \n        for i, c0 in enumerate(progression.chords):\n            for note in melody_windows[i]:\n                fit += 3 * np.min([cycle_distance(note, pit) for pit in c0.pitches] ) \n                # print(fit, melody_windows[i], c0.pitches)\n        # add a small random number for hashing\n        fit += np.random.rand(1)[0]\n        return fit \n    \n    def select(self, population, number, melody_windows):\n        pop = {ele.id:ele for ele in population}\n        fitness_dict = {self.fitness(ele, melody_windows):ele.id for ele in population}\n        sorted_fitness = list(fitness_dict.keys())\n        sorted_fitness.sort()\n        # print(sorted_fitness)\n        # print([(len(pop[fitness_dict[k]].chords), k) for k in sorted_fitness[:50]])\n        new_pop = [pop[fitness_dict[k]] for k in sorted_fitness[:number]]\n        return new_pop, sorted_fitness\n\n    def run(self,\n            epochs = 10,\n            population_size = 100,\n            population_replacement = 0.3,\n            new_population = True,\n            melody = None,\n            number_of_chords = 8,\n            quarter_duration = 4,\n            quarters = 8):\n        \n        # find best euclidean rhythm\n        melody_onsets = [note[\"onset_sec\"] for note in melody]\n        melody_onsets_int = np.round(np.array(melody_onsets) / (10.0/quarters) * quarter_duration)\n        # use up to eight notes\n        number_of_notes = np.min((len(melody_onsets), number_of_chords))\n        spaced_idx = np.round(np.linspace(0, len(melody_onsets_int) - 1, number_of_notes)).astype(int)\n        melody_onsets_int = melody_onsets_int[spaced_idx]\n        positions = quarter_duration * quarters # 8 quarters, 1.25 sec\n        dists = dict()\n        for k in range(positions):\n            euc = euclidean(cycle = positions, pulses = number_of_notes, offset= k)\n            dist = np.sum(np.abs(euc - melody_onsets_int))\n            euc[0] = 0\n            euc_shifted = list(np.roll(euc, -1))[:number_of_notes] + [positions]\n            dists[dist] = [(on, off) for on, off in zip(euc, euc_shifted)]\n\n        min_dist = np.min(list(dists.keys()))\n        rhythm = dists[min_dist]\n\n        na, frames = parttimefromrekorder(melody,\n                                 quarter_duration = quarter_duration,\n                                 num_frames = number_of_notes,\n                                 rhythm = rhythm)\n        print(na, frames)\n\n        if new_population:\n            population = [Progression(number_of_chords= number_of_notes) for po in range(population_size)]    \n        else:\n            population = self.population\n        \n        for epoch in range(epochs): \n            population = self.modify(population)\n            population, sorted_fitness = self.select(population, int(population_replacement * population_size), frames) \n            print(f\"Epoch {epoch} best fitness: {sorted_fitness[0]:.4f}\")\n            population += [Progression(number_of_chords= number_of_notes) \n                           for po in range(population_size - len(population))]\n\n\n        self.population = population\n        return self.population, rhythm", "\n\nclass Optimizer2:\n    def __init__(self):\n        pass    \n\n    def modify(self, population):\n        # add an accidental\n        subpop3 = np.random.choice(population, 10)\n        for element in subpop3:\n            cidx = np.random.randint(len(element.chords))\n            nidx = np.random.randint(4)\n            mod = np.random.choice([-1,1])\n            new_element = element.copy()\n            new_element.chords[cidx].add_repitch(nidx,mod)\n            population.append(new_element)\n        \n        # invert a chord\n        subpop4 = np.random.choice(population, 10)\n        for element in subpop4:\n            cidx = np.random.randint(len(element.chords))\n            nidx = np.random.randint(4)\n            new_element =  element.copy()\n            new_element.chords[cidx].invert(nidx)\n            population.append(new_element)\n\n        # change root of a chord\n        subpop4 = np.random.choice(population, 10)\n        for element in subpop4:\n            cidx = np.random.randint(len(element.chords))\n            nidx = np.random.randint(7)\n            new_element =  element.copy()\n            new_element.chords[cidx].root_id = nidx\n            new_element.chords[cidx].compute_pitch()\n            population.append(new_element)\n            \n        # # join some elements\n        # subpop1 = np.random.choice(population, 30)\n        # subpop2 = np.random.choice(population, 30)\n        # for element0, element1 in zip(subpop1, subpop2):\n        #     elnew1, elnew2 = element0.join(element1)\n        #     population.append(elnew1)\n        #     population.append(elnew2)\n        \n        return population\n    \n    def fitness(self, progression, melody_windows):\n        # the lower the fitness score the better\n        fit = 0\n        for c0,c1 in zip(progression.chords[:-1], progression.chords[1:]):\n            _, dist = chordDistance(c0.pitches, c1.pitches)\n            # penalize big leaps between pitches of adjacent chords\n            fit += dist \n            # penalize big leaps between scale tonic of adjacent chords\n            fit += abs(c0.offset - c1.offset)\n            # penalize small leaps between root of adjacent chords\n            fit += abs(5.0 - cycle_distance(c0.root,c1.root))\n            \n            #penalize sticking on a root\n            if c0.root_id == c1.root_id:\n                fit += 30\n            \n        for i, c0 in enumerate(progression.chords):\n            for note in melody_windows[i]:\n                fit += 3 * np.min([cycle_distance(note, pit) for pit in c0.pitches] ) \n                # print(fit, melody_windows[i], c0.pitches)\n        # add a small random number for hashing\n        fit += np.random.rand(1)[0]\n        return fit \n    \n    def select(self, population, number, melody_windows):\n        pop = {ele.id:ele for ele in population}\n        fitness_dict = {self.fitness(ele, melody_windows):ele.id for ele in population}\n        sorted_fitness = list(fitness_dict.keys())\n        sorted_fitness.sort()\n        # print(sorted_fitness)\n        # print([(len(pop[fitness_dict[k]].chords), k) for k in sorted_fitness[:50]])\n        new_pop = [pop[fitness_dict[k]] for k in sorted_fitness[:number]]\n        return new_pop, sorted_fitness\n\n    def run(self,\n            epochs = 10,\n            population_size = 100,\n            population_replacement = 0.3,\n            new_population = True,\n            melody = None,\n            number_of_chords = 8,\n            quarter_duration = 4,\n            quarters = 8):\n        \n        # find best euclidean rhythm\n        melody_onsets = [note[\"onset_sec\"] for note in melody]\n        melody_onsets_int = np.round(np.array(melody_onsets) / (10.0/quarters) * quarter_duration)\n        # use up to eight notes\n        number_of_notes = np.min((len(melody_onsets), number_of_chords))\n        spaced_idx = np.round(np.linspace(0, len(melody_onsets_int) - 1, number_of_notes)).astype(int)\n        melody_onsets_int = melody_onsets_int[spaced_idx]\n        positions = quarter_duration * quarters # 8 quarters, 1.25 sec\n        dists = dict()\n        for k in range(positions):\n            euc = euclidean(cycle = positions, pulses = number_of_notes, offset= k)\n            dist = np.sum(np.abs(euc - melody_onsets_int))\n            euc[0] = 0\n            euc_shifted = list(np.roll(euc, -1))[:number_of_notes-1] + [positions]\n            dists[dist] = [(on, off) for on, off in zip(euc, euc_shifted)]\n\n        min_dist = np.min(list(dists.keys()))\n        rhythm = dists[min_dist]\n\n        na, frames = parttimefromrekorder(melody,\n                                 quarter_duration = quarter_duration,\n                                 num_frames = number_of_notes,\n                                 rhythm = rhythm)\n        print(na, frames)\n\n        if new_population:\n            population = [Progression(number_of_chords= number_of_notes) for po in range(population_size)]    \n        else:\n            population = self.population\n        \n        for epoch in range(epochs): \n            population = self.modify(population)\n            population, sorted_fitness = self.select(population, int(population_replacement * population_size), frames) \n            print(f\"Epoch {epoch} best fitness: {sorted_fitness[0]:.4f}\")\n            population += [Progression(number_of_chords= number_of_notes) \n                           for po in range(population_size - len(population))]\n\n\n        self.population = population\n        return self.population, rhythm", "    \n\nif __name__ == \"__main__\":\n    pass\n    # fields = [\n    # (\"onset_sec\", \"f4\"),\n    # (\"duration_sec\", \"f4\"),\n    # (\"pitch\", \"i4\"),\n    # (\"velocity\", \"i4\"),\n    # ]", "    # (\"velocity\", \"i4\"),\n    # ]\n    # rows = [\n    # (0.933,1.712,48,100),\n    # (7.176,1.885,51,100),\n    # (2.685,1.777,53,100),\n    # (4.464,2.71,59,100),\n    # ]\n    # note_array = np.array(rows, dtype=fields)\n    # exp = Optimizer()", "    # note_array = np.array(rows, dtype=fields)\n    # exp = Optimizer()\n    # p = exp.run(melody=note_array)"]}
{"filename": "paow/evolutionary/test_evol_loivy.py", "chunked_list": ["from paow.evolutionary import Optimizer, Optimizer2\nfrom paow.utils import partFromProgression, Sequencer, MidiRouter, MidiInputThread\nimport numpy as np\nimport multiprocessing\nfrom collections import defaultdict\n\ndef show(p):\n    for c in p[0].chords:\n        print(c.pitches)\n\ndef note2note_array(notes):\n    fields = [\n    (\"onset_sec\", \"f4\"),\n    (\"duration_sec\", \"f4\"),\n    (\"pitch\", \"i4\"),\n    (\"velocity\", \"i4\"),\n    ]\n    notes_list = list()\n    sounding_notes = {}\n    for note in notes:\n        msg = note[0]\n        time = note[1]\n        note_on = msg.type == \"note_on\"\n        note_off = msg.type == \"note_off\"\n        if note_on or note_off:\n            note = msg.note\n            if note_on and msg.velocity > 0:\n\n                # save the onset time and velocity\n                sounding_notes[note] = time\n            elif note_off or msg.velocity == 0:\n                if note not in sounding_notes:\n                    continue\n                else:\n                    onset = sounding_notes[note]\n                    duration = time - onset\n                    notes_list.append((onset, duration, note, msg.velocity))\n                    del sounding_notes[note]\n    \n    notes_array = np.array(notes_list, dtype=fields)\n    return notes_array", "\ndef note2note_array(notes):\n    fields = [\n    (\"onset_sec\", \"f4\"),\n    (\"duration_sec\", \"f4\"),\n    (\"pitch\", \"i4\"),\n    (\"velocity\", \"i4\"),\n    ]\n    notes_list = list()\n    sounding_notes = {}\n    for note in notes:\n        msg = note[0]\n        time = note[1]\n        note_on = msg.type == \"note_on\"\n        note_off = msg.type == \"note_off\"\n        if note_on or note_off:\n            note = msg.note\n            if note_on and msg.velocity > 0:\n\n                # save the onset time and velocity\n                sounding_notes[note] = time\n            elif note_off or msg.velocity == 0:\n                if note not in sounding_notes:\n                    continue\n                else:\n                    onset = sounding_notes[note]\n                    duration = time - onset\n                    notes_list.append((onset, duration, note, msg.velocity))\n                    del sounding_notes[note]\n    \n    notes_array = np.array(notes_list, dtype=fields)\n    return notes_array", "                \ndef rec(l):\n    notes = l.run()\n    na = note2note_array(notes)\n    return na\n\n    \n\n\ndef recompute(note_array = None, e = 10):\n    if note_array is None:\n        fields = [\n            (\"onset_sec\", \"f4\"),\n            (\"duration_sec\", \"f4\"),\n            (\"pitch\", \"i4\"),\n            (\"velocity\", \"i4\"),\n            ]\n        rows = [\n            (0.933,1.712,48,100),\n            (7.176,1.885,51,100),\n            (2.685,1.777,53,100),\n            (4.464,2.71,59,100),\n        ]\n        note_array = np.array(rows, dtype=fields)\n    exp = Optimizer2()\n    p, r = exp.run(melody=note_array, epochs = e)\n    part = partFromProgression(p[0],quarter_duration = 4,rhythm = r)\n    return part, r", "\ndef recompute(note_array = None, e = 10):\n    if note_array is None:\n        fields = [\n            (\"onset_sec\", \"f4\"),\n            (\"duration_sec\", \"f4\"),\n            (\"pitch\", \"i4\"),\n            (\"velocity\", \"i4\"),\n            ]\n        rows = [\n            (0.933,1.712,48,100),\n            (7.176,1.885,51,100),\n            (2.685,1.777,53,100),\n            (4.464,2.71,59,100),\n        ]\n        note_array = np.array(rows, dtype=fields)\n    exp = Optimizer2()\n    p, r = exp.run(melody=note_array, epochs = e)\n    part = partFromProgression(p[0],quarter_duration = 4,rhythm = r)\n    return part, r", "\ndef st(s = None, re = False):\n    if s is None:\n        queue = multiprocessing.Queue()\n        s = Sequencer(queue=queue,outport_name=\"seq\")\n        s.start()\n    else:\n        s.terminate()\n        s.join()\n        if re:\n            queue =multiprocessing.Queue()\n            s = Sequencer(queue=queue,outport_name=\"seq\")\n            s.start()\n    return s", "\n\nif __name__ == \"__main__\":\n\n    mr = MidiRouter(inport_name=\"inp\")\n    l = MidiInputThread(port = mr.input_port)\n    s = st()\n\n    # part, r = recompute(note_array)\n    # note_array = rec(l)", "    # part, r = recompute(note_array)\n    # note_array = rec(l)\n    # s.up(part)\n\n\n    # s.start()\n    # s.terminate()\n    # s.join()\n", ""]}
{"filename": "paow/23_08_23/random_pseq_markov_file.py", "chunked_list": ["import numpy as np\n\n\ndef euclidean(cycle = 16, \n              pulses = 4, \n              offset= 0):\n    \"\"\"\n    compute an Euclidean rhythm\n    \"\"\"\n    rhythm = []\n    bucket = cycle-pulses\n    for step in range(cycle):\n        bucket = bucket + pulses\n\n        if (bucket >= cycle):\n            bucket = bucket - cycle\n            rhythm.append(step)\n\n    rhythm_array = np.array(rhythm)\n    rhythm_array = (rhythm_array + offset) % cycle\n    rhythm_array = np.sort( rhythm_array)\n    return rhythm_array", "\n\ndef pbinds(file,\n           how_many_seqs=2,\n           euc_seq_min=16,\n           euc_seq_max=16,\n           pulses_min= 4,\n           pulses_max = 6,\n           loop_len = 3.0):\n    \n    file.write(\"// start a server and a MIDIClient and define a MIDIOut with the variable name m.\\n\")\n    file.write(\"(\\n\")\n    file.write(\"var markovmatrix;\\n\")\n    file.write(\"var no_of_seqs = {};\\n\".format(how_many_seqs*3-2))\n    file.write(\"var currentstate = {}.rand;\\n\".format(how_many_seqs*3-2))\n    first = list()\n    second = list()\n    third = list()\n    for pseq_id in range(how_many_seqs*3):\n\n        if pseq_id %3 == 0:\n            first.append(\"\\\\x{0}\".format(pseq_id))\n            seq_len = np.random.randint(euc_seq_min, euc_seq_max+1)\n            pulses = np.random.randint(pulses_min, np.min((pulses_max+1, seq_len)))\n            pitch_plus = 0\n        elif pseq_id %3 == 1:\n            second.append(\"\\\\x{0}\".format(pseq_id))\n            seq_len = np.random.randint(euc_seq_min, euc_seq_max+1)\n            pulses = np.random.randint(pulses_min, np.min((pulses_max+1, seq_len)))\n            pitch_plus = 0\n        else:\n            third.append(\"\\\\x{0}\".format(pseq_id))\n            # seq_len = np.random.randint(euc_seq_min, euc_seq_max+1)\n            pulses = np.max((1, np.floor(pulses/2).astype(int)))\n            pitch_plus = -8\n\n        \n        pulse_onsets = euclidean(seq_len,pulses, np.random.randint(2))\n        pseq_array = [\"Rest(0)\"]*seq_len\n        for po in pulse_onsets:\n            if pitch_plus == 0:\n                pseq_array[po] = \"[{1:d},{0:d}]\".format(2+pseq_id%7, 2+pseq_id%7+ 14)\n            else:\n                pseq_array[po] = \"[{1:d},{0:d}]\".format(pseq_id%2 + pitch_plus, pseq_id%2 + pitch_plus-7)\n        pseq_string = \"Pseq([\"+ \",\".join(pseq_array)+\"],1)\"\n\n        file.write('Pdef(\\\\x{0}, {{ Pbind(\\\\ctranspose, -1, \\\\degree, {1}, \\\\dur,{3}/{2} , \\\\amp, Pseq([0.3,0.31,0.29],inf))}});\\n'.format(pseq_id,pseq_string,seq_len, loop_len))\n\n    first =  [val for val in first for _ in (0, 1, 3)][2:]\n    second =  [val for val in second for _ in (0, 1, 3)][1:-1]\n    third =  [val for val in third for _ in (0, 1, 3)][0:-2]\n\n    file.write(\"{inf.do{\")\n    file.write(\"(Pdef([\" + \",\".join(first)+ \"].at(currentstate)) <> (type: \\midi, midiout: m)).play;\\n\")\n    file.write(\"(Pdef([\" + \",\".join(second)+ \"].at(currentstate)) <> (type: \\midi, midiout: m)).play;\\n\")\n    file.write(\"(Pdef([\" + \",\".join(third)+ \"].at(currentstate)) <> (type: \\midi, midiout: m)).play;\\n\")\n\n    file.write(\"currentstate = (currentstate-1..currentstate+3).wchoose([0.3,18,3,1,0.3].normalizeSum);\\n\")\n    file.write(\"currentstate.postln;\\n\")\n    file.write(\"if ( currentstate>(no_of_seqs - 1),   {currentstate = 0; }, {  } );\\n\")\n    file.write(\"if ( currentstate<0,   {currentstate = 0; }, {  } );\\n\")\n    file.write(\"{}.wait;\\n\".format(seq_len))\n    file.write(\"};}.fork;\\n\")\n    file.write(\")\\n\")", "\n\nif __name__ == \"__main__\":\n\n    with open(\"markov.scd\", \"w\") as f:\n        pbinds(f, 31, 16, 24, 6, 18, 3.4)\n"]}
{"filename": "paow/23_06_14/render_script.py", "chunked_list": ["#!/usr/bin/env python\n\"\"\"\nThis program generates an expressive performance of an input music score\n(in any format supported by the Partitura package) using a (trained)\npredictive model.\n\"\"\"\nimport argparse\nimport json\nimport logging\nimport os", "import logging\nimport os\nimport sys\nimport warnings\n\nfrom typing import Iterable, Dict, List, Tuple\n\nimport numpy as np\nimport torch\nfrom basismixer import TOY_MODEL_CONFIG", "import torch\nfrom basismixer import TOY_MODEL_CONFIG\nfrom basismixer.performance_codec import get_performance_codec\nfrom basismixer.predictive_models import FullPredictiveModel, construct_model\nfrom basismixer.utils import (\n    DEFAULT_VALUES,\n    RENDER_CONFIG,\n    get_all_output_names,\n    get_default_values,\n    load_score,", "    get_default_values,\n    load_score,\n    post_process_predictions,\n    sanitize_performed_part,\n)\nfrom partitura import save_match, save_performance_midi\nfrom partitura.musicanalysis import make_note_feats\nfrom partitura.score import merge_parts, remove_grace_notes, Part\n\nlogging.basicConfig(level=logging.INFO)", "\nlogging.basicConfig(level=logging.INFO)\n\n\nwarnings.filterwarnings(\"ignore\")\n\nLOGGER = logging.getLogger(__name__)\nLOGGER.addHandler(logging.StreamHandler(sys.stdout))\n\n\ndef load_model(\n    model_config: Iterable,\n    default_values: Dict = DEFAULT_VALUES,\n) -> Tuple[FullPredictiveModel, List[str]]:\n    \"\"\"\n    Load a saved model\n\n    Parameters\n    ----------\n    model_config : iterable\n        A list of tuples each containing the filename of the config file\n        of the model and the filename of the parameters of the model.\n    default_values : dict\n        Default values for the parameters not included in the trained\n        model(s) (e.g., the models might just be predicting velocity, and\n        the rest of the parameters (beat_period, timing, etc.)\n        will be set to these parameters.\n\n    Returns\n    -------\n    full_model : basismixer.predictive_models.FullPredictiveModel\n       An instance of a FullPredictiveModel for generating expressive\n       performances.\n    output_names : list\n       List of the expressive parameters included in the models loaded\n       from `model_config`.\n    \"\"\"\n\n    # Load the models\n    models = []\n    for con, par in model_config:\n        # Load config file\n        model_config = json.load(open(con))\n        # Load the parameters\n        params = torch.load(\n            par,\n            map_location=torch.device(\"cpu\"),\n        )[\"state_dict\"]\n        # Construct the model according to its configuration and\n        # set the parameters\n        model = construct_model(\n            model_config,\n            params,\n            device=torch.device(\"cpu\"),\n        )\n        # append the model to the list of models\n        models.append(model)\n\n    # Get all output names (expressive parameters) predicted by the models\n    output_names = list(\n        set(\n            [\n                name\n                for out_name in [m.output_names for m in models]\n                for name in out_name\n            ]\n        )\n    )\n    # Get the name of the inputs (basis functions) included in the models\n    input_names = list(\n        set(\n            [\n                name\n                for in_name in [m.input_names for m in models]\n                for name in in_name\n            ]\n        )\n    )\n    input_names.sort()\n    output_names.sort()\n\n    # Get the appropriate list of expressive parameters for generating\n    # a performance\n    all_output_names = get_all_output_names(output_names)\n    # Get the default values for those parameters not predicted by the\n    # loaded models\n    def_values = get_default_values(default_values, all_output_names)\n    # Construct the full model\n    full_model = FullPredictiveModel(\n        models=models,\n        input_names=input_names,\n        output_names=all_output_names,\n        default_values=def_values,\n    )\n    # Set of expressive parameters not included in the predicted models\n    not_in_model_names = set(all_output_names).difference(output_names)\n\n    LOGGER.info(\n        \"Trained models include the following parameters:\\n\"\n        + \"\\n\".join(output_names)\n        + \"\\n\\nThe following parameters will use default values:\\n\"\n        + \"\\n\"\n        + \"\\n\".join(\n            [\n                \"{0}:{1:.2f}\".format(k, default_values[k])\n                for k in not_in_model_names\n            ]\n        )\n    )\n\n    return full_model, output_names", "\n\ndef load_model(\n    model_config: Iterable,\n    default_values: Dict = DEFAULT_VALUES,\n) -> Tuple[FullPredictiveModel, List[str]]:\n    \"\"\"\n    Load a saved model\n\n    Parameters\n    ----------\n    model_config : iterable\n        A list of tuples each containing the filename of the config file\n        of the model and the filename of the parameters of the model.\n    default_values : dict\n        Default values for the parameters not included in the trained\n        model(s) (e.g., the models might just be predicting velocity, and\n        the rest of the parameters (beat_period, timing, etc.)\n        will be set to these parameters.\n\n    Returns\n    -------\n    full_model : basismixer.predictive_models.FullPredictiveModel\n       An instance of a FullPredictiveModel for generating expressive\n       performances.\n    output_names : list\n       List of the expressive parameters included in the models loaded\n       from `model_config`.\n    \"\"\"\n\n    # Load the models\n    models = []\n    for con, par in model_config:\n        # Load config file\n        model_config = json.load(open(con))\n        # Load the parameters\n        params = torch.load(\n            par,\n            map_location=torch.device(\"cpu\"),\n        )[\"state_dict\"]\n        # Construct the model according to its configuration and\n        # set the parameters\n        model = construct_model(\n            model_config,\n            params,\n            device=torch.device(\"cpu\"),\n        )\n        # append the model to the list of models\n        models.append(model)\n\n    # Get all output names (expressive parameters) predicted by the models\n    output_names = list(\n        set(\n            [\n                name\n                for out_name in [m.output_names for m in models]\n                for name in out_name\n            ]\n        )\n    )\n    # Get the name of the inputs (basis functions) included in the models\n    input_names = list(\n        set(\n            [\n                name\n                for in_name in [m.input_names for m in models]\n                for name in in_name\n            ]\n        )\n    )\n    input_names.sort()\n    output_names.sort()\n\n    # Get the appropriate list of expressive parameters for generating\n    # a performance\n    all_output_names = get_all_output_names(output_names)\n    # Get the default values for those parameters not predicted by the\n    # loaded models\n    def_values = get_default_values(default_values, all_output_names)\n    # Construct the full model\n    full_model = FullPredictiveModel(\n        models=models,\n        input_names=input_names,\n        output_names=all_output_names,\n        default_values=def_values,\n    )\n    # Set of expressive parameters not included in the predicted models\n    not_in_model_names = set(all_output_names).difference(output_names)\n\n    LOGGER.info(\n        \"Trained models include the following parameters:\\n\"\n        + \"\\n\".join(output_names)\n        + \"\\n\\nThe following parameters will use default values:\\n\"\n        + \"\\n\"\n        + \"\\n\".join(\n            [\n                \"{0}:{1:.2f}\".format(k, default_values[k])\n                for k in not_in_model_names\n            ]\n        )\n    )\n\n    return full_model, output_names", "\n\ndef compute_basis_from_score(\n    score_fn: str,\n    input_names: List[str],\n) -> Tuple[np.ndarray, Part]:\n    \"\"\"\n    Load a score and extract input score features through the basis functions\n\n    Parameters\n    ----------\n    score_fn : str\n        Filename of the score. Must be in one of the formats supported\n        by partitura.\n    input_names : list\n        List of input basis functions to extract score features.\n        See `basismixer.basisfunctions` for the full list of functions\n        supported\n\n    Returns\n    -------\n    basis : np.ndarray\n        A 2D array with dimensions (n_notes, n_basis_functions), where n_notes\n        is the number of notes in the score and\n        n_basis_functions == len(input_names) is the number of basis functions.\n        Each i-th row in this array corresponds to the basis functions\n        evaluated for the i-th note in the score. The order of the notes is\n        the same as in `part.notes_tied` (see below).\n    part : partitura.score.Part\n        A `Part` object representing the score.\n    \"\"\"\n    # Load score\n    part = merge_parts(load_score(score_fn))\n    # Delete grace notes\n    remove_grace_notes(part)\n    # Compute basis functions\n    _basis, bf_names = make_note_feats(\n        part, list(set([bf.split(\".\")[0] for bf in input_names]))\n    )\n    basis = np.zeros((len(_basis), len(input_names)))\n    for i, n in enumerate(input_names):\n        try:\n            ix = bf_names.index(n)\n        except ValueError:\n            continue\n        basis[:, i] = _basis[:, ix]\n\n    return basis, part", "\n\ndef predict(\n    model_config: Iterable,\n    score_fn: str,\n    default_values: dict = DEFAULT_VALUES,\n) -> Tuple[np.ndarray, FullPredictiveModel, Part]:\n    \"\"\"\n    Main method for predicting a performance.\n\n    Parameters\n    ----------\n     model_config : iterable\n        A list of tuples each containing the filename of the config\n        file of the model and the filename of the parameters of the model.\n    score_fn : str\n        Filename of the score. Must be in one of the formats supported\n        by partitura.\n    default_values : dict\n        Default values for the parameters not included in the trained\n        model(s) (e.g., the models might just be predicting velocity,\n        and the rest of the parameters (beat_period, timing, etc.)\n        will be set to these parameters.\n\n    Returns\n    -------\n    predictions : structured array\n        Numpy structured array containing the predictions by the model.\n        The fields are the names of the parameters specified by the `model`.\n    model : basismixer.predictive_models.FullPredictiveModel\n       An instance of a FullPredictiveModel for generating expressive\n       performances.\n    part : partitura.score.Part\n        A `Part` object representing the score.\n    \"\"\"\n    # Load predictive model\n    model, predicted_parameter_names = load_model(\n        model_config, default_values=DEFAULT_VALUES\n    )\n\n    # Compute score representation\n    basis, part = compute_basis_from_score(score_fn, model.input_names)\n\n    # Score positions for each note in the score\n    score_onsets = part.beat_map([n.start.t for n in part.notes_tied])\n\n    # make predictions\n    predictions = model.predict(basis, score_onsets)\n\n    return predictions, model, part", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        \"Render a piano performance of a given score file \"\n        \"and save it as a MIDI file\"\n    )\n\n    parser.add_argument(\n        \"score_fn\",\n        help=(\n            \"Score file (MusicXML, MIDI or formats supported by MuseScore 3)\"\n        ),\n    )\n    parser.add_argument(\"midi_fn\", help=\"Output MIDI file\")\n    parser.add_argument(\n        \"--model-config\",\n        \"-c\",\n        help=(\n            \"JSON file specifying the configuration and parameters of the\"\n            \" model\"\n        ),\n    )\n    parser.add_argument(\n        \"--default-values\",\n        \"-d\",\n        help=(\n            \"JSON file specifying the default values for rendering the\"\n            \" performance\"\n        ),\n        default=None,\n    )\n    parser.add_argument(\n        \"--render-config\",\n        \"-r\",\n        help=(\n            \"JSON file specifying the configuration for post-processing the\"\n            \" generated performance\"\n        ),\n        default=None,\n    )\n    parser.add_argument(\n        \"--save-match\",\n        \"-m\",\n        help=\"Export Score-Performance alignment in Matchfile format\",\n        action=\"store_true\",\n        default=False,\n    )\n\n    args = parser.parse_args()\n    # Use toy model if no model is given\n    if args.model_config is None:\n        model_config = TOY_MODEL_CONFIG\n    else:\n        model_config = json.load(open(args.model_config))\n    # Use default config files if not given\n    if args.default_values is None:\n        default_values = DEFAULT_VALUES\n    else:\n        default_values = json.load(open(args.default_values))\n\n    if args.render_config is None:\n        render_config = RENDER_CONFIG\n    else:\n        render_config = json.load(open(args.render_config))\n\n    # Predict performance\n    preds, model, part = predict(\n        model_config=model_config,\n        score_fn=args.score_fn,\n        default_values=default_values,\n    )\n    # Post process predictions\n    post_process_predictions(preds, render_config)\n\n    # decode predictions\n    perf_codec = get_performance_codec(model.output_names)\n    predicted_ppart = perf_codec.decode(\n        part=part,\n        parameters=preds,\n        return_alignment=args.save_match,\n    )\n\n    if args.save_match:\n        predicted_ppart, alignment = predicted_ppart\n\n    # Sanitize part\n    sanitize_performed_part(predicted_ppart)\n\n    # Save MIDI file\n    save_performance_midi(predicted_ppart, args.midi_fn)\n\n    if args.save_match:\n        save_match(\n            alignment=alignment,\n            performance_data=predicted_ppart,\n            score_data=part,\n            out=args.midi_fn.replace(\".mid\", \".match\"),\n            piece=os.path.basename(args.score_fn),\n            performer=\"Basis Mixer\",\n        )", ""]}
{"filename": "paow/minimal_midi/test_all_notes.py", "chunked_list": ["import mido\nimport time\n\nprint(mido.get_output_names())\noutput_name = mido.get_output_names()[-1]\no = mido.open_output(output_name)\n\n\nfor k in range(21,100):\n    o.send(mido.Message(\"note_on\", note = k, velocity = 30))", "for k in range(21,100):\n    o.send(mido.Message(\"note_on\", note = k, velocity = 30))\ntime.sleep(0.5)\nfor k in range(21,100):\n    o.send(mido.Message(\"note_off\", note = k, velocity = 0))\n    \ntime.sleep(0.5)\n\nfor k in range(21,77):\n    o.send(mido.Message(\"note_on\", note = k, velocity = 30))", "for k in range(21,77):\n    o.send(mido.Message(\"note_on\", note = k, velocity = 30))\ntime.sleep(0.5)\nfor k in range(21,77):\n    o.send(mido.Message(\"note_off\", note = k, velocity = 0))\n"]}
{"filename": "paow/minimal_midi/test_minimal.py", "chunked_list": ["import mido\nimport time\n\nprint(mido.get_output_names())\n\nms = mido.Message(\"note_on\", note = 64, velocity = 54)\nme = mido.Message(\"note_off\", note = 64, velocity = 0)\n\noutput_name = mido.get_output_names()[-1]\no = mido.open_output(output_name)", "output_name = mido.get_output_names()[-1]\no = mido.open_output(output_name)\n\n\nfor k in range(19):\n    o.send(ms)\n    time.sleep(0.5)\n    o.send(me)\n    time.sleep(0.1)\n", ""]}
{"filename": "paow/minimal_midi/test_midi_aerophone.py", "chunked_list": ["import mido\n\"\"\"\ntest the different control change message produced and accepted by the aerophone.\n\"\"\"\n\nprint(mido.get_output_names(), mido.get_input_names())\n# if oport is not None:\n#     oport.close()\noport = mido.open_output( 'AE-30 1')\niport  = mido.open_input('AE-30 0')", "oport = mido.open_output( 'AE-30 1')\niport  = mido.open_input('AE-30 0')\n\ndef f(no, val):\n    oport.send(mido.Message(\"control_change\",\n                                channel=0,\n                                control=no, # control number k, can be mapped later as you like\n                                value=val))\n    \ndef g():\n    for msg in iport:\n        if msg.is_cc():\n            # 2 = breath, 3 too?, 11, 9\n            # 4 button\n            if msg.control == 95:# in [2,3,9,11]: \n                print(msg)", "    \ndef g():\n    for msg in iport:\n        if msg.is_cc():\n            # 2 = breath, 3 too?, 11, 9\n            # 4 button\n            if msg.control == 95:# in [2,3,9,11]: \n                print(msg)\n        \ndef h():\n    iport.close()\n    oport.close()", "        \ndef h():\n    iport.close()\n    oport.close()"]}
{"filename": "paow/minimal_midi/test_midi_aerophone2.py", "chunked_list": ["import mido\nimport warnings\nimport numpy as np\n\n\"\"\"\nforce notes from aerophone to s predefined scale.\n\"\"\"\n\ndef scalify(input_array, \n            offset = 0, \n            scale = np.array([0,0,2,3,3,5,5,7,7,9,10,10])):\n    \"\"\"\n    forces notes in an array to some scale\n    \"\"\"\n    output_array = (input_array-offset)-(input_array-offset)%12 + scale[(input_array-offset)%12]+ offset\n    return output_array", "def scalify(input_array, \n            offset = 0, \n            scale = np.array([0,0,2,3,3,5,5,7,7,9,10,10])):\n    \"\"\"\n    forces notes in an array to some scale\n    \"\"\"\n    output_array = (input_array-offset)-(input_array-offset)%12 + scale[(input_array-offset)%12]+ offset\n    return output_array\n\n\ndef f():\n    if oport is not None:\n        oport.close()\n    if iport is not None:\n        iport.close()", "\n\ndef f():\n    if oport is not None:\n        oport.close()\n    if iport is not None:\n        iport.close()\n    \n    \ndef g():\n    sounding_notes = {}\n    for msg in iport:\n        note_on = msg.type == \"note_on\"\n        note_off = msg.type == \"note_off\"\n\n        if (note_on or note_off):\n                    \n            # hash sounding note\n            pitch =  msg.note\n\n            # start note if it's a 'note on' event with velocity > 0\n            if note_on and msg.velocity > 0:\n\n                repitch = scalify(pitch-12)\n                # save the onset time and velocity\n                sounding_notes[pitch] = (repitch)\n                print(pitch, repitch)\n                oport.send(mido.Message(\"note_on\",\n                                channel=msg.channel,\n                                note=repitch,\n                                velocity=msg.velocity))\n        \n\n            # end note if it's a 'note off' event or 'note on' with velocity 0\n            elif note_off or (note_on and msg.velocity == 0):\n\n                if pitch not in sounding_notes:\n                    warnings.warn(\"ignoring MIDI message %s\" % msg)\n                    continue\n\n                else:\n                    \n                    oport.send(mido.Message(\"note_off\",\n                                channel=msg.channel,\n                                note=sounding_notes[pitch],\n                                velocity=0))\n                    \n                    del sounding_notes[pitch]", "    \ndef g():\n    sounding_notes = {}\n    for msg in iport:\n        note_on = msg.type == \"note_on\"\n        note_off = msg.type == \"note_off\"\n\n        if (note_on or note_off):\n                    \n            # hash sounding note\n            pitch =  msg.note\n\n            # start note if it's a 'note on' event with velocity > 0\n            if note_on and msg.velocity > 0:\n\n                repitch = scalify(pitch-12)\n                # save the onset time and velocity\n                sounding_notes[pitch] = (repitch)\n                print(pitch, repitch)\n                oport.send(mido.Message(\"note_on\",\n                                channel=msg.channel,\n                                note=repitch,\n                                velocity=msg.velocity))\n        \n\n            # end note if it's a 'note off' event or 'note on' with velocity 0\n            elif note_off or (note_on and msg.velocity == 0):\n\n                if pitch not in sounding_notes:\n                    warnings.warn(\"ignoring MIDI message %s\" % msg)\n                    continue\n\n                else:\n                    \n                    oport.send(mido.Message(\"note_off\",\n                                channel=msg.channel,\n                                note=sounding_notes[pitch],\n                                velocity=0))\n                    \n                    del sounding_notes[pitch]", "    \n            \nif __name__ == \"__main__\":\n    \n    print(mido.get_output_names(), mido.get_input_names())\n\n    oport = mido.open_output( 'seq 2')\n    iport  = mido.open_input('3- AE-30 3')"]}
