{"filename": "train.py", "chunked_list": ["import argparse\nimport os\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--config')\nparser.add_argument('--name', default=None)\nparser.add_argument('--tag', default=None)\nparser.add_argument('--gpu', default='0')\nparser.add_argument('--resume', default=None)\nparser.add_argument('--seed', default=None, type=int)", "parser.add_argument('--resume', default=None)\nparser.add_argument('--seed', default=None, type=int)\nargs = parser.parse_args()\n\nos.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n\nimport sys\nimport copy\nimport random\nimport time", "import random\nimport time\nimport shutil\nfrom PIL import Image\n\nimport yaml\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm", "import torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport math\nimport torchvision\nfrom einops import repeat, rearrange, reduce, parse_shape\nimport datasets\nimport losses\nimport models", "import losses\nimport models\nimport utils\nfrom models import make_lr_scheduler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef seed_all(seed):\n    log(f'Global seed set to {seed}')\n    random.seed(seed) # Python\n    np.random.seed(seed) # cpu vars\n    torch.manual_seed(seed) # cpu vars\n\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False", "def seed_all(seed):\n    log(f'Global seed set to {seed}')\n    random.seed(seed) # Python\n    np.random.seed(seed) # cpu vars\n    torch.manual_seed(seed) # cpu vars\n\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False", "\n\ndef make_data_loader(spec, tag=''):\n    if spec is None:\n        return None\n\n    dataset = datasets.make(spec['dataset'])\n    log('{} dataset: size={}'.format(tag, len(dataset)))\n    for k, v in dataset[0].items():\n        if type(v) is torch.Tensor:\n            log('  {}: shape={}, dtype={}'.format(k, tuple(v.shape), v.dtype))\n        else:\n            log('  {}: type={}'.format(k, type(v)))\n\n    loader = DataLoader(dataset, batch_size=spec['batch_size'],\n        shuffle=spec['shuffle'], num_workers=min(spec['batch_size'], os.cpu_count(), 32), pin_memory=True)\n    return loader", "\ndef make_data_loaders():\n    train_loader = make_data_loader(config.get('train_dataset'), tag='train')\n    val_loader = make_data_loader(config.get('val_dataset'), tag='val')\n    return train_loader, val_loader\n\ndef prepare_training():\n    if config.get('resume') is not None:\n        log('resume from {}'.format(config['resume']))\n        sv_file = torch.load(config['resume'])\n\n        if not utils.same_dict(sv_file['model'], config['model'], {'sd'}) or \\\n           not utils.same_dict(sv_file['optimizer'], config['optimizer'], {'sd'}):\n            print('from ckpt:')\n            print(yaml.dump(utils.without(sv_file['model'], {'sd'})))\n\n            print('from config:')\n            print(yaml.dump(config['model']))\n\n            which_one, _ = utils.input_checkbox(['ckpt', 'config'], msg='Model/Optimizer configs are different')\n            print('you select', which_one)\n            if which_one == 'config':\n                sv_file['model'].update(config['model'])\n                sv_file['optimizer'].update(config['optimizer'])\n\n        model = models.make(sv_file['model'], load_sd=True).cuda()\n\n        optimizer = utils.make_optimizer(\n            model.parameters(), sv_file['optimizer'], load_sd=config['load_optimizer'])\n        \n        lr_scheduler = make_lr_scheduler(optimizer, config.get('scheduler'))\n        \n        if config.get('run_step') is not None and config['run_step']:\n            epoch_start = sv_file['epoch'] + 1\n            for _ in range(epoch_start - 1):\n                lr_scheduler.step()\n        else:\n            epoch_start = 1\n    else:\n        model = models.make(config['model']).cuda()\n        optimizer = utils.make_optimizer(\n            model.parameters(), config['optimizer'])\n        epoch_start = 1\n        lr_scheduler = make_lr_scheduler(optimizer, config.get('scheduler'))\n\n    log('model: #params={}'.format(utils.compute_num_params(model, text=True)))\n\n    loss_fn = losses.make(config['loss'])\n    return model, optimizer, epoch_start, lr_scheduler, loss_fn", "\ndef debug(model):\n    has_nan = False\n    v_n = []\n    v_v = []\n    v_g = []\n    for name, parameter in model.named_parameters():\n        v_n.append(name)\n        v_v.append(parameter.detach().cpu() if parameter is not None else torch.zeros(1))\n        v_g.append(parameter.grad.detach().cpu() if parameter.grad is not None else torch.zeros(1))\n        has_nan = has_nan or \\\n            torch.any(torch.isnan(v_v[-1])) or \\\n            torch.any(torch.isnan(v_g[-1]))\n    if has_nan:\n        for i in range(len(v_n)):\n            print(f'value {v_n[i]}: {v_v[i].min().item():.3e} ~ {v_v[i].max().item():.3e}')\n            print(f'grad  {v_n[i]}: {v_g[i].min().item():.3e} ~ {v_g[i].max().item():.3e}')\n        exit(0)", "\ndef train(train_loader, model, optimizer, loss_fn, epoch):\n    global global_step\n    model.train()\n    train_loss = utils.Averager()\n\n    with tqdm(train_loader, leave=False, desc='train') as pbar:\n        for batch in pbar:\n            for k, v in batch.items():\n                if type(v) is torch.Tensor:\n                    batch[k] = v.cuda()\n            \n            # with torch.autograd.detect_anomaly():\n            out = model(batch, epoch=epoch, train=True)\n\n            list_of_loss = loss_fn(out, batch, epoch=epoch)\n            loss = list_of_loss['loss']\n\n            train_loss.add(loss.item())\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            loss = None\n\n            writer.add_scalars('step loss', list_of_loss, global_step)\n            global_step += 1\n\n            for k, v in list_of_loss.items():\n                list_of_loss[k] = f'{v.item():.6f}'\n            \n            pbar.set_postfix({\n                **list_of_loss,\n                'avg': train_loss.item()\n            })\n\n    return train_loss.item()", "\ndef val(val_loader, model, img_path, epoch):\n    os.makedirs(img_path, exist_ok=True)\n    model.eval()\n\n    dim = 128\n    _xy = utils.make_coord((dim, dim)).cuda()\n\n    with torch.no_grad():\n        with tqdm(val_loader, leave=False, desc='val') as pbar:\n            for batch in val_loader:\n                b = 0\n                for k, v in batch.items():\n                    if type(v) is torch.Tensor:\n                        batch[k] = v.cuda()\n                        b = v.size(0)\n            \n                xy = repeat(_xy, 'n d -> b n d', b=b)\n\n                batch['xy'] = xy\n                out = model(batch, epoch=epoch, train=False)\n                curves = out['curves']\n                curves_np = curves.detach().cpu().numpy()\n\n                if 'occ' in out:\n                    occ_img = rearrange(out['occ'], 'b (dim1 dim2) -> b () dim1 dim2', dim1=dim).detach().cpu()\n                if 'iter_occs' in out:\n                    iters_occ_img = rearrange(out['iter_occs'][-1], 'b (dim1 dim2) -> b dim1 dim2', dim1=dim).detach().cpu()\n                for i in range(b):\n                    curve_np = curves_np[i]\n                    filename = os.path.join(img_path, f\"{batch['index'][i]}.svg\")\n                    shutil.copyfile(batch['img_path'][i], filename.replace('.svg', '.png'))\n                    if 'img' in batch:\n                        utils.tensor_to_image(batch['img'][i, 0], filename.replace('.svg', '_inp.png'))\n                    if 'refs' in batch:\n                        n_ref = batch['n_ref'][i]\n                        grid = torchvision.utils.make_grid(batch['refs'][i], nrow=5)\n                        utils.tensor_to_image(grid[0], filename.replace('.svg', '_refs.png'))\n\n                    if 'full_img' in batch:\n                        utils.tensor_to_image(batch['full_img'][i, 0], filename.replace('.svg', '_full.png'))\n\n                    if 'img_origin_res' in batch:\n                        utils.tensor_to_image(batch['img_origin_res'][i, 0], filename.replace('.svg', '_origin_res.png'))\n\n                    if 'rec' in out:\n                        utils.tensor_to_image(out['rec'][i, 0], filename.replace('.svg', '_rec.png'))\n\n                    if 'dis' in out:\n                        utils.tensor_to_image(out['dis'][i].view(dim, dim) + 0.5, filename.replace('.svg', '_dis.png'))\n\n                    if 'rendered' in out:\n                        Image.fromarray((out['rendered'][i, 0].cpu().numpy() * 255).astype(np.uint8)).save(filename.replace('.svg', '_render.png'))\n\n                    if 'occ' in out:\n                        Image.fromarray((occ_img[i, 0].numpy() * 255).astype(np.uint8)).save(filename.replace('.svg', '_occ.png'))\n\n                    if 'iter_occs' in out:\n                        utils.tensor_to_image(iters_occ_img[i], filename.replace('.svg', '_occ_iter.png'))\n\n                    if 'sdf' in out:\n                        sdf_img = rearrange(torch.sigmoid(out['sdf']), 'b (dim1 dim2) -> b dim1 dim2', dim1=dim).detach().cpu().numpy()\n                        Image.fromarray((sdf_img[i] * 255).astype(np.uint8)).save(filename.replace('.svg', '_sdf.png'))\n\n                    if hasattr(model, 'write_paths_to_svg'):\n                        utils.curves_to_svg(curve_np, filename)\n                        model.write_paths_to_svg(curve_np, os.path.join(img_path, f\"{batch['index'][i]}_mask.svg\"))\n                \n                break\n\n    return None", "\ndef main(config_, save_path):\n    global config, log, writer, global_step\n    config = config_\n    log, writer = utils.set_save_path(save_path)\n    global_step = 0\n\n    seed_all(config['seed'])\n    with open(os.path.join(save_path, 'config.yaml'), 'w') as f:\n        yaml.dump(config, f, sort_keys=False)\n\n    ckpt_path = os.path.join(save_path, 'ckpt')\n    img_path = os.path.join(save_path, 'img')\n    os.makedirs(ckpt_path, exist_ok=True)\n    os.makedirs(img_path, exist_ok=True)\n\n    train_loader, val_loader = make_data_loaders()\n\n    model, optimizer, epoch_start, lr_scheduler, loss_fn = prepare_training()\n    model.init()\n\n    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))\n    if n_gpus > 1:\n        model = nn.parallel.DataParallel(model)\n\n    epoch_max = config['epoch_max']\n    epoch_val = config.get('epoch_val')\n    epoch_save = config.get('epoch_save')\n\n    timer = utils.Timer()\n\n    val(val_loader, model, os.path.join(img_path, 'test'), epoch=0)\n\n    for epoch in range(epoch_start, epoch_max + 1):\n        t_epoch_start = timer.t()\n        log_info = ['epoch {}/{}'.format(epoch, epoch_max)]\n\n        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n\n        train_loss = train(train_loader, model, optimizer, loss_fn, epoch)\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        log_info.append('train: loss={:.4f}'.format(train_loss))\n        writer.add_scalars('loss', {'train': train_loss}, epoch)\n\n        if n_gpus > 1:\n            model_ = model.module\n        else:\n            model_ = model\n\n        model_spec = copy.deepcopy(config['model'])\n        model_spec['sd'] = model_.state_dict()\n        optimizer_spec = copy.deepcopy(config['optimizer'])\n        optimizer_spec['sd'] = optimizer.state_dict()\n        sv_file = {\n            'model': model_spec,\n            'optimizer': optimizer_spec,\n            'epoch': epoch,\n            'config': config,\n        }\n\n        torch.save(sv_file, os.path.join(ckpt_path, 'epoch-last.pth'))\n\n        if (epoch_save is not None) and (epoch % epoch_save == 0):\n            torch.save(sv_file,\n                os.path.join(ckpt_path, 'epoch-{}.pth'.format(epoch)))\n\n        if (epoch_val is not None) and (epoch % epoch_val == 0):\n            if n_gpus > 1:\n                model_ = model.module\n            else:\n                model_ = model\n            \n            val(val_loader, model_, os.path.join(img_path, str(epoch)), epoch)\n\n        t = timer.t()\n        prog = (epoch - epoch_start + 1) / (epoch_max - epoch_start + 1)\n        t_epoch = utils.time_text(t - t_epoch_start)\n        t_elapsed, t_all = utils.time_text(t), utils.time_text(t / prog)\n        log_info.append('{} {}/{}'.format(t_epoch, t_elapsed, t_all))\n\n        log(', '.join(log_info))\n        writer.flush()", "\nif __name__ == '__main__':\n    with open(args.config, 'r') as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)\n        print('config loaded.')\n\n    save_name = args.name\n    if save_name is None:\n        save_name = '_' + args.config.split('/')[-1][:-len('.yaml')]\n    if args.tag is not None:\n        save_name += '_' + args.tag\n    save_path = os.path.join('./save', save_name)\n\n    if args.seed is None:\n        if 'seed' not in config:\n            config['seed'] = int(time.time() * 1000) % 1000\n    else:\n        config['seed'] = args.seed\n\n    config['cmd_args'] = sys.argv\n    config['resume'] = args.resume\n\n    main(config, save_path)", ""]}
{"filename": "utils.py", "chunked_list": ["import os\nimport time\nimport shutil\nimport math\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nfrom torch import nn\nimport numpy as np", "from torch import nn\nimport numpy as np\nfrom torch.optim import SGD, Adam\n# from optims import NAdam\nfrom tensorboardX import SummaryWriter\n\nimport fnmatch\n\nclass Averager():\n\n    def __init__(self):\n        self.n = 0.0\n        self.v = 0.0\n\n    def add(self, v, n=1.0):\n        self.v = (self.v * self.n + v * n) / (self.n + n)\n        self.n += n\n\n    def item(self):\n        return self.v", "class Averager():\n\n    def __init__(self):\n        self.n = 0.0\n        self.v = 0.0\n\n    def add(self, v, n=1.0):\n        self.v = (self.v * self.n + v * n) / (self.n + n)\n        self.n += n\n\n    def item(self):\n        return self.v", "\n\nclass Timer():\n\n    def __init__(self):\n        self.v = time.time()\n\n    def s(self):\n        self.v = time.time()\n\n    def t(self):\n        return time.time() - self.v", "\n\ndef time_text(t):\n    if t >= 3600:\n        return '{:.1f}h'.format(t / 3600)\n    elif t >= 60:\n        return '{:.1f}m'.format(t / 60)\n    else:\n        return '{:.1f}s'.format(t)\n", "\n\n_log_path = None\n\n\ndef set_log_path(path):\n    global _log_path\n    _log_path = path\n\n\ndef log(obj, filename='log.txt'):\n    print(obj)\n    if _log_path is not None:\n        with open(os.path.join(_log_path, filename), 'a') as f:\n            print(obj, file=f)", "\n\ndef log(obj, filename='log.txt'):\n    print(obj)\n    if _log_path is not None:\n        with open(os.path.join(_log_path, filename), 'a') as f:\n            print(obj, file=f)\n\ndef without(d, exc_keys):\n    dct1 = {k: v for k, v in d.items() if k not in exc_keys}\n    return dct1", "def without(d, exc_keys):\n    dct1 = {k: v for k, v in d.items() if k not in exc_keys}\n    return dct1\n\ndef same_dict(d1, d2, exc_keys):\n    return without(d1, exc_keys) == without(d2, exc_keys)\n\n\ndef input_checkbox(opts, keys=None, default='0', msg=''):\n    prompts = [msg, f'Default: {default}'] + [f'\\t{i}. {opt}' for i, opt in enumerate(opts)] + [f'Please select: ']\n    if keys is None:\n        keys = list(map(str, range(len(opts))))\n\n    ind = input('\\n'.join(prompts))\n    while(ind not in keys and ind != ''):\n        ind = input('Illegal input, input again: ')\n\n    ind = keys.index(default if ind == '' else ind)\n    return opts[ind], ind", "def input_checkbox(opts, keys=None, default='0', msg=''):\n    prompts = [msg, f'Default: {default}'] + [f'\\t{i}. {opt}' for i, opt in enumerate(opts)] + [f'Please select: ']\n    if keys is None:\n        keys = list(map(str, range(len(opts))))\n\n    ind = input('\\n'.join(prompts))\n    while(ind not in keys and ind != ''):\n        ind = input('Illegal input, input again: ')\n\n    ind = keys.index(default if ind == '' else ind)\n    return opts[ind], ind", "\n\ndef ensure_path(path, remove=True):\n    basename = os.path.basename(path.rstrip('/'))\n    if os.path.exists(path):\n        if remove and (basename.startswith('_') or input('{} exists, remove? (y/[n]): '.format(path)) == 'y'):\n            shutil.rmtree(path)\n            os.makedirs(path)\n        else:\n            os.makedirs(path, exist_ok=True)\n    else:\n        os.makedirs(path)", "\ndef include_patterns(*patterns):\n    \"\"\"Factory function that can be used with copytree() ignore parameter.\n\n    Arguments define a sequence of glob-style patterns\n    that are used to specify what files to NOT ignore.\n    Creates and returns a function that determines this for each directory\n    in the file hierarchy rooted at the source directory when used with\n    shutil.copytree().\n    \"\"\"\n    def _ignore_patterns(path, names):\n        keep = set(name for pattern in patterns\n                            for name in fnmatch.filter(names, pattern))\n        ignore = set(name for name in names\n                        if name not in keep and not os.path.isdir(os.path.join(path, name)))\n        return ignore\n    return _ignore_patterns", "\ndef set_save_path(save_path, remove=True):\n    ensure_path(save_path, remove=remove)\n    set_log_path(save_path)\n\n    # copy tree\n    if os.path.exists(os.path.join(save_path, 'src')):\n        shutil.rmtree(os.path.join(save_path, 'src'))\n    shutil.copytree(os.getcwd(), os.path.join(save_path, 'src'), ignore=shutil.ignore_patterns('__pycache__*', 'data*', 'save*', '.git*', 'datasetprocess*'))\n\n    writer = SummaryWriter(os.path.join(save_path, 'tensorboard'))\n    return log, writer", "\n\ndef compute_num_params(model, text=False):\n    tot = int(sum([np.prod(p.shape) for p in model.parameters()]))\n    if text:\n        if tot >= 1e6:\n            return '{:.1f}M'.format(tot / 1e6)\n        else:\n            return '{:.1f}K'.format(tot / 1e3)\n    else:\n        return tot", "\n\ndef make_optimizer(param_list, optimizer_spec, load_sd=False):\n    Optimizer = {\n        'SGD': SGD,\n        'Adam': Adam,\n        # 'NAdam': NAdam,\n    }[optimizer_spec['name']]\n    optimizer = Optimizer(param_list, **optimizer_spec['args'])\n    if load_sd:\n        optimizer.load_state_dict(optimizer_spec['sd'])\n    return optimizer", "\n\ndef make_coord(shape, ranges=None, flatten=True):\n    \"\"\" Make coordinates at grid centers.\n    \"\"\"\n    coord_seqs = []\n    for i, n in enumerate(shape):\n        if ranges is None:\n            v0, v1 = -1, 1\n        else:\n            v0, v1 = ranges[i]\n        r = (v1 - v0) / (2 * n)\n        seq = v0 + r + (2 * r) * torch.arange(n).float()\n        coord_seqs.append(seq)\n    ret = torch.stack(torch.meshgrid(*coord_seqs), dim=-1)\n    if flatten:\n        ret = ret.view(-1, ret.shape[-1])\n    return ret", "\n\ndef to_pixel_samples(img):\n    \"\"\" Convert the image to coord-RGB pairs.\n        img: Tensor, (3, H, W)\n    \"\"\"\n    coord = make_coord(img.shape[-2:])\n    rgb = img.view(3, -1).permute(1, 0)\n    return coord, rgb\n", "\n\ndef init_params(net):\n    '''Init layer parameters.'''\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n            if m.bias:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n            if m.bias:\n                nn.init.constant_(m.bias, 0)", "\nfrom svgpathtools import Path, Line, QuadraticBezier, CubicBezier, Arc, parse_path, wsvg\n\ncolors = []\n\ndef curves_to_svg(curves, filename, box=256, control_polygon=True):\n    if not hasattr(curves_to_svg, \"colors\"):\n        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n        curves_to_svg.colors = colors\n    \n    if isinstance(curves, torch.Tensor):\n        curves = curves.detach().cpu().numpy()\n\n    n_paths, n_curves, n_cp = curves.shape\n    n_cp = n_cp // 2\n\n    bez_paths = []\n    seg_paths = []\n\n    for i in range(n_paths):\n        bez = []\n        segs = []\n        for j in range(n_curves):\n            cps = (curves[i, j] + 1) * box / 2\n            if n_cp == 4:\n                bez.append(CubicBezier(cps[1] + cps[0]*1j,\n                                       cps[3] + cps[2]*1j,\n                                       cps[5] + cps[4]*1j,\n                                       cps[7] + cps[6]*1j))\n                segs.append(Line(cps[1] + cps[0] * 1j, cps[3] + cps[2] * 1j))\n                segs.append(Line(cps[3] + cps[2] * 1j, cps[5] + cps[4] * 1j))\n                segs.append(Line(cps[5] + cps[4] * 1j, cps[7] + cps[6] * 1j))\n            elif n_cp == 3:\n                bez.append(QuadraticBezier(cps[1] + cps[0]*1j,\n                                       cps[3] + cps[2]*1j,\n                                       cps[5] + cps[4]*1j))\n                segs.append(Line(cps[1] + cps[0] * 1j, cps[3] + cps[2] * 1j))\n                segs.append(Line(cps[3] + cps[2] * 1j, cps[5] + cps[4] * 1j))\n            else:\n                raise NotImplementedError('not implemented order of bezier path')\n        bez_paths.append(Path(*bez))\n        seg_paths.append(Path(*segs))\n    \n    dimensions = (200, 200)\n    viewbox = (0, 0, box, box)\n    # colors = ['black'] * len(paths)\n    # colors = curves_to_svg.colors[:len(bez_paths)]\n    colors = curves_to_svg.colors\n\n    attributes = [\n        {\n            'fill': colors[i % len(colors)],\n            'fill-opacity': 0.3,\n        } for i in range(len(bez_paths)) \n    ]\n\n    if control_polygon:\n        attributes += [\n            {\n                'stroke-dasharray': '10,10',\n                'stroke': colors[i % len(colors)],\n                'fill': 'none',\n                'stroke-width': '0.1',\n            } for i in range(len(seg_paths))\n        ] \n\n    if control_polygon:\n        wsvg(bez_paths + seg_paths, \n            # colors=colors, \n            # stroke_widths=stroke_widths, \n            dimensions=dimensions, \n            viewbox=viewbox, \n            attributes=attributes, \n            filename=filename)\n    else:\n        wsvg(bez_paths,\n            # colors=colors, \n            # stroke_widths=stroke_widths, \n            dimensions=dimensions, \n            viewbox=viewbox, \n            attributes=attributes, \n            filename=filename)", "\ndef importance_sampling_create_image(idx, img_values, size = 128):\n    idx_np = idx.detach().cpu().numpy()\n    img_values_np = img_values.detach().cpu().numpy()\n\n    idx_int = (((idx_np+1)/2)*size).astype(int)\n    img = np.zeros([size, size])\n    img[idx_int[:,0], idx_int[:,1]] = img_values_np\n\n    return img", "\ndef tensor_to_image(ten, fname=None):\n    if isinstance(ten, torch.Tensor):\n        ten = ten.detach().cpu().numpy()\n    \n    if ten.dtype == np.float64 or ten.dtype == np.float32:\n        ten = np.clip(ten, 0, 1)\n        ten = (ten * 255).astype(np.uint8)\n    \n    if fname is None:\n        return ten\n    \n    if len(ten.shape) == 3:\n        if ten.shape[0] == 3:\n            ten = ten.transpose(1, 2, 0)\n        elif ten.shape[0] == 1:\n            ten = ten[0]\n    Image.fromarray(ten).save(fname)", "    \ndef batched_ssim(pred, gt):\n    '''\n    pred: n x 1 x dim x dim\n    gt: n x 1 x dim x dim\n    '''\n\n\n    from skimage.metrics import structural_similarity as ssim\n\n    pred = pred.detach().cpu().numpy()\n    gt = gt.detach().cpu().numpy()\n\n    s = 0\n    for i in range(pred.shape[0]):\n        sv = ssim(pred[i, 0], gt[i, 0], data_range=1, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, multichannel=False)\n        s += sv\n    \n    return s / pred.shape[0]"]}
{"filename": "losses/bezier_sdf_loss.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport losses\nfrom losses import BaseLoss\nimport lpips\n\ndef gradient(y, x, grad_outputs=None):\n    if grad_outputs is None:\n        grad_outputs = torch.ones_like(y)\n    grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n    return grad", "def gradient(y, x, grad_outputs=None):\n    if grad_outputs is None:\n        grad_outputs = torch.ones_like(y)\n    grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n    return grad\n\n@losses.register('img-loss')\nclass ImgLoss(BaseLoss):\n    def __init__(self, lams):\n        super().__init__()\n        self.lams = lams\n        if 'pct' in self.lams:\n            self.lpips = lpips.LPIPS(net='vgg').cuda()\n    \n    def loss_terms(self, out, batch, **kwargs):\n        l = {}\n        if 'l2' in self.lams:\n            l['l2'] = self.lams['l2'] * torch.mean((out['rec'] - batch['full_img']) ** 2)\n        \n        if 'pct' in self.lams:\n            norm_rec = out['rec'].repeat(1, 3, 1, 1) * 2 - 1\n            norm_gt = batch['full_img'].repeat(1, 3, 1, 1) * 2 - 1\n            l['pct'] = self.lams['pct'] * torch.mean(self.lpips(norm_rec, norm_gt))\n\n        return l", "\n@losses.register('latent-loss')\nclass LatentLoss(BaseLoss):\n    def __init__(self, lam):\n        super().__init__()\n        self.lam = lam\n    \n    def loss_terms(self, out, batch, epoch):\n        l = {}\n        l['z'] = self.lam * nn.MSELoss()(out['z'], out['z_detach'])\n        return l", "\n@losses.register('sdf-loss')\nclass SdfLoss(BaseLoss):\n    def __init__(self, lams, udf_warmup=20):\n        super().__init__()\n        self.lams = lams\n        self.udf_warmup = udf_warmup\n        self.lpips = None\n    \n    def loss_terms(self, out, batch, epoch):\n        lams = self.lams\n        l = {}\n\n        # rendered with diffvg loss\n        if 'img' in lams:\n            if out['rendered'].shape == batch['full_img'].shape:\n                l['img_l2'] = lams['img'] * torch.mean((out['rendered'] - batch['full_img']) ** 2)\n            elif out['rendered'].shape == batch['img_origin_res'].shape:\n                l['img_l2'] = lams['img'] * torch.mean((out['rendered'] - batch['img_origin_res']) ** 2)\n        \n        if 'pct' in lams:\n            if self.lpips is None:\n                self.lpips = lpips.LPIPS(net='vgg').cuda()\n            norm_rec = out['rendered'].repeat(1, 3, 1, 1) * 2 - 1\n            norm_gt = batch['full_img'].repeat(1, 3, 1, 1) * 2 - 1\n            l['img_pct'] = lams['pct'] * torch.mean(self.lpips(norm_rec, norm_gt))\n        \n        # rendered with occupancy loss \n        if 'occ' in lams:\n            l['occ'] = lams['occ'] * torch.mean((out['occ'] - batch['pixel']) ** 2)\n        if 'occ_l1' in lams:\n            l['occ_l1'] = lams['occ_l1'] * torch.mean(torch.abs(out['occ'] - batch['pixel']))\n        \n        # unsigned distance loss\n        if self.udf_warmup is not None and epoch <= self.udf_warmup:\n            if 'l1' in lams:\n                if 'weight' in batch:\n                    l['l1'] = lams['l1'] * torch.sum(torch.abs(out['dis'] - batch['dis']) * batch['weight']) / torch.sum(batch['weight'])\n                else:\n                    l['l1'] = lams['l1'] * nn.L1Loss()(out['dis'], batch['dis'])\n        \n        # signed distance loss\n        if 'sdf' in lams:\n            gt_sdf = batch['dis'].clone()\n            gt_sdf[batch['pixel'] < 0.5] *= -1\n            l['sdf'] = lams['sdf'] * torch.abs(out['sdf'] - gt_sdf).mean()\n        \n        # gradient of sdf loss\n        if 'eik' in lams:\n            grad = gradient(out['sdf'], batch['xy'])\n            l['eik'] = lams['eik'] * torch.abs(grad.norm(dim=-1) - 1).mean()\n        \n        return l"]}
{"filename": "losses/vae_loss.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport losses\nfrom losses import BaseLoss\n\n@losses.register('kl-loss')\nclass KLLoss(BaseLoss):\n    def __init__(self, lam):\n        self.lam = lam\n    \n    def loss_terms(self, out, batch, **kwargs):\n        mu = out['mu']\n        log_var = out['log_var']\n        return {\n            'kl': self.lam * torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n        }", "class KLLoss(BaseLoss):\n    def __init__(self, lam):\n        self.lam = lam\n    \n    def loss_terms(self, out, batch, **kwargs):\n        mu = out['mu']\n        log_var = out['log_var']\n        return {\n            'kl': self.lam * torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n        }", "\n@losses.register('kl-loss-two')\nclass KLLossTwo(BaseLoss):\n    def __init__(self, lam):\n        self.lam = lam\n    \n    def loss_terms(self, out, batch, **kwargs):\n        mu = out['mu']\n        log_var = out['log_var']\n\n        tgt_mu = out['mu_detach']\n        tgt_log_var = out['log_var_detach']\n\n        return {\n            'kl': self.lam * torch.mean(0.5 * torch.sum(-1 + tgt_log_var - log_var + (log_var.exp() +  (mu - tgt_mu) ** 2) / torch.clamp(tgt_log_var.exp(), min=0.1), dim=1), dim=0)\n        }", "\n\n\n\n"]}
{"filename": "losses/__init__.py", "chunked_list": ["from .tools import register, make, BaseLoss, ListLoss\nfrom . import bezier_sdf_loss\nfrom . import local_loss\nfrom . import vae_loss"]}
{"filename": "losses/tools.py", "chunked_list": ["import copy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom abc import abstractmethod\n\nlosses = {}\n\ndef register(name):\n    def decorator(cls):\n        losses[name] = cls\n        return cls\n    return decorator", "def register(name):\n    def decorator(cls):\n        losses[name] = cls\n        return cls\n    return decorator\n\n\ndef make(loss_spec, args=None):\n    loss_args = copy.deepcopy(loss_spec.get('args', {}))\n    args = args or {}\n    loss_args.update(args)\n\n    loss = losses[loss_spec['name']](**loss_args)\n    return loss", "\nclass BaseLoss(nn.Module):\n    @abstractmethod\n    def loss_terms(self, *args, **kwargs):\n        pass\n\n    def forward(self, *args, **kwargs):\n        l = self.loss_terms(*args, **kwargs)\n\n        loss = 0\n        for k, v in l.items():\n            loss += v\n        \n        l['loss'] = loss\n\n        return l", "\n@register('list-loss')\nclass ListLoss(BaseLoss):\n    def __init__(self, loss_list):\n        super().__init__()\n        self.loss_list = [make(spec) for spec in loss_list]\n    \n    def loss_terms(self, *args, **kwargs):\n        l = {}\n        for loss_ins in self.loss_list:\n            terms = loss_ins.loss_terms(*args, **kwargs)\n            l.update(terms)\n        \n        return l", "\n        "]}
{"filename": "losses/local_loss.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport losses\nfrom losses import BaseLoss\nfrom models.gutils import bezier_length\nfrom einops import reduce, rearrange, repeat\nfrom objprint import objprint as op\n", "from objprint import objprint as op\n\n# import lpips\n\n@losses.register('local-loss')\nclass LocalLoss(BaseLoss):\n    def __init__(self, lam_render, lam_reg=0):\n        super().__init__()\n        self.lam_render = lam_render\n        self.lam_reg = lam_reg\n    \n    def get_curve_length(self, shape):\n        points = shape.points\n        num_control_points = shape.num_control_points\n        n_curve = len(num_control_points)\n        control_points = []\n        start_index = 0\n        total_len = 0\n        assert(num_control_points.sum() + n_curve == points.shape[0])\n        for i in range(n_curve - 1):\n            num_p = num_control_points[i].item()\n            assert(num_p == 1 or num_p == 0)\n            if num_p == 1: # bezier curve, start_index, start_index + 1, start_index + 2\n                control_points.append(points[start_index : start_index + num_p + 2])\n            else: # length\n                total_len += (points[start_index + 1] - points[start_index]).norm()\n            start_index += num_p + 1\n\n        if num_control_points[-1] == 1:\n            index = [start_index, start_index + 1, 0]\n            control_points.append(points[index])\n        elif num_control_points[-1] == 0:\n            total_len += (points[-1] - points[0]).norm()\n        else:\n            op(shape)\n            exit(0)\n        \n        if len(control_points) > 0:\n            control_points = rearrange(control_points, 'b n d -> b n d')\n            curve_len = bezier_length(control_points[:, 0, :], control_points[:, 1, :], control_points[:, 2, :]).sum()\n            total_len += curve_len\n        return total_len \n\n    \n    def loss_terms(self, img_rendered, target, shapes):\n        loss_render = (img_rendered.mean(dim=-1) - target).pow(2).mean()\n        l = {\n            'render': self.lam_render * loss_render,\n        }\n        if self.lam_reg > 0:\n            loss_reg = sum([self.get_curve_length(shape) for shape in shapes])\n            l.update({\n                'len': self.lam_reg * loss_reg,\n            })\n        return l", "        \n\n"]}
{"filename": "eval/refine_svg.py", "chunked_list": ["import argparse, os, sys, subprocess, copy, random, time, shutil, math\nfrom PIL import Image\n\nimport yaml\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F", "from torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torchvision\nfrom einops import repeat, rearrange, reduce, parse_shape\nimport utils, models\n\nfrom svgpathtools import svg2paths, parse_path, wsvg, Line, QuadraticBezier, CubicBezier, Path\nimport svgwrite\n\ndef to_quadratic_bezier_segments(seg):\n    if isinstance(seg, CubicBezier):\n        p1 = seg.start\n        p2 = seg.control1\n        p3 = seg.control2\n        p4 = seg.end\n        return QuadraticBezier(p1, 0.75 * (p2 + p3) - 0.25 * (p1 + p4), p4)\n\n    elif isinstance(seg, Line):\n        return QuadraticBezier(seg.start, (seg.start + seg.end) / 2, seg.end)\n    elif isinstance(seg, QuadraticBezier):\n        return seg\n    else:\n        raise NotImplementedError('not expected type of segment')", "\ndef to_quadratic_bezier_segments(seg):\n    if isinstance(seg, CubicBezier):\n        p1 = seg.start\n        p2 = seg.control1\n        p3 = seg.control2\n        p4 = seg.end\n        return QuadraticBezier(p1, 0.75 * (p2 + p3) - 0.25 * (p1 + p4), p4)\n\n    elif isinstance(seg, Line):\n        return QuadraticBezier(seg.start, (seg.start + seg.end) / 2, seg.end)\n    elif isinstance(seg, QuadraticBezier):\n        return seg\n    else:\n        raise NotImplementedError('not expected type of segment')", "\ndef convert_path_to_control_points(path, pruned=False):\n    sub_paths = path.continuous_subpaths()\n    if pruned:\n        sub_paths = prune_paths(sub_paths)\n    \n    cps_list = []\n    for sub_path in sub_paths:\n        cps = np.array([[i.start, i.control, i.end] for i in sub_path])\n        cps = np.stack((cps.real, cps.imag), axis=-1)\n        n_curve, n, _ = cps.shape\n        cps = cps.reshape(n_curve, n * 2)\n        cps_list.append(cps)\n    return cps_list", "\ndef merge_d_string_single_channel(d_string_list):\n    args = ['node', 'js/merge_sin.js'] + [f'\"{ds}\"'for ds in d_string_list]\n    res = subprocess.run(args, check=True, encoding='utf-8', capture_output=True)\n    d_string = res.stdout\n    path = parse_path(d_string)\n\n    new_segs = []\n    for i in range(len(path)):\n        new_segs.append(to_quadratic_bezier_segments(path[i]))\n\n    new_path = Path(*new_segs)\n    return new_path, d_string", "\ndef merge_d_string(d_string_list):\n    args = ['node', 'js/merge.js'] + [f'\"{ds}\"'for ds in d_string_list]\n    res = subprocess.run(args, check=True, encoding='utf-8', capture_output=True)\n    d_string = res.stdout\n    path = parse_path(d_string)\n\n    new_segs = []\n    for i in range(len(path)):\n        new_segs.append(to_quadratic_bezier_segments(path[i]))\n\n    new_path = Path(*new_segs)\n    return new_path, d_string", "\ndef write_path_to_svg(curve_tensor_list, filename):\n    sl = 256\n    canvas = svgwrite.Drawing(filename=filename, debug=True)\n    canvas.viewbox(0, 0, sl, sl)\n\n    path_d = []\n    for curve_tensor in curve_tensor_list:\n        path_d.append(models.gutils.path_d_from_control_points(curve_tensor, xy_flip=False))\n    \n    path_d = ' '.join(path_d)\n    path = canvas.path(   \n        d=path_d, \n        fill='#000000',\n    )\n    canvas.add(path)\n    canvas.save()\n    return canvas", "\ndef simplify_path(path):\n    new_segs = []\n    last_end = None\n    accumulated_length = 0\n\n    for seg in path:\n        if last_end is None:\n            last_end = seg.start\n        sl = seg.length()\n        if sl + accumulated_length < 1e-3 * 256:\n            accumulated_length += sl\n            continue\n    \n        accumulated_length = 0\n        seg.start = last_end\n        new_segs.append(seg)\n        last_end = seg.end\n    \n    if len(new_segs) >= 2:\n        return Path(*new_segs)\n    else:\n        return None", "\ndef prune_paths(paths, length=1.0, area=50):\n    # list of path / something from path.continuous_subpaths()\n    pruned_paths = []\n    for path in paths:\n        if path.length() < length or abs(path.area()) < area:\n            continue\n\n        new_path = simplify_path(path)\n        if new_path is not None:\n            pruned_paths.append(new_path)\n\n    return pruned_paths", "\ndef connect_cp_for_tensor_list(cp_tensor_list, norm=False, sidelength=0):\n    cp_connected_tensor_list = []\n    for cp in cp_tensor_list:\n        cp_connected = torch.cat([cp, cp[:, :2].roll(shifts=-1, dims=0)], dim=-1)\n        if norm:\n            cp_connected = 2 * cp_connected / sidelength - 1\n        cp_connected_tensor_list.append(cp_connected)\n    \n    return cp_connected_tensor_list", "\ndef refine_svg(control_points_list, target, w, h, num_iter, loss_fn, verbose=False, prog_bar=False):\n    assert(w == h)\n\n    target = torch.as_tensor(target).cuda()\n\n    cp_tensor_list = []\n    for control_points in control_points_list:\n        cp_tensor = torch.tensor(control_points[:, :-2]).cuda()\n        cp_tensor.requires_grad = True\n        cp_tensor_list.append(cp_tensor)\n\n    optim = torch.optim.Adam(cp_tensor_list, lr=2, betas=(0.9, 0.999))\n\n    renderer = models.OccRender(sidelength=w).cuda()\n    imgs = []\n\n    with tqdm(range(num_iter), leave=False, desc='Refine', disable=not prog_bar) as iters:\n        for i in iters:\n            optim.zero_grad()\n            cp_norm_list = connect_cp_for_tensor_list(cp_tensor_list, norm=True, sidelength=w)\n\n            img_render = renderer(cp_norm_list, kernel=4)\n            if verbose:\n                imgs.append(utils.tensor_to_image(img_render))\n\n            list_of_loss = loss_fn(img_render, target, cp_norm_list)\n            loss = list_of_loss['loss']\n            loss.backward()\n            optim.step()\n\n            for k, v in list_of_loss.items():\n                list_of_loss[k] = f'{v.item():.6f}'\n\n            iters.set_postfix({\n                **list_of_loss,\n            })\n            loss = None\n\n    cp_connected = connect_cp_for_tensor_list(cp_tensor_list, norm=False)\n    return {\n        'control_points': cp_connected,\n        'rendered_images' : imgs, \n    }", "\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--svg', type=str, required=True)\n    parser.add_argument('--inter', type=str, default=None)\n    parser.add_argument('--target', type=str, required=True)\n    parser.add_argument('--num_iter', type=int, default=100)\n    args = parser.parse_args()\n\n    assert(os.path.exists(args.svg))\n\n    _, attributes, svg_attributes = svg2paths(args.svg, return_svg_attributes=True)\n\n    viewbox = svg_attributes['viewBox']\n    viewbox = list(map(int, viewbox.split(' ')))\n    svg_w, svg_h = viewbox[2], viewbox[3]\n\n    d_string_list = [a['d'] + ' Z' for a in attributes]\n\n    path, d_string = merge_d_string(d_string_list)\n\n    if args.inter is not None:\n        wsvg(paths=[path], filename=args.inter, attributes=[{'fill': '#000000'}], svg_attributes=svg_attributes)\n\n    cps = convert_path_to_control_points(path, pruned=True)\n\n    img = np.asarray(Image.open(args.target).convert('L').resize((256, 256), resample=Image.BILINEAR)) / 255.\n    refined_path = refine_svg(cps, img, svg_w, svg_h, num_iter=args.num_iter, verbose=True)['control_points']\n    write_path_to_svg(refined_path, f'refine/final_refined.svg')", ""]}
{"filename": "eval/eval_reconstruction.py", "chunked_list": ["import argparse\nimport os\nimport sys\nsys.path.append(os.getcwd())\nimport time\nimport numpy as np\nimport random\nfrom PIL import Image\nimport imageio\n", "import imageio\n\nimport yaml, shutil\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import MultiStepLR\nimport torch.nn.functional as F\n", "import torch.nn.functional as F\n\nimport torchvision\n\nimport datasets\nimport models, losses\nimport utils\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat, reduce, parse_shape\n", "from einops import rearrange, repeat, reduce, parse_shape\n\nimport refine_svg\nfrom svgpathtools import svg2paths, parse_path, wsvg, Line, QuadraticBezier, CubicBezier, Path\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--outdir', required=True, type=str)\nparser.add_argument('--resume', required=True, type=str)\nparser.add_argument('--seed', default=42, type=int)", "parser.add_argument('--resume', required=True, type=str)\nparser.add_argument('--seed', default=42, type=int)\nargs = parser.parse_args()\n\ndef seed_all(seed):\n    random.seed(seed) # Python\n    np.random.seed(seed) # cpu vars\n    torch.manual_seed(seed) # cpu vars\n\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False", "\ndef make_data_loader(spec, tag=''):\n    if spec is None:\n        return None\n\n    dataset = datasets.make(spec['dataset'])\n\n    loader = DataLoader(dataset, batch_size=spec['batch_size'],\n        shuffle=spec['shuffle'], num_workers=spec['batch_size'], pin_memory=True)\n    return loader", "\ndef make_data_loaders(config):\n    val_loader = make_data_loader(config.get('val_dataset'), tag='val')\n    return val_loader\n\nconfig_str = f'''\nval_dataset:\n  dataset:\n    name: deepvecfont-sdf\n    args:", "    name: deepvecfont-sdf\n    args:\n      data_root: ./data/dvf_png/font_pngs/test\n      img_res: 128\n      char_list: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]\n      include_lower_case: true\n      val: true\n      use_cache: false\n      valid_list: null\n      ratio: 1", "      valid_list: null\n      ratio: 1\n      valid_list: ./data/dvf_png/test_valid.txt\n  batch_size: 32\n  shuffle: false\n'''\n\nseed = args.seed\nseed_all(seed)\nprint('seed:', seed)", "seed_all(seed)\nprint('seed:', seed)\n\nconfig = yaml.load(config_str, Loader=yaml.FullLoader)\noutput_dir = args.outdir\nos.makedirs(output_dir, exist_ok=True)\n\nsv_file = torch.load(args.resume) \n\nsystem = models.make(sv_file['model'], load_sd=True).cuda()", "\nsystem = models.make(sv_file['model'], load_sd=True).cuda()\nsystem.init()\nsystem.eval()\nmodels.freeze(system)\n\nsidelength = 256\ndataloader = make_data_loaders(config)\n\nwith open(os.path.join(output_dir, 'seed.txt'), 'w') as f:\n    f.write(str(seed))", "\nwith open(os.path.join(output_dir, 'seed.txt'), 'w') as f:\n    f.write(str(seed))\n\nfor batch in tqdm(dataloader):\n    for k, v in batch.items():\n        if type(v) is torch.Tensor:\n            batch[k] = v.cuda()\n\n    with torch.no_grad():\n        img = batch['img']\n        z = system.encoder(batch)\n        curves = system.decoder(z)\n        img_rec = torch.clamp(system.decode_image_from_latent_vector(z)['rec'], 0, 1)\n\n    n = curves.shape[0]\n    curves_np_raw = curves.detach().cpu().numpy()\n    curves_np = (curves_np_raw + 1) * sidelength / 2\n    targets = img_rec\n\n    for i in range(n):\n        font_name = batch['font_name'][i]\n        save_dir = os.path.join(output_dir, 'rec_init', font_name)\n        os.makedirs(save_dir, exist_ok=True)\n        char_name = batch['char'][i].item()\n\n        svg_path = os.path.join(save_dir, f'{char_name:02d}_init.svg')\n        raw_path = os.path.join(save_dir, f'{char_name:02d}_raw.svg')\n        img_path = os.path.join(save_dir, f'{char_name:02d}_rec.png')\n        if os.path.exists(svg_path) and os.path.exists(raw_path) and os.path.exists(img_path):\n            continue\n\n        system.write_paths_to_svg(curves_np_raw[i], raw_path)\n\n        utils.tensor_to_image(img_rec[i, 0], img_path)\n\n        curve_np = curves_np[i]\n        d_string_list = [models.gutils.path_d_from_control_points(cp, xy_flip=True) for cp in curve_np]\n\n        path, d_string = refine_svg.merge_d_string(d_string_list)\n\n        cps_list = refine_svg.convert_path_to_control_points(path, pruned=True)\n\n        if len(cps_list) == 0:\n            continue\n        refine_svg.write_path_to_svg(cps_list, svg_path)", ""]}
{"filename": "eval/sample_font.py", "chunked_list": ["import argparse\nimport os\nimport sys\nsys.path.append(os.getcwd())\nimport time\nimport numpy as np\nimport random\nfrom PIL import Image\nimport imageio\n", "import imageio\n\nimport yaml, shutil\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import MultiStepLR\nimport torch.nn.functional as F\n", "import torch.nn.functional as F\n\nimport torchvision\n\nimport datasets\nimport models, losses\nimport utils\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat, reduce, parse_shape\n", "from einops import rearrange, repeat, reduce, parse_shape\n\nimport refine_svg\nfrom svgpathtools import svg2paths, parse_path, wsvg, Line, QuadraticBezier, CubicBezier, Path\nimport pydiffvg\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--outdir', required=True, type=str)\nparser.add_argument('--resume', required=True, type=str)\nparser.add_argument('--seed', default=42, type=int)", "parser.add_argument('--resume', required=True, type=str)\nparser.add_argument('--seed', default=42, type=int)\nparser.add_argument('--n-sample', default=20, type=int)\nparser.add_argument('--begin-index', default=0, type=int)\n\nargs = parser.parse_args()\n\ndef seed_all(seed):\n    random.seed(seed) # Python\n    np.random.seed(seed) # cpu vars\n    torch.manual_seed(seed) # cpu vars\n\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False", "\ndef make_data_loader(spec, tag=''):\n    if spec is None:\n        return None\n\n    dataset = datasets.make(spec['dataset'])\n\n    loader = DataLoader(dataset, batch_size=spec['batch_size'],\n        shuffle=spec['shuffle'], num_workers=12, pin_memory=True)\n    return loader", "\ndef make_data_loaders(config):\n    val_loader = make_data_loader(config.get('val_dataset'), tag='val')\n    return val_loader\n\nref_char_list = [0,1]\n\nconfig_str = f'''\nloss:\n  name: local-loss", "loss:\n  name: local-loss\n  args:\n    lam_render: 1\n    lam_reg: 1.e-6\n'''\n\nseed = args.seed\nseed_all(seed)\nprint('seed:', seed)", "seed_all(seed)\nprint('seed:', seed)\n\nconfig = yaml.load(config_str, Loader=yaml.FullLoader)\noutput_dir = args.outdir\nos.makedirs(output_dir, exist_ok=True)\nos.makedirs(os.path.join(output_dir, 'all_rec'), exist_ok=True)\n\nsv_file = torch.load(args.resume)\n", "sv_file = torch.load(args.resume)\n\nsystem = models.make(sv_file['model'], load_sd=True).cuda()\nsystem.init()\nsystem.eval()\nmodels.freeze(system)\n\nsidelength = 256\nloss_fn = losses.make(config['loss'])\nchar_nums = 52", "loss_fn = losses.make(config['loss'])\nchar_nums = 52\n\nn_sample = args.n_sample\nsample_begin = args.begin_index\n\nwith open(os.path.join(output_dir, 'seed.txt'), 'w') as f:\n    f.write(str(seed))\n\nfor sample_i in tqdm(range(sample_begin, sample_begin + n_sample)):\n    parent_dir = os.path.join(output_dir, f'{sample_i:04d}')\n    os.makedirs(parent_dir, exist_ok=True)\n\n    with torch.no_grad():\n        tgt_char_idx = torch.arange(char_nums).cuda()\n        z_style = torch.randn(1, 256).cuda() * 1.5\n        torch.save(z_style.detach().cpu(), os.path.join(parent_dir, 'z_style.pt'))\n\n        z_style = z_style.expand(char_nums, -1)\n        emb_char = system.cls_token(tgt_char_idx)\n        z = system.merge(torch.cat([z_style, emb_char], dim=-1))\n        curves = system.decoder(z)\n\n        img_rec = system.decode_image_from_latent_vector(z)['rec']\n\n    n = curves.shape[0]\n    curves_np = curves.detach().cpu().numpy()\n    curves_np = (curves_np + 1) * sidelength / 2\n    targets = img_rec\n\n    torchvision.utils.save_image(targets, os.path.join(output_dir, 'all_rec', f'{sample_i:04d}_rec.png'), nrow=13)\n\n    for i in range(n):\n        path_prefix = os.path.join(output_dir, f'{sample_i:04d}', f'{i:02d}')\n        if os.path.exists(path_prefix + '_init.svg'):\n            continue\n\n        target = targets[i, 0]\n        utils.tensor_to_image(img_rec[i, 0], path_prefix + '_rec.png')\n\n        curve_np = curves_np[i]\n        d_string_list = [models.gutils.path_d_from_control_points(cp, xy_flip=True) for cp in curve_np]\n        path, d_string = refine_svg.merge_d_string(d_string_list)\n\n        cps_list = refine_svg.convert_path_to_control_points(path, pruned=True)\n\n        if len(cps_list) == 0:\n            continue\n        refine_svg.write_path_to_svg(cps_list, path_prefix + '_init.svg')\n        continue", "\nfor sample_i in tqdm(range(sample_begin, sample_begin + n_sample)):\n    parent_dir = os.path.join(output_dir, f'{sample_i:04d}')\n    os.makedirs(parent_dir, exist_ok=True)\n\n    with torch.no_grad():\n        tgt_char_idx = torch.arange(char_nums).cuda()\n        z_style = torch.randn(1, 256).cuda() * 1.5\n        torch.save(z_style.detach().cpu(), os.path.join(parent_dir, 'z_style.pt'))\n\n        z_style = z_style.expand(char_nums, -1)\n        emb_char = system.cls_token(tgt_char_idx)\n        z = system.merge(torch.cat([z_style, emb_char], dim=-1))\n        curves = system.decoder(z)\n\n        img_rec = system.decode_image_from_latent_vector(z)['rec']\n\n    n = curves.shape[0]\n    curves_np = curves.detach().cpu().numpy()\n    curves_np = (curves_np + 1) * sidelength / 2\n    targets = img_rec\n\n    torchvision.utils.save_image(targets, os.path.join(output_dir, 'all_rec', f'{sample_i:04d}_rec.png'), nrow=13)\n\n    for i in range(n):\n        path_prefix = os.path.join(output_dir, f'{sample_i:04d}', f'{i:02d}')\n        if os.path.exists(path_prefix + '_init.svg'):\n            continue\n\n        target = targets[i, 0]\n        utils.tensor_to_image(img_rec[i, 0], path_prefix + '_rec.png')\n\n        curve_np = curves_np[i]\n        d_string_list = [models.gutils.path_d_from_control_points(cp, xy_flip=True) for cp in curve_np]\n        path, d_string = refine_svg.merge_d_string(d_string_list)\n\n        cps_list = refine_svg.convert_path_to_control_points(path, pruned=True)\n\n        if len(cps_list) == 0:\n            continue\n        refine_svg.write_path_to_svg(cps_list, path_prefix + '_init.svg')\n        continue"]}
{"filename": "eval/eval_generation_multiref.py", "chunked_list": ["import argparse\nimport os\nimport sys\nsys.path.append(os.getcwd())\nimport numpy as np\nimport random\nfrom PIL import Image\nimport imageio\n\nimport yaml, shutil", "\nimport yaml, shutil\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import MultiStepLR\nimport torch.nn.functional as F\n\nimport torchvision", "\nimport torchvision\n\nimport datasets\nimport models, losses\nimport utils\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat, reduce, parse_shape\n\nimport refine_svg", "\nimport refine_svg\nfrom svgpathtools import svg2paths, parse_path, wsvg, Line, QuadraticBezier, CubicBezier, Path\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--outdir', required=True, type=str)\nparser.add_argument('--resume', required=True, type=str)\nparser.add_argument('--seed', default=42, type=int)\nargs = parser.parse_args()\n\ndef seed_all(seed):\n    random.seed(seed) # Python\n    np.random.seed(seed) # cpu vars\n    torch.manual_seed(seed) # cpu vars\n\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False", "args = parser.parse_args()\n\ndef seed_all(seed):\n    random.seed(seed) # Python\n    np.random.seed(seed) # cpu vars\n    torch.manual_seed(seed) # cpu vars\n\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False", "\ndef make_data_loader(spec, tag=''):\n    if spec is None:\n        return None\n\n    dataset = datasets.make(spec['dataset'])\n\n    loader = DataLoader(dataset, batch_size=spec['batch_size'],\n        shuffle=spec['shuffle'], num_workers=12, pin_memory=True)\n    return loader", "\ndef make_data_loaders(config):\n    val_loader = make_data_loader(config.get('val_dataset'), tag='val')\n    return val_loader\n\nseed_all(args.seed)\n\nref_char_list = [0,1,26,27]\n\nconfig_str = f'''", "\nconfig_str = f'''\nval_dataset:\n  dataset:\n    name: dvf-eval\n    args:\n      data_root: ./data/dvf_png/font_pngs/test\n      img_res: 128\n      valid_list: null\n      ref_list: {ref_char_list}", "      valid_list: null\n      ref_list: {ref_char_list}\n  batch_size: 1\n  shuffle: false\n'''\n\nconfig = yaml.load(config_str, Loader=yaml.FullLoader)\noutput_dir = args.outdir\n\nsv_file = torch.load(args.resume)", "\nsv_file = torch.load(args.resume)\n\nsystem = models.make(sv_file['model'], load_sd=True).cuda()\nsystem.init()\nsystem.eval()\nmodels.freeze(system)\n\ndataloader = make_data_loaders(config)\nsidelength = 256", "dataloader = make_data_loaders(config)\nsidelength = 256\n\nchar_nums = 52\n\nfor batch in tqdm(dataloader):\n    for k, v in batch.items():\n        if type(v) is torch.Tensor:\n            batch[k] = v.cuda()\n    \n    with torch.no_grad():\n        refs = batch['refs'] # 1 x 2 x 1 x 64 x 64\n        ref_char_idx = batch['ref_char_idx'] # 1 x 2\n        n_ref = batch['n_ref']\n        tgt_char_idx = torch.arange(char_nums).cuda()\n\n        mu, _ = system.ref_img_encode(batch)\n        mu_r = mu.expand(char_nums, -1) \n        emb_char = system.cls_token(tgt_char_idx)\n\n        z = system.merge(torch.cat([mu_r, emb_char], dim=-1))\n        curves = system.decoder(z)\n        img_rec = system.decode_image_from_latent_vector(z)['rec']\n\n    n = curves.shape[0]\n    curves_np_raw = curves.detach().cpu().numpy()\n    curves_np = (curves_np_raw + 1) * sidelength / 2\n    targets = img_rec\n    assert(n == char_nums)\n\n    font_name = batch['font_name'][0]\n    save_dir = os.path.join(output_dir, 'rec_init', font_name)\n    os.makedirs(save_dir, exist_ok=True)\n    for i in range(n):\n        char_name = i\n\n        svg_path = os.path.join(save_dir, f'{char_name:02d}_init.svg')\n        raw_path = os.path.join(save_dir, f'{char_name:02d}_raw.svg')\n        img_path = os.path.join(save_dir, f'{char_name:02d}_rec.png')\n        if os.path.exists(svg_path) and os.path.exists(raw_path) and os.path.exists(img_path):\n            continue\n\n        system.write_paths_to_svg(curves_np_raw[i], raw_path)\n\n        utils.tensor_to_image(img_rec[i, 0], img_path)\n\n        curve_np = curves_np[i]\n        d_string_list = [models.gutils.path_d_from_control_points(cp, xy_flip=True) for cp in curve_np]\n\n        path, d_string = refine_svg.merge_d_string(d_string_list)\n\n        cps_list = refine_svg.convert_path_to_control_points(path, pruned=True)\n\n        if len(cps_list) == 0:\n            continue\n        refine_svg.write_path_to_svg(cps_list, svg_path)"]}
{"filename": "eval/diffvg_parse_svg.py", "chunked_list": ["import torch\nimport xml.etree.ElementTree as etree\nimport numpy as np\nimport diffvg\nimport os\nimport pydiffvg\nimport svgpathtools\nimport svgpathtools.parser\nimport re\nimport warnings", "import re\nimport warnings\nimport cssutils\nimport logging\nimport matplotlib.colors \ncssutils.log.setLevel(logging.ERROR)\n\ndef remove_namespaces(s):\n    \"\"\"\n        {...} ... -> ...\n    \"\"\"\n    return re.sub('{.*}', '', s)", "\ndef parse_style(s, defs):\n    style_dict = {}\n    for e in s.split(';'):\n        key_value = e.split(':')\n        if len(key_value) == 2:\n            key = key_value[0].strip()\n            value = key_value[1].strip()\n            if key == 'fill' or key == 'stroke':\n                # Special case: convert colors into tensor in definitions so\n                # that different shapes can share the same color\n                value = parse_color(value, defs)\n            style_dict[key] = value\n    return style_dict", "\ndef parse_hex(s):\n    \"\"\"\n        Hex to tuple\n    \"\"\"\n    s = s.lstrip('#')\n    if len(s) == 3:\n        s = s[0] + s[0] + s[1] + s[1] + s[2] + s[2]\n    rgb = tuple(int(s[i:i+2], 16) for i in (0, 2, 4))\n    # sRGB to RGB\n    # return torch.pow(torch.tensor([rgb[0] / 255.0, rgb[1] / 255.0, rgb[2] / 255.0]), 2.2)\n    return torch.pow(torch.tensor([rgb[0] / 255.0, rgb[1] / 255.0, rgb[2] / 255.0]), 1.0)", "\ndef parse_int(s):\n    \"\"\"\n        trim alphabets\n    \"\"\"\n    return int(float(''.join(i for i in s if (not i.isalpha()))))\n\ndef parse_color(s, defs):\n    if s is None:\n        return None\n    if isinstance(s, torch.Tensor):\n        return s\n    s = s.lstrip(' ')\n    color = torch.tensor([0.0, 0.0, 0.0, 1.0])\n    if s[0] == '#':\n        color[:3] = parse_hex(s)\n    elif s[:3] == 'url':\n        # url(#id)\n        color = defs[s[4:-1].lstrip('#')]\n    elif s == 'none':\n        color = None\n    elif s[:4] == 'rgb(':\n        rgb = s[4:-1].split(',')\n        color = torch.tensor([int(rgb[0]) / 255.0, int(rgb[1]) / 255.0, int(rgb[2]) / 255.0, 1.0])\n    elif s == 'none':\n        return None\n    else:\n        try : \n            rgba = matplotlib.colors.to_rgba(s)\n            color = torch.tensor(rgba)\n        except ValueError : \n            warnings.warn('Unknown color command ' + s)\n    return color", "\n# https://github.com/mathandy/svgpathtools/blob/7ebc56a831357379ff22216bec07e2c12e8c5bc6/svgpathtools/parser.py\ndef _parse_transform_substr(transform_substr):\n    type_str, value_str = transform_substr.split('(')\n    value_str = value_str.replace(',', ' ')\n    values = list(map(float, filter(None, value_str.split(' '))))\n\n    transform = np.identity(3)\n    if 'matrix' in type_str:\n        transform[0:2, 0:3] = np.array([values[0:6:2], values[1:6:2]])\n    elif 'translate' in transform_substr:\n        transform[0, 2] = values[0]\n        if len(values) > 1:\n            transform[1, 2] = values[1]\n    elif 'scale' in transform_substr:\n        x_scale = values[0]\n        y_scale = values[1] if (len(values) > 1) else x_scale\n        transform[0, 0] = x_scale\n        transform[1, 1] = y_scale\n    elif 'rotate' in transform_substr:\n        angle = values[0] * np.pi / 180.0\n        if len(values) == 3:\n            offset = values[1:3]\n        else:\n            offset = (0, 0)\n        tf_offset = np.identity(3)\n        tf_offset[0:2, 2:3] = np.array([[offset[0]], [offset[1]]])\n        tf_rotate = np.identity(3)\n        tf_rotate[0:2, 0:2] = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n        tf_offset_neg = np.identity(3)\n        tf_offset_neg[0:2, 2:3] = np.array([[-offset[0]], [-offset[1]]])\n\n        transform = tf_offset.dot(tf_rotate).dot(tf_offset_neg)\n    elif 'skewX' in transform_substr:\n        transform[0, 1] = np.tan(values[0] * np.pi / 180.0)\n    elif 'skewY' in transform_substr:\n        transform[1, 0] = np.tan(values[0] * np.pi / 180.0)\n    else:\n        # Return an identity matrix if the type of transform is unknown, and warn the user\n        warnings.warn('Unknown SVG transform type: {0}'.format(type_str))\n    return transform", "\ndef parse_transform(transform_str):\n    \"\"\"\n        Converts a valid SVG transformation string into a 3x3 matrix.\n        If the string is empty or null, this returns a 3x3 identity matrix\n    \"\"\"\n    if not transform_str:\n        return np.identity(3)\n    elif not isinstance(transform_str, str):\n        raise TypeError('Must provide a string to parse')\n\n    total_transform = np.identity(3)\n    transform_substrs = transform_str.split(')')[:-1]  # Skip the last element, because it should be empty\n    for substr in transform_substrs:\n        total_transform = total_transform.dot(_parse_transform_substr(substr))\n\n    return torch.from_numpy(total_transform).type(torch.float32)", "\ndef parse_linear_gradient(node, transform, defs):\n    begin = torch.tensor([0.0, 0.0])\n    end = torch.tensor([0.0, 0.0])\n    offsets = []\n    stop_colors = []\n    # Inherit from parent\n    for key in node.attrib:\n        if remove_namespaces(key) == 'href':\n            value = node.attrib[key]\n            parent = defs[value.lstrip('#')]\n            begin = parent.begin\n            end = parent.end\n            offsets = parent.offsets\n            stop_colors = parent.stop_colors\n\n    for attrib in node.attrib:\n        attrib = remove_namespaces(attrib)\n        if attrib == 'x1':\n            begin[0] = float(node.attrib['x1'])\n        elif attrib == 'y1':\n            begin[1] = float(node.attrib['y1'])\n        elif attrib == 'x2':\n            end[0] = float(node.attrib['x2'])\n        elif attrib == 'y2':\n            end[1] = float(node.attrib['y2'])\n        elif attrib == 'gradientTransform':\n            transform = transform @ parse_transform(node.attrib['gradientTransform'])\n\n    begin = transform @ torch.cat((begin, torch.ones([1])))\n    begin = begin / begin[2]\n    begin = begin[:2]\n    end = transform @ torch.cat((end, torch.ones([1])))\n    end = end / end[2]\n    end = end[:2]\n\n    for child in node:\n        tag = remove_namespaces(child.tag)\n        if tag == 'stop':\n            offset = float(child.attrib['offset'])\n            color = [0.0, 0.0, 0.0, 1.0]\n            if 'stop-color' in child.attrib:\n                c = parse_color(child.attrib['stop-color'], defs)\n                color[:3] = [c[0], c[1], c[2]]\n            if 'stop-opacity' in child.attrib:\n                color[3] = float(child.attrib['stop-opacity'])\n            if 'style' in child.attrib:\n                style = parse_style(child.attrib['style'], defs)\n                if 'stop-color' in style:\n                    c = parse_color(style['stop-color'], defs)\n                    color[:3] = [c[0], c[1], c[2]]\n                if 'stop-opacity' in style:\n                    color[3] = float(style['stop-opacity'])\n            offsets.append(offset)\n            stop_colors.append(color)\n    if isinstance(offsets, list):\n        offsets = torch.tensor(offsets)\n    if isinstance(stop_colors, list):\n        stop_colors = torch.tensor(stop_colors)\n\n    return pydiffvg.LinearGradient(begin, end, offsets, stop_colors)", "\n\ndef parse_radial_gradient(node, transform, defs):\n    begin = torch.tensor([0.0, 0.0])\n    end = torch.tensor([0.0, 0.0])\n    center = torch.tensor([0.0, 0.0])\n    radius = torch.tensor([0.0, 0.0])\n    offsets = []\n    stop_colors = []\n    # Inherit from parent\n    for key in node.attrib:\n        if remove_namespaces(key) == 'href':\n            value = node.attrib[key]\n            parent = defs[value.lstrip('#')]\n            begin = parent.begin\n            end = parent.end\n            offsets = parent.offsets\n            stop_colors = parent.stop_colors\n\n    for attrib in node.attrib:\n        attrib = remove_namespaces(attrib)\n        if attrib == 'cx':\n            center[0] = float(node.attrib['cx'])\n        elif attrib == 'cy':\n            center[1] = float(node.attrib['cy'])\n        elif attrib == 'fx':\n            radius[0] = float(node.attrib['fx'])\n        elif attrib == 'fy':\n            radius[1] = float(node.attrib['fy'])\n        elif attrib == 'fr':\n            radius[0] = float(node.attrib['fr'])\n            radius[1] = float(node.attrib['fr'])\n        elif attrib == 'gradientTransform':\n            transform = transform @ parse_transform(node.attrib['gradientTransform'])\n\n    # TODO: this is incorrect\n    center = transform @ torch.cat((center, torch.ones([1])))\n    center = center / center[2]\n    center = center[:2]\n\n    for child in node:\n        tag = remove_namespaces(child.tag)\n        if tag == 'stop':\n            offset = float(child.attrib['offset'])\n            color = [0.0, 0.0, 0.0, 1.0]\n            if 'stop-color' in child.attrib:\n                c = parse_color(child.attrib['stop-color'], defs)\n                color[:3] = [c[0], c[1], c[2]]\n            if 'stop-opacity' in child.attrib:\n                color[3] = float(child.attrib['stop-opacity'])\n            if 'style' in child.attrib:\n                style = parse_style(child.attrib['style'], defs)\n                if 'stop-color' in style:\n                    c = parse_color(style['stop-color'], defs)\n                    color[:3] = [c[0], c[1], c[2]]\n                if 'stop-opacity' in style:\n                    color[3] = float(style['stop-opacity'])\n            offsets.append(offset)\n            stop_colors.append(color)\n    if isinstance(offsets, list):\n        offsets = torch.tensor(offsets)\n    if isinstance(stop_colors, list):\n        stop_colors = torch.tensor(stop_colors)\n\n    return pydiffvg.RadialGradient(begin, end, offsets, stop_colors)", "\ndef parse_stylesheet(node, transform, defs):\n    # collect CSS classes\n    sheet = cssutils.parseString(node.text)\n    for rule in sheet:\n        if hasattr(rule, 'selectorText') and hasattr(rule, 'style'):\n            name = rule.selectorText\n            if len(name) >= 2 and name[0] == '.':\n                defs[name[1:]] = parse_style(rule.style.getCssText(), defs)\n    return defs", "\ndef parse_defs(node, transform, defs):\n    for child in node:\n        tag = remove_namespaces(child.tag)\n        if tag == 'linearGradient':\n            if 'id' in child.attrib:\n                defs[child.attrib['id']] = parse_linear_gradient(child, transform, defs)\n        elif tag == 'radialGradient':\n            if 'id' in child.attrib:\n                defs[child.attrib['id']] = parse_radial_gradient(child, transform, defs)\n        elif tag == 'style':\n            defs = parse_stylesheet(child, transform, defs)\n    return defs", "\ndef parse_common_attrib(node, transform, fill_color, defs):\n    attribs = {}\n    if 'class' in node.attrib:\n        attribs.update(defs[node.attrib['class']])\n    attribs.update(node.attrib)\n\n    name = ''\n    if 'id' in node.attrib:\n        name = node.attrib['id']\n\n    stroke_color = None\n    stroke_width = torch.tensor(0.5)\n    use_even_odd_rule = False\n\n    new_transform = transform\n    if 'transform' in attribs:\n        new_transform = transform @ parse_transform(attribs['transform'])\n    if 'fill' in attribs:\n        fill_color = parse_color(attribs['fill'], defs)\n    fill_opacity = 1.0\n    if 'fill-opacity' in attribs:\n        fill_opacity *= float(attribs['fill-opacity'])\n    if 'opacity' in attribs:\n        fill_opacity *= float(attribs['opacity'])\n    # Ignore opacity if the color is a gradient\n    if isinstance(fill_color, torch.Tensor):\n        fill_color[3] = fill_opacity\n\n    if 'fill-rule' in attribs:\n        if attribs['fill-rule'] == \"evenodd\":\n            use_even_odd_rule = True\n        elif attribs['fill-rule'] == \"nonzero\":\n            use_even_odd_rule = False\n        else:\n            warnings.warn('Unknown fill-rule: {}'.format(attribs['fill-rule']))\n\n    if 'stroke' in attribs:\n        stroke_color = parse_color(attribs['stroke'], defs)\n\n    if 'stroke-width' in attribs:\n        stroke_width = attribs['stroke-width']\n        if stroke_width[-2:] == 'px':\n            stroke_width = stroke_width[:-2]\n        stroke_width = torch.tensor(float(stroke_width) / 2.0)\n\n    if 'style' in attribs:\n        style = parse_style(attribs['style'], defs)\n        if 'fill' in style:\n            fill_color = parse_color(style['fill'], defs)\n        fill_opacity = 1.0\n        if 'fill-opacity' in style:\n            fill_opacity *= float(style['fill-opacity'])\n        if 'opacity' in style:\n            fill_opacity *= float(style['opacity'])\n        if 'fill-rule' in style:\n            if style['fill-rule'] == \"evenodd\":\n                use_even_odd_rule = True\n            elif style['fill-rule'] == \"nonzero\":\n                use_even_odd_rule = False\n            else:\n                warnings.warn('Unknown fill-rule: {}'.format(style['fill-rule']))\n        # Ignore opacity if the color is a gradient\n        if isinstance(fill_color, torch.Tensor):\n            fill_color[3] = fill_opacity\n        if 'stroke' in style:\n            if style['stroke'] != 'none':\n                stroke_color = parse_color(style['stroke'], defs)\n                # Ignore opacity if the color is a gradient\n                if isinstance(stroke_color, torch.Tensor):\n                    if 'stroke-opacity' in style:\n                        stroke_color[3] = float(style['stroke-opacity'])\n                    if 'opacity' in style:\n                        stroke_color[3] *= float(style['opacity'])\n                if 'stroke-width' in style:\n                    stroke_width = style['stroke-width']\n                    if stroke_width[-2:] == 'px':\n                        stroke_width = stroke_width[:-2]\n                    stroke_width = torch.tensor(float(stroke_width) / 2.0)\n\n        if isinstance(fill_color, pydiffvg.LinearGradient):\n            fill_color.begin = new_transform @ torch.cat((fill_color.begin, torch.ones([1])))\n            fill_color.begin = fill_color.begin / fill_color.begin[2]\n            fill_color.begin = fill_color.begin[:2]\n            fill_color.end = new_transform @ torch.cat((fill_color.end, torch.ones([1])))\n            fill_color.end = fill_color.end / fill_color.end[2]\n            fill_color.end = fill_color.end[:2]\n        if isinstance(stroke_color, pydiffvg.LinearGradient):\n            stroke_color.begin = new_transform @ torch.cat((stroke_color.begin, torch.ones([1])))\n            stroke_color.begin = stroke_color.begin / stroke_color.begin[2]\n            stroke_color.begin = stroke_color.begin[:2]\n            stroke_color.end = new_transform @ torch.cat((stroke_color.end, torch.ones([1])))\n            stroke_color.end = stroke_color.end / stroke_color.end[2]\n            stroke_color.end = stroke_color.end[:2]\n        if 'filter' in style:\n            print('*** WARNING ***: Ignoring filter for path with id \"{}\"'.format(name))\n\n    return new_transform, fill_color, stroke_color, stroke_width, use_even_odd_rule", "\ndef is_shape(tag):\n    return tag == 'path' or tag == 'polygon' or tag == 'line' or tag == 'circle' or tag == 'rect'\n\ndef parse_shape(node, transform, fill_color, shapes, shape_groups, defs):\n    tag = remove_namespaces(node.tag)\n    new_transform, new_fill_color, stroke_color, stroke_width, use_even_odd_rule = \\\n        parse_common_attrib(node, transform, fill_color, defs)\n    if tag == 'path':\n        d = node.attrib['d']\n        name = ''\n        if 'id' in node.attrib:\n            name = node.attrib['id']\n        force_closing = new_fill_color is not None\n        paths = pydiffvg.from_svg_path(d, new_transform, force_closing)\n        for idx, path in enumerate(paths):\n            assert(path.points.shape[1] == 2)\n            path.stroke_width = stroke_width\n            path.source_id = name\n            path.id = \"{}-{}\".format(name,idx) if len(paths)>1 else name\n        prev_shapes_size = len(shapes)\n        shapes = shapes + paths\n        shape_ids = torch.tensor(list(range(prev_shapes_size, len(shapes))))\n        shape_groups.append(pydiffvg.ShapeGroup(\\\n            shape_ids = shape_ids,\n            fill_color = new_fill_color,\n            stroke_color = stroke_color,\n            use_even_odd_rule = use_even_odd_rule,\n            id = name))\n    elif tag == 'polygon':\n        name = ''\n        if 'id' in node.attrib:\n            name = node.attrib['id']\n        force_closing = new_fill_color is not None\n        pts = node.attrib['points'].strip()\n        pts = pts.split(' ')\n        # import ipdb; ipdb.set_trace()\n        pts = [[float(y) for y in re.split(',| ', x)] for x in pts if x]\n        pts = torch.tensor(pts, dtype=torch.float32).view(-1, 2)\n        polygon = pydiffvg.Polygon(pts, force_closing)\n        polygon.stroke_width = stroke_width\n        shape_ids = torch.tensor([len(shapes)])\n        shapes.append(polygon)\n        shape_groups.append(pydiffvg.ShapeGroup(\\\n            shape_ids = shape_ids,\n            fill_color = new_fill_color,\n            stroke_color = stroke_color,\n            use_even_odd_rule = use_even_odd_rule,\n            shape_to_canvas = new_transform,\n            id = name))\n    elif tag == 'line':\n        x1 = float(node.attrib['x1'])\n        y1 = float(node.attrib['y1'])\n        x2 = float(node.attrib['x2'])\n        y2 = float(node.attrib['y2'])\n        p1 = torch.tensor([x1, y1])\n        p2 = torch.tensor([x2, y2])\n        points = torch.stack((p1, p2))\n        line = pydiffvg.Polygon(points, False)\n        line.stroke_width = stroke_width\n        shape_ids = torch.tensor([len(shapes)])\n        shapes.append(line)\n        shape_groups.append(pydiffvg.ShapeGroup(\\\n            shape_ids = shape_ids,\n            fill_color = new_fill_color,\n            stroke_color = stroke_color,\n            use_even_odd_rule = use_even_odd_rule,\n            shape_to_canvas = new_transform))\n    elif tag == 'circle':\n        radius = float(node.attrib['r'])\n        cx = float(node.attrib['cx'])\n        cy = float(node.attrib['cy'])\n        name = ''\n        if 'id' in node.attrib:\n            name = node.attrib['id']\n        center = torch.tensor([cx, cy])\n        circle = pydiffvg.Circle(radius = torch.tensor(radius),\n                                 center = center)\n        circle.stroke_width = stroke_width\n        shape_ids = torch.tensor([len(shapes)])\n        shapes.append(circle)\n        shape_groups.append(pydiffvg.ShapeGroup(\\\n            shape_ids = shape_ids,\n            fill_color = new_fill_color,\n            stroke_color = stroke_color,\n            use_even_odd_rule = use_even_odd_rule,\n            shape_to_canvas = new_transform))\n    elif tag == 'ellipse':\n        rx = float(node.attrib['rx'])\n        ry = float(node.attrib['ry'])\n        cx = float(node.attrib['cx'])\n        cy = float(node.attrib['cy'])\n        name = ''\n        if 'id' in node.attrib:\n            name = node.attrib['id']\n        center = torch.tensor([cx, cy])\n        circle = pydiffvg.Circle(radius = torch.tensor(radius),\n                                 center = center)\n        circle.stroke_width = stroke_width\n        shape_ids = torch.tensor([len(shapes)])\n        shapes.append(circle)\n        shape_groups.append(pydiffvg.ShapeGroup(\\\n            shape_ids = shape_ids,\n            fill_color = new_fill_color,\n            stroke_color = stroke_color,\n            use_even_odd_rule = use_even_odd_rule,\n            shape_to_canvas = new_transform))\n    elif tag == 'rect':\n        x = 0.0\n        y = 0.0\n        if x in node.attrib:\n            x = float(node.attrib['x'])\n        if y in node.attrib:\n            y = float(node.attrib['y'])\n        w = float(node.attrib['width'])\n        h = float(node.attrib['height'])\n        p_min = torch.tensor([x, y])\n        p_max = torch.tensor([x + w, x + h])\n        rect = pydiffvg.Rect(p_min = p_min, p_max = p_max)\n        rect.stroke_width = stroke_width\n        shape_ids = torch.tensor([len(shapes)])\n        shapes.append(rect)\n        shape_groups.append(pydiffvg.ShapeGroup(\\\n            shape_ids = shape_ids,\n            fill_color = new_fill_color,\n            stroke_color = stroke_color,\n            use_even_odd_rule = use_even_odd_rule,\n            shape_to_canvas = new_transform))\n    return shapes, shape_groups", "\ndef parse_group(node, transform, fill_color, shapes, shape_groups, defs):\n    if 'transform' in node.attrib:\n        transform = transform @ parse_transform(node.attrib['transform'])\n    if 'fill' in node.attrib:\n        fill_color = parse_color(node.attrib['fill'], defs)\n    for child in node:\n        tag = remove_namespaces(child.tag)\n        if is_shape(tag):\n            shapes, shape_groups = parse_shape(\\\n                child, transform, fill_color, shapes, shape_groups, defs)\n        elif tag == 'g':\n            shapes, shape_groups = parse_group(\\\n                child, transform, fill_color, shapes, shape_groups, defs)\n    return shapes, shape_groups", "\ndef parse_scene(node):\n    canvas_width = -1\n    canvas_height = -1\n    defs = {}\n    shapes = []\n    shape_groups = []\n    fill_color = torch.tensor([0.0, 0.0, 0.0, 1.0])\n    transform = torch.eye(3)\n    if 'viewBox' in node.attrib:\n        view_box_array_comma = node.attrib['viewBox'].split(',')\n        view_box_array = node.attrib['viewBox'].split()\n        if len(view_box_array) < len(view_box_array_comma):\n            view_box_array = view_box_array_comma\n        canvas_width = parse_int(view_box_array[2])\n        canvas_height = parse_int(view_box_array[3])\n    else:\n        if 'width' in node.attrib:\n            canvas_width = parse_int(node.attrib['width'])\n        else:\n            print('Warning: Can\\'t find canvas width.')\n        if 'height' in node.attrib:\n            canvas_height = parse_int(node.attrib['height'])\n        else:\n            print('Warning: Can\\'t find canvas height.')\n    for child in node:\n        tag = remove_namespaces(child.tag)\n        if tag == 'defs':\n            defs = parse_defs(child, transform, defs)\n        elif tag == 'style':\n            defs = parse_stylesheet(child, transform, defs)\n        elif tag == 'linearGradient':\n            if 'id' in child.attrib:\n                defs[child.attrib['id']] = parse_linear_gradient(child, transform, defs)\n        elif tag == 'radialGradient':\n            if 'id' in child.attrib:\n                defs[child.attrib['id']] = parse_radial_gradient(child, transform, defs)\n        elif is_shape(tag):\n            shapes, shape_groups = parse_shape(\\\n                child, transform, fill_color, shapes, shape_groups, defs)\n        elif tag == 'g':\n            shapes, shape_groups = parse_group(\\\n                child, transform, fill_color, shapes, shape_groups, defs)\n    return canvas_width, canvas_height, shapes, shape_groups", "\ndef svg_to_scene(filename):\n    \"\"\"\n        Load from a SVG file and convert to PyTorch tensors.\n    \"\"\"\n\n    tree = etree.parse(filename)\n    root = tree.getroot()\n    cwd = os.getcwd()\n    if (os.path.dirname(filename) != ''):\n        os.chdir(os.path.dirname(filename))\n    ret = parse_scene(root)\n    os.chdir(cwd)\n    return ret", ""]}
{"filename": "eval/post_refinement.py", "chunked_list": ["import argparse\nimport os\nimport sys\nsys.path.append(os.getcwd())\nimport numpy as np\nimport random\nfrom PIL import Image\nimport imageio\n\nimport yaml, shutil", "\nimport yaml, shutil\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import MultiStepLR\nimport torch.nn.functional as F\n\nimport torchvision", "\nimport torchvision\n\nimport datasets\nimport models, losses\nimport utils\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat, reduce, parse_shape\n\nimport refine_svg", "\nimport refine_svg\nfrom svgpathtools import svg2paths, parse_path, wsvg, Line, QuadraticBezier, CubicBezier, Path\nimport pydiffvg\nimport itertools\nfrom tqdm import tqdm\nfrom svg_simplification import svg_file_simplification\n\nimport argparse\n", "import argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--outdir', required=True, type=str)\nparser.add_argument('--input', required=True, type=str)\nparser.add_argument('--fmin', default=0, type=int)\nparser.add_argument('--fmax', default=200, type=int)\nargs = parser.parse_args()\n\nexp_dir = args.outdir", "\nexp_dir = args.outdir\nexp_name = 'p4'\n\nconfig_str = '''\nloss:\n  name: local-loss\n  args:\n    lam_render: 1\n    lam_reg: 1.e-6", "    lam_render: 1\n    lam_reg: 1.e-6\n'''\nconfig = yaml.load(config_str, Loader=yaml.FullLoader)\nloss_fn = losses.make(config['loss'])\n\ndef get_reference_image_path(path, font, char):\n    return os.path.join(path, font, f'{char:02d}_rec.png')\n\ndef get_init_svg_path(path, font, char):\n    return os.path.join(path, font, f'{char:02d}_init.svg')", "\ndef get_init_svg_path(path, font, char):\n    return os.path.join(path, font, f'{char:02d}_init.svg')\n\ndef get_dst_svg_path(path, font, char):\n    return os.path.join(get_dst_font_dir(path, font), f'{char:02d}_{exp_name}.svg')\n\ndef get_dst_font_dir(path, font):\n    return os.path.join(exp_dir, font)\n\ndef post_refinement(path, font, glyph):\n    dst_dir = get_dst_font_dir(path, font)\n    os.makedirs(dst_dir, exist_ok=True)\n    init_svg_path = get_init_svg_path(path, font, glyph)\n    dst_svg_path = get_dst_svg_path(path, font, glyph)\n    image_path = get_reference_image_path(path, font, glyph)\n\n    target = torchvision.transforms.ToTensor()(Image.open(image_path).convert('L')).to(pydiffvg.get_device())\n    n_times = 4\n\n    if not os.path.exists(init_svg_path):\n        return None\n    \n    if os.path.exists(dst_svg_path):\n        # skip\n        return None\n    \n\n    for i in range(n_times):\n        if i == 0:\n            canvas_width, canvas_height, shapes, shape_groups = \\\n                pydiffvg.svg_to_scene(init_svg_path)\n        else:\n            if i == 1:\n                group, quad_line, merge, split = True, True, False, True\n            elif i == 2:\n                group, quad_line, merge, split = True, True, False, False\n            elif i == 3:\n                group, quad_line, merge, split = True, True, True, False\n            \n            success = svg_file_simplification(dst_svg_path.replace('.svg', f'_{i-1}.svg'), dst_svg_path.replace('.svg', f'_{i-1}_sim.svg'),\\\n                    group=group, quad_line=quad_line, merge=merge, split=split)\n            if not success:\n                return None\n            canvas_width, canvas_height, shapes, shape_groups = \\\n                pydiffvg.svg_to_scene(dst_svg_path.replace('.svg', f'_{i-1}_sim.svg'))\n        iter_steps = 50 \n\n        scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n            canvas_width, canvas_height, shapes, shape_groups)\n\n        render = pydiffvg.RenderFunction.apply\n        \n        # The output image is in linear RGB space. Do Gamma correction before saving the image.\n        points_vars = []\n        for path in shapes:\n            path.points.requires_grad = True\n            points_vars.append(path.points)\n\n        # Optimize\n        points_optim = torch.optim.Adam(points_vars, lr=0.5, betas=(0.9, 0.999))\n\n        # Adam iterations.\n        with tqdm(range(iter_steps), leave=False, desc='Refine') as iters:\n            for _ in iters:\n                points_optim.zero_grad()\n\n                # Forward pass: render the image.\n                scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n                    canvas_width, canvas_height, shapes, shape_groups)\n                try:\n                    img = render(canvas_width, # width\n                                canvas_height, # height\n                                2,   # num_samples_x\n                                2,   # num_samples_y\n                                42,   # seed\n                                None, # bg\n                                *scene_args)\n                except:\n                    return None\n                # Compose img with white background\n                img = img[:, :, 3:4] * img[:, :, :3] + torch.ones(img.shape[0], img.shape[1], 3, device = pydiffvg.get_device()) * (1 - img[:, :, 3:4])\n                img = img[:, :, :3]\n                loss_dict = loss_fn(img, target, shapes)\n            \n                # Backpropagate the gradients.\n                loss_dict['loss'].backward()\n            \n                # Take a gradient descent step.\n                points_optim.step()\n\n                for k, v in loss_dict.items():\n                    loss_dict[k] = f'{v.item():.6f}'\n\n                iters.set_postfix({\n                    **loss_dict,\n                })\n\n        if i < n_times - 1:\n            pydiffvg.save_svg_paths_only(dst_svg_path.replace('.svg', f'_{i}.svg'), canvas_width, canvas_height, shapes, shape_groups)\n        else:\n            pydiffvg.save_svg_paths_only(dst_svg_path, canvas_width, canvas_height, shapes, shape_groups)\n\n    return float(loss_dict['render'])", "\ndef post_refinement(path, font, glyph):\n    dst_dir = get_dst_font_dir(path, font)\n    os.makedirs(dst_dir, exist_ok=True)\n    init_svg_path = get_init_svg_path(path, font, glyph)\n    dst_svg_path = get_dst_svg_path(path, font, glyph)\n    image_path = get_reference_image_path(path, font, glyph)\n\n    target = torchvision.transforms.ToTensor()(Image.open(image_path).convert('L')).to(pydiffvg.get_device())\n    n_times = 4\n\n    if not os.path.exists(init_svg_path):\n        return None\n    \n    if os.path.exists(dst_svg_path):\n        # skip\n        return None\n    \n\n    for i in range(n_times):\n        if i == 0:\n            canvas_width, canvas_height, shapes, shape_groups = \\\n                pydiffvg.svg_to_scene(init_svg_path)\n        else:\n            if i == 1:\n                group, quad_line, merge, split = True, True, False, True\n            elif i == 2:\n                group, quad_line, merge, split = True, True, False, False\n            elif i == 3:\n                group, quad_line, merge, split = True, True, True, False\n            \n            success = svg_file_simplification(dst_svg_path.replace('.svg', f'_{i-1}.svg'), dst_svg_path.replace('.svg', f'_{i-1}_sim.svg'),\\\n                    group=group, quad_line=quad_line, merge=merge, split=split)\n            if not success:\n                return None\n            canvas_width, canvas_height, shapes, shape_groups = \\\n                pydiffvg.svg_to_scene(dst_svg_path.replace('.svg', f'_{i-1}_sim.svg'))\n        iter_steps = 50 \n\n        scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n            canvas_width, canvas_height, shapes, shape_groups)\n\n        render = pydiffvg.RenderFunction.apply\n        \n        # The output image is in linear RGB space. Do Gamma correction before saving the image.\n        points_vars = []\n        for path in shapes:\n            path.points.requires_grad = True\n            points_vars.append(path.points)\n\n        # Optimize\n        points_optim = torch.optim.Adam(points_vars, lr=0.5, betas=(0.9, 0.999))\n\n        # Adam iterations.\n        with tqdm(range(iter_steps), leave=False, desc='Refine') as iters:\n            for _ in iters:\n                points_optim.zero_grad()\n\n                # Forward pass: render the image.\n                scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n                    canvas_width, canvas_height, shapes, shape_groups)\n                try:\n                    img = render(canvas_width, # width\n                                canvas_height, # height\n                                2,   # num_samples_x\n                                2,   # num_samples_y\n                                42,   # seed\n                                None, # bg\n                                *scene_args)\n                except:\n                    return None\n                # Compose img with white background\n                img = img[:, :, 3:4] * img[:, :, :3] + torch.ones(img.shape[0], img.shape[1], 3, device = pydiffvg.get_device()) * (1 - img[:, :, 3:4])\n                img = img[:, :, :3]\n                loss_dict = loss_fn(img, target, shapes)\n            \n                # Backpropagate the gradients.\n                loss_dict['loss'].backward()\n            \n                # Take a gradient descent step.\n                points_optim.step()\n\n                for k, v in loss_dict.items():\n                    loss_dict[k] = f'{v.item():.6f}'\n\n                iters.set_postfix({\n                    **loss_dict,\n                })\n\n        if i < n_times - 1:\n            pydiffvg.save_svg_paths_only(dst_svg_path.replace('.svg', f'_{i}.svg'), canvas_width, canvas_height, shapes, shape_groups)\n        else:\n            pydiffvg.save_svg_paths_only(dst_svg_path, canvas_width, canvas_height, shapes, shape_groups)\n\n    return float(loss_dict['render'])", "    \ndef main():\n    origin_path = args.input\n    font_list = [f'{i:04d}' for i in range(args.fmin, args.fmax)]\n    glyph_list = list(range(52))\n\n    font_list = font_list if font_list else os.listdir(origin_path)\n    os.makedirs(exp_dir, exist_ok=True)\n\n    task = sorted([(origin_path, f, g) for f, g in itertools.product(font_list, glyph_list)])\n\n    losses = []\n    for path, font, glyph in tqdm(task):\n\n        loss_render = post_refinement(path, font, glyph)\n        if loss_render is not None:\n            losses.append(loss_render)\n    \n    print(sum(losses) / len(losses))", "\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "eval/run_metrics.py", "chunked_list": ["import os\nimport sys\nsys.path.append(os.getcwd())\nimport argparse\nimport numpy as np\nimport torch\nfrom skimage.metrics import structural_similarity as ssim\nfrom PIL import Image\nfrom tqdm import tqdm\nimport cairosvg", "from tqdm import tqdm\nimport cairosvg\nimport math\n\nfrom svgpathtools import svg2paths2\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--name', type=str, required=True)\nparser.add_argument('--gt', type=str, default='../data/dvf_png/font_pngs/test')\nparser.add_argument('--pred', type=str, required=True)", "parser.add_argument('--gt', type=str, default='../data/dvf_png/font_pngs/test')\nparser.add_argument('--pred', type=str, required=True)\nparser.add_argument('--fontmin', type=int, default=0)\nparser.add_argument('--fontmax', type=int, default=100)\nparser.add_argument('--ff', type=str, default='{0:02d}_rec.png')\nparser.add_argument('--mode', type=str, default='svgrender', choices=['svgrender', 'svgcount', 'imgrec', 'multisvg'])\nparser.add_argument('--res', type=int, default=128)\nparser.add_argument('--gt_lowercase', action='store_true', default=False)\nparser.add_argument('--pred_lowercase', action='store_true', default=False) # crop larger to cover most lowercase letters and then scale back\nparser.add_argument('--glyph', type=int, default=52)", "parser.add_argument('--pred_lowercase', action='store_true', default=False) # crop larger to cover most lowercase letters and then scale back\nparser.add_argument('--glyph', type=int, default=52)\nargs = parser.parse_args()\n\nclass Averager():\n    def __init__(self):\n        self.n = 0.0\n        self.v = 0.0\n\n    def add(self, v, n=1.0):\n        self.v = (self.v * self.n + v * n) / (self.n + n)\n        self.n += n\n\n    def item(self):\n        return self.v", "\nclass PNGMetric:\n    def __init__(self):\n        self.m_ssim = Averager()\n        self.m_l1 = Averager()\n        self.m_l2 = Averager()\n        self.m_siou = Averager()\n    \n    def update(self, v_ssim, v_l1, v_l2, inter, union):\n        self.m_ssim.add(v_ssim)\n        self.m_l1.add(v_l1)\n        self.m_l2.add(v_l2)\n        self.m_siou.add(inter / union, union)\n    \n    def info_dict(self):\n        return {\n            'ssim': self.m_ssim.item(),\n            'l1': self.m_l1.item(),\n            'l2': self.m_l2.item(),\n            'siou': self.m_siou.item(),\n            'siou_n': self.m_siou.n,\n        }", "    \n\nclass SVGMetric:\n    def __init__(self):\n        pass\n    \n    def update(self, v_ssim, v_l1, v_l2, v_siou):\n        pass\n    \n    def info_dict(self):\n        pass", "\n\ndef calc_ssim(pred, gt):\n    '''\n    pred: 1 x dim x dim\n    gt: 1 x dim x dim\n    '''\n    return ssim(pred, gt, data_range=1, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, multichannel=False)\n\ndef calc_l1(pred, gt):\n    return np.mean(np.abs(pred - gt))", "\ndef calc_l1(pred, gt):\n    return np.mean(np.abs(pred - gt))\n\ndef calc_l2(pred, gt):\n    return np.mean(np.power(pred - gt, 2))\n\ndef calc_siou(pred, gt):\n    pred = 1. - pred\n    gt = 1. - gt\n    return np.sum(np.abs(pred * gt)), np.sum(np.clip(pred + gt, 0, 1))", "\ndef surface_to_npim(surface):\n    \"\"\" Transforms a Cairo surface into a numpy array. \"\"\"\n    im = +np.frombuffer(surface.get_data(), np.uint8)\n    H,W = surface.get_height(), surface.get_width()\n    im.shape = (H,W,4) # for RGBA\n    return im\n\ndef svg_to_npim(svg_bytestring, w, h):\n    \"\"\" Renders a svg bytestring as a RGB image in a numpy array \"\"\"\n    tree = cairosvg.parser.Tree(bytestring=svg_bytestring)\n    surf = cairosvg.surface.PNGSurface(tree,None,50,output_width=w, output_height=h).cairo\n    return surface_to_npim(surf)", "def svg_to_npim(svg_bytestring, w, h):\n    \"\"\" Renders a svg bytestring as a RGB image in a numpy array \"\"\"\n    tree = cairosvg.parser.Tree(bytestring=svg_bytestring)\n    surf = cairosvg.surface.PNGSurface(tree,None,50,output_width=w, output_height=h).cairo\n    return surface_to_npim(surf)\n\ndef transform_path(path, lowercase=False, resolution=128, origin=256):\n    if lowercase:\n        return path.scaled(1.25*resolution/origin).translated(-resolution/8.)\n    else:\n        return path.scaled(resolution/origin)", "\ndef render_svg_path(paths, svg_str=None, lowercase=False, resolution=128):\n    if svg_str is not None:\n        im = svg_to_npim(svg_str.encode('utf-8'), resolution, resolution)\n        return im[:, :, :3].mean(axis=-1) / 255.\n    else:\n        HEAD = f'''<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n    <svg baseProfile=\"full\" height=\"100%\" version=\"1.1\" viewBox=\"0,0,{resolution},{resolution}\" width=\"100%\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n    <defs />\n    '''\n        TAIL = '\\n</svg>'\n        path_template = '''<path d=\"{0}\" fill=\"black\" stroke=\"none\" />'''\n        strs = []\n        for path in paths:\n            d = transform_path(path, lowercase=lowercase, resolution=resolution).d()\n            strs.append(path_template.format(d))\n        \n        svg_str = HEAD + '\\n'.join(strs) + TAIL\n        im = svg_to_npim(svg_str.encode('utf-8'), resolution, resolution)\n        return 1. - im[:, :, 3] / 255.", "    \n\ndef read_png_gt(path, lowercase=False, resolution=128):\n    if lowercase:\n        cropped = Image.open(path).convert('L').crop((0, 0, 1024, 1280))\n        origin = Image.new('L', (1280, 1280), 255)\n        origin.paste(cropped, (128, 0, 1152, 1280))\n    else:\n        origin = Image.open(path).convert('L').crop((0, 0, 1024, 1024))\n    \n    img_np = np.asarray(origin.resize((resolution, resolution), resample=Image.BICUBIC)) / 255.\n    return img_np", "\ndef read_png_pred(path, lowercase=False, resolution=128):\n    origin = Image.open(path).convert('L')\n    img_np = np.asarray(origin.resize((resolution, resolution), resample=Image.BICUBIC)) / 255.\n    return img_np\n\ndef get_gt_png(font, glyph, lowercase=False, path_only=False, resolution=128):\n    '''\n    path: str\n    img_np: np.array, resolution x resolution\n    pred: 0 - black - in glyph\n    gt:   1 - white - out glyph\n    '''\n    path = os.path.join(args.gt, font, f'{glyph}_1024.png')\n    if path_only:\n        return path\n    if not os.path.exists(path):\n        print(\"not found gt png, \", path)\n        return path, None\n    \n    img_np = read_png_gt(path, lowercase=lowercase, resolution=resolution)\n    return path, img_np", "\ndef get_gt_svg(font, glyph, path_only=False, string_only=False):\n    '''\n    dvf_png/font_svgs/test/0000/svgs\n    '''\n    path = os.path.join(args.gt, font, 'svgs', f'gt_{glyph:02d}.svg')\n    if path_only:\n        return path\n    if not os.path.exists(path):\n        print(\"not found gt svg, \", path)\n        return path, None\n    \n    paths, attributes, svg_attributes = svg2paths2(path)\n    return path, paths", "\ndef get_pred_png(font, glyph, lowercase=False, path_only=False, resolution=128):\n    '''\n    path: str\n    img_np: np.array, resolution x resolution\n    pred: 0 - black - in glyph\n    gt:   1 - white - out glyph\n    '''\n    path = os.path.join(args.pred, font, args.ff.format(glyph))\n    if path_only:\n        return path\n    if not os.path.exists(path):\n        print(\"not found pred png, \", path)\n        return path, None\n    \n    img_np = read_png_pred(path, lowercase=lowercase, resolution=resolution)\n    return path, img_np", "\ndef get_pred_svg(font, glyph, path_only=False):\n    '''\n    dvf_png/font_svgs/test/0000/svgs\n    '''\n    path = os.path.join(args.pred, font, args.ff.format(glyph))\n    if path_only:\n        return path\n    if not os.path.exists(path):\n        print(\"not found pred svg, \", path)\n        return path, None\n    \n    paths, attributes, svg_attributes = svg2paths2(path)\n    return path, paths", "\nif __name__ == '__main__':\n    font_list = [f'{i:04d}' for i in range(args.fontmin, args.fontmax)]\n    glyph_list = list(range(args.glyph))\n\n    result_dir = os.path.join('eval', 'save', args.name, args.mode)\n    dst_txt = os.path.join(result_dir, os.path.abspath(args.pred).replace('/', '_') + f'_{args.res:04d}.txt')\n    if os.path.exists(dst_txt):\n        print('please consider', 'rm', dst_txt)\n        exit(0)\n    \n    print('Compare', args.gt, args.pred)\n    print('Mode', args.mode)\n    \n    os.makedirs(result_dir, exist_ok=True)\n\n    if args.mode in ['svgrender', 'imgrec', 'multisvg']:\n        all_metric = PNGMetric()\n    else:\n        all_metric = SVGMetric()\n\n    font_metrics = []\n    with tqdm(font_list) as pbar:\n        for font in pbar:\n            if args.mode in ['svgrender', 'imgrec', 'multisvg']:\n                font_metric = PNGMetric()\n            else:\n                font_metric = SVGMetric()\n            for i in glyph_list:\n                if args.mode == 'svgrender':\n                    svg_fpath, svgpaths = get_pred_svg(font, i, path_only=False)\n                    if svgpaths is not None:\n                        pred_np = render_svg_path(svgpaths, lowercase=args.pred_lowercase, resolution=args.res)\n                        png_fpath, gt_np = get_gt_png(font, i, path_only=False, lowercase=args.gt_lowercase, resolution=args.res)\n                        if pred_np is not None and gt_np is not None:\n                            v_ssim = calc_ssim(pred_np, gt_np)\n                            v_l1 = calc_l1(pred_np, gt_np)\n                            v_l2 = calc_l2(pred_np, gt_np)\n                            inter, union = calc_siou(pred_np, gt_np)\n                            font_metric.update(v_ssim, v_l1, v_l2, inter, union)\n                            all_metric.update(v_ssim, v_l1, v_l2, inter, union)\n\n                elif args.mode == 'imgrec':\n                    _, pred_np = get_pred_png(font, i, path_only=False, resolution=args.res)\n                    _, gt_np = get_gt_png(font, i, path_only=False, lowercase=args.gt_lowercase, resolution=args.res)\n                    if pred_np is not None and gt_np is not None:\n                        v_ssim = calc_ssim(pred_np, gt_np)\n                        v_l1 = calc_l1(pred_np, gt_np)\n                        v_l2 = calc_l2(pred_np, gt_np)\n                        inter, union = calc_siou(pred_np, gt_np)\n                        font_metric.update(v_ssim, v_l1, v_l2, inter, union)\n                        all_metric.update(v_ssim, v_l1, v_l2, inter, union)\n\n                elif args.mode == 'multisvg':\n                    svg_fpath = get_pred_svg(font, i, path_only=True)\n                    if svg_fpath is not None:\n                        with open(svg_fpath) as svgf:\n                            svg_str = svgf.read()\n                        pred_np = render_svg_path(None, svg_str=svg_str, lowercase=args.pred_lowercase, resolution=args.res)\n                        # Image.fromarray((pred_np*255).astype(np.uint8)).save('test.png')\n                        png_fpath, gt_np = get_gt_png(font, i, path_only=False, lowercase=args.gt_lowercase, resolution=args.res)\n                        # Image.fromarray((gt_np*255).astype(np.uint8)).save('gt.png')\n                        # import pdb;pdb.set_trace()\n                        if pred_np is not None and gt_np is not None:\n                            v_ssim = calc_ssim(pred_np, gt_np)\n                            v_l1 = calc_l1(pred_np, gt_np)\n                            v_l2 = calc_l2(pred_np, gt_np)\n                            inter, union = calc_siou(pred_np, gt_np)\n                            font_metric.update(v_ssim, v_l1, v_l2, inter, union)\n                            all_metric.update(v_ssim, v_l1, v_l2, inter, union)\n            \n            font_metrics.append((font, font_metric.info_dict()))\n            pbar.set_postfix(all_metric.info_dict())\n    \n    with open(dst_txt, 'w') as f:\n        for font, md in font_metrics:\n            print(font, *md.values(), file=f)\n        print('all', *all_metric.info_dict().values(), file=f)", ""]}
{"filename": "eval/svg_simplification.py", "chunked_list": ["import argparse\nimport os\nimport sys\nsys.path.append(os.getcwd())\nimport numpy as np\nimport random\nfrom PIL import Image\nimport imageio\n\nimport yaml, shutil", "\nimport yaml, shutil\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import MultiStepLR\nimport torch.nn.functional as F\n\nimport torchvision", "\nimport torchvision\n\nfrom svgpathtools import svg2paths2, parse_path, wsvg, Line, QuadraticBezier, CubicBezier, Path\nimport re\nimport glob\n\n\ndef implicitize_bezier_curve(a, b, c, norm=True):\n    '''\n    Bt = a(1-t)^2 + bt(1-t) + ct^2\n    '''\n    x1, x2, x3 = a.real, b.real, c.real\n    y1, y2, y3 = a.imag, b.imag, c.imag\n    A = y1**2 - 4*y1*y2 + 2*y1*y3 + 4*y2**2 - 4*y2*y3 + y3**2\n    B = x1**2 - 4*x1*x2 + 2*x1*x3 + 4*x2**2 - 4*x2*x3 + x3**2\n    C = -2*x1*y1 + 4*x1*y2 - 2*x1*y3 + 4*x2*y1 - 8*x2*y2 + 4*x2*y3 - 2*x3*y1 + 4*x3*y2 - 2*x3*y3\n    D = 2*x1*y1*y3 - 4*x1*y2**2 + 4*x1*y2*y3 - 2*x1*y3**2 + 4*x2*y1*y2 -8*x2*y1*y3 + \\\n        2*x3*y1*y3 - 4*x3*y2**2 + 4*x3*y1*y2 - 2*x3*y1**2  + 4*x2*y2*y3\n\n    E = 2*y1*x1*x3 - 4*y1*x2**2 + 4*y1*x2*x3 - 2*y1*x3**2 + 4*y2*x1*x2 -8*y2*x1*x3 + \\\n        2*y3*x1*x3 - 4*y3*x2**2 + 4*y3*x1*x2 - 2*y3*x1**2  + 4*y2*x2*x3\n    \n    F = (x1*y3)**2 - 4*x1*x2*y2*y3 -2*x1*x3*y1*y3 + 4*x1*x3*y2**2 + 4*x2**2*y1*y3 - 4*x2*x3*y1*y2 + (x3*y1)**2\n\n    eff = np.array([A,B,C,D,E,F])\n    if norm:\n        norm_eff = eff / np.linalg.norm(eff, ord=np.inf)\n        return norm_eff\n    else:\n        return eff", "def implicitize_bezier_curve(a, b, c, norm=True):\n    '''\n    Bt = a(1-t)^2 + bt(1-t) + ct^2\n    '''\n    x1, x2, x3 = a.real, b.real, c.real\n    y1, y2, y3 = a.imag, b.imag, c.imag\n    A = y1**2 - 4*y1*y2 + 2*y1*y3 + 4*y2**2 - 4*y2*y3 + y3**2\n    B = x1**2 - 4*x1*x2 + 2*x1*x3 + 4*x2**2 - 4*x2*x3 + x3**2\n    C = -2*x1*y1 + 4*x1*y2 - 2*x1*y3 + 4*x2*y1 - 8*x2*y2 + 4*x2*y3 - 2*x3*y1 + 4*x3*y2 - 2*x3*y3\n    D = 2*x1*y1*y3 - 4*x1*y2**2 + 4*x1*y2*y3 - 2*x1*y3**2 + 4*x2*y1*y2 -8*x2*y1*y3 + \\\n        2*x3*y1*y3 - 4*x3*y2**2 + 4*x3*y1*y2 - 2*x3*y1**2  + 4*x2*y2*y3\n\n    E = 2*y1*x1*x3 - 4*y1*x2**2 + 4*y1*x2*x3 - 2*y1*x3**2 + 4*y2*x1*x2 -8*y2*x1*x3 + \\\n        2*y3*x1*x3 - 4*y3*x2**2 + 4*y3*x1*x2 - 2*y3*x1**2  + 4*y2*x2*x3\n    \n    F = (x1*y3)**2 - 4*x1*x2*y2*y3 -2*x1*x3*y1*y3 + 4*x1*x3*y2**2 + 4*x2**2*y1*y3 - 4*x2*x3*y1*y2 + (x3*y1)**2\n\n    eff = np.array([A,B,C,D,E,F])\n    if norm:\n        norm_eff = eff / np.linalg.norm(eff, ord=np.inf)\n        return norm_eff\n    else:\n        return eff", "\n\ndef quad_to_line(path, angle_threshold=171, length_threshold=1):\n    new_segs = []\n    cos_t = np.cos(angle_threshold/180*np.pi)\n    for seg in path:\n        if isinstance(seg, QuadraticBezier):\n            # A = seg.start\n            # B = seg.control\n            # C = seg.end\n            ab = seg.control - seg.start\n            cb = seg.control - seg.end\n            if abs(ab) < length_threshold or abs(cb) <  length_threshold:\n                new_segs.append(Line(seg.start, seg.end))\n            else:\n                cos_abc = (ab.real * cb.real + ab.imag * cb.imag) / abs(ab) / abs(cb)\n                if cos_abc < cos_t:\n                    new_segs.append(Line(seg.start, seg.end))\n                else:\n                    new_segs.append(seg)\n        else:\n            new_segs.append(seg)\n    \n    return Path(*new_segs)", "\ndef cos_complex(a, b):\n    return (a.real * b.real + a.imag * b.imag) / abs(a) / abs(b)\n\ndef merge_two_line(a, b):\n    return Line(a.start, b.end)\n\ndef vec_intersect(s1, s2, a, b):\n    '''\n    A = s1 + a*t\n    B = s2 + b*t\n    '''\n    ax, ay = a.real, a.imag\n    bx, by = b.real, b.imag\n    x1, y1 = s1.real, s1.imag\n    x2, y2 = s2.real, s2.imag\n    l = np.array([[ay, -ax], [by, -bx]])\n    r = np.array([ay*x1-ax*y1, by*x2-bx*y2])\n    x = np.linalg.solve(l, r)\n    return complex(x[0] + x[1]*1j)", "\ndef merge_two_quad(a, b):\n    s = a.start\n    e = b.end\n    c = vec_intersect(s, e, a.control - s, b.control - e)\n    return QuadraticBezier(s, c, e)\n\ndef group_near_points(path, size, threshold=3):\n    new_segs = []\n    last_end = None\n    accumulated_length = 0\n\n    for seg in path:\n        if last_end is None:\n            last_end = seg.start\n        sl = seg.length()\n        if sl + accumulated_length < threshold:\n            accumulated_length += sl\n            continue\n    \n        accumulated_length = 0\n        seg.start = last_end\n        new_segs.append(seg)\n        last_end = seg.end\n    \n    if accumulated_length > 0:\n        if len(new_segs) == 0:\n            return None\n        new_segs[0].start = last_end\n    \n    if len(new_segs) >= 2:\n        return new_segs\n    else:\n        return None", "\ndef get_wh(svg_attributes):\n    if 'viewBox' in svg_attributes:\n        vb = svg_attributes['viewBox']\n        view_box_array_comma = vb.split(',')\n        view_box_array = vb.split()\n        if len(view_box_array) < len(view_box_array_comma):\n            view_box_array = view_box_array_comma\n        w = int(view_box_array[2])\n        h = int(view_box_array[3])\n        return w, h\n    \n    return int(svg_attributes['width']), int(svg_attributes['height'])", "\ndef split_segments(path, size, length_threshold=0.1, angle_threshold=90):\n    new_segs = []\n    w, h = size\n    cos_th = np.cos(angle_threshold*np.pi/180)\n    for seg in path:\n        if isinstance(seg, QuadraticBezier):\n            s = seg.start\n            c = seg.control\n            e = seg.end\n            if seg.length() > length_threshold * w:\n                c1 = (s + c) / 2\n                c2 = (c + e) / 2\n                mid = (c1 + c2) / 2\n                new_segs.append(QuadraticBezier(s, c1, mid))\n                new_segs.append(QuadraticBezier(mid, c2, e))\n            else:\n                new_segs.append(seg)\n\n        elif isinstance(seg, Line):\n            if seg.length() > length_threshold * w:\n                s = seg.start\n                e = seg.end\n                mid = (s + e) / 2\n                new_segs.append(Line(s, mid))\n                new_segs.append(Line(mid, e))\n            else:\n                new_segs.append(seg)\n    \n    return new_segs", "\ndef merge_segments(path, size, angle_threshold=175, eff_threshold=0.02):\n    '''\n    coord norm to [-0.5, 0.5]\n    consider only adjust segments\n    '''\n    w, h = size\n\n    last_vec = None\n    last_imp = None\n    lp = len(path)\n\n    cos_t = np.cos(angle_threshold/180*np.pi)\n    new_segs = []\n\n    buf = []\n\n    def merge_buf(buf):\n        if len(buf) == 1:\n            return buf[0]\n        fn = merge_two_quad if isinstance(buf[0], QuadraticBezier) else merge_two_line\n        return fn(buf[0], buf[-1])\n\n    for seg in path:\n        if len(buf) == 0:\n            buf.append(seg)\n        elif not (type(seg) is type(buf[0])):\n            res = merge_buf(buf)\n            new_segs.append(res)\n            buf = [seg]\n        else:\n            std = buf[0]\n            if isinstance(seg, Line):\n                vec = seg.end - seg.start\n                vec_std = std.end - std.start\n                cos_vec = cos_complex(vec, vec_std)\n\n                if abs(cos_vec) > abs(cos_t):\n                    buf.append(seg)\n                else:\n                    res = merge_buf(buf)\n                    new_segs.append(res)\n                    buf = [seg]\n            else:\n                eff_std = implicitize_bezier_curve(std.start / w, std.control / w, std.end / w, norm=True)\n                eff = implicitize_bezier_curve(seg.start / w, seg.control / w, seg.end / w, norm=True)\n                err = min(np.linalg.norm(eff_std - eff), np.linalg.norm(eff_std + eff))\n                if err < eff_threshold:\n                    buf.append(seg)\n                else:\n                    res = merge_buf(buf)\n                    new_segs.append(res)\n                    buf = [seg]\n    \n    if len(buf) > 0:\n        res = merge_buf(buf)\n        new_segs.append(res)\n    \n    return new_segs", "\ndef svg_file_simplification(svg_path, dst_path, group=True, quad_line=True, merge=True, split=True):\n    '''\n    svg_path: [path]\n    path may contain multiple isolated paths\n    '''\n    paths, attributes, svg_attributes = svg2paths2(svg_path)\n    assert(len(paths) == 1)\n    l = len(paths[0])\n    iso_paths = paths[0].continuous_subpaths()\n    w, h = get_wh(svg_attributes=svg_attributes)\n    assert(w == h and w == 256)\n\n    new_path = Path()\n    for path in iso_paths:\n        if group:\n            path = group_near_points(path, size=(w, h))\n        if path is not None:\n            if quad_line:\n                path = quad_to_line(path)\n            if merge:\n                path = merge_segments(path, size=(w, h))\n            if split:\n                path = split_segments(path, size=(w,h))\n            new_path += Path(*path)\n    \n    if len(new_path) > 0:\n        wsvg(paths=[new_path], attributes=attributes, svg_attributes=svg_attributes, forceZ=True, filename=dst_path)\n        return True\n    else:\n        return False"]}
{"filename": "models/decoder.py", "chunked_list": ["from typing import Type, Any, Callable, Union, List, Optional\n\nimport math\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch import Tensor\nimport models\nfrom einops import rearrange, reduce, repeat\n\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device=timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    return emb", "from einops import rearrange, reduce, repeat\n\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device=timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    return emb", "\n\ndef nonlinearity(x):\n    # swish\n    return x*torch.sigmoid(x)\n\n\ndef Normalize(in_channels):\n    return torch.nn.GroupNorm(num_groups=min(in_channels, 32), num_channels=in_channels, eps=1e-6, affine=True)\n", "\n\nclass Upsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, x):\n        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n        if self.with_conv:\n            x = self.conv(x)\n        return x", "\n\nclass Downsample(nn.Module):\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=0)\n\n    def forward(self, x):\n        if self.with_conv:\n            pad = (0,1,0,1)\n            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n            x = self.conv(x)\n        else:\n            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n        return x", "\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n                 dropout, temb_channels=512):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.norm1 = Normalize(in_channels)\n        self.conv1 = torch.nn.Conv2d(in_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if temb_channels > 0:\n            self.temb_proj = torch.nn.Linear(temb_channels,\n                                             out_channels)\n        self.norm2 = Normalize(out_channels)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.conv2 = torch.nn.Conv2d(out_channels,\n                                     out_channels,\n                                     kernel_size=3,\n                                     stride=1,\n                                     padding=1)\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = torch.nn.Conv2d(in_channels,\n                                                     out_channels,\n                                                     kernel_size=3,\n                                                     stride=1,\n                                                     padding=1)\n            else:\n                self.nin_shortcut = torch.nn.Conv2d(in_channels,\n                                                    out_channels,\n                                                    kernel_size=1,\n                                                    stride=1,\n                                                    padding=0)\n\n    def forward(self, x, temb):\n        h = x\n        h = self.norm1(h)\n        h = nonlinearity(h)\n        h = self.conv1(h)\n\n        if temb is not None:\n            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n\n        h = self.norm2(h)\n        h = nonlinearity(h)\n        h = self.dropout(h)\n        h = self.conv2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n\n        return x+h", "\n\nclass AttnBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.in_channels = in_channels\n\n        self.norm = Normalize(in_channels)\n        self.q = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.k = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.v = torch.nn.Conv2d(in_channels,\n                                 in_channels,\n                                 kernel_size=1,\n                                 stride=1,\n                                 padding=0)\n        self.proj_out = torch.nn.Conv2d(in_channels,\n                                        in_channels,\n                                        kernel_size=1,\n                                        stride=1,\n                                        padding=0)\n\n\n    def forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b,c,h,w = q.shape\n        q = q.reshape(b,c,h*w)\n        q = q.permute(0,2,1)   # b,hw,c\n        k = k.reshape(b,c,h*w) # b,c,hw\n        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n        w_ = w_ * (int(c)**(-0.5))\n        w_ = torch.nn.functional.softmax(w_, dim=2)\n\n        # attend to values\n        v = v.reshape(b,c,h*w)\n        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n        h_ = h_.reshape(b,c,h,w)\n\n        h_ = self.proj_out(h_)\n\n        return x+h_", "\n\n@models.register('img-encoder')\nclass ImgEncoder(nn.Module):\n    def __init__(self, *, ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, double_z=True, use_attn=True, key='img', **ignore_kwargs):\n        super().__init__()\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.use_attn = use_attn\n\n        # downsampling\n        self.conv_in = torch.nn.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(AttnBlock(block_in))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        if self.use_attn:\n            self.mid.attn_1 = AttnBlock(block_in)\n        \n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        2*z_channels if double_z else z_channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n        \n        self.key = key\n\n    def forward(self, x):\n        if self.key is not None:\n            x = x[self.key]\n        # assert x.shape[2] == x.shape[3] == self.resolution, \"{}, {}, {}\".format(x.shape[2], x.shape[3], self.resolution)\n\n        # timestep embedding\n        temb = None\n\n        # # downsampling\n        # hs = [self.conv_in(x)]\n        # for i_level in range(self.num_resolutions):\n        #     for i_block in range(self.num_res_blocks):\n        #         h = self.down[i_level].block[i_block](hs[-1], temb)\n        #         if len(self.down[i_level].attn) > 0:\n        #             h = self.down[i_level].attn[i_block](h)\n        #         hs.append(h)\n        #     if i_level != self.num_resolutions-1:\n        #         hs.append(self.down[i_level].downsample(hs[-1]))\n            \n        # # middle\n        # h = hs[-1]\n\n        # downsampling\n        h = self.conv_in(x)\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](h, temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n            if i_level != self.num_resolutions-1:\n                h = self.down[i_level].downsample(h)\n            \n        # middle\n        h = self.mid.block_1(h, temb)\n        if self.use_attn:\n            h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n\n        # output: b x channel x 1 x 1 \n        h = h.squeeze()\n        return h", "\n\n\n@models.register('img-decoder')\nclass ImgDecoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, # in_channels,\n                 resolution, z_channels, give_pre_end=False, **ignorekwargs):\n        super().__init__()\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        # self.in_channels = in_channels\n        self.give_pre_end = give_pre_end\n\n        # compute in_ch_mult, block_in and curr_res at lowest res\n        in_ch_mult = (1,)+tuple(ch_mult)\n        block_in = ch*ch_mult[self.num_resolutions-1]\n        curr_res = resolution // 2**(self.num_resolutions-1)\n        self.z_shape = (1,z_channels,curr_res,curr_res)\n        print(\"Working with z of shape {} = {} dimensions.\".format(\n            self.z_shape, np.prod(self.z_shape)))\n\n        # z to block_in\n        self.conv_in = torch.nn.Conv2d(z_channels,\n                                       block_in,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        # middle\n        self.mid = nn.Module()\n        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n        self.mid.attn_1 = AttnBlock(block_in)\n        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n                                       out_channels=block_in,\n                                       temb_channels=self.temb_ch,\n                                       dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(AttnBlock(block_in))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, z):\n        #assert z.shape[1:] == self.z_shape[1:]\n        self.last_z_shape = z.shape\n\n        # timestep embedding\n        temb = None\n\n        # z to block_in\n        h = self.conv_in(z)\n\n        # middle\n        h = self.mid.block_1(h, temb)\n        h = self.mid.attn_1(h)\n        h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](h, temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        if self.give_pre_end:\n            return h\n\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h", "\n@models.register('light-img-encoder')\nclass LightImgEncoder(nn.Module):\n    def __init__(self, *, ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n                 resolution, z_channels, double_z=True, use_attn=True, key='img', **ignore_kwargs):\n        super().__init__()\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.use_attn = use_attn\n\n        # downsampling\n        self.conv_in = torch.nn.Conv2d(in_channels,\n                                       self.ch,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1,)+tuple(ch_mult)\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch*in_ch_mult[i_level]\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(AttnBlock(block_in))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions-1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        # self.mid = nn.Module()\n        # self.mid.block_1 = ResnetBlock(in_channels=block_in,\n        #                                out_channels=block_in,\n        #                                temb_channels=self.temb_ch,\n        #                                dropout=dropout)\n        # if self.use_attn:\n        #     self.mid.attn_1 = AttnBlock(block_in)\n        \n        # self.mid.block_2 = ResnetBlock(in_channels=block_in,\n        #                                out_channels=block_in,\n        #                                temb_channels=self.temb_ch,\n        #                                dropout=dropout)\n\n        # end\n        # self.norm_out = Normalize(block_in)\n        # self.conv_out = torch.nn.Conv2d(block_in,\n        #                                 2*z_channels if double_z else z_channels,\n        #                                 kernel_size=3,\n        #                                 stride=1,\n        #                                 padding=1)\n        \n        self.key = key\n\n    def forward(self, x):\n        if self.key is not None:\n            x = x[self.key]\n        # assert x.shape[2] == x.shape[3] == self.resolution, \"{}, {}, {}\".format(x.shape[2], x.shape[3], self.resolution)\n\n        # timestep embedding\n        temb = None\n\n        # # downsampling\n        # hs = [self.conv_in(x)]\n        # for i_level in range(self.num_resolutions):\n        #     for i_block in range(self.num_res_blocks):\n        #         h = self.down[i_level].block[i_block](hs[-1], temb)\n        #         if len(self.down[i_level].attn) > 0:\n        #             h = self.down[i_level].attn[i_block](h)\n        #         hs.append(h)\n        #     if i_level != self.num_resolutions-1:\n        #         hs.append(self.down[i_level].downsample(hs[-1]))\n            \n        # # middle\n        # h = hs[-1]\n\n        # downsampling\n        h = self.conv_in(x)\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](h, temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n            if i_level != self.num_resolutions-1:\n                h = self.down[i_level].downsample(h)\n            \n        # middle\n        # h = self.mid.block_1(h, temb)\n        # if self.use_attn:\n        #     h = self.mid.attn_1(h)\n        # h = self.mid.block_2(h, temb)\n\n        # end\n        # h = self.norm_out(h)\n        # h = nonlinearity(h)\n        # h = self.conv_out(h)\n\n        # output: b x channel x 1 x 1 \n        h = h.squeeze()\n        return h", "\n\n@models.register('light-img-decoder')\nclass LightImgDecoder(nn.Module):\n    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n                 attn_resolutions, dropout=0.0, resamp_with_conv=True, # in_channels,\n                 resolution, z_channels, give_pre_end=False, **ignorekwargs):\n        super().__init__()\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        # self.in_channels = in_channels\n        self.give_pre_end = give_pre_end\n\n        # compute in_ch_mult, block_in and curr_res at lowest res\n        in_ch_mult = (1,)+tuple(ch_mult)\n        block_in = ch*ch_mult[self.num_resolutions-1]\n        curr_res = resolution // 2**(self.num_resolutions-1)\n        self.z_shape = (1,z_channels,curr_res,curr_res)\n        print(\"Working with z of shape {} = {} dimensions.\".format(\n            self.z_shape, np.prod(self.z_shape)))\n\n        # z to block_in\n        self.conv_in = torch.nn.Conv2d(z_channels,\n                                       block_in,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1)\n\n        # middle\n        # self.mid = nn.Module()\n        # self.mid.block_1 = ResnetBlock(in_channels=block_in,\n        #                                out_channels=block_in,\n        #                                temb_channels=self.temb_ch,\n        #                                dropout=dropout)\n        # self.mid.attn_1 = AttnBlock(block_in)\n        # self.mid.block_2 = ResnetBlock(in_channels=block_in,\n        #                                out_channels=block_in,\n        #                                temb_channels=self.temb_ch,\n        #                                dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch*ch_mult[i_level]\n            for i_block in range(self.num_res_blocks+1):\n                block.append(ResnetBlock(in_channels=block_in,\n                                         out_channels=block_out,\n                                         temb_channels=self.temb_ch,\n                                         dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(AttnBlock(block_in))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up) # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in,\n                                        out_ch,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n\n    def forward(self, z):\n        #assert z.shape[1:] == self.z_shape[1:]\n        self.last_z_shape = z.shape\n\n        # timestep embedding\n        temb = None\n\n        # z to block_in\n        h = self.conv_in(z)\n\n        # middle\n        # h = self.mid.block_1(h, temb)\n        # h = self.mid.attn_1(h)\n        # h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks+1):\n                h = self.up[i_level].block[i_block](h, temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        if self.give_pre_end:\n            return h\n\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h"]}
{"filename": "models/base.py", "chunked_list": ["import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport models\nfrom einops import rearrange, reduce, repeat, parse_shape\nimport pydiffvg\nfrom .gutils import render_bezier_path, path_d_from_control_points, render_sdf\nfrom .mlp import MLP", "from .gutils import render_bezier_path, path_d_from_control_points, render_sdf\nfrom .mlp import MLP\nfrom objprint import op\nimport svgwrite\nfrom PIL import Image\nfrom abc import abstractmethod\n\nclass SVGRender:\n    def __init__(self):\n        self._colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n    \n    @abstractmethod\n    def render_paths_sdf(self, curves, xy, use_occ, **kwargs):\n        pass\n\n    @abstractmethod\n    def render_paths_diffvg(self, curves, sidelength, **kwargs):\n        pass\n\n    @abstractmethod\n    def write_paths_to_svg(self, path_tensor, filename):\n        pass", "\n\nclass DualChannel(SVGRender):\n    def __init__(self):\n        super().__init__()\n\n    def render_paths_sdf(self, curves, xy, use_occ=True, **kwargs):\n        # dis: b p nxy\n        dis, inner = self.dis_cal(curves, xy, return_mask=True)\n        b, p, nxy = dis.shape\n\n        pos_prim = torch.zeros((b, p, nxy), dtype=torch.bool, device=dis.device)\n        pos_prim[:, :p//2, :] = True\n\n        point_in_pos_prim = torch.logical_and(pos_prim, inner)\n        point_out_neg_prim = torch.logical_and(torch.logical_not(pos_prim), torch.logical_not(inner))\n\n        clip_dis = dis.clone()\n        clip_dis[point_in_pos_prim] = 0\n        clip_dis[point_out_neg_prim] = 0\n\n        clip_dis = rearrange(clip_dis, 'b (ch hp) nxy -> b ch hp nxy', ch=2)\n        clip_dis = reduce(clip_dis, 'b ch hp nxy -> b hp nxy', reduction='max')\n        clip_dis = reduce(clip_dis, 'b hp nxy -> b nxy', reduction='min')\n\n        out = {\n            'dis': clip_dis,\n            'curves': curves,\n        }\n\n        if use_occ:\n            occ_dis = dis.clone()\n\n            occ_dis[point_in_pos_prim] *= -1\n            occ_dis[point_out_neg_prim] *= -1\n\n            occ_dis = rearrange(occ_dis, 'b (ch hp) nxy -> b ch hp nxy', ch=2) # channel: pos/neg\n\n            alias_warmup = kwargs['kernel']\n\n            occ = F.sigmoid(occ_dis) * 2 - 1 \n            # kernel size: 4\n            occ = render_sdf(occ, max(alias_warmup, 4/(self.sidelength)))\n            occ = reduce(occ, 'b ch hp nxy -> b hp nxy', reduction='max')\n            occ = reduce(occ, 'b hp nxy -> b nxy', reduction='min')\n\n            out.update({\n                'occ': occ,\n            })\n\n        return out\n    \n    def render_paths_diffvg(self, curves, sidelength=256):\n        out = {\n            'curves': curves,\n        }\n\n        render = pydiffvg.RenderFunction.apply\n\n        b, p, c, pts = curves.shape\n        h, w = sidelength, sidelength\n\n        background = torch.ones([h, w, 4]).to(curves.device)\n        # curves: b p c pts\n        num_control_points = torch.tensor([pts / 2 - 2] * c)\n        all_points = rearrange(curves, 'b p c (n d) -> b p c n d', d=2)[:, :, :, :-1, :]\n        all_points = rearrange(all_points, 'b p c n d -> b p (c n) d')\n\n        assert(h == w)\n        all_points = (all_points + 1) / 2 * h\n\n        imgs_from_diffvg = []\n        for i in range(b):\n            parts = []\n            for j in range(p//2):\n                shapes = []\n                shape_groups = []\n\n                path_pos = pydiffvg.Path(num_control_points=num_control_points, points=all_points[i, j], is_closed=True)\n                shapes.append(path_pos)\n\n                path_group = pydiffvg.ShapeGroup(\n                    shape_ids=torch.tensor([len(shapes) - 1]),\n                    fill_color=torch.tensor([0.0, 0.0, 0.0, 1.0]))\n                shape_groups.append(path_group)\n\n                path_neg = pydiffvg.Path(num_control_points=num_control_points, points=all_points[i, j + p//2], is_closed=True)\n                shapes.append(path_neg)\n\n                path_group = pydiffvg.ShapeGroup(\n                    shape_ids=torch.tensor([len(shapes) - 1]),\n                    fill_color=torch.tensor([1.0, 1.0, 1.0, 1.0]))\n                shape_groups.append(path_group)\n\n                scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n                    w, h, shapes, shape_groups)\n\n                rendered_img = render(w, # width\n                            h, # height\n                            3,   # num_samples_x\n                            3,   # num_samples_y\n                            42,   # seed\n                            background,\n                            *scene_args)\n                parts.append(rendered_img[:, :, 0].transpose(0, 1))\n            \n            parts = rearrange(parts, 'hp h w -> hp h w')\n            img_from_diffvg = reduce(parts, 'hp h w -> h w', 'min')\n            imgs_from_diffvg.append(img_from_diffvg)\n\n        imgs_from_diffvg = rearrange(imgs_from_diffvg, 'b h w -> b () h w')\n        out.update({\n            'rendered': imgs_from_diffvg,\n        })\n\n        return out\n\n    def write_paths_to_svg(self, path_tensor, filename, xy_flip=True):\n        if isinstance(path_tensor, torch.Tensor):\n            path_tensor = path_tensor.detach().cpu().numpy()\n\n        n_paths, n_curves, n_cp = path_tensor.shape\n        n_cp = n_cp // 2\n\n        n_pos_path = n_paths // 2\n\n        sl = 256\n\n        canvas = svgwrite.Drawing(filename=filename, debug=True)\n        canvas.viewbox(0, 0, sl, sl)\n\n        path_tensor = (path_tensor + 1) / 2 * sl\n        canvas_rect = canvas.rect(insert=(0, 0), size=(sl, sl), fill='white')\n\n        for i in range(n_pos_path):\n            path_d = path_d_from_control_points(path_tensor[i], xy_flip=xy_flip)\n            mask_d = path_d_from_control_points(path_tensor[i + n_pos_path], xy_flip=xy_flip)\n\n            mask = canvas.mask(id=f'mask{i}')\n            mask.add(canvas_rect)\n            mask.add(canvas.path(d=mask_d, fill='black'))\n            canvas.defs.add(mask)\n            path = canvas.path(   \n                d=path_d, \n                fill=self._colors[i%len(self._colors)],\n                fill_opacity='0.3',\n                mask=f'url(#mask{i})',\n            )\n            canvas.add(path)\n        canvas.save()\n        \n        return canvas", "\nclass SingleChannel(SVGRender):\n    def __init__(self):\n        super().__init__()\n    \n    def render_paths_sdf(self, curves, xy, use_occ=True, **kwargs):\n        # dis: b p nxy\n        dis, inner = self.dis_cal(curves, xy, return_mask=True)\n        dis[inner] *= -1\n\n        dis = reduce(dis, 'b p nxy -> b nxy', reduction='min')\n\n        out = {\n            'dis': dis,\n            'curves': curves,\n        }\n\n        if use_occ:\n            alias_warmup = kwargs['kernel']\n\n            occ = dis.clone()\n            occ = F.sigmoid(occ) * 2 - 1 \n            # kernel size: 4\n            occ = render_sdf(occ, max(alias_warmup, 4/(self.sidelength)))\n            out.update({\n                'occ': occ,\n            })\n\n            return out\n    \n    def render_paths_diffvg(self, curves, sidelength=256):\n        render = pydiffvg.RenderFunction.apply\n\n        b, p, c, pts = curves.shape\n        h, w = sidelength, sidelength\n\n        background = torch.ones([h, w, 4]).to(curves.device)\n        # curves: b p c pts\n        num_control_points = torch.tensor([pts / 2 - 2] * c)\n        all_points = rearrange(curves, 'b p c (n d) -> b p c n d', d=2)[:, :, :, :-1, :]\n        all_points = rearrange(all_points, 'b p c n d -> b p (c n) d')\n\n        assert(h == w)\n        all_points = (all_points + 1) / 2 * h\n\n        imgs_from_diffvg = []\n        for i in range(b):\n            shapes = []\n            shape_groups = []\n            for j in range(p):\n                points = all_points[i, j]\n                path = pydiffvg.Path(num_control_points=num_control_points, points=points, is_closed=True)\n                shapes.append(path)\n                path_group = pydiffvg.ShapeGroup(\n                    shape_ids=torch.tensor([len(shapes) - 1]),\n                    fill_color=torch.tensor([0.0, 0.0, 0.0, 1.0]))\n                shape_groups.append(path_group)\n\n            scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n                w, h, shapes, shape_groups)\n\n            rendered_img = render(w, # width\n                        h, # height\n                        2,   # num_samples_x\n                        2,   # num_samples_y\n                        42,   # seed\n                        background,\n                        *scene_args)\n            imgs_from_diffvg.append(rendered_img[:, :, 0].transpose(0, 1))\n        imgs_from_diffvg = rearrange(imgs_from_diffvg, 'b h w -> b () h w')\n        return {\n            'rendered': imgs_from_diffvg,\n        }\n\n    def write_paths_to_svg(self, path_tensor, filename):\n        if isinstance(path_tensor, torch.Tensor):\n            path_tensor = path_tensor.detach().cpu().numpy()\n\n        n_paths, n_curves, n_cp = path_tensor.shape\n        n_cp = n_cp // 2\n\n        sl = 256\n\n        canvas = svgwrite.Drawing(filename=filename, debug=True)\n        canvas.viewbox(0, 0, sl, sl)\n\n        path_tensor = (path_tensor + 1) / 2 * sl\n\n        for i in range(n_paths):\n            path_d = path_d_from_control_points(path_tensor[i])\n            path = canvas.path(   \n                d=path_d, \n                fill=self._colors[i%len(self._colors)],\n                fill_opacity='0.3',\n            )\n            canvas.add(path)\n        canvas.save()\n        return canvas", "\n\n"]}
{"filename": "models/mlp.py", "chunked_list": ["import torch.nn as nn\nimport models\n\n@models.register('mlp')\nclass MLP(nn.Module):\n    def __init__(self, layers, bias=True, activate='leaky_relu', slope = 0.01, activate_last=False, **kwargs):\n        super().__init__()\n\n        self.layers = layers\n        assert len(layers) > 1\n        assert activate in ['leaky_relu', 'relu', 'sigmoid', 'tanh', 'gelu']\n        self.in_dim = layers[0]\n        self.out_dim = layers[-1]\n\n        model = []\n        for i in range(len(layers) - 1):\n            model.append(nn.Linear(layers[i], layers[i + 1], bias=bias))\n            if not activate_last and i < len(layers) - 2:\n                if activate == 'leaky_relu':\n                    model.append(nn.LeakyReLU(negative_slope=slope, inplace=True))\n                elif activate == 'relu':\n                    model.append(nn.ReLU(inplace=True))\n                elif activate == 'sigmoid':\n                    model.append(nn.Sigmoid())\n                elif activate == 'tanh':\n                    model.append(nn.Tanh())\n                elif activate == 'gelu':\n                    model.append(nn.GELU())\n                else:\n                    raise NotImplementedError('not implemented activation')\n        \n        self.model = nn.Sequential(*model)\n\n        def init_weights(m):\n            if type(m) == nn.Linear:\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n        \n        self.model.apply(init_weights)\n    \n    def forward(self, x):\n        shape = x.shape[:-1]\n        x = self.model(x.view(-1, x.shape[-1]))\n        return x.view(*shape, -1)", "\n    "]}
{"filename": "models/transformer.py", "chunked_list": ["from re import I\nimport torch\nfrom torch import nn\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\nimport models\n\n# helpers", "\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn, norm=\"layer\"):\n        super().__init__()\n        if norm == \"layer\":\n            self.norm = nn.LayerNorm(dim)\n        elif norm == \"batch\":\n            self.norm = nn.BatchNorm1d(dim)\n        elif norm == \"none\":\n            self.norm = nn.Identity()\n        else:\n            raise NotImplementedError(\"unsupported norm\", norm)\n        self.fn = fn\n        self.norm_name = norm\n    def forward(self, x, **kwargs):\n        if self.norm_name == \"batch\":\n            return self.fn(self.norm(x.transpose(1,2)).transpose(1,2), **kwargs)\n        else:\n            return self.fn(self.norm(x), **kwargs)", "\nclass ResConn(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    \n    def forward(self, x, **kwargs):\n        return x + self.fn(x, **kwargs)\n\nclass PostNorm(nn.Module):\n    def __init__(self, dim, fn, norm=\"layer\"):\n        super().__init__()\n        if norm == \"layer\":\n            self.norm = nn.LayerNorm(dim)\n        elif norm == \"batch\":\n            self.norm = nn.BatchNorm1d(dim)\n        elif norm == \"none\":\n            self.norm = nn.Identity()\n        else:\n            raise NotImplementedError(\"unsupported norm\", norm)\n        self.fn = fn\n        self.norm_name = norm\n    \n    def forward(self, x, **kwargs):\n        if self.norm_name == \"batch\":\n            return self.norm(self.fn(x, **kwargs).transpose(1,2)).transpose(1,2)\n        else:\n            return self.norm(self.fn(x, **kwargs))", "\nclass PostNorm(nn.Module):\n    def __init__(self, dim, fn, norm=\"layer\"):\n        super().__init__()\n        if norm == \"layer\":\n            self.norm = nn.LayerNorm(dim)\n        elif norm == \"batch\":\n            self.norm = nn.BatchNorm1d(dim)\n        elif norm == \"none\":\n            self.norm = nn.Identity()\n        else:\n            raise NotImplementedError(\"unsupported norm\", norm)\n        self.fn = fn\n        self.norm_name = norm\n    \n    def forward(self, x, **kwargs):\n        if self.norm_name == \"batch\":\n            return self.norm(self.fn(x, **kwargs).transpose(1,2)).transpose(1,2)\n        else:\n            return self.norm(self.fn(x, **kwargs))", "\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)", "\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x, mask=None):\n        # mask: b x n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\n        if mask is None:\n            # dots: b x h x n x n\n            dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n        else:\n            b, n = mask.shape\n            h = q.shape[1]\n            attn_mask = mask.view(b, 1, 1, n).expand(-1, h, n, -1)\n            new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n            new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n            attn_mask = new_attn_mask\n            dots = attn_mask + torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)", "\n@models.register('tfm')\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0., norm=\"layer\"):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout), norm=norm),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout), norm=norm)\n            ]))\n            \n    def forward(self, x, mask=None):\n        for attn, ff in self.layers:\n            x = attn(x, mask=mask) + x\n            x = ff(x) + x\n        return x", ""]}
{"filename": "models/__init__.py", "chunked_list": ["from .tools import register, make, make_lr_scheduler, freeze, load_submodule\nfrom . import bezier_sdf\nfrom . import base\nfrom . import decoder\nfrom . import gutils\nfrom . import recon_system\nfrom . import transformer\nfrom . import gen_system\n\n", "\n"]}
{"filename": "models/tools.py", "chunked_list": ["import os\nimport copy\nimport torch\n\n\nmodels = {}\n\n\ndef register(name):\n    def decorator(cls):\n        models[name] = cls\n        return cls\n    return decorator", "def register(name):\n    def decorator(cls):\n        models[name] = cls\n        return cls\n    return decorator\n\ndef freeze(model):\n    for param in model.parameters():\n        param.requires_grad = False\n\ndef make(model_spec, args=None, load_sd=False):\n    if args is not None:\n        model_args = copy.deepcopy(model_spec['args'])\n        model_args.update(args)\n    else:\n        model_args = model_spec['args']\n    model = models[model_spec['name']](**model_args)\n    if load_sd:\n        model.load_state_dict(model_spec['sd'], strict=False)\n    \n    return model", "\ndef make(model_spec, args=None, load_sd=False):\n    if args is not None:\n        model_args = copy.deepcopy(model_spec['args'])\n        model_args.update(args)\n    else:\n        model_args = model_spec['args']\n    model = models[model_spec['name']](**model_args)\n    if load_sd:\n        model.load_state_dict(model_spec['sd'], strict=False)\n    \n    return model", "\ndef make_lr_scheduler(optim, scheduler_spec):\n    if scheduler_spec is None:\n        return None\n    auto_create = [\n        'CosineAnnealingLR',\n        'ExponentialLR',\n        'CosineAnnealingWarmRestarts',\n        # 'LinearLR',\n        'MultiStepLR',\n        'ReduceLROnPlateau',\n        'StepLR',\n    ]\n\n    name = scheduler_spec['name']\n    if name in auto_create:\n        LRCLASS = getattr(torch.optim.lr_scheduler, name)\n        lr_scheduler = LRCLASS(optim, **scheduler_spec['args'])\n        return lr_scheduler\n    elif name == 'LambdaLR':\n        args = copy.deepcopy(scheduler_spec['args'])\n        lr_lambda = make_lambda(args['lr_lambda'])\n        args['lr_lambda'] = lr_lambda\n        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, **args)\n        return lr_scheduler\n    else:\n        raise NotImplementedError", "\ndef warmup_lr(start, end=1.0, total_iters=5, exp_gamma=1):\n    def _warmup(epoch):\n        if epoch + 1 >= total_iters:\n            return end * exp_gamma ** (epoch + 1 - total_iters)\n        else:\n            return epoch / (total_iters - 1) * (end - start) + start \n\n    return _warmup\n\ndef make_lambda(lr_lambda):\n    FUNC = {\n        'warmup_lr': warmup_lr,\n    }[lr_lambda['name']]\n    return FUNC(**lr_lambda['args'])", "\ndef make_lambda(lr_lambda):\n    FUNC = {\n        'warmup_lr': warmup_lr,\n    }[lr_lambda['name']]\n    return FUNC(**lr_lambda['args'])\n\n\ndef load_submodule(module_instance, ckpt, key):\n    submodule_state_dict = {k[len(key) + 1:]: v for k, v in ckpt['model']['sd'].items() if k.startswith(key + '.')}\n    module_instance.load_state_dict(submodule_state_dict, strict=True)", "def load_submodule(module_instance, ckpt, key):\n    submodule_state_dict = {k[len(key) + 1:]: v for k, v in ckpt['model']['sd'].items() if k.startswith(key + '.')}\n    module_instance.load_state_dict(submodule_state_dict, strict=True)\n    \n\n\n"]}
{"filename": "models/gutils.py", "chunked_list": ["import numpy as np\nimport cairosvg\nimport torch\nimport math\n\ndef surface_to_npim(surface):\n    \"\"\" Transforms a Cairo surface into a numpy array. \"\"\"\n    im = +np.frombuffer(surface.get_data(), np.uint8)\n    H,W = surface.get_height(), surface.get_width()\n    im.shape = (H,W,4) # for RGBA\n    return im", "\ndef svg_to_npim(svg_bytestring, w, h):\n    \"\"\" Renders a svg bytestring as a RGB image in a numpy array \"\"\"\n    tree = cairosvg.parser.Tree(bytestring=svg_bytestring)\n    surf = cairosvg.surface.PNGSurface(tree,None,50,output_width=w, output_height=h).cairo\n    return surface_to_npim(surf)\n\ndef render_bezier_path(curves, sidelength):\n    '''\n    curves: c (pts*2)\n    '''\n\n    curves = (curves + 1) * sidelength / 2\n    d = path_d_from_control_points(curves, xy_flip=False)\n\n    svg_string = f'''<?xml version=\"1.0\" ?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" baseProfile=\"full\" height=\"200\" version=\"1.1\" viewBox=\"0 0 {sidelength} {sidelength}\" width=\"200\">\n    <defs/>\n    <path d=\"{d}\" fill=\"black\" stroke=\"none\"/>\n</svg>'''\n    im = svg_to_npim(svg_string.encode('utf-8'), sidelength, sidelength)\n    return im[:, :, 3]", "\ndef render_multi_bezier_paths(paths, sidelength):\n    '''\n    paths: a list: [c (pts*2)]\n    '''\n\n    d = []\n    for curve in paths:\n        curve = (curve + 1) * sidelength / 2\n        dstr = path_d_from_control_points(curve, xy_flip=False)\n        d.append(f'''<path d=\"{dstr}\" fill=\"black\" stroke=\"none\"/>''')\n\n    svg_string = f'''<?xml version=\"1.0\" ?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" baseProfile=\"full\" height=\"200\" version=\"1.1\" viewBox=\"0 0 {sidelength} {sidelength}\" width=\"200\">\n    <defs/>''' +  '\\n'.join(d) + '</svg>'\n    im = svg_to_npim(svg_string.encode('utf-8'), sidelength, sidelength)\n    return im[:, :, 3]", "\n\ndef render_bezier_paths(paths, sidelength):\n    '''\n    render in the same path tag\n    paths: a list:  [c (pts*2)]\n    '''\n\n    d = []\n    for curve in paths:\n        curve = (curve + 1) * sidelength / 2\n        d.append(path_d_from_control_points(curve, xy_flip=False))\n    \n    d = ' '.join(d)\n\n    svg_string = f'''<?xml version=\"1.0\" ?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" baseProfile=\"full\" height=\"200\" version=\"1.1\" viewBox=\"0 0 {sidelength} {sidelength}\" width=\"200\">\n    <defs/>\n    <path d=\"{d}\" fill=\"black\" stroke=\"none\"/>\n</svg>'''\n\n    im = svg_to_npim(svg_string.encode('utf-8'), sidelength, sidelength)\n    return im[:, :, 3]", "\n\ndef path_d_from_control_points(cp, xy_flip=True):\n    if isinstance(cp, torch.Tensor):\n        cp = cp.detach().cpu().numpy()\n\n    # cp: n_curves (cps 2)\n    n, n_cp = cp.shape\n    n_cp = n_cp // 2\n\n    assert(n_cp == 3 or n_cp == 4)\n\n    cc = 'C' if n_cp == 4 else 'Q'\n    \n    d = []\n    for i in range(n):\n        if i == 0:\n            d += ['M']\n            if xy_flip:\n                d += list(map(str, [cp[i, 1], cp[i, 0]]))\n            else:\n                d += list(map(str, [cp[i, 0], cp[i, 1]]))\n        d += [cc]\n        if n_cp == 4:\n            if xy_flip:\n                d += list(map(str, [cp[i, 3], cp[i, 2], cp[i, 5], cp[i, 4], cp[i, 7], cp[i, 6]]))\n            else:\n                d += list(map(str, cp[i, 2:8]))\n        \n        else:\n            if xy_flip:\n                d += list(map(str, [cp[i, 3], cp[i, 2], cp[i, 5], cp[i, 4]]))\n            else:\n                d += list(map(str, cp[i, 2:6]))\n\n    d += ['Z']\n    d_str = ' '.join(d)\n    return d_str", "\ndef antialias_kernel(r):\n    r = -r\n    output = (0.5 + 0.25 * (torch.pow(r, 3) - 3 * r))\n    #   output = -0.5*r + 0.5\n    return output\n\ndef render_sdf(sdf, resolution):\n    normalization = resolution  # 0.70710678*2/resolution\n    normalized_sdf = sdf / normalization\n    clamped_sdf = torch.clamp(normalized_sdf, min=-1, max=1)\n    opacity = antialias_kernel(clamped_sdf)  # multiply by color here\n    return opacity", "\n\ndef gradient(y, x, create_graph=True, allow_unused=False):\n    return torch.autograd.grad(y, [x], create_graph=create_graph, grad_outputs=torch.ones_like(y), allow_unused=allow_unused)[0]\n\nclass SolveCubicNumpy(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, inp):\n        ctx.set_materialize_grads(False)\n        assert(inp.shape[1] == 4)\n        inp_np = inp.cpu().numpy()\n        n = inp_np.shape[0]\n        roots = np.zeros([n, 3], dtype=inp_np.dtype)\n\n        def find_root(i):\n            r = np.roots(inp_np[i])\n            # r.real[np.abs(r.imag)<1e-5] \n            r = r.real[np.isreal(r)]\n            roots[i, :] = r\n\n        for i in range(n):\n            find_root(i)\n        \n        roots = torch.from_numpy(roots).type_as(inp)\n        ctx.save_for_backward(inp, roots)\n        return roots\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if grad_output is None:\n            return None\n        \n        inp, roots = ctx.saved_tensors\n\n        a = inp[:, 0:1]\n        b = inp[:, 1:2]\n        c = inp[:, 2:3]\n\n        grad_inp = torch.zeros_like(inp)\n\n        for i in range(3):\n            t = roots[:, i:(i+1)]\n            dp_dt = t * (3 * a * t + 2 * b) + c\n            dp_dd = torch.ones_like(dp_dt)\n            dp_dc = t\n            dp_db = dp_dc * t\n            dp_da = dp_db * t\n            dp_dC = torch.cat([dp_da, dp_db, dp_dc, dp_dd], dim=-1)\n            dt_dC = - torch.sign(dp_dt) / torch.clamp(torch.abs(dp_dt), min=1e-5) * dp_dC \n\n            grad_inp += dt_dC * grad_output[:, i:(i + 1)]\n\n        return grad_inp", "\ndef rdot(a, b): # row-wise dot\n    return torch.sum(a * b, dim=-1).unsqueeze(-1)\n\ndef vec_norm(a):\n    # in pytorch 1.6.0\n    return a.norm(dim=-1, keepdim=True)\n\n\ndef sd_bezier(A, B, C, p):\n    # def rdot(a, b): # row-wise dot\n    #     return torch.sum(a * b, dim=-1).unsqueeze(-1)\n    \n    # s = abs(torch.sign(B * 2.0 - A - C))\n    # B = (B + 1e-4) * (1 - s) + s * B\n    \n    # a = B - A\n    # b = A - B * 2.0 + C\n    # c = a * 2.0\n    # d = A - p\n    # k = torch.cat([3.*rdot(a,b), 2.*rdot(a,a)+rdot(d,b),rdot(d,a)], dim=-1) / torch.clamp(rdot(b,b), min=1e-4)     \n    # t = torch.clamp(solve_cubic(k), 0.0, 1.0)\n    # t = t.unsqueeze(1)\n\n    # # n x 2 x 3\n    # vec = A.unsqueeze(-1) + (c.unsqueeze(-1) + b.unsqueeze(-1) * t) * t - p.unsqueeze(-1)\n    # dis = torch.min(torch.linalg.norm(vec, dim=1), dim=-1, keepdim=True)[0]\n\n    # return dis\n    return _sd_bezier(A, B, C, p)", "\ndef sd_bezier(A, B, C, p):\n    # def rdot(a, b): # row-wise dot\n    #     return torch.sum(a * b, dim=-1).unsqueeze(-1)\n    \n    # s = abs(torch.sign(B * 2.0 - A - C))\n    # B = (B + 1e-4) * (1 - s) + s * B\n    \n    # a = B - A\n    # b = A - B * 2.0 + C\n    # c = a * 2.0\n    # d = A - p\n    # k = torch.cat([3.*rdot(a,b), 2.*rdot(a,a)+rdot(d,b),rdot(d,a)], dim=-1) / torch.clamp(rdot(b,b), min=1e-4)     \n    # t = torch.clamp(solve_cubic(k), 0.0, 1.0)\n    # t = t.unsqueeze(1)\n\n    # # n x 2 x 3\n    # vec = A.unsqueeze(-1) + (c.unsqueeze(-1) + b.unsqueeze(-1) * t) * t - p.unsqueeze(-1)\n    # dis = torch.min(torch.linalg.norm(vec, dim=1), dim=-1, keepdim=True)[0]\n\n    # return dis\n    return _sd_bezier(A, B, C, p)", "\ndef _sd_bezier(A, B, C, p):\n    def rdot(a, b): # row-wise dot\n        return torch.sum(a * b, dim=-1).unsqueeze(-1)\n    \n    s = abs(torch.sign(B * 2.0 - A - C))\n    B = (B + 1e-4) * (1 - s) + s * B\n    \n    a = B - A\n    b = A - B * 2.0 + C\n    c = a * 2.0\n    d = A - p\n\n    bdb = torch.clamp(rdot(b, b), min=1e-4)\n    # k = torch.cat([3.*rdot(a,b), 2.*rdot(a,a)+rdot(d,b),rdot(d,a)], dim=-1) / torch.clamp(rdot(b,b), min=1e-4)     \n    t = torch.clamp(solve_cubic(3.*rdot(a, b) / bdb, (2.*rdot(a,a)+rdot(d,b))/bdb, rdot(d,a)/bdb), 0.0, 1.0)\n    t = t.unsqueeze(1)\n\n    # n x 2 x 3\n    vec = A.unsqueeze(-1) + (c.unsqueeze(-1) + b.unsqueeze(-1) * t) * t - p.unsqueeze(-1)\n    if hasattr(torch, 'linalg'):\n        dis = torch.min(torch.linalg.norm(vec, dim=1), dim=-1, keepdim=True)[0]\n    else:\n        dis = torch.min(torch.norm(vec, dim=1), dim=-1, keepdim=True)[0]\n\n    return dis", "\ndef _sd_bezier_np(A, B, C, p):\n\n    solve_cubic_np = SolveCubicNumpy.apply\n    \n    s = abs(torch.sign(B * 2.0 - A - C))\n    B = (B + 1e-4) * (1 - s) + s * B\n    \n    a = B - A\n    b = A - B * 2.0 + C\n    c = a * 2.0\n    d = A - p\n\n    # k = torch.cat([3.*rdot(a,b), 2.*rdot(a,a)+rdot(d,b),rdot(d,a)], dim=-1) / torch.clamp(rdot(b,b), min=1e-4)     \n\n    k = torch.cat([rdot(b,b), 3.*rdot(a,b), 2.*rdot(a,a)+rdot(d,b), rdot(d,a)], dim=-1)\n    t = torch.clamp(solve_cubic_np(k), 0.0, 1.0)\n    t = t.unsqueeze(1)\n\n    # n x 2 x 3\n    vec = A.unsqueeze(-1) + (c.unsqueeze(-1) + b.unsqueeze(-1) * t) * t - p.unsqueeze(-1)\n    dis = torch.min(torch.linalg.norm(vec, dim=1), dim=-1, keepdim=True)[0]\n\n    return dis", "\n# copy from https://members.loria.fr/SHornus/quadratic-arc-length.html\ndef bezier_length(a, b, c):\n    '''\n    a, b, c: n x 2\n    '''\n    A = a + c - 2 * b\n    B = b - a\n    C = a\n    F = A + B\n\n    A_norm = vec_norm(A)\n    B_norm = vec_norm(B)\n    F_norm = vec_norm(F)\n    AB_dot = rdot(A, B)\n    AF_dot = rdot(A, F)\n\n    A_norm_clamp = torch.clamp(A_norm, min=1e-8)\n\n    l = (F_norm * AF_dot - B_norm * AB_dot) / A_norm_clamp.pow(2) + \\\n        (A_norm.pow(2) * B_norm.pow(2) - AB_dot.pow(2)) / A_norm_clamp.pow(3) * \\\n        (torch.log(torch.clamp(A_norm * F_norm + AF_dot, min=1e-8)) - torch.log(torch.clamp(A_norm * B_norm + AB_dot, min=1e-8)))\n    \n    return l", "\n# copied from https://www.shadertoy.com/view/ltXSDB by Inigo Quilez\ndef solve_cubic(a, b, c):\n    '''\n    abc: n x 3\n    '''\n    # a = abc[:, 0:1]\n    # b = abc[:, 1:2]\n    # c = abc[:, 2:3]\n    p = b - a*a / 3.\n    p3 = torch.pow(p, 3)\n    q = a * (2.0*a*a - 9.0*b) / 27.0 + c\n    d = q*q + 4.0*p3 / 27.0\n    offset = -a / 3.0\n\n    d_mask = (d >= 0)\n    z = torch.sqrt(torch.clamp(d[d_mask], min=1e-10)).unsqueeze(-1)\n\n    x = (torch.cat([z, -z], dim=-1) - q[d_mask].unsqueeze(-1)) / 2.0\n\n    uv = torch.sign(x) * torch.pow(torch.clamp(torch.abs(x), min=1e-10), 1.0/3)\n\n    root1 = (offset[d_mask].unsqueeze(1) + uv[:, 0:1] + uv[:, 1:2]).repeat(1, 3)\n\n    to_acos = torch.clamp(-torch.sqrt(-27.0 / torch.clamp(p3[~d_mask], max=-1e-8)) * q[~d_mask] / 2.0, -1 + 1e-4, 1 - 1e-4)\n    v = torch.acos(to_acos) / 3.0\n    m = torch.cos(v).unsqueeze(-1)\n    n = torch.sin(v).unsqueeze(-1) * math.sqrt(3)\n    root2 = torch.cat([m + m, -n - m, n - m], dim=-1) \\\n            * torch.sqrt(torch.clamp(-p[~d_mask].unsqueeze(-1) / 3.0, min=1e-10)) \\\n             + offset[~d_mask].unsqueeze(-1)\n\n    root = torch.zeros((a.shape[0], 3), device=a.device)\n    root[d_mask.repeat(1,3)] = root1.flatten()\n    root[~d_mask.repeat(1,3)] = root2.flatten()\n\n    return root", "\ndef solve_cubic_order2_zero(a_, b_, c_):\n    def cubic_root(p):\n        return p.sign() * p.abs().pow(1.0/3)\n\n    # ax^3 + bx + c = 0\n    # a is almost zero\n    a_zero_mask = a_.abs() < 1e-8\n    # a ~= 0\n    root = torch.zeros((a_.shape[0], 3), device=a_.device)\n    root[a_zero_mask.repeat(1,3)] = (- c_ / b_)[a_zero_mask].repeat(1, 3)\n\n    a = a_\n    c = b_\n    d = c_\n    A = -3*a*c\n    B = -9*a*d\n    C = c**2\n\n    delta = B**2 - 4*A*C\n    # delta > 0\n    d_mask = (delta > 0) & ~a_zero_mask\n    d_sqrt = torch.sqrt(delta[d_mask])\n    Y1 = 3*a[d_mask]*(-B[d_mask] - d_sqrt) / 2\n    Y2 = 3*a[d_mask]*(-B[d_mask] + d_sqrt) / 2\n    X1 = (-cubic_root(Y1) - cubic_root(Y2)) / (3*a[d_mask])\n    root[d_mask.repeat(1,3)] = X1.repeat(1, 3)\n\n    # delta <= 0\n    d_mask = (delta <= 0) & ~a_zero_mask\n    theta = torch.arccos(-3*a[d_mask]*B[d_mask]/2/torch.pow(A[d_mask], 1.5))\n    m = torch.sqrt(A[d_mask]) * torch.cos(theta / 3) / 3 / a[d_mask]\n    n = torch.sqrt(A[d_mask]) * torch.sin(theta / 3) / 3 / a[d_mask]\n    X1 = -2 * m\n    X2 = m + n\n    X3 = m - n\n    d_mask_s = d_mask.squeeze()\n    root[d_mask_s, 0] = X1\n    root[d_mask_s, 1] = X2\n    root[d_mask_s, 2] = X3\n    return root", "\n\ndef sd_parabola(params, p):\n    '''\n    params: n x 4, a, theta, x0, y0 \n    '''\n    a, theta, x0, y0 = params.split(1, dim=-1)\n\n    ct = torch.cos(theta)\n    st = torch.sin(theta)\n\n    x, y = p.split(1, dim=-1)\n\n    x_ = x * ct + y * st - x0 \n    y_ = -x * st + y * ct - y0 \n    sig = torch.sign(a * x_ ** 2 - y_)\n\n    # distance between (x_, y_) and y = a * x ** 2\n    # dis'(x) = 2a^2x^3 + (1-2an)x - m\n    # dis(x) = sqrt((x - m)^2 + (x^2 - n)^2)\n\n    # t = solve_cubic_order2_zero(2*a**2, 1-2*a*y_, -x_)\n\n    a_zero_mask = (a.abs() < 1e-5).squeeze(dim=-1)\n\n    t = torch.zeros((a.shape[0], 3), device=a.device)\n    t1 = (x_[a_zero_mask] / (1 - 2*a[a_zero_mask]*y_[a_zero_mask]))\n    t[a_zero_mask] = t1.repeat(1, 3)\n    t2 = solve_cubic(torch.zeros_like(a[~a_zero_mask]), (1-2*a[~a_zero_mask]*y_[~a_zero_mask])/2/a[~a_zero_mask]**2, -x_[~a_zero_mask]/2/a[~a_zero_mask]**2)\n    t[~a_zero_mask] = t2\n\n    dis = torch.min((t - x_) ** 2 + (a * t**2 - y_) ** 2, dim=-1)[0]\n    return sig, torch.sqrt(torch.clamp(dis, min=1e-10)).unsqueeze(-1)", "\n\ndef eval_parabola(params, p):\n    '''\n    params: n x 4, a, theta, x0, y0 \n    '''\n    a, theta, x0, y0 = params.split(1, dim=-1)\n\n    ct = torch.cos(theta)\n    st = torch.sin(theta)\n\n    x, y = p.split(1, dim=-1)\n\n    x_ = x * ct + y * st - x0 \n    y_ = -x * st + y * ct - y0 \n    # sig = torch.sign(a * x_ ** 2 - y_)\n\n    return a * x_ ** 2 - y_", ""]}
{"filename": "models/bezier_sdf.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn as nn\nimport models\nfrom .mlp import MLP\nfrom .gutils import sd_bezier, render_bezier_path\nfrom einops import rearrange, reduce, repeat, parse_shape\nimport svgwrite\nfrom .gutils import sd_parabola, eval_parabola\n", "from .gutils import sd_parabola, eval_parabola\n\n@models.register('analytic-df')\nclass AnalyticBezierDF(nn.Module):\n    def __init__(self, n_control_points):\n        super().__init__()\n        assert(n_control_points == 3)\n        self.n_control_points = n_control_points\n    \n    def forward(self, xy, control_points):\n        assert(control_points.size(1) == 2 * self.n_control_points)\n        sd = sd_bezier(control_points[:, 0:2], control_points[:, 2:4], control_points[:, 4:6], xy)\n        return sd", "\n@models.register('batched-curve-to-dis')\nclass DistanceCalculator(nn.Module):\n    def __init__(self, ds, ckpt=None, sidelength=256):\n        super().__init__()\n        if isinstance(ds, dict):\n            self.diff_sdf = models.make(ds)\n        else:\n            self.diff_sdf = ds\n        self.ckpt = ckpt\n        self.sidelength = sidelength\n    \n    def init(self):\n        if self.ckpt is not None:\n            ckpt = torch.load(self.ckpt)\n            self.diff_sdf.load_state_dict(ckpt['model']['sd'], strict=True)\n            models.freeze(self.diff_sdf)\n            print('***** freeze diff_sdf *****')\n    \n    def forward(self, curves, xy, return_mask=False):\n        b, p, c, pts = curves.shape\n        _, nxy, _ = xy.shape\n\n        curves_r = repeat(curves, 'b p c pts -> (b nxy p c) pts', nxy=nxy)\n        xy_r = repeat(xy, 'b nxy dxy -> (b nxy p c) dxy', p=p, c=c)\n\n        dis = self.diff_sdf(xy_r, curves_r)\n        dis = rearrange(dis, '(b nxy p c) () -> b nxy p c', b=b, nxy=nxy, p=p, c=c)\n        dis = reduce(dis, 'b nxy p c -> b p nxy', reduction='min')\n\n        if return_mask:\n\n            path_mask = np.zeros((b, p, self.sidelength, self.sidelength), dtype=np.bool) # inside is true\n            curves_np = curves.detach().cpu().numpy()\n            for i in range(b):\n                for j in range(p):\n                    path_mask[i, j] = render_bezier_path(curves_np[i, j], self.sidelength) > 128\n            \n            path_mask = torch.tensor(path_mask).to(curves.device)\n            \n            xy_grid = (xy + 1) / 2 * self.sidelength\n            xy_grid = xy_grid.long()\n\n            # xy_inside: b p nxy\n            xy_inside = torch.zeros_like(dis, dtype=torch.bool)\n            for i in range(b):\n                for j in range(p):\n                    xy_inside[i, j] = path_mask[i, j][xy_grid[i, :, 1], xy_grid[i, :, 0]]\n            return dis, xy_inside\n\n        else:\n            # dis: b p nxy\n            return dis", "\n"]}
{"filename": "models/recon_system.py", "chunked_list": ["import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport models\nfrom .mlp import MLP\nfrom einops import rearrange, reduce, repeat, parse_shape\nfrom .gutils import render_bezier_path, path_d_from_control_points, render_sdf\nfrom objprint import op", "from .gutils import render_bezier_path, path_d_from_control_points, render_sdf\nfrom objprint import op\nimport svgwrite\nfrom PIL import Image\nfrom .base import DualChannel, SingleChannel\n\n@models.register('z-to-curve')\nclass CurveDecoder(nn.Module):\n    def __init__(self, z_dim, n_points, n_curves, n_prims, hidden_dim, hidden_layers):\n        super().__init__()\n        self.z_dim = z_dim\n        self.n_points = n_points\n        self.n_curves = n_curves\n        self.n_prims = n_prims\n        self.hidden_dim = hidden_dim\n        self.hidden_layers = hidden_layers\n\n        out_dim = (n_points - 1) * 2 * n_curves * n_prims\n        self.layers = [z_dim] + [hidden_dim] * hidden_layers + [out_dim]\n        self.decoder = MLP(self.layers)\n\n    def forward(self, z):\n        # z: bs x z_dim\n        # x: bs x out_dim \n        x = self.decoder(z)\n        x = rearrange(x, 'b (p c pts) -> b p c pts', p=self.n_prims, c=self.n_curves, pts=(self.n_points - 1) * 2)\n        start = x[:, :, :, :2]\n        end = torch.roll(start, -1, dims=-2)\n        x = torch.cat([x, end], dim=-1)\n        x = x * 0.1\n        return x", "\n\n@models.register('occ-dual-channel')\n# each part is treated as pos subtract neg\nclass OccDualChannnel(nn.Module, DualChannel):\n    def __init__(self, encoder, decoder, dis_cal, img_decoder=None, detach_img_branch=False, submodule_config=None, sidelength=256, use_occ=False, occ_warmup=10):\n        nn.Module.__init__(self)\n        DualChannel.__init__(self)\n        self.encoder = models.make(encoder)\n        self.decoder = models.make(decoder)\n        self.img_decoder = models.make(img_decoder) if img_decoder is not None else None\n        self.dis_cal = models.make(dis_cal)\n\n        self.n_prims = self.decoder.n_prims\n        self.use_occ = use_occ\n        self.sidelength = sidelength\n        self._colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n        self.occ_warmup = occ_warmup\n        self.detach_img_branch = detach_img_branch\n        self.submodule_config = submodule_config\n\n    def init(self):\n        self.dis_cal.init()\n        if self.submodule_config is not None:\n            for d in self.submodule_config:\n                ckpt_path = d['ckpt']\n                assert(os.path.exists(ckpt_path))\n                ckpt = torch.load(ckpt_path)\n                for key, name, freeze in zip(d['ckpt_key'], d['module_name'], d['freeze']):\n                    print('load', name, 'from', ckpt_path + '.' + key, 'freeze:', freeze)\n                    submodule = getattr(self, name)\n                    models.load_submodule(submodule, ckpt, key)\n                    if freeze:\n                        models.freeze(submodule)\n\n    def decode_image_from_latent_vector(self, z, **kwargs):\n        if self.img_decoder is None:\n            return {}\n        \n        if self.detach_img_branch:\n            z = rearrange(z.detach(), 'b z -> b z 1 1')\n        else:\n            z = rearrange(z, 'b z -> b z 1 1')\n        img = self.img_decoder(z)\n        return {\n            'rec': img,\n        }\n    \n    def forward(self, batch, **kwargs):\n        z = self.encoder(batch)\n\n        out = {\n            'z': z,\n        }\n\n        xy = batch['xy']\n        curves = self.decoder(z)\n\n        epoch = kwargs['epoch']\n        is_train = kwargs['train']\n        if is_train:\n            kernel = 1 - ((1-4/(self.sidelength))*epoch)/self.occ_warmup\n        else:\n            kernel = 0\n        res = self.render_paths_sdf(curves, xy, use_occ=True, kernel=kernel)\n        out.update(res)\n\n        res = self.decode_image_from_latent_vector(z, **kwargs)\n        out.update(res)\n\n        return out"]}
{"filename": "models/encoder.py", "chunked_list": ["from typing import Type, Any, Callable, Union, List, Optional\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nimport models\n\ndef conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        groups=groups,\n        bias=False,\n        dilation=dilation,\n    )", "\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\nclass BasicBlock(nn.Module):\n    def __init__(\n        self,\n        inplanes: int, planes: int,\n        stride: int = 1,\n        act: str = 'relu',\n        downsample: Optional[nn.Module] = None,\n        norm_layer: Optional[Callable[..., nn.Module]] = None,\n    ) -> None:\n        super().__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n\n        if act == 'relu':\n            self.relu = nn.ReLU(inplace=True)\n        elif act == 'leaky_relu':\n            self.relu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        else:\n            raise NotImplementedError('not implemented activation')\n\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n    \n    def forward(self, x: Tensor) -> Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out", "\n@models.register('img64-to-z')\nclass ImageEncoder64(nn.Module):\n    def __init__(self, img_ef_dim, z_dim, key='img'):\n        super().__init__()\n        self.key = key\n        self.img_ef_dim = img_ef_dim\n        self.z_dim = z_dim\n        self._norm_layer = nn.BatchNorm2d\n        self.act = 'leaky_relu'\n\n        # 64 x 64\n        self.conv1 = nn.Conv2d(1, self.img_ef_dim, 7, stride=2, padding=3, bias=False)\n        self.bn1 = self._norm_layer(self.img_ef_dim)\n        self.relu = nn.LeakyReLU(inplace=True, negative_slope=0.01)\n\n        self.inplanes = self.img_ef_dim\n        # 32 x 32\n        self.layer1 = self._make_layer(self.img_ef_dim, 2, stride=1)\n        self.layer2 = self._make_layer(self.img_ef_dim * 2, 2, stride=2)\n        self.layer3 = self._make_layer(self.img_ef_dim * 4, 2, stride=2)\n        self.layer4 = self._make_layer(self.img_ef_dim * 8, 3, stride=2)\n\n        self.conv2 = nn.Conv2d(self.img_ef_dim * 8, self.img_ef_dim * 16, 4, stride=1, padding=0, bias=True)\n        self.fc = nn.Linear(self.img_ef_dim * 16, self.z_dim)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, planes: int, blocks: int, stride: int = 1) -> nn.Sequential:\n        norm_layer = self._norm_layer\n        block = BasicBlock\n        downsample = None\n        if stride != 1 or self.inplanes != planes:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes, stride),\n                norm_layer(planes),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, self.act, downsample, norm_layer))\n        self.inplanes = planes\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, act=self.act, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, inp):\n        if self.key is None:\n            x = self.conv1(inp)\n        else:\n            x = self.conv1(inp[self.key])\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.conv2(x)\n        x = torch.flatten(x, start_dim=1)\n\n        x = self.relu(x) # maybe remove\n        x = self.fc(x)\n        \n        return x", "    \n\n@models.register('embed')\nclass Embed(nn.Module):\n    def __init__(self, n_embed, z_dim):\n        super().__init__()\n        self.embed = nn.Embedding(n_embed, z_dim)\n        self.init_embed_weight()\n\n    def init_embed_weight(self):\n        weights = torch.ones_like(self.embed.weight, requires_grad=True).to(self.embed.weight.device)*0.5\n        self.embed.weight = torch.nn.Parameter(weights)\n    \n    def forward(self, x):\n        return self.embed(x)", "\n@models.register('index-to-z')\nclass IndexEmbed(nn.Module):\n    def __init__(self, n_embed, z_dim):\n        super().__init__()\n        self.embed = nn.Embedding(n_embed, z_dim)\n    \n    def forward(self, batch):\n        return self.embed(batch['index'])\n", "\n@models.register('family-char-to-z')\nclass FamilyCharEmbed(nn.Module):\n    def __init__(self, n_family, dim_family, n_char, dim_char):\n        super().__init__()\n        self.family_embed = nn.Embedding(n_family, dim_family)\n        self.char_embed = nn.Embedding(n_char, dim_char)\n    \n    def forward(self, batch):\n        embed_fam = self.family_embed(batch['font_idx'])\n        embed_chr = self.char_embed(batch['char_idx'])\n        out = torch.cat([embed_fam, embed_chr], dim=-1)\n        return out", "\n"]}
{"filename": "models/gen_system.py", "chunked_list": ["import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport models\n\nfrom .mlp import MLP\nfrom einops import rearrange, reduce, repeat, parse_shape", "from .mlp import MLP\nfrom einops import rearrange, reduce, repeat, parse_shape\nfrom .base import DualChannel\n\n@models.register('vae-style-multi-ref-cnn')\nclass VAEStyleMultiRefCNN(nn.Module, DualChannel):\n    def __init__(self, img_encoder, tfm, n_char, decoder, z_dim, dis_cal, latent_encoder, img_decoder, sidelength, encode_type, submodule_config=None, train_latent=True, detach_img_branch=False, use_diffvg=False):\n        nn.Module.__init__(self)\n        DualChannel.__init__(self)\n\n        self.img_encoder = models.make(img_encoder)\n        self.encode_type = encode_type\n        self.tfm = models.make(tfm)\n        self.latent_encoder = models.make(latent_encoder) if latent_encoder is not None else None\n        self.mlp_head = MLP([z_dim, z_dim], bias=True, activate='gelu', activate_last=True)\n        self.fc_mu = nn.Linear(z_dim, z_dim)\n        self.fc_var = nn.Linear(z_dim, z_dim)\n        self.cls_token = nn.Embedding(n_char, z_dim)\n\n        self.decoder = models.make(decoder)\n        self.dis_cal = models.make(dis_cal)\n        self.img_decoder = models.make(img_decoder) if img_decoder is not None else None\n\n        self.merge = nn.Linear(z_dim * 2, z_dim)\n\n        self.submodule_config = submodule_config\n\n        self.n_prims = self.decoder.n_prims\n        self.sidelength = sidelength\n        self.train_latent = train_latent\n        self.detach_img_branch = detach_img_branch\n        self.use_diffvg = use_diffvg\n\n    def init(self):\n        self.dis_cal.init()\n        if self.submodule_config is not None:\n            for d in self.submodule_config:\n                ckpt_path = d['ckpt']\n                assert(os.path.exists(ckpt_path))\n                ckpt = torch.load(ckpt_path)\n                for key, name, freeze in zip(d['ckpt_key'], d['module_name'], d['freeze']):\n                    print('load', name, 'from', ckpt_path + '.' + key, 'freeze:', freeze)\n                    submodule = getattr(self, name)\n                    models.load_submodule(submodule, ckpt, key)\n                    if freeze:\n                        models.freeze(submodule)\n\n    def reparameterize(self, mu, log_var):\n        # mu, log_var: b x z_dim\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return eps * std + mu\n    \n    def ref_img_encode(self, batch):\n        if self.encode_type == 'cnn':\n            b, n = batch['refs'].shape[:2]\n\n            imgs = rearrange(batch['refs'], 'b n c h w -> (b n) c h w')\n            z_imgs = self.img_encoder(imgs)\n            z_imgs = rearrange(z_imgs, '(b n) z -> b n z', b=b)\n\n            mask = torch.arange(n).to(imgs.device).unsqueeze(0).expand(b, -1) >= batch['n_ref'].unsqueeze(1)\n            z_imgs = self.tfm(z_imgs, mask)\n            x = z_imgs.mean(dim=1) \n\n            x = self.mlp_head(x)\n            mu = self.fc_mu(x)\n            log_var = self.fc_var(x)\n            return mu, log_var\n        else:\n            raise NotImplementedError\n    \n    def forward(self, batch, **kwargs):\n        mu, log_var = self.ref_img_encode(batch)\n        z_style = self.reparameterize(mu, log_var)\n        emb_char = self.cls_token(batch['tgt_char_idx'])\n\n        z = self.merge(torch.cat([z_style, emb_char], dim=-1))\n\n        # dis: b p nxy\n        out = {\n            'mu': mu,\n            'log_var': log_var,\n            'z': z,\n        }\n\n        is_train = kwargs['train']\n        if self.train_latent and is_train:\n            if self.latent_encoder is not None:\n                z_tgt = self.latent_encoder(batch).detach()\n                out.update({\n                    'z_detach': z_tgt\n                })\n        \n        if not self.train_latent or not is_train:\n            xy = batch['xy']\n            curves = self.decoder(z)\n\n            if self.use_diffvg:\n                res = self.render_paths_diffvg(curves, sidelength=batch['img_origin_res'].size(-1))\n                out.update(res)\n            else:\n                res = self.render_paths_sdf(curves, xy, use_occ=True, kernel=4/self.sidelength)\n                out.update(res)\n\n            res = self.decode_image_from_latent_vector(z, **kwargs)\n            out.update(res)\n\n        return out\n\n\n    def decode_image_from_latent_vector(self, z, **kwargs):\n        if self.img_decoder is None:\n            return {}\n        \n        if self.detach_img_branch:\n            z = rearrange(z.detach(), 'b z -> b z 1 1')\n        else:\n            z = rearrange(z, 'b z -> b z 1 1')\n        img = self.img_decoder(z)\n        return {\n            'rec': img,\n        }", ""]}
{"filename": "datasets/multi_ref_data.py", "chunked_list": ["import os\nimport json\nimport glob\nfrom PIL import Image\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, random_split", "import torch.nn.functional as F\nfrom torch.utils.data import Dataset, random_split\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom einops import rearrange, reduce, repeat\n\nimport datasets\nimport utils\n", "import utils\n\n@datasets.register('dvf-eval')\nclass DvfEval(Dataset):\n    def __init__(self, data_root,\n                       img_res=224,\n                       ref_list=list(range(26)),\n                       ratio=1,\n                       use_lowercase=True,\n                       valid_list=None):\n        self.data_root = data_root\n        if valid_list is not None:\n            if isinstance(valid_list, list):\n                font_list = sorted(valid_list)\n            else:\n                with open(valid_list) as f:\n                    font_list = sorted([line.strip() for line in f.readlines()])\n        else:\n            font_list = sorted(os.listdir(data_root))\n\n        n_fonts = len(font_list)\n        self.font_list = font_list[:(n_fonts // ratio)]\n        self.ref_list = ref_list\n        self.img_res = img_res\n        self.use_lowercase = use_lowercase\n\n    def __len__(self):\n        return len(self.font_list)\n    \n    def make_tensor(self, t, dtype=torch.float32):\n        return torch.tensor(t, dtype=dtype)\n    \n    def get_image(self, font, char):\n        ref_path = os.path.join(self.data_root, font, f'{char}_1024.png')\n\n        if self.use_lowercase:\n            cropped = Image.open(ref_path).convert('L').crop((0, 0, 1024, 1280))\n            ref_origin = Image.new('L', (1280, 1280), 255)\n            ref_origin.paste(cropped, (128, 0, 1152, 1280))\n        else:\n            ref_origin = Image.open(ref_path).convert('L').crop((0, 0, 1024, 1024))\n        ref_np = np.asarray(ref_origin.resize((self.img_res, self.img_res), resample=Image.BILINEAR))\n        ref_img = self.make_tensor(ref_np / 255.).view(1, self.img_res, self.img_res)\n        \n        return ref_img\n    \n    def __getitem__(self, idx):\n        n_ref = len(self.ref_list)\n        ref_list = self.make_tensor(self.ref_list, dtype=torch.long)\n\n        font_id = idx\n        font_name = self.font_list[font_id]\n        font_path = os.path.join(self.data_root, font_name)\n\n        ref_imgs = []\n        for char in ref_list:\n            ref_imgs.append(self.get_image(font_name, char))\n        \n        ref_imgs = rearrange(ref_imgs, 'n 1 h w -> n 1 h w')\n\n        ret = {\n            'refs': ref_imgs,\n            'ref_char_idx': ref_list,\n            'n_ref': n_ref,\n            'font_idx': font_id,\n            'index': idx,\n            'font_path': font_path,\n            'font_name': font_name, \n        }\n\n        return ret", "\n\n\n@datasets.register('multi-ref-dvf-generation')\nclass MultiRefDvfGeneration(Dataset):\n    def __init__(self, data_root, \n                       img_res=224, \n                       coor_res=64, \n                       n_samples=4000, \n                       char_list=list(range(26)),\n                       val=False, \n                       sample_inside=False, \n                       signed=False,\n                       distance=False,\n                       full=False, \n                       occ=False, \n                       ratio=5,\n                       origin_res=False, # origin resolution\n                       include_lower_case=False,\n                       n_refs=[2,5],\n                       use_cache=True,\n                       length=None,\n                       valid_list=None):\n        self.data_root = data_root\n        if valid_list is not None:\n            with open(valid_list) as f:\n                font_list = sorted([line.strip() for line in f.readlines()])\n        else:\n            font_list = sorted(os.listdir(data_root))\n    \n        if val:\n            print(len(font_list), 'in all for validating')\n        else:\n            print(len(font_list), 'in all for training')\n        \n        n_fonts = len(font_list)\n        self.font_list = font_list[:(n_fonts // ratio)]\n        self.char_list = char_list\n\n        self.img_res = img_res\n        self.coor_res = coor_res\n        self.n_samples = n_samples\n        self.val = val\n        self.sample_inside = sample_inside\n        self.full = full\n        self.occ = occ\n        self.signed = signed\n        self.distance = distance\n        self.origin_res = origin_res\n        self.min_ref = n_refs[0]\n        self.max_ref = n_refs[1]\n        self.include_lower_case = include_lower_case\n        self.use_cache = use_cache\n\n        self.edge_and_sample = [None for i in range(len(self.font_list) * len(self.char_list))]\n        self.images_64 = [None for i in range(len(self.font_list) * len(self.char_list))]\n        self.length = length\n\n    def __len__(self):\n        return self.length or len(self.font_list) * len(self.char_list)\n    \n    def make_tensor(self, t, dtype=torch.float32):\n        return torch.tensor(t, dtype=dtype)\n    \n    def get_image(self, font_id, char_id):\n        index = font_id * len(self.char_list) + char_id\n        if self.images_64[index] is None:\n            ref_path = os.path.join(self.data_root, self.font_list[font_id], f'{self.char_list[char_id]}_1024.png')\n\n            if self.include_lower_case:\n                cropped = Image.open(ref_path).convert('L').crop((0, 0, 1024, 1280))\n                ref_origin = Image.new('L', (1280, 1280), 255)\n                ref_origin.paste(cropped, (128, 0, 1152, 1280))\n            else:\n                ref_origin = Image.open(ref_path).convert('L').crop((0, 0, 1024, 1024))\n\n            ref_np = np.asarray(ref_origin.resize((self.img_res, self.img_res), resample=Image.BILINEAR))\n            ref_img = self.make_tensor(ref_np / 255.).view(1, self.img_res, self.img_res)\n            if self.use_cache:\n                self.images_64[index] = ref_img\n            else:\n                return ref_img\n        \n        return self.images_64[index]\n    \n    def __getitem__(self, idx):\n        if self.length is not None:\n            char_id = torch.randint(len(self.char_list), size=()).item()\n            font_id = torch.randint(len(self.font_list), size=()).item()\n        else:\n            char_id = idx % len(self.char_list)\n            font_id = idx // len(self.char_list)\n        img_path = os.path.join(self.data_root, self.font_list[font_id], f'{self.char_list[char_id]}_1024.png')\n        if not self.include_lower_case:\n            n_ref = torch.randint(low=self.min_ref, high=self.max_ref + 1, size=(1,)).item()\n            ref_char_idx = torch.randperm(26)[:self.max_ref]\n\n            ref_imgs = []\n            for i, ref_char_id in enumerate(ref_char_idx):\n                if i < n_ref:\n                    ref_imgs.append(self.get_image(font_id, ref_char_id.item()))\n                else:\n                    ref_imgs.append(0.5 * torch.ones((1, self.img_res, self.img_res), dtype=torch.float32))\n            \n            ref_imgs = rearrange(ref_imgs, 'n 1 h w -> n 1 h w')\n        else:\n            n_ref = torch.randint(low=self.min_ref, high=self.max_ref + 1, size=(1,)).item()\n            ref_char_idx = torch.randperm(26)[:self.max_ref]\n            ref_char_idx = ref_char_idx.repeat(2,1).transpose(0,1)\n            ref_char_idx[:, 1] += 26\n            ref_char_idx = ref_char_idx.flatten()\n\n            ref_imgs = []\n            for i, ref_char_id in enumerate(ref_char_idx):\n                if i < n_ref * 2:\n                    ref_imgs.append(self.get_image(font_id, ref_char_id.item()))\n                else:\n                    ref_imgs.append(0.5 * torch.ones((1, self.img_res, self.img_res), dtype=torch.float32))\n\n            ref_imgs = rearrange(ref_imgs, 'n 1 h w -> n 1 h w')\n            n_ref = n_ref * 2\n\n        ret = {\n            'refs': ref_imgs,\n            'ref_char_idx': ref_char_idx,\n            'n_ref': n_ref,\n            'tgt_char_idx': char_id,\n            'font_idx': font_id,\n            'img_path': img_path,\n            'index': idx,\n        }\n\n        if self.origin_res:\n            ret['img_origin_res'] = self.get_image(font_id, char_id)\n\n        if self.full:\n            if self.include_lower_case:\n                cropped = Image.open(img_path).convert('L').crop((0, 0, 1024, 1280))\n                img_origin = Image.new('L', (1280, 1280), 255)\n                img_origin.paste(cropped, (128, 0, 1152, 1280))\n            else:\n                img_origin = Image.open(img_path).convert('L').crop((0, 0, 1024, 1024))\n\n            img_np = np.asarray(img_origin.resize((self.coor_res, self.coor_res), resample=Image.BICUBIC))\n            ret['full_img'] = self.make_tensor(img_np / 255.).view(1, self.coor_res, self.coor_res)\n\n        if self.val or self.n_samples == 0:\n            return ret\n\n        if self.edge_and_sample[idx] is None:\n            origin = Image.open(img_path).convert('L').crop((0, 0, 1024, 1024))\n            sdf_img = np.asarray(origin.resize((self.coor_res, self.coor_res), resample=Image.BICUBIC))\n\n            h, w = self.coor_res, self.coor_res\n            mask = sdf_img < 128 # in the glyph\n            edge = []\n            locs = np.where(mask)\n\n            def in_range(i, j, h, w):\n                return 0 <= i < h and 0 <= j < w\n\n            for i, j in zip(*locs):\n                at_edge = (in_range(i - 1, j, h, w) and not mask[i - 1, j]) or \\\n                        (in_range(i, j - 1, h, w) and not mask[i, j - 1]) or \\\n                        (in_range(i, j + 1, h, w) and not mask[i, j + 1]) or \\\n                        (in_range(i + 1, j, h, w) and not mask[i + 1, j]) \n                if at_edge:\n                    edge.append([2 * i / h - 1, 2 * j / w - 1])\n\n            edge = self.make_tensor(np.array(edge))\n            out_points = 2 * self.make_tensor(np.array(np.where(sdf_img >= 128)).T) / self.coor_res - 1\n            in_points = 2 * self.make_tensor(np.array(np.where(sdf_img < 128)).T) / self.coor_res - 1\n            self.edge_and_sample[idx] = (edge, out_points, in_points, sdf_img)\n\n        else:\n            edge, out_points, in_points, sdf_img = self.edge_and_sample[idx]\n        \n        ne = edge.shape[0]\n        \n        if self.sample_inside:\n            all_points = torch.cat([out_points, in_points], dim=0)\n            unif = torch.ones(all_points.shape[0])\n            indices = unif.multinomial(self.n_samples, replacement=self.n_samples > all_points.shape[0])\n            samples = all_points[indices]\n            ret['xy'] = samples\n            if self.distance:\n                samples_r = repeat(samples, 'ns d -> ns ne d', ne=ne)\n                edge_r = repeat(edge, 'ne d -> ns ne d', ns=self.n_samples)\n                dis = torch.pow(samples_r - edge_r, 2)\n                dis = reduce(dis, 'ns ne d -> ns ne', 'sum')\n                dis = reduce(dis, 'ns ne -> ns', 'min')\n                dis = torch.sqrt(dis)\n\n                if self.signed:\n                    dis[indices >= out_points.shape[0]] *= -1\n                else:\n                    dis[indices >= out_points.shape[0]] = 0\n\n                ret['dis'] = dis,\n        else:\n            unif = torch.ones(out_points.shape[0])\n            out_indices = unif.multinomial(self.n_samples, replacement=self.n_samples > out_points.shape[0])\n\n            samples = out_points[out_indices] # n_samples x 2\n            ret['xy'] = samples\n\n            if self.distance:\n                samples_r = repeat(samples, 'ns d -> ns ne d', ne=ne)\n                edge_r = repeat(edge, 'ne d -> ns ne d', ns=self.n_samples)\n                dis = torch.pow(samples_r - edge_r, 2)\n                dis = reduce(dis, 'ns ne d -> ns ne', 'sum')\n                dis = reduce(dis, 'ns ne -> ns', 'min')\n                dis = torch.sqrt(dis)\n                ret['dis'] = dis\n\n        if self.occ:\n            samples = ((ret['xy'] + 1) * self.coor_res / 2).long()\n            occ_gt = torch.tensor(sdf_img / 255.).view(self.coor_res, self.coor_res)\n            occ = occ_gt[samples[:, 0], samples[:, 1]]\n            ret.update({\n                'pixel': occ.float(),\n            })\n\n        return ret", ""]}
{"filename": "datasets/__init__.py", "chunked_list": ["from .tools import register, make\nfrom . import recon_data\nfrom . import multi_ref_data\n"]}
{"filename": "datasets/recon_data.py", "chunked_list": ["import os\nimport json\nimport glob\nfrom PIL import Image\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, random_split", "import torch.nn.functional as F\nfrom torch.utils.data import Dataset, random_split\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom einops import rearrange, reduce, repeat\n\nimport datasets\nimport utils\n", "import utils\n\n@datasets.register('deepvecfont-sdf')\nclass DeepvecfontSDF(Dataset):\n    def __init__(self, data_root, \n                       img_res=224, \n                       coor_res=64, \n                       n_samples=4000, \n                       char_list=list(range(26)),\n                       include_lower_case=False,\n                       val=False, \n                       sample_inside=False, \n                       signed_distance=False,\n                       full=False, \n                       occ=False, \n                       sample_dis=True,\n                       weight=False,\n                       repeat=1,\n                       ratio=5,\n                       use_cache=True,\n                       random_sample=False,\n                       dataset_length=None,\n                       valid_list=None):\n        self.data_root = data_root\n        self.repeat = repeat\n\n        if isinstance(valid_list, list):\n            font_list = sorted(valid_list)\n        elif isinstance(valid_list, str):\n            with open(valid_list) as f:\n                font_list = sorted([line.strip() for line in f.readlines()])\n        else:\n            font_list = sorted(os.listdir(data_root))\n    \n        if val:\n            print(len(font_list), 'in all for validating')\n        else:\n            print(len(font_list), 'in all for training')\n        \n        n_fonts = len(font_list)\n        self.font_list = font_list[:(n_fonts // ratio)]\n        self.char_list = char_list\n\n        self.edge_and_sample = [None for i in range(len(self.font_list) * len(self.char_list))]\n        self.img_res = img_res\n        self.coor_res = coor_res\n        self.n_samples = n_samples\n        self.val = val\n        self.sample_inside = sample_inside\n        self.full = full\n        self.occ = occ\n        self.weight = weight\n        self.signed_distance = signed_distance\n        self.sample_dis = sample_dis\n        self.include_lower_case = include_lower_case\n        self.cache = use_cache\n\n        self.random_sample = random_sample\n        self.dataset_length = dataset_length\n\n    def __len__(self):\n        # return len(self.font_list) * len(self.char_list) * self.repeat if not self.random_sample else self.dataset_length\n        return len(self.font_list) * len(self.char_list) * self.repeat if self.dataset_length is None else self.dataset_length\n    \n    def make_tensor(self, t, dtype=torch.float32):\n        return torch.tensor(t, dtype=dtype)\n    \n    def __getitem__(self, idx):\n        if not self.random_sample:\n            idx = idx % (len(self.font_list) * len(self.char_list))\n            char_id = idx % len(self.char_list)\n            font_id = idx // len(self.char_list)\n        else:\n            char_id = torch.randint(len(self.char_list), (1,)).item()\n            font_id = torch.randint(len(self.font_list), (1,)).item()\n\n        img_path = os.path.join(self.data_root, self.font_list[font_id], f'{self.char_list[char_id]}_1024.png')\n\n        if self.include_lower_case:\n            cropped = Image.open(img_path).convert('L').crop((0, 0, 1024, 1280))\n            origin = Image.new('L', (1280, 1280), 255)\n            origin.paste(cropped, (128, 0, 1152, 1280))\n        else:\n            origin = Image.open(img_path).convert('L').crop((0, 0, 1024, 1024))\n        img_np = np.asarray(origin.resize((self.img_res, self.img_res), resample=Image.BICUBIC))\n        img = self.make_tensor(img_np / 255.).view(1, self.img_res, self.img_res)\n\n        ret = {\n            'img_path': img_path,\n            'img': img,\n            'index': idx,\n            'font_idx': font_id,\n            'char_idx': char_id,\n            'font_name': self.font_list[font_id],\n            'char': self.char_list[char_id],\n        }\n\n        if self.edge_and_sample[idx] is None:\n            sdf_img = np.asarray(origin.resize((self.coor_res, self.coor_res), resample=Image.BICUBIC))\n            h, w = self.coor_res, self.coor_res\n            mask = sdf_img < 128 # in the glyph\n            edge = []\n            locs = np.where(mask)\n\n            def in_range(i, j, h, w):\n                return 0 <= i < h and 0 <= j < w\n\n            for i, j in zip(*locs):\n                at_edge = (in_range(i - 1, j, h, w) and not mask[i - 1, j]) or \\\n                        (in_range(i, j - 1, h, w) and not mask[i, j - 1]) or \\\n                        (in_range(i, j + 1, h, w) and not mask[i, j + 1]) or \\\n                        (in_range(i + 1, j, h, w) and not mask[i + 1, j]) \n                if at_edge:\n                    edge.append([2 * i / h - 1, 2 * j / w - 1])\n\n            edge = self.make_tensor(np.array(edge))\n            out_points = 2 * self.make_tensor(np.array(np.where(sdf_img >= 128)).T) / self.coor_res - 1\n            in_points = 2 * self.make_tensor(np.array(np.where(sdf_img < 128)).T) / self.coor_res - 1\n\n            if self.cache:\n                self.edge_and_sample[idx] = (edge, out_points, in_points, sdf_img)\n        else:\n            edge, out_points, in_points, sdf_img = self.edge_and_sample[idx]\n        \n        if self.full:\n            ret['full_img'] = self.make_tensor(sdf_img / 255.).view(1, self.coor_res, self.coor_res)\n\n        if self.val or self.n_samples == 0:\n            return ret\n        \n        ne = edge.shape[0]\n        if self.sample_inside:\n            all_points = torch.cat([out_points, in_points], dim=0)\n            unif = torch.ones(all_points.shape[0])\n            indices = unif.multinomial(self.n_samples, replacement=self.n_samples > all_points.shape[0])\n            samples = all_points[indices]\n        else:\n            unif = torch.ones(out_points.shape[0])\n            indices = unif.multinomial(self.n_samples, replacement=self.n_samples > out_points.shape[0])\n            samples = out_points[indices] # n_samples x 2\n        \n        ret.update({\n            'xy': samples\n        })\n        \n        if self.sample_dis:\n            samples_r = repeat(samples, 'ns d -> ns ne d', ne=ne)\n            edge_r = repeat(edge, 'ne d -> ns ne d', ns=self.n_samples)\n            dis = torch.pow(samples_r - edge_r, 2)\n            dis = reduce(dis, 'ns ne d -> ns ne', 'sum')\n            dis = reduce(dis, 'ns ne -> ns', 'min')\n            dis = torch.sqrt(dis)\n\n            if self.weight:\n                w = torch.exp(-dis)\n                ret.update({\n                    'weight': w\n                })\n            \n            if self.sample_inside:\n                if self.signed_distance:\n                    dis[indices >= out_points.shape[0]] *= -1\n                else:\n                    dis[indices >= out_points.shape[0]] = 0\n\n            ret.update({\n                'dis': dis,\n            })\n\n    \n        if self.occ:\n            samples = ((ret['xy'] + 1) * self.coor_res / 2).long()\n            occ_gt = torch.tensor(sdf_img / 255.).view(self.coor_res, self.coor_res)\n            occ = occ_gt[samples[:, 0], samples[:, 1]]\n            ret.update({\n                'pixel': occ.float(),\n            })\n\n        return ret"]}
{"filename": "datasets/tools.py", "chunked_list": ["import copy\n\n\ndatasets = {}\n\n\ndef register(name):\n    def decorator(cls):\n        datasets[name] = cls\n        return cls\n    return decorator", "\n\ndef make(dataset_spec, args=None):\n    if args is not None:\n        dataset_args = copy.deepcopy(dataset_spec['args'])\n        dataset_args.update(args)\n    else:\n        dataset_args = dataset_spec['args']\n    dataset = datasets[dataset_spec['name']](**dataset_args)\n    return dataset", ""]}
