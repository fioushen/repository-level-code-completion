{"filename": "scripts/stereo_image_proc_node.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nPerformance test for stereo_image_proc::DisparityNode.", "\"\"\"\nPerformance test for stereo_image_proc::DisparityNode.\n\nThe graph consists of the following:\n- Preprocessors:\n    1. PrepLeftResizeNode, PrepRightResizeNode: resizes images to quarter HD\n- Graph under Test:\n    1. DisparityNode: creates disparity images from stereo pair\n\nRequired:", "\nRequired:\n- Packages:\n    - stereo_image_proc\n- Datasets:\n    - assets/datasets/r2b_dataset/r2b_hideaway\n\"\"\"\n\nimport os\n", "import os\n\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\nfrom ros2_benchmark import ImageResolution\nfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\nIMAGE_RESOLUTION = ImageResolution.QUARTER_HD\nROSBAG_PATH = 'datasets/r2b_dataset/r2b_hideaway'", "IMAGE_RESOLUTION = ImageResolution.QUARTER_HD\nROSBAG_PATH = 'datasets/r2b_dataset/r2b_hideaway'\n\ndef launch_setup(container_prefix, container_sigterm_timeout):\n    \"\"\"Generate launch description for benchmarking stereo_image_proc::DisparityNode.\"\"\"\n\n    env = os.environ.copy()\n    env['OSPL_VERBOSITY'] = '8'  # 8 = OS_NONE\n    # bare minimum formatting for console output matching\n    env['RCUTILS_CONSOLE_OUTPUT_FORMAT'] = '{message}'\n\n    disparity_node = ComposableNode(\n        name='DisparityNode',\n        namespace=TestDisparityNode.generate_namespace(),\n        package='stereo_image_proc',\n        plugin='stereo_image_proc::DisparityNode',\n        parameters=[{\n                'queue_size': 500,\n                'approx': False,\n                'use_system_default_qos': False\n        }],\n        remappings=[\n            ('/left/camera_info', '/left/camera_info'),\n            ('/right/camera_info', '/right/camera_info'),\n            ('/left/image_rect', '/left/image_rect'),\n            ('/right/image_rect', '/right/image_rect')],\n    )\n\n    data_loader_node = ComposableNode(\n        name='DataLoaderNode',\n        namespace=TestDisparityNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        remappings=[('hawk_0_left_rgb_image', 'data_loader/left_image'),\n                    ('hawk_0_left_rgb_camera_info', 'data_loader/left_camera_info'),\n                    ('hawk_0_right_rgb_image', 'data_loader/right_image'),\n                    ('hawk_0_right_rgb_camera_info', 'data_loader/right_camera_info')]\n    )\n\n    prep_left_resize_node = ComposableNode(\n        name='PrepLeftResizeNode',\n        namespace=TestDisparityNode.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/left_image'),\n            ('image/camera_info', 'data_loader/left_camera_info'),\n            ('resize/image_raw', 'buffer/left/image_resized'),\n            ('resize/camera_info', 'buffer/left/camera_info_resized'),\n        ]\n    )\n\n    prep_right_resize_node = ComposableNode(\n        name='PrepRightResizeNode',\n        namespace=TestDisparityNode.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/right_image'),\n            ('image/camera_info', 'data_loader/right_camera_info'),\n            ('resize/image_raw', 'buffer/right/image_resized'),\n            ('resize/camera_info', 'buffer/right/camera_info_resized'),\n        ]\n    )\n\n    playback_node = ComposableNode(\n        name='PlaybackNode',\n        namespace=TestDisparityNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::PlaybackNode',\n        parameters=[{\n            'data_formats': [\n                'sensor_msgs/msg/Image',\n                'sensor_msgs/msg/CameraInfo',\n                'sensor_msgs/msg/Image',\n                'sensor_msgs/msg/CameraInfo'],\n        }],\n        remappings=[('buffer/input0', 'buffer/left/image_resized'),\n                    ('input0', 'left/image_rect'),\n                    ('buffer/input1', 'buffer/left/camera_info_resized'),\n                    ('input1', 'left/camera_info'),\n                    ('buffer/input2', 'buffer/right/image_resized'),\n                    ('input2', 'right/image_rect'),\n                    ('buffer/input3', 'buffer/right/camera_info_resized'),\n                    ('input3', 'right/camera_info')],\n    )\n\n    monitor_node = ComposableNode(\n        name='MonitorNode',\n        namespace=TestDisparityNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        parameters=[{\n            'monitor_data_format': 'stereo_msgs/msg/DisparityImage',\n        }],\n        remappings=[\n            ('output', 'disparity')],\n    )\n\n    composable_node_container = ComposableNodeContainer(\n        name='container',\n        namespace=TestDisparityNode.generate_namespace(),\n        package='rclcpp_components',\n        executable='component_container_mt',\n        prefix=container_prefix,\n        sigterm_timeout=container_sigterm_timeout,\n        composable_node_descriptions=[\n            data_loader_node,\n            prep_left_resize_node,\n            prep_right_resize_node,\n            playback_node,\n            monitor_node,\n            disparity_node\n        ],\n        output='screen'\n    )\n\n    return [composable_node_container]", "\ndef generate_test_description():\n    return TestDisparityNode.generate_test_description_with_nsys(launch_setup)\n\n\nclass TestDisparityNode(ROS2BenchmarkTest):\n    \"\"\"Performance test for stereo_image_proc::DisparityNode.\"\"\"\n\n    # Custom configurations\n    config = ROS2BenchmarkConfig(\n        benchmark_name='stereo_image_proc::DisparityNode Benchmark',\n        input_data_path=ROSBAG_PATH,\n        # Upper and lower bounds of peak throughput search window\n        publisher_upper_frequency=100.0,\n        publisher_lower_frequency=10.0,\n        # The number of frames to be buffered\n        playback_message_buffer_size=50,\n        custom_report_info={'data_resolution': IMAGE_RESOLUTION}\n    )\n\n    def test_benchmark(self):\n        self.run_benchmark()", ""]}
{"filename": "scripts/image_transport_h264_encoder_node.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nPerformance test for image_transport H264 encoder node.", "\"\"\"\nPerformance test for image_transport H264 encoder node.\n\nThe graph consists of the following:\n- Preprocessors:\n    1. PrepResizeNode: resizes images to full HD\n- Graph under Test:\n    1. RepublishNode: encodes images to h264\n\nRequired:", "\nRequired:\n- Packages:\n    - ros2_h264_encoder\n    - h264_msgs\n- Datasets:\n    - assets/datasets/r2b_dataset/r2b_mezzanine\n\"\"\"\n\nfrom launch_ros.actions import ComposableNodeContainer, Node", "\nfrom launch_ros.actions import ComposableNodeContainer, Node\nfrom launch_ros.descriptions import ComposableNode\n\nfrom ros2_benchmark import ImageResolution\nfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\nIMAGE_RESOLUTION = ImageResolution.FULL_HD\nROSBAG_PATH = 'datasets/r2b_dataset/r2b_mezzanine'\n\ndef launch_setup(container_prefix, container_sigterm_timeout):\n    \"\"\"Generate launch description for benchmarking image_transport H264 encoder node.\"\"\"\n\n    republish_node = Node(\n        name='RepublishNode',\n        namespace=TestH264EncoderNode.generate_namespace(),\n        package='image_transport',\n        executable='republish',\n        arguments=['raw', 'h264'],\n        remappings=[\n            ('in', 'image_raw'),\n            ('out/h264', 'image_h264'),\n        ],\n    )\n\n    data_loader_node = ComposableNode(\n        name='DataLoaderNode',\n        namespace=TestH264EncoderNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        remappings=[('hawk_0_left_rgb_image', 'data_loader/left_image'),\n                    ('hawk_0_left_rgb_camera_info', 'data_loader/left_camera_info')]\n    )\n\n    prep_resize_node = ComposableNode(\n        name='PrepResizeNode',\n        namespace=TestH264EncoderNode.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/left_image'),\n            ('image/camera_info', 'data_loader/left_camera_info'),\n            ('resize/image_raw', 'buffer/image_raw'),\n            ('resize/camera_info', 'buffer/camera_info'),\n        ]\n    )\n\n    playback_node = ComposableNode(\n        name='PlaybackNode',\n        namespace=TestH264EncoderNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::PlaybackNode',\n        parameters=[{\n            'data_formats': [\n                'sensor_msgs/msg/Image'],\n        }],\n        remappings=[('buffer/input0', 'buffer/image_raw'),\n                    ('input0', 'image_raw')]\n    )\n\n    monitor_node = ComposableNode(\n        name='MonitorNode',\n        namespace=TestH264EncoderNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        parameters=[{\n            'monitor_data_format': 'h264_msgs/msg/Packet',\n        }],\n        remappings=[\n            ('output', 'image_h264')],\n    )\n\n    composable_node_container = ComposableNodeContainer(\n        name='container',\n        namespace=TestH264EncoderNode.generate_namespace(),\n        package='rclcpp_components',\n        executable='component_container_mt',\n        prefix=container_prefix,\n        sigterm_timeout=container_sigterm_timeout,\n        composable_node_descriptions=[\n            data_loader_node,\n            prep_resize_node,\n            playback_node,\n            monitor_node\n        ],\n        output='screen'\n    )\n\n    return [republish_node, composable_node_container]", "ROSBAG_PATH = 'datasets/r2b_dataset/r2b_mezzanine'\n\ndef launch_setup(container_prefix, container_sigterm_timeout):\n    \"\"\"Generate launch description for benchmarking image_transport H264 encoder node.\"\"\"\n\n    republish_node = Node(\n        name='RepublishNode',\n        namespace=TestH264EncoderNode.generate_namespace(),\n        package='image_transport',\n        executable='republish',\n        arguments=['raw', 'h264'],\n        remappings=[\n            ('in', 'image_raw'),\n            ('out/h264', 'image_h264'),\n        ],\n    )\n\n    data_loader_node = ComposableNode(\n        name='DataLoaderNode',\n        namespace=TestH264EncoderNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        remappings=[('hawk_0_left_rgb_image', 'data_loader/left_image'),\n                    ('hawk_0_left_rgb_camera_info', 'data_loader/left_camera_info')]\n    )\n\n    prep_resize_node = ComposableNode(\n        name='PrepResizeNode',\n        namespace=TestH264EncoderNode.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/left_image'),\n            ('image/camera_info', 'data_loader/left_camera_info'),\n            ('resize/image_raw', 'buffer/image_raw'),\n            ('resize/camera_info', 'buffer/camera_info'),\n        ]\n    )\n\n    playback_node = ComposableNode(\n        name='PlaybackNode',\n        namespace=TestH264EncoderNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::PlaybackNode',\n        parameters=[{\n            'data_formats': [\n                'sensor_msgs/msg/Image'],\n        }],\n        remappings=[('buffer/input0', 'buffer/image_raw'),\n                    ('input0', 'image_raw')]\n    )\n\n    monitor_node = ComposableNode(\n        name='MonitorNode',\n        namespace=TestH264EncoderNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        parameters=[{\n            'monitor_data_format': 'h264_msgs/msg/Packet',\n        }],\n        remappings=[\n            ('output', 'image_h264')],\n    )\n\n    composable_node_container = ComposableNodeContainer(\n        name='container',\n        namespace=TestH264EncoderNode.generate_namespace(),\n        package='rclcpp_components',\n        executable='component_container_mt',\n        prefix=container_prefix,\n        sigterm_timeout=container_sigterm_timeout,\n        composable_node_descriptions=[\n            data_loader_node,\n            prep_resize_node,\n            playback_node,\n            monitor_node\n        ],\n        output='screen'\n    )\n\n    return [republish_node, composable_node_container]", "\ndef generate_test_description():\n    return TestH264EncoderNode.generate_test_description_with_nsys(launch_setup)\n\n\nclass TestH264EncoderNode(ROS2BenchmarkTest):\n    \"\"\"Performance test for image_transport H264 encoder node.\"\"\"\n\n    # Custom configurations\n    config = ROS2BenchmarkConfig(\n        benchmark_name='image_transport H264 Encoder Node Benchmark',\n        input_data_path=ROSBAG_PATH,\n        # Upper and lower bounds of peak throughput search window\n        publisher_upper_frequency=200.0,\n        publisher_lower_frequency=10.0,\n        # The number of frames to be buffered\n        playback_message_buffer_size=100\n    )\n\n    def test_benchmark(self):\n        self.run_benchmark()", ""]}
{"filename": "scripts/apriltag_ros_apriltag_node.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nPerformance test for apriltag_ros.", "\"\"\"\nPerformance test for apriltag_ros.\n\nThe graph consists of the following:\n- Preprocessors:\n    1. PrepResizeNode: resizes images to HD\n- Graph under Test:\n    1. AprilTagNode: detects Apriltags\n\nRequired:", "\nRequired:\n- Packages:\n    - apriltag_ros\n- Datasets:\n    - assets/datasets/r2b_dataset/r2b_storage\n\"\"\"\n\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode", "from launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\nfrom ros2_benchmark import ImageResolution\nfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\nIMAGE_RESOLUTION = ImageResolution.HD\nROSBAG_PATH = 'datasets/r2b_dataset/r2b_storage'\n\ndef launch_setup(container_prefix, container_sigterm_timeout):\n    \"\"\"Generate launch description for benchmarking apriltag_ros.\"\"\"\n\n    # Detector parameters to detect all 36h11 tags\n    cfg_36h11 = {\n        'image_transport': 'raw',\n        'family': '36h11',\n        'size': 0.162\n    }\n\n    apriltag_node = ComposableNode(\n        name='AprilTagNode',\n        namespace=TestAprilTagNode.generate_namespace(),\n        package='apriltag_ros',\n        plugin='AprilTagNode',\n        parameters=[cfg_36h11],\n        remappings=[\n            ('image_rect', 'image'),\n            ('detections', 'apriltag_detections')\n        ]\n    )\n\n    data_loader_node = ComposableNode(\n        name='DataLoaderNode',\n        namespace=TestAprilTagNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        remappings=[('hawk_0_left_rgb_image', 'data_loader/image'),\n                    ('hawk_0_left_rgb_camera_info', 'data_loader/camera_info')]\n    )\n\n    prep_resize_node = ComposableNode(\n        name='PrepResizeNode',\n        namespace=TestAprilTagNode.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/image'),\n            ('image/camera_info', 'data_loader/camera_info'),\n            ('resize/image_raw', 'buffer/image'),\n            ('resize/camera_info', 'buffer/camera_info'),\n        ]\n    )\n\n    playback_node = ComposableNode(\n        name='PlaybackNode',\n        namespace=TestAprilTagNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::PlaybackNode',\n        parameters=[{\n            'data_formats': [\n                'sensor_msgs/msg/Image',\n                'sensor_msgs/msg/CameraInfo'\n            ],\n        }],\n        remappings=[\n            ('buffer/input0', 'buffer/image'),\n            ('input0', 'image'),\n            ('buffer/input1', 'buffer/camera_info'),\n            ('input1', 'camera_info')\n        ]\n    )\n\n    monitor_node = ComposableNode(\n        name='MonitorNode',\n        namespace=TestAprilTagNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        parameters=[{\n            'monitor_data_format': 'apriltag_msgs/msg/AprilTagDetectionArray',\n        }],\n        remappings=[\n            ('output', 'apriltag_detections')\n        ]\n    )\n\n    composable_node_container = ComposableNodeContainer(\n        name='container',\n        namespace=TestAprilTagNode.generate_namespace(),\n        package='rclcpp_components',\n        executable='component_container_mt',\n        prefix=container_prefix,\n        sigterm_timeout=container_sigterm_timeout,\n        composable_node_descriptions=[\n            data_loader_node,\n            prep_resize_node,\n            playback_node,\n            monitor_node,\n            apriltag_node\n        ],\n        output='screen'\n    )\n\n    return [composable_node_container]", "\ndef launch_setup(container_prefix, container_sigterm_timeout):\n    \"\"\"Generate launch description for benchmarking apriltag_ros.\"\"\"\n\n    # Detector parameters to detect all 36h11 tags\n    cfg_36h11 = {\n        'image_transport': 'raw',\n        'family': '36h11',\n        'size': 0.162\n    }\n\n    apriltag_node = ComposableNode(\n        name='AprilTagNode',\n        namespace=TestAprilTagNode.generate_namespace(),\n        package='apriltag_ros',\n        plugin='AprilTagNode',\n        parameters=[cfg_36h11],\n        remappings=[\n            ('image_rect', 'image'),\n            ('detections', 'apriltag_detections')\n        ]\n    )\n\n    data_loader_node = ComposableNode(\n        name='DataLoaderNode',\n        namespace=TestAprilTagNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        remappings=[('hawk_0_left_rgb_image', 'data_loader/image'),\n                    ('hawk_0_left_rgb_camera_info', 'data_loader/camera_info')]\n    )\n\n    prep_resize_node = ComposableNode(\n        name='PrepResizeNode',\n        namespace=TestAprilTagNode.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/image'),\n            ('image/camera_info', 'data_loader/camera_info'),\n            ('resize/image_raw', 'buffer/image'),\n            ('resize/camera_info', 'buffer/camera_info'),\n        ]\n    )\n\n    playback_node = ComposableNode(\n        name='PlaybackNode',\n        namespace=TestAprilTagNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::PlaybackNode',\n        parameters=[{\n            'data_formats': [\n                'sensor_msgs/msg/Image',\n                'sensor_msgs/msg/CameraInfo'\n            ],\n        }],\n        remappings=[\n            ('buffer/input0', 'buffer/image'),\n            ('input0', 'image'),\n            ('buffer/input1', 'buffer/camera_info'),\n            ('input1', 'camera_info')\n        ]\n    )\n\n    monitor_node = ComposableNode(\n        name='MonitorNode',\n        namespace=TestAprilTagNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        parameters=[{\n            'monitor_data_format': 'apriltag_msgs/msg/AprilTagDetectionArray',\n        }],\n        remappings=[\n            ('output', 'apriltag_detections')\n        ]\n    )\n\n    composable_node_container = ComposableNodeContainer(\n        name='container',\n        namespace=TestAprilTagNode.generate_namespace(),\n        package='rclcpp_components',\n        executable='component_container_mt',\n        prefix=container_prefix,\n        sigterm_timeout=container_sigterm_timeout,\n        composable_node_descriptions=[\n            data_loader_node,\n            prep_resize_node,\n            playback_node,\n            monitor_node,\n            apriltag_node\n        ],\n        output='screen'\n    )\n\n    return [composable_node_container]", "\ndef generate_test_description():\n    return TestAprilTagNode.generate_test_description_with_nsys(launch_setup)\n\nclass TestAprilTagNode(ROS2BenchmarkTest):\n    \"\"\"Performance test for AprilTagNode.\"\"\"\n\n    # Custom configurations\n    config = ROS2BenchmarkConfig(\n        benchmark_name='apriltag_ros AprilTagNode Benchmark',\n        input_data_path=ROSBAG_PATH,\n        # The slice of the rosbag to use\n        input_data_start_time=3.0,\n        input_data_end_time=3.5,\n        # Upper and lower bounds of peak throughput search window\n        publisher_upper_frequency=600.0,\n        publisher_lower_frequency=10.0,\n        # The number of frames to be buffered\n        playback_message_buffer_size=10,\n        custom_report_info={'data_resolution': IMAGE_RESOLUTION}\n    )\n\n    def test_benchmark(self):\n        self.run_benchmark()", ""]}
{"filename": "scripts/image_transport_h264_decoder_node.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nPerformance test for image_transport H264 decoder node.", "\"\"\"\nPerformance test for image_transport H264 decoder node.\n\nThe graph consists of the following:\n- Preprocessors:\n    1. PrepResizeNode: resizes images to full HD\n    2. PrepRepublishEncoderNode: encodes images to h264\n- Graph under Test:\n    1. RepublishDecoderNode: decodes compressed images\n", "    1. RepublishDecoderNode: decodes compressed images\n\nRequired:\n- Packages:\n    - ros2_h264_encoder\n    - h264_msgs\n    - h264_image_transport\n- Datasets:\n    - assets/datasets/r2b_dataset/r2b_mezzanine\n\"\"\"", "    - assets/datasets/r2b_dataset/r2b_mezzanine\n\"\"\"\n\nfrom launch_ros.actions import ComposableNodeContainer, Node\nfrom launch_ros.descriptions import ComposableNode\n\nfrom ros2_benchmark import ImageResolution\nfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\nIMAGE_RESOLUTION = ImageResolution.FULL_HD", "\nIMAGE_RESOLUTION = ImageResolution.FULL_HD\nROSBAG_PATH = 'datasets/r2b_dataset/r2b_mezzanine'\n\ndef launch_setup(container_prefix, container_sigterm_timeout):\n    \"\"\"Generate launch description for benchmarking image_transport decoder node.\"\"\"\n\n    republish_node = Node(\n        name='RepublishDecoderNode',\n        namespace=TestH264DecoderNode.generate_namespace(),\n        package='image_transport',\n        executable='republish',\n        arguments=['h264',  'raw', '--ros-args', '--log-level', 'error'],\n        remappings=[\n            ('in/h264', 'compressed_image'),\n            ('out', 'image_uncompressed'),\n        ],\n    )\n\n    data_loader_node = ComposableNode(\n        name='DataLoaderNode',\n        namespace=TestH264DecoderNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        remappings=[('hawk_0_left_rgb_image', 'data_loader/left_image'),\n                    ('hawk_0_left_rgb_camera_info', 'data_loader/left_camera_info')]\n    )\n\n    prep_resize_node = ComposableNode(\n        name='PrepResizeNode',\n        namespace=TestH264DecoderNode.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/left_image'),\n            ('image/camera_info', 'data_loader/left_camera_info'),\n            ('resize/image_raw', 'resized/image_raw'),\n            ('resize/camera_info', 'resized/camera_info'),\n        ]\n    )\n\n    prep_encoder_node = Node(\n        name='PrepRepublishEncoderNode',\n        namespace=TestH264DecoderNode.generate_namespace(),\n        package='image_transport',\n        executable='republish',\n        arguments=['raw', 'h264'],\n        remappings=[\n            ('in', 'resized/image_raw'),\n            ('out/h264', 'image_h264'),\n        ],\n    )\n\n    playback_node = ComposableNode(\n        name='PlaybackNode',\n        namespace=TestH264DecoderNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::PlaybackNode',\n        parameters=[{\n            'data_formats': [\n                'h264_msgs/msg/Packet'],\n        }],\n        remappings=[('buffer/input0', 'image_h264'),\n                    ('input0', 'compressed_image')],\n    )\n\n    monitor_node = ComposableNode(\n        name='MonitorNode',\n        namespace=TestH264DecoderNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        parameters=[{\n            'monitor_data_format': 'sensor_msgs/msg/Image',\n        }],\n        remappings=[\n            ('output', 'image_uncompressed')],\n    )\n\n    composable_node_container = ComposableNodeContainer(\n        name='container',\n        namespace=TestH264DecoderNode.generate_namespace(),\n        package='rclcpp_components',\n        executable='component_container_mt',\n        prefix=container_prefix,\n        sigterm_timeout=container_sigterm_timeout,\n        composable_node_descriptions=[\n            data_loader_node,\n            prep_resize_node,\n            playback_node,\n            monitor_node\n        ],\n        output='screen'\n    )\n\n    return [prep_encoder_node, republish_node, composable_node_container]", "\ndef generate_test_description():\n    return TestH264DecoderNode.generate_test_description_with_nsys(launch_setup)\n\n\nclass TestH264DecoderNode(ROS2BenchmarkTest):\n    \"\"\"Performance test for image_transport H264 decoder node.\"\"\"\n\n    # Custom configurations\n    config = ROS2BenchmarkConfig(\n        benchmark_name='image_transport H264 Decoder Node Benchmark',\n        input_data_path=ROSBAG_PATH,\n        # Upper and lower bounds of peak throughput search window\n        publisher_upper_frequency=200.0,\n        publisher_lower_frequency=10.0,\n        # The number of frames to be buffered\n        playback_message_buffer_size=1\n    )\n\n    def test_benchmark(self):\n        self.run_benchmark()", ""]}
{"filename": "scripts/apriltag_ros_apriltag_graph.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nPerformance test for apriltag_ros graph.", "\"\"\"\nPerformance test for apriltag_ros graph.\n\nThe graph consists of the following:\n- Preprocessors:\n    1. PrepResizeNode: resizes images to HD\n- Graph under Test:\n    1. RectifyNode: rectifies images\n    2. AprilTagNode: detects Apriltags\n", "    2. AprilTagNode: detects Apriltags\n\nRequired:\n- Packages:\n    - apriltag_ros\n- Datasets:\n    - assets/datasets/r2b_dataset/r2b_storage\n\"\"\"\n\nfrom launch_ros.actions import ComposableNodeContainer", "\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\nfrom ros2_benchmark import ImageResolution\nfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\nIMAGE_RESOLUTION = ImageResolution.HD\nROSBAG_PATH = 'datasets/r2b_dataset/r2b_storage'\n\ndef launch_setup(container_prefix, container_sigterm_timeout):\n    \"\"\"Generate launch description for benchmarking apriltag_ros graph.\"\"\"\n\n    # Detector parameters to detect all 36h11 tags\n    cfg_36h11 = {\n        'image_transport': 'raw',\n        'family': '36h11',\n        'size': 0.162\n    }\n\n    rectify_node = ComposableNode(\n        name='RectifyNode',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::RectifyNode'\n    )\n\n    apriltag_node = ComposableNode(\n        name='AprilTagNode',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='apriltag_ros',\n        plugin='AprilTagNode',\n        parameters=[cfg_36h11],\n        remappings=[\n            ('detections', 'apriltag_detections')\n        ]\n    )\n\n    data_loader_node = ComposableNode(\n        name='DataLoaderNode',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        remappings=[('hawk_0_left_rgb_image', 'data_loader/image'),\n                    ('hawk_0_left_rgb_camera_info', 'data_loader/camera_info')]\n    )\n\n    prep_resize_node = ComposableNode(\n        name='PrepResizeNode',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/image'),\n            ('image/camera_info', 'data_loader/camera_info'),\n            ('resize/image_raw', 'buffer/image'),\n            ('resize/camera_info', 'buffer/camera_info'),\n        ]\n    )\n\n    playback_node = ComposableNode(\n        name='PlaybackNode',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::PlaybackNode',\n        parameters=[{\n            'data_formats': [\n                'sensor_msgs/msg/Image',\n                'sensor_msgs/msg/CameraInfo'],\n        }],\n        remappings=[('buffer/input0', 'buffer/image'),\n                    ('input0', 'image'),\n                    ('buffer/input1', 'buffer/camera_info'),\n                    ('input1', 'camera_info')],\n    )\n\n    monitor_node = ComposableNode(\n        name='MonitorNode',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        parameters=[{\n            'monitor_data_format': 'apriltag_msgs/msg/AprilTagDetectionArray',\n        }],\n        remappings=[\n            ('output', 'apriltag_detections')],\n    )\n\n    composable_node_container = ComposableNodeContainer(\n        name='container',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='rclcpp_components',\n        executable='component_container_mt',\n        prefix=container_prefix,\n        sigterm_timeout=container_sigterm_timeout,\n        composable_node_descriptions=[\n            data_loader_node,\n            prep_resize_node,\n            playback_node,\n            monitor_node,\n            rectify_node,\n            apriltag_node\n        ],\n        output='screen'\n    )\n\n    return [composable_node_container]", "ROSBAG_PATH = 'datasets/r2b_dataset/r2b_storage'\n\ndef launch_setup(container_prefix, container_sigterm_timeout):\n    \"\"\"Generate launch description for benchmarking apriltag_ros graph.\"\"\"\n\n    # Detector parameters to detect all 36h11 tags\n    cfg_36h11 = {\n        'image_transport': 'raw',\n        'family': '36h11',\n        'size': 0.162\n    }\n\n    rectify_node = ComposableNode(\n        name='RectifyNode',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::RectifyNode'\n    )\n\n    apriltag_node = ComposableNode(\n        name='AprilTagNode',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='apriltag_ros',\n        plugin='AprilTagNode',\n        parameters=[cfg_36h11],\n        remappings=[\n            ('detections', 'apriltag_detections')\n        ]\n    )\n\n    data_loader_node = ComposableNode(\n        name='DataLoaderNode',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        remappings=[('hawk_0_left_rgb_image', 'data_loader/image'),\n                    ('hawk_0_left_rgb_camera_info', 'data_loader/camera_info')]\n    )\n\n    prep_resize_node = ComposableNode(\n        name='PrepResizeNode',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/image'),\n            ('image/camera_info', 'data_loader/camera_info'),\n            ('resize/image_raw', 'buffer/image'),\n            ('resize/camera_info', 'buffer/camera_info'),\n        ]\n    )\n\n    playback_node = ComposableNode(\n        name='PlaybackNode',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::PlaybackNode',\n        parameters=[{\n            'data_formats': [\n                'sensor_msgs/msg/Image',\n                'sensor_msgs/msg/CameraInfo'],\n        }],\n        remappings=[('buffer/input0', 'buffer/image'),\n                    ('input0', 'image'),\n                    ('buffer/input1', 'buffer/camera_info'),\n                    ('input1', 'camera_info')],\n    )\n\n    monitor_node = ComposableNode(\n        name='MonitorNode',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        parameters=[{\n            'monitor_data_format': 'apriltag_msgs/msg/AprilTagDetectionArray',\n        }],\n        remappings=[\n            ('output', 'apriltag_detections')],\n    )\n\n    composable_node_container = ComposableNodeContainer(\n        name='container',\n        namespace=TestAprilTagGraph.generate_namespace(),\n        package='rclcpp_components',\n        executable='component_container_mt',\n        prefix=container_prefix,\n        sigterm_timeout=container_sigterm_timeout,\n        composable_node_descriptions=[\n            data_loader_node,\n            prep_resize_node,\n            playback_node,\n            monitor_node,\n            rectify_node,\n            apriltag_node\n        ],\n        output='screen'\n    )\n\n    return [composable_node_container]", "\ndef generate_test_description():\n    return TestAprilTagGraph.generate_test_description_with_nsys(launch_setup)\n\nclass TestAprilTagGraph(ROS2BenchmarkTest):\n    \"\"\"Performance test for AprilTag graph.\"\"\"\n\n    # Custom configurations\n    config = ROS2BenchmarkConfig(\n        benchmark_name='apriltag_ros AprilTag Graph Benchmark',\n        input_data_path=ROSBAG_PATH,\n        # The slice of the rosbag to use\n        input_data_start_time=3.0,\n        input_data_end_time=3.5,\n        # Upper and lower bounds of peak throughput search window\n        publisher_upper_frequency=600.0,\n        publisher_lower_frequency=10.0,\n        # The number of frames to be buffered\n        playback_message_buffer_size=10,\n        custom_report_info={'data_resolution': IMAGE_RESOLUTION}\n    )\n\n    def test_benchmark(self):\n        self.run_benchmark()", ""]}
{"filename": "scripts/image_proc_rectify_node.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nPerformance test for image_proc RectifyNode.", "\"\"\"\nPerformance test for image_proc RectifyNode.\n\nThe graph consists of the following:\n- Preprocessors:\n    None\n- Graph under Test:\n    1. RectifyNode: rectifies images\n\nRequired:", "\nRequired:\n- Packages:\n    - image_proc\n- Datasets:\n    - assets/datasets/r2b_dataset/r2b_storage\n\"\"\"\n\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode", "from launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\nfrom ros2_benchmark import ImageResolution\nfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\nIMAGE_RESOLUTION = ImageResolution.HD\nROSBAG_PATH = 'datasets/r2b_dataset/r2b_storage'\n\ndef launch_setup(container_prefix, container_sigterm_timeout):\n    \"\"\"Generate launch description for benchmarking image_proc RectifyNode.\"\"\"\n\n    rectify_node = ComposableNode(\n        name='RectifyNode',\n        namespace=TestRectifyNode.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::RectifyNode',\n        remappings=[('image', 'image_raw')],\n    )\n\n    data_loader_node = ComposableNode(\n        name='DataLoaderNode',\n        namespace=TestRectifyNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        remappings=[('hawk_0_left_rgb_image', 'data_loader/image'),\n                    ('hawk_0_left_rgb_camera_info', 'data_loader/camera_info')]\n    )\n\n    prep_resize_node = ComposableNode(\n        name='PrepResizeNode',\n        namespace=TestRectifyNode.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/image'),\n            ('image/camera_info', 'data_loader/camera_info'),\n            ('resize/image_raw', 'buffer/image'),\n            ('resize/camera_info', 'buffer/camera_info'),\n        ]\n    )\n\n    playback_node = ComposableNode(\n        name='PlaybackNode',\n        namespace=TestRectifyNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::PlaybackNode',\n        parameters=[{\n            'data_formats': [\n                'sensor_msgs/msg/Image',\n                'sensor_msgs/msg/CameraInfo'],\n        }],\n        remappings=[('buffer/input0', 'buffer/image'),\n                    ('input0', 'image_raw'),\n                    ('buffer/input1', 'buffer/camera_info'),\n                    ('input1', 'camera_info')],\n    )\n\n    monitor_node = ComposableNode(\n        name='MonitorNode',\n        namespace=TestRectifyNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        parameters=[{\n            'monitor_data_format': 'sensor_msgs/msg/Image',\n        }],\n        remappings=[\n            ('output', 'image_rect')],\n    )\n\n    composable_node_container = ComposableNodeContainer(\n        name='container',\n        namespace=TestRectifyNode.generate_namespace(),\n        package='rclcpp_components',\n        executable='component_container_mt',\n        prefix=container_prefix,\n        sigterm_timeout=container_sigterm_timeout,\n        composable_node_descriptions=[\n            data_loader_node,\n            prep_resize_node,\n            playback_node,\n            monitor_node,\n            rectify_node\n        ],\n        output='screen'\n    )\n\n    return [composable_node_container]", "\ndef launch_setup(container_prefix, container_sigterm_timeout):\n    \"\"\"Generate launch description for benchmarking image_proc RectifyNode.\"\"\"\n\n    rectify_node = ComposableNode(\n        name='RectifyNode',\n        namespace=TestRectifyNode.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::RectifyNode',\n        remappings=[('image', 'image_raw')],\n    )\n\n    data_loader_node = ComposableNode(\n        name='DataLoaderNode',\n        namespace=TestRectifyNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        remappings=[('hawk_0_left_rgb_image', 'data_loader/image'),\n                    ('hawk_0_left_rgb_camera_info', 'data_loader/camera_info')]\n    )\n\n    prep_resize_node = ComposableNode(\n        name='PrepResizeNode',\n        namespace=TestRectifyNode.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/image'),\n            ('image/camera_info', 'data_loader/camera_info'),\n            ('resize/image_raw', 'buffer/image'),\n            ('resize/camera_info', 'buffer/camera_info'),\n        ]\n    )\n\n    playback_node = ComposableNode(\n        name='PlaybackNode',\n        namespace=TestRectifyNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::PlaybackNode',\n        parameters=[{\n            'data_formats': [\n                'sensor_msgs/msg/Image',\n                'sensor_msgs/msg/CameraInfo'],\n        }],\n        remappings=[('buffer/input0', 'buffer/image'),\n                    ('input0', 'image_raw'),\n                    ('buffer/input1', 'buffer/camera_info'),\n                    ('input1', 'camera_info')],\n    )\n\n    monitor_node = ComposableNode(\n        name='MonitorNode',\n        namespace=TestRectifyNode.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        parameters=[{\n            'monitor_data_format': 'sensor_msgs/msg/Image',\n        }],\n        remappings=[\n            ('output', 'image_rect')],\n    )\n\n    composable_node_container = ComposableNodeContainer(\n        name='container',\n        namespace=TestRectifyNode.generate_namespace(),\n        package='rclcpp_components',\n        executable='component_container_mt',\n        prefix=container_prefix,\n        sigterm_timeout=container_sigterm_timeout,\n        composable_node_descriptions=[\n            data_loader_node,\n            prep_resize_node,\n            playback_node,\n            monitor_node,\n            rectify_node\n        ],\n        output='screen'\n    )\n\n    return [composable_node_container]", "\ndef generate_test_description():\n    return TestRectifyNode.generate_test_description_with_nsys(launch_setup)\n\n\nclass TestRectifyNode(ROS2BenchmarkTest):\n    \"\"\"Performance test for image_proc RectifyNode.\"\"\"\n\n    # Custom configurations\n    config = ROS2BenchmarkConfig(\n        benchmark_name='image_proc::RectifyNode Benchmark',\n        input_data_path=ROSBAG_PATH,\n        # Upper and lower bounds of peak throughput search window\n        publisher_upper_frequency=2500.0,\n        publisher_lower_frequency=10.0,\n        # The number of frames to be buffered\n        playback_message_buffer_size=100,\n        custom_report_info={'data_resolution': IMAGE_RESOLUTION}\n    )\n\n    def test_benchmark(self):\n        self.run_benchmark()", ""]}
{"filename": "scripts/stereo_image_proc_graph.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nPerformance test for stereo_image_proc point cloud graph.", "\"\"\"\nPerformance test for stereo_image_proc point cloud graph.\n\nThe graph consists of the following:\n- Preprocessors:\n    1. PrepLeftResizeNode, PrepRightResizeNode: resizes images to quarter HD\n- Graph under Test:\n    1. DisparityNode: creates disparity images from stereo pair\n    2. PointCloudNode: converts disparity to pointcloud\n", "    2. PointCloudNode: converts disparity to pointcloud\n\nRequired:\n- Packages:\n    - stereo_image_proc\n- Datasets:\n    - assets/datasets/r2b_dataset/r2b_hideaway\n\"\"\"\n\nimport os", "\nimport os\n\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\nfrom ros2_benchmark import ImageResolution\nfrom ros2_benchmark import ROS2BenchmarkConfig, ROS2BenchmarkTest\n\nIMAGE_RESOLUTION = ImageResolution.QUARTER_HD", "\nIMAGE_RESOLUTION = ImageResolution.QUARTER_HD\nROSBAG_PATH = 'datasets/r2b_dataset/r2b_hideaway'\n\ndef launch_setup(container_prefix, container_sigterm_timeout):\n    \"\"\"Generate launch description for benchmarking stereo_image_proc point cloud graph.\"\"\"\n\n    env = os.environ.copy()\n    env['OSPL_VERBOSITY'] = '8'  # 8 = OS_NONE\n    # bare minimum formatting for console output matching\n    env['RCUTILS_CONSOLE_OUTPUT_FORMAT'] = '{message}'\n\n    disparity_node = ComposableNode(\n        name='DisparityNode',\n        namespace=TestStereoGraph.generate_namespace(),\n        package='stereo_image_proc',\n        plugin='stereo_image_proc::DisparityNode',\n        parameters=[{\n                'queue_size': 500,\n                'approx': False,\n                'use_system_default_qos': False\n        }],\n        remappings=[\n            ('/left/camera_info', '/left/camera_info'),\n            ('/right/camera_info', '/right/camera_info'),\n            ('/left/image_rect', '/left/image_rect'),\n            ('/right/image_rect', '/right/image_rect')],\n    )\n\n    pointcloud_node = ComposableNode(\n        name='PointCloudNode',\n        namespace=TestStereoGraph.generate_namespace(),\n        package='stereo_image_proc',\n        plugin='stereo_image_proc::PointCloudNode',\n        parameters=[{\n                'approximate_sync': False,\n                'avoid_point_cloud_padding': False,\n                'use_color': False,\n                'use_system_default_qos': False,\n        }],\n        remappings=[\n            ('/left/camera_info', '/left/camera_info'),\n            ('/right/camera_info', '/right/camera_info'),\n            ('/left/image_rect', '/left/image_rect'),\n            ('/right/image_rect', '/right/image_rect'),\n            ('left/image_rect_color', 'left/image_rect')],\n    )\n\n    data_loader_node = ComposableNode(\n        name='DataLoaderNode',\n        namespace=TestStereoGraph.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        remappings=[('hawk_0_left_rgb_image', 'data_loader/left_image'),\n                    ('hawk_0_left_rgb_camera_info', 'data_loader/left_camera_info'),\n                    ('hawk_0_right_rgb_image', 'data_loader/right_image'),\n                    ('hawk_0_right_rgb_camera_info', 'data_loader/right_camera_info')]\n    )\n\n    prep_left_resize_node = ComposableNode(\n        name='PrepLeftResizeNode',\n        namespace=TestStereoGraph.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/left_image'),\n            ('image/camera_info', 'data_loader/left_camera_info'),\n            ('resize/image_raw', 'buffer/left/image_resized'),\n            ('resize/camera_info', 'buffer/left/camera_info_resized'),\n        ]\n    )\n\n    prep_right_resize_node = ComposableNode(\n        name='PrepRightResizeNode',\n        namespace=TestStereoGraph.generate_namespace(),\n        package='image_proc',\n        plugin='image_proc::ResizeNode',\n        parameters=[{\n            'width': IMAGE_RESOLUTION['width'],\n            'height': IMAGE_RESOLUTION['height'],\n            'use_scale': False,\n        }],\n        remappings=[\n            ('image/image_raw', 'data_loader/right_image'),\n            ('image/camera_info', 'data_loader/right_camera_info'),\n            ('resize/image_raw', 'buffer/right/image_resized'),\n            ('resize/camera_info', 'buffer/right/camera_info_resized'),\n        ]\n    )\n\n    playback_node = ComposableNode(\n        name='PlaybackNode',\n        namespace=TestStereoGraph.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::PlaybackNode',\n        parameters=[{\n            'data_formats': [\n                'sensor_msgs/msg/Image',\n                'sensor_msgs/msg/CameraInfo',\n                'sensor_msgs/msg/Image',\n                'sensor_msgs/msg/CameraInfo'],\n        }],\n        remappings=[('buffer/input0', 'buffer/left/image_resized'),\n                    ('input0', 'left/image_rect'),\n                    ('buffer/input1', 'buffer/left/camera_info_resized'),\n                    ('input1', 'left/camera_info'),\n                    ('buffer/input2', 'buffer/right/image_resized'),\n                    ('input2', 'right/image_rect'),\n                    ('buffer/input3', 'buffer/right/camera_info_resized'),\n                    ('input3', 'right/camera_info')],\n    )\n\n    monitor_node = ComposableNode(\n        name='MonitorNode',\n        namespace=TestStereoGraph.generate_namespace(),\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        parameters=[{\n            'monitor_data_format': 'sensor_msgs/msg/PointCloud2',\n        }],\n        remappings=[\n            ('output', 'points2')],\n    )\n\n    composable_node_container = ComposableNodeContainer(\n        name='container',\n        namespace=TestStereoGraph.generate_namespace(),\n        package='rclcpp_components',\n        executable='component_container_mt',\n        prefix=container_prefix,\n        sigterm_timeout=container_sigterm_timeout,\n        composable_node_descriptions=[\n            data_loader_node,\n            prep_left_resize_node,\n            prep_right_resize_node,\n            playback_node,\n            monitor_node,\n            disparity_node,\n            pointcloud_node\n        ],\n        output='screen'\n    )\n\n    return [composable_node_container]", "\ndef generate_test_description():\n    return TestStereoGraph.generate_test_description_with_nsys(launch_setup)\n\n\nclass TestStereoGraph(ROS2BenchmarkTest):\n    \"\"\"Performance test for stereo image pointcloud graph.\"\"\"\n\n    # Custom configurations\n    config = ROS2BenchmarkConfig(\n        benchmark_name='Stereo Image Pointcloud Graph Benchmark',\n        input_data_path=ROSBAG_PATH,\n        # Upper and lower bounds of peak throughput search window\n        publisher_upper_frequency=100.0,\n        publisher_lower_frequency=10.0,\n        # The number of frames to be buffered\n        playback_message_buffer_size=100,\n        custom_report_info={'data_resolution': IMAGE_RESOLUTION}\n    )\n\n    def test_benchmark(self):\n        self.run_benchmark()", ""]}
{"filename": "ros2_benchmark/test/data_loader_node_pol.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\nimport os", "\nimport os\nimport time\nimport unittest\n\nimport launch\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\nimport launch_testing.actions\nimport rclpy", "import launch_testing.actions\nimport rclpy\n\nfrom ros2_benchmark.utils.ros2_utility import ClientUtility\nfrom ros2_benchmark_interfaces.srv import SetData, StartLoading, StopLoading\n\nDIR_PATH = os.path.dirname(os.path.realpath(__file__))\nROSBAG_PATH = os.path.join(DIR_PATH, 'pol.bag/pol.bag_0.db3')\n\n\ndef generate_test_description():\n    \"\"\"Initialize test nodes and generate test description.\"\"\"\n    data_loader_node = ComposableNode(\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        name='DataLoaderNode',\n    )\n\n    data_loader_container = ComposableNodeContainer(\n        package='rclcpp_components',\n        name='data_loader_container',\n        namespace='',\n        executable='component_container_mt',\n        composable_node_descriptions=[data_loader_node],\n        output='screen'\n    )\n\n    return launch.LaunchDescription([\n        data_loader_container,\n        launch_testing.actions.ReadyToTest()\n    ])", "\n\ndef generate_test_description():\n    \"\"\"Initialize test nodes and generate test description.\"\"\"\n    data_loader_node = ComposableNode(\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::DataLoaderNode',\n        name='DataLoaderNode',\n    )\n\n    data_loader_container = ComposableNodeContainer(\n        package='rclcpp_components',\n        name='data_loader_container',\n        namespace='',\n        executable='component_container_mt',\n        composable_node_descriptions=[data_loader_node],\n        output='screen'\n    )\n\n    return launch.LaunchDescription([\n        data_loader_container,\n        launch_testing.actions.ReadyToTest()\n    ])", "\n\nclass TestDataLoaderNode(unittest.TestCase):\n    \"\"\"An unit test class for DataLoaderNode.\"\"\"\n\n    def test_data_loader_node_services(self):\n        \"\"\"Test services hosted in DataLoaderNode.\"\"\"\n        SERVICE_SETUP_TIMEOUT_SEC = 5\n        SERVICE_FUTURE_TIMEOUT_SEC = 25\n\n        # Create a test ROS node\n        rclpy.init()\n        node = rclpy.create_node('test_node')\n\n        # Create a set_data service client\n        set_data_client = ClientUtility.create_service_client_blocking(\n            node, SetData, 'set_data', SERVICE_SETUP_TIMEOUT_SEC)\n        self.assertIsNotNone(set_data_client)\n\n        # Create a start_loading service client\n        start_loading_client = ClientUtility.create_service_client_blocking(\n            node, StartLoading, 'start_loading', SERVICE_SETUP_TIMEOUT_SEC)\n        self.assertIsNotNone(start_loading_client)\n\n        # Create a stop_loading service client\n        stop_loading_client = ClientUtility.create_service_client_blocking(\n            node, StopLoading, 'stop_loading', SERVICE_SETUP_TIMEOUT_SEC)\n        self.assertIsNotNone(stop_loading_client)\n\n        # Send a request to the set_data service\n        set_data_request = SetData.Request()\n        set_data_request.data_path = ROSBAG_PATH\n        set_data_future = set_data_client.call_async(\n            set_data_request)\n\n        # Wait for the response from the start_recording service\n        set_data_response = ClientUtility.get_service_response_from_future_blocking(\n            node, set_data_future, SERVICE_FUTURE_TIMEOUT_SEC)\n        self.assertIsNotNone(set_data_response)\n        self.assertTrue(set_data_response.success)\n\n        # Send a request to the start_loading service\n        start_loading_request = StartLoading.Request()\n        start_loading_future = start_loading_client.call_async(\n            start_loading_request)\n\n        time.sleep(1)\n\n        # Send a request to the stop_loading service\n        stop_loading_request = StopLoading.Request()\n        stop_loading_future = stop_loading_client.call_async(\n            stop_loading_request)\n\n        # Wait for the response from the start_recording service\n        start_loading_response = ClientUtility.get_service_response_from_future_blocking(\n            node, start_loading_future, SERVICE_FUTURE_TIMEOUT_SEC)\n        self.assertIsNotNone(start_loading_response)\n        node.get_logger().info('Received response from the start_loading service:')\n        node.get_logger().info(str(start_loading_response))\n\n        # Wait for the response from the start_recording service\n        stop_loading_response = ClientUtility.get_service_response_from_future_blocking(\n            node, stop_loading_future, SERVICE_FUTURE_TIMEOUT_SEC)\n        self.assertIsNotNone(stop_loading_response)\n        node.get_logger().info('Received response from the stop_loading service.')", ""]}
{"filename": "ros2_benchmark/test/monitor_node_pol.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\nimport os", "\nimport os\nimport unittest\n\nimport launch\nfrom launch.actions import ExecuteProcess\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\nimport launch_testing.actions\nimport rclpy", "import launch_testing.actions\nimport rclpy\n\nfrom ros2_benchmark.utils.ros2_utility import ClientUtility\nfrom ros2_benchmark_interfaces.srv import StartMonitoring\n\n\ndef generate_test_description():\n    \"\"\"Initialize test nodes and generate test description.\"\"\"\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    rosbag_path = os.path.join(dir_path, 'pol.bag')\n\n    monitor_node0 = ComposableNode(\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        name='MonitorNode0',\n        parameters=[{\n            'monitor_data_format': 'sensor_msgs/msg/Image',\n            'monitor_index': 0\n        }],\n        remappings=[\n            ('output', '/image'),\n        ],\n    )\n\n    monitor_node1 = ComposableNode(\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::MonitorNode',\n        name='MonitorNode1',\n        parameters=[{\n            'monitor_data_format': 'sensor_msgs/msg/CameraInfo',\n            'monitor_index': 1\n        }],\n        remappings=[\n            ('output', '/camera_info'),\n        ],\n    )\n\n    monitor_container = ComposableNodeContainer(\n        package='rclcpp_components',\n        name='monitor_container',\n        namespace='',\n        executable='component_container_mt',\n        composable_node_descriptions=[monitor_node0, monitor_node1],\n        output='screen'\n    )\n\n    # Play rosbag for the monitor node to receive messages\n    rosbag_play = ExecuteProcess(\n        cmd=['ros2', 'bag', 'play', rosbag_path],\n        output='screen')\n\n    return launch.LaunchDescription([\n        rosbag_play,\n        monitor_container,\n        launch_testing.actions.ReadyToTest()\n    ])", "\n\nclass TestMonitorNode(unittest.TestCase):\n    \"\"\"An unit test class for MonitorNode.\"\"\"\n\n    def test_monitor_node_services(self):\n        \"\"\"Test services hosted in MonitorNode.\"\"\"\n        SERVICE_SETUP_TIMEOUT_SEC = 5\n        SERVICE_TIMEOUT_SEC = 20\n        SERVICE_FUTURE_TIMEOUT_SEC = 25\n\n        # Create a test ROS node\n        rclpy.init()\n        node = rclpy.create_node('test_node')\n\n        # Create a start_monitoring0 service client\n        start_monitoring_client0 = ClientUtility.create_service_client_blocking(\n            node, StartMonitoring, 'start_monitoring0', SERVICE_SETUP_TIMEOUT_SEC)\n        self.assertIsNotNone(start_monitoring_client0)\n\n        # Create a start_monitoring1 service client\n        start_monitoring_client1 = ClientUtility.create_service_client_blocking(\n            node, StartMonitoring, 'start_monitoring1', SERVICE_SETUP_TIMEOUT_SEC)\n        self.assertIsNotNone(start_monitoring_client1)\n\n        # Send a request to the start_monitoring0 service\n        start_monitoring_request = StartMonitoring.Request()\n        start_monitoring_request.timeout = SERVICE_TIMEOUT_SEC\n        start_monitoring_request.message_count = 1\n        start_monitoring_future0 = start_monitoring_client0.call_async(\n            start_monitoring_request)\n\n        # Send a request to the start_monitoring1 service\n        start_monitoring_future1 = start_monitoring_client1.call_async(\n            start_monitoring_request)\n\n        # Wait for the response from the start_monitoring0 service\n        start_monitoring_response0 = ClientUtility.get_service_response_from_future_blocking(\n            node, start_monitoring_future0, SERVICE_FUTURE_TIMEOUT_SEC)\n        self.assertIsNotNone(start_monitoring_response0)\n        node.get_logger().info('Received response from the start_monitoring0 service:')\n        node.get_logger().info(str(start_monitoring_response0))\n\n        # Wait for the response from the start_monitoring1 service\n        start_monitoring_response1 = ClientUtility.get_service_response_from_future_blocking(\n            node, start_monitoring_future1, SERVICE_FUTURE_TIMEOUT_SEC)\n        self.assertIsNotNone(start_monitoring_response1)\n        node.get_logger().info('Received response from the start_monitoring1 service:')\n        node.get_logger().info(str(start_monitoring_response1))", ""]}
{"filename": "ros2_benchmark/test/playback_node_pol.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\nimport os", "\nimport os\nimport unittest\n\nimport launch\nfrom launch.actions import ExecuteProcess\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\nimport launch_testing.actions\nimport rclpy", "import launch_testing.actions\nimport rclpy\n\nfrom ros2_benchmark.utils.ros2_utility import ClientUtility\nfrom ros2_benchmark_interfaces.srv import PlayMessages, StartRecording\n\n\ndef generate_test_description():\n    \"\"\"Initialize test nodes and generate test description.\"\"\"\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    rosbag_path = os.path.join(dir_path, 'pol.bag')\n\n    playback_node = ComposableNode(\n        package='ros2_benchmark',\n        plugin='ros2_benchmark::PlaybackNode',\n        name='PlaybackNode',\n        parameters=[{\n            'data_formats': ['sensor_msgs/msg/Image'],\n        }],\n        remappings=[\n            ('buffer/input0', 'buffer/image'),\n            ('input0', '/image')\n        ],\n    )\n\n    playback_container = ComposableNodeContainer(\n        package='rclcpp_components',\n        name='playback_container',\n        namespace='',\n        executable='component_container_mt',\n        composable_node_descriptions=[playback_node],\n        output='screen'\n    )\n\n    # Play rosbag for the playback node to record messages\n    rosbag_play = ExecuteProcess(\n        cmd=['ros2', 'bag', 'play', rosbag_path, '--remap', 'image:=/buffer/image'],\n        output='screen')\n\n    return launch.LaunchDescription([\n        rosbag_play,\n        playback_container,\n        launch_testing.actions.ReadyToTest()\n    ])", "\n\nclass TestPlaybackNode(unittest.TestCase):\n    \"\"\"An unit test class for PlaybackNode.\"\"\"\n\n    def test_playback_node_services(self):\n        \"\"\"Test services hosted in PlaybackNode.\"\"\"\n        SERVICE_SETUP_TIMEOUT_SEC = 5\n        SERVICE_TIMEOUT_SEC = 20\n        SERVICE_FUTURE_TIMEOUT_SEC = 25\n\n        # Create a test ROS node\n        rclpy.init()\n        node = rclpy.create_node('test_node')\n\n        # Create a start_recording service client\n        start_recording_client = ClientUtility.create_service_client_blocking(\n            node, StartRecording, 'start_recording', SERVICE_SETUP_TIMEOUT_SEC)\n        self.assertIsNotNone(start_recording_client)\n\n        # Create a play_messages service client\n        play_messages_client = ClientUtility.create_service_client_blocking(\n            node, PlayMessages, 'play_messages', SERVICE_SETUP_TIMEOUT_SEC)\n        self.assertIsNotNone(play_messages_client)\n\n        # Send a request to the start_recording service\n        start_recording_request = StartRecording.Request()\n        start_recording_request.buffer_length = 10\n        start_recording_request.timeout = SERVICE_TIMEOUT_SEC\n        start_recording_future = start_recording_client.call_async(start_recording_request)\n\n        # Wait for the response from the start_recording service\n        start_recording_response = ClientUtility.get_service_response_from_future_blocking(\n            node, start_recording_future, SERVICE_FUTURE_TIMEOUT_SEC)\n        self.assertIsNotNone(start_recording_response)\n        node.get_logger().info('Received response from the start_recording service:')\n        node.get_logger().info(str(start_recording_response))\n\n        # Send a request to the play_messages service\n        play_messages_request = PlayMessages.Request()\n        play_messages_request.target_publisher_rate = 30.0\n        play_messages_future = play_messages_client.call_async(play_messages_request)\n\n        # Wait for the response from the play_messages service\n        play_messages_response = ClientUtility.get_service_response_from_future_blocking(\n            node, play_messages_future, SERVICE_FUTURE_TIMEOUT_SEC)\n        self.assertIsNotNone(play_messages_response)\n        node.get_logger().info('Received response from the play_messages service:')\n        node.get_logger().info(str(play_messages_response))", ""]}
{"filename": "ros2_benchmark/ros2_benchmark/ros2_benchmark_test.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\nimport datetime", "\nimport datetime\nfrom enum import Enum\nfrom functools import partial\nimport hashlib\nimport json\nfrom math import ceil\nimport os\nimport platform\nimport sys", "import platform\nimport sys\nimport time\nfrom typing import Iterable\nimport unittest\n\nimport launch\nfrom launch.actions import OpaqueFunction\nimport launch_testing.actions\nimport rclpy", "import launch_testing.actions\nimport rclpy\n\nfrom ros2_benchmark_interfaces.srv import GetTopicMessageTimestamps, PlayMessages\nfrom ros2_benchmark_interfaces.srv import SetData, StartLoading, StopLoading\nfrom ros2_benchmark_interfaces.srv import StartMonitoring, StartRecording\n\nimport rosbag2_py\n\nfrom .basic_performance_calculator import BasicPerformanceMetrics", "\nfrom .basic_performance_calculator import BasicPerformanceMetrics\nfrom .ros2_benchmark_config import BenchmarkMode\nfrom .ros2_benchmark_config import ROS2BenchmarkConfig\nfrom .utils.cpu_profiler import CPUProfiler\nfrom .utils.nsys_utility import NsysUtility\nfrom .utils.ros2_utility import ClientUtility\n\n\n# The maximum allowed line width of a performance repeort displayed in the terminal", "\n# The maximum allowed line width of a performance repeort displayed in the terminal\nMAX_REPORT_OUTPUT_WIDTH = 90\nidle_cpu_util = 0.0\n\n\nclass BenchmarkMetadata(Enum):\n    \"\"\"Benchmark metadata items to be included in a final report.\"\"\"\n\n    NAME = 'Test Name'\n    TEST_DATETIME = 'Test Datetime'\n    TEST_FILE_PATH = 'Test File Path'\n    DEVICE_ARCH = 'Device Architecture'\n    DEVICE_OS = 'Device OS'\n    DEVICE_HOSTNAME = 'Device Hostname'\n    BENCHMARK_MODE = 'Benchmark Mode'\n    INPUT_DATA_PATH = 'Input Data Path'\n    INPUT_DATA_HASH = 'Input Data Hash'\n    INPUT_DATA_SIZE = 'Input Data Size (bytes)'\n    INPUT_DATA_START_TIME = 'Input Data Start Time (s)'\n    INPUT_DATA_END_TIME = 'Input Data End Time (s)'\n    DATA_RESOLUTION = 'Data Resolution'\n    IDLE_CPU_UTIL = 'Idle System CPU Util. (%)'\n    PEAK_THROUGHPUT_PREDICTION = 'Peak Throughput Prediction (Hz)'\n    CONFIG = 'Test Configurations'", "\n\nclass ROS2BenchmarkTest(unittest.TestCase):\n    \"\"\"The main ros2_benchmark framework test class.\"\"\"\n\n    # A config object that holds benchmark-relevant settings\n    config = ROS2BenchmarkConfig()\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize ros2_benchmark.\"\"\"\n        # Calculated hash of the data to be loaded from a data loader node\n        self._input_data_hash = ''\n\n        # Size of the data (file)\n        self._input_data_size_bytes = 0\n\n        self._test_datetime = datetime.datetime.now(datetime.timezone.utc)\n\n        # The absolute path of the top level launch script\n        self._test_file_path = os.path.abspath(sys.argv[1])\n\n        self._peak_throughput_prediction = 0.0\n        self._logger_name_stack = ['']\n\n        # Override default configs from env variablees\n        self.override_config_from_env()\n\n        self._cpu_profiler = CPUProfiler()\n        self._cpu_profiler_log_file_path = ''\n\n        super().__init__(*args, **kwargs)\n\n    @classmethod\n    def setUpClass(cls) -> None:\n        \"\"\"Set up before first test method.\"\"\"\n        # Initialize the ROS context for the test node\n        rclpy.init()\n\n    @classmethod\n    def tearDownClass(cls) -> None:\n        \"\"\"Tear down after last test method.\"\"\"\n        # Shutdown the ROS context\n        rclpy.shutdown()\n\n    def setUp(self) -> None:\n        \"\"\"Set up before each test method.\"\"\"\n        # Create a ROS node for benchmark tests\n        self.node = rclpy.create_node('Controller', namespace=self.generate_namespace())\n\n    def tearDown(self) -> None:\n        \"\"\"Tear down after each test method.\"\"\"\n        self.node.destroy_node()\n\n    @classmethod\n    def generate_namespace(cls, *tokens: Iterable[str], absolute=True) -> str:\n        \"\"\"\n        Generate a namespace with an optional list of tokens.\n\n        This function is a utility for producing namespaced topic and service names in\n        such a way that there are no collisions between 'dummy' nodes running for testing\n        and 'normal' nodes running on the same machine.\n\n        Parameters\n        ----------\n        tokens : Iterable[str]\n            List of tokens to include in the namespace. Often used to generate\n            separate namespaces for Isaac ROS and reference implementations.\n\n        absolute: bool\n            Whether or not to generate an absolute namespace, by default True.\n\n        Returns\n        -------\n        str\n            The generated namespace as a slash-delimited string\n\n        \"\"\"\n        return ('/' if absolute else '') + '/'.join(\n            filter(None, [cls.config.benchmark_namespace, *tokens]))\n\n    @staticmethod\n    def generate_test_description(\n        nodes: Iterable[launch.Action], node_startup_delay: float = 5.0\n    ) -> launch.LaunchDescription:\n        \"\"\"\n        Generate a test launch description.\n\n        The nodes included in this launch description will be launched as a test fixture\n        immediately before the first test in the test class runs. Note that the graph is\n        NOT shut down or re-launched between tests within the same class.\n\n        Parameters\n        ----------\n        nodes : Iterable[launch.Action]\n            List of Actions to launch before running the test.\n        node_startup_delay : float, optional\n            Seconds to delay by to account for node startup, by default 2.0\n\n        Returns\n        -------\n        launch.LaunchDescription\n            The LaunchDescription object to launch before running the test\n\n        \"\"\"\n        # Wait until the system CPU usage become stable\n        rclpy.logging.get_logger('r2b').info(\n            'Waiting 10 seconds for measuring idle system CPU utilization...')\n        time.sleep(10)\n        global idle_cpu_util\n        idle_cpu_util = CPUProfiler.get_current_cpu_usage()\n\n        return launch.LaunchDescription(\n            nodes + [\n                # Start tests after a fixed delay for node startup\n                launch.actions.TimerAction(\n                    period=node_startup_delay, actions=[launch_testing.actions.ReadyToTest()])\n            ]\n        )\n\n    @staticmethod\n    def generate_test_description_with_nsys(\n        launch_setup, node_startup_delay: float = 5.0\n    ) -> launch.LaunchDescription:\n        \"\"\"Generate a test launch description with the nsys capability built in.\"\"\"\n        # Wait until the system CPU usage become stable\n        rclpy.logging.get_logger('r2b').info(\n            'Waiting 10 seconds for measuring idle system CPU utilization...')\n        time.sleep(10)\n        global idle_cpu_util\n        idle_cpu_util = CPUProfiler.get_current_cpu_usage()\n\n        launch_args = NsysUtility.generate_launch_args()\n        bound_launch_setup = partial(\n            NsysUtility.launch_setup_wrapper,\n            launch_setup=launch_setup)\n        nodes = launch_args + [OpaqueFunction(function=bound_launch_setup)]\n        return launch.LaunchDescription(\n            nodes + [\n                # Start tests after a fixed delay for node startup\n                launch.actions.TimerAction(\n                    period=node_startup_delay,\n                    actions=[launch_testing.actions.ReadyToTest()])\n            ]\n        )\n\n    def get_logger(self, child_name=''):\n        \"\"\"Get logger with child layers.\"\"\"\n        if hasattr(self, 'node'):\n            base_logger = self.node.get_logger()\n        else:\n            base_logger = rclpy.logging.get_logger('ROS2BenchmarkTest')\n\n        if child_name:\n            return base_logger.get_child(child_name)\n\n        if len(self._logger_name_stack) == 0:\n            return base_logger\n\n        for logger_name in self._logger_name_stack:\n            if logger_name:\n                base_logger = base_logger.get_child(logger_name)\n\n        return base_logger\n\n    def push_logger_name(self, name):\n        \"\"\"Add a child layer to the logger.\"\"\"\n        self._logger_name_stack.append(name)\n\n    def pop_logger_name(self):\n        \"\"\"Pop a child layer from the logger.\"\"\"\n        self._logger_name_stack.pop()\n\n    def override_config_from_env(self):\n        \"\"\"Override config parameters with values from environment variables.\"\"\"\n        override_config_dict = {}\n        for param_key in self.config.__dict__.keys():\n            env_value = os.getenv(f'ROS2_BENCHMARK_OVERRIDE_{param_key.upper()}')\n            if env_value is not None:\n                override_config_dict[param_key] = env_value\n                self.get_logger().info(\n                    f'Updating a benchmark config from env: {param_key} = {env_value}')\n        self.config.apply_to_attributes(override_config_dict)\n\n    @classmethod\n    def get_assets_root_path(cls):\n        \"\"\"Get assets path provided in configurations.\"\"\"\n        return cls.config.assets_root\n\n    def get_input_data_absolute_path(self):\n        \"\"\"Construct the absolute path of the input data file from configurations.\"\"\"\n        return os.path.join(self.config.assets_root, self.config.input_data_path)\n\n    def print_report(self, report: dict, sub_heading: str = '') -> None:\n        \"\"\"Print the given report.\"\"\"\n        heading = self.config.benchmark_name\n\n        # Temporarily remove configs from the report as we don't want it to be printed\n        config_value = None\n        if 'metadata' in report:\n            config_value = report['metadata'].pop(BenchmarkMetadata.CONFIG, None)\n\n        is_prev_dict = False\n        table_blocks = []\n        table_block_rows = []\n\n        def construct_table_blocks_helper(prefix, data):\n            nonlocal is_prev_dict\n            nonlocal table_blocks\n            nonlocal table_block_rows\n            for key, value in data.items():\n                key_str = str(key.value) if isinstance(key, Enum) else str(key)\n                if isinstance(value, dict):\n                    if not is_prev_dict and len(table_block_rows) > 0:\n                        table_blocks.append(table_block_rows)\n                        table_block_rows = []\n                        is_prev_dict = True\n                    construct_table_blocks_helper(f'{prefix}[{key_str}] ', value)\n                    if not is_prev_dict:\n                        table_blocks.append(table_block_rows)\n                        table_block_rows = []\n                        is_prev_dict = True\n                elif isinstance(value, Enum):\n                    table_block_rows.append(f'{prefix}{key_str} : {value.value}')\n                    is_prev_dict = False\n                elif isinstance(value, float):\n                    table_block_rows.append(f'{prefix}{key_str} : {\"{:.3f}\".format(value)}')\n                    is_prev_dict = False\n                else:\n                    table_block_rows.append(f'{prefix}{key_str} : {value}')\n                    is_prev_dict = False\n\n        construct_table_blocks_helper('', report)\n        if len(table_block_rows) > 0:\n            table_blocks.append(table_block_rows)\n        max_row_width = max([len(row) for rows in table_blocks for row in rows] +\n                            [len(heading), len(sub_heading)])\n        max_row_width = min(max_row_width, MAX_REPORT_OUTPUT_WIDTH)\n\n        def print_line_helper():\n            self.get_logger().info('+-{}-+'.format('-'*max_row_width))\n\n        def print_row_helper(row):\n            self.get_logger().info('| {:<{width}} |'.format(row, width=max_row_width))\n\n        def print_table_helper():\n            print_line_helper()\n            self.get_logger().info('| {:^{width}} |'.format(heading, width=max_row_width))\n            if sub_heading:\n                self.get_logger().info('| {:^{width}} |'.format(sub_heading, width=max_row_width))\n            print_line_helper()\n            for rows in table_blocks:\n                for row in rows:\n                    print_row_helper(row)\n                print_line_helper()\n\n        print_table_helper()\n\n        if config_value is not None:\n            report['metadata'][BenchmarkMetadata.CONFIG] = config_value\n\n    def construct_final_report(self, report: dict) -> dict:\n        \"\"\"Construct and return the final report from the given report.\"\"\"\n        final_report = report\n\n        if len(self.config.custom_report_info) > 0:\n            final_report['custom'] = self.config.custom_report_info\n\n        # Add benchmark metadata\n        metadata = {}\n\n        # Benchmark launch info\n        metadata[BenchmarkMetadata.NAME] = self.config.benchmark_name\n        metadata[BenchmarkMetadata.TEST_FILE_PATH] = self._test_file_path\n        metadata[BenchmarkMetadata.TEST_DATETIME] = \\\n            self._test_datetime.strftime('%Y-%m-%dT%H:%M:%SZ')\n\n        # Systeme info\n        uname = platform.uname()\n        metadata[BenchmarkMetadata.DEVICE_HOSTNAME] = uname.node\n        metadata[BenchmarkMetadata.DEVICE_ARCH] = uname.machine\n        metadata[BenchmarkMetadata.DEVICE_OS] = \\\n            f'{uname.system} {uname.release} {uname.version}'\n        metadata[BenchmarkMetadata.CONFIG] = self.config.to_yaml_str()\n        metadata[BenchmarkMetadata.IDLE_CPU_UTIL] = idle_cpu_util\n\n        # Benchmark data info\n        metadata[BenchmarkMetadata.BENCHMARK_MODE] = self.config.benchmark_mode.value\n        if self.config.benchmark_mode in [BenchmarkMode.LOOPING, BenchmarkMode.SWEEPING]:\n            metadata[BenchmarkMetadata.PEAK_THROUGHPUT_PREDICTION] = \\\n                self._peak_throughput_prediction\n        metadata[BenchmarkMetadata.INPUT_DATA_PATH] = self.get_input_data_absolute_path()\n        metadata[BenchmarkMetadata.INPUT_DATA_SIZE] = self._input_data_size_bytes\n        metadata[BenchmarkMetadata.INPUT_DATA_HASH] = self._input_data_hash\n        if self.config.input_data_start_time != -1:\n            metadata[BenchmarkMetadata.INPUT_DATA_START_TIME] = \\\n                self.config.input_data_start_time\n        if self.config.input_data_end_time != -1:\n            metadata[BenchmarkMetadata.INPUT_DATA_END_TIME] = \\\n                self.config.input_data_end_time\n\n        final_report['metadata'] = metadata\n\n        return final_report\n\n    def export_report(self, report: dict) -> None:\n        \"\"\"Export the given report to a JSON file.\"\"\"\n\n        def to_json_compatible_helper(data):\n            data_out = {}\n            for key, value in data.items():\n                if isinstance(value, dict):\n                    data_out[str(key)] = to_json_compatible_helper(value)\n                elif isinstance(value, Enum):\n                    data_out[str(key)] = str(value)\n                else:\n                    try:\n                        json.dumps(value)\n                        data_out[str(key)] = value\n                    except TypeError:\n                        data_out[str(key)] = str(value)\n            return data_out\n\n        if self.config.log_file_name == '':\n            timestr = self._test_datetime.strftime('%Y%m%d-%H%M%S')\n            log_file_path = os.path.join(self.config.log_folder, f'r2b-log-{timestr}.json')\n        else:\n            log_file_path = os.path.join(self.config.log_folder, self.config.log_file_name)\n\n        with open(log_file_path, 'a') as f:\n            f.write(json.dumps(to_json_compatible_helper(report)))\n        self.get_logger().info(f'Exported benchmark report to {log_file_path}')\n\n    def create_service_client_blocking(self, service_type, service_name):\n        \"\"\"Create a service client and wait for it to be available.\"\"\"\n        namespaced_service_name = self.generate_namespace(service_name)\n        service_client = ClientUtility.create_service_client_blocking(\n            self.node, service_type, namespaced_service_name,\n            self.config.setup_service_client_timeout_sec)\n        if not service_client:\n            self.fail(f'Failed to create a {service_name} service client')\n        return service_client\n\n    def get_service_response_from_future_blocking(\n            self,\n            future,\n            check_success=False,\n            timeout_sec=None):\n        \"\"\"Block and wait for a service future to return.\"\"\"\n        if timeout_sec is None:\n            timeout_sec = self.config.default_service_future_timeout_sec\n        future_result = ClientUtility.get_service_response_from_future_blocking(\n            self.node, future, timeout_sec)\n        if not future_result:\n            self.fail('Failed to wait for a service future')\n        if check_success and not future_result.success:\n            self.fail('A service returned with an unsuccess response')\n        return future_result\n\n    def prepare_buffer(self):\n        \"\"\"Load data from a data loader node to a playback node.\"\"\"\n        self.push_logger_name('Loading')\n\n        # Check the input data file\n        input_data_path = self.get_input_data_absolute_path()\n        try:\n            rosbag_info = rosbag2_py.Info()\n            rosbag_metadata = rosbag_info.read_metadata(input_data_path, 'sqlite3')\n            rosbag_file_path = input_data_path\n            if len(rosbag_metadata.files) > 0:\n                rosbag_file_path = os.path.join(input_data_path, rosbag_metadata.files[0].path)\n            elif len(rosbag_metadata.relative_file_paths) > 0:\n                rosbag_file_path = os.path.join(\n                    input_data_path, rosbag_metadata.relative_file_paths[0])\n            self.get_logger().info('Checking input data file...')\n            self.get_logger().info(f' - Rosbag path = {rosbag_file_path}')\n            if rosbag_metadata.compression_mode:\n                self.get_logger().info(\n                    f' - Compression mode = {rosbag_metadata.compression_mode}')\n            if rosbag_metadata.compression_format:\n                self.get_logger().info(\n                    f' - Compression format = {rosbag_metadata.compression_format}')\n            self.get_logger().info('Computing input data file hash...')\n            hash_md5 = hashlib.md5()\n            self._input_data_size_bytes = 0\n            with open(rosbag_file_path, 'rb') as input_data:\n                while True:\n                    next_chunk = input_data.read(4096)\n                    self._input_data_size_bytes += len(next_chunk)\n                    if next_chunk == b'':\n                        break\n                    hash_md5.update(next_chunk)\n            self._input_data_hash = hash_md5.hexdigest()\n            self.get_logger().info(f' - File hash = \"{self._input_data_hash}\"')\n            self.get_logger().info(f' - File size (bytes) = {self._input_data_size_bytes}')\n        except FileNotFoundError:\n            self.fail(f'Could not open the input data file at \"{input_data_path}\"')\n\n        # Create service clients\n        set_data_client = self.create_service_client_blocking(\n            SetData, 'set_data')\n        start_loading_client = self.create_service_client_blocking(\n            StartLoading, 'start_loading')\n        stop_loading_client = self.create_service_client_blocking(\n            StopLoading, 'stop_loading')\n        start_recording_client = self.create_service_client_blocking(\n            StartRecording, 'start_recording')\n\n        # Set and initialize data\n        self.get_logger().info('Requesting to initialize the data loader node')\n        set_data_request = SetData.Request()\n        set_data_request.data_path = input_data_path\n        set_data_request.publish_tf_messages = self.config.publish_tf_messages_in_set_data\n        set_data_request.publish_tf_static_messages = \\\n            self.config.publish_tf_static_messages_in_set_data\n        set_data_future = set_data_client.call_async(set_data_request)\n        self.get_service_response_from_future_blocking(\n            set_data_future,\n            check_success=True,\n            timeout_sec=self.config.set_data_service_future_timeout_sec)\n\n        # Start recording\n        self.get_logger().info('Requesting to record messages.')\n        start_recording_request = StartRecording.Request()\n        start_recording_request.buffer_length = self.config.playback_message_buffer_size\n        start_recording_request.timeout = self.config.start_recording_service_timeout_sec\n        start_recording_request.record_data_timeline = self.config.record_data_timeline\n        if self.config.benchmark_mode == BenchmarkMode.TIMELINE:\n            self.get_logger().info('Requesting to get topic message timestamps.')\n            start_recording_request.topic_message_timestamps = self.get_topic_message_timestamps()\n        start_recording_future = start_recording_client.call_async(start_recording_request)\n\n        # Load and play messages from the data loader node\n        self.get_logger().info('Requesting to load messages.')\n        start_loading_request = StartLoading.Request()\n        start_loading_request.publish_in_real_time = self.config.load_data_in_real_time\n        if self.config.input_data_start_time != -1:\n            start_loading_request.start_time_offset_ns = \\\n                int(self.config.input_data_start_time * (10**9))\n        if self.config.input_data_end_time != -1:\n            start_loading_request.end_time_offset_ns = \\\n                int(self.config.input_data_end_time * (10**9))\n        if self.config.benchmark_mode == BenchmarkMode.TIMELINE:\n            start_loading_request.repeat_data = False\n        else:\n            start_loading_request.repeat_data = True\n        start_loading_future = start_loading_client.call_async(\n            start_loading_request)\n\n        # Wait for the recording request to finish (or time out)\n        self.get_logger().info('Waiting for the recording service to end.')\n        start_recording_response = self.get_service_response_from_future_blocking(\n            start_recording_future,\n            timeout_sec=self.config.start_recording_service_future_timeout_sec)\n        recorded_topic_message_counts = start_recording_response.recorded_topic_message_counts\n        recorded_message_count = start_recording_response.recorded_message_count\n\n        # Stop loading data\n        self.get_logger().info('Requesting to stop loading data.')\n        stop_loading_request = StopLoading.Request()\n        stop_loading_future = stop_loading_client.call_async(stop_loading_request)\n        self.get_service_response_from_future_blocking(stop_loading_future)\n\n        # Wait for data loader service (start_loading) to end\n        self.get_logger().info('Waiting for the start_loading serevice to end.')\n        self.get_service_response_from_future_blocking(\n            start_loading_future, check_success=True)\n\n        self.assertTrue(recorded_message_count > 0, 'No message was recorded')\n\n        if (not self.config.record_data_timeline):\n            for topic_message_count in recorded_topic_message_counts:\n                topic_name = topic_message_count.topic_name\n                recorded_count = topic_message_count.message_count\n                if self.config.benchmark_mode == BenchmarkMode.TIMELINE:\n                    target_count = 0\n                    for topic_message_timestamps in \\\n                            start_recording_request.topic_message_timestamps:\n                        if topic_message_timestamps.topic_name == topic_name:\n                            target_count = len(topic_message_timestamps.timestamps_ns)\n                            break\n                else:\n                    target_count = self.config.playback_message_buffer_size\n                self.assertTrue(\n                    recorded_count >= target_count,\n                    f'Not all messages were loaded ({topic_name}:{recorded_count}/{target_count})')\n\n        self.get_logger().info(\n            f'All {recorded_message_count} messages were sucessfully recorded')\n\n        self.pop_logger_name()\n\n    def get_topic_message_timestamps(self):\n        \"\"\"Get topic message timestamps from the service get_topic_message_timestamps.\"\"\"\n        get_topic_message_timestamps_client = self.create_service_client_blocking(\n            GetTopicMessageTimestamps, 'get_topic_message_timestamps')\n        get_topic_message_timestamps_request = GetTopicMessageTimestamps.Request()\n        if self.config.input_data_start_time != -1:\n            get_topic_message_timestamps_request.start_time_offset_ns = \\\n                int(self.config.input_data_start_time * (10**9))\n        if self.config.input_data_end_time != -1:\n            get_topic_message_timestamps_request.end_time_offset_ns = \\\n                int(self.config.input_data_end_time * (10**9))\n        get_topic_message_timestamps_future = get_topic_message_timestamps_client.call_async(\n            get_topic_message_timestamps_request)\n        get_topic_message_timestamps_response = self.get_service_response_from_future_blocking(\n            get_topic_message_timestamps_future, check_success=True)\n        return get_topic_message_timestamps_response.topic_message_timestamps\n\n    def benchmark_body(self, playback_message_count, target_freq) -> dict:\n        \"\"\"\n        Run benchmark test.\n\n        Parameters\n        ----------\n        playback_message_count :\n            The number of messages to be tested\n        target_freq :\n            Target test publisher rate\n\n        \"\"\"\n        # Create play_messages service\n        play_messages_client = self.create_service_client_blocking(PlayMessages, 'play_messages')\n\n        # Create and send monitor service requests\n        monitor_service_client_map = {}\n        monitor_service_future_map = {}\n        for monitor_info in self.config.monitor_info_list:\n            # Create a monitor service client\n            start_monitoring_client = self.create_service_client_blocking(\n                StartMonitoring, monitor_info.service_name)\n            if start_monitoring_client is None:\n                return\n\n            # Start monitoring messages\n            self.get_logger().info(\n                f'Requesting to monitor end messages from service \"{monitor_info.service_name}\".')\n            start_monitoring_request = StartMonitoring.Request()\n            start_monitoring_request.timeout = self.config.start_monitoring_service_timeout_sec\n            start_monitoring_request.message_count = playback_message_count\n            start_monitoring_request.revise_timestamps_as_message_ids = \\\n                self.config.revise_timestamps_as_message_ids\n            start_monitoring_future = start_monitoring_client.call_async(\n                start_monitoring_request)\n\n            monitor_service_client_map[monitor_info.service_name] = start_monitoring_client\n            monitor_service_future_map[monitor_info.service_name] = start_monitoring_future\n\n        # Start CPU profiler\n        if self.config.enable_cpu_profiler:\n            self._cpu_profiler.stop_profiling()\n            self._cpu_profiler.start_profiling(self.config.cpu_profiling_interval_sec)\n            self.get_logger().info('CPU profiling stared.')\n\n        # Start playing messages\n        self.get_logger().info(\n            f'Requesting to play messages in playback_mode = {self.config.benchmark_mode}.')\n        play_messages_request = PlayMessages.Request()\n        play_messages_request.playback_mode = self.config.benchmark_mode.value\n        play_messages_request.target_publisher_rate = target_freq\n        play_messages_request.message_count = playback_message_count\n        play_messages_request.enforce_publisher_rate = self.config.enforce_publisher_rate\n        play_messages_request.revise_timestamps_as_message_ids = \\\n            self.config.revise_timestamps_as_message_ids\n        play_messages_future = play_messages_client.call_async(play_messages_request)\n\n        # Watch playback node timeout\n        self.get_logger().info('Waiting for the playback service to finish.')\n        play_messages_response = self.get_service_response_from_future_blocking(\n            play_messages_future,\n            check_success=True,\n            timeout_sec=self.config.play_messages_service_future_timeout_sec)\n\n        start_timestamps = {}\n        for i in range(len(play_messages_response.timestamps.keys)):\n            key = play_messages_response.timestamps.keys[i]\n            start_timestamp = play_messages_response.timestamps.timestamps_ns[i]\n            start_timestamps[key] = start_timestamp\n\n        # Get end timestamps from all monitors\n        monitor_end_timestamps_map = {}\n        for monitor_info in self.config.monitor_info_list:\n            self.get_logger().info(\n                f'Waiting for the monitor service \"{monitor_info.service_name}\" to finish.')\n            monitor_response = self.get_service_response_from_future_blocking(\n                monitor_service_future_map[monitor_info.service_name])\n            end_timestamps = {}\n            for i in range(len(monitor_response.timestamps.keys)):\n                key = monitor_response.timestamps.keys[i]\n                end_timestamp = monitor_response.timestamps.timestamps_ns[i]\n                end_timestamps[key] = end_timestamp\n            monitor_end_timestamps_map[monitor_info.service_name] = end_timestamps\n\n        # Stop CPU profiler\n        if self.config.enable_cpu_profiler:\n            self._cpu_profiler.stop_profiling()\n            self.get_logger().info('CPU profiling stopped.')\n\n        # Calculate performance results\n        performance_results = {}\n        for monitor_info in self.config.monitor_info_list:\n            end_timestamps = monitor_end_timestamps_map[monitor_info.service_name]\n            if len(end_timestamps) == 0:\n                error_message = 'No messages were observed from the monitor node ' + \\\n                    monitor_info.service_name\n                self.get_logger().error(error_message)\n                raise RuntimeError(error_message)\n            for calculator in monitor_info.calculators:\n                performance_results.update(\n                    calculator.calculate_performance(start_timestamps, end_timestamps))\n\n        # Add CPU profiler results\n        if self.config.enable_cpu_profiler:\n            performance_results.update(self._cpu_profiler.get_results())\n\n        return performance_results\n\n    def determine_max_sustainable_framerate(self, test_func) -> float:\n        \"\"\"\n        Find the maximum sustainable test pulbisher framerate by using autotuner.\n\n        Parameters\n        ----------\n        test_func\n            The benchmark function to be tested\n\n        Returns\n        -------\n        float\n            The maximum sustainable test pulbisher framerate\n\n        \"\"\"\n        self.push_logger_name('Probing')\n\n        # Phase 1: Binary Search to identify interval\n\n        current_upper_freq = self.config.publisher_upper_frequency\n        current_lower_freq = self.config.publisher_lower_frequency\n\n        # Run a trial run to warm up the graph\n        try:\n            probe_freq = (current_upper_freq + current_lower_freq) / 2\n            message_count = ceil(\n                self.config.benchmark_duration *\n                self.config.binary_search_duration_fraction *\n                probe_freq)\n            self.get_logger().info(f'Starting the trial probe at {probe_freq} Hz')\n            probe_perf_results = test_func(message_count, probe_freq)\n            self.print_report(\n                probe_perf_results,\n                sub_heading=f'Trial Probe {probe_freq}Hz')\n        except Exception:\n            self.get_logger().info(\n                f'Ignoring an exception occured in the trial probe at {probe_freq} Hz')\n        finally:\n            self.get_logger().info(\n                f'Finished the first trial probe at {probe_freq} Hz')\n\n        # Continue binary search until the search window is small enough to justify a linear scan\n        while (\n            abs(current_upper_freq - current_lower_freq) >\n            self.config.binary_search_terminal_interval_width\n        ):\n            probe_freq = (current_upper_freq + current_lower_freq) / 2\n            self.get_logger().info(\n                f'Binary Search: Probing for max sustainable frequency at {probe_freq} Hz')\n\n            # Perform mini-benchmark at this probe frequency\n            message_count = ceil(\n                self.config.benchmark_duration *\n                self.config.binary_search_duration_fraction *\n                probe_freq)\n            probe_perf_results = test_func(message_count, probe_freq)\n            self.print_report(\n                probe_perf_results,\n                sub_heading=f'Throughput Search Probe {probe_freq}Hz')\n\n            # Check if this probe frequency was sustainable\n            first_monitor_perf = self.get_performance_results_of_first_monitor_calculator(\n                probe_perf_results)\n            if (first_monitor_perf[BasicPerformanceMetrics.MEAN_FRAME_RATE] >=\n                first_monitor_perf[BasicPerformanceMetrics.MEAN_PLAYBACK_FRAME_RATE] -\n                self.config.binary_search_acceptable_frame_rate_drop\n                ) and (\n                    first_monitor_perf[BasicPerformanceMetrics.NUM_MISSED_FRAMES] <=\n                    ceil(first_monitor_perf[BasicPerformanceMetrics.NUM_FRAMES_SENT] *\n                         self.config.binary_search_acceptable_frame_loss_fraction)\n            ):\n                current_lower_freq = probe_freq\n            else:\n                current_upper_freq = probe_freq\n\n        target_freq = current_lower_freq\n\n        # Phase 2: Linear scan through interval from low to high\n\n        # Increase probe frequency, until failures occur or ceiling is reached\n        while True:\n            probe_freq = target_freq + self.config.linear_scan_step_size\n            if probe_freq > self.config.publisher_upper_frequency:\n                break\n\n            self.get_logger().info(\n                f'Linear Scan: Probing for max sustainable frequency at {probe_freq} Hz')\n\n            # Perform mini-benchmark at this probe frequency\n            message_count = ceil(\n                self.config.benchmark_duration *\n                self.config.linear_scan_duration_fraction *\n                probe_freq)\n            probe_perf_results = test_func(message_count, probe_freq)\n            self.print_report(\n                probe_perf_results,\n                sub_heading=f'Throughput Search Probe {probe_freq}Hz')\n\n            # Check if this probe frequency was sustainable\n            first_monitor_perf = self.get_performance_results_of_first_monitor_calculator(\n                probe_perf_results)\n            if (first_monitor_perf[BasicPerformanceMetrics.MEAN_FRAME_RATE] >=\n                first_monitor_perf[BasicPerformanceMetrics.MEAN_PLAYBACK_FRAME_RATE] -\n                self.config.linear_scan_acceptable_frame_rate_drop\n                ) and (\n                first_monitor_perf[BasicPerformanceMetrics.NUM_MISSED_FRAMES] <=\n                ceil(first_monitor_perf[BasicPerformanceMetrics.NUM_FRAMES_SENT] *\n                     self.config.linear_scan_acceptable_frame_loss_fraction)\n            ):\n                target_freq = probe_freq\n            else:\n                # The new probe frequency is too high, so terminate the linear scan\n                break\n\n        self.get_logger().info(\n            f'Final predicted max sustainable frequency was {target_freq}Hz')\n\n        # Check if target frequency is at either lower or higher bound of range\n        BOUNDARY_LIMIT_EPSILON = 5  # (Hz)\n        if target_freq >= self.config.publisher_upper_frequency - BOUNDARY_LIMIT_EPSILON:\n            self.get_logger().warn(\n                f'Final playback framerate {target_freq} Hz is close to or above max framerate '\n                f'{self.config.publisher_upper_frequency} Hz used in search window. ')\n            self.get_logger().warn(\n                'Consider increasing this maximum!')\n        elif target_freq <= self.config.publisher_lower_frequency + BOUNDARY_LIMIT_EPSILON:\n            self.get_logger().warn(\n                f'Final playback framerate {target_freq} Hz is close to or below min framerate '\n                f'{self.config.publisher_lower_frequency} Hz used in search window. ')\n            self.get_logger().warn(\n                'Consider decreasing this minimum!')\n\n        self.pop_logger_name()\n        return target_freq\n\n    def pre_benchmark_hook(self):\n        \"\"\"Override for benchamrk setup.\"\"\"\n        pass\n\n    def post_benchmark_hook(self):\n        \"\"\"Override for benchmark cleanup.\"\"\"\n        pass\n\n    def run_benchmark(self):\n        \"\"\"\n        Run benchmarking.\n\n        Entry method for running benchmark method self.benchmark_body() under various\n        benchmark modes.\n        \"\"\"\n        self.get_logger().info(f'Starting {self.config.benchmark_name} Benchmark')\n\n        self.get_logger().info('Executing pre-benchmark setup')\n        self.pre_benchmark_hook()\n\n        # Prepare playback buffers\n        if self.config.enable_trial_buffer_preparation:\n            self.push_logger_name('Trial')\n            self.get_logger().info('Starting trial message buffering')\n            self.prepare_buffer()\n            self.pop_logger_name()\n        self.get_logger().info('Buffering test messages')\n        self.prepare_buffer()\n        self.get_logger().info('Finished buffering test messages')\n\n        self.get_logger().info(f'Running benchmark: mode={self.config.benchmark_mode}')\n        perf_results = {}\n        if self.config.benchmark_mode == BenchmarkMode.TIMELINE:\n            perf_results = self.run_benchmark_timeline_playback_mode()\n        elif (self.config.benchmark_mode == BenchmarkMode.LOOPING) or \\\n             (self.config.benchmark_mode == BenchmarkMode.SWEEPING):\n            perf_results = self.run_benchmark_looping_mode()\n\n        final_report = self.construct_final_report(perf_results)\n        self.print_report(final_report, sub_heading='Final Report')\n        self.export_report(final_report)\n\n    def run_benchmark_timeline_playback_mode(self) -> dict:\n        self.push_logger_name('Timeline')\n        playback_message_count = \\\n            int(self.config.benchmark_duration * self.config.publisher_upper_frequency)\n        perf_results = self.benchmark_body(\n            playback_message_count,\n            self.config.publisher_upper_frequency)\n        self.pop_logger_name()\n        return perf_results\n\n    def run_benchmark_looping_mode(self) -> dict:\n        \"\"\"\n        Run benchmarking.\n\n        This benchmark method aims to test the maximum output framerate for the\n        benchmark method self.benchmark_body().\n        \"\"\"\n        self.push_logger_name('Looping')\n\n        # Run Autotuner\n        self.get_logger().info('Running autotuner')\n        target_freq = self.determine_max_sustainable_framerate(self.benchmark_body)\n        self._peak_throughput_prediction = target_freq\n        self.get_logger().info(\n            'Finished autotuning. '\n            f'Running full-scale benchmark for predicted peak frame rate: {target_freq}')\n\n        playback_message_count = int(self.config.benchmark_duration * target_freq)\n\n        # Run benchmark iterations\n        self.reset_performance_calculators()\n        self._cpu_profiler.reset()\n        for i in range(self.config.test_iterations):\n            self.get_logger().info(f'Starting Iteration {i+1}')\n            performance_results = self.benchmark_body(\n                playback_message_count,\n                target_freq)\n            self.print_report(performance_results, sub_heading=f'{target_freq}Hz #{i+1}')\n\n        # Conclude performance measurements from the iteratoins\n        final_perf_results = {}\n        for monitor_info in self.config.monitor_info_list:\n            for calculator in monitor_info.calculators:\n                final_perf_results.update(calculator.conclude_performance())\n        # Conclude CPU profiler data\n        if self.config.enable_cpu_profiler:\n            final_perf_results.update(self._cpu_profiler.conclude_results())\n\n        # Run additional fixed rate test\n        self.push_logger_name('FixedRate')\n        additional_test_fixed_publisher_rates = \\\n            set(self.config.additional_fixed_publisher_rate_tests)\n        self.get_logger().info(\n            'Starting fixed publisher rate tests for: '\n            f'{additional_test_fixed_publisher_rates}')\n        for target_freq in additional_test_fixed_publisher_rates:\n            first_monitor_perf = self.get_performance_results_of_first_monitor_calculator(\n                final_perf_results)\n            mean_pub_fps = first_monitor_perf[BasicPerformanceMetrics.MEAN_PLAYBACK_FRAME_RATE]\n            if mean_pub_fps*1.05 < target_freq:\n                self.get_logger().info(\n                    f'Skipped testing the fixed publisher rate for {target_freq}fps '\n                    'as it is higher than the previously measured max sustainable '\n                    f'rate: {mean_pub_fps}')\n                continue\n            self.get_logger().info(\n                f'Testing fixed publisher rate at {target_freq}fps')\n            playback_message_count = int(self.config.benchmark_duration * target_freq)\n            performance_results = self.benchmark_body(\n                playback_message_count,\n                target_freq)\n            self.print_report(performance_results, sub_heading=f'Fixed {target_freq} FPS')\n\n            # Add the test result to the output metrics\n            final_perf_results[f'{target_freq}fps'] = performance_results\n\n        self.pop_logger_name()\n        return final_perf_results\n\n    def get_performance_results_of_first_monitor_calculator(self, performance_results):\n        \"\"\"Get dict that contains the first calculator's performance results.\"\"\"\n        calculator = self.config.monitor_info_list[0].calculators[0]\n        if 'report_prefix' in calculator.config and calculator.config['report_prefix']:\n            return performance_results[calculator.config['report_prefix']]\n        return performance_results\n\n    def reset_performance_calculators(self):\n        \"\"\"Reset the states of all performance calculators associated with all monitors.\"\"\"\n        for monitor_info in self.config.monitor_info_list:\n            for calculator in monitor_info.calculators:\n                calculator.reset()", ""]}
{"filename": "ros2_benchmark/ros2_benchmark/__init__.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Imports for ros2_benchmark module.\"\"\"", "\n\"\"\"Imports for ros2_benchmark module.\"\"\"\n\nfrom .basic_performance_calculator import BasicPerformanceCalculator, BasicPerformanceMetrics\nfrom .ros2_benchmark_config import BenchmarkMode, MonitorPerformanceCalculatorsInfo\nfrom .ros2_benchmark_config import ROS2BenchmarkConfig\nfrom .ros2_benchmark_test import BenchmarkMetadata, ROS2BenchmarkTest\nfrom .utils.image_utility import ImageResolution, Resolution\n\n__all__ = [", "\n__all__ = [\n    'BasicPerformanceCalculator',\n    'BasicPerformanceMetrics',\n    'BenchmarkMetadata',\n    'BenchmarkMode',\n    'ImageResolution',\n    'MonitorPerformanceCalculatorsInfo',\n    'Resolution',\n    'ROS2BenchmarkConfig',", "    'Resolution',\n    'ROS2BenchmarkConfig',\n    'ROS2BenchmarkTest',\n]\n"]}
{"filename": "ros2_benchmark/ros2_benchmark/basic_performance_calculator.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\nfrom enum import Enum", "\nfrom enum import Enum\nimport numbers\n\nimport numpy\nimport rclpy\n\n\nclass BasicPerformanceMetrics(Enum):\n    \"\"\"Basic performance metrics.\"\"\"\n\n    RECEIVED_DURATION = 'Delta between First & Last Received Frames (ms)'\n    MEAN_PLAYBACK_FRAME_RATE = 'Mean Playback Frame Rate (fps)'\n    MEAN_FRAME_RATE = 'Mean Frame Rate (fps)'\n    NUM_MISSED_FRAMES = '# of Missed Frames'\n    NUM_FRAMES_SENT = '# of Frames Sent'\n    FIRST_SENT_RECEIVED_LATENCY = 'First Sent to First Received Latency (ms)'\n    LAST_SENT_RECEIVED_LATENCY = 'Last Sent to Last Received Latency (ms)'\n    FIRST_INPUT_LATENCY = 'First Frame End-to-end Latency (ms)'\n    LAST_INPUT_LATENCY = 'Last Frame End-to-end Latency (ms)'\n    MAX_LATENCY = 'Max. End-to-End Latency (ms)'\n    MIN_LATENCY = 'Min. End-to-End Latency (ms)'\n    MEAN_LATENCY = 'Mean End-to-End Latency (ms)'\n    MAX_JITTER = 'Max. Frame-to-Frame Jitter (ms)'\n    MIN_JITTER = 'Min. Frame-to-Frame Jitter (ms)'\n    MEAN_JITTER = 'Mean Frame-to-Frame Jitter (ms)'\n    STD_DEV_JITTER = 'Frame-to-Frame Jitter Std. Deviation (ms)'", "class BasicPerformanceMetrics(Enum):\n    \"\"\"Basic performance metrics.\"\"\"\n\n    RECEIVED_DURATION = 'Delta between First & Last Received Frames (ms)'\n    MEAN_PLAYBACK_FRAME_RATE = 'Mean Playback Frame Rate (fps)'\n    MEAN_FRAME_RATE = 'Mean Frame Rate (fps)'\n    NUM_MISSED_FRAMES = '# of Missed Frames'\n    NUM_FRAMES_SENT = '# of Frames Sent'\n    FIRST_SENT_RECEIVED_LATENCY = 'First Sent to First Received Latency (ms)'\n    LAST_SENT_RECEIVED_LATENCY = 'Last Sent to Last Received Latency (ms)'\n    FIRST_INPUT_LATENCY = 'First Frame End-to-end Latency (ms)'\n    LAST_INPUT_LATENCY = 'Last Frame End-to-end Latency (ms)'\n    MAX_LATENCY = 'Max. End-to-End Latency (ms)'\n    MIN_LATENCY = 'Min. End-to-End Latency (ms)'\n    MEAN_LATENCY = 'Mean End-to-End Latency (ms)'\n    MAX_JITTER = 'Max. Frame-to-Frame Jitter (ms)'\n    MIN_JITTER = 'Min. Frame-to-Frame Jitter (ms)'\n    MEAN_JITTER = 'Mean Frame-to-Frame Jitter (ms)'\n    STD_DEV_JITTER = 'Frame-to-Frame Jitter Std. Deviation (ms)'", "\n\nclass BasicPerformanceCalculator():\n    \"\"\"Calculator that computes performance with basic metrics.\"\"\"\n\n    def __init__(self, config: dict = {}) -> None:\n        \"\"\"Initialize the calculator.\"\"\"\n        self.config = config\n        self._report_prefix = config.get('report_prefix', '')\n        self._message_key_match = config.get('message_key_match', False)\n        self._logger = None\n        self._perf_data_list = []\n\n    def set_logger(self, logger):\n        \"\"\"Set logger that enables to print log messages.\"\"\"\n        self._logger = logger\n\n    def get_logger(self):\n        \"\"\"Get logger for printing log messages.\"\"\"\n        if self._logger is not None:\n            return self._logger\n        return rclpy.logging.get_logger(self.__class__.__name__)\n\n    def get_info(self):\n        \"\"\"Return a dict containing information for loading this calculator class.\"\"\"\n        info = {}\n        info['module_name'] = self.__class__.__module__\n        info['class_name'] = self.__class__.__name__\n        info['config'] = self.config\n        return info\n\n    def reset(self):\n        \"\"\"Reset the calculator state.\"\"\"\n        self._perf_data_list.clear()\n\n    def calculate_performance(self,\n                              start_timestamps_ns: dict,\n                              end_timestamps_ns: dict) -> dict:\n        \"\"\"Calculate performance based on message start and end timestamps.\"\"\"\n        perf_data = {}\n        num_of_frame_sent = len(start_timestamps_ns)\n        num_of_frame_dropped = len(start_timestamps_ns) - len(end_timestamps_ns)\n\n        # BasicPerformanceMetrics.RECEIVED_DURATION\n        last_end_timestamp_ms = list(end_timestamps_ns.values())[-1] / 10**6\n        first_end_timestamp_ms = list(end_timestamps_ns.values())[0] / 10**6\n        received_duration_ms = last_end_timestamp_ms - first_end_timestamp_ms\n        perf_data[BasicPerformanceMetrics.RECEIVED_DURATION] = received_duration_ms\n\n        # BasicPerformanceMetrics.MEAN_PLAYBACK_FRAME_RATE\n        if (len(start_timestamps_ns) > 1):\n            last_sent_time_ms = list(start_timestamps_ns.values())[-1] / 10**6\n            first_sent_time_ms = list(start_timestamps_ns.values())[0] / 10**6\n            sent_duration_ms = last_sent_time_ms - first_sent_time_ms\n            perf_data[BasicPerformanceMetrics.MEAN_PLAYBACK_FRAME_RATE] = len(\n                start_timestamps_ns) / (sent_duration_ms / 1000.0)\n        else:\n            self.get_logger().warning(\n                'Could not compute MEAN_PLAYBACK_FRAME_RATE due to insufficient start '\n                f'timestamps received: {len(start_timestamps_ns)} was received')\n\n        # BasicPerformanceMetrics.MEAN_FRAME_RATE\n        if received_duration_ms > 0:\n            perf_data[BasicPerformanceMetrics.MEAN_FRAME_RATE] = len(\n                end_timestamps_ns) / (received_duration_ms / 1000.0)\n        else:\n            self.get_logger().warning(\n                'Could not compute MEAN_FRAME_RATE due to an invalid value of'\n                f'RECEIVED_DURATION = {received_duration_ms}')\n            self.get_logger().warning(\n                'This could be caused by insufficient timestamps received: '\n                f'#start_timestamps={len(start_timestamps_ns)} '\n                f'#end_timestamps = {len(end_timestamps_ns)}')\n\n        perf_data[BasicPerformanceMetrics.NUM_MISSED_FRAMES] = num_of_frame_dropped\n        perf_data[BasicPerformanceMetrics.NUM_FRAMES_SENT] = num_of_frame_sent\n\n        perf_data[BasicPerformanceMetrics.FIRST_SENT_RECEIVED_LATENCY] = \\\n            first_end_timestamp_ms - first_sent_time_ms\n        perf_data[BasicPerformanceMetrics.LAST_SENT_RECEIVED_LATENCY] = \\\n            last_end_timestamp_ms - last_sent_time_ms\n\n        if self._message_key_match:\n            # Calculate latency between sent and received messages\n            end_to_end_latencies_ms = []\n            for message_key, start_timestamp_ns in start_timestamps_ns.items():\n                if message_key in end_timestamps_ns:\n                    end_to_end_latencies_ms.append(\n                        (end_timestamps_ns[message_key] - start_timestamp_ns) / 10**6)\n\n            if len(end_to_end_latencies_ms) > 0:\n                perf_data[BasicPerformanceMetrics.FIRST_INPUT_LATENCY] = \\\n                    end_to_end_latencies_ms[0]\n                perf_data[BasicPerformanceMetrics.LAST_INPUT_LATENCY] = \\\n                    end_to_end_latencies_ms[-1]\n                perf_data[BasicPerformanceMetrics.MAX_LATENCY] = \\\n                    max(end_to_end_latencies_ms)\n                perf_data[BasicPerformanceMetrics.MIN_LATENCY] = \\\n                    min(end_to_end_latencies_ms)\n                perf_data[BasicPerformanceMetrics.MEAN_LATENCY] = \\\n                    sum(end_to_end_latencies_ms) / len(end_to_end_latencies_ms)\n            else:\n                self.get_logger().warning('No end-to-end latency data available.')\n\n        # Calculate frame-to-frame jitter if at least 3 valid end timestamps are received\n        if len(end_timestamps_ns) > 2:\n            np_end_timestamps_ms = (numpy.array(list(end_timestamps_ns.values())))/(10**6)\n            jitters = numpy.abs(numpy.diff(numpy.diff(np_end_timestamps_ms)))\n            perf_data[BasicPerformanceMetrics.MAX_JITTER] = float(numpy.max(jitters))\n            perf_data[BasicPerformanceMetrics.MIN_JITTER] = float(numpy.min(jitters))\n            perf_data[BasicPerformanceMetrics.MEAN_JITTER] = float(numpy.mean(jitters))\n            perf_data[BasicPerformanceMetrics.STD_DEV_JITTER] = float(numpy.std(\n                jitters))\n        else:\n            self.get_logger().warning(\n                'Received insufficient end timestamps for calculating frame-to-frame jitters.'\n                f'3 were needed but only {len(end_timestamps_ns)} timestamp(s) were received.')\n\n        # Store the current perf results to be concluded later\n        self._perf_data_list.append(perf_data)\n\n        if self._report_prefix != '':\n            return {self._report_prefix: perf_data}\n        return perf_data\n\n    def conclude_performance(self) -> dict:\n        \"\"\"Calculate final statistical performance outcome based on all results.\"\"\"\n        if len(self._perf_data_list) == 0:\n            self.get_logger().warn('No prior performance measurements to conclude')\n            return {}\n\n        MEAN_METRICS = [\n            BasicPerformanceMetrics.NUM_FRAMES_SENT,\n            BasicPerformanceMetrics.FIRST_INPUT_LATENCY,\n            BasicPerformanceMetrics.LAST_INPUT_LATENCY,\n            BasicPerformanceMetrics.FIRST_SENT_RECEIVED_LATENCY,\n            BasicPerformanceMetrics.LAST_SENT_RECEIVED_LATENCY,\n            BasicPerformanceMetrics.MEAN_LATENCY,\n            BasicPerformanceMetrics.NUM_MISSED_FRAMES,\n            BasicPerformanceMetrics.RECEIVED_DURATION,\n            BasicPerformanceMetrics.MEAN_FRAME_RATE,\n            BasicPerformanceMetrics.STD_DEV_JITTER,\n            BasicPerformanceMetrics.MEAN_PLAYBACK_FRAME_RATE,\n        ]\n        MAX_METRICS = [\n            BasicPerformanceMetrics.MAX_LATENCY,\n            BasicPerformanceMetrics.MAX_JITTER,\n            BasicPerformanceMetrics.MEAN_JITTER,\n        ]\n        MIN_METRICS = [\n            BasicPerformanceMetrics.MIN_LATENCY,\n            BasicPerformanceMetrics.MIN_JITTER,\n        ]\n\n        final_perf_data = {}\n        for metric in BasicPerformanceMetrics:\n            metric_value_list = [perf_data.get(metric, None) for perf_data in self._perf_data_list]\n            if not all(isinstance(value, numbers.Number) for value in metric_value_list):\n                continue\n\n            # Remove the best and the worst before concluding the metric\n            metric_value_list.remove(max(metric_value_list))\n            metric_value_list.remove(min(metric_value_list))\n\n            if metric in MEAN_METRICS:\n                final_perf_data[metric] = sum(metric_value_list)/len(metric_value_list)\n            elif metric in MAX_METRICS:\n                final_perf_data[metric] = max(metric_value_list)\n            elif metric in MIN_METRICS:\n                final_perf_data[metric] = min(metric_value_list)\n            else:\n                final_perf_data[metric] = 'INVALID VALUES: NO CONCLUDED METHOD ASSIGNED'\n\n        self.reset()\n        return final_perf_data", ""]}
{"filename": "ros2_benchmark/ros2_benchmark/ros2_benchmark_config.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\nfrom enum import Enum", "\nfrom enum import Enum\nimport importlib\nimport os\nimport sys\n\nimport yaml\n\nfrom .basic_performance_calculator import BasicPerformanceCalculator\nfrom .utils.image_utility import Resolution", "from .basic_performance_calculator import BasicPerformanceCalculator\nfrom .utils.image_utility import Resolution\n\nBUILTIN_ROS2_BENCHMARK_CONFIG_FILE = os.path.join(\n    os.path.dirname(__file__),\n    'default_ros2_benchmark_config.yaml')\n\n\nclass MonitorPerformanceCalculatorsInfo:\n    def __init__(self,\n                 service_name='start_monitoring0',\n                 calculators=[BasicPerformanceCalculator()]) -> None:\n        \"\"\"Initialize a monitor's performance calculators info.\"\"\"\n        self.service_name = service_name\n        self.calculators = calculators\n\n    def get_info(self):\n        \"\"\"Return a dict containing information for setting this monitor info object.\"\"\"\n        info = {}\n        info['service_name'] = self.service_name\n        info['calculators'] = []\n        for calculator in self.calculators:\n            info['calculators'].append(calculator.get_info())\n        return info", "class MonitorPerformanceCalculatorsInfo:\n    def __init__(self,\n                 service_name='start_monitoring0',\n                 calculators=[BasicPerformanceCalculator()]) -> None:\n        \"\"\"Initialize a monitor's performance calculators info.\"\"\"\n        self.service_name = service_name\n        self.calculators = calculators\n\n    def get_info(self):\n        \"\"\"Return a dict containing information for setting this monitor info object.\"\"\"\n        info = {}\n        info['service_name'] = self.service_name\n        info['calculators'] = []\n        for calculator in self.calculators:\n            info['calculators'].append(calculator.get_info())\n        return info", "\n\nclass BenchmarkMode(Enum):\n    \"\"\"\n    Benchmark modes supported in the framework.\n\n    The enum values must match what is defined in ros2_benchmark_interfaces::srv::PlayMessage\n    \"\"\"\n\n    TIMELINE = 0\n    LOOPING = 1\n    SWEEPING = 2", "\n\nclass ROS2BenchmarkConfig():\n    \"\"\"A class that holds configurations for ros2_benchmark.\"\"\"\n\n    __builtin_config_file_path = BUILTIN_ROS2_BENCHMARK_CONFIG_FILE\n\n    # It is only necessary to add a parameter to this map if we want\n    # to enable overriding such a parameter from an env variable and\n    # its type is not string (as env only supports string values).\n    __config_type_map = {\n        'revise_timestamps_as_message_ids': bool,\n        'enable_cpu_profiler': bool,\n        'publish_tf_messages_in_set_data': bool,\n        'publish_tf_static_messages_in_set_data': bool,\n        'load_data_in_real_time': bool,\n        'record_data_timeline': bool,\n        'enable_trial_buffer_preparation': bool,\n        'cpu_profiling_interval_sec': float,\n        'benchmark_duration': float,\n        'setup_service_client_timeout_sec': float,\n        'start_recording_service_timeout_sec': int,\n        'start_monitoring_service_timeout_sec': int,\n        'default_service_future_timeout_sec': float,\n        'set_data_service_future_timeout_sec': float,\n        'start_recording_service_future_timeout_sec': float,\n        'play_messages_service_future_timeout_sec': float,\n        'test_iterations': int,\n        'playback_message_buffer_size': int,\n        'publisher_upper_frequency': float,\n        'publisher_lower_frequency': float,\n        'enforce_publisher_rate': bool,\n        'binary_search_terminal_interval_width': float,\n        'binary_search_duration_fraction': float,\n        'binary_search_acceptable_frame_loss_fraction': float,\n        'binary_search_acceptable_frame_rate_drop': float,\n        'linear_scan_step_size': float,\n        'linear_scan_duration_fraction': float,\n        'linear_scan_acceptable_frame_loss_fraction': float,\n        'linear_scan_acceptable_frame_rate_drop': float,\n        'input_data_start_time': float,\n        'input_data_end_time': float\n    }\n\n    def __init__(self, config_file_path: str = '', *args, **kw):\n        \"\"\"Initialize default and given configs and apply to attribtues.\"\"\"\n        self.apply_to_attributes(dict(*args, **kw))\n\n        if config_file_path != '':\n            try:\n                self.apply_to_attributes(\n                    load_config_file(config_file_path)['ros2_benchmark_config'],\n                    override=False)\n            except (FileNotFoundError, yaml.YAMLError, TypeError) as error:\n                print('Failed to load a custom benchmark config file.')\n                raise error\n        try:\n            self.apply_to_attributes(\n                load_config_file(self.__builtin_config_file_path)['ros2_benchmark_config'],\n                override=False)\n        except (FileNotFoundError, yaml.YAMLError, TypeError) as error:\n            print('Failed to load a default benchmark config file.')\n            raise error\n\n    def apply_to_attributes(self, config_dict, override=True):\n        \"\"\"Apply the given configuration key-value pairs to instance attributes.\"\"\"\n        for key, value in config_dict.items():\n            if override is False and hasattr(self, key):\n                continue\n            if key == 'benchmark_mode' and isinstance(value, str):\n                if value not in BenchmarkMode.__members__:\n                    raise TypeError(f'Unknown benchmark mode: \"{value}\"')\n                setattr(self, key, BenchmarkMode.__members__[value])\n            elif key == 'monitor_info_list':\n                monitor_info_list = []\n                for monitor_info in value:\n                    if isinstance(monitor_info, MonitorPerformanceCalculatorsInfo):\n                        monitor_info_list.append(monitor_info)\n                        continue\n                    calculator_list = []\n                    for calculator in monitor_info['calculators']:\n                        calculator_module = importlib.import_module(calculator['module_name'])\n                        calculator_class = getattr(calculator_module, calculator['class_name'])\n                        calculator_config = calculator['config'] if 'config' in calculator else {}\n                        calculator_list.append(calculator_class(calculator_config))\n                    monitor_info_list.append(\n                        MonitorPerformanceCalculatorsInfo(\n                            monitor_info['service_name'],\n                            calculator_list))\n                setattr(self, key, monitor_info_list)\n            else:\n                if key in self.__config_type_map:\n                    value_type = self.__config_type_map[key]\n                    if value_type is bool and isinstance(value, str):\n                        if value.lower() in ['false', '0']:\n                            value = False\n                        else:\n                            value = True\n                    setattr(self, key, self.__config_type_map[key](value))\n                else:\n                    setattr(self, key, value)\n\n    def to_yaml_str(self):\n        \"\"\"Export all configurations as a YAML string.\"\"\"\n        yaml.add_representer(Resolution, Resolution.yaml_representer)\n\n        config_dict = {}\n        for key, value in self.__dict__.items():\n            if key == 'benchmark_mode':\n                config_dict[key] = str(value)\n            elif key == 'monitor_info_list':\n                monitor_info_list_export = []\n                for monitor_info in value:\n                    monitor_info_list_export.append(monitor_info.get_info())\n                config_dict[key] = monitor_info_list_export\n            else:\n                config_dict[key] = value\n        return yaml.dump({'ros2_benchmark_config': config_dict}, allow_unicode=True)", "\n\ndef load_config_file(config_file_path: str):\n    \"\"\"Load a benchmark configuration file and return its dict object.\"\"\"\n    try:\n        with open(config_file_path) as config_file:\n            return yaml.safe_load(config_file.read())\n    except FileNotFoundError as error:\n        print('Could not find benchmark config file at '\n              f'\"{config_file_path}\".', sys.stderr)\n        raise error\n    except yaml.YAMLError as error:\n        print('Invalid benchmark configs detected in '\n              f'\"{config_file_path}\".', sys.stderr)\n        raise error", ""]}
{"filename": "ros2_benchmark/ros2_benchmark/utils/cpu_profiler.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2021-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"CPU profiler class to measure performance of benchmark tests.\"\"\"", "\n\"\"\"CPU profiler class to measure performance of benchmark tests.\"\"\"\n\nfrom enum import Enum\nimport numbers\nfrom pathlib import Path\nfrom threading import Thread\n\nimport numpy as np\nimport psutil", "import numpy as np\nimport psutil\n\nfrom .profiler import Profiler\n\n\nclass CPUProfilingMetrics(Enum):\n    \"\"\"Metrics for CPU profiling.\"\"\"\n\n    MAX_CPU_UTIL = 'Max. CPU Util. (%)'\n    MIN_CPU_UTIL = 'Min. CPU Util. (%)'\n    MEAN_CPU_UTIL = 'Mean CPU Util. (%)'\n    STD_DEV_CPU_UTIL = 'Std. Deviation CPU Util. (%)'\n    BASELINE_CPU_UTIL = 'Baseline CPU Util. (%)'", "\n\nclass CPUProfiler(Profiler):\n    \"\"\"CPU profiler class to measure CPU performance of benchmark tests.\"\"\"\n\n    def __init__(self):\n        \"\"\"Construct CPU profiler.\"\"\"\n        super().__init__()\n\n    def start_profiling(self, interval: float = 1.0) -> Path:\n        \"\"\"\n        Start CPU profiling thread to keep track of performance metrics.\n\n        Parameters\n        ----------\n        interval: float\n            The interval between measurements, in seconds\n\n        \"\"\"\n        super().start_profiling()\n\n        # While the is_running flag is true, log CPU usage\n        def psutil_log():\n            with open(self._log_file_path, 'w+') as logfile:\n                while self._is_running:\n                    logfile.write(\n                        f'{psutil.cpu_percent(interval=interval, percpu=True)}\\n')\n\n        self.psutil_thread = Thread(target=psutil_log)\n        self.psutil_thread.start()\n\n        return self._log_file_path\n\n    def stop_profiling(self):\n        \"\"\"Stop profiling.\"\"\"\n        if self._is_running:\n            super().stop_profiling()\n            # Wait for thread to stop\n            self.psutil_thread.join()\n\n    @staticmethod\n    def get_current_cpu_usage():\n        \"\"\"Return current CPU usage.\"\"\"\n        return np.mean(psutil.cpu_percent(interval=1.0, percpu=True))\n\n    def get_results(self, log_file_path=None) -> dict:\n        \"\"\"Return CPU profiling results.\"\"\"\n        assert not self._is_running, 'Cannot collect results until profiler has been stopped!'\n\n        log_file_path = self._log_file_path if log_file_path is None else log_file_path\n        assert self._log_file_path is not None, 'No log file for reading CPU  profiling results.'\n\n        profile_data = {}\n        with open(log_file_path) as logfile:\n            cpu_values = []\n            for line in logfile.readlines():\n                # Remove brackets from line before splitting entries by comma\n                cpu_values.append(np.mean([float(v)\n                                  for v in line[1:-2].split(',')]))\n\n            cpu_values = np.array(cpu_values)\n            profile_data[CPUProfilingMetrics.MAX_CPU_UTIL] = np.max(cpu_values)\n            profile_data[CPUProfilingMetrics.MIN_CPU_UTIL] = np.min(cpu_values)\n            profile_data[CPUProfilingMetrics.MEAN_CPU_UTIL] = np.mean(cpu_values)\n            profile_data[CPUProfilingMetrics.STD_DEV_CPU_UTIL] = np.std(cpu_values)\n            profile_data[CPUProfilingMetrics.BASELINE_CPU_UTIL] = cpu_values[0]\n\n        self._profile_data_list.append(profile_data)\n\n        return profile_data\n\n    def reset(self):\n        \"\"\"Reset the profiler state.\"\"\"\n        self._profile_data_list.clear()\n        return\n\n    def conclude_results(self) -> dict:\n        \"\"\"Conclude final profiling outcome based on all previous results.\"\"\"\n        if len(self._profile_data_list) == 0:\n            self.get_logger().warn('No prior profile data to conclude')\n            return {}\n\n        MEAN_METRICS = [\n            CPUProfilingMetrics.MEAN_CPU_UTIL,\n            CPUProfilingMetrics.STD_DEV_CPU_UTIL,\n            CPUProfilingMetrics.BASELINE_CPU_UTIL\n        ]\n        MAX_METRICS = [\n            CPUProfilingMetrics.MAX_CPU_UTIL\n        ]\n        MIN_METRICS = [\n            CPUProfilingMetrics.MIN_CPU_UTIL\n        ]\n\n        final_profile_data = {}\n        for metric in CPUProfilingMetrics:\n            metric_value_list = [profile_data.get(metric, None) for\n                                 profile_data in self._profile_data_list]\n            if not all(isinstance(value, numbers.Number) for value in metric_value_list):\n                continue\n\n            # Remove the best and the worst before concluding the metric\n            metric_value_list.remove(max(metric_value_list))\n            metric_value_list.remove(min(metric_value_list))\n\n            if metric in MEAN_METRICS:\n                final_profile_data[metric] = sum(metric_value_list)/len(metric_value_list)\n            elif metric in MAX_METRICS:\n                final_profile_data[metric] = max(metric_value_list)\n            elif metric in MIN_METRICS:\n                final_profile_data[metric] = min(metric_value_list)\n            else:\n                final_profile_data[metric] = 'INVALID VALUES: NO CONCLUDED METHOD ASSIGNED'\n\n        self.reset()\n        return final_profile_data", ""]}
{"filename": "ros2_benchmark/ros2_benchmark/utils/image_utility.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\n\nclass Resolution:\n    \"\"\"Generic resolution value holder.\"\"\"\n\n    def __init__(self, width, height, name=''):\n        \"\"\"Initialize the resolution.\"\"\"\n        self._dict = {}\n        self._dict['width'] = int(width)\n        self._dict['height'] = int(height)\n        self._dict['name'] = name\n\n    def __str__(self) -> str:\n        out_str = f'({self[\"width\"]},{self[\"height\"]})'\n        if self['name']:\n            out_str = f'{self[\"name\"]} {out_str}'\n        return out_str\n\n    def __setitem__(self, key, value):\n        self._dict[key] = value\n\n    def __getitem__(self, key):\n        return self._dict[key]\n\n    def __repr__(self):\n        return f'Resolution({self[\"width\"]}, {self[\"height\"]}, {self[\"name\"]})'\n\n    def yaml_representer(dumper, data):\n        \"\"\"Support dumping a Resolution object in YAML.\"\"\"\n        return dumper.represent_scalar('tag:yaml.org,2002:str', repr(data))", "\n\nclass Resolution:\n    \"\"\"Generic resolution value holder.\"\"\"\n\n    def __init__(self, width, height, name=''):\n        \"\"\"Initialize the resolution.\"\"\"\n        self._dict = {}\n        self._dict['width'] = int(width)\n        self._dict['height'] = int(height)\n        self._dict['name'] = name\n\n    def __str__(self) -> str:\n        out_str = f'({self[\"width\"]},{self[\"height\"]})'\n        if self['name']:\n            out_str = f'{self[\"name\"]} {out_str}'\n        return out_str\n\n    def __setitem__(self, key, value):\n        self._dict[key] = value\n\n    def __getitem__(self, key):\n        return self._dict[key]\n\n    def __repr__(self):\n        return f'Resolution({self[\"width\"]}, {self[\"height\"]}, {self[\"name\"]})'\n\n    def yaml_representer(dumper, data):\n        \"\"\"Support dumping a Resolution object in YAML.\"\"\"\n        return dumper.represent_scalar('tag:yaml.org,2002:str', repr(data))", "\n\nclass ImageResolution:\n    \"\"\"Common image resolutions.\"\"\"\n\n    QUARTER_HD = Resolution(960, 540, 'Quarter HD')\n    VGA = Resolution(640, 480, 'VGA')\n    WVGA = Resolution(720, 480, 'WVGA')\n    HD = Resolution(1280, 720, 'HD')\n    FULL_HD = Resolution(1920, 1080, 'Full HD')\n    FOUR_K = Resolution(3840, 2160, '4K')", ""]}
{"filename": "ros2_benchmark/ros2_benchmark/utils/nsys_utility.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\nimport datetime", "\nimport datetime\nfrom inspect import signature\nimport platform\n\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.conditions import IfCondition\nfrom launch.substitutions import LaunchConfiguration\n\n\nclass NsysUtility():\n    \"\"\"Utilities for enabling Nsight System profiling.\"\"\"\n\n    @staticmethod\n    def generate_launch_args():\n        \"\"\"\n        Generate launch args for nsight systme profiling.\n\n        Usage Example: launch_test <path to your script>\n                       enable_nsys:=<true, false>\n                       nsys_profile_name:=<your profile output name>\n                       nsys_profile_flags:=<your profiling flags>\n        \"\"\"\n        return [\n            DeclareLaunchArgument('enable_nsys', default_value='false',\n                                  description='Enable nsys profiling'),\n            DeclareLaunchArgument('nsys_profile_name', default_value='',\n                                  description='Label to append for nsys profile output'),\n            DeclareLaunchArgument('nsys_profile_flags', default_value='--trace=osrt,nvtx,cuda',\n                                  description='Flags for nsys profile')\n        ]\n\n    @staticmethod\n    def generate_nsys_prefix(context):\n        \"\"\"Generate prefi for nsight systme profiling.\"\"\"\n        enable_nsys = IfCondition(LaunchConfiguration(\n            'enable_nsys')).evaluate(context)\n        nsys_profile_name = LaunchConfiguration(\n            'nsys_profile_name').perform(context)\n        nsys_profile_flags = LaunchConfiguration(\n            'nsys_profile_flags').perform(context)\n\n        container_prefix = ''\n        if enable_nsys:\n            if(not nsys_profile_name):\n                current_time = datetime.datetime.now(datetime.timezone.utc).\\\n                               strftime('%Y-%m-%dT%H:%M:%SZ')\n                nsys_profile_name = f'profile_{platform.machine()}_{current_time}'\n            container_prefix = f'nsys profile {nsys_profile_flags} -o {nsys_profile_name}'\n        return (enable_nsys, container_prefix)\n\n    @staticmethod\n    def launch_setup_wrapper(context, launch_setup):\n        \"\"\"Invoke the launch_setup method with nsys parameters for ComposableNodeContainer.\"\"\"\n        enable_nsys, container_prefix = NsysUtility.generate_nsys_prefix(context)\n        launch_setup_parameters = signature(launch_setup).parameters\n        if (not all(param in launch_setup_parameters for param in\n                    ['container_prefix', 'container_sigterm_timeout'])):\n            if enable_nsys:\n                raise RuntimeError(\n                    'Incorrect launch_setup signature. '\n                    'When Nsys is enbaled, the signature must be: '\n                    'def launch_setup(container_prefix, container_sigterm_timeout)')\n            else:\n                return launch_setup()\n        container_sigterm_timeout = '1000' if enable_nsys else '5'\n        return launch_setup(\n            container_prefix=container_prefix,\n            container_sigterm_timeout=container_sigterm_timeout)", "\n\nclass NsysUtility():\n    \"\"\"Utilities for enabling Nsight System profiling.\"\"\"\n\n    @staticmethod\n    def generate_launch_args():\n        \"\"\"\n        Generate launch args for nsight systme profiling.\n\n        Usage Example: launch_test <path to your script>\n                       enable_nsys:=<true, false>\n                       nsys_profile_name:=<your profile output name>\n                       nsys_profile_flags:=<your profiling flags>\n        \"\"\"\n        return [\n            DeclareLaunchArgument('enable_nsys', default_value='false',\n                                  description='Enable nsys profiling'),\n            DeclareLaunchArgument('nsys_profile_name', default_value='',\n                                  description='Label to append for nsys profile output'),\n            DeclareLaunchArgument('nsys_profile_flags', default_value='--trace=osrt,nvtx,cuda',\n                                  description='Flags for nsys profile')\n        ]\n\n    @staticmethod\n    def generate_nsys_prefix(context):\n        \"\"\"Generate prefi for nsight systme profiling.\"\"\"\n        enable_nsys = IfCondition(LaunchConfiguration(\n            'enable_nsys')).evaluate(context)\n        nsys_profile_name = LaunchConfiguration(\n            'nsys_profile_name').perform(context)\n        nsys_profile_flags = LaunchConfiguration(\n            'nsys_profile_flags').perform(context)\n\n        container_prefix = ''\n        if enable_nsys:\n            if(not nsys_profile_name):\n                current_time = datetime.datetime.now(datetime.timezone.utc).\\\n                               strftime('%Y-%m-%dT%H:%M:%SZ')\n                nsys_profile_name = f'profile_{platform.machine()}_{current_time}'\n            container_prefix = f'nsys profile {nsys_profile_flags} -o {nsys_profile_name}'\n        return (enable_nsys, container_prefix)\n\n    @staticmethod\n    def launch_setup_wrapper(context, launch_setup):\n        \"\"\"Invoke the launch_setup method with nsys parameters for ComposableNodeContainer.\"\"\"\n        enable_nsys, container_prefix = NsysUtility.generate_nsys_prefix(context)\n        launch_setup_parameters = signature(launch_setup).parameters\n        if (not all(param in launch_setup_parameters for param in\n                    ['container_prefix', 'container_sigterm_timeout'])):\n            if enable_nsys:\n                raise RuntimeError(\n                    'Incorrect launch_setup signature. '\n                    'When Nsys is enbaled, the signature must be: '\n                    'def launch_setup(container_prefix, container_sigterm_timeout)')\n            else:\n                return launch_setup()\n        container_sigterm_timeout = '1000' if enable_nsys else '5'\n        return launch_setup(\n            container_prefix=container_prefix,\n            container_sigterm_timeout=container_sigterm_timeout)", ""]}
{"filename": "ros2_benchmark/ros2_benchmark/utils/__init__.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n", ""]}
{"filename": "ros2_benchmark/ros2_benchmark/utils/ros2_utility.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\nimport time", "\nimport time\n\nimport rclpy\n\n\nclass ClientUtility:\n    \"\"\"A class for hosting utility methods for ROS 2 serevice clients.\"\"\"\n\n    @staticmethod\n    def create_service_client_blocking(node, service_type, service_name, timeout_sec):\n        \"\"\"Create a service client and wait for it to be available.\"\"\"\n        service_client = node.create_client(service_type, service_name)\n        start_time = time.time()\n        while not service_client.wait_for_service(timeout_sec=1):\n            node.get_logger().info(\n                f'{service_name} service is not available yet, waiting...')\n            if (time.time() - start_time) > timeout_sec:\n                node.get_logger().info(\n                    f'Creating {service_name} service client timed out')\n                return None\n        return service_client\n\n    @staticmethod\n    def get_service_response_from_future_blocking(node, future, timeout_sec):\n        \"\"\"Block and wait for a service future to return.\"\"\"\n        start_time = time.time()\n        while not future.done():\n            rclpy.spin_once(node)\n            if (time.time() - start_time) > timeout_sec:\n                node.get_logger().info(\n                    f'Waiting for a service future timed out ({timeout_sec}s)')\n                return None\n        return future.result()", ""]}
{"filename": "ros2_benchmark/ros2_benchmark/utils/profiler.py", "chunked_list": ["# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES\n# Copyright (c) 2021-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Profiler base class to measure the performance of benchmark tests.\"\"\"", "\n\"\"\"Profiler base class to measure the performance of benchmark tests.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nimport os\n\n\nclass Profiler(ABC):\n    \"\"\"Profiler base class to measure the performance of benchmark tests.\"\"\"\n\n    DEFAILT_LOG_DIR = '/tmp'\n\n    @abstractmethod\n    def __init__(self):\n        \"\"\"Construct profiler.\"\"\"\n        self._is_running = False\n\n        # Logfile path is generated once start_profiling() is called\n        self._log_file_path = None\n\n        self._profile_data_list = []\n\n    @abstractmethod\n    def start_profiling(self, log_dir=DEFAILT_LOG_DIR) -> None:\n        \"\"\"\n        Run profiling program to keep track of performance metrics.\n\n        Parameters\n        ----------\n        log_dir\n            Path to write the logs to\n\n        \"\"\"\n        assert not self._is_running, 'Profiler has already been started!'\n        self._is_running = True\n\n        # Create log file folders if they don't exist already\n        os.makedirs(log_dir, exist_ok=True)\n\n        self._log_file_path = os.path.join(\n            log_dir,\n            f'{type(self).__name__}_{datetime.timestamp(datetime.now())}.log')\n\n        return self._log_file_path\n\n    @abstractmethod\n    def stop_profiling(self) -> None:\n        \"\"\"Stop profiling.\"\"\"\n        self._is_running = False\n\n    @abstractmethod\n    def get_results(self, log_file_path=None) -> dict:\n        \"\"\"Return profiling results.\"\"\"\n        return {}\n\n    @abstractmethod\n    def reset(self):\n        \"\"\"Reset the profiler state.\"\"\"\n        self._profile_data_list.clear()\n        return\n\n    @abstractmethod\n    def conclude_results(self) -> dict:\n        \"\"\"Conclude final profiling outcome based on all previous results.\"\"\"\n        return {}", "class Profiler(ABC):\n    \"\"\"Profiler base class to measure the performance of benchmark tests.\"\"\"\n\n    DEFAILT_LOG_DIR = '/tmp'\n\n    @abstractmethod\n    def __init__(self):\n        \"\"\"Construct profiler.\"\"\"\n        self._is_running = False\n\n        # Logfile path is generated once start_profiling() is called\n        self._log_file_path = None\n\n        self._profile_data_list = []\n\n    @abstractmethod\n    def start_profiling(self, log_dir=DEFAILT_LOG_DIR) -> None:\n        \"\"\"\n        Run profiling program to keep track of performance metrics.\n\n        Parameters\n        ----------\n        log_dir\n            Path to write the logs to\n\n        \"\"\"\n        assert not self._is_running, 'Profiler has already been started!'\n        self._is_running = True\n\n        # Create log file folders if they don't exist already\n        os.makedirs(log_dir, exist_ok=True)\n\n        self._log_file_path = os.path.join(\n            log_dir,\n            f'{type(self).__name__}_{datetime.timestamp(datetime.now())}.log')\n\n        return self._log_file_path\n\n    @abstractmethod\n    def stop_profiling(self) -> None:\n        \"\"\"Stop profiling.\"\"\"\n        self._is_running = False\n\n    @abstractmethod\n    def get_results(self, log_file_path=None) -> dict:\n        \"\"\"Return profiling results.\"\"\"\n        return {}\n\n    @abstractmethod\n    def reset(self):\n        \"\"\"Reset the profiler state.\"\"\"\n        self._profile_data_list.clear()\n        return\n\n    @abstractmethod\n    def conclude_results(self) -> dict:\n        \"\"\"Conclude final profiling outcome based on all previous results.\"\"\"\n        return {}", ""]}
