{"filename": "toy/toy.py", "chunked_list": ["from copy import deepcopy\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm, ticker\nfrom matplotlib.colors import LogNorm\nfrom tqdm import tqdm\nfrom scipy.optimize import minimize, Bounds, minimize_scalar\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time", "import numpy as np\nimport time\nimport torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport seaborn as sns\nimport sys\n\n################################################################################\n#", "################################################################################\n#\n# Define the Optimization Problem\n#\n################################################################################\nLOWER = 0.000005\n\n\nclass Toy(nn.Module):\n\n    def __init__(self):\n        super(Toy, self).__init__()\n        self.centers = torch.Tensor([[-3.0, 0], [3.0, 0]])\n\n    def forward(self, x, compute_grad=False):\n        x1 = x[0]\n        x2 = x[1]\n\n        f1 = torch.clamp((0.5 * (-x1 - 7) - torch.tanh(-x2)).abs(), LOWER).log() + 6\n        f2 = torch.clamp((0.5 * (-x1 + 3) + torch.tanh(-x2) + 2).abs(), LOWER).log() + 6\n        c1 = torch.clamp(torch.tanh(x2 * 0.5), 0)\n\n        f1_sq = ((-x1 + 7).pow(2) + 0.1 * (-x2 - 8).pow(2)) / 10 - 20\n        f2_sq = ((-x1 - 7).pow(2) + 0.1 * (-x2 - 8).pow(2)) / 10 - 20\n        c2 = torch.clamp(torch.tanh(-x2 * 0.5), 0)\n\n        f1 = f1 * c1 + f1_sq * c2\n        f2 = f2 * c1 + f2_sq * c2\n\n        f = torch.tensor([f1, f2])\n        if compute_grad:\n            g11 = torch.autograd.grad(f1, x1, retain_graph=True)[0].item()\n            g12 = torch.autograd.grad(f1, x2, retain_graph=True)[0].item()\n            g21 = torch.autograd.grad(f2, x1, retain_graph=True)[0].item()\n            g22 = torch.autograd.grad(f2, x2, retain_graph=True)[0].item()\n            g = torch.Tensor([[g11, g21], [g12, g22]])\n            return f, g\n        else:\n            return f\n\n    def batch_forward(self, x):\n        x1 = x[:, 0]\n        x2 = x[:, 1]\n\n        f1 = torch.clamp((0.5 * (-x1 - 7) - torch.tanh(-x2)).abs(), LOWER).log() + 6\n        f2 = torch.clamp((0.5 * (-x1 + 3) + torch.tanh(-x2) + 2).abs(), LOWER).log() + 6\n        c1 = torch.clamp(torch.tanh(x2 * 0.5), 0)\n\n        f1_sq = ((-x1 + 7).pow(2) + 0.1 * (-x2 - 8).pow(2)) / 10 - 20\n        f2_sq = ((-x1 - 7).pow(2) + 0.1 * (-x2 - 8).pow(2)) / 10 - 20\n        c2 = torch.clamp(torch.tanh(-x2 * 0.5), 0)\n\n        f1 = f1 * c1 + f1_sq * c2\n        f2 = f2 * c1 + f2_sq * c2\n\n        f = torch.cat([f1.view(-1, 1), f2.view(-1, 1)], -1)\n        return f", "class Toy(nn.Module):\n\n    def __init__(self):\n        super(Toy, self).__init__()\n        self.centers = torch.Tensor([[-3.0, 0], [3.0, 0]])\n\n    def forward(self, x, compute_grad=False):\n        x1 = x[0]\n        x2 = x[1]\n\n        f1 = torch.clamp((0.5 * (-x1 - 7) - torch.tanh(-x2)).abs(), LOWER).log() + 6\n        f2 = torch.clamp((0.5 * (-x1 + 3) + torch.tanh(-x2) + 2).abs(), LOWER).log() + 6\n        c1 = torch.clamp(torch.tanh(x2 * 0.5), 0)\n\n        f1_sq = ((-x1 + 7).pow(2) + 0.1 * (-x2 - 8).pow(2)) / 10 - 20\n        f2_sq = ((-x1 - 7).pow(2) + 0.1 * (-x2 - 8).pow(2)) / 10 - 20\n        c2 = torch.clamp(torch.tanh(-x2 * 0.5), 0)\n\n        f1 = f1 * c1 + f1_sq * c2\n        f2 = f2 * c1 + f2_sq * c2\n\n        f = torch.tensor([f1, f2])\n        if compute_grad:\n            g11 = torch.autograd.grad(f1, x1, retain_graph=True)[0].item()\n            g12 = torch.autograd.grad(f1, x2, retain_graph=True)[0].item()\n            g21 = torch.autograd.grad(f2, x1, retain_graph=True)[0].item()\n            g22 = torch.autograd.grad(f2, x2, retain_graph=True)[0].item()\n            g = torch.Tensor([[g11, g21], [g12, g22]])\n            return f, g\n        else:\n            return f\n\n    def batch_forward(self, x):\n        x1 = x[:, 0]\n        x2 = x[:, 1]\n\n        f1 = torch.clamp((0.5 * (-x1 - 7) - torch.tanh(-x2)).abs(), LOWER).log() + 6\n        f2 = torch.clamp((0.5 * (-x1 + 3) + torch.tanh(-x2) + 2).abs(), LOWER).log() + 6\n        c1 = torch.clamp(torch.tanh(x2 * 0.5), 0)\n\n        f1_sq = ((-x1 + 7).pow(2) + 0.1 * (-x2 - 8).pow(2)) / 10 - 20\n        f2_sq = ((-x1 - 7).pow(2) + 0.1 * (-x2 - 8).pow(2)) / 10 - 20\n        c2 = torch.clamp(torch.tanh(-x2 * 0.5), 0)\n\n        f1 = f1 * c1 + f1_sq * c2\n        f2 = f2 * c1 + f2_sq * c2\n\n        f = torch.cat([f1.view(-1, 1), f2.view(-1, 1)], -1)\n        return f", "\n\n################################################################################\n#\n# Plot Utils\n#\n################################################################################\n\n\ndef plotme(F, all_traj=None, xl=11):\n    n = 500\n    x = np.linspace(-xl, xl, n)\n    y = np.linspace(-xl, xl, n)\n    X, Y = np.meshgrid(x, y)\n\n    Xs = torch.Tensor(np.transpose(np.array([list(X.flat), list(Y.flat)]))).double()\n    Ys = F.batch_forward(Xs)\n\n    colormaps = {\n        \"sgd\": \"tab:blue\",\n        \"pcgrad\": \"tab:orange\",\n        \"mgd\": \"tab:cyan\",\n        \"cagrad\": \"tab:red\",\n        \"sdmgrad\": \"tab:green\"\n    }\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(131)\n    c = plt.contour(X, Y, Ys[:, 0].view(n, n))\n    if all_traj is not None:\n        for i, (k, v) in enumerate(all_traj.items()):\n            plt.plot(all_traj[k][:, 0], all_traj[k][:, 1], '--', c=colormaps[k], label=k)\n    plt.title(\"L1(x)\")\n\n    plt.subplot(132)\n    c = plt.contour(X, Y, Ys[:, 1].view(n, n))\n    if all_traj is not None:\n        for i, (k, v) in enumerate(all_traj.items()):\n            plt.plot(all_traj[k][:, 0], all_traj[k][:, 1], '--', c=colormaps[k], label=k)\n    plt.title(\"L2(x)\")\n\n    plt.subplot(133)\n    c = plt.contour(X, Y, Ys.mean(1).view(n, n))\n    if all_traj is not None:\n        for i, (k, v) in enumerate(all_traj.items()):\n            plt.plot(all_traj[k][:, 0], all_traj[k][:, 1], '--', c=colormaps[k], label=k)\n    plt.legend()\n    plt.title(\"0.5*(L1(x)+L2(x))\")\n\n    plt.tight_layout()\n    plt.savefig(f\"toy_ct.png\")", "\ndef plotme(F, all_traj=None, xl=11):\n    n = 500\n    x = np.linspace(-xl, xl, n)\n    y = np.linspace(-xl, xl, n)\n    X, Y = np.meshgrid(x, y)\n\n    Xs = torch.Tensor(np.transpose(np.array([list(X.flat), list(Y.flat)]))).double()\n    Ys = F.batch_forward(Xs)\n\n    colormaps = {\n        \"sgd\": \"tab:blue\",\n        \"pcgrad\": \"tab:orange\",\n        \"mgd\": \"tab:cyan\",\n        \"cagrad\": \"tab:red\",\n        \"sdmgrad\": \"tab:green\"\n    }\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(131)\n    c = plt.contour(X, Y, Ys[:, 0].view(n, n))\n    if all_traj is not None:\n        for i, (k, v) in enumerate(all_traj.items()):\n            plt.plot(all_traj[k][:, 0], all_traj[k][:, 1], '--', c=colormaps[k], label=k)\n    plt.title(\"L1(x)\")\n\n    plt.subplot(132)\n    c = plt.contour(X, Y, Ys[:, 1].view(n, n))\n    if all_traj is not None:\n        for i, (k, v) in enumerate(all_traj.items()):\n            plt.plot(all_traj[k][:, 0], all_traj[k][:, 1], '--', c=colormaps[k], label=k)\n    plt.title(\"L2(x)\")\n\n    plt.subplot(133)\n    c = plt.contour(X, Y, Ys.mean(1).view(n, n))\n    if all_traj is not None:\n        for i, (k, v) in enumerate(all_traj.items()):\n            plt.plot(all_traj[k][:, 0], all_traj[k][:, 1], '--', c=colormaps[k], label=k)\n    plt.legend()\n    plt.title(\"0.5*(L1(x)+L2(x))\")\n\n    plt.tight_layout()\n    plt.savefig(f\"toy_ct.png\")", "\n\ndef plot3d(F, xl=11):\n    n = 500\n    x = np.linspace(-xl, xl, n)\n    y = np.linspace(-xl, xl, n)\n    X, Y = np.meshgrid(x, y)\n\n    Xs = torch.Tensor(np.transpose(np.array([list(X.flat), list(Y.flat)]))).double()\n    Ys = F.batch_forward(Xs)\n    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\n    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.grid(False)\n    Yv = Ys.mean(1).view(n, n)\n    surf = ax.plot_surface(X, Y, Yv.numpy(), cmap=cm.viridis)\n    print(Ys.mean(1).min(), Ys.mean(1).max())\n\n    ax.set_zticks([-16, -8, 0, 8])\n    ax.set_zlim(-20, 10)\n\n    ax.set_xticks([-10, 0, 10])\n    ax.set_yticks([-10, 0, 10])\n    for tick in ax.xaxis.get_major_ticks():\n        tick.label.set_fontsize(15)\n    for tick in ax.yaxis.get_major_ticks():\n        tick.label.set_fontsize(15)\n    for tick in ax.zaxis.get_major_ticks():\n        tick.label.set_fontsize(15)\n\n    ax.view_init(25)\n    plt.tight_layout()\n    plt.savefig(f\"3d-obj.png\", dpi=1000)", "\n\ndef plot_contour(F, task=1, traj=None, xl=11, plotbar=False, name=\"tmp\"):\n    n = 500\n    x = np.linspace(-xl, xl, n)\n    y = np.linspace(-xl, xl, n)\n\n    X, Y = np.meshgrid(x, y)\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    Xs = torch.Tensor(np.transpose(np.array([list(X.flat), list(Y.flat)]))).double()\n    Ys = F.batch_forward(Xs)\n\n    cmap = cm.get_cmap('viridis')\n\n    yy = -8.3552\n    if task == 0:\n        Yv = Ys.mean(1)\n        plt.plot(-8.5, 7.5, marker='o', markersize=10, zorder=5, color='k')\n        plt.plot(-8.5, -5, marker='o', markersize=10, zorder=5, color='k')\n        plt.plot(9, 9, marker='o', markersize=10, zorder=5, color='k')\n        plt.plot([-7, 7], [yy, yy], linewidth=8.0, zorder=0, color='gray')\n        plt.plot(0, yy, marker='*', markersize=15, zorder=5, color='k')\n    elif task == 1:\n        Yv = Ys[:, 0]\n        plt.plot(7, yy, marker='*', markersize=15, zorder=5, color='k')\n    else:\n        Yv = Ys[:, 1]\n        plt.plot(-7, yy, marker='*', markersize=15, zorder=5, color='k')\n\n    c = plt.contour(X, Y, Yv.view(n, n), cmap=cm.viridis, linewidths=4.0)\n\n    if traj is not None:\n        for tt in traj:\n            l = tt.shape[0]\n            color_list = np.zeros((l, 3))\n            color_list[:, 0] = 1.\n            color_list[:, 1] = np.linspace(0, 1, l)\n            #color_list[:,2] = 1-np.linspace(0, 1, l)\n            ax.scatter(tt[:, 0], tt[:, 1], color=color_list, s=6, zorder=10)\n\n    if plotbar:\n        cbar = fig.colorbar(c, ticks=[-15, -10, -5, 0, 5])\n        cbar.ax.tick_params(labelsize=15)\n\n    ax.set_aspect(1.0 / ax.get_data_ratio(), adjustable='box')\n    plt.xticks([-10, -5, 0, 5, 10], fontsize=15)\n    plt.yticks([-10, -5, 0, 5, 10], fontsize=15)\n    plt.tight_layout()\n    plt.savefig(f\"{name}.png\", dpi=100)\n    plt.close()", "\n\ndef smooth(x, n=20):\n    l = len(x)\n    y = []\n    for i in range(l):\n        ii = max(0, i - n)\n        jj = min(i + n, l - 1)\n        v = np.array(x[ii:jj]).astype(np.float64)\n        if i < 3:\n            y.append(x[i])\n        else:\n            y.append(v.mean())\n    return y", "\n\ndef plot_loss(trajs, name=\"tmp\"):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    colormaps = {\n        \"sgd\": \"tab:blue\",\n        \"pcgrad\": \"tab:orange\",\n        \"mgd\": \"tab:purple\",\n        \"cagrad\": \"tab:red\",\n        \"sdmgrad\": \"tab:cyan\"\n    }\n    maps = {\"sgd\": \"Adam\", \"pcgrad\": \"PCGrad\", \"mgd\": \"MGDA\", \"cagrad\": \"CAGrad\", \"sdmgrad\": \"SDMGrad (Ours)\"}\n    for method in [\"sgd\", \"mgd\", \"pcgrad\", \"cagrad\", \"sdmgrad\"]:\n        traj = trajs[method][::100]\n        Ys = F.batch_forward(traj)\n        x = np.arange(traj.shape[0])\n        #y = torch.cummin(Ys.mean(1), 0)[0]\n        y = Ys.mean(1)\n\n        ax.plot(x, smooth(list(y)), color=colormaps[method], linestyle='-', label=maps[method], linewidth=4.)\n\n    plt.xticks([0, 200, 400, 600, 800, 1000], [\"0\", \"20K\", \"40K\", \"60K\", \"80K\", \"100K\"], fontsize=15)\n\n    plt.yticks(fontsize=15)\n    ax.grid()\n    plt.legend(fontsize=15)\n\n    ax.set_aspect(1.0 / ax.get_data_ratio(), adjustable='box')\n    plt.tight_layout()\n    plt.savefig(f\"{name}.png\", dpi=100)\n    plt.close()", "\n\n################################################################################\n#\n# Multi-Objective Optimization Solver\n#\n################################################################################\n\n\ndef mean_grad(grads):\n    return grads.mean(1)", "\ndef mean_grad(grads):\n    return grads.mean(1)\n\n\ndef pcgrad(grads):\n    g1 = grads[:, 0]\n    g2 = grads[:, 1]\n    g11 = g1.dot(g1).item()\n    g12 = g1.dot(g2).item()\n    g22 = g2.dot(g2).item()\n    if g12 < 0:\n        return ((1 - g12 / g11) * g1 + (1 - g12 / g22) * g2) / 2\n    else:\n        return (g1 + g2) / 2", "\n\ndef mgd(grads):\n    g1 = grads[:, 0]\n    g2 = grads[:, 1]\n\n    g11 = g1.dot(g1).item()\n    g12 = g1.dot(g2).item()\n    g22 = g2.dot(g2).item()\n\n    if g12 < min(g11, g22):\n        x = (g22 - g12) / (g11 + g22 - 2 * g12 + 1e-8)\n    elif g11 < g22:\n        x = 1\n    else:\n        x = 0\n\n    g_mgd = x * g1 + (1 - x) * g2  # mgd gradient g_mgd\n    return g_mgd", "\n\ndef cagrad(grads, c=0.5):\n    g1 = grads[:, 0]\n    g2 = grads[:, 1]\n    g0 = (g1 + g2) / 2\n\n    g11 = g1.dot(g1).item()\n    g12 = g1.dot(g2).item()\n    g22 = g2.dot(g2).item()\n\n    g0_norm = 0.5 * np.sqrt(g11 + g22 + 2 * g12 + 1e-4)\n\n    # want to minimize g_w^Tg_0 + c*||g_0||*||g_w||\n    coef = c * g0_norm\n\n    def obj(x):\n        # g_w^T g_0: x*0.5*(g11+g22-2g12)+(0.5+x)*(g12-g22)+g22\n        # g_w^T g_w: x^2*(g11+g22-2g12)+2*x*(g12-g22)+g22\n        return coef * np.sqrt(x**2*(g11+g22-2*g12)+2*x*(g12-g22)+g22+1e-4) + \\\n                0.5*x*(g11+g22-2*g12)+(0.5+x)*(g12-g22)+g22\n\n    res = minimize_scalar(obj, bounds=(0, 1), method='bounded')\n    x = res.x\n\n    gw = x * g1 + (1 - x) * g2\n    gw_norm = np.sqrt(x**2 * g11 + (1 - x)**2 * g22 + 2 * x * (1 - x) * g12 + 1e-4)\n\n    lmbda = coef / (gw_norm + 1e-4)\n    g = g0 + lmbda * gw\n    return g / (1 + c)", "\n\n### Our SDMGrad ###\ndef sdmgrad(grads, lmbda):\n\n    g1 = grads[:, 0]\n    g2 = grads[:, 1]\n    g0 = (g1 + g2) / 2\n\n    g11 = g1.dot(g1).item()\n    g12 = g1.dot(g2).item()\n    g22 = g2.dot(g2).item()\n\n    def obj(x):\n        # g_w^T g_0: x*0.5*(g11+g22-2g12)+(0.5+x)*(g12-g22)+g22\n        # g_w^T g_w: x^2*(g11+g22-2g12)+2*x*(g12-g22)+g22\n        return (x**2*(g11+g22-2*g12)+2*x*(g12-g22)+g22+1e-4) + \\\n                2 * lmbda * (0.5*x*(g11+g22-2*g12)+(0.5+x)*(g12-g22)+g22) + \\\n                lmbda**2 * 0.25 * (g11+g22+2*g12+1e-4)\n\n    res = minimize_scalar(obj, bounds=(0, 1), method='bounded')\n    x = res.x\n\n    gw = x * g1 + (1 - x) * g2\n    g = lmbda * g0 + gw\n    return g / (1 + lmbda)", "\n\n### Add noise ###\ndef add_noise(grads, coef=0.2):\n    grads_ = grads + coef * torch.randn_like(grads)\n    return grads_\n\n\n### Define the problem ###\nF = Toy()", "### Define the problem ###\nF = Toy()\n\nmaps = {\"sgd\": mean_grad, \"cagrad\": cagrad, \"mgd\": mgd, \"pcgrad\": pcgrad, \"sdmgrad\": sdmgrad}\n\n### Start experiments ###\n\n\ndef run_all():\n    all_traj = {}\n\n    # the initial positions\n    inits = [\n        torch.Tensor([-8.5, 7.5]),\n        torch.Tensor([-8.5, -5.]),\n        torch.Tensor([9., 9.]),\n    ]\n\n    for i, init in enumerate(inits):\n        for m in tqdm([\"sgd\", \"mgd\", \"pcgrad\", \"cagrad\", \"sdmgrad\"]):\n            all_traj[m] = None\n            traj = []\n            solver = maps[m]\n            x = init.clone()\n            x.requires_grad = True\n\n            n_iter = 70000\n            opt = torch.optim.Adam([x], lr=0.002)\n            # scheduler = ExponentialLR(opt, gamma = 0.9999)\n\n            for it in range(n_iter):\n                traj.append(x.detach().numpy().copy())\n\n                # if it % 1000 == 0:\n                #     print(f'\\niteration {it}, before update x: ', x.detach().numpy().copy())\n\n                f, grads = F(x, True)\n\n                grads = add_noise(grads, coef=0.2)\n                # grads = add_element_noise(grads, coef=1.0, it=it)\n\n                if m == \"cagrad\":\n                    g = solver(grads, c=0.5)\n                elif m == \"sdmgrad\":\n                    g = solver(grads, lmbda=0.01)\n                else:\n                    g = solver(grads)\n                opt.zero_grad()\n                x.grad = g\n                opt.step()\n                # scheduler.step()\n\n            all_traj[m] = torch.tensor(np.array(traj))\n        torch.save(all_traj, f\"toy{i}.pt\")\n    plot_loss(all_traj)\n    plot_results()", "def run_all():\n    all_traj = {}\n\n    # the initial positions\n    inits = [\n        torch.Tensor([-8.5, 7.5]),\n        torch.Tensor([-8.5, -5.]),\n        torch.Tensor([9., 9.]),\n    ]\n\n    for i, init in enumerate(inits):\n        for m in tqdm([\"sgd\", \"mgd\", \"pcgrad\", \"cagrad\", \"sdmgrad\"]):\n            all_traj[m] = None\n            traj = []\n            solver = maps[m]\n            x = init.clone()\n            x.requires_grad = True\n\n            n_iter = 70000\n            opt = torch.optim.Adam([x], lr=0.002)\n            # scheduler = ExponentialLR(opt, gamma = 0.9999)\n\n            for it in range(n_iter):\n                traj.append(x.detach().numpy().copy())\n\n                # if it % 1000 == 0:\n                #     print(f'\\niteration {it}, before update x: ', x.detach().numpy().copy())\n\n                f, grads = F(x, True)\n\n                grads = add_noise(grads, coef=0.2)\n                # grads = add_element_noise(grads, coef=1.0, it=it)\n\n                if m == \"cagrad\":\n                    g = solver(grads, c=0.5)\n                elif m == \"sdmgrad\":\n                    g = solver(grads, lmbda=0.01)\n                else:\n                    g = solver(grads)\n                opt.zero_grad()\n                x.grad = g\n                opt.step()\n                # scheduler.step()\n\n            all_traj[m] = torch.tensor(np.array(traj))\n        torch.save(all_traj, f\"toy{i}.pt\")\n    plot_loss(all_traj)\n    plot_results()", "\n\ndef plot_results():\n    plot3d(F)\n    plot_contour(F, 1, name=\"toy_task_1\")\n    plot_contour(F, 2, name=\"toy_task_2\")\n    t1 = torch.load(f\"toy0.pt\")\n    t2 = torch.load(f\"toy1.pt\")\n    t3 = torch.load(f\"toy2.pt\")\n\n    length = t1[\"sdmgrad\"].shape[0]\n\n    for method in [\"sgd\", \"mgd\", \"pcgrad\", \"cagrad\", \"sdmgrad\"]:\n        ranges = list(range(10, length, 1000))\n        ranges.append(length - 1)\n        for t in tqdm(ranges):\n            plot_contour(\n                F,\n                task=0,  # task == 0 meeas plot for both tasks\n                traj=[t1[method][:t], t2[method][:t], t3[method][:t]],\n                plotbar=(method == \"sdmgrad\"),\n                name=f\"./imgs/toy_{method}_{t}\")", "\n\nif __name__ == \"__main__\":\n    run_all()\n"]}
{"filename": "nyuv2/model_segnet_stan.py", "chunked_list": ["import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\n\nfrom create_dataset import *\nfrom utils import *\n\nparser = argparse.ArgumentParser(description='Single-task: Attention Network')\nparser.add_argument('--task', default='semantic', type=str, help='choose task: semantic, depth, normal')", "parser = argparse.ArgumentParser(description='Single-task: Attention Network')\nparser.add_argument('--task', default='semantic', type=str, help='choose task: semantic, depth, normal')\nparser.add_argument('--dataroot', default='nyuv2', type=str, help='dataset root')\nparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(1):\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        if opt.task == 'semantic':\n            self.pred_task = self.conv_layer([filter[0], self.class_nb], pred=True)\n        if opt.task == 'depth':\n            self.pred_task = self.conv_layer([filter[0], 1], pred=True)\n        if opt.task == 'normal':\n            self.pred_task = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for two tasks\n        atten_encoder, atten_decoder = ([0] * 3 for _ in range(2))\n        for i in range(3):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(3):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(1):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        if opt.task == 'semantic':\n            pred = F.log_softmax(self.pred_task(atten_decoder[0][-1][-1]), dim=1)\n        if opt.task == 'depth':\n            pred = self.pred_task(atten_decoder[0][-1][-1])\n        if opt.task == 'normal':\n            pred = self.pred_task(atten_decoder[0][-1][-1])\n            pred = pred / torch.norm(pred, p=2, dim=1, keepdim=True)\n        return pred", "\n\n# define model, optimiser and scheduler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSegNet_STAN = SegNet().to(device)\noptimizer = optim.Adam(SegNet_STAN.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_STAN),\n                                                         count_parameters(SegNet_STAN) / 24981069))", "print('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_STAN),\n                                                         count_parameters(SegNet_STAN) / 24981069))\nprint(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\n# define dataset\ndataset_path = opt.dataroot\nif opt.apply_augmentation:\n    nyuv2_train_set = NYUv2(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation on NYUv2.')\nelse:\n    nyuv2_train_set = NYUv2(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')", "\nnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\nbatch_size = 2\nnyuv2_train_loader = torch.utils.data.DataLoader(dataset=nyuv2_train_set, batch_size=batch_size, shuffle=True)\n\nnyuv2_test_loader = torch.utils.data.DataLoader(dataset=nyuv2_test_set, batch_size=batch_size, shuffle=False)\n\n# Train and evaluate single-task network\nsingle_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_STAN, device, optimizer, scheduler, opt, 200)", "# Train and evaluate single-task network\nsingle_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_STAN, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "nyuv2/model_segnet_mtan.py", "chunked_list": ["import numpy as np\nimport random\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nimport torch.utils.data.sampler as sampler\n\nfrom create_dataset import *\nfrom utils import *", "from create_dataset import *\nfrom utils import *\n\nparser = argparse.ArgumentParser(description='Multi-task: Attention Network')\nparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\nparser.add_argument('--dataroot', default='nyuv2', type=str, help='dataset root')\nparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\nparser.add_argument('--seed', default=0, type=int, help='the seed')\nparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()", "parser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        # define task attention layers\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(3):\n            if j < 2:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n        self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for tasks\n        atten_encoder, atten_decoder = ([0] * 3 for _ in range(2))\n        for i in range(3):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(3):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(3):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n        t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred, t3_pred], self.logsigma", "\n\n# control seed\ntorch.backends.cudnn.enabled = False\ntorch.manual_seed(opt.seed)\nnp.random.seed(opt.seed)\nrandom.seed(opt.seed)\ntorch.cuda.manual_seed_all(opt.seed)\n\n# define model, optimiser and scheduler", "\n# define model, optimiser and scheduler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSegNet_MTAN = SegNet().to(device)\noptimizer = optim.Adam(SegNet_MTAN.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_MTAN),\n                                                         count_parameters(SegNet_MTAN) / 24981069))\nprint(", "                                                         count_parameters(SegNet_MTAN) / 24981069))\nprint(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\n# define dataset\ndataset_path = opt.dataroot\nif opt.apply_augmentation:\n    nyuv2_train_set = NYUv2(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation on NYUv2.')\nelse:\n    nyuv2_train_set = NYUv2(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')", "\nnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\nbatch_size = 2\nnyuv2_train_loader = torch.utils.data.DataLoader(dataset=nyuv2_train_set, batch_size=batch_size, shuffle=True)\n\nnyuv2_test_loader = torch.utils.data.DataLoader(dataset=nyuv2_test_set, batch_size=batch_size, shuffle=False)\n\n# Train and evaluate multi-task network\nmulti_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_MTAN, device, optimizer, scheduler, opt, 200)", "# Train and evaluate multi-task network\nmulti_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_MTAN, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "nyuv2/model_segnet_split.py", "chunked_list": ["import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nimport torch.utils.data.sampler as sampler\n\nfrom create_dataset import *\nfrom utils import *\n\nparser = argparse.ArgumentParser(description='Multi-task: Split')", "\nparser = argparse.ArgumentParser(description='Multi-task: Split')\nparser.add_argument('--type', default='standard', type=str, help='split type: standard, wide, deep')\nparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\nparser.add_argument('--dataroot', default='nyuv2', type=str, help='dataset root')\nparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\nparser.add_argument('--seed', default=0, type=int, help='the seed')\nparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()\n", "opt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        if opt.type == 'wide':\n            filter = [64, 128, 256, 512, 1024]\n        else:\n            filter = [64, 128, 256, 512, 512]\n\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        # define task specific layers\n        self.pred_task1 = nn.Sequential(\n            nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n            nn.Conv2d(in_channels=filter[0], out_channels=self.class_nb, kernel_size=1, padding=0))\n        self.pred_task2 = nn.Sequential(\n            nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n            nn.Conv2d(in_channels=filter[0], out_channels=1, kernel_size=1, padding=0))\n        self.pred_task3 = nn.Sequential(\n            nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n            nn.Conv2d(in_channels=filter[0], out_channels=3, kernel_size=1, padding=0))\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    # define convolutional block\n    def conv_layer(self, channel):\n        if opt.type == 'deep':\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]), nn.ReLU(inplace=True))\n        return conv_block\n\n    def forward(self, x):\n        import pdb\n        pdb.set_trace()\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # global shared encoder-decoder network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(g_decoder[i][1]), dim=1)\n        t2_pred = self.pred_task2(g_decoder[i][1])\n        t3_pred = self.pred_task3(g_decoder[i][1])\n        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred, t3_pred], self.logsigma", "\n\n# control seed\ntorch.backends.cudnn.enabled = False\ntorch.manual_seed(opt.seed)\nnp.random.seed(opt.seed)\nrandom.seed(opt.seed)\ntorch.cuda.manual_seed_all(opt.seed)\n\n# define model, optimiser and scheduler", "\n# define model, optimiser and scheduler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSegNet_SPLIT = SegNet().to(device)\noptimizer = optim.Adam(SegNet_SPLIT.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_SPLIT),\n                                                         count_parameters(SegNet_SPLIT) / 24981069))\nprint(", "                                                         count_parameters(SegNet_SPLIT) / 24981069))\nprint(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\n# define dataset\ndataset_path = opt.dataroot\nif opt.apply_augmentation:\n    nyuv2_train_set = NYUv2(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation on NYUv2.')\nelse:\n    nyuv2_train_set = NYUv2(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')", "\nnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\nbatch_size = 2  ###org 2\nnyuv2_train_loader = torch.utils.data.DataLoader(dataset=nyuv2_train_set, batch_size=batch_size, shuffle=True)\n\nnyuv2_test_loader = torch.utils.data.DataLoader(dataset=nyuv2_test_set, batch_size=batch_size, shuffle=False)\nimport pdb\n\npdb.set_trace()", "\npdb.set_trace()\n\n# Train and evaluate multi-task network\nmulti_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_SPLIT, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "nyuv2/evaluate.py", "chunked_list": ["import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport torch\nimport itertools\n\nmethods = [\n    \"sdmgrad-1e-1\", \"sdmgrad-2e-1\", \"sdmgrad-3e-1\", \"sdmgrad-4e-1\", \"sdmgrad-5e-1\", \"sdmgrad-6e-1\", \"sdmgrad-7e-1\",\n    \"sdmgrad-8e-1\", \"sdmgrad-9e-1\", \"sdmgrad-1e0\"", "    \"sdmgrad-1e-1\", \"sdmgrad-2e-1\", \"sdmgrad-3e-1\", \"sdmgrad-4e-1\", \"sdmgrad-5e-1\", \"sdmgrad-6e-1\", \"sdmgrad-7e-1\",\n    \"sdmgrad-8e-1\", \"sdmgrad-9e-1\", \"sdmgrad-1e0\"\n]\n\ncolors = [\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"C8\", \"C9\", \"tab:green\", \"tab:cyan\", \"tab:blue\", \"tab:red\"]\nstats = [\n    \"semantic loss\", \"mean iou\", \"pix acc\", \"depth loss\", \"abs err\", \"rel err\", \"normal loss\", \"mean\", \"median\",\n    \"<11.25\", \"<22.5\", \"<30\"\n]\n", "]\n\ndelta_stats = [\"mean iou\", \"pix acc\", \"abs err\", \"rel err\", \"mean\", \"median\", \"<11.25\", \"<22.5\", \"<30\"]\n\nstats_idx_map = [4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 16, 17]\n\ntime_idx = 34\n\n# change random seeds used in the experiments here\nseeds = [0, 1, 2]", "# change random seeds used in the experiments here\nseeds = [0, 1, 2]\n\nlogs = {}\nmin_epoch = 100000\n\nfor m in methods:\n    logs[m] = {\"train\": [None for _ in range(3)], \"test\": [None for _ in range(3)]}\n\n    for seed in seeds:\n        logs[m][\"train\"][seed] = {}\n        logs[m][\"test\"][seed] = {}\n\n    for stat in stats:\n        for seed in seeds:\n            logs[m][\"train\"][seed][stat] = []\n            logs[m][\"test\"][seed][stat] = []\n\n    for seed in seeds:\n        logs[m][\"train\"][seed][\"time\"] = []\n\n    for seed in seeds:\n        fname = f\"logs/{m}-sd{seed}.log\"\n        with open(fname, \"r\") as f:\n            lines = f.readlines()\n            for line in lines:\n                if line.startswith(\"Epoch\"):\n                    ws = line.split(\" \")\n                    for i, stat in enumerate(stats):\n                        logs[m][\"train\"][seed][stat].append(float(ws[stats_idx_map[i]]))\n                        logs[m][\"test\"][seed][stat].append(float(ws[stats_idx_map[i] + 15]))\n                    logs[m][\"train\"][seed][\"time\"].append(float(ws[time_idx]))\n            min_epoch = min(min(min_epoch, len(logs[m][\"train\"][seed][\"semantic loss\"])),\n                            len(logs[m][\"test\"][seed][\"semantic loss\"]))", "\ntest_stats = {}\ntrain_stats = {}\nlearning_time = {}\n\nprint(\" \" * 25 + \" | \".join([f\"{s:5s}\" for s in stats]))\n\nfor mi, mode in enumerate([\"train\", \"test\"]):\n    if mi == 1:\n        print(mode)\n    for mmi, m in enumerate(methods):\n        if m not in test_stats:\n            test_stats[m] = {}\n            train_stats[m] = {}\n\n        string = f\"{m:30s} \"\n        for stat in stats:\n            x = []\n            for seed in seeds:\n                x.append(np.array(logs[m][mode][seed][stat][min_epoch - 10:min_epoch]).mean())\n            x = np.array(x)\n            if mode == \"test\":\n                test_stats[m][stat] = x.copy()\n            else:\n                train_stats[m][stat] = x.copy()\n            mu = x.mean()\n            std = x.std() / np.sqrt(3)\n            string += f\" | {mu:5.4f}\"\n        if mode == \"test\":\n            print(string)", "\nfor m in methods:\n    learning_time[m] = np.array([np.array(logs[m][\"train\"][sd][\"time\"]).mean() for sd in seeds])\n\n### print average training loss\nfor method in methods:\n    average_loss = np.mean([\n        train_stats[method][\"semantic loss\"].mean(), train_stats[method][\"depth loss\"].mean(),\n        train_stats[method][\"normal loss\"].mean()\n    ])\n    print(f\"{method} average training loss {average_loss}\")", "\n### print delta M\n\nbase = np.array([0.3830, 0.6376, 0.6754, 0.2780, 25.01, 19.21, 0.3014, 0.5720, 0.6915])\nsign = np.array([1, 1, 0, 0, 0, 0, 1, 1, 1])\nkk = np.ones(9) * -1\n\n\ndef delta_fn(a):\n    return (kk**sign * (a - base) / base).mean() * 100.  # *100 for percentage", "def delta_fn(a):\n    return (kk**sign * (a - base) / base).mean() * 100.  # *100 for percentage\n\n\ndeltas = {}\nfor method in methods:\n    tmp = np.zeros(9)\n    for i, stat in enumerate(delta_stats):\n        tmp[i] = test_stats[method][stat].mean()\n    deltas[method] = delta_fn(tmp)\n    print(f\"{method:30s} delta: {deltas[method]:4.3f}\")", ""]}
{"filename": "nyuv2/utils.py", "chunked_list": ["import numpy as np\nimport time\nimport torch\nimport torch.nn.functional as F\n\nfrom copy import deepcopy\nfrom min_norm_solvers import MinNormSolver\nfrom scipy.optimize import minimize, Bounds, minimize_scalar\n\n\ndef euclidean_proj_simplex(v, s=1):\n    \"\"\" Compute the Euclidean projection on a positive simplex\n    Solves the optimisation problem (using the algorithm from [1]):\n        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0 \n    Parameters\n    ----------\n    v: (n,) numpy array,\n       n-dimensional vector to project\n    s: int, optional, default: 1,\n       radius of the simplex\n    Returns\n    -------\n    w: (n,) numpy array,\n       Euclidean projection of v on the simplex\n    Notes\n    -----\n    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n    However, this implementation still easily scales to millions of dimensions.\n    References\n    ----------\n    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n        International Conference on Machine Learning (ICML 2008)\n        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n    [2] Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application\n        Weiran Wang, Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n. arXiv:1309.1541\n        https://arxiv.org/pdf/1309.1541.pdf\n    [3] https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246#file-simplex_projection-py\n    \"\"\"\n    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n    v = v.astype(np.float64)\n    n, = v.shape  # will raise ValueError if v is not 1-D\n    # check if we are already on the simplex\n    if v.sum() == s and np.alltrue(v >= 0):\n        # best projection: itself!\n        return v\n    # get the array of cumulative sums of a sorted (decreasing) copy of v\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    # get the number of > 0 components of the optimal solution\n    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - s))[0][-1]\n    # compute the Lagrange multiplier associated to the simplex constraint\n    theta = float(cssv[rho] - s) / (rho + 1)\n    # compute the projection by thresholding v using theta\n    w = (v - theta).clip(min=0)\n    return w", "\n\ndef euclidean_proj_simplex(v, s=1):\n    \"\"\" Compute the Euclidean projection on a positive simplex\n    Solves the optimisation problem (using the algorithm from [1]):\n        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0 \n    Parameters\n    ----------\n    v: (n,) numpy array,\n       n-dimensional vector to project\n    s: int, optional, default: 1,\n       radius of the simplex\n    Returns\n    -------\n    w: (n,) numpy array,\n       Euclidean projection of v on the simplex\n    Notes\n    -----\n    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n    However, this implementation still easily scales to millions of dimensions.\n    References\n    ----------\n    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n        International Conference on Machine Learning (ICML 2008)\n        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n    [2] Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application\n        Weiran Wang, Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n. arXiv:1309.1541\n        https://arxiv.org/pdf/1309.1541.pdf\n    [3] https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246#file-simplex_projection-py\n    \"\"\"\n    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n    v = v.astype(np.float64)\n    n, = v.shape  # will raise ValueError if v is not 1-D\n    # check if we are already on the simplex\n    if v.sum() == s and np.alltrue(v >= 0):\n        # best projection: itself!\n        return v\n    # get the array of cumulative sums of a sorted (decreasing) copy of v\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    # get the number of > 0 components of the optimal solution\n    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - s))[0][-1]\n    # compute the Lagrange multiplier associated to the simplex constraint\n    theta = float(cssv[rho] - s) / (rho + 1)\n    # compute the projection by thresholding v using theta\n    w = (v - theta).clip(min=0)\n    return w", "\n\n\"\"\"\nDefine task metrics, loss functions and model trainer here.\n\"\"\"\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n", "\n\ndef model_fit(x_pred, x_output, task_type):\n    device = x_pred.device\n\n    # binary mark to mask out undefined pixel space\n    binary_mask = (torch.sum(x_output, dim=1) != 0).float().unsqueeze(1).to(device)\n\n    if task_type == 'semantic':\n        # semantic loss: depth-wise cross entropy\n        loss = F.nll_loss(x_pred, x_output, ignore_index=-1)\n\n    if task_type == 'depth':\n        # depth loss: l1 norm\n        loss = torch.sum(torch.abs(x_pred - x_output) * binary_mask) / torch.nonzero(binary_mask,\n                                                                                     as_tuple=False).size(0)\n\n    if task_type == 'normal':\n        # normal loss: dot product\n        loss = 1 - torch.sum((x_pred * x_output) * binary_mask) / torch.nonzero(binary_mask, as_tuple=False).size(0)\n\n    return loss", "\n\n# Legacy: compute mIoU and Acc. for each image and average across all images.\n\n# def compute_miou(x_pred, x_output):\n#     _, x_pred_label = torch.max(x_pred, dim=1)\n#     x_output_label = x_output\n#     batch_size = x_pred.size(0)\n#     class_nb = x_pred.size(1)\n#     device = x_pred.device", "#     class_nb = x_pred.size(1)\n#     device = x_pred.device\n#     for i in range(batch_size):\n#         true_class = 0\n#         first_switch = True\n#         invalid_mask = (x_output[i] >= 0).float()\n#         for j in range(class_nb):\n#             pred_mask = torch.eq(x_pred_label[i], j * torch.ones(x_pred_label[i].shape).long().to(device))\n#             true_mask = torch.eq(x_output_label[i], j * torch.ones(x_output_label[i].shape).long().to(device))\n#             mask_comb = pred_mask.float() + true_mask.float()", "#             true_mask = torch.eq(x_output_label[i], j * torch.ones(x_output_label[i].shape).long().to(device))\n#             mask_comb = pred_mask.float() + true_mask.float()\n#             union = torch.sum((mask_comb > 0).float() * invalid_mask)  # remove non-defined pixel predictions\n#             intsec = torch.sum((mask_comb > 1).float())\n#             if union == 0:\n#                 continue\n#             if first_switch:\n#                 class_prob = intsec / union\n#                 first_switch = False\n#             else:", "#                 first_switch = False\n#             else:\n#                 class_prob = intsec / union + class_prob\n#             true_class += 1\n#         if i == 0:\n#             batch_avg = class_prob / true_class\n#         else:\n#             batch_avg = class_prob / true_class + batch_avg\n#     return batch_avg / batch_size\n#", "#     return batch_avg / batch_size\n#\n#\n# def compute_iou(x_pred, x_output):\n#     _, x_pred_label = torch.max(x_pred, dim=1)\n#     x_output_label = x_output\n#     batch_size = x_pred.size(0)\n#     for i in range(batch_size):\n#         if i == 0:\n#             pixel_acc = torch.div(", "#         if i == 0:\n#             pixel_acc = torch.div(\n#                 torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).float()),\n#                 torch.sum((x_output_label[i] >= 0).float()))\n#         else:\n#             pixel_acc = pixel_acc + torch.div(\n#                 torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).float()),\n#                 torch.sum((x_output_label[i] >= 0).float()))\n#     return pixel_acc / batch_size\n", "#     return pixel_acc / batch_size\n\n\n# New mIoU and Acc. formula: accumulate every pixel and average across all pixels in all images\nclass ConfMatrix(object):\n\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.mat = None\n\n    def update(self, pred, target):\n        n = self.num_classes\n        if self.mat is None:\n            self.mat = torch.zeros((n, n), dtype=torch.int64, device=pred.device)\n        with torch.no_grad():\n            k = (target >= 0) & (target < n)\n            inds = n * target[k].to(torch.int64) + pred[k]\n            self.mat += torch.bincount(inds, minlength=n**2).reshape(n, n)\n\n    def get_metrics(self):\n        h = self.mat.float()\n        acc = torch.diag(h).sum() / h.sum()\n        iu = torch.diag(h) / (h.sum(1) + h.sum(0) - torch.diag(h))\n        return torch.mean(iu).item(), acc.item()", "\n\ndef depth_error(x_pred, x_output):\n    device = x_pred.device\n    binary_mask = (torch.sum(x_output, dim=1) != 0).unsqueeze(1).to(device)\n    x_pred_true = x_pred.masked_select(binary_mask)\n    x_output_true = x_output.masked_select(binary_mask)\n    abs_err = torch.abs(x_pred_true - x_output_true)\n    rel_err = torch.abs(x_pred_true - x_output_true) / x_output_true\n    return (torch.sum(abs_err) / torch.nonzero(binary_mask, as_tuple=False).size(0)).item(), \\\n           (torch.sum(rel_err) / torch.nonzero(binary_mask, as_tuple=False).size(0)).item()", "\n\ndef normal_error(x_pred, x_output):\n    binary_mask = (torch.sum(x_output, dim=1) != 0)\n    error = torch.acos(torch.clamp(torch.sum(x_pred * x_output, 1).masked_select(binary_mask), -1,\n                                   1)).detach().cpu().numpy()\n    error = np.degrees(error)\n    return np.mean(error), np.median(error), np.mean(error < 11.25), np.mean(error < 22.5), np.mean(error < 30)\n\n", "\n\n\"\"\"\n=========== Universal Multi-task Trainer ===========\n\"\"\"\n\n\ndef multi_task_trainer(train_loader, test_loader, multi_task_model, device, optimizer, scheduler, opt, total_epoch=200):\n    start_time = time.time()\n    train_batch = len(train_loader)\n    test_batch = len(test_loader)\n    T = opt.temp\n    avg_cost = np.zeros([total_epoch, 24], dtype=np.float32)\n    lambda_weight = np.ones([3, total_epoch])\n    for index in range(total_epoch):\n        epoch_start_time = time.time()\n        cost = np.zeros(24, dtype=np.float32)\n\n        # apply Dynamic Weight Average\n        if opt.weight == 'dwa':\n            if index == 0 or index == 1:\n                lambda_weight[:, index] = 1.0\n            else:\n                w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n                w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n                w_3 = avg_cost[index - 1, 6] / avg_cost[index - 2, 6]\n                lambda_weight[0, index] = 3 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n                lambda_weight[1, index] = 3 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n                lambda_weight[2, index] = 3 * np.exp(w_3 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n\n        # iteration for all batches\n        multi_task_model.train()\n        train_dataset = iter(train_loader)\n        conf_mat = ConfMatrix(multi_task_model.class_nb)\n        for k in range(train_batch):\n            train_data, train_label, train_depth, train_normal = train_dataset.next()\n            train_data, train_label = train_data.to(device), train_label.long().to(device)\n            train_depth, train_normal = train_depth.to(device), train_normal.to(device)\n\n            train_pred, logsigma = multi_task_model(train_data)\n\n            optimizer.zero_grad()\n            train_loss = [\n                model_fit(train_pred[0], train_label, 'semantic'),\n                model_fit(train_pred[1], train_depth, 'depth'),\n                model_fit(train_pred[2], train_normal, 'normal')\n            ]\n\n            if opt.weight == 'equal' or opt.weight == 'dwa':\n                loss = sum([lambda_weight[i, index] * train_loss[i] for i in range(3)])\n                #loss = sum([w[i] * train_loss[i] for i in range(3)])\n            else:\n                loss = sum(1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2 for i in range(3))\n\n            loss.backward()\n            optimizer.step()\n\n            # accumulate label prediction for every pixel in training images\n            conf_mat.update(train_pred[0].argmax(1).flatten(), train_label.flatten())\n\n            cost[0] = train_loss[0].item()\n            cost[3] = train_loss[1].item()\n            cost[4], cost[5] = depth_error(train_pred[1], train_depth)\n            cost[6] = train_loss[2].item()\n            cost[7], cost[8], cost[9], cost[10], cost[11] = normal_error(train_pred[2], train_normal)\n            avg_cost[index, :12] += cost[:12] / train_batch\n\n        # compute mIoU and acc\n        avg_cost[index, 1:3] = conf_mat.get_metrics()\n\n        # evaluating test data\n        multi_task_model.eval()\n        conf_mat = ConfMatrix(multi_task_model.class_nb)\n        with torch.no_grad():  # operations inside don't track history\n            test_dataset = iter(test_loader)\n            for k in range(test_batch):\n                test_data, test_label, test_depth, test_normal = test_dataset.next()\n                test_data, test_label = test_data.to(device), test_label.long().to(device)\n                test_depth, test_normal = test_depth.to(device), test_normal.to(device)\n\n                test_pred, _ = multi_task_model(test_data)\n                test_loss = [\n                    model_fit(test_pred[0], test_label, 'semantic'),\n                    model_fit(test_pred[1], test_depth, 'depth'),\n                    model_fit(test_pred[2], test_normal, 'normal')\n                ]\n\n                conf_mat.update(test_pred[0].argmax(1).flatten(), test_label.flatten())\n\n                cost[12] = test_loss[0].item()\n                cost[15] = test_loss[1].item()\n                cost[16], cost[17] = depth_error(test_pred[1], test_depth)\n                cost[18] = test_loss[2].item()\n                cost[19], cost[20], cost[21], cost[22], cost[23] = normal_error(test_pred[2], test_normal)\n                avg_cost[index, 12:] += cost[12:] / test_batch\n\n            # compute mIoU and acc\n            avg_cost[index, 13:15] = conf_mat.get_metrics()\n\n        scheduler.step()\n        epoch_end_time = time.time()\n        print(\n            'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} ||'\n            'TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} | {:.4f}'.\n            format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n                   avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8],\n                   avg_cost[index, 9], avg_cost[index, 10], avg_cost[index, 11], avg_cost[index, 12],\n                   avg_cost[index, 13], avg_cost[index, 14], avg_cost[index, 15], avg_cost[index, 16],\n                   avg_cost[index, 17], avg_cost[index, 18], avg_cost[index, 19], avg_cost[index, 20],\n                   avg_cost[index, 21], avg_cost[index, 22], avg_cost[index, 23], epoch_end_time - epoch_start_time))\n    end_time = time.time()\n    print(\"Training time: \", end_time - start_time)", "\n\n\"\"\"\n=========== Universal Single-task Trainer ===========\n\"\"\"\n\n\ndef single_task_trainer(train_loader,\n                        test_loader,\n                        single_task_model,\n                        device,\n                        optimizer,\n                        scheduler,\n                        opt,\n                        total_epoch=200):\n    train_batch = len(train_loader)\n    test_batch = len(test_loader)\n    avg_cost = np.zeros([total_epoch, 24], dtype=np.float32)\n    for index in range(total_epoch):\n        cost = np.zeros(24, dtype=np.float32)\n\n        # iteration for all batches\n        single_task_model.train()\n        train_dataset = iter(train_loader)\n        conf_mat = ConfMatrix(single_task_model.class_nb)\n        for k in range(train_batch):\n            train_data, train_label, train_depth, train_normal = train_dataset.next()\n            train_data, train_label = train_data.to(device), train_label.long().to(device)\n            train_depth, train_normal = train_depth.to(device), train_normal.to(device)\n\n            train_pred = single_task_model(train_data)\n            optimizer.zero_grad()\n\n            if opt.task == 'semantic':\n                train_loss = model_fit(train_pred, train_label, opt.task)\n                train_loss.backward()\n                optimizer.step()\n\n                conf_mat.update(train_pred.argmax(1).flatten(), train_label.flatten())\n                cost[0] = train_loss.item()\n\n            if opt.task == 'depth':\n                train_loss = model_fit(train_pred, train_depth, opt.task)\n                train_loss.backward()\n                optimizer.step()\n                cost[3] = train_loss.item()\n                cost[4], cost[5] = depth_error(train_pred, train_depth)\n\n            if opt.task == 'normal':\n                train_loss = model_fit(train_pred, train_normal, opt.task)\n                train_loss.backward()\n                optimizer.step()\n                cost[6] = train_loss.item()\n                cost[7], cost[8], cost[9], cost[10], cost[11] = normal_error(train_pred, train_normal)\n\n            avg_cost[index, :12] += cost[:12] / train_batch\n\n        if opt.task == 'semantic':\n            avg_cost[index, 1:3] = conf_mat.get_metrics()\n\n        # evaluating test data\n        single_task_model.eval()\n        conf_mat = ConfMatrix(single_task_model.class_nb)\n        with torch.no_grad():  # operations inside don't track history\n            test_dataset = iter(test_loader)\n            for k in range(test_batch):\n                test_data, test_label, test_depth, test_normal = test_dataset.next()\n                test_data, test_label = test_data.to(device), test_label.long().to(device)\n                test_depth, test_normal = test_depth.to(device), test_normal.to(device)\n\n                test_pred = single_task_model(test_data)\n\n                if opt.task == 'semantic':\n                    test_loss = model_fit(test_pred, test_label, opt.task)\n\n                    conf_mat.update(test_pred.argmax(1).flatten(), test_label.flatten())\n                    cost[12] = test_loss.item()\n\n                if opt.task == 'depth':\n                    test_loss = model_fit(test_pred, test_depth, opt.task)\n                    cost[15] = test_loss.item()\n                    cost[16], cost[17] = depth_error(test_pred, test_depth)\n\n                if opt.task == 'normal':\n                    test_loss = model_fit(test_pred, test_normal, opt.task)\n                    cost[18] = test_loss.item()\n                    cost[19], cost[20], cost[21], cost[22], cost[23] = normal_error(test_pred, test_normal)\n\n                avg_cost[index, 12:] += cost[12:] / test_batch\n            if opt.task == 'semantic':\n                avg_cost[index, 13:15] = conf_mat.get_metrics()\n\n        scheduler.step()\n        if opt.task == 'semantic':\n            print('Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f}'.format(\n                index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 12],\n                avg_cost[index, 13], avg_cost[index, 14]))\n        if opt.task == 'depth':\n            print('Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f}'.format(\n                index, avg_cost[index, 3], avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 15],\n                avg_cost[index, 16], avg_cost[index, 17]))\n        if opt.task == 'normal':\n            print(\n                'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'\n                .format(index, avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8], avg_cost[index, 9],\n                        avg_cost[index, 10], avg_cost[index, 11], avg_cost[index, 18], avg_cost[index, 19],\n                        avg_cost[index, 20], avg_cost[index, 21], avg_cost[index, 22], avg_cost[index, 23]))", "\n\n''' ===== multi task MGD trainer ==== '''\n\n\ndef multi_task_mgd_trainer(train_loader,\n                           test_loader,\n                           multi_task_model,\n                           device,\n                           optimizer,\n                           scheduler,\n                           opt,\n                           total_epoch=200,\n                           method='sumloss',\n                           alpha=0.5,\n                           seed=0):\n    start_time = time.time()\n    niter = opt.niter\n\n    def graddrop(grads):\n        P = 0.5 * (1. + grads.sum(1) / (grads.abs().sum(1) + 1e-8))\n        U = torch.rand_like(grads[:, 0])\n        M = P.gt(U).view(-1, 1) * grads.gt(0) + P.lt(U).view(-1, 1) * grads.lt(0)\n        g = (grads * M.float()).mean(1)\n        return g\n\n    def mgd(grads):\n        grads_cpu = grads.t().cpu()\n        sol, min_norm = MinNormSolver.find_min_norm_element([grads_cpu[t] for t in range(grads.shape[-1])])\n        w = torch.FloatTensor(sol).to(grads.device)\n        g = grads.mm(w.view(-1, 1)).view(-1)\n        return g\n\n    def pcgrad(grads, rng):\n        grad_vec = grads.t()\n        num_tasks = 3\n\n        shuffled_task_indices = np.zeros((num_tasks, num_tasks - 1), dtype=int)\n        for i in range(num_tasks):\n            task_indices = np.arange(num_tasks)\n            task_indices[i] = task_indices[-1]\n            shuffled_task_indices[i] = task_indices[:-1]\n            rng.shuffle(shuffled_task_indices[i])\n        shuffled_task_indices = shuffled_task_indices.T\n\n        normalized_grad_vec = grad_vec / (grad_vec.norm(dim=1, keepdim=True) + 1e-8)  # num_tasks x dim\n        modified_grad_vec = deepcopy(grad_vec)\n        for task_indices in shuffled_task_indices:\n            normalized_shuffled_grad = normalized_grad_vec[task_indices]  # num_tasks x dim\n            dot = (modified_grad_vec * normalized_shuffled_grad).sum(dim=1, keepdim=True)  # num_tasks x dim\n            modified_grad_vec -= torch.clamp_max(dot, 0) * normalized_shuffled_grad\n        g = modified_grad_vec.mean(dim=0)\n        return g\n\n    def cagrad(grads, alpha=0.5, rescale=1):\n        GG = grads.t().mm(grads).cpu()  # [num_tasks, num_tasks]\n        g0_norm = (GG.mean() + 1e-8).sqrt()  # norm of the average gradient\n\n        x_start = np.ones(3) / 3\n        bnds = tuple((0, 1) for x in x_start)\n        cons = ({'type': 'eq', 'fun': lambda x: 1 - sum(x)})\n        A = GG.numpy()\n        b = x_start.copy()\n        c = (alpha * g0_norm + 1e-8).item()\n\n        def objfn(x):\n            return (x.reshape(1, 3).dot(A).dot(b.reshape(3, 1)) +\n                    c * np.sqrt(x.reshape(1, 3).dot(A).dot(x.reshape(3, 1)) + 1e-8)).sum()\n\n        res = minimize(objfn, x_start, bounds=bnds, constraints=cons)\n        w_cpu = res.x\n        ww = torch.Tensor(w_cpu).to(grads.device)\n        gw = (grads * ww.view(1, -1)).sum(1)\n        gw_norm = gw.norm()\n        lmbda = c / (gw_norm + 1e-8)\n        g = grads.mean(1) + lmbda * gw\n        if rescale == 0:\n            return g\n        elif rescale == 1:\n            return g / (1 + alpha**2)\n        else:\n            return g / (1 + alpha)\n\n    def sdmgrad(w, grads, alpha, niter=20):\n        GG = torch.mm(grads.t(), grads)\n        scale = torch.mean(torch.sqrt(torch.diag(GG) + 1e-4))\n        GG = GG / scale.pow(2)\n        Gg = torch.mean(GG, dim=1)\n        gg = torch.mean(Gg)\n\n        w.requires_grad = True\n        optimizer = torch.optim.SGD([w], lr=10, momentum=0.5)\n        for i in range(niter):\n            optimizer.zero_grad()\n            obj = torch.dot(w, torch.mv(GG, w)) + 2 * alpha * torch.dot(w, Gg) + alpha**2 * gg\n            obj.backward()\n            optimizer.step()\n            proj = euclidean_proj_simplex(w.data.cpu().numpy())\n            w.data.copy_(torch.from_numpy(proj).data)\n        w.requires_grad = False\n\n        g0 = torch.mean(grads, dim=1)\n        gw = torch.mv(grads, w)\n        g = (gw + alpha * g0) / (1 + alpha)\n        return g\n\n    def grad2vec(m, grads, grad_dims, task):\n        # store the gradients\n        grads[:, task].fill_(0.0)\n        cnt = 0\n        for mm in m.shared_modules():\n            for p in mm.parameters():\n                grad = p.grad\n                if grad is not None:\n                    grad_cur = grad.data.detach().clone()\n                    beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n                    en = sum(grad_dims[:cnt + 1])\n                    grads[beg:en, task].copy_(grad_cur.data.view(-1))\n                cnt += 1\n\n    def overwrite_grad(m, newgrad, grad_dims):\n        newgrad = newgrad * 3  # to match the sum loss\n        cnt = 0\n        for mm in m.shared_modules():\n            for param in mm.parameters():\n                beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n                en = sum(grad_dims[:cnt + 1])\n                this_grad = newgrad[beg:en].contiguous().view(param.data.size())\n                param.grad = this_grad.data.clone()\n                cnt += 1\n\n    rng = np.random.default_rng()\n    grad_dims = []\n    for mm in multi_task_model.shared_modules():\n        for param in mm.parameters():\n            grad_dims.append(param.data.numel())\n    grads = torch.Tensor(sum(grad_dims), 3).cuda()\n    w = 1 / 3 * torch.ones(3).cuda()\n\n    train_batch = len(train_loader)\n    test_batch = len(test_loader)\n    T = opt.temp\n    avg_cost = np.zeros([total_epoch, 24], dtype=np.float32)\n    lambda_weight = np.ones([3, total_epoch])\n\n    neg_trace = []\n    obj_trace = []\n    for index in range(total_epoch):\n        epoch_start_time = time.time()\n        cost = np.zeros(24, dtype=np.float32)\n\n        # apply Dynamic Weight Average\n        if opt.weight == 'dwa':\n            if index == 0 or index == 1:\n                lambda_weight[:, index] = 1.0\n            else:\n                w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n                w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n                w_3 = avg_cost[index - 1, 6] / avg_cost[index - 2, 6]\n                lambda_weight[0, index] = 3 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n                lambda_weight[1, index] = 3 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n                lambda_weight[2, index] = 3 * np.exp(w_3 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n\n        # iteration for all batches\n        multi_task_model.train()\n        train_dataset = iter(train_loader)\n        conf_mat = ConfMatrix(multi_task_model.class_nb)\n        for k in range(train_batch):\n            train_data, train_label, train_depth, train_normal = train_dataset.next()\n            train_data, train_label = train_data.to(device), train_label.long().to(device)\n            train_depth, train_normal = train_depth.to(device), train_normal.to(device)\n\n            train_pred, logsigma = multi_task_model(train_data)\n\n            train_loss = [\n                model_fit(train_pred[0], train_label, 'semantic'),\n                model_fit(train_pred[1], train_depth, 'depth'),\n                model_fit(train_pred[2], train_normal, 'normal')\n            ]\n\n            train_loss_tmp = [0, 0, 0]\n\n            if opt.weight == 'equal' or opt.weight == 'dwa':\n                for i in range(3):\n                    train_loss_tmp[i] = train_loss[i] * lambda_weight[i, index]\n            else:\n                for i in range(3):\n                    train_loss_tmp[i] = 1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2\n\n            optimizer.zero_grad()\n            if method == \"graddrop\":\n                for i in range(3):\n                    if i < 3:\n                        train_loss_tmp[i].backward(retain_graph=True)\n                    else:\n                        train_loss_tmp[i].backward()\n                    grad2vec(multi_task_model, grads, grad_dims, i)\n                    multi_task_model.zero_grad_shared_modules()\n                g = graddrop(grads)\n                overwrite_grad(multi_task_model, g, grad_dims)\n                optimizer.step()\n            elif method == \"mgd\":\n                for i in range(3):\n                    if i < 3:\n                        train_loss_tmp[i].backward(retain_graph=True)\n                    else:\n                        train_loss_tmp[i].backward()\n                    grad2vec(multi_task_model, grads, grad_dims, i)\n                    multi_task_model.zero_grad_shared_modules()\n                g = mgd(grads)\n                overwrite_grad(multi_task_model, g, grad_dims)\n                optimizer.step()\n            elif method == \"pcgrad\":\n                for i in range(3):\n                    if i < 3:\n                        train_loss_tmp[i].backward(retain_graph=True)\n                    else:\n                        train_loss_tmp[i].backward()\n                    grad2vec(multi_task_model, grads, grad_dims, i)\n                    multi_task_model.zero_grad_shared_modules()\n                g = pcgrad(grads, rng)\n                overwrite_grad(multi_task_model, g, grad_dims)\n                optimizer.step()\n            elif method == \"cagrad\":\n                for i in range(3):\n                    if i < 3:\n                        train_loss_tmp[i].backward(retain_graph=True)\n                    else:\n                        train_loss_tmp[i].backward()\n                    grad2vec(multi_task_model, grads, grad_dims, i)\n                    multi_task_model.zero_grad_shared_modules()\n                g = cagrad(grads, alpha, rescale=1)\n                overwrite_grad(multi_task_model, g, grad_dims)\n                optimizer.step()\n            elif method == \"sdmgrad\":\n                for i in range(3):\n                    if i < 3:\n                        train_loss_tmp[i].backward(retain_graph=True)\n                    else:\n                        train_loss_tmp[i].backward()\n                    grad2vec(multi_task_model, grads, grad_dims, i)\n                    multi_task_model.zero_grad_shared_modules()\n                g = sdmgrad(w, grads, alpha, niter=niter)\n                overwrite_grad(multi_task_model, g, grad_dims)\n                optimizer.step()\n\n            # accumulate label prediction for every pixel in training images\n            conf_mat.update(train_pred[0].argmax(1).flatten(), train_label.flatten())\n\n            cost[0] = train_loss[0].item()\n            cost[3] = train_loss[1].item()\n            cost[4], cost[5] = depth_error(train_pred[1], train_depth)\n            cost[6] = train_loss[2].item()\n            cost[7], cost[8], cost[9], cost[10], cost[11] = normal_error(train_pred[2], train_normal)\n            avg_cost[index, :12] += cost[:12] / train_batch\n\n        # compute mIoU and acc\n        avg_cost[index, 1:3] = conf_mat.get_metrics()\n\n        # evaluating test data\n        multi_task_model.eval()\n        conf_mat = ConfMatrix(multi_task_model.class_nb)\n        with torch.no_grad():  # operations inside don't track history\n            test_dataset = iter(test_loader)\n            for k in range(test_batch):\n                test_data, test_label, test_depth, test_normal = test_dataset.next()\n                test_data, test_label = test_data.to(device), test_label.long().to(device)\n                test_depth, test_normal = test_depth.to(device), test_normal.to(device)\n\n                test_pred, _ = multi_task_model(test_data)\n                test_loss = [\n                    model_fit(test_pred[0], test_label, 'semantic'),\n                    model_fit(test_pred[1], test_depth, 'depth'),\n                    model_fit(test_pred[2], test_normal, 'normal')\n                ]\n\n                conf_mat.update(test_pred[0].argmax(1).flatten(), test_label.flatten())\n\n                cost[12] = test_loss[0].item()\n                cost[15] = test_loss[1].item()\n                cost[16], cost[17] = depth_error(test_pred[1], test_depth)\n                cost[18] = test_loss[2].item()\n                cost[19], cost[20], cost[21], cost[22], cost[23] = normal_error(test_pred[2], test_normal)\n                avg_cost[index, 12:] += cost[12:] / test_batch\n\n            # compute mIoU and acc\n            avg_cost[index, 13:15] = conf_mat.get_metrics()\n\n        scheduler.step()\n        if method == \"mean\":\n            torch.save(torch.Tensor(neg_trace), \"trace.pt\")\n\n        if \"debug\" in method:\n            torch.save(torch.Tensor(obj_trace), f\"{method}_obj.pt\")\n\n        epoch_end_time = time.time()\n        print(\n            'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} ||'\n            'TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} | {:.4f}'.\n            format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n                   avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8],\n                   avg_cost[index, 9], avg_cost[index, 10], avg_cost[index, 11], avg_cost[index, 12],\n                   avg_cost[index, 13], avg_cost[index, 14], avg_cost[index, 15], avg_cost[index, 16],\n                   avg_cost[index, 17], avg_cost[index, 18], avg_cost[index, 19], avg_cost[index, 20],\n                   avg_cost[index, 21], avg_cost[index, 22], avg_cost[index, 23], epoch_end_time - epoch_start_time))\n        if \"cagrad\" in method:\n            torch.save(multi_task_model.state_dict(), f\"models/{method}-{opt.weight}-{alpha}-{seed}.pt\")\n        elif \"sdmgrad\" in method:\n            torch.save(multi_task_model.state_dict(), f\"models/{method}-{opt.weight}-{alpha}-{seed}-{niter}.pt\")\n        else:\n            torch.save(multi_task_model.state_dict(), f\"models/{method}-{opt.weight}-{seed}.pt\")\n    end_time = time.time()\n    print(\"Training time: \", end_time - start_time)", ""]}
{"filename": "nyuv2/min_norm_solvers.py", "chunked_list": ["# This code is from\n# Multi-Task Learning as Multi-Objective Optimization\n# Ozan Sener, Vladlen Koltun\n# Neural Information Processing Systems (NeurIPS) 2018\n# https://github.com/intel-isl/MultiObjectiveOptimization\n\nimport numpy as np\nimport torch\n\n\nclass MinNormSolver:\n    MAX_ITER = 20\n    STOP_CRIT = 1e-5\n\n    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n        \"\"\"\n        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n        d is the distance (objective) optimzed\n        v1v1 = <x1,x1>\n        v1v2 = <x1,x2>\n        v2v2 = <x2,x2>\n        \"\"\"\n        if v1v2 >= v1v1:\n            # Case: Fig 1, third column\n            gamma = 0.999\n            cost = v1v1\n            return gamma, cost\n        if v1v2 >= v2v2:\n            # Case: Fig 1, first column\n            gamma = 0.001\n            cost = v2v2\n            return gamma, cost\n        # Case: Fig 1, second column\n        gamma = -1.0 * ((v1v2 - v2v2) / (v1v1 + v2v2 - 2 * v1v2))\n        cost = v2v2 + gamma * (v1v2 - v2v2)\n        return gamma, cost\n\n    def _min_norm_2d(vecs, dps):\n        \"\"\"\n        Find the minimum norm solution as combination of two points\n        This is correct only in 2D\n        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n        \"\"\"\n        dmin = np.inf\n        for i in range(len(vecs)):\n            for j in range(i + 1, len(vecs)):\n                if (i, j) not in dps:\n                    dps[(i, j)] = (vecs[i] * vecs[j]).sum().item()\n                    dps[(j, i)] = dps[(i, j)]\n                if (i, i) not in dps:\n                    dps[(i, i)] = (vecs[i] * vecs[i]).sum().item()\n                if (j, j) not in dps:\n                    dps[(j, j)] = (vecs[j] * vecs[j]).sum().item()\n                c, d = MinNormSolver._min_norm_element_from2(dps[(i, i)], dps[(i, j)], dps[(j, j)])\n                if d < dmin:\n                    dmin = d\n                    sol = [(i, j), c, d]\n        return sol, dps\n\n    def _projection2simplex(y):\n        \"\"\"\n        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n        \"\"\"\n        m = len(y)\n        sorted_y = np.flip(np.sort(y), axis=0)\n        tmpsum = 0.0\n        tmax_f = (np.sum(y) - 1.0) / m\n        for i in range(m - 1):\n            tmpsum += sorted_y[i]\n            tmax = (tmpsum - 1) / (i + 1.0)\n            if tmax > sorted_y[i + 1]:\n                tmax_f = tmax\n                break\n        return np.maximum(y - tmax_f, np.zeros(y.shape))\n\n    def _next_point(cur_val, grad, n):\n        proj_grad = grad - (np.sum(grad) / n)\n        tm1 = -1.0 * cur_val[proj_grad < 0] / proj_grad[proj_grad < 0]\n        tm2 = (1.0 - cur_val[proj_grad > 0]) / (proj_grad[proj_grad > 0])\n\n        skippers = np.sum(tm1 < 1e-7) + np.sum(tm2 < 1e-7)\n        t = 1\n        if len(tm1[tm1 > 1e-7]) > 0:\n            t = np.min(tm1[tm1 > 1e-7])\n        if len(tm2[tm2 > 1e-7]) > 0:\n            t = min(t, np.min(tm2[tm2 > 1e-7]))\n\n        next_point = proj_grad * t + cur_val\n        next_point = MinNormSolver._projection2simplex(next_point)\n        return next_point\n\n    def find_min_norm_element(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            grad_dir = -1.0 * np.dot(grad_mat, sol_vec)\n            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n            # Re-compute the inner products for line search\n            v1v1 = 0.0\n            v1v2 = 0.0\n            v2v2 = 0.0\n            for i in range(n):\n                for j in range(n):\n                    v1v1 += sol_vec[i] * sol_vec[j] * dps[(i, j)]\n                    v1v2 += sol_vec[i] * new_point[j] * dps[(i, j)]\n                    v2v2 += new_point[i] * new_point[j] * dps[(i, j)]\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec + (1 - nc) * new_point\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec\n\n    def find_min_norm_element_FW(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the Frank Wolfe until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            t_iter = np.argmin(np.dot(grad_mat, sol_vec))\n\n            v1v1 = np.dot(sol_vec, np.dot(grad_mat, sol_vec))\n            v1v2 = np.dot(sol_vec, grad_mat[:, t_iter])\n            v2v2 = grad_mat[t_iter, t_iter]\n\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec\n            new_sol_vec[t_iter] += 1 - nc\n\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec", "\n\nclass MinNormSolver:\n    MAX_ITER = 20\n    STOP_CRIT = 1e-5\n\n    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n        \"\"\"\n        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n        d is the distance (objective) optimzed\n        v1v1 = <x1,x1>\n        v1v2 = <x1,x2>\n        v2v2 = <x2,x2>\n        \"\"\"\n        if v1v2 >= v1v1:\n            # Case: Fig 1, third column\n            gamma = 0.999\n            cost = v1v1\n            return gamma, cost\n        if v1v2 >= v2v2:\n            # Case: Fig 1, first column\n            gamma = 0.001\n            cost = v2v2\n            return gamma, cost\n        # Case: Fig 1, second column\n        gamma = -1.0 * ((v1v2 - v2v2) / (v1v1 + v2v2 - 2 * v1v2))\n        cost = v2v2 + gamma * (v1v2 - v2v2)\n        return gamma, cost\n\n    def _min_norm_2d(vecs, dps):\n        \"\"\"\n        Find the minimum norm solution as combination of two points\n        This is correct only in 2D\n        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n        \"\"\"\n        dmin = np.inf\n        for i in range(len(vecs)):\n            for j in range(i + 1, len(vecs)):\n                if (i, j) not in dps:\n                    dps[(i, j)] = (vecs[i] * vecs[j]).sum().item()\n                    dps[(j, i)] = dps[(i, j)]\n                if (i, i) not in dps:\n                    dps[(i, i)] = (vecs[i] * vecs[i]).sum().item()\n                if (j, j) not in dps:\n                    dps[(j, j)] = (vecs[j] * vecs[j]).sum().item()\n                c, d = MinNormSolver._min_norm_element_from2(dps[(i, i)], dps[(i, j)], dps[(j, j)])\n                if d < dmin:\n                    dmin = d\n                    sol = [(i, j), c, d]\n        return sol, dps\n\n    def _projection2simplex(y):\n        \"\"\"\n        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n        \"\"\"\n        m = len(y)\n        sorted_y = np.flip(np.sort(y), axis=0)\n        tmpsum = 0.0\n        tmax_f = (np.sum(y) - 1.0) / m\n        for i in range(m - 1):\n            tmpsum += sorted_y[i]\n            tmax = (tmpsum - 1) / (i + 1.0)\n            if tmax > sorted_y[i + 1]:\n                tmax_f = tmax\n                break\n        return np.maximum(y - tmax_f, np.zeros(y.shape))\n\n    def _next_point(cur_val, grad, n):\n        proj_grad = grad - (np.sum(grad) / n)\n        tm1 = -1.0 * cur_val[proj_grad < 0] / proj_grad[proj_grad < 0]\n        tm2 = (1.0 - cur_val[proj_grad > 0]) / (proj_grad[proj_grad > 0])\n\n        skippers = np.sum(tm1 < 1e-7) + np.sum(tm2 < 1e-7)\n        t = 1\n        if len(tm1[tm1 > 1e-7]) > 0:\n            t = np.min(tm1[tm1 > 1e-7])\n        if len(tm2[tm2 > 1e-7]) > 0:\n            t = min(t, np.min(tm2[tm2 > 1e-7]))\n\n        next_point = proj_grad * t + cur_val\n        next_point = MinNormSolver._projection2simplex(next_point)\n        return next_point\n\n    def find_min_norm_element(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            grad_dir = -1.0 * np.dot(grad_mat, sol_vec)\n            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n            # Re-compute the inner products for line search\n            v1v1 = 0.0\n            v1v2 = 0.0\n            v2v2 = 0.0\n            for i in range(n):\n                for j in range(n):\n                    v1v1 += sol_vec[i] * sol_vec[j] * dps[(i, j)]\n                    v1v2 += sol_vec[i] * new_point[j] * dps[(i, j)]\n                    v2v2 += new_point[i] * new_point[j] * dps[(i, j)]\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec + (1 - nc) * new_point\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec\n\n    def find_min_norm_element_FW(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the Frank Wolfe until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            t_iter = np.argmin(np.dot(grad_mat, sol_vec))\n\n            v1v1 = np.dot(sol_vec, np.dot(grad_mat, sol_vec))\n            v1v2 = np.dot(sol_vec, grad_mat[:, t_iter])\n            v2v2 = grad_mat[t_iter, t_iter]\n\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec\n            new_sol_vec[t_iter] += 1 - nc\n\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec", "\n\ndef gradient_normalizers(grads, losses, normalization_type):\n    gn = {}\n    if normalization_type == 'l2':\n        for t in grads:\n            gn[t] = np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n    elif normalization_type == 'loss':\n        for t in grads:\n            gn[t] = losses[t]\n    elif normalization_type == 'loss+':\n        for t in grads:\n            gn[t] = losses[t] * np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n    elif normalization_type == 'none':\n        for t in grads:\n            gn[t] = 1.0\n    else:\n        print('ERROR: Invalid Normalization Type')\n    return gn", ""]}
{"filename": "nyuv2/model_segnet_cross.py", "chunked_list": ["import numpy as np\nimport random\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nimport torch.utils.data.sampler as sampler\n\nfrom create_dataset import *\nfrom utils import *", "from create_dataset import *\nfrom utils import *\n\nparser = argparse.ArgumentParser(description='Multi-task: Cross')\nparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\nparser.add_argument('--dataroot', default='nyuv2', type=str, help='dataset root')\nparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\nparser.add_argument('--seed', default=0, type=int, help='the seed')\nparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()", "parser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block_t = nn.ModuleList(\n            [nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)])])\n        self.decoder_block_t = nn.ModuleList(\n            [nn.ModuleList([self.conv_layer([filter[0], filter[0], filter[0]], bottle_neck=True)])])\n\n        for j in range(3):\n            if j < 2:\n                self.encoder_block_t.append(\n                    nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)]))\n                self.decoder_block_t.append(\n                    nn.ModuleList([self.conv_layer([filter[0], filter[0], filter[0]], bottle_neck=True)]))\n            for i in range(4):\n                if i == 0:\n                    self.encoder_block_t[j].append(\n                        self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=True))\n                    self.decoder_block_t[j].append(\n                        self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=True))\n                else:\n                    self.encoder_block_t[j].append(\n                        self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=False))\n                    self.decoder_block_t[j].append(\n                        self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=False))\n\n        # define cross-stitch units\n        self.cs_unit_encoder = nn.Parameter(data=torch.ones(4, 3))\n        self.cs_unit_decoder = nn.Parameter(data=torch.ones(5, 3))\n\n        # define task specific layers\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], bottle_neck=True, pred_layer=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], bottle_neck=True, pred_layer=True)\n        self.pred_task3 = self.conv_layer([filter[0], 3], bottle_neck=True, pred_layer=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Parameter):\n                nn.init.constant(m.weight, 1)\n\n    def conv_layer(self, channel, bottle_neck, pred_layer=False):\n        if bottle_neck:\n            if not pred_layer:\n                conv_block = nn.Sequential(\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                    nn.BatchNorm2d(channel[1]),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n                    nn.BatchNorm2d(channel[2]),\n                    nn.ReLU(inplace=True),\n                )\n            else:\n                conv_block = nn.Sequential(\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n                )\n\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[2]),\n                nn.ReLU(inplace=True),\n            )\n        return conv_block\n\n    def forward(self, x):\n        encoder_conv_t, decoder_conv_t, encoder_samp_t, decoder_samp_t, indices_t = ([0] * 3 for _ in range(5))\n        for i in range(3):\n            encoder_conv_t[i], decoder_conv_t[i], encoder_samp_t[i], decoder_samp_t[i], indices_t[i] = (\n                [0] * 5 for _ in range(5))\n\n        # task branch 1\n        for i in range(5):\n            for j in range(3):\n                if i == 0:\n                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](x)\n                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n                else:\n                    encoder_cross_stitch = self.cs_unit_encoder[i - 1][0] * encoder_samp_t[0][i - 1] + \\\n                                           self.cs_unit_encoder[i - 1][1] * encoder_samp_t[1][i - 1] + \\\n                                           self.cs_unit_encoder[i - 1][2] * encoder_samp_t[2][i - 1]\n                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](encoder_cross_stitch)\n                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n\n        for i in range(5):\n            for j in range(3):\n                if i == 0:\n                    decoder_cross_stitch = self.cs_unit_decoder[i][0] * encoder_samp_t[0][-1] + \\\n                                           self.cs_unit_decoder[i][1] * encoder_samp_t[1][-1] + \\\n                                           self.cs_unit_decoder[i][2] * encoder_samp_t[2][-1]\n                    decoder_samp_t[j][i] = self.up_sampling(decoder_cross_stitch, indices_t[j][-i - 1])\n                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](decoder_samp_t[j][i])\n                else:\n                    decoder_cross_stitch = self.cs_unit_decoder[i][0] * decoder_conv_t[0][i - 1] + \\\n                                           self.cs_unit_decoder[i][1] * decoder_conv_t[1][i - 1] + \\\n                                           self.cs_unit_decoder[i][2] * decoder_conv_t[2][i - 1]\n                    decoder_samp_t[j][i] = self.up_sampling(decoder_cross_stitch, indices_t[j][-i - 1])\n                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](decoder_samp_t[j][i])\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(decoder_conv_t[0][-1]), dim=1)\n        t2_pred = self.pred_task2(decoder_conv_t[1][-1])\n        t3_pred = self.pred_task3(decoder_conv_t[2][-1])\n        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred, t3_pred], self.logsigma", "\n\n# control seed\ntorch.backends.cudnn.enabled = False\ntorch.manual_seed(opt.seed)\nnp.random.seed(opt.seed)\nrandom.seed(opt.seed)\ntorch.cuda.manual_seed_all(opt.seed)\n\n# define model, optimiser and scheduler", "\n# define model, optimiser and scheduler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSegNet_CROSS = SegNet().to(device)\noptimizer = optim.Adam(SegNet_CROSS.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_CROSS),\n                                                         count_parameters(SegNet_CROSS) / 24981069))\nprint(", "                                                         count_parameters(SegNet_CROSS) / 24981069))\nprint(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\n# define dataset\ndataset_path = opt.dataroot\nif opt.apply_augmentation:\n    nyuv2_train_set = NYUv2(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation on NYUv2.')\nelse:\n    nyuv2_train_set = NYUv2(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')", "\nnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\nbatch_size = 2\nnyuv2_train_loader = torch.utils.data.DataLoader(dataset=nyuv2_train_set, batch_size=batch_size, shuffle=True)\n\nnyuv2_test_loader = torch.utils.data.DataLoader(dataset=nyuv2_test_set, batch_size=batch_size, shuffle=False)\n\n# Train and evaluate multi-task network\nmulti_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_CROSS, device, optimizer, scheduler, opt, 200)", "# Train and evaluate multi-task network\nmulti_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_CROSS, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "nyuv2/model_segnet_mt.py", "chunked_list": ["import numpy as np\nimport random\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nimport torch.utils.data.sampler as sampler\n\nfrom create_dataset import *\nfrom utils import *", "from create_dataset import *\nfrom utils import *\n\nparser = argparse.ArgumentParser(description='Multi-task: Split')\nparser.add_argument('--type', default='standard', type=str, help='split type: standard, wide, deep')\nparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\nparser.add_argument('--dataroot', default='nyuv2', type=str, help='dataset root')\nparser.add_argument('--method', default='sdmgrad', type=str, help='optimization method')\nparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\nparser.add_argument('--alpha', default=0.3, type=float, help='the alpha')", "parser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\nparser.add_argument('--alpha', default=0.3, type=float, help='the alpha')\nparser.add_argument('--lr', default=1e-4, type=float, help='the learning rate')\nparser.add_argument('--seed', default=1, type=int, help='the seed')\nparser.add_argument('--niter', default=20, type=int, help='number of inner iteration')\nparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        # define task attention layers\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(3):\n            if j < 2:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n        self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def shared_modules(self):\n        return [\n            self.encoder_block,\n            self.decoder_block,\n            self.conv_block_enc,\n            self.conv_block_dec,\n            #self.encoder_att, self.decoder_att,\n            self.encoder_block_att,\n            self.decoder_block_att,\n            self.down_sampling,\n            self.up_sampling\n        ]\n\n    def zero_grad_shared_modules(self):\n        for mm in self.shared_modules():\n            mm.zero_grad()\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for tasks\n        atten_encoder, atten_decoder = ([0] * 3 for _ in range(2))\n        for i in range(3):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(3):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(3):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n        t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred, t3_pred], self.logsigma", "\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        # define task attention layers\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(3):\n            if j < 2:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n        self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def shared_modules(self):\n        return [\n            self.encoder_block,\n            self.decoder_block,\n            self.conv_block_enc,\n            self.conv_block_dec,\n            #self.encoder_att, self.decoder_att,\n            self.encoder_block_att,\n            self.decoder_block_att,\n            self.down_sampling,\n            self.up_sampling\n        ]\n\n    def zero_grad_shared_modules(self):\n        for mm in self.shared_modules():\n            mm.zero_grad()\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for tasks\n        atten_encoder, atten_decoder = ([0] * 3 for _ in range(2))\n        for i in range(3):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(3):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(3):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n        t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred, t3_pred], self.logsigma", "\n\nclass SegNetSplit(nn.Module):\n\n    def __init__(self):\n        super(SegNetSplit, self).__init__()\n        # initialise network parameters\n        if opt.type == 'wide':\n            filter = [64, 128, 256, 512, 1024]\n        else:\n            filter = [64, 128, 256, 512, 512]\n\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        # define task specific layers\n        self.pred_task1 = nn.Sequential(\n            nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n            nn.Conv2d(in_channels=filter[0], out_channels=self.class_nb, kernel_size=1, padding=0))\n        self.pred_task2 = nn.Sequential(\n            nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n            nn.Conv2d(in_channels=filter[0], out_channels=1, kernel_size=1, padding=0))\n        self.pred_task3 = nn.Sequential(\n            nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n            nn.Conv2d(in_channels=filter[0], out_channels=3, kernel_size=1, padding=0))\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    # define convolutional block\n    def conv_layer(self, channel):\n        if opt.type == 'deep':\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]), nn.ReLU(inplace=True))\n        return conv_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # global shared encoder-decoder network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(g_decoder[i][1]), dim=1)\n        t2_pred = self.pred_task2(g_decoder[i][1])\n        t3_pred = self.pred_task3(g_decoder[i][1])\n        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred, t3_pred], self.logsigma", "\n\n# control seed\ntorch.backends.cudnn.enabled = False\ntorch.manual_seed(opt.seed)\nnp.random.seed(opt.seed)\nrandom.seed(opt.seed)\ntorch.cuda.manual_seed_all(opt.seed)\n\n# define model, optimiser and scheduler", "\n# define model, optimiser and scheduler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSegNet_MTAN = SegNet().to(device)\noptimizer = optim.Adam(SegNet_MTAN.parameters(), lr=opt.lr)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_MTAN),\n                                                         count_parameters(SegNet_MTAN) / 24981069))\nprint(", "                                                         count_parameters(SegNet_MTAN) / 24981069))\nprint(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\n# define dataset\ndataset_path = opt.dataroot\nif opt.apply_augmentation:\n    nyuv2_train_set = NYUv2(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation on NYUv2.')\nelse:\n    nyuv2_train_set = NYUv2(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')", "\nnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\nbatch_size = 2\nnyuv2_train_loader = torch.utils.data.DataLoader(dataset=nyuv2_train_set, batch_size=batch_size, shuffle=True)\n\nnyuv2_test_loader = torch.utils.data.DataLoader(dataset=nyuv2_test_set, batch_size=batch_size, shuffle=False)\n\n# Train and evaluate multi-task network\nmulti_task_mgd_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_MTAN, device, optimizer, scheduler, opt, 200,", "# Train and evaluate multi-task network\nmulti_task_mgd_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_MTAN, device, optimizer, scheduler, opt, 200,\n                       opt.method, opt.alpha, opt.seed)\n"]}
{"filename": "nyuv2/create_dataset.py", "chunked_list": ["from torch.utils.data.dataset import Dataset\n\nimport os\nimport torch\nimport torch.nn.functional as F\nimport fnmatch\nimport numpy as np\nimport random\n\n\nclass RandomScaleCrop(object):\n    \"\"\"\n    Credit to Jialong Wu from https://github.com/lorenmt/mtan/issues/34.\n    \"\"\"\n\n    def __init__(self, scale=[1.0, 1.2, 1.5]):\n        self.scale = scale\n\n    def __call__(self, img, label, depth, normal):\n        height, width = img.shape[-2:]\n        sc = self.scale[random.randint(0, len(self.scale) - 1)]\n        h, w = int(height / sc), int(width / sc)\n        i = random.randint(0, height - h)\n        j = random.randint(0, width - w)\n        img_ = F.interpolate(img[None, :, i:i + h, j:j + w], size=(height, width), mode='bilinear',\n                             align_corners=True).squeeze(0)\n        label_ = F.interpolate(label[None, None, i:i + h, j:j + w], size=(height, width),\n                               mode='nearest').squeeze(0).squeeze(0)\n        depth_ = F.interpolate(depth[None, :, i:i + h, j:j + w], size=(height, width), mode='nearest').squeeze(0)\n        normal_ = F.interpolate(normal[None, :, i:i + h, j:j + w],\n                                size=(height, width),\n                                mode='bilinear',\n                                align_corners=True).squeeze(0)\n        return img_, label_, depth_ / sc, normal_", "\n\nclass RandomScaleCrop(object):\n    \"\"\"\n    Credit to Jialong Wu from https://github.com/lorenmt/mtan/issues/34.\n    \"\"\"\n\n    def __init__(self, scale=[1.0, 1.2, 1.5]):\n        self.scale = scale\n\n    def __call__(self, img, label, depth, normal):\n        height, width = img.shape[-2:]\n        sc = self.scale[random.randint(0, len(self.scale) - 1)]\n        h, w = int(height / sc), int(width / sc)\n        i = random.randint(0, height - h)\n        j = random.randint(0, width - w)\n        img_ = F.interpolate(img[None, :, i:i + h, j:j + w], size=(height, width), mode='bilinear',\n                             align_corners=True).squeeze(0)\n        label_ = F.interpolate(label[None, None, i:i + h, j:j + w], size=(height, width),\n                               mode='nearest').squeeze(0).squeeze(0)\n        depth_ = F.interpolate(depth[None, :, i:i + h, j:j + w], size=(height, width), mode='nearest').squeeze(0)\n        normal_ = F.interpolate(normal[None, :, i:i + h, j:j + w],\n                                size=(height, width),\n                                mode='bilinear',\n                                align_corners=True).squeeze(0)\n        return img_, label_, depth_ / sc, normal_", "\n\nclass NYUv2(Dataset):\n    \"\"\"\n    We could further improve the performance with the data augmentation of NYUv2 defined in:\n        [1] PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing\n        [2] Pattern affinitive propagation across depth, surface normal and semantic segmentation\n        [3] Mti-net: Multiscale task interaction networks for multi-task learning\n\n        1. Random scale in a selected raio 1.0, 1.2, and 1.5.\n        2. Random horizontal flip.\n\n    Please note that: all baselines and MTAN did NOT apply data augmentation in the original paper.\n    \"\"\"\n\n    def __init__(self, root, train=True, augmentation=False):\n        self.train = train\n        self.root = os.path.expanduser(root)\n        self.augmentation = augmentation\n\n        # read the data file\n        if train:\n            self.data_path = root + '/train'\n        else:\n            self.data_path = root + '/val'\n\n        # calculate data length\n        self.data_len = len(fnmatch.filter(os.listdir(self.data_path + '/image'), '*.npy'))\n\n    def __getitem__(self, index):\n        # load data from the pre-processed npy files\n        image = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/image/{:d}.npy'.format(index)), -1, 0))\n        semantic = torch.from_numpy(np.load(self.data_path + '/label/{:d}.npy'.format(index)))\n        depth = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/depth/{:d}.npy'.format(index)), -1, 0))\n        normal = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/normal/{:d}.npy'.format(index)), -1, 0))\n\n        # apply data augmentation if required\n        if self.augmentation:\n            image, semantic, depth, normal = RandomScaleCrop()(image, semantic, depth, normal)\n            if torch.rand(1) < 0.5:\n                image = torch.flip(image, dims=[2])\n                semantic = torch.flip(semantic, dims=[1])\n                depth = torch.flip(depth, dims=[2])\n                normal = torch.flip(normal, dims=[2])\n                normal[0, :, :] = -normal[0, :, :]\n\n        return image.float(), semantic.float(), depth.float(), normal.float()\n\n    def __len__(self):\n        return self.data_len", ""]}
{"filename": "nyuv2/model_segnet_single.py", "chunked_list": ["import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\n\nfrom create_dataset import *\nfrom utils import *\n\nparser = argparse.ArgumentParser(description='Single-task: One Task')\nparser.add_argument('--task', default='semantic', type=str, help='choose task: semantic, depth, normal')", "parser = argparse.ArgumentParser(description='Single-task: One Task')\nparser.add_argument('--task', default='semantic', type=str, help='choose task: semantic, depth, normal')\nparser.add_argument('--dataroot', default='nyuv2', type=str, help='dataset root')\nparser.add_argument('--seed', default=0, type=int, help='the seed')\nparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        if opt.task == 'semantic':\n            self.pred_task = self.conv_layer([filter[0], self.class_nb], pred=True)\n        if opt.task == 'depth':\n            self.pred_task = self.conv_layer([filter[0], 1], pred=True)\n        if opt.task == 'normal':\n            self.pred_task = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task prediction layers\n        if opt.task == 'semantic':\n            pred = F.log_softmax(self.pred_task(g_decoder[-1][-1]), dim=1)\n        if opt.task == 'depth':\n            pred = self.pred_task(g_decoder[-1][-1])\n        if opt.task == 'normal':\n            pred = self.pred_task(g_decoder[-1][-1])\n            pred = pred / torch.norm(pred, p=2, dim=1, keepdim=True)\n        return pred", "class SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        if opt.task == 'semantic':\n            self.pred_task = self.conv_layer([filter[0], self.class_nb], pred=True)\n        if opt.task == 'depth':\n            self.pred_task = self.conv_layer([filter[0], 1], pred=True)\n        if opt.task == 'normal':\n            self.pred_task = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task prediction layers\n        if opt.task == 'semantic':\n            pred = F.log_softmax(self.pred_task(g_decoder[-1][-1]), dim=1)\n        if opt.task == 'depth':\n            pred = self.pred_task(g_decoder[-1][-1])\n        if opt.task == 'normal':\n            pred = self.pred_task(g_decoder[-1][-1])\n            pred = pred / torch.norm(pred, p=2, dim=1, keepdim=True)\n        return pred", "\n\n# control seed\ntorch.backends.cudnn.enabled = False\ntorch.manual_seed(opt.seed)\nnp.random.seed(opt.seed)\nrandom.seed(opt.seed)\ntorch.cuda.manual_seed_all(opt.seed)\n\n# define model, optimiser and scheduler", "\n# define model, optimiser and scheduler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSegNet = SegNet().to(device)\noptimizer = optim.Adam(SegNet.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet), count_parameters(SegNet) / 24981069))\nprint(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')", "print(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\n# define dataset\ndataset_path = opt.dataroot\nif opt.apply_augmentation:\n    nyuv2_train_set = NYUv2(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation on NYUv2.')\nelse:\n    nyuv2_train_set = NYUv2(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')", "\nnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\nbatch_size = 2\nnyuv2_train_loader = torch.utils.data.DataLoader(dataset=nyuv2_train_set, batch_size=batch_size, shuffle=True)\n\nnyuv2_test_loader = torch.utils.data.DataLoader(dataset=nyuv2_test_set, batch_size=batch_size, shuffle=False)\n\n# Train and evaluate single-task network\nsingle_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet, device, optimizer, scheduler, opt, 200)", "# Train and evaluate single-task network\nsingle_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "consistency/train.py", "chunked_list": ["import numpy as np\n\nimport torch\nimport torch.utils.data\nfrom torch import linalg as LA\nfrom torch.autograd import Variable\n\nfrom model_lenet import RegressionModel, RegressionTrain\nfrom model_resnet import MnistResNet, RegressionTrainResNet\nfrom utils import *", "from model_resnet import MnistResNet, RegressionTrainResNet\nfrom utils import *\n\nimport pickle\nimport argparse\n\nparser = argparse.ArgumentParser(description='Multi-Fashion-MNIST')\nparser.add_argument('--base', default='lenet', type=str, help='base model')\nparser.add_argument('--solver', default='sdmgrad', type=str, help='which optimization algorithm to use')\nparser.add_argument('--alpha', default=0.5, type=float, help='the alpha used in cagrad')", "parser.add_argument('--solver', default='sdmgrad', type=str, help='which optimization algorithm to use')\nparser.add_argument('--alpha', default=0.5, type=float, help='the alpha used in cagrad')\nparser.add_argument('--lmbda', default=0.5, type=float, help='the lmbda used in sdmgrad')\nparser.add_argument('--seed', default=0, type=int, help='the seed')\nparser.add_argument('--niter', default=100, type=int, help='step of (outer) iteration')\nparser.add_argument('--initer', default=20, type=int, help='step of inner itration')\nargs = parser.parse_args()\n\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)", "torch.manual_seed(args.seed)\nnp.random.seed(args.seed)\ntorch.cuda.manual_seed_all(args.seed)\n\n\ndef train(dataset, base_model, solver, alpha, lmbda, niter, initer):\n\n    # generate #npref preference vectors\n    n_tasks = 2\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    # load dataset\n\n    # MultiMNIST: multi_mnist.pickle\n    if dataset == 'mnist':\n        with open('./data/multi_mnist.pickle', 'rb') as f:\n            trainX, trainLabel, testX, testLabel = pickle.load(f)\n\n    # MultiFashionMNIST: multi_fashion.pickle\n    if dataset == 'fashion':\n        with open('./data/multi_fashion.pickle', 'rb') as f:\n            trainX, trainLabel, testX, testLabel = pickle.load(f)\n\n    # Multi-(Fashion+MNIST): multi_fashion_and_mnist.pickle\n    if dataset == 'fashion_and_mnist':\n        with open('./data/multi_fashion_and_mnist.pickle', 'rb') as f:\n            trainX, trainLabel, testX, testLabel = pickle.load(f)\n\n    trainX = torch.from_numpy(trainX.reshape(120000, 1, 36, 36)).float()\n    trainLabel = torch.from_numpy(trainLabel).long()\n    testX = torch.from_numpy(testX.reshape(20000, 1, 36, 36)).float()\n    testLabel = torch.from_numpy(testLabel).long()\n\n    train_set = torch.utils.data.TensorDataset(trainX, trainLabel)\n    test_set = torch.utils.data.TensorDataset(testX, testLabel)\n\n    batch_size = 256\n    train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\n    print('==>>> total trainning batch number: {}'.format(len(train_loader)))\n    print('==>>> total testing batch number: {}'.format(len(test_loader)))\n\n    # define the base model for ParetoMTL\n    if base_model == 'lenet':\n        model = RegressionModel(n_tasks).to(device)\n    if base_model == 'resnet18':\n        model = MnistResNet(n_tasks).to(device)\n\n    # choose different optimizer for different base model\n    if base_model == 'lenet':\n        optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 30, 45, 60, 75, 90], gamma=0.5)\n\n    if base_model == 'resnet18':\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20], gamma=0.1)\n\n    # store infomation during optimization\n    task_train_losses = []\n    train_accs = []\n\n    # grad\n    grad_dims = []\n    for mm in model.shared_modules():\n        for param in mm.parameters():\n            grad_dims.append(param.data.numel())\n    grads = torch.Tensor(sum(grad_dims), n_tasks).to(device)\n    w = torch.ones(n_tasks).to(device) / n_tasks\n\n    # run niter epochs\n    for t in range(niter):\n\n        model.train()\n        for it, (X, ts) in enumerate(train_loader):\n\n            X, ts = X.to(device), ts.to(device)\n\n            optimizer.zero_grad()\n            # compute stochastic gradient\n            task_loss = model.forward_loss(X, ts)\n\n            # \\nabla F, grads [n_model, n_tasks]\n            for i in range(n_tasks):\n                if i == 0:\n                    task_loss[i].backward(retain_graph=True)\n                else:\n                    task_loss[i].backward()\n                grad2vec(model, grads, grad_dims, i)\n                model.zero_grad_shared_modules()\n\n            if solver == 'cagrad':\n                g = cagrad(grads, alpha, rescale=1)\n            elif solver == 'mgd':\n                g = mgd(grads)\n            elif solver == 'sgd':\n                g = mean_grad(grads)\n            elif solver == 'sdmgrad':\n                g = sdmgrad(w, grads, lmbda, initer)\n            else:\n                raise ValueError('Not supported solver.')\n            overwrite_grad(model, g, grad_dims)\n\n            # optimization step\n            optimizer.step()\n            scheduler.step()\n\n        # calculate and record performance\n        if t == 0 or (t + 1) % 2 == 0:\n\n            model.eval()\n            with torch.no_grad():\n\n                total_train_loss = []\n                train_acc = []\n\n                correct1_train = 0\n                correct2_train = 0\n\n                for it, (X, ts) in enumerate(train_loader):\n\n                    X, ts = X.to(device), ts.to(device)\n\n                    valid_train_loss = model.forward_loss(X, ts)\n                    total_train_loss.append(valid_train_loss)\n                    output1 = model(X).max(2, keepdim=True)[1][:, 0]\n                    output2 = model(X).max(2, keepdim=True)[1][:, 1]\n                    correct1_train += output1.eq(ts[:, 0].view_as(output1)).sum().item()\n                    correct2_train += output2.eq(ts[:, 1].view_as(output2)).sum().item()\n\n                train_acc = np.stack([\n                    1.0 * correct1_train / len(train_loader.dataset), 1.0 * correct2_train / len(train_loader.dataset)\n                ])\n\n                total_train_loss = torch.stack(total_train_loss)\n                average_train_loss = torch.mean(total_train_loss, dim=0)\n\n            # record and print\n            task_train_losses.append(average_train_loss.data.cpu().numpy())\n            train_accs.append(train_acc)\n\n            print('{}/{}: train_loss={}, train_acc={}'.format(t + 1, niter, task_train_losses[-1], train_accs[-1]))\n\n    save_path = './saved_model/%s_%s_solver_%s_niter_%d_seed_%d.pickle' % (dataset, base_model, solver, niter,\n                                                                           args.seed)\n    torch.save(model.state_dict(), save_path)", "\n\ndef run(dataset='mnist', base_model='lenet', solver='sdmgrad', alpha=0.5, lmbda=0.5, niter=100, initer=20):\n    \"\"\"\n    run stochatic moo algorithms\n    \"\"\"\n\n    train(dataset, base_model, solver, alpha, lmbda, niter, initer)\n\n", "\n\nrun(dataset='fashion_and_mnist',\n    base_model=args.base,\n    solver=args.solver,\n    alpha=args.alpha,\n    lmbda=args.lmbda,\n    niter=args.niter,\n    initer=args.initer)\n", "    initer=args.initer)\n"]}
{"filename": "consistency/utils.py", "chunked_list": ["import numpy as np\nfrom min_norm_solvers import MinNormSolver\nfrom scipy.optimize import minimize, Bounds, minimize_scalar\n\nimport torch\nfrom torch import linalg as LA\nfrom torch.nn import functional as F\n\n\ndef euclidean_proj_simplex(v, s=1):\n    \"\"\" Compute the Euclidean projection on a positive simplex\n    Solves the optimisation problem (using the algorithm from [1]):\n        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0\n    Parameters\n    ----------\n    v: (n,) numpy array,\n       n-dimensional vector to project\n    s: int, optional, default: 1,\n       radius of the simplex\n    Returns\n    -------\n    w: (n,) numpy array,\n       Euclidean projection of v on the simplex\n    Notes\n    -----\n    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n    However, this implementation still easily scales to millions of dimensions.\n    References\n    ----------\n    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n        International Conference on Machine Learning (ICML 2008)\n        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n    [2] Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application\n        Weiran Wang, Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n. arXiv:1309.1541\n        https://arxiv.org/pdf/1309.1541.pdf\n    [3] https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246#file-simplex_projection-py\n    \"\"\"\n    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n    v = v.astype(np.float64)\n    n, = v.shape  # will raise ValueError if v is not 1-D\n    # check if we are already on the simplex\n    if v.sum() == s and np.alltrue(v >= 0):\n        # best projection: itself!\n        return v\n    # get the array of cumulative sums of a sorted (decreasing) copy of v\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    # get the number of > 0 components of the optimal solution\n    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - s))[0][-1]\n    # compute the Lagrange multiplier associated to the simplex constraint\n    theta = float(cssv[rho] - s) / (rho + 1)\n    # compute the projection by thresholding v using theta\n    w = (v - theta).clip(min=0)\n    return w", "\ndef euclidean_proj_simplex(v, s=1):\n    \"\"\" Compute the Euclidean projection on a positive simplex\n    Solves the optimisation problem (using the algorithm from [1]):\n        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0\n    Parameters\n    ----------\n    v: (n,) numpy array,\n       n-dimensional vector to project\n    s: int, optional, default: 1,\n       radius of the simplex\n    Returns\n    -------\n    w: (n,) numpy array,\n       Euclidean projection of v on the simplex\n    Notes\n    -----\n    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n    However, this implementation still easily scales to millions of dimensions.\n    References\n    ----------\n    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n        International Conference on Machine Learning (ICML 2008)\n        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n    [2] Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application\n        Weiran Wang, Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n. arXiv:1309.1541\n        https://arxiv.org/pdf/1309.1541.pdf\n    [3] https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246#file-simplex_projection-py\n    \"\"\"\n    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n    v = v.astype(np.float64)\n    n, = v.shape  # will raise ValueError if v is not 1-D\n    # check if we are already on the simplex\n    if v.sum() == s and np.alltrue(v >= 0):\n        # best projection: itself!\n        return v\n    # get the array of cumulative sums of a sorted (decreasing) copy of v\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    # get the number of > 0 components of the optimal solution\n    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - s))[0][-1]\n    # compute the Lagrange multiplier associated to the simplex constraint\n    theta = float(cssv[rho] - s) / (rho + 1)\n    # compute the projection by thresholding v using theta\n    w = (v - theta).clip(min=0)\n    return w", "\n\ndef grad2vec(m, grads, grad_dims, task):\n    # store the gradients\n    grads[:, task].fill_(0.0)\n    cnt = 0\n    for mm in m.shared_modules():\n        for p in mm.parameters():\n            grad = p.grad\n            if grad is not None:\n                grad_cur = grad.data.detach().clone()\n                beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n                en = sum(grad_dims[:cnt + 1])\n                grads[beg:en, task].copy_(grad_cur.data.view(-1))\n            cnt += 1", "\n\ndef overwrite_grad(m, newgrad, grad_dims):\n    # newgrad = newgrad * 2 # to match the sum loss\n    cnt = 0\n    for mm in m.shared_modules():\n        for param in mm.parameters():\n            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n            en = sum(grad_dims[:cnt + 1])\n            this_grad = newgrad[beg:en].contiguous().view(param.data.size())\n            param.grad = this_grad.data.clone()\n            cnt += 1", "\n\ndef mean_grad(grads):\n    return grads.mean(1)\n\n\ndef mgd(grads):\n    grads_cpu = grads.t().cpu()\n    sol, min_norm = MinNormSolver.find_min_norm_element([grads_cpu[t] for t in range(grads.shape[-1])])\n    w = torch.FloatTensor(sol).to(grads.device)\n    g = grads.mm(w.view(-1, 1)).view(-1)\n    return g", "\n\ndef cagrad(grads, alpha=0.5, rescale=0):\n    g1 = grads[:, 0]\n    g2 = grads[:, 1]\n\n    g11 = g1.dot(g1).item()\n    g12 = g1.dot(g2).item()\n    g22 = g2.dot(g2).item()\n\n    g0_norm = 0.5 * np.sqrt(g11 + g22 + 2 * g12)\n\n    # want to minimize g_w^Tg_0 + c*||g_0||*||g_w||\n    coef = alpha * g0_norm\n\n    def obj(x):\n        # g_w^T g_0: x*0.5*(g11+g22-2g12)+(0.5+x)*(g12-g22)+g22\n        # g_w^T g_w: x^2*(g11+g22-2g12)+2*x*(g12-g22)+g22\n        return coef * np.sqrt(x**2 * (g11 + g22 - 2 * g12) + 2 * x * (g12 - g22) + g22 +\n                              1e-8) + 0.5 * x * (g11 + g22 - 2 * g12) + (0.5 + x) * (g12 - g22) + g22\n\n    res = minimize_scalar(obj, bounds=(0, 1), method='bounded')\n    x = res.x\n\n    gw_norm = np.sqrt(x**2 * g11 + (1 - x)**2 * g22 + 2 * x * (1 - x) * g12 + 1e-8)\n    lmbda = coef / (gw_norm + 1e-8)\n    g = (0.5 + lmbda * x) * g1 + (0.5 + lmbda * (1 - x)) * g2  # g0 + lmbda*gw\n    if rescale == 0:\n        return g\n    elif rescale == 1:\n        return g / (1 + alpha**2)\n    else:\n        return g / (1 + alpha)", "\n\ndef sdmgrad(w, grads, lmbda, niter=20):\n    \"\"\"\n    our proposed sdmgrad\n    \"\"\"\n    GG = torch.mm(grads.t(), grads)\n    scale = torch.mean(torch.sqrt(torch.diag(GG) + 1e-4))\n    GG = GG / scale.pow(2)\n    Gg = torch.mean(GG, dim=1)\n    gg = torch.mean(Gg)\n\n    w.requires_grad = True\n    optimizer = torch.optim.SGD([w], lr=10, momentum=0.5)\n    for i in range(niter):\n        optimizer.zero_grad()\n        obj = torch.dot(w, torch.mv(GG, w)) + 2 * lmbda * torch.dot(w, Gg) + lmbda**2 * gg\n        obj.backward()\n        optimizer.step()\n        proj = euclidean_proj_simplex(w.data.cpu().numpy())\n        w.data.copy_(torch.from_numpy(proj).data)\n    w.requires_grad = False\n\n    g0 = torch.mean(grads, dim=1)\n    gw = torch.mv(grads, w)\n    g = (gw + lmbda * g0) / (1 + lmbda)", ""]}
{"filename": "consistency/min_norm_solvers.py", "chunked_list": ["# This code is from\n# Multi-Task Learning as Multi-Objective Optimization\n# Ozan Sener, Vladlen Koltun\n# Neural Information Processing Systems (NeurIPS) 2018\n# https://github.com/intel-isl/MultiObjectiveOptimization\n\nimport numpy as np\nimport torch\n\n\nclass MinNormSolver:\n    MAX_ITER = 20\n    STOP_CRIT = 1e-5\n\n    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n        \"\"\"\n        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n        d is the distance (objective) optimzed\n        v1v1 = <x1,x1>\n        v1v2 = <x1,x2>\n        v2v2 = <x2,x2>\n        \"\"\"\n        if v1v2 >= v1v1:\n            # Case: Fig 1, third column\n            gamma = 0.999\n            cost = v1v1\n            return gamma, cost\n        if v1v2 >= v2v2:\n            # Case: Fig 1, first column\n            gamma = 0.001\n            cost = v2v2\n            return gamma, cost\n        # Case: Fig 1, second column\n        gamma = -1.0 * ((v1v2 - v2v2) / (v1v1 + v2v2 - 2 * v1v2))\n        cost = v2v2 + gamma * (v1v2 - v2v2)\n        return gamma, cost\n\n    def _min_norm_2d(vecs, dps):\n        \"\"\"\n        Find the minimum norm solution as combination of two points\n        This is correct only in 2D\n        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n        \"\"\"\n        dmin = np.inf\n        for i in range(len(vecs)):\n            for j in range(i + 1, len(vecs)):\n                if (i, j) not in dps:\n                    dps[(i, j)] = (vecs[i] * vecs[j]).sum().item()\n                    dps[(j, i)] = dps[(i, j)]\n                if (i, i) not in dps:\n                    dps[(i, i)] = (vecs[i] * vecs[i]).sum().item()\n                if (j, j) not in dps:\n                    dps[(j, j)] = (vecs[j] * vecs[j]).sum().item()\n                c, d = MinNormSolver._min_norm_element_from2(dps[(i, i)], dps[(i, j)], dps[(j, j)])\n                if d < dmin:\n                    dmin = d\n                    sol = [(i, j), c, d]\n        return sol, dps\n\n    def _projection2simplex(y):\n        \"\"\"\n        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n        \"\"\"\n        m = len(y)\n        sorted_y = np.flip(np.sort(y), axis=0)\n        tmpsum = 0.0\n        tmax_f = (np.sum(y) - 1.0) / m\n        for i in range(m - 1):\n            tmpsum += sorted_y[i]\n            tmax = (tmpsum - 1) / (i + 1.0)\n            if tmax > sorted_y[i + 1]:\n                tmax_f = tmax\n                break\n        return np.maximum(y - tmax_f, np.zeros(y.shape))\n\n    def _next_point(cur_val, grad, n):\n        proj_grad = grad - (np.sum(grad) / n)\n        tm1 = -1.0 * cur_val[proj_grad < 0] / proj_grad[proj_grad < 0]\n        tm2 = (1.0 - cur_val[proj_grad > 0]) / (proj_grad[proj_grad > 0])\n\n        skippers = np.sum(tm1 < 1e-7) + np.sum(tm2 < 1e-7)\n        t = 1\n        if len(tm1[tm1 > 1e-7]) > 0:\n            t = np.min(tm1[tm1 > 1e-7])\n        if len(tm2[tm2 > 1e-7]) > 0:\n            t = min(t, np.min(tm2[tm2 > 1e-7]))\n\n        next_point = proj_grad * t + cur_val\n        next_point = MinNormSolver._projection2simplex(next_point)\n        return next_point\n\n    def find_min_norm_element(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            grad_dir = -1.0 * np.dot(grad_mat, sol_vec)\n            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n            # Re-compute the inner products for line search\n            v1v1 = 0.0\n            v1v2 = 0.0\n            v2v2 = 0.0\n            for i in range(n):\n                for j in range(n):\n                    v1v1 += sol_vec[i] * sol_vec[j] * dps[(i, j)]\n                    v1v2 += sol_vec[i] * new_point[j] * dps[(i, j)]\n                    v2v2 += new_point[i] * new_point[j] * dps[(i, j)]\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec + (1 - nc) * new_point\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec\n\n    def find_min_norm_element_FW(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the Frank Wolfe until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            t_iter = np.argmin(np.dot(grad_mat, sol_vec))\n\n            v1v1 = np.dot(sol_vec, np.dot(grad_mat, sol_vec))\n            v1v2 = np.dot(sol_vec, grad_mat[:, t_iter])\n            v2v2 = grad_mat[t_iter, t_iter]\n\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec\n            new_sol_vec[t_iter] += 1 - nc\n\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec", "\n\nclass MinNormSolver:\n    MAX_ITER = 20\n    STOP_CRIT = 1e-5\n\n    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n        \"\"\"\n        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n        d is the distance (objective) optimzed\n        v1v1 = <x1,x1>\n        v1v2 = <x1,x2>\n        v2v2 = <x2,x2>\n        \"\"\"\n        if v1v2 >= v1v1:\n            # Case: Fig 1, third column\n            gamma = 0.999\n            cost = v1v1\n            return gamma, cost\n        if v1v2 >= v2v2:\n            # Case: Fig 1, first column\n            gamma = 0.001\n            cost = v2v2\n            return gamma, cost\n        # Case: Fig 1, second column\n        gamma = -1.0 * ((v1v2 - v2v2) / (v1v1 + v2v2 - 2 * v1v2))\n        cost = v2v2 + gamma * (v1v2 - v2v2)\n        return gamma, cost\n\n    def _min_norm_2d(vecs, dps):\n        \"\"\"\n        Find the minimum norm solution as combination of two points\n        This is correct only in 2D\n        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n        \"\"\"\n        dmin = np.inf\n        for i in range(len(vecs)):\n            for j in range(i + 1, len(vecs)):\n                if (i, j) not in dps:\n                    dps[(i, j)] = (vecs[i] * vecs[j]).sum().item()\n                    dps[(j, i)] = dps[(i, j)]\n                if (i, i) not in dps:\n                    dps[(i, i)] = (vecs[i] * vecs[i]).sum().item()\n                if (j, j) not in dps:\n                    dps[(j, j)] = (vecs[j] * vecs[j]).sum().item()\n                c, d = MinNormSolver._min_norm_element_from2(dps[(i, i)], dps[(i, j)], dps[(j, j)])\n                if d < dmin:\n                    dmin = d\n                    sol = [(i, j), c, d]\n        return sol, dps\n\n    def _projection2simplex(y):\n        \"\"\"\n        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n        \"\"\"\n        m = len(y)\n        sorted_y = np.flip(np.sort(y), axis=0)\n        tmpsum = 0.0\n        tmax_f = (np.sum(y) - 1.0) / m\n        for i in range(m - 1):\n            tmpsum += sorted_y[i]\n            tmax = (tmpsum - 1) / (i + 1.0)\n            if tmax > sorted_y[i + 1]:\n                tmax_f = tmax\n                break\n        return np.maximum(y - tmax_f, np.zeros(y.shape))\n\n    def _next_point(cur_val, grad, n):\n        proj_grad = grad - (np.sum(grad) / n)\n        tm1 = -1.0 * cur_val[proj_grad < 0] / proj_grad[proj_grad < 0]\n        tm2 = (1.0 - cur_val[proj_grad > 0]) / (proj_grad[proj_grad > 0])\n\n        skippers = np.sum(tm1 < 1e-7) + np.sum(tm2 < 1e-7)\n        t = 1\n        if len(tm1[tm1 > 1e-7]) > 0:\n            t = np.min(tm1[tm1 > 1e-7])\n        if len(tm2[tm2 > 1e-7]) > 0:\n            t = min(t, np.min(tm2[tm2 > 1e-7]))\n\n        next_point = proj_grad * t + cur_val\n        next_point = MinNormSolver._projection2simplex(next_point)\n        return next_point\n\n    def find_min_norm_element(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            grad_dir = -1.0 * np.dot(grad_mat, sol_vec)\n            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n            # Re-compute the inner products for line search\n            v1v1 = 0.0\n            v1v2 = 0.0\n            v2v2 = 0.0\n            for i in range(n):\n                for j in range(n):\n                    v1v1 += sol_vec[i] * sol_vec[j] * dps[(i, j)]\n                    v1v2 += sol_vec[i] * new_point[j] * dps[(i, j)]\n                    v2v2 += new_point[i] * new_point[j] * dps[(i, j)]\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec + (1 - nc) * new_point\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec\n\n    def find_min_norm_element_FW(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the Frank Wolfe until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            t_iter = np.argmin(np.dot(grad_mat, sol_vec))\n\n            v1v1 = np.dot(sol_vec, np.dot(grad_mat, sol_vec))\n            v1v2 = np.dot(sol_vec, grad_mat[:, t_iter])\n            v2v2 = grad_mat[t_iter, t_iter]\n\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec\n            new_sol_vec[t_iter] += 1 - nc\n\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec", "\n\ndef gradient_normalizers(grads, losses, normalization_type):\n    gn = {}\n    if normalization_type == 'l2':\n        for t in grads:\n            gn[t] = np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n    elif normalization_type == 'loss':\n        for t in grads:\n            gn[t] = losses[t]\n    elif normalization_type == 'loss+':\n        for t in grads:\n            gn[t] = losses[t] * np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n    elif normalization_type == 'none':\n        for t in grads:\n            gn[t] = 1.0\n    else:\n        print('ERROR: Invalid Normalization Type')\n    return gn(base)", ""]}
{"filename": "consistency/model_lenet.py", "chunked_list": ["# lenet base model for Pareto MTL\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.loss import CrossEntropyLoss\n\n\nclass RegressionTrain(torch.nn.Module):\n\n    def __init__(self, model, init_weight):\n        super(RegressionTrain, self).__init__()\n\n        self.model = model\n        self.weights = torch.nn.Parameter(torch.from_numpy(init_weight).float())\n        self.ce_loss = CrossEntropyLoss()\n\n    def forward(self, x, ts):\n        n_tasks = 2\n        ys = self.model(x)\n\n        task_loss = []\n        for i in range(n_tasks):\n            task_loss.append(self.ce_loss(ys[:, i], ts[:, i]))\n        task_loss = torch.stack(task_loss)\n\n        return task_loss", "class RegressionTrain(torch.nn.Module):\n\n    def __init__(self, model, init_weight):\n        super(RegressionTrain, self).__init__()\n\n        self.model = model\n        self.weights = torch.nn.Parameter(torch.from_numpy(init_weight).float())\n        self.ce_loss = CrossEntropyLoss()\n\n    def forward(self, x, ts):\n        n_tasks = 2\n        ys = self.model(x)\n\n        task_loss = []\n        for i in range(n_tasks):\n            task_loss.append(self.ce_loss(ys[:, i], ts[:, i]))\n        task_loss = torch.stack(task_loss)\n\n        return task_loss", "\n\nclass RegressionModel(torch.nn.Module):\n\n    def __init__(self, n_tasks):\n        super(RegressionModel, self).__init__()\n        self.n_tasks = n_tasks\n        self.conv1 = nn.Conv2d(1, 10, 9, 1)\n        self.conv2 = nn.Conv2d(10, 20, 5, 1)\n        self.fc1 = nn.Linear(5 * 5 * 20, 50)\n        self.ce_loss = CrossEntropyLoss()\n\n        for i in range(self.n_tasks):\n            setattr(self, 'task_{}'.format(i), nn.Linear(50, 10))\n\n    def shared_modules(self):\n        return [self.conv1, self.conv2, self.fc1]\n\n    def zero_grad_shared_modules(self):\n        for mm in self.shared_modules():\n            mm.zero_grad()\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 5 * 5 * 20)\n        x = F.relu(self.fc1(x))\n\n        outs = []\n        for i in range(self.n_tasks):\n            layer = getattr(self, 'task_{}'.format(i))\n            outs.append(layer(x))\n\n        return torch.stack(outs, dim=1)\n\n    def forward_loss(self, x, ts):\n        ys = self.forward(x)\n        task_loss = []\n        for i in range(self.n_tasks):\n            task_loss.append(self.ce_loss(ys[:, i], ts[:, i]))\n        task_loss = torch.stack(task_loss)\n        return task_loss", ""]}
{"filename": "consistency/model_resnet.py", "chunked_list": ["# resnet18 base model for Pareto MTL\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.loss import CrossEntropyLoss\nfrom torchvision import models\n\n\nclass RegressionTrainResNet(torch.nn.Module):\n\n    def __init__(self, model, init_weight):\n        super(RegressionTrainResNet, self).__init__()\n\n        self.model = model\n        self.weights = torch.nn.Parameter(torch.from_numpy(init_weight).float())\n        self.ce_loss = CrossEntropyLoss()\n\n    def forward(self, x, ts):\n        n_tasks = 2\n        ys = self.model(x)\n\n        task_loss = []\n        for i in range(n_tasks):\n            task_loss.append(self.ce_loss(ys[:, i], ts[:, i]))\n        task_loss = torch.stack(task_loss)\n\n        return task_loss", "\nclass RegressionTrainResNet(torch.nn.Module):\n\n    def __init__(self, model, init_weight):\n        super(RegressionTrainResNet, self).__init__()\n\n        self.model = model\n        self.weights = torch.nn.Parameter(torch.from_numpy(init_weight).float())\n        self.ce_loss = CrossEntropyLoss()\n\n    def forward(self, x, ts):\n        n_tasks = 2\n        ys = self.model(x)\n\n        task_loss = []\n        for i in range(n_tasks):\n            task_loss.append(self.ce_loss(ys[:, i], ts[:, i]))\n        task_loss = torch.stack(task_loss)\n\n        return task_loss", "\n\nclass MnistResNet(torch.nn.Module):\n\n    def __init__(self, n_tasks):\n        super(MnistResNet, self).__init__()\n        self.n_tasks = n_tasks\n        self.feature_extractor = models.resnet18(pretrained=False)\n        self.feature_extractor.conv1 = torch.nn.Conv2d(1,\n                                                       64,\n                                                       kernel_size=(7, 7),\n                                                       stride=(2, 2),\n                                                       padding=(3, 3),\n                                                       bias=False)\n        fc_in_features = self.feature_extractor.fc.in_features\n        self.feature_extractor.fc = torch.nn.Linear(fc_in_features, 100)\n        self.ce_loss = CrossEntropyLoss()\n\n        for i in range(self.n_tasks):\n            setattr(self, 'task_{}'.format(i), nn.Linear(100, 10))\n\n    def shared_modules(self):\n        return [self.feature_extractor]\n\n    def zero_grad_shared_modules(self):\n        for mm in self.shared_modules():\n            mm.zero_grad()\n\n    def forward(self, x):\n        x = F.relu(self.feature_extractor(x))\n        outs = []\n        for i in range(self.n_tasks):\n            layer = getattr(self, 'task_{}'.format(i))\n            outs.append(layer(x))\n\n        return torch.stack(outs, dim=1)\n\n    def forward_loss(self, x, ts):\n        ys = self.forward(x)\n        task_loss = []\n        for i in range(self.n_tasks):\n            task_loss.append(self.ce_loss(ys[:, i], ts[:, i]))\n        task_loss = torch.stack(task_loss)\n        return task_loss", ""]}
{"filename": "mtrl/mtrl_files/sdmgrad.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nfrom copy import deepcopy\nfrom typing import Iterable, List, Optional, Tuple\n\nimport numpy as np\nimport time\nimport torch\nfrom omegaconf import OmegaConf\n\nfrom mtrl.agent import grad_manipulation as grad_manipulation_agent", "\nfrom mtrl.agent import grad_manipulation as grad_manipulation_agent\nfrom mtrl.utils.types import ConfigType, TensorType\n#from mtrl.agent.mgda import MinNormSolver\n\n\ndef euclidean_proj_simplex(v, s=1):\n    \"\"\" Compute the Euclidean projection on a positive simplex\n    Solves the optimisation problem (using the algorithm from [1]):\n        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0 \n    Parameters\n    ----------\n    v: (n,) numpy array,\n       n-dimensional vector to project\n    s: int, optional, default: 1,\n       radius of the simplex\n    Returns\n    -------\n    w: (n,) numpy array,\n       Euclidean projection of v on the simplex\n    Notes\n    -----\n    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n    However, this implementation still easily scales to millions of dimensions.\n    References\n    ----------\n    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n        International Conference on Machine Learning (ICML 2008)\n        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n    [2] Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application\n        Weiran Wang, Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n. arXiv:1309.1541\n        https://arxiv.org/pdf/1309.1541.pdf\n    [3] https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246#file-simplex_projection-py\n    \"\"\"\n    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n    v = v.astype(np.float64)\n    n, = v.shape  # will raise ValueError if v is not 1-D\n    # check if we are already on the simplex\n    if v.sum() == s and np.alltrue(v >= 0):\n        # best projection: itself!\n        return v\n    # get the array of cumulative sums of a sorted (decreasing) copy of v\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    # get the number of > 0 components of the optimal solution\n    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - s))[0][-1]\n    # compute the Lagrange multiplier associated to the simplex constraint\n    theta = float(cssv[rho] - s) / (rho + 1)\n    # compute the projection by thresholding v using theta\n    w = (v - theta).clip(min=0)\n    return w", "\n\ndef _check_param_device(param: TensorType, old_param_device: Optional[int]) -> int:\n    \"\"\"This helper function is to check if the parameters are located\n        in the same device. Currently, the conversion between model parameters\n        and single vector form is not supported for multiple allocations,\n        e.g. parameters in different GPUs, or mixture of CPU/GPU.\n\n        The implementation is taken from: https://github.com/pytorch/pytorch/blob/22a34bcf4e5eaa348f0117c414c3dd760ec64b13/torch/nn/utils/convert_parameters.py#L57\n\n    Args:\n        param ([TensorType]): a Tensor of a parameter of a model.\n        old_param_device ([int]): the device where the first parameter\n            of a model is allocated.\n\n    Returns:\n        old_param_device (int): report device for the first time\n\n    \"\"\"\n    # Meet the first parameter\n    if old_param_device is None:\n        old_param_device = param.get_device() if param.is_cuda else -1\n    else:\n        warn = False\n        if param.is_cuda:  # Check if in same GPU\n            warn = param.get_device() != old_param_device\n        else:  # Check if in CPU\n            warn = old_param_device != -1\n        if warn:\n            raise TypeError(\"Found two parameters on different devices, \"\n                            \"this is currently not supported.\")\n    return old_param_device", "\n\ndef apply_vector_grad_to_parameters(vec: TensorType, parameters: Iterable[TensorType], accumulate: bool = False):\n    \"\"\"Apply vector gradients to the parameters\n\n    Args:\n        vec (TensorType): a single vector represents the gradients of a model.\n        parameters (Iterable[TensorType]): an iterator of Tensors that are the\n            parameters of a model.\n    \"\"\"\n    # Ensure vec of type Tensor\n    if not isinstance(vec, torch.Tensor):\n        raise TypeError(\"expected torch.Tensor, but got: {}\".format(torch.typename(vec)))\n    # Flag for the device where the parameter is located\n    param_device = None\n\n    # Pointer for slicing the vector for each parameter\n    pointer = 0\n    for param in parameters:\n        # Ensure the parameters are located in the same device\n        param_device = _check_param_device(param, param_device)\n\n        # The length of the parameter\n        num_param = param.numel()\n        # Slice the vector, reshape it, and replace the old grad of the parameter\n        if accumulate:\n            param.grad = (param.grad + vec[pointer:pointer + num_param].view_as(param).data)\n        else:\n            param.grad = vec[pointer:pointer + num_param].view_as(param).data\n\n        # Increment the pointer\n        pointer += num_param", "\n\nclass Agent(grad_manipulation_agent.Agent):\n\n    def __init__(\n        self,\n        env_obs_shape: List[int],\n        action_shape: List[int],\n        action_range: Tuple[int, int],\n        device: torch.device,\n        agent_cfg: ConfigType,\n        multitask_cfg: ConfigType,\n        cfg_to_load_model: Optional[ConfigType] = None,\n        should_complete_init: bool = True,\n    ):\n        \"\"\"Regularized gradient algorithm.\"\"\"\n        agent_cfg_copy = deepcopy(agent_cfg)\n        del agent_cfg_copy['sdmgrad_lmbda']\n        del agent_cfg_copy['sdmgrad_method']\n\n        OmegaConf.set_struct(agent_cfg_copy, False)\n        agent_cfg_copy.cfg_to_load_model = None\n        agent_cfg_copy.should_complete_init = False\n        agent_cfg_copy.loss_reduction = \"none\"\n        OmegaConf.set_struct(agent_cfg_copy, True)\n\n        super().__init__(\n            env_obs_shape=env_obs_shape,\n            action_shape=action_shape,\n            action_range=action_range,\n            multitask_cfg=multitask_cfg,\n            agent_cfg=agent_cfg_copy,\n            device=device,\n        )\n        self.agent._compute_gradient = self._compute_gradient\n        self._rng = np.random.default_rng()\n\n        self.sdmgrad_lmbda = agent_cfg['sdmgrad_lmbda']\n        self.sdmgrad_method = agent_cfg['sdmgrad_method']\n\n        fn_maps = {\n            \"sdmgrad\": self.sdmgrad,\n        }\n        for k in range(2, 50):\n            fn_maps[f\"sdmgrad_os{k}\"] = self.sdmgrad_os\n\n        fn_names = \", \".join(fn_maps.keys())\n        assert self.sdmgrad_method in fn_maps, \\\n                f\"[error] unrealized fn {self.sdmgrad_method}, currently we have {fn_names}\"\n        self.sdmgrad_fn = fn_maps[self.sdmgrad_method]\n        self.wi_map = {}\n        self.num_param_block = -1\n        self.conflicts = []\n        self.last_w = None\n        self.save_target = 500000\n        if \"os\" in self.sdmgrad_method:\n            num_tasks = multitask_cfg['num_envs']\n            self.os_n = int(self.sdmgrad_method[self.sdmgrad_method.find(\"os\") + 2:])\n\n        if should_complete_init:\n            self.complete_init(cfg_to_load_model=cfg_to_load_model)\n\n    def _compute_gradient(\n        self,\n        loss: TensorType,  # batch x 1\n        parameters: List[TensorType],\n        step: int,\n        component_names: List[str],\n        env_metadata: grad_manipulation_agent.EnvMetadata,\n        retain_graph: bool = False,\n        allow_unused: bool = False,\n    ) -> None:\n\n        #t0 = time.time()\n        task_loss = self._convert_loss_into_task_loss(loss=loss, env_metadata=env_metadata)\n        num_tasks = task_loss.shape[0]\n        grad = []\n\n        if \"os\" in self.sdmgrad_method:\n            n = self.os_n\n            while True:\n                idx = np.random.binomial(1, n / num_tasks, num_tasks)\n                sample_idx = np.where(idx == 1)[0]\n                n_sample = sample_idx.shape[0]\n                if n_sample:\n                    break\n            losses = [0] * n_sample\n            for j in range(n_sample):\n                losses[j] = task_loss[sample_idx[j]]\n            for loss in losses:\n                grad.append(\n                    tuple(_grad.contiguous() for _grad in torch.autograd.grad(\n                        loss,\n                        parameters,\n                        retain_graph=True,\n                        allow_unused=allow_unused,\n                    )))\n        else:\n            for index in range(num_tasks):\n                grad.append(\n                    tuple(_grad.contiguous() for _grad in torch.autograd.grad(\n                        task_loss[index],\n                        parameters,\n                        retain_graph=(retain_graph or index != num_tasks - 1),\n                        allow_unused=allow_unused,\n                    )))\n\n        grad_vec = torch.cat(\n            list(map(lambda x: torch.nn.utils.parameters_to_vector(x).unsqueeze(0), grad)),\n            dim=0,\n        )  # num_tasks x dim\n\n        regularized_grad = self.sdmgrad_fn(grad_vec, num_tasks)\n        apply_vector_grad_to_parameters(regularized_grad, parameters)\n\n    def sdmgrad(self, grad_vec, num_tasks):\n        \"\"\"\n        grad_vec: [num_tasks, dim]\n        \"\"\"\n        grads = grad_vec\n\n        GG = torch.mm(grads, grads.t()).cpu()\n        scale = torch.mean(torch.sqrt(torch.diag(GG) + 1e-4))\n        GG = GG / scale.pow(2)\n        Gg = torch.mean(GG, dim=1)\n        gg = torch.mean(Gg)\n\n        w = torch.ones(num_tasks) / num_tasks\n        w.requires_grad = True\n        if num_tasks == 50:\n            w_opt = torch.optim.SGD([w], lr=50, momentum=0.5)\n        else:\n            w_opt = torch.optim.SGD([w], lr=25, momentum=0.5)\n\n        lmbda = self.sdmgrad_lmbda\n\n        w_best = None\n        obj_best = np.inf\n        for i in range(21):\n            w_opt.zero_grad()\n            obj = torch.dot(w, torch.mv(GG, w)) + 2 * lmbda * torch.dot(w, Gg) + lmbda**2 * gg\n            if obj.item() < obj_best:\n                obj_best = obj.item()\n                w_best = w.clone()\n            if i < 20:\n                obj.backward()\n                w_opt.step()\n                proj = euclidean_proj_simplex(w.data.cpu().numpy())\n                w.data.copy_(torch.from_numpy(proj).data)\n\n        g0 = torch.mean(grads, dim=0)\n        gw = torch.mv(grads.t(), w_best.to(grads.device))\n        g = (gw + lmbda * g0) / (1 + lmbda)\n        return g\n\n    def sdmgrad_os(self, grad_vec, num_tasks):\n        \"\"\"\n        objective sampling\n        grad_vec: [num_tasks, dim]\n        \"\"\"\n        grads = grad_vec\n        n = grads.size(0)\n\n        GG = torch.mm(grads, grads.t()).cpu()\n        scale = (torch.diag(GG) + 1e-4).sqrt().mean()\n        GG = GG / scale.pow(2)\n        Gg = torch.mean(GG, dim=1)\n        gg = torch.mean(Gg)\n\n        w = torch.ones(n) / n\n        w.requires_grad = True\n        w_opt = torch.optim.SGD([w], lr=50, momentum=0.5)\n\n        lmbda = self.sdmgrad_lmbda\n\n        w_best = None\n        obj_best = np.inf\n        for i in range(21):\n            w_opt.zero_grad()\n            obj = torch.dot(w, torch.mv(GG, w)) + 2 * lmbda * torch.dot(w, Gg) + lmbda**2 * gg\n            if obj.item() < obj_best:\n                obj_best = obj.item()\n                w_best = w.clone()\n            if i < 20:\n                obj.backward()\n                w_opt.step()\n                proj = euclidean_proj_simplex(w.data.cpu().numpy())\n                w.data.copy_(torch.from_numpy(proj).data)\n\n        g0 = torch.mean(grads, dim=0)\n        gw = torch.mv(grads.t(), w_best.to(grads.device))\n        g = (gw + lmbda * g0) / (1 + lmbda)\n        return g", ""]}
{"filename": "mtrl/mtrl_files/config.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\"\"\"Code to interface with the config.\"\"\"\nimport datetime\nimport hashlib\nimport os\nfrom copy import deepcopy\nfrom typing import Any, Dict, cast\n\nimport hydra\nfrom omegaconf import OmegaConf", "import hydra\nfrom omegaconf import OmegaConf\n\nfrom mtrl.utils import utils\nfrom mtrl.utils.types import ConfigType\n\n\ndef dict_to_config(dictionary: Dict) -> ConfigType:\n    \"\"\"Convert the dictionary to a config.\n\n    Args:\n        dictionary (Dict): dictionary to convert.\n\n    Returns:\n        ConfigType: config made from the dictionary.\n    \"\"\"\n    return OmegaConf.create(dictionary)", "\n\ndef make_config_mutable(config: ConfigType) -> ConfigType:\n    \"\"\"Set the config to be mutable.\n\n    Args:\n        config (ConfigType):\n\n    Returns:\n        ConfigType:\n    \"\"\"\n    OmegaConf.set_readonly(config, False)\n    return config", "\n\ndef make_config_immutable(config: ConfigType) -> ConfigType:\n    \"\"\"Set the config to be immutable.\n\n    Args:\n        config (ConfigType):\n\n    Returns:\n        ConfigType:\n    \"\"\"\n    OmegaConf.set_readonly(config, True)\n    return config", "\n\ndef set_struct(config: ConfigType) -> ConfigType:\n    \"\"\"Set the struct flag in the config.\n\n    Args:\n        config (ConfigType):\n\n    Returns:\n        ConfigType:\n    \"\"\"\n    OmegaConf.set_struct(config, True)\n    return config", "\n\ndef unset_struct(config: ConfigType) -> ConfigType:\n    \"\"\"Unset the struct flag in the config.\n\n    Args:\n        config (ConfigType):\n\n    Returns:\n        ConfigType:\n    \"\"\"\n    OmegaConf.set_struct(config, False)\n    return config", "\n\ndef to_dict(config: ConfigType) -> Dict[str, Any]:\n    \"\"\"Convert config to a dictionary.\n\n    Args:\n        config (ConfigType):\n\n    Returns:\n        Dict:\n    \"\"\"\n    dict_config = cast(Dict[str, Any], OmegaConf.to_container(deepcopy(config), resolve=False))\n    return dict_config", "\n\ndef process_config(config: ConfigType, should_make_dir: bool = True) -> ConfigType:\n    \"\"\"Process the config.\n\n    Args:\n        config (ConfigType): config object to process.\n        should_make_dir (bool, optional): should make dir for saving logs, models etc? Defaults to True.\n\n    Returns:\n        ConfigType: processed config.\n    \"\"\"\n    config = _process_setup_config(config=config)\n    config = _process_experiment_config(config=config, should_make_dir=should_make_dir)\n    return set_struct(make_config_immutable(config))", "\n\ndef read_config_from_file(config_path: str) -> ConfigType:\n    \"\"\"Read the config from filesystem.\n\n    Args:\n        config_path (str): path to read config from.\n\n    Returns:\n        ConfigType:\n    \"\"\"\n    config = OmegaConf.load(config_path)\n    assert isinstance(config, ConfigType)\n    return set_struct(make_config_immutable(config))", "\n\ndef _process_setup_config(config: ConfigType) -> ConfigType:\n    \"\"\"Process the `setup` node of the config.\n\n    Args:\n        config (ConfigType): config object.\n\n    Returns:\n        [ConfigType]: processed config.\n    \"\"\"\n\n    setup_config = config.setup\n\n    if setup_config.base_path is None:\n        setup_config.base_path = hydra.utils.get_original_cwd()\n    if not setup_config.debug.should_enable:\n        #setup_config.id = f\"{hashlib.sha224(setup_config.description.encode()).hexdigest()}_issue_{setup_config.git.issue_id}_seed_{setup_config.seed}\"\n        if \"sdmgrad\" in config.agent.name:\n            setup_config.id = f\"{config.env.name}_{config.agent.name}_\"+\\\n                    f\"{config.agent.builder.agent_cfg.sdmgrad_method}_\"+\\\n                    f\"c{config.agent.builder.agent_cfg.sdmgrad_lmbda}_seed_{setup_config.seed}\"\n        else:\n            setup_config.id = f\"{config.env.name}_{config.agent.name}_seed_{setup_config.seed}\"\n\n    current_commit_id = utils.get_current_commit_id()\n    if not setup_config.git.commit_id:\n        setup_config.git.commit_id = current_commit_id\n    else:\n        # if the commit id is already set, assert that the commit id (in the\n        # config) is the same as the current commit id.\n        if setup_config.git.commit_id != current_commit_id:\n            raise RuntimeError(f\"\"\"The current commit id ({current_commit_id}) does\n                 not match the commit id from the config\n                 ({setup_config.git.commit_id})\"\"\")\n    if setup_config.git.has_uncommitted_changes == \"\":\n        setup_config.git.has_uncommitted_changes = utils.has_uncommitted_changes()\n\n    if not setup_config.date:\n        setup_config.date = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    slurm_id = []\n    env_var_names = [\"SLURM_JOB_ID\", \"SLURM_STEP_ID\"]\n    for var_name in env_var_names:\n        if var_name in os.environ:\n            slurm_id.append(str(os.environ[var_name]))\n    if slurm_id:\n        setup_config.slurm_id = \"-\".join(slurm_id)\n    else:\n        setup_config.slurm_id = \"-1\"\n\n    return config", "\n\ndef _process_experiment_config(config: ConfigType, should_make_dir: bool) -> ConfigType:\n    \"\"\"Process the `experiment` section of the config.\n\n    Args:\n        config (ConfigType): config object.\n        should_make_dir (bool): should make dir.\n\n    Returns:\n        ConfigType: Processed config\n    \"\"\"\n    if should_make_dir:\n        utils.make_dir(path=config.experiment.save_dir)\n    return config", "\n\ndef pretty_print(config, resolve: bool = True):\n    \"\"\"Prettyprint the config.\n\n    Args:\n        config ([type]):\n        resolve (bool, optional): should resolve the config before printing. Defaults to True.\n    \"\"\"\n    print(OmegaConf.to_yaml(config, resolve=resolve))", "\n\ndef get_env_params_from_config(config: ConfigType) -> ConfigType:\n    \"\"\"Get the params needed for building the environment from a config.\n\n    Args:\n        config (ConfigType):\n\n    Returns:\n        ConfigType: params for building the environment, encoded as a config.\n    \"\"\"\n    env_params = deepcopy(config.env.builder)\n    env_params = make_config_mutable(env_params)\n    env_params = unset_struct(env_params)\n    env_params.pop(\"_target_\")\n    return env_params", ""]}
{"filename": "cityscapes/model_segnet_stan.py", "chunked_list": ["import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\n\nfrom create_dataset import *\nfrom utils import *\n\nparser = argparse.ArgumentParser(description='Single-task: Attention Network')\nparser.add_argument('--task', default='semantic', type=str, help='choose task: semantic, depth, normal')", "parser = argparse.ArgumentParser(description='Single-task: Attention Network')\nparser.add_argument('--task', default='semantic', type=str, help='choose task: semantic, depth, normal')\nparser.add_argument('--dataroot', default='cityscapes', type=str, help='dataset root')\nparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 7\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        # define task attention layers\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(2):\n            if j < 1:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n        #self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for tasks\n        atten_encoder, atten_decoder = ([0] * 2 for _ in range(2))\n        for i in range(2):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(2):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(2):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n        #t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred], self.logsigma", "\n\n# define model, optimiser and scheduler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSegNet_STAN = SegNet().to(device)\noptimizer = optim.Adam(SegNet_STAN.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_STAN),\n                                                         count_parameters(SegNet_STAN) / 24981069))", "print('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_STAN),\n                                                         count_parameters(SegNet_STAN) / 24981069))\nprint(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\n# define dataset\ndataset_path = opt.dataroot\nif opt.apply_augmentation:\n    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation.')\nelse:\n    train_set = CityScapes(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')", "\ntest_set = CityScapes(root=dataset_path, train=False)\n\nbatch_size = 8\ntrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\n# Train and evaluate single-task network\nsingle_task_trainer(train_loader, test_loader, SegNet_STAN, device, optimizer, scheduler, opt, 200)", "# Train and evaluate single-task network\nsingle_task_trainer(train_loader, test_loader, SegNet_STAN, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "cityscapes/model_segnet_mtan.py", "chunked_list": ["import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nimport torch.utils.data.sampler as sampler\n\nfrom create_dataset import *\nfrom utils import *\n\nparser = argparse.ArgumentParser(description='Multi-task: Attention Network')", "\nparser = argparse.ArgumentParser(description='Multi-task: Attention Network')\nparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\nparser.add_argument('--dataroot', default='cityscapes', type=str, help='dataset root')\nparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\nparser.add_argument('--seed', default=0, type=int, help='control seed')\nparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 7\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        # define task attention layers\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(2):\n            if j < 1:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n        #self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for tasks\n        atten_encoder, atten_decoder = ([0] * 2 for _ in range(2))\n        for i in range(2):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(2):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(2):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n        #t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred], self.logsigma", "\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 7\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        # define task attention layers\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(2):\n            if j < 1:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n        #self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for tasks\n        atten_encoder, atten_decoder = ([0] * 2 for _ in range(2))\n        for i in range(2):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(2):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(2):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n        #t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred], self.logsigma", "\n\ncontrol_seed(opt.seed)\n# define model, optimiser and scheduler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSegNet_MTAN = SegNet().to(device)\noptimizer = optim.Adam(SegNet_MTAN.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_MTAN),", "\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_MTAN),\n                                                         count_parameters(SegNet_MTAN) / 24981069))\nprint(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\n# define dataset\ndataset_path = opt.dataroot\nif opt.apply_augmentation:\n    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation.')\nelse:\n    train_set = CityScapes(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')", "if opt.apply_augmentation:\n    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation.')\nelse:\n    train_set = CityScapes(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')\n\ntest_set = CityScapes(root=dataset_path, train=False)\n\nbatch_size = 8", "\nbatch_size = 8\ntrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\n# Train and evaluate multi-task network\nmulti_task_trainer(train_loader, test_loader, SegNet_MTAN, device, optimizer, scheduler, opt, 200)\n", ""]}
{"filename": "cityscapes/model_segnet_split.py", "chunked_list": ["import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nimport torch.utils.data.sampler as sampler\n\nfrom create_dataset import *\nfrom utils import *\n\nparser = argparse.ArgumentParser(description='Multi-task: Split')", "\nparser = argparse.ArgumentParser(description='Multi-task: Split')\nparser.add_argument('--type', default='standard', type=str, help='split type: standard, wide, deep')\nparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\nparser.add_argument('--dataroot', default='cityscapes', type=str, help='dataset root')\nparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\nparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 7\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        # define task attention layers\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(2):\n            if j < 1:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n        #self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for tasks\n        atten_encoder, atten_decoder = ([0] * 2 for _ in range(2))\n        for i in range(2):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(2):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(2):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n        #t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred], self.logsigma", "\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 7\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        # define task attention layers\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(2):\n            if j < 1:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n        #self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for tasks\n        atten_encoder, atten_decoder = ([0] * 2 for _ in range(2))\n        for i in range(2):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(2):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(2):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n        #t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred], self.logsigma", "\n\n# define model, optimiser and scheduler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSegNet_SPLIT = SegNet().to(device)\noptimizer = optim.Adam(SegNet_SPLIT.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_SPLIT),\n                                                         count_parameters(SegNet_SPLIT) / 24981069))", "print('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_SPLIT),\n                                                         count_parameters(SegNet_SPLIT) / 24981069))\nprint(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\n# define dataset\ndataset_path = opt.dataroot\nif opt.apply_augmentation:\n    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation.')\nelse:\n    train_set = CityScapes(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')", "\ntest_set = CityScapes(root=dataset_path, train=False)\n\nbatch_size = 8\ntrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\n# Train and evaluate multi-task network\nmulti_task_trainer(train_loader, test_loader, SegNet_SPLIT, device, optimizer, scheduler, opt, 200)", "# Train and evaluate multi-task network\nmulti_task_trainer(train_loader, test_loader, SegNet_SPLIT, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "cityscapes/evaluate.py", "chunked_list": ["import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport torch\n\nmethods = [\n    \"sdmgrad-1e-1\", \"sdmgrad-2e-1\", \"sdmgrad-3e-1\", \"sdmgrad-4e-1\", \"sdmgrad-5e-1\", \"sdmgrad-6e-1\", \"sdmgrad-7e-1\",\n    \"sdmgrad-8e-1\", \"sdmgrad-9e-1\", \"sdmgrad-1e0\"\n]\n", "]\n\ncolors = [\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"C8\", \"C9\", \"tab:green\", \"tab:cyan\", \"tab:blue\", \"tab:red\"]\nstats = [\"semantic loss\", \"mean iou\", \"pix acc\", \"depth loss\", \"abs err\", \"rel err\"]\nstats_idx_map = [4, 5, 6, 8, 9, 10]\n\ndelta_stats = [\"mean iou\", \"pix acc\", \"abs err\", \"rel err\"]\n\ntime_idx = 22\n", "time_idx = 22\n\n# change random seeds used in the experiments here\nseeds = [0, 1, 2]\n\nlogs = {}\nmin_epoch = 100000\n\nfor m in methods:\n    logs[m] = {\"train\": [None for _ in range(3)], \"test\": [None for _ in range(3)]}\n\n    for seed in seeds:\n        logs[m][\"train\"][seed] = {}\n        logs[m][\"test\"][seed] = {}\n\n    for stat in stats:\n        for seed in seeds:\n            logs[m][\"train\"][seed][stat] = []\n            logs[m][\"test\"][seed][stat] = []\n\n    for seed in seeds:\n        logs[m][\"train\"][seed][\"time\"] = []\n\n    for seed in seeds:\n        fname = f\"logs/{m}-sd{seed}.log\"\n        with open(fname, \"r\") as f:\n            lines = f.readlines()\n            for line in lines:\n                if line.startswith(\"Epoch\"):\n                    ws = line.split(\" \")\n                    for i, stat in enumerate(stats):\n                        logs[m][\"train\"][seed][stat].append(float(ws[stats_idx_map[i]]))\n                        logs[m][\"test\"][seed][stat].append(float(ws[stats_idx_map[i] + 9]))\n                    logs[m][\"train\"][seed][\"time\"].append(float(ws[time_idx]))\n\n            n_epoch = min(len(logs[m][\"train\"][seed][\"semantic loss\"]), len(logs[m][\"test\"][seed][\"semantic loss\"]))\n            if n_epoch < min_epoch:\n                min_epoch = n_epoch\n                print(m, n_epoch)", "for m in methods:\n    logs[m] = {\"train\": [None for _ in range(3)], \"test\": [None for _ in range(3)]}\n\n    for seed in seeds:\n        logs[m][\"train\"][seed] = {}\n        logs[m][\"test\"][seed] = {}\n\n    for stat in stats:\n        for seed in seeds:\n            logs[m][\"train\"][seed][stat] = []\n            logs[m][\"test\"][seed][stat] = []\n\n    for seed in seeds:\n        logs[m][\"train\"][seed][\"time\"] = []\n\n    for seed in seeds:\n        fname = f\"logs/{m}-sd{seed}.log\"\n        with open(fname, \"r\") as f:\n            lines = f.readlines()\n            for line in lines:\n                if line.startswith(\"Epoch\"):\n                    ws = line.split(\" \")\n                    for i, stat in enumerate(stats):\n                        logs[m][\"train\"][seed][stat].append(float(ws[stats_idx_map[i]]))\n                        logs[m][\"test\"][seed][stat].append(float(ws[stats_idx_map[i] + 9]))\n                    logs[m][\"train\"][seed][\"time\"].append(float(ws[time_idx]))\n\n            n_epoch = min(len(logs[m][\"train\"][seed][\"semantic loss\"]), len(logs[m][\"test\"][seed][\"semantic loss\"]))\n            if n_epoch < min_epoch:\n                min_epoch = n_epoch\n                print(m, n_epoch)", "\ntest_stats = {}\ntrain_stats = {}\nlearning_time = {}\n\nprint(\" \" * 25 + \" | \".join([f\"{s:5s}\" for s in stats]))\n\nfor mi, mode in enumerate([\"train\", \"test\"]):\n    if mi == 1:\n        print(mode)\n    for mmi, m in enumerate(methods):\n        if m not in test_stats:\n            test_stats[m] = {}\n            train_stats[m] = {}\n\n        string = f\"{m:30s} \"\n        for stat in stats:\n            x = []\n            for seed in seeds:\n                x.append(np.array(logs[m][mode][seed][stat][min_epoch - 10:min_epoch]).mean())\n            x = np.array(x)\n            if mode == \"test\":\n                test_stats[m][stat] = x.copy()\n            else:\n                train_stats[m][stat] = x.copy()\n            mu = x.mean()\n            std = x.std() / np.sqrt(3)\n            string += f\" | {mu:5.4f}\"\n        if mode == \"test\":\n            print(string)", "\nfor m in methods:\n    learning_time[m] = np.array([np.array(logs[m][\"train\"][sd][\"time\"]).mean() for sd in seeds])\n\n### print average training loss\nfor method in methods:\n    average_loss = np.mean([train_stats[method][\"semantic loss\"].mean(), train_stats[method][\"depth loss\"].mean()])\n    print(f\"{method} average training loss {average_loss}\")\n\n### print delta M", "\n### print delta M\n\nbase = np.array([0.7401, 0.9316, 0.0125, 27.77])\nsign = np.array([1, 1, 0, 0])\nkk = np.ones(4) * -1\n\n\ndef delta_fn(a):\n    return (kk**sign * (a - base) / base).mean() * 100.  # *100 for percentage", "def delta_fn(a):\n    return (kk**sign * (a - base) / base).mean() * 100.  # *100 for percentage\n\n\ndeltas = {}\nfor method in methods:\n    tmp = np.zeros(4)\n    for i, stat in enumerate(delta_stats):\n        tmp[i] = test_stats[method][stat].mean()\n    deltas[method] = delta_fn(tmp)\n    print(f\"{method:30s} delta: {deltas[method]:4.3f}\")", ""]}
{"filename": "cityscapes/utils.py", "chunked_list": ["import torch\nimport torch.nn.functional as F\nimport numpy as np\nimport random\nimport time\n\nfrom copy import deepcopy\nfrom min_norm_solvers import MinNormSolver\nfrom scipy.optimize import minimize, Bounds, minimize_scalar\n", "from scipy.optimize import minimize, Bounds, minimize_scalar\n\n\ndef euclidean_proj_simplex(v, s=1):\n    \"\"\" Compute the Euclidean projection on a positive simplex\n    Solves the optimisation problem (using the algorithm from [1]):\n        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0 \n    Parameters\n    ----------\n    v: (n,) numpy array,\n       n-dimensional vector to project\n    s: int, optional, default: 1,\n       radius of the simplex\n    Returns\n    -------\n    w: (n,) numpy array,\n       Euclidean projection of v on the simplex\n    Notes\n    -----\n    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n    However, this implementation still easily scales to millions of dimensions.\n    References\n    ----------\n    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n        International Conference on Machine Learning (ICML 2008)\n        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n    [2] Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application\n        Weiran Wang, Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n. arXiv:1309.1541\n        https://arxiv.org/pdf/1309.1541.pdf\n    [3] https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246#file-simplex_projection-py\n    \"\"\"\n    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n    v = v.astype(np.float64)\n    n, = v.shape  # will raise ValueError if v is not 1-D\n    # check if we are already on the simplex\n    if v.sum() == s and np.alltrue(v >= 0):\n        # best projection: itself!\n        return v\n    # get the array of cumulative sums of a sorted (decreasing) copy of v\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    # get the number of > 0 components of the optimal solution\n    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - s))[0][-1]\n    # compute the Lagrange multiplier associated to the simplex constraint\n    theta = float(cssv[rho] - s) / (rho + 1)\n    # compute the projection by thresholding v using theta\n    w = (v - theta).clip(min=0)\n    return w", "\n\n\"\"\"\nDefine task metrics, loss functions and model trainer here.\n\"\"\"\n\n\ndef control_seed(seed):\n    torch.backends.cudnn.enabled = False\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.cuda.manual_seed_all(seed)", "\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef model_fit(x_pred, x_output, task_type):\n    device = x_pred.device\n\n    # binary mark to mask out undefined pixel space\n    binary_mask = (torch.sum(x_output, dim=1) != 0).float().unsqueeze(1).to(device)\n\n    if task_type == 'semantic':\n        # semantic loss: depth-wise cross entropy\n        loss = F.nll_loss(x_pred, x_output, ignore_index=-1)\n\n    if task_type == 'depth':\n        # depth loss: l1 norm\n        loss = torch.sum(torch.abs(x_pred - x_output) * binary_mask) / torch.nonzero(binary_mask,\n                                                                                     as_tuple=False).size(0)\n\n    if task_type == 'normal':\n        # normal loss: dot product\n        loss = 1 - torch.sum((x_pred * x_output) * binary_mask) / torch.nonzero(binary_mask, as_tuple=False).size(0)\n\n    return loss", "\n\n# Legacy: compute mIoU and Acc. for each image and average across all images.\n\n# def compute_miou(x_pred, x_output):\n#     _, x_pred_label = torch.max(x_pred, dim=1)\n#     x_output_label = x_output\n#     batch_size = x_pred.size(0)\n#     class_nb = x_pred.size(1)\n#     device = x_pred.device", "#     class_nb = x_pred.size(1)\n#     device = x_pred.device\n#     for i in range(batch_size):\n#         true_class = 0\n#         first_switch = True\n#         invalid_mask = (x_output[i] >= 0).float()\n#         for j in range(class_nb):\n#             pred_mask = torch.eq(x_pred_label[i], j * torch.ones(x_pred_label[i].shape).long().to(device))\n#             true_mask = torch.eq(x_output_label[i], j * torch.ones(x_output_label[i].shape).long().to(device))\n#             mask_comb = pred_mask.float() + true_mask.float()", "#             true_mask = torch.eq(x_output_label[i], j * torch.ones(x_output_label[i].shape).long().to(device))\n#             mask_comb = pred_mask.float() + true_mask.float()\n#             union = torch.sum((mask_comb > 0).float() * invalid_mask)  # remove non-defined pixel predictions\n#             intsec = torch.sum((mask_comb > 1).float())\n#             if union == 0:\n#                 continue\n#             if first_switch:\n#                 class_prob = intsec / union\n#                 first_switch = False\n#             else:", "#                 first_switch = False\n#             else:\n#                 class_prob = intsec / union + class_prob\n#             true_class += 1\n#         if i == 0:\n#             batch_avg = class_prob / true_class\n#         else:\n#             batch_avg = class_prob / true_class + batch_avg\n#     return batch_avg / batch_size\n#", "#     return batch_avg / batch_size\n#\n#\n# def compute_iou(x_pred, x_output):\n#     _, x_pred_label = torch.max(x_pred, dim=1)\n#     x_output_label = x_output\n#     batch_size = x_pred.size(0)\n#     for i in range(batch_size):\n#         if i == 0:\n#             pixel_acc = torch.div(", "#         if i == 0:\n#             pixel_acc = torch.div(\n#                 torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).float()),\n#                 torch.sum((x_output_label[i] >= 0).float()))\n#         else:\n#             pixel_acc = pixel_acc + torch.div(\n#                 torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).float()),\n#                 torch.sum((x_output_label[i] >= 0).float()))\n#     return pixel_acc / batch_size\n", "#     return pixel_acc / batch_size\n\n\n# New mIoU and Acc. formula: accumulate every pixel and average across all pixels in all images\nclass ConfMatrix(object):\n\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.mat = None\n\n    def update(self, pred, target):\n        n = self.num_classes\n        if self.mat is None:\n            self.mat = torch.zeros((n, n), dtype=torch.int64, device=pred.device)\n        with torch.no_grad():\n            k = (target >= 0) & (target < n)\n            inds = n * target[k].to(torch.int64) + pred[k]\n            self.mat += torch.bincount(inds, minlength=n**2).reshape(n, n)\n\n    def get_metrics(self):\n        h = self.mat.float()\n        acc = torch.diag(h).sum() / h.sum()\n        iu = torch.diag(h) / (h.sum(1) + h.sum(0) - torch.diag(h))\n        return torch.mean(iu).item(), acc.item()", "\n\ndef depth_error(x_pred, x_output):\n    device = x_pred.device\n    binary_mask = (torch.sum(x_output, dim=1) != 0).unsqueeze(1).to(device)\n    x_pred_true = x_pred.masked_select(binary_mask)\n    x_output_true = x_output.masked_select(binary_mask)\n    abs_err = torch.abs(x_pred_true - x_output_true)\n    rel_err = torch.abs(x_pred_true - x_output_true) / x_output_true\n    return (torch.sum(abs_err) / torch.nonzero(binary_mask, as_tuple=False).size(0)).item(), \\\n           (torch.sum(rel_err) / torch.nonzero(binary_mask, as_tuple=False).size(0)).item()", "\n\ndef normal_error(x_pred, x_output):\n    binary_mask = (torch.sum(x_output, dim=1) != 0)\n    error = torch.acos(torch.clamp(torch.sum(x_pred * x_output, 1).masked_select(binary_mask), -1,\n                                   1)).detach().cpu().numpy()\n    error = np.degrees(error)\n    return np.mean(error), np.median(error), np.mean(error < 11.25), np.mean(error < 22.5), np.mean(error < 30)\n\n", "\n\n\"\"\"\n=========== Universal Multi-task Trainer ===========\n\"\"\"\n\n\ndef multi_task_trainer(train_loader, test_loader, multi_task_model, device, optimizer, scheduler, opt, total_epoch=200):\n    train_batch = len(train_loader)\n    test_batch = len(test_loader)\n    T = opt.temp\n    avg_cost = np.zeros([total_epoch, 12], dtype=np.float32)\n    lambda_weight = np.ones([2, total_epoch])\n    for index in range(total_epoch):\n        t0 = time.time()\n        cost = np.zeros(12, dtype=np.float32)\n\n        # apply Dynamic Weight Average\n        if opt.weight == 'dwa':\n            if index == 0 or index == 1:\n                lambda_weight[:, index] = 1.0\n            else:\n                w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n                w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n                lambda_weight[0, index] = 2 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T))\n                lambda_weight[1, index] = 2 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T))\n\n        # iteration for all batches\n        multi_task_model.train()\n        train_dataset = iter(train_loader)\n        conf_mat = ConfMatrix(multi_task_model.class_nb)\n        for k in range(train_batch):\n            train_data, train_label, train_depth = train_dataset.next()\n            train_data, train_label = train_data.to(device), train_label.long().to(device)\n            train_depth = train_depth.to(device)\n\n            train_pred, logsigma = multi_task_model(train_data)\n\n            optimizer.zero_grad()\n            train_loss = [\n                model_fit(train_pred[0], train_label, 'semantic'),\n                model_fit(train_pred[1], train_depth, 'depth')\n            ]\n\n            if opt.weight == 'equal' or opt.weight == 'dwa':\n                loss = sum([lambda_weight[i, index] * train_loss[i] for i in range(2)])\n            else:\n                loss = sum(1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2 for i in range(2))\n\n            loss.backward()\n            optimizer.step()\n\n            # accumulate label prediction for every pixel in training images\n            conf_mat.update(train_pred[0].argmax(1).flatten(), train_label.flatten())\n\n            cost[0] = train_loss[0].item()\n            cost[3] = train_loss[1].item()\n            cost[4], cost[5] = depth_error(train_pred[1], train_depth)\n            avg_cost[index, :6] += cost[:6] / train_batch\n\n        # compute mIoU and acc\n        avg_cost[index, 1:3] = conf_mat.get_metrics()\n\n        # evaluating test data\n        multi_task_model.eval()\n        conf_mat = ConfMatrix(multi_task_model.class_nb)\n        with torch.no_grad():  # operations inside don't track history\n            test_dataset = iter(test_loader)\n            for k in range(test_batch):\n                test_data, test_label, test_depth = test_dataset.next()\n                test_data, test_label = test_data.to(device), test_label.long().to(device)\n                test_depth = test_depth.to(device)\n\n                test_pred, _ = multi_task_model(test_data)\n                test_loss = [\n                    model_fit(test_pred[0], test_label, 'semantic'),\n                    model_fit(test_pred[1], test_depth, 'depth')\n                ]\n\n                conf_mat.update(test_pred[0].argmax(1).flatten(), test_label.flatten())\n\n                cost[6] = test_loss[0].item()\n                cost[9] = test_loss[1].item()\n                cost[10], cost[11] = depth_error(test_pred[1], test_depth)\n                avg_cost[index, 6:] += cost[6:] / test_batch\n\n            # compute mIoU and acc\n            avg_cost[index, 7:9] = conf_mat.get_metrics()\n\n        scheduler.step()\n        t1 = time.time()\n        print(\n            'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} || TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | TIME: {:.4f}'\n            .format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n                    avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8],\n                    avg_cost[index, 9], avg_cost[index, 10], avg_cost[index, 11], t1 - t0))", "\n\n\"\"\"\n=========== Universal Single-task Trainer ===========\n\"\"\"\n\n\ndef single_task_trainer(train_loader,\n                        test_loader,\n                        single_task_model,\n                        device,\n                        optimizer,\n                        scheduler,\n                        opt,\n                        total_epoch=200):\n    train_batch = len(train_loader)\n    test_batch = len(test_loader)\n    avg_cost = np.zeros([total_epoch, 12], dtype=np.float32)\n    for index in range(total_epoch):\n        cost = np.zeros(12, dtype=np.float32)\n\n        # iteration for all batches\n        single_task_model.train()\n        train_dataset = iter(train_loader)\n        conf_mat = ConfMatrix(single_task_model.class_nb)\n        for k in range(train_batch):\n            train_data, train_label, train_depth = train_dataset.next()\n            train_data, train_label = train_data.to(device), train_label.long().to(device)\n            train_depth = train_depth.to(device)\n\n            train_pred = single_task_model(train_data)\n            optimizer.zero_grad()\n\n            if opt.task == 'semantic':\n                train_loss = model_fit(train_pred, train_label, opt.task)\n                train_loss.backward()\n                optimizer.step()\n\n                conf_mat.update(train_pred.argmax(1).flatten(), train_label.flatten())\n                cost[0] = train_loss.item()\n\n            if opt.task == 'depth':\n                train_loss = model_fit(train_pred, train_depth, opt.task)\n                train_loss.backward()\n                optimizer.step()\n                cost[3] = train_loss.item()\n                cost[4], cost[5] = depth_error(train_pred, train_depth)\n\n            avg_cost[index, :6] += cost[:6] / train_batch\n\n        if opt.task == 'semantic':\n            avg_cost[index, 1:3] = conf_mat.get_metrics()\n\n        # evaluating test data\n        single_task_model.eval()\n        conf_mat = ConfMatrix(single_task_model.class_nb)\n        with torch.no_grad():  # operations inside don't track history\n            test_dataset = iter(test_loader)\n            for k in range(test_batch):\n                test_data, test_label, test_depth = test_dataset.next()\n                test_data, test_label = test_data.to(device), test_label.long().to(device)\n                test_depth = test_depth.to(device)\n\n                test_pred = single_task_model(test_data)\n\n                if opt.task == 'semantic':\n                    test_loss = model_fit(test_pred, test_label, opt.task)\n\n                    conf_mat.update(test_pred.argmax(1).flatten(), test_label.flatten())\n                    cost[6] = test_loss.item()\n\n                if opt.task == 'depth':\n                    test_loss = model_fit(test_pred, test_depth, opt.task)\n                    cost[9] = test_loss.item()\n                    cost[10], cost[11] = depth_error(test_pred, test_depth)\n\n                avg_cost[index, 6:] += cost[6:] / test_batch\n            if opt.task == 'semantic':\n                avg_cost[index, 7:9] = conf_mat.get_metrics()\n\n        scheduler.step()\n        if opt.task == 'semantic':\n            print('Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f}'.format(\n                index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 6],\n                avg_cost[index, 7], avg_cost[index, 8]))\n        if opt.task == 'depth':\n            print('Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f}'.format(\n                index, avg_cost[index, 3], avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 9],\n                avg_cost[index, 10], avg_cost[index, 11]))\n        torch.save(single_task_model.state_dict(), f\"models/single-{opt.task}-{opt.seed}.pt\")", "\n\n\"\"\"\n=========== Universal Gradient Manipulation Multi-task Trainer ===========\n\"\"\"\n\n\ndef multi_task_rg_trainer(train_loader,\n                          test_loader,\n                          multi_task_model,\n                          device,\n                          optimizer,\n                          scheduler,\n                          opt,\n                          total_epoch=200):\n    method = opt.method\n    alpha = opt.alpha\n    niter = opt.niter\n\n    # warm_niter = opt.warm_niter\n\n    def graddrop(grads):\n        P = 0.5 * (1. + grads.sum(1) / (grads.abs().sum(1) + 1e-8))\n        U = torch.rand_like(grads[:, 0])\n        M = P.gt(U).view(-1, 1) * grads.gt(0) + P.lt(U).view(-1, 1) * grads.lt(0)\n        g = (grads * M.float()).mean(1)\n        return g\n\n    def mgd(grads):\n        grads_cpu = grads.t().cpu()\n        sol, min_norm = MinNormSolver.find_min_norm_element([grads_cpu[t] for t in range(grads.shape[-1])])\n        w = torch.FloatTensor(sol).to(grads.device)\n        g = grads.mm(w.view(-1, 1)).view(-1)\n        return g\n\n    def pcgrad(grads, rng):\n        grad_vec = grads.t()\n        num_tasks = 2\n\n        shuffled_task_indices = np.zeros((num_tasks, num_tasks - 1), dtype=int)\n        for i in range(num_tasks):\n            task_indices = np.arange(num_tasks)\n            task_indices[i] = task_indices[-1]\n            shuffled_task_indices[i] = task_indices[:-1]\n            rng.shuffle(shuffled_task_indices[i])\n        shuffled_task_indices = shuffled_task_indices.T\n\n        normalized_grad_vec = grad_vec / (grad_vec.norm(dim=1, keepdim=True) + 1e-8)  # num_tasks x dim\n        modified_grad_vec = deepcopy(grad_vec)\n        for task_indices in shuffled_task_indices:\n            normalized_shuffled_grad = normalized_grad_vec[task_indices]  # num_tasks x dim\n            dot = (modified_grad_vec * normalized_shuffled_grad).sum(dim=1, keepdim=True)  # num_tasks x dim\n            modified_grad_vec -= torch.clamp_max(dot, 0) * normalized_shuffled_grad\n        g = modified_grad_vec.mean(dim=0)\n        return g\n\n    def cagrad(grads, alpha=0.5, rescale=0):\n        g1 = grads[:, 0]\n        g2 = grads[:, 1]\n\n        g11 = g1.dot(g1).item()\n        g12 = g1.dot(g2).item()\n        g22 = g2.dot(g2).item()\n\n        g0_norm = 0.5 * np.sqrt(g11 + g22 + 2 * g12)\n\n        # want to minimize g_w^Tg_0 + c*||g_0||*||g_w||\n        coef = alpha * g0_norm\n\n        def obj(x):\n            # g_w^T g_0: x*0.5*(g11+g22-2g12)+(0.5+x)*(g12-g22)+g22\n            # g_w^T g_w: x^2*(g11+g22-2g12)+2*x*(g12-g22)+g22\n            return coef * np.sqrt(x**2 * (g11 + g22 - 2 * g12) + 2 * x * (g12 - g22) + g22 +\n                                  1e-8) + 0.5 * x * (g11 + g22 - 2 * g12) + (0.5 + x) * (g12 - g22) + g22\n\n        res = minimize_scalar(obj, bounds=(0, 1), method='bounded')\n        x = res.x\n\n        gw_norm = np.sqrt(x**2 * g11 + (1 - x)**2 * g22 + 2 * x * (1 - x) * g12 + 1e-8)\n        lmbda = coef / (gw_norm + 1e-8)\n        g = (0.5 + lmbda * x) * g1 + (0.5 + lmbda * (1 - x)) * g2  # g0 + lmbda*gw\n        if rescale == 0:\n            return g\n        elif rescale == 1:\n            return g / (1 + alpha**2)\n        else:\n            return g / (1 + alpha)\n\n    def sdmgrad(w, grads, alpha, niter=20):\n        GG = torch.mm(grads.t(), grads)\n        scale = torch.mean(torch.sqrt(torch.diag(GG) + 1e-4))\n        GG = GG / scale.pow(2)\n        Gg = torch.mean(GG, dim=1)\n        gg = torch.mean(Gg)\n\n        w.requires_grad = True\n        optimizer = torch.optim.SGD([w], lr=10, momentum=0.5)\n        for i in range(niter):\n            optimizer.zero_grad()\n            obj = torch.dot(w, torch.mv(GG, w)) + 2 * alpha * torch.dot(w, Gg) + alpha**2 * gg\n            obj.backward()\n            optimizer.step()\n            proj = euclidean_proj_simplex(w.data.cpu().numpy())\n            w.data.copy_(torch.from_numpy(proj).data)\n        w.requires_grad = False\n\n        g0 = torch.mean(grads, dim=1)\n        gw = torch.mv(grads, w)\n        g = (gw + alpha * g0) / (1 + alpha)\n        return g\n\n    def grad2vec(m, grads, grad_dims, task):\n        # store the gradients\n        grads[:, task].fill_(0.0)\n        cnt = 0\n        for mm in m.shared_modules():\n            for p in mm.parameters():\n                grad = p.grad\n                if grad is not None:\n                    grad_cur = grad.data.detach().clone()\n                    beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n                    en = sum(grad_dims[:cnt + 1])\n                    grads[beg:en, task].copy_(grad_cur.data.view(-1))\n                cnt += 1\n\n    def overwrite_grad(m, newgrad, grad_dims):\n        newgrad = newgrad * 2  # to match the sum loss\n        cnt = 0\n        for mm in m.shared_modules():\n            for param in mm.parameters():\n                beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n                en = sum(grad_dims[:cnt + 1])\n                this_grad = newgrad[beg:en].contiguous().view(param.data.size())\n                param.grad = this_grad.data.clone()\n                cnt += 1\n\n    rng = np.random.default_rng()\n    grad_dims = []\n    for mm in multi_task_model.shared_modules():\n        for param in mm.parameters():\n            grad_dims.append(param.data.numel())\n    grads = torch.Tensor(sum(grad_dims), 2).cuda()\n    w = 1 / 2 * torch.ones(2).cuda()\n\n    train_batch = len(train_loader)\n    test_batch = len(test_loader)\n    T = opt.temp\n    avg_cost = np.zeros([total_epoch, 12], dtype=np.float32)\n    lambda_weight = np.ones([2, total_epoch])\n    for index in range(total_epoch):\n        t0 = time.time()\n        cost = np.zeros(12, dtype=np.float32)\n\n        # apply Dynamic Weight Average\n        if opt.weight == 'dwa':\n            if index == 0 or index == 1:\n                lambda_weight[:, index] = 1.0\n            else:\n                w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n                w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n                lambda_weight[0, index] = 2 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T))\n                lambda_weight[1, index] = 2 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T))\n\n        # iteration for all batches\n        multi_task_model.train()\n        train_dataset = iter(train_loader)\n        conf_mat = ConfMatrix(multi_task_model.class_nb)\n        for k in range(train_batch):\n            train_data, train_label, train_depth = train_dataset.next()\n            train_data, train_label = train_data.to(device), train_label.long().to(device)\n            train_depth = train_depth.to(device)\n\n            train_pred, logsigma = multi_task_model(train_data)\n\n            train_loss = [\n                model_fit(train_pred[0], train_label, 'semantic'),\n                model_fit(train_pred[1], train_depth, 'depth')\n            ]\n\n            train_loss_tmp = [0, 0]\n            if opt.weight == 'equal' or opt.weight == 'dwa':\n                for i in range(2):\n                    train_loss_tmp[i] = train_loss[i] * lambda_weight[i, index]\n            else:\n                for i in range(2):\n                    train_loss_tmp[i] = 1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2\n\n            optimizer.zero_grad()\n            if method == \"graddrop\":\n                for i in range(2):\n                    if i == 0:\n                        train_loss_tmp[i].backward(retain_graph=True)\n                    else:\n                        train_loss_tmp[i].backward()\n                    grad2vec(multi_task_model, grads, grad_dims, i)\n                    multi_task_model.zero_grad_shared_modules()\n                g = graddrop(grads)\n                overwrite_grad(multi_task_model, g, grad_dims)\n                optimizer.step()\n            elif method == \"pcgrad\":\n                for i in range(2):\n                    if i == 0:\n                        train_loss_tmp[i].backward(retain_graph=True)\n                    else:\n                        train_loss_tmp[i].backward()\n                    grad2vec(multi_task_model, grads, grad_dims, i)\n                    multi_task_model.zero_grad_shared_modules()\n                g = pcgrad(grads, rng)\n                overwrite_grad(multi_task_model, g, grad_dims)\n                optimizer.step()\n            elif method == \"mgd\":\n                for i in range(2):\n                    if i == 0:\n                        train_loss_tmp[i].backward(retain_graph=True)\n                    else:\n                        train_loss_tmp[i].backward()\n                    grad2vec(multi_task_model, grads, grad_dims, i)\n                    multi_task_model.zero_grad_shared_modules()\n                g = mgd(grads)\n                overwrite_grad(multi_task_model, g, grad_dims)\n                optimizer.step()\n            elif method == \"cagrad\":\n                for i in range(2):\n                    if i == 0:\n                        train_loss_tmp[i].backward(retain_graph=True)\n                    else:\n                        train_loss_tmp[i].backward()\n                    grad2vec(multi_task_model, grads, grad_dims, i)\n                    multi_task_model.zero_grad_shared_modules()\n                g = cagrad(grads, alpha, rescale=1)\n                overwrite_grad(multi_task_model, g, grad_dims)\n                optimizer.step()\n            elif method == \"sdmgrad\":\n                for i in range(2):\n                    if i == 0:\n                        train_loss_tmp[i].backward(retain_graph=True)\n                    else:\n                        train_loss_tmp[i].backward()\n                    grad2vec(multi_task_model, grads, grad_dims, i)\n                    multi_task_model.zero_grad_shared_modules()\n                g = sdmgrad(w, grads, alpha, niter=niter)\n                overwrite_grad(multi_task_model, g, grad_dims)\n                optimizer.step()\n\n            # accumulate label prediction for every pixel in training images\n            conf_mat.update(train_pred[0].argmax(1).flatten(), train_label.flatten())\n\n            cost[0] = train_loss[0].item()\n            cost[3] = train_loss[1].item()\n            cost[4], cost[5] = depth_error(train_pred[1], train_depth)\n            avg_cost[index, :6] += cost[:6] / train_batch\n\n        # compute mIoU and acc\n        avg_cost[index, 1:3] = conf_mat.get_metrics()\n\n        # evaluating test data\n        multi_task_model.eval()\n        conf_mat = ConfMatrix(multi_task_model.class_nb)\n        with torch.no_grad():  # operations inside don't track history\n            test_dataset = iter(test_loader)\n            for k in range(test_batch):\n                test_data, test_label, test_depth = test_dataset.next()\n                test_data, test_label = test_data.to(device), test_label.long().to(device)\n                test_depth = test_depth.to(device)\n\n                test_pred, _ = multi_task_model(test_data)\n                test_loss = [\n                    model_fit(test_pred[0], test_label, 'semantic'),\n                    model_fit(test_pred[1], test_depth, 'depth')\n                ]\n\n                conf_mat.update(test_pred[0].argmax(1).flatten(), test_label.flatten())\n\n                cost[6] = test_loss[0].item()\n                cost[9] = test_loss[1].item()\n                cost[10], cost[11] = depth_error(test_pred[1], test_depth)\n                avg_cost[index, 6:] += cost[6:] / test_batch\n\n            # compute mIoU and acc\n            avg_cost[index, 7:9] = conf_mat.get_metrics()\n\n        scheduler.step()\n        t1 = time.time()\n        print(\n            'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} || TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | TIME: {:.4f}'\n            .format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n                    avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8],\n                    avg_cost[index, 9], avg_cost[index, 10], avg_cost[index, 11], t1 - t0))\n        torch.save(multi_task_model.state_dict(), f\"models/{method}-{opt.weight}-{alpha}-{opt.seed}.pt\")", ""]}
{"filename": "cityscapes/min_norm_solvers.py", "chunked_list": ["# This code is from\n# Multi-Task Learning as Multi-Objective Optimization\n# Ozan Sener, Vladlen Koltun\n# Neural Information Processing Systems (NeurIPS) 2018\n# https://github.com/intel-isl/MultiObjectiveOptimization\n\nimport numpy as np\nimport torch\n\n\nclass MinNormSolver:\n    MAX_ITER = 20\n    STOP_CRIT = 1e-5\n\n    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n        \"\"\"\n        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n        d is the distance (objective) optimzed\n        v1v1 = <x1,x1>\n        v1v2 = <x1,x2>\n        v2v2 = <x2,x2>\n        \"\"\"\n        if v1v2 >= v1v1:\n            # Case: Fig 1, third column\n            gamma = 0.999\n            cost = v1v1\n            return gamma, cost\n        if v1v2 >= v2v2:\n            # Case: Fig 1, first column\n            gamma = 0.001\n            cost = v2v2\n            return gamma, cost\n        # Case: Fig 1, second column\n        gamma = -1.0 * ((v1v2 - v2v2) / (v1v1 + v2v2 - 2 * v1v2))\n        cost = v2v2 + gamma * (v1v2 - v2v2)\n        return gamma, cost\n\n    def _min_norm_2d(vecs, dps):\n        \"\"\"\n        Find the minimum norm solution as combination of two points\n        This is correct only in 2D\n        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n        \"\"\"\n        dmin = np.inf\n        for i in range(len(vecs)):\n            for j in range(i + 1, len(vecs)):\n                if (i, j) not in dps:\n                    dps[(i, j)] = (vecs[i] * vecs[j]).sum().item()\n                    dps[(j, i)] = dps[(i, j)]\n                if (i, i) not in dps:\n                    dps[(i, i)] = (vecs[i] * vecs[i]).sum().item()\n                if (j, j) not in dps:\n                    dps[(j, j)] = (vecs[j] * vecs[j]).sum().item()\n                c, d = MinNormSolver._min_norm_element_from2(dps[(i, i)], dps[(i, j)], dps[(j, j)])\n                if d < dmin:\n                    dmin = d\n                    sol = [(i, j), c, d]\n        return sol, dps\n\n    def _projection2simplex(y):\n        \"\"\"\n        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n        \"\"\"\n        m = len(y)\n        sorted_y = np.flip(np.sort(y), axis=0)\n        tmpsum = 0.0\n        tmax_f = (np.sum(y) - 1.0) / m\n        for i in range(m - 1):\n            tmpsum += sorted_y[i]\n            tmax = (tmpsum - 1) / (i + 1.0)\n            if tmax > sorted_y[i + 1]:\n                tmax_f = tmax\n                break\n        return np.maximum(y - tmax_f, np.zeros(y.shape))\n\n    def _next_point(cur_val, grad, n):\n        proj_grad = grad - (np.sum(grad) / n)\n        tm1 = -1.0 * cur_val[proj_grad < 0] / proj_grad[proj_grad < 0]\n        tm2 = (1.0 - cur_val[proj_grad > 0]) / (proj_grad[proj_grad > 0])\n\n        skippers = np.sum(tm1 < 1e-7) + np.sum(tm2 < 1e-7)\n        t = 1\n        if len(tm1[tm1 > 1e-7]) > 0:\n            t = np.min(tm1[tm1 > 1e-7])\n        if len(tm2[tm2 > 1e-7]) > 0:\n            t = min(t, np.min(tm2[tm2 > 1e-7]))\n\n        next_point = proj_grad * t + cur_val\n        next_point = MinNormSolver._projection2simplex(next_point)\n        return next_point\n\n    def find_min_norm_element(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            grad_dir = -1.0 * np.dot(grad_mat, sol_vec)\n            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n            # Re-compute the inner products for line search\n            v1v1 = 0.0\n            v1v2 = 0.0\n            v2v2 = 0.0\n            for i in range(n):\n                for j in range(n):\n                    v1v1 += sol_vec[i] * sol_vec[j] * dps[(i, j)]\n                    v1v2 += sol_vec[i] * new_point[j] * dps[(i, j)]\n                    v2v2 += new_point[i] * new_point[j] * dps[(i, j)]\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec + (1 - nc) * new_point\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec\n\n    def find_min_norm_element_FW(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the Frank Wolfe until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            t_iter = np.argmin(np.dot(grad_mat, sol_vec))\n\n            v1v1 = np.dot(sol_vec, np.dot(grad_mat, sol_vec))\n            v1v2 = np.dot(sol_vec, grad_mat[:, t_iter])\n            v2v2 = grad_mat[t_iter, t_iter]\n\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec\n            new_sol_vec[t_iter] += 1 - nc\n\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec", "\n\nclass MinNormSolver:\n    MAX_ITER = 20\n    STOP_CRIT = 1e-5\n\n    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n        \"\"\"\n        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n        d is the distance (objective) optimzed\n        v1v1 = <x1,x1>\n        v1v2 = <x1,x2>\n        v2v2 = <x2,x2>\n        \"\"\"\n        if v1v2 >= v1v1:\n            # Case: Fig 1, third column\n            gamma = 0.999\n            cost = v1v1\n            return gamma, cost\n        if v1v2 >= v2v2:\n            # Case: Fig 1, first column\n            gamma = 0.001\n            cost = v2v2\n            return gamma, cost\n        # Case: Fig 1, second column\n        gamma = -1.0 * ((v1v2 - v2v2) / (v1v1 + v2v2 - 2 * v1v2))\n        cost = v2v2 + gamma * (v1v2 - v2v2)\n        return gamma, cost\n\n    def _min_norm_2d(vecs, dps):\n        \"\"\"\n        Find the minimum norm solution as combination of two points\n        This is correct only in 2D\n        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n        \"\"\"\n        dmin = np.inf\n        for i in range(len(vecs)):\n            for j in range(i + 1, len(vecs)):\n                if (i, j) not in dps:\n                    dps[(i, j)] = (vecs[i] * vecs[j]).sum().item()\n                    dps[(j, i)] = dps[(i, j)]\n                if (i, i) not in dps:\n                    dps[(i, i)] = (vecs[i] * vecs[i]).sum().item()\n                if (j, j) not in dps:\n                    dps[(j, j)] = (vecs[j] * vecs[j]).sum().item()\n                c, d = MinNormSolver._min_norm_element_from2(dps[(i, i)], dps[(i, j)], dps[(j, j)])\n                if d < dmin:\n                    dmin = d\n                    sol = [(i, j), c, d]\n        return sol, dps\n\n    def _projection2simplex(y):\n        \"\"\"\n        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n        \"\"\"\n        m = len(y)\n        sorted_y = np.flip(np.sort(y), axis=0)\n        tmpsum = 0.0\n        tmax_f = (np.sum(y) - 1.0) / m\n        for i in range(m - 1):\n            tmpsum += sorted_y[i]\n            tmax = (tmpsum - 1) / (i + 1.0)\n            if tmax > sorted_y[i + 1]:\n                tmax_f = tmax\n                break\n        return np.maximum(y - tmax_f, np.zeros(y.shape))\n\n    def _next_point(cur_val, grad, n):\n        proj_grad = grad - (np.sum(grad) / n)\n        tm1 = -1.0 * cur_val[proj_grad < 0] / proj_grad[proj_grad < 0]\n        tm2 = (1.0 - cur_val[proj_grad > 0]) / (proj_grad[proj_grad > 0])\n\n        skippers = np.sum(tm1 < 1e-7) + np.sum(tm2 < 1e-7)\n        t = 1\n        if len(tm1[tm1 > 1e-7]) > 0:\n            t = np.min(tm1[tm1 > 1e-7])\n        if len(tm2[tm2 > 1e-7]) > 0:\n            t = min(t, np.min(tm2[tm2 > 1e-7]))\n\n        next_point = proj_grad * t + cur_val\n        next_point = MinNormSolver._projection2simplex(next_point)\n        return next_point\n\n    def find_min_norm_element(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            grad_dir = -1.0 * np.dot(grad_mat, sol_vec)\n            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n            # Re-compute the inner products for line search\n            v1v1 = 0.0\n            v1v2 = 0.0\n            v2v2 = 0.0\n            for i in range(n):\n                for j in range(n):\n                    v1v1 += sol_vec[i] * sol_vec[j] * dps[(i, j)]\n                    v1v2 += sol_vec[i] * new_point[j] * dps[(i, j)]\n                    v2v2 += new_point[i] * new_point[j] * dps[(i, j)]\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec + (1 - nc) * new_point\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec\n\n    def find_min_norm_element_FW(vecs):\n        \"\"\"\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the Frank Wolfe until convergence\n        \"\"\"\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            t_iter = np.argmin(np.dot(grad_mat, sol_vec))\n\n            v1v1 = np.dot(sol_vec, np.dot(grad_mat, sol_vec))\n            v1v2 = np.dot(sol_vec, grad_mat[:, t_iter])\n            v2v2 = grad_mat[t_iter, t_iter]\n\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec\n            new_sol_vec[t_iter] += 1 - nc\n\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec", "\n\ndef gradient_normalizers(grads, losses, normalization_type):\n    gn = {}\n    if normalization_type == 'l2':\n        for t in grads:\n            gn[t] = np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n    elif normalization_type == 'loss':\n        for t in grads:\n            gn[t] = losses[t]\n    elif normalization_type == 'loss+':\n        for t in grads:\n            gn[t] = losses[t] * np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n    elif normalization_type == 'none':\n        for t in grads:\n            gn[t] = 1.0\n    else:\n        print('ERROR: Invalid Normalization Type')\n    return gn", ""]}
{"filename": "cityscapes/model_segnet_cross.py", "chunked_list": ["import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nimport torch.utils.data.sampler as sampler\n\nfrom create_dataset import *\nfrom utils import *\n\nparser = argparse.ArgumentParser(description='Multi-task: Cross')", "\nparser = argparse.ArgumentParser(description='Multi-task: Cross')\nparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\nparser.add_argument('--dataroot', default='cityscapes', type=str, help='dataset root')\nparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\nparser.add_argument('--seed', default=0, type=int, help='control seed')\nparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 7\n\n        # define encoder decoder layers\n        self.encoder_block_t = nn.ModuleList(\n            [nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)])])\n        self.decoder_block_t = nn.ModuleList(\n            [nn.ModuleList([self.conv_layer([filter[0], filter[0], filter[0]], bottle_neck=True)])])\n\n        for j in range(2):\n            if j < 1:\n                self.encoder_block_t.append(\n                    nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)]))\n                self.decoder_block_t.append(\n                    nn.ModuleList([self.conv_layer([filter[0], filter[0], filter[0]], bottle_neck=True)]))\n            for i in range(4):\n                if i == 0:\n                    self.encoder_block_t[j].append(\n                        self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=True))\n                    self.decoder_block_t[j].append(\n                        self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=True))\n                else:\n                    self.encoder_block_t[j].append(\n                        self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=False))\n                    self.decoder_block_t[j].append(\n                        self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=False))\n\n        # define cross-stitch units\n        self.cs_unit_encoder = nn.Parameter(data=torch.ones(4, 2))\n        self.cs_unit_decoder = nn.Parameter(data=torch.ones(5, 2))\n\n        # define task specific layers\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], bottle_neck=True, pred_layer=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], bottle_neck=True, pred_layer=True)\n        #self.pred_task3 = self.conv_layer([filter[0], 3], bottle_neck=True, pred_layer=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Parameter):\n                nn.init.constant(m.weight, 1)\n\n    def conv_layer(self, channel, bottle_neck, pred_layer=False):\n        if bottle_neck:\n            if not pred_layer:\n                conv_block = nn.Sequential(\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                    nn.BatchNorm2d(channel[1]),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n                    nn.BatchNorm2d(channel[2]),\n                    nn.ReLU(inplace=True),\n                )\n            else:\n                conv_block = nn.Sequential(\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n                )\n\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[2]),\n                nn.ReLU(inplace=True),\n            )\n        return conv_block\n\n    def forward(self, x):\n        encoder_conv_t, decoder_conv_t, encoder_samp_t, decoder_samp_t, indices_t = ([0] * 2 for _ in range(5))\n        for i in range(2):\n            encoder_conv_t[i], decoder_conv_t[i], encoder_samp_t[i], decoder_samp_t[i], indices_t[i] = (\n                [0] * 5 for _ in range(5))\n\n        # task branch 1\n        for i in range(5):\n            for j in range(2):\n                if i == 0:\n                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](x)\n                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n                else:\n                    encoder_cross_stitch = self.cs_unit_encoder[i - 1][0] * encoder_samp_t[0][i - 1] + \\\n                                           self.cs_unit_encoder[i - 1][1] * encoder_samp_t[1][i - 1]\n                    #self.cs_unit_encoder[i - 1][2] * encoder_samp_t[2][i - 1]\n                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](encoder_cross_stitch)\n                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n\n        for i in range(5):\n            for j in range(2):\n                if i == 0:\n                    decoder_cross_stitch = self.cs_unit_decoder[i][0] * encoder_samp_t[0][-1] + \\\n                                           self.cs_unit_decoder[i][1] * encoder_samp_t[1][-1]\n                    #self.cs_unit_decoder[i][2] * encoder_samp_t[2][-1]\n                    decoder_samp_t[j][i] = self.up_sampling(decoder_cross_stitch, indices_t[j][-i - 1])\n                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](decoder_samp_t[j][i])\n                else:\n                    decoder_cross_stitch = self.cs_unit_decoder[i][0] * decoder_conv_t[0][i - 1] + \\\n                                           self.cs_unit_decoder[i][1] * decoder_conv_t[1][i - 1]\n                    #self.cs_unit_decoder[i][2] * decoder_conv_t[2][i - 1]\n                    decoder_samp_t[j][i] = self.up_sampling(decoder_cross_stitch, indices_t[j][-i - 1])\n                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](decoder_samp_t[j][i])\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(decoder_conv_t[0][-1]), dim=1)\n        t2_pred = self.pred_task2(decoder_conv_t[1][-1])\n        #t3_pred = self.pred_task3(decoder_conv_t[2][-1])\n        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred], self.logsigma", "\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 7\n\n        # define encoder decoder layers\n        self.encoder_block_t = nn.ModuleList(\n            [nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)])])\n        self.decoder_block_t = nn.ModuleList(\n            [nn.ModuleList([self.conv_layer([filter[0], filter[0], filter[0]], bottle_neck=True)])])\n\n        for j in range(2):\n            if j < 1:\n                self.encoder_block_t.append(\n                    nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)]))\n                self.decoder_block_t.append(\n                    nn.ModuleList([self.conv_layer([filter[0], filter[0], filter[0]], bottle_neck=True)]))\n            for i in range(4):\n                if i == 0:\n                    self.encoder_block_t[j].append(\n                        self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=True))\n                    self.decoder_block_t[j].append(\n                        self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=True))\n                else:\n                    self.encoder_block_t[j].append(\n                        self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=False))\n                    self.decoder_block_t[j].append(\n                        self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=False))\n\n        # define cross-stitch units\n        self.cs_unit_encoder = nn.Parameter(data=torch.ones(4, 2))\n        self.cs_unit_decoder = nn.Parameter(data=torch.ones(5, 2))\n\n        # define task specific layers\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], bottle_neck=True, pred_layer=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], bottle_neck=True, pred_layer=True)\n        #self.pred_task3 = self.conv_layer([filter[0], 3], bottle_neck=True, pred_layer=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Parameter):\n                nn.init.constant(m.weight, 1)\n\n    def conv_layer(self, channel, bottle_neck, pred_layer=False):\n        if bottle_neck:\n            if not pred_layer:\n                conv_block = nn.Sequential(\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                    nn.BatchNorm2d(channel[1]),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n                    nn.BatchNorm2d(channel[2]),\n                    nn.ReLU(inplace=True),\n                )\n            else:\n                conv_block = nn.Sequential(\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n                )\n\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[2]),\n                nn.ReLU(inplace=True),\n            )\n        return conv_block\n\n    def forward(self, x):\n        encoder_conv_t, decoder_conv_t, encoder_samp_t, decoder_samp_t, indices_t = ([0] * 2 for _ in range(5))\n        for i in range(2):\n            encoder_conv_t[i], decoder_conv_t[i], encoder_samp_t[i], decoder_samp_t[i], indices_t[i] = (\n                [0] * 5 for _ in range(5))\n\n        # task branch 1\n        for i in range(5):\n            for j in range(2):\n                if i == 0:\n                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](x)\n                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n                else:\n                    encoder_cross_stitch = self.cs_unit_encoder[i - 1][0] * encoder_samp_t[0][i - 1] + \\\n                                           self.cs_unit_encoder[i - 1][1] * encoder_samp_t[1][i - 1]\n                    #self.cs_unit_encoder[i - 1][2] * encoder_samp_t[2][i - 1]\n                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](encoder_cross_stitch)\n                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n\n        for i in range(5):\n            for j in range(2):\n                if i == 0:\n                    decoder_cross_stitch = self.cs_unit_decoder[i][0] * encoder_samp_t[0][-1] + \\\n                                           self.cs_unit_decoder[i][1] * encoder_samp_t[1][-1]\n                    #self.cs_unit_decoder[i][2] * encoder_samp_t[2][-1]\n                    decoder_samp_t[j][i] = self.up_sampling(decoder_cross_stitch, indices_t[j][-i - 1])\n                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](decoder_samp_t[j][i])\n                else:\n                    decoder_cross_stitch = self.cs_unit_decoder[i][0] * decoder_conv_t[0][i - 1] + \\\n                                           self.cs_unit_decoder[i][1] * decoder_conv_t[1][i - 1]\n                    #self.cs_unit_decoder[i][2] * decoder_conv_t[2][i - 1]\n                    decoder_samp_t[j][i] = self.up_sampling(decoder_cross_stitch, indices_t[j][-i - 1])\n                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](decoder_samp_t[j][i])\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(decoder_conv_t[0][-1]), dim=1)\n        t2_pred = self.pred_task2(decoder_conv_t[1][-1])\n        #t3_pred = self.pred_task3(decoder_conv_t[2][-1])\n        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred], self.logsigma", "\n\ncontrol_seed(opt.seed)\n# define model, optimiser and scheduler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSegNet_CROSS = SegNet().to(device)\noptimizer = optim.Adam(SegNet_CROSS.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_CROSS),", "\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_CROSS),\n                                                         count_parameters(SegNet_CROSS) / 24981069))\nprint(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\n# define dataset\ndataset_path = opt.dataroot\nif opt.apply_augmentation:\n    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation on CityScapes.')\nelse:\n    train_set = CityScapes(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')", "if opt.apply_augmentation:\n    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation on CityScapes.')\nelse:\n    train_set = CityScapes(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')\n\ntest_set = CityScapes(root=dataset_path, train=False)\n\nbatch_size = 8", "\nbatch_size = 8\ntrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\n# Train and evaluate multi-task network\nmulti_task_trainer(train_loader, test_loader, SegNet_CROSS, device, optimizer, scheduler, opt, 200)\n", ""]}
{"filename": "cityscapes/model_segnet_mt.py", "chunked_list": ["import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nimport torch.utils.data.sampler as sampler\n\nfrom create_dataset import *\nfrom utils import *\n\nparser = argparse.ArgumentParser(description='Multi-task: Attention Network')", "\nparser = argparse.ArgumentParser(description='Multi-task: Attention Network')\nparser.add_argument('--method', default='sdmgrad', type=str, help='which optimization algorithm to use')\nparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\nparser.add_argument('--dataroot', default='cityscapes', type=str, help='dataset root')\nparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\nparser.add_argument('--alpha', default=0.3, type=float, help='the alpha')\nparser.add_argument('--lr', default=1e-4, type=float, help='the learning rate')\nparser.add_argument('--seed', default=1, type=int, help='control seed')\nparser.add_argument('--niter', default=20, type=int, help='number of inner iteration')", "parser.add_argument('--seed', default=1, type=int, help='control seed')\nparser.add_argument('--niter', default=20, type=int, help='number of inner iteration')\nparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 7\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        # define task attention layers\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(2):\n            if j < 1:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n        #self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def shared_modules(self):\n        return [\n            self.encoder_block, self.decoder_block, self.conv_block_enc, self.conv_block_dec, self.encoder_block_att,\n            self.decoder_block_att, self.down_sampling, self.up_sampling\n        ]\n\n    def zero_grad_shared_modules(self):\n        for mm in self.shared_modules():\n            mm.zero_grad()\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for tasks\n        atten_encoder, atten_decoder = ([0] * 2 for _ in range(2))\n        for i in range(2):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(2):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(2):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n                                                           scale_factor=2,\n                                                           mode='bilinear',\n                                                           align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n        #t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred], self.logsigma", "\n\ncontrol_seed(opt.seed)\n# define model, optimiser and scheduler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSegNet_MTAN = SegNet().to(device)\noptimizer = optim.Adam(SegNet_MTAN.parameters(), lr=opt.lr)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_MTAN),", "\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_MTAN),\n                                                         count_parameters(SegNet_MTAN) / 24981069))\nprint(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\n# define dataset\ndataset_path = opt.dataroot\nif opt.apply_augmentation:\n    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation.')\nelse:\n    train_set = CityScapes(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')", "if opt.apply_augmentation:\n    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation.')\nelse:\n    train_set = CityScapes(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')\n\ntest_set = CityScapes(root=dataset_path, train=False)\n\nbatch_size = 8", "\nbatch_size = 8\ntrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\n# Train and evaluate multi-task network\nmulti_task_rg_trainer(train_loader, test_loader, SegNet_MTAN, device, optimizer, scheduler, opt, 200)\n", ""]}
{"filename": "cityscapes/create_dataset.py", "chunked_list": ["from torch.utils.data.dataset import Dataset\n\nimport os\nimport torch\nimport torch.nn.functional as F\nimport fnmatch\nimport numpy as np\nimport random\n\n\nclass RandomScaleCrop(object):\n    \"\"\"\n    Credit to Jialong Wu from https://github.com/lorenmt/mtan/issues/34.\n    \"\"\"\n\n    def __init__(self, scale=[1.0, 1.2, 1.5]):\n        self.scale = scale\n\n    def __call__(self, img, label, depth, normal):\n        height, width = img.shape[-2:]\n        sc = self.scale[random.randint(0, len(self.scale) - 1)]\n        h, w = int(height / sc), int(width / sc)\n        i = random.randint(0, height - h)\n        j = random.randint(0, width - w)\n        img_ = F.interpolate(img[None, :, i:i + h, j:j + w], size=(height, width), mode='bilinear',\n                             align_corners=True).squeeze(0)\n        label_ = F.interpolate(label[None, None, i:i + h, j:j + w], size=(height, width),\n                               mode='nearest').squeeze(0).squeeze(0)\n        depth_ = F.interpolate(depth[None, :, i:i + h, j:j + w], size=(height, width), mode='nearest').squeeze(0)\n        normal_ = F.interpolate(normal[None, :, i:i + h, j:j + w],\n                                size=(height, width),\n                                mode='bilinear',\n                                align_corners=True).squeeze(0)\n        return img_, label_, depth_ / sc, normal_", "\n\nclass RandomScaleCrop(object):\n    \"\"\"\n    Credit to Jialong Wu from https://github.com/lorenmt/mtan/issues/34.\n    \"\"\"\n\n    def __init__(self, scale=[1.0, 1.2, 1.5]):\n        self.scale = scale\n\n    def __call__(self, img, label, depth, normal):\n        height, width = img.shape[-2:]\n        sc = self.scale[random.randint(0, len(self.scale) - 1)]\n        h, w = int(height / sc), int(width / sc)\n        i = random.randint(0, height - h)\n        j = random.randint(0, width - w)\n        img_ = F.interpolate(img[None, :, i:i + h, j:j + w], size=(height, width), mode='bilinear',\n                             align_corners=True).squeeze(0)\n        label_ = F.interpolate(label[None, None, i:i + h, j:j + w], size=(height, width),\n                               mode='nearest').squeeze(0).squeeze(0)\n        depth_ = F.interpolate(depth[None, :, i:i + h, j:j + w], size=(height, width), mode='nearest').squeeze(0)\n        normal_ = F.interpolate(normal[None, :, i:i + h, j:j + w],\n                                size=(height, width),\n                                mode='bilinear',\n                                align_corners=True).squeeze(0)\n        return img_, label_, depth_ / sc, normal_", "\n\nclass RandomScaleCropCityScapes(object):\n    \"\"\"\n    Credit to Jialong Wu from https://github.com/lorenmt/mtan/issues/34.\n    \"\"\"\n\n    def __init__(self, scale=[1.0, 1.2, 1.5]):\n        self.scale = scale\n\n    def __call__(self, img, label, depth):\n        height, width = img.shape[-2:]\n        sc = self.scale[random.randint(0, len(self.scale) - 1)]\n        h, w = int(height / sc), int(width / sc)\n        i = random.randint(0, height - h)\n        j = random.randint(0, width - w)\n        img_ = F.interpolate(img[None, :, i:i + h, j:j + w], size=(height, width), mode='bilinear',\n                             align_corners=True).squeeze(0)\n        label_ = F.interpolate(label[None, None, i:i + h, j:j + w], size=(height, width),\n                               mode='nearest').squeeze(0).squeeze(0)\n        depth_ = F.interpolate(depth[None, :, i:i + h, j:j + w], size=(height, width), mode='nearest').squeeze(0)\n        return img_, label_, depth_ / sc", "\n\nclass NYUv2(Dataset):\n    \"\"\"\n    We could further improve the performance with the data augmentation of NYUv2 defined in:\n        [1] PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing\n        [2] Pattern affinitive propagation across depth, surface normal and semantic segmentation\n        [3] Mti-net: Multiscale task interaction networks for multi-task learning\n\n        1. Random scale in a selected raio 1.0, 1.2, and 1.5.\n        2. Random horizontal flip.\n\n    Please note that: all baselines and MTAN did NOT apply data augmentation in the original paper.\n    \"\"\"\n\n    def __init__(self, root, train=True, augmentation=False):\n        self.train = train\n        self.root = os.path.expanduser(root)\n        self.augmentation = augmentation\n\n        # read the data file\n        if train:\n            self.data_path = root + '/train'\n        else:\n            self.data_path = root + '/val'\n\n        # calculate data length\n        self.data_len = len(fnmatch.filter(os.listdir(self.data_path + '/image'), '*.npy'))\n\n    def __getitem__(self, index):\n        # load data from the pre-processed npy files\n        image = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/image/{:d}.npy'.format(index)), -1, 0))\n        semantic = torch.from_numpy(np.load(self.data_path + '/label/{:d}.npy'.format(index)))\n        depth = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/depth/{:d}.npy'.format(index)), -1, 0))\n        normal = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/normal/{:d}.npy'.format(index)), -1, 0))\n\n        # apply data augmentation if required\n        if self.augmentation:\n            image, semantic, depth, normal = RandomScaleCrop()(image, semantic, depth, normal)\n            if torch.rand(1) < 0.5:\n                image = torch.flip(image, dims=[2])\n                semantic = torch.flip(semantic, dims=[1])\n                depth = torch.flip(depth, dims=[2])\n                normal = torch.flip(normal, dims=[2])\n                normal[0, :, :] = -normal[0, :, :]\n\n        return image.float(), semantic.float(), depth.float(), normal.float()\n\n    def __len__(self):\n        return self.data_len", "\n\nclass CityScapes(Dataset):\n    \"\"\"\n    We could further improve the performance with the data augmentation of NYUv2 defined in:\n        [1] PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing\n        [2] Pattern affinitive propagation across depth, surface normal and semantic segmentation\n        [3] Mti-net: Multiscale task interaction networks for multi-task learning\n\n        1. Random scale in a selected raio 1.0, 1.2, and 1.5.\n        2. Random horizontal flip.\n\n    Please note that: all baselines and MTAN did NOT apply data augmentation in the original paper.\n    \"\"\"\n\n    def __init__(self, root, train=True, augmentation=False):\n        self.train = train\n        self.root = os.path.expanduser(root)\n        self.augmentation = augmentation\n\n        # read the data file\n        if train:\n            self.data_path = root + '/train'\n        else:\n            self.data_path = root + '/val'\n\n        # calculate data length\n        self.data_len = len(fnmatch.filter(os.listdir(self.data_path + '/image'), '*.npy'))\n\n    def __getitem__(self, index):\n        # load data from the pre-processed npy files\n        image = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/image/{:d}.npy'.format(index)), -1, 0))\n        semantic = torch.from_numpy(np.load(self.data_path + '/label_7/{:d}.npy'.format(index)))\n        depth = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/depth/{:d}.npy'.format(index)), -1, 0))\n\n        # apply data augmentation if required\n        if self.augmentation:\n            image, semantic, depth = RandomScaleCropCityScapes()(image, semantic, depth)\n            if torch.rand(1) < 0.5:\n                image = torch.flip(image, dims=[2])\n                semantic = torch.flip(semantic, dims=[1])\n                depth = torch.flip(depth, dims=[2])\n\n        return image.float(), semantic.float(), depth.float()\n\n    def __len__(self):\n        return self.data_len", ""]}
{"filename": "cityscapes/model_segnet_single.py", "chunked_list": ["import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\n\nfrom create_dataset import *\nfrom utils import *\n\nparser = argparse.ArgumentParser(description='Single-task: One Task')\nparser.add_argument('--task', default='semantic', type=str, help='choose task: semantic, depth')", "parser = argparse.ArgumentParser(description='Single-task: One Task')\nparser.add_argument('--task', default='semantic', type=str, help='choose task: semantic, depth')\nparser.add_argument('--dataroot', default='cityscapes', type=str, help='dataset root')\nparser.add_argument('--seed', default=0, type=int, help='control seed')\nparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 7\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        if opt.task == 'semantic':\n            self.pred_task = self.conv_layer([filter[0], self.class_nb], pred=True)\n        if opt.task == 'depth':\n            self.pred_task = self.conv_layer([filter[0], 1], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task prediction layers\n        if opt.task == 'semantic':\n            pred = F.log_softmax(self.pred_task(g_decoder[-1][-1]), dim=1)\n        if opt.task == 'depth':\n            pred = self.pred_task(g_decoder[-1][-1])\n        return pred", "class SegNet(nn.Module):\n\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 7\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(\n                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(\n                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\n        if opt.task == 'semantic':\n            self.pred_task = self.conv_layer([filter[0], self.class_nb], pred=True)\n        if opt.task == 'depth':\n            self.pred_task = self.conv_layer([filter[0], 1], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task prediction layers\n        if opt.task == 'semantic':\n            pred = F.log_softmax(self.pred_task(g_decoder[-1][-1]), dim=1)\n        if opt.task == 'depth':\n            pred = self.pred_task(g_decoder[-1][-1])\n        return pred", "\n\ncontrol_seed(opt.seed)\n# define model, optimiser and scheduler\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nSegNet = SegNet().to(device)\noptimizer = optim.Adam(SegNet.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet), count_parameters(SegNet) / 24981069))", "\nprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet), count_parameters(SegNet) / 24981069))\nprint(\n    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\n# define dataset\ndataset_path = opt.dataroot\nif opt.apply_augmentation:\n    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n    print('Applying data augmentation.')\nelse:\n    train_set = CityScapes(root=dataset_path, train=True)\n    print('Standard training strategy without data augmentation.')", "\ntest_set = CityScapes(root=dataset_path, train=False)\n\nbatch_size = 8\ntrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\n# Train and evaluate single-task network\nsingle_task_trainer(train_loader, test_loader, SegNet, device, optimizer, scheduler, opt, 200)", "# Train and evaluate single-task network\nsingle_task_trainer(train_loader, test_loader, SegNet, device, optimizer, scheduler, opt, 200)\n"]}
