{"filename": "deps/jax-tcnn/setup.py", "chunked_list": ["#!/usr/bin/env python\n\nimport os\nimport subprocess\n\nfrom setuptools import Extension, find_packages, setup\nfrom setuptools.command.build_ext import build_ext\n\nHERE = os.path.dirname(os.path.realpath(__file__))\n", "HERE = os.path.dirname(os.path.realpath(__file__))\n\n\nclass CMakeBuildExt(build_ext):\n    def build_extensions(self):\n        # First: configure CMake build\n        import platform\n        import sys\n        import sysconfig\n\n        import pybind11\n\n        # Work out the relevant Python paths to pass to CMake, adapted from the\n        # PyTorch build system\n        if platform.system() == \"Windows\":\n            cmake_python_library = \"{}/libs/python{}.lib\".format(\n                sysconfig.get_config_var(\"prefix\"),\n                sysconfig.get_config_var(\"VERSION\"),\n            )\n            if not os.path.exists(cmake_python_library):\n                cmake_python_library = \"{}/libs/python{}.lib\".format(\n                    sys.base_prefix,\n                    sysconfig.get_config_var(\"VERSION\"),\n                )\n        else:\n            cmake_python_library = \"{}/{}\".format(\n                sysconfig.get_config_var(\"LIBDIR\"),\n                sysconfig.get_config_var(\"INSTSONAME\"),\n            )\n        cmake_python_include_dir = sysconfig.get_path(\"include\")\n\n        install_dir = os.path.abspath(\n            os.path.dirname(self.get_ext_fullpath(\"dummy\"))\n        )\n        os.makedirs(install_dir, exist_ok=True)\n        cmake_args = [\n            \"-DCMAKE_INSTALL_PREFIX={}\".format(install_dir),\n            \"-DPython_EXECUTABLE={}\".format(sys.executable),\n            \"-DPython_LIBRARIES={}\".format(cmake_python_library),\n            \"-DPython_INCLUDE_DIRS={}\".format(cmake_python_include_dir),\n            \"-DCMAKE_BUILD_TYPE={}\".format(\n                \"Debug\" if self.debug else \"Release\"\n            ),\n            \"-DCMAKE_PREFIX_PATH={}\".format(pybind11.get_cmake_dir()),\n            \"-G Ninja\",\n        ] + os.environ[\"cmakeFlags\"].split()\n        os.makedirs(self.build_temp, exist_ok=True)\n        subprocess.check_call(\n            [\"cmake\", HERE] + cmake_args, cwd=self.build_temp\n        )\n\n        # Build all the extensions\n        super().build_extensions()\n\n        # Finally run install\n        subprocess.check_call(\n            [\"cmake\", \"--build\", \".\", \"--target\", \"install\"],\n            cwd=self.build_temp,\n        )\n\n    def build_extension(self, ext):\n        target_name = ext.name.split(\".\")[-1]\n        subprocess.check_call(\n            [\"cmake\", \"--build\", \".\", \"--target\", target_name],\n            cwd=self.build_temp,\n        )", "\nextensions = [\n    Extension(\n        \"jax_tcnn.tcnnutils\",  # Python dotted name, whose final component should be a buildable target defined in CMakeLists.txt\n        [  # source paths, relative to this setup.py file\n            \"lib/ffi.cc\",\n            \"lib/impl/hashgrid.cu\",\n        ],\n    ),\n]", "    ),\n]\n\nsetup(\n    name=\"jax-tcnn\",\n    author=\"blurgyy\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    include_package_data=True,\n    install_requires=[\"jax\", \"jaxlib\", \"chex\"],", "    include_package_data=True,\n    install_requires=[\"jax\", \"jaxlib\", \"chex\"],\n    ext_modules=extensions,\n    cmdclass={\"build_ext\": CMakeBuildExt},\n)\n"]}
{"filename": "deps/jax-tcnn/src/jaxtcnn/__init__.py", "chunked_list": ["from .hashgrid_tcnn import HashGridMetadata, hashgrid_encode\n\n\n__all__ = [\n    \"HashGridMetadata\",\n    \"hashgrid_encode\"\n]\n"]}
{"filename": "deps/jax-tcnn/src/jaxtcnn/hashgrid_tcnn/impl.py", "chunked_list": ["import dataclasses\nimport functools\nfrom typing import Tuple\n\nimport chex\nimport jax\nfrom jax.interpreters import mlir, xla\nfrom jax.lib import xla_client\n\nfrom . import abstract, lowering", "\nfrom . import abstract, lowering\nfrom .. import tcnnutils\n\n\n# register GPU XLA custom calls\nfor name, value in tcnnutils.get_hashgrid_registrations().items():\n    xla_client.register_custom_call_target(name, value, platform=\"gpu\")\n\n", "\n\n# primitives\nhashgrid_encode_p = jax.core.Primitive(\"hashgrid\ud83c\udfc1\")\nhashgrid_encode_p.multiple_results = True\nhashgrid_encode_p.def_impl(functools.partial(xla.apply_primitive, hashgrid_encode_p))\nhashgrid_encode_p.def_abstract_eval(abstract.hashgrid_encode_abstract)\n\nhashgrid_encode_backward_p = jax.core.Primitive(\"hashgrid\ud83c\udfc1backward\")\nhashgrid_encode_backward_p.multiple_results = True", "hashgrid_encode_backward_p = jax.core.Primitive(\"hashgrid\ud83c\udfc1backward\")\nhashgrid_encode_backward_p.multiple_results = True\nhashgrid_encode_backward_p.def_impl(functools.partial(xla.apply_primitive, hashgrid_encode_backward_p))\nhashgrid_encode_backward_p.def_abstract_eval(abstract.hashgrid_encode_backward_abstract)\n\n\n# lowering rules\nmlir.register_lowering(\n    prim=hashgrid_encode_p,\n    rule=lowering.hashgrid_encode_lowering_rule,", "    prim=hashgrid_encode_p,\n    rule=lowering.hashgrid_encode_lowering_rule,\n    platform=\"gpu\",\n)\nmlir.register_lowering(\n    prim=hashgrid_encode_backward_p,\n    rule=lowering.hashgrid_encode_backward_lowering_rule,\n    platform=\"gpu\",\n)\n", ")\n\n\n@jax.tree_util.register_pytree_node_class\n@dataclasses.dataclass(frozen=True, kw_only=True)\nclass HashGridMetadata:\n    # number of levels, \"n_levels\" in tcnn\n    L: int\n\n    # number of features that each level should output, \"n_features_per_level\" in tcnn\n    F: int\n\n    # coarsest resolution, \"base_resolution\" in tcnn\n    N_min: int\n\n    # scale factor between consecutive levels\n    per_level_scale: float\n\n    def tree_flatten(self):\n        children = ()\n        aux = (self.L, self.F, self.N_min, self.per_level_scale)\n        return children, aux\n\n    @classmethod\n    def tree_unflatten(cls, aux, children):\n        L, F, N_min, per_level_scale = aux\n        return cls(\n            L=L,\n            F=F,\n            N_min=N_min,\n            per_level_scale=per_level_scale,\n        )", "\n\n@functools.partial(jax.custom_vjp, nondiff_argnums=[0])\ndef __hashgrid_encode(\n    desc: HashGridMetadata,\n    offset_table_data: jax.Array,\n    coords_rm: jax.Array,\n    params: jax.Array,\n):\n    encoded_coords_rm, dy_dcoords_rm = hashgrid_encode_p.bind(\n        offset_table_data,\n        coords_rm,\n        params,\n        L=desc.L,\n        F=desc.F,\n        N_min=desc.N_min,\n        per_level_scale=desc.per_level_scale,\n    )\n    return encoded_coords_rm, dy_dcoords_rm", "\n\ndef __hashgrid_encode_fwd(\n    desc: HashGridMetadata,\n    offset_table_data: jax.Array,\n    coords_rm: jax.Array,\n    params: jax.Array,\n):\n    primal_outputs = __hashgrid_encode(\n        desc=desc,\n        offset_table_data=offset_table_data,\n        coords_rm=coords_rm,\n        params=params,\n    )\n    encoded_coords_rm, dy_dcoords_rm = primal_outputs\n    aux = {\n        \"in.offset_table_data\": offset_table_data,\n        \"in.coords_rm\": coords_rm,\n        \"in.params\": params,\n        \"out.dy_dcoords_rm\": dy_dcoords_rm,\n    }\n    return primal_outputs, aux", "\n\ndef __hashgrid_encode_bwd(desc: HashGridMetadata, aux, grads):\n    dL_dy_rm, _ = grads\n    dL_dparams, dL_dcoords_rm = hashgrid_encode_backward_p.bind(\n        aux[\"in.offset_table_data\"],\n        aux[\"in.coords_rm\"],\n        aux[\"in.params\"],\n        dL_dy_rm,\n        aux[\"out.dy_dcoords_rm\"],\n\n        L=desc.L,\n        F=desc.F,\n        N_min=desc.N_min,\n        per_level_scale=desc.per_level_scale,\n    )\n    return None, dL_dcoords_rm, dL_dparams", "\n\n__hashgrid_encode.defvjp(\n    fwd=__hashgrid_encode_fwd,\n    bwd=__hashgrid_encode_bwd,\n)\n"]}
{"filename": "deps/jax-tcnn/src/jaxtcnn/hashgrid_tcnn/lowering.py", "chunked_list": ["from jax.interpreters import mlir\nfrom jax.interpreters.mlir import ir\n\nfrom .. import tcnnutils\n\ntry:\n    from jaxlib.mhlo_helpers import custom_call\nexcept ModuleNotFoundError:\n    # A more recent jaxlib would have `hlo_helpers` instead of `mhlo_helpers`\n    # <https://github.com/google/jax/commit/b8ae8e3fa10f9abe998459fac1513915acee776d#diff-50658d597212b4ce070b8bd8c1fc522deeee1845ba387a0a5b507c446e8ea12a>\n    from jaxlib.hlo_helpers import custom_call", "\n\n# helper function for mapping given shapes to their default mlir layouts\ndef default_layouts(*shapes):\n    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\n\ndef hashgrid_encode_lowering_rule(\n    ctx: mlir.LoweringRule,\n\n    # arrays\n    offset_table_data: ir.Value,\n    coords_rm: ir.Value,\n    params: ir.Value,\n\n    # static args\n    L: int,\n    F: int,\n    N_min: int,\n    per_level_scale: float,\n):\n    dim, n_coords = ir.RankedTensorType(coords_rm.type).shape\n    n_params, _ = ir.RankedTensorType(params.type).shape\n\n    opaque = tcnnutils.make_hashgrid_descriptor(\n        n_coords,\n        L,\n        F,\n        N_min,\n        per_level_scale,\n    )\n\n    shapes = {\n        \"in.offset_table_data\": (L + 1,),\n        \"in.coords_rm\": (dim, n_coords),\n        \"in.params\": (n_params, F),\n\n        \"out.encoded_coords_rm\": (L * F, n_coords),\n        \"out.dy_dcoords_rm\": (dim * L * F, n_coords),\n    }\n\n    return custom_call(\n        call_target_name=\"hashgrid_encode\",\n        out_types=[\n            ir.RankedTensorType.get(shapes[\"out.encoded_coords_rm\"], ir.F32Type.get()),\n            ir.RankedTensorType.get(shapes[\"out.dy_dcoords_rm\"], ir.F32Type.get()),\n        ],\n        operands=[\n            offset_table_data,\n            coords_rm,\n            params,\n        ],\n        backend_config=opaque,\n        operand_layouts=default_layouts(\n            shapes[\"in.offset_table_data\"],\n            shapes[\"in.coords_rm\"],\n            shapes[\"in.params\"],\n        ),\n        result_layouts=default_layouts(\n            shapes[\"out.encoded_coords_rm\"],\n            shapes[\"out.dy_dcoords_rm\"],\n        ),\n    )", "\n\ndef hashgrid_encode_backward_lowering_rule(\n    ctx: mlir.LoweringRule,\n\n    offset_table_data: ir.Value,\n    coords_rm: ir.Value,\n    params: ir.Value,  # only for determining shape of dL_dparams\n    dL_dy_rm: ir.Value,\n    dy_dcoords_rm: ir.Value,\n\n    # static args\n    L: int,\n    F: int,\n    N_min: int,\n    per_level_scale: float,\n):\n    dim, n_coords = ir.RankedTensorType(coords_rm.type).shape\n    n_params, _ = ir.RankedTensorType(params.type).shape\n\n    opaque = tcnnutils.make_hashgrid_descriptor(\n        n_coords,\n        L,\n        F,\n        N_min,\n        per_level_scale,\n    )\n\n    shapes = {\n        \"in.offset_table_data\": (L + 1,),\n        \"in.coords_rm\": (dim, n_coords),\n        # \"in.params\": (n_params, F),\n        \"in.dL_dy_rm\": (L * F, n_coords),\n        \"in.dy_dcoords_rm\": (dim * L * F, n_coords),\n\n        \"out.dL_dparams\": (n_params, F),\n        \"out.dL_dcoords_rm\": (dim, n_coords),\n    }\n\n    return custom_call(\n        call_target_name=\"hashgrid_encode_backward\",\n        out_types=[\n            ir.RankedTensorType.get(shapes[\"out.dL_dparams\"], ir.F32Type.get()),\n            ir.RankedTensorType.get(shapes[\"out.dL_dcoords_rm\"], ir.F32Type.get()),\n        ],\n        operands=[\n            offset_table_data,\n            coords_rm,\n            dL_dy_rm,\n            dy_dcoords_rm\n        ],\n        backend_config=opaque,\n        result_layouts=default_layouts(\n            shapes[\"out.dL_dparams\"],\n            shapes[\"out.dL_dcoords_rm\"],\n        ),\n        operand_layouts=default_layouts(\n            shapes[\"in.offset_table_data\"],\n            shapes[\"in.coords_rm\"],\n            shapes[\"in.dL_dy_rm\"],\n            shapes[\"in.dy_dcoords_rm\"],\n        ),\n    )", ""]}
{"filename": "deps/jax-tcnn/src/jaxtcnn/hashgrid_tcnn/abstract.py", "chunked_list": ["import chex\nimport jax\nimport jax.numpy as jnp\n\n\ndef hashgrid_encode_abstract(\n    # arrays\n    offset_table_data: jax.Array,\n    coords_rm: jax.Array,\n    params: jax.Array,\n\n    # static args\n    L: int,\n    F: int,\n    N_min: int,\n    per_level_scale: float,\n):\n    dim, n_coords = coords_rm.shape\n    if dim != 3:\n        raise NotImplementedError(\n            \"hashgrid encoding is only implemented for 3D coordinates, expected input coordinates to have shape ({}, n_coords), but got shape {}\".format(\n                dim, coords_rm.shape\n            )\n        )\n\n    n_params, _ = params.shape\n\n    chex.assert_shape(offset_table_data, (L + 1,))\n    chex.assert_shape(coords_rm, (dim, n_coords))\n    chex.assert_shape(params, (n_params, F))\n\n    chex.assert_scalar(L)\n    chex.assert_scalar(F)\n    chex.assert_scalar(N_min)\n    chex.assert_scalar(per_level_scale)\n    chex.assert_type([L, F, N_min], int)\n    chex.assert_type(per_level_scale, float)\n\n    offset_dtype = jax.dtypes.canonicalize_dtype(offset_table_data.dtype)\n    if offset_dtype != jnp.uint32:\n        raise RuntimeError(\n            \"hashgrid encoding expects `offset_table_data` (a prefix sum of the hash table sizes of each level) to be of type uint32, got {}\".format(offset_dtype)\n        )\n\n    coord_dtype = jax.dtypes.canonicalize_dtype(coords_rm.dtype)\n    if coord_dtype != jnp.float32:\n        raise NotImplementedError(\n            \"hashgrid encoding is only implemented for input coordinates of type float32, got {}\".format(\n                coord_dtype\n            )\n        )\n\n    param_dtype = jax.dtypes.canonicalize_dtype(params.dtype)\n    if param_dtype != jnp.float32:\n        raise NotImplementedError(\n            \"hashgrid encoding is only implemented for parameters of type float32, got {}\".format(\n                param_dtype\n            )\n        )\n\n    out_shapes = {\n        \"encoded_coords_rm\": (L * F, n_coords),\n        \"dy_dcoords_rm\": (dim * L * F, n_coords),\n    }\n\n    return (\n        jax.ShapedArray(shape=out_shapes[\"encoded_coords_rm\"], dtype=jnp.float32),\n        jax.ShapedArray(shape=out_shapes[\"dy_dcoords_rm\"], dtype=jnp.float32),\n    )", "\n\ndef hashgrid_encode_backward_abstract(\n    offset_table_data: jax.ShapedArray,\n    coords_rm: jax.ShapedArray,\n    params: jax.ShapedArray,  # only for determining shape of dL_dparams\n    dL_dy_rm: jax.ShapedArray,\n    dy_dcoords_rm: jax.ShapedArray,\n\n    # static args\n    L: int,\n    F: int,\n    N_min: int,\n    per_level_scale: float,\n):\n    dim, n_coords = coords_rm.shape\n    if dim != 3:\n        raise NotImplementedError(\n            \"hashgrid encoding is only implemented for 3D coordinates, expected input coordinates to have shape ({}, n_coords), but got shape {}\".format(\n                dim, coords_rm.shape\n            )\n        )\n\n    n_params, _ = params.shape\n\n    chex.assert_shape(offset_table_data, (L + 1,))\n    chex.assert_shape(coords_rm, (dim, n_coords))\n    chex.assert_shape(params, (n_params, F))\n    chex.assert_shape(dL_dy_rm, (L*F, n_coords))\n    chex.assert_shape(dy_dcoords_rm, (dim*L*F, n_coords))\n\n    chex.assert_scalar(L)\n    chex.assert_scalar(F)\n    chex.assert_scalar(N_min)\n    chex.assert_scalar(per_level_scale)\n    chex.assert_type([L, F, N_min], int)\n    chex.assert_type(per_level_scale, float)\n\n    offset_dtype = jax.dtypes.canonicalize_dtype(offset_table_data.dtype)\n    if offset_dtype != jnp.uint32:\n        raise RuntimeError(\n            \"hashgrid encoding expects `offset_table_data` (a prefix sum of the hash table sizes of each level) to be of type uint32, got {}\".format(offset_dtype)\n        )\n\n    coord_dtype = jax.dtypes.canonicalize_dtype(coords_rm.dtype)\n    if coord_dtype != jnp.float32:\n        raise NotImplementedError(\n            \"hashgrid encoding is only implemented for input coordinates of type float32, got {}\".format(\n                coord_dtype\n            )\n        )\n\n    param_dtype = jax.dtypes.canonicalize_dtype(params.dtype)\n    if param_dtype != jnp.float32:\n        raise NotImplementedError(\n            \"hashgrid encoding is only implemented for parameters of type float32, got {}\".format(\n                param_dtype\n            )\n        )\n\n    out_shapes = {\n        \"dL_dparams\": (n_params, F),\n        \"dL_dcoords_rm\": (dim, n_coords),\n    }\n\n    return (\n        jax.ShapedArray(shape=out_shapes[\"dL_dparams\"], dtype=jnp.float32),\n        jax.ShapedArray(shape=out_shapes[\"dL_dcoords_rm\"], dtype=jnp.float32),\n    )", ""]}
{"filename": "deps/jax-tcnn/src/jaxtcnn/hashgrid_tcnn/__init__.py", "chunked_list": ["import jax\n\nfrom .impl import HashGridMetadata, __hashgrid_encode\n\n\ndef hashgrid_encode(\n    desc: HashGridMetadata,\n\n    offset_table_data: jax.Array,\n    coords_rm: jax.Array,\n    params: jax.Array,\n):\n    encoded_coords_rm, dy_dcoords_rm = __hashgrid_encode(\n        desc,\n        offset_table_data,\n        coords_rm,\n        params,\n    )\n    return encoded_coords_rm", ""]}
{"filename": "deps/volume-rendering-jax/setup.py", "chunked_list": ["#!/usr/bin/env python\n\nimport os\nimport subprocess\n\nfrom setuptools import Extension, find_packages, setup\nfrom setuptools.command.build_ext import build_ext\n\nHERE = os.path.dirname(os.path.realpath(__file__))\n", "HERE = os.path.dirname(os.path.realpath(__file__))\n\n\nclass CMakeBuildExt(build_ext):\n    def build_extensions(self):\n        # First: configure CMake build\n        import platform\n        import sys\n        import sysconfig\n\n        import pybind11\n\n        # Work out the relevant Python paths to pass to CMake, adapted from the\n        # PyTorch build system\n        if platform.system() == \"Windows\":\n            cmake_python_library = \"{}/libs/python{}.lib\".format(\n                sysconfig.get_config_var(\"prefix\"),\n                sysconfig.get_config_var(\"VERSION\"),\n            )\n            if not os.path.exists(cmake_python_library):\n                cmake_python_library = \"{}/libs/python{}.lib\".format(\n                    sys.base_prefix,\n                    sysconfig.get_config_var(\"VERSION\"),\n                )\n        else:\n            cmake_python_library = \"{}/{}\".format(\n                sysconfig.get_config_var(\"LIBDIR\"),\n                sysconfig.get_config_var(\"INSTSONAME\"),\n            )\n        cmake_python_include_dir = sysconfig.get_path(\"include\")\n\n        install_dir = os.path.abspath(\n            os.path.dirname(self.get_ext_fullpath(\"dummy\"))\n        )\n        os.makedirs(install_dir, exist_ok=True)\n        cmake_args = [\n            \"-DCMAKE_INSTALL_PREFIX={}\".format(install_dir),\n            \"-DPython_EXECUTABLE={}\".format(sys.executable),\n            \"-DPython_LIBRARIES={}\".format(cmake_python_library),\n            \"-DPython_INCLUDE_DIRS={}\".format(cmake_python_include_dir),\n            \"-DCMAKE_BUILD_TYPE={}\".format(\n                \"Debug\" if self.debug else \"Release\"\n            ),\n            \"-DCMAKE_PREFIX_PATH={}\".format(pybind11.get_cmake_dir()),\n            \"-G Ninja\",\n        ]\n        os.makedirs(self.build_temp, exist_ok=True)\n        subprocess.check_call(\n            [\"cmake\", HERE] + cmake_args, cwd=self.build_temp\n        )\n\n        # Build all the extensions\n        super().build_extensions()\n\n        # Finally run install\n        subprocess.check_call(\n            [\"cmake\", \"--build\", \".\", \"--target\", \"install\"],\n            cwd=self.build_temp,\n        )\n\n    def build_extension(self, ext):\n        target_name = ext.name.split(\".\")[-1]\n        subprocess.check_call(\n            [\"cmake\", \"--build\", \".\", \"--target\", target_name],\n            cwd=self.build_temp,\n        )", "\nextensions = [\n    Extension(\n        \"volrendjax.volrendutils_cuda\",  # Python dotted name, whose final component should be a buildable target defined in CMakeLists.txt\n        [  # source paths, relative to this setup.py file\n            \"lib/ffi.cc\",\n            \"lib/impl/packbits.cu\",\n            \"lib/impl/marching.cu\",\n            \"lib/impl/integrating.cu\",\n        ],", "            \"lib/impl/integrating.cu\",\n        ],\n    ),\n]\n\nsetup(\n    name=\"volume-rendering-jax\",\n    author=\"blurgyy\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),", "    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    include_package_data=True,\n    install_requires=[\"jax\", \"jaxlib\", \"chex\"],\n    ext_modules=extensions,\n    cmdclass={\"build_ext\": CMakeBuildExt},\n)\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/__init__.py", "chunked_list": ["from .packbits import packbits\nfrom .marching import march_rays, march_rays_inference\nfrom .morton3d import morton3d, morton3d_invert\nfrom .integrating import integrate_rays, integrate_rays_inference\n\n\n__all__ = [\n    \"integrate_rays\",\n    \"integrate_rays_inference\",\n    \"march_rays\",", "    \"integrate_rays_inference\",\n    \"march_rays\",\n    \"march_rays_inference\",\n    \"morton3d\",\n    \"morton3d_invert\",\n    \"packbits\",\n]\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/morton3d/impl.py", "chunked_list": ["import functools\n\nimport jax\nfrom jax.interpreters import mlir, xla\nfrom jax.lib import xla_client\n\nfrom . import abstract, lowering\nfrom .. import volrendutils_cuda\n\n", "\n\n# register GPU XLA custom calls\nfor name, value in volrendutils_cuda.get_morton3d_registrations().items():\n    xla_client.register_custom_call_target(name, value, platform=\"gpu\")\n\nmorton3d_p = jax.core.Primitive(\"morton3d\u26a1\")\nmorton3d_p.multiple_results = False\nmorton3d_p.def_impl(functools.partial(xla.apply_primitive, morton3d_p))\nmorton3d_p.def_abstract_eval(abstract.morton3d_abstract)", "morton3d_p.def_impl(functools.partial(xla.apply_primitive, morton3d_p))\nmorton3d_p.def_abstract_eval(abstract.morton3d_abstract)\n\nmorton3d_invert_p = jax.core.Primitive(\"morton3d\u26a1invert\")\nmorton3d_invert_p.multiple_results = False\nmorton3d_invert_p.def_impl(functools.partial(xla.apply_primitive, morton3d_invert_p))\nmorton3d_invert_p.def_abstract_eval(abstract.morton3d_invert_abstract)\n\n# register mlir lowering rules\nmlir.register_lowering(", "# register mlir lowering rules\nmlir.register_lowering(\n    prim=morton3d_p,\n    rule=lowering.morton3d_lowering_rule,\n    platform=\"gpu\",\n)\nmlir.register_lowering(\n    prim=morton3d_invert_p,\n    rule=lowering.morton3d_invert_lowering_rule,\n    platform=\"gpu\",", "    rule=lowering.morton3d_invert_lowering_rule,\n    platform=\"gpu\",\n)\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/morton3d/lowering.py", "chunked_list": ["from jax.interpreters import mlir\nfrom jax.interpreters.mlir import ir\n\nfrom .. import volrendutils_cuda\n\ntry:\n    from jaxlib.mhlo_helpers import custom_call\nexcept ModuleNotFoundError:\n    # A more recent jaxlib would have `hlo_helpers` instead of `mhlo_helpers`\n    # <https://github.com/google/jax/commit/b8ae8e3fa10f9abe998459fac1513915acee776d#diff-50658d597212b4ce070b8bd8c1fc522deeee1845ba387a0a5b507c446e8ea12a>\n    from jaxlib.hlo_helpers import custom_call", "\n\n# helper function for mapping given shapes to their default mlir layouts\ndef default_layouts(*shapes):\n    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\n\ndef morton3d_lowering_rule(\n    ctx: mlir.LoweringRule,\n\n    # input array\n    xyzs: ir.Value,\n):\n    length, _ = ir.RankedTensorType(xyzs.type).shape\n\n    opaque = volrendutils_cuda.make_morton3d_descriptor(length)\n\n    shapes = {\n        \"in.xyzs\": (length, 3),\n\n        \"out.idcs\": (length,),\n    }\n\n    return [custom_call(\n        call_target_name=\"morton3d\",\n        out_types=[\n            ir.RankedTensorType.get(shapes[\"out.idcs\"], ir.IntegerType.get_unsigned(32)),\n        ],\n        operands=[\n            xyzs,\n        ],\n        backend_config=opaque,\n        operand_layouts=default_layouts(\n            shapes[\"in.xyzs\"],\n        ),\n        result_layouts=default_layouts(\n            shapes[\"out.idcs\"],\n        ),\n    )]", "\n\ndef morton3d_invert_lowering_rule(\n    ctx: mlir.LoweringRule,\n\n    # input array\n    idcs: ir.Value,\n):\n    length, = ir.RankedTensorType(idcs.type).shape\n\n    opaque = volrendutils_cuda.make_morton3d_descriptor(length)\n\n    shapes = {\n        \"in.idcs\": (length,),\n\n        \"out.xyzs\": (length, 3),\n    }\n\n    return [custom_call(\n        call_target_name=\"morton3d_invert\",\n        out_types=[\n            ir.RankedTensorType.get(shapes[\"out.xyzs\"], ir.IntegerType.get_unsigned(32)),\n        ],\n        operands=[\n            idcs,\n        ],\n        backend_config=opaque,\n        operand_layouts=default_layouts(\n            shapes[\"in.idcs\"],\n        ),\n        result_layouts=default_layouts(\n            shapes[\"out.xyzs\"],\n        ),\n    )]", ""]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/morton3d/abstract.py", "chunked_list": ["import jax\nimport jax.numpy as jnp\n\n\n# jit rules\ndef morton3d_abstract(\n    # input array\n    xyzs: jax.ShapedArray,\n):\n    length, _ = xyzs.shape\n\n    dtype = jax.dtypes.canonicalize_dtype(xyzs.dtype)\n    if dtype != jnp.uint32:\n        raise NotImplementedError(\n            \"morton3d is only implemented for input coordinates of type `jnp.uint32`, got {}\".format(\n                dtype,\n            )\n        )\n\n    out_shapes = {\n        \"idcs\": (length,),\n    }\n\n    return jax.ShapedArray(shape=out_shapes[\"idcs\"], dtype=jnp.uint32)", "\n\ndef morton3d_invert_abstract(\n    # input array\n    idcs: jax.ShapedArray,\n):\n    length, = idcs.shape\n\n    dtype = jax.dtypes.canonicalize_dtype(idcs.dtype)\n    if dtype != jnp.uint32:\n        raise NotImplementedError(\n            \"morton3d_invert is only implemented for input indices of type `jnp.uint32`, got {}\".format(\n                dtype,\n            )\n        )\n\n    out_shapes = {\n        \"xyzs\": (length, 3),\n    }\n\n    return jax.ShapedArray(shape=out_shapes[\"xyzs\"], dtype=jnp.uint32)", ""]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/morton3d/__init__.py", "chunked_list": ["import jax\n\nfrom . import impl\n\n\ndef morton3d(xyzs: jax.Array):\n    return impl.morton3d_p.bind(xyzs)\n\ndef morton3d_invert(idcs: jax.Array):\n    return impl.morton3d_invert_p.bind(idcs)", "def morton3d_invert(idcs: jax.Array):\n    return impl.morton3d_invert_p.bind(idcs)\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/integrating/impl.py", "chunked_list": ["import functools\nfrom typing import Tuple\n\nimport jax\nfrom jax.interpreters import mlir, xla\nfrom jax.lib import xla_client\n\nfrom . import abstract, lowering\nfrom .. import volrendutils_cuda\n", "from .. import volrendutils_cuda\n\n\n# register GPU XLA custom calls\nfor name, value in volrendutils_cuda.get_integrating_registrations().items():\n    xla_client.register_custom_call_target(name, value, platform=\"gpu\")\n\n\n# primitives\nintegrate_rays_p = jax.core.Primitive(\"integrate_rays\ud83c\udfa8\")", "# primitives\nintegrate_rays_p = jax.core.Primitive(\"integrate_rays\ud83c\udfa8\")\nintegrate_rays_p.multiple_results = True\nintegrate_rays_p.def_impl(functools.partial(xla.apply_primitive, integrate_rays_p))\nintegrate_rays_p.def_abstract_eval(abstract.integrate_rays_abstract)\n\nintegrate_rays_bwd_p = jax.core.Primitive(\"integrate_rays\ud83c\udfa8backward\")\nintegrate_rays_bwd_p.multiple_results = True\nintegrate_rays_bwd_p.def_impl(functools.partial(xla.apply_primitive, integrate_rays_bwd_p))\nintegrate_rays_bwd_p.def_abstract_eval(abstract.integrate_rays_backward_abstract)", "integrate_rays_bwd_p.def_impl(functools.partial(xla.apply_primitive, integrate_rays_bwd_p))\nintegrate_rays_bwd_p.def_abstract_eval(abstract.integrate_rays_backward_abstract)\n\nintegrate_rays_inference_p = jax.core.Primitive(\"integrate_rays\ud83c\udfa8inference\")\nintegrate_rays_inference_p.multiple_results = True\nintegrate_rays_inference_p.def_impl(functools.partial(xla.apply_primitive, integrate_rays_inference_p))\nintegrate_rays_inference_p.def_abstract_eval(abstract.integrate_rays_inference_abstract)\n\n# register mlir lowering rules\nmlir.register_lowering(", "# register mlir lowering rules\nmlir.register_lowering(\n    prim=integrate_rays_p,\n    rule=lowering.integrate_rays_lowering_rule,\n    platform=\"gpu\",\n)\nmlir.register_lowering(\n    prim=integrate_rays_bwd_p,\n    rule=lowering.integrate_rays_backward_lowring_rule,\n    platform=\"gpu\",", "    rule=lowering.integrate_rays_backward_lowring_rule,\n    platform=\"gpu\",\n)\nmlir.register_lowering(\n    prim=integrate_rays_inference_p,\n    rule=lowering.integrate_rays_inference_lowering_rule,\n    platform=\"gpu\",\n)\n\n@jax.custom_vjp\ndef __integrate_rays(\n    near_distance: float,\n    rays_sample_startidx: jax.Array,\n    rays_n_samples: jax.Array,\n    bgs: jax.Array,\n    dss: jax.Array,\n    z_vals: jax.Array,\n    drgbs: jax.Array,\n) -> Tuple[jax.Array, jax.Array, jax.Array, jax.Array]:\n    bgs = jax.numpy.broadcast_to(bgs, (rays_sample_startidx.shape[0], 3))\n\n    measured_batch_size, final_rgbds, final_opacities = integrate_rays_p.bind(\n        rays_sample_startidx,\n        rays_n_samples,\n        bgs,\n        dss,\n        z_vals,\n        drgbs,\n    )\n\n    return measured_batch_size, final_rgbds, final_opacities", "\n@jax.custom_vjp\ndef __integrate_rays(\n    near_distance: float,\n    rays_sample_startidx: jax.Array,\n    rays_n_samples: jax.Array,\n    bgs: jax.Array,\n    dss: jax.Array,\n    z_vals: jax.Array,\n    drgbs: jax.Array,\n) -> Tuple[jax.Array, jax.Array, jax.Array, jax.Array]:\n    bgs = jax.numpy.broadcast_to(bgs, (rays_sample_startidx.shape[0], 3))\n\n    measured_batch_size, final_rgbds, final_opacities = integrate_rays_p.bind(\n        rays_sample_startidx,\n        rays_n_samples,\n        bgs,\n        dss,\n        z_vals,\n        drgbs,\n    )\n\n    return measured_batch_size, final_rgbds, final_opacities", "\ndef __fwd_integrate_rays(\n    near_distance: float,\n    rays_sample_startidx: jax.Array,\n    rays_n_samples: jax.Array,\n    bgs: jax.Array,\n    dss: jax.Array,\n    z_vals: jax.Array,\n    drgbs: jax.Array,\n):\n    bgs = jax.numpy.broadcast_to(bgs, (rays_sample_startidx.shape[0], 3))\n\n    primal_outputs = __integrate_rays(\n        near_distance=near_distance,\n        rays_sample_startidx=rays_sample_startidx,\n        rays_n_samples=rays_n_samples,\n        bgs=bgs,\n        dss=dss,\n        z_vals=z_vals,\n        drgbs=drgbs,\n    )\n    measured_batch_size, final_rgbds, final_opacities = primal_outputs\n    aux = {\n        \"in.near_distance\": near_distance,\n        \"in.rays_sample_startidx\": rays_sample_startidx,\n        \"in.rays_n_samples\": rays_n_samples,\n        \"in.bgs\": bgs,\n        \"in.dss\": dss,\n        \"in.z_vals\": z_vals,\n        \"in.drgbs\": drgbs,\n\n        \"out.measured_batch_size\": measured_batch_size,\n        \"out.final_rgbds\": final_rgbds,\n        \"out.final_opacities\": final_opacities,\n    }\n    return primal_outputs, aux", "\ndef __bwd_integrate_rays(aux, grads):\n    _, dL_dfinal_rgbds, dL_dfinal_opacities = grads  # dL_dfinal_rgbds should be zeros everywhere\n    dL_dbgs, dL_dz_vals, dL_ddrgbs = integrate_rays_bwd_p.bind(\n        aux[\"in.rays_sample_startidx\"],\n        aux[\"in.rays_n_samples\"],\n        aux[\"in.bgs\"],\n        aux[\"in.dss\"],\n        aux[\"in.z_vals\"],\n        aux[\"in.drgbs\"],\n\n        aux[\"out.final_rgbds\"],\n        aux[\"out.final_opacities\"],\n\n        dL_dfinal_rgbds,\n\n        near_distance=aux[\"in.near_distance\"],\n    )\n    return (\n        # The first primal input is `near_distance`, a static argument, return no gradient for it.\n        None,\n        # The next 2 primal inputs are integer-valued arrays (`rays_sample_startidx`,\n        # `rays_n_samples`), return no gradient for them.\n        # REF:\n        #   <https://jax.readthedocs.io/en/latest/jep/4008-custom-vjp-update.html#what-to-update>:\n        #   Wherever we used to use nondiff_argnums for array values, we should just pass those as\n        #   regular arguments.  In the bwd rule, we need to produce values for them, but we can just\n        #   produce `None` values to indicate there\u2019s no corresponding gradient value.\n        None, None,\n        # 4-th primal input is `dss`, no gradient\n        None,\n        # gradients for background colors, z_vals and model predictions (densites and rgbs)\n        dL_dbgs, dL_dz_vals, dL_ddrgbs\n    )", "\n__integrate_rays.defvjp(\n    fwd=__fwd_integrate_rays,\n    bwd=__bwd_integrate_rays,\n)\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/integrating/lowering.py", "chunked_list": ["from jax.interpreters import mlir\nfrom jax.interpreters.mlir import ir\n\nfrom .. import volrendutils_cuda\n\ntry:\n    from jaxlib.mhlo_helpers import custom_call\nexcept ModuleNotFoundError:\n    # A more recent jaxlib would have `hlo_helpers` instead of `mhlo_helpers`\n    # <https://github.com/google/jax/commit/b8ae8e3fa10f9abe998459fac1513915acee776d#diff-50658d597212b4ce070b8bd8c1fc522deeee1845ba387a0a5b507c446e8ea12a>\n    from jaxlib.hlo_helpers import custom_call", "\n\n# helper function for mapping given shapes to their default mlir layouts\ndef default_layouts(*shapes):\n    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\n\ndef integrate_rays_lowering_rule(\n    ctx: mlir.LoweringRuleContext,\n\n    rays_sample_startidx: ir.Value,\n    rays_n_samples: ir.Value,\n\n    bgs: ir.Value,\n    dss: ir.Value,\n    z_vals: ir.Value,\n    drgbs: ir.Value,\n):\n    n_rays, = ir.RankedTensorType(rays_sample_startidx.type).shape\n    total_samples, = ir.RankedTensorType(z_vals.type).shape\n\n    opaque = volrendutils_cuda.make_integrating_descriptor(n_rays, total_samples)\n\n    shapes = {\n        \"in.rays_sample_startidx\": (n_rays,),\n        \"in.rays_n_samples\": (n_rays,),\n\n        \"in.bgs\": (n_rays, 3),\n        \"in.dss\": (total_samples,),\n        \"in.z_vals\": (total_samples,),\n        \"in.drgbs\": (total_samples, 4),\n\n        \"helper.measured_batch_size\": (1,),\n\n        \"out.final_rgbds\": (n_rays, 4),\n        \"out.final_opacities\": (n_rays,),\n    }\n\n    return custom_call(\n        call_target_name=\"integrate_rays\",\n        out_types=[\n            ir.RankedTensorType.get(shapes[\"helper.measured_batch_size\"], ir.IntegerType.get_unsigned(32)),\n            ir.RankedTensorType.get(shapes[\"out.final_rgbds\"], ir.F32Type.get()),\n            ir.RankedTensorType.get(shapes[\"out.final_opacities\"], ir.F32Type.get()),\n        ],\n        operands=[\n            rays_sample_startidx,\n            rays_n_samples,\n            bgs,\n            dss,\n            z_vals,\n            drgbs,\n        ],\n        backend_config=opaque,\n        operand_layouts=default_layouts(\n            shapes[\"in.rays_sample_startidx\"],\n            shapes[\"in.rays_n_samples\"],\n            shapes[\"in.bgs\"],\n            shapes[\"in.dss\"],\n            shapes[\"in.z_vals\"],\n            shapes[\"in.drgbs\"],\n        ),\n        result_layouts=default_layouts(\n            shapes[\"helper.measured_batch_size\"],\n            shapes[\"out.final_rgbds\"],\n            shapes[\"out.final_opacities\"],\n        ),\n    )", "\n\ndef integrate_rays_backward_lowring_rule(\n    ctx: mlir.LoweringRuleContext,\n\n    rays_sample_startidx: ir.Value,\n    rays_n_samples: ir.Value,\n\n    # original inputs\n    bgs: ir.Value,\n    dss: ir.Value,\n    z_vals: ir.Value,\n    drgbs: ir.Value,\n\n    # original outputs\n    final_rgbds: ir.Value,\n    final_opacities: ir.Value,\n\n    # gradient inputs\n    dL_dfinal_rgbds: ir.Value,\n\n    # static argument\n    near_distance: float,\n):\n    n_rays, = ir.RankedTensorType(rays_sample_startidx.type).shape\n    total_samples, = ir.RankedTensorType(z_vals.type).shape\n\n    opaque = volrendutils_cuda.make_integrating_backward_descriptor(n_rays, total_samples, near_distance)\n\n    shapes = {\n        \"in.rays_sample_startidx\": (n_rays,),\n        \"in.rays_n_samples\": (n_rays,),\n\n        \"in.bgs\": (n_rays, 3),\n        \"in.dss\": (total_samples,),\n        \"in.z_vals\": (total_samples,),\n        \"in.drgbs\": (total_samples, 4),\n\n        \"in.final_rgbds\": (n_rays, 4),\n        \"in.final_opacities\": (n_rays,),\n\n        \"in.dL_dfinal_rgbds\": (n_rays, 4),\n\n        \"out.dL_dbgs\": (n_rays, 3),\n        \"out.dL_dz_vals\": (total_samples,),\n        \"out.dL_ddrgbs\": (total_samples, 4),\n    }\n\n    return custom_call(\n        call_target_name=\"integrate_rays_backward\",\n        out_types=[\n            ir.RankedTensorType.get(shapes[\"out.dL_dbgs\"], ir.F32Type.get()),\n            ir.RankedTensorType.get(shapes[\"out.dL_dz_vals\"], ir.F32Type.get()),\n            ir.RankedTensorType.get(shapes[\"out.dL_ddrgbs\"], ir.F32Type.get()),\n        ],\n        operands=[\n            rays_sample_startidx,\n            rays_n_samples,\n\n            bgs,\n            dss,\n            z_vals,\n            drgbs,\n\n            final_rgbds,\n            final_opacities,\n\n            dL_dfinal_rgbds,\n        ],\n        backend_config=opaque,\n        operand_layouts=default_layouts(\n            shapes[\"in.rays_sample_startidx\"],\n            shapes[\"in.rays_n_samples\"],\n            shapes[\"in.bgs\"],\n            shapes[\"in.dss\"],\n            shapes[\"in.z_vals\"],\n            shapes[\"in.drgbs\"],\n\n            shapes[\"in.final_rgbds\"],\n            shapes[\"in.final_opacities\"],\n\n            shapes[\"in.dL_dfinal_rgbds\"],\n        ),\n        result_layouts=default_layouts(\n            shapes[\"out.dL_dbgs\"],\n            shapes[\"out.dL_dz_vals\"],\n            shapes[\"out.dL_ddrgbs\"],\n        ),\n    )", "\n\ndef integrate_rays_inference_lowering_rule(\n    ctx: mlir.LoweringRuleContext,\n\n    rays_bg: ir.Value,\n    rays_rgbd: ir.Value,\n    rays_T: ir.Value,\n\n    n_samples: ir.Value,\n    indices: ir.Value,\n    dss: ir.Value,\n    z_vals: ir.Value,\n    drgbs: ir.Value,\n):\n    (n_total_rays, _) = ir.RankedTensorType(rays_rgbd.type).shape\n    (n_rays, march_steps_cap) = ir.RankedTensorType(dss.type).shape\n\n    opaque = volrendutils_cuda.make_integrating_inference_descriptor(n_total_rays, n_rays, march_steps_cap)\n\n    shapes = {\n        \"in.rays_bg\": (n_total_rays, 3),\n        \"in.rays_rgbd\": (n_total_rays, 4),\n        \"in.rays_T\": (n_total_rays,),\n\n        \"in.n_samples\": (n_rays,),\n        \"in.indices\": (n_rays,),\n        \"in.dss\": (n_rays, march_steps_cap),\n        \"in.z_vals\": (n_rays, march_steps_cap),\n        \"in.drgbs\": (n_rays, march_steps_cap, 4),\n\n        \"out.terminate_cnt\": (1,),\n        \"out.terminated\": (n_rays,),\n        \"out.rays_rgbd\": (n_rays, 4),\n        \"out.rays_T\": (n_rays,),\n    }\n\n    return custom_call(\n        call_target_name=\"integrate_rays_inference\",\n        out_types=[\n            ir.RankedTensorType.get(shapes[\"out.terminate_cnt\"], ir.IntegerType.get_unsigned(32)),\n            ir.RankedTensorType.get(shapes[\"out.terminated\"], ir.IntegerType.get_signless(1)),\n            ir.RankedTensorType.get(shapes[\"out.rays_rgbd\"], ir.F32Type.get()),\n            ir.RankedTensorType.get(shapes[\"out.rays_T\"], ir.F32Type.get()),\n        ],\n        operands=[\n            rays_bg,\n            rays_rgbd,\n            rays_T,\n\n            n_samples,\n            indices,\n            dss,\n            z_vals,\n            drgbs,\n        ],\n        backend_config=opaque,\n        operand_layouts=default_layouts(\n            shapes[\"in.rays_bg\"],\n            shapes[\"in.rays_rgbd\"],\n            shapes[\"in.rays_T\"],\n\n            shapes[\"in.n_samples\"],\n            shapes[\"in.indices\"],\n            shapes[\"in.dss\"],\n            shapes[\"in.z_vals\"],\n            shapes[\"in.drgbs\"],\n        ),\n        result_layouts=default_layouts(\n            shapes[\"out.terminate_cnt\"],\n            shapes[\"out.terminated\"],\n            shapes[\"out.rays_rgbd\"],\n            shapes[\"out.rays_T\"],\n        ),\n    )", ""]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/integrating/abstract.py", "chunked_list": ["import chex\nimport jax\nimport jax.numpy as jnp\n\n\n# jit rules\ndef integrate_rays_abstract(\n    rays_sample_startidx: jax.Array,\n    rays_n_samples: jax.Array,\n\n    bgs: jax.Array,\n    dss: jax.Array,\n    z_vals: jax.Array,\n    drgbs: jax.Array,\n):\n    (n_rays,), (total_samples,) = rays_sample_startidx.shape, dss.shape\n\n    chex.assert_shape([rays_sample_startidx, rays_n_samples], (n_rays,))\n    chex.assert_shape(bgs, (n_rays, 3))\n    chex.assert_shape(z_vals, (total_samples,))\n    chex.assert_shape(drgbs, (total_samples, 4))\n\n    dtype = jax.dtypes.canonicalize_dtype(drgbs.dtype)\n    if dtype != jnp.float32:\n        raise NotImplementedError(\n            \"integrate_rays is only implemented for input prediction (density, color) of `jnp.float32` type, got {}\".format(\n                dtype,\n            )\n        )\n\n    shapes = {\n        \"helper.measured_batch_size\": (1,),\n\n        \"out.final_rgbds\": (n_rays, 4),\n        \"out.final_opacities\": (n_rays,),\n    }\n\n    return (\n        jax.ShapedArray(shape=shapes[\"helper.measured_batch_size\"], dtype=jnp.uint32),\n\n        jax.ShapedArray(shape=shapes[\"out.final_rgbds\"], dtype=jnp.float32),\n        jax.ShapedArray(shape=shapes[\"out.final_opacities\"], dtype=jnp.float32),\n    )", "\ndef integrate_rays_backward_abstract(\n    rays_sample_startidx: jax.Array,\n    rays_n_samples: jax.Array,\n\n    # original inputs\n    bgs: jax.Array,\n    dss: jax.Array,\n    z_vals: jax.Array,\n    drgbs: jax.Array,\n\n    # original outputs\n    final_rgbds: jax.Array,\n    final_opacities: jax.Array,\n\n    # gradient inputs\n    dL_dfinal_rgbds: jax.Array,\n\n    # static argument\n    near_distance: float,\n):\n    (n_rays,), (total_samples,) = rays_sample_startidx.shape, dss.shape\n\n    chex.assert_shape([rays_sample_startidx, rays_n_samples, final_opacities], (n_rays,))\n    chex.assert_shape(bgs, (n_rays, 3))\n    chex.assert_shape(z_vals, (total_samples,))\n    chex.assert_shape(drgbs, (total_samples, 4))\n    chex.assert_shape([final_rgbds, dL_dfinal_rgbds], (n_rays, 4))\n\n    chex.assert_scalar_non_negative(near_distance)\n\n    dtype = jax.dtypes.canonicalize_dtype(drgbs.dtype)\n    if dtype != jnp.float32:\n        raise NotImplementedError(\n            \"integrate_rays is only implemented for input color of `jnp.float32` type, got {}\".format(\n                dtype,\n            )\n        )\n\n    out_shapes = {\n        \"dL_dbgs\": (n_rays, 3),\n        \"dL_dz_vals\": (total_samples,),\n        \"dL_ddrgbs\": (total_samples, 4),\n    }\n\n    return (\n        jax.ShapedArray(shape=out_shapes[\"dL_dbgs\"], dtype=jnp.float32),\n        jax.ShapedArray(shape=out_shapes[\"dL_dz_vals\"], dtype=jnp.float32),\n        jax.ShapedArray(shape=out_shapes[\"dL_ddrgbs\"], dtype=jnp.float32),\n    )", "\n\ndef integrate_rays_inference_abstract(\n    rays_bg: jax.ShapedArray,\n    rays_rgbd: jax.ShapedArray,\n    rays_T: jax.ShapedArray,\n\n    n_samples: jax.ShapedArray,\n    indices: jax.ShapedArray,\n    dss: jax.ShapedArray,\n    z_vals: jax.ShapedArray,\n    drgbs: jax.ShapedArray,\n):\n    (n_total_rays, _), (n_rays, march_steps_cap) = rays_rgbd.shape, dss.shape\n\n    chex.assert_shape(rays_bg, (n_total_rays, 3))\n    chex.assert_shape(rays_rgbd, (n_total_rays, 4))\n    chex.assert_shape(rays_T, (n_total_rays,))\n    chex.assert_shape([n_samples, indices], (n_rays,))\n    chex.assert_shape([dss, z_vals], (n_rays, march_steps_cap))\n    chex.assert_shape(drgbs, (n_rays, march_steps_cap, 4))\n\n    out_shapes = {\n        \"terminate_cnt\": (1,),\n        \"terminated\": (n_rays,),\n        \"rays_rgbd\": (n_rays, 4),\n        \"rays_T\": (n_rays,),\n    }\n\n    return (\n        jax.ShapedArray(shape=out_shapes[\"terminate_cnt\"], dtype=jnp.uint32),\n        jax.ShapedArray(shape=out_shapes[\"terminated\"], dtype=jnp.bool_),\n        jax.ShapedArray(shape=out_shapes[\"rays_rgbd\"], dtype=jnp.float32),\n        jax.ShapedArray(shape=out_shapes[\"rays_T\"], dtype=jnp.float32),\n    )", ""]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/integrating/__init__.py", "chunked_list": ["from typing import Tuple\n\nimport jax\n\nfrom . import impl\n\n\n# this function is a wrapper on top of `__integrate_rays` which has custom vjp (wrapping the\n# `__integrate_rays` function because the @jax.custom_vjp decorator makes the decorated function's\n# docstring invisible to LSPs).\ndef integrate_rays(\n    near_distance: float,\n    rays_sample_startidx: jax.Array,\n    rays_n_samples: jax.Array,\n    bgs: jax.Array,\n    dss: jax.Array,\n    z_vals: jax.Array,\n    drgbs: jax.Array,\n) -> Tuple[jax.Array, jax.Array, jax.Array, jax.Array]:\n    \"\"\"\n    Inputs:\n        near_distance `float`: camera's near distance, samples behind the camera's near plane with\n                               non-negligible introduce a penalty on their densities\\n\"\n\n        rays_sample_startidx `[n_rays]`: i-th element is the index of the first sample in z_vals,\n                                         densities, and rgbs of the i-th ray\n        rays_n_samples `[n_rays]`: i-th element is the number of samples for the i-th ray\n\n        bgs `[n_rays, 3]`: background colors of each ray\n        dss [total_samples]: it means `ds`s, the notation `ds` comes from the article \"Local and\n                             global illumination in the volume rendering integral\" written by Nelson\n                             Max and Min Chen, 2005.  The product of `ds[i]` and `densities[i]`\n                             represents the probability of the ray terminates anywhere between\n                             `z_vals[i]` and `z_vals[i]+ds[i]`.\n                             Note that `ds[i]` is _not_ the same as `z_vals[i+1]-z_vals[i]` (though\n                             they may equal), because: (1) if empty spaces are skipped during ray\n                             marching, `z_vals[i+1]-z_vals[i]` may be very large, in which case it's\n                             no longer appropriate to assume the density is constant along this\n                             large segment; (2) `z_vals[i+1]` is not defined for the last sample.\n        z_vals [total_samples]: z_vals[i] is the distance of the i-th sample from the camera\n        drgbs [total_samples, 4]: density (1) and rgb (3) values along a ray\n\n    Returns:\n        measured_batch_size `uint`: total number of samples that got composited into output\n        final_rgbds `[n_rays, 4]`: integrated ray colors and estimated depths according to input\n                                   densities and rgbs.\n        final_opacities `[n_rays]`: accumulated opacities along each ray\n    \"\"\"\n    measured_batch_size, final_rgbds, final_opacities = impl.__integrate_rays(\n        near_distance=near_distance,\n        rays_sample_startidx=rays_sample_startidx,\n        rays_n_samples=rays_n_samples,\n        bgs=bgs,\n        dss=dss,\n        z_vals=z_vals,\n        drgbs=drgbs,\n    )\n\n    return measured_batch_size[0], final_rgbds, final_opacities", "# `__integrate_rays` function because the @jax.custom_vjp decorator makes the decorated function's\n# docstring invisible to LSPs).\ndef integrate_rays(\n    near_distance: float,\n    rays_sample_startidx: jax.Array,\n    rays_n_samples: jax.Array,\n    bgs: jax.Array,\n    dss: jax.Array,\n    z_vals: jax.Array,\n    drgbs: jax.Array,\n) -> Tuple[jax.Array, jax.Array, jax.Array, jax.Array]:\n    \"\"\"\n    Inputs:\n        near_distance `float`: camera's near distance, samples behind the camera's near plane with\n                               non-negligible introduce a penalty on their densities\\n\"\n\n        rays_sample_startidx `[n_rays]`: i-th element is the index of the first sample in z_vals,\n                                         densities, and rgbs of the i-th ray\n        rays_n_samples `[n_rays]`: i-th element is the number of samples for the i-th ray\n\n        bgs `[n_rays, 3]`: background colors of each ray\n        dss [total_samples]: it means `ds`s, the notation `ds` comes from the article \"Local and\n                             global illumination in the volume rendering integral\" written by Nelson\n                             Max and Min Chen, 2005.  The product of `ds[i]` and `densities[i]`\n                             represents the probability of the ray terminates anywhere between\n                             `z_vals[i]` and `z_vals[i]+ds[i]`.\n                             Note that `ds[i]` is _not_ the same as `z_vals[i+1]-z_vals[i]` (though\n                             they may equal), because: (1) if empty spaces are skipped during ray\n                             marching, `z_vals[i+1]-z_vals[i]` may be very large, in which case it's\n                             no longer appropriate to assume the density is constant along this\n                             large segment; (2) `z_vals[i+1]` is not defined for the last sample.\n        z_vals [total_samples]: z_vals[i] is the distance of the i-th sample from the camera\n        drgbs [total_samples, 4]: density (1) and rgb (3) values along a ray\n\n    Returns:\n        measured_batch_size `uint`: total number of samples that got composited into output\n        final_rgbds `[n_rays, 4]`: integrated ray colors and estimated depths according to input\n                                   densities and rgbs.\n        final_opacities `[n_rays]`: accumulated opacities along each ray\n    \"\"\"\n    measured_batch_size, final_rgbds, final_opacities = impl.__integrate_rays(\n        near_distance=near_distance,\n        rays_sample_startidx=rays_sample_startidx,\n        rays_n_samples=rays_n_samples,\n        bgs=bgs,\n        dss=dss,\n        z_vals=z_vals,\n        drgbs=drgbs,\n    )\n\n    return measured_batch_size[0], final_rgbds, final_opacities", "\n\ndef integrate_rays_inference(\n    rays_bg: jax.Array,\n    rays_rgbd: jax.Array,\n    rays_T: jax.Array,\n\n    n_samples: jax.Array,\n    indices: jax.Array,\n    dss: jax.Array,\n    z_vals: jax.Array,\n    drgbs: jax.Array,\n):\n    \"\"\"\n    Inputs:\n        rays_bg `float` `[n_total_rays, 3]`: normalized background color of each ray in question\n        rays_rgbd `float` `[n_total_rays, 4]`: target array to write rendered colors and estimated\n                                               depths to\n        rays_T `float` `[n_total_rays]`: accumulated transmittance of each ray\n\n        n_samples `uint32` `[n_rays]`: output of ray marching, specifies how many samples are\n                                        generated for this ray at this iteration\n        indices `uint32` `[n_rays]`: values are in range [0, n_total_rays), specifies the location\n                                     in `rays_bg`, `rays_rgbd`, `rays_T`, and `rays_depth`\n                                     corresponding to this ray\n        dss `float` `[n_rays, march_steps_cap]`: each sample's `ds`\n        z_vals `float` `[n_rays, march_steps_cap]`: each sample's distance to its ray origin\n        drgbs `float` `[n_rays, march_steps_cap, 4]`: predicted density (1) and RGB (3) values from a NeRF model\n\n    Returns:\n        terminate_cnt `uint32`: number of rays that terminated this iteration\n        terminated `bool` `[n_rays]`: a binary mask, the i-th location being True means the i-th ray\n                                       has terminated\n        rays_rgbd `float` `[n_total_rays, 3]`: the input `rays_rgbd` with ray colors and estimated\n                                               depths updated\n        rays_T `float` `[n_total_rays]`: the input `rays_T` with transmittance values updated\n    \"\"\"\n    terminate_cnt, terminated, rays_rgbd_out, rays_T_out = impl.integrate_rays_inference_p.bind(\n        rays_bg,\n        rays_rgbd,\n        rays_T,\n\n        n_samples,\n        indices,\n        dss,\n        z_vals,\n        drgbs,\n    )\n    rays_rgbd = rays_rgbd.at[indices].set(rays_rgbd_out)\n    rays_T = rays_T.at[indices].set(rays_T_out)\n    return terminate_cnt[0], terminated, rays_rgbd, rays_T", ""]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/packbits/impl.py", "chunked_list": ["import functools\n\nimport jax\nfrom jax.interpreters import mlir, xla\nfrom jax.lib import xla_client\n\nfrom . import abstract, lowering\nfrom .. import volrendutils_cuda\n\n", "\n\n# register GPU XLA custom calls\nfor name, value in volrendutils_cuda.get_packbits_registrations().items():\n    xla_client.register_custom_call_target(name, value, platform=\"gpu\")\n\npackbits_p = jax.core.Primitive(\"packbits\ud83c\udfb1\")\npackbits_p.multiple_results = True\npackbits_p.def_impl(functools.partial(xla.apply_primitive, packbits_p))\npackbits_p.def_abstract_eval(abstract.pack_density_into_bits_abstract)", "packbits_p.def_impl(functools.partial(xla.apply_primitive, packbits_p))\npackbits_p.def_abstract_eval(abstract.pack_density_into_bits_abstract)\n\nmlir.register_lowering(\n    prim=packbits_p,\n    rule=lowering.packbits_lowering_rule,\n    platform=\"gpu\",\n)\n", ""]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/packbits/lowering.py", "chunked_list": ["from jax.interpreters import mlir\nfrom jax.interpreters.mlir import ir\n\nfrom .. import volrendutils_cuda\n\ntry:\n    from jaxlib.mhlo_helpers import custom_call\nexcept ModuleNotFoundError:\n    # A more recent jaxlib would have `hlo_helpers` instead of `mhlo_helpers`\n    # <https://github.com/google/jax/commit/b8ae8e3fa10f9abe998459fac1513915acee776d#diff-50658d597212b4ce070b8bd8c1fc522deeee1845ba387a0a5b507c446e8ea12a>\n    from jaxlib.hlo_helpers import custom_call", "\n\n# helper function for mapping given shapes to their default mlir layouts\ndef default_layouts(*shapes):\n    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\n\ndef packbits_lowering_rule(\n    ctx: mlir.LoweringRule,\n\n    # input array\n    density_threshold: ir.Value,\n    density_grid: ir.Value,\n):\n    n_bits = ir.RankedTensorType(density_grid.type).shape[0]\n    n_bytes = n_bits // 8\n\n    opaque = volrendutils_cuda.make_packbits_descriptor(n_bytes)\n\n    shapes = {\n        \"in.density_threshold\": (n_bits,),\n        \"in.density_grid\": (n_bits,),\n\n        \"out.occupied_mask\": (n_bits,),\n        \"out.occupancy_bitfield\": (n_bytes,),\n    }\n\n    return custom_call(\n        call_target_name=\"pack_density_into_bits\",\n        out_types = [\n            ir.RankedTensorType.get(shapes[\"out.occupied_mask\"], ir.IntegerType.get_signless(1)),\n            ir.RankedTensorType.get(shapes[\"out.occupancy_bitfield\"], ir.IntegerType.get_unsigned(8)),\n        ],\n        operands=[\n            density_threshold,\n            density_grid,\n        ],\n        backend_config=opaque,\n        operand_layouts=default_layouts(\n            shapes[\"in.density_threshold\"],\n            shapes[\"in.density_grid\"],\n        ),\n        result_layouts=default_layouts(\n            shapes[\"out.occupied_mask\"],\n            shapes[\"out.occupancy_bitfield\"],\n        ),\n    )", ""]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/packbits/abstract.py", "chunked_list": ["import chex\nimport jax\nimport jax.numpy as jnp\n\n\n# jit rules\ndef pack_density_into_bits_abstract(\n    # input array\n    density_threshold: jax.ShapedArray,\n    density_grid: jax.ShapedArray,\n):\n    chex.assert_rank([density_threshold, density_grid], 1)\n    chex.assert_shape(density_threshold, density_grid.shape)\n    n_bits = density_grid.shape[0]\n    if n_bits % 8 != 0:\n        raise ValueError(\n            \"pack_density_into_bits expects size of density grid to be divisible by 8, got {}\".format(\n                n_bits,\n            )\n        )\n    n_bytes = n_bits // 8\n\n    dtype = jax.dtypes.canonicalize_dtype(density_grid.dtype)\n    if dtype != jnp.float32:\n        raise NotImplementedError(\n            \"pack_density_into_bits is only implemented for densities of `jnp.float32` type, got {}\".format(\n                dtype,\n            )\n        )\n\n    out_shapes = {\n        \"occupied_mask\": (n_bits,),\n        \"occupancy_bitfield\": (n_bytes,),\n    }\n    return (\n        jax.ShapedArray(out_shapes[\"occupied_mask\"], jnp.bool_),\n        jax.ShapedArray(out_shapes[\"occupancy_bitfield\"], jnp.uint8),\n    )", ""]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/packbits/__init__.py", "chunked_list": ["from typing import Tuple\nimport jax\nimport jax.numpy as jnp\n\nfrom . import impl\n\n\ndef packbits(\n    density_threshold: float,\n    density_grid: jax.Array,\n) -> Tuple[jax.Array, jax.Array]:\n    \"\"\"\n    Pack the given `density_grid` into a compact representation of type uint8, where each bit is\n    high if its corresponding density grid cell's density is larger than `density_threshold`, low\n    otherwise.\n\n    Inputs:\n        density_threshold `broadcastable to [N]`\n        density_grid `[N]`\n\n    Returns:\n        occ_mask `[N] bool`: boolean mask that indicates whether this grid is occupied\n        occ_bitfield `[N//8]`\n    \"\"\"\n    return impl.packbits_p.bind(\n        jnp.broadcast_to(density_threshold, density_grid.shape),\n        density_grid,\n    )", ""]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/marching/impl.py", "chunked_list": ["import functools\n\nimport jax\nfrom jax.interpreters import mlir, xla\nfrom jax.lib import xla_client\n\nfrom . import abstract, lowering\nfrom .. import volrendutils_cuda\n\n", "\n\n# register GPU XLA custom calls\nfor name, value in volrendutils_cuda.get_marching_registrations().items():\n    xla_client.register_custom_call_target(name, value, platform=\"gpu\")\n\nmarch_rays_p = jax.core.Primitive(\"march_rays\ud83d\udde7\")\nmarch_rays_p.multiple_results = True\nmarch_rays_p.def_impl(functools.partial(xla.apply_primitive, march_rays_p))\nmarch_rays_p.def_abstract_eval(abstract.march_rays_abstract)", "march_rays_p.def_impl(functools.partial(xla.apply_primitive, march_rays_p))\nmarch_rays_p.def_abstract_eval(abstract.march_rays_abstract)\n\nmarch_rays_inference_p = jax.core.Primitive(\"march_rays_inference\ud83d\udde7\")\nmarch_rays_inference_p.multiple_results = True\nmarch_rays_inference_p.def_impl(functools.partial(xla.apply_primitive, march_rays_inference_p))\nmarch_rays_inference_p.def_abstract_eval(abstract.march_rays_inference_abstract)\n\n# register mlir lowering rules\nmlir.register_lowering(", "# register mlir lowering rules\nmlir.register_lowering(\n    prim=march_rays_p,\n    rule=lowering.march_rays_lowering_rule,\n    platform=\"gpu\",\n)\nmlir.register_lowering(\n    prim=march_rays_inference_p,\n    rule=lowering.march_rays_inference_lowering_rule,\n    platform=\"gpu\",", "    rule=lowering.march_rays_inference_lowering_rule,\n    platform=\"gpu\",\n)\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/marching/lowering.py", "chunked_list": ["from jax.interpreters import mlir\nfrom jax.interpreters.mlir import ir\n\nfrom .. import volrendutils_cuda\n\ntry:\n    from jaxlib.mhlo_helpers import custom_call\nexcept ModuleNotFoundError:\n    # A more recent jaxlib would have `hlo_helpers` instead of `mhlo_helpers`\n    # <https://github.com/google/jax/commit/b8ae8e3fa10f9abe998459fac1513915acee776d#diff-50658d597212b4ce070b8bd8c1fc522deeee1845ba387a0a5b507c446e8ea12a>\n    from jaxlib.hlo_helpers import custom_call", "\n\n# helper function for mapping given shapes to their default mlir layouts\ndef default_layouts(*shapes):\n    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\n\ndef march_rays_lowering_rule(\n    ctx: mlir.LoweringRule,\n\n    # arrays\n    rays_o: ir.Value,\n    rays_d: ir.Value,\n    t_starts: ir.Value,\n    t_ends: ir.Value,\n    noises: ir.Value,\n    occupancy_bitfield: ir.Value,\n\n    # static args\n    total_samples: int,  # int\n    diagonal_n_steps: int,  # int\n    K: int,  # int\n    G: int,  # int\n    bound: float,  # float\n    stepsize_portion: float,  # float\n):\n    n_rays, _ = ir.RankedTensorType(rays_o.type).shape\n\n    opaque = volrendutils_cuda.make_marching_descriptor(\n        n_rays,\n        total_samples,\n        diagonal_n_steps,\n        K,\n        G,\n        bound,\n        stepsize_portion,\n    )\n\n    shapes = {\n        \"in.rays_o\": (n_rays, 3),\n        \"in.rays_d\": (n_rays, 3),\n        \"in.t_starts\": (n_rays,),\n        \"in.t_ends\": (n_rays,),\n        \"in.noises\": (n_rays,),\n        \"in.occupancy_bitfield\": (K*G*G*G//8,),\n\n        \"helper.next_sample_write_location\": (1,),\n        \"helper.number_of_exceeded_samples\": (1,),\n        \"helper.ray_is_valid\": (n_rays,),\n\n        \"out.rays_n_samples\": (n_rays,),\n        \"out.rays_sample_startidx\": (n_rays,),\n        \"out.idcs\": (total_samples,),\n        \"out.xyzs\": (total_samples, 3),\n        \"out.dirs\": (total_samples, 3),\n        \"out.dss\": (total_samples,),\n        \"out.z_vals\": (total_samples,),\n    }\n\n    return custom_call(\n        call_target_name=\"march_rays\",\n        out_types=[\n            ir.RankedTensorType.get(shapes[\"helper.next_sample_write_location\"], ir.IntegerType.get_unsigned(32)),\n            ir.RankedTensorType.get(shapes[\"helper.number_of_exceeded_samples\"], ir.IntegerType.get_unsigned(32)),\n            ir.RankedTensorType.get(shapes[\"helper.ray_is_valid\"], ir.IntegerType.get_signless(1)),\n            ir.RankedTensorType.get(shapes[\"out.rays_n_samples\"], ir.IntegerType.get_unsigned(32)),\n            ir.RankedTensorType.get(shapes[\"out.rays_sample_startidx\"], ir.IntegerType.get_unsigned(32)),\n            ir.RankedTensorType.get(shapes[\"out.idcs\"], ir.IntegerType.get_unsigned(32)),\n            ir.RankedTensorType.get(shapes[\"out.xyzs\"], ir.F32Type.get()),\n            ir.RankedTensorType.get(shapes[\"out.dirs\"], ir.F32Type.get()),\n            ir.RankedTensorType.get(shapes[\"out.dss\"], ir.F32Type.get()),\n            ir.RankedTensorType.get(shapes[\"out.z_vals\"], ir.F32Type.get()),\n        ],\n        operands=[\n            rays_o,\n            rays_d,\n            t_starts,\n            t_ends,\n            noises,\n            occupancy_bitfield,\n        ],\n        backend_config=opaque,\n        operand_layouts=default_layouts(\n            shapes[\"in.rays_o\"],\n            shapes[\"in.rays_d\"],\n            shapes[\"in.t_starts\"],\n            shapes[\"in.t_ends\"],\n            shapes[\"in.noises\"],\n            shapes[\"in.occupancy_bitfield\"],\n        ),\n        result_layouts=default_layouts(\n            shapes[\"helper.next_sample_write_location\"],\n            shapes[\"helper.number_of_exceeded_samples\"],\n            shapes[\"helper.ray_is_valid\"],\n            shapes[\"out.rays_n_samples\"],\n            shapes[\"out.rays_sample_startidx\"],\n            shapes[\"out.idcs\"],\n            shapes[\"out.xyzs\"],\n            shapes[\"out.dirs\"],\n            shapes[\"out.dss\"],\n            shapes[\"out.z_vals\"],\n        ),\n    )", "\n\ndef march_rays_inference_lowering_rule(\n    ctx: mlir.LoweringRule,\n\n    # arrays\n    rays_o: ir.BlockArgument,\n    rays_d: ir.BlockArgument,\n    t_starts: ir.BlockArgument,\n    t_ends: ir.BlockArgument,\n    occupancy_bitfield: ir.BlockArgument,\n    next_ray_index_in: ir.BlockArgument,\n    terminated: ir.BlockArgument,\n    indices_in: ir.BlockArgument,\n\n    # static args\n    diagonal_n_steps: int,\n    K: int,\n    G: int,\n    march_steps_cap: int,\n    bound: float,\n    stepsize_portion: float,\n):\n    (n_total_rays, _), (n_rays,) = ir.RankedTensorType(rays_o.type).shape, ir.RankedTensorType(terminated.type).shape\n\n    opaque = volrendutils_cuda.make_marching_inference_descriptor(\n        n_total_rays,\n        n_rays,\n        diagonal_n_steps,\n        K,\n        G,\n        march_steps_cap,\n        bound,\n        stepsize_portion,\n    )\n\n    shapes = {\n        \"in.rays_o\": (n_total_rays, 3),\n        \"in.rays_d\": (n_total_rays, 3),\n        \"in.t_starts\": (n_total_rays,),\n        \"in.t_ends\": (n_total_rays,),\n        \"in.occupancy_bitfield\": (K*G*G*G//8,),\n        \"in.next_ray_index_in\": (1,),\n        \"in.terminated\": (n_rays,),\n        \"in.indices_in\": (n_rays,),\n\n        \"out.next_ray_index\": (1,),\n        \"out.indices_out\": (n_rays,),\n        \"out.n_samples\": (n_rays,),\n        \"out.t_starts\": (n_rays,),\n        \"out.xyzs\": (n_rays, march_steps_cap, 3),\n        \"out.dss\": (n_rays, march_steps_cap),\n        \"out.z_vals\": (n_rays, march_steps_cap),\n    }\n\n    return custom_call(\n        call_target_name=\"march_rays_inference\",\n        out_types=[\n            ir.RankedTensorType.get(shapes[\"out.next_ray_index\"], ir.IntegerType.get_unsigned(32)),\n            ir.RankedTensorType.get(shapes[\"out.indices_out\"], ir.IntegerType.get_unsigned(32)),\n            ir.RankedTensorType.get(shapes[\"out.n_samples\"], ir.IntegerType.get_unsigned(32)),\n            ir.RankedTensorType.get(shapes[\"out.t_starts\"], ir.F32Type.get()),\n            ir.RankedTensorType.get(shapes[\"out.xyzs\"], ir.F32Type.get()),\n            ir.RankedTensorType.get(shapes[\"out.dss\"], ir.F32Type.get()),\n            ir.RankedTensorType.get(shapes[\"out.z_vals\"], ir.F32Type.get()),\n        ],\n        operands=[\n            rays_o,\n            rays_d,\n            t_starts,\n            t_ends,\n            occupancy_bitfield,\n            next_ray_index_in,\n            terminated,\n            indices_in,\n        ],\n        backend_config=opaque,\n        operand_layouts=default_layouts(\n            shapes[\"in.rays_o\"],\n            shapes[\"in.rays_d\"],\n            shapes[\"in.t_starts\"],\n            shapes[\"in.t_ends\"],\n            shapes[\"in.occupancy_bitfield\"],\n            shapes[\"in.next_ray_index_in\"],\n            shapes[\"in.terminated\"],\n            shapes[\"in.indices_in\"],\n        ),\n        result_layouts=default_layouts(\n            shapes[\"out.next_ray_index\"],\n            shapes[\"out.indices_out\"],\n            shapes[\"out.n_samples\"],\n            shapes[\"out.t_starts\"],\n            shapes[\"out.xyzs\"],\n            shapes[\"out.dss\"],\n            shapes[\"out.z_vals\"],\n        ),\n    )", ""]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/marching/abstract.py", "chunked_list": ["import chex\nimport jax\nimport jax.numpy as jnp\n\n\n# jit rules\ndef march_rays_abstract(\n    # arrays\n    rays_o: jax.ShapedArray,\n    rays_d: jax.ShapedArray,\n    t_starts: jax.ShapedArray,\n    t_ends: jax.ShapedArray,\n    noises: jax.ShapedArray,\n    occupancy_bitfield: jax.ShapedArray,\n\n    # static args\n    total_samples: int,\n    diagonal_n_steps: int,\n    K: int,\n    G: int,\n    bound: float,\n    stepsize_portion: float,\n):\n    n_rays, _ = rays_o.shape\n\n    chex.assert_shape([rays_o, rays_d], (n_rays, 3))\n    chex.assert_shape([t_starts, t_ends, noises], (n_rays,))\n\n    chex.assert_shape(occupancy_bitfield, (K*G*G*G//8,))\n    chex.assert_type(occupancy_bitfield, jnp.uint8)\n\n    chex.assert_scalar_positive(total_samples)\n    chex.assert_scalar_positive(diagonal_n_steps)\n    chex.assert_scalar_positive(K)\n    chex.assert_scalar_positive(G)\n    chex.assert_scalar_positive(bound)\n    chex.assert_scalar_non_negative(stepsize_portion)\n\n    dtype = jax.dtypes.canonicalize_dtype(rays_o.dtype)\n    if dtype != jnp.float32:\n        raise NotImplementedError(\n            \"march_rays is only implemented for input coordinates of `jnp.float32` type, got {}\".format(\n                dtype,\n            )\n        )\n\n    shapes = {\n        \"helper.next_sample_write_location\": (1,),\n        \"helper.number_of_exceeded_samples\": (1,),\n        \"helper.ray_is_valid\": (n_rays,),\n\n        \"out.rays_n_samples\": (n_rays,),\n        \"out.rays_sample_startidx\": (n_rays,),\n        \"out.idcs\": (total_samples,),\n        \"out.xyzs\": (total_samples, 3),\n        \"out.dirs\": (total_samples, 3),\n        \"out.dss\": (total_samples,),\n        \"out.z_vals\": (total_samples,),\n    }\n\n    return (\n        jax.ShapedArray(shape=shapes[\"helper.next_sample_write_location\"], dtype=jnp.uint32),\n        jax.ShapedArray(shape=shapes[\"helper.number_of_exceeded_samples\"], dtype=jnp.uint32),\n        jax.ShapedArray(shape=shapes[\"helper.ray_is_valid\"], dtype=jnp.bool_),\n        jax.ShapedArray(shape=shapes[\"out.rays_n_samples\"], dtype=jnp.uint32),\n        jax.ShapedArray(shape=shapes[\"out.rays_sample_startidx\"], dtype=jnp.uint32),\n        jax.ShapedArray(shape=shapes[\"out.idcs\"], dtype=jnp.uint32),\n        jax.ShapedArray(shape=shapes[\"out.xyzs\"], dtype=jnp.float32),\n        jax.ShapedArray(shape=shapes[\"out.dirs\"], dtype=jnp.float32),\n        jax.ShapedArray(shape=shapes[\"out.dss\"], dtype=jnp.float32),\n        jax.ShapedArray(shape=shapes[\"out.z_vals\"], dtype=jnp.float32),\n    )", "\n\ndef march_rays_inference_abstract(\n    # arrays\n    rays_o: jax.ShapedArray,\n    rays_d: jax.ShapedArray,\n    t_starts: jax.ShapedArray,\n    t_ends: jax.ShapedArray,\n    occupancy_bitfield: jax.ShapedArray,\n    next_ray_index: jax.ShapedArray,\n    terminated: jax.ShapedArray,\n    indices_in: jax.ShapedArray,\n\n    # static args\n    diagonal_n_steps: int,\n    K: int,\n    G: int,\n    march_steps_cap: int,\n    bound: float,\n    stepsize_portion: float,\n):\n    (n_total_rays, _), (n_rays,) = rays_o.shape, terminated.shape\n\n    chex.assert_shape([rays_o, rays_d], (n_total_rays, 3))\n    chex.assert_shape([t_starts, t_ends], (n_total_rays,))\n    chex.assert_shape(occupancy_bitfield, (K*G*G*G//8,))\n    chex.assert_type(occupancy_bitfield, jnp.uint8)\n    chex.assert_shape(next_ray_index, (1,))\n    chex.assert_shape([terminated, indices_in], (n_rays,))\n\n    out_shapes = {\n        \"next_ray_index\": (1,),\n        \"indices_out\": (n_rays,),\n        \"n_samples\": (n_rays,),\n        \"t_starts\": (n_rays,),\n        \"xyzs\": (n_rays, march_steps_cap, 3),\n        \"dss\": (n_rays, march_steps_cap),\n        \"z_vals\": (n_rays, march_steps_cap),\n    }\n\n    return (\n        jax.ShapedArray(shape=out_shapes[\"next_ray_index\"], dtype=jnp.uint32),\n        jax.ShapedArray(shape=out_shapes[\"indices_out\"], dtype=jnp.uint32),\n        jax.ShapedArray(shape=out_shapes[\"n_samples\"], dtype=jnp.uint32),\n        jax.ShapedArray(shape=out_shapes[\"t_starts\"], dtype=jnp.float32),\n        jax.ShapedArray(shape=out_shapes[\"xyzs\"], dtype=jnp.float32),\n        jax.ShapedArray(shape=out_shapes[\"dss\"], dtype=jnp.float32),\n        jax.ShapedArray(shape=out_shapes[\"z_vals\"], dtype=jnp.float32),\n    )", ""]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/marching/__init__.py", "chunked_list": ["from typing import Tuple\n\nimport jax\nimport jax.numpy as jnp\n\nfrom . import impl\n\ndef march_rays(\n    # static\n    total_samples: int,\n    diagonal_n_steps: int,\n    K: int,\n    G: int,\n    bound: float,\n    stepsize_portion: float,\n\n    # inputs\n    rays_o: jax.Array,\n    rays_d: jax.Array,\n    t_starts: jax.Array,\n    t_ends: jax.Array,\n    noises: jax.Array,\n    occupancy_bitfield: jax.Array,\n) -> Tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:\n    \"\"\"\n    Given a pack of rays (`rays_o`, `rays_d`), their intersection time with the scene bounding box\n    (`t_starts`, `t_ends`), and an occupancy grid (`occupancy_bitfield`), generate samples along\n    each ray.\n\n    Inputs:\n        total_samples `int`: ,\n        diagonal_n_steps `int`: the length of a minimal ray marching step is calculated internally\n                                as:\n                                    \u0394\ud835\udc61 := \u221a3 / diagonal_n_steps;\n                                the NGP paper uses diagonal_n_steps=1024 (as described in appendix\n                                E.1).\n        K `int`: total number of cascades of `occupancy_bitfield`\n        G `int`: occupancy grid resolution, the paper uses 128 for every cascade\n        bound `float`: the half length of the longest axis of the scene\u2019s bounding box,\n                       e.g. the `bound` of the bounding box [-1, 1]^3 is 1\n        stepsize_portion: next step size is calculated as t * stepsize_portion, the paper uses 1/256\n\n        rays_o `[n_rays, 3]`: ray origins\n        rays_d `[n_rays, 3]`: **unit** vectors representing ray directions\n        t_starts `[n_rays]`: time of the ray entering the scene bounding box\n        t_ends `[n_rays]`: time of the ray leaving the scene bounding box\n        noises `broadcastable to [n_rays]`: noises to perturb the starting point of ray marching\n        occupancy_bitfield `[K*(G**3)//8]`: the occupancy grid represented as a bit array, grid\n                                            cells are laid out in Morton (z-curve) order, as\n                                            described in appendix E.2 of the NGP paper\n\n    Returns:\n        measured_batch_size_before_compaction `int`: total number of generated samples of all rays\n        ray_is_valid `bool` `[n_rays]`: a mask, where a true value denotes the ray's gradients\n                                        should flow, even if there are no samples generated for it\n        idcs `[total_samples]`: indices indicating which ray the i-th sample comes from.\n        rays_n_samples `[n_rays]`: number of samples of each ray, its sum is `total_samples`\n                                   referenced below\n        rays_sample_startidx `[n_rays]`: indices of each ray's first sample\n        xyzs `[total_samples, 3]`: spatial coordinates of the generated samples, invalid array\n                                   locations are masked out with zeros\n        dirs `[total_samples, 3]`: spatial coordinates of the generated samples, invalid array\n                                   locations are masked out with zeros.\n        dss `[total_samples]`: `ds`s of each sample, for a more detailed explanation of this\n                               notation, see documentation of function `volrendjax.integrate_rays`,\n                               invalid array locations are masked out with zeros.\n        z_vals `[total_samples]`: samples' distances to their origins, invalid array\n                                  locations are masked out with zeros.\n    \"\"\"\n    n_rays, _ = rays_o.shape\n    noises = jnp.broadcast_to(noises, (n_rays,))\n\n    next_sample_write_location, number_of_exceeded_samples, ray_is_valid, rays_n_samples, rays_sample_startidx, idcs, xyzs, dirs, dss, z_vals = impl.march_rays_p.bind(\n        # arrays\n        rays_o,\n        rays_d,\n        t_starts,\n        t_ends,\n        noises,\n        occupancy_bitfield,\n\n        # static args\n        total_samples=total_samples,\n        diagonal_n_steps=diagonal_n_steps,\n        K=K,\n        G=G,\n        bound=bound,\n        stepsize_portion=stepsize_portion,\n    )\n\n    measured_batch_size_before_compaction = next_sample_write_location[0] - number_of_exceeded_samples[0]\n\n    return measured_batch_size_before_compaction, ray_is_valid, rays_n_samples, rays_sample_startidx, idcs, xyzs, dirs, dss, z_vals", "\n\ndef march_rays_inference(\n    # static\n    diagonal_n_steps: int,\n    K: int,\n    G: int,\n    march_steps_cap: int,\n    bound: float,\n    stepsize_portion: float,\n\n    # inputs\n    rays_o: jax.Array,\n    rays_d: jax.Array,\n    t_starts: jax.Array,\n    t_ends: jax.Array,\n    occupancy_bitfield: jax.Array,\n    next_ray_index_in: jax.Array,\n    terminated: jax.Array,\n    indices: jax.Array,\n):\n    \"\"\"\n    Inputs:\n        diagonal_n_steps, K, G, bound, stepsize_portion: see explanations in function `march_rays`\n        march_steps_cap `int`: maximum steps to march for each ray in this iteration\n\n        rays_o `float` `[n_total_rays, 3]`: ray origins\n        rays_d `float` `[n_total_rays, 3]`: ray directions\n        t_starts `float` `n_total_rays`: distance of each ray's starting point to its origin\n        t_ends `float` `n_total_rays`: distance of each ray's ending point to its origin\n        occupancy_bitfield `uint8` `[K*(G**3)//8]`: the occupancy grid represented as a bit array\n        next_ray_index_in `uint32`: helper variable to keep record of the latest ray that got rendered\n        terminated `bool` `[n_rays]`: output of `integrate_rays_inference`, a binary mask indicating\n                                      each ray's termination status\n        indices `[n_rays]`: each ray's location in the global arrays\n\n    Returns:\n        next_ray_index `uint32` `[1]`: for use in next iteration\n        indices `uint32` `[n_rays]`: for use in the integrate_rays_inference immediately after\n        n_samples `uint32` `[n_rays]`: number of generated samples of each ray in question\n        t_starts `float` `[n_rays]`: advanced values of `t` for use in next iteration\n        xyzs `float` `[n_rays, march_steps_cap, 3]`: each sample's XYZ coordinate\n        dss `float` `[n_rays, march_steps_cap]`: `ds` of each sample\n        z_vals `float` `[n_rays, march_steps_cap]`: distance of each sample to their ray origins\n    \"\"\"\n    next_ray_index, indices, n_samples, t_starts_out, xyzs, dss, z_vals = impl.march_rays_inference_p.bind(\n        rays_o,\n        rays_d,\n        t_starts,\n        t_ends,\n        occupancy_bitfield,\n        next_ray_index_in,\n        terminated,\n        indices,\n\n        diagonal_n_steps=diagonal_n_steps,\n        K=K,\n        G=G,\n        march_steps_cap=march_steps_cap,\n        bound=bound,\n        stepsize_portion=stepsize_portion,\n    )\n    t_starts = t_starts.at[indices].set(t_starts_out)\n    return next_ray_index, indices, n_samples, t_starts, xyzs, dss, z_vals", ""]}
{"filename": "deps/spherical-harmonics-encoding-jax/setup.py", "chunked_list": ["#!/usr/bin/env python\n\nimport os\nimport subprocess\n\nfrom setuptools import Extension, find_packages, setup\nfrom setuptools.command.build_ext import build_ext\n\nHERE = os.path.dirname(os.path.realpath(__file__))\n", "HERE = os.path.dirname(os.path.realpath(__file__))\n\n\nclass CMakeBuildExt(build_ext):\n    def build_extensions(self):\n        # First: configure CMake build\n        import platform\n        import sys\n        import sysconfig\n\n        import pybind11\n\n        # Work out the relevant Python paths to pass to CMake, adapted from the\n        # PyTorch build system\n        if platform.system() == \"Windows\":\n            cmake_python_library = \"{}/libs/python{}.lib\".format(\n                sysconfig.get_config_var(\"prefix\"),\n                sysconfig.get_config_var(\"VERSION\"),\n            )\n            if not os.path.exists(cmake_python_library):\n                cmake_python_library = \"{}/libs/python{}.lib\".format(\n                    sys.base_prefix,\n                    sysconfig.get_config_var(\"VERSION\"),\n                )\n        else:\n            cmake_python_library = \"{}/{}\".format(\n                sysconfig.get_config_var(\"LIBDIR\"),\n                sysconfig.get_config_var(\"INSTSONAME\"),\n            )\n        cmake_python_include_dir = sysconfig.get_path(\"include\")\n\n        install_dir = os.path.abspath(\n            os.path.dirname(self.get_ext_fullpath(\"dummy\"))\n        )\n        os.makedirs(install_dir, exist_ok=True)\n        cmake_args = [\n            \"-DCMAKE_INSTALL_PREFIX={}\".format(install_dir),\n            \"-DPython_EXECUTABLE={}\".format(sys.executable),\n            \"-DPython_LIBRARIES={}\".format(cmake_python_library),\n            \"-DPython_INCLUDE_DIRS={}\".format(cmake_python_include_dir),\n            \"-DCMAKE_BUILD_TYPE={}\".format(\n                \"Debug\" if self.debug else \"Release\"\n            ),\n            \"-DCMAKE_PREFIX_PATH={}\".format(pybind11.get_cmake_dir()),\n            \"-G Ninja\",\n        ]\n        os.makedirs(self.build_temp, exist_ok=True)\n        subprocess.check_call(\n            [\"cmake\", HERE] + cmake_args, cwd=self.build_temp\n        )\n\n        # Build all the extensions\n        super().build_extensions()\n\n        # Finally run install\n        subprocess.check_call(\n            [\"cmake\", \"--build\", \".\", \"--target\", \"install\"],\n            cwd=self.build_temp,\n        )\n\n    def build_extension(self, ext):\n        target_name = ext.name.split(\".\")[-1]\n        subprocess.check_call(\n            [\"cmake\", \"--build\", \".\", \"--target\", target_name],\n            cwd=self.build_temp,\n        )", "\nextensions = [\n    Extension(\n        \"shjax.cudaops\",  # Python dotted name, whose final component should be a buildable target defined in CMakeLists.txt\n        [  # source paths, relative to this setup.py file\n            \"lib/ffi.cc\",\n            \"lib/impl/spherical_harmonics_encoding.cu\",\n        ],\n    )\n]", "    )\n]\n\nsetup(\n    name=\"spherical-harmonics-encoding-jax\",\n    author=\"blurgyy\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    include_package_data=True,\n    install_requires=[\"jax\", \"jaxlib\", \"chex\"],", "    include_package_data=True,\n    install_requires=[\"jax\", \"jaxlib\", \"chex\"],\n    ext_modules=extensions,\n    cmdclass={\"build_ext\": CMakeBuildExt},\n)\n"]}
{"filename": "deps/spherical-harmonics-encoding-jax/src/shjax/__init__.py", "chunked_list": ["import functools\n\nimport chex\nimport jax\nfrom jax.abstract_arrays import ShapedArray\nfrom jax.interpreters import batching, mlir, xla\nfrom jax.interpreters.mlir import ir\nfrom jax.lib import xla_client\nimport jax.numpy as jnp\n", "import jax.numpy as jnp\n\nfrom . import cudaops\n\ntry:\n    from jaxlib.mhlo_helpers import custom_call\nexcept ModuleNotFoundError:\n    # A more recent jaxlib would have `hlo_helpers` instead of `mhlo_helpers`\n    # <https://github.com/google/jax/commit/b8ae8e3fa10f9abe998459fac1513915acee776d#diff-50658d597212b4ce070b8bd8c1fc522deeee1845ba387a0a5b507c446e8ea12a>\n    from jaxlib.hlo_helpers import custom_call", "\n\n# register GPU XLA custom calls\nfor name, value in cudaops.get_registrations().items():\n    xla_client.register_custom_call_target(name, value, platform=\"gpu\")\n\n# jit rules, infer returned shape according to input\ndef _spherical_harmonics_encoding_abstract(coord: jax.Array, hint: jax.Array):\n    \"\"\"\n    Inputs:\n        coord [..., 3] float: input coordinates\n        hint [degree]: an array with shape [L], this is used to hint the function with the desired\n                  spherical harmonics degrees\n    \"\"\"\n    (*n, _), dtype = coord.shape, coord.dtype\n    n = functools.reduce(lambda x, y: x * y, n)\n    degree, = hint.shape\n    dtype = jax.dtypes.canonicalize_dtype(coord.dtype)\n    return ShapedArray(shape=(n, degree * degree), dtype=dtype)", "\n# register the primitive\nsh_enc_p = jax.core.Primitive(\"spherical_harmonics_encoding\ud83c\udf10\")\nsh_enc_p.multiple_results = False\nsh_enc_p.def_impl(functools.partial(xla.apply_primitive, sh_enc_p))\nsh_enc_p.def_abstract_eval(_spherical_harmonics_encoding_abstract)\n\n# helper function for mapping given shapes to their default mlir layouts\ndef default_layouts(*shapes):\n    return [range(len(shape) - 1, -1, -1) for shape in shapes]", "def default_layouts(*shapes):\n    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\n# mlir lowering rule\ndef _spherical_harmonics_encoding_lowering_cuda(\n        ctx: mlir.LoweringRuleContext,\n        coord: ir.Value,\n        hint: ir.Value,\n    ):\n    coord_type = ir.RankedTensorType(coord.type)\n    coord_shape = coord_type.shape\n\n    n, _ = coord_shape\n    degree, = ir.RankedTensorType(hint.type).shape\n\n    result_shape = (n, degree * degree)\n\n    opaque = cudaops.make_spherical_harmonics_encoding_descriptor(n, degree)\n\n    # Documentation says directly return the `custom_call` would suffice, but directly returning\n    # here results in error \"Output of translation rule must be iterable: ...\", so let's make it\n    # iterable.\n    # NOTE:\n    #   A newer jaxlib (current 0.3.22) may require this to be a single custom_call(...), instead of\n    #   an iterable, as documentation suggests.\n    # REF:\n    #   documentation: <https://jax.readthedocs.io/en/latest/Custom_Operation_for_GPUs.html#lowering-to-mlir-custom-call>\n    #   tutorial: <https://github.com/dfm/extending-jax/blob/1cb4c39c524bccb5e3068c5a7f57a425ab0426a2/src/kepler_jax/kepler_jax.py#L113>\n    return [custom_call(\n        \"spherical_harmonics_encoding_cuda_f32\",  # the name of the registered XLA custom call at the top of this script\n        out_types=[\n            ir.RankedTensorType.get(result_shape, coord_type.element_type),\n        ],\n        operands=[coord],\n        backend_config=opaque,\n        operand_layouts=default_layouts(coord_shape),\n        result_layouts=default_layouts(result_shape),\n    )]", "\nmlir.register_lowering(\n    prim=sh_enc_p,\n    rule=_spherical_harmonics_encoding_lowering_cuda,\n    platform=\"gpu\",\n)\n\n# vmap support. REF: <https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html#batching>\ndef spherical_harmonics_encoding_batch(args, axes):\n    \"\"\"\n    The primitive is already able to handle arbitrary shape (except for the last axis, which must\n    have a dimension of 3), directly binding to the primitive impl should suffice.\n\n    Inputs:\n        args: Passed to def_impl, contains two tensors: `coord` and `hint`, where only `coord` is\n              batched.  args is (coord, hint)\n        axes: The axes that are being batched, one value for each arg, value is an integer if the\n              arg is batched, value is None if the arg is not batched.  In this case,\n              coord.shape[axes[0]] = B, and axes[1] = None.\n    \"\"\"\n    coord, hint = args\n    assert coord.shape[-1] == 3, \"spatial coordinates must be the last dimension\"\n\n    enc = sh_enc_p.bind(coord, hint)\n    # or:\n    # enc = spherical_harmonics_encoding(\n    #     coord=coord,\n    #     degree=hint.shape[0],\n    # )\n\n    # return the result, and the result axis that was batched\n    return enc, axes[0]", "def spherical_harmonics_encoding_batch(args, axes):\n    \"\"\"\n    The primitive is already able to handle arbitrary shape (except for the last axis, which must\n    have a dimension of 3), directly binding to the primitive impl should suffice.\n\n    Inputs:\n        args: Passed to def_impl, contains two tensors: `coord` and `hint`, where only `coord` is\n              batched.  args is (coord, hint)\n        axes: The axes that are being batched, one value for each arg, value is an integer if the\n              arg is batched, value is None if the arg is not batched.  In this case,\n              coord.shape[axes[0]] = B, and axes[1] = None.\n    \"\"\"\n    coord, hint = args\n    assert coord.shape[-1] == 3, \"spatial coordinates must be the last dimension\"\n\n    enc = sh_enc_p.bind(coord, hint)\n    # or:\n    # enc = spherical_harmonics_encoding(\n    #     coord=coord,\n    #     degree=hint.shape[0],\n    # )\n\n    # return the result, and the result axis that was batched\n    return enc, axes[0]", "\nbatching.primitive_batchers[sh_enc_p] = spherical_harmonics_encoding_batch\n\n# the only exposed function\ndef spherical_harmonics_encoding(coord: jax.Array, degree: int) -> jax.Array:\n    \"\"\"\n    Spherical harmonics encoding with GPU acceleration, expects unit vectors as input.\n\n    Inputs:\n        coord [..., 3] float: input 3D coordinates\n        degree int: highest degree used in spherical harmonics\n\n    Returns:\n        outputs [..., degree**2] float: encoded coordinates\n    \"\"\"\n    chex.assert_rank(coord, 2)\n    chex.assert_axis_dimension(coord, -1, 3)\n    chex.assert_scalar_non_negative(degree)\n    return sh_enc_p.bind(coord, jnp.empty((degree,)))", ""]}
{"filename": "utils/__main__.py", "chunked_list": ["#!/usr/bin/env python3\n\nfrom dataclasses import dataclass\nfrom functools import partial, reduce\nimport os\nfrom pathlib import Path\nfrom typing import Annotated, List\nfrom typing_extensions import assert_never\n\nfrom PIL import Image", "\nfrom PIL import Image\nimport numpy as np\nimport tyro\n\nfrom utils.common import setup_logging\nfrom utils.data import (\n    add_border,\n    blend_rgba_image_array,\n    create_scene_from_single_camera_image_collection,", "    blend_rgba_image_array,\n    create_scene_from_single_camera_image_collection,\n    create_scene_from_video,\n    psnr,\n    side_by_side,\n)\nfrom utils.types import RGBColor, SceneCreationOptions\n\n\n@dataclass(frozen=True, kw_only=True)\nclass Concatenate:\n    image_paths: tyro.conf.Positional[List[Path]]\n    # output image save path, the path will be overwritten with a warning\n    out: Path\n    # if specified, concatenate vertically instead of horizontally\n    vertical: bool=False\n    # gap between adjacent images, in pixels\n    gap: int=0\n    # border in pixels\n    border: int=0\n    bg: RGBColor=(1.0, 1.0, 1.0)", "\n@dataclass(frozen=True, kw_only=True)\nclass Concatenate:\n    image_paths: tyro.conf.Positional[List[Path]]\n    # output image save path, the path will be overwritten with a warning\n    out: Path\n    # if specified, concatenate vertically instead of horizontally\n    vertical: bool=False\n    # gap between adjacent images, in pixels\n    gap: int=0\n    # border in pixels\n    border: int=0\n    bg: RGBColor=(1.0, 1.0, 1.0)", "\n\n@dataclass(frozen=True, kw_only=True)\nclass Metrics:\n    gt: Path\n    image_paths: tyro.conf.Positional[List[Path]]\n    psnr: bool=True\n    bg: RGBColor=(1.0, 1.0, 1.0)\n\n", "\n\n@dataclass(frozen=True, kw_only=True)\nclass CreateScene:\n    # path to a video or a directory of image collection\n    src: tyro.conf.Positional[Path]\n\n    # where to write the images and transforms_{train,val,test}.json\n    root_dir: Path\n\n    # how many frames to extract per second, only required when src is a video\n    fps: int | None=None\n\n    scene_opts: tyro.conf.OmitArgPrefixes[SceneCreationOptions]", "\nCmdCat = Annotated[\n    Concatenate,\n    tyro.conf.subcommand(\n        name=\"cat\",\n        prefix_name=False,\n        description=\"concatenate images horizontally or vertically\",\n    ),\n]\nCmdMetrics = Annotated[", "]\nCmdMetrics = Annotated[\n    Metrics,\n    tyro.conf.subcommand(\n        name=\"metrics\",\n        prefix_name=False,\n        description=\"compute metrics between images\",\n    ),\n]\nCmdCreateScene = Annotated[", "]\nCmdCreateScene = Annotated[\n    CreateScene,\n    tyro.conf.subcommand(\n        name=\"create-scene\",\n        prefix_name=False,\n        description=\"create a instant-ngp-compatible scene from a video or a directory of images\",\n    ),\n]\n", "]\n\n\nArgs = CmdCat | CmdCreateScene | CmdMetrics\n\n\ndef main(args: Args):\n    logger = setup_logging(\"utils\", level=\"DEBUG\")\n    if isinstance(args, Concatenate):\n        if args.out.is_dir():\n            logger.error(\"output path '{}' is a directory\".format(args.out))\n            exit(1)\n        if args.out.exists():\n            logger.warn(\"output path '{}' exists and will be overwritten\".format(args.out))\n            if not os.access(args.out, os.W_OK):\n                logger.error(\"output path '{}' is readonly\".format(args.out))\n                exit(2)\n        if args.out.suffix.lower() not in map(lambda x: \".\" + x, [\"jpg\", \"jpeg\", \"png\", \"tif\", \"tiff\", \"bmp\", \"webp\"]):\n            logger.warn(\"the file extension '{}' might not be supported\".format(args.out.suffix))\n\n        images = list(map(\n            lambda img: blend_rgba_image_array(img, bg=args.bg) if img.shape[-1] == 4 else img,\n            map(np.asarray, map(Image.open, args.image_paths)),\n        ))\n        height, width = images[0].shape[:2]\n        oimg = reduce(\n            partial(\n                side_by_side,\n                height=(None if args.vertical else height),\n                width=(width if args.vertical else None),\n                vertical=args.vertical,\n                gap=args.gap,\n            ),\n            images,\n        )\n        oimg = add_border(oimg, border_pixels=args.border)\n        logger.info(\"saving image ...\")\n        Image.fromarray(np.asarray(oimg)).save(args.out)\n        logger.info(\"image ({}x{}) saved to '{}'\".format(oimg.shape[1], oimg.shape[0], args.out))\n\n    elif isinstance(args, Metrics):\n        gt_image = np.asarray(Image.open(args.gt))\n        if gt_image.shape[-1] == 4:\n            gt_image = blend_rgba_image_array(gt_image, bg=args.bg)\n        images = list(map(\n            lambda img: blend_rgba_image_array(img, bg=args.bg) if img.shape[-1] == 4 else img,\n            map(np.asarray, map(Image.open, args.image_paths)),\n        ))\n        for impath, img in zip(args.image_paths, images):\n            if args.psnr:\n                logger.info(\"psnr={} ({})\".format(psnr(gt_image, img), impath))\n\n    elif isinstance(args, CreateScene):\n        if args.src.is_dir():\n            create_scene_from_single_camera_image_collection(\n                raw_images_dir=args.src,\n                scene_root_dir=args.root_dir,\n                opts=args.scene_opts,\n            )\n        else:\n            assert args.fps is not None, \"must specify extracted frames per second via --fps for video source\"\n            create_scene_from_video(\n                video_path=args.src,\n                scene_root_dir=args.root_dir,\n                fps=args.fps,\n                opts=args.scene_opts,\n            )\n\n    else:\n        assert_never(\"tyro already ensures subcommand passed here are valid, this line should never be executed\")", "\n\nif __name__ == \"__main__\":\n    args = tyro.cli(Args)\n    main(args)\n"]}
{"filename": "utils/types.py", "chunked_list": ["from concurrent.futures import ThreadPoolExecutor\nimport dataclasses\nimport functools\nimport json\nimport math\nfrom pathlib import Path\nfrom typing import Callable, List, Literal, Tuple, Type\nfrom typing_extensions import assert_never\n\nfrom PIL import Image", "\nfrom PIL import Image\nimport chex\nfrom flax import struct\nfrom flax.struct import dataclass\nfrom flax.training.train_state import TrainState\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jran\nimport numpy as np", "import jax.random as jran\nimport numpy as np\nimport pydantic\nfrom tqdm import tqdm\nfrom volrendjax import morton3d_invert, packbits\n\nfrom ._constants import tqdm_format\n\n\nCameraModelType = Literal[", "\nCameraModelType = Literal[\n    \"SIMPLE_PINHOLE\",\n    \"PINHOLE\",\n    \"SIMPLE_RADIAL\",\n    \"RADIAL\",\n    \"OPENCV\",\n    \"OPENCV_FISHEYE\",\n]\nPositionalEncodingType = Literal[\"identity\", \"frequency\", \"hashgrid\", \"tcnn-hashgrid\"]", "]\nPositionalEncodingType = Literal[\"identity\", \"frequency\", \"hashgrid\", \"tcnn-hashgrid\"]\nDirectionalEncodingType = Literal[\"identity\", \"sh\", \"shcuda\"]\nEncodingType = Literal[PositionalEncodingType, DirectionalEncodingType]\nActivationType = Literal[\n    \"exponential\",\n    \"relu\",\n    \"sigmoid\",\n    \"thresholded_exponential\",\n    \"truncated_exponential\",", "    \"thresholded_exponential\",\n    \"truncated_exponential\",\n    \"truncated_thresholded_exponential\",\n]\n\nColmapMatcherType = Literal[\"Exhaustive\", \"Sequential\"]\nLogLevel = Literal[\"DEBUG\", \"INFO\", \"WARN\", \"WARNING\", \"ERROR\", \"CRITICAL\"]\nTransformsProvider = Literal[\"loaded\", \"orbit\"]\n\nDensityAndRGB = Tuple[jax.Array, jax.Array]", "\nDensityAndRGB = Tuple[jax.Array, jax.Array]\nRGBColor = Tuple[float, float, float]\nRGBColorU8 = Tuple[int, int, int]\nFourFloats = Tuple[float, float, float, float]\nMatrix4x4 = Tuple[FourFloats, FourFloats, FourFloats, FourFloats]\n\n\ndef empty_impl(clz):\n    if \"__dataclass_fields__\" not in clz.__dict__:\n        raise TypeError(\"class `{}` is not a dataclass\".format(clz.__name__))\n\n    fields = clz.__dict__[\"__dataclass_fields__\"]\n\n    def empty_fn(cls, /, **kwargs):\n        \"\"\"\n        Create an empty instance of the given class, with untransformed fields set to given values.\n        \"\"\"\n        for field_name, annotation in fields.items():\n            if field_name not in kwargs:\n                kwargs[field_name] = getattr(annotation.type, \"empty\", lambda: None)()\n        return cls(**kwargs)\n\n    setattr(clz, \"empty\", classmethod(empty_fn))\n    return clz", "def empty_impl(clz):\n    if \"__dataclass_fields__\" not in clz.__dict__:\n        raise TypeError(\"class `{}` is not a dataclass\".format(clz.__name__))\n\n    fields = clz.__dict__[\"__dataclass_fields__\"]\n\n    def empty_fn(cls, /, **kwargs):\n        \"\"\"\n        Create an empty instance of the given class, with untransformed fields set to given values.\n        \"\"\"\n        for field_name, annotation in fields.items():\n            if field_name not in kwargs:\n                kwargs[field_name] = getattr(annotation.type, \"empty\", lambda: None)()\n        return cls(**kwargs)\n\n    setattr(clz, \"empty\", classmethod(empty_fn))\n    return clz", "\n\ndef replace_impl(clz):\n    if \"__dataclass_fields__\" not in clz.__dict__:\n        raise TypeError(\"class `{}` is not a dataclass\".format(clz.__name__))\n\n    fields = clz.__dict__[\"__dataclass_fields__\"]\n\n    def replace_fn(self, /, **kwargs) -> Type[clz]:\n        for k in kwargs.keys():\n            if k not in fields:\n                raise RuntimeError(\"class `{}` does not have a field with name '{}'\".format(clz.__name__, k))\n        ret = dataclasses.replace(self, **kwargs)\n        return ret\n\n    setattr(clz, \"replace\", replace_fn)\n    return clz", "\n\n@empty_impl\n@dataclass\nclass OccupancyDensityGrid:\n    # float32, full-precision density values\n    density: jax.Array\n    # bool, a non-compact representation of the occupancy bitfield\n    occ_mask: jax.Array\n    # uint8, each bit is an occupancy value of a grid cell\n    occupancy: jax.Array\n\n    # uint32, indices of the grids that are alive (trainable)\n    alive_indices: jax.Array\n\n    # list of `int`s, upper bound of each cascade\n    alive_indices_offset: List[int]=struct.field(pytree_node=False)\n\n    @classmethod\n    def create(cls, cascades: int, grid_resolution: int=128):\n        \"\"\"\n        Inputs:\n            cascades: number of cascades, paper: \ud835\udc3e = 1 for all synthetic NeRF scenes (single grid)\n                      and \ud835\udc3e \u2208 [1, 5] for larger real-world scenes (up to 5 grids, depending on scene\n                      size)\n            grid_resolution: resolution of the occupancy grid, the NGP paper uses 128.\n\n        Example usage:\n            ogrid = OccupancyDensityGrid.create(cascades=5, grid_resolution=128)\n        \"\"\"\n        G3 = grid_resolution**3\n        n_grids = cascades * G3\n        occupancy = 255 * jnp.ones(\n            shape=(n_grids // 8,),  # each bit is an occupancy value\n            dtype=jnp.uint8,\n        )\n        density = jnp.zeros(\n            shape=(n_grids,),\n            dtype=jnp.float32,\n        )\n        occ_mask = jnp.zeros(\n            shape=(n_grids,),\n            dtype=jnp.bool_,\n        )\n        return cls(\n            density=density,\n            occ_mask=occ_mask,\n            occupancy=occupancy,\n            alive_indices=jnp.arange(n_grids, dtype=jnp.uint32),\n            alive_indices_offset=np.cumsum([0] + [G3] * cascades).tolist(),\n        )\n\n    def mean_density_up_to_cascade(self, cas: int) -> float | jax.Array:\n        return self.density[self.alive_indices[:self.alive_indices_offset[cas]]].mean()", "\n\n@dataclass\nclass Camera:\n    # resolutions\n    width: int=struct.field(pytree_node=False)\n    height: int=struct.field(pytree_node=False)\n\n    # focal length\n    fx: float=struct.field(pytree_node=False)\n    fy: float=struct.field(pytree_node=False)\n\n    # principal point\n    cx: float=struct.field(pytree_node=False)\n    cy: float=struct.field(pytree_node=False)\n\n    near: float=struct.field(pytree_node=False)\n\n    # distortion parameters\n    k1: float=struct.field(default=0.0, pytree_node=False)\n    k2: float=struct.field(default=0.0, pytree_node=False)\n    k3: float=struct.field(default=0.0, pytree_node=False)\n    k4: float=struct.field(default=0.0, pytree_node=False)\n    p1: float=struct.field(default=0.0, pytree_node=False)\n    p2: float=struct.field(default=0.0, pytree_node=False)\n\n    model: CameraModelType=struct.field(default=\"OPENCV\", pytree_node=False)\n\n    @property\n    def has_distortion(self) -> bool:\n        return (\n            True\n            or self.k1 != 0.0\n            or self.k2 != 0.0\n            or self.k3 != 0.0\n            or self.k4 != 0.0\n            or self.p1 != 0.0\n            or self.p2 != 0.0\n        )\n\n    @property\n    def _type(self) -> Literal[\"PERSPECTIVE\", \"FISHEYE\"]:\n        if \"fisheye\" in self.model.lower():\n            return \"FISHEYE\"\n        else:\n            return \"PERSPECTIVE\"\n\n    @property\n    def n_pixels(self) -> int:\n        return self.height * self.width\n\n    @property\n    def K_numpy(self) -> np.ndarray:\n        return np.asarray([\n            [self.fx,      0., self.cx],\n            [     0., self.fy, self.cy],\n            [     0.,      0.,      1.]\n        ])\n\n    @property\n    def K(self) -> jax.Array:\n        return jnp.asarray(self.K_numpy)\n\n    @classmethod\n    def from_colmap_txt(cls, txt_path: str | Path) -> \"Camera\":\n        \"\"\"Initialize a camera from a colmap's TXT format model.\n        References of camera parameters from colmap (search for `InitializeParamsInfo`):\n            <https://github.com/colmap/colmap/blob/fac2fa6217a1f5498830769d64861b54c67009dc/src/colmap/base/camera_models.h#L778>\n\n        Example usage:\n            cam = PinholeCamera.from_colmap_txt(\"path/to/txt\")\n        \"\"\"\n        with open(txt_path, \"r\") as f:\n            lines = f.readlines()\n        cam_line = lines[-1].strip()\n        cam_desc = cam_line.split()\n        _front = 0\n        def next_descs(cnt: int) -> Tuple[str, ...]:\n            nonlocal _front\n            rear = _front + cnt\n            ret = cam_desc[_front:rear]\n            _front = rear\n            return ret[0] if cnt == 1 else ret\n        assert int(next_descs(1)) == 1, \"creating scenes with multiple cameras is not supported\"\n        camera_model: CameraModelType = next_descs(1)\n        width, height = next_descs(2)\n        k1, k2, k3, k4, p1, p2 = [0.] * 6\n        if camera_model == \"SIMPLE_PINHOLE\":\n            f, cx, cy = next_descs(3)\n            fx = fy = f\n        elif camera_model == \"SIMPLE_RADIAL\":\n            f, cx, cy, k1 = next_descs(4)\n            fx = fy = f\n        elif camera_model == \"RADIAL\":\n            f, cx, cy, k1, k2 = next_descs(5)\n            fx = fy = f\n        else:\n            fx, fy, cx, cy = next_descs(4)\n            if camera_model == \"PINHOLE\":\n                pass\n            elif camera_model == \"OPENCV\":\n                k1, k2, p1, p2 = next_descs(4)\n            elif camera_model == \"OPENCV_FISHEYE\":\n                k1, k2, k3, k4 = next_descs(4)\n            else:\n                assert_never(camera_model)\n        return cls(\n            width=int(width),\n            height=int(height),\n            fx=float(fx),\n            fy=float(fy),\n            cx=float(cx),\n            cy=float(cy),\n            near=0.,\n            k1=float(k1),\n            k2=float(k2),\n            k3=float(k3),\n            k4=float(k4),\n            p1=float(p1),\n            p2=float(p2),\n        )\n\n    def scale_resolution(self, scale: int | float) -> \"Camera\":\n        return self.replace(\n            width=int(self.width * scale),\n            height=int(self.height * scale),\n            fx=self.fx * scale,\n            fy=self.fy * scale,\n            cx=self.cx * scale,\n            cy=self.cy * scale,\n        )\n\n    def distort(self, x: jax.Array, y: jax.Array) -> jax.Array:\n        \"\"\"Computes distorted coords.\n        REF:\n            * <https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html>\n            * <https://en.wikipedia.org/wiki/Distortion_%28optics%29>\n            * <https://docs.opencv.org/3.4/db/d58/group__calib3d__fisheye.html>\n\n        Inputs:\n            x, y `float`: normalized undistorted coordinates\n\n        Returns:\n            x, y `float`: distorted coordinates\n        \"\"\"\n        k1, k2, k3, k4, = self.k1, self.k2, self.k3, self.k4\n        xx, yy = jnp.square(x), jnp.square(y)\n        rr = xx + yy\n        if self._type == \"FISHEYE\":\n            r = jnp.sqrt(rr)\n            theta = jnp.arctan(r)\n            thth = theta * theta\n            thetad = theta * (1. + thth * (k1 + thth * (k2 + thth * (k3 + thth * k4))))\n            dist = thetad / r - 1.\n            dx, dy = (\n                jnp.where(r < 1e-15, 0., x * dist),\n                jnp.where(r < 1e-15, 0., y * dist),\n            )\n        else:\n            radial = rr * (k1 + rr * (k2 + rr * (k3 + rr * k4)))\n\n            # radial distort\n            dx, dy = x * radial, y * radial\n\n            p1, p2 = self.p1, self.p2\n\n            # tangential distort\n            xy = x * y\n            dx += 2 * p1 * xy + p2 * (rr + 2 * xx)\n            dx += 2 * p2 * xy + p1 * (rr + 2 * xx)\n\n        return x + dx, y + dy\n\n    def undistort(self, x: jax.Array, y: jax.Array, eps: float=1e-3, max_iterations: int=10) -> jax.Array:\n        \"\"\"Computes undistorted coords.\n        REF:\n            * <https://github.com/google-research/multinerf/blob/b02228160d3179300c7d499dca28cb9ca3677f32/internal/camera_utils.py#L477-L509>\n            * <https://github.com/nerfstudio-project/nerfstudio/blob/004d8ca9d24b294b1877d4d5599879c4ce812bc7/nerfstudio/cameras/camera_utils.py#L411-L448>\n\n        Inputs:\n            x, y `float`: normalized (x_normalized = (x + cx) / fx) distorted coordinates\n            eps `float`: epsilon for the convergence\n            max_iterations `int`: maximum number of iterations to perform\n\n        Returns:\n            x, y `float`: undistorted coordinates\n        \"\"\"\n        # the original distorted coordinates\n        xd, yd = x.copy(), y.copy()\n\n        @jax.jit\n        def compute_residual_and_jacobian(x, y) -> Tuple[Tuple[jax.Array, jax.Array], Tuple[jax.Array, jax.Array, jax.Array, jax.Array]]:\n            \"\"\"Auxiliary function of radial_and_tangential_undistort() that computes residuals and\n            jacobians.\n            REF:\n                * <https://github.com/google-research/multinerf/blob/b02228160d3179300c7d499dca28cb9ca3677f32/internal/camera_utils.py#L427-L474>\n                * <https://github.com/nerfstudio-project/nerfstudio/blob/004d8ca9d24b294b1877d4d5599879c4ce812bc7/nerfstudio/cameras/camera_utils.py#L345-L407>\n\n            Inputs:\n                x: The updated x coordinates.\n                y: The updated y coordinates.\n\n            Returns:\n                The residuals (fx, fy) and jacobians (fx_x, fx_y, fy_x, fy_y).\n            \"\"\"\n            k1, k2, k3, k4 = self.k1, self.k2, self.k3, self.k4\n            p1, p2 = self.p1, self.p2\n            # let r(x, y) = x^2 + y^2;\n            #     d(x, y) = 1 + k1 * r(x, y) + k2 * r(x, y) ^2 + k3 * r(x, y)^3 +\n            #                   k4 * r(x, y)^4;\n            r = x * x + y * y\n            d = 1.0 + r * (k1 + r * (k2 + r * (k3 + r * k4)))\n\n            # The perfect projection is:\n            # xd = x * d(x, y) + 2 * p1 * x * y + p2 * (r(x, y) + 2 * x^2);\n            # yd = y * d(x, y) + 2 * p2 * x * y + p1 * (r(x, y) + 2 * y^2);\n            #\n            # Let's define\n            #\n            # fx(x, y) = x * d(x, y) + 2 * p1 * x * y + p2 * (r(x, y) + 2 * x^2) - xd;\n            # fy(x, y) = y * d(x, y) + 2 * p2 * x * y + p1 * (r(x, y) + 2 * y^2) - yd;\n            #\n            # We are looking for a solution that satisfies\n            # fx(x, y) = fy(x, y) = 0;\n            fx = d * x + 2 * p1 * x * y + p2 * (r + 2 * x * x) - xd\n            fy = d * y + 2 * p2 * x * y + p1 * (r + 2 * y * y) - yd\n\n            # Compute derivative of d over [x, y]\n            d_r = k1 + r * (2.0 * k2 + r * (3.0 * k3 + r * 4.0 * k4))\n            d_x = 2.0 * x * d_r\n            d_y = 2.0 * y * d_r\n\n            # Compute derivative of fx over x and y.\n            fx_x = d + d_x * x + 2.0 * p1 * y + 6.0 * p2 * x\n            fx_y = d_y * x + 2.0 * p1 * x + 2.0 * p2 * y\n\n            # Compute derivative of fy over x and y.\n            fy_x = d_x * y + 2.0 * p2 * y + 2.0 * p1 * x\n            fy_y = d + d_y * y + 2.0 * p2 * x + 6.0 * p1 * y\n\n            return (fx, fy), (fx_x, fx_y, fy_x, fy_y)\n\n        for _ in range(max_iterations):\n            (fx, fy), (fx_x, fx_y, fy_x, fy_y) = compute_residual_and_jacobian(x, y)\n            denominator = fy_x * fx_y - fx_x * fy_y\n            x_numerator = fx * fy_y - fy * fx_y\n            y_numerator = fy * fx_x - fx * fy_x\n            step_x = jnp.where(jnp.abs(denominator) > eps, x_numerator / denominator, jnp.zeros_like(denominator))\n            step_y = jnp.where(jnp.abs(denominator) > eps, y_numerator / denominator, jnp.zeros_like(denominator))\n\n            x = x + step_x\n            y = y + step_y\n\n        return x, y\n\n    def make_ray_directions_from_pixel_coordinates(\n        self,\n        x: jax.Array,\n        y: jax.Array,\n        use_pixel_center: bool,\n    ) -> jax.Array:\n        \"\"\"Given distorted unnormalized pixel coordinates, generate a ray direction for each of them\n\n        Inputs:\n            x, y `uint32` `[N]`: unnormalized pixel coordinates\n\n        Returns:\n            dirs `float` `[N, 3]`: directions that have taken distortion into account\n        \"\"\"\n        chex.assert_type([x, y], jnp.uint32)\n        chex.assert_rank([x, y], 1)\n        chex.assert_equal_shape([x, y])\n        pixel_offset = 0.5 if use_pixel_center else 0.0\n        x, y, z = (  # in CV coordinates, axes are flipped later\n            ((x + pixel_offset) - self.cx) / self.fx,\n            ((y + pixel_offset) - self.cy) / self.fy,\n            jnp.ones_like(x),\n        )\n        if self.has_distortion:\n            x, y = self.undistort(x, y)\n        if self._type == \"FISHEYE\":\n            theta = jnp.sqrt(jnp.square(x) + jnp.square(y))\n            theta = jnp.clip(theta, 0., jnp.pi)\n            co, si = jnp.cos(theta), jnp.sin(theta)\n            x, y, z = (\n                x * si / theta,\n                y * si / theta,\n                z * co,\n            )\n        dirs = jnp.stack([x, y, z], axis=-1)\n\n        # flip axis from CV coordinates to CG coordinates\n        dirs = dirs @ jnp.diag(jnp.asarray([1., -1., -1.]))\n\n        return dirs / jnp.linalg.norm(dirs, axis=-1, keepdims=True)", "\n\n@empty_impl\n@dataclass\nclass RayMarchingOptions:\n    # for calculating the length of a minimal ray marching step, the NGP paper uses 1024 (appendix\n    # E.1)\n    diagonal_n_steps: int\n\n    # whether to fluctuate the first sample along the ray with a tiny perturbation\n    perturb: bool\n\n    # resolution for the auxiliary density/occupancy grid, the NGP paper uses 128 (appendix E.2)\n    density_grid_res: int", "\n\n@empty_impl\n@dataclass\nclass RenderingOptions:\n    # background color for transparent parts of the image, has no effect if `random_bg` is True\n    bg: RGBColor\n\n    # ignore `bg` specification and use random color for transparent parts of the image\n    random_bg: bool", "\n\n@empty_impl\n@replace_impl\n@pydantic.dataclasses.dataclass(frozen=True)\nclass CameraOverrideOptions:\n    width: int | None=None\n    height: int | None=None\n    focal: float | None=None\n    near: float | None=None\n    k1: float | None=None\n    k2: float | None=None\n    k3: float | None=None\n    k4: float | None=None\n    p1: float | None=None\n    p2: float | None=None\n    model: CameraModelType | None=None\n    distortion: bool=True\n\n    @property\n    def fx(self) -> float:\n        return self.focal\n    @property\n    def fy(self) -> float:\n        return self.focal\n    @property\n    def cx(self) -> float:\n        return self.width / 2.\n    @property\n    def cy(self) -> float:\n        return self.height / 2.\n\n    def __post_init__(self):\n        if self.width is None and self.height is None:\n            return\n        if int(self.width is not None) + int(self.height is not None) == 1:\n            side = self.width if self.width is not None else self.height\n            self.__init__(\n                width=side,\n                height=side,\n                focal=self.focal,\n                near=self.near,\n                k1=self.k1,\n                k2=self.k2,\n                k3=self.k3,\n                k4=self.k4,\n                p1=self.p1,\n                p2=self.p2,\n                model=self.model,\n                distortion=self.distortion,\n            )\n        assert self.width > 0 and self.height > 0\n        if self.focal is None:\n            self.__init__(\n                width=self.width,\n                height=self.height,\n                focal=min(self.width, self.height),\n                near=self.near,\n                k1=self.k1,\n                k2=self.k2,\n                k3=self.k3,\n                k4=self.k4,\n                p1=self.p1,\n                p2=self.p2,\n                model=self.model,\n                distortion=self.distortion,\n            )\n\n    def update_camera(self, camera: Camera | None=None) -> Camera:\n        def try_override(name: str) -> int | float | CameraModelType:\n            return getattr(self, name) or getattr(camera, name)\n        def try_override_if_has_distortion_else_zero(name: str) -> float:\n            return try_override(name) if self.distortion else 0.\n        width, height, fx, fy, cx, cy, near, model = map(\n            try_override,\n            [\"width\", \"height\", \"fx\", \"fy\", \"cx\", \"cy\", \"near\", \"model\"],\n        )\n        k1, k2, k3, k4, p1, p2 = map(\n            try_override_if_has_distortion_else_zero,\n            [\"k1\", \"k2\", \"k3\", \"k4\", \"p1\", \"p2\"],\n        )\n        return camera.replace(\n            width=width,\n            height=height,\n            fx=fx,\n            fy=fy,\n            cx=cx,\n            cy=cy,\n            near=near,\n            k1=k1,\n            k2=k2,\n            k3=k3,\n            k4=k4,\n            p1=p1,\n            p2=p2,\n            model=model,\n        )\n\n    @property\n    def enabled(self) -> bool:\n        return any(map(\n            lambda name: getattr(self, name) is not None,\n            [\"width\", \"height\", \"focal\", \"near\", \"model\"]\n            + [\"k1\", \"k2\", \"k3\", \"k4\", \"p1\", \"p2\"],\n        ))", "\n\n@empty_impl\n@replace_impl\n@pydantic.dataclasses.dataclass(frozen=True)\nclass SceneOptions:\n    # images with sharpness lower than this value will be discarded\n    sharpness_threshold: float\n\n    # scale input images in case they are too large, camera intrinsics are also scaled to match the\n    # updated image resolution.\n    resolution_scale: float\n\n    camera_near: float\n\n    # maximum GPU memory to consume in MB, the pixels are reloaded into GPU memory before each epoch\n    # if the scene has more than this number of pixels, otherwise all pixels are loaded once\n    max_mem_mbytes: int\n\n    # overrides `aabb_scale` in transforms.json\n    bound: float | None=None\n\n    # overrides `up` in transforms.json\n    up: Tuple[float, float, float] | None=None\n\n    def __post_init__(self):\n        assert 0 <= self.resolution_scale <= 1, (\n            \"resolution_scale must be in range [0, 1], got {}\".format(self.resolution_scale)\n        )\n\n    @property\n    def up_unitvec(self) -> Tuple[float, float, float] | None:\n        if self.up is None:\n            return None\n        up = np.asarray(self.up)\n        up = up / np.linalg.norm(up)\n        return tuple(up.tolist())", "\n\n@dataclass\nclass RigidTransformation:\n    # [3, 3] rotation matrix\n    rotation: jax.Array\n\n    # [3] translation vector\n    translation: jax.Array\n\n    def __post_init__(self):\n        chex.assert_shape([self.rotation, self.translation], [(3, 3), (3,)])", "\n\n@replace_impl\n@pydantic.dataclasses.dataclass(frozen=True)\nclass TransformJsonFrame:\n    file_path: Path | None\n    transform_matrix: Matrix4x4\n\n    # unused, kept for compatibility with the original nerf_synthetic dataset\n    rotation: float=0.0\n\n    # unused, kept for compatibility with instant-ngp\n    sharpness: float=1e5\n\n    @property\n    def transform_matrix_numpy(self) -> np.ndarray:\n        return np.asarray(self.transform_matrix)\n\n    @property\n    def transform_matrix_jax_array(self) -> jax.Array:\n        return jnp.asarray(self.transform_matrix)\n\n    def rotate_world_up(self, up: Tuple[float, float, float] | np.ndarray) -> \"TransformJsonFrame\":\n        def rotmat(a, b):\n            \"copied from NVLabs/instant-ngp/scripts/colmap2nerf.py\"\n            a, b = a / np.linalg.norm(a), b / np.linalg.norm(b)\n            v = np.cross(a, b)\n            c = np.dot(a, b)\n            # handle exception for the opposite direction input\n            if c < -1 + 1e-10:\n                return rotmat(a + np.random.uniform(-1e-2, 1e-2, 3), b)\n            s = np.linalg.norm(v)\n            kmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n            return np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2 + 1e-10))\n        up = np.asarray(up)\n        R = rotmat(up, [0, 0, 1])\n        R = np.pad(R, [0, 1])\n        R[-1, -1] = 1\n        new_transform_matrix = R @ self.transform_matrix_numpy\n        return self.replace(\n            transform_matrix=new_transform_matrix.tolist(),\n        )\n\n    def scale_camera_positions(self, scale: float) -> \"TransformJsonFrame\":\n        new_transform_matrix = self.transform_matrix_numpy\n        new_transform_matrix[:3, 3] *= scale * 2\n        return self.replace(\n            transform_matrix=new_transform_matrix.tolist(),\n        )", "\n\n@replace_impl\n@pydantic.dataclasses.dataclass(frozen=True)\nclass TransformJsonBase:\n    frames: Tuple[TransformJsonFrame, ...]\n\n    # scene's bound, the name `aabb_scale` is for compatibility with instant-ngp (note that\n    # instant-ngp requires this value to be a power of 2, other than that a value that can work with\n    # instant-ngp will work with this code base as well).\n    aabb_scale: float=dataclasses.field(default=1., kw_only=True)\n\n    # scale camera's translation vectors by this factor while loading (default value taken from\n    # NVLabs/instant-ngp/include/neural-graphics-primitives/nerf_loader.h), since current\n    # implementation (this codebase) represents the scene inside a 2^3 cube centered at origin, to\n    # achieve the same scene scale as that of NVLabs/instant-ngp while using the same\n    # transform*.json files, the camera translation vectors will be scaled by 2 time this value.\n    # I.e. if the transform*.json specifies `\"scale\": 0.3`, loaded cameras' translation vectors will\n    # be scaled by `0.6`.  See `utils.types.TransformJsonFrame.scale_camera_positions` for details.\n    # NOTE: this value does not affect scene's bounding box\n    scale: float=dataclasses.field(default=1/3, kw_only=True)\n\n    bg: bool=dataclasses.field(default=False, kw_only=True)\n\n    up: Tuple[float, float, float]=dataclasses.field(default=(0, 0, 1), kw_only=True)\n\n    n_extra_learnable_dims: int=dataclasses.field(default=0, kw_only=True)\n\n    def rotate_world_up(self) -> \"TransformJsonBase\":\n        return self.replace(\n            frames=tuple(map(lambda f: f.rotate_world_up(self.up), self.frames)),\n            up=(0., 0., 1.),  # so that this operation is idempotent\n        )\n\n    def scale_camera_positions(self) -> \"TransformJsonBase\":\n        return self.replace(\n            frames=tuple(map(lambda f: f.scale_camera_positions(self.scale), self.frames)),\n        )\n\n    def merge(self, rhs: \"TransformJsonBase\") -> \"TransformJsonBase\":\n        if rhs is None:\n            return self\n        # sanity checks, transforms to be merged should have same camera intrinsics\n        assert isinstance(rhs, type(self))\n        assert all(\n            getattr(rhs, attr_name) == getattr(self, attr_name)\n            for attr_name in (\n                field\n                for field in self.__dataclass_fields__\n                if field != \"frames\"\n            )\n        )\n        return self.replace(frames=self.frames + rhs.frames)\n\n    def make_absolute(self, parent_dir: Path | str) -> \"TransformJsonBase\":\n        parent_dir = Path(parent_dir)\n        return self.replace(\n            frames=tuple(map(\n                lambda f: f.replace(\n                    file_path=(\n                        f.file_path\n                        if f.file_path.is_absolute()\n                        else parent_dir.joinpath(f.file_path).absolute().as_posix()\n                    ),\n                ),\n                self.frames,\n            )),\n        )\n\n    def as_json(self, /, indent: int=2) -> str:\n        d = dataclasses.asdict(self)\n        d = jax.tree_util.tree_map(lambda x: x.as_posix() if isinstance(x, Path) else x, d)\n        return json.dumps(d, indent=indent)\n\n    @classmethod\n    def from_json(cls, jsonstr: str) -> \"TransformJsonBase\":\n        return cls(**json.loads(jsonstr))\n\n    def save(self, path: str | Path) -> None:\n        path = Path(path)\n        path.write_text(self.as_json())\n\n    @classmethod\n    def load(cls, path: str | Path):\n        path = Path(path)\n        return cls.from_json(path.read_text())", "\n\n@replace_impl\n@pydantic.dataclasses.dataclass(frozen=True)\nclass TransformJsonNeRFSynthetic(TransformJsonBase):\n    camera_angle_x: float\n\n\n@replace_impl\n@pydantic.dataclasses.dataclass(frozen=True)\nclass TransformJsonNGP(TransformJsonBase):\n    fl_x: float\n    fl_y: float\n    cx: float\n    cy: float\n\n    w: int\n    h: int\n\n    k1: float=0.\n    k2: float=0.\n    k3: float=0.\n    k4: float=0.\n    p1: float=0.\n    p2: float=0.\n\n    camera_model: CameraModelType=\"OPENCV\"", "@replace_impl\n@pydantic.dataclasses.dataclass(frozen=True)\nclass TransformJsonNGP(TransformJsonBase):\n    fl_x: float\n    fl_y: float\n    cx: float\n    cy: float\n\n    w: int\n    h: int\n\n    k1: float=0.\n    k2: float=0.\n    k3: float=0.\n    k4: float=0.\n    p1: float=0.\n    p2: float=0.\n\n    camera_model: CameraModelType=\"OPENCV\"", "\n\n@replace_impl\n@pydantic.dataclasses.dataclass(frozen=True)\nclass SceneCreationOptions:\n    # given that the cameras' average distance to the origin is (4.0 * `camera_scale`), what would\n    # the scene's bound be?\n    bound: float\n\n    # `Sequntial` for continuous frames, `Exhaustive` for all possible pairs\n    matcher: ColmapMatcherType\n\n    camera_model: CameraModelType=\"OPENCV\"\n\n    # upon loading the created scene during training/inference, scale the camera positions with this\n    # factor\n    camera_scale: float=dataclasses.field(default=1/3)\n\n    # whether to enable background model\n    bg: bool=dataclasses.field(default=False)\n\n    # dimension of NeRF-W-style per-image appearance embeddings, set to 0 to disable\n    n_extra_learnable_dims: int=dataclasses.field(default=16)\n\n    # whether undistort the images and write them to disk, so that the camera in transforms.json is\n    # a pinhole camera\n    undistort: bool=dataclasses.field(default=False)", "\n\n@dataclass\nclass ImageMetadata:\n    H: int\n    W: int\n    xys: jax.Array  # int,[H*W, 2]: original integer coordinates in range [0, W] for x and [0, H] for y\n    uvs: jax.Array  # float,[H*W, 2]: normalized coordinates in range [0, 1]\n    rgbs: jax.Array  # float,[H*W, 3]: normalized rgb values in range [0, 1]\n", "\n\n@dataclass\nclass OrbitTrajectoryOptions:\n    # cameras' distance to the orbiting axis\n    radius: float=1.8\n\n    # lowest height of generated trajectory\n    low: float=0.0\n\n    # highest height of generated trajectory\n    high: float=1.3\n\n    # how many frames should be rendered per orbit\n    n_frames_per_orbit: int=144\n\n    n_orbit: int=2\n\n    # all orbiting cameras will look at this point\n    centroid: Tuple[float, float, float]=(0., 0., 0.2)\n\n    @property\n    def n_frames(self) -> int:\n        return self.n_frames_per_orbit * self.n_orbit", "\n\n# scene's metadata (computed from SceneOptions and TransformJson)\n@dataclass\nclass SceneMeta:\n    # half width of axis-aligned bounding-box, i.e. aabb's width is `bound*2`\n    bound: float\n\n    # whether the scene should be modeled with a background that is not part of the scene geometry\n    bg: bool\n\n    # the camera model used to render this scene\n    camera: Camera\n\n    n_extra_learnable_dims: int\n\n    frames: Tuple[TransformJsonFrame, ...]=struct.field(pytree_node=False)\n\n    @property\n    def cascades(self) -> int:\n        return max(1, int(1 + math.ceil(math.log2(self.bound))))\n\n    @property\n    def n_pixels(self) -> float:\n        return self.camera.n_pixels * len(self.frames)\n\n    @property\n    def sharpness_range(self) -> float:\n        return functools.reduce(\n            lambda prev, frame: (min(prev[0], frame.sharpness), max(prev[1], frame.sharpness)),\n            self.frames,\n            (1e9, -1e9),\n        )\n\n    # this is the same thing as `dt_gamma` in ashawkey/torch-ngp\n    @property\n    def stepsize_portion(self) -> float:\n        if self.bound >= 64:\n            return 1/128\n        elif self.bound >= 16:\n            return 6e-3\n        elif self.bound >= 4:\n            return 5e-3\n        elif self.bound > 1:\n            return 1/256\n        else:\n            return 0\n\n    def make_frames_with_orbiting_trajectory(\n        self,\n        opts: OrbitTrajectoryOptions,\n    ) -> \"SceneMeta\":\n        assert isinstance(opts, OrbitTrajectoryOptions)\n\n        thetas = np.linspace(0, opts.n_orbit * 2 * np.pi, opts.n_frames + 1)[:-1]\n        xs = np.cos(thetas) * opts.radius\n        ys = np.sin(thetas) * opts.radius\n        elevation_range = opts.high - opts.low\n        mid_elevation = opts.low + .5 * elevation_range\n        zs = mid_elevation + .5 * elevation_range * np.sin(np.linspace(0, 2 * np.pi, opts.n_frames + 1)[:-1])\n        xyzs = np.stack([xs, ys, zs]).T\n\n        view_dirs = (jnp.asarray(opts.centroid) - xyzs) / np.linalg.norm(xyzs, axis=-1, keepdims=True)\n        right_dirs = np.stack([-np.sin(thetas), np.cos(thetas), np.zeros_like(thetas)]).T\n        up_dirs = -np.cross(view_dirs, right_dirs)\n        up_dirs = up_dirs / np.linalg.norm(up_dirs, axis=-1, keepdims=True)\n\n        rot_cws = np.concatenate([right_dirs[..., None], up_dirs[..., None], -view_dirs[..., None]], axis=-1)\n\n        frames = tuple(map(\n            lambda rot_cw, t_cw: TransformJsonFrame(\n                file_path=None,\n                transform_matrix=np.concatenate(\n                    [np.concatenate([rot_cw, t_cw.reshape(3, 1)], axis=-1), np.ones((1, 4))],\n                    axis=0,\n                ).tolist(),\n            ),\n            rot_cws,\n            xyzs,\n        ))\n\n        return self.replace(frames=frames)", "\n\n@dataclass\nclass SceneData:\n    @dataclass\n    class ViewData:\n        width: int\n        height: int\n        transform: RigidTransformation\n        file: Path=struct.field(pytree_node=False)\n\n        @property\n        def image_pil(self) -> Image.Image:\n            image = Image.open(self.file)\n            image = image.resize((self.width, self.height), resample=Image.LANCZOS)\n            return image\n\n        @property\n        def image_rgba_u8(self) -> jax.Array:\n            image = np.asarray(self.image_pil)\n            if image.shape[-1] == 1:\n                image = np.concatenate([image] * 3 + [255 * np.ones_like(image[..., :1])], axis=-1)\n            elif image.shape[-1] == 3:\n                image = np.concatenate([image, 255 * np.ones_like(image[..., :1])], axis=-1)\n            chex.assert_axis_dimension(image, -1, 4)\n            return image\n\n        # float, [H*W, 4]: rgba values of type uint8\n        @property\n        def rgba_u8(self) -> jax.Array:\n            return self.image_rgba_u8.reshape(-1, 4)\n\n    meta: SceneMeta\n\n    # maximum GPU memory to consume in MB, the pixels are reloaded into GPU memory before each epoch\n    # if the scene has more than this number of pixels, otherwise all pixels are loaded once\n    max_mem_mbytes: int\n\n    # uint32, [n_pixels]\n    _view_indices: jax.Array | None=None\n\n    # uint32, [n_pixels]\n    _pixel_indices: jax.Array | None=None\n\n    # uint8, [n_pixels, 4]\n    rgbas_u8: jax.Array | None=None\n\n    def _free(self):\n        backend = jax.lib.xla_bridge.get_backend()\n        if self._view_indices is not None: backend.buffer_from_pyval(self._view_indices).delete()\n        if self._pixel_indices is not None: backend.buffer_from_pyval(self._pixel_indices).delete()\n        if self.rgbas_u8 is not None: backend.buffer_from_pyval(self.rgbas_u8).delete()\n        backend.buffer_from_pyval(self.transforms).delete()\n        jax.lib.xla_bridge.get_backend().defragment()\n\n    @functools.cached_property\n    def all_views(self) -> Tuple[ViewData, ...]:\n        return tuple(map(\n            lambda frame: self.ViewData(\n                width=self.meta.camera.width,\n                height=self.meta.camera.height,\n                transform=RigidTransformation(\n                    rotation=frame.transform_matrix_numpy[:3, :3],\n                    translation=frame.transform_matrix_numpy[:3, 3],\n                ),\n                file=frame.file_path,\n            ),\n            self.meta.frames,\n        ))\n\n    # float, [n_views, 9+3]\n    @functools.cached_property\n    def transforms(self) -> jax.Array:\n        return jnp.stack(list(ThreadPoolExecutor().map(\n            lambda view: jnp.concatenate([\n                view.transform.rotation.ravel(),\n                view.transform.translation,\n            ]),\n            self.all_views,\n        )))\n\n    def _should_load_all_pixels(self, /, max_mem_mbytes: int) -> bool:\n        n_bytes = max_mem_mbytes * 1024 * 1024\n        required_total_mem_bytes = 4 * self.meta.n_pixels\n        return n_bytes >= required_total_mem_bytes\n\n    @property\n    def load_all_pixels(self) -> bool:\n        return self._should_load_all_pixels(max_mem_mbytes=self.max_mem_mbytes)\n\n    @property\n    def n_views(self) -> int:\n        return len(self.all_views)\n\n    def _calculate_num_pixels(self, /, max_mem_mbytes: int) -> int:\n        if self._should_load_all_pixels(max_mem_mbytes=max_mem_mbytes):\n            return self.meta.n_pixels\n        else:\n            # Dividing by 3 because also need to keep track of view indices and pixel indices, each\n            # consuming the same amount of memory as rgba_u8.\n            n_bytes = max_mem_mbytes * 1024 * 1024\n            return int(n_bytes * .33 / 4)\n\n    @property\n    def n_pixels(self) -> int:\n        return self._calculate_num_pixels(max_mem_mbytes=self.max_mem_mbytes)\n\n    def get_view_indices(self, perm: jax.Array) -> jax.Array:\n        if self._view_indices is not None:\n            return self._view_indices[perm]\n        else:\n            return jnp.floor_divide(perm, self.meta.camera.n_pixels)\n\n    def get_pixel_indices(self, perm: jax.Array) -> jax.Array:\n        if self._pixel_indices is not None:\n            return self._pixel_indices[perm]\n        else:\n            return jnp.mod(perm, self.meta.camera.n_pixels)\n\n    def resample_pixels(self, /, KEY: jran.KeyArray, new_max_mem_mbytes: int | None=None) -> \"SceneData\":\n        load_all_pixels = self._should_load_all_pixels(new_max_mem_mbytes)\n        if (\n            (self.load_all_pixels and self.rgbas_u8 is not None)\n            and (new_max_mem_mbytes is None or load_all_pixels)\n        ):\n            return dataclasses.replace(self, max_mem_mbytes=new_max_mem_mbytes)\n        if new_max_mem_mbytes is None:\n            new_max_mem_mbytes = self.max_mem_mbytes\n\n        # free up GPU memory\n        self._free()\n        n_pixels = self._calculate_num_pixels(max_mem_mbytes=new_max_mem_mbytes)\n        make_progress_bar = functools.partial(\n            tqdm,\n            total=self.n_views,\n            desc=\"| {} {} ({:.2f}% of {}) training pixels\".format(\n                \"loading\"\n                if self._view_indices is None\n                else \"resampling\",\n                n_pixels,\n                n_pixels / self.meta.n_pixels * 100,\n                self.meta.n_pixels,\n            ),\n            bar_format=tqdm_format,\n        )\n\n        if load_all_pixels:\n            rgbas_u8 = jnp.concatenate(list(make_progress_bar(ThreadPoolExecutor().map(\n                lambda view: view.rgba_u8,\n                self.all_views,\n            ))))\n            return dataclasses.replace(\n                self,\n                max_mem_mbytes=new_max_mem_mbytes,\n                _view_indices=None,\n                _pixel_indices=None,\n                rgbas_u8=rgbas_u8,\n            )\n        else:\n            KEY, key_n, key_view_perm, key_pixel_idcs = jran.split(KEY, 4)\n            ns = jran.uniform(key_n, shape=(self.n_views - 1,), minval=7, maxval=13)\n            ns = ns / ns.sum()\n            ns = (ns * n_pixels / self.n_views * (self.n_views - 1)).astype(jnp.uint32)\n            ns = jnp.concatenate([ns, (n_pixels - ns.sum()) * jnp.ones_like(ns[:1])])\n            assert ns.sum() == n_pixels\n            sections, ns = jnp.cumsum(ns), ns.tolist()\n\n            pixel_idcs = jran.choice(\n                key=key_pixel_idcs,\n                a=self.meta.camera.n_pixels,\n                shape=(n_pixels,),\n                replace=True,\n            )\n            pixel_idcs_per_view = jnp.split(pixel_idcs, sections)\n\n            view_perm = jran.permutation(key=key_view_perm, x=self.n_views)\n            view_idcs = jnp.concatenate(list(ThreadPoolExecutor().map(\n                lambda vi, pidcs: vi * jnp.ones(pidcs.shape[0], dtype=jnp.uint32),\n                view_perm,\n                pixel_idcs_per_view,\n            )))\n\n            rgbas_u8 = jnp.concatenate(list(make_progress_bar(ThreadPoolExecutor().map(\n                lambda vi, pidcs: self.all_views[vi].rgba_u8[pidcs],\n                view_perm,\n                pixel_idcs_per_view,\n            ))))\n\n            return dataclasses.replace(\n                self,\n                max_mem_mbytes=new_max_mem_mbytes,\n                _view_indices=view_idcs,\n                _pixel_indices=pixel_idcs,\n                rgbas_u8=rgbas_u8,\n            )", "\n\n@dataclass\nclass RenderedImage:\n    bg: jax.Array\n    rgb: jax.Array\n    depth: jax.Array\n\n\n@empty_impl\nclass NeRFState(TrainState):\n    # WARN:\n    #   do not annotate fields with jax.Array as members with flax.truct.field(pytree_node=False),\n    #   otherwise weird issues happen, e.g. jax tracer leak, array-to-boolean conversion exception\n    #   while calling a jitted function with no helpful traceback.\n    ogrid: OccupancyDensityGrid\n\n    raymarch: RayMarchingOptions=struct.field(pytree_node=False)\n    render: RenderingOptions=struct.field(pytree_node=False)\n    scene_options: SceneOptions=struct.field(pytree_node=False)\n    scene_meta: SceneMeta=struct.field(pytree_node=False)\n\n    nerf_fn: Callable=struct.field(pytree_node=False)\n    bg_fn: Callable=struct.field(pytree_node=False)\n\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return super().create(apply_fn=None, *args, **kwargs)\n\n    def __post_init__(self):\n        assert self.apply_fn is None\n\n    def update_ogrid_density(\n        self,\n        KEY: jran.KeyArray,\n        cas: int,\n        update_all: bool,\n        max_inference: int,\n    ) -> \"NeRFState\":\n        G3 = self.raymarch.density_grid_res**3\n        cas_slice = slice(cas * G3, (cas + 1) * G3)\n        cas_alive_indices = self.ogrid.alive_indices[self.ogrid.alive_indices_offset[cas]:self.ogrid.alive_indices_offset[cas+1]]\n        aligned_indices = cas_alive_indices % G3  # values are in range [0, G3)\n        n_grids = aligned_indices.shape[0]\n\n        decay = .95\n        cas_occ_mask = self.ogrid.occ_mask[cas_slice]\n        cas_density_grid = self.ogrid.density[cas_slice].at[aligned_indices].set(self.ogrid.density[cas_slice][aligned_indices] * decay)\n\n        if update_all:\n            # During the first 256 training steps, we sample M = K * 128^{3} cells uniformly without\n            # repetition.\n            cas_updated_indices = aligned_indices\n        else:\n            M = max(1, n_grids // 2)\n            # The first M/2 cells are sampled uniformly among all cells.\n            KEY, key_firsthalf, key_secondhalf = jran.split(KEY, 3)\n            indices_firsthalf = jran.choice(\n                key=key_firsthalf,\n                a=aligned_indices,\n                shape=(max(1, M//2),),\n                replace=True,  # allow duplicated choices\n            )\n            # Rejection sampling is used for the remaining samples to restrict selection to cells\n            # that are currently occupied.\n            # NOTE: Below is just uniformly sampling the occupied cells, not rejection sampling.\n            cas_alive_occ_mask = cas_occ_mask[aligned_indices]\n            indices_secondhalf = jran.choice(\n                key=key_secondhalf,\n                a=aligned_indices,\n                shape=(max(1, M//2),),\n                replace=True,  # allow duplicated choices\n                p=cas_alive_occ_mask.astype(jnp.float32),  # only care about occupied grids\n            )\n            cas_updated_indices = jnp.concatenate([indices_firsthalf, indices_secondhalf])\n\n        coordinates = morton3d_invert(cas_updated_indices).astype(jnp.float32)\n        coordinates = coordinates / (self.raymarch.density_grid_res - 1) * 2 - 1  # in [-1, 1]\n        mip_bound = min(self.scene_meta.bound, 2**cas)\n        half_cell_width = mip_bound / self.raymarch.density_grid_res\n        coordinates *= mip_bound - half_cell_width  # in [-mip_bound+half_cell_width, mip_bound-half_cell_width]\n        # random point inside grid cells\n        KEY, key = jran.split(KEY, 2)\n        coordinates += jran.uniform(\n            key,\n            coordinates.shape,\n            coordinates.dtype,\n            minval=-half_cell_width,\n            maxval=half_cell_width,\n        )\n\n        new_densities = map(\n            lambda coords_part: jax.jit(self.nerf_fn)(\n                {\"params\": self.locked_params[\"nerf\"]},\n                coords_part,\n                None,\n                None,\n            )[0].ravel(),\n            jnp.array_split(jax.lax.stop_gradient(coordinates), max(1, n_grids // (max_inference))),\n        )\n        new_densities = jnp.concatenate(list(new_densities))\n\n        cas_density_grid = cas_density_grid.at[cas_updated_indices].set(\n            jnp.maximum(cas_density_grid[cas_updated_indices], new_densities)\n        )\n        new_ogrid = self.ogrid.replace(\n            density=self.ogrid.density.at[cas_slice].set(cas_density_grid),\n        )\n        return self.replace(ogrid=new_ogrid)\n\n    @jax.jit\n    def threshold_ogrid(self) -> \"NeRFState\":\n        mean_density = self.ogrid.mean_density_up_to_cascade(1)\n        density_threshold = jnp.minimum(self.density_threshold_from_min_step_size, mean_density)\n        occupied_mask, occupancy_bitfield = packbits(\n            density_threshold=density_threshold,\n            density_grid=self.ogrid.density,\n        )\n        new_ogrid = self.ogrid.replace(\n            occ_mask=occupied_mask,\n            occupancy=occupancy_bitfield,\n        )\n        return self.replace(ogrid=new_ogrid)\n\n    def mark_untrained_density_grid(self) -> \"NeRFState\":\n        G = self.raymarch.density_grid_res\n        G3 = G*G*G\n        n_grids = self.scene_meta.cascades * G3\n        all_indices = jnp.arange(n_grids, dtype=jnp.uint32)\n        level, pos_idcs = all_indices // G3, all_indices % G3\n        mip_bound = jnp.minimum(2 ** level, self.scene_meta.bound).astype(jnp.float32)\n        cell_width = 2 * mip_bound / G\n        grid_xyzs = morton3d_invert(pos_idcs).astype(jnp.float32)  # [G3, 3]\n        grid_xyzs /= G  # in range [0, 1)\n        grid_xyzs -= 0.5  # in range [-0.5, 0.5)\n        grid_xyzs *= 2 * mip_bound[:, None]  # in range [-mip_bound, mip_bound)\n        vertex_offsets = cell_width[:, None, None] * jnp.asarray([\n            [0, 0, 0],\n            [0, 0, 1],\n            [0, 1, 0],\n            [0, 1, 1],\n            [1, 0, 0],\n            [1, 0, 1],\n            [1, 1, 0],\n            [1, 1, 1],\n        ], dtype=jnp.float32)\n        all_grid_vertices = grid_xyzs[:, None, :] + vertex_offsets\n\n        @jax.jit\n        def mark_untrained_density_grid_single_frame(\n            alive_marker: jax.Array,\n            transform_cw: jax.Array,\n            grid_vertices: jax.Array,\n        ):\n            rot_cw, t_cw = transform_cw[:3, :3], transform_cw[:3, 3]\n            # p_world, p_cam, T: [3, 1]\n            # rot_cw: [3, 3]\n            # p_world = rot_cw @ p_cam + t_cw\n            p_aligned = grid_vertices - t_cw\n            p_cam = (p_aligned[..., None, :] * rot_cw.T).sum(-1)\n\n            # camera looks along the -z axis\n            in_front_of_camera = p_cam[..., -1] < 0\n\n            u, v = jnp.split(p_cam[..., :2] / (-p_cam[..., -1:]), [1], axis=-1)\n\n            if self.scene_meta.camera.has_distortion:\n                # distort\n                u, v = self.scene_meta.camera.distort(u, v)\n\n                # Pixel coordinates outside the image plane may produce the same `u, v` as those inside\n                # the image plane, check if the produced `u, v` match the ray we started with.\n                # REF: <https://github.com/NVlabs/instant-ngp/blob/99aed93bbe8c8e074a90ec6c56c616e4fe217a42/src/testbed_nerf.cu#L481-L483>\n                re_u, re_v = self.scene_meta.camera.undistort(u, v)\n                redir = jnp.concatenate([re_u, re_v, -jnp.ones_like(re_u)], axis=-1)\n                redir = redir / jnp.linalg.norm(redir, axis=-1, keepdims=True)\n                ogdir = p_cam / jnp.linalg.norm(p_cam, axis=-1, keepdims=True)\n                same_ray = (redir * ogdir).sum(axis=-1) > 1. - 1e-3\n            else:\n                same_ray = True\n\n            uv = jnp.concatenate([\n                u * self.scene_meta.camera.fx + self.scene_meta.camera.cx,\n                v * self.scene_meta.camera.fy + self.scene_meta.camera.cy,\n            ], axis=-1)\n            uv = uv / jnp.asarray([self.scene_meta.camera.width, self.scene_meta.camera.height], dtype=jnp.float32)\n\n            within_frame_range = (uv >= 0.) & (uv < 1.)\n            within_frame_range = (\n                within_frame_range  # shape is [n_grids, 8, 2]\n                    .all(axis=-1)  # u and v must both be within frame\n            )\n\n            visible_by_camera = (in_front_of_camera & within_frame_range & same_ray).any(axis=-1)  # grid should be trained if any of its 8 vertices is visible\n\n            return alive_marker | visible_by_camera\n\n        # cam_t = np.asarray(list(map(\n        #     lambda frame: frame.transform_matrix_numpy[:3, 3],\n        #     self.scene_meta.frames,\n        # )))\n        # np.savetxt(\"cams.xyz\", cam_t)\n\n        alive_marker = jnp.zeros(n_grids, dtype=jnp.bool_)\n        for frame in (pbar := tqdm(self.scene_meta.frames, desc=\"| marking trainable grids\".format(n_grids), bar_format=tqdm_format)):\n            new_alive_marker_parts = map(\n                lambda alive_marker_part, grid_vertices_part: mark_untrained_density_grid_single_frame(\n                    alive_marker=alive_marker_part,\n                    transform_cw=frame.transform_matrix_jax_array,\n                    grid_vertices=grid_vertices_part,\n                ),\n                jnp.array_split(alive_marker, self.scene_meta.cascades),  # alive_marker_part\n                jnp.array_split(all_grid_vertices, self.scene_meta.cascades),  # grid_vertices_part\n            )\n            alive_marker = jnp.concatenate(list(new_alive_marker_parts), axis=0)\n            n_alive_grids = alive_marker.sum()\n            ratio_trainable = n_alive_grids / n_grids\n            pbar.set_description_str(\"| marked {}/{} ({:.2f}%) grids as trainable\".format(n_alive_grids, n_grids, ratio_trainable * 100))\n            if n_alive_grids == n_grids:\n                pbar.close()\n                break\n\n        marked_density = jnp.where(alive_marker, self.ogrid.density, -1.)\n        marked_occ_mask, marked_occupancy = packbits(\n            density_threshold=min(self.density_threshold_from_min_step_size, self.ogrid.mean_density_up_to_cascade(1)) if self.step > 0 else -.5,\n            density_grid=marked_density\n        )\n\n        # rgb = jnp.stack([~marked_occ_mask, jnp.zeros_like(marked_occ_mask, dtype=jnp.float32), marked_occ_mask]).T\n        # xyzrgb = np.asarray(jnp.concatenate([grid_xyzs, rgb], axis=-1))\n        # np.savetxt(\"blue_for_trainable.txt\", xyzrgb)\n        # np.savetxt(\"trainable.txt\", xyzrgb[np.where(marked_occ_mask)])\n        # np.savetxt(\"untrainable.txt\", xyzrgb[np.where(~marked_occ_mask)])\n\n        return self.replace(\n            ogrid=self.ogrid.replace(\n                density=marked_density,\n                occ_mask=marked_occ_mask,\n                occupancy=marked_occupancy,\n                alive_indices=all_indices[alive_marker],\n                alive_indices_offset=np.cumsum([0] + list(map(\n                    lambda cas_alive_marker: int(cas_alive_marker.sum()),\n                    jnp.split(alive_marker, self.scene_meta.cascades),\n                ))).tolist(),\n            ),\n        )\n\n    def epoch(self, iters: int) -> int:\n        return self.step // iters\n\n    @property\n    def density_threshold_from_min_step_size(self) -> float:\n        return .01 * self.raymarch.diagonal_n_steps / (2 * min(self.scene_meta.bound, 1) * 3**.5)\n\n    @property\n    def use_background_model(self) -> bool:\n        return self.scene_meta.bg and self.params.get(\"bg\") is not None\n\n    @property\n    def locked_params(self):\n        return jax.lax.stop_gradient(self.params)\n\n    @property\n    def update_ogrid_interval(self) -> int:\n        return min(16, self.step // 16 + 1)\n\n    @property\n    def should_call_update_ogrid(self) -> bool:\n        return (\n            self.step > 0\n            and self.step % self.update_ogrid_interval == 0\n        )\n\n    @property\n    def should_update_all_ogrid_cells(self) -> bool:\n        return self.step < 256\n\n    @property\n    def should_write_batch_metrics(self) -> bool:\n        return self.step % 16 == 0", "\n@empty_impl\nclass NeRFState(TrainState):\n    # WARN:\n    #   do not annotate fields with jax.Array as members with flax.truct.field(pytree_node=False),\n    #   otherwise weird issues happen, e.g. jax tracer leak, array-to-boolean conversion exception\n    #   while calling a jitted function with no helpful traceback.\n    ogrid: OccupancyDensityGrid\n\n    raymarch: RayMarchingOptions=struct.field(pytree_node=False)\n    render: RenderingOptions=struct.field(pytree_node=False)\n    scene_options: SceneOptions=struct.field(pytree_node=False)\n    scene_meta: SceneMeta=struct.field(pytree_node=False)\n\n    nerf_fn: Callable=struct.field(pytree_node=False)\n    bg_fn: Callable=struct.field(pytree_node=False)\n\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return super().create(apply_fn=None, *args, **kwargs)\n\n    def __post_init__(self):\n        assert self.apply_fn is None\n\n    def update_ogrid_density(\n        self,\n        KEY: jran.KeyArray,\n        cas: int,\n        update_all: bool,\n        max_inference: int,\n    ) -> \"NeRFState\":\n        G3 = self.raymarch.density_grid_res**3\n        cas_slice = slice(cas * G3, (cas + 1) * G3)\n        cas_alive_indices = self.ogrid.alive_indices[self.ogrid.alive_indices_offset[cas]:self.ogrid.alive_indices_offset[cas+1]]\n        aligned_indices = cas_alive_indices % G3  # values are in range [0, G3)\n        n_grids = aligned_indices.shape[0]\n\n        decay = .95\n        cas_occ_mask = self.ogrid.occ_mask[cas_slice]\n        cas_density_grid = self.ogrid.density[cas_slice].at[aligned_indices].set(self.ogrid.density[cas_slice][aligned_indices] * decay)\n\n        if update_all:\n            # During the first 256 training steps, we sample M = K * 128^{3} cells uniformly without\n            # repetition.\n            cas_updated_indices = aligned_indices\n        else:\n            M = max(1, n_grids // 2)\n            # The first M/2 cells are sampled uniformly among all cells.\n            KEY, key_firsthalf, key_secondhalf = jran.split(KEY, 3)\n            indices_firsthalf = jran.choice(\n                key=key_firsthalf,\n                a=aligned_indices,\n                shape=(max(1, M//2),),\n                replace=True,  # allow duplicated choices\n            )\n            # Rejection sampling is used for the remaining samples to restrict selection to cells\n            # that are currently occupied.\n            # NOTE: Below is just uniformly sampling the occupied cells, not rejection sampling.\n            cas_alive_occ_mask = cas_occ_mask[aligned_indices]\n            indices_secondhalf = jran.choice(\n                key=key_secondhalf,\n                a=aligned_indices,\n                shape=(max(1, M//2),),\n                replace=True,  # allow duplicated choices\n                p=cas_alive_occ_mask.astype(jnp.float32),  # only care about occupied grids\n            )\n            cas_updated_indices = jnp.concatenate([indices_firsthalf, indices_secondhalf])\n\n        coordinates = morton3d_invert(cas_updated_indices).astype(jnp.float32)\n        coordinates = coordinates / (self.raymarch.density_grid_res - 1) * 2 - 1  # in [-1, 1]\n        mip_bound = min(self.scene_meta.bound, 2**cas)\n        half_cell_width = mip_bound / self.raymarch.density_grid_res\n        coordinates *= mip_bound - half_cell_width  # in [-mip_bound+half_cell_width, mip_bound-half_cell_width]\n        # random point inside grid cells\n        KEY, key = jran.split(KEY, 2)\n        coordinates += jran.uniform(\n            key,\n            coordinates.shape,\n            coordinates.dtype,\n            minval=-half_cell_width,\n            maxval=half_cell_width,\n        )\n\n        new_densities = map(\n            lambda coords_part: jax.jit(self.nerf_fn)(\n                {\"params\": self.locked_params[\"nerf\"]},\n                coords_part,\n                None,\n                None,\n            )[0].ravel(),\n            jnp.array_split(jax.lax.stop_gradient(coordinates), max(1, n_grids // (max_inference))),\n        )\n        new_densities = jnp.concatenate(list(new_densities))\n\n        cas_density_grid = cas_density_grid.at[cas_updated_indices].set(\n            jnp.maximum(cas_density_grid[cas_updated_indices], new_densities)\n        )\n        new_ogrid = self.ogrid.replace(\n            density=self.ogrid.density.at[cas_slice].set(cas_density_grid),\n        )\n        return self.replace(ogrid=new_ogrid)\n\n    @jax.jit\n    def threshold_ogrid(self) -> \"NeRFState\":\n        mean_density = self.ogrid.mean_density_up_to_cascade(1)\n        density_threshold = jnp.minimum(self.density_threshold_from_min_step_size, mean_density)\n        occupied_mask, occupancy_bitfield = packbits(\n            density_threshold=density_threshold,\n            density_grid=self.ogrid.density,\n        )\n        new_ogrid = self.ogrid.replace(\n            occ_mask=occupied_mask,\n            occupancy=occupancy_bitfield,\n        )\n        return self.replace(ogrid=new_ogrid)\n\n    def mark_untrained_density_grid(self) -> \"NeRFState\":\n        G = self.raymarch.density_grid_res\n        G3 = G*G*G\n        n_grids = self.scene_meta.cascades * G3\n        all_indices = jnp.arange(n_grids, dtype=jnp.uint32)\n        level, pos_idcs = all_indices // G3, all_indices % G3\n        mip_bound = jnp.minimum(2 ** level, self.scene_meta.bound).astype(jnp.float32)\n        cell_width = 2 * mip_bound / G\n        grid_xyzs = morton3d_invert(pos_idcs).astype(jnp.float32)  # [G3, 3]\n        grid_xyzs /= G  # in range [0, 1)\n        grid_xyzs -= 0.5  # in range [-0.5, 0.5)\n        grid_xyzs *= 2 * mip_bound[:, None]  # in range [-mip_bound, mip_bound)\n        vertex_offsets = cell_width[:, None, None] * jnp.asarray([\n            [0, 0, 0],\n            [0, 0, 1],\n            [0, 1, 0],\n            [0, 1, 1],\n            [1, 0, 0],\n            [1, 0, 1],\n            [1, 1, 0],\n            [1, 1, 1],\n        ], dtype=jnp.float32)\n        all_grid_vertices = grid_xyzs[:, None, :] + vertex_offsets\n\n        @jax.jit\n        def mark_untrained_density_grid_single_frame(\n            alive_marker: jax.Array,\n            transform_cw: jax.Array,\n            grid_vertices: jax.Array,\n        ):\n            rot_cw, t_cw = transform_cw[:3, :3], transform_cw[:3, 3]\n            # p_world, p_cam, T: [3, 1]\n            # rot_cw: [3, 3]\n            # p_world = rot_cw @ p_cam + t_cw\n            p_aligned = grid_vertices - t_cw\n            p_cam = (p_aligned[..., None, :] * rot_cw.T).sum(-1)\n\n            # camera looks along the -z axis\n            in_front_of_camera = p_cam[..., -1] < 0\n\n            u, v = jnp.split(p_cam[..., :2] / (-p_cam[..., -1:]), [1], axis=-1)\n\n            if self.scene_meta.camera.has_distortion:\n                # distort\n                u, v = self.scene_meta.camera.distort(u, v)\n\n                # Pixel coordinates outside the image plane may produce the same `u, v` as those inside\n                # the image plane, check if the produced `u, v` match the ray we started with.\n                # REF: <https://github.com/NVlabs/instant-ngp/blob/99aed93bbe8c8e074a90ec6c56c616e4fe217a42/src/testbed_nerf.cu#L481-L483>\n                re_u, re_v = self.scene_meta.camera.undistort(u, v)\n                redir = jnp.concatenate([re_u, re_v, -jnp.ones_like(re_u)], axis=-1)\n                redir = redir / jnp.linalg.norm(redir, axis=-1, keepdims=True)\n                ogdir = p_cam / jnp.linalg.norm(p_cam, axis=-1, keepdims=True)\n                same_ray = (redir * ogdir).sum(axis=-1) > 1. - 1e-3\n            else:\n                same_ray = True\n\n            uv = jnp.concatenate([\n                u * self.scene_meta.camera.fx + self.scene_meta.camera.cx,\n                v * self.scene_meta.camera.fy + self.scene_meta.camera.cy,\n            ], axis=-1)\n            uv = uv / jnp.asarray([self.scene_meta.camera.width, self.scene_meta.camera.height], dtype=jnp.float32)\n\n            within_frame_range = (uv >= 0.) & (uv < 1.)\n            within_frame_range = (\n                within_frame_range  # shape is [n_grids, 8, 2]\n                    .all(axis=-1)  # u and v must both be within frame\n            )\n\n            visible_by_camera = (in_front_of_camera & within_frame_range & same_ray).any(axis=-1)  # grid should be trained if any of its 8 vertices is visible\n\n            return alive_marker | visible_by_camera\n\n        # cam_t = np.asarray(list(map(\n        #     lambda frame: frame.transform_matrix_numpy[:3, 3],\n        #     self.scene_meta.frames,\n        # )))\n        # np.savetxt(\"cams.xyz\", cam_t)\n\n        alive_marker = jnp.zeros(n_grids, dtype=jnp.bool_)\n        for frame in (pbar := tqdm(self.scene_meta.frames, desc=\"| marking trainable grids\".format(n_grids), bar_format=tqdm_format)):\n            new_alive_marker_parts = map(\n                lambda alive_marker_part, grid_vertices_part: mark_untrained_density_grid_single_frame(\n                    alive_marker=alive_marker_part,\n                    transform_cw=frame.transform_matrix_jax_array,\n                    grid_vertices=grid_vertices_part,\n                ),\n                jnp.array_split(alive_marker, self.scene_meta.cascades),  # alive_marker_part\n                jnp.array_split(all_grid_vertices, self.scene_meta.cascades),  # grid_vertices_part\n            )\n            alive_marker = jnp.concatenate(list(new_alive_marker_parts), axis=0)\n            n_alive_grids = alive_marker.sum()\n            ratio_trainable = n_alive_grids / n_grids\n            pbar.set_description_str(\"| marked {}/{} ({:.2f}%) grids as trainable\".format(n_alive_grids, n_grids, ratio_trainable * 100))\n            if n_alive_grids == n_grids:\n                pbar.close()\n                break\n\n        marked_density = jnp.where(alive_marker, self.ogrid.density, -1.)\n        marked_occ_mask, marked_occupancy = packbits(\n            density_threshold=min(self.density_threshold_from_min_step_size, self.ogrid.mean_density_up_to_cascade(1)) if self.step > 0 else -.5,\n            density_grid=marked_density\n        )\n\n        # rgb = jnp.stack([~marked_occ_mask, jnp.zeros_like(marked_occ_mask, dtype=jnp.float32), marked_occ_mask]).T\n        # xyzrgb = np.asarray(jnp.concatenate([grid_xyzs, rgb], axis=-1))\n        # np.savetxt(\"blue_for_trainable.txt\", xyzrgb)\n        # np.savetxt(\"trainable.txt\", xyzrgb[np.where(marked_occ_mask)])\n        # np.savetxt(\"untrainable.txt\", xyzrgb[np.where(~marked_occ_mask)])\n\n        return self.replace(\n            ogrid=self.ogrid.replace(\n                density=marked_density,\n                occ_mask=marked_occ_mask,\n                occupancy=marked_occupancy,\n                alive_indices=all_indices[alive_marker],\n                alive_indices_offset=np.cumsum([0] + list(map(\n                    lambda cas_alive_marker: int(cas_alive_marker.sum()),\n                    jnp.split(alive_marker, self.scene_meta.cascades),\n                ))).tolist(),\n            ),\n        )\n\n    def epoch(self, iters: int) -> int:\n        return self.step // iters\n\n    @property\n    def density_threshold_from_min_step_size(self) -> float:\n        return .01 * self.raymarch.diagonal_n_steps / (2 * min(self.scene_meta.bound, 1) * 3**.5)\n\n    @property\n    def use_background_model(self) -> bool:\n        return self.scene_meta.bg and self.params.get(\"bg\") is not None\n\n    @property\n    def locked_params(self):\n        return jax.lax.stop_gradient(self.params)\n\n    @property\n    def update_ogrid_interval(self) -> int:\n        return min(16, self.step // 16 + 1)\n\n    @property\n    def should_call_update_ogrid(self) -> bool:\n        return (\n            self.step > 0\n            and self.step % self.update_ogrid_interval == 0\n        )\n\n    @property\n    def should_update_all_ogrid_cells(self) -> bool:\n        return self.step < 256\n\n    @property\n    def should_write_batch_metrics(self) -> bool:\n        return self.step % 16 == 0", ""]}
{"filename": "utils/_constants.py", "chunked_list": ["from colorama import Fore, Style\n\n\n_tqdm_format = \"SBRIGHT{desc}RESET: HI{percentage:3.0f}%RESET {n_fmt}/{total_fmt} [{elapsed}<HI{remaining}RESET, {rate_fmt}]\"\ntqdm_format = _tqdm_format \\\n    .replace(\"HI\", Fore.CYAN) \\\n    .replace(\"SBRIGHT\", Style.BRIGHT) \\\n    .replace(\"RESET\", Style.RESET_ALL)\n", ""]}
{"filename": "utils/data.py", "chunked_list": ["import collections\nfrom concurrent.futures import ThreadPoolExecutor\nimport functools\nimport json\nfrom pathlib import Path\nfrom typing import List, Literal, Sequence, Tuple\nimport warnings\n\nfrom PIL import Image, ImageFilter, UnidentifiedImageError\nimport chex", "from PIL import Image, ImageFilter, UnidentifiedImageError\nimport chex\nimport ffmpeg\nimport imageio\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jran\nfrom matplotlib import pyplot as plt\nfrom natsort import natsorted\nimport numpy as np", "from natsort import natsorted\nimport numpy as np\n\nfrom . import sfm\nfrom .common import jit_jaxfn_with, mkValueError, tqdm\nfrom .types import (\n    ImageMetadata,\n    Camera,\n    RGBColor,\n    RGBColorU8,", "    RGBColor,\n    RGBColorU8,\n    SceneCreationOptions,\n    SceneData,\n    SceneMeta,\n    SceneOptions,\n    TransformJsonFrame,\n    TransformJsonNGP,\n    TransformJsonNeRFSynthetic,\n)", "    TransformJsonNeRFSynthetic,\n)\n\n\ndef to_cpu(array: jax.Array) -> jax.Array:\n    return jax.device_put(array, device=jax.devices(\"cpu\")[0])\n\n\n@jax.jit\ndef f32_to_u8(img: jax.Array) -> jax.Array:\n    return jnp.clip(jnp.round(img * 255), 0, 255).astype(jnp.uint8)", "@jax.jit\ndef f32_to_u8(img: jax.Array) -> jax.Array:\n    return jnp.clip(jnp.round(img * 255), 0, 255).astype(jnp.uint8)\n\n\ndef mono_to_rgb(img: jax.Array, cm: Literal[\"inferno\", \"jet\", \"turbo\"]=\"inferno\") -> jax.Array:\n    return plt.get_cmap(cm)(img)\n\n\ndef sharpness_of(path: str | Path) -> float | None:\n    try:\n        image = Image.open(path)\n    except (IsADirectoryError, UnidentifiedImageError) as e:\n        warnings.warn(\n            \"failed loading '{}': {}\".format(path, str(e)))\n        return None\n    laplacian = image.convert(\"L\").filter(ImageFilter.FIND_EDGES)\n    return float(np.asarray(laplacian).var())", "\ndef sharpness_of(path: str | Path) -> float | None:\n    try:\n        image = Image.open(path)\n    except (IsADirectoryError, UnidentifiedImageError) as e:\n        warnings.warn(\n            \"failed loading '{}': {}\".format(path, str(e)))\n        return None\n    laplacian = image.convert(\"L\").filter(ImageFilter.FIND_EDGES)\n    return float(np.asarray(laplacian).var())", "\n\ndef video_to_images(\n    video_in: Path,\n    images_dir: Path,\n    fmt: str=\"%04d.png\",\n    fps: int=3,\n):\n    video_in, images_dir = Path(video_in), Path(images_dir)\n    images_dir.mkdir(parents=True, exist_ok=True)\n    (ffmpeg.input(video_in)\n        .output(\n            images_dir.joinpath(fmt).as_posix(),\n            r=fps,\n            pix_fmt=\"rgb24\",  # colmap only supports 8-bit color depth\n        )\n        .run(\n            capture_stdout=False,\n            capture_stderr=False,\n        )\n    )", "\n\ndef qvec2rotmat(qvec):\n    \"copied from NVLabs/instant-ngp/scripts/colmap2nerf.py\"\n    return np.asarray([\n        [\n            1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n            2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n            2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]\n        ], [\n            2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n            1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n            2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]\n        ], [\n            2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n            2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],\n            1 - 2 * qvec[1]**2 - 2 * qvec[2]**2\n        ]\n    ])", "def rotmat(a, b):\n    \"copied from NVLabs/instant-ngp/scripts/colmap2nerf.py\"\n    a, b = a / np.linalg.norm(a), b / np.linalg.norm(b)\n    v = np.cross(a, b)\n    c = np.dot(a, b)\n    # handle exception for the opposite direction input\n    if c < -1 + 1e-10:\n        return rotmat(a + np.random.uniform(-1e-2, 1e-2, 3), b)\n    s = np.linalg.norm(v)\n    kmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n    return np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2 + 1e-10))", "def closest_point_2_lines(oa, da, ob, db):\n    \"\"\"\n    (copied from NVLabs/instant-ngp/scripts/colmap2nerf.py)\n    returns point closest to both rays of form o+t*d, and a weight factor that goes to 0 if the lines are parallel\n    \"\"\"\n    da = da / np.linalg.norm(da)\n    db = db / np.linalg.norm(db)\n    c = np.cross(da, db)\n    denom = np.linalg.norm(c)**2\n    t = ob - oa\n    ta = np.linalg.det([t, db, c]) / (denom + 1e-10)\n    tb = np.linalg.det([t, da, c]) / (denom + 1e-10)\n    if ta > 0:\n        ta = 0\n    if tb > 0:\n        tb = 0\n    return (oa+ta*da+ob+tb*db) * 0.5, denom", "\n\ndef write_sharpness_json(raw_images_dir: str | Path):\n    raw_images_dir = Path(raw_images_dir)\n    out_filename = \"sharpnesses.jaxngp.json\"\n    image_candidates = tuple(filter(lambda x: x.name != out_filename, raw_images_dir.iterdir()))\n    sharpnesses = ThreadPoolExecutor().map(sharpness_of, image_candidates)\n    path_sharpness_tuples = zip(map(lambda p: p.absolute().as_posix(), raw_images_dir.iterdir()), sharpnesses)\n    path_sharpness_tuples = filter(lambda tup: tup[1] is not None, path_sharpness_tuples)\n    path_sharpness_tuples = sorted(tqdm(path_sharpness_tuples, desc=\"| estimating sharpness of image collection\"), key=lambda tup: tup[1], reverse=True)\n    with open(raw_images_dir.joinpath(out_filename), \"w\") as f:\n        json.dump(path_sharpness_tuples, f)", "\n\ndef write_transforms_json(\n    scene_root_dir: Path,\n    images_dir: Path,\n    text_model_dir: Path,\n    opts: SceneCreationOptions,\n):\n    \"adapted from NVLabs/instant-ngp/scripts/colmap2nerf.py\"\n    scene_root_dir, images_dir, text_model_dir = (\n        Path(scene_root_dir),\n        Path(images_dir),\n        Path(text_model_dir),\n    )\n    rel_prefix = images_dir.relative_to(scene_root_dir)\n\n    camera = Camera.from_colmap_txt(text_model_dir.joinpath(\"cameras.txt\"))\n\n    images_txt = text_model_dir.joinpath(\"images.txt\")\n    images_lines = list(filter(lambda line: line[0] != \"#\", open(images_txt).readlines()))[::2]\n    up = np.zeros(3)\n    bottom_row = np.asarray((0, 0, 0, 1.0)).reshape(1, 4)\n    frames: List[TransformJsonFrame] = []\n    for line in images_lines:\n        # IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n        _, qw, qx, qy, qz, tx, ty, tz, _, name = line.strip().split()\n        R = qvec2rotmat(tuple(map(float, (qw, qx, qy, qz))))\n        T = np.asarray(tuple(map(float, (tx, ty, tz)))).reshape(3, 1)\n        m = np.concatenate([R, T], axis=-1)\n        m = np.concatenate([m, bottom_row], axis=0)\n        c2w = np.linalg.inv(m)\n\n        c2w[0:3,2] *= -1 # flip the y and z axis\n        c2w[0:3,1] *= -1\n        c2w = c2w[[1,0,2,3],:]\n        c2w[2,:] *= -1 # flip whole world upside down\n        up += c2w[0:3,1]\n\n        frames.append(TransformJsonFrame(\n            file_path=rel_prefix.joinpath(name).as_posix(),\n            transform_matrix=c2w.tolist(),\n        ))\n\n    # estimate sharpness\n    sharpnesses = ThreadPoolExecutor().map(lambda f: sharpness_of(scene_root_dir.joinpath(f.file_path)), frames)\n    for i, sharpness in enumerate(tqdm(sharpnesses, total=len(frames), desc=\"| estimating sharpness of image collection\")):\n        frames[i] = frames[i].replace(sharpness=sharpness)\n\n    # reorient the scene to be easier to work with\n    up = up / np.linalg.norm(up)\n    print(\"up vector:\", up, \"->\", [0, 0, 1])\n    R = rotmat(up,[0,0,1]) # rotate up vector to [0,0,1]\n    R = np.pad(R,[0,1])\n    R[-1, -1] = 1\n\n    for i, f in enumerate(frames):\n        frames[i] = f.replace(transform_matrix=np.matmul(R, f.transform_matrix_numpy).tolist())\n\n    # find a central point they are all looking at\n    totw = 0.0\n    totp = np.array([0.0, 0.0, 0.0])\n    for f in frames:\n        mf = f.transform_matrix_numpy[0:3,:]\n        for g in frames:\n            mg = g.transform_matrix_numpy[0:3,:]\n            p, w = closest_point_2_lines(mf[:,3], mf[:,2], mg[:,3], mg[:,2])\n            if w > 1e-5:\n                totp += p*w\n                totw += w\n    if totw > 0.0:\n        totp /= totw\n    # the cameras are looking at totp\n    print(\"the cameras are looking at:\", totp, \"->\", [0, 0, 0])\n    for i, f in enumerate(frames):\n        new_m = f.transform_matrix_numpy\n        new_m[0:3,3] -= totp\n        frames[i] = f.replace(transform_matrix=new_m.tolist())\n\n    avglen = 0.\n    for f in frames:\n        avglen += np.linalg.norm(f.transform_matrix_numpy[0:3,3])\n    avglen /= len(frames)\n    print(\"average camera distance from origin:\", avglen, \"->\", 4.0)\n    for i, f in enumerate(frames):\n        # scale to \"nerf sized\"\n        new_m = f.transform_matrix_numpy\n        new_m[0:3, 3] *= 4.0 / avglen\n        frames[i] = f.replace(transform_matrix=new_m.tolist())\n\n    print(\"scene bound (i.e. half width of scene's aabb):\", opts.bound)\n    all_transform_json = TransformJsonNGP(\n        frames=frames,\n        fl_x=camera.fx,\n        fl_y=camera.fy,\n        cx=camera.cx,\n        cy=camera.cy,\n        w=camera.width,\n        h=camera.height,\n        k1=camera.k1,\n        k2=camera.k2,\n        k3=camera.k3,\n        k4=camera.k4,\n        p1=camera.p1,\n        p2=camera.p2,\n        aabb_scale=opts.bound,\n    )\n    all_transform_json: TransformJsonNGP = all_transform_json.replace(\n        scale=opts.camera_scale,\n        bg=opts.bg,\n        up=[0, 0, 1],\n        n_extra_learnable_dims=opts.n_extra_learnable_dims,\n    )\n    all_transform_json.save(scene_root_dir.joinpath(\"transforms.json\"))\n    return all_transform_json", "\n\ndef create_scene_from_single_camera_image_collection(\n    raw_images_dir: Path,\n    scene_root_dir: Path,\n    opts: SceneCreationOptions,\n):\n    raw_images_dir, scene_root_dir = Path(raw_images_dir), Path(scene_root_dir)\n    scene_root_dir.mkdir(parents=True, exist_ok=True)\n\n    artifacts_dir = scene_root_dir.joinpath(\"artifacts\")\n    artifacts_dir.mkdir(parents=True, exist_ok=True)\n    db_path = artifacts_dir.joinpath(\"colmap.db\")\n    sparse_reconstructions_dir = artifacts_dir.joinpath(\"sparse\")\n    undistorted_images_dir = scene_root_dir.joinpath(\"images-undistorted\")\n    out_model_dir = (\n        undistorted_images_dir.joinpath(\"sparse\")\n        if opts.undistort\n        else sparse_reconstructions_dir.joinpath(\"0\")\n    )\n    out_images_dir = (\n        undistorted_images_dir.joinpath(\"images\")\n        if opts.undistort\n        else raw_images_dir\n    )\n    text_model_dir = artifacts_dir.joinpath(\"text\")\n\n    sfm.extract_features(images_dir=raw_images_dir, db_path=db_path, camera_model=opts.camera_model)\n\n    sfm.match_features(matcher=opts.matcher, db_path=db_path)\n\n    maps = sfm.sparse_reconstruction(\n        images_dir=raw_images_dir,\n        sparse_reconstructions_dir=sparse_reconstructions_dir,\n        db_path=db_path,\n        matcher=opts.matcher,\n    )\n    if len(maps) == 0:\n        raise RuntimeError(\"mapping with colmap failed\")\n    elif len(maps) > 1:\n        warnings.warn(\n            \"colmap reconstructed more than 1 maps, we will only use the first map\")\n\n    sparse_recon_dir = sparse_reconstructions_dir.joinpath(\"0\")\n\n    sfm.colmap_bundle_adjustment(sparse_reconstruction_dir=sparse_recon_dir, max_num_iterations=200)\n\n    if opts.undistort:\n        sfm.undistort(\n            images_dir=raw_images_dir,\n            sparse_reconstruction_dir=sparse_recon_dir,\n            undistorted_images_dir=undistorted_images_dir,\n        )\n\n    sfm.export_text_format_model(\n        sparse_reconstruction_dir=out_model_dir,\n        text_model_dir=text_model_dir,\n    )\n\n    write_transforms_json(\n        scene_root_dir=scene_root_dir,\n        images_dir=out_images_dir,\n        text_model_dir=text_model_dir,\n        opts=opts,\n    )", "\n\ndef create_scene_from_video(\n    video_path: Path,\n    scene_root_dir: Path,\n    fps: int,\n    opts: SceneCreationOptions,\n):\n    video_path, scene_root_dir = Path(video_path), Path(scene_root_dir)\n    raw_images_dir = scene_root_dir.joinpath(\"images-raw\")\n    video_to_images(\n        video_in=video_path,\n        images_dir=raw_images_dir,\n        fps=fps,\n    )\n    create_scene_from_single_camera_image_collection(\n        raw_images_dir=raw_images_dir,\n        scene_root_dir=scene_root_dir,\n        opts=opts,\n    )", "\n\ndef to_unit_cube_2d(xys: jax.Array, W: int, H: int):\n    \"Normalizes coordinate (x, y) into range [0, 1], where 0<=x<W, 0<=y<H\"\n    uvs = xys / jnp.asarray([[W-1, H-1]])\n    return uvs\n\n\n@jit_jaxfn_with(static_argnames=[\"height\", \"width\", \"vertical\", \"gap\", \"gap_color\"])\ndef side_by_side(\n    lhs: jax.Array,\n    rhs: jax.Array,\n    height: int=None,\n    width: int=None,\n    vertical: bool=False,\n    gap: int=5,\n    gap_color: RGBColorU8=(0xab, 0xcd, 0xef),\n) -> jax.Array:\n    chex.assert_not_both_none(height, width)\n    chex.assert_scalar_non_negative(vertical)\n    chex.assert_type([lhs, rhs], jnp.uint8)\n    if len(lhs.shape) == 2 or lhs.shape[-1] == 1:\n        lhs = jnp.tile(lhs[..., None], (1, 1, 3))\n    if len(rhs.shape) == 2 or rhs.shape[-1] == 1:\n        rhs = jnp.tile(rhs[..., None], (1, 1, 3))\n    if rhs.shape[-1] == 3:\n        rhs = jnp.concatenate([rhs, 255 * jnp.ones_like(rhs[..., -1:], dtype=jnp.uint8)], axis=-1)\n    if lhs.shape[-1] == 3:\n        lhs = jnp.concatenate([lhs, 255 * jnp.ones_like(lhs[..., -1:], dtype=jnp.uint8)], axis=-1)\n    if vertical:\n        chex.assert_axis_dimension(lhs, 1, width)\n        chex.assert_axis_dimension(rhs, 1, width)\n    else:\n        chex.assert_axis_dimension(lhs, 0, height)\n        chex.assert_axis_dimension(rhs, 0, height)\n    concat_axis = 0 if vertical else 1\n    if gap > 0:\n        gap_color = jnp.asarray(gap_color + (0xff,), dtype=jnp.uint8)\n        gap = jnp.broadcast_to(gap_color, (gap, width, 4) if vertical else (height, gap, 4))\n        return jnp.concatenate([lhs, gap, rhs], axis=concat_axis)\n    else:\n        return jnp.concatenate([lhs, rhs], axis=concat_axis)", "@jit_jaxfn_with(static_argnames=[\"height\", \"width\", \"vertical\", \"gap\", \"gap_color\"])\ndef side_by_side(\n    lhs: jax.Array,\n    rhs: jax.Array,\n    height: int=None,\n    width: int=None,\n    vertical: bool=False,\n    gap: int=5,\n    gap_color: RGBColorU8=(0xab, 0xcd, 0xef),\n) -> jax.Array:\n    chex.assert_not_both_none(height, width)\n    chex.assert_scalar_non_negative(vertical)\n    chex.assert_type([lhs, rhs], jnp.uint8)\n    if len(lhs.shape) == 2 or lhs.shape[-1] == 1:\n        lhs = jnp.tile(lhs[..., None], (1, 1, 3))\n    if len(rhs.shape) == 2 or rhs.shape[-1] == 1:\n        rhs = jnp.tile(rhs[..., None], (1, 1, 3))\n    if rhs.shape[-1] == 3:\n        rhs = jnp.concatenate([rhs, 255 * jnp.ones_like(rhs[..., -1:], dtype=jnp.uint8)], axis=-1)\n    if lhs.shape[-1] == 3:\n        lhs = jnp.concatenate([lhs, 255 * jnp.ones_like(lhs[..., -1:], dtype=jnp.uint8)], axis=-1)\n    if vertical:\n        chex.assert_axis_dimension(lhs, 1, width)\n        chex.assert_axis_dimension(rhs, 1, width)\n    else:\n        chex.assert_axis_dimension(lhs, 0, height)\n        chex.assert_axis_dimension(rhs, 0, height)\n    concat_axis = 0 if vertical else 1\n    if gap > 0:\n        gap_color = jnp.asarray(gap_color + (0xff,), dtype=jnp.uint8)\n        gap = jnp.broadcast_to(gap_color, (gap, width, 4) if vertical else (height, gap, 4))\n        return jnp.concatenate([lhs, gap, rhs], axis=concat_axis)\n    else:\n        return jnp.concatenate([lhs, rhs], axis=concat_axis)", "\n\n@jit_jaxfn_with(static_argnames=[\"border_pixels\", \"color\"])\ndef add_border(\n    img: jax.Array,\n    border_pixels: int=5,\n    color: RGBColorU8=(0xfe, 0xdc, 0xba)\n) -> jax.Array:\n    chex.assert_rank(img, 3)\n    chex.assert_axis_dimension(img, -1, 4)\n    chex.assert_scalar_non_negative(border_pixels)\n    chex.assert_type(img, jnp.uint8)\n    color = jnp.asarray(color + (0xff,), dtype=jnp.uint8)\n    height, width = img.shape[:2]\n    leftright = jnp.broadcast_to(color, (height, border_pixels, 4))\n    img = jnp.concatenate([leftright, img, leftright], axis=1)\n    topbottom = jnp.broadcast_to(color, (border_pixels, width+2*border_pixels, 4))\n    img = jnp.concatenate([topbottom, img, topbottom], axis=0)\n    return img", "\n\n@jax.jit\ndef linear_to_db(val: float, maxval: float):\n    return 20 * jnp.log10(jnp.sqrt(maxval / val))\n\n\n@jax.jit\ndef psnr(lhs: jax.Array, rhs: jax.Array):\n    chex.assert_type([lhs, rhs], jnp.uint8)\n    mse = ((lhs.astype(float) - rhs.astype(float)) ** 2).mean()\n    return jnp.clip(20 * jnp.log10(255 / jnp.sqrt(mse + 1e-15)), 0, 100)", "def psnr(lhs: jax.Array, rhs: jax.Array):\n    chex.assert_type([lhs, rhs], jnp.uint8)\n    mse = ((lhs.astype(float) - rhs.astype(float)) ** 2).mean()\n    return jnp.clip(20 * jnp.log10(255 / jnp.sqrt(mse + 1e-15)), 0, 100)\n\n\ndef write_video(dest: Path, images: Sequence, *, fps: int=24, loop: int=3):\n    images = list(images) * loop\n    assert len(images) > 0, \"cannot write empty video\"\n    video_writer = imageio.get_writer(dest, mode=\"I\", fps=fps)\n    try:\n        for im in tqdm(images, desc=\"| writing video to {}\".format(dest.as_posix())):\n            video_writer.append_data(np.asarray(im))\n    except (BrokenPipeError, IOError) as e:  # sometimes ffmpeg encounters io error for no apparent reason\n        warnings.warn(\n            \"failed writing video: {}\".format(str(e)), RuntimeWarning)\n        warnings.warn(\n            \"skipping saving video '{}'\".format(dest.as_posix()), RuntimeWarning)", "\n\n@jax.jit\ndef set_pixels(imgarr: jax.Array, xys: jax.Array, selected: jax.Array, preds: jax.Array) -> jax.Array:\n    chex.assert_type(imgarr, jnp.uint8)\n    H, W = imgarr.shape[:2]\n    if len(imgarr.shape) == 3:\n        interm = imgarr.reshape(H*W, -1)\n    else:\n        interm = imgarr.ravel()\n    idcs = xys[selected, 1] * W + xys[selected, 0]\n    interm = interm.at[idcs].set(f32_to_u8(preds))\n    if len(imgarr.shape) == 3:\n        return interm.reshape(H, W, -1)\n    else:\n        return interm.reshape(H, W)", "\n\ndef blend_rgba_image_array(imgarr, bg: jax.Array):\n    \"\"\"\n    Blend the given background color according to the given alpha channel from `imgarr`.\n    WARN: this function SHOULD NOT be used for blending background colors into volume-rendered\n          pixels because the colors of volume-rendered pixels already have the alpha channel\n          factored-in.  To blend background for volume-rendered pixels, directly add the scaled\n          background color.\n          E.g.: `final_color = ray_accumulated_color + (1 - ray_opacity) * bg`\n    \"\"\"\n    if isinstance(imgarr, Image.Image):\n        imgarr = np.asarray(imgarr)\n    chex.assert_shape(imgarr, [..., 4])\n    chex.assert_type(imgarr, bg.dtype)\n    rgbs, alpha = imgarr[..., :-1], imgarr[..., -1:]\n    bg = jnp.broadcast_to(bg, rgbs.shape)\n    if imgarr.dtype == jnp.uint8:\n        rgbs, alpha = rgbs.astype(float) / 255, alpha.astype(float) / 255\n        rgbs = rgbs * alpha + bg * (1 - alpha)\n        rgbs = f32_to_u8(rgbs)\n    else:\n        rgbs = rgbs * alpha + bg * (1 - alpha)\n    return rgbs", "\n\ndef get_xyrgbas(imgarr: jax.Array) -> Tuple[jax.Array, jax.Array]:\n    assert imgarr.dtype == jnp.uint8\n    H, W, C = imgarr.shape\n\n    x, y = jnp.meshgrid(jnp.arange(W), jnp.arange(H))\n    x, y = x.reshape(-1, 1), y.reshape(-1, 1)\n    xys = jnp.concatenate([x, y], axis=-1)\n\n    flattened = imgarr.reshape(H*W, C) / 255\n    if C == 3:\n        # images without an alpha channel is equivalent to themselves with an all-opaque alpha\n        # channel\n        rgbas = jnp.concatenate([flattened, jnp.ones_like(flattened[:, :1])], axis=-1)\n        return xys, rgbas\n    elif C == 4:\n        rgbas = flattened\n        return xys, rgbas\n    else:\n        raise mkValueError(\n            desc=\"number of image channels\",\n            value=C,\n            type=Literal[3, 4],\n        )", "\n\n_ImageSourceType = jax.Array | np.ndarray | Image.Image | Path | str\ndef make_image_metadata(\n    image: _ImageSourceType,\n    bg: RGBColor,\n) -> ImageMetadata:\n    if isinstance(image, jax.Array):\n        pass\n    elif isinstance(image, Image.Image):\n        image = jnp.asarray(image)\n    elif isinstance(image, (Path, str)):\n        image = jnp.asarray(Image.open(image))\n    elif isinstance(image, np.ndarray):\n        image = jnp.asarray(image)\n    else:\n        raise mkValueError(\n            desc=\"image source type\",\n            value=image,\n            type=_ImageSourceType,\n        )\n\n    raise NotImplementedError(\n        \"function get_xyrgbs has been renamed to get_xyrgbas and this part has not been updated \"\n        \"accordingly\"\n    )\n    xys, rgbs = get_xyrgbs(image, bg=bg)\n\n    H, W = image.shape[:2]\n    uvs = to_unit_cube_2d(xys, W, H)\n\n    return ImageMetadata(\n        H=H,\n        W=W,\n        xys=jnp.asarray(xys),\n        uvs=jnp.asarray(uvs),\n        rgbs=jnp.asarray(rgbs),\n    )", "\n\ndef merge_transforms(transforms: Sequence[TransformJsonNGP | TransformJsonNeRFSynthetic]) -> TransformJsonNGP | TransformJsonNeRFSynthetic:\n    return functools.reduce(\n        lambda lhs, rhs: lhs.merge(rhs) if lhs is not None else rhs,\n        transforms,\n    )\n\n\ndef load_transform_json_recursive(src: Path | str) -> TransformJsonNGP | TransformJsonNeRFSynthetic | None:\n    \"\"\"\n    returns a single transforms object with the `file_path` in its `frames` attribute converted to\n    absolute paths\n    \"\"\"\n    src = Path(src)\n\n    if src.is_dir():\n        all_transforms = tuple(filter(\n            lambda xform: xform is not None,\n            map(load_transform_json_recursive, src.iterdir()),\n        ))\n        if len(all_transforms) == 0:\n            return None\n\n        # merge transforms found from descendants if any\n        transforms = merge_transforms(all_transforms)\n\n    elif src.suffix == \".json\":  # skip other files for speed\n        try:\n            transforms = json.load(open(src))\n        except:\n            # unreadable, or not a json\n            return None\n        if isinstance(transforms, dict):\n            try:\n                transforms = (\n                    TransformJsonNeRFSynthetic(**transforms)\n                    if transforms.get(\"camera_angle_x\") is not None\n                    else TransformJsonNGP(**transforms)\n                )\n                transforms = transforms.make_absolute(src.parent).scale_camera_positions()\n            except TypeError:\n                # not a valid transform.json\n                return None\n        else:\n            return None\n\n    else:\n        return None\n\n    return transforms", "\ndef load_transform_json_recursive(src: Path | str) -> TransformJsonNGP | TransformJsonNeRFSynthetic | None:\n    \"\"\"\n    returns a single transforms object with the `file_path` in its `frames` attribute converted to\n    absolute paths\n    \"\"\"\n    src = Path(src)\n\n    if src.is_dir():\n        all_transforms = tuple(filter(\n            lambda xform: xform is not None,\n            map(load_transform_json_recursive, src.iterdir()),\n        ))\n        if len(all_transforms) == 0:\n            return None\n\n        # merge transforms found from descendants if any\n        transforms = merge_transforms(all_transforms)\n\n    elif src.suffix == \".json\":  # skip other files for speed\n        try:\n            transforms = json.load(open(src))\n        except:\n            # unreadable, or not a json\n            return None\n        if isinstance(transforms, dict):\n            try:\n                transforms = (\n                    TransformJsonNeRFSynthetic(**transforms)\n                    if transforms.get(\"camera_angle_x\") is not None\n                    else TransformJsonNGP(**transforms)\n                )\n                transforms = transforms.make_absolute(src.parent).scale_camera_positions()\n            except TypeError:\n                # not a valid transform.json\n                return None\n        else:\n            return None\n\n    else:\n        return None\n\n    return transforms", "\n\ndef try_image_extensions(\n    file_path: Path,\n    extensions: List[str]=[\"png\", \"jpg\", \"jpeg\"],\n) -> Path | None:\n    if \"\" not in extensions:\n        extensions = [\"\"] + list(extensions)\n    for ext in extensions:\n        if len(ext) > 0 and ext[0] != \".\":\n            ext = \".\" + ext\n        p = Path(file_path.as_posix() + ext)\n        if p.exists():\n            return p\n        p = Path(file_path.with_suffix(ext))\n        if p.exists():\n            return p\n    warnings.warn(\n        \"could not find a file at '{}' with any extension of {}\".format(file_path, extensions),\n        RuntimeWarning,\n    )\n    return None", "\n\ndef load_scene(\n    srcs: Sequence[Path | str],\n    scene_options: SceneOptions,\n    sort_frames: bool=False,\n) -> SceneData:\n    \"\"\"\n    Inputs:\n        srcs: sequence of paths to recursively load transforms.json\n        scene_options: see :class:`SceneOptions`\n        sort_frames: whether to sort the frames by their filenames, (uses natural sort if enabled)\n    \"\"\"\n\n    assert isinstance(srcs, collections.abc.Sequence) and not isinstance(srcs, str), (\n        \"load_scene accepts a sequence of paths as srcs to load, did you mean '{}'?\".format([srcs])\n    )\n    srcs = list(map(Path, srcs))\n\n    transforms = merge_transforms(map(load_transform_json_recursive, srcs))\n    if scene_options.up_unitvec is not None:\n        transforms = transforms.replace(up=scene_options.up_unitvec).rotate_world_up()\n    if transforms is None:\n        raise FileNotFoundError(\"could not load transforms from any of {}\".format(srcs))\n\n    loaded_frames, discarded_frames = functools.reduce(\n        lambda prev, frame: (\n            prev[0] + ((frame,) if (\n                frame.file_path is not None\n                and frame.sharpness >= scene_options.sharpness_threshold\n            ) else ()),\n            prev[1] + (() if (\n                frame.file_path is not None\n                and frame.sharpness >= scene_options.sharpness_threshold\n            ) else (frame,)),\n        ),\n        map(\n            lambda f: f.replace(file_path=try_image_extensions(f.file_path)),\n            transforms.frames,\n        ),\n        (tuple(), tuple()),\n    )\n\n    if len(loaded_frames) == 0:\n        raise RuntimeError(\"loaded 0 frame from '{}' (discarded {} frame(s))\".format(srcs, len(discarded_frames)))\n\n    transforms = transforms.replace(frames=loaded_frames)\n\n    if sort_frames:\n        transforms = transforms.replace(\n            frames=natsorted(transforms.frames, key=lambda f: f.file_path),\n        )\n\n    # shared camera model\n    if isinstance(transforms, TransformJsonNeRFSynthetic):\n        _img = Image.open(try_image_extensions(transforms.frames[0].file_path))\n        fovx = transforms.camera_angle_x\n        focal = float(.5 * _img.width / np.tan(fovx / 2))\n        camera = Camera(\n            width=_img.width,\n            height=_img.height,\n            fx=focal,\n            fy=focal,\n            cx=_img.width / 2,\n            cy=_img.height / 2,\n            near=scene_options.camera_near,\n        )\n\n    elif isinstance(transforms, TransformJsonNGP):\n        camera = Camera(\n            width=transforms.w,\n            height=transforms.h,\n            fx=transforms.fl_x,\n            fy=transforms.fl_y,\n            cx=transforms.cx,\n            cy=transforms.cy,\n            near=scene_options.camera_near,\n            k1=transforms.k1,\n            k2=transforms.k2,\n            k3=transforms.k3,\n            k4=transforms.k4,\n            p1=transforms.p1,\n            p2=transforms.p2,\n            model=transforms.camera_model,\n        )\n\n    else:\n        raise TypeError(\"unexpected type for transforms: {}, expected one of {}\".format(\n            type(transforms),\n            [TransformJsonNeRFSynthetic, TransformJsonNGP],\n        ))\n\n    scene_meta = SceneMeta(\n        bound=scene_options.bound\n            if scene_options.bound is not None\n            else transforms.aabb_scale,\n        bg=transforms.bg,\n        camera=camera.scale_resolution(scene_options.resolution_scale),\n        n_extra_learnable_dims=transforms.n_extra_learnable_dims,\n        frames=transforms.frames,\n    )\n\n    return SceneData(meta=scene_meta, max_mem_mbytes=scene_options.max_mem_mbytes)", "\n\n@jit_jaxfn_with(static_argnames=[\"size\", \"loop\", \"shuffle\"])\ndef make_permutation(\n    key: jran.KeyArray,\n    size: int,\n    loop: int=1,\n    shuffle: bool=True,\n) -> jax.Array:\n    if shuffle:\n        perm = jran.permutation(key, size * loop)\n    else:\n        perm = jnp.arange(size * loop)\n    return perm % size", "\n\ndef main():\n    scene, views = load_scene(\n        rootdir=\"data/nerf/nerf_synthetic/lego\",\n        split=\"train\",\n    )\n    print(scene.all_xys.shape)\n    print(scene.all_rgbas.shape)\n    print(scene.all_transforms.shape)\n    print(len(views))", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "utils/sfm.py", "chunked_list": ["from pathlib import Path\nfrom typing import Dict\nfrom typing_extensions import get_args\n\nimport pycolmap\n\nfrom .common import mkValueError\nfrom .types import CameraModelType, ColmapMatcherType\n\n\ndef extract_features(\n    images_dir: Path,\n    db_path: Path,\n    camera_model: CameraModelType,\n):\n    images_dir, db_path = Path(images_dir), Path(db_path)\n    pycolmap.extract_features(\n        database_path=db_path,\n        image_path=images_dir,\n        # REF:\n        #   <https://github.com/colmap/colmap/blob/43de802cfb3ed2bd155150e7e5e3e8c8dd5aaa3e/src/exe/feature.h#L44-L52>\n        #   <https://github.com/colmap/pycolmap/blob/bdcdf47e0d40240c6f53dc463d7ceaa2cef923fd/pipeline/extract_features.cc#L63>\n        camera_mode=\"SINGLE\",\n        # REF:\n        #   <https://github.com/colmap/pycolmap/blob/bdcdf47e0d40240c6f53dc463d7ceaa2cef923fd/pipeline/extract_features.cc#L64>\n        camera_model=camera_model,\n        reader_options=pycolmap.ImageReaderOptions(\n            # camera_model=\"OPENCV\",  # NOTE: this is obsolete, see camera_model above\n            # single_camera=True,  # NOTE: this is obsolete, see camera_mode above\n        ),\n        sift_options=pycolmap.SiftExtractionOptions(\n            estimate_affine_shape=True,\n            domain_size_pooling=True,\n        ),\n    )", "\n\ndef extract_features(\n    images_dir: Path,\n    db_path: Path,\n    camera_model: CameraModelType,\n):\n    images_dir, db_path = Path(images_dir), Path(db_path)\n    pycolmap.extract_features(\n        database_path=db_path,\n        image_path=images_dir,\n        # REF:\n        #   <https://github.com/colmap/colmap/blob/43de802cfb3ed2bd155150e7e5e3e8c8dd5aaa3e/src/exe/feature.h#L44-L52>\n        #   <https://github.com/colmap/pycolmap/blob/bdcdf47e0d40240c6f53dc463d7ceaa2cef923fd/pipeline/extract_features.cc#L63>\n        camera_mode=\"SINGLE\",\n        # REF:\n        #   <https://github.com/colmap/pycolmap/blob/bdcdf47e0d40240c6f53dc463d7ceaa2cef923fd/pipeline/extract_features.cc#L64>\n        camera_model=camera_model,\n        reader_options=pycolmap.ImageReaderOptions(\n            # camera_model=\"OPENCV\",  # NOTE: this is obsolete, see camera_model above\n            # single_camera=True,  # NOTE: this is obsolete, see camera_mode above\n        ),\n        sift_options=pycolmap.SiftExtractionOptions(\n            estimate_affine_shape=True,\n            domain_size_pooling=True,\n        ),\n    )", "\n\ndef match_features(\n    matcher: ColmapMatcherType,\n    db_path: Path,\n):\n    db_path = Path(db_path)\n    if matcher not in get_args(ColmapMatcherType):\n        raise mkValueError(\n            desc=\"colmap matcher\",\n            value=matcher,\n            type=ColmapMatcherType,\n        )\n    match_fn = getattr(pycolmap, \"match_{}\".format(matcher.lower()))\n    return match_fn(\n        database_path=db_path,\n        sift_options=pycolmap.SiftMatchingOptions(\n            guided_matching=True,\n        ),\n    )", "\n\ndef sparse_reconstruction(\n    images_dir: Path,\n    sparse_reconstructions_dir: Path,\n    db_path: Path,\n    matcher: ColmapMatcherType,\n) -> Dict[int, pycolmap.Reconstruction]:\n    images_dir, sparse_reconstructions_dir = Path(images_dir), Path(sparse_reconstructions_dir)\n    mapping_options = pycolmap.IncrementalMapperOptions(\n        # principal point estimation is an ill-posed problem in general (`False` is already the\n        # default, setting to False here explicitly works as a reminder to self)\n        ba_refine_principal_point=False,\n        # <colmap/colmap>:src/colmap/util/option_manager.cc:ModifyForExtremeQuality\n        ba_local_max_num_iterations=40,\n        ba_local_max_refinements=3,\n        ba_global_max_num_iterations=100,\n        # below 3 options are for individual/video data, for internet photos, they should be left\n        # default\n        # <colmap/colmap>:src/colmap/util/option_manager.cc:ModifyForVideoData,ModifyForIndividualData\n        min_focal_length_ratio=0.1,\n        max_focal_length_ratio=10,\n        max_extra_param=1e15,\n    )\n    if matcher == \"Sequential\":\n        # <colmap/colmap>:src/colmap/util/option_manager.cc:ModifyForVideoData\n        mapping_options.ba_global_images_ratio = 1.4\n        mapping_options.ba_global_points_ratio = 1.4\n    maps = pycolmap.incremental_mapping(\n        database_path=db_path,\n        image_path=images_dir,\n        output_path=sparse_reconstructions_dir,\n        options=mapping_options,\n    )\n    return maps", "\n\ndef colmap_bundle_adjustment(\n    sparse_reconstruction_dir: Path,\n    max_num_iterations: int,\n) -> pycolmap.Reconstruction:\n    sparse_reconstruction_dir = Path(sparse_reconstruction_dir)\n    ba_options = {\n        \"refine_principal_point\": True,\n        \"solver_options\": {\n            \"max_num_iterations\": max_num_iterations,\n        },\n    }\n    recon = pycolmap.bundle_adjustment(\n        input_path=sparse_reconstruction_dir,\n        output_path=sparse_reconstruction_dir,\n        options=ba_options,\n    )\n    return recon", "\n\ndef undistort(\n    images_dir: Path,\n    sparse_reconstruction_dir: Path,\n    undistorted_images_dir: Path,\n):\n    images_dir, sparse_reconstruction_dir, undistorted_images_dir = (\n        Path(images_dir),\n        Path(sparse_reconstruction_dir),\n        Path(undistorted_images_dir),\n    )\n    pycolmap.undistort_images(\n        output_path=undistorted_images_dir,\n        input_path=sparse_reconstruction_dir,\n        image_path=images_dir,\n    )", "\n\ndef export_text_format_model(\n    sparse_reconstruction_dir: Path,\n    text_model_dir: Path,\n):\n    sparse_reconstruction_dir, text_model_dir = (\n        Path(sparse_reconstruction_dir),\n        Path(text_model_dir),\n    )\n    text_model_dir.mkdir(parents=True, exist_ok=True)\n    reconstruction = pycolmap.Reconstruction(sparse_reconstruction_dir)\n    reconstruction.write_text(text_model_dir.as_posix())", ""]}
{"filename": "utils/common.py", "chunked_list": ["from concurrent.futures import Executor, Future, ThreadPoolExecutor\nimport functools\nimport logging\nimport logging\nimport os\nfrom pathlib import Path\nimport random\nimport shutil\nfrom typing import Any, Dict, Hashable, Iterable, Sequence, get_args\n", "from typing import Any, Dict, Hashable, Iterable, Sequence, get_args\n\nimport colorama\nfrom colorama import Back, Fore, Style\nfrom flax.metrics import tensorboard\nimport git\nimport jax\nfrom jax._src.lib import xla_client as xc\nimport jax.random as jran\nimport numpy as np", "import jax.random as jran\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm as tqdm_original\n\nfrom ._constants import tqdm_format\nfrom .types import LogLevel\n\n\nclass Logger(logging.Logger):\n    _tb: tensorboard.SummaryWriter | None=None\n    _executor: Executor | None=None\n\n    _last_job: Future=None\n\n    def __init__(self, name: str, level: int | LogLevel) -> None:\n        super().__init__(name, level)\n\n    def setup_tensorboard(self, tb: tensorboard.SummaryWriter, executor: Executor) -> None:\n        self._tb = tb\n        self._executor = executor\n\n    def wait_last_job(self):\n        if self._last_job is not None and not self._last_job.done():\n            return self._last_job.result()\n\n    def write_scalar(self, tag: str, value: Any, step: int) -> None:\n        if self._tb is not None:\n            self.wait_last_job()\n            # NOTE: writing scalars is fast(ish) enough to not need a thread pool\n            self._executor.submit(self._tb.scalar, tag, value, step)\n    def write_image(self, tag: str, image: Any, step: int, max_outputs: int) -> None:\n        if self._tb is not None:\n            self.wait_last_job()\n            self._last_job = self._executor.submit(self._tb.image, tag, image, step, max_outputs)\n    def write_hparams(self, hparams: Dict[str, Any]) -> None:\n        if self._tb is not None:\n            self.wait_last_job()\n            self._last_job = self._executor.submit(self._tb.hparams, hparams)\n\n    def write_metrics_to_tensorboard(\n        self,\n        metrics: Dict[str, jax.Array | float],\n        step: jax.Array | int,\n    ) -> None:\n        def linear_to_db(val: float, maxval: float):\n            return 20 * np.log10(np.sqrt(maxval / val))\n        self.write_scalar(\n            \"batch/\u2193loss (rgb)\",\n            metrics[\"loss\"][\"rgb\"],\n            step,\n        )\n        self.write_scalar(\n            \"batch/\u2191estimated PSNR (db)\",\n            linear_to_db(metrics[\"loss\"][\"rgb\"], maxval=1.),\n            step,\n        )\n        self.write_scalar(\n            \"batch/\u2193loss (total variation)\",\n            metrics[\"loss\"][\"total_variation\"],\n            step,\n        )\n        self.write_scalar(\n            \"batch/effective batch size (not compacted)\",\n            metrics[\"measured_batch_size_before_compaction\"],\n            step,\n        )\n        self.write_scalar(\n            \"batch/\u2191effective batch size (compacted)\",\n            metrics[\"measured_batch_size\"],\n            step,\n        )\n        self.write_scalar(\n            \"rendering/\u2193effective samples per ray\",\n            metrics[\"measured_batch_size\"] / metrics[\"n_valid_rays\"],\n            step,\n        )\n        self.write_scalar(\n            \"rendering/\u2193marched samples per ray\",\n            metrics[\"measured_batch_size_before_compaction\"] / metrics[\"n_valid_rays\"],\n            step,\n        )\n        self.write_scalar(\n            \"rendering/\u2191number of marched rays\",\n            metrics[\"n_valid_rays\"],\n            step,\n        )", "\nclass Logger(logging.Logger):\n    _tb: tensorboard.SummaryWriter | None=None\n    _executor: Executor | None=None\n\n    _last_job: Future=None\n\n    def __init__(self, name: str, level: int | LogLevel) -> None:\n        super().__init__(name, level)\n\n    def setup_tensorboard(self, tb: tensorboard.SummaryWriter, executor: Executor) -> None:\n        self._tb = tb\n        self._executor = executor\n\n    def wait_last_job(self):\n        if self._last_job is not None and not self._last_job.done():\n            return self._last_job.result()\n\n    def write_scalar(self, tag: str, value: Any, step: int) -> None:\n        if self._tb is not None:\n            self.wait_last_job()\n            # NOTE: writing scalars is fast(ish) enough to not need a thread pool\n            self._executor.submit(self._tb.scalar, tag, value, step)\n    def write_image(self, tag: str, image: Any, step: int, max_outputs: int) -> None:\n        if self._tb is not None:\n            self.wait_last_job()\n            self._last_job = self._executor.submit(self._tb.image, tag, image, step, max_outputs)\n    def write_hparams(self, hparams: Dict[str, Any]) -> None:\n        if self._tb is not None:\n            self.wait_last_job()\n            self._last_job = self._executor.submit(self._tb.hparams, hparams)\n\n    def write_metrics_to_tensorboard(\n        self,\n        metrics: Dict[str, jax.Array | float],\n        step: jax.Array | int,\n    ) -> None:\n        def linear_to_db(val: float, maxval: float):\n            return 20 * np.log10(np.sqrt(maxval / val))\n        self.write_scalar(\n            \"batch/\u2193loss (rgb)\",\n            metrics[\"loss\"][\"rgb\"],\n            step,\n        )\n        self.write_scalar(\n            \"batch/\u2191estimated PSNR (db)\",\n            linear_to_db(metrics[\"loss\"][\"rgb\"], maxval=1.),\n            step,\n        )\n        self.write_scalar(\n            \"batch/\u2193loss (total variation)\",\n            metrics[\"loss\"][\"total_variation\"],\n            step,\n        )\n        self.write_scalar(\n            \"batch/effective batch size (not compacted)\",\n            metrics[\"measured_batch_size_before_compaction\"],\n            step,\n        )\n        self.write_scalar(\n            \"batch/\u2191effective batch size (compacted)\",\n            metrics[\"measured_batch_size\"],\n            step,\n        )\n        self.write_scalar(\n            \"rendering/\u2193effective samples per ray\",\n            metrics[\"measured_batch_size\"] / metrics[\"n_valid_rays\"],\n            step,\n        )\n        self.write_scalar(\n            \"rendering/\u2193marched samples per ray\",\n            metrics[\"measured_batch_size_before_compaction\"] / metrics[\"n_valid_rays\"],\n            step,\n        )\n        self.write_scalar(\n            \"rendering/\u2191number of marched rays\",\n            metrics[\"n_valid_rays\"],\n            step,\n        )", "\n\ntqdm = functools.partial(tqdm_original, bar_format=tqdm_format)\n\n\ndef backup_current_codebase(\n    exp_dir: Path | str,\n    /,\n    name_prefix: str,\n    note: str | None,\n) -> Path:\n    \"\"\"Backup current codebase to a directory named 'src' under the specified `exp_dir` directory,\n    creating it if it does not exist.\n    \"\"\"\n    repo = git.Repo(\".\", search_parent_directories=True)\n    try:\n        __initial_commit = repo.commit(\"aa9f8c73c2a2d164e9e117b99a6543893eeed23f\")\n    except ValueError:\n        raise ValueError(\"This is not the jaxngp repository\")\n\n    os.chdir(repo.git.working_dir)\n\n    exp_dir = Path(exp_dir)\n    save_root_dir = exp_dir.joinpath(\"runs\")\n    save_root_dir.mkdir(parents=False, exist_ok=True)\n    epoch = 0\n    def save_dir_name(note: str | None) -> str:\n        ret = \"{}{:04d}\".format(name_prefix, epoch)\n        if note is not None:\n            ret += \"#\" + note\n        return ret\n    save_dir = save_root_dir.joinpath(save_dir_name(note=note))\n    while len(tuple(save_root_dir.glob(save_dir_name(note=None) + \"*\"))) > 0:\n        epoch += 1\n        save_dir = save_root_dir.joinpath(save_dir_name(note=note))\n\n    latest_run_lnk = exp_dir.joinpath(\"{}latest-run\".format(name_prefix))\n    if latest_run_lnk.exists():\n        if (\n            latest_run_lnk.is_symlink()\n            and latest_run_lnk.readlink().parent.absolute() == save_dir.parent.absolute()\n        ):\n            latest_run_lnk.unlink()\n        else:\n            raise RuntimeError(\n                \"the path '{}' exists but is not a symlink to a previous run\".format(latest_run_lnk)\n            )\n    elif latest_run_lnk.is_symlink():  # the link does not exist, but it is a symlink, that makes it a broken symlink\n        latest_run_lnk.unlink()\n\n    save_dir.mkdir(parents=False, exist_ok=False)\n    latest_run_lnk.symlink_to(save_dir.absolute())\n\n    shutil.copyfile(\"flake.nix\", save_dir.joinpath(\"flake.nix\"))\n    shutil.copyfile(\"flake.lock\", save_dir.joinpath(\"flake.lock\"))\n    shutil.copyfile(\"pyproject.toml\", save_dir.joinpath(\"pyproject.toml\"))\n    shutil.copyfile(\"README.md\", save_dir.joinpath(\"README.md\"))\n\n    def ignored_files(dir, files):\n        return [\n            \"result\",\n            \".clangd\",\n            \".clang-format\",\n        ]\n\n    shutil.copytree(\"app\", save_dir.joinpath(\"app\"), dirs_exist_ok=False, ignore=ignored_files)\n    shutil.copytree(\"deps\", save_dir.joinpath(\"deps\"), dirs_exist_ok=False, ignore=ignored_files)\n    shutil.copytree(\"models\", save_dir.joinpath(\"models\"), dirs_exist_ok=False, ignore=ignored_files)\n    shutil.copytree(\"utils\", save_dir.joinpath(\"utils\"), dirs_exist_ok=False, ignore=ignored_files)\n\n    with open(save_dir.joinpath(\"commit-sha\"), \"w\") as f:\n        f.write(repo.head.object.hexsha)\n    with open(save_dir.joinpath(\"working-directory\"), \"w\") as f:\n        f.write(os.getcwd())\n\n    return save_dir", "\n\ndef compose(*fns):\n    def _inner(x):\n        for fn in fns:\n            x = fn(x)\n        return x\n    return _inner\n\n\ndef mkValueError(desc, value, type):\n    variants = get_args(type)\n    assert value not in variants\n    return ValueError(\"Unexpected {}: '{}', expected one of [{}]\".format(desc, value, \"|\".join(variants)))", "\n\ndef mkValueError(desc, value, type):\n    variants = get_args(type)\n    assert value not in variants\n    return ValueError(\"Unexpected {}: '{}', expected one of [{}]\".format(desc, value, \"|\".join(variants)))\n\n\n# NOTE:\n#   Jitting a vmapped function seems to give the desired performance boost, while vmapping a jitted", "# NOTE:\n#   Jitting a vmapped function seems to give the desired performance boost, while vmapping a jitted\n#   function might not work at all.  Except for the experiments I conducted myself, some related\n#   issues:\n# REF:\n#   * <https://github.com/google/jax/issues/6312>\n#   * <https://github.com/google/jax/issues/7449>\ndef vmap_jaxfn_with(\n        # kwargs copied from `jax.vmap` source\n        in_axes: int | Sequence[Any]=0,\n        out_axes: Any = 0,\n        axis_name: Hashable | None = None,\n        axis_size: int | None = None,\n        spmd_axis_name: Hashable | None = None,\n    ):\n    return functools.partial(\n        jax.vmap,\n        in_axes=in_axes,\n        out_axes=out_axes,\n        axis_name=axis_name,\n        axis_size=axis_size,\n        spmd_axis_name=spmd_axis_name,\n    )", "\n\ndef jit_jaxfn_with(\n        # kwargs copied from `jax.jit` source\n        static_argnums: int | Iterable[int] | None = None,\n        static_argnames: str | Iterable[str] | None = None,\n        device: xc.Device | None = None,\n        backend: str | None = None,\n        donate_argnums: int | Iterable[int] = (),\n        inline: bool = False,\n        keep_unused: bool = False,\n        abstracted_axes: Any | None = None,\n    ):\n    return functools.partial(\n        jax.jit,\n        static_argnums=static_argnums,\n        static_argnames=static_argnames,\n        device=device,\n        backend=backend,\n        donate_argnums=donate_argnums,\n        inline=inline,\n        keep_unused=keep_unused,\n        abstracted_axes=abstracted_axes,\n    )", "\n\ndef setup_logging(\n    name: str,\n    /,\n    file: str | Path | None=None,\n    with_tensorboard: bool=False,\n    level: LogLevel=\"INFO\",\n    file_level: LogLevel=\"DEBUG\",\n) -> Logger:\n    colorama.just_fix_windows_console()\n\n    class _formatter(logging.Formatter):\n        def __init__(self, datefmt, rich_color: bool):\n            fore = {\n                \"blue\": Fore.BLUE if rich_color else \"[\",\n                \"green\": Fore.GREEN if rich_color else \"[\",\n                \"yellow\": Fore.YELLOW if rich_color else \"[\",\n                \"red\": Fore.RED if rich_color else \"[\",\n                \"black\": Fore.BLACK if rich_color else \"[\",\n            }\n            back = {\n                \"red\": Back.RED if rich_color else \"[\",\n                \"yellow\": Back.YELLOW if rich_color else \"[\",\n            }\n            style = {\n                \"bright\": Style.BRIGHT if rich_color else \"[\",\n                \"reset_all\": Style.RESET_ALL if rich_color else \"]\",\n            }\n\n            pathfmt = \"%(module)s::%(funcName)s\"\n            fmt = \"| %(asctime)s.%(msecs)03dZ LVL {bold}{pathfmt}{reset}: %(message)s\".format(\n                bold=style[\"bright\"],\n                pathfmt=pathfmt,\n                reset=style[\"reset_all\"],\n            )\n            formats = {\n                logging.DEBUG: fmt.replace(\"LVL\", fore[\"blue\"] + \"DEBUG\" + style[\"reset_all\"]),\n                logging.INFO: fmt.replace(\"LVL\", \" \" + fore[\"green\"] + \"INFO\" + style[\"reset_all\"]),\n                logging.WARN: fmt.replace(\"LVL\", \" \" + back[\"yellow\"] + fore[\"black\"] + \"WARN\" + style[\"reset_all\"]),\n                logging.ERROR: fmt.replace(\"LVL\", back[\"red\"] + fore[\"black\"] + \"ERROR\" + style[\"reset_all\"]),\n                logging.CRITICAL: fmt.replace(\"LVL\", \" \" + back[\"red\"] + fore[\"black\"] + style[\"bright\"] + \"CRIT\" + style[\"reset_all\"]),\n            }\n            self.formatters = {\n                level: logging.Formatter(fmt=format, datefmt=datefmt)\n                for level, format in formats.items()\n            }\n\n        def format(self, record):\n            return self.formatters.get(record.levelno).format(record)\n\n    datefmt = \"%Y-%m-%dT%T\"\n\n    logger = Logger(name=name, level=level)\n    logger.propagate = False\n\n    # console handler\n    ch = logging.StreamHandler()\n    ch.setLevel(level)\n    ch.setFormatter(_formatter(datefmt=datefmt, rich_color=True))\n    logger.addHandler(ch)\n    logger.setLevel(level)\n\n    # file handler\n    if file is not None:\n        fh = logging.FileHandler(filename=file)\n        fh.setLevel(file_level)\n        fh.setFormatter(_formatter(datefmt=datefmt, rich_color=False))\n        logger.addHandler(fh)\n        def loglevel2int(log_level: LogLevel) -> int:\n            return getattr(logging, log_level)\n        logger.setLevel(min(loglevel2int(level), loglevel2int(file_level)))\n\n    if with_tensorboard:\n        tb = tensorboard.SummaryWriter(log_dir=file.parent, auto_flush=True)\n        executor = ThreadPoolExecutor(\n            max_workers=1,\n            thread_name_prefix=\"logger({})-\".format(name),\n        )\n        logger.setup_tensorboard(tb=tb, executor=executor)\n\n    # logger complains about `warn` being deprecated with another warning\n    logger.warn = logger.warning\n    return logger", "\n\ndef set_deterministic(seed: int) -> jran.KeyArray:\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    return jran.PRNGKey(seed)\n"]}
{"filename": "utils/args.py", "chunked_list": ["from dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport tyro\n\nfrom utils.types import (\n    CameraOverrideOptions,\n    LogLevel,\n    OrbitTrajectoryOptions,", "    LogLevel,\n    OrbitTrajectoryOptions,\n    RayMarchingOptions,\n    RenderingOptions,\n    SceneOptions,\n    TransformsProvider,\n)\n\n\n@dataclass(frozen=True, kw_only=True)\nclass CommonArgs:\n    # log level\n    logging: LogLevel = \"INFO\"\n\n    # random seed\n    seed: int = 1_000_000_007\n\n    # display model information after model init\n    summary: bool=False", "\n@dataclass(frozen=True, kw_only=True)\nclass CommonArgs:\n    # log level\n    logging: LogLevel = \"INFO\"\n\n    # random seed\n    seed: int = 1_000_000_007\n\n    # display model information after model init\n    summary: bool=False", "\n\n@dataclass(frozen=True, kw_only=True)\nclass TrainingArgs:\n    # learning rate\n    lr: float\n\n    # scalar multiplied to total variation loss, set this to a positive value to enable calculation\n    # of TV loss\n    tv_scale: float\n\n    # batch size\n    bs: int\n\n    # number of epochs to train\n    epochs: int\n\n    # batches per epoch\n    iters: int\n\n    # loop within training data for this number of iterations, this helps reduce the effective\n    # dataloader overhead.\n    data_loop: int\n\n    # will validate every `validate_every` epochs, set this to a large value to disable validation\n    validate_every: int\n\n    # number of latest checkpoints to keep\n    keep: int=1\n\n    # how many epochs should a new checkpoint to be kept (in addition to keeping the last `keep`\n    # checkpoints)\n    keep_every: int | None=8\n\n    @property\n    def keep_every_n_steps(self) -> int | None:\n        if self.keep_every is None:\n            return None\n        else:\n            return self.keep_every * self.iters", "\n@dataclass(frozen=True, kw_only=True)\nclass ImageFitArgs:\n    common: tyro.conf.OmitArgPrefixes[CommonArgs]=CommonArgs()\n    train: tyro.conf.OmitArgPrefixes[TrainingArgs]=TrainingArgs(\n        # paper:\n        #   We observed fastest convergence with a learning rate of 10^{-4} for signed distance\n        #   functions and 10^{-2} otherwise\n        #\n        # We use a smaller learning rate since our batch size is much smaller the paper (see below).\n        lr=1e-3,\n        tv_scale=0.,\n        # paper:\n        #   ...as well a a batch size of 2^{14} for neural radiance caching and 2^{18} otherwise.\n        #\n        # In our case, setting the batch size to a larger number hinders data loading performance,\n        # and thus causes the GPU not being fully occupied.  On the other hand, setting the batch\n        # size to a smaller one utilizes the GPU fully, but the iterations per second capped at\n        # some rate which results in lower throughput. setting bs to 2^{10} achieves a satisfying\n        # tradeoff here.\n        bs=2**10,\n        epochs=32,\n        iters=2**30,\n        data_loop=1,\n        validate_every=1,\n    )", "\n\n@dataclass(frozen=True, kw_only=True)\nclass NeRFArgsBase:\n    raymarch: RayMarchingOptions\n    render: RenderingOptions\n    scene: SceneOptions\n\n    common: tyro.conf.OmitArgPrefixes[CommonArgs]=CommonArgs()\n", "\n\n@dataclass(frozen=True, kw_only=True)\nclass _SharedNeRFTrainingArgs(NeRFArgsBase):\n    # each experiment run will save its own code, config, training logs, and checkpoints into a\n    # separate subdirectory of this path\n    exp_dir: Path\n\n    # directories or transform.json files containing data for training\n    frames_train: tyro.conf.Positional[Tuple[Path, ...]]\n\n    # an optional description of this run\n    note: str | None=None\n\n    # directories or transform.json files containing data for validation\n    frames_val: Tuple[Path, ...]=()\n\n    # if specified, continue training from this checkpoint\n    ckpt: Path | None=None\n\n    # training hyper parameters\n    train: tyro.conf.OmitArgPrefixes[TrainingArgs]=TrainingArgs(\n        # This is a relatively large learning rate, should be used jointly with\n        # `threasholded_exponential` as density activation, and random color as supervision for\n        # transparent pixels.\n        lr=1e-2,\n        tv_scale=0.,\n        bs=1024 * (1<<10),\n        epochs=50,\n        iters=2**10,\n        data_loop=1,\n        validate_every=10,\n    )\n\n    # raymarching/rendering options during training\n    raymarch: RayMarchingOptions=RayMarchingOptions(\n        diagonal_n_steps=1<<10,\n        perturb=True,\n        density_grid_res=128,\n    )\n    render: RenderingOptions=RenderingOptions(\n        bg=(1.0, 1.0, 1.0),  # white, but ignored by default due to random_bg=True\n        random_bg=True,\n    )\n    scene: SceneOptions=SceneOptions(\n        sharpness_threshold=-1.,\n        resolution_scale=1.0,\n        camera_near=0.3,\n        max_mem_mbytes=2500,  # ~300 1920x1080 8bit RGBA images\n    )\n\n    # raymarching/rendering options for validating during training\n    raymarch_eval: RayMarchingOptions=RayMarchingOptions(\n        diagonal_n_steps=1<<10,\n        perturb=False,\n        density_grid_res=128,\n    )\n    render_eval: RenderingOptions=RenderingOptions(\n        bg=(0.0, 0.0, 0.0),  # black\n        random_bg=False,\n    )", "\n\n@dataclass(frozen=True, kw_only=True)\nclass NeRFTrainingArgs(_SharedNeRFTrainingArgs): ...\n\n\n@dataclass(frozen=True, kw_only=True)\nclass NeRFTestingArgs(NeRFArgsBase):\n    # testing logs and results are saved to this directory, overwriting its content if any\n    logs_dir: Path\n\n    frames: tyro.conf.Positional[Tuple[Path, ...]]\n\n    camera_override: CameraOverrideOptions=CameraOverrideOptions()\n\n    # use checkpoint from this path (can be a directory) for testing\n    ckpt: Path\n\n    # if specified, render with a generated orbiting trajectory instead of the loaded frame\n    # transformations\n    trajectory: TransformsProvider=\"loaded\"\n\n    orbit: OrbitTrajectoryOptions\n\n    # naturally sort frames according to their file names before testing\n    sort_frames: bool=False\n\n    # if specified value contains \"video\", a video will be saved; if specified value contains\n    # \"image\", rendered images will be saved.  Value can contain both \"video\" and \"image\", e.g.,\n    # `--save-as \"video-image\"` will save both video and images.\n    save_as: str=\"image and video\"\n\n    # specifies frames per second for saved video\n    fps: int=24\n\n    # loop rendered images this many times in saved video\n    loop: int=3\n\n    # raymarching/rendering options during testing\n    raymarch: RayMarchingOptions=RayMarchingOptions(\n        diagonal_n_steps=1<<10,\n        perturb=False,\n        density_grid_res=128,\n    )\n    render: RenderingOptions=RenderingOptions(\n        bg=(0.0, 0.0, 0.0),  # black\n        random_bg=False,\n    )\n    scene: SceneOptions=SceneOptions(\n        sharpness_threshold=-1.,\n        resolution_scale=1.0,\n        camera_near=0.3,\n        max_mem_mbytes=0,  # this is testing, no images is loaded to GPU\n    )\n\n    @property\n    def report_metrics(self) -> bool:\n        return self.camera_override.enabled or self.trajectory != \"loaded\"", "\n\n@dataclass(frozen=True, kw_only=True)\nclass NeRFGUIArgs(_SharedNeRFTrainingArgs):\n\n    @dataclass(frozen=True, kw_only=True)\n    class ViewportOptions:\n        W: int=1024\n        H: int=768\n\n        resolution_scale: float=0.3\n\n        control_window_width: int=300\n\n        #max number of loss steps shown on gui\n        max_show_loss_step: int=200\n\n    viewport: ViewportOptions=ViewportOptions()\n\n    train: TrainingArgs=TrainingArgs(\n        lr=1e-2,\n        tv_scale=0.,\n        bs=1<<18,\n        iters=5,  # render a frame every 5 steps\n        epochs=50,  # ignored\n        data_loop=1,  # ignored\n        validate_every=10,  # ignored\n    )", ""]}
{"filename": "models/nerfs.py", "chunked_list": ["import functools\nfrom typing import Callable, List, Tuple\n\nimport flax.linen as nn\nimport jax\nfrom jax.nn.initializers import Initializer\nimport jax.numpy as jnp\n\nfrom models.encoders import (\n    Encoder,", "from models.encoders import (\n    Encoder,\n    FrequencyEncoder,\n    HashGridEncoder,\n    SphericalHarmonicsEncoder,\n    SphericalHarmonicsEncoderCuda,\n    TCNNHashGridEncoder,\n)\nfrom utils.common import mkValueError\nfrom utils.types import (", "from utils.common import mkValueError\nfrom utils.types import (\n    ActivationType,\n    DirectionalEncodingType,\n    PositionalEncodingType,\n    empty_impl,\n)\n\n\n@empty_impl\nclass NeRF(nn.Module):\n    bound: float\n\n    position_encoder: Encoder\n    direction_encoder: Encoder\n\n    density_mlp: nn.Module\n    rgb_mlp: nn.Module\n\n    density_activation: Callable\n    rgb_activation: Callable\n\n    @nn.compact\n    def __call__(\n        self,\n        xyz: jax.Array,\n        dir: jax.Array | None,\n        appearance_embeddings: jax.Array | None,\n    ) -> jax.Array | Tuple[jax.Array, jax.Array]:\n        \"\"\"\n        Inputs:\n            xyz `[..., 3]`: coordinates in $\\R^3$\n            dir `[..., 3]`: **unit** vectors, representing viewing directions.  If `None`, only\n                            return densities\n            appearance_embeddings `[..., n_extra_learnable_dims]` or `[n_extra_learnable_dims]`:\n                per-image latent code to model illumination, if it's a 1D vector of length\n                `n_extra_learnable_dims`, all sampled points will use this embedding.\n\n        Returns:\n            density `[..., 1]`: density (ray terminating probability) of each query points\n            rgb `[..., 3]`: predicted color for each query point\n        \"\"\"\n        original_aux_shapes = xyz.shape[:-1]\n        n_samples = functools.reduce(int.__mul__, original_aux_shapes)\n        xyz = xyz.reshape(n_samples, 3)\n\n        # [n_samples, D_pos], `float32`\n        pos_enc, tv = self.position_encoder(xyz, self.bound)\n\n        x = self.density_mlp(pos_enc)\n        # [n_samples, 1], [n_samples, density_MLP_out-1]\n        density, _ = jnp.split(x, [1], axis=-1)\n\n        if dir is None:\n            return density.reshape(*original_aux_shapes, 1), tv\n        dir = dir.reshape(n_samples, 3)\n\n        # [n_samples, D_dir]\n        dir_enc = self.direction_encoder(dir)\n\n        # [n_samples, 3]\n        rgb = self.rgb_mlp(jnp.concatenate([\n            x,\n            dir_enc,\n            jnp.broadcast_to(appearance_embeddings, (n_samples, appearance_embeddings.shape[-1])),\n        ], axis=-1))\n\n        density, rgb = self.density_activation(density), self.rgb_activation(rgb)\n\n        return jnp.concatenate([density, rgb], axis=-1).reshape(*original_aux_shapes, 4), tv", "\n@empty_impl\nclass NeRF(nn.Module):\n    bound: float\n\n    position_encoder: Encoder\n    direction_encoder: Encoder\n\n    density_mlp: nn.Module\n    rgb_mlp: nn.Module\n\n    density_activation: Callable\n    rgb_activation: Callable\n\n    @nn.compact\n    def __call__(\n        self,\n        xyz: jax.Array,\n        dir: jax.Array | None,\n        appearance_embeddings: jax.Array | None,\n    ) -> jax.Array | Tuple[jax.Array, jax.Array]:\n        \"\"\"\n        Inputs:\n            xyz `[..., 3]`: coordinates in $\\R^3$\n            dir `[..., 3]`: **unit** vectors, representing viewing directions.  If `None`, only\n                            return densities\n            appearance_embeddings `[..., n_extra_learnable_dims]` or `[n_extra_learnable_dims]`:\n                per-image latent code to model illumination, if it's a 1D vector of length\n                `n_extra_learnable_dims`, all sampled points will use this embedding.\n\n        Returns:\n            density `[..., 1]`: density (ray terminating probability) of each query points\n            rgb `[..., 3]`: predicted color for each query point\n        \"\"\"\n        original_aux_shapes = xyz.shape[:-1]\n        n_samples = functools.reduce(int.__mul__, original_aux_shapes)\n        xyz = xyz.reshape(n_samples, 3)\n\n        # [n_samples, D_pos], `float32`\n        pos_enc, tv = self.position_encoder(xyz, self.bound)\n\n        x = self.density_mlp(pos_enc)\n        # [n_samples, 1], [n_samples, density_MLP_out-1]\n        density, _ = jnp.split(x, [1], axis=-1)\n\n        if dir is None:\n            return density.reshape(*original_aux_shapes, 1), tv\n        dir = dir.reshape(n_samples, 3)\n\n        # [n_samples, D_dir]\n        dir_enc = self.direction_encoder(dir)\n\n        # [n_samples, 3]\n        rgb = self.rgb_mlp(jnp.concatenate([\n            x,\n            dir_enc,\n            jnp.broadcast_to(appearance_embeddings, (n_samples, appearance_embeddings.shape[-1])),\n        ], axis=-1))\n\n        density, rgb = self.density_activation(density), self.rgb_activation(rgb)\n\n        return jnp.concatenate([density, rgb], axis=-1).reshape(*original_aux_shapes, 4), tv", "\n\nclass CoordinateBasedMLP(nn.Module):\n    \"Coordinate-based MLP\"\n\n    # hidden layer widths\n    Ds: List[int]\n    out_dim: int\n    skip_in_layers: List[int]\n\n    # as described in the paper\n    kernel_init: Initializer=nn.initializers.glorot_uniform()\n\n    @nn.compact\n    def __call__(self, x: jax.Array) -> jax.Array:\n        in_x = x\n        for i, d in enumerate(self.Ds):\n            if i in self.skip_in_layers:\n                x = jnp.concatenate([in_x, x], axis=-1)\n            x = nn.Dense(\n                d,\n                use_bias=False,\n                kernel_init=self.kernel_init,\n            )(x)\n            x = nn.relu(x)\n        x = nn.Dense(\n            self.out_dim,\n            use_bias=False,\n            kernel_init=self.kernel_init,\n        )(x)\n        return x", "\n\nclass BackgroundModel(nn.Module): ...\n\n\n@empty_impl\nclass SkySphereBg(BackgroundModel):\n    \"\"\"\n    A sphere that centers at the origin and encloses a bounded scene and provides all the background\n    color, this is an over-simplified model.\n\n    When a ray intersects with the sphere from inside, it calculates the intersection point's\n    coordinate and predicts a color based on the intersection point and the viewing direction.\n    \"\"\"\n\n    # radius\n    r: float\n\n    # encoder for position\n    position_encoder: Encoder\n\n    # encoder for viewing direction\n    direction_encoder: Encoder\n\n    # color predictor\n    rgb_mlp: CoordinateBasedMLP\n\n    activation: Callable\n\n    @nn.compact\n    def __call__(\n        self,\n        rays_o: jax.Array,\n        rays_d: jax.Array,\n        appearance_embeddings: jax.Array,\n    ) -> jax.Array:\n        # the distance of a point (o+td) on the ray to the origin is given by:\n        #\n        #   dist(t) = (dx^2 + dy^2 + dz^2)t^2 + 2(dx*ox + dy*oy + dz*oz)t + ox^2 + oy^2 + oz^2\n        #\n        # the minimal distance is achieved when\n        #\n        #   2(dx^2 + dy^2 + dz^2)t + 2(dx*ox + dy*oy + dz*oz) = 0,\n        #       ==> t = -(dx*ox + dy*oy + dz*oz) / (dx^2 + dy^2 + dz^2)\n        a = (rays_d * rays_d).sum(axis=-1)\n        b = 2 * (rays_o * rays_d).sum(axis=-1)\n        c = (rays_o * rays_o).sum(axis=-1) - self.r ** 2\n\n        # if min_dist < self.r, there are at most two intersections, given by:\n        #\n        #   dist(t) = r^2\n        #\n        # want the farther intersection point\n        t = jnp.maximum(\n            (-b + jnp.sqrt(b ** 2 - 4 * a * c)) / (2 * a),\n            (-b - jnp.sqrt(b ** 2 - 4 * a * c)) / (2 * a),\n        )\n        t = t.reshape(-1, 1)\n\n        finite_mask = jnp.isfinite(t)\n\n        pos = rays_o + t * rays_d\n        pos_dirs = jnp.where(\n            finite_mask,\n            pos / (jnp.linalg.norm(pos) + 1e-15),\n            0.,\n        )\n\n        n_rays = functools.reduce(int.__mul__, rays_d.shape[:-1])\n        # we then encode the positions/directions, and predict a view-dependent color for each ray\n        pos_enc = self.position_encoder(pos_dirs)\n        dir_enc = self.direction_encoder(rays_d)\n        appearance_embeddings = jnp.broadcast_to(\n            appearance_embeddings,\n            shape=(n_rays, appearance_embeddings.shape[-1]),\n        )\n\n        colors = self.rgb_mlp(jnp.concatenate([pos_enc, dir_enc, appearance_embeddings], axis=-1))\n        colors = self.activation(colors)\n\n        return jnp.where(\n            finite_mask,\n            colors,\n            0.,\n        )", "\n\ndef make_activation(act: ActivationType):\n    if act == \"sigmoid\":\n        return nn.sigmoid\n    elif act == \"exponential\":\n        return jnp.exp\n    elif act == \"truncated_exponential\":\n        @jax.custom_vjp\n        def trunc_exp(x):\n            \"Exponential function, except its gradient calculation uses a truncated input value\"\n            return jnp.exp(x)\n        def __fwd_trunc_exp(x):\n            y = trunc_exp(x)\n            aux = x  # aux contains additional information that is useful in the backward pass\n            return y, aux\n        def __bwd_trunc_exp(aux, grad_y):\n            # REF: <https://github.com/NVlabs/instant-ngp/blob/d0d35d215c7c63c382a128676f905ecb676fa2b8/src/testbed_nerf.cu#L303>\n            grad_x = jnp.exp(jnp.clip(aux, -15, 15)) * grad_y\n            return (grad_x, )\n        trunc_exp.defvjp(\n            fwd=__fwd_trunc_exp,\n            bwd=__bwd_trunc_exp,\n        )\n        return trunc_exp\n\n    elif act == \"thresholded_exponential\":\n        def thresh_exp(x, thresh):\n            \"\"\"\n            Exponential function translated along -y direction by 1e-2, and thresholded to have\n            non-negative values.\n            \"\"\"\n            # paper:\n            #   the occupancy grids ... is updated every 16 steps ... corresponds to thresholding\n            #   the opacity of a minimal ray marching step by 1 \u2212 exp(\u22120.01) \u2248 0.01\n            return nn.relu(jnp.exp(x) - thresh)\n        return functools.partial(thresh_exp, thresh=1e-2)\n\n    elif act == \"truncated_thresholded_exponential\":\n        @jax.custom_vjp\n        def trunc_thresh_exp(x, thresh):\n            \"\"\"\n            Exponential, but value is translated along -y axis by value `thresh`, negative values\n            are removed, and gradient is truncated.\n            \"\"\"\n            return nn.relu(jnp.exp(x) - thresh)\n        def __fwd_trunc_threash_exp(x, thresh):\n            y = trunc_thresh_exp(x, thresh=thresh)\n            aux = x, thresh  # aux contains additional information that is useful in the backward pass\n            return y, aux\n        def __bwd_trunc_threash_exp(aux, grad_y):\n            x, thresh = aux\n            grad_x = jnp.exp(jnp.clip(x, -15, 15)) * grad_y\n            # clip gradient for values that has been thresholded by relu during forward pass\n            grad_x = jnp.signbit(jnp.log(thresh) - x) * grad_x\n            # first tuple element is gradient for input, second tuple element is gradient for the\n            # `threshold` value.\n            return (grad_x, 0)\n        trunc_thresh_exp.defvjp(\n            fwd=__fwd_trunc_threash_exp,\n            bwd=__bwd_trunc_threash_exp,\n        )\n        return functools.partial(trunc_thresh_exp, thresh=1e-2)\n    elif act == \"relu\":\n        return nn.relu\n    else:\n        raise mkValueError(\n            desc=\"activation\",\n            value=act,\n            type=ActivationType,\n        )", "\n\ndef make_nerf(\n    bound: float,\n\n    # encodings\n    pos_enc: PositionalEncodingType,\n    dir_enc: DirectionalEncodingType,\n\n    # total variation\n    tv_scale: float,\n\n    # encoding levels\n    pos_levels: int,\n    dir_levels: int,\n\n    # layer widths\n    density_Ds: List[int],\n    rgb_Ds: List[int],\n\n    # output dimensions\n    density_out_dim: int,\n    rgb_out_dim: int,\n\n    # skip connections\n    density_skip_in_layers: List[int],\n    rgb_skip_in_layers: List[int],\n\n    # activations\n    density_act: ActivationType,\n    rgb_act: ActivationType,\n) -> NeRF:\n    if pos_enc == \"identity\":\n        position_encoder = lambda x: x\n    elif pos_enc == \"frequency\":\n        raise NotImplementedError(\"Frequency encoding for NeRF is not tuned\")\n        position_encoder = FrequencyEncoder(L=10)\n    elif \"hashgrid\" in pos_enc:\n        HGEncoder = TCNNHashGridEncoder if \"tcnn\" in pos_enc else HashGridEncoder\n        position_encoder = HGEncoder(\n            L=pos_levels,\n            T=2**19,\n            F=2,\n            N_min=2**4,\n            N_max=int(2**11 * bound),\n            tv_scale=tv_scale,\n            param_dtype=jnp.float32,\n        )\n    else:\n        raise mkValueError(\n            desc=\"positional encoding\",\n            value=pos_enc,\n            type=PositionalEncodingType,\n        )\n\n    if dir_enc == \"identity\":\n        direction_encoder = lambda x: x\n    elif dir_enc == \"sh\":\n        direction_encoder = SphericalHarmonicsEncoder(L=dir_levels)\n    elif dir_enc == \"shcuda\":\n        direction_encoder = SphericalHarmonicsEncoderCuda(L=dir_levels)\n    else:\n        raise mkValueError(\n            desc=\"directional encoding\",\n            value=dir_enc,\n            type=DirectionalEncodingType,\n        )\n\n    density_mlp = CoordinateBasedMLP(\n        Ds=density_Ds,\n        out_dim=density_out_dim,\n        skip_in_layers=density_skip_in_layers\n    )\n    rgb_mlp = CoordinateBasedMLP(\n        Ds=rgb_Ds,\n        out_dim=rgb_out_dim,\n        skip_in_layers=rgb_skip_in_layers\n    )\n\n    density_activation = make_activation(density_act)\n    rgb_activation = make_activation(rgb_act)\n\n    model = NeRF(\n        bound=bound,\n\n        position_encoder=position_encoder,\n        direction_encoder=direction_encoder,\n\n        density_mlp=density_mlp,\n        rgb_mlp=rgb_mlp,\n\n        density_activation=density_activation,\n        rgb_activation=rgb_activation,\n    )\n\n    return model", "\n\ndef make_skysphere_background_model(\n    radius: float,\n\n    pos_levels: int,\n    dir_levels: int,\n\n    Ds: List[int],\n    skip_in_layers: List[int],\n\n    act: ActivationType,\n) -> SkySphereBg:\n    position_encoder = SphericalHarmonicsEncoder(L=pos_levels)\n    direction_encoder = SphericalHarmonicsEncoder(L=dir_levels)\n    rgb_mlp = CoordinateBasedMLP(\n        Ds=Ds,\n        out_dim=3,\n        skip_in_layers=skip_in_layers,\n    )\n    activation = make_activation(act)\n    return SkySphereBg(\n        r=radius,\n        position_encoder=position_encoder,\n        direction_encoder=direction_encoder,\n        rgb_mlp=rgb_mlp,\n        activation=activation,\n    )", "\n\ndef make_skysphere_background_model_ngp(bound: float) -> SkySphereBg:\n    return make_skysphere_background_model(\n        radius=bound*4,\n        pos_levels=2,\n        dir_levels=4,\n        Ds=[32, 32],\n        skip_in_layers=[],\n        act=\"sigmoid\",\n    )", "\n\ndef make_nerf_ngp(\n    bound: float,\n    inference: bool,\n    tv_scale: float=0.,\n) -> NeRF:\n    return make_nerf(\n        bound=bound,\n\n        pos_enc=(\n            # TCNN's hash grid encoding runs faster at inference-time, but during training the JAX\n            # implementation is slightly faster.  It's possible to use one at training-time and\n            # another at inference-time, because the two implementations optimizes in an identical\n            # way (up to numerical error).\n            \"tcnn-hashgrid\"\n            if inference\n            else \"hashgrid\"\n        ),\n        dir_enc=\"sh\",\n        tv_scale=tv_scale,\n\n        pos_levels=16,\n        dir_levels=4,\n\n        density_Ds=[64],\n        density_out_dim=16,\n        density_skip_in_layers=[],\n        density_act=\"truncated_exponential\",\n\n        rgb_Ds=[64, 64],\n        rgb_out_dim=3,\n        rgb_skip_in_layers=[],\n        rgb_act=\"sigmoid\",\n    )", "\n\ndef make_debug_nerf(bound: float) -> NeRF:\n    return NeRF(\n        bound=bound,\n        position_encoder=lambda x: x,\n        direction_encoder=lambda x: x,\n        density_mlp=CoordinateBasedMLP(\n            Ds=[64],\n            out_dim=16,\n            skip_in_layers=[],\n        ),\n        rgb_mlp=CoordinateBasedMLP(\n            Ds=[64, 64],\n            out_dim=3,\n            skip_in_layers=[],\n        ),\n        density_activation=lambda x: x,\n        rgb_activation=lambda x: x,\n    )", "\n\ndef make_test_cube(width: int, bound: float, density: float=32) -> NeRF:\n    @jax.jit\n    @jax.vmap\n    def cube_density_fn(x: jax.Array) -> jax.Array:\n        # x is pre-normalized unit cube, we map it back to specified aabb.\n        x = (x + bound) / (2 * bound)\n        mask = (abs(x).max(axis=-1, keepdims=True) <= width/2).astype(float)\n        # concatenate input xyz for use in later rgb querying\n        return jnp.concatenate([density * mask, x], axis=-1)\n\n    @jax.jit\n    @jax.vmap\n    def cube_rgb_fn(density_xyz_dir: jax.Array) -> jax.Array:\n        # xyz(3) + dir(3), only take xyz to infer color\n        x = density_xyz_dir[:3]\n        x = jnp.clip(x, -width/2, width/2)\n        return x / width + .5\n\n    return NeRF(\n        bound=bound,\n        position_encoder=lambda x: x,\n        direction_encoder=lambda x: x,\n        density_mlp=cube_density_fn,\n        rgb_mlp=cube_rgb_fn,\n        density_activation=lambda x: x,\n        rgb_activation=lambda x: x,\n    )", "\n\ndef main():\n    import jax.numpy as jnp\n    import jax.random as jran\n\n    KEY = jran.PRNGKey(0)\n    KEY, key = jran.split(KEY, 2)\n\n    m = make_nerf_ngp()\n\n    xyz = jnp.ones((100, 3))\n    dir = jnp.ones((100, 2))\n    params = m.init(key, xyz, dir)\n    print(m.tabulate(key, xyz, dir))\n\n    m = make_test_cube(\n        width=2,\n        bound=1,\n        density=32,\n    )\n    # params = m.init(key, xyz, dir)\n    # print(m.tabulate(key, xyz, dir))\n    density, rgb = m.apply(\n        {},\n        jnp.asarray([[0, 0, 0], [1, 1, 1], [1.1, 0, 0], [0.6, 0.9, -0.5], [0.99, 0.99, 0.99]]),\n        jnp.asarray([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]]),\n    )\n    print(density)\n    print(rgb)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "models/imagefit.py", "chunked_list": ["from typing import Literal\n\nimport chex\nimport flax.linen as nn\nfrom flax.linen.dtypes import Dtype\nimport jax\n\nfrom models.encoders import FrequencyEncoder, HashGridEncoder\n\n\nclass ImageFitter(nn.Module):\n    encoding: Literal[\"hashgrid\", \"frequency\"]\n    encoding_dtype: Dtype\n\n    @nn.compact\n    def __call__(self, uv: jax.Array) -> jax.Array:\n        \"\"\"\n        Inputs:\n            uv [..., 2]: coordinates in $\\\\R^2$ (normalized in range [0, 1]).\n\n        Returns:\n            rgb [..., 3]: predicted color for each input uv coordinate (normalized in range [0, 1]).\n        \"\"\"\n        chex.assert_axis_dimension(uv, -1, 2)\n\n        if self.encoding == \"hashgrid\":\n            # [..., L*F]\n            x = HashGridEncoder(\n                dim=2,\n                L=16,\n                # ~1Mi entries per level\n                T=2**20,\n                F=2,\n                N_min=16,\n                N_max=2**19,  # 524288\n                param_dtype=self.encoding_dtype,\n            )(uv)\n        elif self.encoding == \"frequency\":\n            # [..., dim*L]\n            x = FrequencyEncoder(dim=2, L=10)(uv)\n        else:\n            raise ValueError(\"Unexpected encoding type '{}'\".format(self.encoding))\n\n        DenseLayer = lambda dim, name: nn.Dense(\n                features=dim,\n                name=name,\n                # the paper uses glorot initialization, in practice glorot initialization converges\n                # to a better result than kaiming initialization, though the gap is small.\n                # TODO:\n                #   experiment with initializers (or not)\n                kernel_init=nn.initializers.lecun_normal(),\n                bias_init=nn.initializers.zeros,\n                param_dtype=x.dtype\n            )\n        # feed to the MLP\n        x = DenseLayer(128, name=\"linear1\")(x)\n        x = nn.relu(x)\n        x = DenseLayer(128, name=\"linear2\")(x)\n        x = nn.relu(x)\n\n        if self.encoding == \"frequency\":\n            x = nn.relu(DenseLayer(256, name=\"linear3\")(x))\n            x = nn.relu(DenseLayer(512, name=\"linear4\")(x))\n            x = nn.relu(DenseLayer(512, name=\"linear5\")(x))\n            x = nn.relu(DenseLayer(512, name=\"linear6\")(x))\n            x = nn.relu(DenseLayer(512, name=\"linear7\")(x))\n\n        x = nn.Dense(3, name=\"color_predictor\", param_dtype=x.dtype)(x)\n        rgb = nn.sigmoid(x)\n\n        return rgb", "\n\nclass ImageFitter(nn.Module):\n    encoding: Literal[\"hashgrid\", \"frequency\"]\n    encoding_dtype: Dtype\n\n    @nn.compact\n    def __call__(self, uv: jax.Array) -> jax.Array:\n        \"\"\"\n        Inputs:\n            uv [..., 2]: coordinates in $\\\\R^2$ (normalized in range [0, 1]).\n\n        Returns:\n            rgb [..., 3]: predicted color for each input uv coordinate (normalized in range [0, 1]).\n        \"\"\"\n        chex.assert_axis_dimension(uv, -1, 2)\n\n        if self.encoding == \"hashgrid\":\n            # [..., L*F]\n            x = HashGridEncoder(\n                dim=2,\n                L=16,\n                # ~1Mi entries per level\n                T=2**20,\n                F=2,\n                N_min=16,\n                N_max=2**19,  # 524288\n                param_dtype=self.encoding_dtype,\n            )(uv)\n        elif self.encoding == \"frequency\":\n            # [..., dim*L]\n            x = FrequencyEncoder(dim=2, L=10)(uv)\n        else:\n            raise ValueError(\"Unexpected encoding type '{}'\".format(self.encoding))\n\n        DenseLayer = lambda dim, name: nn.Dense(\n                features=dim,\n                name=name,\n                # the paper uses glorot initialization, in practice glorot initialization converges\n                # to a better result than kaiming initialization, though the gap is small.\n                # TODO:\n                #   experiment with initializers (or not)\n                kernel_init=nn.initializers.lecun_normal(),\n                bias_init=nn.initializers.zeros,\n                param_dtype=x.dtype\n            )\n        # feed to the MLP\n        x = DenseLayer(128, name=\"linear1\")(x)\n        x = nn.relu(x)\n        x = DenseLayer(128, name=\"linear2\")(x)\n        x = nn.relu(x)\n\n        if self.encoding == \"frequency\":\n            x = nn.relu(DenseLayer(256, name=\"linear3\")(x))\n            x = nn.relu(DenseLayer(512, name=\"linear4\")(x))\n            x = nn.relu(DenseLayer(512, name=\"linear5\")(x))\n            x = nn.relu(DenseLayer(512, name=\"linear6\")(x))\n            x = nn.relu(DenseLayer(512, name=\"linear7\")(x))\n\n        x = nn.Dense(3, name=\"color_predictor\", param_dtype=x.dtype)(x)\n        rgb = nn.sigmoid(x)\n\n        return rgb", ""]}
{"filename": "models/encoders.py", "chunked_list": ["import math\n\nimport chex\nimport flax.linen as nn\nfrom flax.linen.dtypes import Dtype\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jran\nfrom jaxtcnn import HashGridMetadata, hashgrid_encode\nimport shjax", "from jaxtcnn import HashGridMetadata, hashgrid_encode\nimport shjax\n\nfrom utils.common import jit_jaxfn_with, vmap_jaxfn_with\nfrom utils.types import empty_impl\n\n\ncell_vert_offsets = {\n    2: jnp.asarray([\n        [0., 0.],", "    2: jnp.asarray([\n        [0., 0.],\n        [0., 1.],\n        [1., 0.],\n        [1., 1.],\n    ]),\n    3: jnp.asarray([\n        [0., 0., 0.],\n        [0., 0., 1.],\n        [0., 1., 0.],", "        [0., 0., 1.],\n        [0., 1., 0.],\n        [0., 1., 1.],\n        [1., 0., 0.],\n        [1., 0., 1.],\n        [1., 1., 0.],\n        [1., 1., 1.],\n    ]),\n}\n", "}\n\nadjacent_offsets = {\n    2: jnp.asarray([\n        [0., 1.],\n        [1., 0.],\n        [0., -1.],\n        [-1., 0.],\n    ]),\n    3: jnp.asarray([", "    ]),\n    3: jnp.asarray([\n        [0., 0., 1.],\n        [0., 1., 0.],\n        [1., 0., 0.],\n        [0., 0., -1.],\n        [0., -1., 0.],\n        [-1., 0., 0.],\n    ]),\n}", "    ]),\n}\n\n\nclass Encoder(nn.Module): ...\n\n\n# TODO: enforce types used in arrays\n@empty_impl\nclass HashGridEncoder(Encoder):\n    # Let's use the same notations as in the paper\n\n    # Number of levels (16).\n    L: int\n    # Maximum entries per level (hash table size) (2**14 to 2**24).\n    T: int\n    # Number of feature dimensions per entry (2).\n    F: int\n    # Coarsest resolution (16).\n    N_min: int\n    # Finest resolution (512 to 524288).\n    N_max: int\n\n    tv_scale: float\n\n    param_dtype: Dtype = jnp.float32\n\n    @property\n    def b(self) -> float:\n        # Equation(3)\n        # Essentially, it is $(n_max / n_min) ** (1/(L - 1))$\n        return math.exp((math.log(self.N_max) - math.log(self.N_min)) / (self.L - 1))\n\n    @nn.compact\n    def __call__(self, pos: jax.Array, bound: float) -> jax.Array:\n        dim = pos.shape[-1]\n\n        # CAVEAT: hashgrid encoder is defined only in the unit cube [0, 1)^3\n        pos = (pos + bound) / (2 * bound)\n\n        scales, resolutions, first_hash_level, offsets = [], [], 0, [0]\n        for i in range(self.L):\n            scale = self.N_min * (self.b**i) - 1\n            scales.append(scale)\n            res = math.ceil(scale) + 1\n            resolutions.append(res)\n\n            n_entries = res ** dim\n\n            if n_entries <= self.T:\n                first_hash_level += 1\n            else:\n                n_entries = self.T\n\n            offsets.append(offsets[-1] + n_entries)\n\n        latents = self.param(\n            \"latent codes stored on grid vertices\",\n            # paper:\n            #   We initialize the hash table entries using the uniform distribution U(\u221210^{\u22124}, 10^{\u22124})\n            #   to provide a small amount of randomness while encouraging initial predictions close\n            #   to zero.\n            lambda key, shape, dtype: jran.uniform(key, shape, dtype, -1e-4, 1e-4),\n            (offsets[-1], self.F),\n            self.param_dtype,\n        )\n\n        @jax.vmap\n        @jax.vmap\n        def make_vert_pos(pos_scaled: jax.Array):\n            # [dim]\n            pos_floored = jnp.floor(pos_scaled)\n            # [2**dim, dim]\n            vert_pos = pos_floored[None, :] + cell_vert_offsets[dim]\n            return vert_pos.astype(jnp.uint32)\n\n        @jax.vmap\n        @jax.vmap\n        def make_adjacent_pos(pos_scaled: jax.Array):\n            # [dim]\n            pos_floored = jnp.floor(pos_scaled)\n            # [dim * 2, dim]\n            adjacent_pos = pos_floored[None, :] + adjacent_offsets[dim]\n            return adjacent_pos.astype(jnp.uint32)\n\n        @vmap_jaxfn_with(in_axes=(0, 0))\n        @vmap_jaxfn_with(in_axes=(None, 0))\n        def make_tiled_indices(res, vert_pos):\n            \"\"\"(first 2 axes `[L, n_points]` are vmapped away)\n            Inputs:\n                res `uint32` `[L]`: each hierarchy's resolution\n                vert_pos `uint32` `[L, n_points, B, dim]`: integer positions of grid cell's\n                                                           vertices, of each level\n\n            Returns:\n                indices `uint32` `[L, n_points, B]`: grid cell indices of the vertices\n            \"\"\"\n            # [dim]\n            if dim == 2:\n                strides = jnp.stack([jnp.ones_like(res), res]).T\n            elif dim == 3:\n                strides = jnp.stack([jnp.ones_like(res), res, res ** 2]).T\n            else:\n                raise NotImplementedError(\"{} is only implemented for 2D and 3D data\".format(__class__.__name__))\n            # [2**dim]\n            indices = jnp.sum(strides[None, :] * vert_pos, axis=-1)\n            return indices\n\n        @jax.vmap\n        @jax.vmap\n        def make_hash_indices(vert_pos):\n            \"\"\"(first 2 axes `[L, n_points]` are vmapped away)\n            Inputs:\n                vert_pos `uint32` `[L, n_points, B, dim]`: integer positions of grid cell's\n                                                           vertices, of each level\n\n            Returns:\n                indices `uint32` `[L, n_points, B]`: grid cell indices of the vertices\n            \"\"\"\n            # use primes as reported in the paper\n            primes = jnp.asarray([1, 2_654_435_761, 805_459_861], dtype=jnp.uint32)\n            # [2**dim]\n            if dim == 2:\n                indices = vert_pos[:, 0] ^ (vert_pos[:, 1] * primes[1])\n            elif dim == 3:\n                indices = vert_pos[:, 0] ^ (vert_pos[:, 1] * primes[1]) ^ (vert_pos[:, 2] * primes[2])\n            else:\n                raise NotImplementedError(\"{} is only implemented for 2D and 3D data\".format(__class__.__name__))\n            return indices\n\n        def make_indices(vert_pos, resolutions, first_hash_level):\n            if first_hash_level > 0:\n                resolutions = jnp.asarray(resolutions, dtype=jnp.uint32)\n                indices = make_tiled_indices(resolutions[:first_hash_level], vert_pos[:first_hash_level, ...])\n            else:\n                indices = jnp.empty(0, dtype=jnp.uint32)\n            if first_hash_level < self.L:\n                indices = jnp.concatenate([indices, make_hash_indices(vert_pos[first_hash_level:, ...])], axis=0)\n            indices = jnp.mod(indices, self.T)\n            indices += jnp.asarray(offsets[:-1], dtype=jnp.uint32)[:, None, None]\n            return indices\n\n        @jax.vmap\n        @jax.vmap\n        def lerp_weights(pos_scaled: jax.Array):\n            \"\"\"(first 2 axes `[L, n_points]` are vmapped away)\n            Inputs:\n                pos_scaled `float` `[L, n_points, dim]`: coordinates of query points, scaled to the\n                                                         hierarchy in question\n\n            Returns:\n                weights `float` `[L, n_points, 2**dim]`: linear interpolation weights for each cell\n                                                         vertex\n            \"\"\"\n            # [dim]\n            pos_offset, _ = jnp.modf(pos_scaled)\n            # [2**dim, dim]\n            widths = jnp.clip(\n                # cell_vert_offsets: [2**dim, dim]\n                (1 - cell_vert_offsets[dim]) + (2 * cell_vert_offsets[dim] - 1) * pos_offset[None, :],\n                0,\n                1,\n            )\n            # [2**dim]\n            return jnp.prod(widths, axis=-1)\n\n        # [L]\n        scales = jnp.asarray(scales, dtype=jnp.float32)\n        # [L, n_points, dim]\n        pos_scaled = pos[None, :, :] * scales[:, None, None] + 0.5\n        # [L, n_points, 2**dim, dim]\n        vert_pos = make_vert_pos(pos_scaled)\n\n        # [L, n_points, 2**dim]\n        indices = make_indices(vert_pos, resolutions, first_hash_level)\n\n        # [L, n_points, 2**dim, F]\n        vert_latents = latents[indices]\n        # [L, n_points, 2**dim]\n        vert_weights = lerp_weights(pos_scaled)\n\n        # [L, n_points, F]\n        encodings = (vert_latents * vert_weights[..., None]).sum(axis=-2)\n        # [n_points, L*F]\n        encodings = encodings.transpose(1, 0, 2).reshape(-1, self.L * self.F)\n\n        ## Total variation\n        if self.tv_scale > 0:\n            # [L, n_points, dim * 2, dim]\n            adjacent_pos = make_adjacent_pos(pos_scaled)\n\n            # [L, n_points, dim * 2]\n            adjacent_indices = make_indices(adjacent_pos, resolutions, first_hash_level)\n\n            # [L, n_points, dim * 2, F]\n            adjacent_latents = latents[adjacent_indices]\n\n            # [L, n_points, dim * 2, F]\n            tv = self.tv_scale * jnp.square(adjacent_latents - vert_latents[:, :, :1, :])\n\n            # [L, n_points]\n            tv = tv.sum(axis=(-2, -1))\n\n            tv = tv.mean()\n        else:\n            tv = 0\n\n        return encodings, tv", "@empty_impl\nclass HashGridEncoder(Encoder):\n    # Let's use the same notations as in the paper\n\n    # Number of levels (16).\n    L: int\n    # Maximum entries per level (hash table size) (2**14 to 2**24).\n    T: int\n    # Number of feature dimensions per entry (2).\n    F: int\n    # Coarsest resolution (16).\n    N_min: int\n    # Finest resolution (512 to 524288).\n    N_max: int\n\n    tv_scale: float\n\n    param_dtype: Dtype = jnp.float32\n\n    @property\n    def b(self) -> float:\n        # Equation(3)\n        # Essentially, it is $(n_max / n_min) ** (1/(L - 1))$\n        return math.exp((math.log(self.N_max) - math.log(self.N_min)) / (self.L - 1))\n\n    @nn.compact\n    def __call__(self, pos: jax.Array, bound: float) -> jax.Array:\n        dim = pos.shape[-1]\n\n        # CAVEAT: hashgrid encoder is defined only in the unit cube [0, 1)^3\n        pos = (pos + bound) / (2 * bound)\n\n        scales, resolutions, first_hash_level, offsets = [], [], 0, [0]\n        for i in range(self.L):\n            scale = self.N_min * (self.b**i) - 1\n            scales.append(scale)\n            res = math.ceil(scale) + 1\n            resolutions.append(res)\n\n            n_entries = res ** dim\n\n            if n_entries <= self.T:\n                first_hash_level += 1\n            else:\n                n_entries = self.T\n\n            offsets.append(offsets[-1] + n_entries)\n\n        latents = self.param(\n            \"latent codes stored on grid vertices\",\n            # paper:\n            #   We initialize the hash table entries using the uniform distribution U(\u221210^{\u22124}, 10^{\u22124})\n            #   to provide a small amount of randomness while encouraging initial predictions close\n            #   to zero.\n            lambda key, shape, dtype: jran.uniform(key, shape, dtype, -1e-4, 1e-4),\n            (offsets[-1], self.F),\n            self.param_dtype,\n        )\n\n        @jax.vmap\n        @jax.vmap\n        def make_vert_pos(pos_scaled: jax.Array):\n            # [dim]\n            pos_floored = jnp.floor(pos_scaled)\n            # [2**dim, dim]\n            vert_pos = pos_floored[None, :] + cell_vert_offsets[dim]\n            return vert_pos.astype(jnp.uint32)\n\n        @jax.vmap\n        @jax.vmap\n        def make_adjacent_pos(pos_scaled: jax.Array):\n            # [dim]\n            pos_floored = jnp.floor(pos_scaled)\n            # [dim * 2, dim]\n            adjacent_pos = pos_floored[None, :] + adjacent_offsets[dim]\n            return adjacent_pos.astype(jnp.uint32)\n\n        @vmap_jaxfn_with(in_axes=(0, 0))\n        @vmap_jaxfn_with(in_axes=(None, 0))\n        def make_tiled_indices(res, vert_pos):\n            \"\"\"(first 2 axes `[L, n_points]` are vmapped away)\n            Inputs:\n                res `uint32` `[L]`: each hierarchy's resolution\n                vert_pos `uint32` `[L, n_points, B, dim]`: integer positions of grid cell's\n                                                           vertices, of each level\n\n            Returns:\n                indices `uint32` `[L, n_points, B]`: grid cell indices of the vertices\n            \"\"\"\n            # [dim]\n            if dim == 2:\n                strides = jnp.stack([jnp.ones_like(res), res]).T\n            elif dim == 3:\n                strides = jnp.stack([jnp.ones_like(res), res, res ** 2]).T\n            else:\n                raise NotImplementedError(\"{} is only implemented for 2D and 3D data\".format(__class__.__name__))\n            # [2**dim]\n            indices = jnp.sum(strides[None, :] * vert_pos, axis=-1)\n            return indices\n\n        @jax.vmap\n        @jax.vmap\n        def make_hash_indices(vert_pos):\n            \"\"\"(first 2 axes `[L, n_points]` are vmapped away)\n            Inputs:\n                vert_pos `uint32` `[L, n_points, B, dim]`: integer positions of grid cell's\n                                                           vertices, of each level\n\n            Returns:\n                indices `uint32` `[L, n_points, B]`: grid cell indices of the vertices\n            \"\"\"\n            # use primes as reported in the paper\n            primes = jnp.asarray([1, 2_654_435_761, 805_459_861], dtype=jnp.uint32)\n            # [2**dim]\n            if dim == 2:\n                indices = vert_pos[:, 0] ^ (vert_pos[:, 1] * primes[1])\n            elif dim == 3:\n                indices = vert_pos[:, 0] ^ (vert_pos[:, 1] * primes[1]) ^ (vert_pos[:, 2] * primes[2])\n            else:\n                raise NotImplementedError(\"{} is only implemented for 2D and 3D data\".format(__class__.__name__))\n            return indices\n\n        def make_indices(vert_pos, resolutions, first_hash_level):\n            if first_hash_level > 0:\n                resolutions = jnp.asarray(resolutions, dtype=jnp.uint32)\n                indices = make_tiled_indices(resolutions[:first_hash_level], vert_pos[:first_hash_level, ...])\n            else:\n                indices = jnp.empty(0, dtype=jnp.uint32)\n            if first_hash_level < self.L:\n                indices = jnp.concatenate([indices, make_hash_indices(vert_pos[first_hash_level:, ...])], axis=0)\n            indices = jnp.mod(indices, self.T)\n            indices += jnp.asarray(offsets[:-1], dtype=jnp.uint32)[:, None, None]\n            return indices\n\n        @jax.vmap\n        @jax.vmap\n        def lerp_weights(pos_scaled: jax.Array):\n            \"\"\"(first 2 axes `[L, n_points]` are vmapped away)\n            Inputs:\n                pos_scaled `float` `[L, n_points, dim]`: coordinates of query points, scaled to the\n                                                         hierarchy in question\n\n            Returns:\n                weights `float` `[L, n_points, 2**dim]`: linear interpolation weights for each cell\n                                                         vertex\n            \"\"\"\n            # [dim]\n            pos_offset, _ = jnp.modf(pos_scaled)\n            # [2**dim, dim]\n            widths = jnp.clip(\n                # cell_vert_offsets: [2**dim, dim]\n                (1 - cell_vert_offsets[dim]) + (2 * cell_vert_offsets[dim] - 1) * pos_offset[None, :],\n                0,\n                1,\n            )\n            # [2**dim]\n            return jnp.prod(widths, axis=-1)\n\n        # [L]\n        scales = jnp.asarray(scales, dtype=jnp.float32)\n        # [L, n_points, dim]\n        pos_scaled = pos[None, :, :] * scales[:, None, None] + 0.5\n        # [L, n_points, 2**dim, dim]\n        vert_pos = make_vert_pos(pos_scaled)\n\n        # [L, n_points, 2**dim]\n        indices = make_indices(vert_pos, resolutions, first_hash_level)\n\n        # [L, n_points, 2**dim, F]\n        vert_latents = latents[indices]\n        # [L, n_points, 2**dim]\n        vert_weights = lerp_weights(pos_scaled)\n\n        # [L, n_points, F]\n        encodings = (vert_latents * vert_weights[..., None]).sum(axis=-2)\n        # [n_points, L*F]\n        encodings = encodings.transpose(1, 0, 2).reshape(-1, self.L * self.F)\n\n        ## Total variation\n        if self.tv_scale > 0:\n            # [L, n_points, dim * 2, dim]\n            adjacent_pos = make_adjacent_pos(pos_scaled)\n\n            # [L, n_points, dim * 2]\n            adjacent_indices = make_indices(adjacent_pos, resolutions, first_hash_level)\n\n            # [L, n_points, dim * 2, F]\n            adjacent_latents = latents[adjacent_indices]\n\n            # [L, n_points, dim * 2, F]\n            tv = self.tv_scale * jnp.square(adjacent_latents - vert_latents[:, :, :1, :])\n\n            # [L, n_points]\n            tv = tv.sum(axis=(-2, -1))\n\n            tv = tv.mean()\n        else:\n            tv = 0\n\n        return encodings, tv", "\n\n@empty_impl\nclass TCNNHashGridEncoder(HashGridEncoder):\n    @nn.compact\n    def __call__(self, pos: jax.Array, bound: float) -> jax.Array:\n        dim = pos.shape[-1]\n\n        # CAVEAT: hashgrid encoder is defined only in the unit cube [0, 1)^3\n        pos = (pos + bound) / (2 * bound)\n\n        scales, resolutions, first_hash_level, offsets = [], [], 0, [0]\n        for i in range(self.L):\n            scale = self.N_min * (self.b**i) - 1\n            scales.append(scale)\n            res = math.ceil(scale) + 1\n            resolutions.append(res)\n\n            n_entries = res ** dim\n\n            if n_entries <= self.T:\n                first_hash_level += 1\n            else:\n                n_entries = self.T\n\n            offsets.append(offsets[-1] + n_entries)\n\n        latents = self.param(\n            \"latent codes stored on grid vertices\",\n            # paper:\n            #   We initialize the hash table entries using the uniform distribution U(\u221210^{\u22124}, 10^{\u22124})\n            #   to provide a small amount of randomness while encouraging initial predictions close\n            #   to zero.\n            lambda key, shape, dtype: jran.uniform(key, shape, dtype, -1e-4, 1e-4),\n            (offsets[-1], self.F),\n            self.param_dtype,\n        )\n\n        return hashgrid_encode(\n            desc=HashGridMetadata(\n                L=self.L,\n                F=self.F,\n                N_min=self.N_min,\n                per_level_scale=self.b,\n            ),\n            offset_table_data=jnp.asarray(offsets, dtype=jnp.uint32),\n            coords_rm=pos.T,\n            params=latents,\n        ).T, 0", "\n\n@empty_impl\nclass FrequencyEncoder(Encoder):\n    \"\"\"\n    Frequency encoding from Equation(4) of the NeRF paper, except the encoded frequency orders are\n    different.\n    \"\"\"\n\n    # number of frequencies\n    L: int\n\n    # NOTE:\n    #   adding @nn.compact makes this not directly callable (CallCompactUnboundModuleError)\n    #   See: <https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.CallCompactUnboundModuleError>\n    #\n    # @nn.compact\n    # TODO:\n    #   using a function for this (vmap, then jit the vmapped function) seems to be faster (~47ms vs\n    #   ~57ms)\n    def __call__(self, pos: jax.Array, bound: float) -> jax.Array:\n        \"\"\"\n        Inuts:\n            pos [..., dim]: `dim`-d coordinates to be frequency-encoded\n\n        Returns:\n            encodings [..., 2*dim*L]: frequency-encoded coordinates\n        \"\"\"\n        dim = pos.shape[-1]\n        pos = (pos + bound) / (2 * bound)\n        # [..., dim, L]: 2^{l} * pi * p\n        A = jnp.exp2(jnp.arange(self.L)) * jnp.pi * pos[..., None]\n        # [..., dim, L], [..., dim, L]\n        senc, cenc = jnp.sin(A), jnp.cos(A)\n        # [..., dim*L], [..., dim*L]\n        senc, cenc = senc.reshape(*A.shape[:-2], dim*self.L), cenc.reshape(*A.shape[:-2], dim*self.L)\n        # [..., 2*dim*L]\n        encodings = jnp.stack([senc, cenc], axis=-1).reshape(*A.shape[:-2], 2*dim*self.L)\n        return (\n            encodings,\n            0.,  # placeholder for the total variation loss\n        )", "\n\n@empty_impl\nclass SphericalHarmonicsEncoderCuda(Encoder):\n    # highest degree\n    L: int\n\n    def __call__(self, dirs: jax.Array) -> jax.Array:\n        \"Just a thin wrapper on top of :func:`shjax.spherical_harmonics_encoding()`\"\n        return shjax.spherical_harmonics_encoding(dirs, self.L)", "\n\n@empty_impl\nclass SphericalHarmonicsEncoder(Encoder):\n    # highest degree\n    L: int\n\n    def __call__(self, dirs: jax.Array) -> jax.Array:\n        \"\"\"\n        Adapted from <https://github.com/NVlabs/tiny-cuda-nn/blob/39df2387a684e4fe0cfa33542aebf5eab237716b/include/tiny-cuda-nn/encodings/spherical_harmonics.h#L52-L123>\n\n        Inputs:\n            dirs [..., 3]: **unit** vectors, representing directions.\n\n        Returns:\n            encodings [..., L**2]: real parts of the spherical harmonics up to the L-th degree.\n        \"\"\"\n        chex.assert_axis_dimension(dirs, -1, 3)\n        x, y, z = dirs[..., 0], dirs[..., 1], dirs[..., 2]\n        xy, xz, yz = x*y, x*z, y*z\n        x2, y2, z2 = x*x, y*y, z*z\n        x4, y4, z4 = x2*x2, y2*y2, z2*z2\n        x6, y6, z6 = x4*x2, y4*y2, z4*z2\n\n        encodings = jnp.empty((*dirs.shape[:-1], self.L**2))\n\n        encodings = encodings.at[..., 0].set(0.28209479177387814)  # 1/(2*sqrt(pi))\n        if self.L <= 1: return encodings\n\n        encodings = encodings.at[..., 1].set(-0.48860251190291987*y)  # -sqrt(3)*y/(2*sqrt(pi))\n        encodings = encodings.at[..., 2].set(0.48860251190291987*z)  # sqrt(3)*z/(2*sqrt(pi))\n        encodings = encodings.at[..., 3].set(-0.48860251190291987*x)  # -sqrt(3)*x/(2*sqrt(pi))\n        if self.L <= 2: return encodings;\n\n        encodings = encodings.at[..., 4].set(1.0925484305920792*xy)  # sqrt(15)*xy/(2*sqrt(pi))\n        encodings = encodings.at[..., 5].set(-1.0925484305920792*yz)  # -sqrt(15)*yz/(2*sqrt(pi))\n        encodings = encodings.at[..., 6].set(0.94617469575755997*z2 - 0.31539156525251999)  # sqrt(5)*(3*z2 - 1)/(4*sqrt(pi))\n        encodings = encodings.at[..., 7].set(-1.0925484305920792*xz)  # -sqrt(15)*xz/(2*sqrt(pi))\n        encodings = encodings.at[..., 8].set(0.54627421529603959*x2 - 0.54627421529603959*y2)  # sqrt(15)*(x2 - y2)/(4*sqrt(pi))\n        if self.L <= 3: return encodings\n\n        encodings = encodings.at[..., 9].set(0.59004358992664352*y*(-3.0*x2 + y2))  # sqrt(70)*y*(-3*x2 + y2)/(8*sqrt(pi))\n        encodings = encodings.at[..., 10].set(2.8906114426405538*xy*z)  # sqrt(105)*xy*z/(2*sqrt(pi))\n        encodings = encodings.at[..., 11].set(0.45704579946446572*y*(1.0 - 5.0*z2))  # sqrt(42)*y*(1 - 5*z2)/(8*sqrt(pi))\n        encodings = encodings.at[..., 12].set(0.3731763325901154*z*(5.0*z2 - 3.0))  # sqrt(7)*z*(5*z2 - 3)/(4*sqrt(pi))\n        encodings = encodings.at[..., 13].set(0.45704579946446572*x*(1.0 - 5.0*z2))  # sqrt(42)*x*(1 - 5*z2)/(8*sqrt(pi))\n        encodings = encodings.at[..., 14].set(1.4453057213202769*z*(x2 - y2))  # sqrt(105)*z*(x2 - y2)/(4*sqrt(pi))\n        encodings = encodings.at[..., 15].set(0.59004358992664352*x*(-x2 + 3.0*y2))  # sqrt(70)*x*(-x2 + 3*y2)/(8*sqrt(pi))\n        if self.L <= 4: return encodings\n\n        encodings = encodings.at[..., 16].set(2.5033429417967046*xy*(x2 - y2))  # 3*sqrt(35)*xy*(x2 - y2)/(4*sqrt(pi))\n        encodings = encodings.at[..., 17].set(1.7701307697799304*yz*(-3.0*x2 + y2))  # 3*sqrt(70)*yz*(-3*x2 + y2)/(8*sqrt(pi))\n        encodings = encodings.at[..., 18].set(0.94617469575756008*xy*(7.0*z2 - 1.0))  # 3*sqrt(5)*xy*(7*z2 - 1)/(4*sqrt(pi))\n        encodings = encodings.at[..., 19].set(0.66904654355728921*yz*(3.0 - 7.0*z2))  # 3*sqrt(10)*yz*(3 - 7*z2)/(8*sqrt(pi))\n        encodings = encodings.at[..., 20].set(-3.1735664074561294*z2 + 3.7024941420321507*z4 + 0.31735664074561293)  # 3*(-30*z2 + 35*z4 + 3)/(16*sqrt(pi))\n        encodings = encodings.at[..., 21].set(0.66904654355728921*xz*(3.0 - 7.0*z2))  # 3*sqrt(10)*xz*(3 - 7*z2)/(8*sqrt(pi))\n        encodings = encodings.at[..., 22].set(0.47308734787878004*(x2 - y2)*(7.0*z2 - 1.0))  # 3*sqrt(5)*(x2 - y2)*(7*z2 - 1)/(8*sqrt(pi))\n        encodings = encodings.at[..., 23].set(1.7701307697799304*xz*(-x2 + 3.0*y2))  # 3*sqrt(70)*xz*(-x2 + 3*y2)/(8*sqrt(pi))\n        encodings = encodings.at[..., 24].set(-3.7550144126950569*x2*y2 + 0.62583573544917614*x4 + 0.62583573544917614*y4)  # 3*sqrt(35)*(-6*x2*y2 + x4 + y4)/(16*sqrt(pi))\n        if self.L <= 5: return encodings\n\n        encodings = encodings.at[..., 25].set(0.65638205684017015*y*(10.0*x2*y2 - 5.0*x4 - y4))  # 3*sqrt(154)*y*(10*x2*y2 - 5*x4 - y4)/(32*sqrt(pi))\n        encodings = encodings.at[..., 26].set(8.3026492595241645*xy*z*(x2 - y2))  # 3*sqrt(385)*xy*z*(x2 - y2)/(4*sqrt(pi))\n        encodings = encodings.at[..., 27].set(-0.48923829943525038*y*(3.0*x2 - y2)*(9.0*z2 - 1.0))  # -sqrt(770)*y*(3*x2 - y2)*(9*z2 - 1)/(32*sqrt(pi))\n        encodings = encodings.at[..., 28].set(4.7935367849733241*xy*z*(3.0*z2 - 1.0))  # sqrt(1155)*xy*z*(3*z2 - 1)/(4*sqrt(pi))\n        encodings = encodings.at[..., 29].set(0.45294665119569694*y*(14.0*z2 - 21.0*z4 - 1.0))  # sqrt(165)*y*(14*z2 - 21*z4 - 1)/(16*sqrt(pi))\n        encodings = encodings.at[..., 30].set(0.1169503224534236*z*(-70.0*z2 + 63.0*z4 + 15.0))  # sqrt(11)*z*(-70*z2 + 63*z4 + 15)/(16*sqrt(pi))\n        encodings = encodings.at[..., 31].set(0.45294665119569694*x*(14.0*z2 - 21.0*z4 - 1.0))  # sqrt(165)*x*(14*z2 - 21*z4 - 1)/(16*sqrt(pi))\n        encodings = encodings.at[..., 32].set(2.3967683924866621*z*(x2 - y2)*(3.0*z2 - 1.0))  # sqrt(1155)*z*(x2 - y2)*(3*z2 - 1)/(8*sqrt(pi))\n        encodings = encodings.at[..., 33].set(-0.48923829943525038*x*(x2 - 3.0*y2)*(9.0*z2 - 1.0))  # -sqrt(770)*x*(x2 - 3*y2)*(9*z2 - 1)/(32*sqrt(pi))\n        encodings = encodings.at[..., 34].set(2.0756623148810411*z*(-6.0*x2*y2 + x4 + y4))  # 3*sqrt(385)*z*(-6*x2*y2 + x4 + y4)/(16*sqrt(pi))\n        encodings = encodings.at[..., 35].set(0.65638205684017015*x*(10.0*x2*y2 - x4 - 5.0*y4))  # 3*sqrt(154)*x*(10*x2*y2 - x4 - 5*y4)/(32*sqrt(pi))\n        if self.L <= 6: return encodings\n\n        encodings = encodings.at[..., 36].set(1.3663682103838286*xy*(-10.0*x2*y2 + 3.0*x4 + 3.0*y4))  # sqrt(6006)*xy*(-10*x2*y2 + 3*x4 + 3*y4)/(32*sqrt(pi))\n        encodings = encodings.at[..., 37].set(2.3666191622317521*yz*(10.0*x2*y2 - 5.0*x4 - y4))  # 3*sqrt(2002)*yz*(10*x2*y2 - 5*x4 - y4)/(32*sqrt(pi))\n        encodings = encodings.at[..., 38].set(2.0182596029148963*xy*(x2 - y2)*(11.0*z2 - 1.0))  # 3*sqrt(91)*xy*(x2 - y2)*(11*z2 - 1)/(8*sqrt(pi))\n        encodings = encodings.at[..., 39].set(-0.92120525951492349*yz*(3.0*x2 - y2)*(11.0*z2 - 3.0))  # -sqrt(2730)*yz*(3*x2 - y2)*(11*z2 - 3)/(32*sqrt(pi))\n        encodings = encodings.at[..., 40].set(0.92120525951492349*xy*(-18.0*z2 + 33.0*z4 + 1.0))  # sqrt(2730)*xy*(-18*z2 + 33*z4 + 1)/(32*sqrt(pi))\n        encodings = encodings.at[..., 41].set(0.58262136251873131*yz*(30.0*z2 - 33.0*z4 - 5.0))  # sqrt(273)*yz*(30*z2 - 33*z4 - 5)/(16*sqrt(pi))\n        encodings = encodings.at[..., 42].set(6.6747662381009842*z2 - 20.024298714302954*z4 + 14.684485723822165*z6 - 0.31784601133814211)  # sqrt(13)*(105*z2 - 315*z4 + 231*z6 - 5)/(32*sqrt(pi))\n        encodings = encodings.at[..., 43].set(0.58262136251873131*xz*(30.0*z2 - 33.0*z4 - 5.0))  # sqrt(273)*xz*(30*z2 - 33*z4 - 5)/(16*sqrt(pi))\n        encodings = encodings.at[..., 44].set(0.46060262975746175*(x2 - y2)*(11.0*z2*(3.0*z2 - 1.0) - 7.0*z2 + 1.0))  # sqrt(2730)*(x2 - y2)*(11*z2*(3*z2 - 1) - 7*z2 + 1)/(64*sqrt(pi))\n        encodings = encodings.at[..., 45].set(-0.92120525951492349*xz*(x2 - 3.0*y2)*(11.0*z2 - 3.0))  # -sqrt(2730)*xz*(x2 - 3*y2)*(11*z2 - 3)/(32*sqrt(pi))\n        encodings = encodings.at[..., 46].set(0.50456490072872406*(11.0*z2 - 1.0)*(-6.0*x2*y2 + x4 + y4))  # 3*sqrt(91)*(11*z2 - 1)*(-6*x2*y2 + x4 + y4)/(32*sqrt(pi))\n        encodings = encodings.at[..., 47].set(2.3666191622317521*xz*(10.0*x2*y2 - x4 - 5.0*y4))  # 3*sqrt(2002)*xz*(10*x2*y2 - x4 - 5*y4)/(32*sqrt(pi))\n        encodings = encodings.at[..., 48].set(10.247761577878714*x2*y4 - 10.247761577878714*x4*y2 + 0.6831841051919143*x6 - 0.6831841051919143*y6)  # sqrt(6006)*(15*x2*y4 - 15*x4*y2 + x6 - y6)/(64*sqrt(pi))\n        if self.L <= 7: return encodings\n\n        encodings = encodings.at[..., 49].set(0.70716273252459627*y*(-21.0*x2*y4 + 35.0*x4*y2 - 7.0*x6 + y6))  # 3*sqrt(715)*y*(-21*x2*y4 + 35*x4*y2 - 7*x6 + y6)/(64*sqrt(pi))\n        encodings = encodings.at[..., 50].set(5.2919213236038001*xy*z*(-10.0*x2*y2 + 3.0*x4 + 3.0*y4))  # 3*sqrt(10010)*xy*z*(-10*x2*y2 + 3*x4 + 3*y4)/(32*sqrt(pi))\n        encodings = encodings.at[..., 51].set(-0.51891557872026028*y*(13.0*z2 - 1.0)*(-10.0*x2*y2 + 5.0*x4 + y4))  # -3*sqrt(385)*y*(13*z2 - 1)*(-10*x2*y2 + 5*x4 + y4)/(64*sqrt(pi))\n        encodings = encodings.at[..., 52].set(4.1513246297620823*xy*z*(x2 - y2)*(13.0*z2 - 3.0))  # 3*sqrt(385)*xy*z*(x2 - y2)*(13*z2 - 3)/(8*sqrt(pi))\n        encodings = encodings.at[..., 53].set(-0.15645893386229404*y*(3.0*x2 - y2)*(13.0*z2*(11.0*z2 - 3.0) - 27.0*z2 + 3.0))  # -3*sqrt(35)*y*(3*x2 - y2)*(13*z2*(11*z2 - 3) - 27*z2 + 3)/(64*sqrt(pi))\n        encodings = encodings.at[..., 54].set(0.44253269244498261*xy*z*(-110.0*z2 + 143.0*z4 + 15.0))  # 3*sqrt(70)*xy*z*(-110*z2 + 143*z4 + 15)/(32*sqrt(pi))\n        encodings = encodings.at[..., 55].set(0.090331607582517306*y*(-135.0*z2 + 495.0*z4 - 429.0*z6 + 5.0))  # sqrt(105)*y*(-135*z2 + 495*z4 - 429*z6 + 5)/(64*sqrt(pi))\n        encodings = encodings.at[..., 56].set(0.068284276912004949*z*(315.0*z2 - 693.0*z4 + 429.0*z6 - 35.0))  # sqrt(15)*z*(315*z2 - 693*z4 + 429*z6 - 35)/(32*sqrt(pi))\n        encodings = encodings.at[..., 57].set(0.090331607582517306*x*(-135.0*z2 + 495.0*z4 - 429.0*z6 + 5.0))  # sqrt(105)*x*(-135*z2 + 495*z4 - 429*z6 + 5)/(64*sqrt(pi))\n        encodings = encodings.at[..., 58].set(0.07375544874083044*z*(x2 - y2)*(143.0*z2*(3.0*z2 - 1.0) - 187.0*z2 + 45.0))  # sqrt(70)*z*(x2 - y2)*(143*z2*(3*z2 - 1) - 187*z2 + 45)/(64*sqrt(pi))\n        encodings = encodings.at[..., 59].set(-0.15645893386229404*x*(x2 - 3.0*y2)*(13.0*z2*(11.0*z2 - 3.0) - 27.0*z2 + 3.0))  # -3*sqrt(35)*x*(x2 - 3*y2)*(13*z2*(11*z2 - 3) - 27*z2 + 3)/(64*sqrt(pi))\n        encodings = encodings.at[..., 60].set(1.0378311574405206*z*(13.0*z2 - 3.0)*(-6.0*x2*y2 + x4 + y4))  # 3*sqrt(385)*z*(13*z2 - 3)*(-6*x2*y2 + x4 + y4)/(32*sqrt(pi))\n        encodings = encodings.at[..., 61].set(-0.51891557872026028*x*(13.0*z2 - 1.0)*(-10.0*x2*y2 + x4 + 5.0*y4))  # -3*sqrt(385)*x*(13*z2 - 1)*(-10*x2*y2 + x4 + 5*y4)/(64*sqrt(pi))\n        encodings = encodings.at[..., 62].set(2.6459606618019*z*(15.0*x2*y4 - 15.0*x4*y2 + x6 - y6))  # 3*sqrt(10010)*z*(15*x2*y4 - 15*x4*y2 + x6 - y6)/(64*sqrt(pi))\n        encodings = encodings.at[..., 63].set(0.70716273252459627*x*(-35.0*x2*y4 + 21.0*x4*y2 - x6 + 7.0*y6))  # 3*sqrt(715)*x*(-35*x2*y4 + 21*x4*y2 - x6 + 7*y6)/(64*sqrt(pi))\n        if self.L <= 8: return encodings\n\n        raise NotImplementedError(\"Largest supported degree of spherical harmonics is 8, got {}\".format(self.L))", "\n\ndef bench_sh():\n    import time\n\n    L = 8\n\n    sh = SphericalHarmonicsEncoder(L=L)\n    shcuda = SphericalHarmonicsEncoderCuda(L=L)\n\n    @jax.jit\n    def shjax_jitted(x):\n        return sh(x)\n    @jit_jaxfn_with(static_argnames=[\"L\"])\n    # def shcuda_jitted(x):\n    #     return shcuda(x)\n    def shcuda_jitted(x, L):\n        return shjax.spherical_harmonics_encoding(x, L)\n\n    d = jnp.asarray([[.1, .5, -.7]])\n    d /= jnp.linalg.norm(d, axis=-1, keepdims=True) + 1e-15\n\n    result = sh(d)\n    result_cuda = shcuda(d)\n    print(abs(result - result_cuda).sum())\n\n    K = jran.PRNGKey(0xdeadbeef)\n    for i in range(100):\n        K, key = jran.split(K, 2)\n        n = 800*800\n        d = jran.normal(key, (n, 3))\n        d /= jnp.linalg.norm(d) + 1e-15\n\n        print(\"{:03d}-th check ({} coordinates, degree={}): \".format(i+1, n, L), end=\"\")\n\n        stime = time.time()\n        print(\"|jax...\", end=\"\")\n        result = shjax_jitted(d).block_until_ready()\n        etime = time.time()\n        durms = 1000 * (etime - stime)\n        print(\"{:.2f}ms|\".format(durms), end=\"\")\n\n        stime = time.time()\n        print(\"|cuda...\", end=\"\")\n        result_cuda = shcuda_jitted(d, L).block_until_ready()\n        etime = time.time()\n        durms = 1000 * (etime - stime)\n        print(\"{:.2f}ms|\".format(durms), end=\"\")\n\n        diff = abs(result - result_cuda).sum()\n        print(\"diff(total)={:.3e}|diff(mean)={:.3e}\".format(diff, diff/n))", "\n\ndef bench_hg():\n    import time\n\n    dim=3\n    L=16\n    F=2\n    T=2**19\n    N_min=16\n    per_level_scale=2.\n    N_max=int(N_min * per_level_scale**(L - 1))\n\n    hg = HashGridEncoder(\n        dim=dim,\n        L=L,\n        T=T,\n        F=F,\n        N_min=N_min,\n        N_max=N_max,\n    )\n    K = jran.PRNGKey(0xabcdef)\n\n    variables = hg.init(K, jnp.zeros([5, 3]))\n    (params_array,) = variables[\"params\"][\"latent codes stored on grid vertices\"],\n\n    print(params_array.shape)\n\n    @jax.jit\n    def hgjax_jitted(d):\n        return hg.apply(variables, d)\n\n    hgmeta = HashGridMetadata(\n        L=L,\n        F=F,\n        N_min=N_min,\n        per_level_scale=per_level_scale,\n    )\n\n    resolutions, offsets = [], [0]\n    for i in range(L):\n        res = int(N_min * (per_level_scale**i))\n        resolutions.append(res)\n\n        n_entries = (res + 1) ** dim\n\n        if n_entries <= T:\n            pass\n        else:\n            n_entries = T\n\n        offsets.append(offsets[-1] + n_entries)\n\n    assert offsets[-1] == params_array.shape[0]\n\n    @jax.jit\n    def hgtcnn_jitted(d_row_major):  # expects input coordinates to have shape (dim, n_coords)\n        return hashgrid_encode(\n            desc=hgmeta,\n            offset_table_data=jnp.asarray(offsets, dtype=jnp.uint32),\n            coords_rm=d_row_major,\n            params=params_array,\n        )\n\n    print(\"starting!\")\n\n    for i in range(100):\n        K, key = jran.split(K, 2)\n        n = 256_000\n        d = jran.uniform(key, (n, 3), minval=0, maxval=1.)\n        d_T = jran.uniform(key, (3, n), minval=0, maxval=1.)\n\n        print(\"{:03d}-th check ({} coordinates, degree={}): \".format(i+1, n, L), end=\"\")\n\n        stime = time.time()\n        print(\"|jax...\", end=\"\")\n        result = hgjax_jitted(d).block_until_ready()\n        etime = time.time()\n        durms = 1000 * (etime - stime)\n        print(\"{:.2f}ms\".format(durms), end=\"\")\n\n        stime = time.time()\n        print(\"|tcnn...\", end=\"\")\n        result = hgtcnn_jitted(d_T).block_until_ready()\n        etime = time.time()\n        durms = 1000 * (etime - stime)\n        print(\"{:.2f}ms|\".format(durms), end=\"\")\n\n        print()", "\n\nif __name__ == \"__main__\":\n    print(\"bench_hg\")\n    bench_hg()\n\n    # print()\n    # print(\"bench_sh:\")\n    # bench_sh()\n", "    # bench_sh()\n"]}
{"filename": "models/renderers/__init__.py", "chunked_list": ["from .cuda import render_image_inference, render_rays_train\n\n\n__all__ = [\n    \"render_rays_train\",\n    \"render_image_inference\",\n]\n"]}
{"filename": "models/renderers/cuda.py", "chunked_list": ["from dataclasses import dataclass\nimport math\nfrom typing import Callable\n\nfrom flax.core.scope import FrozenVariableDict\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jran\nfrom volrendjax import (\n    integrate_rays,", "from volrendjax import (\n    integrate_rays,\n    integrate_rays_inference,\n    march_rays,\n    march_rays_inference,\n)\n\nfrom utils.common import jit_jaxfn_with\nfrom utils.data import f32_to_u8\nfrom utils.types import Camera, CameraOverrideOptions, NeRFState, RigidTransformation", "from utils.data import f32_to_u8\nfrom utils.types import Camera, CameraOverrideOptions, NeRFState, RigidTransformation\n\n\n@jax.jit\ndef make_rays_worldspace(\n    camera: Camera,\n    transform_cw: RigidTransformation,\n):\n    \"\"\"\n    Generate world-space rays for each pixel in the given camera's projection plane.\n\n    Inputs:\n        camera: the camera model in use\n        transform_cw[rotation, translation]: camera to world transformation\n            rotation [3, 3]: rotation matrix\n            translation [3]: translation vector\n\n    Returns:\n        o_world [H*W, 3]: ray origins, in world-space\n        d_world [H*W, 3]: ray directions, in world-space\n    \"\"\"\n    # [H*W, 1]\n    d_cam_idcs = jnp.arange(camera.n_pixels)\n    x, y = (\n        jnp.mod(d_cam_idcs, camera.width),\n        jnp.floor_divide(d_cam_idcs, camera.width),\n    )\n    # [H*W, 3]\n    d_cam = camera.make_ray_directions_from_pixel_coordinates(x, y, use_pixel_center=True)\n\n    # [H*W, 3]\n    o_world = jnp.broadcast_to(transform_cw.translation, d_cam.shape)\n    # [H*W, 3]\n    d_world = d_cam @ transform_cw.rotation.T\n\n    return o_world, d_world", "\n\n@jax.jit\ndef make_near_far_from_bound(\n    bound: float,\n    o: jax.Array,  # [n_rays, 3]\n    d: jax.Array,  # [n_rays, 3]\n):\n    \"Calculates near and far intersections with the bounding box [-bound, bound]^3 for each ray.\"\n\n    # avoid d[j] being zero\n    eps = 1e-15\n    d = jnp.where(\n        jnp.signbit(d),  # True for negatives, False for non-negatives\n        jnp.clip(d, None, -eps * jnp.ones_like(d)),  # if negative, upper-bound is -eps\n        jnp.clip(d, eps * jnp.ones_like(d)),  # if non-negative, lower-bound is eps\n    )\n\n    # [n_rays]\n    tx0, tx1 = (\n        (-bound - o[:, 0]) / d[:, 0],\n        (bound - o[:, 0]) / d[:, 0],\n    )\n    ty0, ty1 = (\n        (-bound - o[:, 1]) / d[:, 1],\n        (bound - o[:, 1]) / d[:, 1],\n    )\n    tz0, tz1 = (\n        (-bound - o[:, 2]) / d[:, 2],\n        (bound - o[:, 2]) / d[:, 2],\n    )\n    tx_start, tx_end = jnp.minimum(tx0, tx1), jnp.maximum(tx0, tx1)\n    ty_start, ty_end = jnp.minimum(ty0, ty1), jnp.maximum(ty0, ty1)\n    tz_start, tz_end = jnp.minimum(tz0, tz1), jnp.maximum(tz0, tz1)\n\n    # when t_start<0, or t_start>t_end, ray does not intersect with aabb, these cases are handled in\n    # the `march_rays` implementation\n    t_start = jnp.maximum(jnp.maximum(tx_start, ty_start), tz_start)  # last axis that gose inside the bbox\n    t_end = jnp.minimum(jnp.minimum(tx_end, ty_end), tz_end)  # first axis that goes out of the bbox\n\n    t_start = jnp.maximum(0., t_start)\n\n    # [n_rays], [n_rays]\n    return t_start, t_end", "\n\n@jit_jaxfn_with(static_argnames=[\"total_samples\"])\ndef render_rays_train(\n    KEY: jran.KeyArray,\n    o_world: jax.Array,\n    d_world: jax.Array,\n    appearance_embeddings: jax.Array,\n    bg: jax.Array,\n    total_samples: int,\n    state: NeRFState,\n):\n    # skip the empty space between camera and scene bbox\n    # [n_rays], [n_rays]\n    t_starts, t_ends = make_near_far_from_bound(\n        bound=state.scene_meta.bound,\n        o=o_world,\n        d=d_world,\n    )\n\n    if state.raymarch.perturb:\n        KEY, key = jran.split(KEY, 2)\n        noises = jran.uniform(key, shape=t_starts.shape, dtype=t_starts.dtype, minval=0., maxval=1.)\n    else:\n        noises = 0.\n    measured_batch_size_before_compaction, ray_is_valid, rays_n_samples, rays_sample_startidx, ray_idcs, xyzs, dirs, dss, z_vals = march_rays(\n        total_samples=total_samples,\n        diagonal_n_steps=state.raymarch.diagonal_n_steps,\n        K=state.scene_meta.cascades,\n        G=state.raymarch.density_grid_res,\n        bound=state.scene_meta.bound,\n        stepsize_portion=state.scene_meta.stepsize_portion,\n        rays_o=o_world,\n        rays_d=d_world,\n        t_starts=t_starts.ravel(),\n        t_ends=t_ends.ravel(),\n        noises=noises,\n        occupancy_bitfield=state.ogrid.occupancy,\n    )\n\n    drgbs, tv = state.nerf_fn(\n        {\"params\": state.params[\"nerf\"]},\n        xyzs,\n        dirs,\n        appearance_embeddings[ray_idcs],\n    )\n\n    effective_samples, final_rgbds, final_opacities = integrate_rays(\n        near_distance=state.scene_meta.camera.near,\n        rays_sample_startidx=rays_sample_startidx,\n        rays_n_samples=rays_n_samples,\n        bgs=bg,\n        dss=dss,\n        z_vals=z_vals,\n        drgbs=drgbs,\n    )\n\n    batch_metrics = {\n        \"n_valid_rays\": ray_is_valid.sum(),\n        \"ray_is_valid\": ray_is_valid,\n        \"measured_batch_size_before_compaction\": measured_batch_size_before_compaction,\n        \"measured_batch_size\": jnp.where(effective_samples > 0, effective_samples, 0).sum(),\n    }\n\n    return batch_metrics, final_rgbds, tv", "\n\n@dataclass(frozen=True, kw_only=True)\nclass MarchAndIntegrateInferencePayload:\n    march_steps_cap: int\n    diagonal_n_steps: int\n    cascades: int\n    density_grid_res: int\n    bound: float\n    stepsize_portion: float\n    nerf_fn: Callable", "\n\n@jit_jaxfn_with(\n    static_argnames=[\"payload\"],\n    donate_argnums=tuple(range(7)),  # NOTE: this only works for positional arguments, see <https://jax.readthedocs.io/en/latest/faq.html#buffer-donation>\n)\ndef march_and_integrate_inference(\n    # rw (donated) fields (7)\n    next_ray_index: jax.Array,\n    terminated: jax.Array,\n    indices: jax.Array,\n    rays_rgbd: jax.Array,\n    rays_T: jax.Array,\n    rays_cost: jax.Array | None,\n    t_starts: jax.Array,\n\n    # static\n    payload: MarchAndIntegrateInferencePayload,\n\n    # ro fields\n    locked_nerf_params: FrozenVariableDict,\n    appearance_embedding: jax.Array,\n    rays_o: jax.Array,\n    rays_d: jax.Array,\n    t_ends: jax.Array,\n    rays_bg: jax.Array,\n    occupancy_bitfield: jax.Array,\n):\n    next_ray_index, indices, n_samples, t_starts, xyzs, dss, z_vals = march_rays_inference(\n        diagonal_n_steps=payload.diagonal_n_steps,\n        K=payload.cascades,\n        G=payload.density_grid_res,\n        march_steps_cap=payload.march_steps_cap,\n        bound=payload.bound,\n        stepsize_portion=payload.stepsize_portion,\n        rays_o=rays_o,\n        rays_d=rays_d,\n        t_starts=t_starts,\n        t_ends=t_ends,\n        occupancy_bitfield=occupancy_bitfield,\n        next_ray_index_in=next_ray_index,\n        terminated=terminated,\n        indices=indices,\n    )\n    if rays_cost is not None:\n        rays_cost = rays_cost.at[indices].set(rays_cost[indices] + n_samples)\n\n    xyzs = jax.lax.stop_gradient(xyzs)\n    drgbs, _ = payload.nerf_fn(\n        {\"params\": locked_nerf_params},\n        xyzs,\n        jnp.broadcast_to(rays_d[indices, None, :], xyzs.shape),\n        jax.lax.stop_gradient(appearance_embedding),\n    )\n\n    terminate_cnt, terminated, rays_rgbd, rays_T = integrate_rays_inference(\n        rays_bg=rays_bg,\n        rays_rgbd=rays_rgbd,\n        rays_T=rays_T,\n\n        n_samples=n_samples,\n        indices=indices,\n        dss=dss,\n        z_vals=z_vals,\n        drgbs=drgbs,\n    )\n\n    return terminate_cnt, next_ray_index, terminated, indices, rays_rgbd, rays_T, rays_cost, t_starts", "\n\ndef render_image_inference(\n    KEY: jran.KeyArray,\n    transform_cw: RigidTransformation,\n    state: NeRFState,\n    camera_override: None | CameraOverrideOptions=None,\n    render_cost: bool=False,\n    appearance_embedding_index: int=0,\n):\n    if isinstance(camera_override, Camera):\n        state = state.replace(scene_meta=state.scene_meta.replace(camera=camera_override))\n    elif isinstance(camera_override, CameraOverrideOptions):\n        state = state.replace(\n            scene_meta=state.scene_meta.replace(\n                camera=camera_override.update_camera(state.scene_meta.camera),\n            ),\n        )\n    elif camera_override is None:\n        pass\n    else:\n        raise RuntimeError(\n            \"expected `camera_override` to be of type `Camera` or `CameraOverrideOptions`, got {}\".format(\n                type(camera_override)\n            )\n        )\n\n    o_world, d_world = make_rays_worldspace(camera=state.scene_meta.camera, transform_cw=transform_cw)\n    appearance_embedding = (\n        state.locked_params[\"appearance_embeddings\"][appearance_embedding_index]\n            if \"appearance_embeddings\" in state.locked_params\n            else jnp.empty(0)\n    )\n    t_starts, t_ends = make_near_far_from_bound(state.scene_meta.bound, o_world, d_world)\n    rays_rgbd = jnp.zeros((state.scene_meta.camera.n_pixels, 4), dtype=jnp.float32)\n    rays_T = jnp.ones(state.scene_meta.camera.n_pixels, dtype=jnp.float32)\n    if render_cost:\n        rays_cost = jnp.zeros(state.scene_meta.camera.n_pixels, dtype=jnp.uint32)\n    else:\n        rays_cost = None\n    if state.use_background_model:\n        bg = state.bg_fn(\n            {\"params\": state.locked_params[\"bg\"]},\n            o_world,\n            d_world,\n            appearance_embedding,\n        )\n    elif state.render.random_bg:\n        KEY, key = jran.split(KEY, 2)\n        bg = jran.uniform(key, (3,), dtype=jnp.float32, minval=0, maxval=1)\n    else:\n        bg = state.render.bg\n    rays_bg = jnp.broadcast_to(jnp.asarray(bg), (state.scene_meta.camera.n_pixels, 3))\n\n    (\n        o_world,\n        d_world,\n        appearance_embedding,\n        t_starts,\n        t_ends,\n        rays_bg,\n        rays_rgbd,\n        rays_cost,\n        rays_T,\n    ) = jax.lax.stop_gradient((\n        o_world,\n        d_world,\n        appearance_embedding,\n        t_starts,\n        t_ends,\n        rays_bg,\n        rays_rgbd,\n        rays_cost,\n        rays_T,\n    ))\n\n    march_steps_cap = 8\n    n_rays = min(8192, state.scene_meta.camera.n_pixels)\n\n    next_ray_index = jnp.zeros(1, dtype=jnp.uint32)\n    terminated = jnp.ones(n_rays, dtype=jnp.bool_)  # all rays are terminated at the beginning\n    indices = jnp.zeros(n_rays, dtype=jnp.uint32)\n    n_rendered_rays = 0\n\n    while n_rendered_rays < state.scene_meta.camera.n_pixels:\n        iters = max(1, (state.scene_meta.camera.n_pixels - n_rendered_rays) // n_rays)\n        iters = 2 ** int(math.log2(iters) + 1)\n\n        for _ in range(iters):\n            terminate_cnt, next_ray_index, terminated, indices, rays_rgbd, rays_T, rays_cost, t_starts = march_and_integrate_inference(\n                # rw (donated) fields\n                next_ray_index,\n                terminated,\n                indices,\n                rays_rgbd,\n                rays_T,\n                rays_cost,\n                t_starts,\n\n                # static\n                payload=MarchAndIntegrateInferencePayload(\n                    march_steps_cap=march_steps_cap,\n                    diagonal_n_steps=state.raymarch.diagonal_n_steps,\n                    cascades=state.scene_meta.cascades,\n                    density_grid_res=state.raymarch.density_grid_res,\n                    bound=state.scene_meta.bound,\n                    stepsize_portion=state.scene_meta.stepsize_portion,\n                    nerf_fn=state.nerf_fn,\n                ),\n\n                # ro fields\n                locked_nerf_params=state.locked_params[\"nerf\"],\n                appearance_embedding=appearance_embedding,\n                rays_o=o_world,\n                rays_d=d_world,\n                t_ends=t_ends,\n                rays_bg=rays_bg,\n                occupancy_bitfield=state.ogrid.occupancy,\n            )\n            n_rendered_rays += terminate_cnt\n\n    bg_array_f32 = rays_bg.reshape((state.scene_meta.camera.height, state.scene_meta.camera.width, 3))\n    rays_rgb, rays_depth = jnp.array_split(rays_rgbd, [3], axis=-1)\n    image_array_u8 = f32_to_u8(rays_rgb).reshape((state.scene_meta.camera.height, state.scene_meta.camera.width, 3))\n    distance_array_u8 = f32_to_u8((rays_depth - rays_depth.min()) / (rays_depth.max() - rays_depth.min() + 1e-15))\n    distance_array_u8 = distance_array_u8.reshape((state.scene_meta.camera.height, state.scene_meta.camera.width))\n    if render_cost:\n        cost_array_u8 = f32_to_u8(rays_cost.astype(jnp.float32) / (rays_cost.astype(jnp.float32).max() + 1.)).reshape((state.scene_meta.camera.height, state.scene_meta.camera.width))\n    else:\n        cost_array_u8 = None\n\n    return bg_array_f32, image_array_u8, distance_array_u8, cost_array_u8", ""]}
{"filename": "app/imagefit.py", "chunked_list": ["#!/usr/bin/env python3\n\nfrom pathlib import Path\nfrom typing import Literal\n\nfrom PIL import Image\nfrom flax.training.train_state import TrainState\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jran", "import jax.numpy as jnp\nimport jax.random as jran\nimport numpy as np\nimport optax\nfrom tqdm import tqdm\nimport tyro\n\nfrom models.imagefit import ImageFitter\nfrom utils import common, data\nfrom utils.args import ImageFitArgs", "from utils import common, data\nfrom utils.args import ImageFitArgs\n\n\nlogger = common.setup_logging(\"imagefit\")\n\n\n@jax.jit\ndef train_step(state: TrainState, uvs, rgbs, perm):\n    def loss(params, x, y):\n        preds = state.apply_fn({\"params\": params}, x)\n        loss = jnp.square(preds - y).mean()\n        return loss\n\n    loss_grad_fn = jax.value_and_grad(loss)\n\n    loss, grads = loss_grad_fn(state.params, uvs[perm], rgbs[perm])\n    state = state.apply_gradients(grads=grads)\n    metrics = {\n        \"loss\": loss * perm.shape[0],\n    }\n    return state, metrics", "def train_step(state: TrainState, uvs, rgbs, perm):\n    def loss(params, x, y):\n        preds = state.apply_fn({\"params\": params}, x)\n        loss = jnp.square(preds - y).mean()\n        return loss\n\n    loss_grad_fn = jax.value_and_grad(loss)\n\n    loss, grads = loss_grad_fn(state.params, uvs[perm], rgbs[perm])\n    state = state.apply_gradients(grads=grads)\n    metrics = {\n        \"loss\": loss * perm.shape[0],\n    }\n    return state, metrics", "\n\ndef train_epoch(\n        image_metadata: data.ImageMetadata,\n        permutation: data.Dataset,\n        total_batches: int,\n        state: TrainState,\n        ep_log: int,\n    ):\n    loss = 0\n    for perm in tqdm(permutation, total=total_batches, desc=\"ep#{:03d}\".format(ep_log), bar_format=common.tqdm_format):\n        state, metrics = train_step(state, image_metadata.uvs, image_metadata.rgbs, perm)\n        loss += metrics[\"loss\"]\n    return loss, state", "\n\n@jax.jit\ndef eval_step(state, uvs, perm):\n    preds = state.apply_fn({\"params\": state.params}, uvs[perm])\n    return preds\n\n\ndef eval(\n        image_array,\n        image_metadata: data.ImageMetadata,\n        state: TrainState,\n    ):\n    H, W = image_array.shape[:2]\n\n    @common.jit_jaxfn_with(static_argnames=[\"chunk_size\"])\n    def get_perms(chunk_size: int) -> list[jax.Array]:\n        all_perms = jnp.arange(H*W)\n        if chunk_size >= H*W:\n            n_chunks = 1\n        else:\n            n_chunks = H*W // chunk_size\n        perms = jnp.array_split(all_perms, n_chunks)\n        return perms\n\n    for perm in tqdm(get_perms(chunk_size=2**15), desc=\"evaluating\", bar_format=common.tqdm_format):\n        # preds = state.apply_fn({\"params\": state.params}, uv)\n        preds = eval_step(state, image_metadata.uvs, perm)\n        image_array = data.set_pixels(image_array, image_metadata.xys, perm, preds)\n\n    return image_array", "def eval(\n        image_array,\n        image_metadata: data.ImageMetadata,\n        state: TrainState,\n    ):\n    H, W = image_array.shape[:2]\n\n    @common.jit_jaxfn_with(static_argnames=[\"chunk_size\"])\n    def get_perms(chunk_size: int) -> list[jax.Array]:\n        all_perms = jnp.arange(H*W)\n        if chunk_size >= H*W:\n            n_chunks = 1\n        else:\n            n_chunks = H*W // chunk_size\n        perms = jnp.array_split(all_perms, n_chunks)\n        return perms\n\n    for perm in tqdm(get_perms(chunk_size=2**15), desc=\"evaluating\", bar_format=common.tqdm_format):\n        # preds = state.apply_fn({\"params\": state.params}, uv)\n        preds = eval_step(state, image_metadata.uvs, perm)\n        image_array = data.set_pixels(image_array, image_metadata.xys, perm, preds)\n\n    return image_array", "\n\ndef main(\n        args: ImageFitArgs,\n        in_image: Path,\n        out_path: Path,\n        encoding: Literal[\"hashgrid\", \"frequency\"],\n        # Enable this to suppress prompt if out_path exists and directly overwrite the file.\n        overwrite: bool = False,\n        encoding_prec: int = 32,\n        model_summary: bool = False,\n    ):\n    logger.setLevel(args.common.logging.upper())\n\n    if not out_path.parent.is_dir():\n        logger.err(\"Output path's parent '{}' does not exist or is not a directory!\".format(out_path.parent))\n        exit(1)\n\n    if out_path.exists() and not overwrite:\n        logger.warn(\"Output path '{}' exists and will be overwritten!\".format(out_path))\n        try:\n            r = input(\"Continue? [y/N] \")\n            if (r.strip() + \"n\").lower()[0] != \"y\":\n                exit(0)\n        except EOFError:\n            print()\n            exit(0)\n        except KeyboardInterrupt:\n            print()\n            exit(0)\n\n    encoding_dtype = getattr(jnp, \"float{}\".format(encoding_prec))\n    dtype = getattr(jnp, \"float{}\".format(args.common.prec))\n\n    # deterministic\n    K = common.set_deterministic(args.common.seed)\n\n    # model parameters\n    K, key = jran.split(K, 2)\n    model, init_input = (\n        ImageFitter(encoding=encoding, encoding_dtype=encoding_dtype),\n        jnp.zeros((1, 2), dtype=dtype),\n    )\n    variables = model.init(key, init_input)\n    if model_summary:\n        print(model.tabulate(key, init_input))\n\n    # training state\n    state = TrainState.create(\n        apply_fn=model.apply,\n        params=variables[\"params\"],\n        tx=optax.adam(\n            learning_rate=args.train.lr,\n            b1=0.9,\n            b2=0.99,\n            # paper:\n            #   the small value of \ud835\udf16 = 10^{\u221215} can significantly accelerate the convergence of the\n            #   hash table entries when their gradients are sparse and weak.\n            eps=1e-15,\n        ),\n    )\n\n    # data\n    in_image = np.asarray(Image.open(in_image))\n    image_metadata = data.make_image_metadata(\n        image=in_image,\n        bg=(1.0, 1.0, 1.0),\n    )\n\n    for ep in range(args.train.n_epochs):\n        ep_log = ep + 1\n        K, key = jran.split(K, 2)\n        permutation = data.make_permutation_dataset(\n            key,\n            size=image_metadata.W * image_metadata.H,\n            shuffle=True\n        )\\\n            .batch(args.train.bs, drop_remainder=True)\\\n            .repeat(args.data.loop)\n        loss, state = train_epoch(\n            image_metadata=image_metadata,\n            permutation=permutation.take(args.train.n_batches).as_numpy_iterator(),\n            total_batches=args.train.n_batches,\n            state=state,\n            ep_log=ep_log,\n        )\n\n        image = np.asarray(Image.new(\"RGB\", in_image.shape[:2][::-1]))\n        image = eval(image, image_metadata, state)\n        logger.debug(\"saving image of shape {} to {}\".format(image.shape, out_path))\n        Image.fromarray(np.asarray(image)).save(out_path)\n\n        logger.info(\n            \"epoch#{:03d}: per-pixel loss={:.2e}, psnr={}\".format(\n                ep_log,\n                loss / (image_metadata.H * image_metadata.W),\n                data.psnr(in_image, image),\n            )\n        )", "\n\nif __name__ == \"__main__\":\n    tyro.cli(main)\n"]}
{"filename": "app/nerf/__main__.py", "chunked_list": ["#!/usr/bin/env python3\n\nfrom typing import Annotated\nfrom typing_extensions import assert_never\n\nimport tyro\n\nfrom utils.args import NeRFTrainingArgs,NeRFTestingArgs,NeRFGUIArgs\nfrom utils import common\n", "from utils import common\n\n\n\nCmdTrain = Annotated[\n    NeRFTrainingArgs,\n    tyro.conf.subcommand(\n        name=\"train\",\n        prefix_name=False,\n    ),", "        prefix_name=False,\n    ),\n]\nCmdTest = Annotated[\n    NeRFTestingArgs,\n    tyro.conf.subcommand(\n        name=\"test\",\n        prefix_name=False,\n    ),\n]", "    ),\n]\nCmdGui = Annotated[\n    NeRFGUIArgs,\n    tyro.conf.subcommand(\n        name=\"gui\",\n        prefix_name=False,\n    ),\n]\n", "]\n\n\nMainArgsType = CmdTrain | CmdTest | CmdGui\n\n\ndef main(args: MainArgsType):\n    logger = common.setup_logging(\"nerf\")\n    KEY = common.set_deterministic(args.common.seed)\n\n    if isinstance(args, NeRFTrainingArgs):\n        from app.nerf.train import train\n        return train(KEY, args, logger)\n    elif isinstance(args, NeRFTestingArgs):\n        from app.nerf.test import test\n        return test(KEY, args, logger)\n    elif isinstance(args, NeRFGUIArgs):\n        from app.nerf.gui import GuiWindow\n        return GuiWindow(KEY, args, logger)\n    else:\n        assert_never(args)", "\n\nif __name__ == \"__main__\":\n    args = tyro.cli(MainArgsType)\n    exit(main(args))\n"]}
{"filename": "app/nerf/train.py", "chunked_list": ["import dataclasses\nimport functools\nimport gc\nimport time\nfrom typing import Any, Dict, List, Tuple\n\nfrom flax.training import checkpoints\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jran", "import jax.numpy as jnp\nimport jax.random as jran\nimport tyro\n\nfrom models.nerfs import make_nerf_ngp, make_skysphere_background_model_ngp\nfrom models.renderers import render_image_inference\nfrom utils import common, data\nfrom utils.args import NeRFTrainingArgs\nfrom utils.types import (\n    NeRFState,", "from utils.types import (\n    NeRFState,\n    OccupancyDensityGrid,\n    RenderedImage,\n    RigidTransformation,\n    SceneData,\n)\n\nfrom ._utils import make_optimizer, train_step, format_metrics\n", "from ._utils import make_optimizer, train_step, format_metrics\n\n\ndef train_epoch(\n    KEY: jran.KeyArray,\n    state: NeRFState,\n    scene: SceneData,\n    iters: int,\n    total_samples: int,\n    ep_log: int,\n    total_epochs: int,\n    logger: common.Logger,\n) -> Tuple[NeRFState, Dict[str, Any]]:\n    n_processed_rays = 0\n    total_loss = None\n    interrupted = False\n\n    try:\n        with common.tqdm(range(iters), desc=\"Training epoch#{:03d}/{:d}\".format(ep_log, total_epochs)) as pbar:\n            start = int(state.step) % iters\n            pbar.update(start)\n            for _ in range(start, iters):\n                KEY, key_perm, key_train_step = jran.split(KEY, 3)\n                perm = jran.choice(key_perm, scene.n_pixels, shape=(total_samples,), replace=True)\n                state, metrics = train_step(\n                    state,\n                    KEY=key_train_step,\n                    total_samples=total_samples,\n                    scene=scene,\n                    perm=perm,\n                )\n                n_processed_rays += metrics[\"n_valid_rays\"]\n                loss = metrics[\"loss\"]\n                if total_loss is None:\n                    total_loss = loss\n                else:\n                    total_loss = jax.tree_util.tree_map(\n                        lambda total, new: total + new * metrics[\"n_valid_rays\"],\n                        total_loss,\n                        loss,\n                    )\n\n                pbar.set_description_str(\n                    desc=\"Training epoch#{:03d}/{:d} \".format(\n                        ep_log,\n                        total_epochs,\n                    ) + format_metrics(metrics),\n                )\n                pbar.update(1)\n\n                if state.should_call_update_ogrid:\n                    # update occupancy grid\n                    for cas in range(state.scene_meta.cascades):\n                        KEY, key = jran.split(KEY, 2)\n                        state = state.update_ogrid_density(\n                            KEY=key,\n                            cas=cas,\n                            update_all=bool(state.should_update_all_ogrid_cells),\n                            max_inference=total_samples,\n                        )\n                    state = state.threshold_ogrid()\n\n                if state.should_write_batch_metrics:\n                    logger.write_metrics_to_tensorboard(metrics, state.step)\n    except (InterruptedError, KeyboardInterrupt):\n        interrupted = True\n\n    return state, {\n        \"total_loss\": total_loss,\n        \"n_processed_rays\": n_processed_rays,\n        \"interrupted\": interrupted,\n    }", "\n\ndef train(KEY: jran.KeyArray, args: NeRFTrainingArgs, logger: common.Logger) -> int:\n    if args.ckpt is not None and not args.ckpt.exists():\n        logger.error(\"specified checkpoint '{}' does not exist\".format(args.ckpt))\n        return 2\n\n    args.exp_dir.mkdir(parents=True, exist_ok=True)\n    save_dir = common.backup_current_codebase(args.exp_dir, name_prefix=\"train-\", note=args.note)\n    config_save_path = save_dir.joinpath(\"config.yaml\")\n    config_save_path.write_text(tyro.to_yaml(args))\n\n    logs_dir = save_dir.joinpath(\"logs\")\n    logs_dir.mkdir(parents=True, exist_ok=True)\n    logger = common.setup_logging(\n        \"nerf.train\",\n        file=logs_dir.joinpath(\"train.log\"),\n        with_tensorboard=True,\n        level=args.common.logging.upper(),\n        file_level=\"DEBUG\",\n    )\n    logger.write_hparams(dataclasses.asdict(args))\n\n    logger.info(\"code saved to '{}', configurations saved at '{}'\".format(\n        save_dir,\n        config_save_path,\n    ))\n\n    # data\n    logger.info(\"loading training frames\")\n    scene_train = data.load_scene(\n        srcs=args.frames_train,\n        scene_options=args.scene,\n    )\n    logger.debug(\"sharpness_min={:.3f}, sharpness_max={:.3f}\".format(*scene_train.meta.sharpness_range))\n\n    if len(args.frames_val) > 0:\n        logger.info(\"loading validation frames\")\n        scene_val = data.load_scene(\n            srcs=args.frames_val,\n            scene_options=args.scene,\n        )\n        assert scene_train.meta.replace(frames=None) == scene_val.meta.replace(frames=None)\n    else:\n        logger.warn(\"got empty validation set, this run will not do validation\")\n\n    scene_meta = scene_train.meta\n\n    # model parameters\n    nerf_model, init_input = (\n        make_nerf_ngp(bound=scene_meta.bound, inference=False, tv_scale=args.train.tv_scale),\n        (\n            jnp.zeros((1, 3), dtype=jnp.float32),\n            jnp.zeros((1, 3), dtype=jnp.float32),\n            jnp.zeros((1, scene_meta.n_extra_learnable_dims), dtype=jnp.float32),\n        )\n    )\n    KEY, key = jran.split(KEY, 2)\n    nerf_variables = nerf_model.init(key, *init_input)\n    if args.common.summary:\n        print(nerf_model.tabulate(key, *init_input))\n\n    if scene_meta.bg:\n        bg_model, init_input = (\n            make_skysphere_background_model_ngp(bound=scene_meta.bound),\n            (\n                jnp.zeros((1, 3), dtype=jnp.float32),\n                jnp.zeros((1, 3), dtype=jnp.float32),\n                jnp.zeros((1, scene_meta.n_extra_learnable_dims), dtype=jnp.float32),\n            ),\n        )\n        KEY, key = jran.split(KEY, 2)\n        bg_variables = bg_model.init(key, *init_input)\n\n    KEY, key = jran.split(KEY, 2)\n    # training state\n    state = NeRFState.create(\n        ogrid=OccupancyDensityGrid.create(\n            cascades=scene_meta.cascades,\n            grid_resolution=args.raymarch.density_grid_res,\n        ),\n        raymarch=args.raymarch,\n        render=args.render,\n        scene_options=args.scene,\n        scene_meta=scene_meta,\n        # unfreeze the frozen dict so that the weight_decay mask can apply, see:\n        #   <https://github.com/deepmind/optax/issues/160>\n        #   <https://github.com/google/flax/issues/1223>\n        nerf_fn=nerf_model.apply,\n        bg_fn=bg_model.apply if scene_meta.bg else None,\n        params={\n            \"nerf\": nerf_variables[\"params\"].unfreeze(),\n            \"bg\": bg_variables[\"params\"].unfreeze() if scene_meta.bg else None,\n            \"appearance_embeddings\": jran.uniform(\n                key=key,\n                shape=(len(scene_meta.frames), scene_meta.n_extra_learnable_dims),\n                dtype=jnp.float32,\n                minval=-1,\n                maxval=1,\n            ),\n        },\n        tx=make_optimizer(args.train.lr),\n    )\n    if args.ckpt is not None:\n        state = checkpoints.restore_checkpoint(args.ckpt, target=state)\n        if state.step == 0:\n            logger.error(\"an empty checkpoint was loaded from '{}'\".format(args.ckpt))\n            return 3\n        logger.info(\"checkpoint loaded from '{}' (step={})\".format(args.ckpt, int(state.step)))\n    state = state.mark_untrained_density_grid()  # still needs to mark grids even if state is loaded\n\n    logger.info(\"starting training\")\n    # training loop\n    for ep in range(state.epoch(args.train.iters), args.train.epochs):\n        gc.collect()\n\n        ep_log = ep + 1\n\n        KEY, key_resample, key_train = jran.split(KEY, 3)\n        scene_train = scene_train.resample_pixels(\n            KEY=key_resample,\n            new_max_mem_mbytes=args.scene.max_mem_mbytes,\n        )\n        state, metrics = train_epoch(\n            KEY=key_train,\n            state=state,\n            scene=scene_train,\n            iters=args.train.iters,\n            total_samples=args.train.bs,\n            ep_log=ep_log,\n            total_epochs=args.train.epochs,\n            logger=logger,\n        )\n        if metrics[\"interrupted\"]:\n            logger.warn(\"aborted at epoch {}\".format(ep_log))\n            logger.info(\"saving training state ... \")\n            ckpt_name = checkpoints.save_checkpoint(logs_dir, state, step=\"ep{}aborted\".format(ep_log), overwrite=True, keep=2**30)\n            logger.info(\"training state of epoch {} saved to: {}\".format(ep_log, ckpt_name))\n            logger.info(\"exiting cleanly ...\")\n            return 0\n\n        mean_loss = jax.tree_util.tree_map(\n            lambda val: val / metrics[\"n_processed_rays\"],\n            metrics[\"total_loss\"],\n        )\n        logger.info(\"epoch#{:03d}: loss:{{rgb={:.3e}({:.2f}dB),tv={:.3e}}}\".format(\n            ep_log,\n            mean_loss[\"rgb\"],\n            data.linear_to_db(mean_loss[\"rgb\"], maxval=1,),\n            mean_loss[\"total_variation\"],\n        ))\n        logger.write_scalar(\"epoch/\u2193loss (rgb)\", mean_loss[\"rgb\"], step=ep_log)\n        logger.write_scalar(\"epoch/\u2191estimated PSNR (db)\", data.linear_to_db(mean_loss[\"rgb\"], maxval=1), step=ep_log)\n        logger.write_scalar(\"batch/\u2193loss (total variation)\", mean_loss[\"total_variation\"], state.step)\n\n        logger.info(\"saving training state ... \")\n        ckpt_name = checkpoints.save_checkpoint(\n            logs_dir,\n            state,\n            step=ep_log * args.train.iters,\n            overwrite=True,\n            keep=args.train.keep,\n            keep_every_n_steps=args.train.keep_every_n_steps,\n        )\n        logger.info(\"training state of epoch {} saved to: {}\".format(ep_log, ckpt_name))\n\n        if ep_log % args.train.validate_every == 0:\n            if len(args.frames_val) == 0:\n                logger.warn(\"empty validation set, skipping validation\")\n                continue\n\n            val_start_time = time.time()\n            rendered_images: List[RenderedImage] = []\n            state_eval = state\\\n                .replace(raymarch=args.raymarch_eval)\\\n                .replace(render=args.render_eval)\n            for val_i, val_view in enumerate(common.tqdm(scene_val.all_views, desc=\"| validating\")):\n                logger.debug(\"validating on {}\".format(val_view.file))\n                val_transform = RigidTransformation(\n                    rotation=scene_val.transforms[val_i, :9].reshape(3, 3),\n                    translation=scene_val.transforms[val_i, -3:].reshape(3),\n                )\n                KEY, key = jran.split(KEY, 2)\n                bg, rgb, depth, _ = data.to_cpu(render_image_inference(\n                    KEY=key,\n                    transform_cw=val_transform,\n                    state=state_eval,\n                ))\n                rendered_images.append(RenderedImage(\n                    bg=bg,\n                    rgb=rgb,\n                    depth=depth,  # call to data.mono_to_rgb is deferred below so as to minimize impact on rendering speed\n                ))\n            val_end_time = time.time()\n            logger.write_scalar(\n                tag=\"validation/\u2193rendering time (ms) per image\",\n                value=(val_end_time - val_start_time) / len(rendered_images) * 1000,\n                step=ep_log,\n            )\n\n            gt_rgbs_f32 = list(map(\n                lambda val_view, rendered_image: data.blend_rgba_image_array(\n                    val_view.image_rgba_u8.astype(jnp.float32) / 255,\n                    rendered_image.bg,\n                ),\n                scene_val.all_views,\n                rendered_images,\n            ))\n\n            logger.debug(\"calculating psnr\")\n            mean_psnr = sum(map(\n                data.psnr,\n                map(data.f32_to_u8, gt_rgbs_f32),\n                map(lambda ri: ri.rgb, rendered_images),\n            )) / len(rendered_images)\n            logger.info(\"validated {} images, mean psnr={}\".format(len(rendered_images), mean_psnr))\n            logger.write_scalar(\"validation/\u2191mean psnr\", mean_psnr, step=ep_log)\n\n            logger.debug(\"writing images to tensorboard\")\n            concatenate_fn = lambda gt, rendered_image: data.add_border(functools.reduce(\n                functools.partial(\n                    data.side_by_side,\n                    width=scene_meta.camera.width,\n                    height=scene_meta.camera.height,\n                ),\n                [\n                    gt,\n                    rendered_image.rgb,\n                    common.compose(data.mono_to_rgb, data.f32_to_u8)(rendered_image.depth),\n                ],\n            ))\n            logger.write_image(\n                tag=\"validation/[gt|rendered|depth]\",\n                image=list(map(\n                    concatenate_fn,\n                    map(data.f32_to_u8, gt_rgbs_f32),\n                    rendered_images,\n                )),\n                step=ep_log,\n                max_outputs=len(rendered_images),\n            )\n\n            del state_eval\n            del gt_rgbs_f32\n            del rendered_images\n    return 0", ""]}
{"filename": "app/nerf/gui.py", "chunked_list": ["from copy import deepcopy\nfrom enum import Enum\nimport logging\nfrom pathlib import Path\nimport numpy as np\nfrom typing import List, Any, Tuple, Union\nimport jax\nimport jax.random as jran\nimport jax.numpy as jnp\nfrom flax.training import checkpoints", "import jax.numpy as jnp\nfrom flax.training import checkpoints\nfrom dataclasses import dataclass, field\nimport threading\nimport dearpygui.dearpygui as dpg\nimport ctypes\nfrom utils.args import NeRFGUIArgs\nfrom .train import *\nfrom utils.types import (RGBColor, SceneData, SceneMeta, Camera)\nfrom models.nerfs import (NeRF, SkySphereBg)", "from utils.types import (RGBColor, SceneData, SceneMeta, Camera)\nfrom models.nerfs import (NeRF, SkySphereBg)\nfrom PIL import Image\nimport time\n\n\n@dataclass\nclass CKPT():\n    need_load_ckpt = False\n    ckpt_file_path: Path = Path(\"\")\n    step: int = 0\n\n    def parse_ckpt(self, ckpt_name: str, ckpt_path: str) -> str:\n        success = False\n        s = ckpt_name.split(\"_\")\n        if s[0] == \"checkpoint\" and Path(ckpt_path).exists:\n            try:\n                self.step = int(s[1].split(\".\")[0])\n                self.ckpt_file_path = Path(ckpt_path)\n                self.need_load_ckpt = True\n                success = True\n            except TypeError or ValueError as e:\n                self.logger.error(e)\n            finally:\n                if success:\n                    return \"checkpoint loaded from '{}'\".format(self.ckpt_file_path)\n                return \"Fail to load checkpoint, causing the file is not a checkpoint\"", "\n\n@dataclass\nclass CameraPose():\n    theta: float = 160.0\n    phi: float = -30.0\n    radius: float = 4.0\n    tx: float = 0.0\n    ty: float = 0.0\n    centroid: np.ndarray = np.asarray([0., 0., 0.])\n\n    def pose_spherical(self, theta, phi, radius):\n        trans_t = lambda t: np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, t],\n                                      [0, 0, 0, 1]], np.float32)\n        rot_phi = lambda phi: np.array(\n            [[1, 0, 0, 0], [0, np.cos(phi), -np.sin(phi), 0],\n             [0, np.sin(phi), np.cos(phi), 0], [0, 0, 0, 1]], np.float32)\n        rot_theta = lambda theta: np.array(\n            [[np.cos(theta), 0, -np.sin(theta), 0], [0, 1, 0, 0],\n             [np.sin(theta), 0, np.cos(theta), 0], [0, 0, 0, 1]], np.float32)\n        c2w = trans_t(radius)\n        #rotate\n        c2w = np.matmul(rot_phi(phi / 180. * np.pi), c2w)\n        c2w = np.matmul(rot_theta(theta / 180. * np.pi), c2w)\n\n        return c2w\n\n    @property\n    def pose(self):\n        mod = lambda x: x % 360\n        self.theta = mod(self.theta)\n        self.phi = mod(self.phi)\n        c2w = self.pose_spherical(self.theta, self.phi, self.radius)\n        #translate\n        self.centroid = np.asarray(\n            self.centroid) + self.tx * c2w[:3, 0] + self.ty * c2w[:3, 1]\n        self.tx, self.ty = 0, 0\n        trans_centroid = np.array(\n            [[1, 0, 0, self.centroid[0]], [0, 1, 0, self.centroid[1]],\n             [0, 0, 1, self.centroid[2]], [0, 0, 0, 1]], np.float32)\n        c2w = np.matmul(trans_centroid, c2w)\n        c2w = np.matmul(np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0,1]]), c2w)\n        return jnp.asarray(c2w)\n\n    def move(self, dx, dy):\n        self.theta += .3 * dx\n        self.phi -= .2 * dy\n        return self.pose\n\n    def trans(self, dx, dy):\n        velocity = 8e-4\n        self.tx -= dx * velocity * self.radius\n        self.ty += dy * velocity * self.radius\n        return self.pose\n\n    def change_radius(self, rate):\n        self.radius *= 1.1**(-rate)\n        return self.pose", "\n\n@dataclass\nclass Gui_trainer():\n    KEY: jran.KeyArray\n    args: NeRFGUIArgs\n    logger: common.Logger\n    camera_pose: jnp.array\n    back_color: RGBColor\n\n    scene_train: SceneData = field(init=False)\n    scene_meta: SceneMeta = field(init=False)\n\n    nerf_model_train: NeRF = field(init=False)\n    nerf_model_inference: NeRF = field(init=False)\n    nerf_variables: Any = field(init=False)\n    bg_model: SkySphereBg = field(init=False, default=None)\n    bg_variables: Any = field(init=False)\n    optimizer: Any = field(init=False)\n\n    state: NeRFState = field(init=False)\n    last_resample_step: int = -1\n    cur_step: int = 0\n    log_step: int = 0\n    loss_log: str = \"--\"\n\n    istraining: bool = field(init=False)\n\n    data_step: List[int] = field(default_factory=list, init=False)\n    data_pixel_quality: List[float] = field(default_factory=list, init=False)\n\n    compacted_batch: int = -1\n    not_compacted_batch: int = -1\n    rays_num: int = -1\n    mean_effective_samples_per_ray: int = -1\n    mean_samples_per_ray: int = -1\n    camera_near: float = 0.1\n    camera: Camera = field(init=False)\n    need_exit: bool = False\n\n    loading_ckpt: bool = False\n    ckpt: CKPT = CKPT()\n\n    def __post_init__(self):\n        self.data_step = []\n        self.data_pixel_quality = []\n        self.cur_step = 0\n\n        self.istraining = True\n\n        self.args.exp_dir.mkdir(parents=True, exist_ok=True)\n        save_dir = common.backup_current_codebase(self.args.exp_dir, name_prefix=\"gui-\",\n                                                  note=self.args.note)\n        config_save_path = save_dir.joinpath(\"config.yaml\")\n        config_save_path.write_text(tyro.to_yaml(self.args))\n\n        logs_dir = save_dir.joinpath(\"logs\")\n        logs_dir.mkdir(parents=True, exist_ok=True)\n        self.logger = common.setup_logging(\n            \"nerf.gui\",\n            file=logs_dir.joinpath(\"gui.log\"),\n            with_tensorboard=True,\n            level=self.args.common.logging.upper(),\n            file_level=\"DEBUG\",\n        )\n        self.logger.write_hparams(dataclasses.asdict(self.args))\n\n        self.logger.info(\"code saved to '{}', configurations saved at '{}'\".format(\n            save_dir,\n            config_save_path,\n        ))\n\n        # load data\n        self.scene_train = data.load_scene(\n            srcs=self.args.frames_train,\n            scene_options=self.args.scene,\n        )\n        self.scene_meta = self.scene_train.meta\n\n        # model parameters\n        self.nerf_model_train, self.nerf_model_inference, init_input = (\n            make_nerf_ngp(bound=self.scene_meta.bound,\n                          inference=False,\n                          tv_scale=self.args.train.tv_scale),\n            make_nerf_ngp(bound=self.scene_meta.bound,\n                          inference=True), (jnp.zeros((1, 3), dtype=jnp.float32),\n                                            jnp.zeros((1, 3), dtype=jnp.float32),\n                                            jnp.zeros((1, self.scene_meta.n_extra_learnable_dims),\n                                                      dtype=jnp.float32)))\n        self.KEY, key = jran.split(self.KEY, 2)\n        self.nerf_variables = self.nerf_model_train.init(key, *init_input)\n        if self.args.common.summary:\n            print(self.nerf_model_train.tabulate(key, *init_input))\n\n        if self.scene_meta.bg:\n            self.bg_model, init_input = (make_skysphere_background_model_ngp(\n                bound=self.scene_meta.bound), (jnp.zeros((1, 3),dtype=jnp.float32),\n                                               jnp.zeros((1, 3),dtype=jnp.float32),\n                                               jnp.zeros((1, self.scene_meta.n_extra_learnable_dims),\n                                                      dtype=jnp.float32)))\n            self.KEY, key = jran.split(self.KEY, 2)\n            self.bg_variables = self.bg_model.init(key, *init_input)\n\n        self.optimizer = make_optimizer(self.args.train.lr)\n        if self.ckpt.need_load_ckpt:\n            self.load_checkpoint(self.ckpt.ckpt_file_path, self.ckpt.step)\n        else:\n            self.KEY, key = jran.split(self.KEY, 2)\n            self.state = NeRFState.create(\n                ogrid=OccupancyDensityGrid.create(\n                    cascades=self.scene_meta.cascades,\n                    grid_resolution=self.args.raymarch.density_grid_res,\n                ),\n                raymarch=self.args.raymarch,\n                render=self.args.render,\n                scene_options=self.args.scene,\n                scene_meta=self.scene_meta,\n                # unfreeze the frozen dict so that the weight_decay mask can apply, see:\n                #   <https://github.com/deepmind/optax/issues/160>\n                #   <https://github.com/google/flax/issues/1223>\n                nerf_fn=self.nerf_model_train.apply,\n                bg_fn=self.bg_model.apply if self.scene_meta.bg else None,\n                params={\n                    \"nerf\":\n                    self.nerf_variables[\"params\"].unfreeze(),\n                    \"bg\":\n                    self.bg_variables[\"params\"].unfreeze()\n                    if self.scene_meta.bg else None,\n                    \"appearance_embeddings\": jran.uniform(\n                        key=key,\n                        shape=(len(self.scene_meta.frames), self.scene_meta.n_extra_learnable_dims),\n                        dtype=jnp.float32,\n                        minval=-1,\n                        maxval=1,\n                    )\n                },\n                tx=self.optimizer,\n            )\n            self.state = self.state.mark_untrained_density_grid()\n        self.camera = Camera(\n            width=self.args.viewport.W,\n            height=self.args.viewport.H,\n            fx=self.scene_meta.camera.fx,\n            fy=self.scene_meta.camera.fy,\n            cx=self.args.viewport.W / 2,\n            cy=self.args.viewport.H / 2,\n            near=self.camera_near,\n        )\n\n    def set_render_camera(self, _scale, _H, _W) -> Camera:\n        self.camera = Camera(\n            width=_W,\n            height=_H,\n            fx=self.scene_meta.camera.fx,\n            fy=self.scene_meta.camera.fy,\n            cx=_W / 2,\n            cy=_H / 2,\n            near=self.camera_near,\n        )\n        self.camera = self.camera.scale_resolution(_scale)\n\n    def render_frame(self, _scale: float, _H: int, _W: int, render_cost: bool):\n        self.set_render_camera(_scale, _H, _W)\n        #camera pose\n        transform = RigidTransformation(\n            rotation=self.camera_pose[:3, :3],\n            translation=jnp.squeeze(self.camera_pose[:3, 3].reshape(-1, 3),\n                                    axis=0))\n        self.KEY, key = jran.split(self.KEY, 2)\n        bg, rgb, depth, cost = render_image_inference(\n            KEY=key,\n            transform_cw=transform,\n            state=self.state.replace(\n                raymarch=self.args.raymarch_eval,\n                render=self.args.render_eval.replace(bg=self.back_color),\n                nerf_fn=self.nerf_model_inference.apply,\n            ),\n            camera_override=self.camera,\n            render_cost=render_cost)\n\n        bg = self.get_npf32_image(bg,\n                                  W=self.args.viewport.W,\n                                  H=self.args.viewport.H)\n        rgb = self.get_npf32_image(rgb,\n                                   W=self.args.viewport.W,\n                                   H=self.args.viewport.H)\n        depth = self.color_depth(depth,\n                                 W=self.args.viewport.W,\n                                 H=self.args.viewport.H)\n        if render_cost:\n            cost = self.get_cost_image(cost,\n                                       W=self.args.viewport.W,\n                                       H=self.args.viewport.H)\n        return (bg, rgb, depth, cost)\n\n    def get_cost_image(self, cost, W, H):\n        img = Image.fromarray(np.array(cost, dtype=np.uint8))\n        img = img.convert('RGB')\n        img = img.resize(size=(W, H), resample=Image.NEAREST)\n        cost = np.array(img, dtype=np.float32) / 255.\n        return cost\n\n    def color_depth(self, depth, W, H):\n        depth = np.array(data.f32_to_u8(data.mono_to_rgb(depth)),\n                         dtype=np.uint8)\n        img = Image.fromarray(depth, mode='RGBA')\n        img = img.convert('RGB')\n        img = img.resize(size=(W, H), resample=Image.NEAREST)\n        depth = np.array(img, dtype=np.float32) / 255.\n        return depth\n\n    def load_checkpoint(self, path: Path, step: int):\n        self.loading_ckpt = True\n        try:\n            if not path.exists():\n                raise FileNotFoundError(\"{} does not exist\".format(path))\n            self.logger.info(\"loading checkpoint from '{}'\".format(path))\n            state: NeRFState = checkpoints.restore_checkpoint(\n                path,\n                target=NeRFState.empty(\n                    raymarch=self.args.raymarch,\n                    render=self.args.render,\n                    scene_options=self.args.scene,\n                    scene_meta=self.scene_meta,\n                    nerf_fn=self.nerf_model_train.apply,\n                    bg_fn=self.bg_model.apply if self.scene_meta.bg else None,\n                    tx=self.optimizer,\n                ),\n            )\n            # WARN:\n            #   flax.checkpoints.restore_checkpoint() returns a pytree with all arrays of numpy's array type,\n            #   which slows down inference.  use jax.device_put() to move them to jax's default device.\n            # REF: <https://github.com/google/flax/discussions/1199#discussioncomment-635132>\n            self.state = jax.device_put(state)\n            self.state = self.state.mark_untrained_density_grid()\n            self.logger.info(\"checkpoint loaded from '{}'\".format(path))\n            self.cur_step = step\n            self.loading_ckpt = False\n            return \"checkpoint loaded from '{}'\".format(path)\n        except BaseException as e:\n            self.logger.error(e)\n            return e\n\n    def train_steps(self, steps: int) -> Tuple[np.array, np.array, np.array]:\n        if self.loading_ckpt:\n            return\n        gc.collect()\n        try:\n            if self.istraining:\n                if self.last_resample_step < 0 or self.state.step - self.last_resample_step >= 1024:\n                    self.KEY, key_resample = jran.split(self.KEY, 2)\n                    self.scene_train = self.scene_train.resample_pixels(\n                        KEY=key_resample,\n                        new_max_mem_mbytes=self.args.scene.max_mem_mbytes,\n                    )\n                    self.last_resample_step = self.state.step\n                self.KEY, key_train = jran.split(self.KEY, 2)\n                self.state = self.gui_train_epoch(\n                    KEY=key_train,\n                    state=self.state,\n                    scene=self.scene_train,\n                    iters=steps,\n                    total_samples=self.args.train.bs,\n                    #total_samples=self.args.train.bs,\n                    cur_steps=self.cur_step,\n                    logger=self.logger,\n                )\n                self.cur_step = self.cur_step + steps\n        except UnboundLocalError as e:\n            self.logger.exception(e)\n\n    def get_npf32_image(self, img: jnp.array, W, H) -> np.array:\n        img = Image.fromarray(np.array(img, dtype=np.uint8))\n        img = img.resize(size=(W, H), resample=Image.NEAREST)\n        img = np.array(img, dtype=np.float32) / 255.\n        return img\n\n    def gui_train_epoch(\n        self,\n        KEY: jran.KeyArray,\n        state: NeRFState,\n        scene: SceneData,\n        iters: int,\n        total_samples: int,\n        cur_steps: int,\n        logger: common.Logger,\n    ):\n        self.log_step = 0\n        for _ in (pbar := common.tqdm(range(iters),\n                                      desc=\"Training step#{:03d}\".format(cur_steps),\n                                      leave=False)):\n            if self.need_exit:\n                raise KeyboardInterrupt\n            if not self.istraining:\n                logger.warn(\"aborted at step {}\".format(cur_steps))\n                logger.debug(\"exiting cleanly ...\")\n                exit()\n            KEY, key_perm, key_train_step = jran.split(KEY, 3)\n            perm = jran.choice(key_perm,\n                               scene.n_pixels,\n                               shape=(total_samples, ),\n                               replace=True)\n            state, metrics = train_step(\n                state,\n                KEY=key_train_step,\n                total_samples=total_samples,\n                scene=scene,\n                perm=perm,\n            )\n            self.log_step += 1\n            cur_steps = cur_steps + 1\n            loss = metrics[\"loss\"]\n\n            self.data_step, self.data_pixel_quality = (  # the 2 lists are ploted so should be updated simultaneously\n                self.data_step + [self.log_step + self.cur_step],\n                self.data_pixel_quality +\n                [data.linear_to_db(loss[\"rgb\"], maxval=1)])\n\n            self.mean_effective_samples_per_ray = metrics[\"measured_batch_size\"] / metrics[\"n_valid_rays\"]\n            self.mean_samples_per_ray = metrics[\"measured_batch_size_before_compaction\"] / metrics[\"n_valid_rays\"]\n\n            pbar.set_description_str(\n                desc=\"Training step#{:03d} \".format(cur_steps) + format_metrics(metrics))\n\n            if state.should_call_update_ogrid:\n                # update occupancy grid\n                for cas in range(state.scene_meta.cascades):\n                    KEY, key = jran.split(KEY, 2)\n                    state = state.update_ogrid_density(\n                        KEY=key,\n                        cas=cas,\n                        update_all=bool(state.should_update_all_ogrid_cells),\n                        max_inference=total_samples,\n                    )\n                state = state.threshold_ogrid()\n\n            self.compacted_batch = metrics[\"measured_batch_size\"]\n            self.not_compacted_batch = metrics[\n                \"measured_batch_size_before_compaction\"]\n            self.rays_num = metrics[\"n_valid_rays\"]\n            if state.should_write_batch_metrics:\n                logger.write_metrics_to_tensorboard(metrics, state.step)\n\n        return state\n\n    def stop_trainer(self):\n        self.istraining = False\n\n    def setBackColor(self, color: RGBColor):\n        self.back_color = color\n\n    def get_currentStep(self):\n        return self.cur_step\n\n    def get_logStep(self):\n        return self.log_step\n\n    def get_state(self) -> NeRFState:\n        return self.state\n\n    def get_plotData(self):\n        return (self.data_step, self.data_pixel_quality)\n\n    def get_effective_samples_nums(self):\n        return self.mean_effective_samples_per_ray\n\n    def get_samples_nums(self):\n        return self.mean_samples_per_ray\n\n    def get_compactedBatch(self):\n        return self.compacted_batch\n\n    def get_notCompactedBatch(self):\n        return self.not_compacted_batch\n\n    def get_raysNum(self):\n        return self.rays_num", "\n\nclass TrainThread(threading.Thread):\n\n    def __init__(self, KEY, args: NeRFGUIArgs, logger, camera_pose, step,\n                 back_color, ckpt):\n        super(TrainThread, self).__init__()\n\n        self.KEY = KEY\n        self.args = args\n        self.logger = logger\n        self.camera_pose = camera_pose\n\n        self.istraining = True\n        self.needUpdate = True\n        self.istesting = False\n        self.needtesting = False\n        self.step = step\n        self.scale = self.args.viewport.resolution_scale\n\n        self.H, self.W = self.args.viewport.H, self.args.viewport.W\n        self.back_color = back_color\n        self.framebuff = None\n        self.rgb = None\n        self.depth = None\n        self.trainer = None\n        self.initFrame()\n        self.train_infer_time = -1\n        self.render_infer_time = -1\n        self.data_step = []\n        self.data_pixel_quality = []\n\n        self.compacted_batch = -1\n        self.not_compacted_batch = -1\n        self.rays_num = -1\n\n        self.frame_updated = False\n        self.mode = Mode.Render\n        self.havestart = False\n        self.ckpt = ckpt\n\n    def initFrame(self):\n        frame_init = np.tile(np.asarray(self.back_color, dtype=np.float32),\n                             (self.H, self.W, 1))\n        self.framebuff = frame_init.copy()\n        self.rgb = frame_init.copy()\n        self.depth = frame_init.copy()\n        self.cost = frame_init.copy()\n        self.frame_updated = True\n\n    def setMode(self, mode):\n        self.mode = mode\n\n    def setBackColor(self, color: RGBColor):\n        self.back_color = color\n        if self.trainer:\n            self.trainer.setBackColor(self.back_color)\n\n    def run(self):\n        try:\n            self.trainer = Gui_trainer(KEY=self.KEY,\n                                       args=self.args,\n                                       logger=self.logger,\n                                       camera_pose=self.camera_pose,\n                                       back_color=self.back_color,\n                                       ckpt=self.ckpt)\n        except Exception as e:\n            self.logger.exception(e)\n            self.needUpdate = False\n        while self.needUpdate:\n            try:\n                if self.istraining and self.trainer:\n                    start_time = time.time()\n                    self.trainer.train_steps(self.step)\n                    end_time = time.time()\n                    self.train_infer_time = end_time - start_time\n                    self.test()\n                if self.istesting and self.needtesting:\n                    self.havestart = True\n                    start_time = time.time()\n                    self.trainer.setBackColor(self.back_color)\n                    _, self.rgb, self.depth, self.cost = self.trainer.render_frame(\n                        self.scale, self.H, self.W, self.mode == Mode.Cost)\n                    if self.mode == Mode.Render:\n                        self.framebuff = self.rgb\n                    elif self.mode == Mode.depth:\n                        self.framebuff = self.depth\n                    elif self.mode == Mode.Cost:\n                        if self.cost is not None:\n                            self.framebuff = self.cost\n                        else:\n                            self.framebuff = np.tile(np.asarray(self.back_color, dtype=np.float32),\n                                                     (self.H, self.W, 1))\n                    else:\n                        raise NotImplementedError(\"visualization mode '{}' is not implemented\"\n                                                  .format(self.mode))\n                    self.frame_updated = True\n                    end_time = time.time()\n                    self.render_infer_time = end_time - start_time\n                    self.istesting = False\n            except Exception as e:\n                self.logger.exception(e)\n                break\n\n    def get_TrainInferTime(self):\n        if self.train_infer_time != -1:\n            return \"{:.6f}\".format(self.train_infer_time)\n        else:\n            return \"no data\"\n\n    def get_RenderInferTime(self):\n        if self.render_infer_time != -1:\n            return \"{:.6f}\".format(self.render_infer_time)\n        else:\n            return \"no data\"\n\n    def get_Fps(self):\n        if self.train_infer_time == -1 and self.render_infer_time == -1:\n            return \"no data\"\n        elif self.render_infer_time == -1:\n            return \"{:.3f}\".format(1.0 / (self.train_infer_time))\n        elif self.train_infer_time == -1 or not self.istraining:\n            return \"{:.3f}\".format(1.0 / (self.render_infer_time))\n        else:\n            return \"{:.3f}\".format(\n                1.0 / (self.render_infer_time + self.train_infer_time))\n\n    def get_compactedBatch(self):\n        if self.trainer:\n            self.compacted_batch = self.trainer.get_compactedBatch()\n            if self.compacted_batch != -1:\n                return \"{:d}\".format(self.compacted_batch)\n            else:\n                return \"no data\"\n        return \"no data\"\n\n    def get_notCompactedBatch(self):\n        if self.trainer:\n            self.not_compacted_batch = self.trainer.get_notCompactedBatch()\n            if self.not_compacted_batch != -1:\n                return \"{:d}\".format(self.not_compacted_batch)\n            else:\n                return \"no data\"\n        return \"no data\"\n\n    def get_raysNum(self):\n        if self.trainer:\n            self.rays_num = self.trainer.get_raysNum()\n            if self.rays_num != -1:\n                return \"{:d}\".format(self.rays_num)\n            else:\n                return \"no data\"\n        return \"no data\"\n\n    def stop(self):\n        self.istraining = False\n        self.needUpdate = False\n        if self.trainer:\n\n            self.trainer.stop_trainer()\n        thread_id = self.get_id()\n        self.logger.debug(\"throwing training thread exit Exception\")\n        res = ctypes.pythonapi.PyThreadState_SetAsyncExc(\n            thread_id, ctypes.py_object(SystemExit))\n        if res > 1:\n            ctypes.pythonapi.PyThreadState_SetAsyncExc(thread_id, 0)\n            self.logger.warn(\"exception raise failure\",\n                             category=None,\n                             stacklevel=1)\n\n    def set_scale(self, _scale):\n        self.scale = _scale\n\n    def get_scale(self):\n        return self.scale\n\n    def get_id(self):\n        # returns id of the respective thread\n        if hasattr(self, '_thread_id'):\n            return self._thread_id\n        for id, thread in threading._active.items():\n            if thread is self:\n                return id\n\n    def get_state(self) -> NeRFState:\n        return self.trainer.get_state()\n\n    def set_camera_pose(self, camera_pose):\n        if self.trainer:\n            self.trainer.camera_pose = camera_pose\n\n    def change_WH(self, W, H):\n        self.W = W\n        self.H = H\n\n    def get_logStep(self):\n        if self.trainer:\n            return self.trainer.get_logStep()\n        return 0\n\n    def get_currentStep(self):\n        if self.trainer:\n            return self.trainer.get_currentStep()\n        return 0\n\n    def get_plotData(self):\n        if self.trainer:\n            self.data_step, self.data_pixel_quality = self.trainer.get_plotData(\n            )\n        return (self.data_step, self.data_pixel_quality)\n\n    def get_effective_samples_nums(self):\n        if self.trainer:\n            return \"{:.3f}\".format(self.trainer.mean_effective_samples_per_ray)\n        else:\n            return \"no data\"\n\n    def get_samples_nums(self):\n        if self.trainer:\n            return \"{:.3f}\".format(\n                self.trainer.mean_samples_per_ray)\n        else:\n            return \"no data\"\n\n    def test(self):\n        self.istesting = True\n\n    def finishUpdate(self):\n        self.frame_updated = False\n\n    def canUpdate(self):\n        return self.frame_updated\n\n    def setStep(self, step):\n        self.step = step\n\n    def setCamNear(self, near):\n        if self.trainer:\n            self.trainer.camera_near = near\n\n    def getPinholeCam(self):\n        if self.trainer:\n            return self.trainer.camera\n        return None", "\n\nclass Mode(Enum):\n    Render = 1\n    depth = 2\n    Cost = 3\n\n\n@dataclass\nclass NeRFGUI():\n    framebuff: Any = field(init=False)\n    H: int = field(init=False)\n    W: int = field(init=False)\n\n    need_train: bool = False\n    istesting: bool = False\n    train_thread: TrainThread = field(init=False)\n\n    args: NeRFGUIArgs = None\n\n    KEY: jran.KeyArray = None\n    logger: logging.Logger = None\n    cameraPose: CameraPose = CameraPose()\n    cameraPosePrev: CameraPose = CameraPose()\n    cameraPoseNext: CameraPose = CameraPose()\n    scale_slider: Union[int, str] = field(init=False)\n    back_color: RGBColor = field(init=False)\n    scale: float = field(init=False)\n    data_step: List[int] = field(default_factory=list, init=False)\n    data_pixel_quality: List[float] = field(default_factory=list, init=False)\n\n    texture_H: int = field(init=False)\n    texture_W: int = field(init=False)\n    View_H: int = field(init=False)\n    View_W: int = field(init=False)\n\n    exit_flag: bool = False\n    mode: Mode = Mode.Render\n\n    mouse_pressed: bool = False\n    need_test: bool = True\n\n    #ckpt\n    ckpt: CKPT = CKPT()\n\n    @property\n    def _effective_resolution_display(self) -> str:\n        return \"{}x{}\".format(\n            *map(lambda val: int(val * self.scale), (self.W, self.H)))\n\n    def __post_init__(self):\n        self.H, self.W = self.args.viewport.H, self.args.viewport.W\n        self.back_color = self.args.render_eval.bg\n        self.scale = self.args.viewport.resolution_scale\n        self.texture_H, self.texture_W = self.H, self.W\n        self.framebuff = np.tile(np.asarray(self.back_color, dtype=np.float32),\n                                 (self.H, self.W, 3))\n        radius_init = 4.\n        self.cameraPose, self.cameraPosePrev, self.cameraPoseNext = (\n            CameraPose(radius=radius_init),\n            CameraPose(radius=radius_init),\n            CameraPose(radius=radius_init),\n        )\n        dpg.create_context()\n        self.train_thread = None\n        self.ItemsLayout()\n\n    def ItemsLayout(self):\n\n        def callback_backgroundColor():\n            self.back_color = tuple(\n                map(lambda val: val / 255,\n                    dpg.get_value(\"_BackColor\")[:3]))\n            self.setFrameColor()\n\n        def callback_mouseDrag(_, app_data):\n            if not dpg.is_item_focused(\"_primary_window\"):\n                return\n            if not self.need_test:\n                return\n            dx = app_data[1]\n            dy = app_data[2]\n            self.cameraPoseNext = deepcopy(self.cameraPosePrev)\n            self.cameraPoseNext.move(dx, dy)\n            if self.train_thread:\n                self.train_thread.set_camera_pose(self.cameraPoseNext.pose)\n                self.train_thread.test()\n                self.show_cam_angle(self.cameraPoseNext.theta,\n                                    self.cameraPoseNext.phi)\n\n        def callback_midmouseDrag(_, app_data):\n            if not self.need_test:\n                return\n            dx = app_data[1]\n            dy = app_data[2]\n            self.cameraPoseNext = deepcopy(self.cameraPosePrev)\n            self.cameraPoseNext.trans(dx, dy)\n            if self.train_thread:\n                self.train_thread.set_camera_pose(self.cameraPoseNext.pose)\n                self.train_thread.test()\n                self.show_cam_centroid(self.cameraPoseNext.centroid[0],\n                                       self.cameraPoseNext.centroid[1],\n                                       self.cameraPoseNext.centroid[2])\n\n        def callback_mouseDown(_, app_data):\n            if not dpg.is_item_hovered(\"_primary_window\"):\n                return\n            if not self.need_test:\n                return\n            self.mouse_pressed = True\n            if app_data[1] < 1e-5:\n                self.cameraPosePrev = self.cameraPose\n            if self.train_thread:\n                self.train_thread.setStep(1)\n\n        def callback_mouseRelease():\n            if not self.need_test:\n                return\n            self.mouse_pressed = False\n            self.cameraPose = self.cameraPoseNext\n            if self.train_thread:\n                self.train_thread.setStep(self.args.train.iters)\n\n        def callback_mouseWheel(_, app_data):\n            if not dpg.is_item_hovered(\"_primary_window\"):\n                return\n            if not self.need_test:\n                return\n            if self.train_thread:\n                self.cameraPose.change_radius(app_data)\n                self.train_thread.set_camera_pose(self.cameraPose.pose)\n                self.train_thread.test()\n                self.show_cam_radius(self.cameraPose.radius)\n\n        def callback_train():\n            if self.need_train:\n                self.need_train = False\n                self.istesting = True\n                if self.train_thread:\n                    self.train_thread.istraining = False\n                _label = \"continue\" if (self.train_thread != None) else \"start\"\n                dpg.configure_item(\"_button_train\", label=_label)\n            else:\n                dpg.configure_item(\"_button_train\", label=\"pause\")\n                self.need_train = True\n                if self.train_thread:\n                    self.train_thread.istraining = True\n                else:\n                    self.train_thread = TrainThread(\n                        KEY=self.KEY,\n                        args=self.args,\n                        logger=self.logger,\n                        camera_pose=self.cameraPose.pose,\n                        step=self.args.train.iters,\n                        back_color=self.back_color,\n                        ckpt=self.ckpt)\n                    self.train_thread.setDaemon(True)\n                    self.train_thread.start()\n\n        def callback_checkpoint(sender):\n            if sender == \"_button_check_save\":\n                if self.train_thread and self.train_thread.trainer:\n                    self.logger.info(\"saving training state ... \")\n                    ckpt_name = checkpoints.save_checkpoint(\n                        self.args.exp_dir,\n                        self.train_thread.get_state(),\n                        step=self.train_thread.get_currentStep(),\n                        overwrite=True,\n                        keep=self.args.train.keep,\n                    )\n                    dpg.set_value(\n                        \"_log_ckpt\",\n                        \"Checkpoint saved path: {}\".format(ckpt_name))\n                    self.logger.info(\n                        \"training state saved to: {}\".format(ckpt_name))\n                else:\n                    dpg.set_value(\n                        \"_log_ckpt\",\n                        \"Checkpoint save path: failed ,cause no training\")\n                    self.logger.info(\n                        \"saving training state failed ,cause no training\")\n\n        def callback_change_scale(_, new_scale):\n            self.scale = new_scale\n            dpg.set_value(\"_cam_WH\", self._effective_resolution_display)\n            if self.train_thread:\n                self.train_thread.set_scale(self.scale)\n                if self.train_thread.havestart:\n                    self.train_thread.test()\n\n        def callback_reset():\n            self.need_train = False\n            if self.train_thread:\n                self.train_thread.stop()\n                dpg.configure_item(\"_button_train\", label=\"start\")\n                self.train_thread = None\n            self.framebuff = np.tile(\n                np.asarray(self.back_color, dtype=np.float32),\n                (self.texture_H, self.texture_W, 3))\n            self.clear_plot()\n            self.ckpt = CKPT()\n            dpg.set_value(\"_log_ckpt\", \"\")\n\n        def callback_Render():\n            if self.need_test:\n                dpg.configure_item(\"_button_Render\",\n                                   label=\"continue rendering\")\n            else:\n                dpg.configure_item(\"_button_Render\", label=\"pause rendering\")\n            self.need_test = not self.need_test\n\n        def callback_mode(_, app_data):\n            if app_data == \"render\":\n                self.mode = Mode.Render\n            elif app_data == \"depth\":\n                self.mode = Mode.depth\n            elif app_data == \"cost\":\n                self.mode = Mode.Cost\n            else:\n                raise NotImplementedError(\"visualization mode '{}' is not implemented\"\n                                          .format(self.mode))\n            if self.train_thread:\n                self.train_thread.test()\n\n        def callback_loadCheckpoint(_, app_data):\n            file_name = app_data['file_name']\n            file_path_name = app_data['file_path_name'][:-2]\n            dpg.set_value('_log_ckpt',\n                          self.ckpt.parse_ckpt(file_name, file_path_name))\n\n        self.View_W, self.View_H = self.W + self.args.viewport.control_window_width, self.H\n        dpg.create_viewport(title='NeRF',\n                            width=self.View_W,\n                            height=self.View_H,\n                            min_width=250 +\n                            self.args.viewport.control_window_width,\n                            min_height=250,\n                            x_pos=0,\n                            y_pos=0)\n\n        with dpg.window(tag=\"_main_window\", no_scrollbar=True):\n            dpg.set_primary_window(\"_main_window\", True)\n\n            with dpg.file_dialog(directory_selector=False,\n                                 show=False,\n                                 callback=callback_loadCheckpoint,\n                                 tag=\"checkpoint_file_dialog\",\n                                 width=700,\n                                 height=400):\n                dpg.add_file_extension(\".*\")\n                dpg.add_file_extension(\"\",\n                                       color=(150, 255, 150, 255),\n                                       custom_text=\"[Checkpoint]\")\n\n            with dpg.group(horizontal=True):\n                #texture\n                with dpg.group(tag=\"_render_texture\"):\n                    with dpg.texture_registry(show=False):\n                        dpg.add_raw_texture(width=self.W,\n                                            height=self.H,\n                                            default_value=self.framebuff,\n                                            format=dpg.mvFormat_Float_rgb,\n                                            tag=\"_texture\")\n                    with dpg.child_window(tag=\"_primary_window\",\n                                          width=self.W,\n                                          no_scrollbar=True):\n                        dpg.add_image(\"_texture\",\n                                      tag=\"_img\",\n                                      parent=\"_primary_window\",\n                                      width=self.W - 15,\n                                      height=self.H - 32)\n                #control panel\n                with dpg.child_window(tag=\"_control_window\",\n                                      no_scrollbar=True):\n                    with dpg.theme() as theme_head:\n                        with dpg.theme_component(dpg.mvAll):\n                            dpg.add_theme_color(dpg.mvThemeCol_Header,\n                                                (0, 62, 89))\n                    #control\n                    with dpg.collapsing_header(tag=\"_control_panel\",\n                                               label=\"Control Panel\",\n                                               default_open=True):\n                        dpg.bind_item_theme(\"_control_panel\", theme_head)\n                        #mode\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Visualization mode:   \")\n                            items = [\"render\", \"depth\", \"cost\"]\n                            dpg.add_combo(items=items,\n                                          callback=callback_mode,\n                                          width=max(map(len, items)) * 10,\n                                          default_value=\"render\")\n                        # train / stop/reset\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Train: \")\n                            dpg.add_button(label=\"start\",\n                                           tag=\"_button_train\",\n                                           callback=callback_train)\n                            dpg.add_button(label=\"reset\",\n                                           tag=\"_button_reset\",\n                                           callback=callback_reset)\n                        #need render\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Render: \")\n                            dpg.add_button(label=\"pause rendering\",\n                                           tag=\"_button_Render\",\n                                           callback=callback_Render)\n                        # save ckpt\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Checkpoint: \")\n                            dpg.add_button(label=\"save\",\n                                           tag=\"_button_check_save\",\n                                           callback=callback_checkpoint)\n                            dpg.add_button(label=\"load\",\n                                           tag=\"_button_check_load\",\n                                           callback=lambda: dpg.show_item(\n                                               'checkpoint_file_dialog'))\n                        dpg.add_text(\n                            \"\",\n                            tag=\"_log_ckpt\",\n                            wrap=self.args.viewport.control_window_width - 40)\n                        #resolution\n                        dpg.add_text(\"resolution scale:\")\n                        self.scale_slider = dpg.add_slider_float(\n                            tag=\"_resolutionScale\",\n                            label=\"\",\n                            default_value=self.args.viewport.resolution_scale,\n                            clamped=True,\n                            min_value=0.1,\n                            max_value=1.0,\n                            width=self.args.viewport.control_window_width - 40,\n                            format=\"%.1f\",\n                            callback=callback_change_scale,\n                        )\n                        dpg.add_text(\"Background color: \")\n                        dpg.add_color_edit(\n                            tag=\"_BackColor\",\n                            default_value=tuple(\n                                map(lambda val: int(val * 255 + .5),\n                                    self.args.render_eval.bg)),\n                            no_alpha=True,\n                            width=self.args.viewport.control_window_width - 40,\n                            callback=callback_backgroundColor)\n                        with dpg.value_registry():\n                            dpg.add_float_value(default_value=0.0,\n                                                tag=\"float_value\")\n                        #camera\n                        dpg.add_text(\"camera set:\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"near plane\")\n                            dpg.add_input_text(tag=\"_camera_near\",\n                                               width=40,\n                                               default_value=0.1,\n                                               decimal=True)\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"centroid:\")\n                            dpg.add_text(\"x\")\n                            dpg.add_input_text(\n                                tag=\"_centroid_x\",\n                                width=40,\n                                default_value=self.cameraPose.centroid[0],\n                                decimal=True)\n                            dpg.add_text(\"y\")\n                            dpg.add_input_text(\n                                tag=\"_centroid_y\",\n                                width=40,\n                                default_value=self.cameraPose.centroid[1],\n                                decimal=True)\n                            dpg.add_text(\"z\")\n                            dpg.add_input_text(\n                                tag=\"_centroid_z\",\n                                width=40,\n                                default_value=self.cameraPose.centroid[2],\n                                decimal=True)\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"theta\")\n                            dpg.add_input_text(\n                                tag=\"_theta\",\n                                width=40,\n                                default_value=self.cameraPose.theta,\n                                decimal=True)\n                            dpg.add_text(\"phi\")\n                            dpg.add_input_text(\n                                tag=\"_phi\",\n                                width=40,\n                                default_value=self.cameraPose.phi,\n                                decimal=True)\n                            dpg.add_text(\"radius\")\n                            dpg.add_input_text(\n                                tag=\"_radius\",\n                                width=40,\n                                default_value=self.cameraPose.radius,\n                                decimal=True)\n                    with dpg.collapsing_header(tag=\"_para_panel\",\n                                               label=\"Parameter Monitor\",\n                                               default_open=True):\n                        dpg.bind_item_theme(\"_para_panel\", theme_head)\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Resolution(W*H): \")\n                            dpg.add_text(self._effective_resolution_display,\n                                         tag=\"_cam_WH\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Current training step: \")\n                            dpg.add_text(\"no data\", tag=\"_cur_train_step\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Train time: \")\n                            dpg.add_text(\"no data\", tag=\"_log_train_time\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Infer time: \")\n                            dpg.add_text(\"no data\", tag=\"_log_infer_time\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"FPS: \")\n                            dpg.add_text(\"no data\", tag=\"_fps\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Mean samples/ray: \")\n                            dpg.add_text(\"no data\", tag=\"_samples\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Mean effective samples/ray: \")\n                            dpg.add_text(\"no data\", tag=\"_effective_samples\")\n\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Batch size: \")\n                            dpg.add_text(\"no data\",\n                                         tag=\"_not_compacted_batch_size\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Batch size(compacted): \")\n                            dpg.add_text(\"no data\",\n                                         tag=\"_compacted_batch_size\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Number of rays: \")\n                            dpg.add_text(\"no data\", tag=\"_rays_num\")\n                        # create plot\n                        with dpg.plot(\n                                label=\"pixel quality\",\n                                height=self.args.viewport.control_window_width\n                                - 40,\n                                width=self.args.viewport.control_window_width -\n                                40):\n                            # optionally create legend\n                            dpg.add_plot_legend()\n                            # REQUIRED: create x and y axes\n                            dpg.add_plot_axis(dpg.mvXAxis,\n                                              label=\"step\",\n                                              tag=\"x_axis\")\n                            dpg.add_plot_axis(dpg.mvYAxis,\n                                              label=\"PSNR (estimated)\",\n                                              tag=\"y_axis\")\n                            # series belong to a y axis\n                            dpg.add_line_series(self.data_step,\n                                                self.data_pixel_quality,\n                                                label=\"~PSNR\",\n                                                parent=\"y_axis\",\n                                                tag=\"_plot\")\n                    with dpg.collapsing_header(tag=\"_tip_panel\",\n                                               label=\"Tips\",\n                                               default_open=True):\n                        dpg.bind_item_theme(\"_tip_panel\", theme_head)\n                        tip1 = \"* Drag the left mouse button to rotate the camera\\n\"\n                        tip2 = \"* The mouse wheel zooms the distance between the camera and the object\\n\"\n                        tip3 = \"* Drag the window to resize\\n\"\n                        tip4 = \"* Drag the middle mouse button to translate the camera\\n\"\n                        dpg.add_text(\n                            tip1,\n                            wrap=self.args.viewport.control_window_width - 40)\n                        dpg.add_text(\n                            tip2,\n                            wrap=self.args.viewport.control_window_width - 40)\n                        dpg.add_text(\n                            tip3,\n                            wrap=self.args.viewport.control_window_width - 40)\n                        dpg.add_text(\n                            tip4,\n                            wrap=self.args.viewport.control_window_width - 40)\n\n        def callback_key(_, appdata):\n            if appdata == dpg.mvKey_Q:\n                self.exit_flag = True\n\n        #IO\n        with dpg.handler_registry():\n            dpg.add_mouse_drag_handler(button=dpg.mvMouseButton_Left,\n                                       callback=callback_mouseDrag)\n            dpg.add_mouse_release_handler(button=dpg.mvMouseButton_Left,\n                                          callback=callback_mouseRelease)\n            dpg.add_mouse_down_handler(button=dpg.mvMouseButton_Left,\n                                       callback=callback_mouseDown)\n            dpg.add_mouse_wheel_handler(callback=callback_mouseWheel)\n            #mouse middle\n            dpg.add_mouse_drag_handler(button=dpg.mvMouseButton_Middle,\n                                       callback=callback_midmouseDrag)\n            dpg.add_mouse_down_handler(button=dpg.mvMouseButton_Middle,\n                                       callback=callback_mouseDown)\n            dpg.add_mouse_release_handler(button=dpg.mvMouseButton_Middle,\n                                          callback=callback_mouseRelease)\n\n            dpg.add_key_release_handler(callback=callback_key)\n\n        dpg.setup_dearpygui()\n        dpg.show_viewport()\n\n    def update_frame(self):\n        self.framebuff = self.train_thread.framebuff\n        dpg.set_value(\"_texture\", self.framebuff)\n\n    def adapt_size(self):\n        if self.View_H != dpg.get_viewport_height(\n        ) or self.View_W != dpg.get_viewport_width():\n            self.View_H = dpg.get_viewport_height()\n            self.View_W = dpg.get_viewport_width()\n            self.H, self.W = self.View_H, self.View_W - self.args.viewport.control_window_width\n            dpg.set_item_width(\"_primary_window\", self.W)\n            dpg.delete_item(\"_img\")\n            dpg.add_image(\"_texture\",\n                          tag=\"_img\",\n                          parent=\"_primary_window\",\n                          width=self.W - 15,\n                          height=self.H - 32)\n            dpg.configure_item(\"_control_panel\",\n                               label=\"Control Panel\",\n                               default_open=True)\n            dpg.configure_item(\"_para_panel\",\n                               label=\"Parameter Monitor\",\n                               default_open=True)\n            dpg.configure_item(\"_tip_panel\", label=\"Tips\", default_open=True)\n            dpg.set_value(\"_cam_WH\", self._effective_resolution_display)\n            if self.train_thread:\n                self.train_thread.test()\n\n    def setFrameColor(self):\n        if self.train_thread:\n            self.train_thread.setBackColor(self.back_color)\n        if self.train_thread and self.train_thread.havestart:\n            self.train_thread.test()\n        else:\n            self.framebuff = np.tile(\n                np.asarray(self.back_color, dtype=np.float32),\n                (self.texture_H, self.texture_W, 1))\n        dpg.set_value(\"_texture\", self.framebuff)\n\n    def clear_plot(self):\n        self.data_step.clear()\n        self.data_pixel_quality.clear()\n        self.update_plot()\n\n    def update_plot(self):\n        if len(self.data_pixel_quality\n               ) > self.args.viewport.max_show_loss_step:\n            self.data_pixel_quality = self.data_pixel_quality[\n                -self.args.viewport.max_show_loss_step - 1:]\n            self.data_step = self.data_step[\n                -self.args.viewport.max_show_loss_step - 1:]\n        dpg.set_value('_plot', [self.data_step, self.data_pixel_quality])\n        dpg.fit_axis_data(\"y_axis\")\n        dpg.fit_axis_data(\"x_axis\")\n\n    def set_cam_angle(self):\n        try:\n            theta = float(dpg.get_value(\"_theta\"))\n            phi = float(dpg.get_value(\"_phi\"))\n            radius = float(dpg.get_value(\"_radius\"))\n            if theta != self.cameraPose.theta or phi != self.cameraPose.phi or radius != self.cameraPose.radius:\n                self.cameraPose.theta = theta\n                self.cameraPose.phi = phi\n                self.cameraPose.radius = radius\n                self.train_thread.set_camera_pose(self.cameraPose.pose)\n                self.train_thread.test()\n        except BaseException as e:\n            self.logger.error(e)\n\n    def show_cam_angle(self, _theta, _phi):\n        _theta = float('{:.3f}'.format(_theta))\n        _phi = float('{:.3f}'.format(_phi))\n        try:\n            theta = float(dpg.get_value(\"_theta\"))\n            phi = float(dpg.get_value(\"_phi\"))\n            if theta != _theta or phi != _phi:\n                dpg.set_value(\"_theta\", _theta)\n                dpg.set_value(\"_phi\", _phi)\n        except BaseException as e:\n            self.logger.error(e)\n\n    def show_cam_radius(self, _radius):\n        _radius = float('{:.3f}'.format(_radius))\n        try:\n            radius = float(dpg.get_value(\"_radius\"))\n            if radius != _radius:\n                dpg.set_value(\"_radius\", _radius)\n        except BaseException as e:\n            self.logger.error(e)\n\n    def set_cam_centroid(self):\n        try:\n            x = float(dpg.get_value(\"_centroid_x\"))\n            y = float(dpg.get_value(\"_centroid_y\"))\n            z = float(dpg.get_value(\"_centroid_z\"))\n            if x != self.cameraPose.centroid[\n                    0] or y != self.cameraPose.centroid[\n                        1] or z != self.cameraPose.centroid[2]:\n                self.cameraPose.centroid[0] = x\n                self.cameraPose.centroid[1] = y\n                self.cameraPose.centroid[2] = z\n                self.train_thread.set_camera_pose(self.cameraPose.pose)\n                self.train_thread.test()\n        except BaseException as e:\n            self.logger.error(e)\n\n    def set_cam_near(self):\n        try:\n            cam_near = float(dpg.get_value(\"_camera_near\"))\n            camera = self.train_thread.getPinholeCam()\n            if camera and camera.near != cam_near:\n                self.train_thread.setCamNear(cam_near)\n                self.train_thread.test()\n        except BaseException as e:\n            self.logger.exception(e)\n\n    def show_cam_centroid(self, _x, _y, _z):\n        _x = float('{:.3f}'.format(_x))\n        _y = float('{:.3f}'.format(_y))\n        _z = float('{:.3f}'.format(_z))\n        try:\n            x = float(dpg.get_value(\"_centroid_x\"))\n            y = float(dpg.get_value(\"_centroid_y\"))\n            z = float(dpg.get_value(\"_centroid_z\"))\n            if x != _x or y != _y or z != _z:\n                dpg.set_value(\"_centroid_x\", _x)\n                dpg.set_value(\"_centroid_y\", _y)\n                dpg.set_value(\"_centroid_z\", _z)\n        except BaseException as e:\n            self.logger.error(e)\n\n    def update_panel(self):\n        dpg.set_value(\n            \"_cur_train_step\",\n            \"{} (+{}/{})\".format(self.train_thread.get_currentStep(),\n                                 self.train_thread.get_logStep(),\n                                 self.train_thread.step))\n        dpg.set_value(\"_log_train_time\",\n                      \"{}\".format(self.train_thread.get_TrainInferTime()))\n        dpg.set_value(\"_log_infer_time\",\n                      \"{}\".format(self.train_thread.get_RenderInferTime()))\n        dpg.set_value(\"_fps\", \"{}\".format(self.train_thread.get_Fps()))\n        dpg.set_value(\"_samples\",\n                      \"{}\".format(self.train_thread.get_samples_nums()))\n        dpg.set_value(\n            \"_effective_samples\",\n            \"{}\".format(self.train_thread.get_effective_samples_nums()))\n\n        dpg.set_value(\"_compacted_batch_size\",\n                      \"{}\".format(self.train_thread.get_compactedBatch()))\n        dpg.set_value(\"_not_compacted_batch_size\",\n                      \"{}\".format(self.train_thread.get_notCompactedBatch()))\n        dpg.set_value(\"_rays_num\",\n                      \"{}\".format(self.train_thread.get_raysNum()))\n        self.data_step, self.data_pixel_quality = self.train_thread.get_plotData(\n        )\n        self.update_plot()\n\n    def load_ckpt(self):\n        if self.train_thread and self.train_thread.havestart:\n            self.train_thread.trainer.load_checkpoint(self.ckpt.ckpt_file_path,\n                                                      self.ckpt.step)\n            self.clear_plot()\n            self.ckpt.need_load_ckpt = False\n\n    def render(self) -> int:\n        while dpg.is_dearpygui_running():\n            self.adapt_size()\n            if self.train_thread:\n                if self.ckpt.need_load_ckpt:\n                    self.load_ckpt()\n                if not self.mouse_pressed:\n                    self.set_cam_angle()\n                    self.set_cam_centroid()\n                    self.set_cam_near()\n                self.train_thread.setMode(self.mode)\n                self.train_thread.set_scale(self.scale)\n                self.train_thread.change_WH(self.W, self.H)\n                self.update_panel()\n                if self.need_test:\n                    self.train_thread.needtesting = True\n                    if self.train_thread.canUpdate():\n                        self.update_frame()\n                        self.train_thread.finishUpdate()\n                else:\n                    self.train_thread.needtesting = False\n            else:\n                dpg.set_value(\"_texture\", self.framebuff)\n            dpg.render_dearpygui_frame()\n            time.sleep(.01 if self.train_thread and self.train_thread.istraining else .1)\n            if self.exit_flag:\n                if self.train_thread:\n                    self.train_thread.stop()\n                while self.train_thread and self.train_thread.is_alive():\n                    pass\n                self.logger.debug(\"thread killed successfully\")\n                self.logger.info(\"exiting cleanly ...\")\n                break\n        dpg.destroy_context()\n        return 0", "@dataclass\nclass NeRFGUI():\n    framebuff: Any = field(init=False)\n    H: int = field(init=False)\n    W: int = field(init=False)\n\n    need_train: bool = False\n    istesting: bool = False\n    train_thread: TrainThread = field(init=False)\n\n    args: NeRFGUIArgs = None\n\n    KEY: jran.KeyArray = None\n    logger: logging.Logger = None\n    cameraPose: CameraPose = CameraPose()\n    cameraPosePrev: CameraPose = CameraPose()\n    cameraPoseNext: CameraPose = CameraPose()\n    scale_slider: Union[int, str] = field(init=False)\n    back_color: RGBColor = field(init=False)\n    scale: float = field(init=False)\n    data_step: List[int] = field(default_factory=list, init=False)\n    data_pixel_quality: List[float] = field(default_factory=list, init=False)\n\n    texture_H: int = field(init=False)\n    texture_W: int = field(init=False)\n    View_H: int = field(init=False)\n    View_W: int = field(init=False)\n\n    exit_flag: bool = False\n    mode: Mode = Mode.Render\n\n    mouse_pressed: bool = False\n    need_test: bool = True\n\n    #ckpt\n    ckpt: CKPT = CKPT()\n\n    @property\n    def _effective_resolution_display(self) -> str:\n        return \"{}x{}\".format(\n            *map(lambda val: int(val * self.scale), (self.W, self.H)))\n\n    def __post_init__(self):\n        self.H, self.W = self.args.viewport.H, self.args.viewport.W\n        self.back_color = self.args.render_eval.bg\n        self.scale = self.args.viewport.resolution_scale\n        self.texture_H, self.texture_W = self.H, self.W\n        self.framebuff = np.tile(np.asarray(self.back_color, dtype=np.float32),\n                                 (self.H, self.W, 3))\n        radius_init = 4.\n        self.cameraPose, self.cameraPosePrev, self.cameraPoseNext = (\n            CameraPose(radius=radius_init),\n            CameraPose(radius=radius_init),\n            CameraPose(radius=radius_init),\n        )\n        dpg.create_context()\n        self.train_thread = None\n        self.ItemsLayout()\n\n    def ItemsLayout(self):\n\n        def callback_backgroundColor():\n            self.back_color = tuple(\n                map(lambda val: val / 255,\n                    dpg.get_value(\"_BackColor\")[:3]))\n            self.setFrameColor()\n\n        def callback_mouseDrag(_, app_data):\n            if not dpg.is_item_focused(\"_primary_window\"):\n                return\n            if not self.need_test:\n                return\n            dx = app_data[1]\n            dy = app_data[2]\n            self.cameraPoseNext = deepcopy(self.cameraPosePrev)\n            self.cameraPoseNext.move(dx, dy)\n            if self.train_thread:\n                self.train_thread.set_camera_pose(self.cameraPoseNext.pose)\n                self.train_thread.test()\n                self.show_cam_angle(self.cameraPoseNext.theta,\n                                    self.cameraPoseNext.phi)\n\n        def callback_midmouseDrag(_, app_data):\n            if not self.need_test:\n                return\n            dx = app_data[1]\n            dy = app_data[2]\n            self.cameraPoseNext = deepcopy(self.cameraPosePrev)\n            self.cameraPoseNext.trans(dx, dy)\n            if self.train_thread:\n                self.train_thread.set_camera_pose(self.cameraPoseNext.pose)\n                self.train_thread.test()\n                self.show_cam_centroid(self.cameraPoseNext.centroid[0],\n                                       self.cameraPoseNext.centroid[1],\n                                       self.cameraPoseNext.centroid[2])\n\n        def callback_mouseDown(_, app_data):\n            if not dpg.is_item_hovered(\"_primary_window\"):\n                return\n            if not self.need_test:\n                return\n            self.mouse_pressed = True\n            if app_data[1] < 1e-5:\n                self.cameraPosePrev = self.cameraPose\n            if self.train_thread:\n                self.train_thread.setStep(1)\n\n        def callback_mouseRelease():\n            if not self.need_test:\n                return\n            self.mouse_pressed = False\n            self.cameraPose = self.cameraPoseNext\n            if self.train_thread:\n                self.train_thread.setStep(self.args.train.iters)\n\n        def callback_mouseWheel(_, app_data):\n            if not dpg.is_item_hovered(\"_primary_window\"):\n                return\n            if not self.need_test:\n                return\n            if self.train_thread:\n                self.cameraPose.change_radius(app_data)\n                self.train_thread.set_camera_pose(self.cameraPose.pose)\n                self.train_thread.test()\n                self.show_cam_radius(self.cameraPose.radius)\n\n        def callback_train():\n            if self.need_train:\n                self.need_train = False\n                self.istesting = True\n                if self.train_thread:\n                    self.train_thread.istraining = False\n                _label = \"continue\" if (self.train_thread != None) else \"start\"\n                dpg.configure_item(\"_button_train\", label=_label)\n            else:\n                dpg.configure_item(\"_button_train\", label=\"pause\")\n                self.need_train = True\n                if self.train_thread:\n                    self.train_thread.istraining = True\n                else:\n                    self.train_thread = TrainThread(\n                        KEY=self.KEY,\n                        args=self.args,\n                        logger=self.logger,\n                        camera_pose=self.cameraPose.pose,\n                        step=self.args.train.iters,\n                        back_color=self.back_color,\n                        ckpt=self.ckpt)\n                    self.train_thread.setDaemon(True)\n                    self.train_thread.start()\n\n        def callback_checkpoint(sender):\n            if sender == \"_button_check_save\":\n                if self.train_thread and self.train_thread.trainer:\n                    self.logger.info(\"saving training state ... \")\n                    ckpt_name = checkpoints.save_checkpoint(\n                        self.args.exp_dir,\n                        self.train_thread.get_state(),\n                        step=self.train_thread.get_currentStep(),\n                        overwrite=True,\n                        keep=self.args.train.keep,\n                    )\n                    dpg.set_value(\n                        \"_log_ckpt\",\n                        \"Checkpoint saved path: {}\".format(ckpt_name))\n                    self.logger.info(\n                        \"training state saved to: {}\".format(ckpt_name))\n                else:\n                    dpg.set_value(\n                        \"_log_ckpt\",\n                        \"Checkpoint save path: failed ,cause no training\")\n                    self.logger.info(\n                        \"saving training state failed ,cause no training\")\n\n        def callback_change_scale(_, new_scale):\n            self.scale = new_scale\n            dpg.set_value(\"_cam_WH\", self._effective_resolution_display)\n            if self.train_thread:\n                self.train_thread.set_scale(self.scale)\n                if self.train_thread.havestart:\n                    self.train_thread.test()\n\n        def callback_reset():\n            self.need_train = False\n            if self.train_thread:\n                self.train_thread.stop()\n                dpg.configure_item(\"_button_train\", label=\"start\")\n                self.train_thread = None\n            self.framebuff = np.tile(\n                np.asarray(self.back_color, dtype=np.float32),\n                (self.texture_H, self.texture_W, 3))\n            self.clear_plot()\n            self.ckpt = CKPT()\n            dpg.set_value(\"_log_ckpt\", \"\")\n\n        def callback_Render():\n            if self.need_test:\n                dpg.configure_item(\"_button_Render\",\n                                   label=\"continue rendering\")\n            else:\n                dpg.configure_item(\"_button_Render\", label=\"pause rendering\")\n            self.need_test = not self.need_test\n\n        def callback_mode(_, app_data):\n            if app_data == \"render\":\n                self.mode = Mode.Render\n            elif app_data == \"depth\":\n                self.mode = Mode.depth\n            elif app_data == \"cost\":\n                self.mode = Mode.Cost\n            else:\n                raise NotImplementedError(\"visualization mode '{}' is not implemented\"\n                                          .format(self.mode))\n            if self.train_thread:\n                self.train_thread.test()\n\n        def callback_loadCheckpoint(_, app_data):\n            file_name = app_data['file_name']\n            file_path_name = app_data['file_path_name'][:-2]\n            dpg.set_value('_log_ckpt',\n                          self.ckpt.parse_ckpt(file_name, file_path_name))\n\n        self.View_W, self.View_H = self.W + self.args.viewport.control_window_width, self.H\n        dpg.create_viewport(title='NeRF',\n                            width=self.View_W,\n                            height=self.View_H,\n                            min_width=250 +\n                            self.args.viewport.control_window_width,\n                            min_height=250,\n                            x_pos=0,\n                            y_pos=0)\n\n        with dpg.window(tag=\"_main_window\", no_scrollbar=True):\n            dpg.set_primary_window(\"_main_window\", True)\n\n            with dpg.file_dialog(directory_selector=False,\n                                 show=False,\n                                 callback=callback_loadCheckpoint,\n                                 tag=\"checkpoint_file_dialog\",\n                                 width=700,\n                                 height=400):\n                dpg.add_file_extension(\".*\")\n                dpg.add_file_extension(\"\",\n                                       color=(150, 255, 150, 255),\n                                       custom_text=\"[Checkpoint]\")\n\n            with dpg.group(horizontal=True):\n                #texture\n                with dpg.group(tag=\"_render_texture\"):\n                    with dpg.texture_registry(show=False):\n                        dpg.add_raw_texture(width=self.W,\n                                            height=self.H,\n                                            default_value=self.framebuff,\n                                            format=dpg.mvFormat_Float_rgb,\n                                            tag=\"_texture\")\n                    with dpg.child_window(tag=\"_primary_window\",\n                                          width=self.W,\n                                          no_scrollbar=True):\n                        dpg.add_image(\"_texture\",\n                                      tag=\"_img\",\n                                      parent=\"_primary_window\",\n                                      width=self.W - 15,\n                                      height=self.H - 32)\n                #control panel\n                with dpg.child_window(tag=\"_control_window\",\n                                      no_scrollbar=True):\n                    with dpg.theme() as theme_head:\n                        with dpg.theme_component(dpg.mvAll):\n                            dpg.add_theme_color(dpg.mvThemeCol_Header,\n                                                (0, 62, 89))\n                    #control\n                    with dpg.collapsing_header(tag=\"_control_panel\",\n                                               label=\"Control Panel\",\n                                               default_open=True):\n                        dpg.bind_item_theme(\"_control_panel\", theme_head)\n                        #mode\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Visualization mode:   \")\n                            items = [\"render\", \"depth\", \"cost\"]\n                            dpg.add_combo(items=items,\n                                          callback=callback_mode,\n                                          width=max(map(len, items)) * 10,\n                                          default_value=\"render\")\n                        # train / stop/reset\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Train: \")\n                            dpg.add_button(label=\"start\",\n                                           tag=\"_button_train\",\n                                           callback=callback_train)\n                            dpg.add_button(label=\"reset\",\n                                           tag=\"_button_reset\",\n                                           callback=callback_reset)\n                        #need render\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Render: \")\n                            dpg.add_button(label=\"pause rendering\",\n                                           tag=\"_button_Render\",\n                                           callback=callback_Render)\n                        # save ckpt\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Checkpoint: \")\n                            dpg.add_button(label=\"save\",\n                                           tag=\"_button_check_save\",\n                                           callback=callback_checkpoint)\n                            dpg.add_button(label=\"load\",\n                                           tag=\"_button_check_load\",\n                                           callback=lambda: dpg.show_item(\n                                               'checkpoint_file_dialog'))\n                        dpg.add_text(\n                            \"\",\n                            tag=\"_log_ckpt\",\n                            wrap=self.args.viewport.control_window_width - 40)\n                        #resolution\n                        dpg.add_text(\"resolution scale:\")\n                        self.scale_slider = dpg.add_slider_float(\n                            tag=\"_resolutionScale\",\n                            label=\"\",\n                            default_value=self.args.viewport.resolution_scale,\n                            clamped=True,\n                            min_value=0.1,\n                            max_value=1.0,\n                            width=self.args.viewport.control_window_width - 40,\n                            format=\"%.1f\",\n                            callback=callback_change_scale,\n                        )\n                        dpg.add_text(\"Background color: \")\n                        dpg.add_color_edit(\n                            tag=\"_BackColor\",\n                            default_value=tuple(\n                                map(lambda val: int(val * 255 + .5),\n                                    self.args.render_eval.bg)),\n                            no_alpha=True,\n                            width=self.args.viewport.control_window_width - 40,\n                            callback=callback_backgroundColor)\n                        with dpg.value_registry():\n                            dpg.add_float_value(default_value=0.0,\n                                                tag=\"float_value\")\n                        #camera\n                        dpg.add_text(\"camera set:\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"near plane\")\n                            dpg.add_input_text(tag=\"_camera_near\",\n                                               width=40,\n                                               default_value=0.1,\n                                               decimal=True)\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"centroid:\")\n                            dpg.add_text(\"x\")\n                            dpg.add_input_text(\n                                tag=\"_centroid_x\",\n                                width=40,\n                                default_value=self.cameraPose.centroid[0],\n                                decimal=True)\n                            dpg.add_text(\"y\")\n                            dpg.add_input_text(\n                                tag=\"_centroid_y\",\n                                width=40,\n                                default_value=self.cameraPose.centroid[1],\n                                decimal=True)\n                            dpg.add_text(\"z\")\n                            dpg.add_input_text(\n                                tag=\"_centroid_z\",\n                                width=40,\n                                default_value=self.cameraPose.centroid[2],\n                                decimal=True)\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"theta\")\n                            dpg.add_input_text(\n                                tag=\"_theta\",\n                                width=40,\n                                default_value=self.cameraPose.theta,\n                                decimal=True)\n                            dpg.add_text(\"phi\")\n                            dpg.add_input_text(\n                                tag=\"_phi\",\n                                width=40,\n                                default_value=self.cameraPose.phi,\n                                decimal=True)\n                            dpg.add_text(\"radius\")\n                            dpg.add_input_text(\n                                tag=\"_radius\",\n                                width=40,\n                                default_value=self.cameraPose.radius,\n                                decimal=True)\n                    with dpg.collapsing_header(tag=\"_para_panel\",\n                                               label=\"Parameter Monitor\",\n                                               default_open=True):\n                        dpg.bind_item_theme(\"_para_panel\", theme_head)\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Resolution(W*H): \")\n                            dpg.add_text(self._effective_resolution_display,\n                                         tag=\"_cam_WH\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Current training step: \")\n                            dpg.add_text(\"no data\", tag=\"_cur_train_step\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Train time: \")\n                            dpg.add_text(\"no data\", tag=\"_log_train_time\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Infer time: \")\n                            dpg.add_text(\"no data\", tag=\"_log_infer_time\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"FPS: \")\n                            dpg.add_text(\"no data\", tag=\"_fps\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Mean samples/ray: \")\n                            dpg.add_text(\"no data\", tag=\"_samples\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Mean effective samples/ray: \")\n                            dpg.add_text(\"no data\", tag=\"_effective_samples\")\n\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Batch size: \")\n                            dpg.add_text(\"no data\",\n                                         tag=\"_not_compacted_batch_size\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Batch size(compacted): \")\n                            dpg.add_text(\"no data\",\n                                         tag=\"_compacted_batch_size\")\n                        with dpg.group(horizontal=True):\n                            dpg.add_text(\"Number of rays: \")\n                            dpg.add_text(\"no data\", tag=\"_rays_num\")\n                        # create plot\n                        with dpg.plot(\n                                label=\"pixel quality\",\n                                height=self.args.viewport.control_window_width\n                                - 40,\n                                width=self.args.viewport.control_window_width -\n                                40):\n                            # optionally create legend\n                            dpg.add_plot_legend()\n                            # REQUIRED: create x and y axes\n                            dpg.add_plot_axis(dpg.mvXAxis,\n                                              label=\"step\",\n                                              tag=\"x_axis\")\n                            dpg.add_plot_axis(dpg.mvYAxis,\n                                              label=\"PSNR (estimated)\",\n                                              tag=\"y_axis\")\n                            # series belong to a y axis\n                            dpg.add_line_series(self.data_step,\n                                                self.data_pixel_quality,\n                                                label=\"~PSNR\",\n                                                parent=\"y_axis\",\n                                                tag=\"_plot\")\n                    with dpg.collapsing_header(tag=\"_tip_panel\",\n                                               label=\"Tips\",\n                                               default_open=True):\n                        dpg.bind_item_theme(\"_tip_panel\", theme_head)\n                        tip1 = \"* Drag the left mouse button to rotate the camera\\n\"\n                        tip2 = \"* The mouse wheel zooms the distance between the camera and the object\\n\"\n                        tip3 = \"* Drag the window to resize\\n\"\n                        tip4 = \"* Drag the middle mouse button to translate the camera\\n\"\n                        dpg.add_text(\n                            tip1,\n                            wrap=self.args.viewport.control_window_width - 40)\n                        dpg.add_text(\n                            tip2,\n                            wrap=self.args.viewport.control_window_width - 40)\n                        dpg.add_text(\n                            tip3,\n                            wrap=self.args.viewport.control_window_width - 40)\n                        dpg.add_text(\n                            tip4,\n                            wrap=self.args.viewport.control_window_width - 40)\n\n        def callback_key(_, appdata):\n            if appdata == dpg.mvKey_Q:\n                self.exit_flag = True\n\n        #IO\n        with dpg.handler_registry():\n            dpg.add_mouse_drag_handler(button=dpg.mvMouseButton_Left,\n                                       callback=callback_mouseDrag)\n            dpg.add_mouse_release_handler(button=dpg.mvMouseButton_Left,\n                                          callback=callback_mouseRelease)\n            dpg.add_mouse_down_handler(button=dpg.mvMouseButton_Left,\n                                       callback=callback_mouseDown)\n            dpg.add_mouse_wheel_handler(callback=callback_mouseWheel)\n            #mouse middle\n            dpg.add_mouse_drag_handler(button=dpg.mvMouseButton_Middle,\n                                       callback=callback_midmouseDrag)\n            dpg.add_mouse_down_handler(button=dpg.mvMouseButton_Middle,\n                                       callback=callback_mouseDown)\n            dpg.add_mouse_release_handler(button=dpg.mvMouseButton_Middle,\n                                          callback=callback_mouseRelease)\n\n            dpg.add_key_release_handler(callback=callback_key)\n\n        dpg.setup_dearpygui()\n        dpg.show_viewport()\n\n    def update_frame(self):\n        self.framebuff = self.train_thread.framebuff\n        dpg.set_value(\"_texture\", self.framebuff)\n\n    def adapt_size(self):\n        if self.View_H != dpg.get_viewport_height(\n        ) or self.View_W != dpg.get_viewport_width():\n            self.View_H = dpg.get_viewport_height()\n            self.View_W = dpg.get_viewport_width()\n            self.H, self.W = self.View_H, self.View_W - self.args.viewport.control_window_width\n            dpg.set_item_width(\"_primary_window\", self.W)\n            dpg.delete_item(\"_img\")\n            dpg.add_image(\"_texture\",\n                          tag=\"_img\",\n                          parent=\"_primary_window\",\n                          width=self.W - 15,\n                          height=self.H - 32)\n            dpg.configure_item(\"_control_panel\",\n                               label=\"Control Panel\",\n                               default_open=True)\n            dpg.configure_item(\"_para_panel\",\n                               label=\"Parameter Monitor\",\n                               default_open=True)\n            dpg.configure_item(\"_tip_panel\", label=\"Tips\", default_open=True)\n            dpg.set_value(\"_cam_WH\", self._effective_resolution_display)\n            if self.train_thread:\n                self.train_thread.test()\n\n    def setFrameColor(self):\n        if self.train_thread:\n            self.train_thread.setBackColor(self.back_color)\n        if self.train_thread and self.train_thread.havestart:\n            self.train_thread.test()\n        else:\n            self.framebuff = np.tile(\n                np.asarray(self.back_color, dtype=np.float32),\n                (self.texture_H, self.texture_W, 1))\n        dpg.set_value(\"_texture\", self.framebuff)\n\n    def clear_plot(self):\n        self.data_step.clear()\n        self.data_pixel_quality.clear()\n        self.update_plot()\n\n    def update_plot(self):\n        if len(self.data_pixel_quality\n               ) > self.args.viewport.max_show_loss_step:\n            self.data_pixel_quality = self.data_pixel_quality[\n                -self.args.viewport.max_show_loss_step - 1:]\n            self.data_step = self.data_step[\n                -self.args.viewport.max_show_loss_step - 1:]\n        dpg.set_value('_plot', [self.data_step, self.data_pixel_quality])\n        dpg.fit_axis_data(\"y_axis\")\n        dpg.fit_axis_data(\"x_axis\")\n\n    def set_cam_angle(self):\n        try:\n            theta = float(dpg.get_value(\"_theta\"))\n            phi = float(dpg.get_value(\"_phi\"))\n            radius = float(dpg.get_value(\"_radius\"))\n            if theta != self.cameraPose.theta or phi != self.cameraPose.phi or radius != self.cameraPose.radius:\n                self.cameraPose.theta = theta\n                self.cameraPose.phi = phi\n                self.cameraPose.radius = radius\n                self.train_thread.set_camera_pose(self.cameraPose.pose)\n                self.train_thread.test()\n        except BaseException as e:\n            self.logger.error(e)\n\n    def show_cam_angle(self, _theta, _phi):\n        _theta = float('{:.3f}'.format(_theta))\n        _phi = float('{:.3f}'.format(_phi))\n        try:\n            theta = float(dpg.get_value(\"_theta\"))\n            phi = float(dpg.get_value(\"_phi\"))\n            if theta != _theta or phi != _phi:\n                dpg.set_value(\"_theta\", _theta)\n                dpg.set_value(\"_phi\", _phi)\n        except BaseException as e:\n            self.logger.error(e)\n\n    def show_cam_radius(self, _radius):\n        _radius = float('{:.3f}'.format(_radius))\n        try:\n            radius = float(dpg.get_value(\"_radius\"))\n            if radius != _radius:\n                dpg.set_value(\"_radius\", _radius)\n        except BaseException as e:\n            self.logger.error(e)\n\n    def set_cam_centroid(self):\n        try:\n            x = float(dpg.get_value(\"_centroid_x\"))\n            y = float(dpg.get_value(\"_centroid_y\"))\n            z = float(dpg.get_value(\"_centroid_z\"))\n            if x != self.cameraPose.centroid[\n                    0] or y != self.cameraPose.centroid[\n                        1] or z != self.cameraPose.centroid[2]:\n                self.cameraPose.centroid[0] = x\n                self.cameraPose.centroid[1] = y\n                self.cameraPose.centroid[2] = z\n                self.train_thread.set_camera_pose(self.cameraPose.pose)\n                self.train_thread.test()\n        except BaseException as e:\n            self.logger.error(e)\n\n    def set_cam_near(self):\n        try:\n            cam_near = float(dpg.get_value(\"_camera_near\"))\n            camera = self.train_thread.getPinholeCam()\n            if camera and camera.near != cam_near:\n                self.train_thread.setCamNear(cam_near)\n                self.train_thread.test()\n        except BaseException as e:\n            self.logger.exception(e)\n\n    def show_cam_centroid(self, _x, _y, _z):\n        _x = float('{:.3f}'.format(_x))\n        _y = float('{:.3f}'.format(_y))\n        _z = float('{:.3f}'.format(_z))\n        try:\n            x = float(dpg.get_value(\"_centroid_x\"))\n            y = float(dpg.get_value(\"_centroid_y\"))\n            z = float(dpg.get_value(\"_centroid_z\"))\n            if x != _x or y != _y or z != _z:\n                dpg.set_value(\"_centroid_x\", _x)\n                dpg.set_value(\"_centroid_y\", _y)\n                dpg.set_value(\"_centroid_z\", _z)\n        except BaseException as e:\n            self.logger.error(e)\n\n    def update_panel(self):\n        dpg.set_value(\n            \"_cur_train_step\",\n            \"{} (+{}/{})\".format(self.train_thread.get_currentStep(),\n                                 self.train_thread.get_logStep(),\n                                 self.train_thread.step))\n        dpg.set_value(\"_log_train_time\",\n                      \"{}\".format(self.train_thread.get_TrainInferTime()))\n        dpg.set_value(\"_log_infer_time\",\n                      \"{}\".format(self.train_thread.get_RenderInferTime()))\n        dpg.set_value(\"_fps\", \"{}\".format(self.train_thread.get_Fps()))\n        dpg.set_value(\"_samples\",\n                      \"{}\".format(self.train_thread.get_samples_nums()))\n        dpg.set_value(\n            \"_effective_samples\",\n            \"{}\".format(self.train_thread.get_effective_samples_nums()))\n\n        dpg.set_value(\"_compacted_batch_size\",\n                      \"{}\".format(self.train_thread.get_compactedBatch()))\n        dpg.set_value(\"_not_compacted_batch_size\",\n                      \"{}\".format(self.train_thread.get_notCompactedBatch()))\n        dpg.set_value(\"_rays_num\",\n                      \"{}\".format(self.train_thread.get_raysNum()))\n        self.data_step, self.data_pixel_quality = self.train_thread.get_plotData(\n        )\n        self.update_plot()\n\n    def load_ckpt(self):\n        if self.train_thread and self.train_thread.havestart:\n            self.train_thread.trainer.load_checkpoint(self.ckpt.ckpt_file_path,\n                                                      self.ckpt.step)\n            self.clear_plot()\n            self.ckpt.need_load_ckpt = False\n\n    def render(self) -> int:\n        while dpg.is_dearpygui_running():\n            self.adapt_size()\n            if self.train_thread:\n                if self.ckpt.need_load_ckpt:\n                    self.load_ckpt()\n                if not self.mouse_pressed:\n                    self.set_cam_angle()\n                    self.set_cam_centroid()\n                    self.set_cam_near()\n                self.train_thread.setMode(self.mode)\n                self.train_thread.set_scale(self.scale)\n                self.train_thread.change_WH(self.W, self.H)\n                self.update_panel()\n                if self.need_test:\n                    self.train_thread.needtesting = True\n                    if self.train_thread.canUpdate():\n                        self.update_frame()\n                        self.train_thread.finishUpdate()\n                else:\n                    self.train_thread.needtesting = False\n            else:\n                dpg.set_value(\"_texture\", self.framebuff)\n            dpg.render_dearpygui_frame()\n            time.sleep(.01 if self.train_thread and self.train_thread.istraining else .1)\n            if self.exit_flag:\n                if self.train_thread:\n                    self.train_thread.stop()\n                while self.train_thread and self.train_thread.is_alive():\n                    pass\n                self.logger.debug(\"thread killed successfully\")\n                self.logger.info(\"exiting cleanly ...\")\n                break\n        dpg.destroy_context()\n        return 0", "\n\ndef gui_exit():\n    import sys\n    sys.exit()\n\n\ndef GuiWindow(KEY: jran.KeyArray, args: NeRFGUIArgs, logger: logging.Logger) -> int:\n    nerfGui = NeRFGUI(args=args, KEY=KEY, logger=logger)\n    return nerfGui.render()", ""]}
{"filename": "app/nerf/_utils.py", "chunked_list": ["from typing import Dict, Tuple\n\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jran\nimport optax\n\nfrom models.renderers import render_rays_train\nfrom utils import common, data\nfrom utils.types import NeRFState, SceneData", "from utils import common, data\nfrom utils.types import NeRFState, SceneData\n\n\n__all__ = [\n    \"make_optimizer\",\n    \"train_step\",\n]\n\n\ndef make_optimizer(lr: float) -> optax.GradientTransformation:\n    lr_sch = optax.exponential_decay(\n        init_value=lr,\n        transition_steps=10_000,\n        decay_rate=1/3,  # decay to `1/3 * init_lr` after `transition_steps` steps\n        staircase=True,  # use integer division to determine lr drop step\n        transition_begin=10_000,  # hold the initial lr value for the initial 10k steps (but first lr drop happens at 20k steps because `staircase` is specified)\n        end_value=lr / 100,  # stop decaying at `1/100 * init_lr`\n    )\n    optimizer_network = optax.adam(\n        learning_rate=lr_sch,\n        b1=0.9,\n        b2=0.99,\n        # paper:\n        #   the small value of \ud835\udf16 = 10^{\u221215} can significantly accelerate the convergence of the\n        #   hash table entries when their gradients are sparse and weak.\n        eps=1e-15,\n        eps_root=1e-15,\n    )\n    optimizer_ae = optax.adam(\n        learning_rate=1e-4,\n        b1=.9,\n        b2=.99,\n        eps=1e-8,\n        eps_root=0,\n    )\n    return optax.chain(\n        optax.multi_transform(\n            transforms={\n                \"network\": optimizer_network,\n                \"ae\": optimizer_ae,\n            },\n            param_labels={\n                \"nerf\": \"network\",\n                \"bg\": \"network\",\n                \"appearance_embeddings\": \"ae\",\n            },\n        ),\n        optax.add_decayed_weights(\n            # In NeRF experiments, the network can converge to a reasonably low loss during the\n            # first ~50k training steps (with 1024 rays per batch and 1024 samples per ray), but the\n            # loss becomes NaN after about 50~150k training steps.\n            # paper:\n            #   To prevent divergence after long training periods, we apply a weak L2 regularization\n            #   (factor 10^{\u22126}) to the neural network weights, ...\n            weight_decay=1e-6,\n            # paper:\n            #   ... to the neural network weights, but not to the hash table entries.\n            mask={\n                \"nerf\": {\n                    \"density_mlp\": True,\n                    \"rgb_mlp\": True,\n                    \"position_encoder\": False,\n                },\n                \"bg\": True,\n                \"appearance_embeddings\": False,\n            },\n        ),\n    )", "\n\ndef make_optimizer(lr: float) -> optax.GradientTransformation:\n    lr_sch = optax.exponential_decay(\n        init_value=lr,\n        transition_steps=10_000,\n        decay_rate=1/3,  # decay to `1/3 * init_lr` after `transition_steps` steps\n        staircase=True,  # use integer division to determine lr drop step\n        transition_begin=10_000,  # hold the initial lr value for the initial 10k steps (but first lr drop happens at 20k steps because `staircase` is specified)\n        end_value=lr / 100,  # stop decaying at `1/100 * init_lr`\n    )\n    optimizer_network = optax.adam(\n        learning_rate=lr_sch,\n        b1=0.9,\n        b2=0.99,\n        # paper:\n        #   the small value of \ud835\udf16 = 10^{\u221215} can significantly accelerate the convergence of the\n        #   hash table entries when their gradients are sparse and weak.\n        eps=1e-15,\n        eps_root=1e-15,\n    )\n    optimizer_ae = optax.adam(\n        learning_rate=1e-4,\n        b1=.9,\n        b2=.99,\n        eps=1e-8,\n        eps_root=0,\n    )\n    return optax.chain(\n        optax.multi_transform(\n            transforms={\n                \"network\": optimizer_network,\n                \"ae\": optimizer_ae,\n            },\n            param_labels={\n                \"nerf\": \"network\",\n                \"bg\": \"network\",\n                \"appearance_embeddings\": \"ae\",\n            },\n        ),\n        optax.add_decayed_weights(\n            # In NeRF experiments, the network can converge to a reasonably low loss during the\n            # first ~50k training steps (with 1024 rays per batch and 1024 samples per ray), but the\n            # loss becomes NaN after about 50~150k training steps.\n            # paper:\n            #   To prevent divergence after long training periods, we apply a weak L2 regularization\n            #   (factor 10^{\u22126}) to the neural network weights, ...\n            weight_decay=1e-6,\n            # paper:\n            #   ... to the neural network weights, but not to the hash table entries.\n            mask={\n                \"nerf\": {\n                    \"density_mlp\": True,\n                    \"rgb_mlp\": True,\n                    \"position_encoder\": False,\n                },\n                \"bg\": True,\n                \"appearance_embeddings\": False,\n            },\n        ),\n    )", "\n\n@common.jit_jaxfn_with(\n    static_argnames=[\"total_samples\"],\n    donate_argnums=(0,),  # NOTE: this only works for positional arguments, see <https://jax.readthedocs.io/en/latest/faq.html#buffer-donation>\n)\ndef train_step(\n    state: NeRFState,\n    /,\n    KEY: jran.KeyArray,\n    total_samples: int,\n    scene: SceneData,\n    perm: jax.Array,\n) -> Tuple[NeRFState, Dict[str, jax.Array | float]]:\n    # indices of views and pixels\n    view_idcs, pixel_idcs = scene.get_view_indices(perm), scene.get_pixel_indices(perm)\n\n    # TODO:\n    #   merge this and `models.renderers.make_rays_worldspace` as a single function\n    def make_rays_worldspace() -> Tuple[jax.Array, jax.Array]:\n        # [N], [N]\n        x, y = (\n            jnp.mod(pixel_idcs, scene.meta.camera.width),\n            jnp.floor_divide(pixel_idcs, scene.meta.camera.width),\n        )\n        # [N, 3]\n        d_cam = scene.meta.camera.make_ray_directions_from_pixel_coordinates(x, y, use_pixel_center=True)\n\n        # [N, 3]\n        o_world = scene.transforms[view_idcs, -3:]\n\n        # [N, 3, 3]\n        R_cws = scene.transforms[view_idcs, :9].reshape(-1, 3, 3)\n        # [N, 3]\n        # equavalent to performing `d_cam[i] @ R_cws[i].T` for each i in [0, N)\n        d_world = (d_cam[:, None, :] * R_cws).sum(-1)\n\n        return o_world, d_world\n\n    # CAVEAT: gradient is only calculate w.r.t. the first parameter of this function\n    # (`params_to_optimize here`), any parameters that need to be optimized should be taken from\n    # this parameter, instead from the outer-scope `state.params`.\n    def loss_fn(params_to_optimize, gt_rgba_f32, KEY):\n        o_world, d_world = make_rays_worldspace()\n        appearance_embeddings = (\n            params_to_optimize[\"appearance_embeddings\"][view_idcs]\n                if \"appearance_embeddings\" in params_to_optimize\n                else jnp.empty(0)\n        )\n        if state.use_background_model:\n            bg = state.bg_fn(\n                {\"params\": params_to_optimize[\"bg\"]},\n                o_world,\n                d_world,\n                appearance_embeddings,\n            )\n        elif state.render.random_bg:\n            KEY, key = jran.split(KEY, 2)\n            bg = jran.uniform(key, shape=(o_world.shape[0], 3), dtype=jnp.float32, minval=0, maxval=1)\n        else:\n            bg = jnp.asarray(state.render.bg)\n        KEY, key = jran.split(KEY, 2)\n        batch_metrics, pred_rgbds, tv = render_rays_train(\n            KEY=key,\n            o_world=o_world,\n            d_world=d_world,\n            appearance_embeddings=appearance_embeddings,\n            bg=bg,\n            total_samples=total_samples,\n            state=state.replace(params=params_to_optimize),\n        )\n        pred_rgbs, pred_depths = jnp.array_split(pred_rgbds, [3], axis=-1)\n        gt_rgbs = data.blend_rgba_image_array(imgarr=gt_rgba_f32, bg=bg)\n        batch_metrics[\"loss\"] = {\n            \"rgb\": jnp.where(\n                batch_metrics[\"ray_is_valid\"],\n                optax.huber_loss(pred_rgbs, gt_rgbs, delta=0.1).mean(axis=-1),\n                0.,\n            ).sum() / batch_metrics[\"n_valid_rays\"],\n            \"total_variation\": tv,\n        }\n        loss = jax.tree_util.tree_reduce(lambda x, y: x + y, batch_metrics[\"loss\"])\n        return loss, batch_metrics\n\n    loss_grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n    KEY, key = jran.split(KEY, 2)\n    (_, batch_metrics), grads = loss_grad_fn(\n        state.params,\n        scene.rgbas_u8[perm].astype(jnp.float32) / 255,\n        key,\n    )\n    state = state.apply_gradients(grads=grads)\n    return state, batch_metrics", "\n\ndef format_metrics(metrics: Dict[str, jax.Array | float]) -> str:\n    loss = metrics[\"loss\"]\n    return \"batch_size={}/{} samp./ray={:.1f}/{:.1f} n_rays={} loss:{{rgb={:.2e}({:.2f}dB),tv={:.2e}}}\".format(\n        metrics[\"measured_batch_size\"],\n        metrics[\"measured_batch_size_before_compaction\"],\n        metrics[\"measured_batch_size\"] / metrics[\"n_valid_rays\"],\n        metrics[\"measured_batch_size_before_compaction\"] / metrics[\"n_valid_rays\"],\n        metrics[\"n_valid_rays\"],\n        loss[\"rgb\"],\n        data.linear_to_db(loss[\"rgb\"], maxval=1.),\n        loss[\"total_variation\"],\n    )", ""]}
{"filename": "app/nerf/test.py", "chunked_list": ["from concurrent.futures import ThreadPoolExecutor\nfrom typing import List\nfrom typing_extensions import assert_never\n\nfrom PIL import Image\nfrom flax.training import checkpoints\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jran\nimport numpy as np", "import jax.random as jran\nimport numpy as np\n\nfrom models.nerfs import make_nerf_ngp, make_skysphere_background_model_ngp\nfrom models.renderers import render_image_inference\nfrom utils import common, data\nfrom utils.args import NeRFTestingArgs\nfrom utils.types import NeRFState, RenderedImage, RigidTransformation\n\n\ndef test(KEY: jran.KeyArray, args: NeRFTestingArgs, logger: common.Logger) -> int:\n    args.logs_dir.mkdir(parents=True, exist_ok=True)\n    logger = common.setup_logging(\n        \"nerf.test\",\n        file=args.logs_dir.joinpath(\"test.log\"),\n        level=args.common.logging.upper(),\n        file_level=\"DEBUG\",\n    )\n    if not args.ckpt.exists():\n        logger.error(\"specified checkpoint '{}' does not exist\".format(args.ckpt))\n        return 1\n\n    scene_data = data.load_scene(\n        srcs=args.frames,\n        scene_options=args.scene,\n        sort_frames=args.sort_frames,\n    )\n\n    scene_meta = scene_data.meta\n\n    if args.report_metrics:\n        logger.warn(\"will not load gt images because either the intrinsics or the extrinsics of the camera have been changed\")\n        if args.trajectory == \"orbit\":\n            scene_meta = scene_meta.make_frames_with_orbiting_trajectory(args.orbit)\n            logger.info(\"generated {} camera transforms for testing\".format(len(scene_meta.frames)))\n    else:\n        logger.debug(\"loading testing frames from {}\".format(args.frames))\n        logger.info(\"loaded {} camera transforms for testing\".format(len(scene_meta.frames)))\n\n    if args.camera_override.enabled:\n        scene_meta = scene_meta.replace(camera=args.camera_override.update_camera(scene_meta.camera))\n\n    # load parameters\n    logger.debug(\"loading checkpoint from '{}'\".format(args.ckpt))\n    state: NeRFState = checkpoints.restore_checkpoint(\n        args.ckpt,\n        target=NeRFState.empty(\n            raymarch=args.raymarch,\n            render=args.render,\n            scene_options=args.scene,\n            scene_meta=scene_meta,\n            nerf_fn=make_nerf_ngp(bound=scene_meta.bound, inference=True).apply,\n            bg_fn=make_skysphere_background_model_ngp(bound=scene_meta.bound).apply if scene_meta.bg else None,\n        ),\n    )\n    # WARN:\n    #   flax.checkpoints.restore_checkpoint() returns a pytree with all arrays of numpy's array type,\n    #   which slows down inference.  use jax.device_put() to move them to jax's default device.\n    # REF: <https://github.com/google/flax/discussions/1199#discussioncomment-635132>\n    state = jax.device_put(state)\n    if state.step == 0:\n        logger.error(\"an empty checkpoint was loaded from '{}'\".format(args.ckpt))\n        return 2\n    logger.info(\"checkpoint loaded from '{}' (step={})\".format(args.ckpt, int(state.step)))\n\n    rendered_images: List[RenderedImage] = []\n    try:\n        n_frames = len(scene_meta.frames)\n        logger.info(\"starting testing (totally {} transform(s) to test)\".format(n_frames))\n        for test_i in common.tqdm(range(n_frames), desc=\"testing (resolultion: {}x{})\".format(scene_meta.camera.width, scene_meta.camera.height)):\n            logger.debug(\"testing on frame {}\".format(scene_meta.frames[test_i]))\n            transform = RigidTransformation(\n                rotation=scene_meta.frames[test_i].transform_matrix_jax_array[:3, :3],\n                translation=scene_meta.frames[test_i].transform_matrix_jax_array[:3, 3],\n            )\n            KEY, key = jran.split(KEY, 2)\n            bg, rgb, depth, _ = data.to_cpu(render_image_inference(\n                KEY=key,\n                transform_cw=transform,\n                state=state,\n            ))\n            rendered_images.append(RenderedImage(\n                bg=bg,\n                rgb=rgb,\n                depth=depth,  # call to data.mono_to_rgb is deferred below so as to minimize impact on rendering speed\n            ))\n    except KeyboardInterrupt:\n        logger.warn(\"keyboard interrupt, tested {} images\".format(len(rendered_images)))\n\n    if args.trajectory == \"loaded\":\n        if len(rendered_images) == 0:\n            logger.warn(\"tested 0 image, not calculating psnr\")\n        else:\n            gt_rgbs_f32 = map(\n                lambda test_view, rendered_image: data.blend_rgba_image_array(\n                    test_view.image_rgba_u8.astype(jnp.float32) / 255,\n                    rendered_image.bg,\n                ),\n                scene_data.all_views,\n                rendered_images,\n            )\n            logger.debug(\"calculating psnr\")\n            mean_psnr = sum(map(\n                data.psnr,\n                map(data.f32_to_u8, gt_rgbs_f32),\n                map(lambda ri: ri.rgb, rendered_images),\n            )) / len(rendered_images)\n            logger.info(\"tested {} images, mean psnr={}\".format(len(rendered_images), mean_psnr))\n\n    elif args.trajectory == \"orbit\":\n        logger.debug(\"using generated orbiting trajectory, not calculating psnr\")\n\n    else:\n        assert_never(\"\")\n\n    save_dest = args.logs_dir.joinpath(\"test\")\n    save_dest.mkdir(parents=True, exist_ok=True)\n\n    if \"video\" in args.save_as:\n        dest_rgb_video = save_dest.joinpath(\"rgb.mp4\")\n        dest_depth_video = save_dest.joinpath(\"depth.mp4\")\n\n        logger.debug(\"saving predicted color images as a video at '{}'\".format(dest_rgb_video))\n        data.write_video(\n            save_dest.joinpath(\"rgb.mp4\"),\n            map(lambda img: img.rgb, rendered_images),\n            fps=args.fps,\n            loop=args.loop,\n        )\n\n        logger.debug(\"saving predicted disparities as a video at '{}'\".format(dest_depth_video))\n        data.write_video(\n            save_dest.joinpath(\"depth.mp4\"),\n            map(lambda img: common.compose(data.mono_to_rgb, data.f32_to_u8)(img.depth), rendered_images),\n            fps=args.fps,\n            loop=args.loop,\n        )\n\n    if \"image\" in args.save_as:\n        dest_rgb = save_dest.joinpath(\"rgb\")\n        dest_depth = save_dest.joinpath(\"depth\")\n\n        dest_rgb.mkdir(parents=True, exist_ok=True)\n        dest_depth.mkdir(parents=True, exist_ok=True)\n\n        logger.debug(\"saving as images\")\n        def save_rgb_and_depth(save_i: int, img: RenderedImage):\n            common.compose(\n                np.asarray,\n                Image.fromarray\n            )(img.rgb).save(dest_rgb.joinpath(\"{:04d}.png\".format(save_i)))\n            common.compose(\n                data.mono_to_rgb,\n                data.f32_to_u8,\n                np.asarray,\n                Image.fromarray\n            )(img.depth).save(dest_depth.joinpath(\"{:04d}.png\".format(save_i)))\n        for _ in common.tqdm(\n            ThreadPoolExecutor().map(\n                save_rgb_and_depth,\n                range(len(rendered_images)),\n                rendered_images,\n            ),\n            total=len(rendered_images),\n            desc=\"| saving images\",\n        ):\n            pass\n    return 0", "\n\ndef test(KEY: jran.KeyArray, args: NeRFTestingArgs, logger: common.Logger) -> int:\n    args.logs_dir.mkdir(parents=True, exist_ok=True)\n    logger = common.setup_logging(\n        \"nerf.test\",\n        file=args.logs_dir.joinpath(\"test.log\"),\n        level=args.common.logging.upper(),\n        file_level=\"DEBUG\",\n    )\n    if not args.ckpt.exists():\n        logger.error(\"specified checkpoint '{}' does not exist\".format(args.ckpt))\n        return 1\n\n    scene_data = data.load_scene(\n        srcs=args.frames,\n        scene_options=args.scene,\n        sort_frames=args.sort_frames,\n    )\n\n    scene_meta = scene_data.meta\n\n    if args.report_metrics:\n        logger.warn(\"will not load gt images because either the intrinsics or the extrinsics of the camera have been changed\")\n        if args.trajectory == \"orbit\":\n            scene_meta = scene_meta.make_frames_with_orbiting_trajectory(args.orbit)\n            logger.info(\"generated {} camera transforms for testing\".format(len(scene_meta.frames)))\n    else:\n        logger.debug(\"loading testing frames from {}\".format(args.frames))\n        logger.info(\"loaded {} camera transforms for testing\".format(len(scene_meta.frames)))\n\n    if args.camera_override.enabled:\n        scene_meta = scene_meta.replace(camera=args.camera_override.update_camera(scene_meta.camera))\n\n    # load parameters\n    logger.debug(\"loading checkpoint from '{}'\".format(args.ckpt))\n    state: NeRFState = checkpoints.restore_checkpoint(\n        args.ckpt,\n        target=NeRFState.empty(\n            raymarch=args.raymarch,\n            render=args.render,\n            scene_options=args.scene,\n            scene_meta=scene_meta,\n            nerf_fn=make_nerf_ngp(bound=scene_meta.bound, inference=True).apply,\n            bg_fn=make_skysphere_background_model_ngp(bound=scene_meta.bound).apply if scene_meta.bg else None,\n        ),\n    )\n    # WARN:\n    #   flax.checkpoints.restore_checkpoint() returns a pytree with all arrays of numpy's array type,\n    #   which slows down inference.  use jax.device_put() to move them to jax's default device.\n    # REF: <https://github.com/google/flax/discussions/1199#discussioncomment-635132>\n    state = jax.device_put(state)\n    if state.step == 0:\n        logger.error(\"an empty checkpoint was loaded from '{}'\".format(args.ckpt))\n        return 2\n    logger.info(\"checkpoint loaded from '{}' (step={})\".format(args.ckpt, int(state.step)))\n\n    rendered_images: List[RenderedImage] = []\n    try:\n        n_frames = len(scene_meta.frames)\n        logger.info(\"starting testing (totally {} transform(s) to test)\".format(n_frames))\n        for test_i in common.tqdm(range(n_frames), desc=\"testing (resolultion: {}x{})\".format(scene_meta.camera.width, scene_meta.camera.height)):\n            logger.debug(\"testing on frame {}\".format(scene_meta.frames[test_i]))\n            transform = RigidTransformation(\n                rotation=scene_meta.frames[test_i].transform_matrix_jax_array[:3, :3],\n                translation=scene_meta.frames[test_i].transform_matrix_jax_array[:3, 3],\n            )\n            KEY, key = jran.split(KEY, 2)\n            bg, rgb, depth, _ = data.to_cpu(render_image_inference(\n                KEY=key,\n                transform_cw=transform,\n                state=state,\n            ))\n            rendered_images.append(RenderedImage(\n                bg=bg,\n                rgb=rgb,\n                depth=depth,  # call to data.mono_to_rgb is deferred below so as to minimize impact on rendering speed\n            ))\n    except KeyboardInterrupt:\n        logger.warn(\"keyboard interrupt, tested {} images\".format(len(rendered_images)))\n\n    if args.trajectory == \"loaded\":\n        if len(rendered_images) == 0:\n            logger.warn(\"tested 0 image, not calculating psnr\")\n        else:\n            gt_rgbs_f32 = map(\n                lambda test_view, rendered_image: data.blend_rgba_image_array(\n                    test_view.image_rgba_u8.astype(jnp.float32) / 255,\n                    rendered_image.bg,\n                ),\n                scene_data.all_views,\n                rendered_images,\n            )\n            logger.debug(\"calculating psnr\")\n            mean_psnr = sum(map(\n                data.psnr,\n                map(data.f32_to_u8, gt_rgbs_f32),\n                map(lambda ri: ri.rgb, rendered_images),\n            )) / len(rendered_images)\n            logger.info(\"tested {} images, mean psnr={}\".format(len(rendered_images), mean_psnr))\n\n    elif args.trajectory == \"orbit\":\n        logger.debug(\"using generated orbiting trajectory, not calculating psnr\")\n\n    else:\n        assert_never(\"\")\n\n    save_dest = args.logs_dir.joinpath(\"test\")\n    save_dest.mkdir(parents=True, exist_ok=True)\n\n    if \"video\" in args.save_as:\n        dest_rgb_video = save_dest.joinpath(\"rgb.mp4\")\n        dest_depth_video = save_dest.joinpath(\"depth.mp4\")\n\n        logger.debug(\"saving predicted color images as a video at '{}'\".format(dest_rgb_video))\n        data.write_video(\n            save_dest.joinpath(\"rgb.mp4\"),\n            map(lambda img: img.rgb, rendered_images),\n            fps=args.fps,\n            loop=args.loop,\n        )\n\n        logger.debug(\"saving predicted disparities as a video at '{}'\".format(dest_depth_video))\n        data.write_video(\n            save_dest.joinpath(\"depth.mp4\"),\n            map(lambda img: common.compose(data.mono_to_rgb, data.f32_to_u8)(img.depth), rendered_images),\n            fps=args.fps,\n            loop=args.loop,\n        )\n\n    if \"image\" in args.save_as:\n        dest_rgb = save_dest.joinpath(\"rgb\")\n        dest_depth = save_dest.joinpath(\"depth\")\n\n        dest_rgb.mkdir(parents=True, exist_ok=True)\n        dest_depth.mkdir(parents=True, exist_ok=True)\n\n        logger.debug(\"saving as images\")\n        def save_rgb_and_depth(save_i: int, img: RenderedImage):\n            common.compose(\n                np.asarray,\n                Image.fromarray\n            )(img.rgb).save(dest_rgb.joinpath(\"{:04d}.png\".format(save_i)))\n            common.compose(\n                data.mono_to_rgb,\n                data.f32_to_u8,\n                np.asarray,\n                Image.fromarray\n            )(img.depth).save(dest_depth.joinpath(\"{:04d}.png\".format(save_i)))\n        for _ in common.tqdm(\n            ThreadPoolExecutor().map(\n                save_rgb_and_depth,\n                range(len(rendered_images)),\n                rendered_images,\n            ),\n            total=len(rendered_images),\n            desc=\"| saving images\",\n        ):\n            pass\n    return 0", ""]}
