{"filename": "ops/scripts/rename_whl.py", "chunked_list": ["\"\"\"Rename Python wheel to contain commit ID\"\"\"\nimport argparse\nimport pathlib\n\n\ndef main(args: argparse.Namespace) -> None:\n    \"\"\"Rename Python wheel\"\"\"\n    wheel_dir = pathlib.Path(args.wheel_dir).expanduser().resolve()\n    if not wheel_dir.is_dir():\n        raise ValueError(\"wheel_dir argument must be a directory\")\n\n    for whl_path in wheel_dir.glob(\"*.whl\"):\n        basename = whl_path.name\n        tokens = basename.split(\"-\")\n        assert len(tokens) == 5\n        keywords = {\n            \"pkg_name\": tokens[0],\n            \"version\": tokens[1],\n            \"commit_id\": args.commit_id,\n            \"platform_tag\": args.platform_tag,\n        }\n        new_name = (\n            \"{pkg_name}-{version}+{commit_id}-py3-none-{platform_tag}.whl\".format(\n                **keywords\n            )\n        )\n        new_path = whl_path.parent / new_name\n        print(f\"Renaming {whl_path} to {new_path}...\")\n        whl_path.rename(whl_path.parent / new_path)", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=(\n            \"Script to rename wheel(s) using a commit ID and platform tag.\"\n            \"Note: This script will not recurse into subdirectories.\"\n        )\n    )\n    parser.add_argument(\"wheel_dir\", type=str, help=\"Directory containing wheels\")\n    parser.add_argument(\"commit_id\", type=str, help=\"Hash of current git commit\")\n    parser.add_argument(\n        \"platform_tag\", type=str, help=\"Platform tag, PEP 425 compliant\"\n    )\n    parsed_args = parser.parse_args()\n    main(parsed_args)", ""]}
{"filename": "tests/python/test_model_query.py", "chunked_list": ["\"\"\"Tests for rich model query functions\"\"\"\n\nimport collections\n\nimport pytest\n\nimport tl2cgen\n\nfrom .metadata import format_libpath_for_example_model, load_example_model\nfrom .util import os_compatible_toolchains, os_platform", "from .metadata import format_libpath_for_example_model, load_example_model\nfrom .util import os_compatible_toolchains, os_platform\n\nModelFact = collections.namedtuple(\n    \"ModelFact\",\n    \"num_tree num_feature num_class pred_transform global_bias sigmoid_alpha ratio_c \"\n    \"threshold_type leaf_output_type\",\n)\nMODEL_FACTS = {\n    \"mushroom\": ModelFact(2, 127, 1, \"sigmoid\", 0.0, 1.0, 1.0, \"float32\", \"float32\"),", "MODEL_FACTS = {\n    \"mushroom\": ModelFact(2, 127, 1, \"sigmoid\", 0.0, 1.0, 1.0, \"float32\", \"float32\"),\n    \"dermatology\": ModelFact(60, 33, 6, \"softmax\", 0.5, 1.0, 1.0, \"float32\", \"float32\"),\n    \"toy_categorical\": ModelFact(\n        30, 2, 1, \"identity\", 0.0, 1.0, 1.0, \"float64\", \"float64\"\n    ),\n    \"sparse_categorical\": ModelFact(\n        1, 5057, 1, \"sigmoid\", 0.0, 1.0, 1.0, \"float64\", \"float64\"\n    ),\n}", "    ),\n}\n\n\n@pytest.mark.parametrize(\n    \"dataset\", [\"mushroom\", \"dermatology\", \"toy_categorical\", \"sparse_categorical\"]\n)\ndef test_model_query(tmpdir, dataset):\n    \"\"\"Test all query functions for every example model\"\"\"\n    if dataset == \"sparse_categorical\":\n        if os_platform() == \"windows\":\n            pytest.xfail(\"MSVC cannot handle long if conditional\")\n        elif os_platform() == \"osx\":\n            pytest.xfail(\"Apple Clang cannot handle long if conditional\")\n\n    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n    model = load_example_model(dataset)\n    assert model.num_feature == MODEL_FACTS[dataset].num_feature\n    assert model.num_class == MODEL_FACTS[dataset].num_class\n    assert model.num_tree == MODEL_FACTS[dataset].num_tree\n\n    # pylint: disable=R0801\n    toolchain = os_compatible_toolchains()[0]\n    tl2cgen.export_lib(\n        model,\n        toolchain=toolchain,\n        libpath=libpath,\n        params={\"quantize\": 1, \"parallel_comp\": model.num_tree},\n        verbose=True,\n    )\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    assert predictor.num_feature == MODEL_FACTS[dataset].num_feature\n    assert predictor.num_class == MODEL_FACTS[dataset].num_class\n    assert predictor.pred_transform == MODEL_FACTS[dataset].pred_transform\n    assert predictor.global_bias == MODEL_FACTS[dataset].global_bias\n    assert predictor.sigmoid_alpha == MODEL_FACTS[dataset].sigmoid_alpha\n    assert predictor.ratio_c == MODEL_FACTS[dataset].ratio_c\n    assert predictor.threshold_type == MODEL_FACTS[dataset].threshold_type\n    assert predictor.leaf_output_type == MODEL_FACTS[dataset].leaf_output_type", ""]}
{"filename": "tests/python/test_model_builder.py", "chunked_list": ["\"\"\"Tests for model builder interface\"\"\"\nimport os\nimport pathlib\n\nimport numpy as np\nimport pytest\nimport treelite\n\nimport tl2cgen\nfrom tl2cgen.contrib.util import _libext", "import tl2cgen\nfrom tl2cgen.contrib.util import _libext\n\nfrom .util import os_compatible_toolchains\n\n\n@pytest.mark.parametrize(\"test_round_trip\", [True, False])\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\ndef test_node_insert_delete(tmpdir, toolchain, test_round_trip):\n    # pylint: disable=R0914\n    \"\"\"Test ability to add and remove nodes\"\"\"\n    num_feature = 3\n    builder = treelite.ModelBuilder(num_feature=num_feature)\n    builder.append(treelite.ModelBuilder.Tree())\n    builder[0][1].set_root()\n    builder[0][1].set_numerical_test_node(\n        feature_id=2,\n        opname=\"<\",\n        threshold=-0.5,\n        default_left=True,\n        left_child_key=5,\n        right_child_key=10,\n    )\n    builder[0][5].set_leaf_node(-1)\n    builder[0][10].set_numerical_test_node(\n        feature_id=0,\n        opname=\"<=\",\n        threshold=0.5,\n        default_left=False,\n        left_child_key=7,\n        right_child_key=8,\n    )\n    builder[0][7].set_leaf_node(0.0)\n    builder[0][8].set_leaf_node(1.0)\n    del builder[0][1]\n    del builder[0][5]\n    builder[0][5].set_categorical_test_node(\n        feature_id=1,\n        left_categories=[1, 2, 4],\n        default_left=True,\n        left_child_key=20,\n        right_child_key=10,\n    )\n    builder[0][20].set_leaf_node(2.0)\n    builder[0][5].set_root()\n\n    model = builder.commit()\n    if test_round_trip:\n        checkpoint_path = os.path.join(tmpdir, \"checkpoint.bin\")\n        model.serialize(checkpoint_path)\n        model = treelite.Model.deserialize(checkpoint_path)\n    assert model.num_feature == num_feature\n    assert model.num_class == 1\n    assert model.num_tree == 1\n\n    libpath = pathlib.Path(tmpdir) / (\"libtest\" + _libext())\n    tl2cgen.export_lib(model, toolchain=toolchain, libpath=libpath, verbose=True)\n    predictor = tl2cgen.Predictor(libpath=libpath)\n    assert predictor.num_feature == num_feature\n    assert predictor.num_class == 1\n    assert predictor.pred_transform == \"identity\"\n    assert predictor.global_bias == 0.0\n    assert predictor.sigmoid_alpha == 1.0\n    assert predictor.ratio_c == 1.0\n    for f0 in [-0.5, 0.5, 1.5, np.nan]:\n        for f1 in [0, 1, 2, 3, 4, np.nan]:\n            for f2 in [-1.0, -0.5, 1.0, np.nan]:\n                x = np.array([[f0, f1, f2]])\n                dmat = tl2cgen.DMatrix(x, dtype=\"float32\")\n                pred = predictor.predict(dmat)\n                if f1 in [1, 2, 4] or np.isnan(f1):\n                    expected_pred = 2.0\n                elif f0 <= 0.5 and not np.isnan(f0):\n                    expected_pred = 0.0\n                else:\n                    expected_pred = 1.0\n                if pred != expected_pred:\n                    raise ValueError(\n                        f\"Prediction wrong for f0={f0}, f1={f1}, f2={f2}: \"\n                        + f\"expected_pred = {expected_pred} vs actual_pred = {pred}\"\n                    )", "def test_node_insert_delete(tmpdir, toolchain, test_round_trip):\n    # pylint: disable=R0914\n    \"\"\"Test ability to add and remove nodes\"\"\"\n    num_feature = 3\n    builder = treelite.ModelBuilder(num_feature=num_feature)\n    builder.append(treelite.ModelBuilder.Tree())\n    builder[0][1].set_root()\n    builder[0][1].set_numerical_test_node(\n        feature_id=2,\n        opname=\"<\",\n        threshold=-0.5,\n        default_left=True,\n        left_child_key=5,\n        right_child_key=10,\n    )\n    builder[0][5].set_leaf_node(-1)\n    builder[0][10].set_numerical_test_node(\n        feature_id=0,\n        opname=\"<=\",\n        threshold=0.5,\n        default_left=False,\n        left_child_key=7,\n        right_child_key=8,\n    )\n    builder[0][7].set_leaf_node(0.0)\n    builder[0][8].set_leaf_node(1.0)\n    del builder[0][1]\n    del builder[0][5]\n    builder[0][5].set_categorical_test_node(\n        feature_id=1,\n        left_categories=[1, 2, 4],\n        default_left=True,\n        left_child_key=20,\n        right_child_key=10,\n    )\n    builder[0][20].set_leaf_node(2.0)\n    builder[0][5].set_root()\n\n    model = builder.commit()\n    if test_round_trip:\n        checkpoint_path = os.path.join(tmpdir, \"checkpoint.bin\")\n        model.serialize(checkpoint_path)\n        model = treelite.Model.deserialize(checkpoint_path)\n    assert model.num_feature == num_feature\n    assert model.num_class == 1\n    assert model.num_tree == 1\n\n    libpath = pathlib.Path(tmpdir) / (\"libtest\" + _libext())\n    tl2cgen.export_lib(model, toolchain=toolchain, libpath=libpath, verbose=True)\n    predictor = tl2cgen.Predictor(libpath=libpath)\n    assert predictor.num_feature == num_feature\n    assert predictor.num_class == 1\n    assert predictor.pred_transform == \"identity\"\n    assert predictor.global_bias == 0.0\n    assert predictor.sigmoid_alpha == 1.0\n    assert predictor.ratio_c == 1.0\n    for f0 in [-0.5, 0.5, 1.5, np.nan]:\n        for f1 in [0, 1, 2, 3, 4, np.nan]:\n            for f2 in [-1.0, -0.5, 1.0, np.nan]:\n                x = np.array([[f0, f1, f2]])\n                dmat = tl2cgen.DMatrix(x, dtype=\"float32\")\n                pred = predictor.predict(dmat)\n                if f1 in [1, 2, 4] or np.isnan(f1):\n                    expected_pred = 2.0\n                elif f0 <= 0.5 and not np.isnan(f0):\n                    expected_pred = 0.0\n                else:\n                    expected_pred = 1.0\n                if pred != expected_pred:\n                    raise ValueError(\n                        f\"Prediction wrong for f0={f0}, f1={f1}, f2={f2}: \"\n                        + f\"expected_pred = {expected_pred} vs actual_pred = {pred}\"\n                    )", ""]}
{"filename": "tests/python/test_lightgbm_integration.py", "chunked_list": ["\"\"\"Tests for LightGBM integration\"\"\"\nfrom __future__ import annotations\n\nimport pathlib\nfrom typing import Any, Dict, Optional, Union\n\nimport numpy as np\nimport pytest\nimport scipy.sparse\nimport treelite", "import scipy.sparse\nimport treelite\nfrom hypothesis import given, settings\nfrom hypothesis.strategies import sampled_from\nfrom sklearn.datasets import load_iris, load_svmlight_file\nfrom sklearn.model_selection import train_test_split\n\nimport tl2cgen\nfrom tl2cgen.contrib.util import _libext\n", "from tl2cgen.contrib.util import _libext\n\nfrom .hypothesis_util import standard_regression_datasets, standard_settings\nfrom .metadata import (\n    example_model_db,\n    format_libpath_for_example_model,\n    load_example_model,\n)\nfrom .util import (\n    TemporaryDirectory,", "from .util import (\n    TemporaryDirectory,\n    check_predictor,\n    has_pandas,\n    os_compatible_toolchains,\n    os_platform,\n)\n\ntry:\n    import lightgbm\nexcept ImportError:\n    # skip this test suite if LightGBM is not installed\n    pytest.skip(\"LightGBM not installed; skipping\", allow_module_level=True)", "try:\n    import lightgbm\nexcept ImportError:\n    # skip this test suite if LightGBM is not installed\n    pytest.skip(\"LightGBM not installed; skipping\", allow_module_level=True)\n\n\ndef _compile_lightgbm_model(\n    *,\n    model_path: Optional[pathlib.Path] = None,\n    model_obj: Optional[lightgbm.Booster] = None,\n    libname: str,\n    prefix: Union[pathlib.Path, str],\n    toolchain: str,\n    params: Dict[str, Any],\n    verbose: bool = False,\n) -> tl2cgen.Predictor:\n    if model_path is not None:\n        model = treelite.Model.load(str(model_path), model_format=\"lightgbm\")\n    elif model_obj is not None:\n        model = treelite.Model.from_lightgbm(model_obj)\n    else:\n        raise RuntimeError(\"Either model_path or model_obj must be provided\")\n    libpath = pathlib.Path(prefix).expanduser().resolve().joinpath(libname + _libext())\n    tl2cgen.export_lib(\n        model, toolchain=toolchain, libpath=libpath, params=params, verbose=verbose\n    )\n    return tl2cgen.Predictor(libpath=libpath, verbose=True)", "\n\n@given(\n    toolchain=sampled_from(os_compatible_toolchains()),\n    objective=sampled_from([\"regression\", \"regression_l1\", \"huber\"]),\n    reg_sqrt=sampled_from([True, False]),\n    dataset=standard_regression_datasets(),\n)\n@settings(**standard_settings())\ndef test_lightgbm_regression(toolchain, objective, reg_sqrt, dataset):\n    # pylint: disable=too-many-locals\n    \"\"\"Test a regressor\"\"\"\n    X, y = dataset\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, shuffle=False\n    )\n    dtrain = lightgbm.Dataset(X_train, y_train, free_raw_data=False)\n    dtest = lightgbm.Dataset(X_test, y_test, reference=dtrain, free_raw_data=False)\n    param = {\n        \"task\": \"train\",\n        \"boosting_type\": \"gbdt\",\n        \"objective\": objective,\n        \"reg_sqrt\": reg_sqrt,\n        \"metric\": \"rmse\",\n        \"num_leaves\": 31,\n        \"learning_rate\": 0.05,\n    }\n    bst = lightgbm.train(\n        param,\n        dtrain,\n        num_boost_round=10,\n        valid_sets=[dtrain, dtest],\n        valid_names=[\"train\", \"test\"],\n    )\n\n    with TemporaryDirectory() as tmpdir:\n        predictor = _compile_lightgbm_model(\n            model_obj=bst,\n            libname=f\"regression_{objective}\",\n            prefix=tmpdir,\n            toolchain=toolchain,\n            params={\"quantize\": 1},\n            verbose=True,\n        )\n        dmat = tl2cgen.DMatrix(X_test, dtype=\"float64\")\n        out_pred = predictor.predict(dmat)\n        expected_pred = bst.predict(X_test).reshape((X_test.shape[0], -1))\n        np.testing.assert_almost_equal(out_pred, expected_pred, decimal=4)", "@settings(**standard_settings())\ndef test_lightgbm_regression(toolchain, objective, reg_sqrt, dataset):\n    # pylint: disable=too-many-locals\n    \"\"\"Test a regressor\"\"\"\n    X, y = dataset\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, shuffle=False\n    )\n    dtrain = lightgbm.Dataset(X_train, y_train, free_raw_data=False)\n    dtest = lightgbm.Dataset(X_test, y_test, reference=dtrain, free_raw_data=False)\n    param = {\n        \"task\": \"train\",\n        \"boosting_type\": \"gbdt\",\n        \"objective\": objective,\n        \"reg_sqrt\": reg_sqrt,\n        \"metric\": \"rmse\",\n        \"num_leaves\": 31,\n        \"learning_rate\": 0.05,\n    }\n    bst = lightgbm.train(\n        param,\n        dtrain,\n        num_boost_round=10,\n        valid_sets=[dtrain, dtest],\n        valid_names=[\"train\", \"test\"],\n    )\n\n    with TemporaryDirectory() as tmpdir:\n        predictor = _compile_lightgbm_model(\n            model_obj=bst,\n            libname=f\"regression_{objective}\",\n            prefix=tmpdir,\n            toolchain=toolchain,\n            params={\"quantize\": 1},\n            verbose=True,\n        )\n        dmat = tl2cgen.DMatrix(X_test, dtype=\"float64\")\n        out_pred = predictor.predict(dmat)\n        expected_pred = bst.predict(X_test).reshape((X_test.shape[0], -1))\n        np.testing.assert_almost_equal(out_pred, expected_pred, decimal=4)", "\n\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n@pytest.mark.parametrize(\"boosting_type\", [\"gbdt\", \"rf\"])\n@pytest.mark.parametrize(\"objective\", [\"multiclass\", \"multiclassova\"])\ndef test_lightgbm_multiclass_classification(\n    tmpdir, objective, boosting_type, toolchain\n):\n    # pylint: disable=too-many-locals\n    \"\"\"Test a multi-class classifier\"\"\"\n    model_path = pathlib.Path(tmpdir) / \"iris_lightgbm.txt\"\n\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, shuffle=False\n    )\n    dtrain = lightgbm.Dataset(X_train, y_train, free_raw_data=False)\n    dtest = lightgbm.Dataset(X_test, y_test, reference=dtrain, free_raw_data=False)\n    param = {\n        \"task\": \"train\",\n        \"boosting\": boosting_type,\n        \"objective\": objective,\n        \"metric\": \"multi_logloss\",\n        \"num_class\": 3,\n        \"num_leaves\": 31,\n        \"learning_rate\": 0.05,\n    }\n    if boosting_type == \"rf\":\n        param.update({\"bagging_fraction\": 0.8, \"bagging_freq\": 1})\n    bst = lightgbm.train(\n        param,\n        dtrain,\n        num_boost_round=10,\n        valid_sets=[dtrain, dtest],\n        valid_names=[\"train\", \"test\"],\n    )\n    bst.save_model(model_path)\n\n    predictor = _compile_lightgbm_model(\n        model_path=model_path,\n        libname=f\"iris_{objective}\",\n        prefix=tmpdir,\n        toolchain=toolchain,\n        params={\"quantize\": 1},\n        verbose=True,\n    )\n\n    dmat = tl2cgen.DMatrix(X_test, dtype=\"float64\")\n    out_pred = predictor.predict(dmat)\n    expected_pred = bst.predict(X_test)\n    np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)", "\n\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n@pytest.mark.parametrize(\"objective\", [\"binary\", \"xentlambda\", \"xentropy\"])\ndef test_lightgbm_binary_classification(tmpdir, objective, toolchain):\n    # pylint: disable=too-many-locals\n    \"\"\"Test a binary classifier\"\"\"\n    dataset = \"mushroom\"\n    model_path = pathlib.Path(tmpdir) / \"mushroom_lightgbm.txt\"\n    dtest_path = example_model_db[dataset].dtest\n\n    dtrain = lightgbm.Dataset(example_model_db[dataset].dtrain)\n    dtest = lightgbm.Dataset(dtest_path, reference=dtrain)\n    param = {\n        \"task\": \"train\",\n        \"boosting_type\": \"gbdt\",\n        \"objective\": objective,\n        \"metric\": \"auc\",\n        \"num_leaves\": 7,\n        \"learning_rate\": 0.1,\n    }\n    bst = lightgbm.train(\n        param,\n        dtrain,\n        num_boost_round=10,\n        valid_sets=[dtrain, dtest],\n        valid_names=[\"train\", \"test\"],\n    )\n    bst.save_model(model_path)\n\n    shape = (dtest.num_data(), -1)\n    expected_prob = bst.predict(dtest_path).reshape(shape)\n    expected_margin = bst.predict(dtest_path, raw_score=True).reshape(shape)\n\n    predictor = _compile_lightgbm_model(\n        model_path=model_path,\n        libname=f\"agaricus_{objective}\",\n        prefix=tmpdir,\n        toolchain=toolchain,\n        params={},\n        verbose=True,\n    )\n    dmat = tl2cgen.DMatrix(\n        load_svmlight_file(dtest_path, zero_based=True)[0], dtype=\"float64\"\n    )\n\n    out_prob = predictor.predict(dmat)\n    np.testing.assert_almost_equal(out_prob, expected_prob, decimal=5)\n    out_margin = predictor.predict(dmat, pred_margin=True)\n    np.testing.assert_almost_equal(out_margin, expected_margin, decimal=5)", "\n\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n@pytest.mark.parametrize(\"parallel_comp\", [None, 2])\n@pytest.mark.parametrize(\"quantize\", [True, False])\ndef test_categorical_data(tmpdir, quantize, parallel_comp, toolchain):\n    \"\"\"\n    LightGBM is able to produce categorical splits directly, so that\n    categorical data don't have to be one-hot encoded. Test if Treelite is\n    able to handle categorical splits.\n\n    This toy example contains two features, both of which are categorical.\n    The first has cardinality 3 and the second 5. The label was generated using\n    the formula\n\n       y = f(x0) + g(x1) + [noise with std=0.1]\n\n    where f and g are given by the tables\n\n       x0  f(x0)        x1  g(x1)\n        0    -20         0     -2\n        1    -10         1     -1\n        2      0         2      0\n                         3      1\n                         4      2\n    \"\"\"\n    dataset = \"toy_categorical\"\n    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n    model = load_example_model(dataset)\n\n    # pylint: disable=R0801\n    params = {\n        \"quantize\": (1 if quantize else 0),\n        \"parallel_comp\": (parallel_comp if parallel_comp else 0),\n    }\n    tl2cgen.export_lib(\n        model, toolchain=toolchain, libpath=libpath, params=params, verbose=True\n    )\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    check_predictor(predictor, dataset)", "\n\n@pytest.mark.skipif(not has_pandas(), reason=\"Pandas required\")\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\ndef test_categorical_data_pandas_df_with_dummies(tmpdir, toolchain):\n    # pylint: disable=too-many-locals\n    \"\"\"\n    This toy example contains two features, both of which are categorical.\n    The first has cardinality 3 and the second 5. The label was generated using\n    the formula\n\n       y = f(x0) + g(x1) + [noise with std=0.1]\n\n    where f and g are given by the tables\n\n       x0  f(x0)        x1  g(x1)\n        0    -20         0     -2\n        1    -10         1     -1\n        2      0         2      0\n                         3      1\n                         4      2\n\n    Unlike test_categorical_data(), this test will convert the categorical features into\n    dummies using OneHotEncoder and store them into a Pandas Dataframe.\n    \"\"\"\n    import pandas as pd  # pylint: disable=import-outside-toplevel\n\n    rng = np.random.default_rng(seed=0)\n    n_row = 100\n    x0 = rng.integers(low=0, high=3, size=n_row)\n    x1 = rng.integers(low=0, high=5, size=n_row)\n    noise = rng.standard_normal(size=n_row) * 0.1\n    y = (x0 * 10 - 20) + (x1 - 2) + noise\n    df = pd.DataFrame({\"x0\": x0, \"x1\": x1}).astype(\"category\")\n    df_dummies = pd.get_dummies(df, prefix=[\"x0\", \"x1\"], prefix_sep=\"=\")\n    assert df_dummies.columns.tolist() == [\n        \"x0=0\",\n        \"x0=1\",\n        \"x0=2\",\n        \"x1=0\",\n        \"x1=1\",\n        \"x1=2\",\n        \"x1=3\",\n        \"x1=4\",\n    ]\n\n    model_path = pathlib.Path(tmpdir) / \"toy_dummies.txt\"\n\n    dtrain = lightgbm.Dataset(df_dummies, label=y)\n    param = {\n        \"task\": \"train\",\n        \"boosting_type\": \"gbdt\",\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"num_leaves\": 7,\n        \"learning_rate\": 0.1,\n    }\n    bst = lightgbm.train(\n        param, dtrain, num_boost_round=15, valid_sets=[dtrain], valid_names=[\"train\"]\n    )\n    lgb_out = bst.predict(df_dummies).reshape((df_dummies.shape[0], -1))\n    bst.save_model(model_path)\n\n    predictor = _compile_lightgbm_model(\n        model_path=model_path,\n        libname=\"dummies\",\n        prefix=tmpdir,\n        toolchain=toolchain,\n        params={},\n        verbose=True,\n    )\n    tl_out = predictor.predict(tl2cgen.DMatrix(df_dummies.values, dtype=\"float32\"))\n    np.testing.assert_almost_equal(lgb_out, tl_out, decimal=5)", "\n\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n@pytest.mark.parametrize(\"quantize\", [True, False])\ndef test_sparse_ranking_model(tmpdir, quantize, toolchain):\n    # pylint: disable=too-many-locals\n    \"\"\"Generate a LightGBM ranking model with highly sparse data.\n    This example is inspired by https://github.com/dmlc/treelite/issues/222. It verifies that\n    Treelite is able to accommodate the unique behavior of LightGBM when it comes to handling\n    missing values.\n\n    LightGBM offers two modes of handling missing values:\n\n    1. Assign default direction per each test node: This is similar to how XGBoost handles missing\n       values.\n    2. Replace missing values with zeroes (0.0): This behavior is unique to LightGBM.\n\n    The mode is controlled by the missing_value_to_zero_ field of each test node.\n\n    This example is crafted so as to invoke the second mode of missing value handling.\n    \"\"\"\n    rng = np.random.default_rng(seed=2020)\n    X = scipy.sparse.random(\n        m=10, n=206947, format=\"csr\", dtype=np.float64, random_state=0, density=0.0001\n    )\n    X.data = rng.standard_normal(size=X.data.shape[0], dtype=np.float64)\n    y = rng.integers(low=0, high=5, size=X.shape[0])\n\n    params = {\n        \"objective\": \"lambdarank\",\n        \"num_leaves\": 32,\n        \"lambda_l1\": 0.0,\n        \"lambda_l2\": 0.0,\n        \"min_gain_to_split\": 0.0,\n        \"learning_rate\": 1.0,\n        \"min_data_in_leaf\": 1,\n    }\n\n    model_path = pathlib.Path(tmpdir) / \"sparse_ranking_lightgbm.txt\"\n\n    dtrain = lightgbm.Dataset(X, label=y, group=[X.shape[0]])\n\n    bst = lightgbm.train(params, dtrain, num_boost_round=1)\n    lgb_out = bst.predict(X).reshape((X.shape[0], -1))\n    bst.save_model(model_path)\n\n    predictor = _compile_lightgbm_model(\n        model_path=model_path,\n        libname=\"sparse_ranking_lgb\",\n        prefix=tmpdir,\n        toolchain=toolchain,\n        params={\"quantize\": (1 if quantize else 0)},\n        verbose=True,\n    )\n    dmat = tl2cgen.DMatrix(X)\n    out = predictor.predict(dmat)\n    np.testing.assert_almost_equal(out, lgb_out)", "\n\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n@pytest.mark.parametrize(\"quantize\", [True, False])\ndef test_sparse_categorical_model(tmpdir, quantize, toolchain):\n    \"\"\"\n    LightGBM is able to produce categorical splits directly, so that\n    categorical data don't have to be one-hot encoded. Test if Treelite is\n    able to handle categorical splits.\n\n    This example produces a model with high-cardinality categorical variables.\n    The training data has many missing values, so we need to match LightGBM\n    when it comes to handling missing values\n    \"\"\"\n    if toolchain == \"clang\":\n        pytest.xfail(reason=\"Clang cannot handle long if conditional\")\n    if os_platform() == \"windows\":\n        pytest.xfail(reason=\"MSVC cannot handle long if conditional\")\n    if os_platform() == \"osx\":\n        pytest.xfail(reason=\"Apple Clang cannot handle long if conditional\")\n    dataset = \"sparse_categorical\"\n    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n    model = load_example_model(dataset)\n    params = {\"quantize\": (1 if quantize else 0)}\n    tl2cgen.export_lib(\n        model,\n        toolchain=toolchain,\n        libpath=libpath,\n        params=params,\n        verbose=True,\n        options=[\"-O0\"],\n    )\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    check_predictor(predictor, dataset)", "\n\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\ndef test_nan_handling_with_categorical_splits(tmpdir, toolchain):\n    \"\"\"Test that NaN inputs are handled correctly in categorical splits\"\"\"\n\n    # Test case taken from https://github.com/dmlc/treelite/issues/277\n    X = np.array(30 * [[1]] + 30 * [[2]] + 30 * [[0]])\n    y = np.array(60 * [5] + 30 * [10])\n    train_data = lightgbm.Dataset(X, label=y, categorical_feature=[0])\n    bst = lightgbm.train({}, train_data, 1)\n\n    model_path = pathlib.Path(tmpdir) / \"dummy_categorical.txt\"\n\n    input_with_nan = np.array([[np.NaN], [0.0]])\n    lgb_pred = bst.predict(input_with_nan).reshape((input_with_nan.shape[0], -1))\n    bst.save_model(model_path)\n\n    predictor = _compile_lightgbm_model(\n        model_path=model_path,\n        libname=\"dummy_categorical_lgb\",\n        prefix=tmpdir,\n        toolchain=toolchain,\n        params={},\n        verbose=True,\n    )\n    dmat = tl2cgen.DMatrix(input_with_nan)\n    tl_pred = predictor.predict(dmat)\n    np.testing.assert_almost_equal(tl_pred, lgb_pred)", ""]}
{"filename": "tests/python/test_basic.py", "chunked_list": ["\"\"\"Suite of basic tests\"\"\"\nimport itertools\nimport os\nimport pathlib\nimport subprocess\nimport sys\nfrom zipfile import ZipFile\n\nimport pytest\nfrom scipy.sparse import csr_matrix", "import pytest\nfrom scipy.sparse import csr_matrix\n\nimport tl2cgen\n\nfrom .metadata import (\n    example_model_db,\n    format_libpath_for_example_model,\n    load_example_model,\n)", "    load_example_model,\n)\nfrom .util import check_predictor, does_not_raise, os_compatible_toolchains, os_platform\n\n\n@pytest.mark.parametrize(\n    \"dataset,use_annotation,parallel_comp,quantize,toolchain\",\n    list(\n        itertools.product(\n            [\"mushroom\", \"dermatology\"],", "        itertools.product(\n            [\"mushroom\", \"dermatology\"],\n            [True, False],\n            [None, 4],\n            [True, False],\n            os_compatible_toolchains(),\n        )\n    )\n    + [\n        (\"toy_categorical\", False, 30, True, os_compatible_toolchains()[0]),", "    + [\n        (\"toy_categorical\", False, 30, True, os_compatible_toolchains()[0]),\n    ],\n)\ndef test_basic(\n    tmpdir, annotation, dataset, use_annotation, quantize, parallel_comp, toolchain\n):\n    # pylint: disable=too-many-arguments\n    \"\"\"Basic test for C codegen\"\"\"\n    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n    model = load_example_model(dataset)\n    annotation_path = os.path.join(tmpdir, \"annotation.json\")\n\n    if use_annotation:\n        if annotation[dataset] is None:\n            pytest.skip(\"No training data available. Skipping annotation\")\n        with open(annotation_path, \"w\", encoding=\"UTF-8\") as f:\n            f.write(annotation[dataset])\n\n    params = {\n        \"annotate_in\": (annotation_path if use_annotation else \"NULL\"),\n        \"quantize\": (1 if quantize else 0),\n        \"parallel_comp\": (parallel_comp if parallel_comp else 0),\n    }\n    tl2cgen.export_lib(\n        model, toolchain=toolchain, libpath=libpath, params=params, verbose=True\n    )\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    check_predictor(predictor, dataset)", "\n\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n@pytest.mark.parametrize(\"use_elf\", [True, False])\n@pytest.mark.parametrize(\"dataset\", [\"mushroom\", \"dermatology\", \"toy_categorical\"])\ndef test_failsafe_compiler(tmpdir, dataset, use_elf, toolchain):\n    \"\"\"Test 'failsafe' compiler\"\"\"\n    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n    model = load_example_model(dataset)\n\n    params = {\"dump_array_as_elf\": (1 if use_elf else 0)}\n\n    is_linux = sys.platform.startswith(\"linux\")\n    # Expect TL2cgen to throw error if we try to use dump_array_as_elf on non-Linux OS\n    # Also, failsafe compiler is only available for XGBoost models\n    if ((not is_linux) and use_elf) or example_model_db[dataset].format != \"xgboost\":\n        expect_raises = pytest.raises(tl2cgen.TL2cgenError)\n    else:\n        expect_raises = does_not_raise()\n    with expect_raises:\n        tl2cgen.export_lib(\n            model,\n            compiler=\"failsafe\",\n            toolchain=toolchain,\n            libpath=libpath,\n            params=params,\n            verbose=True,\n        )\n        predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n        check_predictor(predictor, dataset)", "\n\n@pytest.mark.skipif(os_platform() == \"windows\", reason=\"Make unavailable on Windows\")\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n@pytest.mark.parametrize(\"dataset\", [\"mushroom\", \"dermatology\", \"toy_categorical\"])\ndef test_srcpkg(tmpdir, dataset, toolchain):\n    \"\"\"Test feature to export a source tarball\"\"\"\n    pkgpath = pathlib.Path(tmpdir) / \"srcpkg.zip\"\n    model = load_example_model(dataset)\n    tl2cgen.export_srcpkg(\n        model,\n        toolchain=toolchain,\n        pkgpath=pkgpath,\n        libname=example_model_db[dataset].libname,\n        params={\"parallel_comp\": 4},\n        verbose=True,\n    )\n    with ZipFile(pkgpath, \"r\") as zip_ref:\n        zip_ref.extractall(tmpdir)\n    nproc = os.cpu_count()\n    subprocess.check_call(\n        [\"make\", \"-C\", example_model_db[dataset].libname, f\"-j{nproc}\"], cwd=tmpdir\n    )\n\n    libpath = pathlib.Path(tmpdir) / example_model_db[dataset].libname\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    check_predictor(predictor, dataset)", "\n\n@pytest.mark.parametrize(\"dataset\", [\"mushroom\", \"dermatology\", \"toy_categorical\"])\ndef test_srcpkg_cmake(tmpdir, dataset):  # pylint: disable=R0914\n    \"\"\"Test feature to export a source tarball\"\"\"\n    pkgpath = pathlib.Path(tmpdir) / \"srcpkg.zip\"\n    model = load_example_model(dataset)\n    tl2cgen.export_srcpkg(\n        model,\n        toolchain=\"cmake\",\n        pkgpath=pkgpath,\n        libname=example_model_db[dataset].libname,\n        params={\"parallel_comp\": 4},\n        verbose=True,\n    )\n    with ZipFile(pkgpath, \"r\") as zip_ref:\n        zip_ref.extractall(tmpdir)\n    build_dir = pathlib.Path(tmpdir) / example_model_db[dataset].libname / \"build\"\n    build_dir.mkdir()\n    nproc = os.cpu_count()\n    win_opts = [\"-A\", \"x64\"] if os_platform() == \"windows\" else []\n    subprocess.check_call([\"cmake\", \"..\"] + win_opts, cwd=build_dir)\n    subprocess.check_call(\n        [\"cmake\", \"--build\", \".\", \"--config\", \"Release\", \"--parallel\", str(nproc)],\n        cwd=build_dir,\n    )\n\n    predictor = tl2cgen.Predictor(libpath=build_dir, verbose=True)\n    check_predictor(predictor, dataset)", "\n\ndef test_deficient_matrix(tmpdir):\n    \"\"\"Test if TL2cgen correctly handles sparse matrix with fewer columns than the training data\n    used for the model. In this case, the matrix should be padded with zeros.\"\"\"\n    libpath = format_libpath_for_example_model(\"mushroom\", prefix=tmpdir)\n    model = load_example_model(\"mushroom\")\n    toolchain = os_compatible_toolchains()[0]\n    tl2cgen.export_lib(\n        model,\n        toolchain=toolchain,\n        libpath=libpath,\n        params={\"quantize\": 1},\n        verbose=True,\n    )\n\n    X = csr_matrix(([], ([], [])), shape=(3, 3))\n    dmat = tl2cgen.DMatrix(X, dtype=\"float32\")\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    assert predictor.num_feature == 127\n    predictor.predict(dmat)  # should not crash", "\n\ndef test_too_wide_matrix(tmpdir):\n    \"\"\"Test if TL2cgen correctly handles sparse matrix with more columns than the training data\n    used for the model. In this case, an exception should be thrown\"\"\"\n    libpath = format_libpath_for_example_model(\"mushroom\", prefix=tmpdir)\n    model = load_example_model(\"mushroom\")\n    toolchain = os_compatible_toolchains()[0]\n    tl2cgen.export_lib(\n        model,\n        toolchain=toolchain,\n        libpath=libpath,\n        params={\"quantize\": 1},\n        verbose=True,\n    )\n\n    X = csr_matrix(([], ([], [])), shape=(3, 1000))\n    dmat = tl2cgen.DMatrix(X, dtype=\"float32\")\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    assert predictor.num_feature == 127\n    pytest.raises(tl2cgen.TL2cgenError, predictor.predict, dmat)", ""]}
{"filename": "tests/python/metadata.py", "chunked_list": ["\"\"\"Metadata for datasets and models used for testing\"\"\"\n\nimport collections\nimport pathlib\nfrom typing import Optional, Union\n\nimport treelite\n\nfrom tl2cgen.contrib.util import _libext\n", "from tl2cgen.contrib.util import _libext\n\nCURRENT_DIR = pathlib.Path(__file__).parent.expanduser().resolve()\nDPATH = CURRENT_DIR.parent / \"example_data\"\n\nDataset = collections.namedtuple(\n    \"Dataset\",\n    \"model format dtrain dtest libname expected_prob expected_margin is_multiclass dtype\",\n)\n", ")\n\n_dataset_db = {\n    \"mushroom\": Dataset(\n        model=\"mushroom.model\",\n        format=\"xgboost\",\n        dtrain=\"agaricus.train\",\n        dtest=\"agaricus.test\",\n        libname=\"agaricus\",\n        expected_prob=\"agaricus.test.prob\",", "        libname=\"agaricus\",\n        expected_prob=\"agaricus.test.prob\",\n        expected_margin=\"agaricus.test.margin\",\n        is_multiclass=False,\n        dtype=\"float32\",\n    ),\n    \"dermatology\": Dataset(\n        model=\"dermatology.model\",\n        format=\"xgboost\",\n        dtrain=\"dermatology.train\",", "        format=\"xgboost\",\n        dtrain=\"dermatology.train\",\n        dtest=\"dermatology.test\",\n        libname=\"dermatology\",\n        expected_prob=\"dermatology.test.prob\",\n        expected_margin=\"dermatology.test.margin\",\n        is_multiclass=True,\n        dtype=\"float32\",\n    ),\n    \"toy_categorical\": Dataset(", "    ),\n    \"toy_categorical\": Dataset(\n        model=\"toy_categorical_model.txt\",\n        format=\"lightgbm\",\n        dtrain=None,\n        dtest=\"toy_categorical.test\",\n        libname=\"toycat\",\n        expected_prob=None,\n        expected_margin=\"toy_categorical.test.pred\",\n        is_multiclass=False,", "        expected_margin=\"toy_categorical.test.pred\",\n        is_multiclass=False,\n        dtype=\"float64\",\n    ),\n    \"sparse_categorical\": Dataset(\n        model=\"sparse_categorical_model.txt\",\n        format=\"lightgbm\",\n        dtrain=None,\n        dtest=\"sparse_categorical.test\",\n        libname=\"sparsecat\",", "        dtest=\"sparse_categorical.test\",\n        libname=\"sparsecat\",\n        expected_prob=None,\n        expected_margin=\"sparse_categorical.test.margin\",\n        is_multiclass=False,\n        dtype=\"float64\",\n    ),\n    \"xgb_toy_categorical\": Dataset(\n        model=\"xgb_toy_categorical_model.json\",\n        format=\"xgboost_json\",", "        model=\"xgb_toy_categorical_model.json\",\n        format=\"xgboost_json\",\n        dtrain=None,\n        dtest=\"xgb_toy_categorical.test\",\n        libname=\"xgbtoycat\",\n        expected_prob=None,\n        expected_margin=\"xgb_toy_categorical.test.pred\",\n        is_multiclass=False,\n        dtype=\"float32\",\n    ),", "        dtype=\"float32\",\n    ),\n}\n\n\ndef _qualify_path(prefix: str, path: Optional[str]) -> Optional[str]:\n    if path is None:\n        return None\n    new_path = DPATH / prefix / path\n    assert new_path.exists()\n    return str(new_path)", "\n\ndef format_libpath_for_example_model(\n    example_id: str,\n    prefix: Union[pathlib.Path, str],\n) -> pathlib.Path:\n    \"\"\"Format path for the shared lib corresponding to an example model\"\"\"\n    prefix = pathlib.Path(prefix).expanduser().resolve()\n    return prefix.joinpath(example_model_db[example_id].libname + _libext())\n", "\n\ndef load_example_model(example_id: str) -> treelite.Model:\n    \"\"\"Load an example model\"\"\"\n    return treelite.Model.load(\n        example_model_db[example_id].model,\n        model_format=example_model_db[example_id].format,\n    )\n\n", "\n\nexample_model_db = {\n    k: v._replace(\n        model=_qualify_path(k, v.model),\n        dtrain=_qualify_path(k, v.dtrain),\n        dtest=_qualify_path(k, v.dtest),\n        expected_prob=_qualify_path(k, v.expected_prob),\n        expected_margin=_qualify_path(k, v.expected_margin),\n    )", "        expected_margin=_qualify_path(k, v.expected_margin),\n    )\n    for k, v in _dataset_db.items()\n}\n"]}
{"filename": "tests/python/hypothesis_util.py", "chunked_list": ["\"\"\"Utility functions for hypothesis-based testing\"\"\"\n# pylint: disable=differing-param-doc,missing-type-doc\n\nfrom sys import platform as _platform\n\nimport numpy as np\nfrom hypothesis import assume\nfrom hypothesis.strategies import composite, integers, just, none\nfrom sklearn.datasets import make_classification, make_regression\n", "from sklearn.datasets import make_classification, make_regression\n\n\ndef _get_limits(strategy):\n    \"\"\"Try to find the strategy's limits.\n    Raises AttributeError if limits cannot be determined.\n\n    Credit: Carl Simon Adorf (@csadorf)\n    https://github.com/rapidsai/cuml/blob/447bded/python/cuml/testing/strategies.py\n    \"\"\"\n    # unwrap if lazy\n    strategy = getattr(strategy, \"wrapped_strategy\", strategy)\n\n    try:\n        yield getattr(strategy, \"value\")  # just(...)\n    except AttributeError:\n        # assume numbers strategy\n        yield strategy.start\n        yield strategy.stop", "\n\n@composite\ndef standard_classification_datasets(\n    draw,\n    n_samples=integers(min_value=100, max_value=200),\n    n_features=integers(min_value=10, max_value=20),\n    *,\n    n_informative=None,\n    n_redundant=None,\n    n_repeated=just(0),\n    n_classes=just(2),\n    n_clusters_per_class=just(2),\n    weights=none(),\n    flip_y=just(0.01),\n    class_sep=just(1.0),\n    hypercube=just(True),\n    shift=just(0.0),\n    scale=just(1.0),\n    shuffle=just(True),\n    random_state=None,\n):\n    # pylint: disable=too-many-locals\n    \"\"\"\n    Returns a strategy to generate classification problem input datasets.\n    Note:\n    This function uses the sklearn.datasets.make_classification function to\n    generate the classification problem from the provided search strategies.\n\n    Credit: Carl Simon Adorf (@csadorf)\n    https://github.com/rapidsai/cuml/blob/447bded/python/cuml/testing/strategies.py\n\n    Parameters\n    ----------\n    draw:\n        Callback function, to be used internally by Hypothesis\n    n_samples: SearchStrategy[int]\n        Returned arrays will have number of rows drawn from these values.\n    n_features: SearchStrategy[int]\n        Returned arrays will have number of columns drawn from these values.\n    n_informative: SearchStrategy[int], default=none\n        A search strategy for the number of informative features. If none,\n        will use 10% of the actual number of features, but not less than 1\n        unless the number of features is zero.\n    n_redundant: SearchStrategy[int], default=none\n        A search strategy for the number of redundant features. Redundant features\n        will be generated as linear combinations of informative features. If none,\n        will use 10% of the actual number of features, but not less than 1\n        unless the number of features is zero.\n    n_repeated: SearchStrategy[int], default=just(0)\n        A search strategy for the number of duplicated features.\n    n_classes: SearchStrategy[int], default=just(2)\n        A search strategy for the number of classes in the classification problem.\n    n_clusters_per_class: SearchStrategy[int], default=just(2)\n        A search strategy for the number of clusters per class\n    weights: SearchStrategy[array], default=none\n        A search strategy for the proportions of samples assigned to each class. If\n        none, always generate classification problems with balanced classes.\n    flip_y: SearchStrategy[float], default=just(0.01)\n        A search strategy for the fraction of samples whose class is assigned randomly.\n        Larger value for this value introduces noise in the labels and make the\n        classification problem harder.\n    class_sep: SearchStrategy[float], default=just(1.0)\n        A search strategy for the parameter class_sep.\n        See sklearn.dataset.make_classification() for a detailed explanation of this\n        parameter.\n    hypercube: SearchStrategy[bool], default=just(True)\n        A search strategy for the parameter hypercube.\n        See sklearn.dataset.make_classification() for a detailed explanation of this\n        parameter.\n    shift: SearchStrategy[float], default=just(0.0)\n        A search strategy for the parameter shift.\n        See sklearn.dataset.make_classification() for a detailed explanation of this\n        parameter.\n    scale: SearchStrategy[float], default=just(1.0)\n        A search strategy for the parameter scale.\n        See sklearn.dataset.make_classification() for a detailed explanation of this\n        parameter.\n    shuffle: SearchStrategy[bool], default=just(True)\n        A boolean search strategy to determine whether samples and features\n        are shuffled.\n    random_state: int, RandomState instance or None, default=None\n        Pass a random state or integer to determine the random number\n        generation for data set generation.\n    Returns\n    -------\n    (X, y):  SearchStrategy[array], SearchStrategy[array]\n        A tuple of search strategies for arrays subject to the constraints of\n        the provided parameters.\n    \"\"\"\n    n_features_ = draw(n_features)\n    if n_informative is None:\n        try:\n            # Try to meet:\n            #   log_2(n_classes * n_clusters_per_class) <= n_informative\n            n_classes_min = min(_get_limits(n_classes))\n            n_clusters_per_class_min = min(_get_limits(n_clusters_per_class))\n            n_informative_min = int(\n                np.ceil(np.log2(n_classes_min * n_clusters_per_class_min))\n            )\n        except AttributeError:\n            # Otherwise aim for 10% of n_features, but at least 1.\n            n_informative_min = max(1, int(0.1 * n_features_))\n\n        n_informative = just(min(n_features_, n_informative_min))\n    if n_redundant is None:\n        n_redundant = just(max(min(n_features_, 1), int(0.1 * n_features_)))\n\n    # Check whether the\n    #   log_2(n_classes * n_clusters_per_class) <= n_informative\n    # inequality can in principle be met.\n    try:\n        n_classes_min = min(_get_limits(n_classes))\n        n_clusters_per_class_min = min(_get_limits(n_clusters_per_class))\n        n_informative_max = max(_get_limits(n_informative))\n    except AttributeError:\n        pass  # unable to determine limits\n    else:\n        if np.log2(n_classes_min * n_clusters_per_class_min) > n_informative_max:\n            raise ValueError(\n                \"Assumptions cannot be met, the following inequality must \"\n                \"hold: log_2(n_classes * n_clusters_per_class) \"\n                \"<= n_informative .\"\n            )\n\n    # Check base assumption concerning the composition of feature vectors.\n    n_informative_ = draw(n_informative)\n    n_redundant_ = draw(n_redundant)\n    n_repeated_ = draw(n_repeated)\n    assume(n_informative_ + n_redundant_ + n_repeated_ <= n_features_)\n\n    # Check base assumption concerning relationship of number of clusters and\n    # informative features.\n    n_classes_ = draw(n_classes)\n    n_clusters_per_class_ = draw(n_clusters_per_class)\n    assume(np.log2(n_classes_ * n_clusters_per_class_) <= n_informative_)\n\n    X, y = make_classification(\n        n_samples=draw(n_samples),\n        n_features=n_features_,\n        n_informative=n_informative_,\n        n_redundant=n_redundant_,\n        n_repeated=n_repeated_,\n        n_classes=n_classes_,\n        n_clusters_per_class=n_clusters_per_class_,\n        weights=draw(weights),\n        flip_y=draw(flip_y),\n        class_sep=draw(class_sep),\n        hypercube=draw(hypercube),\n        shift=draw(shift),\n        scale=draw(scale),\n        shuffle=draw(shuffle),\n        random_state=random_state,\n    )\n    return X.astype(np.float32), y.astype(np.int32)", "\n\n@composite\ndef standard_regression_datasets(\n    draw,\n    n_samples=integers(min_value=100, max_value=200),\n    n_features=integers(min_value=100, max_value=200),\n    *,\n    n_informative=None,\n    n_targets=just(1),\n    bias=just(0.0),\n    effective_rank=none(),\n    tail_strength=just(0.5),\n    noise=just(0.0),\n    shuffle=just(True),\n    random_state=None,\n):\n    \"\"\"\n    Returns a strategy to generate regression problem input datasets.\n    Note:\n    This function uses the sklearn.datasets.make_regression function to\n    generate the regression problem from the provided search strategies.\n\n    Credit: Carl Simon Adorf (@csadorf)\n    https://github.com/rapidsai/cuml/blob/447bded/python/cuml/testing/strategies.py\n\n    Parameters\n    ----------\n    draw:\n        Callback function, to be used internally by Hypothesis\n    n_samples: SearchStrategy[int]\n        Returned arrays will have number of rows drawn from these values.\n    n_features: SearchStrategy[int]\n        Returned arrays will have number of columns drawn from these values.\n    n_informative: SearchStrategy[int], default=none\n        A search strategy for the number of informative features. If none,\n        will use 10% of the actual number of features, but not less than 1\n        unless the number of features is zero.\n    n_targets: SearchStrategy[int], default=just(1)\n        A search strategy for the number of targets, that means the number of\n        columns of the returned y output array.\n    bias: SearchStrategy[float], default=just(0.0)\n        A search strategy for the bias term.\n    effective_rank:\n        If not None, a search strategy for the effective rank of the input data\n        for the regression problem. See sklearn.dataset.make_regression() for a\n        detailed explanation of this parameter.\n    tail_strength: SearchStrategy[float], default=just(0.5)\n        See sklearn.dataset.make_regression() for a detailed explanation of\n        this parameter.\n    noise: SearchStrategy[float], default=just(0.0)\n        A search strategy for the standard deviation of the gaussian noise.\n    shuffle: SearchStrategy[bool], default=just(True)\n        A boolean search strategy to determine whether samples and features\n        are shuffled.\n    random_state: int, RandomState instance or None, default=None\n        Pass a random state or integer to determine the random number\n        generation for data set generation.\n    Returns\n    -------\n    (X, y):  SearchStrategy[array], SearchStrategy[array]\n        A tuple of search strategies for arrays subject to the constraints of\n        the provided parameters.\n    \"\"\"\n    n_features_ = draw(n_features)\n    if n_informative is None:\n        n_informative = just(max(min(n_features_, 1), int(0.1 * n_features_)))\n    # pylint: disable=unbalanced-tuple-unpacking\n    X, y = make_regression(\n        n_samples=draw(n_samples),\n        n_features=n_features_,\n        n_informative=draw(n_informative),\n        n_targets=draw(n_targets),\n        bias=draw(bias),\n        effective_rank=draw(effective_rank),\n        tail_strength=draw(tail_strength),\n        noise=draw(noise),\n        shuffle=draw(shuffle),\n        random_state=random_state,\n    )\n    return X.astype(np.float32), y.astype(np.float32)", "\n\ndef standard_settings():\n    \"\"\"Default hypothesis settings. Set a smaller max_examples on Windows\"\"\"\n    kwargs = {\n        \"deadline\": None,\n        \"max_examples\": 20,\n        \"print_blob\": True,\n    }\n    if _platform == \"win32\":\n        kwargs[\"max_examples\"] = 3\n    return kwargs", ""]}
{"filename": "tests/python/__init__.py", "chunked_list": [""]}
{"filename": "tests/python/test_code_folding.py", "chunked_list": ["\"\"\"Tests for reading/writing Protocol Buffers\"\"\"\nimport itertools\nimport os\n\nimport pytest\n\nimport tl2cgen\n\nfrom .metadata import format_libpath_for_example_model, load_example_model\nfrom .util import check_predictor, os_compatible_toolchains", "from .metadata import format_libpath_for_example_model, load_example_model\nfrom .util import check_predictor, os_compatible_toolchains\n\n\n@pytest.mark.parametrize(\"code_folding_factor\", [0.0, 1.0, 2.0, 3.0])\n@pytest.mark.parametrize(\n    \"dataset,toolchain\",\n    list(\n        itertools.product(\n            [\"dermatology\", \"toy_categorical\"], os_compatible_toolchains()", "        itertools.product(\n            [\"dermatology\", \"toy_categorical\"], os_compatible_toolchains()\n        )\n    ),\n)\ndef test_code_folding(tmpdir, annotation, dataset, toolchain, code_folding_factor):\n    \"\"\"Test suite for testing code folding feature\"\"\"\n    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n    model = load_example_model(dataset)\n    annotation_path = os.path.join(tmpdir, \"annotation.json\")\n\n    if annotation[dataset] is None:\n        annotation_path = None\n    else:\n        with open(annotation_path, \"w\", encoding=\"UTF-8\") as f:\n            f.write(annotation[dataset])\n\n    params = {\n        \"annotate_in\": (annotation_path if annotation_path else \"NULL\"),\n        \"quantize\": 1,\n        \"parallel_comp\": model.num_tree,\n        \"code_folding_req\": code_folding_factor,\n    }\n\n    tl2cgen.export_lib(\n        model, toolchain=toolchain, libpath=libpath, params=params, verbose=True\n    )\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    check_predictor(predictor, dataset)", ""]}
{"filename": "tests/python/test_invalid_categorical_input.py", "chunked_list": ["\"\"\"Test whether TL2cgen handles invalid category values correctly\"\"\"\nimport pathlib\n\nimport numpy as np\nimport pytest\nimport treelite\n\nimport tl2cgen\nfrom tl2cgen.contrib.util import _libext\n", "from tl2cgen.contrib.util import _libext\n\nfrom .util import os_compatible_toolchains\n\n\n@pytest.fixture(name=\"toy_model\")\ndef toy_model_fixture():\n    \"\"\"A toy model with a single tree containing a categorical split\"\"\"\n    builder = treelite.ModelBuilder(num_feature=2)\n    tree = treelite.ModelBuilder.Tree()\n    tree[0].set_categorical_test_node(\n        feature_id=1,\n        left_categories=[0],\n        default_left=True,\n        left_child_key=1,\n        right_child_key=2,\n    )\n    tree[1].set_leaf_node(-1.0)\n    tree[2].set_leaf_node(1.0)\n    tree[0].set_root()\n    builder.append(tree)\n    model = builder.commit()\n\n    return model", "\n\n@pytest.fixture(name=\"test_data\")\ndef test_data_fixture():\n    \"\"\"Generate test data consisting of two columns\"\"\"\n    categorical_column = np.array(\n        [-1, -0.6, -0.5, 0, 0.3, 0.7, 1, np.nan, np.inf, 1e10, -1e10], dtype=np.float32\n    )\n    dummy_column = np.zeros(categorical_column.shape[0], dtype=np.float32)\n    return np.column_stack((dummy_column, categorical_column))", "\n\n@pytest.fixture(name=\"ref_pred\")\ndef ref_pred_fixture():\n    \"\"\"Return correct output for the test data\"\"\"\n    # Negative inputs are mapped to the right child node\n    # 0.3 and 0.7 are mapped to the left child node, since they get rounded toward the zero.\n    # Missing value gets mapped to the left child node, since default_left=True\n    # inf, 1e10, and -1e10 don't match any element of left_categories, so they get mapped to the\n    # right child.\n    return np.array([1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1], dtype=np.float32).reshape(\n        (-1, 1)\n    )", "\n\ndef test_invalid_categorical_input(tmpdir, toy_model, test_data, ref_pred):\n    \"\"\"Test whether TL2cgen handles invalid category values correctly\"\"\"\n    libpath = pathlib.Path(tmpdir).joinpath(\"mylib\" + _libext())\n    toolchain = os_compatible_toolchains()[0]\n    tl2cgen.export_lib(toy_model, toolchain=toolchain, libpath=libpath)\n\n    predictor = tl2cgen.Predictor(libpath=libpath)\n    dmat = tl2cgen.DMatrix(test_data)\n    pred = predictor.predict(dmat)\n    np.testing.assert_equal(pred, ref_pred)", ""]}
{"filename": "tests/python/test_sklearn_integration.py", "chunked_list": ["\"\"\"Tests for scikit-learn importer\"\"\"\nimport os\nimport pathlib\n\nimport numpy as np\nimport pytest\nimport treelite\nfrom hypothesis import given, settings\nfrom hypothesis.strategies import sampled_from\nfrom sklearn.datasets import load_breast_cancer, load_iris", "from hypothesis.strategies import sampled_from\nfrom sklearn.datasets import load_breast_cancer, load_iris\nfrom sklearn.ensemble import (\n    ExtraTreesClassifier,\n    ExtraTreesRegressor,\n    GradientBoostingClassifier,\n    GradientBoostingRegressor,\n    IsolationForest,\n    RandomForestClassifier,\n    RandomForestRegressor,", "    RandomForestClassifier,\n    RandomForestRegressor,\n)\n\nimport tl2cgen\nfrom tl2cgen.contrib.util import _libext\n\nfrom .hypothesis_util import standard_regression_datasets, standard_settings\nfrom .util import TemporaryDirectory, os_compatible_toolchains\n", "from .util import TemporaryDirectory, os_compatible_toolchains\n\n\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n@pytest.mark.parametrize(\n    \"clazz\", [RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier]\n)\ndef test_skl_converter_multiclass_classifier(tmpdir, clazz, toolchain):\n    # pylint: disable=too-many-locals\n    \"\"\"Convert scikit-learn multi-class classifier\"\"\"\n    X, y = load_iris(return_X_y=True)\n    kwargs = {}\n    if clazz == GradientBoostingClassifier:\n        kwargs[\"init\"] = \"zero\"\n    clf = clazz(max_depth=3, random_state=0, n_estimators=10, **kwargs)\n    clf.fit(X, y)\n    expected_prob = clf.predict_proba(X)\n\n    model = treelite.sklearn.import_model(clf)\n    assert model.num_feature == clf.n_features_in_\n    assert model.num_class == clf.n_classes_\n    assert model.num_tree == clf.n_estimators * (\n        clf.n_classes_ if clazz == GradientBoostingClassifier else 1\n    )\n\n    dtrain = tl2cgen.DMatrix(X, dtype=\"float64\")\n    annotation_path = pathlib.Path(tmpdir) / \"annotation.json\"\n    tl2cgen.annotate_branch(model, dtrain, path=annotation_path, verbose=True)\n\n    libpath = pathlib.Path(tmpdir) / (\"skl\" + _libext())\n    tl2cgen.export_lib(\n        model,\n        toolchain=toolchain,\n        libpath=libpath,\n        params={\"annotate_in\": annotation_path},\n        verbose=True,\n    )\n    predictor = tl2cgen.Predictor(libpath=libpath)\n    assert predictor.num_feature == clf.n_features_in_\n    assert predictor.num_class == clf.n_classes_\n    assert predictor.pred_transform == (\n        \"softmax\" if clazz == GradientBoostingClassifier else \"identity_multiclass\"\n    )\n    assert predictor.global_bias == 0.0\n    assert predictor.sigmoid_alpha == 1.0\n    out_prob = predictor.predict(dtrain)\n    np.testing.assert_almost_equal(out_prob, expected_prob)", "\n\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\n@pytest.mark.parametrize(\n    \"clazz\", [RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier]\n)\ndef test_skl_converter_binary_classifier(tmpdir, clazz, toolchain):\n    # pylint: disable=too-many-locals\n    \"\"\"Convert scikit-learn binary classifier\"\"\"\n    X, y = load_breast_cancer(return_X_y=True)\n    kwargs = {}\n    if clazz == GradientBoostingClassifier:\n        kwargs[\"init\"] = \"zero\"\n    clf = clazz(max_depth=3, random_state=0, n_estimators=10, **kwargs)\n    clf.fit(X, y)\n    expected_prob = clf.predict_proba(X)[:, 1:]\n\n    model = treelite.sklearn.import_model(clf)\n    assert model.num_feature == clf.n_features_in_\n    assert model.num_class == 1\n    assert model.num_tree == clf.n_estimators\n\n    dtrain = tl2cgen.DMatrix(X, dtype=\"float64\")\n    libpath = pathlib.Path(tmpdir) / (\"skl\" + _libext())\n    tl2cgen.export_lib(\n        model,\n        toolchain=toolchain,\n        libpath=libpath,\n        verbose=True,\n    )\n    predictor = tl2cgen.Predictor(libpath=libpath)\n    assert predictor.num_feature == clf.n_features_in_\n    assert model.num_class == 1\n    assert predictor.pred_transform == (\n        \"sigmoid\" if clazz == GradientBoostingClassifier else \"identity\"\n    )\n    assert predictor.global_bias == 0.0\n    assert predictor.sigmoid_alpha == 1.0\n    out_prob = predictor.predict(dtrain)\n    np.testing.assert_almost_equal(out_prob, expected_prob)", "\n\n@given(\n    toolchain=sampled_from(os_compatible_toolchains()),\n    clazz=sampled_from(\n        [RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor]\n    ),\n    dataset=standard_regression_datasets(),\n)\n@settings(**standard_settings())\ndef test_skl_converter_regressor(toolchain, clazz, dataset):\n    # pylint: disable=too-many-locals\n    \"\"\"Convert scikit-learn regressor\"\"\"\n    X, y = dataset\n    kwargs = {}\n    if clazz == GradientBoostingRegressor:\n        kwargs[\"init\"] = \"zero\"\n    clf = clazz(max_depth=3, random_state=0, n_estimators=10, **kwargs)\n    clf.fit(X, y)\n    expected_pred = clf.predict(X).reshape((X.shape[0], -1))\n\n    model = treelite.sklearn.import_model(clf)\n    assert model.num_feature == clf.n_features_in_\n    assert model.num_class == 1\n    assert model.num_tree == clf.n_estimators\n\n    with TemporaryDirectory() as tmpdir:\n        dtrain = tl2cgen.DMatrix(X, dtype=\"float32\")\n        libpath = os.path.join(tmpdir, \"skl\" + _libext())\n        tl2cgen.export_lib(\n            model,\n            toolchain=toolchain,\n            libpath=libpath,\n            verbose=True,\n        )\n        predictor = tl2cgen.Predictor(libpath=libpath)\n        assert predictor.num_feature == clf.n_features_in_\n        assert model.num_class == 1\n        assert predictor.pred_transform == \"identity\"\n        assert predictor.global_bias == 0.0\n        assert predictor.sigmoid_alpha == 1.0\n        out_pred = predictor.predict(dtrain)\n        np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)", ")\n@settings(**standard_settings())\ndef test_skl_converter_regressor(toolchain, clazz, dataset):\n    # pylint: disable=too-many-locals\n    \"\"\"Convert scikit-learn regressor\"\"\"\n    X, y = dataset\n    kwargs = {}\n    if clazz == GradientBoostingRegressor:\n        kwargs[\"init\"] = \"zero\"\n    clf = clazz(max_depth=3, random_state=0, n_estimators=10, **kwargs)\n    clf.fit(X, y)\n    expected_pred = clf.predict(X).reshape((X.shape[0], -1))\n\n    model = treelite.sklearn.import_model(clf)\n    assert model.num_feature == clf.n_features_in_\n    assert model.num_class == 1\n    assert model.num_tree == clf.n_estimators\n\n    with TemporaryDirectory() as tmpdir:\n        dtrain = tl2cgen.DMatrix(X, dtype=\"float32\")\n        libpath = os.path.join(tmpdir, \"skl\" + _libext())\n        tl2cgen.export_lib(\n            model,\n            toolchain=toolchain,\n            libpath=libpath,\n            verbose=True,\n        )\n        predictor = tl2cgen.Predictor(libpath=libpath)\n        assert predictor.num_feature == clf.n_features_in_\n        assert model.num_class == 1\n        assert predictor.pred_transform == \"identity\"\n        assert predictor.global_bias == 0.0\n        assert predictor.sigmoid_alpha == 1.0\n        out_pred = predictor.predict(dtrain)\n        np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)", "\n\n@given(\n    toolchain=sampled_from(os_compatible_toolchains()),\n    dataset=standard_regression_datasets(),\n)\n@settings(**standard_settings())\ndef test_skl_converter_iforest(toolchain, dataset):  # pylint: disable=W0212\n    # pylint: disable=too-many-locals\n    \"\"\"Convert scikit-learn Isolation forest\"\"\"\n    X, _ = dataset\n    clf = IsolationForest(max_samples=64, random_state=0, n_estimators=10)\n    clf.fit(X)\n    expected_pred = clf._compute_chunked_score_samples(X)  # pylint: disable=W0212\n    expected_pred = expected_pred.reshape((X.shape[0], -1))\n\n    model = treelite.sklearn.import_model(clf)\n    assert model.num_feature == clf.n_features_in_\n    assert model.num_class == 1\n    assert model.num_tree == clf.n_estimators\n\n    with TemporaryDirectory() as tmpdir:\n        dtrain = tl2cgen.DMatrix(X, dtype=\"float32\")\n        annotation_path = os.path.join(tmpdir, \"annotation.json\")\n        tl2cgen.annotate_branch(model, dmat=dtrain, path=annotation_path, verbose=True)\n\n        libpath = os.path.join(tmpdir, \"skl\" + _libext())\n        tl2cgen.export_lib(\n            model,\n            toolchain=toolchain,\n            libpath=libpath,\n            params={\"annotate_in\": annotation_path},\n            verbose=True,\n        )\n        predictor = tl2cgen.Predictor(libpath=libpath)\n        assert predictor.num_feature == clf.n_features_in_\n        assert model.num_class == 1\n        assert predictor.pred_transform == \"exponential_standard_ratio\"\n        assert predictor.global_bias == 0.0\n        assert predictor.sigmoid_alpha == 1.0\n        assert predictor.ratio_c > 0\n        assert predictor.ratio_c < 64\n        out_pred = predictor.predict(dtrain)\n        np.testing.assert_almost_equal(out_pred, expected_pred, decimal=2)", ""]}
{"filename": "tests/python/util.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\"\"\"Utility functions for tests\"\"\"\nimport os\nimport tempfile\nfrom contextlib import contextmanager\nfrom sys import platform as _platform\nfrom typing import Iterator, List, Optional, Tuple\n\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file", "import numpy as np\nfrom sklearn.datasets import load_svmlight_file\n\nimport tl2cgen\nfrom tl2cgen.contrib.util import _libext\n\nfrom .metadata import example_model_db\n\n\ndef load_txt(filename: str) -> Optional[np.ndarray]:\n    \"\"\"Get 1D array from text file\"\"\"\n    if filename is None:\n        return None\n    content = []\n    with open(filename, \"r\", encoding=\"UTF-8\") as f:\n        for line in f:\n            content.append(float(line))\n    return np.array(content, dtype=np.float32)", "\ndef load_txt(filename: str) -> Optional[np.ndarray]:\n    \"\"\"Get 1D array from text file\"\"\"\n    if filename is None:\n        return None\n    content = []\n    with open(filename, \"r\", encoding=\"UTF-8\") as f:\n        for line in f:\n            content.append(float(line))\n    return np.array(content, dtype=np.float32)", "\n\ndef os_compatible_toolchains() -> List[str]:\n    \"\"\"Get the list of C compilers to test with the current OS\"\"\"\n    if _platform == \"darwin\":\n        gcc = os.environ.get(\"GCC_PATH\", \"gcc\")\n        toolchains = [gcc]\n    elif _platform == \"win32\":\n        toolchains = [\"msvc\"]\n    else:\n        toolchains = [\"gcc\", \"clang\"]\n    return toolchains", "\n\ndef os_platform() -> str:\n    \"\"\"Detect OS that's running this program\"\"\"\n    if _platform == \"darwin\":\n        return \"osx\"\n    if _platform in [\"win32\", \"cygwin\"]:\n        return \"windows\"\n    return \"unix\"\n", "\n\ndef libname(fmt: str) -> str:\n    \"\"\"Format name for a shared library, using appropriate file extension\"\"\"\n    return fmt.format(_libext())\n\n\n@contextmanager\ndef does_not_raise() -> Iterator[None]:\n    \"\"\"Placeholder to indicate that a section of code is not expected to raise any exception\"\"\"\n    yield", "def does_not_raise() -> Iterator[None]:\n    \"\"\"Placeholder to indicate that a section of code is not expected to raise any exception\"\"\"\n    yield\n\n\ndef has_pandas():\n    \"\"\"Check whether pandas is available\"\"\"\n    try:\n        import pandas  # pylint: disable=unused-import,import-outside-toplevel\n\n        return True\n    except ImportError:\n        return False", "\n\ndef check_predictor(predictor: tl2cgen.Predictor, dataset: str) -> None:\n    \"\"\"Check whether a predictor produces correct predictions for a given dataset\"\"\"\n    dmat = tl2cgen.DMatrix(\n        load_svmlight_file(example_model_db[dataset].dtest, zero_based=True)[0],\n        dtype=example_model_db[dataset].dtype,\n    )\n    out_margin = predictor.predict(dmat, pred_margin=True)\n    out_prob = predictor.predict(dmat)\n    check_predictor_output(dataset, dmat.shape, out_margin, out_prob)", "\n\ndef check_predictor_output(\n    dataset: str, shape: Tuple[int, ...], out_margin: np.ndarray, out_prob: np.ndarray\n) -> None:\n    \"\"\"Check whether a predictor produces correct predictions\"\"\"\n    expected_margin = load_txt(example_model_db[dataset].expected_margin)\n    if expected_margin is not None:\n        if example_model_db[dataset].is_multiclass:\n            expected_margin = expected_margin.reshape((shape[0], -1))\n        else:\n            expected_margin = expected_margin.reshape((-1, 1))\n        assert (\n            out_margin.shape == expected_margin.shape\n        ), f\"out_margin.shape = {out_margin.shape}, expected_margin.shape = {expected_margin.shape}\"\n        np.testing.assert_almost_equal(out_margin, expected_margin, decimal=5)\n\n    expected_prob = load_txt(example_model_db[dataset].expected_prob)\n    if expected_prob is not None:\n        if example_model_db[dataset].is_multiclass:\n            expected_prob = expected_prob.reshape((shape[0], -1))\n        else:\n            expected_prob = expected_prob.reshape((-1, 1))\n        np.testing.assert_almost_equal(out_prob, expected_prob, decimal=5)", "\n\n@contextmanager\ndef TemporaryDirectory(*args, **kwargs) -> Iterator[str]:\n    # pylint: disable=C0103\n    \"\"\"\n    Simulate the effect of 'ignore_cleanup_errors' parameter of tempfile.TemporaryDirectory.\n    The parameter is only available for Python >= 3.10.\n    \"\"\"\n    if \"PYTEST_TMPDIR\" in os.environ and \"dir\" not in kwargs:\n        kwargs[\"dir\"] = os.environ[\"PYTEST_TMPDIR\"]\n    tmpdir = tempfile.TemporaryDirectory(*args, **kwargs)\n    try:\n        yield tmpdir.name\n    finally:\n        try:\n            tmpdir.cleanup()\n        except (PermissionError, NotADirectoryError):\n            if _platform != \"win32\":\n                raise", ""]}
{"filename": "tests/python/test_xgboost_integration.py", "chunked_list": ["\"\"\"Tests for XGBoost integration\"\"\"\n# pylint: disable=R0201, R0915\nimport math\nimport os\n\nimport numpy as np\nimport pytest\nimport treelite\nfrom hypothesis import assume, given, settings\nfrom hypothesis.strategies import integers, sampled_from", "from hypothesis import assume, given, settings\nfrom hypothesis.strategies import integers, sampled_from\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nimport tl2cgen\nfrom tl2cgen.contrib.util import _libext\n\nfrom .hypothesis_util import standard_regression_datasets, standard_settings\nfrom .metadata import format_libpath_for_example_model, load_example_model", "from .hypothesis_util import standard_regression_datasets, standard_settings\nfrom .metadata import format_libpath_for_example_model, load_example_model\nfrom .util import TemporaryDirectory, check_predictor, os_compatible_toolchains\n\ntry:\n    import xgboost as xgb\nexcept ImportError:\n    # skip this test suite if XGBoost is not installed\n    pytest.skip(\"XGBoost not installed; skipping\", allow_module_level=True)\n", "\n\n@given(\n    toolchain=sampled_from(os_compatible_toolchains()),\n    objective=sampled_from(\n        [\n            \"reg:linear\",\n            \"reg:squarederror\",\n            \"reg:squaredlogerror\",\n            \"reg:pseudohubererror\",", "            \"reg:squaredlogerror\",\n            \"reg:pseudohubererror\",\n        ]\n    ),\n    model_format=sampled_from([\"binary\", \"json\"]),\n    num_parallel_tree=integers(min_value=1, max_value=10),\n    dataset=standard_regression_datasets(),\n)\n@settings(**standard_settings())\ndef test_xgb_regression(toolchain, objective, model_format, num_parallel_tree, dataset):\n    # pylint: disable=too-many-locals\n    \"\"\"Test a random regression dataset\"\"\"\n    X, y = dataset\n    if objective == \"reg:squaredlogerror\":\n        assume(np.all(y > -1))\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, shuffle=False\n    )\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dtest = xgb.DMatrix(X_test, label=y_test)\n    param = {\n        \"max_depth\": 8,\n        \"eta\": 1,\n        \"verbosity\": 0,\n        \"objective\": objective,\n        \"num_parallel_tree\": num_parallel_tree,\n    }\n    num_round = 10\n    bst = xgb.train(\n        param,\n        dtrain,\n        num_boost_round=num_round,\n        evals=[(dtrain, \"train\"), (dtest, \"test\")],\n    )\n    with TemporaryDirectory() as tmpdir:\n        if model_format == \"json\":\n            model_name = \"regression.json\"\n            model_path = os.path.join(tmpdir, model_name)\n            bst.save_model(model_path)\n            model = treelite.Model.load(\n                filename=model_path, model_format=\"xgboost_json\"\n            )\n        else:\n            model_name = \"regression.model\"\n            model_path = os.path.join(tmpdir, model_name)\n            bst.save_model(model_path)\n            model = treelite.Model.load(filename=model_path, model_format=\"xgboost\")\n\n        assert model.num_feature == dtrain.num_col()\n        assert model.num_class == 1\n        assert model.num_tree == num_round * num_parallel_tree\n        libpath = os.path.join(tmpdir, \"regression\" + _libext())\n        tl2cgen.export_lib(\n            model,\n            toolchain=toolchain,\n            libpath=libpath,\n            params={\"parallel_comp\": model.num_tree},\n            verbose=True,\n        )\n\n        predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n        assert predictor.num_feature == dtrain.num_col()\n        assert predictor.num_class == 1\n        assert predictor.pred_transform == \"identity\"\n        assert predictor.global_bias == 0.5\n        assert predictor.sigmoid_alpha == 1.0\n        dmat = tl2cgen.DMatrix(X_test, dtype=\"float32\")\n        out_pred = predictor.predict(dmat)\n        expected_pred = bst.predict(dtest, strict_shape=True)\n        np.testing.assert_almost_equal(out_pred, expected_pred, decimal=3)", "@settings(**standard_settings())\ndef test_xgb_regression(toolchain, objective, model_format, num_parallel_tree, dataset):\n    # pylint: disable=too-many-locals\n    \"\"\"Test a random regression dataset\"\"\"\n    X, y = dataset\n    if objective == \"reg:squaredlogerror\":\n        assume(np.all(y > -1))\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, shuffle=False\n    )\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dtest = xgb.DMatrix(X_test, label=y_test)\n    param = {\n        \"max_depth\": 8,\n        \"eta\": 1,\n        \"verbosity\": 0,\n        \"objective\": objective,\n        \"num_parallel_tree\": num_parallel_tree,\n    }\n    num_round = 10\n    bst = xgb.train(\n        param,\n        dtrain,\n        num_boost_round=num_round,\n        evals=[(dtrain, \"train\"), (dtest, \"test\")],\n    )\n    with TemporaryDirectory() as tmpdir:\n        if model_format == \"json\":\n            model_name = \"regression.json\"\n            model_path = os.path.join(tmpdir, model_name)\n            bst.save_model(model_path)\n            model = treelite.Model.load(\n                filename=model_path, model_format=\"xgboost_json\"\n            )\n        else:\n            model_name = \"regression.model\"\n            model_path = os.path.join(tmpdir, model_name)\n            bst.save_model(model_path)\n            model = treelite.Model.load(filename=model_path, model_format=\"xgboost\")\n\n        assert model.num_feature == dtrain.num_col()\n        assert model.num_class == 1\n        assert model.num_tree == num_round * num_parallel_tree\n        libpath = os.path.join(tmpdir, \"regression\" + _libext())\n        tl2cgen.export_lib(\n            model,\n            toolchain=toolchain,\n            libpath=libpath,\n            params={\"parallel_comp\": model.num_tree},\n            verbose=True,\n        )\n\n        predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n        assert predictor.num_feature == dtrain.num_col()\n        assert predictor.num_class == 1\n        assert predictor.pred_transform == \"identity\"\n        assert predictor.global_bias == 0.5\n        assert predictor.sigmoid_alpha == 1.0\n        dmat = tl2cgen.DMatrix(X_test, dtype=\"float32\")\n        out_pred = predictor.predict(dmat)\n        expected_pred = bst.predict(dtest, strict_shape=True)\n        np.testing.assert_almost_equal(out_pred, expected_pred, decimal=3)", "\n\n@pytest.mark.parametrize(\"num_parallel_tree\", [1, 3, 5])\n@pytest.mark.parametrize(\"model_format\", [\"binary\", \"json\"])\n@pytest.mark.parametrize(\n    \"objective,expected_pred_transform\",\n    [(\"multi:softmax\", \"max_index\"), (\"multi:softprob\", \"softmax\")],\n    ids=[\"multi:softmax\", \"multi:softprob\"],\n)\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\ndef test_xgb_iris(\n    tmpdir,\n    toolchain,\n    objective,\n    model_format,\n    expected_pred_transform,\n    num_parallel_tree,\n):\n    # pylint: disable=too-many-locals, too-many-arguments\n    \"\"\"Test Iris data (multi-class classification)\"\"\"\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, shuffle=False\n    )\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dtest = xgb.DMatrix(X_test, label=y_test)\n    num_class = 3\n    num_round = 10\n    param = {\n        \"max_depth\": 6,\n        \"eta\": 0.05,\n        \"num_class\": num_class,\n        \"verbosity\": 0,\n        \"objective\": objective,\n        \"metric\": \"mlogloss\",\n        \"num_parallel_tree\": num_parallel_tree,\n    }\n    bst = xgb.train(\n        param,\n        dtrain,\n        num_boost_round=num_round,\n        evals=[(dtrain, \"train\"), (dtest, \"test\")],\n    )\n\n    if model_format == \"json\":\n        model_name = \"iris.json\"\n        model_path = os.path.join(tmpdir, model_name)\n        bst.save_model(model_path)\n        model = treelite.Model.load(filename=model_path, model_format=\"xgboost_json\")\n    else:\n        model_name = \"iris.model\"\n        model_path = os.path.join(tmpdir, model_name)\n        bst.save_model(model_path)\n        model = treelite.Model.load(filename=model_path, model_format=\"xgboost\")\n    assert model.num_feature == dtrain.num_col()\n    assert model.num_class == num_class\n    assert model.num_tree == num_round * num_class * num_parallel_tree\n    libpath = os.path.join(tmpdir, \"iris\" + _libext())\n    tl2cgen.export_lib(\n        model, toolchain=toolchain, libpath=libpath, params={}, verbose=True\n    )\n\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    assert predictor.num_feature == dtrain.num_col()\n    assert predictor.num_class == num_class\n    assert predictor.pred_transform == expected_pred_transform\n    assert predictor.global_bias == 0.5\n    assert predictor.sigmoid_alpha == 1.0\n    dmat = tl2cgen.DMatrix(X_test, dtype=\"float32\")\n    out_pred = predictor.predict(dmat)\n    expected_pred = bst.predict(dtest, strict_shape=True)\n    np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)", ")\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\ndef test_xgb_iris(\n    tmpdir,\n    toolchain,\n    objective,\n    model_format,\n    expected_pred_transform,\n    num_parallel_tree,\n):\n    # pylint: disable=too-many-locals, too-many-arguments\n    \"\"\"Test Iris data (multi-class classification)\"\"\"\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, shuffle=False\n    )\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dtest = xgb.DMatrix(X_test, label=y_test)\n    num_class = 3\n    num_round = 10\n    param = {\n        \"max_depth\": 6,\n        \"eta\": 0.05,\n        \"num_class\": num_class,\n        \"verbosity\": 0,\n        \"objective\": objective,\n        \"metric\": \"mlogloss\",\n        \"num_parallel_tree\": num_parallel_tree,\n    }\n    bst = xgb.train(\n        param,\n        dtrain,\n        num_boost_round=num_round,\n        evals=[(dtrain, \"train\"), (dtest, \"test\")],\n    )\n\n    if model_format == \"json\":\n        model_name = \"iris.json\"\n        model_path = os.path.join(tmpdir, model_name)\n        bst.save_model(model_path)\n        model = treelite.Model.load(filename=model_path, model_format=\"xgboost_json\")\n    else:\n        model_name = \"iris.model\"\n        model_path = os.path.join(tmpdir, model_name)\n        bst.save_model(model_path)\n        model = treelite.Model.load(filename=model_path, model_format=\"xgboost\")\n    assert model.num_feature == dtrain.num_col()\n    assert model.num_class == num_class\n    assert model.num_tree == num_round * num_class * num_parallel_tree\n    libpath = os.path.join(tmpdir, \"iris\" + _libext())\n    tl2cgen.export_lib(\n        model, toolchain=toolchain, libpath=libpath, params={}, verbose=True\n    )\n\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    assert predictor.num_feature == dtrain.num_col()\n    assert predictor.num_class == num_class\n    assert predictor.pred_transform == expected_pred_transform\n    assert predictor.global_bias == 0.5\n    assert predictor.sigmoid_alpha == 1.0\n    dmat = tl2cgen.DMatrix(X_test, dtype=\"float32\")\n    out_pred = predictor.predict(dmat)\n    expected_pred = bst.predict(dtest, strict_shape=True)\n    np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)", "\n\n@pytest.mark.parametrize(\"model_format\", [\"binary\", \"json\"])\n@pytest.mark.parametrize(\n    \"objective,max_label,expected_global_bias\",\n    [\n        (\"binary:logistic\", 2, 0),\n        (\"binary:hinge\", 2, 0.5),\n        (\"binary:logitraw\", 2, 0.5),\n        (\"count:poisson\", 4, math.log(0.5)),", "        (\"binary:logitraw\", 2, 0.5),\n        (\"count:poisson\", 4, math.log(0.5)),\n        (\"rank:pairwise\", 5, 0.5),\n        (\"rank:ndcg\", 5, 0.5),\n        (\"rank:map\", 5, 0.5),\n    ],\n    ids=[\n        \"binary:logistic\",\n        \"binary:hinge\",\n        \"binary:logitraw\",", "        \"binary:hinge\",\n        \"binary:logitraw\",\n        \"count:poisson\",\n        \"rank:pairwise\",\n        \"rank:ndcg\",\n        \"rank:map\",\n    ],\n)\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\ndef test_nonlinear_objective(\n    tmpdir, objective, max_label, expected_global_bias, toolchain, model_format\n):\n    # pylint: disable=too-many-locals,too-many-arguments\n    \"\"\"Test non-linear objectives with dummy data\"\"\"\n    np.random.seed(0)\n    nrow = 16\n    ncol = 8\n    X = np.random.randn(nrow, ncol)\n    y = np.random.randint(0, max_label, size=nrow)\n    assert np.min(y) == 0\n    assert np.max(y) == max_label - 1\n\n    num_round = 4\n    dtrain = xgb.DMatrix(X, label=y)\n    if objective.startswith(\"rank:\"):\n        dtrain.set_group([nrow])\n    bst = xgb.train(\n        {\"objective\": objective, \"base_score\": 0.5, \"seed\": 0},\n        dtrain=dtrain,\n        num_boost_round=num_round,\n    )\n\n    objective_tag = objective.replace(\":\", \"_\")\n    if model_format == \"json\":\n        model_name = f\"nonlinear_{objective_tag}.json\"\n    else:\n        model_name = f\"nonlinear_{objective_tag}.bin\"\n    model_path = os.path.join(tmpdir, model_name)\n    bst.save_model(model_path)\n\n    model = treelite.Model.load(\n        filename=model_path,\n        model_format=(\"xgboost_json\" if model_format == \"json\" else \"xgboost\"),\n    )\n    assert model.num_feature == dtrain.num_col()\n    assert model.num_class == 1\n    assert model.num_tree == num_round\n    libpath = os.path.join(tmpdir, objective_tag + _libext())\n    tl2cgen.export_lib(\n        model, toolchain=toolchain, libpath=libpath, params={}, verbose=True\n    )\n\n    expected_pred_transform = {\n        \"binary:logistic\": \"sigmoid\",\n        \"binary:hinge\": \"hinge\",\n        \"binary:logitraw\": \"identity\",\n        \"count:poisson\": \"exponential\",\n        \"rank:pairwise\": \"identity\",\n        \"rank:ndcg\": \"identity\",\n        \"rank:map\": \"identity\",\n    }\n\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    assert predictor.num_feature == dtrain.num_col()\n    assert predictor.num_class == 1\n    assert predictor.pred_transform == expected_pred_transform[objective]\n    np.testing.assert_almost_equal(\n        predictor.global_bias, expected_global_bias, decimal=5\n    )\n    assert predictor.sigmoid_alpha == 1.0\n    dmat = tl2cgen.DMatrix(X, dtype=\"float32\")\n    out_pred = predictor.predict(dmat)\n    expected_pred = bst.predict(dtrain, strict_shape=True)\n    np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)", "@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\ndef test_nonlinear_objective(\n    tmpdir, objective, max_label, expected_global_bias, toolchain, model_format\n):\n    # pylint: disable=too-many-locals,too-many-arguments\n    \"\"\"Test non-linear objectives with dummy data\"\"\"\n    np.random.seed(0)\n    nrow = 16\n    ncol = 8\n    X = np.random.randn(nrow, ncol)\n    y = np.random.randint(0, max_label, size=nrow)\n    assert np.min(y) == 0\n    assert np.max(y) == max_label - 1\n\n    num_round = 4\n    dtrain = xgb.DMatrix(X, label=y)\n    if objective.startswith(\"rank:\"):\n        dtrain.set_group([nrow])\n    bst = xgb.train(\n        {\"objective\": objective, \"base_score\": 0.5, \"seed\": 0},\n        dtrain=dtrain,\n        num_boost_round=num_round,\n    )\n\n    objective_tag = objective.replace(\":\", \"_\")\n    if model_format == \"json\":\n        model_name = f\"nonlinear_{objective_tag}.json\"\n    else:\n        model_name = f\"nonlinear_{objective_tag}.bin\"\n    model_path = os.path.join(tmpdir, model_name)\n    bst.save_model(model_path)\n\n    model = treelite.Model.load(\n        filename=model_path,\n        model_format=(\"xgboost_json\" if model_format == \"json\" else \"xgboost\"),\n    )\n    assert model.num_feature == dtrain.num_col()\n    assert model.num_class == 1\n    assert model.num_tree == num_round\n    libpath = os.path.join(tmpdir, objective_tag + _libext())\n    tl2cgen.export_lib(\n        model, toolchain=toolchain, libpath=libpath, params={}, verbose=True\n    )\n\n    expected_pred_transform = {\n        \"binary:logistic\": \"sigmoid\",\n        \"binary:hinge\": \"hinge\",\n        \"binary:logitraw\": \"identity\",\n        \"count:poisson\": \"exponential\",\n        \"rank:pairwise\": \"identity\",\n        \"rank:ndcg\": \"identity\",\n        \"rank:map\": \"identity\",\n    }\n\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    assert predictor.num_feature == dtrain.num_col()\n    assert predictor.num_class == 1\n    assert predictor.pred_transform == expected_pred_transform[objective]\n    np.testing.assert_almost_equal(\n        predictor.global_bias, expected_global_bias, decimal=5\n    )\n    assert predictor.sigmoid_alpha == 1.0\n    dmat = tl2cgen.DMatrix(X, dtype=\"float32\")\n    out_pred = predictor.predict(dmat)\n    expected_pred = bst.predict(dtrain, strict_shape=True)\n    np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)", "\n\n@given(\n    toolchain=sampled_from(os_compatible_toolchains()),\n    dataset=standard_regression_datasets(),\n)\n@settings(**standard_settings())\ndef test_xgb_deserializers(toolchain, dataset):\n    # pylint: disable=too-many-locals\n    \"\"\"Test a random regression dataset and test serializers\"\"\"\n    X, y = dataset\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, shuffle=False\n    )\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dtest = xgb.DMatrix(X_test, label=y_test)\n    param = {\"max_depth\": 8, \"eta\": 1, \"silent\": 1, \"objective\": \"reg:linear\"}\n    num_round = 10\n    bst = xgb.train(\n        param,\n        dtrain,\n        num_boost_round=num_round,\n        evals=[(dtrain, \"train\"), (dtest, \"test\")],\n    )\n\n    with TemporaryDirectory() as tmpdir:\n        # Serialize xgboost model\n        model_bin_path = os.path.join(tmpdir, \"serialized.model\")\n        bst.save_model(model_bin_path)\n        model_json_path = os.path.join(tmpdir, \"serialized.json\")\n        bst.save_model(model_json_path)\n        model_json_str = bst.save_raw(raw_format=\"json\")\n\n        # Construct Treelite models from xgboost serializations\n        model_bin = treelite.Model.load(model_bin_path, model_format=\"xgboost\")\n        model_json = treelite.Model.load(model_json_path, model_format=\"xgboost_json\")\n        model_json_str = treelite.Model.from_xgboost_json(model_json_str)\n\n        # Compile models to libraries\n        libext = _libext()\n        model_bin_lib = os.path.join(tmpdir, f\"bin{libext}\")\n        tl2cgen.export_lib(\n            model_bin,\n            toolchain=toolchain,\n            libpath=model_bin_lib,\n            params={\"parallel_comp\": model_bin.num_tree},\n        )\n        model_json_lib = os.path.join(tmpdir, f\"json{libext}\")\n        tl2cgen.export_lib(\n            model_json,\n            toolchain=toolchain,\n            libpath=model_json_lib,\n            params={\"parallel_comp\": model_json.num_tree},\n        )\n        model_json_str_lib = os.path.join(tmpdir, f\"json_str{libext}\")\n        tl2cgen.export_lib(\n            model_json_str,\n            toolchain=toolchain,\n            libpath=model_json_str_lib,\n            params={\"parallel_comp\": model_json_str.num_tree},\n        )\n\n        # Generate predictors from compiled libraries\n        predictor_bin = tl2cgen.Predictor(model_bin_lib)\n        assert predictor_bin.num_feature == dtrain.num_col()\n        assert predictor_bin.num_class == 1\n        assert predictor_bin.pred_transform == \"identity\"\n        assert predictor_bin.global_bias == pytest.approx(0.5)\n        assert predictor_bin.sigmoid_alpha == pytest.approx(1.0)\n\n        predictor_json = tl2cgen.Predictor(model_json_lib)\n        assert predictor_json.num_feature == dtrain.num_col()\n        assert predictor_json.num_class == 1\n        assert predictor_json.pred_transform == \"identity\"\n        assert predictor_json.global_bias == pytest.approx(0.5)\n        assert predictor_json.sigmoid_alpha == pytest.approx(1.0)\n\n        predictor_json_str = tl2cgen.Predictor(model_json_str_lib)\n        assert predictor_json_str.num_feature == dtrain.num_col()\n        assert predictor_json_str.num_class == 1\n        assert predictor_json_str.pred_transform == \"identity\"\n        assert predictor_json_str.global_bias == pytest.approx(0.5)\n        assert predictor_json_str.sigmoid_alpha == pytest.approx(1.0)\n\n        # Run inference with each predictor\n        dmat = tl2cgen.DMatrix(X_test, dtype=\"float32\")\n        bin_pred = predictor_bin.predict(dmat)\n        json_pred = predictor_json.predict(dmat)\n        json_str_pred = predictor_json_str.predict(dmat)\n\n        expected_pred = bst.predict(dtest, strict_shape=True)\n        np.testing.assert_almost_equal(bin_pred, expected_pred, decimal=4)\n        np.testing.assert_almost_equal(json_pred, expected_pred, decimal=4)\n        np.testing.assert_almost_equal(json_str_pred, expected_pred, decimal=4)", "\n\n@pytest.mark.parametrize(\"parallel_comp\", [None, 5])\n@pytest.mark.parametrize(\"quantize\", [True, False])\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\ndef test_xgb_categorical_split(tmpdir, toolchain, quantize, parallel_comp):\n    \"\"\"Test toy XGBoost model with categorical splits\"\"\"\n    dataset = \"xgb_toy_categorical\"\n    model = load_example_model(dataset)\n    libpath = format_libpath_for_example_model(dataset, prefix=tmpdir)\n\n    params = {\n        \"quantize\": (1 if quantize else 0),\n        \"parallel_comp\": (parallel_comp if parallel_comp else 0),\n    }\n    tl2cgen.export_lib(\n        model, toolchain=toolchain, libpath=libpath, params=params, verbose=True\n    )\n\n    predictor = tl2cgen.Predictor(libpath)\n    check_predictor(predictor, dataset)", "\n\n@pytest.mark.parametrize(\"model_format\", [\"binary\", \"json\"])\n@pytest.mark.parametrize(\"toolchain\", os_compatible_toolchains())\ndef test_xgb_dart(tmpdir, toolchain, model_format):\n    # pylint: disable=too-many-locals,too-many-arguments\n    \"\"\"Test dart booster with dummy data\"\"\"\n    np.random.seed(0)\n    nrow = 16\n    ncol = 8\n    X = np.random.randn(nrow, ncol)\n    y = np.random.randint(0, 2, size=nrow)\n    assert np.min(y) == 0\n    assert np.max(y) == 1\n\n    num_round = 50\n    dtrain = xgb.DMatrix(X, label=y)\n    param = {\n        \"booster\": \"dart\",\n        \"max_depth\": 5,\n        \"learning_rate\": 0.1,\n        \"objective\": \"binary:logistic\",\n        \"sample_type\": \"uniform\",\n        \"normalize_type\": \"tree\",\n        \"rate_drop\": 0.1,\n        \"skip_drop\": 0.5,\n    }\n    bst = xgb.train(param, dtrain=dtrain, num_boost_round=num_round)\n\n    if model_format == \"json\":\n        model_json_path = os.path.join(tmpdir, \"serialized.json\")\n        bst.save_model(model_json_path)\n        model = treelite.Model.load(model_json_path, model_format=\"xgboost_json\")\n    else:\n        model_bin_path = os.path.join(tmpdir, \"serialized.model\")\n        bst.save_model(model_bin_path)\n        model = treelite.Model.load(model_bin_path, model_format=\"xgboost\")\n\n    assert model.num_feature == dtrain.num_col()\n    assert model.num_class == 1\n    assert model.num_tree == num_round\n    libpath = os.path.join(tmpdir, \"dart\" + _libext())\n    tl2cgen.export_lib(\n        model, toolchain=toolchain, libpath=libpath, params={}, verbose=True\n    )\n\n    predictor = tl2cgen.Predictor(libpath=libpath, verbose=True)\n    assert predictor.num_feature == dtrain.num_col()\n    assert predictor.num_class == 1\n    assert predictor.pred_transform == \"sigmoid\"\n    np.testing.assert_almost_equal(predictor.global_bias, 0, decimal=5)\n    assert predictor.sigmoid_alpha == 1.0\n    dmat = tl2cgen.DMatrix(X, dtype=\"float32\")\n    out_pred = predictor.predict(dmat)\n    expected_pred = bst.predict(dtrain, strict_shape=True)\n    np.testing.assert_almost_equal(out_pred, expected_pred, decimal=5)", ""]}
{"filename": "tests/python/conftest.py", "chunked_list": ["\"\"\"Pytest fixtures to initialize tests\"\"\"\nimport pathlib\nimport tempfile\n\nimport pytest\nimport treelite\nfrom sklearn.datasets import load_svmlight_file\n\nimport tl2cgen\n", "import tl2cgen\n\nfrom .metadata import example_model_db\n\n\n@pytest.fixture(scope=\"session\")\ndef annotation():\n    \"\"\"Pre-computed branch annotation information for example datasets\"\"\"\n    with tempfile.TemporaryDirectory(dir=\".\") as tmpdir:\n\n        def compute_annotation(dataset):\n            model = treelite.Model.load(\n                example_model_db[dataset].model,\n                model_format=example_model_db[dataset].format,\n            )\n            if example_model_db[dataset].dtrain is None:\n                return None\n            dtrain = tl2cgen.DMatrix(\n                load_svmlight_file(example_model_db[dataset].dtrain, zero_based=True)[0]\n            )\n            annotation_path = pathlib.Path(tmpdir) / f\"{dataset}.json\"\n            tl2cgen.annotate_branch(\n                model=model,\n                dmat=dtrain,\n                path=annotation_path,\n                verbose=True,\n            )\n            with open(annotation_path, \"r\", encoding=\"utf-8\") as f:\n                return f.read()\n\n        annotation_db = {k: compute_annotation(k) for k in example_model_db}\n    return annotation_db", ""]}
{"filename": "java_runtime/tl2cgen4j/create_jni.py", "chunked_list": ["#!/usr/bin/env python\n\"\"\"Build native lib for tl2cgen4j\"\"\"\n# pylint: disable=invalid-name\nimport errno\nimport os\nimport shutil\nimport subprocess\nimport sys\nfrom contextlib import contextmanager\n", "from contextlib import contextmanager\n\n\n@contextmanager\ndef cd(path):\n    \"\"\"Change current working directory temporarily\"\"\"\n    path = normpath(path)\n    cwd = os.getcwd()\n    os.chdir(path)\n    print(\"cd \" + path)\n    try:\n        yield path\n    finally:\n        os.chdir(cwd)", "\n\ndef maybe_makedirs(path):\n    \"\"\"Make directory if not exist\"\"\"\n    path = normpath(path)\n    print(\"mkdir -p \" + path)\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise", "\n\ndef run(command, **kwargs):\n    \"\"\"Run command\"\"\"\n    print(command)\n    subprocess.check_call(command, shell=True, **kwargs)\n\n\ndef cp(source, target):\n    \"\"\"Copy file\"\"\"\n    source = normpath(source)\n    target = normpath(target)\n    print(f\"cp {source} {target}\")\n    shutil.copy(source, target)", "def cp(source, target):\n    \"\"\"Copy file\"\"\"\n    source = normpath(source)\n    target = normpath(target)\n    print(f\"cp {source} {target}\")\n    shutil.copy(source, target)\n\n\ndef normpath(path):\n    \"\"\"Normalize UNIX path to a native path.\"\"\"\n    normalized = os.path.join(*path.split(\"/\"))\n    if os.path.isabs(path):\n        return os.path.abspath(\"/\") + normalized\n    return normalized", "def normpath(path):\n    \"\"\"Normalize UNIX path to a native path.\"\"\"\n    normalized = os.path.join(*path.split(\"/\"))\n    if os.path.isabs(path):\n        return os.path.abspath(\"/\") + normalized\n    return normalized\n\n\nif __name__ == \"__main__\":\n    if sys.platform == \"darwin\":\n        os.environ[\"JAVA_HOME\"] = (\n            subprocess.check_output(\"/usr/libexec/java_home\").strip().decode()\n        )\n\n    print(\"Building tl2cgen4j library\")\n    with cd(\"../..\"):\n        maybe_makedirs(\"build\")\n        with cd(\"build\"):\n            if sys.platform == \"win32\":\n                maybe_generator = ' -G\"Visual Studio 17 2022\" -A x64'\n            else:\n                maybe_generator = \"\"\n            if sys.platform == \"linux\":\n                maybe_parallel_build = \" -- -j$(nproc)\"\n            else:\n                maybe_parallel_build = \"\"\n            if \"cpp-coverage\" in sys.argv:\n                maybe_generator += \" -DTEST_COVERAGE=ON\"\n            run(\n                \"cmake .. -DBUILD_JVM_RUNTIME=ON -DCMAKE_VERBOSE_MAKEFILE=ON\"\n                + maybe_generator\n            )\n            run(\"cmake --build . --config Release\" + maybe_parallel_build)\n\n    print(\"Copying tl2cgen4j library\")\n    library_name = {\n        \"win32\": \"tl2cgen4j.dll\",\n        \"darwin\": \"libtl2cgen4j.dylib\",\n        \"linux\": \"libtl2cgen4j.so\",\n    }[sys.platform]\n    maybe_makedirs(\"src/main/resources/lib\")\n    cp(\"../../build/java_runtime/\" + library_name, \"src/main/resources/lib\")\n\n    print(\"building mushroom example\")\n    with cd(\"src/test/resources/mushroom_example\"):\n        run(\"cmake . \" + maybe_generator)\n        run(\"cmake --build . --config Release\")", "if __name__ == \"__main__\":\n    if sys.platform == \"darwin\":\n        os.environ[\"JAVA_HOME\"] = (\n            subprocess.check_output(\"/usr/libexec/java_home\").strip().decode()\n        )\n\n    print(\"Building tl2cgen4j library\")\n    with cd(\"../..\"):\n        maybe_makedirs(\"build\")\n        with cd(\"build\"):\n            if sys.platform == \"win32\":\n                maybe_generator = ' -G\"Visual Studio 17 2022\" -A x64'\n            else:\n                maybe_generator = \"\"\n            if sys.platform == \"linux\":\n                maybe_parallel_build = \" -- -j$(nproc)\"\n            else:\n                maybe_parallel_build = \"\"\n            if \"cpp-coverage\" in sys.argv:\n                maybe_generator += \" -DTEST_COVERAGE=ON\"\n            run(\n                \"cmake .. -DBUILD_JVM_RUNTIME=ON -DCMAKE_VERBOSE_MAKEFILE=ON\"\n                + maybe_generator\n            )\n            run(\"cmake --build . --config Release\" + maybe_parallel_build)\n\n    print(\"Copying tl2cgen4j library\")\n    library_name = {\n        \"win32\": \"tl2cgen4j.dll\",\n        \"darwin\": \"libtl2cgen4j.dylib\",\n        \"linux\": \"libtl2cgen4j.so\",\n    }[sys.platform]\n    maybe_makedirs(\"src/main/resources/lib\")\n    cp(\"../../build/java_runtime/\" + library_name, \"src/main/resources/lib\")\n\n    print(\"building mushroom example\")\n    with cd(\"src/test/resources/mushroom_example\"):\n        run(\"cmake . \" + maybe_generator)\n        run(\"cmake --build . --config Release\")", ""]}
{"filename": "python/hatch_build.py", "chunked_list": ["\"\"\"\nCustom hook to customize the behavior of Hatchling.\nHere, we customize the tag of the generated wheels.\n\"\"\"\nimport sysconfig\nfrom typing import Any, Dict\n\nfrom hatchling.builders.hooks.plugin.interface import BuildHookInterface\n\n\ndef get_tag() -> str:\n    \"\"\"Get appropriate wheel tag according to system\"\"\"\n    tag_platform = sysconfig.get_platform().replace(\"-\", \"_\").replace(\".\", \"_\")\n    return f\"py3-none-{tag_platform}\"", "\n\ndef get_tag() -> str:\n    \"\"\"Get appropriate wheel tag according to system\"\"\"\n    tag_platform = sysconfig.get_platform().replace(\"-\", \"_\").replace(\".\", \"_\")\n    return f\"py3-none-{tag_platform}\"\n\n\nclass CustomBuildHook(BuildHookInterface):\n    \"\"\"A custom build hook\"\"\"\n\n    def initialize(self, version: str, build_data: Dict[str, Any]) -> None:\n        \"\"\"This step ccurs immediately before each build.\"\"\"\n        build_data[\"tag\"] = get_tag()", "class CustomBuildHook(BuildHookInterface):\n    \"\"\"A custom build hook\"\"\"\n\n    def initialize(self, version: str, build_data: Dict[str, Any]) -> None:\n        \"\"\"This step ccurs immediately before each build.\"\"\"\n        build_data[\"tag\"] = get_tag()\n"]}
{"filename": "python/tl2cgen/libloader.py", "chunked_list": ["# coding: utf-8\n\"\"\"Find the path to TL2cgen dynamic library files.\"\"\"\n\nimport ctypes\nimport os\nimport pathlib\nimport sys\nimport warnings\nfrom typing import List\n", "from typing import List\n\nfrom .exception import TL2cgenError, TL2cgenLibraryNotFound\nfrom .util import py_str\n\n\ndef _find_lib_path() -> List[pathlib.Path]:\n    \"\"\"Find the path to TL2cgen dynamic library files.\n\n    Returns\n    -------\n    lib_path\n       List of all found library path to TL2cgen\n    \"\"\"\n    curr_path = pathlib.Path(__file__).expanduser().absolute().parent\n    dll_path = [\n        # When installed, libtl2cgen will be installed in <site-package-dir>/lib\n        curr_path / \"lib\",\n        # Editable installation\n        curr_path.parent.parent / \"build\",\n        # Use libtl2cgen from a system prefix, if available. This should be the last option.\n        pathlib.Path(sys.prefix).expanduser().resolve() / \"lib\",\n    ]\n\n    if sys.platform == \"win32\":\n        # On Windows, Conda may install libs in different paths\n        sys_prefix = pathlib.Path(sys.prefix)\n        dll_path.extend(\n            [\n                sys_prefix / \"bin\",\n                sys_prefix / \"Library\",\n                sys_prefix / \"Library\" / \"bin\",\n                sys_prefix / \"Library\" / \"lib\",\n            ]\n        )\n        dll_path = [p.joinpath(\"tl2cgen.dll\") for p in dll_path]\n    elif sys.platform.startswith((\"linux\", \"freebsd\", \"emscripten\", \"OS400\")):\n        dll_path = [p.joinpath(\"libtl2cgen.so\") for p in dll_path]\n    elif sys.platform == \"darwin\":\n        dll_path = [p.joinpath(\"libtl2cgen.dylib\") for p in dll_path]\n    elif sys.platform == \"cygwin\":\n        dll_path = [p.joinpath(\"cygtl2cgen.dll\") for p in dll_path]\n    else:\n        raise RuntimeError(f\"Unrecognized platform: {sys.platform}\")\n\n    lib_path = [p for p in dll_path if p.exists() and p.is_file()]\n\n    # TL2CGEN_BUILD_DOC is defined by sphinx conf.\n    if not lib_path and not os.environ.get(\"TL2CGEN_BUILD_DOC\", False):\n        link = \"https://tl2cgen.readthedocs.io/en/latest/install.html\"\n        msg = (\n            \"Cannot find TL2cgen Library in the candidate path.  \"\n            + \"List of candidates:\\n- \"\n            + (\"\\n- \".join(str(x) for x in dll_path))\n            + \"\\nTL2cgen Python package path: \"\n            + str(curr_path)\n            + \"\\nsys.prefix: \"\n            + sys.prefix\n            + \"\\nSee: \"\n            + link\n            + \" for installing TL2cgen.\"\n        )\n        raise TL2cgenLibraryNotFound(msg)\n    return lib_path", "\n\n@ctypes.CFUNCTYPE(None, ctypes.c_char_p)\ndef _log_callback(msg: bytes) -> None:\n    \"\"\"Redirect logs from native library into Python console\"\"\"\n    print(msg.decode(\"utf-8\"))\n\n\n@ctypes.CFUNCTYPE(None, ctypes.c_char_p)\ndef _warn_callback(msg: bytes) -> None:\n    \"\"\"Redirect warnings from native library into Python console\"\"\"\n    warnings.warn(msg.decode(\"utf-8\"))", "@ctypes.CFUNCTYPE(None, ctypes.c_char_p)\ndef _warn_callback(msg: bytes) -> None:\n    \"\"\"Redirect warnings from native library into Python console\"\"\"\n    warnings.warn(msg.decode(\"utf-8\"))\n\n\ndef _load_lib() -> ctypes.CDLL:\n    \"\"\"Load TL2cgen Library.\"\"\"\n    lib_paths = _find_lib_path()\n    if not lib_paths:\n        # This happens only when building document.\n        return None  # type: ignore\n    if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n        # pylint: disable=no-member\n        os.add_dll_directory(\n            os.path.join(os.path.normpath(sys.prefix), \"Library\", \"bin\")\n        )\n    os_error_list = []\n    lib = None\n    for lib_path in lib_paths:\n        try:\n            lib = ctypes.cdll.LoadLibrary(str(lib_path))\n            setattr(lib, \"path\", lib_path)\n        except OSError as e:\n            os_error_list.append(str(e))\n            continue\n    if not lib:\n        libname = lib_paths[0].name\n        raise TL2cgenError(\n            f\"\"\"\nTL2cgen Library ({libname}) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): {os_error_list}\n\"\"\"\n        )\n    lib.TL2cgenGetLastError.restype = ctypes.c_char_p\n    lib.log_callback = _log_callback  # type: ignore\n    lib.warn_callback = _warn_callback  # type: ignore\n    if lib.TL2cgenRegisterLogCallback(lib.log_callback) != 0:\n        raise TL2cgenError(py_str(lib.TL2cgenGetLastError()))\n    if lib.TL2cgenRegisterWarningCallback(lib.warn_callback) != 0:\n        raise TL2cgenError(py_str(lib.TL2cgenGetLastError()))\n    return lib", "\n\ndef _check_call(ret):\n    \"\"\"Check the return value of C API call\n\n    This function will raise exception when error occurs.\n    Wrap every API call with this function\n\n    Parameters\n    ----------\n    ret : int\n        return value from API calls\n    \"\"\"\n    if ret != 0:\n        raise TL2cgenError(_LIB.TL2cgenGetLastError().decode(\"utf-8\"))", "\n\n# Load native library in the global scope\n_LIB = _load_lib()\n"]}
{"filename": "python/tl2cgen/exception.py", "chunked_list": ["\"\"\"Exception classes used in TL2cgen\"\"\"\n\n\nclass TL2cgenError(Exception):\n    \"\"\"Error thrown by TL2cgen\"\"\"\n\n\nclass TL2cgenLibraryNotFound(Exception):\n    \"\"\"Error thrown by when TL2cgen is not found\"\"\"\n", ""]}
{"filename": "python/tl2cgen/dtypes.py", "chunked_list": ["\"\"\"Utility functions to handle types\"\"\"\n\nimport ctypes\nfrom typing import Any, Dict\n\nimport numpy as np\nimport numpy.typing as npt\n\n_CTYPES_TYPE_TABLE: Dict[str, Any] = {\n    \"uint32\": ctypes.c_uint32,", "_CTYPES_TYPE_TABLE: Dict[str, Any] = {\n    \"uint32\": ctypes.c_uint32,\n    \"float32\": ctypes.c_float,\n    \"float64\": ctypes.c_double,\n}\n\n_NUMPY_TYPE_TABLE: Dict[str, npt.DTypeLike] = {\n    \"uint32\": np.uint32,\n    \"float32\": np.float32,\n    \"float64\": np.float64,", "    \"float32\": np.float32,\n    \"float64\": np.float64,\n}\n\n\ndef type_info_to_ctypes_type(type_info: str) -> Any:\n    \"\"\"Obtain ctypes type corresponding to a given TypeInfo\"\"\"\n    return _CTYPES_TYPE_TABLE[type_info]\n\n\ndef type_info_to_numpy_type(type_info: str) -> npt.DTypeLike:\n    \"\"\"Obtain ctypes type corresponding to a given TypeInfo\"\"\"\n    return _NUMPY_TYPE_TABLE[type_info]", "\n\ndef type_info_to_numpy_type(type_info: str) -> npt.DTypeLike:\n    \"\"\"Obtain ctypes type corresponding to a given TypeInfo\"\"\"\n    return _NUMPY_TYPE_TABLE[type_info]\n\n\ndef numpy_type_to_type_info(type_info: npt.DTypeLike) -> str:\n    \"\"\"Obtain TypeInfo corresponding to a given NumPy type\"\"\"\n    if type_info == np.uint32:\n        return \"uint32\"\n    if type_info == np.float32:\n        return \"float32\"\n    if type_info == np.float64:\n        return \"float64\"\n    raise ValueError(f\"Unrecognized NumPy type: {type_info}\")", ""]}
{"filename": "python/tl2cgen/create_shared.py", "chunked_list": ["\"\"\"Launcher for C compiler to build shared libs\"\"\"\nimport pathlib\nimport time\nimport warnings\nfrom multiprocessing import cpu_count\nfrom typing import List, Optional, Union\n\nfrom .contrib.gcc import _create_shared_gcc\nfrom .contrib.msvc import _create_shared_msvc\nfrom .contrib.util import _toolchain_exist_check", "from .contrib.msvc import _create_shared_msvc\nfrom .contrib.util import _toolchain_exist_check\nfrom .exception import TL2cgenError\nfrom .util import _open_and_validate_recipe, _process_options\n\n\ndef create_shared(\n    toolchain: str,\n    dirpath: Union[str, pathlib.Path],\n    *,\n    nthread: Optional[int] = None,\n    verbose: bool = False,\n    options: Optional[List[str]] = None,\n    long_build_time_warning: bool = True,\n):  # pylint: disable=R0914\n    \"\"\"Create shared library.\n\n    Parameters\n    ----------\n    toolchain :\n        Which toolchain to use. You may choose one of \"msvc\", \"clang\", and \"gcc\".\n        You may also specify a specific variation of clang or gcc (e.g. \"gcc-7\")\n    dirpath :\n        Directory containing the header and source files previously generated\n        by :py:meth:`generate_c_code`. The directory must contain recipe.json\n        which specifies build dependencies.\n    nthread :\n        Number of threads to use in creating the shared library.\n        Defaults to the number of cores in the system.\n    verbose :\n        Whether to produce extra messages\n    options :\n        Additional options to pass to toolchain\n    long_build_time_warning :\n        If set to False, suppress the warning about potentially long build time\n\n    Returns\n    -------\n    libpath :\n        Absolute path of created shared library\n\n    Example\n    -------\n    The following command uses Visual C++ toolchain to generate\n    ``./my/model/model.dll``:\n\n    .. code-block:: python\n\n        tl2cgen.generate_c_code(model, dirpath=\"./my/model\",\n                                params={})\n        tl2cgen.create_shared(toolchain=\"msvc\", dirpath=\"./my/model\")\n\n    Later, the shared library can be referred to by its directory name:\n\n    .. code-block:: python\n\n        predictor = tl2cgen.Predictor(libpath=\"./my/model\")\n        # looks for ./my/model/model.dll\n\n    Alternatively, one may specify the library down to its file name:\n\n    .. code-block:: python\n\n        predictor = tl2cgen.Predictor(libpath=\"./my/model/model.dll\")\n    \"\"\"\n\n    # pylint: disable=R0912\n\n    if nthread is None or nthread <= 0:\n        ncore = cpu_count()\n        nthread = ncore\n    dirpath = pathlib.Path(dirpath).expanduser().resolve()\n    if not dirpath.exists() or not dirpath.is_dir():\n        raise TL2cgenError(f\"Directory {dirpath} does not exist\")\n    recipe = _open_and_validate_recipe(dirpath / \"recipe.json\")\n    options = _process_options(options)\n\n    # Write warning for potentially long compile time\n    if long_build_time_warning:\n        warn = False\n        for source in recipe[\"sources\"]:\n            if int(source[\"length\"]) > 10000:\n                warn = True\n                break\n        if warn:\n            warnings.warn(\n                \"WARNING: some of the source files are long. Expect long build time. \"\n                \"You may want to adjust the parameter 'parallel_comp'.\"\n            )\n\n    tstart = time.perf_counter()\n    _toolchain_exist_check(toolchain)\n    if toolchain == \"msvc\":\n        _create_shared = _create_shared_msvc\n    else:\n        _create_shared = _create_shared_gcc\n    libpath = _create_shared(\n        dirpath, toolchain, recipe, nthread=nthread, options=options, verbose=verbose\n    )\n    if verbose:\n        elapsed_time = time.perf_counter() - tstart\n        print(f\"Generated shared library in {elapsed_time:.2f} seconds\")\n    return libpath", ""]}
{"filename": "python/tl2cgen/__init__.py", "chunked_list": ["# coding: utf-8\n\"\"\"\nTL2cgen (TreeLite 2 C GENerator):\nModel compiler for decision tree ensembles\n\"\"\"\nfrom .core import _py_version, annotate_branch, generate_c_code\nfrom .create_shared import create_shared\nfrom .data import DMatrix\nfrom .exception import TL2cgenError\nfrom .generate_makefile import generate_cmakelists, generate_makefile", "from .exception import TL2cgenError\nfrom .generate_makefile import generate_cmakelists, generate_makefile\nfrom .predictor import Predictor\nfrom .shortcuts import export_lib, export_srcpkg\n\n__version__ = _py_version()\n\n__all__ = [\n    \"annotate_branch\",\n    \"create_shared\",", "    \"annotate_branch\",\n    \"create_shared\",\n    \"export_lib\",\n    \"export_srcpkg\",\n    \"generate_c_code\",\n    \"generate_cmakelists\",\n    \"generate_makefile\",\n    \"DMatrix\",\n    \"Predictor\",\n    \"TL2cgenError\",", "    \"Predictor\",\n    \"TL2cgenError\",\n]\n"]}
{"filename": "python/tl2cgen/core.py", "chunked_list": ["\"\"\"Core module of TL2cgen\"\"\"\nimport pathlib\nfrom typing import Any, Dict, Optional, Union\n\nimport treelite\n\nfrom .data import DMatrix\nfrom .handle_class import _Annotator, _Compiler, _TreeliteModel\n\n\ndef _py_version() -> str:\n    \"\"\"Get the TL2cgen version from Python version file.\"\"\"\n    version_file = pathlib.Path(__file__).parent / \"VERSION\"\n    with open(version_file, encoding=\"utf-8\") as f:\n        return f.read().strip()", "\n\ndef _py_version() -> str:\n    \"\"\"Get the TL2cgen version from Python version file.\"\"\"\n    version_file = pathlib.Path(__file__).parent / \"VERSION\"\n    with open(version_file, encoding=\"utf-8\") as f:\n        return f.read().strip()\n\n\ndef generate_c_code(\n    model: treelite.Model,\n    dirpath: Union[str, pathlib.Path],\n    params: Optional[Dict[str, Any]],\n    compiler: str = \"ast_native\",\n    *,\n    verbose: bool = False,\n) -> None:\n    \"\"\"\n    Generate prediction code from a tree ensemble model. The code will be C99\n    compliant. One header file (.h) will be generated, along with one or more\n    source files (.c). Use :py:meth:`create_shared` method to package\n    prediction code as a dynamic shared library (.so/.dll/.dylib).\n\n    Parameters\n    ----------\n    model :\n        Model to convert to C code\n    dirpath :\n        Directory to store header and source files\n    params :\n        Parameters for compiler. See :py:doc:`this page </compiler_param>`\n        for the list of compiler parameters.\n    compiler :\n        Kind of C code generator to use. Currently, there are two possible values:\n        {\"ast_native\", \"failsafe\"}\n    verbose :\n        Whether to print extra messages during compilation\n\n    Example\n    -------\n    The following populates the directory ``./model`` with source and header\n    files:\n\n    .. code-block:: python\n\n       tl2cgen.compile(model, dirpath=\"./my/model\", params={}, verbose=True)\n\n    If parallel compilation is enabled (parameter ``parallel_comp``), the files\n    are in the form of ``./my/model/header.h``, ``./my/model/main.c``,\n    ``./my/model/tu0.c``, ``./my/model/tu1.c`` and so forth, depending on\n    the value of ``parallel_comp``. Otherwise, there will be exactly two files:\n    ``./model/header.h``, ``./my/model/main.c``\n    \"\"\"\n    _model = _TreeliteModel(model)\n    compiler_obj = _Compiler(params, compiler, verbose)\n    compiler_obj.compile(_model, dirpath)", "\ndef generate_c_code(\n    model: treelite.Model,\n    dirpath: Union[str, pathlib.Path],\n    params: Optional[Dict[str, Any]],\n    compiler: str = \"ast_native\",\n    *,\n    verbose: bool = False,\n) -> None:\n    \"\"\"\n    Generate prediction code from a tree ensemble model. The code will be C99\n    compliant. One header file (.h) will be generated, along with one or more\n    source files (.c). Use :py:meth:`create_shared` method to package\n    prediction code as a dynamic shared library (.so/.dll/.dylib).\n\n    Parameters\n    ----------\n    model :\n        Model to convert to C code\n    dirpath :\n        Directory to store header and source files\n    params :\n        Parameters for compiler. See :py:doc:`this page </compiler_param>`\n        for the list of compiler parameters.\n    compiler :\n        Kind of C code generator to use. Currently, there are two possible values:\n        {\"ast_native\", \"failsafe\"}\n    verbose :\n        Whether to print extra messages during compilation\n\n    Example\n    -------\n    The following populates the directory ``./model`` with source and header\n    files:\n\n    .. code-block:: python\n\n       tl2cgen.compile(model, dirpath=\"./my/model\", params={}, verbose=True)\n\n    If parallel compilation is enabled (parameter ``parallel_comp``), the files\n    are in the form of ``./my/model/header.h``, ``./my/model/main.c``,\n    ``./my/model/tu0.c``, ``./my/model/tu1.c`` and so forth, depending on\n    the value of ``parallel_comp``. Otherwise, there will be exactly two files:\n    ``./model/header.h``, ``./my/model/main.c``\n    \"\"\"\n    _model = _TreeliteModel(model)\n    compiler_obj = _Compiler(params, compiler, verbose)\n    compiler_obj.compile(_model, dirpath)", "\n\ndef annotate_branch(\n    model: treelite.Model,\n    dmat: DMatrix,\n    path: Union[str, pathlib.Path],\n    *,\n    nthread: Optional[int] = None,\n    verbose: bool = False,\n) -> None:\n    \"\"\"\n    Annotate branches in a given model using frequency patterns in the training data and save\n    the annotation data to a JSON file. Each node gets the count of the instances that belong to it.\n\n    Parameters\n    ----------\n    dmat :\n        Data matrix representing the training data\n    path :\n        Location of JSON file\n    model :\n        Model to annotate\n    nthread :\n        Number of threads to use while annotating. If missing, use all physical cores in the system.\n    verbose :\n        Whether to print extra messages\n    \"\"\"\n    _model = _TreeliteModel(model)\n    nthread = nthread if nthread is not None else 0\n    annotator = _Annotator(_model, dmat, nthread, verbose)\n    annotator.save(path)", ""]}
{"filename": "python/tl2cgen/data.py", "chunked_list": ["\"\"\"Data matrix\"\"\"\nimport ctypes\nfrom typing import Optional, Tuple, Union\n\nimport numpy as np\nimport numpy.typing as npt\nimport scipy  # type: ignore\n\nfrom .dtypes import (\n    numpy_type_to_type_info,", "from .dtypes import (\n    numpy_type_to_type_info,\n    type_info_to_ctypes_type,\n    type_info_to_numpy_type,\n)\nfrom .exception import TL2cgenError\nfrom .libloader import _LIB, _check_call\nfrom .util import c_str\n\n\nclass DMatrix:\n    \"\"\"Data matrix used in TL2cgen.\n\n    Parameters\n    ----------\n    data :\n        Data source\n    dtype :\n        If specified, the data will be casted into the corresponding data type.\n    missing :\n        Value in the data that represents a missing entry. If set to ``None``,\n        ``numpy.nan`` will be used.\n    \"\"\"\n\n    # pylint: disable=R0902,R0903,R0913\n\n    def __init__(\n        self,\n        data: Union[str, npt.NDArray, scipy.sparse.csr_matrix],\n        *,\n        dtype: Optional[str] = None,\n        missing: Optional[float] = None,\n    ):\n        if data is None:\n            raise TL2cgenError(\"'data' argument cannot be None\")\n\n        self.handle = ctypes.c_void_p()\n\n        if isinstance(data, (str,)):\n            raise TL2cgenError(\n                \"'data' argument cannot be a string. Did you mean to load data from a text file? \"\n                \"Please use the following packages to load the text file:\\n\"\n                \"   * CSV file: Use pandas.read_csv() or numpy.loadtxt()\\n\"\n                \"   * LIBSVM file: Use sklearn.datasets.load_svmlight_file()\"\n            )\n        if isinstance(data, scipy.sparse.csr_matrix):\n            self._init_from_csr(data, dtype=dtype)\n        elif isinstance(data, scipy.sparse.csc_matrix):\n            self._init_from_csr(data.tocsr(), dtype=dtype)\n        elif isinstance(data, np.ndarray):\n            self._init_from_npy2d(data, missing=missing, dtype=dtype)\n        else:  # any type that's convertible to CSR matrix is O.K.\n            try:\n                csr = scipy.sparse.csr_matrix(data)\n                self._init_from_csr(csr, dtype=dtype)\n            except Exception as e:\n                raise TypeError(\n                    f\"Cannot initialize DMatrix from {type(data).__name__}\"\n                ) from e\n        num_row, num_col, nelem = self._get_dims()\n        self.shape = (num_row, num_col)\n        self.size = nelem\n\n    def _init_from_csr(\n        self, csr: scipy.sparse.csr_matrix, *, dtype: Optional[str] = None\n    ) -> None:\n        \"\"\"Initialize data from a CSR (Compressed Sparse Row) matrix\"\"\"\n        if len(csr.indices) != len(csr.data):\n            raise ValueError(\n                f\"indices and data not of same length: {len(csr.indices)} vs {len(csr.data)}\"\n            )\n        if len(csr.indptr) != csr.shape[0] + 1:\n            raise ValueError(\n                \"len(indptr) must be equal to 1 + [number of rows]\"\n                f\"len(indptr) = {len(csr.indptr)} vs 1 + [number of rows] = {1 + csr.shape[0]}\"\n            )\n        if csr.indptr[-1] != len(csr.data):\n            raise ValueError(\n                \"last entry of indptr must be equal to len(data)\"\n                f\"indptr[-1] = {csr.indptr[-1]} vs len(data) = {len(csr.data)}\"\n            )\n\n        if dtype is None:\n            data_type = csr.data.dtype\n        else:\n            data_type = type_info_to_numpy_type(dtype)\n        data_type_code = numpy_type_to_type_info(data_type)\n        data_ptr_type = ctypes.POINTER(type_info_to_ctypes_type(data_type_code))\n        if data_type_code not in [\"float32\", \"float64\"]:\n            raise ValueError(\"data should be either float32 or float64 type\")\n\n        data = np.array(csr.data, copy=False, dtype=data_type, order=\"C\")\n        indices = np.array(csr.indices, copy=False, dtype=np.uintc, order=\"C\")\n        indptr = np.array(csr.indptr, copy=False, dtype=np.uintp, order=\"C\")\n        _check_call(\n            _LIB.TL2cgenDMatrixCreateFromCSR(\n                data.ctypes.data_as(data_ptr_type),\n                c_str(data_type_code),\n                indices.ctypes.data_as(ctypes.POINTER(ctypes.c_uint)),\n                indptr.ctypes.data_as(ctypes.POINTER(ctypes.c_size_t)),\n                ctypes.c_size_t(csr.shape[0]),\n                ctypes.c_size_t(csr.shape[1]),\n                ctypes.byref(self.handle),\n            )\n        )\n\n    def _init_from_npy2d(\n        self,\n        mat: npt.NDArray,\n        *,\n        missing: Optional[float] = None,\n        dtype: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        Initialize data from a 2-D numpy matrix.\n        If ``mat`` does not have ``order='C'`` (also known as row-major) or is not\n        contiguous, a temporary copy will be made.\n        If ``mat`` does not have ``dtype=numpy.float32``, a temporary copy will be\n        made also.\n        Thus, as many as two temporary copies of data can be made. One should set\n        input layout and type judiciously to conserve memory.\n        \"\"\"\n        if len(mat.shape) != 2:\n            raise ValueError(\"Input numpy.ndarray must be two-dimensional\")\n        data_type: npt.DTypeLike = (\n            mat.dtype if dtype is None else type_info_to_numpy_type(dtype)\n        )\n        data_type_code = numpy_type_to_type_info(data_type)\n        data_ptr_type = ctypes.POINTER(type_info_to_ctypes_type(data_type_code))\n        if data_type_code not in [\"float32\", \"float64\"]:\n            raise ValueError(\"data should be either float32 or float64 type\")\n        # flatten the array by rows and ensure it is float32.\n        # we try to avoid data copies if possible\n        # (reshape returns a view when possible and we explicitly tell np.array to\n        #  avoid copying)\n        data = np.array(mat.reshape(mat.size), copy=False, dtype=data_type)\n        missing = missing if missing is not None else np.nan\n        missing_ar = np.array([missing], dtype=data_type, order=\"C\")\n        _check_call(\n            _LIB.TL2cgenDMatrixCreateFromMat(\n                data.ctypes.data_as(data_ptr_type),\n                c_str(data_type_code),\n                ctypes.c_size_t(mat.shape[0]),\n                ctypes.c_size_t(mat.shape[1]),\n                missing_ar.ctypes.data_as(data_ptr_type),\n                ctypes.byref(self.handle),\n            )\n        )\n\n    def _get_dims(self) -> Tuple[int, int, int]:\n        num_row = ctypes.c_size_t()\n        num_col = ctypes.c_size_t()\n        nelem = ctypes.c_size_t()\n        _check_call(\n            _LIB.TL2cgenDMatrixGetDimension(\n                self.handle,\n                ctypes.byref(num_row),\n                ctypes.byref(num_col),\n                ctypes.byref(nelem),\n            )\n        )\n        return num_row.value, num_col.value, nelem.value\n\n    def __del__(self):\n        if self.handle:\n            _check_call(_LIB.TL2cgenDMatrixFree(self.handle))\n            self.handle = None\n\n    def __repr__(self):\n        return (\n            f\"<{self.shape[0]}x{self.shape[1]} sparse matrix of type tl2cgen.DMatrix\\n\"\n            f\"        with {self.size} stored elements in Compressed Sparse Row format>\"\n        )", "\n\nclass DMatrix:\n    \"\"\"Data matrix used in TL2cgen.\n\n    Parameters\n    ----------\n    data :\n        Data source\n    dtype :\n        If specified, the data will be casted into the corresponding data type.\n    missing :\n        Value in the data that represents a missing entry. If set to ``None``,\n        ``numpy.nan`` will be used.\n    \"\"\"\n\n    # pylint: disable=R0902,R0903,R0913\n\n    def __init__(\n        self,\n        data: Union[str, npt.NDArray, scipy.sparse.csr_matrix],\n        *,\n        dtype: Optional[str] = None,\n        missing: Optional[float] = None,\n    ):\n        if data is None:\n            raise TL2cgenError(\"'data' argument cannot be None\")\n\n        self.handle = ctypes.c_void_p()\n\n        if isinstance(data, (str,)):\n            raise TL2cgenError(\n                \"'data' argument cannot be a string. Did you mean to load data from a text file? \"\n                \"Please use the following packages to load the text file:\\n\"\n                \"   * CSV file: Use pandas.read_csv() or numpy.loadtxt()\\n\"\n                \"   * LIBSVM file: Use sklearn.datasets.load_svmlight_file()\"\n            )\n        if isinstance(data, scipy.sparse.csr_matrix):\n            self._init_from_csr(data, dtype=dtype)\n        elif isinstance(data, scipy.sparse.csc_matrix):\n            self._init_from_csr(data.tocsr(), dtype=dtype)\n        elif isinstance(data, np.ndarray):\n            self._init_from_npy2d(data, missing=missing, dtype=dtype)\n        else:  # any type that's convertible to CSR matrix is O.K.\n            try:\n                csr = scipy.sparse.csr_matrix(data)\n                self._init_from_csr(csr, dtype=dtype)\n            except Exception as e:\n                raise TypeError(\n                    f\"Cannot initialize DMatrix from {type(data).__name__}\"\n                ) from e\n        num_row, num_col, nelem = self._get_dims()\n        self.shape = (num_row, num_col)\n        self.size = nelem\n\n    def _init_from_csr(\n        self, csr: scipy.sparse.csr_matrix, *, dtype: Optional[str] = None\n    ) -> None:\n        \"\"\"Initialize data from a CSR (Compressed Sparse Row) matrix\"\"\"\n        if len(csr.indices) != len(csr.data):\n            raise ValueError(\n                f\"indices and data not of same length: {len(csr.indices)} vs {len(csr.data)}\"\n            )\n        if len(csr.indptr) != csr.shape[0] + 1:\n            raise ValueError(\n                \"len(indptr) must be equal to 1 + [number of rows]\"\n                f\"len(indptr) = {len(csr.indptr)} vs 1 + [number of rows] = {1 + csr.shape[0]}\"\n            )\n        if csr.indptr[-1] != len(csr.data):\n            raise ValueError(\n                \"last entry of indptr must be equal to len(data)\"\n                f\"indptr[-1] = {csr.indptr[-1]} vs len(data) = {len(csr.data)}\"\n            )\n\n        if dtype is None:\n            data_type = csr.data.dtype\n        else:\n            data_type = type_info_to_numpy_type(dtype)\n        data_type_code = numpy_type_to_type_info(data_type)\n        data_ptr_type = ctypes.POINTER(type_info_to_ctypes_type(data_type_code))\n        if data_type_code not in [\"float32\", \"float64\"]:\n            raise ValueError(\"data should be either float32 or float64 type\")\n\n        data = np.array(csr.data, copy=False, dtype=data_type, order=\"C\")\n        indices = np.array(csr.indices, copy=False, dtype=np.uintc, order=\"C\")\n        indptr = np.array(csr.indptr, copy=False, dtype=np.uintp, order=\"C\")\n        _check_call(\n            _LIB.TL2cgenDMatrixCreateFromCSR(\n                data.ctypes.data_as(data_ptr_type),\n                c_str(data_type_code),\n                indices.ctypes.data_as(ctypes.POINTER(ctypes.c_uint)),\n                indptr.ctypes.data_as(ctypes.POINTER(ctypes.c_size_t)),\n                ctypes.c_size_t(csr.shape[0]),\n                ctypes.c_size_t(csr.shape[1]),\n                ctypes.byref(self.handle),\n            )\n        )\n\n    def _init_from_npy2d(\n        self,\n        mat: npt.NDArray,\n        *,\n        missing: Optional[float] = None,\n        dtype: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        Initialize data from a 2-D numpy matrix.\n        If ``mat`` does not have ``order='C'`` (also known as row-major) or is not\n        contiguous, a temporary copy will be made.\n        If ``mat`` does not have ``dtype=numpy.float32``, a temporary copy will be\n        made also.\n        Thus, as many as two temporary copies of data can be made. One should set\n        input layout and type judiciously to conserve memory.\n        \"\"\"\n        if len(mat.shape) != 2:\n            raise ValueError(\"Input numpy.ndarray must be two-dimensional\")\n        data_type: npt.DTypeLike = (\n            mat.dtype if dtype is None else type_info_to_numpy_type(dtype)\n        )\n        data_type_code = numpy_type_to_type_info(data_type)\n        data_ptr_type = ctypes.POINTER(type_info_to_ctypes_type(data_type_code))\n        if data_type_code not in [\"float32\", \"float64\"]:\n            raise ValueError(\"data should be either float32 or float64 type\")\n        # flatten the array by rows and ensure it is float32.\n        # we try to avoid data copies if possible\n        # (reshape returns a view when possible and we explicitly tell np.array to\n        #  avoid copying)\n        data = np.array(mat.reshape(mat.size), copy=False, dtype=data_type)\n        missing = missing if missing is not None else np.nan\n        missing_ar = np.array([missing], dtype=data_type, order=\"C\")\n        _check_call(\n            _LIB.TL2cgenDMatrixCreateFromMat(\n                data.ctypes.data_as(data_ptr_type),\n                c_str(data_type_code),\n                ctypes.c_size_t(mat.shape[0]),\n                ctypes.c_size_t(mat.shape[1]),\n                missing_ar.ctypes.data_as(data_ptr_type),\n                ctypes.byref(self.handle),\n            )\n        )\n\n    def _get_dims(self) -> Tuple[int, int, int]:\n        num_row = ctypes.c_size_t()\n        num_col = ctypes.c_size_t()\n        nelem = ctypes.c_size_t()\n        _check_call(\n            _LIB.TL2cgenDMatrixGetDimension(\n                self.handle,\n                ctypes.byref(num_row),\n                ctypes.byref(num_col),\n                ctypes.byref(nelem),\n            )\n        )\n        return num_row.value, num_col.value, nelem.value\n\n    def __del__(self):\n        if self.handle:\n            _check_call(_LIB.TL2cgenDMatrixFree(self.handle))\n            self.handle = None\n\n    def __repr__(self):\n        return (\n            f\"<{self.shape[0]}x{self.shape[1]} sparse matrix of type tl2cgen.DMatrix\\n\"\n            f\"        with {self.size} stored elements in Compressed Sparse Row format>\"\n        )", ""]}
{"filename": "python/tl2cgen/util.py", "chunked_list": ["\"\"\"Utility functions\"\"\"\nimport ctypes\nimport json\nimport pathlib\nfrom typing import Any, Dict, List, Optional\n\nfrom .exception import TL2cgenError\n\n\ndef c_str(string):\n    \"\"\"Convert a Python string to C string\"\"\"\n    return ctypes.c_char_p(string.encode(\"utf-8\"))", "\ndef c_str(string):\n    \"\"\"Convert a Python string to C string\"\"\"\n    return ctypes.c_char_p(string.encode(\"utf-8\"))\n\n\ndef py_str(string):\n    \"\"\"Convert C string back to Python string\"\"\"\n    return string.decode(\"utf-8\")\n", "\n\ndef _open_and_validate_recipe(recipe_path: pathlib.Path) -> Dict[str, Any]:\n    \"\"\"Ensure that the build recipe contains necessary fields\"\"\"\n    try:\n        with open(recipe_path, \"r\", encoding=\"utf-8\") as f:\n            recipe = json.load(f)\n    except IOError as e:\n        raise TL2cgenError(\"Failed to open recipe.json\") from e\n    if \"sources\" not in recipe or \"target\" not in recipe:\n        raise TL2cgenError(\"Malformed recipe.json\")\n    return recipe", "\n\ndef _process_options(options: Optional[List[str]]) -> List[str]:\n    \"\"\"Ensure that options are in the form of list of strings.\"\"\"\n    if options is not None:\n        try:\n            _ = iter(options)\n            options = [str(x) for x in options]\n        except TypeError as e:\n            raise TL2cgenError(\"options must be a list of string\") from e\n    else:\n        options = []\n    return options", ""]}
{"filename": "python/tl2cgen/shortcuts.py", "chunked_list": ["\"\"\"Convenience functions\"\"\"\nimport pathlib\nimport shutil\nfrom tempfile import TemporaryDirectory\nfrom typing import Any, Dict, List, Optional, Union\n\nimport treelite\n\nfrom .contrib.util import _toolchain_exist_check\nfrom .core import generate_c_code", "from .contrib.util import _toolchain_exist_check\nfrom .core import generate_c_code\nfrom .create_shared import create_shared\nfrom .generate_makefile import generate_cmakelists, generate_makefile\n\n\ndef export_lib(\n    model: treelite.Model,\n    toolchain: str,\n    libpath: Union[str, pathlib.Path],\n    params: Optional[Dict[str, Any]] = None,\n    compiler: str = \"ast_native\",\n    *,\n    nthread: Optional[int] = None,\n    verbose: bool = False,\n    options: Optional[List[str]] = None,\n):\n    \"\"\"\n    Convenience function: Generate prediction code and immediately turn it\n    into a dynamic shared library. A temporary directory will be created to\n    hold the source files.\n\n    Parameters\n    ----------\n    model :\n        Model to convert to C code\n    toolchain :\n        Which toolchain to use. You may choose one of 'msvc', 'clang', and 'gcc'.\n        You may also specify a specific variation of clang or gcc (e.g. 'gcc-7')\n    libpath :\n        Location to save the generated dynamic shared library\n    params :\n        Parameters to be passed to the compiler. See\n        :py:doc:`this page </compiler_param>` for the list of compiler\n        parameters.\n    compiler :\n        Kind of C code generator to use. Currently, there are two possible values:\n        {\"ast_native\", \"failsafe\"}\n    nthread :\n        Number of threads to use in creating the shared library.\n        Defaults to the number of cores in the system.\n    verbose :\n        Whether to produce extra messages\n    options :\n        Additional options to pass to toolchain\n\n    Example\n    -------\n    The one-line command\n\n    .. code-block:: python\n\n        tl2cgen.export_lib(model, toolchain=\"msvc\", libpath=\"./mymodel.dll\",\n                           params={})\n\n    is equivalent to the following sequence of commands:\n\n    .. code-block:: python\n\n        tl2cgen.generate_c_code(model, dirpath=\"/temporary/directory\",\n                                params={})\n        tl2cgen.create_shared(toolchain=\"msvc\",\n                              dirpath=\"/temporary/directory\")\n        # Move the library out of the temporary directory\n        shutil.move(\"/temporary/directory/mymodel.dll\", \"./mymodel.dll\")\n\n    \"\"\"\n    _toolchain_exist_check(toolchain)\n    libpath = pathlib.Path(libpath).expanduser().resolve()\n    long_build_time_warning = not (params and \"parallel_comp\" in params)\n\n    with TemporaryDirectory() as tempdir:\n        generate_c_code(model, tempdir, params, compiler, verbose=verbose)\n        temp_libpath = create_shared(\n            toolchain,\n            tempdir,\n            nthread=nthread,\n            verbose=verbose,\n            options=options,\n            long_build_time_warning=long_build_time_warning,\n        )\n        if libpath.is_file():\n            libpath.unlink()\n        shutil.move(temp_libpath, libpath)", "\n\ndef export_srcpkg(\n    model: treelite.Model,\n    toolchain: str,\n    pkgpath: Union[str, pathlib.Path],\n    libname: str,\n    params: Optional[Dict[str, Any]] = None,\n    compiler: str = \"ast_native\",\n    *,\n    verbose: bool = False,\n    options: Optional[List[str]] = None,\n):  # pylint: disable=R0913\n    \"\"\"\n    Convenience function: Generate prediction code and create a zipped source\n    package for deployment. The resulting zip file will also contain a Makefile\n    (or CMakeLists.txt, if you set toolchain=\"cmake\").\n\n    Parameters\n    ----------\n    model :\n        Model to convert to C code\n    toolchain :\n        Which toolchain to use. You may choose one of \"msvc\", \"clang\", \"gcc\", and \"cmake\".\n        You may also specify a specific variation of clang or gcc (e.g. \"gcc-7\")\n    pkgpath :\n        Location to save the zipped source package\n    libname :\n        Name of model shared library to be built\n    params :\n        Parameters to be passed to the compiler. See\n        :py:doc:`this page </compiler_param>` for the list of compiler\n        parameters.\n    compiler :\n        Name of compiler to use in C code generation\n    verbose :\n        Whether to produce extra messages\n    options :\n        Additional options to pass to toolchain\n\n    Example\n    -------\n    The one-line command\n\n    .. code-block:: python\n\n        tl2cgen.export_srcpkg(model, toolchain=\"gcc\",\n                              pkgpath=\"./mymodel_pkg.zip\",\n                              libname=\"mymodel.so\", params={})\n\n    is equivalent to the following sequence of commands:\n\n    .. code-block:: python\n\n        tl2cgen.generate_c_code(model, dirpath=\"/temporary/directory/mymodel\",\n                                params={})\n        tl2cgen.generate_makefile(dirpath=\"/temporary/directory/mymodel\",\n                                  toolchain=\"gcc\")\n        # Zip the directory containing C code and Makefile\n        shutil.make_archive(base_name=\"./mymodel_pkg\", format=\"zip\",\n                            root_dir=\"/temporary/directory\",\n                            base_dir=\"mymodel/\")\n    \"\"\"\n    # Check for file extension\n    pkgpath = pathlib.Path(pkgpath).expanduser().resolve()\n    if pkgpath.suffix != \".zip\":\n        raise ValueError(\"Source package file should have .zip extension\")\n    if toolchain != \"cmake\":\n        _toolchain_exist_check(toolchain)\n\n    with TemporaryDirectory() as temp_dir:\n        target = pathlib.Path(libname).stem\n        # Create a child directory to get desired name for target\n        dirpath = pathlib.Path(temp_dir) / target\n        dirpath.mkdir()\n        if params is None:\n            params = {}\n        params[\"native_lib_name\"] = target\n        generate_c_code(model, dirpath, params, compiler, verbose=verbose)\n        if toolchain == \"cmake\":\n            generate_cmakelists(dirpath, options)\n        else:\n            generate_makefile(dirpath, toolchain, options)\n        shutil.make_archive(\n            base_name=str(pkgpath.with_suffix(\"\")),\n            format=\"zip\",\n            root_dir=temp_dir,\n            base_dir=target,\n        )", ""]}
{"filename": "python/tl2cgen/predictor.py", "chunked_list": ["\"\"\"\nPredictor module\n\"\"\"\nimport ctypes\nimport pathlib\nfrom typing import Optional, Union\n\nfrom .contrib.util import _libext\nfrom .data import DMatrix\nfrom .exception import TL2cgenError", "from .data import DMatrix\nfrom .exception import TL2cgenError\nfrom .handle_class import _OutputVector\nfrom .libloader import _LIB, _check_call\nfrom .util import c_str, py_str\n\n\nclass Predictor:\n    \"\"\"\n    Predictor class is a convenient wrapper for loading shared libs.\n    TL2cgen uses OpenMP to launch multiple CPU threads to perform predictions\n    in parallel.\n\n    Parameters\n    ----------\n    libpath :\n        location of dynamic shared library (.dll/.so/.dylib)\n    nthread :\n        number of worker threads to use; if unspecified, use maximum number of\n        hardware threads\n    verbose :\n        Whether to print extra messages during construction\n    \"\"\"\n\n    def __init__(\n        self,\n        libpath: Union[str, pathlib.Path],\n        *,\n        nthread: Optional[int] = None,\n        verbose: bool = False,\n    ):\n        nthread = nthread if nthread is not None else -1\n        libpath = pathlib.Path(libpath).expanduser().resolve()\n        if libpath.is_dir():\n            # directory is given; locate shared library inside it\n            resolved_libpath = None\n            ext = _libext()\n            for candidate in libpath.glob(f\"*{ext}\"):\n                try:\n                    resolved_libpath = candidate.resolve(strict=True)\n                    break\n                except FileNotFoundError:\n                    continue\n            if not resolved_libpath:\n                raise TL2cgenError(\n                    f\"Directory {libpath} doesn't appear to have any dynamic shared library.\"\n                )\n        else:  # libpath is actually the name of shared library file\n            if not libpath.exists():\n                raise TL2cgenError(f\"Shared library not found at location {libpath}\")\n            resolved_libpath = libpath\n\n        self.handle = ctypes.c_void_p()\n        _check_call(\n            _LIB.TL2cgenPredictorLoad(\n                c_str(str(resolved_libpath)),\n                ctypes.c_int(nthread),\n                ctypes.byref(self.handle),\n            )\n        )\n        self._load_metadata(self.handle)\n\n        if verbose:\n            print(\n                f\"Dynamic shared library {resolved_libpath} has been successfully \"\n                \"loaded into memory\"\n            )\n\n    def __del__(self):\n        if self.handle:\n            _check_call(_LIB.TL2cgenPredictorFree(self.handle))\n            self.handle = None\n\n    @property\n    def num_feature(self):\n        \"\"\"Query number of features used in the model\"\"\"\n        return self.num_feature_\n\n    @property\n    def num_class(self):\n        \"\"\"Query number of output groups of the model\"\"\"\n        return self.num_class_\n\n    @property\n    def pred_transform(self):\n        \"\"\"Query pred transform of the model\"\"\"\n        return self.pred_transform_\n\n    @property\n    def global_bias(self):\n        \"\"\"Query global bias of the model\"\"\"\n        return self.global_bias_\n\n    @property\n    def sigmoid_alpha(self):\n        \"\"\"Query sigmoid alpha of the model\"\"\"\n        return self.sigmoid_alpha_\n\n    @property\n    def ratio_c(self):\n        \"\"\"Query sigmoid alpha of the model\"\"\"\n        return self.ratio_c_\n\n    @property\n    def threshold_type(self):\n        \"\"\"Query threshold type of the model\"\"\"\n        return self.threshold_type_\n\n    @property\n    def leaf_output_type(self):\n        \"\"\"Query threshold type of the model\"\"\"\n        return self.leaf_output_type_\n\n    def predict(\n        self,\n        dmat: DMatrix,\n        *,\n        verbose: bool = False,\n        pred_margin: bool = False,\n    ):\n        \"\"\"\n        Perform batch prediction with a 2D sparse data matrix. Worker threads will\n        internally divide up work for batch prediction. **Note that this function\n        may be called by only one thread at a time.**\n\n        Parameters\n        ----------\n        dmat:\n            Batch of rows for which predictions will be made\n        verbose :\n            Whether to print extra messages during prediction\n        pred_margin:\n            Whether to produce raw margins rather than transformed probabilities\n        \"\"\"\n        if not isinstance(dmat, DMatrix):\n            raise TL2cgenError(\"dmat must be of type DMatrix\")\n        result_size = ctypes.c_size_t()\n        _check_call(\n            _LIB.TL2cgenPredictorQueryResultSize(\n                self.handle, dmat.handle, ctypes.byref(result_size)\n            )\n        )\n\n        out_result = _OutputVector(\n            predictor_handle=self.handle,\n            dmat_handle=dmat.handle,\n        )\n        out_result_size = ctypes.c_size_t()\n        _check_call(\n            _LIB.TL2cgenPredictorPredictBatch(\n                self.handle,\n                dmat.handle,\n                ctypes.c_int(1 if verbose else 0),\n                ctypes.c_int(1 if pred_margin else 0),\n                out_result.handle,\n                ctypes.byref(out_result_size),\n            )\n        )\n\n        out_result_array = out_result.toarray()\n        idx = int(out_result_size.value)\n        res = out_result_array[0:idx].reshape((dmat.shape[0], -1))\n        if self.num_class_ > 1 and dmat.shape[0] != idx:\n            res = res.reshape((-1, self.num_class_))\n\n        return res\n\n    def _load_metadata(self, handle: ctypes.c_void_p) -> None:\n        # Save # of features\n        num_feature = ctypes.c_size_t()\n        _check_call(\n            _LIB.TL2cgenPredictorQueryNumFeature(handle, ctypes.byref(num_feature))\n        )\n        self.num_feature_ = num_feature.value\n        # Save # of classes\n        num_class = ctypes.c_size_t()\n        _check_call(_LIB.TL2cgenPredictorQueryNumClass(handle, ctypes.byref(num_class)))\n        self.num_class_ = num_class.value\n        # Save # of pred transform\n        pred_transform = ctypes.c_char_p()\n        _check_call(\n            _LIB.TL2cgenPredictorQueryPredTransform(\n                handle, ctypes.byref(pred_transform)\n            )\n        )\n        self.pred_transform_ = py_str(pred_transform.value)\n        # Save # of sigmoid alpha\n        sigmoid_alpha = ctypes.c_float()\n        _check_call(\n            _LIB.TL2cgenPredictorQuerySigmoidAlpha(handle, ctypes.byref(sigmoid_alpha))\n        )\n        self.sigmoid_alpha_ = sigmoid_alpha.value\n        # Save # of ratio C\n        ratio_c = ctypes.c_float()\n        _check_call(_LIB.TL2cgenPredictorQueryRatioC(handle, ctypes.byref(ratio_c)))\n        self.ratio_c_ = ratio_c.value\n        # Save # of global bias\n        global_bias = ctypes.c_float()\n        _check_call(\n            _LIB.TL2cgenPredictorQueryGlobalBias(handle, ctypes.byref(global_bias))\n        )\n        self.global_bias_ = global_bias.value\n        # Save threshold type\n        threshold_type = ctypes.c_char_p()\n        _check_call(\n            _LIB.TL2cgenPredictorQueryThresholdType(\n                handle, ctypes.byref(threshold_type)\n            )\n        )\n        self.threshold_type_ = py_str(threshold_type.value)\n        # Save leaf output type\n        leaf_output_type = ctypes.c_char_p()\n        _check_call(\n            _LIB.TL2cgenPredictorQueryLeafOutputType(\n                handle, ctypes.byref(leaf_output_type)\n            )\n        )\n        self.leaf_output_type_ = py_str(leaf_output_type.value)", ""]}
{"filename": "python/tl2cgen/generate_makefile.py", "chunked_list": ["\"\"\"Generator for Makefile and CMakeLists.txt\"\"\"\nimport pathlib\nfrom typing import List, Optional, Union\n\nfrom .contrib import gcc, msvc\nfrom .contrib.util import _libext, _toolchain_exist_check\nfrom .exception import TL2cgenError\nfrom .util import _open_and_validate_recipe, _process_options\n\n\ndef generate_makefile(\n    dirpath: Union[str, pathlib.Path],\n    toolchain: str,\n    options: Optional[List[str]] = None,\n) -> None:\n    \"\"\"\n    Generate a Makefile for a given directory of headers and sources. The\n    resulting Makefile will be stored in the directory. This function is useful\n    for deploying a model on a different machine.\n\n    Parameters\n    ----------\n    dirpath :\n        Directory containing the header and source files previously generated\n        by :py:meth:`Model.compile`. The directory must contain recipe.json\n        which specifies build dependencies.\n    toolchain :\n        Which toolchain to use. You may choose one of 'msvc', 'clang', and 'gcc'.\n        You may also specify a specific variation of clang or gcc (e.g. 'gcc-7')\n    options :\n        Additional options to pass to toolchain\n    \"\"\"\n    # pylint: disable=R0912,R0914,W0212\n    dirpath = pathlib.Path(dirpath).expanduser().resolve()\n    if not dirpath.is_dir():\n        raise TL2cgenError(f\"Directory {dirpath} does not exist\")\n    recipe = _open_and_validate_recipe(dirpath / \"recipe.json\")\n    options = _process_options(options)\n\n    # Determine file extensions for object and library files\n    _toolchain_exist_check(toolchain)\n    if toolchain == \"msvc\":\n        _obj_ext = msvc._obj_ext\n        _obj_cmd = msvc._obj_cmd\n        _lib_cmd = msvc._lib_cmd\n    else:\n        _obj_ext = gcc._obj_ext\n        _obj_cmd = gcc._obj_cmd\n        _lib_cmd = gcc._lib_cmd\n    obj_ext = _obj_ext()\n    lib_ext = _libext()\n\n    with open(dirpath / \"Makefile\", \"w\", encoding=\"UTF-8\") as f:\n        objects = [x[\"name\"] + obj_ext for x in recipe[\"sources\"]]\n        objects.extend(recipe.get(\"extra\", []))\n        target = recipe[\"target\"] + lib_ext\n        objects_str = \" \".join(objects)\n        lib_cmd = _lib_cmd(\n            objects=objects,\n            target=recipe[\"target\"],\n            lib_ext=lib_ext,\n            toolchain=toolchain,\n            options=options,\n        )\n\n        print(f\"{target}: {objects_str}\", file=f)\n        print(f\"\\t{lib_cmd}\", file=f)\n        for source in recipe[\"sources\"]:\n            source_file = source[\"name\"] + \".c\"\n            obj_file = source[\"name\"] + obj_ext\n            obj_cmd = _obj_cmd(\n                source=source[\"name\"], toolchain=toolchain, options=options\n            )\n            print(f\"{obj_file}: {source_file}\", file=f)\n            print(f\"\\t{obj_cmd}\", file=f)", "\n\ndef generate_makefile(\n    dirpath: Union[str, pathlib.Path],\n    toolchain: str,\n    options: Optional[List[str]] = None,\n) -> None:\n    \"\"\"\n    Generate a Makefile for a given directory of headers and sources. The\n    resulting Makefile will be stored in the directory. This function is useful\n    for deploying a model on a different machine.\n\n    Parameters\n    ----------\n    dirpath :\n        Directory containing the header and source files previously generated\n        by :py:meth:`Model.compile`. The directory must contain recipe.json\n        which specifies build dependencies.\n    toolchain :\n        Which toolchain to use. You may choose one of 'msvc', 'clang', and 'gcc'.\n        You may also specify a specific variation of clang or gcc (e.g. 'gcc-7')\n    options :\n        Additional options to pass to toolchain\n    \"\"\"\n    # pylint: disable=R0912,R0914,W0212\n    dirpath = pathlib.Path(dirpath).expanduser().resolve()\n    if not dirpath.is_dir():\n        raise TL2cgenError(f\"Directory {dirpath} does not exist\")\n    recipe = _open_and_validate_recipe(dirpath / \"recipe.json\")\n    options = _process_options(options)\n\n    # Determine file extensions for object and library files\n    _toolchain_exist_check(toolchain)\n    if toolchain == \"msvc\":\n        _obj_ext = msvc._obj_ext\n        _obj_cmd = msvc._obj_cmd\n        _lib_cmd = msvc._lib_cmd\n    else:\n        _obj_ext = gcc._obj_ext\n        _obj_cmd = gcc._obj_cmd\n        _lib_cmd = gcc._lib_cmd\n    obj_ext = _obj_ext()\n    lib_ext = _libext()\n\n    with open(dirpath / \"Makefile\", \"w\", encoding=\"UTF-8\") as f:\n        objects = [x[\"name\"] + obj_ext for x in recipe[\"sources\"]]\n        objects.extend(recipe.get(\"extra\", []))\n        target = recipe[\"target\"] + lib_ext\n        objects_str = \" \".join(objects)\n        lib_cmd = _lib_cmd(\n            objects=objects,\n            target=recipe[\"target\"],\n            lib_ext=lib_ext,\n            toolchain=toolchain,\n            options=options,\n        )\n\n        print(f\"{target}: {objects_str}\", file=f)\n        print(f\"\\t{lib_cmd}\", file=f)\n        for source in recipe[\"sources\"]:\n            source_file = source[\"name\"] + \".c\"\n            obj_file = source[\"name\"] + obj_ext\n            obj_cmd = _obj_cmd(\n                source=source[\"name\"], toolchain=toolchain, options=options\n            )\n            print(f\"{obj_file}: {source_file}\", file=f)\n            print(f\"\\t{obj_cmd}\", file=f)", "\n\ndef generate_cmakelists(\n    dirpath: Union[str, pathlib.Path],\n    options: Optional[List[str]] = None,\n) -> None:\n    \"\"\"\n    Generate a CMakeLists.txt for a given directory of headers and sources. The\n    resulting CMakeLists.txt will be stored in the directory. This function is useful\n    for deploying a model on a different machine.\n\n    Parameters\n    ----------\n    dirpath :\n        Directory containing the header and source files previously generated\n        by :py:meth:`Model.compile`. The directory must contain recipe.json\n        which specifies build dependencies.\n    options :\n        Additional options to pass to toolchain\n    \"\"\"\n    dirpath = pathlib.Path(dirpath).expanduser().resolve()\n    if not dirpath.is_dir():\n        raise TL2cgenError(f\"Directory {dirpath} does not exist\")\n    recipe = _open_and_validate_recipe(dirpath / \"recipe.json\")\n    options = _process_options(options)\n\n    target = recipe[\"target\"]\n    sources = \" \".join([x[\"name\"] + \".c\" for x in recipe[\"sources\"]])\n    options_str = \" \".join(options)\n    with open(dirpath / \"CMakeLists.txt\", \"w\", encoding=\"UTF-8\") as f:\n        print(\"cmake_minimum_required(VERSION 3.13)\", file=f)\n        print(\"project(mushroom LANGUAGES C)\\n\", file=f)\n        print(f\"add_library({target} SHARED)\", file=f)\n        print(f\"target_sources({target} PRIVATE header.h {sources})\", file=f)\n        print(f\"target_compile_options({target} PRIVATE {options_str})\", file=f)\n        print(\n            f'target_include_directories({target} PRIVATE \"${{PROJECT_BINARY_DIR}}\")',\n            file=f,\n        )\n        print(f\"set_target_properties({target} PROPERTIES\", file=f)\n        print(\n            \"\"\"POSITION_INDEPENDENT_CODE ON\n            C_STANDARD 99\n            C_STANDARD_REQUIRED ON\n            PREFIX \"\"\n            RUNTIME_OUTPUT_DIRECTORY \"${PROJECT_BINARY_DIR}\"\n            RUNTIME_OUTPUT_DIRECTORY_DEBUG \"${PROJECT_BINARY_DIR}\"\n            RUNTIME_OUTPUT_DIRECTORY_RELEASE \"${PROJECT_BINARY_DIR}\"\n            RUNTIME_OUTPUT_DIRECTORY_RELWITHDEBINFO \"${PROJECT_BINARY_DIR}\"\n            RUNTIME_OUTPUT_DIRECTORY_MINSIZEREL \"${PROJECT_BINARY_DIR}\"\n            LIBRARY_OUTPUT_DIRECTORY \"${PROJECT_BINARY_DIR}\"\n            LIBRARY_OUTPUT_DIRECTORY_DEBUG \"${PROJECT_BINARY_DIR}\"\n            LIBRARY_OUTPUT_DIRECTORY_RELEASE \"${PROJECT_BINARY_DIR}\"\n            LIBRARY_OUTPUT_DIRECTORY_RELWITHDEBINFO \"${PROJECT_BINARY_DIR}\"\n            LIBRARY_OUTPUT_DIRECTORY_MINSIZEREL \"${PROJECT_BINARY_DIR}\")\n        \"\"\",\n            file=f,\n        )", ""]}
{"filename": "python/tl2cgen/handle_class.py", "chunked_list": ["\"\"\"Internal classes to hold native handles\"\"\"\nimport ctypes\nimport json\nimport pathlib\nfrom typing import Any, Dict, Optional, Union\n\nimport numpy as np\nimport treelite\n\nfrom .data import DMatrix", "\nfrom .data import DMatrix\nfrom .dtypes import type_info_to_ctypes_type\nfrom .libloader import _LIB, _check_call\nfrom .util import c_str, py_str\n\n\nclass _TreeliteModel:\n    \"\"\"\n    Internal class holding a handle to Treelite model. We maintain a separate internal class,\n    to maintain **loose coupling** between Treelite and TL2cgen. This way, TL2cgen can support\n    past and future versions of Treelite (within the same major version).\n    \"\"\"\n\n    def __init__(self, model: treelite.Model):\n        model_bytes = model.serialize_bytes()\n        model_bytes_len = len(model_bytes)\n        buffer = ctypes.create_string_buffer(model_bytes, model_bytes_len)\n        self.handle = ctypes.c_void_p()\n        _check_call(\n            _LIB.TL2cgenLoadTreeliteModelFromBytes(\n                ctypes.pointer(buffer),\n                ctypes.c_size_t(model_bytes_len),\n                ctypes.byref(self.handle),\n            )\n        )\n        major_ver, minor_ver, patch_ver = (\n            ctypes.c_int32(),\n            ctypes.c_int32(),\n            ctypes.c_int32(),\n        )\n        _check_call(\n            _LIB.TL2cgenQueryTreeliteModelVersion(\n                self.handle,\n                ctypes.byref(major_ver),\n                ctypes.byref(minor_ver),\n                ctypes.byref(patch_ver),\n            )\n        )\n        self.__version__ = f\"{major_ver.value}.{minor_ver.value}.{patch_ver.value}\"\n\n    def __del__(self):\n        if self.handle:\n            _check_call(_LIB.TL2cgenFreeTreeliteModel(self.handle))\n            self.handle = None", "\n\nclass _Annotator:\n    \"\"\"Annotator object\"\"\"\n\n    def __init__(\n        self,\n        model: _TreeliteModel,\n        dmat: DMatrix,\n        nthread: int,\n        verbose: bool = False,\n    ):\n        self.handle = ctypes.c_void_p()\n        _check_call(\n            _LIB.TL2cgenAnnotateBranch(\n                model.handle,\n                dmat.handle,\n                ctypes.c_int(nthread),\n                ctypes.c_int(1 if verbose else 0),\n                ctypes.byref(self.handle),\n            )\n        )\n\n    def save(self, path: Union[str, pathlib.Path]):\n        \"\"\"Save annotation data to a JSON file\"\"\"\n        path = pathlib.Path(path).expanduser().resolve()\n        _check_call(_LIB.TL2cgenAnnotationSave(self.handle, c_str(str(path))))\n\n    def __del__(self):\n        if self.handle:\n            _check_call(_LIB.TL2cgenAnnotationFree(self.handle))\n            self.handle = None", "\n\nclass _Compiler:\n    \"\"\"Compiler object\"\"\"\n\n    def __init__(\n        self,\n        params: Optional[Dict[str, Any]],\n        compiler: str = \"ast_native\",\n        verbose: bool = False,\n    ):\n        self.handle = ctypes.c_void_p()\n        if params is None:\n            params = {}\n        if verbose:\n            params[\"verbose\"] = 1\n        if isinstance(params.get(\"annotate_in\"), pathlib.Path):\n            params[\"annotate_in\"] = str(params[\"annotate_in\"])\n        params_json_str = json.dumps(params)\n        _check_call(\n            _LIB.TL2cgenCompilerCreate(\n                c_str(compiler), c_str(params_json_str), ctypes.byref(self.handle)\n            )\n        )\n\n    def compile(self, model: _TreeliteModel, dirpath: Union[str, pathlib.Path]) -> None:\n        \"\"\"Generate prediction code\"\"\"\n        dirpath = pathlib.Path(dirpath).expanduser().resolve()\n        _check_call(\n            _LIB.TL2cgenCompilerGenerateCode(\n                self.handle, model.handle, c_str(str(dirpath))\n            )\n        )\n\n    def __del__(self):\n        if self.handle:\n            _check_call(_LIB.TL2cgenCompilerFree(self.handle))\n            self.handle = None", "\n\nclass _OutputVector:\n    \"\"\"Output vector object, used to hold prediction results from Predictor\"\"\"\n\n    def __init__(\n        self,\n        predictor_handle: ctypes.c_void_p,\n        dmat_handle: ctypes.c_void_p,\n    ):\n        self.handle = ctypes.c_void_p()\n        _check_call(\n            _LIB.TL2cgenPredictorCreateOutputVector(\n                predictor_handle, dmat_handle, ctypes.byref(self.handle)\n            )\n        )\n        type_str = ctypes.c_char_p()\n        _check_call(\n            _LIB.TL2cgenPredictorQueryLeafOutputType(\n                predictor_handle, ctypes.byref(type_str)\n            )\n        )\n        self.typestr_ = py_str(type_str.value)\n        length = ctypes.c_size_t()\n        _check_call(\n            _LIB.TL2cgenPredictorQueryResultSize(\n                predictor_handle, dmat_handle, ctypes.byref(length)\n            )\n        )\n        self.length_ = length.value\n\n    def __del__(self):\n        if self.handle:\n            _check_call(_LIB.TL2cgenPredictorDeleteOutputVector(self.handle))\n            self.handle = None\n\n    def toarray(self):\n        \"\"\"Convert to NumPy array\"\"\"\n        ptr = ctypes.c_void_p()\n        _check_call(\n            _LIB.TL2cgenPredictorGetRawPointerFromOutputVector(\n                self.handle, ctypes.byref(ptr)\n            )\n        )\n        ptr_type = ctypes.POINTER(type_info_to_ctypes_type(self.typestr_))\n        casted_ptr = ctypes.cast(ptr, ptr_type)\n        return np.copy(\n            np.ctypeslib.as_array(casted_ptr, shape=(self.length_,)), order=\"C\"\n        )", ""]}
{"filename": "python/tl2cgen/contrib/msvc.py", "chunked_list": ["\"\"\"\nTools to interact with Microsoft Visual C++ (MSVC)\n\"\"\"\nimport itertools\nimport os\nimport pathlib\nimport re\nimport sys\nfrom typing import Any, Dict, List\n", "from typing import Any, Dict, List\n\nfrom packaging.version import parse as parse_version\n\nfrom .create_shared import _create_shared_base\nfrom .util import _libext\n\nLIBEXT = _libext()\n\n\nif sys.platform != \"win32\":\n\n    class WindowsError(Exception):  # pylint: disable=C0115,W0622\n        pass", "\n\nif sys.platform != \"win32\":\n\n    class WindowsError(Exception):  # pylint: disable=C0115,W0622\n        pass\n\n\ndef _is_64bit_windows() -> bool:\n    return \"PROGRAMFILES(X86)\" in os.environ", "def _is_64bit_windows() -> bool:\n    return \"PROGRAMFILES(X86)\" in os.environ\n\n\ndef _varsall_bat_path() -> pathlib.Path:  # pylint: disable=R0912\n    if sys.platform != \"win32\":\n        raise RuntimeError(\"_varsall_bat_path() supported only on Windows\")\n\n    # if a custom location is given, try that first\n    if \"TL2CGEN_VCVARSALL\" in os.environ:\n        candidate = pathlib.Path(os.environ[\"TL2CGEN_VCVARSALL\"]).resolve()\n        if candidate.name.lower() != \"vcvarsall.bat\":\n            raise OSError(\n                \"Environment variable TL2CGEN_VCVARSALL must point to file vcvarsall.bat\"\n            )\n        if candidate.is_file():\n            return candidate\n        raise OSError(\n            \"Environment variable TL2CGEN_VCVARSALL does not refer to existing vcvarsall.bat\"\n        )\n\n    # == Bunch of heuristics to locate vcvarsall.bat ==\n\n    # List of possible paths to vcvarsall.bat\n    candidate_paths = []\n    try:\n        import winreg  # pylint: disable=E0401,C0415\n\n        if _is_64bit_windows():\n            key_name = r\"SOFTWARE\\Wow6432Node\\Microsoft\\VisualStudio\\SxS\\VS7\"\n        else:\n            key_name = r\"SOFTWARE\\Microsoft\\VisualStudio\\SxS\\VC7\"\n        key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, key_name)\n        i = 0\n        while True:\n            try:\n                version, vcroot, _ = winreg.EnumValue(key, i)\n                vcroot = pathlib.Path(vcroot).resolve()\n                if parse_version(version) >= parse_version(\"15.0\"):\n                    # Visual Studio 2017 revamped directory structure\n                    candidate_paths.append(vcroot / r\"VC\\Auxiliary\\Build\\vcvarsall.bat\")\n                else:\n                    candidate_paths.append(vcroot / r\"VC\\vcvarsall.bat\")\n            except WindowsError:  # pylint: disable=E0602\n                break\n            i += 1\n    except FileNotFoundError:\n        pass  # No registry key found\n    except ImportError:\n        pass  # No winreg module\n\n    for candidate in candidate_paths:\n        if candidate.is_file():\n            return candidate\n\n    # If registry method fails, try a bunch of pre-defined paths\n\n    # Visual Studio 2017 and higher\n    candidate_paths = list(\n        itertools.chain(\n            pathlib.Path(r\"C:\\Program Files (x86)\\Microsoft Visual Studio\").glob(\"*\"),\n            pathlib.Path(r\"C:\\Program Files\\Microsoft Visual Studio\").glob(\"*\"),\n        )\n    )\n    for vcroot in candidate_paths:\n        if re.fullmatch(r\"[0-9]+\", vcroot.name):\n            for candidate in vcroot.glob(r\"*\\VC\\Auxiliary\\Build\\vcvarsall.bat\"):\n                if candidate.is_file():\n                    return candidate\n    # Previous versions of Visual Studio\n    candidate_paths = list(\n        itertools.chain(\n            pathlib.Path(r\"C:\\Program Files (x86)\").glob(\n                r\"Microsoft Visual Studio*\\VC\\vcvarsall.bat\"\n            ),\n            pathlib.Path(r\"C:\\Program Files\").glob(\n                r\"Microsoft Visual Studio*\\VC\\vcvarsall.bat\"\n            ),\n        )\n    )\n    for candidate in candidate_paths:\n        if candidate.is_file():\n            return candidate\n\n    raise OSError(\n        \"vcvarsall.bat not found; please specify its full path in the environment \"\n        \"variable TL2CGEN_VCVARSALL\"\n    )", "\n\ndef _obj_ext() -> str:\n    return \".obj\"\n\n\n# pylint: disable=W0613\ndef _obj_cmd(\n    source: str,\n    toolchain: str,\n    options: List[str],\n):\n    source_file = source + \".c\"\n    options_str = \" \".join(options)\n    return f\"cl.exe /c /openmp /Ox {source_file} {options_str}\"", "\n\n# pylint: disable=W0613\ndef _lib_cmd(\n    objects: List[str],\n    target: str,\n    lib_ext: str,\n    toolchain: str,\n    options: List[str],\n) -> str:\n    objects_str = \" \".join(objects)\n    options_str = \" \".join(options)\n    return f\"cl.exe /LD /Fe{target} /openmp {objects_str} {options_str}\"", "\n\n# pylint: disable=R0913\ndef _create_shared_msvc(\n    dirpath: pathlib.Path,\n    toolchain: str,\n    recipe: Dict[str, Any],\n    *,\n    nthread: int,\n    options: List[str],\n    verbose: bool,\n) -> pathlib.Path:\n    # Specify command to compile an object file\n    recipe[\"object_ext\"] = _obj_ext()\n    recipe[\"library_ext\"] = LIBEXT\n\n    # pylint: disable=R0801\n\n    def _obj_cmd_wrapped(source: str) -> str:\n        return _obj_cmd(source, toolchain, options)\n\n    def _lib_cmd_wrapped(objects: List[str], target: str) -> str:\n        return _lib_cmd(objects, target, LIBEXT, toolchain, options)\n\n    recipe[\"create_object_cmd\"] = _obj_cmd_wrapped\n    recipe[\"create_library_cmd\"] = _lib_cmd_wrapped\n    plat_target = \"amd64\" if _is_64bit_windows() else \"x86\"\n    recipe[\"initial_cmd\"] = f'\"{_varsall_bat_path()}\" {plat_target}'\n    return _create_shared_base(dirpath, recipe, nthread=nthread, verbose=verbose)", ""]}
{"filename": "python/tl2cgen/contrib/create_shared.py", "chunked_list": ["\"\"\"Logic for launching C compiler to build shared libs\"\"\"\nimport pathlib\nimport subprocess\nfrom typing import Any, Dict\n\nfrom ..exception import TL2cgenError\nfrom .util import (\n    _create_log_cmd_unix,\n    _create_log_cmd_windows,\n    _is_windows,", "    _create_log_cmd_windows,\n    _is_windows,\n    _save_retcode_cmd_unix,\n    _save_retcode_cmd_windows,\n    _shell,\n)\n\n\ndef _enqueue(args: Dict[str, Any]) -> subprocess.Popen:\n    tid = args[\"tid\"]\n    queue = args[\"queue\"]\n    dirpath = args[\"dirpath\"]\n    init_cmd = args[\"init_cmd\"]\n    create_log_cmd = args[\"create_log_cmd\"]\n    save_retcode_cmd = args[\"save_retcode_cmd\"]\n\n    # pylint: disable=R1732\n    proc = subprocess.Popen(\n        _shell(),\n        shell=True,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        cwd=dirpath,\n    )\n    assert proc.stdin is not None\n    proc.stdin.write((init_cmd + \"\\n\").encode())\n    proc.stdin.write(create_log_cmd(f\"retcode_cpu{tid}.txt\" + \"\\n\").encode())\n    for command in queue:\n        proc.stdin.write((command + \"\\n\").encode())\n        proc.stdin.write((save_retcode_cmd(f\"retcode_cpu{tid}.txt\") + \"\\n\").encode())\n    proc.stdin.flush()\n\n    return proc", "def _enqueue(args: Dict[str, Any]) -> subprocess.Popen:\n    tid = args[\"tid\"]\n    queue = args[\"queue\"]\n    dirpath = args[\"dirpath\"]\n    init_cmd = args[\"init_cmd\"]\n    create_log_cmd = args[\"create_log_cmd\"]\n    save_retcode_cmd = args[\"save_retcode_cmd\"]\n\n    # pylint: disable=R1732\n    proc = subprocess.Popen(\n        _shell(),\n        shell=True,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        cwd=dirpath,\n    )\n    assert proc.stdin is not None\n    proc.stdin.write((init_cmd + \"\\n\").encode())\n    proc.stdin.write(create_log_cmd(f\"retcode_cpu{tid}.txt\" + \"\\n\").encode())\n    for command in queue:\n        proc.stdin.write((command + \"\\n\").encode())\n        proc.stdin.write((save_retcode_cmd(f\"retcode_cpu{tid}.txt\") + \"\\n\").encode())\n    proc.stdin.flush()\n\n    return proc", "\n\ndef _wait(proc: subprocess.Popen, args: Dict[str, Any]):\n    tid = args[\"tid\"]\n    dirpath = args[\"dirpath\"]\n    stdout, _ = proc.communicate()\n    with open(dirpath / f\"retcode_cpu{tid}.txt\", \"r\", encoding=\"UTF-8\") as f:\n        retcode = [int(line) for line in f]\n    return {\"stdout\": stdout.decode(), \"retcode\": retcode}\n", "\n\ndef _create_shared_base(\n    dirpath: pathlib.Path,\n    recipe: Dict[str, Any],\n    *,\n    nthread: int,\n    verbose: bool,\n):  # pylint: disable=R0914\n    # Fetch toolchain-specific commands\n    obj_cmd = recipe[\"create_object_cmd\"]\n    lib_cmd = recipe[\"create_library_cmd\"]\n    create_log_cmd = _create_log_cmd_windows if _is_windows() else _create_log_cmd_unix\n    save_retcode_cmd = (\n        _save_retcode_cmd_windows if _is_windows() else _save_retcode_cmd_unix\n    )\n\n    # 1. Compile sources in parallel\n    if verbose:\n        obj_ext = recipe[\"object_ext\"]\n        print(\n            f\"Compiling sources files in directory {dirpath} into object files (*{obj_ext})...\"\n        )\n    workqueue = [\n        {\n            \"tid\": tid,\n            \"queue\": [],\n            \"dirpath\": dirpath,\n            \"init_cmd\": recipe[\"initial_cmd\"],\n            \"create_log_cmd\": create_log_cmd,\n            \"save_retcode_cmd\": save_retcode_cmd,\n        }\n        for tid in range(nthread)\n    ]\n    for i, source in enumerate(recipe[\"sources\"]):\n        workqueue[i % nthread][\"queue\"].append(obj_cmd(source[\"name\"]))\n    proc = [_enqueue(workqueue[tid]) for tid in range(nthread)]\n    result = []\n    for tid in range(nthread):\n        result.append(_wait(proc[tid], workqueue[tid]))\n\n    for tid in range(nthread):\n        if not all(x == 0 for x in result[tid][\"retcode\"]):\n            log_path = dirpath / f\"log_cpu{tid}.txt\"\n            with open(log_path, \"w\", encoding=\"UTF-8\") as f:\n                f.write(result[tid][\"stdout\"] + \"\\n\")\n            raise TL2cgenError(\n                f\"Error occured in worker #{tid}: \" + result[tid][\"stdout\"]\n            )\n\n    # 2. Package objects into a dynamic shared library\n    full_libpath = dirpath.joinpath(recipe[\"target\"] + recipe[\"library_ext\"])\n    if verbose:\n        print(f\"Generating dynamic shared library {full_libpath}...\")\n    objects = [x[\"name\"] + recipe[\"object_ext\"] for x in recipe[\"sources\"]]\n    objects.extend(recipe.get(\"extra\", []))\n    workqueue = [\n        {\n            \"tid\": 0,\n            \"queue\": [lib_cmd(objects, recipe[\"target\"])],\n            \"dirpath\": dirpath,\n            \"init_cmd\": recipe[\"initial_cmd\"],\n            \"create_log_cmd\": create_log_cmd,\n            \"save_retcode_cmd\": save_retcode_cmd,\n        }\n    ]\n    proc = [_enqueue(workqueue[0])]\n    result = [_wait(proc[0], workqueue[0])]\n\n    if result[0][\"retcode\"][0] != 0:\n        with open(dirpath / \"log_cpu0.txt\", \"w\", encoding=\"UTF-8\") as f:\n            f.write(result[0][\"stdout\"] + \"\\n\")\n        raise TL2cgenError(\n            \"Error occured while creating dynamic library: \" + result[0][\"stdout\"]\n        )\n\n    # 3. Clean up\n    for tid in range(nthread):\n        dirpath.joinpath(f\"retcode_cpu{tid}.txt\").unlink()\n\n    # Return full path of shared library\n    return dirpath.joinpath(full_libpath)", ""]}
{"filename": "python/tl2cgen/contrib/__init__.py", "chunked_list": ["\"\"\"\nContrib APIs of Treelite python package.\nContrib API provides ways to interact with third-party libraries and tools.\n\"\"\"\n"]}
{"filename": "python/tl2cgen/contrib/util.py", "chunked_list": ["\"\"\"Utilities for contrib module\"\"\"\nimport os\nimport subprocess\nfrom sys import platform as _platform\n\n\ndef _is_windows() -> bool:\n    return _platform == \"win32\"\n\n\ndef _shell() -> str:\n    if _is_windows():\n        return \"cmd.exe\"\n    if \"SHELL\" in os.environ:\n        return os.environ[\"SHELL\"]\n    return \"/bin/sh\"  # use POSIX-compliant shell if SHELL is not set", "\n\ndef _shell() -> str:\n    if _is_windows():\n        return \"cmd.exe\"\n    if \"SHELL\" in os.environ:\n        return os.environ[\"SHELL\"]\n    return \"/bin/sh\"  # use POSIX-compliant shell if SHELL is not set\n\n\ndef _libext():\n    if _platform == \"darwin\":\n        return \".dylib\"\n    if _platform in (\"win32\", \"cygwin\"):\n        return \".dll\"\n    return \".so\"", "\n\ndef _libext():\n    if _platform == \"darwin\":\n        return \".dylib\"\n    if _platform in (\"win32\", \"cygwin\"):\n        return \".dll\"\n    return \".so\"\n\n\ndef _toolchain_exist_check(toolchain: str) -> None:\n    if toolchain != \"msvc\":\n        retcode = subprocess.call(\n            f\"{toolchain} --version\",\n            shell=True,\n            stdin=subprocess.DEVNULL,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n        )\n        if retcode != 0:\n            raise ValueError(\n                f\"Toolchain {toolchain} not found. Ensure that it is installed and \"\n                \"that it is a variant of GCC or Clang.\"\n            )", "\n\ndef _toolchain_exist_check(toolchain: str) -> None:\n    if toolchain != \"msvc\":\n        retcode = subprocess.call(\n            f\"{toolchain} --version\",\n            shell=True,\n            stdin=subprocess.DEVNULL,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n        )\n        if retcode != 0:\n            raise ValueError(\n                f\"Toolchain {toolchain} not found. Ensure that it is installed and \"\n                \"that it is a variant of GCC or Clang.\"\n            )", "\n\ndef _create_log_cmd_unix(logfile: str) -> str:\n    return f\"true > {logfile}\"\n\n\ndef _save_retcode_cmd_unix(logfile: str) -> str:\n    if _shell().endswith(\"fish\"):  # special handling for fish shell\n        return f\"echo $status >> {logfile}\"\n    return f\"echo $? >> {logfile}\"", "\n\ndef _create_log_cmd_windows(logfile: str) -> str:\n    return f\"type NUL > {logfile}\"\n\n\ndef _save_retcode_cmd_windows(logfile: str) -> str:\n    return f\"echo %errorlevel% >> {logfile}\"\n", ""]}
{"filename": "python/tl2cgen/contrib/gcc.py", "chunked_list": ["\"\"\"\nTools to interact with toolchains GCC, Clang, and other UNIX compilers\n\"\"\"\nimport pathlib\nfrom typing import Any, Dict, List\n\nfrom .create_shared import _create_shared_base\nfrom .util import _libext\n\nLIBEXT = _libext()", "\nLIBEXT = _libext()\n\n\ndef _obj_ext() -> str:\n    return \".o\"\n\n\ndef _obj_cmd(\n    source: str,\n    toolchain: str,\n    options: List[str],\n) -> str:\n    obj_ext = _obj_ext()\n    source_file = source + \".c\"\n    obj_file = source + obj_ext\n    options_str = \" \".join(options)\n    return (\n        f\"{toolchain} -c -O3 -o {obj_file} {source_file} -fPIC -std=c99 {options_str}\"\n    )", "def _obj_cmd(\n    source: str,\n    toolchain: str,\n    options: List[str],\n) -> str:\n    obj_ext = _obj_ext()\n    source_file = source + \".c\"\n    obj_file = source + obj_ext\n    options_str = \" \".join(options)\n    return (\n        f\"{toolchain} -c -O3 -o {obj_file} {source_file} -fPIC -std=c99 {options_str}\"\n    )", "\n\ndef _lib_cmd(\n    objects: List[str],\n    target: str,\n    lib_ext: str,\n    toolchain: str,\n    options: List[str],\n) -> str:\n    objects_str = \" \".join(objects)\n    options_str = \" \".join(options)\n    return f\"{toolchain} -shared -O3 -o {target + lib_ext} {objects_str} -std=c99 {options_str}\"", "\n\ndef _create_shared_gcc(\n    dirpath: pathlib.Path,\n    toolchain: str,\n    recipe: Dict[str, Any],\n    *,\n    nthread: int,\n    options: List[str],\n    verbose: bool,\n) -> pathlib.Path:\n    options += [\"-lm\"]\n    # Specify command to compile an object file\n    recipe[\"object_ext\"] = _obj_ext()\n    recipe[\"library_ext\"] = LIBEXT\n\n    # pylint: disable=R0801\n\n    def _obj_cmd_wrapped(source: str) -> str:\n        return _obj_cmd(source, toolchain, options)\n\n    def _lib_cmd_wrapped(objects: List[str], target: str) -> str:\n        return _lib_cmd(objects, target, LIBEXT, toolchain, options)\n\n    recipe[\"create_object_cmd\"] = _obj_cmd_wrapped\n    recipe[\"create_library_cmd\"] = _lib_cmd_wrapped\n    recipe[\"initial_cmd\"] = \"\"\n    return _create_shared_base(dirpath, recipe, nthread=nthread, verbose=verbose)", ""]}
{"filename": "python/packager/nativelib.py", "chunked_list": ["\"\"\"\nFunctions for building libtl2cgen\n\"\"\"\nimport logging\nimport os\nimport pathlib\nimport shutil\nimport subprocess\nimport sys\nfrom platform import system", "import sys\nfrom platform import system\nfrom typing import Optional\n\nfrom .build_config import BuildConfiguration\n\n\ndef _lib_name() -> str:\n    \"\"\"Return platform dependent shared object name.\"\"\"\n    if system() in [\"Linux\", \"OS400\"] or system().upper().endswith(\"BSD\"):\n        name = \"libtl2cgen.so\"\n    elif system() == \"Darwin\":\n        name = \"libtl2cgen.dylib\"\n    elif system() == \"Windows\":\n        name = \"tl2cgen.dll\"\n    else:\n        raise NotImplementedError(f\"System {system()} not supported\")\n    return name", "\n\ndef build_libtl2cgen(\n    cpp_src_dir: pathlib.Path,\n    build_dir: pathlib.Path,\n    build_config: BuildConfiguration,\n) -> pathlib.Path:\n    \"\"\"Build libtl2cgen in a temporary directory and obtain the path to built libtl2cgen\"\"\"\n    logger = logging.getLogger(\"tl2cgen.packager.build_libtl2cgen\")\n\n    if not cpp_src_dir.is_dir():\n        raise RuntimeError(f\"Expected {cpp_src_dir} to be a directory\")\n    logger.info(\n        \"Building %s from the C++ source files in %s...\", _lib_name(), str(cpp_src_dir)\n    )\n\n    def _build(*, generator: str) -> None:\n        cmake_cmd = [\n            \"cmake\",\n            str(cpp_src_dir),\n            generator,\n        ]\n        cmake_cmd.extend(build_config.get_cmake_args())\n\n        # Flag for cross-compiling for Apple Silicon\n        # We use environment variable because it's the only way to pass down custom flags\n        # through the cibuildwheel package, which calls `pip wheel` command.\n        if \"CIBW_TARGET_OSX_ARM64\" in os.environ:\n            cmake_cmd.extend(\n                [\"-DCMAKE_OSX_ARCHITECTURES=arm64\", \"-DDETECT_CONDA_ENV=OFF\"]\n            )\n\n        logger.info(\"CMake args: %s\", str(cmake_cmd))\n        subprocess.check_call(cmake_cmd, cwd=build_dir)\n\n        if system() == \"Windows\":\n            subprocess.check_call(\n                [\"cmake\", \"--build\", \".\", \"--config\", \"Release\"], cwd=build_dir\n            )\n        else:\n            nproc = os.cpu_count()\n            assert build_tool is not None\n            subprocess.check_call([build_tool, f\"-j{nproc}\"], cwd=build_dir)\n\n    if system() == \"Windows\":\n        supported_generators = (\n            \"-GVisual Studio 17 2022\",\n            \"-GVisual Studio 16 2019\",\n            \"-GVisual Studio 15 2017\",\n            \"-GMinGW Makefiles\",\n        )\n        for generator in supported_generators:\n            try:\n                _build(generator=generator)\n                logger.info(\n                    \"Successfully built %s using generator %s\", _lib_name(), generator\n                )\n                break\n            except subprocess.CalledProcessError as e:\n                logger.info(\n                    \"Tried building with generator %s but failed with exception %s\",\n                    generator,\n                    str(e),\n                )\n                # Empty build directory\n                shutil.rmtree(build_dir)\n                build_dir.mkdir()\n        else:\n            raise RuntimeError(\n                \"None of the supported generators produced a successful build!\"\n                f\"Supported generators: {supported_generators}\"\n            )\n    else:\n        build_tool = \"ninja\" if shutil.which(\"ninja\") else \"make\"\n        generator = \"-GNinja\" if build_tool == \"ninja\" else \"-GUnix Makefiles\"\n        try:\n            _build(generator=generator)\n        except subprocess.CalledProcessError as e:\n            logger.info(\"Failed to build with OpenMP. Exception: %s\", str(e))\n            build_config.use_openmp = False\n            _build(generator=generator)\n\n    return build_dir / _lib_name()", "\n\ndef locate_local_libtl2cgen(\n    toplevel_dir: pathlib.Path,\n    logger: logging.Logger,\n) -> Optional[pathlib.Path]:\n    \"\"\"\n    Locate libtl2cgen from the local project directory's lib/ subdirectory.\n    \"\"\"\n    libtl2cgen = toplevel_dir.parent / \"build\" / _lib_name()\n    if libtl2cgen.exists():\n        logger.info(\"Found %s at %s\", libtl2cgen.name, str(libtl2cgen.parent))\n        return libtl2cgen\n    logger.info(\"Did not find %s at %s\", libtl2cgen.name, str(libtl2cgen.parent))\n    return None", "\n\ndef locate_or_build_libtl2cgen(\n    toplevel_dir: pathlib.Path,\n    build_dir: pathlib.Path,\n    build_config: BuildConfiguration,\n) -> pathlib.Path:\n    \"\"\"Locate libtl2cgen; if not exist, build it\"\"\"\n    logger = logging.getLogger(\"tl2cgen.packager.locate_or_build_libtl2cgen\")\n\n    if build_config.use_system_libtl2cgen:\n        # Find libtl2cgen from system prefix\n        sys_prefix = pathlib.Path(sys.prefix)\n        sys_prefix_candidates = [\n            sys_prefix / \"lib\",\n            # Paths possibly used on Windows\n            sys_prefix / \"bin\",\n            sys_prefix / \"Library\",\n            sys_prefix / \"Library\" / \"bin\",\n            sys_prefix / \"Library\" / \"lib\",\n        ]\n        sys_prefix_candidates = [\n            p.expanduser().resolve() for p in sys_prefix_candidates\n        ]\n        for candidate_dir in sys_prefix_candidates:\n            libtl2cgen_sys = candidate_dir / _lib_name()\n            if libtl2cgen_sys.exists():\n                logger.info(\"Using system tl2cgen: %s\", str(libtl2cgen_sys))\n                return libtl2cgen_sys\n        raise RuntimeError(\n            f\"use_system_libtl2cgen was specified but {_lib_name()} is \"\n            f\"not found. Paths searched (in order): \\n\"\n            + \"\\n\".join([f\"* {str(p)}\" for p in sys_prefix_candidates])\n        )\n\n    libtl2cgen = locate_local_libtl2cgen(toplevel_dir, logger=logger)\n    if libtl2cgen is not None:\n        return libtl2cgen\n\n    if toplevel_dir.joinpath(\"cpp_src\").exists():\n        # Source distribution; all C++ source files to be found in cpp_src/\n        cpp_src_dir = toplevel_dir.joinpath(\"cpp_src\")\n    else:\n        # Probably running \"pip install .\" from python-package/\n        cpp_src_dir = toplevel_dir.parent\n        if not cpp_src_dir.joinpath(\"CMakeLists.txt\").exists():\n            raise RuntimeError(f\"Did not find CMakeLists.txt from {cpp_src_dir}\")\n    return build_libtl2cgen(cpp_src_dir, build_dir=build_dir, build_config=build_config)", ""]}
{"filename": "python/packager/pep517.py", "chunked_list": ["\"\"\"\nCustom build backend for TL2cgen Python package.\nBuilds source distribution and binary wheels, following PEP 517 / PEP 660.\nReuses components of Hatchling (https://github.com/pypa/hatch/tree/master/backend) for the sake\nof brevity.\n\"\"\"\nimport dataclasses\nimport logging\nimport os\nimport pathlib", "import os\nimport pathlib\nimport tempfile\nfrom contextlib import contextmanager\nfrom typing import Any, Dict, Iterator, Optional, Union\n\nimport hatchling.build\n\nfrom .build_config import BuildConfiguration\nfrom .nativelib import locate_local_libtl2cgen, locate_or_build_libtl2cgen", "from .build_config import BuildConfiguration\nfrom .nativelib import locate_local_libtl2cgen, locate_or_build_libtl2cgen\nfrom .sdist import copy_cpp_src_tree\nfrom .util import copy_with_logging, copytree_with_logging\n\n\n@contextmanager\ndef cd(path: Union[str, pathlib.Path]) -> Iterator[str]:  # pylint: disable=C0103\n    \"\"\"\n    Temporarily change working directory.\n    TODO(hcho3): Remove this once we adopt Python 3.11, which implements contextlib.chdir.\n    \"\"\"\n    path = str(path)\n    path = os.path.realpath(path)\n    cwd = os.getcwd()\n    os.chdir(path)\n    try:\n        yield path\n    finally:\n        os.chdir(cwd)", "\n\nTOPLEVEL_DIR = pathlib.Path(__file__).parent.parent.absolute().resolve()\nlogging.basicConfig(level=logging.INFO)\n\n\n# Aliases\nget_requires_for_build_sdist = hatchling.build.get_requires_for_build_sdist\nget_requires_for_build_wheel = hatchling.build.get_requires_for_build_wheel\nget_requires_for_build_editable = hatchling.build.get_requires_for_build_editable", "get_requires_for_build_wheel = hatchling.build.get_requires_for_build_wheel\nget_requires_for_build_editable = hatchling.build.get_requires_for_build_editable\n\n\ndef build_wheel(\n    wheel_directory: str,\n    config_settings: Optional[Dict[str, Any]] = None,\n    metadata_directory: Optional[str] = None,\n) -> str:\n    \"\"\"Build a wheel\"\"\"\n    logger = logging.getLogger(\"tl2cgen.packager.build_wheel\")\n\n    build_config = BuildConfiguration()\n    build_config.update(config_settings)\n    logger.info(\"Parsed build configuration: %s\", dataclasses.asdict(build_config))\n\n    # Create tempdir with Python package + libtl2cgen\n    with tempfile.TemporaryDirectory() as td:\n        td_path = pathlib.Path(td)\n        build_dir = td_path / \"libbuild\"\n        build_dir.mkdir()\n\n        workspace = td_path / \"whl_workspace\"\n        workspace.mkdir()\n        logger.info(\"Copying project files to temporary directory %s\", str(workspace))\n\n        copy_with_logging(TOPLEVEL_DIR / \"pyproject.toml\", workspace, logger=logger)\n        copy_with_logging(TOPLEVEL_DIR / \"hatch_build.py\", workspace, logger=logger)\n        copy_with_logging(TOPLEVEL_DIR / \"README.rst\", workspace, logger=logger)\n\n        pkg_path = workspace / \"tl2cgen\"\n        copytree_with_logging(TOPLEVEL_DIR / \"tl2cgen\", pkg_path, logger=logger)\n        lib_path = pkg_path / \"lib\"\n        lib_path.mkdir()\n        libtl2cgen = locate_or_build_libtl2cgen(\n            TOPLEVEL_DIR, build_dir=build_dir, build_config=build_config\n        )\n        if not build_config.use_system_libtl2cgen:\n            copy_with_logging(libtl2cgen, lib_path, logger=logger)\n\n        with cd(workspace):\n            wheel_name = hatchling.build.build_wheel(\n                wheel_directory, config_settings, metadata_directory\n            )\n    return wheel_name", "\n\ndef build_sdist(\n    sdist_directory: str,\n    config_settings: Optional[Dict[str, Any]] = None,\n) -> str:\n    \"\"\"Build a source distribution\"\"\"\n    logger = logging.getLogger(\"tl2cgen.packager.build_sdist\")\n\n    if config_settings:\n        raise NotImplementedError(\n            \"TL2cgen's custom build backend doesn't support config_settings option \"\n            f\"when building sdist. {config_settings=}\"\n        )\n\n    cpp_src_dir = TOPLEVEL_DIR.parent\n    if not cpp_src_dir.joinpath(\"CMakeLists.txt\").exists():\n        raise RuntimeError(f\"Did not find CMakeLists.txt from {cpp_src_dir}\")\n\n    # Create tempdir with Python package + C++ sources\n    with tempfile.TemporaryDirectory() as td:\n        td_path = pathlib.Path(td)\n\n        workspace = td_path / \"sdist_workspace\"\n        workspace.mkdir()\n        logger.info(\"Copying project files to temporary directory %s\", str(workspace))\n\n        copy_with_logging(TOPLEVEL_DIR / \"pyproject.toml\", workspace, logger=logger)\n        copy_with_logging(TOPLEVEL_DIR / \"hatch_build.py\", workspace, logger=logger)\n        copy_with_logging(TOPLEVEL_DIR / \"README.rst\", workspace, logger=logger)\n\n        copytree_with_logging(\n            TOPLEVEL_DIR / \"tl2cgen\", workspace / \"tl2cgen\", logger=logger\n        )\n        copytree_with_logging(\n            TOPLEVEL_DIR / \"packager\", workspace / \"packager\", logger=logger\n        )\n\n        temp_cpp_src_dir = workspace / \"cpp_src\"\n        copy_cpp_src_tree(cpp_src_dir, target_dir=temp_cpp_src_dir, logger=logger)\n\n        with cd(workspace):\n            sdist_name = hatchling.build.build_sdist(sdist_directory, config_settings)\n    return sdist_name", "\n\ndef build_editable(\n    wheel_directory: str,\n    config_settings: Optional[Dict[str, Any]] = None,\n    metadata_directory: Optional[str] = None,\n) -> str:\n    \"\"\"Build an editable installation. We mostly delegate to Hatchling.\"\"\"\n    logger = logging.getLogger(\"tl2cgen.packager.build_editable\")\n\n    if config_settings:\n        raise NotImplementedError(\n            \"TL2cgen's custom build backend doesn't support config_settings option \"\n            f\"when building editable installation. {config_settings=}\"\n        )\n\n    if locate_local_libtl2cgen(TOPLEVEL_DIR, logger=logger) is None:\n        raise RuntimeError(\n            \"To use the editable installation, first build libtl2cgen with CMake. \"\n            \"See https://tl2cgen.readthedocs.io/en/latest/build.html for detailed instructions.\"\n        )\n\n    return hatchling.build.build_editable(\n        wheel_directory, config_settings, metadata_directory\n    )", ""]}
{"filename": "python/packager/build_config.py", "chunked_list": ["\"\"\"Build configuration\"\"\"\nimport dataclasses\nfrom typing import Any, Dict, List, Optional\n\n\n@dataclasses.dataclass\nclass BuildConfiguration:  # pylint: disable=R0902\n    \"\"\"Configurations use when building libtl2cgen\"\"\"\n\n    # Whether to enable OpenMP\n    use_openmp: bool = True\n    # Whether to hide C++ symbols\n    hide_cxx_symbols: bool = True\n    # Whether to use the TL2cgen library that's installed in the system prefix\n    use_system_libtl2cgen: bool = False\n\n    def _set_config_setting(self, config_settings: Dict[str, Any]) -> None:\n        for field_name in config_settings:\n            setattr(\n                self,\n                field_name,\n                (config_settings[field_name].lower() in [\"true\", \"1\", \"on\"]),\n            )\n\n    def update(self, config_settings: Optional[Dict[str, Any]]) -> None:\n        \"\"\"Parse config_settings from Pip (or other PEP 517 frontend)\"\"\"\n        if config_settings is not None:\n            self._set_config_setting(config_settings)\n\n    def get_cmake_args(self) -> List[str]:\n        \"\"\"Convert build configuration to CMake args\"\"\"\n        cmake_args = []\n        for field_name in [x.name for x in dataclasses.fields(self)]:\n            if field_name in [\"use_system_libtl2cgen\"]:\n                continue\n            cmake_option = field_name.upper()\n            cmake_value = \"ON\" if getattr(self, field_name) is True else \"OFF\"\n            cmake_args.append(f\"-D{cmake_option}={cmake_value}\")\n        return cmake_args", ""]}
{"filename": "python/packager/__init__.py", "chunked_list": [""]}
{"filename": "python/packager/util.py", "chunked_list": ["\"\"\"\nUtility functions for implementing PEP 517 backend\n\"\"\"\nimport logging\nimport pathlib\nimport shutil\n\n\ndef copytree_with_logging(\n    src: pathlib.Path, dest: pathlib.Path, logger: logging.Logger\n) -> None:\n    \"\"\"Call shutil.copytree() with logging\"\"\"\n    logger.info(\"Copying %s -> %s\", str(src), str(dest))\n    shutil.copytree(src, dest)", "def copytree_with_logging(\n    src: pathlib.Path, dest: pathlib.Path, logger: logging.Logger\n) -> None:\n    \"\"\"Call shutil.copytree() with logging\"\"\"\n    logger.info(\"Copying %s -> %s\", str(src), str(dest))\n    shutil.copytree(src, dest)\n\n\ndef copy_with_logging(\n    src: pathlib.Path, dest: pathlib.Path, logger: logging.Logger\n) -> None:\n    \"\"\"Call shutil.copy() with logging\"\"\"\n    if dest.is_dir():\n        logger.info(\"Copying %s -> %s\", str(src), str(dest / src.name))\n    else:\n        logger.info(\"Copying %s -> %s\", str(src), str(dest))\n    shutil.copy(src, dest)", "def copy_with_logging(\n    src: pathlib.Path, dest: pathlib.Path, logger: logging.Logger\n) -> None:\n    \"\"\"Call shutil.copy() with logging\"\"\"\n    if dest.is_dir():\n        logger.info(\"Copying %s -> %s\", str(src), str(dest / src.name))\n    else:\n        logger.info(\"Copying %s -> %s\", str(src), str(dest))\n    shutil.copy(src, dest)\n", ""]}
{"filename": "python/packager/sdist.py", "chunked_list": ["\"\"\"\nFunctions for building sdist\n\"\"\"\nimport logging\nimport pathlib\n\nfrom .util import copy_with_logging, copytree_with_logging\n\n\ndef copy_cpp_src_tree(\n    cpp_src_dir: pathlib.Path, target_dir: pathlib.Path, logger: logging.Logger\n) -> None:\n    \"\"\"Copy C++ source tree into build directory\"\"\"\n\n    for subdir in [\n        \"src\",\n        \"include\",\n        \"cmake\",\n    ]:\n        copytree_with_logging(cpp_src_dir / subdir, target_dir / subdir, logger=logger)\n\n    for filename in [\"CMakeLists.txt\", \"LICENSE\"]:\n        copy_with_logging(cpp_src_dir.joinpath(filename), target_dir, logger=logger)", "\ndef copy_cpp_src_tree(\n    cpp_src_dir: pathlib.Path, target_dir: pathlib.Path, logger: logging.Logger\n) -> None:\n    \"\"\"Copy C++ source tree into build directory\"\"\"\n\n    for subdir in [\n        \"src\",\n        \"include\",\n        \"cmake\",\n    ]:\n        copytree_with_logging(cpp_src_dir / subdir, target_dir / subdir, logger=logger)\n\n    for filename in [\"CMakeLists.txt\", \"LICENSE\"]:\n        copy_with_logging(cpp_src_dir.joinpath(filename), target_dir, logger=logger)", ""]}
{"filename": "dev/prepare_pypi_release.py", "chunked_list": ["\"\"\"Simple script for preparing a PyPI release.\nIt fetches Python wheels from the CI pipelines.\ntqdm, packaging are required to run this script.\n\"\"\"\n\nimport argparse\nimport pathlib\nimport subprocess\nfrom typing import List, Optional\nfrom urllib.request import urlretrieve", "from typing import List, Optional\nfrom urllib.request import urlretrieve\n\nimport tqdm\nfrom packaging import version\n\nPREFIX = \"https://tl2cgen-wheels.s3.amazonaws.com/\"\nPROJECT_ROOT = pathlib.Path(__file__).expanduser().resolve().parent.parent\nDIST_DIR = PROJECT_ROOT / \"python\" / \"dist\"\n", "DIST_DIR = PROJECT_ROOT / \"python\" / \"dist\"\n\n\npbar = None  # pylint: disable=invalid-name\n\n\ndef show_progress(block_num, block_size, total_size):\n    \"\"\"Show file download progress.\"\"\"\n    global pbar  # pylint: disable=global-statement\n    if pbar is None:\n        pbar = tqdm.tqdm(total=total_size / 1024, unit=\"kB\")\n\n    downloaded = block_num * block_size\n    if downloaded < total_size:\n        upper = (total_size - downloaded) / 1024\n        pbar.update(min(block_size / 1024, upper))\n    else:\n        pbar.close()\n        pbar = None", "\n\ndef retrieve(url, filename=None):\n    \"\"\"Download a file from URL\"\"\"\n    print(f\"{url} -> {filename}\")\n    return urlretrieve(url, filename, reporthook=show_progress)\n\n\ndef latest_hash() -> str:\n    \"\"\"Get latest commit hash.\"\"\"\n    ret = subprocess.run([\"git\", \"rev-parse\", \"HEAD\"], capture_output=True, check=True)\n    assert ret.returncode == 0, \"Failed to get latest commit hash.\"\n    commit_hash = ret.stdout.decode(\"utf-8\").strip()\n    return commit_hash", "def latest_hash() -> str:\n    \"\"\"Get latest commit hash.\"\"\"\n    ret = subprocess.run([\"git\", \"rev-parse\", \"HEAD\"], capture_output=True, check=True)\n    assert ret.returncode == 0, \"Failed to get latest commit hash.\"\n    commit_hash = ret.stdout.decode(\"utf-8\").strip()\n    return commit_hash\n\n\ndef download_wheels(\n    platforms: List[str],\n    url_prefix: str,\n    dest_dir: pathlib.Path,\n    src_filename_prefix: str,\n    target_filename_prefix: str,\n    ext: str = \"whl\",\n) -> List[str]:\n    \"\"\"Download all binary wheels. url_prefix is the URL for remote directory storing\n    the release wheels\n    \"\"\"\n    # pylint: disable=too-many-arguments\n\n    filenames = []\n    for platform in platforms:\n        src_wheel = src_filename_prefix + platform + \".\" + ext\n        url = url_prefix + src_wheel\n\n        target_wheel = target_filename_prefix + platform + \".\" + ext\n        print(f\"{src_wheel} -> {target_wheel}\")\n        filename = dest_dir / target_wheel\n        filenames.append(str(filename))\n        retrieve(url=url, filename=filename)\n    return filenames", "def download_wheels(\n    platforms: List[str],\n    url_prefix: str,\n    dest_dir: pathlib.Path,\n    src_filename_prefix: str,\n    target_filename_prefix: str,\n    ext: str = \"whl\",\n) -> List[str]:\n    \"\"\"Download all binary wheels. url_prefix is the URL for remote directory storing\n    the release wheels\n    \"\"\"\n    # pylint: disable=too-many-arguments\n\n    filenames = []\n    for platform in platforms:\n        src_wheel = src_filename_prefix + platform + \".\" + ext\n        url = url_prefix + src_wheel\n\n        target_wheel = target_filename_prefix + platform + \".\" + ext\n        print(f\"{src_wheel} -> {target_wheel}\")\n        filename = dest_dir / target_wheel\n        filenames.append(str(filename))\n        retrieve(url=url, filename=filename)\n    return filenames", "\n\ndef download_py_packages(version_str: str, commit_hash: str) -> None:\n    \"\"\"Download Python packages\"\"\"\n    platforms = [\n        \"win_amd64\",\n        \"manylinux2014_x86_64\",\n        \"macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64\",\n        \"macosx_12_0_arm64\",\n    ]\n\n    if not DIST_DIR.exists():\n        DIST_DIR.mkdir()\n\n    # Binary wheels (*.whl)\n    src_filename_prefix = f\"tl2cgen-{version_str}%2B{commit_hash}-py3-none-\"\n    target_filename_prefix = f\"tl2cgen-{version_str}-py3-none-\"\n    filenames = download_wheels(\n        platforms, PREFIX, DIST_DIR, src_filename_prefix, target_filename_prefix\n    )\n    print(f\"List of downloaded wheels: {filenames}\\n\")\n\n    # Source distribution (*.tar.gz)\n    src_filename_prefix = f\"tl2cgen-{version_str}%2B{commit_hash}\"\n    target_filename_prefix = f\"tl2cgen-{version_str}\"\n    filenames = download_wheels(\n        [\"\"],\n        PREFIX,\n        DIST_DIR,\n        src_filename_prefix,\n        target_filename_prefix,\n        \"tar.gz\",\n    )\n    print(f\"List of downloaded sdist: {filenames}\\n\")\n    print(\n        \"\"\"\nFollowing steps should be done manually:\n- Upload pypi package by `python -m twine upload python/dist/*` for all wheels.\n- Check the uploaded files on `https://pypi.org/project/tl2cgen/<VERSION>/#files` and `pip\n  install tl2cgen==<VERSION>` \"\"\"\n    )", "\n\ndef main(args: argparse.Namespace) -> None:\n    \"\"\"Entry function\"\"\"\n    rel = version.parse(args.release)\n    assert isinstance(rel, version.Version)\n\n    if not rel.is_prerelease:\n        # Major release\n        rc: Optional[str] = None\n    else:\n        # RC release\n        assert rel.pre is not None\n        rc, _ = rel.pre\n        assert rc == \"rc\"\n\n    commit_hash = latest_hash()\n\n    download_py_packages(args.release, commit_hash)", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--release\",\n        type=str,\n        required=True,\n        help=\"Version tag, e.g. '1.3.2', or '1.5.0rc1'\",\n    )\n    parsed_args = parser.parse_args()\n    main(parsed_args)", ""]}
{"filename": "dev/run_pylint.py", "chunked_list": ["\"\"\"Wrapper for Pylint\"\"\"\n\nimport os\nimport pathlib\nimport subprocess\nimport sys\n\nROOT_PATH = pathlib.Path(__file__).parent.parent.expanduser().resolve()\nPYPKG_PATH = ROOT_PATH / \"python\"\nPYLINTRC_PATH = PYPKG_PATH / \".pylintrc\"", "PYPKG_PATH = ROOT_PATH / \"python\"\nPYLINTRC_PATH = PYPKG_PATH / \".pylintrc\"\n\n\ndef main():\n    \"\"\"Wrapper for Pylint. Add tl2cgen to PYTHONPATH so that pylint doesn't error out\"\"\"\n    new_env = os.environ.copy()\n    new_env[\"PYTHONPATH\"] = str(PYPKG_PATH)\n\n    # sys.argv[1:]: List of source files to check\n    subprocess.run(\n        [\"pylint\", \"-rn\", \"-sn\", \"--rcfile\", str(PYLINTRC_PATH)] + sys.argv[1:],\n        check=True,\n        env=new_env,\n    )", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "dev/change_version.py", "chunked_list": ["\"\"\"\nThis script changes the version field in different parts of the code base.\n\"\"\"\n\nimport argparse\nimport pathlib\nimport re\nfrom typing import Optional, TypeVar\n\nR = TypeVar(\"R\")", "\nR = TypeVar(\"R\")\nROOT = pathlib.Path(__file__).parent.parent.expanduser().resolve()\nPY_PACKAGE = ROOT / \"python\"\nJAVA_PACKAGE = ROOT / \"java_runtime\"\n\n\ndef update_cmake(major: int, minor: int, patch: int) -> None:\n    \"\"\"Change version in CMakeLists.txt\"\"\"\n    version = f\"{major}.{minor}.{patch}\"\n    with open(ROOT / \"CMakeLists.txt\", \"r\", encoding=\"utf-8\") as fd:\n        cmakelist = fd.read()\n    pattern = r\"project\\(tl2cgen LANGUAGES .* VERSION ([0-9]+\\.[0-9]+\\.[0-9]+)\\)\"\n    matched = re.search(pattern, cmakelist)\n    assert matched, \"Couldn't find the version string in CMakeLists.txt.\"\n    cmakelist = cmakelist[: matched.start(1)] + version + cmakelist[matched.end(1) :]\n    with open(ROOT / \"CMakeLists.txt\", \"w\", encoding=\"utf-8\") as fd:\n        fd.write(cmakelist)", "\n\ndef update_pypkg(\n    major: int,\n    minor: int,\n    patch: int,\n    *,\n    is_rc: bool,\n    is_dev: bool,\n    rc_ver: Optional[int] = None,\n) -> None:\n    \"\"\"Change version in the Python package\"\"\"\n    version = f\"{major}.{minor}.{patch}\"\n    if is_rc:\n        assert rc_ver\n        version = version + f\"rc{rc_ver}\"\n    if is_dev:\n        version = version + \"-dev\"\n\n    pyver_path = PY_PACKAGE / \"tl2cgen\" / \"VERSION\"\n    with open(pyver_path, \"w\", encoding=\"utf-8\") as fd:\n        fd.write(version + \"\\n\")\n\n    pyprj_path = PY_PACKAGE / \"pyproject.toml\"\n    with open(pyprj_path, \"r\", encoding=\"utf-8\") as fd:\n        pyprj = fd.read()\n    matched = re.search('version = \"' + r\"([0-9]+\\.[0-9]+\\.[0-9]+.*)\" + '\"', pyprj)\n    assert matched, \"Couldn't find version string in pyproject.toml.\"\n    pyprj = pyprj[: matched.start(1)] + version + pyprj[matched.end(1) :]\n    with open(pyprj_path, \"w\", encoding=\"utf-8\") as fd:\n        fd.write(pyprj)", "\n\ndef update_java_pkg(\n    major: int,\n    minor: int,\n    patch: int,\n    *,\n    is_rc: bool,\n    is_dev: bool,\n    rc_ver: Optional[int] = None,\n) -> None:\n    \"\"\"Change version in the Java package\"\"\"\n    version = f\"{major}.{minor}.{patch}\"\n    if is_rc:\n        assert rc_ver\n        version = version + f\"-RC{rc_ver}\"\n    if is_dev:\n        version = version + \"-SNAPSHOT\"\n\n    pom_path = JAVA_PACKAGE / \"tl2cgen4j\" / \"pom.xml\"\n    with open(pom_path, \"r\", encoding=\"utf-8\") as fd:\n        pom = fd.read()\n    matched = re.search(\"<version>\" + r\"([0-9]+\\.[0-9]+\\.[0-9]+.*)\" + \"</version>\", pom)\n    assert matched, \"Couldn't find version string in pom.xml.\"\n    pom = pom[: matched.start(1)] + version + pom[matched.end(1) :]\n    with open(pom_path, \"w\", encoding=\"utf-8\") as fd:\n        fd.write(pom)", "\n\ndef main(args: argparse.Namespace) -> None:\n    \"\"\"Perform version change in all relevant parts of the code base.\"\"\"\n    if args.is_rc and args.is_dev:\n        raise ValueError(\"A release version cannot be both RC and dev.\")\n    if args.is_rc:\n        assert args.rc is not None, \"rc field must be specified if is_rc is specified\"\n        assert args.rc >= 1, \"RC version must start from 1.\"\n    else:\n        assert args.rc is None, \"is_rc must be specified in order to specify rc field\"\n    update_cmake(args.major, args.minor, args.patch)\n    update_pypkg(\n        args.major,\n        args.minor,\n        args.patch,\n        is_rc=args.is_rc,\n        is_dev=args.is_dev,\n        rc_ver=args.rc,\n    )\n    update_java_pkg(\n        args.major,\n        args.minor,\n        args.patch,\n        is_rc=args.is_rc,\n        is_dev=args.is_dev,\n        rc_ver=args.rc,\n    )", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--major\", type=int, required=True)\n    parser.add_argument(\"--minor\", type=int, required=True)\n    parser.add_argument(\"--patch\", type=int, required=True)\n    parser.add_argument(\"--rc\", type=int)\n    parser.add_argument(\"--is-rc\", type=int, choices=[0, 1], default=0)\n    parser.add_argument(\"--is-dev\", type=int, choices=[0, 1], default=0)\n    parsed_args = parser.parse_args()\n    main(parsed_args)", ""]}
{"filename": "docs/conf.py", "chunked_list": ["# pylint: skip-file\n# documentation build configuration file\nimport os\nimport pathlib\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport warnings\n", "import warnings\n\nfrom sh.contrib import git\n\nPROJECT_ROOT = pathlib.Path(__file__).expanduser().resolve().parent.parent\nCURR_PATH = PROJECT_ROOT / \"docs\"\nDOX_DIR = PROJECT_ROOT / \"doxygen\"\n\n\ndef run_doxygen():\n    \"\"\"Run the doxygen make command in the designated folder.\"\"\"\n    tmpdir = CURR_PATH / \"tmp\"\n    if tmpdir.exists():\n        shutil.rmtree(tmpdir)\n    else:\n        tmpdir.mkdir()\n    try:\n        if not DOX_DIR.exists():\n            DOX_DIR.mkdir()\n        os.chdir(os.path.join(PROJECT_ROOT, DOX_DIR))\n        subprocess.run(\n            [\"cmake\", \"..\", \"-DBUILD_DOXYGEN=ON\", \"-GNinja\"], check=True, cwd=DOX_DIR\n        )\n        subprocess.run([\"ninja\", \"tl2cgen_doc_doxygen\"], check=True, cwd=DOX_DIR)\n        shutil.copytree(DOX_DIR / \"doc_doxygen\" / \"html\", tmpdir / \"dev\")\n    except OSError as e:\n        raise RuntimeError(f\"Doxygen execution failed {str(e)}\") from e", "\ndef run_doxygen():\n    \"\"\"Run the doxygen make command in the designated folder.\"\"\"\n    tmpdir = CURR_PATH / \"tmp\"\n    if tmpdir.exists():\n        shutil.rmtree(tmpdir)\n    else:\n        tmpdir.mkdir()\n    try:\n        if not DOX_DIR.exists():\n            DOX_DIR.mkdir()\n        os.chdir(os.path.join(PROJECT_ROOT, DOX_DIR))\n        subprocess.run(\n            [\"cmake\", \"..\", \"-DBUILD_DOXYGEN=ON\", \"-GNinja\"], check=True, cwd=DOX_DIR\n        )\n        subprocess.run([\"ninja\", \"tl2cgen_doc_doxygen\"], check=True, cwd=DOX_DIR)\n        shutil.copytree(DOX_DIR / \"doc_doxygen\" / \"html\", tmpdir / \"dev\")\n    except OSError as e:\n        raise RuntimeError(f\"Doxygen execution failed {str(e)}\") from e", "\n\ndef is_readthedocs_build():\n    if os.environ.get(\"READTHEDOCS\", None) == \"True\":\n        return True\n    warnings.warn(\n        \"Skipping Doxygen build... You won't have documentation for C/C++ functions. \"\n        \"Set environment variable READTHEDOCS=True if you want to build Doxygen. \"\n        \"(If you do opt in, make sure to install Doxygen, Graphviz, CMake, and C++ compiler \"\n        \"on your system.)\"\n    )\n    return False", "\n\nif is_readthedocs_build():\n    run_doxygen()\n\n\ngit_branch_env = os.getenv(\"SPHINX_GIT_BRANCH\", default=None)\nif not git_branch_env:\n    # If SPHINX_GIT_BRANCH environment variable is not given, run git\n    # to determine branch name\n    git_branch = [\n        re.sub(r\"origin/\", \"\", x.lstrip(\" \"))\n        for x in str(git.branch(\"-r\", \"--contains\", \"HEAD\")).rstrip(\"\\n\").split(\"\\n\")\n    ]\n    git_branch = [x for x in git_branch if \"HEAD\" not in x]\nelse:\n    git_branch = [git_branch_env]", "print(f\"git_branch = {git_branch[0]}\")\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nlibpath = PROJECT_ROOT / \"python\"\nsys.path.insert(0, str(libpath))\nsys.path.insert(0, str(CURR_PATH))\n\n# -- General configuration ------------------------------------------------", "\n# -- General configuration ------------------------------------------------\n\n# General information about the project.\nproject = \"TL2cgen\"\nauthor = f\"{project} developers\"\ncopyright = f\"2023, {author}\"\ngithub_doc_root = \"https://github.com/dmlc/tl2cgen/tree/mainline/docs/\"\n\nos.environ[\"TL2CGEN_BUILD_DOC\"] = \"1\"", "\nos.environ[\"TL2CGEN_BUILD_DOC\"] = \"1\"\n# Version information.\nwith open(PROJECT_ROOT / \"python\" / \"tl2cgen\" / \"VERSION\", \"r\", encoding=\"utf-8\") as f:\n    version = f.read().rstrip()\nrelease = version\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones\nextensions = [", "# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones\nextensions = [\n    \"matplotlib.sphinxext.plot_directive\",\n    \"sphinxcontrib.jquery\",\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.mathjax\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx_gallery.gen_gallery\",\n    \"breathe\",", "    \"sphinx_gallery.gen_gallery\",\n    \"breathe\",\n    \"autodocsumm\",\n]\n\nsphinx_gallery_conf = {\n    # path to your example scripts\n    \"examples_dirs\": [],\n    # path to where to save gallery generated output\n    \"gallery_dirs\": [],", "    # path to where to save gallery generated output\n    \"gallery_dirs\": [],\n    \"matplotlib_animations\": True,\n}\n\nautodoc_typehints = \"description\"\n\nautodoc_default_options = {\n    \"autosummary\": True,\n}", "    \"autosummary\": True,\n}\n\ngraphviz_output_format = \"png\"\nplot_formats = [(\"svg\", 300), (\"png\", 100), (\"hires.png\", 300)]\nplot_html_show_source_link = False\nplot_html_show_formats = False\n\n# Breathe extension variables\nbreathe_projects = {}\nif is_readthedocs_build():\n    breathe_projects = {\n        \"tl2cgen\": os.path.join(PROJECT_ROOT, DOX_DIR, \"doc_doxygen/xml\")\n    }", "# Breathe extension variables\nbreathe_projects = {}\nif is_readthedocs_build():\n    breathe_projects = {\n        \"tl2cgen\": os.path.join(PROJECT_ROOT, DOX_DIR, \"doc_doxygen/xml\")\n    }\nbreathe_default_project = \"tl2cgen\"\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]", "# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\nsource_suffix = [\".rst\", \".md\"]\n\n# The encoding of source files.\n# source_encoding = 'utf-8-sig'\n", "# source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.", "# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = \"en\"\n\nautoclass_content = \"both\"\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = ''\n# Else, today_fmt is used as the format for a strftime call.", "# today = ''\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\"_build\"]\nhtml_extra_path = []\nif is_readthedocs_build():\n    html_extra_path = [os.path.join(CURR_PATH, \"tmp\")]", "if is_readthedocs_build():\n    html_extra_path = [os.path.join(CURR_PATH, \"tmp\")]\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n", "# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n", "# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built documents.\n# keep_warnings = False", "# If true, keep warnings as \"system message\" paragraphs in the built documents.\n# keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.", "# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \"sphinx_rtd_theme\"\nhtml_theme_options = {\"logo_only\": False}\n\nhtml_css_files = [\"css/custom.css\"]\n\nhtml_sidebars = {\"**\": [\"logo-text.html\", \"globaltoc.html\", \"searchbox.html\"]}\n\n# Add any paths that contain custom static files (such as style sheets) here,", "\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = project + \"doc\"\n\n# -- Options for LaTeX output ---------------------------------------------", "\n# -- Options for LaTeX output ---------------------------------------------\nlatex_elements = {}  # type: ignore\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, f\"{project}.tex\", project, author, \"manual\"),\n]", "    (master_doc, f\"{project}.tex\", project, author, \"manual\"),\n]\n\nintersphinx_mapping = {\n    \"python\": (\"https://docs.python.org/3.8\", None),\n    \"numpy\": (\"https://numpy.org/doc/stable/\", None),\n    \"scipy\": (\"https://docs.scipy.org/doc/scipy/\", None),\n    \"pandas\": (\"https://pandas.pydata.org/pandas-docs/stable/\", None),\n    \"sklearn\": (\"https://scikit-learn.org/stable\", None),\n    \"treelite\": (\"https://treelite.readthedocs.io/en/latest/\", None),", "    \"sklearn\": (\"https://scikit-learn.org/stable\", None),\n    \"treelite\": (\"https://treelite.readthedocs.io/en/latest/\", None),\n}\n\n\ndef setup(app):\n    app.add_css_file(\"custom.css\")\n"]}
