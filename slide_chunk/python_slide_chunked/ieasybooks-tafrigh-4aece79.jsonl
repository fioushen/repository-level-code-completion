{"filename": "tafrigh/writer.py", "chunked_list": ["import os\nfrom pathlib import Path\nfrom typing import Union\n\nfrom tafrigh.config import Config\nfrom tafrigh.types.transcript_type import TranscriptType\nfrom tafrigh.utils import time_utils\n\n\nclass Writer:\n    def write_all(\n        self,\n        file_name: str,\n        segments: list[dict[str, Union[str, float]]],\n        output_config: Config.Output,\n    ) -> None:\n        if output_config.save_files_before_compact:\n            for output_format in output_config.output_formats:\n                self.write(\n                    output_format,\n                    os.path.join(output_config.output_dir, f'{file_name}-original.{output_format}'),\n                    segments,\n                )\n\n        if not output_config.save_files_before_compact or output_config.min_words_per_segment != 0:\n            compacted_segments = self.compact_segments(segments, output_config.min_words_per_segment)\n\n            for output_format in output_config.output_formats:\n                self.write(\n                    output_format,\n                    os.path.join(output_config.output_dir, f'{file_name}.{output_format}'),\n                    compacted_segments,\n                )\n\n    def write(\n        self,\n        format: TranscriptType,\n        file_path: str,\n        segments: list[dict[str, Union[str, float]]],\n    ) -> None:\n        if format == TranscriptType.TXT:\n            self.write_txt(file_path, segments)\n        elif format == TranscriptType.SRT:\n            self.write_srt(file_path, segments)\n        elif format == TranscriptType.VTT:\n            self.write_vtt(file_path, segments)\n\n    def write_txt(\n        self,\n        file_path: str,\n        segments: list[dict[str, Union[str, float]]],\n    ) -> None:\n        self._write_to_file(file_path, self.generate_txt(segments))\n\n    def write_srt(\n        self,\n        file_path: str,\n        segments: list[dict[str, Union[str, float]]],\n    ) -> None:\n        self._write_to_file(file_path, self.generate_srt(segments))\n\n    def write_vtt(\n        self,\n        file_path: str,\n        segments: list[dict[str, Union[str, float]]],\n    ) -> None:\n        self._write_to_file(file_path, self.generate_vtt(segments))\n\n    def generate_txt(self, segments: list[dict[str, Union[str, float]]]) -> str:\n        return '\\n'.join(list(map(lambda segment: segment['text'].strip(), segments))) + '\\n'\n\n    def generate_srt(self, segments: list[dict[str, Union[str, float]]]) -> str:\n        return ''.join(\n            f\"{i}\\n\"\n            f\"{time_utils.format_timestamp(segment['start'], include_hours=True, decimal_marker=',')} --> \"\n            f\"{time_utils.format_timestamp(segment['end'], include_hours=True, decimal_marker=',')}\\n\"\n            f\"{segment['text'].strip()}\\n\\n\"\n            for i, segment in enumerate(segments, start=1)\n        )\n\n    def generate_vtt(self, segments: list[dict[str, Union[str, float]]]) -> str:\n        return 'WEBVTT\\n\\n' + ''.join(\n            f\"{time_utils.format_timestamp(segment['start'])} --> {time_utils.format_timestamp(segment['end'])}\\n\"\n            f\"{segment['text'].strip()}\\n\\n\"\n            for segment in segments\n        )\n\n    def compact_segments(\n        self,\n        segments: list[dict[str, Union[str, float]]],\n        min_words_per_segment: int,\n    ) -> list[dict[str, Union[str, float]]]:\n        if min_words_per_segment == 0:\n            return segments\n\n        compacted_segments = []\n        tmp_segment = None\n\n        for segment in segments:\n            if tmp_segment:\n                tmp_segment['text'] += f\" {segment['text'].strip()}\"\n                tmp_segment['end'] = segment['end']\n\n                if len(tmp_segment['text'].split()) >= min_words_per_segment:\n                    compacted_segments.append(tmp_segment)\n                    tmp_segment = None\n            elif len(segment['text'].split()) < min_words_per_segment:\n                tmp_segment = dict(segment)\n            elif len(segment['text'].split()) >= min_words_per_segment:\n                compacted_segments.append(dict(segment))\n\n        if tmp_segment:\n            compacted_segments.append(tmp_segment)\n\n        return compacted_segments\n\n    def is_output_exist(self, file_name: str, output_config: Config.Output):\n        if output_config.save_files_before_compact and not all(\n            Path(output_config.output_dir, f'{file_name}-original.{output_format}').is_file()\n            for output_format in output_config.output_formats\n        ):\n            return False\n\n        if (not output_config.save_files_before_compact or output_config.min_words_per_segment != 0) and not all(\n            Path(output_config.output_dir, f'{file_name}.{output_format}').is_file()\n            for output_format in output_config.output_formats\n        ):\n            return False\n\n        return True\n\n    def _write_to_file(self, file_path: str, content: str) -> None:\n        with open(file_path, 'w', encoding='utf-8') as fp:\n            fp.write(content)", "\nclass Writer:\n    def write_all(\n        self,\n        file_name: str,\n        segments: list[dict[str, Union[str, float]]],\n        output_config: Config.Output,\n    ) -> None:\n        if output_config.save_files_before_compact:\n            for output_format in output_config.output_formats:\n                self.write(\n                    output_format,\n                    os.path.join(output_config.output_dir, f'{file_name}-original.{output_format}'),\n                    segments,\n                )\n\n        if not output_config.save_files_before_compact or output_config.min_words_per_segment != 0:\n            compacted_segments = self.compact_segments(segments, output_config.min_words_per_segment)\n\n            for output_format in output_config.output_formats:\n                self.write(\n                    output_format,\n                    os.path.join(output_config.output_dir, f'{file_name}.{output_format}'),\n                    compacted_segments,\n                )\n\n    def write(\n        self,\n        format: TranscriptType,\n        file_path: str,\n        segments: list[dict[str, Union[str, float]]],\n    ) -> None:\n        if format == TranscriptType.TXT:\n            self.write_txt(file_path, segments)\n        elif format == TranscriptType.SRT:\n            self.write_srt(file_path, segments)\n        elif format == TranscriptType.VTT:\n            self.write_vtt(file_path, segments)\n\n    def write_txt(\n        self,\n        file_path: str,\n        segments: list[dict[str, Union[str, float]]],\n    ) -> None:\n        self._write_to_file(file_path, self.generate_txt(segments))\n\n    def write_srt(\n        self,\n        file_path: str,\n        segments: list[dict[str, Union[str, float]]],\n    ) -> None:\n        self._write_to_file(file_path, self.generate_srt(segments))\n\n    def write_vtt(\n        self,\n        file_path: str,\n        segments: list[dict[str, Union[str, float]]],\n    ) -> None:\n        self._write_to_file(file_path, self.generate_vtt(segments))\n\n    def generate_txt(self, segments: list[dict[str, Union[str, float]]]) -> str:\n        return '\\n'.join(list(map(lambda segment: segment['text'].strip(), segments))) + '\\n'\n\n    def generate_srt(self, segments: list[dict[str, Union[str, float]]]) -> str:\n        return ''.join(\n            f\"{i}\\n\"\n            f\"{time_utils.format_timestamp(segment['start'], include_hours=True, decimal_marker=',')} --> \"\n            f\"{time_utils.format_timestamp(segment['end'], include_hours=True, decimal_marker=',')}\\n\"\n            f\"{segment['text'].strip()}\\n\\n\"\n            for i, segment in enumerate(segments, start=1)\n        )\n\n    def generate_vtt(self, segments: list[dict[str, Union[str, float]]]) -> str:\n        return 'WEBVTT\\n\\n' + ''.join(\n            f\"{time_utils.format_timestamp(segment['start'])} --> {time_utils.format_timestamp(segment['end'])}\\n\"\n            f\"{segment['text'].strip()}\\n\\n\"\n            for segment in segments\n        )\n\n    def compact_segments(\n        self,\n        segments: list[dict[str, Union[str, float]]],\n        min_words_per_segment: int,\n    ) -> list[dict[str, Union[str, float]]]:\n        if min_words_per_segment == 0:\n            return segments\n\n        compacted_segments = []\n        tmp_segment = None\n\n        for segment in segments:\n            if tmp_segment:\n                tmp_segment['text'] += f\" {segment['text'].strip()}\"\n                tmp_segment['end'] = segment['end']\n\n                if len(tmp_segment['text'].split()) >= min_words_per_segment:\n                    compacted_segments.append(tmp_segment)\n                    tmp_segment = None\n            elif len(segment['text'].split()) < min_words_per_segment:\n                tmp_segment = dict(segment)\n            elif len(segment['text'].split()) >= min_words_per_segment:\n                compacted_segments.append(dict(segment))\n\n        if tmp_segment:\n            compacted_segments.append(tmp_segment)\n\n        return compacted_segments\n\n    def is_output_exist(self, file_name: str, output_config: Config.Output):\n        if output_config.save_files_before_compact and not all(\n            Path(output_config.output_dir, f'{file_name}-original.{output_format}').is_file()\n            for output_format in output_config.output_formats\n        ):\n            return False\n\n        if (not output_config.save_files_before_compact or output_config.min_words_per_segment != 0) and not all(\n            Path(output_config.output_dir, f'{file_name}.{output_format}').is_file()\n            for output_format in output_config.output_formats\n        ):\n            return False\n\n        return True\n\n    def _write_to_file(self, file_path: str, content: str) -> None:\n        with open(file_path, 'w', encoding='utf-8') as fp:\n            fp.write(content)", ""]}
{"filename": "tafrigh/config.py", "chunked_list": ["import logging\n\nfrom tafrigh.types.transcript_type import TranscriptType\n\n\nclass Config:\n    def __init__(\n        self,\n        urls_or_paths: list[str],\n        skip_if_output_exist: bool,\n        playlist_items: str,\n        verbose: bool,\n        model_name_or_path: str,\n        task: str,\n        language: str,\n        use_faster_whisper: bool,\n        use_whisper_jax: bool,\n        beam_size: int,\n        ct2_compute_type: str,\n        wit_client_access_token: str,\n        max_cutting_duration: int,\n        min_words_per_segment: int,\n        save_files_before_compact: bool,\n        save_yt_dlp_responses: bool,\n        output_sample: int,\n        output_formats: list[str],\n        output_dir: str,\n    ):\n        self.input = self.Input(urls_or_paths, skip_if_output_exist, playlist_items, verbose)\n\n        self.whisper = self.Whisper(\n            model_name_or_path,\n            task,\n            language,\n            use_faster_whisper,\n            use_whisper_jax,\n            beam_size,\n            ct2_compute_type,\n        )\n\n        self.wit = self.Wit(wit_client_access_token, max_cutting_duration)\n\n        self.output = self.Output(\n            min_words_per_segment,\n            save_files_before_compact,\n            save_yt_dlp_responses,\n            output_sample,\n            output_formats,\n            output_dir,\n        )\n\n    def use_wit(self) -> bool:\n        return self.wit.wit_client_access_token != ''\n\n    class Input:\n        def __init__(self, urls_or_paths: list[str], skip_if_output_exist: bool, playlist_items: str, verbose: bool):\n            self.urls_or_paths = urls_or_paths\n            self.skip_if_output_exist = skip_if_output_exist\n            self.playlist_items = playlist_items\n            self.verbose = verbose\n\n    class Whisper:\n        def __init__(\n            self,\n            model_name_or_path: str,\n            task: str,\n            language: str,\n            use_faster_whisper: bool,\n            use_whisper_jax: bool,\n            beam_size: int,\n            ct2_compute_type: str,\n        ):\n            if model_name_or_path.endswith('.en'):\n                logging.warn(f'{model_name_or_path} is an English-only model, setting language to English.')\n                language = 'en'\n\n            self.model_name_or_path = model_name_or_path\n            self.task = task\n            self.language = language\n            self.use_faster_whisper = use_faster_whisper\n            self.use_whisper_jax = use_whisper_jax\n            self.beam_size = beam_size\n            self.ct2_compute_type = ct2_compute_type\n\n    class Wit:\n        def __init__(self, wit_client_access_token: str, max_cutting_duration: int):\n            self.wit_client_access_token = wit_client_access_token\n            self.max_cutting_duration = max_cutting_duration\n\n    class Output:\n        def __init__(\n            self,\n            min_words_per_segment: int,\n            save_files_before_compact: bool,\n            save_yt_dlp_responses: bool,\n            output_sample: int,\n            output_formats: list[str],\n            output_dir: str,\n        ):\n            if 'all' in output_formats:\n                output_formats = list(TranscriptType)\n            else:\n                output_formats = [TranscriptType(output_format) for output_format in output_formats]\n\n            if TranscriptType.ALL in output_formats:\n                output_formats.remove(TranscriptType.ALL)\n\n            if TranscriptType.NONE in output_formats:\n                output_formats.remove(TranscriptType.NONE)\n\n            self.min_words_per_segment = min_words_per_segment\n            self.save_files_before_compact = save_files_before_compact\n            self.save_yt_dlp_responses = save_yt_dlp_responses\n            self.output_sample = output_sample\n            self.output_formats = output_formats\n            self.output_dir = output_dir", ""]}
{"filename": "tafrigh/__init__.py", "chunked_list": ["from tafrigh.cli import farrigh\nfrom tafrigh.config import Config\nfrom tafrigh.downloader import Downloader\nfrom tafrigh.types.transcript_type import TranscriptType\nfrom tafrigh.writer import Writer\n\ntry:\n    from tafrigh.recognizers.whisper_recognizer import WhisperRecognizer\nexcept ModuleNotFoundError:\n    pass", "\ntry:\n    from tafrigh.audio_splitter import AudioSplitter\n    from tafrigh.recognizers.wit_recognizer import WitRecognizer\nexcept ModuleNotFoundError:\n    pass\n"]}
{"filename": "tafrigh/audio_splitter.py", "chunked_list": ["import os\nimport tempfile\n\nimport numpy as np\nfrom auditok.core import split\nfrom scipy.io import wavfile\n\n\nclass AudioSplitter:\n    def split(\n        self,\n        file_path: str,\n        output_dir: str,\n        min_dur: float = 0.5,\n        max_dur: float = 15,\n        max_silence: float = 0.5,\n        energy_threshold: float = 50,\n        expand_segments_with_noise: bool = False,\n        noise_seconds: int = 1,\n        noise_amplitude: int = 10,\n    ) -> list[tuple[str, float, float]]:\n        sampling_rate, data = self._read_audio(file_path)\n        temp_file_name = self._write_temp_audio(sampling_rate, data)\n        segments = self._split_audio(temp_file_name, min_dur, max_dur, max_silence, energy_threshold)\n\n        os.remove(temp_file_name)\n\n        if expand_segments_with_noise:\n            expanded_segments = self._expand_segments_with_noise(\n                segments,\n                noise_seconds,\n                noise_amplitude,\n                sampling_rate,\n                data.dtype,\n            )\n        else:\n            expanded_segments = [(segment, segment.meta.start, segment.meta.end) for segment in segments]\n\n        return self._save_segments(output_dir, sampling_rate, expanded_segments)\n\n    def _read_audio(self, file_path: str) -> tuple[int, np.ndarray]:\n        sampling_rate, data = wavfile.read(file_path)\n\n        if len(data.shape) > 1 and data.shape[1] > 1:\n            data = np.mean(data, axis=1)\n\n        return sampling_rate, data\n\n    def _write_audio(self, file_path: str, sampling_rate: int, data: np.ndarray) -> None:\n        wavfile.write(file_path, sampling_rate, data.astype(np.int16))\n\n    def _write_temp_audio(self, sampling_rate: int, data: np.ndarray) -> str:\n        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:\n            temp_file_name = temp_file.name\n            self._write_audio(temp_file_name, sampling_rate, data)\n\n        return temp_file_name\n\n    def _split_audio(\n        self,\n        temp_file_name: str,\n        min_dur: float,\n        max_dur: float,\n        max_silence: float,\n        energy_threshold: float,\n    ):\n        return split(\n            temp_file_name,\n            min_dur=min_dur,\n            max_dur=max_dur,\n            max_silence=max_silence,\n            energy_threshold=energy_threshold,\n        )\n\n    def _expand_segments_with_noise(\n        self,\n        segments: list,\n        noise_seconds: int,\n        noise_amplitude: int,\n        sampling_rate: int,\n        dtype: np.dtype,\n    ) -> list[tuple[np.ndarray, float, float]]:\n        expanded_segments = []\n\n        for segment in segments:\n            # Have different noise in the beginning and the end gave us better results :).\n            prepend_noise = np.random.normal(0, noise_amplitude, int(noise_seconds * sampling_rate)).astype(dtype)\n            append_noise = np.random.normal(0, noise_amplitude, int(noise_seconds * sampling_rate)).astype(dtype)\n\n            expanded_segment = np.concatenate((prepend_noise, segment, append_noise))\n            expanded_segments.append((expanded_segment, segment.meta.start, segment.meta.end))\n\n        return expanded_segments\n\n    def _save_segments(\n        self,\n        output_dir: str,\n        sampling_rate: int,\n        expanded_segments: list[tuple[np.ndarray, float, float]],\n    ) -> list[tuple[str, float, float]]:\n        segments = []\n\n        for i, (expanded_segment, start, end) in enumerate(expanded_segments):\n            output_file = os.path.join(output_dir, f\"segment_{i + 1}.wav\")\n            self._write_audio(output_file, sampling_rate, expanded_segment)\n            segments.append((output_file, start, end))\n\n        return segments", "class AudioSplitter:\n    def split(\n        self,\n        file_path: str,\n        output_dir: str,\n        min_dur: float = 0.5,\n        max_dur: float = 15,\n        max_silence: float = 0.5,\n        energy_threshold: float = 50,\n        expand_segments_with_noise: bool = False,\n        noise_seconds: int = 1,\n        noise_amplitude: int = 10,\n    ) -> list[tuple[str, float, float]]:\n        sampling_rate, data = self._read_audio(file_path)\n        temp_file_name = self._write_temp_audio(sampling_rate, data)\n        segments = self._split_audio(temp_file_name, min_dur, max_dur, max_silence, energy_threshold)\n\n        os.remove(temp_file_name)\n\n        if expand_segments_with_noise:\n            expanded_segments = self._expand_segments_with_noise(\n                segments,\n                noise_seconds,\n                noise_amplitude,\n                sampling_rate,\n                data.dtype,\n            )\n        else:\n            expanded_segments = [(segment, segment.meta.start, segment.meta.end) for segment in segments]\n\n        return self._save_segments(output_dir, sampling_rate, expanded_segments)\n\n    def _read_audio(self, file_path: str) -> tuple[int, np.ndarray]:\n        sampling_rate, data = wavfile.read(file_path)\n\n        if len(data.shape) > 1 and data.shape[1] > 1:\n            data = np.mean(data, axis=1)\n\n        return sampling_rate, data\n\n    def _write_audio(self, file_path: str, sampling_rate: int, data: np.ndarray) -> None:\n        wavfile.write(file_path, sampling_rate, data.astype(np.int16))\n\n    def _write_temp_audio(self, sampling_rate: int, data: np.ndarray) -> str:\n        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:\n            temp_file_name = temp_file.name\n            self._write_audio(temp_file_name, sampling_rate, data)\n\n        return temp_file_name\n\n    def _split_audio(\n        self,\n        temp_file_name: str,\n        min_dur: float,\n        max_dur: float,\n        max_silence: float,\n        energy_threshold: float,\n    ):\n        return split(\n            temp_file_name,\n            min_dur=min_dur,\n            max_dur=max_dur,\n            max_silence=max_silence,\n            energy_threshold=energy_threshold,\n        )\n\n    def _expand_segments_with_noise(\n        self,\n        segments: list,\n        noise_seconds: int,\n        noise_amplitude: int,\n        sampling_rate: int,\n        dtype: np.dtype,\n    ) -> list[tuple[np.ndarray, float, float]]:\n        expanded_segments = []\n\n        for segment in segments:\n            # Have different noise in the beginning and the end gave us better results :).\n            prepend_noise = np.random.normal(0, noise_amplitude, int(noise_seconds * sampling_rate)).astype(dtype)\n            append_noise = np.random.normal(0, noise_amplitude, int(noise_seconds * sampling_rate)).astype(dtype)\n\n            expanded_segment = np.concatenate((prepend_noise, segment, append_noise))\n            expanded_segments.append((expanded_segment, segment.meta.start, segment.meta.end))\n\n        return expanded_segments\n\n    def _save_segments(\n        self,\n        output_dir: str,\n        sampling_rate: int,\n        expanded_segments: list[tuple[np.ndarray, float, float]],\n    ) -> list[tuple[str, float, float]]:\n        segments = []\n\n        for i, (expanded_segment, start, end) in enumerate(expanded_segments):\n            output_file = os.path.join(output_dir, f\"segment_{i + 1}.wav\")\n            self._write_audio(output_file, sampling_rate, expanded_segment)\n            segments.append((output_file, start, end))\n\n        return segments", ""]}
{"filename": "tafrigh/cli.py", "chunked_list": ["import csv\nimport logging\nimport os\nimport random\nimport re\nimport sys\nfrom collections import deque\nfrom pathlib import Path\nfrom typing import Any, Generator, Union\n", "from typing import Any, Generator, Union\n\nfrom tqdm import tqdm\n\nfrom tafrigh.config import Config\nfrom tafrigh.downloader import Downloader\nfrom tafrigh.utils import cli_utils, file_utils, time_utils\nfrom tafrigh.writer import Writer\n\ntry:\n    import requests\n\n    from tafrigh.recognizers.wit_recognizer import WitRecognizer\n    from tafrigh.utils.wit import file_utils as wit_file_utils\nexcept ModuleNotFoundError:\n    pass", "\ntry:\n    import requests\n\n    from tafrigh.recognizers.wit_recognizer import WitRecognizer\n    from tafrigh.utils.wit import file_utils as wit_file_utils\nexcept ModuleNotFoundError:\n    pass\n\ntry:\n    from tafrigh.recognizers.whisper_recognizer import WhisperRecognizer\n    from tafrigh.types.whisper.type_hints import WhisperModel\n    from tafrigh.utils.whisper import whisper_utils\nexcept ModuleNotFoundError:\n    pass", "\ntry:\n    from tafrigh.recognizers.whisper_recognizer import WhisperRecognizer\n    from tafrigh.types.whisper.type_hints import WhisperModel\n    from tafrigh.utils.whisper import whisper_utils\nexcept ModuleNotFoundError:\n    pass\n\n\ndef main():\n    args = cli_utils.parse_args(sys.argv[1:])\n\n    config = Config(\n        urls_or_paths=args.urls_or_paths,\n        skip_if_output_exist=args.skip_if_output_exist,\n        playlist_items=args.playlist_items,\n        verbose=args.verbose,\n        #\n        model_name_or_path=args.model_name_or_path,\n        task=args.task,\n        language=args.language,\n        use_faster_whisper=args.use_faster_whisper,\n        use_whisper_jax=args.use_whisper_jax,\n        beam_size=args.beam_size,\n        ct2_compute_type=args.ct2_compute_type,\n        #\n        wit_client_access_token=args.wit_client_access_token,\n        max_cutting_duration=args.max_cutting_duration,\n        min_words_per_segment=args.min_words_per_segment,\n        #\n        save_files_before_compact=args.save_files_before_compact,\n        save_yt_dlp_responses=args.save_yt_dlp_responses,\n        output_sample=args.output_sample,\n        output_formats=args.output_formats,\n        output_dir=args.output_dir,\n    )\n\n    if config.use_wit() and config.input.skip_if_output_exist:\n        retries = 3\n\n        while retries > 0:\n            try:\n                deque(farrigh(config), maxlen=0)\n                break\n            except requests.exceptions.RetryError:\n                retries -= 1\n    else:\n        deque(farrigh(config), maxlen=0)", "\ndef main():\n    args = cli_utils.parse_args(sys.argv[1:])\n\n    config = Config(\n        urls_or_paths=args.urls_or_paths,\n        skip_if_output_exist=args.skip_if_output_exist,\n        playlist_items=args.playlist_items,\n        verbose=args.verbose,\n        #\n        model_name_or_path=args.model_name_or_path,\n        task=args.task,\n        language=args.language,\n        use_faster_whisper=args.use_faster_whisper,\n        use_whisper_jax=args.use_whisper_jax,\n        beam_size=args.beam_size,\n        ct2_compute_type=args.ct2_compute_type,\n        #\n        wit_client_access_token=args.wit_client_access_token,\n        max_cutting_duration=args.max_cutting_duration,\n        min_words_per_segment=args.min_words_per_segment,\n        #\n        save_files_before_compact=args.save_files_before_compact,\n        save_yt_dlp_responses=args.save_yt_dlp_responses,\n        output_sample=args.output_sample,\n        output_formats=args.output_formats,\n        output_dir=args.output_dir,\n    )\n\n    if config.use_wit() and config.input.skip_if_output_exist:\n        retries = 3\n\n        while retries > 0:\n            try:\n                deque(farrigh(config), maxlen=0)\n                break\n            except requests.exceptions.RetryError:\n                retries -= 1\n    else:\n        deque(farrigh(config), maxlen=0)", "\n\ndef farrigh(config: Config) -> Generator[dict[str, int], None, None]:\n    prepare_output_dir(config.output.output_dir)\n\n    model = None\n    if not config.use_wit():\n        model = whisper_utils.load_model(config.whisper)\n\n    segments = []\n\n    for idx, item in enumerate(tqdm(config.input.urls_or_paths, desc='URLs or local paths')):\n        progress_info = {\n            'outer_total': len(config.input.urls_or_paths),\n            'outer_current': idx + 1,\n            'outer_status': 'processing',\n        }\n\n        if Path(item).exists():\n            file_or_folder = Path(item)\n            for progress_info, local_elements_segments in process_local(file_or_folder, model, config, progress_info):\n                segments.extend(local_elements_segments)\n                yield progress_info\n        elif re.match('(https?://)', item):\n            for progress_info, url_elements_segments in process_url(item, model, config, progress_info):\n                segments.extend(url_elements_segments)\n                yield progress_info\n        else:\n            logging.error(f'Path {item} does not exist and is not a URL either.')\n\n            progress_info['outer_status'] = 'completed'\n            yield progress_info\n\n            continue\n\n        progress_info['outer_status'] = 'completed'\n        yield progress_info\n\n    write_output_sample(segments, config.output)", "\n\ndef prepare_output_dir(output_dir: str) -> None:\n    os.makedirs(output_dir, exist_ok=True)\n\n\ndef process_local(\n    path: Path,\n    model: 'WhisperModel',\n    config: Config,\n    progress_info: dict,\n) -> Generator[tuple[dict[str, int], list[list[dict[str, Union[str, float]]]]], None, None]:\n    filtered_media_files: list[Path] = file_utils.filter_media_files([path] if path.is_file() else path.iterdir())\n    files: list[dict[str, Any]] = [{'file_name': file.name, 'file_path': file} for file in filtered_media_files]\n\n    for idx, file in enumerate(tqdm(files, desc='Local files')):\n        new_progress_info = progress_info.copy()\n        new_progress_info.update(\n            {\n                'inner_total': len(files),\n                'inner_current': idx + 1,\n                'inner_status': 'processing',\n                'progress': 0.0,\n                'remaining_time': None,\n            }\n        )\n        yield new_progress_info, []\n\n        writer = Writer()\n        if config.input.skip_if_output_exist and writer.is_output_exist(Path(file['file_name']).stem, config.output):\n            new_progress_info['inner_status'] = 'completed'\n            yield new_progress_info, []\n\n            continue\n\n        file_path = str(file['file_path'].absolute())\n\n        if config.use_wit():\n            wav_file_path = str(wit_file_utils.convert_to_wav(file['file_path']).absolute())\n            recognize_generator = WitRecognizer(verbose=config.input.verbose).recognize(wav_file_path, config.wit)\n        else:\n            recognize_generator = WhisperRecognizer(verbose=config.input.verbose).recognize(\n                file_path,\n                model,\n                config.whisper,\n            )\n\n        while True:\n            try:\n                new_progress_info.update(next(recognize_generator))\n                yield new_progress_info, []\n            except StopIteration as exception:\n                segments = exception.value\n                break\n\n        if config.use_wit() and file['file_path'].suffix != '.wav':\n            Path(wav_file_path).unlink(missing_ok=True)\n\n        writer.write_all(Path(file['file_name']).stem, segments, config.output)\n\n        for segment in segments:\n            segment['url'] = f\"file://{file_path}&t={int(segment['start'])}\"\n            segment['file_path'] = file_path\n\n        new_progress_info['inner_status'] = 'completed'\n        new_progress_info['progress'] = 100.0\n        yield new_progress_info, writer.compact_segments(segments, config.output.min_words_per_segment)", "\n\ndef process_url(\n    url: str,\n    model: 'WhisperModel',\n    config: Config,\n    progress_info: dict,\n) -> Generator[tuple[dict[str, int], list[list[dict[str, Union[str, float]]]]], None, None]:\n    url_data = Downloader(playlist_items=config.input.playlist_items, output_dir=config.output.output_dir).download(\n        url,\n        save_response=config.output.save_yt_dlp_responses,\n    )\n\n    if '_type' in url_data and url_data['_type'] == 'playlist':\n        url_data = url_data['entries']\n    else:\n        url_data = [url_data]\n\n    for idx, element in enumerate(tqdm(url_data, desc='URL elements')):\n        if not element:\n            continue\n\n        new_progress_info = progress_info.copy()\n        new_progress_info.update(\n            {\n                'inner_total': len(url_data),\n                'inner_current': idx + 1,\n                'inner_status': 'processing',\n                'progress': 0.0,\n                'remaining_time': None,\n            }\n        )\n        yield new_progress_info, []\n\n        writer = Writer()\n        if config.input.skip_if_output_exist and writer.is_output_exist(element['id'], config.output):\n            new_progress_info['inner_status'] = 'completed'\n            yield new_progress_info, []\n\n            continue\n\n        file_path = os.path.join(config.output.output_dir, f\"{element['id']}.wav\")\n\n        if config.use_wit():\n            recognize_generator = WitRecognizer(verbose=config.input.verbose).recognize(file_path, config.wit)\n        else:\n            recognize_generator = WhisperRecognizer(verbose=config.input.verbose).recognize(\n                file_path,\n                model,\n                config.whisper,\n            )\n\n        while True:\n            try:\n                new_progress_info.update(next(recognize_generator))\n                yield new_progress_info, []\n            except StopIteration as exception:\n                segments = exception.value\n                break\n\n        writer.write_all(element['id'], segments, config.output)\n\n        for segment in segments:\n            segment['url'] = f\"https://youtube.com/watch?v={element['id']}&t={int(segment['start'])}\"\n            segment['file_path'] = file_path\n\n        new_progress_info['inner_status'] = 'completed'\n        new_progress_info['progress'] = 100.0\n        yield new_progress_info, writer.compact_segments(segments, config.output.min_words_per_segment)", "\n\ndef write_output_sample(segments: list[dict[str, Union[str, float]]], output: Config.Output) -> None:\n    if output.output_sample == 0:\n        return\n\n    random.shuffle(segments)\n\n    with open(os.path.join(output.output_dir, 'sample.csv'), 'w') as fp:\n        writer = csv.DictWriter(fp, fieldnames=['start', 'end', 'text', 'url', 'file_path'])\n        writer.writeheader()\n\n        for segment in segments[: output.output_sample]:\n            segment['start'] = time_utils.format_timestamp(segment['start'], include_hours=True, decimal_marker=',')\n            segment['end'] = time_utils.format_timestamp(segment['end'], include_hours=True, decimal_marker=',')\n            writer.writerow(segment)", ""]}
{"filename": "tafrigh/downloader.py", "chunked_list": ["import json\nimport os\nfrom typing import Any, Union\n\nimport yt_dlp\n\n\nclass Downloader:\n    def __init__(self, playlist_items: str, output_dir: str):\n        self.playlist_items = playlist_items\n        self.output_dir = output_dir\n        self.youtube_dl_with_archive = yt_dlp.YoutubeDL(self._config(os.path.join(self.output_dir, 'archive.txt')))\n        self.youtube_dl_without_archive = yt_dlp.YoutubeDL(self._config(False))\n\n    def _config(self, download_archive: Union[str, bool]) -> dict[str, Any]:\n        return {\n            'quiet': True,\n            'verbose': False,\n            'format': 'wav/bestaudio/best',\n            'outtmpl': os.path.join(self.output_dir, '%(id)s.%(ext)s'),\n            'ignoreerrors': True,\n            'download_archive': download_archive,\n            'playlist_items': self.playlist_items,\n            'postprocessors': [\n                {\n                    'key': 'FFmpegExtractAudio',\n                    'preferredcodec': 'wav',\n                },\n            ],\n        }\n\n    def download(self, url: str, save_response: bool = False) -> dict[str, Any]:\n        self.youtube_dl_with_archive.download(url)\n        url_data = self.youtube_dl_without_archive.extract_info(url, download=False)\n\n        if save_response:\n            self._save_response(url_data)\n\n        return url_data\n\n    def _save_response(self, url_data: dict[str, Any]) -> None:\n        if '_type' in url_data and url_data['_type'] == 'playlist':\n            for entry in url_data['entries']:\n                if entry and 'requested_downloads' in entry:\n                    self._remove_postprocessors(entry['requested_downloads'])\n        elif 'requested_downloads' in url_data:\n            self._remove_postprocessors(url_data['requested_downloads'])\n\n        file_path = os.path.join(self.output_dir, f\"{url_data['id']}.json\")\n\n        with open(file_path, 'w', encoding='utf-8') as fp:\n            json.dump(url_data, fp, indent=2, ensure_ascii=False)\n\n    def _remove_postprocessors(self, requested_downloads: list[dict[str, Any]]) -> None:\n        for requested_download in requested_downloads:\n            requested_download.pop('__postprocessors')", ""]}
{"filename": "tafrigh/recognizers/__init__.py", "chunked_list": [""]}
{"filename": "tafrigh/recognizers/wit_recognizer.py", "chunked_list": ["import json\nimport logging\nimport multiprocessing\nimport os\nimport shutil\nimport tempfile\nimport time\nfrom typing import Generator, Union\n\nimport requests", "\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom tqdm import tqdm\nfrom urllib3.util.retry import Retry\n\nfrom tafrigh.audio_splitter import AudioSplitter\nfrom tafrigh.config import Config\nfrom tafrigh.utils.decorators import minimum_execution_time\n", "from tafrigh.utils.decorators import minimum_execution_time\n\n\nclass WitRecognizer:\n    def __init__(self, verbose: bool):\n        self.verbose = verbose\n\n    def recognize(\n        self,\n        file_path: str,\n        wit_config: Config.Wit,\n    ) -> Generator[dict[str, float], None, list[dict[str, Union[str, float]]]]:\n        temp_directory = tempfile.mkdtemp()\n\n        segments = AudioSplitter().split(\n            file_path,\n            temp_directory,\n            max_dur=wit_config.max_cutting_duration,\n            expand_segments_with_noise=True,\n        )\n\n        retry_strategy = Retry(\n            total=5,\n            status_forcelist=[429, 500, 502, 503, 504],\n            allowed_methods=['POST'],\n            backoff_factor=1,\n        )\n\n        adapter = HTTPAdapter(max_retries=retry_strategy)\n\n        session = requests.Session()\n        session.mount('https://', adapter)\n\n        with multiprocessing.Pool(processes=min(4, multiprocessing.cpu_count() - 1)) as pool:\n            async_results = [\n                pool.apply_async(self._process_segment, (segment, file_path, wit_config, session))\n                for segment in segments\n            ]\n\n            transcriptions = []\n\n            with tqdm(total=len(segments), disable=self.verbose is not False) as pbar:\n                while async_results:\n                    if async_results[0].ready():\n                        transcriptions.append(async_results.pop(0).get())\n                        pbar.update(1)\n\n                    yield {\n                        'progress': round(len(transcriptions) / len(segments) * 100, 2),\n                        'remaining_time': (pbar.total - pbar.n) / pbar.format_dict['rate']\n                        if pbar.format_dict['rate'] and pbar.total\n                        else None,\n                    }\n\n                    time.sleep(0.5)\n\n        shutil.rmtree(temp_directory)\n\n        return transcriptions\n\n    @minimum_execution_time(min(4, multiprocessing.cpu_count() - 1) + 1)\n    def _process_segment(\n        self,\n        segment: tuple[str, float, float],\n        file_path: str,\n        wit_config: Config.Wit,\n        session: requests.Session,\n    ) -> dict[str, Union[str, float]]:\n        segment_file_path, start, end = segment\n\n        with open(segment_file_path, 'rb') as wav_file:\n            audio_content = wav_file.read()\n\n        retries = 5\n\n        text = ''\n        while retries > 0:\n            response = session.post(\n                'https://api.wit.ai/speech',\n                headers={\n                    'Accept': 'application/vnd.wit.20200513+json',\n                    'Content-Type': 'audio/wav',\n                    'Authorization': f'Bearer {wit_config.wit_client_access_token}',\n                },\n                data=audio_content,\n            )\n\n            if response.status_code == 200:\n                try:\n                    text = json.loads(response.text)['text']\n                    break\n                except KeyError:\n                    retries -= 1\n            else:\n                retries -= 1\n                time.sleep(min(4, multiprocessing.cpu_count() - 1) + 1)\n\n        if retries == 0:\n            logging.warn(\n                f\"The segment from `{file_path}` file that starts at {start} and ends at {end}\"\n                \" didn't transcribed successfully.\"\n            )\n\n        os.remove(segment_file_path)\n\n        return {\n            'start': start,\n            'end': end,\n            'text': text.strip(),\n        }", ""]}
{"filename": "tafrigh/recognizers/whisper_recognizer.py", "chunked_list": ["import warnings\nfrom typing import Generator, Union\n\nimport faster_whisper\nimport whisper\nimport whisper_jax\nfrom tqdm import tqdm\n\nfrom tafrigh.config import Config\nfrom tafrigh.types.whisper.type_hints import WhisperModel", "from tafrigh.config import Config\nfrom tafrigh.types.whisper.type_hints import WhisperModel\n\n\nclass WhisperRecognizer:\n    def __init__(self, verbose: bool):\n        self.verbose = verbose\n\n    def recognize(\n        self,\n        file_path: str,\n        model: WhisperModel,\n        whisper_config: Config.Whisper,\n    ) -> Generator[dict[str, float], None, list[dict[str, Union[str, float]]]]:\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n\n            if isinstance(model, whisper.Whisper):\n                whisper_generator = self._recognize_stable_whisper(file_path, model, whisper_config)\n            elif isinstance(model, faster_whisper.WhisperModel):\n                whisper_generator = self._recognize_faster_whisper(file_path, model, whisper_config)\n            elif isinstance(model, whisper_jax.FlaxWhisperPipline):\n                whisper_generator = self._recognize_jax_whisper(file_path, model, whisper_config)\n\n            while True:\n                try:\n                    yield next(whisper_generator)\n                except StopIteration as e:\n                    return e.value\n\n    def _recognize_stable_whisper(\n        self,\n        audio_file_path: str,\n        model: whisper.Whisper,\n        whisper_config: Config.Whisper,\n    ) -> Generator[dict[str, float], None, list[dict[str, Union[str, float]]]]:\n        yield {'progress': 0.0, 'remaining_time': None}\n\n        segments = model.transcribe(\n            audio=audio_file_path,\n            verbose=self.verbose,\n            task=whisper_config.task,\n            language=whisper_config.language,\n            beam_size=whisper_config.beam_size,\n        ).segments\n\n        return [\n            {\n                'start': segment.start,\n                'end': segment.end,\n                'text': segment.text.strip(),\n            }\n            for segment in segments\n        ]\n\n    def _recognize_faster_whisper(\n        self,\n        audio_file_path: str,\n        model: faster_whisper.WhisperModel,\n        whisper_config: Config.Whisper,\n    ) -> Generator[dict[str, float], None, list[dict[str, Union[str, float]]]]:\n        segments, info = model.transcribe(\n            audio=audio_file_path,\n            task=whisper_config.task,\n            language=whisper_config.language,\n            beam_size=whisper_config.beam_size,\n        )\n\n        converted_segments = []\n        last_end = 0\n        with tqdm(\n            total=round(info.duration, 2),\n            unit='sec',\n            bar_format='{desc}: {percentage:.2f}%|{bar}| {n:.2f}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]',\n            disable=self.verbose is not False,\n        ) as pbar:\n            for segment in segments:\n                converted_segments.append(\n                    {\n                        'start': segment.start,\n                        'end': segment.end,\n                        'text': segment.text.strip(),\n                    }\n                )\n\n                pbar_update = min(segment.end - last_end, info.duration - pbar.n)\n                pbar.update(pbar_update)\n                last_end = segment.end\n\n                yield {\n                    'progress': round(pbar.n / pbar.total * 100, 2),\n                    'remaining_time': (pbar.total - pbar.n) / pbar.format_dict['rate']\n                    if pbar.format_dict['rate'] and pbar.total\n                    else None,\n                }\n\n        return converted_segments\n\n    def _recognize_jax_whisper(\n        self,\n        audio_file_path: str,\n        model: whisper_jax.FlaxWhisperPipline,\n        whisper_config: Config.Whisper,\n    ) -> Generator[dict[str, float], None, list[dict[str, Union[str, float]]]]:\n        yield {'progress': 0.0, 'remaining_time': None}\n\n        segments = model(\n            audio_file_path,\n            task=whisper_config.task,\n            language=whisper_config.language,\n            return_timestamps=True,\n        )['chunks']\n\n        return [\n            {\n                'start': segment['timestamp'][0],\n                'end': segment['timestamp'][1],\n                'text': segment['text'].strip(),\n            }\n            for segment in segments\n        ]", ""]}
{"filename": "tafrigh/utils/decorators.py", "chunked_list": ["import time\nfrom functools import wraps\nfrom typing import Callable, TypeVar\n\nT = TypeVar(\"T\", bound=Callable)\n\n\ndef minimum_execution_time(minimum_time: float) -> Callable[[T], T]:\n    def decorator(func: T) -> T:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            start_time = time.time()\n            result = func(*args, **kwargs)\n            end_time = time.time()\n\n            elapsed_time = end_time - start_time\n            if elapsed_time < minimum_time:\n                time.sleep(minimum_time - elapsed_time)\n\n            return result\n\n        return wrapper\n\n    return decorator", ""]}
{"filename": "tafrigh/utils/time_utils.py", "chunked_list": ["def format_timestamp(seconds: float, include_hours: bool = False, decimal_marker: str = '.') -> str:\n    assert seconds >= 0, 'Non-negative timestamp expected'\n\n    total_milliseconds = int(round(seconds * 1_000))\n\n    hours, total_milliseconds = divmod(total_milliseconds, 3_600_000)\n    minutes, total_milliseconds = divmod(total_milliseconds, 60_000)\n    seconds, milliseconds = divmod(total_milliseconds, 1_000)\n\n    if include_hours or hours > 0:\n        time_str = f\"{hours:02d}:{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n    else:\n        time_str = f\"{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n\n    return time_str", ""]}
{"filename": "tafrigh/utils/file_utils.py", "chunked_list": ["import mimetypes\nfrom pathlib import Path\n\nmimetypes.init()\n\n\ndef filter_media_files(paths: list[Path]) -> list[Path]:\n    # Filter out non audio or video files\n    filtered_media_files: list[str] = []\n    for path in paths:\n        mime = mimetypes.guess_type(path)[0]\n        if mime is None:\n            continue\n        mime_type = mime.split('/')[0]\n        if mime_type not in ('audio', 'video'):\n            continue\n        filtered_media_files.append(path)\n    return filtered_media_files", ""]}
{"filename": "tafrigh/utils/cli_utils.py", "chunked_list": ["import argparse\nimport re\n\nfrom tafrigh.types.transcript_type import TranscriptType\n\nPLAYLIST_ITEMS_RE = re.compile(\n    r'''(?x)\n        (?P<start>[+-]?\\d+)?\n        (?P<range>[:-]\n            (?P<end>[+-]?\\d+|inf(?:inite)?)?", "        (?P<range>[:-]\n            (?P<end>[+-]?\\d+|inf(?:inite)?)?\n            (?::(?P<step>[+-]?\\d+))?\n        )?'''\n)\n\n\ndef parse_args(argv: list[str]) -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n\n    input_group = parser.add_argument_group('Input')\n\n    input_group.add_argument(\n        'urls_or_paths',\n        nargs='+',\n        help='Video/Playlist URLs or local folder/file(s) to transcribe.',\n    )\n\n    input_group.add_argument(\n        '--skip_if_output_exist',\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help='Whether to skip generating the output if the output file already exists.',\n    )\n\n    input_group.add_argument(\n        '--playlist_items',\n        type=parse_playlist_items,\n        help='Comma separated playlist_index of the items to download. You can specify a range using \"[START]:[STOP][:STEP]\".',\n    )\n\n    input_group.add_argument(\n        '--verbose',\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help='Whether to print out the progress and debug messages.',\n    )\n\n    whisper_group = parser.add_argument_group('Whisper')\n\n    whisper_group.add_argument(\n        '-m',\n        '--model_name_or_path',\n        default='small',\n        help='Name or path of the Whisper model to use.',\n    )\n\n    whisper_group.add_argument(\n        '-t',\n        '--task',\n        default='transcribe',\n        choices=[\n            'transcribe',\n            'translate',\n        ],\n        help=\"Whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate').\",\n    )\n\n    whisper_group.add_argument(\n        '-l',\n        '--language',\n        default=None,\n        choices=['af', 'am', 'ar', 'as', 'az', 'ba', 'be', 'bg', 'bn', 'bo', 'br', 'bs', 'ca', 'cs', 'cy', 'da', 'de']\n        + ['el', 'en', 'es', 'et', 'eu', 'fa', 'fi', 'fo', 'fr', 'gl', 'gu', 'ha', 'haw', 'he', 'hi', 'hr', 'ht', 'hu']\n        + ['hy', 'id', 'is', 'it', 'ja', 'jw', 'ka', 'kk', 'km', 'kn', 'ko', 'la', 'lb', 'ln', 'lo', 'lt', 'lv', 'mg']\n        + ['mi', 'mk', 'ml', 'mn', 'mr', 'ms', 'mt', 'my', 'ne', 'nl', 'nn', 'no', 'oc', 'pa', 'pl', 'ps', 'pt', 'ro']\n        + ['ru', 'sa', 'sd', 'si', 'sk', 'sl', 'sn', 'so', 'sq', 'sr', 'su', 'sv', 'sw', 'ta', 'te', 'tg', 'th', 'tk']\n        + ['tl', 'tr', 'tt', 'uk', 'ur', 'uz', 'vi', 'yi', 'yo', 'zh'],\n        help='Language spoken in the audio, skip to perform language detection.',\n    )\n\n    whisper_group.add_argument(\n        '--use_faster_whisper',\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help='Whether to use Faster Whisper implementation.',\n    )\n\n    whisper_group.add_argument(\n        '--use_whisper_jax',\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help='Whether to use Whisper JAX implementation. Make sure to have JAX installed before using this option.',\n    )\n\n    whisper_group.add_argument(\n        '--beam_size',\n        type=int,\n        default=5,\n        help='Number of beams in beam search, only applicable when temperature is zero.',\n    )\n\n    whisper_group.add_argument(\n        '--ct2_compute_type',\n        default='default',\n        choices=[\n            'default',\n            'int8',\n            'int8_float16',\n            'int16',\n            'float16',\n        ],\n        help='Quantization type applied while converting the model to CTranslate2 format.',\n    )\n\n    wit_group = parser.add_argument_group('Wit')\n\n    wit_group.add_argument(\n        '-w',\n        '--wit_client_access_token',\n        default='',\n        help='wit.ai client access token. If provided, wit.ai APIs will be used to do the transcription, otherwise whisper will be used.',\n    )\n\n    wit_group.add_argument(\n        '--max_cutting_duration',\n        type=int,\n        default=15,\n        choices=range(1, 17),\n        metavar='[1-17]',\n        help='The maximum allowed cutting duration. It should be between 1 and 17.',\n    )\n\n    output_group = parser.add_argument_group('Output')\n\n    output_group.add_argument(\n        '--min_words_per_segment',\n        type=int,\n        default=1,\n        help='The minimum number of words should appear in each transcript segment. Any segment have words count less than this threshold will be merged with the next one. Pass 0 to disable this behavior.',\n    )\n\n    output_group.add_argument(\n        '--save_files_before_compact',\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help='Saves the output files before applying the compact logic that is based on --min_words_per_segment.',\n    )\n\n    output_group.add_argument(\n        '--save_yt_dlp_responses',\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help='Whether to save the yt-dlp library JSON responses or not.',\n    )\n\n    output_group.add_argument(\n        '--output_sample',\n        type=int,\n        default=0,\n        help='Samples random compacted segments from the output and generates a CSV file contains the sampled data. Pass 0 to disable this behavior.',\n    )\n\n    output_group.add_argument(\n        '-f',\n        '--output_formats',\n        nargs='+',\n        default='all',\n        choices=[transcript_type.value for transcript_type in TranscriptType],\n        help='Format of the output file; if not specified, all available formats will be produced.',\n    )\n\n    output_group.add_argument('-o', '--output_dir', default='.', help='Directory to save the outputs.')\n\n    return parser.parse_args(argv)", "\n\ndef parse_playlist_items(arg_value: str) -> str:\n    for segment in arg_value.split(','):\n        if not segment:\n            raise ValueError('There is two or more consecutive commas.')\n\n        mobj = PLAYLIST_ITEMS_RE.fullmatch(segment)\n        if not mobj:\n            raise ValueError(f'{segment!r} is not a valid specification.')\n\n        _, _, step, _ = mobj.group('start', 'end', 'step', 'range')\n        if int_or_none(step) == 0:\n            raise ValueError(f'Step in {segment!r} cannot be zero.')\n\n    return arg_value", "\n\ndef int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):\n    if get_attr and v is not None:\n        v = getattr(v, get_attr, None)\n\n    try:\n        return int(v) * invscale // scale\n    except (ValueError, TypeError, OverflowError):\n        return default", ""]}
{"filename": "tafrigh/utils/__init__.py", "chunked_list": [""]}
{"filename": "tafrigh/utils/wit/file_utils.py", "chunked_list": ["from pathlib import Path\n\nfrom pydub import AudioSegment\n\n\ndef convert_to_wav(file: Path) -> Path:\n    audio_file = AudioSegment.from_file(str(file))\n    converted_file_path = file.with_suffix('.wav')\n    audio_file.export(str(converted_file_path), format='wav')\n    return converted_file_path", ""]}
{"filename": "tafrigh/utils/wit/__init__.py", "chunked_list": [""]}
{"filename": "tafrigh/utils/whisper/whisper_utils.py", "chunked_list": ["import faster_whisper\nimport stable_whisper\nimport whisper_jax\n\nfrom tafrigh.config import Config\nfrom tafrigh.types.whisper.type_hints import WhisperModel\n\n\ndef load_model(whisper_config: Config.Whisper) -> WhisperModel:\n    if whisper_config.use_whisper_jax:\n        return whisper_jax.FlaxWhisperPipline(f'openai/whisper-{whisper_config.model_name_or_path}')\n    elif whisper_config.use_faster_whisper:\n        return faster_whisper.WhisperModel(\n            whisper_config.model_name_or_path,\n            compute_type=whisper_config.ct2_compute_type,\n        )\n    else:\n        return stable_whisper.load_model(whisper_config.model_name_or_path)", "def load_model(whisper_config: Config.Whisper) -> WhisperModel:\n    if whisper_config.use_whisper_jax:\n        return whisper_jax.FlaxWhisperPipline(f'openai/whisper-{whisper_config.model_name_or_path}')\n    elif whisper_config.use_faster_whisper:\n        return faster_whisper.WhisperModel(\n            whisper_config.model_name_or_path,\n            compute_type=whisper_config.ct2_compute_type,\n        )\n    else:\n        return stable_whisper.load_model(whisper_config.model_name_or_path)", ""]}
{"filename": "tafrigh/utils/whisper/__init__.py", "chunked_list": [""]}
{"filename": "tafrigh/types/transcript_type.py", "chunked_list": ["from enum import Enum\n\n\nclass TranscriptType(Enum):\n    ALL = 'all'\n    TXT = 'txt'\n    SRT = 'srt'\n    VTT = 'vtt'\n    NONE = 'none'\n\n    def __str__(self):\n        return self.value", ""]}
{"filename": "tafrigh/types/__init__.py", "chunked_list": [""]}
{"filename": "tafrigh/types/whisper/type_hints.py", "chunked_list": ["from typing import TypeVar\n\nimport faster_whisper\nimport whisper\nimport whisper_jax\n\nWhisperModel = TypeVar(\n    'WhisperModel',\n    whisper.Whisper,\n    faster_whisper.WhisperModel,", "    whisper.Whisper,\n    faster_whisper.WhisperModel,\n    whisper_jax.FlaxWhisperPipline,\n)\n"]}
{"filename": "tafrigh/types/whisper/__init__.py", "chunked_list": [""]}
