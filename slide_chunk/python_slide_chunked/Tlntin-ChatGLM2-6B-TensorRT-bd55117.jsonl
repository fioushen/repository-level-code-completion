{"filename": "demo.py", "chunked_list": ["import torch\nimport torch.nn as nn\nfrom typing import Tuple, List, Optional\nimport re\nimport time\nfrom kernel.logits_processor import (\n    LogitsProcessorList,\n    InvalidScoreLogitsProcessor,\n    TemperatureLogitsWarper,\n    TopPLogitsWarper,", "    TemperatureLogitsWarper,\n    TopPLogitsWarper,\n    TopKLogitsWarper\n)\n\nfrom kernel import ckernel\n\nKernel = ckernel.Kernel\n# from ckernel import Kernel\n", "# from ckernel import Kernel\n\n# from kernel.ckernel import Kernel\n\n\nclass Model(nn.Module):\n    def __init__(self, engine_path: str, batch_size: int):\n        self.batch_size_ = batch_size\n        self.kernel = Kernel(engine_path, batch_size)\n        self.num_layers_ = 28\n        self.logits_processor = LogitsProcessorList()\n        self.logits_warper = LogitsProcessorList()\n\n    def chat(\n        self, \n        tokenizer,\n        query: str,\n        history = None,\n        max_length: int = 2048,\n        max_new_tokens: int = 1024,\n        num_beams=1,\n        do_sample=True,\n        top_p=0.7,\n        top_k=50,\n        temperature=1.0,\n        **kwargs\n    ):\n        # \u521d\u59cb\u5316 history\n        if history is None:\n            history = []\n        # \u521d\u59cb\u5316\u540e\u5904\u7406\n        self.logits_processor = LogitsProcessorList()\n        self.logits_processor.append(InvalidScoreLogitsProcessor())\n        self.logits_warper = LogitsProcessorList()\n        self.logits_warper.append(TemperatureLogitsWarper(temperature))\n        self.logits_warper.append(TopPLogitsWarper(top_p))\n        self.logits_warper.append(TopKLogitsWarper(top_k))\n        # \u7ec4\u88c5prompt\n        if not history:\n            prompt = query\n        else:\n            prompt = \"\"\n            for i, (old_query, response) in enumerate(history):\n                prompt += \"[Round {}]\\n\u95ee\uff1a{}\\n\u7b54\uff1a{}\\n\".format(i, old_query, response)\n            prompt += \"[Round {}]\\n\u95ee\uff1a{}\\n\u7b54\uff1a\".format(len(history), query)\n        # \u7b2c\u4e00\u6b21\u63a8\u7406\n        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda().to(torch.int32)\n        ori_len = len(input_ids[0])\n        attention_mask, position_ids = self.pre_processing_step_1(tokenizer, input_ids)\n        input_tensors = [input_ids, position_ids, attention_mask]\n        outputs_1 = self.kernel.forward(input_tensors)\n        ori_input_ids = input_ids\n        ori_input_ids, input_tensors = self.post_processing_step(\n            ori_input_ids, input_tensors, outputs_1\n        )\n        # \u91cd\u590d\u63a8\u7406\u76f4\u5230\u6761\u4ef6\u7ec8\u6b62\n        while len(ori_input_ids[0]) < max_length \\\n                and len(ori_input_ids[0]) - ori_len < max_new_tokens \\\n                and tokenizer.eos_token_id not in ori_input_ids[0]:\n            outputs_x = self.kernel.forward(input_tensors)\n            ori_input_ids, input_tensors = self.post_processing_step(\n                ori_input_ids, input_tensors, outputs_x)\n            # print(tokenizer.decode(ori_input_ids[0]))\n        # \u5904\u7406\u56de\u7b54\n        response = tokenizer.decode(ori_input_ids[0][ori_len:])\n        response = self.process_response(response)\n        history = history + [(query, response)]\n        return response, history\n\n    def stream_chat(\n        self, \n        tokenizer,\n        query: str,\n        history = None,\n        max_length: int = 2048,\n        max_new_tokens: int = 1024,\n        num_beams=1,\n        do_sample=True,\n        top_p=0.7,\n        top_k=50,\n        temperature=1.0,\n        **kwargs\n    ):\n        # \u521d\u59cb\u5316 history\n        if history is None:\n            history = []\n        # \u521d\u59cb\u5316\u540e\u5904\u7406\n        self.logits_processor = LogitsProcessorList()\n        self.logits_processor.append(InvalidScoreLogitsProcessor())\n        self.logits_warper = LogitsProcessorList()\n        self.logits_warper.append(TemperatureLogitsWarper(temperature))\n        self.logits_warper.append(TopPLogitsWarper(top_p))\n        self.logits_warper.append(TopKLogitsWarper(top_k))\n        # \u7ec4\u88c5prompt\n        if not history:\n            prompt = query\n        else:\n            prompt = \"\"\n            for i, (old_query, response) in enumerate(history):\n                prompt += \"[Round {}]\\n\u95ee\uff1a{}\\n\u7b54\uff1a{}\\n\".format(i, old_query, response)\n            prompt += \"[Round {}]\\n\u95ee\uff1a{}\\n\u7b54\uff1a\".format(len(history), query)\n        # \u7b2c\u4e00\u6b21\u63a8\u7406\n        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda().to(torch.int32)\n        ori_len = len(input_ids[0])\n        attention_mask, position_ids = self.pre_processing_step_1(tokenizer, input_ids)\n        input_tensors = [input_ids, position_ids, attention_mask]\n        outputs_1 = self.kernel.forward(input_tensors)\n        ori_input_ids = input_ids\n        ori_input_ids, input_tensors = self.post_processing_step(\n            ori_input_ids, input_tensors, outputs_1\n        )\n        # \u91cd\u590d\u63a8\u7406\u76f4\u5230\u6761\u4ef6\u7ec8\u6b62\n        while len(ori_input_ids[0]) < max_length \\\n                and len(ori_input_ids[0]) - ori_len < max_new_tokens \\\n                and tokenizer.eos_token_id not in ori_input_ids[0]:\n            outputs_x = self.kernel.forward(input_tensors)\n            ori_input_ids, input_tensors = self.post_processing_step(\n                ori_input_ids, input_tensors, outputs_x)\n            # \u5904\u7406\u56de\u7b54\n            response = tokenizer.decode(ori_input_ids[0][ori_len:])\n            response = self.process_response(response)\n            history = history + [(query, response)]\n            yield response, history\n\n    def pre_processing_step_1(self, tokenizer, input_ids: torch.Tensor):\n        BOS = tokenizer.bos_token_id\n        MASK = tokenizer.mask_token_id\n        gMASK = tokenizer.gmask_token_id\n        batch_size, seq_length = input_ids.shape\n        # \u8f93\u5165\u5f20\u91cf\u6269\u5c55\u5e38\u91cf\n        input_range = torch.arange(seq_length, dtype=torch.int32).repeat((batch_size, 1)).to(input_ids.device)\n        input_upper = torch.tril(torch.ones((batch_size, seq_length, seq_length), dtype=torch.int32)).to(\n            input_ids.device)\n        # \u83b7\u53d6 attention_mask\n        context_lengths = torch.argmax((input_ids == BOS).to(torch.int32), dim=1)\n        context_mask = (input_range + 1) <= context_lengths.unsqueeze(1)\n        padding_mask = context_mask.unsqueeze(1)\n        attention_mask = torch.logical_not(torch.logical_or(input_upper, padding_mask)).unsqueeze(1)\n        # \u5224\u65adMASK\u4f4d\u7f6e\n        is_gmasks = (input_ids == gMASK).to(torch.int32)\n        is_masks = (input_ids == MASK).to(torch.int32)\n        use_gmasks = torch.sum(is_gmasks, dim=1) > 0\n        # \u83b7\u53d6 position_ids\n        mask_positions = torch.where(use_gmasks, torch.argmax(is_gmasks, dim=1), torch.argmax(is_masks, dim=1)).to(\n            torch.int32).unsqueeze(1)\n        position_ids_pre = torch.where(context_mask, input_range, mask_positions)\n        block_position_ids = torch.clamp(input_range - context_lengths.unsqueeze(1) + 1, min=0)\n        position_ids = torch.stack((position_ids_pre, block_position_ids), dim=1).to(torch.int32)\n        return attention_mask, position_ids\n\n    def post_processing_step(self,\n            ori_input_ids: torch.Tensor,\n            input_tensors: List[torch.Tensor],\n            output_tensors: List[torch.Tensor]\n        ):\n        logits = output_tensors[-1]\n        next_token_logits = logits[:, -1, :]\n        # \u4e00\u4e9b\u540e\u5904\u7406\u903b\u8f91\n        next_token_scores = self.logits_processor(input_tensors[0], next_token_logits)\n        next_token_scores = self.logits_warper(input_tensors[0], next_token_scores)\n        # \u91c7\u6837\u4e0b\u4e00\u4e2atoken\n        probs = torch.nn.functional.softmax(next_token_scores, dim=-1)\n        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n        ori_input_ids = torch.cat((ori_input_ids, next_tokens[:, None]), dim=-1)\n        # \u8f93\u51fa\u4e0b\u4e00\u8f6e\u7684 input_ids, position_ids, attention_mask\n        input_tensors[2] = input_tensors[2][..., -1:, -1:]\n        input_tensors[1] = torch.cat(\n            (\n                input_tensors[1][:, :-1, -1:],\n                input_tensors[1][:, -1:, -1:] + \\\n                    torch.tensor(1, dtype=input_tensors[1].dtype)\n            ), \n            dim=1\n        )\n        input_tensors[0] = ori_input_ids[:, -1:].to(torch.int32)\n        if len(input_tensors) == 3:\n            input_tensors.extend(output_tensors[:-1])\n        else:\n            # for i in range(len(output_tensors) - 1):\n            #     input_tensors[3 + i] = output_tensors[i]\n            input_tensors[3:] = output_tensors[:-1]\n        return ori_input_ids, input_tensors\n\n    def process_response(self, response):\n        response = response.strip()\n        # response = response.replace(\"[[\u8bad\u7ec3\u65f6\u95f4]]\", \"2023\u5e74\")\n        punkts = [\n            [\",\", \"\uff0c\"],\n            [\"!\", \"\uff01\"],\n            [\":\", \"\uff1a\"],\n            [\";\", \"\uff1b\"],\n            [\"\\?\", \"\uff1f\"],\n        ]\n        for item in punkts:\n            response = re.sub(r\"([\\u4e00-\\u9fff])%s\" % item[0], r\"\\1%s\" % item[1], response)\n            response = re.sub(r\"%s([\\u4e00-\\u9fff])\" % item[0], r\"%s\\1\" % item[1], response)\n        return response", "\n\nif __name__ == \"__main__\":\n    from transformers import AutoTokenizer\n    import time\n    from tqdm import trange\n    tokenizer = AutoTokenizer.from_pretrained(\"chatglm_6b\", trust_remote_code=True)\n    model = Model(\"models/chatglm6b-bs1-18.5G.plan\", 1)\n    all_res = []\n    st = time.time()\n    for i in trange(10):\n        responses, history = model.chat(\n            tokenizer=tokenizer,\n            query=\"\u4f60\u597d, \u8bf7\u7528python\u5199\u4e00\u4e2a\u94fe\u8868\u3002\"\n        )\n        all_res.append(responses)\n    et = time.time()\n    print(all_res)\n    tokens = tokenizer.encode(\"\".join(all_res), return_tensors=\"pt\")[0]\n    token_num = len(tokens)\n    speed = round(token_num / (et - st), 1)\n    print(\"speed: {} tokens/s\".format(speed))", "    \n"]}
{"filename": "cli_demo.py", "chunked_list": ["import os\nimport platform\nimport signal\nfrom transformers import AutoTokenizer, AutoConfig\n# from stream_demo import ChatGLMForConditionalGeneration\nfrom demo import Model\nimport readline\n\ntokenizer = AutoTokenizer.from_pretrained(\"chatglm_6b\", trust_remote_code=True)\nmodel = Model(", "tokenizer = AutoTokenizer.from_pretrained(\"chatglm_6b\", trust_remote_code=True)\nmodel = Model(\n    engine_path=\"models/chatglm6b-bs1-11.5G.plan\",\n    batch_size=1, \n)\n\nos_name = platform.system()\nclear_command = 'cls' if os_name == 'Windows' else 'clear'\nstop_stream = False\n", "stop_stream = False\n\n\ndef build_prompt(history):\n    prompt = \"\u6b22\u8fce\u4f7f\u7528 ChatGLM-6B \u6a21\u578b\uff0c\u8f93\u5165\u5185\u5bb9\u5373\u53ef\u8fdb\u884c\u5bf9\u8bdd\uff0cclear \u6e05\u7a7a\u5bf9\u8bdd\u5386\u53f2\uff0cstop \u7ec8\u6b62\u7a0b\u5e8f\"\n    for query, response in history:\n        prompt += f\"\\n\\n\u7528\u6237\uff1a{query}\"\n        prompt += f\"\\n\\nChatGLM-6B\uff1a{response}\"\n    return prompt\n", "\n\ndef signal_handler(signal, frame):\n    global stop_stream\n    stop_stream = True\n\n\ndef main():\n    history = []\n    global stop_stream\n    print(\"\u6b22\u8fce\u4f7f\u7528 ChatGLM-6B \u6a21\u578b\uff0c\u8f93\u5165\u5185\u5bb9\u5373\u53ef\u8fdb\u884c\u5bf9\u8bdd\uff0cclear \u6e05\u7a7a\u5bf9\u8bdd\u5386\u53f2\uff0cstop \u7ec8\u6b62\u7a0b\u5e8f\")\n    while True:\n        query = input(\"\\n\u7528\u6237\uff1a\")\n        if query.strip() == \"stop\":\n            break\n        if query.strip() == \"clear\":\n            history = []\n            os.system(clear_command)\n            print(\"\u6b22\u8fce\u4f7f\u7528 ChatGLM-6B \u6a21\u578b\uff0c\u8f93\u5165\u5185\u5bb9\u5373\u53ef\u8fdb\u884c\u5bf9\u8bdd\uff0cclear \u6e05\u7a7a\u5bf9\u8bdd\u5386\u53f2\uff0cstop \u7ec8\u6b62\u7a0b\u5e8f\")\n            continue\n        count = 0\n        for response, history in model.stream_chat(tokenizer, query, history=history):\n            if stop_stream:\n                stop_stream = False\n                break\n            else:\n                count += 1\n                if count % 8 == 0:\n                    os.system(clear_command)\n                    print(build_prompt(history), flush=True)\n                    signal.signal(signal.SIGINT, signal_handler)\n        os.system(clear_command)\n        print(build_prompt(history), flush=True)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "onnx_export/export_test_v2.py", "chunked_list": ["import os\n# from transformers import AutoTokenizer, AutoModel, AutoConfig\nimport torch\nimport sys\nimport argparse\nfrom transformers.generation.utils import LogitsProcessorList\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\nsys.path.append(project_dir)\nfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig", "sys.path.append(project_dir)\nfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\nfrom chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\nfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\nfrom onnx_export.utils import build_inputs\nfrom transformers.models.bloom import BloomOnnxConfig\nparser = argparse.ArgumentParser(description='export pytorch model to onnx')\nparser.add_argument(\n    '--data_type',\n    default=\"fp32\",", "    '--data_type',\n    default=\"fp32\",\n    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n)\n\nargs = parser.parse_args()\nif args.data_type == \"fp16\":\n    device = 'cuda'\nelse:\n    device = 'cpu'", "\n\noutput_dir = os.path.join(project_dir, \"output\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\nonnx_output_dir = os.path.join(output_dir, \"onnx_output\")\nif not os.path.exists(onnx_output_dir):\n    os.mkdir(onnx_output_dir)\n\nquery = \"\u60f3\u8981\u51fa\u56fd\u7559\u5b66\uff0c\u5e94\u8be5\u600e\u4e48\u529e\uff1f\"", "\nquery = \"\u60f3\u8981\u51fa\u56fd\u7559\u5b66\uff0c\u5e94\u8be5\u600e\u4e48\u529e\uff1f\"\nhistory = [\n    (\n        \"\u4f60\u597d\",\n        \"\u4f60\u597d\ud83d\udc4b!\u6211\u662f\u4eba\u5de5\u667a\u80fd\u52a9\u624b ChatGLM2-6B,\u5f88\u9ad8\u5174\u89c1\u5230\u4f60,\u6b22\u8fce\u95ee\u6211\u4efb\u4f55\u95ee\u9898\u3002\",\n    )\n]\n\nmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")", "\nmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\ntokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\nconfig = ChatGLMConfig.from_pretrained(model_dir)\n# config.num_layers = 1\nmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\nif device == \"cuda\":\n    model = model.half().cuda()\nelse:\n    model = model.float().cpu()", "device = torch.device(device)\nmodel.eval()\n# input_tensors\ninput_tensors = build_inputs(device, tokenizer, query, history)\n\n# --debug for chat --\n# response, history = model.chat(tokenizer, query, history)\n# print(\"res\", response)\nprint(\"=\" * 50)\nprint(\" ---forward first --- \")", "print(\"=\" * 50)\nprint(\" ---forward first --- \")\noutputs = model.forward(\n    **input_tensors\n)\n\nprint(\" ---forward first with no attention_mask --- \")\noutputs2 = model.forward(\n    input_ids=input_tensors[\"input_ids\"],\n    position_ids=input_tensors[\"position_ids\"],", "    input_ids=input_tensors[\"input_ids\"],\n    position_ids=input_tensors[\"position_ids\"],\n    attention_mask=torch.tensor([], device=device)\n)\n\n\ndef compare_diff(outputs_1, outputs_2):\n    print(\"--- compare diff ---\")\n    max_diff = 0\n    logits_diff = (outputs_2[\"logits\"] - outputs_1[\"logits\"]).max().item()\n    if logits_diff > max_diff:\n        max_diff = logits_diff\n    print(\"logits diff is \", logits_diff)\n    past_key_values0 = outputs_1[\"past_key_values\"]\n    past_key_values1 = outputs_2[\"past_key_values\"]\n    for i in range(model.config.num_layers):\n        present_key_name = f\"present_key_values.{i}.key\"\n        present_value_name = f\"present_key_values.{i}.value\"\n        diff1 = (past_key_values0[i][0] - past_key_values1[i][0]).max().item()\n        diff2 = (past_key_values0[i][1] - past_key_values1[i][1]).max().item()\n        print(f\"{present_key_name} diff: \", diff1)\n        print(f\"{present_value_name} diff: \", diff2)\n        if diff1 > max_diff:\n            max_diff = diff1\n        if diff2 > max_diff:\n            max_diff = diff2\n\n    print(\"max diff is: \", max_diff)", "\n\ncompare_diff(outputs, outputs2)\nprint(\"=\" * 50)\n\n\nprint(\"=\" * 50)\nprint(\" ---forward second --- \")\nattention_mask = input_tensors[\"attention_mask\"]\nposition_ids = input_tensors[\"position_ids\"]", "attention_mask = input_tensors[\"attention_mask\"]\nposition_ids = input_tensors[\"position_ids\"]\npast_key_values = outputs[\"past_key_values\"]\n# copy from forward in second time\ninput_ids = torch.tensor([[30910]]).to(device)\nattention_mask = torch.cat(\n    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n)\nnew_position_id = position_ids[..., -1:].clone()\nnew_position_id += 1", "new_position_id = position_ids[..., -1:].clone()\nnew_position_id += 1\nposition_ids = torch.cat(\n    [position_ids, new_position_id], dim=-1\n)\n# copy from prepare_inputs_for_generation in modeling_chatglm.py\nposition_ids = position_ids[..., -1:]\npast_key_values1 = outputs[\"past_key_values\"]\noutputs_3 = model.forward(\n    input_ids=input_ids,", "outputs_3 = model.forward(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    position_ids=position_ids,\n    past_key_values=past_key_values1\n)\n\nprint(\" ---forward second with no attention_mask --- \")\noutputs_4 = model.forward(\n    input_ids=input_ids,", "outputs_4 = model.forward(\n    input_ids=input_ids,\n    position_ids=position_ids,\n    attention_mask=torch.tensor([], device=device),\n    past_key_values=past_key_values1\n)\ncompare_diff(outputs_3, outputs_4)\nprint(\"=\" * 50)\n\n", "\n\n"]}
{"filename": "onnx_export/modeling_chatglm.py", "chunked_list": ["\"\"\" PyTorch ChatGLM model. \"\"\"\n\nimport math\nimport copy\nimport warnings\nimport re\nimport sys\n\nimport torch\nimport torch.utils.checkpoint", "import torch\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, LayerNorm\nfrom torch.nn.utils import skip_init\nfrom typing import Optional, Tuple, Union, List, Callable, Dict, Any\n\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,", "from transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import logging\nfrom transformers.generation.logits_process import LogitsProcessor\nfrom transformers.generation.utils import LogitsProcessorList, StoppingCriteriaList, GenerationConfig, ModelOutput\n\nfrom .configuration_chatglm import ChatGLMConfig", "\nfrom .configuration_chatglm import ChatGLMConfig\n\n# flags required to enable jit fusion kernels\n\nif sys.platform != 'darwin':\n    torch._C._jit_set_profiling_mode(False)\n    torch._C._jit_set_profiling_executor(False)\n    torch._C._jit_override_can_fuse_on_cpu(True)\n    torch._C._jit_override_can_fuse_on_gpu(True)", "\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"THUDM/ChatGLM2-6B\"\n_CONFIG_FOR_DOC = \"ChatGLM6BConfig\"\n\nCHATGLM_6B_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"THUDM/chatglm2-6b\",\n    # See all ChatGLM models at https://huggingface.co/models?filter=chatglm\n]", "    # See all ChatGLM models at https://huggingface.co/models?filter=chatglm\n]\n\n\ndef default_init(cls, *args, **kwargs):\n    return cls(*args, **kwargs)\n\n\nclass InvalidScoreLogitsProcessor(LogitsProcessor):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        if torch.isnan(scores).any() or torch.isinf(scores).any():\n            scores.zero_()\n            scores[..., 5] = 5e4\n        return scores", "class InvalidScoreLogitsProcessor(LogitsProcessor):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        if torch.isnan(scores).any() or torch.isinf(scores).any():\n            scores.zero_()\n            scores[..., 5] = 5e4\n        return scores\n\n\nclass PrefixEncoder(torch.nn.Module):\n    \"\"\"\n    The torch.nn model to encode the prefix\n    Input shape: (batch-size, prefix-length)\n    Output shape: (batch-size, prefix-length, 2*layers*hidden)\n    \"\"\"\n\n    def __init__(self, config: ChatGLMConfig):\n        super().__init__()\n        self.prefix_projection = config.prefix_projection\n        if self.prefix_projection:\n            # Use a two-layer MLP to encode the prefix\n            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.hidden_size)\n            self.trans = torch.nn.Sequential(\n                torch.nn.Linear(config.hidden_size, config.hidden_size),\n                torch.nn.Tanh(),\n                torch.nn.Linear(config.hidden_size, config.num_layers * config.hidden_size * 2)\n            )\n        else:\n            self.embedding = torch.nn.Embedding(config.pre_seq_len,\n                                                config.num_layers * config.kv_channels * config.multi_query_group_num * 2)\n\n    def forward(self, prefix: torch.Tensor):\n        if self.prefix_projection:\n            prefix_tokens = self.embedding(prefix)\n            past_key_values = self.trans(prefix_tokens)\n        else:\n            past_key_values = self.embedding(prefix)\n        return past_key_values", "class PrefixEncoder(torch.nn.Module):\n    \"\"\"\n    The torch.nn model to encode the prefix\n    Input shape: (batch-size, prefix-length)\n    Output shape: (batch-size, prefix-length, 2*layers*hidden)\n    \"\"\"\n\n    def __init__(self, config: ChatGLMConfig):\n        super().__init__()\n        self.prefix_projection = config.prefix_projection\n        if self.prefix_projection:\n            # Use a two-layer MLP to encode the prefix\n            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.hidden_size)\n            self.trans = torch.nn.Sequential(\n                torch.nn.Linear(config.hidden_size, config.hidden_size),\n                torch.nn.Tanh(),\n                torch.nn.Linear(config.hidden_size, config.num_layers * config.hidden_size * 2)\n            )\n        else:\n            self.embedding = torch.nn.Embedding(config.pre_seq_len,\n                                                config.num_layers * config.kv_channels * config.multi_query_group_num * 2)\n\n    def forward(self, prefix: torch.Tensor):\n        if self.prefix_projection:\n            prefix_tokens = self.embedding(prefix)\n            past_key_values = self.trans(prefix_tokens)\n        else:\n            past_key_values = self.embedding(prefix)\n        return past_key_values", "\n\ndef split_tensor_along_last_dim(\n        tensor: torch.Tensor,\n        num_partitions: int,\n        contiguous_split_chunks: bool = False,\n) -> List[torch.Tensor]:\n    \"\"\"Split a tensor along its last dimension.\n\n    Arguments:\n        tensor: input tensor.\n        num_partitions: number of partitions to split the tensor\n        contiguous_split_chunks: If True, make each chunk contiguous\n                                 in memory.\n\n    Returns:\n        A list of Tensors\n    \"\"\"\n    # Get the size and dimension.\n    last_dim = tensor.dim() - 1\n    last_dim_size = tensor.size()[last_dim] // num_partitions\n    # Split.\n    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n    # Note: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tensor_list", "\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim, original_impl=False, device=None, dtype=None):\n        super().__init__()\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device).to(dtype=dtype) / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n        self.dim = dim\n        self.original_impl = original_impl\n\n    def forward_impl(\n            self, seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000\n    ):\n        \"\"\"Enhanced Transformer with Rotary Position Embedding.\n\n        Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n        transformers/rope/__init__.py. MIT License:\n        https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n        \"\"\"\n        # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n        theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))\n\n        # Create position indexes `[0, 1, ..., seq_len - 1]`\n        seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n\n        # Calculate the product of position index and $\\theta_i$\n        idx_theta = torch.outer(seq_idx, theta).float()\n\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n\n        # this is to mimic the behaviour of complex32, else we will get different results\n        if dtype in (torch.float16, torch.bfloat16, torch.int8):\n            cache = cache.bfloat16() if dtype == torch.bfloat16 else cache.half()\n        return cache\n\n    def forward(self, max_seq_len, offset=0):\n        return self.forward_impl(\n            max_seq_len, self.dim, dtype=self.inv_freq.dtype, device=self.inv_freq.device\n        )", "\n\n@torch.jit.script\ndef apply_rotary_pos_emb(x: torch.Tensor, rope_cache: torch.Tensor) -> torch.Tensor:\n    # x: [sq, b, np, hn]\n    sq, b, np, hn = x.size(0), x.size(1), x.size(2), x.size(3)\n    rot_dim = rope_cache.shape[-2] * 2\n    x, x_pass = x[..., :rot_dim], x[..., rot_dim:]\n    # truncate to support variable sizes\n    rope_cache = rope_cache[:sq]\n    xshaped = x.reshape(sq, -1, np, rot_dim // 2, 2)\n    rope_cache = rope_cache.view(sq, -1, 1, xshaped.size(3), 2)\n    x_out2 = torch.stack(\n        [\n            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n        ],\n        -1,\n    )\n    x_out2 = x_out2.flatten(3)\n    return torch.cat((x_out2, x_pass), dim=-1)", "\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(self, normalized_shape, eps=1e-5, device=None, dtype=None, **kwargs):\n        super().__init__()\n        self.weight = torch.nn.Parameter(torch.empty(normalized_shape, device=device, dtype=dtype))\n        self.eps = eps\n\n    def forward(self, hidden_states: torch.Tensor):\n        input_dtype = hidden_states.dtype\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)\n\n        return (self.weight * hidden_states).to(input_dtype)", "\n\nclass CoreAttention(torch.nn.Module):\n    def __init__(self, config: ChatGLMConfig, layer_number):\n        super(CoreAttention, self).__init__()\n\n        self.apply_query_key_layer_scaling = config.apply_query_key_layer_scaling\n        self.attention_softmax_in_fp32 = config.attention_softmax_in_fp32\n        if self.apply_query_key_layer_scaling:\n            self.attention_softmax_in_fp32 = True\n        self.layer_number = max(1, layer_number)\n\n        projection_size = config.kv_channels * config.num_attention_heads\n\n        # Per attention head and per partition values.\n        self.hidden_size_per_partition = projection_size\n        self.hidden_size_per_attention_head = projection_size // config.num_attention_heads\n        self.num_attention_heads_per_partition = config.num_attention_heads\n\n        coeff = None\n        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)\n        if self.apply_query_key_layer_scaling:\n            coeff = self.layer_number\n            self.norm_factor *= coeff\n        self.coeff = coeff\n\n        self.attention_dropout = torch.nn.Dropout(config.attention_dropout)\n\n    def forward(self, query_layer, key_layer, value_layer, attention_mask):\n        pytorch_major_version = int(torch.__version__.split('.')[0])\n        if pytorch_major_version >= 2:\n            query_layer, key_layer, value_layer = [k.permute(1, 2, 0, 3) for k in [query_layer, key_layer, value_layer]]\n            if attention_mask is None and query_layer.shape[2] == key_layer.shape[2]:\n                context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,\n                                                                                 is_causal=True)\n            else:\n                if attention_mask is not None:\n                    attention_mask = ~attention_mask\n                context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,\n                                                                                 attention_mask)\n            context_layer = context_layer.permute(2, 0, 1, 3)\n            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)\n            context_layer = context_layer.reshape(*new_context_layer_shape)\n        else:\n            # Raw attention scores\n\n            # [b, np, sq, sk]\n            output_size = (query_layer.size(1), query_layer.size(2), query_layer.size(0), key_layer.size(0))\n\n            # [sq, b, np, hn] -> [sq, b * np, hn]\n            query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)\n            # [sk, b, np, hn] -> [sk, b * np, hn]\n            key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)\n\n            # preallocting input tensor: [b * np, sq, sk]\n            matmul_input_buffer = torch.empty(\n                output_size[0] * output_size[1], output_size[2], output_size[3], dtype=query_layer.dtype,\n                device=query_layer.device\n            )\n\n            # Raw attention scores. [b * np, sq, sk]\n            matmul_result = torch.baddbmm(\n                matmul_input_buffer,\n                query_layer.transpose(0, 1),  # [b * np, sq, hn]\n                key_layer.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]\n                beta=0.0,\n                alpha=(1.0 / self.norm_factor),\n            )\n\n            # change view to [b, np, sq, sk]\n            attention_scores = matmul_result.view(*output_size)\n\n            # ===========================\n            # Attention probs and dropout\n            # ===========================\n\n            # attention scores and attention mask [b, np, sq, sk]\n            if self.attention_softmax_in_fp32:\n                attention_scores = attention_scores.float()\n            if self.coeff is not None:\n                attention_scores = attention_scores * self.coeff\n            if attention_mask is None and attention_scores.shape[2] == attention_scores.shape[3]:\n                attention_mask = torch.ones(output_size[0], 1, output_size[2], output_size[3],\n                                            device=attention_scores.device, dtype=torch.bool)\n                attention_mask.tril_()\n                attention_mask = ~attention_mask\n            if attention_mask is not None:\n                attention_scores = attention_scores.masked_fill(attention_mask, float(\"-inf\"))\n            attention_probs = F.softmax(attention_scores, dim=-1)\n            attention_probs = attention_probs.type_as(value_layer)\n\n            # This is actually dropping out entire tokens to attend to, which might\n            # seem a bit unusual, but is taken from the original Transformer paper.\n            attention_probs = self.attention_dropout(attention_probs)\n            # =========================\n            # Context layer. [sq, b, hp]\n            # =========================\n\n            # value_layer -> context layer.\n            # [sk, b, np, hn] --> [b, np, sq, hn]\n\n            # context layer shape: [b, np, sq, hn]\n            output_size = (value_layer.size(1), value_layer.size(2), query_layer.size(0), value_layer.size(3))\n            # change view [sk, b * np, hn]\n            value_layer = value_layer.view(value_layer.size(0), output_size[0] * output_size[1], -1)\n            # change view [b * np, sq, sk]\n            attention_probs = attention_probs.view(output_size[0] * output_size[1], output_size[2], -1)\n            # matmul: [b * np, sq, hn]\n            context_layer = torch.bmm(attention_probs, value_layer.transpose(0, 1))\n            # change view [b, np, sq, hn]\n            context_layer = context_layer.view(*output_size)\n            # [b, np, sq, hn] --> [sq, b, np, hn]\n            context_layer = context_layer.permute(2, 0, 1, 3).contiguous()\n            # [sq, b, np, hn] --> [sq, b, hp]\n            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)\n            context_layer = context_layer.view(*new_context_layer_shape)\n\n        return context_layer", "\n\nclass SelfAttention(torch.nn.Module):\n    \"\"\"Parallel self-attention layer abstract class.\n\n    Self-attention layer takes input with size [s, b, h]\n    and returns output of the same size.\n    \"\"\"\n\n    def __init__(self, config: ChatGLMConfig, layer_number, device=None):\n        super(SelfAttention, self).__init__()\n        self.layer_number = max(1, layer_number)\n\n        self.projection_size = config.kv_channels * config.num_attention_heads\n\n        # Per attention head and per partition values.\n        self.hidden_size_per_attention_head = self.projection_size // config.num_attention_heads\n        self.num_attention_heads_per_partition = config.num_attention_heads\n\n        self.multi_query_attention = config.multi_query_attention\n        self.qkv_hidden_size = 3 * self.projection_size\n        if self.multi_query_attention:\n            self.num_multi_query_groups_per_partition = config.multi_query_group_num\n            self.qkv_hidden_size = (\n                    self.projection_size + 2 * self.hidden_size_per_attention_head * config.multi_query_group_num\n            )\n        self.query_key_value = nn.Linear(config.hidden_size, self.qkv_hidden_size,\n                                         bias=config.add_bias_linear or config.add_qkv_bias,\n                                         device=device, **_config_to_kwargs(config)\n                                         )\n\n        self.core_attention = CoreAttention(config, self.layer_number)\n\n        # Output.\n        self.dense = nn.Linear(self.projection_size, config.hidden_size, bias=config.add_bias_linear,\n                               device=device, **_config_to_kwargs(config)\n                               )\n\n    def _allocate_memory(self, inference_max_sequence_len, batch_size, device=None, dtype=None):\n        if self.multi_query_attention:\n            num_attention_heads = self.num_multi_query_groups_per_partition\n        else:\n            num_attention_heads = self.num_attention_heads_per_partition\n        return torch.empty(\n            inference_max_sequence_len,\n            batch_size,\n            num_attention_heads,\n            self.hidden_size_per_attention_head,\n            dtype=dtype,\n            device=device,\n        )\n\n    def forward(\n            self, hidden_states, attention_mask, rotary_pos_emb, kv_cache=None, use_cache=True\n    ):\n        # hidden_states: [sq, b, h]\n\n        # =================================================\n        # Pre-allocate memory for key-values for inference.\n        # =================================================\n        # =====================\n        # Query, Key, and Value\n        # =====================\n\n        # Attention heads [sq, b, h] --> [sq, b, (np * 3 * hn)]\n        mixed_x_layer = self.query_key_value(hidden_states)\n\n        if self.multi_query_attention:\n            (query_layer, key_layer, value_layer) = mixed_x_layer.split(\n                [\n                    self.num_attention_heads_per_partition * self.hidden_size_per_attention_head,\n                    self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,\n                    self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,\n                ],\n                dim=-1,\n            )\n            query_layer = query_layer.view(\n                query_layer.size()[:-1] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)\n            )\n            key_layer = key_layer.view(\n                key_layer.size()[:-1] + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)\n            )\n            value_layer = value_layer.view(\n                value_layer.size()[:-1]\n                + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)\n            )\n        else:\n            new_tensor_shape = mixed_x_layer.size()[:-1] + \\\n                               (self.num_attention_heads_per_partition,\n                                3 * self.hidden_size_per_attention_head)\n            mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)\n\n            # [sq, b, np, 3 * hn] --> 3 [sq, b, np, hn]\n            (query_layer, key_layer, value_layer) = split_tensor_along_last_dim(mixed_x_layer, 3)\n\n        # apply relative positional encoding (rotary embedding)\n        if rotary_pos_emb is not None:\n            query_layer = apply_rotary_pos_emb(query_layer, rotary_pos_emb)\n            key_layer = apply_rotary_pos_emb(key_layer, rotary_pos_emb)\n\n        # adjust key and value for inference\n        if use_cache:\n            if kv_cache is not None:\n                cache_k, cache_v = kv_cache\n                key_layer = torch.cat((cache_k, key_layer), dim=0)\n                value_layer = torch.cat((cache_v, value_layer), dim=0)\n            kv_cache = (key_layer, value_layer)\n        else:\n            kv_cache = None\n\n        if self.multi_query_attention:\n            key_layer = key_layer.unsqueeze(-2)\n            key_layer = key_layer.expand(\n                -1, -1, -1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, -1\n            )\n            key_layer = key_layer.contiguous().view(\n                key_layer.size()[:2] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)\n            )\n            value_layer = value_layer.unsqueeze(-2)\n            value_layer = value_layer.expand(\n                -1, -1, -1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, -1\n            )\n            value_layer = value_layer.contiguous().view(\n                value_layer.size()[:2] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)\n            )\n\n        # ==================================\n        # core attention computation\n        # ==================================\n\n        context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)\n\n        # =================\n        # Output. [sq, b, h]\n        # =================\n\n        output = self.dense(context_layer)\n\n        return output, kv_cache", "\n\ndef _config_to_kwargs(args):\n    common_kwargs = {\n        \"dtype\": args.torch_dtype,\n    }\n    return common_kwargs\n\n\nclass MLP(torch.nn.Module):\n    \"\"\"MLP.\n\n    MLP will take the input with h hidden state, project it to 4*h\n    hidden dimension, perform nonlinear transformation, and project the\n    state back into h hidden dimension.\n    \"\"\"\n\n    def __init__(self, config: ChatGLMConfig, device=None):\n        super(MLP, self).__init__()\n\n        self.add_bias = config.add_bias_linear\n\n        # Project to 4h. If using swiglu double the output width, see https://arxiv.org/pdf/2002.05202.pdf\n        self.dense_h_to_4h = nn.Linear(\n            config.hidden_size,\n            config.ffn_hidden_size * 2,\n            bias=self.add_bias,\n            device=device,\n            **_config_to_kwargs(config)\n        )\n\n        def swiglu(x):\n            x = torch.chunk(x, 2, dim=-1)\n            return F.silu(x[0]) * x[1]\n\n        self.activation_func = swiglu\n\n        # Project back to h.\n        self.dense_4h_to_h = nn.Linear(\n            config.ffn_hidden_size,\n            config.hidden_size,\n            bias=self.add_bias,\n            device=device,\n            **_config_to_kwargs(config)\n        )\n\n    def forward(self, hidden_states):\n        # [s, b, 4hp]\n        intermediate_parallel = self.dense_h_to_4h(hidden_states)\n        intermediate_parallel = self.activation_func(intermediate_parallel)\n        # [s, b, h]\n        output = self.dense_4h_to_h(intermediate_parallel)\n        return output", "\nclass MLP(torch.nn.Module):\n    \"\"\"MLP.\n\n    MLP will take the input with h hidden state, project it to 4*h\n    hidden dimension, perform nonlinear transformation, and project the\n    state back into h hidden dimension.\n    \"\"\"\n\n    def __init__(self, config: ChatGLMConfig, device=None):\n        super(MLP, self).__init__()\n\n        self.add_bias = config.add_bias_linear\n\n        # Project to 4h. If using swiglu double the output width, see https://arxiv.org/pdf/2002.05202.pdf\n        self.dense_h_to_4h = nn.Linear(\n            config.hidden_size,\n            config.ffn_hidden_size * 2,\n            bias=self.add_bias,\n            device=device,\n            **_config_to_kwargs(config)\n        )\n\n        def swiglu(x):\n            x = torch.chunk(x, 2, dim=-1)\n            return F.silu(x[0]) * x[1]\n\n        self.activation_func = swiglu\n\n        # Project back to h.\n        self.dense_4h_to_h = nn.Linear(\n            config.ffn_hidden_size,\n            config.hidden_size,\n            bias=self.add_bias,\n            device=device,\n            **_config_to_kwargs(config)\n        )\n\n    def forward(self, hidden_states):\n        # [s, b, 4hp]\n        intermediate_parallel = self.dense_h_to_4h(hidden_states)\n        intermediate_parallel = self.activation_func(intermediate_parallel)\n        # [s, b, h]\n        output = self.dense_4h_to_h(intermediate_parallel)\n        return output", "\n\nclass GLMBlock(torch.nn.Module):\n    \"\"\"A single transformer layer.\n\n    Transformer layer takes input with size [s, b, h] and returns an\n    output of the same size.\n    \"\"\"\n\n    def __init__(self, config: ChatGLMConfig, layer_number, device=None):\n        super(GLMBlock, self).__init__()\n        self.layer_number = layer_number\n\n        self.apply_residual_connection_post_layernorm = config.apply_residual_connection_post_layernorm\n\n        self.fp32_residual_connection = config.fp32_residual_connection\n\n        LayerNormFunc = RMSNorm if config.rmsnorm else LayerNorm\n        # Layernorm on the input data.\n        self.input_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,\n                                             dtype=config.torch_dtype)\n\n        # Self attention.\n        self.self_attention = SelfAttention(config, layer_number, device=device)\n        self.hidden_dropout = config.hidden_dropout\n\n        # Layernorm on the attention output\n        self.post_attention_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,\n                                                      dtype=config.torch_dtype)\n\n        # MLP\n        self.mlp = MLP(config, device=device)\n\n    def forward(\n            self, hidden_states, attention_mask, rotary_pos_emb, kv_cache=None, use_cache=True,\n    ):\n        # hidden_states: [s, b, h]\n\n        # Layer norm at the beginning of the transformer layer.\n        layernorm_output = self.input_layernorm(hidden_states)\n        # Self attention.\n        attention_output, kv_cache = self.self_attention(\n            layernorm_output,\n            attention_mask,\n            rotary_pos_emb,\n            kv_cache=kv_cache,\n            use_cache=use_cache\n        )\n\n        # Residual connection.\n        if self.apply_residual_connection_post_layernorm:\n            residual = layernorm_output\n        else:\n            residual = hidden_states\n\n        layernorm_input = torch.nn.functional.dropout(attention_output, p=self.hidden_dropout, training=self.training)\n        layernorm_input = residual + layernorm_input\n\n        # Layer norm post the self attention.\n        layernorm_output = self.post_attention_layernorm(layernorm_input)\n\n        # MLP.\n        mlp_output = self.mlp(layernorm_output)\n\n        # Second residual connection.\n        if self.apply_residual_connection_post_layernorm:\n            residual = layernorm_output\n        else:\n            residual = layernorm_input\n\n        output = torch.nn.functional.dropout(mlp_output, p=self.hidden_dropout, training=self.training)\n        output = residual + output\n\n        return output, kv_cache", "\n\nclass GLMTransformer(torch.nn.Module):\n    \"\"\"Transformer class.\"\"\"\n\n    def __init__(self, config: ChatGLMConfig, device=None):\n        super(GLMTransformer, self).__init__()\n\n        self.fp32_residual_connection = config.fp32_residual_connection\n        self.post_layer_norm = config.post_layer_norm\n\n        # Number of layers.\n        self.num_layers = config.num_layers\n\n        # Transformer layers.\n        def build_layer(layer_number):\n            return GLMBlock(config, layer_number, device=device)\n\n        self.layers = torch.nn.ModuleList([build_layer(i + 1) for i in range(self.num_layers)])\n\n        if self.post_layer_norm:\n            LayerNormFunc = RMSNorm if config.rmsnorm else LayerNorm\n            # Final layer norm before output.\n            self.final_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,\n                                                 dtype=config.torch_dtype)\n\n        self.gradient_checkpointing = False\n\n    def _get_layer(self, layer_number):\n        return self.layers[layer_number]\n\n    def forward(\n            self, hidden_states, attention_mask, rotary_pos_emb, kv_caches=None,\n            use_cache: Optional[bool] = True,\n            output_hidden_states: Optional[bool] = False,\n    ):\n        if not kv_caches:\n            kv_caches = [None for _ in range(self.num_layers)]\n        presents = () if use_cache else None\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        all_self_attentions = None\n        all_hidden_states = () if output_hidden_states else None\n        for index in range(self.num_layers):\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer = self._get_layer(index)\n            if self.gradient_checkpointing and self.training:\n                layer_ret = torch.utils.checkpoint.checkpoint(\n                    layer,\n                    hidden_states,\n                    attention_mask,\n                    rotary_pos_emb,\n                    kv_caches[index],\n                    use_cache\n                )\n            else:\n                layer_ret = layer(\n                    hidden_states,\n                    attention_mask,\n                    rotary_pos_emb,\n                    kv_cache=kv_caches[index],\n                    use_cache=use_cache\n                )\n            hidden_states, kv_cache = layer_ret\n            if use_cache:\n                presents = presents + (kv_cache,)\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        # Final layer norm.\n        if self.post_layer_norm:\n            hidden_states = self.final_layernorm(hidden_states)\n\n        return hidden_states, presents, all_hidden_states, all_self_attentions", "\n\nclass ChatGLMPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and\n    a simple interface for downloading and loading pretrained models.\n    \"\"\"\n\n    is_parallelizable = False\n    supports_gradient_checkpointing = True\n    config_class = ChatGLMConfig\n    base_model_prefix = \"transformer\"\n    _no_split_modules = [\"GLMBlock\"]\n\n    def _init_weights(self, module: nn.Module):\n        \"\"\"Initialize the weights.\"\"\"\n        return\n\n    def get_masks(self, input_ids, past_key_values, padding_mask=None):\n        batch_size, seq_length = input_ids.shape\n        full_attention_mask = torch.ones(batch_size, seq_length, seq_length, device=input_ids.device)\n        full_attention_mask.tril_()\n        past_length = 0\n        if past_key_values:\n            past_length = past_key_values[0][0].shape[0]\n        if past_length:\n            full_attention_mask = torch.cat((torch.ones(batch_size, seq_length, past_length,\n                                                        device=input_ids.device), full_attention_mask), dim=-1)\n        if padding_mask is not None:\n            full_attention_mask = full_attention_mask * padding_mask.unsqueeze(1)\n        if not past_length and padding_mask is not None:\n            full_attention_mask -= padding_mask.unsqueeze(-1) - 1\n        full_attention_mask = (full_attention_mask < 0.5).bool()\n        full_attention_mask.unsqueeze_(1)\n        return full_attention_mask\n\n    def get_position_ids(self, input_ids, device):\n        batch_size, seq_length = input_ids.shape\n        position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).repeat(batch_size, 1)\n        return position_ids\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, GLMTransformer):\n            module.gradient_checkpointing = value", "\n\nclass Embedding(torch.nn.Module):\n    \"\"\"Language model embeddings.\"\"\"\n\n    def __init__(self, config: ChatGLMConfig, device=None):\n        super(Embedding, self).__init__()\n\n        self.hidden_size = config.hidden_size\n        # Word embeddings (parallel).\n        self.word_embeddings = nn.Embedding(\n            config.padded_vocab_size,\n            self.hidden_size,\n            dtype=config.torch_dtype,\n            device=device\n        )\n        self.fp32_residual_connection = config.fp32_residual_connection\n\n    def forward(self, input_ids):\n        # Embeddings.\n        words_embeddings = self.word_embeddings(input_ids)\n        embeddings = words_embeddings\n        # Data format change to avoid explicit tranposes : [b s h] --> [s b h].\n        embeddings = embeddings.transpose(0, 1).contiguous()\n        # If the input flag for fp32 residual connection is set, convert for float.\n        if self.fp32_residual_connection:\n            embeddings = embeddings.float()\n        return embeddings", "\n\nclass ChatGLMModel(ChatGLMPreTrainedModel):\n    def __init__(self, config: ChatGLMConfig, device=None, empty_init=True):\n        super().__init__(config)\n        if empty_init:\n            init_method = skip_init\n        else:\n            init_method = default_init\n        init_kwargs = {}\n        if device is not None:\n            init_kwargs[\"device\"] = device\n        self.embedding = init_method(Embedding, config, **init_kwargs)\n        self.num_layers = config.num_layers\n        self.multi_query_group_num = config.multi_query_group_num\n        self.kv_channels = config.kv_channels\n\n        # Rotary positional embeddings\n        self.seq_length = config.seq_length\n        rotary_dim = (\n            config.hidden_size // config.num_attention_heads if config.kv_channels is None else config.kv_channels\n        )\n\n        self.rotary_pos_emb = RotaryEmbedding(rotary_dim // 2, original_impl=config.original_rope, device=device,\n                                              dtype=config.torch_dtype)\n        self.encoder = init_method(GLMTransformer, config, **init_kwargs)\n        self.output_layer = init_method(nn.Linear, config.hidden_size, config.padded_vocab_size, bias=False,\n                                        dtype=config.torch_dtype, **init_kwargs)\n        self.pre_seq_len = config.pre_seq_len\n        self.prefix_projection = config.prefix_projection\n        if self.pre_seq_len is not None:\n            for param in self.parameters():\n                param.requires_grad = False\n            self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n            self.prefix_encoder = PrefixEncoder(config)\n            self.dropout = torch.nn.Dropout(0.1)\n\n    def get_input_embeddings(self):\n        return self.embedding.word_embeddings\n\n    def get_prompt(self, batch_size, device, dtype=torch.half):\n        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(device)\n        past_key_values = self.prefix_encoder(prefix_tokens).type(dtype)\n        past_key_values = past_key_values.view(\n            batch_size,\n            self.pre_seq_len,\n            self.num_layers * 2,\n            self.multi_query_group_num,\n            self.kv_channels\n        )\n        # seq_len, b, nh, hidden_size\n        past_key_values = self.dropout(past_key_values)\n        past_key_values = past_key_values.permute([2, 1, 0, 3, 4]).split(2)\n        return past_key_values\n\n    def forward(\n            self,\n            input_ids,\n            position_ids: Optional[torch.Tensor] = None,\n            attention_mask: Optional[torch.BoolTensor] = None,\n            full_attention_mask: Optional[torch.BoolTensor] = None,\n            past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n            inputs_embeds: Optional[torch.Tensor] = None,\n            use_cache: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n    ):\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        batch_size, seq_length = input_ids.shape\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embedding(input_ids)\n\n        if full_attention_mask is None:\n            if (attention_mask is not None and not attention_mask.all()) or (past_key_values and seq_length != 1):\n                full_attention_mask = self.get_masks(input_ids, past_key_values, padding_mask=attention_mask)\n\n        # Rotary positional embeddings\n        rotary_pos_emb = self.rotary_pos_emb(self.seq_length)\n        if position_ids is not None:\n            rotary_pos_emb = rotary_pos_emb[position_ids]\n        else:\n            rotary_pos_emb = rotary_pos_emb[None, :seq_length]\n        rotary_pos_emb = rotary_pos_emb.transpose(0, 1).contiguous()\n\n        if past_key_values is None:\n            if self.pre_seq_len is not None:\n                past_key_values = self.get_prompt(batch_size=batch_size, device=input_ids.device,\n                                                  dtype=inputs_embeds.dtype)\n\n        # Run encoder.\n        hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(\n            inputs_embeds, full_attention_mask, rotary_pos_emb=rotary_pos_emb,\n            kv_caches=past_key_values, use_cache=use_cache, output_hidden_states=output_hidden_states\n        )\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=presents,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n        )\n\n    def quantize(self, weight_bit_width: int):\n        from .quantization import quantize\n        quantize(self.encoder, weight_bit_width)\n        return self", "\n\nclass ChatGLMForConditionalGeneration(ChatGLMPreTrainedModel):\n    def __init__(self, config: ChatGLMConfig, empty_init=True, device=None):\n        super().__init__(config)\n\n        self.max_sequence_length = config.max_length\n        self.transformer = ChatGLMModel(config, empty_init=empty_init, device=device)\n        self.config = config\n        self.quantized = False\n\n        if self.config.quantization_bit:\n            self.quantize(self.config.quantization_bit, empty_init=True)\n\n    def _update_model_kwargs_for_generation(\n            self,\n            outputs: ModelOutput,\n            model_kwargs: Dict[str, Any],\n            is_encoder_decoder: bool = False,\n            standardize_cache_format: bool = False,\n    ) -> Dict[str, Any]:\n        # update past_key_values\n        model_kwargs[\"past_key_values\"] = self._extract_past_from_model_output(\n            outputs, standardize_cache_format=standardize_cache_format\n        )\n\n        # update attention mask\n        if \"attention_mask\" in model_kwargs:\n            attention_mask = model_kwargs[\"attention_mask\"]\n            model_kwargs[\"attention_mask\"] = torch.cat(\n                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n            )\n\n        # update position ids\n        if \"position_ids\" in model_kwargs:\n            position_ids = model_kwargs[\"position_ids\"]\n            new_position_id = position_ids[..., -1:].clone()\n            new_position_id += 1\n            model_kwargs[\"position_ids\"] = torch.cat(\n                [position_ids, new_position_id], dim=-1\n            )\n\n        model_kwargs[\"is_first_forward\"] = False\n        return model_kwargs\n\n    def prepare_inputs_for_generation(\n            self,\n            input_ids: torch.LongTensor,\n            past_key_values: Optional[torch.Tensor] = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.Tensor] = None,\n            is_first_forward: bool = True,\n            **kwargs\n    ) -> dict:\n        # only last token for input_ids if past is not None\n        if position_ids is None:\n            position_ids = self.get_position_ids(input_ids, device=input_ids.device)\n        if not is_first_forward:\n            position_ids = position_ids[..., -1:]\n            input_ids = input_ids[:, -1:]\n        return {\n            \"input_ids\": input_ids,\n            \"past_key_values\": past_key_values,\n            \"position_ids\": position_ids,\n            \"attention_mask\": attention_mask,\n            \"return_last_logit\": True\n        }\n\n    def forward(\n            self,\n            input_ids: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.Tensor] = None,\n            past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n            inputs_embeds: Optional[torch.Tensor] = None,\n            labels: Optional[torch.Tensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            return_last_logit: Optional[bool] = False,\n    ):\n        attention_mask: Optional[torch.Tensor] = None\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.transformer(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            attention_mask=attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = transformer_outputs[0]\n        if return_last_logit:\n            hidden_states = hidden_states[-1:]\n        lm_logits = self.transformer.output_layer(hidden_states)\n        lm_logits = lm_logits.transpose(0, 1).contiguous()\n\n        loss = None\n        if labels is not None:\n            lm_logits = lm_logits.to(torch.float32)\n\n            # Shift so that tokens < n predict n\n            shift_logits = lm_logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss(ignore_index=-100)\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\n            lm_logits = lm_logits.to(hidden_states.dtype)\n            loss = loss.to(hidden_states.dtype)\n\n        if not return_dict:\n            output = (lm_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=lm_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )\n\n    @staticmethod\n    def _reorder_cache(\n            past: Tuple[Tuple[torch.Tensor, torch.Tensor], ...], beam_idx: torch.LongTensor\n    ) -> Tuple[Tuple[torch.Tensor, torch.Tensor], ...]:\n        \"\"\"\n        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n        beam_idx at every generation step.\n\n        Output shares the same memory storage as `past`.\n        \"\"\"\n        return tuple(\n            (\n                layer_past[0].index_select(1, beam_idx.to(layer_past[0].device)),\n                layer_past[1].index_select(1, beam_idx.to(layer_past[1].device)),\n            )\n            for layer_past in past\n        )\n\n    def process_response(self, response):\n        response = response.strip()\n        response = response.replace(\"[[\u8bad\u7ec3\u65f6\u95f4]]\", \"2023\u5e74\")\n        return response\n\n    def build_inputs(self, tokenizer, query: str, history: List[Tuple[str, str]] = None):\n        prompt = tokenizer.build_prompt(query, history=history)\n        inputs = tokenizer([prompt], return_tensors=\"pt\")\n        inputs = inputs.to(self.device)\n        return inputs\n\n    def build_stream_inputs(self, tokenizer, query: str, history: List[Tuple[str, str]] = None):\n        if history:\n            prompt = \"\\n\\n[Round {}]\\n\\n\u95ee\uff1a{}\\n\\n\u7b54\uff1a\".format(len(history) + 1, query)\n            input_ids = tokenizer.encode(prompt, add_special_tokens=False)\n            input_ids = input_ids[1:]\n            inputs = tokenizer.batch_encode_plus([(input_ids, None)], return_tensors=\"pt\", add_special_tokens=False)\n        else:\n            prompt = \"[Round {}]\\n\\n\u95ee\uff1a{}\\n\\n\u7b54\uff1a\".format(len(history) + 1, query)\n            inputs = tokenizer([prompt], return_tensors=\"pt\")\n        inputs = inputs.to(self.device)\n        return inputs\n\n    @torch.no_grad()\n    def chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None, max_length: int = 8192, num_beams=1,\n             do_sample=True, top_p=0.8, temperature=0.8, logits_processor=None, **kwargs):\n        if history is None:\n            history = []\n        if logits_processor is None:\n            logits_processor = LogitsProcessorList()\n        logits_processor.append(InvalidScoreLogitsProcessor())\n        gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams, \"do_sample\": do_sample, \"top_p\": top_p,\n                      \"temperature\": temperature, \"logits_processor\": logits_processor, **kwargs}\n        inputs = self.build_inputs(tokenizer, query, history=history)\n        outputs = self.generate(**inputs, **gen_kwargs)\n        outputs = outputs.tolist()[0][len(inputs[\"input_ids\"][0]):]\n        response = tokenizer.decode(outputs)\n        response = self.process_response(response)\n        history = history + [(query, response)]\n        return response, history\n\n    @torch.no_grad()\n    def stream_chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None, past_key_values=None,\n                    max_length: int = 8192, do_sample=True, top_p=0.8, temperature=0.8, logits_processor=None,\n                    return_past_key_values=False, **kwargs):\n        if history is None:\n            history = []\n        if logits_processor is None:\n            logits_processor = LogitsProcessorList()\n        logits_processor.append(InvalidScoreLogitsProcessor())\n        gen_kwargs = {\"max_length\": max_length, \"do_sample\": do_sample, \"top_p\": top_p,\n                      \"temperature\": temperature, \"logits_processor\": logits_processor, **kwargs}\n        if past_key_values is None and not return_past_key_values:\n            inputs = self.build_inputs(tokenizer, query, history=history)\n        else:\n            inputs = self.build_stream_inputs(tokenizer, query, history=history)\n        if past_key_values is not None:\n            past_length = past_key_values[0][0].shape[0]\n            if self.transformer.pre_seq_len is not None:\n                past_length -= self.transformer.pre_seq_len\n            inputs.position_ids += past_length\n            attention_mask = inputs.attention_mask\n            attention_mask = torch.cat((attention_mask.new_ones(1, past_length), attention_mask), dim=1)\n            inputs['attention_mask'] = attention_mask\n        for outputs in self.stream_generate(**inputs, past_key_values=past_key_values,\n                                            return_past_key_values=return_past_key_values, **gen_kwargs):\n            if return_past_key_values:\n                outputs, past_key_values = outputs\n            outputs = outputs.tolist()[0][len(inputs[\"input_ids\"][0]):]\n            response = tokenizer.decode(outputs)\n            if response and response[-1] != \"\ufffd\":\n                response = self.process_response(response)\n                new_history = history + [(query, response)]\n                if return_past_key_values:\n                    yield response, new_history, past_key_values\n                else:\n                    yield response, new_history\n\n    @torch.no_grad()\n    def stream_generate(\n            self,\n            input_ids,\n            generation_config: Optional[GenerationConfig] = None,\n            logits_processor: Optional[LogitsProcessorList] = None,\n            stopping_criteria: Optional[StoppingCriteriaList] = None,\n            prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n            return_past_key_values=False,\n            **kwargs,\n    ):\n        batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n\n        if generation_config is None:\n            generation_config = self.generation_config\n        generation_config = copy.deepcopy(generation_config)\n        model_kwargs = generation_config.update(**kwargs)\n        bos_token_id, eos_token_id = generation_config.bos_token_id, generation_config.eos_token_id\n\n        if isinstance(eos_token_id, int):\n            eos_token_id = [eos_token_id]\n\n        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n        if has_default_max_length and generation_config.max_new_tokens is None:\n            warnings.warn(\n                f\"Using `max_length`'s default ({generation_config.max_length}) to control the generation length. \"\n                \"This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we\"\n                \" recommend using `max_new_tokens` to control the maximum length of the generation.\",\n                UserWarning,\n            )\n        elif generation_config.max_new_tokens is not None:\n            generation_config.max_length = generation_config.max_new_tokens + input_ids_seq_length\n            if not has_default_max_length:\n                logger.warn(\n                    f\"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(=\"\n                    f\"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. \"\n                    \"Please refer to the documentation for more information. \"\n                    \"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\",\n                    UserWarning,\n                )\n\n        if input_ids_seq_length >= generation_config.max_length:\n            input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n            logger.warning(\n                f\"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to\"\n                f\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\n                \" increasing `max_new_tokens`.\"\n            )\n\n        # 2. Set generation parameters if not already defined\n        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n\n        logits_processor = self._get_logits_processor(\n            generation_config=generation_config,\n            input_ids_seq_length=input_ids_seq_length,\n            encoder_input_ids=input_ids,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            logits_processor=logits_processor,\n        )\n\n        stopping_criteria = self._get_stopping_criteria(\n            generation_config=generation_config, stopping_criteria=stopping_criteria\n        )\n        logits_warper = self._get_logits_warper(generation_config)\n\n        unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n        scores = None\n        while True:\n            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n            # forward pass to get next token\n            outputs = self(\n                **model_inputs,\n                return_dict=True,\n                output_attentions=False,\n                output_hidden_states=False,\n            )\n\n            next_token_logits = outputs.logits[:, -1, :]\n\n            # pre-process distribution\n            next_token_scores = logits_processor(input_ids, next_token_logits)\n            next_token_scores = logits_warper(input_ids, next_token_scores)\n\n            # sample\n            probs = nn.functional.softmax(next_token_scores, dim=-1)\n            if generation_config.do_sample:\n                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n            else:\n                next_tokens = torch.argmax(probs, dim=-1)\n\n            # update generated ids, model inputs, and length for next step\n            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n            model_kwargs = self._update_model_kwargs_for_generation(\n                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n            )\n            unfinished_sequences = unfinished_sequences.mul((sum(next_tokens != i for i in eos_token_id)).long())\n            if return_past_key_values:\n                yield input_ids, outputs.past_key_values\n            else:\n                yield input_ids\n            # stop when each sentence is finished, or if we exceed the maximum length\n            if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n                break\n\n    def quantize(self, bits: int, empty_init=False, device=None, **kwargs):\n        if bits == 0:\n            return\n\n        from .quantization import quantize\n\n        if self.quantized:\n            logger.info(\"Already quantized.\")\n            return self\n\n        self.quantized = True\n\n        self.config.quantization_bit = bits\n\n        self.transformer.encoder = quantize(self.transformer.encoder, bits, empty_init=empty_init, device=device,\n                                            **kwargs)\n        return self", ""]}
{"filename": "onnx_export/export_compare_data.py", "chunked_list": ["import os\nimport sys\nimport torch\nimport argparse\n\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\nsys.path.append(project_dir)\nfrom onnx_export.utils import build_inputs\nfrom chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration", "from onnx_export.utils import build_inputs\nfrom chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\nfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\nfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\nparser = argparse.ArgumentParser(description='export pytorch model to onnx')\nparser.add_argument(\n    '--data_type',\n    default=\"fp32\",\n    help='use fp16/fp32 to export input/output, Defualt is fp32'\n)", "    help='use fp16/fp32 to export input/output, Defualt is fp32'\n)\n\nargs = parser.parse_args()\nif args.data_type == \"fp16\":\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\noutput_dir = os.path.join(project_dir, \"output\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)", "\noutput_dir = os.path.join(project_dir, \"output\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# save input tensor\npt_input_path1 = os.path.join(output_dir, \"pt_input1.pt\")\npt_input_path2 = os.path.join(output_dir, \"pt_input2.pt\")\npt_input_dict1 = dict()\npt_input_dict2 = dict()", "pt_input_dict1 = dict()\npt_input_dict2 = dict()\n# save output tensor\npt_output_path1 = os.path.join(output_dir, \"pt_output1.pt\")\npt_output_path2 = os.path.join(output_dir, \"pt_output2.pt\")\npt_output_dict1 = dict()\npt_output_dict2 = dict()\n\n\nclass Container(torch.nn.Module):\n    def __init__(self, my_values):\n        super().__init__()\n        for key in my_values:\n            setattr(self, key, my_values[key])", "\nclass Container(torch.nn.Module):\n    def __init__(self, my_values):\n        super().__init__()\n        for key in my_values:\n            setattr(self, key, my_values[key])\n\n\nquery = \"\u665a\u4e0a\u7761\u4e0d\u7740\u5e94\u8be5\u600e\u4e48\u529e\"\nhistory = [", "query = \"\u665a\u4e0a\u7761\u4e0d\u7740\u5e94\u8be5\u600e\u4e48\u529e\"\nhistory = [\n    (\n        \"\u4f60\u597d\",\n        \"\u4f60\u597d\ud83d\udc4b!\u6211\u662f\u4eba\u5de5\u667a\u80fd\u52a9\u624b ChatGLM-6B,\u5f88\u9ad8\u5174\u89c1\u5230\u4f60,\u6b22\u8fce\u95ee\u6211\u4efb\u4f55\u95ee\u9898\u3002\",\n    )\n]\n\nmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\ntokenizer = ChatGLMTokenizer.from_pretrained(model_dir)", "model_dir = os.path.join(project_dir, \"chatglm2_6b\")\ntokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\ninput_tensors = build_inputs(device, tokenizer, query, history)\n# model = ChatGLMForConditionalGeneration.from_pretrained(model_dir)\nconfig = ChatGLMConfig.from_pretrained(model_dir)\n# config.num_layers = 1\nmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\nif device == \"cuda\":\n    model = model.half().cuda()\nelse:\n    model = model.float().cpu()", "model.eval()\n\n# debug this to get input2\n# model.chat(tokenizer=tokenizer, query=prompt)\n\n# test chat speed\n\"\"\"\nall_res = []\nprint(\"test chat speed for pytorch model, may cost a lot time\", )\ntest_text = \"\u4f60\u597d, \u8bf7\u7528python\u5199\u4e00\u4e2a\u94fe\u8868\u3002\"", "print(\"test chat speed for pytorch model, may cost a lot time\", )\ntest_text = \"\u4f60\u597d, \u8bf7\u7528python\u5199\u4e00\u4e2a\u94fe\u8868\u3002\"\nst = time.time()\nfor i in trange(10):\n    responses, history = model.chat(tokenizer=tokenizer, query=test_text)\n    all_res.append(responses)\net = time.time()\ntokens = tokenizer.encode(\"\".join(all_res), return_tensors=\"pt\")[0]\ntoken_num = len(tokens)\nspeed = round(token_num / (et - st), 1)", "token_num = len(tokens)\nspeed = round(token_num / (et - st), 1)\nprint(\"speed: {} tokens/s\".format(speed))\n\"\"\"\n\n# --- prepare data for input1 ---\ninput_ids1 = input_tensors[\"input_ids\"]\nposition_ids1 = input_tensors[\"position_ids\"]\n# save input1\npt_input_dict1[\"input_ids\"] = input_ids1[:1].detach().cpu()", "# save input1\npt_input_dict1[\"input_ids\"] = input_ids1[:1].detach().cpu()\npt_input_dict1[\"position_ids\"] = position_ids1[:1].detach()\nif args.data_type == \"fp16\":\n    dtype = torch.float16\nelse:\n    dtype = torch.float32\noutput_dict1 = model.forward(\n    input_ids=input_ids1,\n    position_ids=position_ids1,", "    input_ids=input_ids1,\n    position_ids=position_ids1,\n)\n\n# save output1 logists\npt_output_dict1[\"logits\"] = output_dict1[\"logits\"][:1].detach().cpu()\npt_output_dict1[\"num_layers\"] = config.num_layers\npast_key_values_1 = output_dict1[\"past_key_values\"]\nprint(\"one past_key_shape for input 1 is \", past_key_values_1[0][0].shape)\nprint(\"logits for input1 shape is \", output_dict1[\"logits\"].shape)", "print(\"one past_key_shape for input 1 is \", past_key_values_1[0][0].shape)\nprint(\"logits for input1 shape is \", output_dict1[\"logits\"].shape)\n\n# --- prepare data for input2 ---\n# copy from forward in second time\ninput_ids2 = torch.tensor([[30910]]).to(device)\n\n# copy from _update_model_kwargs_for_generation in modeling_chatglm.py\nnew_position_id = position_ids1[..., -1:].clone()\nnew_position_id += 1", "new_position_id = position_ids1[..., -1:].clone()\nnew_position_id += 1\nposition_ids2 = torch.cat(\n    [position_ids1, new_position_id], dim=-1\n)\n# input_ids2 = torch.cat((input_ids2, input_ids2), dim=0)\n# position_ids2 = torch.cat((position_ids2, position_ids2), dim=0)\n# attention_mask2 = torch.cat((attention_mask2, attention_mask2), dim=0)\noutput_dict2 = model.forward(\n    input_ids=input_ids2,", "output_dict2 = model.forward(\n    input_ids=input_ids2,\n    position_ids=position_ids2,\n    past_key_values=past_key_values_1,\n)\npast_key_values_2 = output_dict2[\"past_key_values\"]\nprint(\"one past_key_shape for input 2 is \", past_key_values_2[0][0].shape)\nprint(\"logits for input2 shape is \", output_dict2[\"logits\"].shape)\n\n# save input2", "\n# save input2\npt_input_dict2[\"input_ids\"] = input_ids2[:1].detach().cpu()\npt_input_dict2[\"position_ids\"] = position_ids2[:1].detach().cpu()\n\n# save logits2\npt_output_dict2[\"logits\"] = output_dict2[\"logits\"][:1].detach().cpu()\npt_output_dict2[\"num_layers\"] = config.num_layers\n\nfor layer_idx in range(model.config.num_layers):\n    # --- input key and value ---\n    past_key_name = f\"past_key_values.{layer_idx}.key\"\n    past_value_name = f\"past_key_values.{layer_idx}.value\"\n    # --- output key and value ---\n    present_key_name = f\"present_key_values.{layer_idx}.key\"\n    present_value_name = f\"present_key_values.{layer_idx}.value\"\n\n    # save output1 present_key_values \n    present_key = past_key_values_1[layer_idx][0][:,:1].detach().cpu()\n    present_value = past_key_values_1[layer_idx][1][:, :1].detach().cpu()\n    pt_output_dict1[present_key_name] = present_key\n    pt_output_dict1[present_value_name] = present_value\n\n    # save input2 past_key_values\n    # input2 past_key_values is same as output1 present_key_values\n    pt_input_dict2[past_key_name] = present_key\n    pt_input_dict2[past_value_name] = present_value\n\n    # save output2 present_key_values\n    present_key2 = past_key_values_2[layer_idx][0][:, :1].detach().cpu()\n    present_value2 = past_key_values_2[layer_idx][1][:, :1].detach().cpu()\n    pt_output_dict2[present_key_name] = present_key2\n    pt_output_dict2[present_value_name] = present_value2", "\nfor layer_idx in range(model.config.num_layers):\n    # --- input key and value ---\n    past_key_name = f\"past_key_values.{layer_idx}.key\"\n    past_value_name = f\"past_key_values.{layer_idx}.value\"\n    # --- output key and value ---\n    present_key_name = f\"present_key_values.{layer_idx}.key\"\n    present_value_name = f\"present_key_values.{layer_idx}.value\"\n\n    # save output1 present_key_values \n    present_key = past_key_values_1[layer_idx][0][:,:1].detach().cpu()\n    present_value = past_key_values_1[layer_idx][1][:, :1].detach().cpu()\n    pt_output_dict1[present_key_name] = present_key\n    pt_output_dict1[present_value_name] = present_value\n\n    # save input2 past_key_values\n    # input2 past_key_values is same as output1 present_key_values\n    pt_input_dict2[past_key_name] = present_key\n    pt_input_dict2[past_value_name] = present_value\n\n    # save output2 present_key_values\n    present_key2 = past_key_values_2[layer_idx][0][:, :1].detach().cpu()\n    present_value2 = past_key_values_2[layer_idx][1][:, :1].detach().cpu()\n    pt_output_dict2[present_key_name] = present_key2\n    pt_output_dict2[present_value_name] = present_value2", "\n\n# save input1\ninput_container1 = torch.jit.script(Container(pt_input_dict1))\ninput_container1.save(pt_input_path1)\n\n# save output1\noutput1_container = torch.jit.script(Container(pt_output_dict1))\noutput1_container.save(pt_output_path1)\n", "output1_container.save(pt_output_path1)\n\n# save input2\ninput2_container = torch.jit.script(Container(pt_input_dict2))\ninput2_container.save(pt_input_path2)\n\n# save output2\noutput2_container = torch.jit.script(Container(pt_output_dict2))\noutput2_container.save(pt_output_path2)", "output2_container.save(pt_output_path2)"]}
{"filename": "onnx_export/export_test_v3.py", "chunked_list": ["import os\n# from transformers import AutoTokenizer, AutoModel, AutoConfig\nimport torch\nimport sys\nimport argparse\nfrom transformers.generation.utils import LogitsProcessorList\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\nsys.path.append(project_dir)\nfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig", "sys.path.append(project_dir)\nfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\nfrom chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\nfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\nfrom onnx_export.utils import build_inputs\nfrom transformers.models.bloom import BloomOnnxConfig\nparser = argparse.ArgumentParser(description='export pytorch model to onnx')\nparser.add_argument(\n    '--data_type',\n    default=\"fp32\",", "    '--data_type',\n    default=\"fp32\",\n    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n)\n\nargs = parser.parse_args()\nif args.data_type == \"fp16\":\n    device = 'cuda'\nelse:\n    device = 'cpu'", "output_dir = os.path.join(project_dir, \"output\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\nonnx_output_dir = os.path.join(output_dir, \"onnx_output\")\nif not os.path.exists(onnx_output_dir):\n    os.mkdir(onnx_output_dir)\n\nquery = \"\u60f3\u8981\u51fa\u56fd\u7559\u5b66\uff0c\u5e94\u8be5\u600e\u4e48\u529e\uff1f\"\nhistory = [\n    (", "history = [\n    (\n        \"\u4f60\u597d\",\n        \"\u4f60\u597d\ud83d\udc4b!\u6211\u662f\u4eba\u5de5\u667a\u80fd\u52a9\u624b ChatGLM2-6B,\u5f88\u9ad8\u5174\u89c1\u5230\u4f60,\u6b22\u8fce\u95ee\u6211\u4efb\u4f55\u95ee\u9898\u3002\",\n    )\n]\n\nmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\ntokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\nconfig = ChatGLMConfig.from_pretrained(model_dir)", "tokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\nconfig = ChatGLMConfig.from_pretrained(model_dir)\n# config.num_layers = 1\nmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\nif device == \"cuda\":\n    model = model.half().cuda()\nelse:\n    model = model.float().cpu()\ndevice = torch.device(device)\nmodel.eval()", "device = torch.device(device)\nmodel.eval()\n# input_tensors\ninput_tensors = build_inputs(device, tokenizer, query, history)\n\n# --debug for chat --\n# response, history = model.chat(tokenizer, query, history)\n# print(\"res\", response)\nprint(\"=\" * 50)\nprint(\" ---forward first --- \")", "print(\"=\" * 50)\nprint(\" ---forward first --- \")\noutputs = model.forward(\n    **input_tensors\n)\n\nprint(\" ---forward first with fake past_key_values --- \")\ninput_ids = input_tensors[\"input_ids\"]\nbatch = input_ids.shape[0]\npake_past_key_values = [", "batch = input_ids.shape[0]\npake_past_key_values = [\n    [\n        torch.zeros([0, batch, 2, 128], device=input_ids.device)\n        for _ in range(2)\n    ]\n    for _ in range(model.config.num_layers)\n]\noutputs2 = model.forward(\n    **input_tensors,", "outputs2 = model.forward(\n    **input_tensors,\n    past_key_values=pake_past_key_values\n)\n\n\ndef compare_diff(outputs_1, outputs_2):\n    print(\"--- compare diff ---\")\n    max_diff = 0\n    logits_diff = (outputs_2[\"logits\"] - outputs_1[\"logits\"]).max().item()\n    if logits_diff > max_diff:\n        max_diff = logits_diff\n    print(\"logits diff is \", logits_diff)\n    past_key_values0 = outputs_1[\"past_key_values\"]\n    past_key_values1 = outputs_2[\"past_key_values\"]\n    for i in range(model.config.num_layers):\n        present_key_name = f\"present_key_values.{i}.key\"\n        present_value_name = f\"present_key_values.{i}.value\"\n        diff1 = (past_key_values0[i][0] - past_key_values1[i][0]).max().item()\n        diff2 = (past_key_values0[i][1] - past_key_values1[i][1]).max().item()\n        print(f\"{present_key_name} diff: \", diff1)\n        print(f\"{present_value_name} diff: \", diff2)\n        if diff1 > max_diff:\n            max_diff = diff1\n        if diff2 > max_diff:\n            max_diff = diff2\n\n    print(\"max diff is: \", max_diff)", "\n\ncompare_diff(outputs, outputs2)\nprint(\"=\" * 50)\n\n\n\n"]}
{"filename": "onnx_export/change_onnx2.py", "chunked_list": ["import onnx\nimport os\n\n# Load the ONNX model\nmodel = onnx.load(\"onnx_output/chatglm_6b.onnx\")\nfor node in model.graph.node:\n  # if node.name in [\"If_92\", \"If_100\"]:\n  if node.op_type == \"If\":\n    # print(node)\n    attr_name = node.attribute[1].name\n    output1 = node.attribute[0].g.output[0]\n    output2 = node.attribute[1].g.output[0]\n    output_dim1 = output1.type.tensor_type.shape.dim\n    output_dim2 = output2.type.tensor_type.shape.dim\n    # print(len(output_dim1))\n    # print(len(output_dim2))\n    if len(output_dim1) != len(output_dim2):\n        print(\"======old node======\")\n        print(node)\n        attr_name = node.attribute[1].name\n        output_name = node.attribute[1].g.output[0].name\n        sub_graph_name = node.attribute[1].g.name\n        node.attribute[1].CopyFrom(node.attribute[0])\n        node.attribute[1].g.name = sub_graph_name\n        node.attribute[1].name = attr_name\n        node.attribute[1].g.output[0].name = output_name\n        node.attribute[1].g.node[-1].output[0] = output_name\n        node.attribute[1].g.node[0].output[0] = \"my_\" + node.attribute[1].g.node[0].output[0]\n        node.attribute[1].g.node[1].input[1] = \"my_\" + node.attribute[1].g.node[1].input[1]\n        for n in node.attribute[1].g.node:\n            n.name = \"my_\" + n.name\n        print(\"======new node======\")\n        print(node)", "# Create an external data directory to save tensors\nexternal_data_dir = \"onnx_output2\"\nif not os.path.exists(external_data_dir):\n    os.makedirs(external_data_dir)\nelse:\n    for file in os.listdir(external_data_dir):\n        os.remove(os.path.join(external_data_dir, file))\n\n# Save the model with external data\n# not support onnx memory > 2GB, to sovel it, see this url: https://github.com/onnx/onnx/issues/3275", "# Save the model with external data\n# not support onnx memory > 2GB, to sovel it, see this url: https://github.com/onnx/onnx/issues/3275\nonnx.save(\n    model,\n    \"onnx_output2/chatglm_6b.onnx\",\n    save_as_external_data=True,\n    all_tensors_to_one_file=False\n)\nprint(\"onnx saved success\")", "print(\"onnx saved success\")"]}
{"filename": "onnx_export/utils.py", "chunked_list": ["import torch\nfrom typing import List, Tuple\n\n\ndef build_inputs(device, tokenizer, query: str,\n                 history: List[Tuple[str, str]] = None):\n    prompt = \"\"\n    for i, (old_query, response) in enumerate(history):\n        prompt += \"[Round {}]\\n\\n\u95ee\uff1a{}\\n\\n\u7b54\uff1a{}\\n\\n\".format(i + 1, old_query,\n                                                            response)\n    prompt += \"[Round {}]\\n\\n\u95ee\uff1a{}\\n\\n\u7b54\uff1a\".format(len(history) + 1, query)\n    inputs = tokenizer([prompt], return_tensors=\"pt\")\n    inputs = inputs.to(device)\n    return inputs", ""]}
{"filename": "onnx_export/run_onnx_cpu.py", "chunked_list": ["import onnxruntime as ort\nimport torch\nimport numpy as np\nimport os\nfrom colored import fg, stylize\n\n\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\noutput_dir = os.path.join(project_dir, \"output\")", "project_dir = os.path.dirname(now_dir)\noutput_dir = os.path.join(project_dir, \"output\")\n# model_dir = os.path.join(project_dir, \"chatglm_6b\")\nonnx_path_with_cache = os.path.join(output_dir, \"onnx_output\", \"chatglm2_6b.onnx\")\nonnx_path_no_cache = os.path.join(output_dir, \"onnx_output_no_cache\", \"chatglm2_6b.onnx\")\n\n\ndef compare_value(pre_numpy: np.array, true_numpy: np.array):\n    assert pre_numpy.shape == true_numpy.shape\n    diff = np.abs(pre_numpy - true_numpy).max()\n    if diff > 5e-4:\n        print(stylize(f\"diff: {diff} is_pass: failed\", fg(\"red\")))\n    else:\n        print(stylize(f\"diff: {diff} is_pass: OK\", fg(\"green\")))\n    return diff", "\n\ndef run_cpu_onnx_inference(onnx_path, input_path: str, output_path):\n    \"\"\"\n    \"\"\"\n    # ndWtokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n    # providers = [(\"CUDAExecutionProvider\", {\"cudnn_conv_use_max_workspace\": '1'})]\n    providers = [\"CPUExecutionProvider\"]\n    sess_options = ort.SessionOptions()\n    # sess_options.optimized_model_filepath = new_onnx_path\n    # sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n    print(f\"loading onnx {onnx_path}, please wait...\")\n    session = ort.InferenceSession(\n        onnx_path, sess_options=sess_options, providers=providers\n    )\n    print(session.get_providers())\n    input_dict = torch.jit.load(input_path)\n    output_dict = torch.jit.load(output_path)\n    input_ids = input_dict.input_ids.data.cpu().numpy().astype(np.int64)\n    position_ids = input_dict.position_ids.data.cpu().numpy().astype(np.int64)\n    logits = output_dict.logits.data.cpu().numpy()\n    key = \"present_key_values.0.key\"\n    one_present_key = getattr(output_dict, key).data.cpu().numpy()\n    num_layers = getattr(output_dict, \"num_layers\")\n\n    io_binding = session.io_binding()\n    print(\"input number\", len(session.get_inputs()))\n    print(\"output number\", len(session.get_outputs()))\n    input_names = [_.name for _ in session.get_inputs()]\n    print(\"=================input names=================\")\n    print(input_names)\n    output_names = [_.name for _ in session.get_outputs()]\n    print(\"=================output names=================\")\n    print(output_names)\n    io_binding.bind_cpu_input(\n        \"input_ids\",\n        input_ids\n    )\n    io_binding.bind_cpu_input(\n        \"position_ids\",\n        position_ids\n    )\n\n    for layer_idx in range(num_layers):\n        input_names = [\n            f\"past_key_values.{layer_idx}.key\",\n            f\"past_key_values.{layer_idx}.value\"\n        ]\n        # inputs[input_names[0]] = past_key_values\n        # inputs[input_names[1]] = past_key_values\n        for name in input_names:\n            try:\n                past_key_values = getattr(input_dict, name).data.cpu().numpy()\n            except Exception:\n                past_key_values = np.zeros(\n                    [0, input_ids.shape[1], 2, 128],\n                    dtype=one_present_key.dtype\n                )\n            io_binding.bind_cpu_input(\n                name,\n                past_key_values\n            )\n\n\n        output_name = [\n            f\"present_key_values.{layer_idx}.key\",\n            f\"present_key_values.{layer_idx}.value\"\n        ]\n        for name in output_name:\n            io_binding.bind_output(\n                name,\n                device_type=\"cpu\",\n                device_id=0,\n                element_type=one_present_key.dtype,\n                shape=one_present_key.shape,\n            )\n    io_binding.bind_output(\n        name=\"logits\",\n        device_type=\"cpu\",\n        device_id=0,\n        element_type=logits.dtype,\n        shape=logits.shape,\n    )\n\n    # print(inputs)\n    session.run_with_iobinding(io_binding)\n    max_diff = 0\n    # compile logists\n    print('=' * 20)\n    print(\"compare logits\")\n    pred_outputs = io_binding.copy_outputs_to_cpu()\n    diff1 = compare_value(pred_outputs[-1], logits)\n    if diff1 > max_diff:\n        max_diff = diff1\n\n    # compile present_key_values\n    for i in range(num_layers):\n        key_name = f\"present_key_values.{i}.key\"\n        value_name = f\"present_key_values.{i}.value\"\n        print('=' * 20)\n        print(f\"compare {key_name}\")\n        # key_numpy = [key_name]\n        key_true = getattr(output_dict, key_name).data.cpu().numpy()\n        key_pred = pred_outputs[i * 2]\n        diff2 = compare_value(key_pred, key_true)\n        if diff2 > max_diff:\n            max_diff = diff2\n        print('=' * 20)\n        print(f\"compare {value_name}\")\n        value_pred = pred_outputs[i * 2 + 1]\n        value_true = getattr(output_dict, value_name).data.cpu().numpy()\n        diff3 = compare_value(value_pred, value_true)\n        if diff3 > max_diff:\n            max_diff = diff3\n    print('=' * 20)\n    print(f\"max diff: {max_diff}\")", "\n\nif __name__ == \"__main__\":\n    input_path1 = os.path.join(output_dir, \"pt_input1.pt\")\n    output_path1 = os.path.join(output_dir, \"pt_output1.pt\")\n    run_cpu_onnx_inference(onnx_path_with_cache, input_path1, output_path1)\n    print(\"\\n\")\n    input_path2 = os.path.join(output_dir, \"pt_input2.pt\")\n    output_path2 = os.path.join(output_dir, \"pt_output2.pt\")\n    run_cpu_onnx_inference(onnx_path_with_cache, input_path2, output_path2)"]}
{"filename": "onnx_export/export2onnx_v2_no_cache.py", "chunked_list": ["import os\n# from transformers import AutoTokenizer, AutoModel, AutoConfig\nimport torch\nimport sys\nimport argparse\nfrom transformers.generation.utils import LogitsProcessorList\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\nsys.path.append(project_dir)\nfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig", "sys.path.append(project_dir)\nfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\nfrom chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\nfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\nfrom onnx_export.utils import build_inputs\nfrom transformers.models.bloom import BloomOnnxConfig\n\n\nparser = argparse.ArgumentParser(description='export pytorch model to onnx')\nparser.add_argument(", "parser = argparse.ArgumentParser(description='export pytorch model to onnx')\nparser.add_argument(\n    '--data_type',\n    default=\"fp32\",\n    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n)\n\nargs = parser.parse_args()\nif args.data_type == \"fp16\":\n    device = 'cuda'\nelse:\n    device = 'cpu'", "if args.data_type == \"fp16\":\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\n\noutput_dir = os.path.join(project_dir, \"output\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\nonnx_output_dir = os.path.join(output_dir, \"onnx_output_no_cache\")\nif not os.path.exists(onnx_output_dir):\n    os.mkdir(onnx_output_dir)\nelse:\n    for file in os.listdir(onnx_output_dir):\n        os.remove(os.path.join(onnx_output_dir, file))", "onnx_output_dir = os.path.join(output_dir, \"onnx_output_no_cache\")\nif not os.path.exists(onnx_output_dir):\n    os.mkdir(onnx_output_dir)\nelse:\n    for file in os.listdir(onnx_output_dir):\n        os.remove(os.path.join(onnx_output_dir, file))\nonnx_model_path = os.path.join(onnx_output_dir, \"chatglm2_6b.onnx\")\n\nquery = \"\u60f3\u8981\u51fa\u56fd\u7559\u5b66\uff0c\u5e94\u8be5\u600e\u4e48\u529e\uff1f\"\nhistory = [", "query = \"\u60f3\u8981\u51fa\u56fd\u7559\u5b66\uff0c\u5e94\u8be5\u600e\u4e48\u529e\uff1f\"\nhistory = [\n    (\n        \"\u4f60\u597d\",\n        \"\u4f60\u597d\ud83d\udc4b!\u6211\u662f\u4eba\u5de5\u667a\u80fd\u52a9\u624b ChatGLM2-6B,\u5f88\u9ad8\u5174\u89c1\u5230\u4f60,\u6b22\u8fce\u95ee\u6211\u4efb\u4f55\u95ee\u9898\u3002\",\n    )\n]\n\nmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\ntokenizer = ChatGLMTokenizer.from_pretrained(model_dir)", "model_dir = os.path.join(project_dir, \"chatglm2_6b\")\ntokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\nconfig = ChatGLMConfig.from_pretrained(model_dir)\n# config.num_layers = 1\nmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\nif device == \"cuda\":\n    model = model.half().cuda()\nelse:\n    model = model.float().cpu()\ndevice = torch.device(device)", "device = torch.device(device)\nmodel.eval()\n# input_tensors\ninput_tensors = build_inputs(device, tokenizer, query, history)\ndel input_tensors[\"attention_mask\"]\n# --debug for chat --\n# response, history = model.chat(tokenizer, query, history)\n# print(\"res\", response)\n\nprint(\" ---forward first --- \")", "\nprint(\" ---forward first --- \")\noutputs = model.forward(\n    **input_tensors\n)\n\nprint(\"--- export onnx ---\")\n# ---prepare for onnx export ---\ninput_names = [\"input_ids\", 'position_ids', \"attention_mask\"]\noutput_names = [\"logits\"]", "input_names = [\"input_ids\", 'position_ids', \"attention_mask\"]\noutput_names = [\"logits\"]\ndynamic_axes = {\n    'input_ids': {0: \"batch_size\", 1: \"sequence\"},\n    'position_ids': {0: \"batch_size\", 1: \"sequence\"},\n    \"logits\": {0: \"batch_size\", 1: \"sequence\"}\n}\nfor layer_idx in range(model.config.num_layers):\n    # --- input key and value ---\n    # --- output key and value ---\n    present_key_name = f\"present_key_values.{layer_idx}.key\"\n    present_value_name = f\"present_key_values.{layer_idx}.value\"\n    output_names += [present_key_name, present_value_name]\n    dynamic_axes.update({\n        present_key_name: {\n            0: \"past_sequence + 1\",\n            1: \"batch_size\"\n        },\n        present_value_name: {\n            0: \"past_sequence + 1\",\n            1: \"batch_size\"\n        }\n    })", "\n\nwith torch.no_grad():\n    torch.onnx.export(\n        model,\n        args=(\n            input_tensors[\"input_ids\"],\n            input_tensors[\"position_ids\"],\n        ),\n        f=onnx_model_path,\n        opset_version=14,\n        input_names=input_names,\n        output_names=output_names,\n        dynamic_axes=dynamic_axes,\n        training=torch.onnx.TrainingMode.EVAL,\n    )"]}
{"filename": "onnx_export/run_onnx_trt.py", "chunked_list": ["import onnxruntime as ort\nimport torch\nimport numpy as np\nimport os\nfrom colored import fg, stylize\n\n\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\noutput_dir = os.path.join(project_dir, \"output\")", "project_dir = os.path.dirname(now_dir)\noutput_dir = os.path.join(project_dir, \"output\")\n# model_dir = os.path.join(project_dir, \"chatglm_6b\")\nonnx_path = os.path.join(output_dir, \"onnx_output\", \"chatglm_6b.onnx\")\nnew_onnx_dir = os.path.join(project_dir, \"output\", \"new_onnx_output\")\nif not os.path.exists(new_onnx_dir):\n    os.mkdir(new_onnx_dir)\nnew_onnx_path = os.path.join(new_onnx_dir, \"chatglm_6b.onnx\")\n\n\ndef compare_value(pre_numpy: np.array, true_numpy: np.array):\n    assert pre_numpy.shape == true_numpy.shape\n    diff = np.abs(pre_numpy - true_numpy).max()\n    if diff > 1e-3:\n        print(stylize(f\"diff: {diff} is_pass: failed\", fg(\"red\")))\n    else:\n        print(stylize(f\"diff: {diff} is_pass: OK\", fg(\"green\")))\n    return diff", "\n\ndef compare_value(pre_numpy: np.array, true_numpy: np.array):\n    assert pre_numpy.shape == true_numpy.shape\n    diff = np.abs(pre_numpy - true_numpy).max()\n    if diff > 1e-3:\n        print(stylize(f\"diff: {diff} is_pass: failed\", fg(\"red\")))\n    else:\n        print(stylize(f\"diff: {diff} is_pass: OK\", fg(\"green\")))\n    return diff", "\ndef run_cuda_onnx_inference(input_path: str, output_path):\n    providers = [\"TensorrtExecutionProvider\", \"CUDAExecutionProvider\"]\n    sess_options = ort.SessionOptions()\n    sess_options.optimized_model_filepath = new_onnx_path\n    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n\n    session = ort.InferenceSession(\n        onnx_path, sess_options=sess_options, providers=providers\n    )\n    print(session.get_providers())\n\n    # cuda device id\n    device_id = 0\n    \n    input_dict = torch.jit.load(input_path)\n    output_dict = torch.jit.load(output_path)\n    input_ids = input_dict.input_ids.data.cpu().numpy().astype(np.int64)\n    position_ids = input_dict.position_ids.data.cpu().numpy().astype(np.int64)\n    attention_mask = input_dict.attention_mask.data.cpu().numpy()\n    logits = output_dict.logits.data.cpu().numpy()\n    key = \"present_key_values.0.decorder.key\"\n    one_present_key = getattr(output_dict, key).data.cpu().numpy()\n    num_layers = getattr(output_dict, \"num_layers\")\n    io_binding = session.io_binding()\n    io_binding.bind_ortvalue_input(\n        \"input_ids\",\n        ort.OrtValue.ortvalue_from_numpy(input_ids, \"cuda\", device_id=device_id)\n    )\n    io_binding.bind_ortvalue_input(\n        \"position_ids\",\n        ort.OrtValue.ortvalue_from_numpy(position_ids, \"cuda\", device_id=device_id)\n    )\n    io_binding.bind_ortvalue_input(\n        \"attention_mask\",\n        ort.OrtValue.ortvalue_from_numpy(attention_mask, \"cuda\", device_id=device_id)\n    )\n    for layer_idx in range(num_layers):\n        input_names = [\n            f\"past_key_values.{layer_idx}.decorder.key\",\n            f\"past_key_values.{layer_idx}.decorder.value\"\n        ]\n        # inputs[input_names[0]] = past_key_values\n        # inputs[input_names[1]] = past_key_values\n        for name in input_names:\n            try:\n                past_key_values = getattr(input_dict, name).data.cpu().numpy()\n            except Exception:\n                past_key_values = np.zeros(\n                    [1, input_ids.shape[1], 32, 128],\n                    dtype=one_present_key.dtype\n                )\n            io_binding.bind_cpu_input(\n                name,\n                past_key_values\n            )\n            io_binding.bind_ortvalue_input(\n                name=name,\n                ortvalue=ort.OrtValue.ortvalue_from_numpy(\n                    past_key_values, \"cuda\", device_id=device_id\n                )\n            )\n        output_name = [\n            f\"present_key_values.{layer_idx}.decorder.key\",\n            f\"present_key_values.{layer_idx}.decorder.value\"\n        ]\n        for name in output_name:\n            output_value = np.zeros_like(\n                one_present_key,\n                dtype=one_present_key.dtype\n            )\n            io_binding.bind_ortvalue_output(\n                name=name,\n                ortvalue=ort.OrtValue.ortvalue_from_numpy(\n                    output_value, \"cuda\", device_id=device_id\n                )\n            )\n    logits_numpy = np.zeros_like(logits, dtype=logits.dtype)\n    io_binding.bind_ortvalue_output(\n        name=\"logits\",\n        ortvalue=ort.OrtValue.ortvalue_from_numpy(\n            logits_numpy\n        )\n    )\n\n    # print(inputs)\n    session.run_with_iobinding(io_binding)\n    # compile logists\n    print('=' * 20)\n    print(\"compare logits\")\n    pred_outputs = io_binding.copy_outputs_to_cpu()\n    compare_value(pred_outputs[-1], logits)\n\n    # compile present_key_values\n    for i in range(num_layers):\n        key_name = f\"present_key_values.{i}.decorder.key\"\n        value_name = f\"present_key_values.{i}.decorder.value\"\n        print('=' * 20)\n        print(f\"compare {key_name}\")\n        # key_numpy = [key_name]\n        key_true = getattr(output_dict, key_name).data.cpu().numpy()\n        key_pred = pred_outputs[i * 2]\n        compare_value(key_pred, key_true)\n        print('=' * 20)\n        print(f\"compare {value_name}\")\n        value_pred = pred_outputs[i * 2 + 1]\n        value_true = getattr(output_dict, value_name).data.cpu().numpy()\n        compare_value(value_pred, value_true)", "\n\n\nif __name__ == \"__main__\":\n    input_path1 = os.path.join(output_dir, \"pt_input1.pt\")\n    output_path1 = os.path.join(output_dir, \"pt_output1.pt\")\n    run_cuda_onnx_inference(input_path1, output_path1)\n    print(\"\\n\")\n    input_path2 = os.path.join(output_dir, \"pt_input2.pt\")\n    output_path2 = os.path.join(output_dir, \"pt_output2.pt\")\n    run_cuda_onnx_inference(input_path2, output_path2)", "\n\n"]}
{"filename": "onnx_export/export2onnx_v2.py", "chunked_list": ["import os\n# from transformers import AutoTokenizer, AutoModel, AutoConfig\nimport torch\nimport sys\nimport argparse\nfrom transformers.generation.utils import LogitsProcessorList\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\nsys.path.append(project_dir)\nfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig", "sys.path.append(project_dir)\nfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\nfrom chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\nfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\nfrom onnx_export.utils import build_inputs\nfrom transformers.models.bloom import BloomOnnxConfig\n\n\nparser = argparse.ArgumentParser(description='export pytorch model to onnx')\nparser.add_argument(", "parser = argparse.ArgumentParser(description='export pytorch model to onnx')\nparser.add_argument(\n    '--data_type',\n    default=\"fp32\",\n    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n)\n\nargs = parser.parse_args()\nif args.data_type == \"fp16\":\n    device = 'cuda'\nelse:\n    device = 'cpu'", "if args.data_type == \"fp16\":\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\n\noutput_dir = os.path.join(project_dir, \"output\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\nonnx_output_dir = os.path.join(output_dir, \"onnx_output\")\nif not os.path.exists(onnx_output_dir):\n    os.mkdir(onnx_output_dir)\nelse:\n    for file in os.listdir(onnx_output_dir):\n        os.remove(os.path.join(onnx_output_dir, file))", "onnx_output_dir = os.path.join(output_dir, \"onnx_output\")\nif not os.path.exists(onnx_output_dir):\n    os.mkdir(onnx_output_dir)\nelse:\n    for file in os.listdir(onnx_output_dir):\n        os.remove(os.path.join(onnx_output_dir, file))\nonnx_model_path = os.path.join(onnx_output_dir, \"chatglm2_6b.onnx\")\n\nquery = \"\u60f3\u8981\u51fa\u56fd\u7559\u5b66\uff0c\u5e94\u8be5\u600e\u4e48\u529e\uff1f\"\nhistory = [", "query = \"\u60f3\u8981\u51fa\u56fd\u7559\u5b66\uff0c\u5e94\u8be5\u600e\u4e48\u529e\uff1f\"\nhistory = [\n    (\n        \"\u4f60\u597d\",\n        \"\u4f60\u597d\ud83d\udc4b!\u6211\u662f\u4eba\u5de5\u667a\u80fd\u52a9\u624b ChatGLM-6B,\u5f88\u9ad8\u5174\u89c1\u5230\u4f60,\u6b22\u8fce\u95ee\u6211\u4efb\u4f55\u95ee\u9898\u3002\",\n    )\n]\n\nmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\ntokenizer = ChatGLMTokenizer.from_pretrained(model_dir)", "model_dir = os.path.join(project_dir, \"chatglm2_6b\")\ntokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\nconfig = ChatGLMConfig.from_pretrained(model_dir)\n# config.num_layers = 1\nmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\nif device == \"cuda\":\n    model = model.half().cuda()\nelse:\n    model = model.float().cpu()\ndevice = torch.device(device)", "device = torch.device(device)\nmodel.eval()\n# input_tensors\ninput_tensors = build_inputs(device, tokenizer, query, history)\ndel input_tensors[\"attention_mask\"]\n# --debug for chat --\n# response, history = model.chat(tokenizer, query, history)\n# print(\"res\", response)\n\nprint(\" ---forward first --- \")", "\nprint(\" ---forward first --- \")\noutputs = model.forward(\n    **input_tensors\n)\n\nprint(\"--second forward ---\")\n# input_ids = input_tensors[\"input_ids\"]\nposition_ids = input_tensors[\"position_ids\"]\npast_key_values = outputs[\"past_key_values\"]", "position_ids = input_tensors[\"position_ids\"]\npast_key_values = outputs[\"past_key_values\"]\n# copy from forward in second time\nnew_input_ids = torch.tensor([[30910]]).to(device)\ninput_ids = torch.cat([input_tensors[\"input_ids\"], new_input_ids], dim=-1)\n\n# copy from _update_model_kwargs_for_generation in modeling_chatglm.py\nnew_position_id = position_ids[..., -1:].clone()\nnew_position_id += 1\nposition_ids = torch.cat(", "new_position_id += 1\nposition_ids = torch.cat(\n    [position_ids, new_position_id], dim=-1\n)\n# copy from prepare_inputs_for_generation in modeling_chatglm.py\n# position_ids = position_ids[..., -1:]\n# input_ids = input_ids[..., -1:]\n# print shape\nprint(\n    \"input_ids shape:\", input_ids.shape,", "print(\n    \"input_ids shape:\", input_ids.shape,\n    \"; type:\", input_ids.dtype\n)\nprint(\n    \"position_ids shape:\", position_ids.shape,\n    \"; type: \", input_ids.dtype\n)\nprint(\n    \"first forward one past_key_value shape: \", past_key_values[0][0].shape,", "print(\n    \"first forward one past_key_value shape: \", past_key_values[0][0].shape,\n    \"; type:\", past_key_values[0][0].dtype\n)\nprint(\"first forward logits shape: \", outputs[\"logits\"].shape)\noutputs2 = model.forward(\n    input_ids=input_ids,\n    position_ids=position_ids,\n    past_key_values=past_key_values\n)", "    past_key_values=past_key_values\n)\nprint(\"--- export onnx ---\")\n# ---prepare for onnx export ---\ninput_names = [\"input_ids\", 'position_ids']\noutput_names = [\"logits\"]\ndynamic_axes = {\n    'input_ids': {0: \"batch_size\", 1: \"sequence\"},\n    'position_ids': {0: \"batch_size\", 1: \"sequence\"},\n    \"logits\": {0: \"batch_size\", 1: \"sequence\"}", "    'position_ids': {0: \"batch_size\", 1: \"sequence\"},\n    \"logits\": {0: \"batch_size\", 1: \"sequence\"}\n}\nfor layer_idx in range(model.config.num_layers):\n    # --- input key and value ---\n    past_key_name = f\"past_key_values.{layer_idx}.key\"\n    past_value_name = f\"past_key_values.{layer_idx}.value\"\n    input_names += [past_key_name, past_value_name]\n    # --- output key and value ---\n    present_key_name = f\"present_key_values.{layer_idx}.key\"\n    present_value_name = f\"present_key_values.{layer_idx}.value\"\n    output_names += [present_key_name, present_value_name]\n    dynamic_axes.update({\n        past_key_name: {\n            0: \"past_sequence\",\n            1: \"batch_size\",\n        },\n        past_value_name: {\n            0: \"past_sequence\",\n            1: \"batch_size\",\n        },\n        present_key_name: {\n            0: \"past_sequence + sequence\",\n            1: \"batch_size\"\n        },\n        present_value_name: {\n            0: \"past_sequence + sequence\",\n            1: \"batch_size\"\n        }\n    })", "\n\nwith torch.no_grad():\n    torch.onnx.export(\n        model,\n        args=(\n            input_ids,\n            position_ids,\n            past_key_values\n        ),\n        f=onnx_model_path,\n        opset_version=14,\n        input_names=input_names,\n        output_names=output_names,\n        dynamic_axes=dynamic_axes,\n        training=torch.onnx.TrainingMode.EVAL,\n    )"]}
{"filename": "onnx_export/export_test.py", "chunked_list": ["import os\n# from transformers import AutoTokenizer, AutoModel, AutoConfig\nimport torch\nimport sys\nimport argparse\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\nsys.path.append(project_dir)\nfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\nfrom chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration", "from chatglm2_6b.configuration_chatglm import ChatGLMConfig\nfrom chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\nfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\nfrom onnx_export.utils import build_inputs\nparser = argparse.ArgumentParser(description='export pytorch model to onnx')\nparser.add_argument(\n    '--data_type',\n    default=\"fp32\",\n    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n)", "    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n)\n\nargs = parser.parse_args()\nif args.data_type == \"fp16\":\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\n", "\n\noutput_dir = os.path.join(project_dir, \"output\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\nonnx_output_dir = os.path.join(output_dir, \"onnx_output\")\nif not os.path.exists(onnx_output_dir):\n    os.mkdir(onnx_output_dir)\n\n", "\n\nquery = \"\u60f3\u8981\u51fa\u56fd\u7559\u5b66\uff0c\u5e94\u8be5\u600e\u4e48\u529e\uff1f\"\nhistory = [\n    (\n        \"\u4f60\u597d\",\n        \"\u4f60\u597d\ud83d\udc4b!\u6211\u662f\u4eba\u5de5\u667a\u80fd\u52a9\u624b ChatGLM2-6B,\u5f88\u9ad8\u5174\u89c1\u5230\u4f60,\u6b22\u8fce\u95ee\u6211\u4efb\u4f55\u95ee\u9898\u3002\",\n    )\n]\n", "]\n\nmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\ntokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\nconfig = ChatGLMConfig.from_pretrained(model_dir)\n# config.num_layers = 1\nmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\nif device == \"cuda\":\n    model = model.half().cuda()\nelse:\n    model = model.float().cpu()", "device = torch.device(device)\nmodel.eval()\n# input_tensors\ninput_tensors = build_inputs(device, tokenizer, query, history)\n\n# --debug for chat --\n# response, history = model.chat(tokenizer, query, history)\n# print(\"res\", response)\nprint(\"=\" * 50)\nprint(\" ---forward first --- \")", "print(\"=\" * 50)\nprint(\" ---forward first --- \")\noutputs = model.forward(\n    **input_tensors\n)\n\nprint(\" ---forward first with no attention_mask --- \")\noutputs2 = model.forward(\n    input_ids=input_tensors[\"input_ids\"],\n    position_ids=input_tensors[\"position_ids\"],", "    input_ids=input_tensors[\"input_ids\"],\n    position_ids=input_tensors[\"position_ids\"],\n)\n\n\ndef compare_diff(outputs_1, outputs_2):\n    print(\"--- compare diff ---\")\n    max_diff = 0\n    logits_diff = (outputs_2[\"logits\"] - outputs_1[\"logits\"]).max().item()\n    if logits_diff > max_diff:\n        max_diff = logits_diff\n    print(\"logits diff is \", logits_diff)\n    past_key_values0 = outputs_1[\"past_key_values\"]\n    past_key_values1 = outputs_2[\"past_key_values\"]\n    for i in range(model.config.num_layers):\n        present_key_name = f\"present_key_values.{i}.key\"\n        present_value_name = f\"present_key_values.{i}.value\"\n        diff1 = (past_key_values0[i][0] - past_key_values1[i][0]).max().item()\n        diff2 = (past_key_values0[i][1] - past_key_values1[i][1]).max().item()\n        print(f\"{present_key_name} diff: \", diff1)\n        print(f\"{present_value_name} diff: \", diff2)\n        if diff1 > max_diff:\n            max_diff = diff1\n        if diff2 > max_diff:\n            max_diff = diff2\n\n    print(\"max diff is: \", max_diff)", "\n\ncompare_diff(outputs, outputs2)\nprint(\"=\" * 50)\n\n\nprint(\"=\" * 50)\nprint(\" ---forward second --- \")\nattention_mask = input_tensors[\"attention_mask\"]\nposition_ids = input_tensors[\"position_ids\"]", "attention_mask = input_tensors[\"attention_mask\"]\nposition_ids = input_tensors[\"position_ids\"]\npast_key_values = outputs[\"past_key_values\"]\n# copy from forward in second time\ninput_ids = torch.tensor([[30910]]).to(device)\nattention_mask = torch.cat(\n    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n)\nnew_position_id = position_ids[..., -1:].clone()\nnew_position_id += 1", "new_position_id = position_ids[..., -1:].clone()\nnew_position_id += 1\nposition_ids = torch.cat(\n    [position_ids, new_position_id], dim=-1\n)\n# copy from prepare_inputs_for_generation in modeling_chatglm.py\nposition_ids = position_ids[..., -1:]\npast_key_values1 = outputs[\"past_key_values\"]\noutputs_3 = model.forward(\n    input_ids=input_ids,", "outputs_3 = model.forward(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    position_ids=position_ids,\n    past_key_values=past_key_values1\n)\n\nprint(\" ---forward second with no attention_mask --- \")\noutputs_4 = model.forward(\n    input_ids=input_ids,", "outputs_4 = model.forward(\n    input_ids=input_ids,\n    position_ids=position_ids,\n    past_key_values=past_key_values1\n)\ncompare_diff(outputs_3, outputs_4)\nprint(\"=\" * 50)\n\n\n", "\n"]}
{"filename": "onnx_export/run_onnx_cuda.py", "chunked_list": ["import onnxruntime as ort\nimport torch\nimport numpy as np\nimport os\nfrom colored import fg, stylize\n\n\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\noutput_dir = os.path.join(project_dir, \"output\")", "project_dir = os.path.dirname(now_dir)\noutput_dir = os.path.join(project_dir, \"output\")\n# model_dir = os.path.join(project_dir, \"chatglm_6b\")\nonnx_path_with_cache = os.path.join(output_dir, \"onnx_output\", \"chatglm2_6b.onnx\")\nonnx_path_no_cache = os.path.join(output_dir, \"onnx_output_no_cache\", \"chatglm2_6b.onnx\")\n\n\ndef compare_value(pre_numpy: np.array, true_numpy: np.array):\n    assert pre_numpy.shape == true_numpy.shape\n    diff = np.abs(pre_numpy - true_numpy).max()\n    if diff > 1e-3:\n        print(stylize(f\"diff: {diff} is_pass: failed\", fg(\"red\")))\n    else:\n        print(stylize(f\"diff: {diff} is_pass: OK\", fg(\"green\")))\n    return diff", "\n\ndef run_cuda_onnx_inference(onnx_path, input_path: str, output_path):\n    providers = [(\"CUDAExecutionProvider\", {'enable_cuda_graph': False})]\n    sess_options = ort.SessionOptions()\n\n    session = ort.InferenceSession(\n        onnx_path, sess_options=sess_options, providers=providers\n    )\n    print(session.get_providers())\n\n    # cuda device id\n    device_id = 0\n    \n    input_dict = torch.jit.load(input_path)\n    output_dict = torch.jit.load(output_path)\n    input_ids = input_dict.input_ids.data.cpu().numpy().astype(np.int64)\n    position_ids = input_dict.position_ids.data.cpu().numpy().astype(np.int64)\n    attention_mask = input_dict.attention_mask.data.cpu().numpy()\n    logits = output_dict.logits.data.cpu().numpy()\n    key = \"present_key_values.0.key\"\n    one_present_key = getattr(output_dict, key).data.cpu().numpy()\n    num_layers = getattr(output_dict, \"num_layers\")\n    io_binding = session.io_binding()\n    io_binding.bind_ortvalue_input(\n        \"input_ids\",\n        ort.OrtValue.ortvalue_from_numpy(input_ids, \"cuda\", device_id=device_id)\n    )\n    io_binding.bind_ortvalue_input(\n        \"position_ids\",\n        ort.OrtValue.ortvalue_from_numpy(position_ids, \"cuda\", device_id=device_id)\n    )\n    io_binding.bind_ortvalue_input(\n        \"attention_mask\",\n        ort.OrtValue.ortvalue_from_numpy(attention_mask, \"cuda\", device_id=device_id)\n    )\n    for layer_idx in range(num_layers):\n        input_names = [\n            f\"past_key_values.{layer_idx}.key\",\n            f\"past_key_values.{layer_idx}.value\"\n        ]\n        # inputs[input_names[0]] = past_key_values\n        # inputs[input_names[1]] = past_key_values\n        for name in input_names:\n            try:\n                past_key_values = getattr(input_dict, name).data.cpu().numpy()\n                io_binding.bind_ortvalue_input(\n                    name=name,\n                    ortvalue=ort.OrtValue.ortvalue_from_numpy(\n                        past_key_values, \"cuda\", device_id=device_id\n                    )\n                )\n            except Exception:\n                past_key_values = np.zeros(\n                    [1, input_ids.shape[1], 32, 128],\n                    dtype=one_present_key.dtype\n                )\n            # io_binding.bind_cpu_input(\n            #     name,\n            #     past_key_values\n            # )\n\n        output_name = [\n            f\"present_key_values.{layer_idx}.key\",\n            f\"present_key_values.{layer_idx}.value\"\n        ]\n        for name in output_name:\n            output_value = np.zeros_like(\n                one_present_key,\n                dtype=one_present_key.dtype\n            )\n            io_binding.bind_ortvalue_output(\n                name=name,\n                ortvalue=ort.OrtValue.ortvalue_from_numpy(\n                    output_value, \"cuda\", device_id=device_id\n                )\n            )\n    logits_numpy = np.zeros_like(logits, dtype=logits.dtype)\n    io_binding.bind_ortvalue_output(\n        name=\"logits\",\n        ortvalue=ort.OrtValue.ortvalue_from_numpy(\n            logits_numpy\n        )\n    )\n\n    # print(inputs)\n    session.run_with_iobinding(io_binding)\n    # compile logists\n    print('=' * 20)\n    print(\"compare logits\")\n    pred_outputs = io_binding.copy_outputs_to_cpu()\n    compare_value(pred_outputs[-1], logits)\n\n    # compile present_key_values\n    for i in range(num_layers):\n        key_name = f\"present_key_values.{i}.key\"\n        value_name = f\"present_key_values.{i}.value\"\n        print('=' * 20)\n        print(f\"compare {key_name}\")\n        # key_numpy = [key_name]\n        key_true = getattr(output_dict, key_name).data.cpu().numpy()\n        key_pred = pred_outputs[i * 2]\n        compare_value(key_pred, key_true)\n        print('=' * 20)\n        print(f\"compare {value_name}\")\n        value_pred = pred_outputs[i * 2 + 1]\n        value_true = getattr(output_dict, value_name).data.cpu().numpy()\n        compare_value(value_pred, value_true)", "\n\nif __name__ == \"__main__\":\n    input_path1 = os.path.join(output_dir, \"pt_input1.pt\")\n    output_path1 = os.path.join(output_dir, \"pt_output1.pt\")\n    run_cuda_onnx_inference(onnx_path_with_cache, input_path1, output_path1)\n    print(\"\\n\")\n    input_path2 = os.path.join(output_dir, \"pt_input2.pt\")\n    output_path2 = os.path.join(output_dir, \"pt_output2.pt\")\n    run_cuda_onnx_inference(onnx_path_no_cache, input_path2, output_path2)", "\n\n"]}
{"filename": "onnx_export/export2onnx.py", "chunked_list": ["import os\n# from transformers import AutoTokenizer, AutoModel, AutoConfig\nimport torch\nimport sys\nimport argparse\nfrom transformers.generation.utils import LogitsProcessorList\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\nsys.path.append(project_dir)\nfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig", "sys.path.append(project_dir)\nfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\nfrom chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\nfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\nfrom onnx_export.utils import build_inputs\nfrom transformers.models.bloom import BloomOnnxConfig\n\n\nparser = argparse.ArgumentParser(description='export pytorch model to onnx')\nparser.add_argument(", "parser = argparse.ArgumentParser(description='export pytorch model to onnx')\nparser.add_argument(\n    '--data_type',\n    default=\"fp32\",\n    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n)\n\nargs = parser.parse_args()\nif args.data_type == \"fp16\":\n    device = 'cuda'\nelse:\n    device = 'cpu'", "if args.data_type == \"fp16\":\n    device = 'cuda'\nelse:\n    device = 'cpu'\n\n\noutput_dir = os.path.join(project_dir, \"output\")\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\nonnx_output_dir = os.path.join(output_dir, \"onnx_output\")\nif not os.path.exists(onnx_output_dir):\n    os.mkdir(onnx_output_dir)\nelse:\n    for file in os.listdir(onnx_output_dir):\n        os.remove(os.path.join(onnx_output_dir, file))", "onnx_output_dir = os.path.join(output_dir, \"onnx_output\")\nif not os.path.exists(onnx_output_dir):\n    os.mkdir(onnx_output_dir)\nelse:\n    for file in os.listdir(onnx_output_dir):\n        os.remove(os.path.join(onnx_output_dir, file))\nonnx_model_path = os.path.join(onnx_output_dir, \"chatglm2_6b.onnx\")\n\nquery = \"\u60f3\u8981\u51fa\u56fd\u7559\u5b66\uff0c\u5e94\u8be5\u600e\u4e48\u529e\uff1f\"\nhistory = [", "query = \"\u60f3\u8981\u51fa\u56fd\u7559\u5b66\uff0c\u5e94\u8be5\u600e\u4e48\u529e\uff1f\"\nhistory = [\n    (\n        \"\u4f60\u597d\",\n        \"\u4f60\u597d\ud83d\udc4b!\u6211\u662f\u4eba\u5de5\u667a\u80fd\u52a9\u624b ChatGLM2-6B,\u5f88\u9ad8\u5174\u89c1\u5230\u4f60,\u6b22\u8fce\u95ee\u6211\u4efb\u4f55\u95ee\u9898\u3002\",\n    )\n]\n\nmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\ntokenizer = ChatGLMTokenizer.from_pretrained(model_dir)", "model_dir = os.path.join(project_dir, \"chatglm2_6b\")\ntokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\nconfig = ChatGLMConfig.from_pretrained(model_dir)\n# config.num_layers = 1\nmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\nif device == \"cuda\":\n    model = model.half().cuda()\nelse:\n    model = model.float().cpu()\ndevice = torch.device(device)", "device = torch.device(device)\nmodel.eval()\n# input_tensors\ninput_tensors = build_inputs(device, tokenizer, query, history)\n\n# --debug for chat --\n# response, history = model.chat(tokenizer, query, history)\n# print(\"res\", response)\n\nprint(\" ---forward first --- \")", "\nprint(\" ---forward first --- \")\noutputs = model.forward(\n    **input_tensors\n)\n\nprint(\"--second forward ---\")\n# input_ids = input_tensors[\"input_ids\"]\nattention_mask = input_tensors[\"attention_mask\"]\nposition_ids = input_tensors[\"position_ids\"]", "attention_mask = input_tensors[\"attention_mask\"]\nposition_ids = input_tensors[\"position_ids\"]\npast_key_values = outputs[\"past_key_values\"]\n# copy from forward in second time\ninput_ids = torch.tensor([[30910]]).to(device)\n\n# copy from _update_model_kwargs_for_generation in modeling_chatglm.py\nattention_mask = torch.cat(\n    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n)", "    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n)\nnew_position_id = position_ids[..., -1:].clone()\nnew_position_id += 1\nposition_ids = torch.cat(\n    [position_ids, new_position_id], dim=-1\n)\n# copy from prepare_inputs_for_generation in modeling_chatglm.py\nposition_ids = position_ids[..., -1:]\n# print shape", "position_ids = position_ids[..., -1:]\n# print shape\nprint(\n    \"input_ids shape:\", input_ids.shape,\n    \"; type:\", input_ids.dtype\n)\nprint(\n    \"position_ids shape:\", position_ids.shape,\n    \"; type: \", input_ids.dtype\n)", "    \"; type: \", input_ids.dtype\n)\nprint(\n    \"attention_mask shape:\", attention_mask.shape,\n    \"; type: \", attention_mask.dtype\n)\nprint(\n    \"one past_key_value shape: \", past_key_values[0][0].shape,\n    \"; type:\", past_key_values[0][0].dtype\n)", "    \"; type:\", past_key_values[0][0].dtype\n)\nprint(\"logits shape: \", outputs[\"logits\"].shape)\noutputs2 = model.forward(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    position_ids=position_ids,\n    past_key_values=past_key_values\n)\nprint(\"--- export onnx ---\")", ")\nprint(\"--- export onnx ---\")\n# ---prepare for onnx export ---\ninput_names = [\"input_ids\", 'position_ids', \"attention_mask\"]\noutput_names = [\"logits\"]\ndynamic_axes = {\n    'input_ids': {0: \"batch_size\", 1: \"sequence\"},\n    'position_ids': {0: \"batch_size\", 1: \"sequence\"},\n    \"attention_mask\": {0: \"batch_size\", 1: \"past_sequence + sequence\"},\n    \"logits\": {0: \"batch_size\", 1: \"sequence\"}", "    \"attention_mask\": {0: \"batch_size\", 1: \"past_sequence + sequence\"},\n    \"logits\": {0: \"batch_size\", 1: \"sequence\"}\n}\nfor layer_idx in range(model.config.num_layers):\n    # --- input key and value ---\n    past_key_name = f\"past_key_values.{layer_idx}.key\"\n    past_value_name = f\"past_key_values.{layer_idx}.value\"\n    input_names += [past_key_name, past_value_name]\n    # --- output key and value ---\n    present_key_name = f\"present_key_values.{layer_idx}.key\"\n    present_value_name = f\"present_key_values.{layer_idx}.value\"\n    output_names += [present_key_name, present_value_name]\n    dynamic_axes.update({\n        past_key_name: {\n            0: \"past_sequence\",\n            1: \"batch_size\",\n        },\n        past_value_name: {\n            0: \"past_sequence\",\n            1: \"batch_size\",\n        },\n        present_key_name: {\n            0: \"past_sequence + 1\",\n            1: \"batch_size\"\n        },\n        present_value_name: {\n            0: \"past_sequence + 1\",\n            1: \"batch_size\"\n        }\n    })", "\n\nwith torch.no_grad():\n    torch.onnx.export(\n        model,\n        args=(\n            input_ids,\n            position_ids,\n            attention_mask, \n            past_key_values\n        ),\n        f=onnx_model_path,\n        opset_version=14,\n        input_names=input_names,\n        output_names=output_names,\n        dynamic_axes=dynamic_axes,\n        training=torch.onnx.TrainingMode.EVAL,\n    )"]}
{"filename": "kernel/ckernel.py", "chunked_list": ["import os\nimport torch\nimport pybind11\nfrom torch.utils import cpp_extension\nfrom torch.utils.cpp_extension import load\n\ninput_dirs = cpp_extension.include_paths()\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\ncustom_include_dir = os.path.join(project_dir, \"include\")", "project_dir = os.path.dirname(now_dir)\ncustom_include_dir = os.path.join(project_dir, \"include\")\ncuda_include_dir = \"/usr/local/cuda/include\"\ninput_dirs += [custom_include_dir, cuda_include_dir]\n\ntorch_dir = os.path.dirname(torch.__path__[0])\ntorch_lib_dir = os.path.join(torch_dir, \"lib\")\nlibrary_dirs = [\"/usr/local/cuda/lib64\", torch_lib_dir]\nlibraries = [\"nvinfer\"]\ntorch_lib_dir = os.path.join(torch.__path__[0], \"lib\")", "libraries = [\"nvinfer\"]\ntorch_lib_dir = os.path.join(torch.__path__[0], \"lib\")\nlibrary_dirs.append(torch_lib_dir)\nlibraries.extend([\"torch\", \"torch_cuda\", \"torch_cpu\", \"c10\", \"cudart\", \"c10_cuda\"])\nlibrary_dirs.append(pybind11.get_include(False))\nextra_link_args = [\n    \"-std=c++17\",\n    \"-L/usr/local/cuda/lib64\",\n    f\"-L{torch_lib_dir}\",\n  ]", "    f\"-L{torch_lib_dir}\",\n  ]\nextra_cuda_cflags = [\n    \"-std=c++17\",\n    \"-L/usr/local/cuda/lib64\",\n    f\"-L{torch_lib_dir}\",\n    \"-lnvinfer\"\n]\n\n", "\n\nsources=['kernel.cpp', \"bind.cpp\"]\nsources = [os.path.join(now_dir, s) for s in sources]\nckernel = load(\n  name='ckernel',\n  sources=sources,\n  extra_include_paths=input_dirs,\n  extra_cflags=extra_link_args,\n  extra_cuda_cflags=extra_cuda_cflags,", "  extra_cflags=extra_link_args,\n  extra_cuda_cflags=extra_cuda_cflags,\n  extra_ldflags=extra_cuda_cflags,\n  with_cuda=True,\n  verbose=False\n)\n# print(ckernel)\n# print(ckernel.Kernel)\n# print(ckernel.Kernel.forward)\n", "# print(ckernel.Kernel.forward)\n"]}
{"filename": "kernel/logits_processor.py", "chunked_list": ["import inspect\nimport torch\n\n\nclass LogitsProcessorList(list):\n    def __call__(self, input_ids, scores, **kwargs):\n        for processor in self:\n            function_args = inspect.signature(processor.__call__).parameters\n            if len(function_args) > 2:\n                if not all(arg in kwargs for arg in list(function_args.keys())[2:]):\n                    raise ValueError(\n                        f\"Make sure that all the required parameters: {list(function_args.keys())} for \"\n                        f\"{processor.__class__} are passed to the logits processor.\"\n                    )\n                scores = processor(input_ids, scores, **kwargs)\n            else:\n                scores = processor(input_ids, scores)\n        return scores", "\n\nclass LogitsWarper:\n    def __call__(self, input_ids, scores):\n        raise NotImplementedError(\n            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n        )\n\n\nclass TemperatureLogitsWarper(LogitsWarper):\n    def __init__(self, temperature: float):\n        if not isinstance(temperature, float) or not (temperature > 0):\n            raise ValueError(f\"`temperature` has to be a strictly positive float, but is {temperature}\")\n\n        self.temperature = temperature\n\n    def __call__(self, input_ids: torch.Tensor, scores: torch.Tensor) -> torch.FloatTensor:\n        scores = scores / self.temperature\n        return scores", "\nclass TemperatureLogitsWarper(LogitsWarper):\n    def __init__(self, temperature: float):\n        if not isinstance(temperature, float) or not (temperature > 0):\n            raise ValueError(f\"`temperature` has to be a strictly positive float, but is {temperature}\")\n\n        self.temperature = temperature\n\n    def __call__(self, input_ids: torch.Tensor, scores: torch.Tensor) -> torch.FloatTensor:\n        scores = scores / self.temperature\n        return scores", "\n\nclass TopPLogitsWarper(LogitsWarper):\n    def __init__(self, top_p: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n        top_p = float(top_p)\n        if top_p < 0 or top_p > 1.0:\n            raise ValueError(f\"`top_p` has to be a float > 0 and < 1, but is {top_p}\")\n\n        self.top_p = top_p\n        self.filter_value = filter_value\n        self.min_tokens_to_keep = min_tokens_to_keep\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        sorted_logits, sorted_indices = torch.sort(scores, descending=False)\n        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n\n        # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\n        sorted_indices_to_remove = cumulative_probs <= (1 - self.top_p)\n        if self.min_tokens_to_keep > 1:\n            # Keep at least min_tokens_to_keep\n            sorted_indices_to_remove[..., -self.min_tokens_to_keep :] = 0\n\n        # scatter sorted tensors to original indexing\n        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n        return scores", "\n\nclass TopKLogitsWarper(LogitsWarper):\n    def __init__(self, top_k: int, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n        if not isinstance(top_k, int) or top_k <= 0:\n            raise ValueError(f\"`top_k` has to be a strictly positive integer, but is {top_k}\")\n\n        self.top_k = max(top_k, min_tokens_to_keep)\n        self.filter_value = filter_value\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        top_k = min(self.top_k, scores.size(-1))  # Safety check\n        # Remove all tokens with a probability less than the last token of the top-k\n        indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]\n        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n        return scores", "\n\nclass LogitsProcessor:\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        \"\"\"Torch method for processing logits.\"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n        )\n\n\nclass InvalidScoreLogitsProcessor(LogitsProcessor):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        if torch.isnan(scores).any() or torch.isinf(scores).any():\n            scores.zero_()\n            scores[..., 5] = 5e4\n        return scores", "\n\nclass InvalidScoreLogitsProcessor(LogitsProcessor):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        if torch.isnan(scores).any() or torch.isinf(scores).any():\n            scores.zero_()\n            scores[..., 5] = 5e4\n        return scores\n", ""]}
{"filename": "kernel/setup.py", "chunked_list": ["import os\nimport torch\nimport pybind11\nfrom setuptools import setup, Extension\nfrom torch.utils import cpp_extension\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\ninput_dirs = cpp_extension.include_paths()\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)", "now_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\ncustom_include_dir = os.path.join(project_dir, \"include\")\ncuda_include_dir = \"/usr/local/cuda/include\"\ninput_dirs += [custom_include_dir, cuda_include_dir]\n\ntorch_dir = os.path.dirname(torch.__path__[0])\ntorch_lib_dir = os.path.join(torch_dir, \"lib\")\nlibrary_dirs = [\"/usr/local/cuda/lib64\", torch_lib_dir]\nlibraries = [\"nvinfer\"]", "library_dirs = [\"/usr/local/cuda/lib64\", torch_lib_dir]\nlibraries = [\"nvinfer\"]\ntorch_lib_dir = os.path.join(torch.__path__[0], \"lib\")\nlibrary_dirs.append(torch_lib_dir)\nlibraries.extend([\"torch\", \"torch_cuda\", \"torch_cpu\", \"c10\", \"cudart\", \"c10_cuda\"])\nlibrary_dirs.append(pybind11.get_include(False))\nextra_link_args = [\"-D_GLIBCXX_USE_CXX11_ABI=1\", \"-std=c++17\"]\n# extra_link_args = [\"-D_GLIBCXX_USE_CXX11_ABI=0\"]\n#   \"-L/usr/local/cuda/lib64\", \"-lnvinfer\", f\"-L{torch_lib_dir}\"]\n", "#   \"-L/usr/local/cuda/lib64\", \"-lnvinfer\", f\"-L{torch_lib_dir}\"]\n\nsetup(name='ckernel',\n      version=\"0.0.1\",\n      ext_modules=[\n        CUDAExtension(\n          name='ckernel',\n          sources=['kernel.cpp', \"bind.cpp\"],\n          include_dirs=input_dirs,\n          language=\"c++\",", "          include_dirs=input_dirs,\n          language=\"c++\",\n          extra_link_args=extra_link_args,\n          libraries=libraries,\n          library_dirs=library_dirs\n        )\n      ],\n      cmdclass={'kernel': BuildExtension}\n    )", "    )"]}
{"filename": "kernel/pykernel_no_past_old.py", "chunked_list": ["import os\nimport sys\nimport os.path\nimport torch\nimport tensorrt as trt\nfrom colored import stylize, fg\nfrom typing import List, Tuple\nimport pycuda.driver as cuda\nimport pycuda.autoinit\n", "import pycuda.autoinit\n\n\nclass MyLogger(trt.ILogger):\n    def __init__(self):\n        trt.ILogger.__init__(self)\n\n    def log(self, severity, msg):\n        if severity == trt.ILogger.ERROR:\n            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.WARNING:\n            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # \u9ec4\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INTERNAL_ERROR:\n            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INFO:\n            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # \u7eff\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.VERBOSE:\n            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # \u84dd\u8272\u5b57\u4f53\n        else:\n            print(\"[UNKNOWN] \" + msg)", "\n\nclass KernelNoPast:\n    def __init__(self, engine_path: str, batch_size: int = 1, num_layers: int = 28):\n        assert os.path.exists(engine_path), print(f\"{engine_path} not exists.\")\n        self.batch_size_ = batch_size\n        self.n_input_ = 2\n        self.n_output_ = num_layers * 2 + 1\n        self.n_total_ = self.n_input_ + self.n_output_\n        self.tensor_names_ = []\n        # self.logger_ = trt.Logger(trt.Logger.INFO)\n        self.logger_ = MyLogger()\n        self.runtime_ = trt.Runtime(self.logger_)\n        # load engine\n        with open(engine_path, \"rb\") as f:\n            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n        # verify io number\n        self.verify_io_number()\n        # init stream and context\n        self.stream_ = cuda.Stream()\n        self.context_ = self.engine_.create_execution_context()\n        print(stylize(\"init context and stream OK\", fg(\"green\")))\n        # self.context_.set_optimization_profile_async(0, self.stream_.handle)\n\n    #def __del__(self):\n    #    pass\n\n    def verify_io_number(self):\n        n_io = self.engine_.num_io_tensors\n        if n_io != self.n_total_:\n            raise RuntimeError(stylize(\n                \"Number of IO tensors is not correct, \" +\n                f\"must be {self.n_total_}, but you have {n_io} tensors\",\n                fg(\"red\")))\n        n_input = 0\n        n_output = 0\n        for i in range(n_io):\n            name = self.engine_.get_tensor_name(i)\n            self.tensor_names_.append(name)\n            if self.engine_.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                n_input += 1\n            else:\n                n_output += 1\n        if n_input != self.n_input_:\n            raise RuntimeError(stylize(\n                \"Number of input tensors is not correct, \" +\n                f\"must be {self.n_input_}, but you have {n_input} tensors\",\n                fg(\"red\")))\n        if n_output != self.n_output_:\n            raise RuntimeError(stylize(\n                \"Number of output tensors is not correct, \" +\n                f\"must be {self.n_output_}, but you have {n_output} tensors\",\n                fg(\"red\")))\n        n_profile = self.engine_.num_optimization_profiles\n        if n_profile != 1:\n            raise RuntimeError(stylize(\n                \"Number of optimization profiles is not correct, \" +\n                f\"must be 1, but you have {n_profile} profiles\",\n                fg(\"red\")))\n        print(stylize(f\"number of profile: {n_profile}\", fg(\"green\")))\n\n    def set_input_shape(self, seq_len: int):\n        self.context_.set_input_shape(\"input_ids\", (self.batch_size_, seq_len))\n        self.context_.set_input_shape(\"position_ids\", (self.batch_size_, seq_len))\n\n    def get_tensor_size(self):\n        shape_list = []\n        data_type_list = []\n        for i in range(self.n_total_):\n            tensor_name = self.tensor_names_[i]\n            shape = self.context_.get_tensor_shape(tensor_name)\n            shape_list.append(shape)\n            data_type = self.engine_.get_tensor_dtype(tensor_name)\n            data_type_list.append(data_type)\n        return shape_list, data_type_list\n\n    def forward(self, input_tensors: Tuple[torch.Tensor, torch.Tensor]):\n        assert len(input_tensors) == 2, \\\n            print(\"this number of input tensor must be 2\")\n        seq_len = input_tensors[0].size(1)\n        device = input_tensors[0].device\n        self.set_input_shape(seq_len)\n        shape_list, data_type_list = self.get_tensor_size()\n        # --- prepare for output --- #\n        output_tensors = []\n        for i in range(self.n_input_, self.n_total_):\n            if data_type_list[i] == trt.DataType.FLOAT:\n                torch_type = torch.float32\n            elif data_type_list[i] == trt.DataType.HALF:\n                torch_type = torch.float16\n            elif data_type_list[i] == trt.DataType.INT32:\n                torch_type = torch.int32\n            elif data_type_list[i] == trt.DataType.INT8:\n                torch_type = torch.int8\n            else:\n                raise Exception(f\"not support type {data_type_list[i]}\")\n            tensor = torch.empty(\n                size=tuple(shape_list[i]), dtype=torch_type, device=device\n            )\n            output_tensors.append(tensor)\n        # === run inference with V3 ===\n        for i in range(self.n_input_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                input_tensors[i].data_ptr()\n            )\n        for i in range(self.n_input_, self.n_total_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                output_tensors[i - self.n_input_].data_ptr()\n            )\n        self.context_.execute_async_v3(stream_handle=self.stream_.handle)\n        # cuda.synchronize()\n        # cuda.streams.synchronize(self.stream_)\n        return output_tensors", "\n\nif __name__ == \"__main__\":\n    now_dir = os.path.dirname(os.path.abspath(__file__))\n    project_dir = os.path.dirname(now_dir)\n    model_dir = os.path.join(project_dir, \"models\")\n    engine_path1 = os.path.join(model_dir, \"chatglm6b2-bs1_no_cache.plan\")\n    kernel = KernelNoPast(engine_path1)\n    input_ids = torch.ones([1, 4], dtype=torch.int64).cuda()\n    position_ids = torch.ones([1, 4], dtype=torch.int64).cuda()\n    input_tensors1 = (input_ids, position_ids)\n    output_tensors1 = kernel.forward(input_tensors1)\n    print(\"first shape\", output_tensors1[0].shape)\n    print(\"last shape\", output_tensors1[-1].shape)", "\n\n\n\n\n"]}
{"filename": "kernel/pykernel_no_past_new.py", "chunked_list": ["import os\nimport sys\nimport os.path\nimport torch\nimport tensorrt as trt\nfrom colored import stylize, fg\nfrom typing import List, Tuple\nfrom cuda import cudart\n\n\nclass MyLogger(trt.ILogger):\n    def __init__(self):\n        trt.ILogger.__init__(self)\n\n    def log(self, severity, msg):\n        if severity == trt.ILogger.ERROR:\n            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.WARNING:\n            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # \u9ec4\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INTERNAL_ERROR:\n            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INFO:\n            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # \u7eff\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.VERBOSE:\n            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # \u84dd\u8272\u5b57\u4f53\n        else:\n            print(\"[UNKNOWN] \" + msg)", "\n\nclass MyLogger(trt.ILogger):\n    def __init__(self):\n        trt.ILogger.__init__(self)\n\n    def log(self, severity, msg):\n        if severity == trt.ILogger.ERROR:\n            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.WARNING:\n            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # \u9ec4\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INTERNAL_ERROR:\n            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INFO:\n            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # \u7eff\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.VERBOSE:\n            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # \u84dd\u8272\u5b57\u4f53\n        else:\n            print(\"[UNKNOWN] \" + msg)", "\n\ndef gpu_check(error: cudart.cudaError_t):\n    if error != cudart.cudaError_t.cudaSuccess:\n        error_name = cudart.cudaGetErrorName(error)\n        error_info = cudart.cudaGetErrorString(error)\n        print(stylize(f\"ERROR [{error_name}]: {error_info}\", fg(\"red\")))\n        raise Exception(f\"ERROR [{error_name}]: {error_info}\")\n\n\nclass KernelNoPast:\n    def __init__(self, engine_path: str, batch_size: int = 1, num_layers: int = 28):\n        assert os.path.exists(engine_path), print(f\"{engine_path} not exists.\")\n        self.batch_size_ = batch_size\n        self.n_input_ = 2 + num_layers * 2\n        self.n_output_ = num_layers * 2 + 1\n        self.n_total_ = self.n_input_ + self.n_output_\n        self.tensor_names_ = []\n        # self.logger_ = trt.Logger(trt.Logger.INFO)\n        self.logger_ = MyLogger()\n        self.runtime_ = trt.Runtime(self.logger_)\n        # load engine\n        with open(engine_path, \"rb\") as f:\n            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n        # verify io number\n        self.verify_io_number()\n        # init stream and context\n        error_log, self.stream_ = cudart.cudaStreamCreate()\n        gpu_check(error_log)\n        self.context_ = self.engine_.create_execution_context()\n        print(stylize(\"init context and stream OK\", fg(\"green\")))\n        self.context_.set_optimization_profile_async(0, self.stream_)\n\n    def __del__(self):\n        cudart.cudaStreamDestroy(self.stream_)\n\n    def verify_io_number(self):\n        n_io = self.engine_.num_io_tensors\n        if n_io != self.n_total_:\n            raise RuntimeError(stylize(\n                \"Number of IO tensors is not correct, \" +\n                f\"must be {self.n_total_}, but you have {n_io} tensors\",\n                fg(\"red\")))\n        n_input = 0\n        n_output = 0\n        for i in range(n_io):\n            name = self.engine_.get_tensor_name(i)\n            self.tensor_names_.append(name)\n            if self.engine_.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                n_input += 1\n            else:\n                n_output += 1\n        if n_input != self.n_input_:\n            raise RuntimeError(stylize(\n                \"Number of input tensors is not correct, \" +\n                f\"must be {self.n_input_}, but you have {n_input} tensors\",\n                fg(\"red\")))\n        if n_output != self.n_output_:\n            raise RuntimeError(stylize(\n                \"Number of output tensors is not correct, \" +\n                f\"must be {self.n_output_}, but you have {n_output} tensors\",\n                fg(\"red\")))\n        n_profile = self.engine_.num_optimization_profiles\n        if n_profile != 1:\n            raise RuntimeError(stylize(\n                \"Number of optimization profiles is not correct, \" +\n                f\"must be 1, but you have {n_profile} profiles\",\n                fg(\"red\")))\n        print(stylize(f\"number of profile: {n_profile}\", fg(\"green\")))\n\n    def set_input_shape(self, seq_len: int):\n        self.context_.set_input_shape(\"input_ids\", (self.batch_size_, seq_len))\n        self.context_.set_input_shape(\"position_ids\", (self.batch_size_, seq_len))\n        for i in range(2, self.n_input_):\n            self.context_.set_input_shape(\n                self.tensor_names_[i],\n                (0, self.batch_size_, 2, 128)\n            )\n\n\n    def get_tensor_size(self):\n        shape_list = []\n        data_type_list = []\n        for i in range(self.n_total_):\n            tensor_name = self.tensor_names_[i]\n            shape = self.context_.get_tensor_shape(tensor_name)\n            shape_list.append(shape)\n            data_type = self.engine_.get_tensor_dtype(tensor_name)\n            data_type_list.append(data_type)\n        return shape_list, data_type_list\n\n    @staticmethod \n    def get_data_type(raw_data_type: trt.DataType):\n        if raw_data_type == trt.DataType.FLOAT:\n            torch_type = torch.float32\n        elif raw_data_type == trt.DataType.HALF:\n            torch_type = torch.float16\n        elif raw_data_type == trt.DataType.INT32:\n            torch_type = torch.int32\n        elif raw_data_type == trt.DataType.INT8:\n            torch_type = torch.int8\n        else:\n            raise Exception(f\"not support type {raw_data_type}\")\n        return torch_type\n\n    def forward(self, input_tensors: Tuple[torch.Tensor, torch.Tensor]):\n        seq_len = input_tensors[0].size(1)\n        device = input_tensors[0].device\n        self.set_input_shape(seq_len)\n        shape_list, data_type_list = self.get_tensor_size()\n        # --- prepare for output --- #\n        output_tensors = []\n        for i in range(self.n_input_, self.n_total_):\n            torch_type = self.get_data_type(data_type_list[i]) \n            tensor = torch.zeros(\n                size=tuple(shape_list[i]), dtype=torch_type, device=device\n            )\n            output_tensors.append(tensor)\n        # === run inference with V3 ===\n        for i in range(2):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                input_tensors[i].data_ptr()\n            )\n        torch_type = self.get_data_type(data_type_list[2])\n        sample_tensor = torch.zeros([1], dtype=torch_type, device=device)\n        for i in range(2, self.n_input_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                sample_tensor.data_ptr()\n            )\n        for i in range(self.n_input_, self.n_total_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                output_tensors[i - self.n_input_].data_ptr()\n            )\n        self.context_.execute_async_v3(stream_handle=self.stream_)\n        (error_info,) = cudart.cudaDeviceSynchronize()\n        gpu_check(error_info)\n        (error_info,) = cudart.cudaStreamSynchronize(self.stream_)\n        gpu_check(error_info)\n        return output_tensors", "\n\nclass KernelNoPast:\n    def __init__(self, engine_path: str, batch_size: int = 1, num_layers: int = 28):\n        assert os.path.exists(engine_path), print(f\"{engine_path} not exists.\")\n        self.batch_size_ = batch_size\n        self.n_input_ = 2 + num_layers * 2\n        self.n_output_ = num_layers * 2 + 1\n        self.n_total_ = self.n_input_ + self.n_output_\n        self.tensor_names_ = []\n        # self.logger_ = trt.Logger(trt.Logger.INFO)\n        self.logger_ = MyLogger()\n        self.runtime_ = trt.Runtime(self.logger_)\n        # load engine\n        with open(engine_path, \"rb\") as f:\n            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n        # verify io number\n        self.verify_io_number()\n        # init stream and context\n        error_log, self.stream_ = cudart.cudaStreamCreate()\n        gpu_check(error_log)\n        self.context_ = self.engine_.create_execution_context()\n        print(stylize(\"init context and stream OK\", fg(\"green\")))\n        self.context_.set_optimization_profile_async(0, self.stream_)\n\n    def __del__(self):\n        cudart.cudaStreamDestroy(self.stream_)\n\n    def verify_io_number(self):\n        n_io = self.engine_.num_io_tensors\n        if n_io != self.n_total_:\n            raise RuntimeError(stylize(\n                \"Number of IO tensors is not correct, \" +\n                f\"must be {self.n_total_}, but you have {n_io} tensors\",\n                fg(\"red\")))\n        n_input = 0\n        n_output = 0\n        for i in range(n_io):\n            name = self.engine_.get_tensor_name(i)\n            self.tensor_names_.append(name)\n            if self.engine_.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                n_input += 1\n            else:\n                n_output += 1\n        if n_input != self.n_input_:\n            raise RuntimeError(stylize(\n                \"Number of input tensors is not correct, \" +\n                f\"must be {self.n_input_}, but you have {n_input} tensors\",\n                fg(\"red\")))\n        if n_output != self.n_output_:\n            raise RuntimeError(stylize(\n                \"Number of output tensors is not correct, \" +\n                f\"must be {self.n_output_}, but you have {n_output} tensors\",\n                fg(\"red\")))\n        n_profile = self.engine_.num_optimization_profiles\n        if n_profile != 1:\n            raise RuntimeError(stylize(\n                \"Number of optimization profiles is not correct, \" +\n                f\"must be 1, but you have {n_profile} profiles\",\n                fg(\"red\")))\n        print(stylize(f\"number of profile: {n_profile}\", fg(\"green\")))\n\n    def set_input_shape(self, seq_len: int):\n        self.context_.set_input_shape(\"input_ids\", (self.batch_size_, seq_len))\n        self.context_.set_input_shape(\"position_ids\", (self.batch_size_, seq_len))\n        for i in range(2, self.n_input_):\n            self.context_.set_input_shape(\n                self.tensor_names_[i],\n                (0, self.batch_size_, 2, 128)\n            )\n\n\n    def get_tensor_size(self):\n        shape_list = []\n        data_type_list = []\n        for i in range(self.n_total_):\n            tensor_name = self.tensor_names_[i]\n            shape = self.context_.get_tensor_shape(tensor_name)\n            shape_list.append(shape)\n            data_type = self.engine_.get_tensor_dtype(tensor_name)\n            data_type_list.append(data_type)\n        return shape_list, data_type_list\n\n    @staticmethod \n    def get_data_type(raw_data_type: trt.DataType):\n        if raw_data_type == trt.DataType.FLOAT:\n            torch_type = torch.float32\n        elif raw_data_type == trt.DataType.HALF:\n            torch_type = torch.float16\n        elif raw_data_type == trt.DataType.INT32:\n            torch_type = torch.int32\n        elif raw_data_type == trt.DataType.INT8:\n            torch_type = torch.int8\n        else:\n            raise Exception(f\"not support type {raw_data_type}\")\n        return torch_type\n\n    def forward(self, input_tensors: Tuple[torch.Tensor, torch.Tensor]):\n        seq_len = input_tensors[0].size(1)\n        device = input_tensors[0].device\n        self.set_input_shape(seq_len)\n        shape_list, data_type_list = self.get_tensor_size()\n        # --- prepare for output --- #\n        output_tensors = []\n        for i in range(self.n_input_, self.n_total_):\n            torch_type = self.get_data_type(data_type_list[i]) \n            tensor = torch.zeros(\n                size=tuple(shape_list[i]), dtype=torch_type, device=device\n            )\n            output_tensors.append(tensor)\n        # === run inference with V3 ===\n        for i in range(2):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                input_tensors[i].data_ptr()\n            )\n        torch_type = self.get_data_type(data_type_list[2])\n        sample_tensor = torch.zeros([1], dtype=torch_type, device=device)\n        for i in range(2, self.n_input_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                sample_tensor.data_ptr()\n            )\n        for i in range(self.n_input_, self.n_total_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                output_tensors[i - self.n_input_].data_ptr()\n            )\n        self.context_.execute_async_v3(stream_handle=self.stream_)\n        (error_info,) = cudart.cudaDeviceSynchronize()\n        gpu_check(error_info)\n        (error_info,) = cudart.cudaStreamSynchronize(self.stream_)\n        gpu_check(error_info)\n        return output_tensors", "\n\nif __name__ == \"__main__\":\n    now_dir = os.path.dirname(os.path.abspath(__file__))\n    project_dir = os.path.dirname(now_dir)\n    model_dir = os.path.join(project_dir, \"models\")\n    engine_path1 = os.path.join(model_dir, \"chatglm6b2-bs1_with_cache.plan\")\n    kernel = KernelNoPast(engine_path1)\n    input_ids = torch.ones([1, 4], dtype=torch.int64).cuda()\n    position_ids = torch.ones([1, 4], dtype=torch.int64).cuda()\n    input_tensors1 = (input_ids, position_ids)\n    output_tensors1 = kernel.forward(input_tensors1)\n    print(\"first shape\", output_tensors1[0].shape)\n    print(\"last shape\", output_tensors1[-1].shape)", "\n"]}
{"filename": "kernel/__init__.py", "chunked_list": ["# from .ckernel import Kernel\n# from .logits_processor import *\n# from .ckernel import ckernel"]}
{"filename": "kernel/pykernel.py", "chunked_list": ["import tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nimport torch\nimport torch.nn as nn\nfrom typing import Tuple, List, Optional\nimport re\nfrom .logits_processor import *\n\n", "\n\n\nclass Kernel(nn.Module):\n    def __init__(self, engine_path: str, batch_size: int):\n        self.logger_ = trt.Logger(trt.Logger.INFO)\n        self.runtime_ = trt.Runtime(self.logger_)\n        self.engine_ = None\n        self.context_1_ = None\n        self.context_2_ = None\n        self.stream_1_ = cuda.Stream()\n        self.stream_2_ = cuda.Stream()\n        self.batch_size_ = batch_size\n        self.n_total_ = 116\n        self.n_input_ = 59\n        self.n_output_ = 57\n        self.tensor_names_ = []\n        self.num_layers_ = 28\n        self.logits_processor = None\n        self.logits_warper = None\n        self.load_engine(engine_path)\n\n    def __del__(self):\n        pass\n\n    def load_engine(self, engine_path: str):\n        # Load TensorRT Engine\n        with open(engine_path, 'rb') as f:\n            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n        self.context_1_ = self.engine.create_execution_context()\n        self.context_1_.set_optimization_profile_async(0, self.stream_1_.handle)\n        self.context_2_ = self.engine.create_execution_context()\n        self.context_2_.set_optimization_profile_async(1, self.stream_2_.handle)\n\n    def vertify_io_number(self):\n        pass\n\n    def init_execute_context(self):\n        pass\n\n    def forward(self,\n                input_ids: Optional[torch.Tensor] = None,\n                position_ids: Optional[torch.Tensor] = None,\n                attention_mask: Optional[torch.Tensor] = None,\n                past_key_values: Optional[Tuple[torch.FloatTensor]] = None):\n        if past_key_values is None:\n            outputs = self.inference_step_1(input_ids, attention_mask, position_ids)\n            return outputs\n        else:\n            outputs = self.inference_step_x(input_ids, input_ids, attention_mask, position_ids, past_key_values)\n            return outputs\n\n    def set_input_for_context1(self, seq_length):\n        pass\n\n    def set_input_for_context2(self, past_seq_length):\n        pass\n\n    def run_gpu_inference(\n            self,\n            input_ids,\n            position_ids,\n            attention_mask,\n            past_key_values,\n            bytes_list,\n            type_bytes_list,\n            context,\n            stream\n    ):\n        pass\n\n    def chat(\n            self,\n            tokenizer,\n            query: str,\n            history: List[Tuple[str, str]] = None,\n            max_length: int = 2048,\n            max_new_tokens: int = 40,\n            num_beams=1,\n            do_sample=True,\n            top_p=0.7,\n            top_k=50,\n            temperature=1.0,\n            **kwargs\n    ):\n        # \u521d\u59cb\u5316 history\n        if history is None:\n            history = []\n        # \u521d\u59cb\u5316\u540e\u5904\u7406\n        self.logits_processor = LogitsProcessorList()\n        self.logits_processor.append(InvalidScoreLogitsProcessor())\n        self.logits_warper = LogitsProcessorList()\n        self.logits_warper.append(TemperatureLogitsWarper(temperature))\n        self.logits_warper.append(TopPLogitsWarper(top_p))\n        self.logits_warper.append(TopKLogitsWarper(top_k))\n        # \u7ec4\u88c5prompt\n        if not history:\n            prompt = query\n        else:\n            prompt = \"\"\n            for i, (old_query, response) in enumerate(history):\n                prompt += \"[Round {}]\\n\u95ee\uff1a{}\\n\u7b54\uff1a{}\\n\".format(i, old_query, response)\n            prompt += \"[Round {}]\\n\u95ee\uff1a{}\\n\u7b54\uff1a\".format(len(history), query)\n        # \u7b2c\u4e00\u6b21\u63a8\u7406\n        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda().to(torch.int32)\n        ori_len = len(input_ids[0])\n        attention_mask, position_ids = self.pre_processing_step_1(tokenizer, input_ids)\n        outputs_1 = self.inference_step_1(input_ids, attention_mask, position_ids)\n        ori_input_ids, input_ids, attention_mask, position_ids, past_key_values = self.post_processing_step_1(\n            outputs_1, input_ids, attention_mask, position_ids)\n        # \u91cd\u590d\u63a8\u7406\u76f4\u5230\u6761\u4ef6\u7ec8\u6b62\n        while len(ori_input_ids[0]) < max_length \\\n                and len(ori_input_ids[0]) - ori_len < max_new_tokens \\\n                and tokenizer.eos_token_id not in ori_input_ids[0]:\n            outputs_x = self.inference_step_x(ori_input_ids, input_ids, attention_mask, position_ids, past_key_values)\n            ori_input_ids, input_ids, attention_mask, position_ids, past_key_values = self.post_processing_step_x(\n                outputs_x, ori_input_ids, input_ids, attention_mask, position_ids)\n            # print(tokenizer.decode(ori_input_ids[0]))\n        # \u5904\u7406\u56de\u7b54\n        response = tokenizer.decode(ori_input_ids[0][ori_len:])\n        response = self.process_response(response)\n        history = history + [(query, response)]\n        return response, history\n\n    def pre_processing_step_1(self, tokenizer, input_ids: torch.Tensor):\n        BOS = tokenizer.bos_token_id\n        MASK = tokenizer.mask_token_id\n        gMASK = tokenizer.gmask_token_id\n        batch_size, seq_length = input_ids.shape\n        # \u8f93\u5165\u5f20\u91cf\u6269\u5c55\u5e38\u91cf\n        input_range = torch.arange(seq_length, dtype=torch.int32).repeat((batch_size, 1)).to(input_ids.device)\n        input_upper = torch.tril(torch.ones((batch_size, seq_length, seq_length), dtype=torch.int32)).to(\n            input_ids.device)\n        # \u83b7\u53d6 attention_mask\n        context_lengths = torch.argmax((input_ids == BOS).to(torch.int32), dim=1)\n        context_mask = (input_range + 1) <= context_lengths.unsqueeze(1)\n        padding_mask = context_mask.unsqueeze(1)\n        attention_mask = torch.logical_not(torch.logical_or(input_upper, padding_mask)).unsqueeze(1)\n        # \u5224\u65adMASK\u4f4d\u7f6e\n        is_gmasks = (input_ids == gMASK).to(torch.int32)\n        is_masks = (input_ids == MASK).to(torch.int32)\n        use_gmasks = torch.sum(is_gmasks, dim=1) > 0\n        # \u83b7\u53d6 position_ids\n        mask_positions = torch.where(use_gmasks, torch.argmax(is_gmasks, dim=1), torch.argmax(is_masks, dim=1)).to(\n            torch.int32).unsqueeze(1)\n        position_ids_pre = torch.where(context_mask, input_range, mask_positions)\n        block_position_ids = torch.clamp(input_range - context_lengths.unsqueeze(1) + 1, min=0)\n        position_ids = torch.stack((position_ids_pre, block_position_ids), dim=1).to(torch.int32)\n        return attention_mask, position_ids\n\n    def inference_step_1(self,\n                         input_ids: torch.Tensor,\n                         attention_mask: torch.Tensor,\n                         position_ids: torch.Tensor):\n        seq_len = input_ids.size(1)\n        bindings = []\n        outputs_1 = {}\n        for binding in self.engine_:\n            if binding.endswith(\"[profile 1]\"):\n                continue\n            if self.engine_.binding_is_input(binding):\n                if binding == 'input_ids':\n                    tensor = input_ids\n                elif binding == 'attention_mask':\n                    tensor = attention_mask\n                elif binding == 'position_ids':\n                    tensor = position_ids\n                elif binding.startswith(\"past\"):\n                    tensor = torch.empty(size=(0, 1, 32, 128), dtype=torch.float16, device=torch.device('cuda'))\n                else:\n                    assert 0\n                self.context_1_.set_input_shape(binding, tuple(tensor.shape))\n            else:\n                if binding == \"logits\":\n                    tensor = torch.empty(size=(1, seq_len, 130528), dtype=torch.float16, device=torch.device('cuda'))\n                elif binding.startswith(\"present\"):\n                    tensor = torch.empty(size=(seq_len, 1, 32, 128), dtype=torch.float16, device=torch.device('cuda'))\n                else:\n                    assert 0\n                outputs_1[binding] = tensor\n            bindings.append(tensor.data_ptr())\n        assert self.context_1_.execute_async_v2(bindings, torch.cuda.current_stream().cuda_stream)\n        return outputs_1\n\n    def post_processing_step_1(self,\n                               outputs_1,\n                               input_ids: torch.Tensor,\n                               attention_mask: torch.Tensor,\n                               position_ids: torch.Tensor):\n        logits = outputs_1['logits']\n        next_token_logits = logits[:, -1, :]\n        # \u4e00\u4e9b\u540e\u5904\u7406\u903b\u8f91\n        next_token_scores = self.logits_processor(input_ids, next_token_logits)\n        next_token_scores = self.logits_warper(input_ids, next_token_scores)\n        # \u91c7\u6837\u4e0b\u4e00\u4e2atoken\n        probs = torch.nn.functional.softmax(next_token_scores, dim=-1)\n        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n        ori_input_ids = torch.cat((input_ids, next_tokens[:, None]), dim=-1)\n        # \u8f93\u51fa\u4e0b\u4e00\u8f6e\u7684 input_ids, position_ids, attention_mask\n        attention_mask = attention_mask[..., -1:, -1:]\n        position_ids = torch.cat(\n            (position_ids[:, :-1, -1:], position_ids[:, -1:, -1:] + torch.tensor(1, dtype=position_ids.dtype)), dim=1)\n        input_ids = ori_input_ids[:, -1:].to(torch.int32)\n        past_key_values = ()\n        for i in range(self.num_layers_):\n            past_key = outputs_1[f'present_key_values.{i}.decorder.key']\n            past_value = outputs_1[f'present_key_values.{i}.decorder.value']\n            past_key_values += ((past_key, past_value),)\n        return ori_input_ids, input_ids, attention_mask, position_ids, past_key_values\n\n    def inference_step_x(self,\n                         ori_input_ids: torch.Tensor,\n                         input_ids: torch.Tensor,\n                         attention_mask: torch.Tensor,\n                         position_ids: torch.Tensor,\n                         past_key_values: Tuple[torch.Tensor]):\n        seq_len = ori_input_ids.size(1)\n        bindings = []\n        outputs = {}\n        for binding in self.engine_:\n            if not binding.endswith(\"[profile 1]\"):\n                bindings.append(0)\n                continue\n            if self.engine_.binding_is_input(binding):\n                if binding == 'input_ids [profile 1]':\n                    tensor = input_ids\n                elif binding == 'attention_mask [profile 1]':\n                    tensor = attention_mask\n                elif binding == 'position_ids [profile 1]':\n                    tensor = position_ids\n                elif binding.startswith(\"past\"):\n                    layer_id = int(binding.split('.')[1])\n                    tuple_id = 0 if '.key' in binding else 1\n                    tensor = past_key_values[layer_id][tuple_id]\n                else:\n                    assert 0\n                self.context_2_.set_input_shape(binding[:-12], tuple(tensor.shape))\n            else:\n                if binding == \"logits [profile 1]\":\n                    tensor = torch.empty(size=(1, 1, 130528), dtype=torch.float16, device=input_ids.device)\n                elif binding.startswith(\"present\"):\n                    tensor = torch.empty(size=(seq_len, 1, 32, 128), dtype=torch.float16, device=input_ids.device)\n                else:\n                    assert 0\n                outputs[binding] = tensor\n            bindings.append(tensor.data_ptr())\n        assert self.context_2_.execute_async_v2(bindings, torch.cuda.current_stream().cuda_stream)\n        return outputs\n\n    def post_processing_step_x(self,\n                               outputs_x,\n                               ori_input_ids: torch.Tensor,\n                               input_ids: torch.Tensor,\n                               attention_mask: torch.Tensor,\n                               position_ids: torch.Tensor):\n        logits = outputs_x['logits [profile 1]']\n        next_token_logits = logits[:, -1, :]\n        # \u4e00\u4e9b\u540e\u5904\u7406\u903b\u8f91\n        next_token_scores = self.logits_processor(input_ids, next_token_logits)\n        next_token_scores = self.logits_warper(input_ids, next_token_scores)\n        # \u91c7\u6837\u4e0b\u4e00\u4e2atoken\n        probs = torch.nn.functional.softmax(next_token_scores, dim=-1)\n        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n        ori_input_ids = torch.cat((ori_input_ids, next_tokens[:, None]), dim=-1)\n        # \u8f93\u51fa\u4e0b\u4e00\u8f6e\u7684 input_ids, position_ids, attention_mask\n        attention_mask = attention_mask[..., -1:, -1:]\n        position_ids = torch.cat(\n            (position_ids[:, :-1, -1:], position_ids[:, -1:, -1:] + torch.tensor(1, dtype=position_ids.dtype)), dim=1)\n        input_ids = ori_input_ids[:, -1:].to(torch.int32)\n        past_key_values = ()\n        for i in range(self.num_layers_):\n            past_key = outputs_x[f'present_key_values.{i}.decorder.key [profile 1]']\n            past_value = outputs_x[f'present_key_values.{i}.decorder.value [profile 1]']\n            past_key_values += ((past_key, past_value),)\n        return ori_input_ids, input_ids, attention_mask, position_ids, past_key_values\n\n    def process_response(self, response):\n        response = response.strip()\n        # response = response.replace(\"[[\u8bad\u7ec3\u65f6\u95f4]]\", \"2023\u5e74\")\n        punkts = [\n            [\",\", \"\uff0c\"],\n            [\"!\", \"\uff01\"],\n            [\":\", \"\uff1a\"],\n            [\";\", \"\uff1b\"],\n            [\"\\?\", \"\uff1f\"],\n        ]\n        for item in punkts:\n            response = re.sub(r\"([\\u4e00-\\u9fff])%s\" % item[0], r\"\\1%s\" % item[1], response)\n            response = re.sub(r\"%s([\\u4e00-\\u9fff])\" % item[0], r\"%s\\1\" % item[1], response)\n        return response", "\n\nif __name__ == \"__main__\":\n    kernel = Kernel(\"../models/chatglm6b-bs1-12.5G.plan\", 1)"]}
{"filename": "kernel/pykernel_with_past.py", "chunked_list": ["import os\nimport sys\nimport os.path\nimport torch\nimport tensorrt as trt\nfrom colored import stylize, fg\nfrom typing import List, Tuple\nfrom cuda import cudart\n\n\nclass MyLogger(trt.ILogger):\n    def __init__(self):\n        trt.ILogger.__init__(self)\n\n    def log(self, severity, msg):\n        if severity == trt.ILogger.ERROR:\n            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.WARNING:\n            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # \u9ec4\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INTERNAL_ERROR:\n            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INFO:\n            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # \u7eff\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.VERBOSE:\n            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # \u84dd\u8272\u5b57\u4f53\n        else:\n            print(\"[UNKNOWN] \" + msg)", "\n\nclass MyLogger(trt.ILogger):\n    def __init__(self):\n        trt.ILogger.__init__(self)\n\n    def log(self, severity, msg):\n        if severity == trt.ILogger.ERROR:\n            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.WARNING:\n            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # \u9ec4\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INTERNAL_ERROR:\n            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INFO:\n            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # \u7eff\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.VERBOSE:\n            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # \u84dd\u8272\u5b57\u4f53\n        else:\n            print(\"[UNKNOWN] \" + msg)", "\n\ndef gpu_check(error: cudart.cudaError_t):\n    if error != cudart.cudaError_t.cudaSuccess:\n        error_name = cudart.cudaGetErrorName(error)\n        error_info = cudart.cudaGetErrorString(error)\n        print(stylize(f\"ERROR [{error_name}]: {error_info}\", fg(\"red\")))\n        raise Exception(f\"ERROR [{error_name}]: {error_info}\")\n\n\nclass KernelWithPast:\n    def __init__(self, engine_path: str, batch_size: int = 1, num_layers: int = 28):\n        assert os.path.exists(engine_path), print(f\"{engine_path} not exists.\")\n        self.batch_size_ = batch_size\n        self.n_input_ = 2 + num_layers * 2\n        self.n_output_ = num_layers * 2 + 1\n        self.num_layers = num_layers\n        self.n_total_ = self.n_input_ + self.n_output_\n        self.tensor_names_ = []\n        # self.logger_ = trt.Logger(trt.Logger.INFO)\n        self.logger_ = MyLogger()\n        self.runtime_ = trt.Runtime(self.logger_)\n        # load engine\n        with open(engine_path, \"rb\") as f:\n            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n        # verify io number\n        self.verify_io_number()\n        # init stream and context\n        error_log, self.stream_ = cudart.cudaStreamCreate()\n        gpu_check(error_log)\n        self.context_ = self.engine_.create_execution_context()\n        print(stylize(\"init context and stream OK\", fg(\"green\")))\n        self.context_.set_optimization_profile_async(0, self.stream_)\n\n    def __del__(self):\n        cudart.cudaStreamDestroy(self.stream_)\n\n    def verify_io_number(self):\n        n_io = self.engine_.num_io_tensors\n        if n_io != self.n_total_:\n            raise RuntimeError(stylize(\n                \"Number of IO tensors is not correct, \" +\n                f\"must be {self.n_total_}, but you have {n_io} tensors\",\n                fg(\"red\")))\n        n_input = 0\n        n_output = 0\n        for i in range(n_io):\n            name = self.engine_.get_tensor_name(i)\n            self.tensor_names_.append(name)\n            if self.engine_.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                n_input += 1\n            else:\n                n_output += 1\n        if n_input != self.n_input_:\n            raise RuntimeError(stylize(\n                \"Number of input tensors is not correct, \" +\n                f\"must be {self.n_input_}, but you have {n_input} tensors\",\n                fg(\"red\")))\n        if n_output != self.n_output_:\n            raise RuntimeError(stylize(\n                \"Number of output tensors is not correct, \" +\n                f\"must be {self.n_output_}, but you have {n_output} tensors\",\n                fg(\"red\")))\n        n_profile = self.engine_.num_optimization_profiles\n        if n_profile != 1:\n            raise RuntimeError(stylize(\n                \"Number of optimization profiles is not correct, \" +\n                f\"must be 1, but you have {n_profile} profiles\",\n                fg(\"red\")))\n        print(stylize(f\"number of profile: {n_profile}\", fg(\"green\")))\n\n    def set_input_shape(self, seq_len: int):\n        self.context_.set_input_shape(\"input_ids\", (self.batch_size_, 1))\n        self.context_.set_input_shape(\"position_ids\", (self.batch_size_, 1))\n        for i in range(2, self.n_input_):\n            self.context_.set_input_shape(\n                self.tensor_names_[i],\n                (seq_len, self.batch_size_, 2, 128)\n            )\n\n    def get_tensor_size(self):\n        shape_list = []\n        data_type_list = []\n        for i in range(self.n_total_):\n            tensor_name = self.tensor_names_[i]\n            shape = self.context_.get_tensor_shape(tensor_name)\n            shape_list.append(shape)\n            data_type = self.engine_.get_tensor_dtype(tensor_name)\n            data_type_list.append(data_type)\n        return shape_list, data_type_list\n\n    def forward(self, input_tensors: Tuple[torch.Tensor, ...]):\n        assert len(input_tensors) == self.n_input_, \\\n            print(f\"this number of input tensor must be {self.n_input_}\")\n        device = input_tensors[0].device\n        past_seq_len = input_tensors[2].shape[0]\n        self.set_input_shape(past_seq_len)\n        shape_list, data_type_list = self.get_tensor_size()\n        # --- prepare for output --- #\n        output_tensors = []\n        for i in range(self.n_input_, self.n_total_):\n            if data_type_list[i] == trt.DataType.FLOAT:\n                torch_type = torch.float32\n            elif data_type_list[i] == trt.DataType.HALF:\n                torch_type = torch.float16\n            elif data_type_list[i] == trt.DataType.INT32:\n                torch_type = torch.int32\n            elif data_type_list[i] == trt.DataType.INT8:\n                torch_type = torch.int8\n            else:\n                raise Exception(f\"not support type {data_type_list[i]}\")\n            tensor = torch.zeros(\n                size=tuple(shape_list[i]), dtype=torch_type, device=device\n            )\n            output_tensors.append(tensor)\n        # === run inference with V3 ===\n        for i in range(self.n_input_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                input_tensors[i].data_ptr()\n            )\n        for i in range(self.n_input_, self.n_total_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                output_tensors[i - self.n_input_].data_ptr()\n            )\n        self.context_.execute_async_v3(stream_handle=self.stream_)\n        (error_info,) = cudart.cudaDeviceSynchronize()\n        gpu_check(error_info)\n        (error_info,) = cudart.cudaStreamSynchronize(self.stream_)\n        gpu_check(error_info)\n        return output_tensors", "\n\nclass KernelWithPast:\n    def __init__(self, engine_path: str, batch_size: int = 1, num_layers: int = 28):\n        assert os.path.exists(engine_path), print(f\"{engine_path} not exists.\")\n        self.batch_size_ = batch_size\n        self.n_input_ = 2 + num_layers * 2\n        self.n_output_ = num_layers * 2 + 1\n        self.num_layers = num_layers\n        self.n_total_ = self.n_input_ + self.n_output_\n        self.tensor_names_ = []\n        # self.logger_ = trt.Logger(trt.Logger.INFO)\n        self.logger_ = MyLogger()\n        self.runtime_ = trt.Runtime(self.logger_)\n        # load engine\n        with open(engine_path, \"rb\") as f:\n            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n        # verify io number\n        self.verify_io_number()\n        # init stream and context\n        error_log, self.stream_ = cudart.cudaStreamCreate()\n        gpu_check(error_log)\n        self.context_ = self.engine_.create_execution_context()\n        print(stylize(\"init context and stream OK\", fg(\"green\")))\n        self.context_.set_optimization_profile_async(0, self.stream_)\n\n    def __del__(self):\n        cudart.cudaStreamDestroy(self.stream_)\n\n    def verify_io_number(self):\n        n_io = self.engine_.num_io_tensors\n        if n_io != self.n_total_:\n            raise RuntimeError(stylize(\n                \"Number of IO tensors is not correct, \" +\n                f\"must be {self.n_total_}, but you have {n_io} tensors\",\n                fg(\"red\")))\n        n_input = 0\n        n_output = 0\n        for i in range(n_io):\n            name = self.engine_.get_tensor_name(i)\n            self.tensor_names_.append(name)\n            if self.engine_.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                n_input += 1\n            else:\n                n_output += 1\n        if n_input != self.n_input_:\n            raise RuntimeError(stylize(\n                \"Number of input tensors is not correct, \" +\n                f\"must be {self.n_input_}, but you have {n_input} tensors\",\n                fg(\"red\")))\n        if n_output != self.n_output_:\n            raise RuntimeError(stylize(\n                \"Number of output tensors is not correct, \" +\n                f\"must be {self.n_output_}, but you have {n_output} tensors\",\n                fg(\"red\")))\n        n_profile = self.engine_.num_optimization_profiles\n        if n_profile != 1:\n            raise RuntimeError(stylize(\n                \"Number of optimization profiles is not correct, \" +\n                f\"must be 1, but you have {n_profile} profiles\",\n                fg(\"red\")))\n        print(stylize(f\"number of profile: {n_profile}\", fg(\"green\")))\n\n    def set_input_shape(self, seq_len: int):\n        self.context_.set_input_shape(\"input_ids\", (self.batch_size_, 1))\n        self.context_.set_input_shape(\"position_ids\", (self.batch_size_, 1))\n        for i in range(2, self.n_input_):\n            self.context_.set_input_shape(\n                self.tensor_names_[i],\n                (seq_len, self.batch_size_, 2, 128)\n            )\n\n    def get_tensor_size(self):\n        shape_list = []\n        data_type_list = []\n        for i in range(self.n_total_):\n            tensor_name = self.tensor_names_[i]\n            shape = self.context_.get_tensor_shape(tensor_name)\n            shape_list.append(shape)\n            data_type = self.engine_.get_tensor_dtype(tensor_name)\n            data_type_list.append(data_type)\n        return shape_list, data_type_list\n\n    def forward(self, input_tensors: Tuple[torch.Tensor, ...]):\n        assert len(input_tensors) == self.n_input_, \\\n            print(f\"this number of input tensor must be {self.n_input_}\")\n        device = input_tensors[0].device\n        past_seq_len = input_tensors[2].shape[0]\n        self.set_input_shape(past_seq_len)\n        shape_list, data_type_list = self.get_tensor_size()\n        # --- prepare for output --- #\n        output_tensors = []\n        for i in range(self.n_input_, self.n_total_):\n            if data_type_list[i] == trt.DataType.FLOAT:\n                torch_type = torch.float32\n            elif data_type_list[i] == trt.DataType.HALF:\n                torch_type = torch.float16\n            elif data_type_list[i] == trt.DataType.INT32:\n                torch_type = torch.int32\n            elif data_type_list[i] == trt.DataType.INT8:\n                torch_type = torch.int8\n            else:\n                raise Exception(f\"not support type {data_type_list[i]}\")\n            tensor = torch.zeros(\n                size=tuple(shape_list[i]), dtype=torch_type, device=device\n            )\n            output_tensors.append(tensor)\n        # === run inference with V3 ===\n        for i in range(self.n_input_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                input_tensors[i].data_ptr()\n            )\n        for i in range(self.n_input_, self.n_total_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                output_tensors[i - self.n_input_].data_ptr()\n            )\n        self.context_.execute_async_v3(stream_handle=self.stream_)\n        (error_info,) = cudart.cudaDeviceSynchronize()\n        gpu_check(error_info)\n        (error_info,) = cudart.cudaStreamSynchronize(self.stream_)\n        gpu_check(error_info)\n        return output_tensors", "\n\nif __name__ == \"__main__\":\n    now_dir = os.path.dirname(os.path.abspath(__file__))\n    project_dir = os.path.dirname(now_dir)\n    model_dir = os.path.join(project_dir, \"models\")\n    num_layers = 28\n    engine_path1 = os.path.join(model_dir, \"chatglm6b2-bs1_with_cache.plan\")\n    kernel = KernelWithPast(engine_path1)\n    input_ids = torch.ones([1, 1], dtype=torch.int64).cuda()\n    position_ids = torch.ones([1, 1], dtype=torch.int64).cuda()\n    input_tensors1 = [input_ids, position_ids]\n    device1 = torch.device(\"cuda:0\")\n    for _ in range(num_layers):\n        for _ in range(2):\n            on_key_value = torch.rand([65, 1, 2, 128]).to(device1)\n            input_tensors1.append(on_key_value)\n    output_tensors1 = kernel.forward(tuple(input_tensors1))\n    print(\"first shape\", output_tensors1[0].shape)\n    print(\"last shape\", output_tensors1[-1].shape)", "\n"]}
{"filename": "kernel/pykernel_no_past.py", "chunked_list": ["import os\nimport sys\nimport os.path\nimport torch\nimport tensorrt as trt\nfrom colored import stylize, fg\nfrom typing import List, Tuple\nfrom cuda import cudart\n\n\nclass MyLogger(trt.ILogger):\n    def __init__(self):\n        trt.ILogger.__init__(self)\n\n    def log(self, severity, msg):\n        if severity == trt.ILogger.ERROR:\n            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.WARNING:\n            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # \u9ec4\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INTERNAL_ERROR:\n            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INFO:\n            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # \u7eff\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.VERBOSE:\n            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # \u84dd\u8272\u5b57\u4f53\n        else:\n            print(\"[UNKNOWN] \" + msg)", "\n\nclass MyLogger(trt.ILogger):\n    def __init__(self):\n        trt.ILogger.__init__(self)\n\n    def log(self, severity, msg):\n        if severity == trt.ILogger.ERROR:\n            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.WARNING:\n            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # \u9ec4\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INTERNAL_ERROR:\n            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.INFO:\n            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # \u7eff\u8272\u5b57\u4f53\n        elif severity == trt.ILogger.VERBOSE:\n            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # \u84dd\u8272\u5b57\u4f53\n        else:\n            print(\"[UNKNOWN] \" + msg)", "\n\ndef gpu_check(error: cudart.cudaError_t):\n    if error != cudart.cudaError_t.cudaSuccess:\n        error_name = cudart.cudaGetErrorName(error)\n        error_info = cudart.cudaGetErrorString(error)\n        print(stylize(f\"ERROR [{error_name}]: {error_info}\", fg(\"red\")))\n        raise Exception(f\"ERROR [{error_name}]: {error_info}\")\n\n\nclass KernelNoPast:\n    def __init__(self, engine_path: str, batch_size: int = 1, num_layers: int = 28):\n        assert os.path.exists(engine_path), print(f\"{engine_path} not exists.\")\n        self.batch_size_ = batch_size\n        self.n_input_ = 2\n        self.n_output_ = num_layers * 2 + 1\n        self.n_total_ = self.n_input_ + self.n_output_\n        self.tensor_names_ = []\n        # self.logger_ = trt.Logger(trt.Logger.INFO)\n        self.logger_ = MyLogger()\n        self.runtime_ = trt.Runtime(self.logger_)\n        # load engine\n        with open(engine_path, \"rb\") as f:\n            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n        # verify io number\n        self.verify_io_number()\n        # init stream and context\n        error_log, self.stream_ = cudart.cudaStreamCreate()\n        gpu_check(error_log)\n        self.context_ = self.engine_.create_execution_context()\n        print(stylize(\"init context and stream OK\", fg(\"green\")))\n        self.context_.set_optimization_profile_async(0, self.stream_)\n\n    def __del__(self):\n        cudart.cudaStreamDestroy(self.stream_)\n\n    def verify_io_number(self):\n        n_io = self.engine_.num_io_tensors\n        if n_io != self.n_total_:\n            raise RuntimeError(stylize(\n                \"Number of IO tensors is not correct, \" +\n                f\"must be {self.n_total_}, but you have {n_io} tensors\",\n                fg(\"red\")))\n        n_input = 0\n        n_output = 0\n        for i in range(n_io):\n            name = self.engine_.get_tensor_name(i)\n            self.tensor_names_.append(name)\n            if self.engine_.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                n_input += 1\n            else:\n                n_output += 1\n        if n_input != self.n_input_:\n            raise RuntimeError(stylize(\n                \"Number of input tensors is not correct, \" +\n                f\"must be {self.n_input_}, but you have {n_input} tensors\",\n                fg(\"red\")))\n        if n_output != self.n_output_:\n            raise RuntimeError(stylize(\n                \"Number of output tensors is not correct, \" +\n                f\"must be {self.n_output_}, but you have {n_output} tensors\",\n                fg(\"red\")))\n        n_profile = self.engine_.num_optimization_profiles\n        if n_profile != 1:\n            raise RuntimeError(stylize(\n                \"Number of optimization profiles is not correct, \" +\n                f\"must be 1, but you have {n_profile} profiles\",\n                fg(\"red\")))\n        print(stylize(f\"number of profile: {n_profile}\", fg(\"green\")))\n\n    def set_input_shape(self, seq_len: int):\n        self.context_.set_input_shape(\"input_ids\", (self.batch_size_, seq_len))\n        self.context_.set_input_shape(\"position_ids\", (self.batch_size_, seq_len))\n\n    def get_tensor_size(self):\n        shape_list = []\n        data_type_list = []\n        for i in range(self.n_total_):\n            tensor_name = self.tensor_names_[i]\n            shape = self.context_.get_tensor_shape(tensor_name)\n            shape_list.append(shape)\n            data_type = self.engine_.get_tensor_dtype(tensor_name)\n            data_type_list.append(data_type)\n        return shape_list, data_type_list\n\n    def forward(self, input_tensors: Tuple[torch.Tensor, torch.Tensor]):\n        assert len(input_tensors) == self.n_input_, \\\n            print(f\"this number of input tensor must be {self.n_input_}\")\n        seq_len = input_tensors[0].size(1)\n        device = input_tensors[0].device\n        self.set_input_shape(seq_len)\n        shape_list, data_type_list = self.get_tensor_size()\n        # --- prepare for output --- #\n        output_tensors = []\n        for i in range(self.n_input_, self.n_total_):\n            if data_type_list[i] == trt.DataType.FLOAT:\n                torch_type = torch.float32\n            elif data_type_list[i] == trt.DataType.HALF:\n                torch_type = torch.float16\n            elif data_type_list[i] == trt.DataType.INT32:\n                torch_type = torch.int32\n            elif data_type_list[i] == trt.DataType.INT8:\n                torch_type = torch.int8\n            else:\n                raise Exception(f\"not support type {data_type_list[i]}\")\n            tensor = torch.zeros(\n                size=tuple(shape_list[i]), dtype=torch_type, device=device\n            )\n            output_tensors.append(tensor)\n        # === run inference with V3 ===\n        for i in range(self.n_input_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                input_tensors[i].data_ptr()\n            )\n        for i in range(self.n_input_, self.n_total_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                output_tensors[i - self.n_input_].data_ptr()\n            )\n        self.context_.execute_async_v3(stream_handle=self.stream_)\n        (error_info,) = cudart.cudaDeviceSynchronize()\n        gpu_check(error_info)\n        (error_info,) = cudart.cudaStreamSynchronize(self.stream_)\n        gpu_check(error_info)\n        return output_tensors", "\n\nclass KernelNoPast:\n    def __init__(self, engine_path: str, batch_size: int = 1, num_layers: int = 28):\n        assert os.path.exists(engine_path), print(f\"{engine_path} not exists.\")\n        self.batch_size_ = batch_size\n        self.n_input_ = 2\n        self.n_output_ = num_layers * 2 + 1\n        self.n_total_ = self.n_input_ + self.n_output_\n        self.tensor_names_ = []\n        # self.logger_ = trt.Logger(trt.Logger.INFO)\n        self.logger_ = MyLogger()\n        self.runtime_ = trt.Runtime(self.logger_)\n        # load engine\n        with open(engine_path, \"rb\") as f:\n            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n        # verify io number\n        self.verify_io_number()\n        # init stream and context\n        error_log, self.stream_ = cudart.cudaStreamCreate()\n        gpu_check(error_log)\n        self.context_ = self.engine_.create_execution_context()\n        print(stylize(\"init context and stream OK\", fg(\"green\")))\n        self.context_.set_optimization_profile_async(0, self.stream_)\n\n    def __del__(self):\n        cudart.cudaStreamDestroy(self.stream_)\n\n    def verify_io_number(self):\n        n_io = self.engine_.num_io_tensors\n        if n_io != self.n_total_:\n            raise RuntimeError(stylize(\n                \"Number of IO tensors is not correct, \" +\n                f\"must be {self.n_total_}, but you have {n_io} tensors\",\n                fg(\"red\")))\n        n_input = 0\n        n_output = 0\n        for i in range(n_io):\n            name = self.engine_.get_tensor_name(i)\n            self.tensor_names_.append(name)\n            if self.engine_.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n                n_input += 1\n            else:\n                n_output += 1\n        if n_input != self.n_input_:\n            raise RuntimeError(stylize(\n                \"Number of input tensors is not correct, \" +\n                f\"must be {self.n_input_}, but you have {n_input} tensors\",\n                fg(\"red\")))\n        if n_output != self.n_output_:\n            raise RuntimeError(stylize(\n                \"Number of output tensors is not correct, \" +\n                f\"must be {self.n_output_}, but you have {n_output} tensors\",\n                fg(\"red\")))\n        n_profile = self.engine_.num_optimization_profiles\n        if n_profile != 1:\n            raise RuntimeError(stylize(\n                \"Number of optimization profiles is not correct, \" +\n                f\"must be 1, but you have {n_profile} profiles\",\n                fg(\"red\")))\n        print(stylize(f\"number of profile: {n_profile}\", fg(\"green\")))\n\n    def set_input_shape(self, seq_len: int):\n        self.context_.set_input_shape(\"input_ids\", (self.batch_size_, seq_len))\n        self.context_.set_input_shape(\"position_ids\", (self.batch_size_, seq_len))\n\n    def get_tensor_size(self):\n        shape_list = []\n        data_type_list = []\n        for i in range(self.n_total_):\n            tensor_name = self.tensor_names_[i]\n            shape = self.context_.get_tensor_shape(tensor_name)\n            shape_list.append(shape)\n            data_type = self.engine_.get_tensor_dtype(tensor_name)\n            data_type_list.append(data_type)\n        return shape_list, data_type_list\n\n    def forward(self, input_tensors: Tuple[torch.Tensor, torch.Tensor]):\n        assert len(input_tensors) == self.n_input_, \\\n            print(f\"this number of input tensor must be {self.n_input_}\")\n        seq_len = input_tensors[0].size(1)\n        device = input_tensors[0].device\n        self.set_input_shape(seq_len)\n        shape_list, data_type_list = self.get_tensor_size()\n        # --- prepare for output --- #\n        output_tensors = []\n        for i in range(self.n_input_, self.n_total_):\n            if data_type_list[i] == trt.DataType.FLOAT:\n                torch_type = torch.float32\n            elif data_type_list[i] == trt.DataType.HALF:\n                torch_type = torch.float16\n            elif data_type_list[i] == trt.DataType.INT32:\n                torch_type = torch.int32\n            elif data_type_list[i] == trt.DataType.INT8:\n                torch_type = torch.int8\n            else:\n                raise Exception(f\"not support type {data_type_list[i]}\")\n            tensor = torch.zeros(\n                size=tuple(shape_list[i]), dtype=torch_type, device=device\n            )\n            output_tensors.append(tensor)\n        # === run inference with V3 ===\n        for i in range(self.n_input_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                input_tensors[i].data_ptr()\n            )\n        for i in range(self.n_input_, self.n_total_):\n            self.context_.set_tensor_address(\n                self.tensor_names_[i],\n                output_tensors[i - self.n_input_].data_ptr()\n            )\n        self.context_.execute_async_v3(stream_handle=self.stream_)\n        (error_info,) = cudart.cudaDeviceSynchronize()\n        gpu_check(error_info)\n        (error_info,) = cudart.cudaStreamSynchronize(self.stream_)\n        gpu_check(error_info)\n        return output_tensors", "\n\nif __name__ == \"__main__\":\n    now_dir = os.path.dirname(os.path.abspath(__file__))\n    project_dir = os.path.dirname(now_dir)\n    model_dir = os.path.join(project_dir, \"models\")\n    engine_path1 = os.path.join(model_dir, \"chatglm6b2-bs1_no_cache.plan\")\n    kernel = KernelNoPast(engine_path1)\n    input_ids = torch.ones([1, 4], dtype=torch.int64).cuda()\n    position_ids = torch.ones([1, 4], dtype=torch.int64).cuda()\n    input_tensors1 = (input_ids, position_ids)\n    output_tensors1 = kernel.forward(input_tensors1)\n    print(\"first shape\", output_tensors1[0].shape)\n    print(\"last shape\", output_tensors1[-1].shape)", "\n"]}
{"filename": "tensorrt_export/onnx_trt_compare_runing.py", "chunked_list": ["#!/usr/bin/env python3\n# Template auto-generated by polygraphy [v0.47.1] on 06/05/23 at 12:36:31\n# code gen with onnx_trt_compare.sh\n# but i edit the code to make it more readable\nimport tensorrt as trt\nimport os\nimport numpy as np\nfrom polygraphy.logger import G_LOGGER\nG_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}\n", "G_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}\n\nfrom polygraphy import constants\nimport onnx\nfrom polygraphy.backend.common import BytesFromPath\nfrom polygraphy.backend.onnx import BytesFromOnnx, ModifyOutputs as ModifyOnnxOutputs, OnnxFromPath\nfrom polygraphy.backend.onnxrt import OnnxrtRunner, SessionFromOnnx\nfrom polygraphy.backend.trt import EngineFromBytes, TrtRunner\nfrom polygraphy.common import TensorMetadata\nfrom polygraphy.comparator import Comparator, CompareFunc, DataLoader", "from polygraphy.common import TensorMetadata\nfrom polygraphy.comparator import Comparator, CompareFunc, DataLoader\nfrom polygraphy.exception import PolygraphyException\n# --dir info--\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\n\n\n\noutput_dir = os.path.join(project_dir, \"output\")", "\noutput_dir = os.path.join(project_dir, \"output\")\nnew_onnx_dir = os.path.join(output_dir, \"onnx_output_no_cache_new\")\nnew_onnx_path = os.path.join(new_onnx_dir, \"chatglm2_6b.onnx\")\nmodel_dir = os.path.join(project_dir, \"models\")\ntrt_model_path = os.path.join(model_dir, \"model-no-cache-FP32-MarkAll.plan\")\nnum_layers = 1\n\n\n", "\n\n# Data Loader\ndata_loader = DataLoader(\n    input_metadata=TensorMetadata()\n    .add('input_ids', dtype=np.int32, shape=(1, 512), min_shape=None, max_shape=None)\n    .add('position_ids', dtype=np.int32, shape=(1, 512), min_shape=None, max_shape=None)\n)\n\nprint(\"load onnx\")", "\nprint(\"load onnx\")\n# build_onnxrt_session = SessionFromOnnx(new_onnx_path, providers=[\"CUDAExecutionProvider\"])\nbuild_onnxrt_session = SessionFromOnnx(new_onnx_path, providers=[\"CPUExecutionProvider\"])\nprint(\"load TensorRT engine\")\nengine_bytes = BytesFromPath(trt_model_path)\ndeserialize_engine = EngineFromBytes(engine_bytes)\n\n# Runners\nrunners = [", "# Runners\nrunners = [\n    OnnxrtRunner(build_onnxrt_session),\n    # TrtRunner(deserialize_engine),\n]\n\n# Runner Execution\nresults = Comparator.run(runners, data_loader=data_loader)\n\nsuccess = True", "\nsuccess = True\n# Accuracy Comparison for fp32\n# compare_func = CompareFunc.simple(rtol={'': 5e-4}, atol={'': 5e-4})\n# Accuracy Comparison for fp16\ncompare_func = CompareFunc.simple(rtol={'': 5e-3}, atol={'': 5e-3})\nsuccess &= bool(Comparator.compare_accuracy(results, compare_func=compare_func))\n\n# Report Results\nif not success:\n    raise PolygraphyException('FAILED')", "# Report Results\nif not success:\n    raise PolygraphyException('FAILED')\n"]}
{"filename": "tensorrt_export/onnx_trt_compare_fp16.py", "chunked_list": ["#!/usr/bin/env python3\n# Template auto-generated by polygraphy [v0.47.1] on 06/05/23 at 12:36:31\n# code gen with onnx_trt_compare.sh\n# but i edit the code to make it more readable\nimport tensorrt as trt\nimport os\nimport shutil\nimport numpy as np\nfrom polygraphy.logger import G_LOGGER\nG_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}", "from polygraphy.logger import G_LOGGER\nG_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}\nfrom colored import stylize, fg\nfrom polygraphy import constants\nimport onnx\nfrom polygraphy.backend.common import SaveBytes\nfrom polygraphy.backend.onnx import modify_outputs, onnx_from_path, ModifyOutputs\nfrom polygraphy.backend.onnxrt import OnnxrtRunner, SessionFromOnnx\nfrom polygraphy.backend.trt import CreateConfig as CreateTrtConfig, EngineBytesFromNetwork, EngineFromBytes, ModifyNetworkOutputs, NetworkFromOnnxPath, Profile, TrtRunner\nfrom polygraphy.common import TensorMetadata", "from polygraphy.backend.trt import CreateConfig as CreateTrtConfig, EngineBytesFromNetwork, EngineFromBytes, ModifyNetworkOutputs, NetworkFromOnnxPath, Profile, TrtRunner\nfrom polygraphy.common import TensorMetadata\nfrom polygraphy.comparator import Comparator, CompareFunc, DataLoader\nfrom polygraphy.exception import PolygraphyException\nfrom polygraphy.backend.trt import network_from_onnx_path\n\n\n\n# --dir info--\nnow_dir = os.path.dirname(os.path.abspath(__file__))", "# --dir info--\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\nimport sys\nsys.path.append(project_dir)\nfrom tensorrt_export.onnx2trt_no_cache import (\n    get_network_profiles,\n    MyLogger,\n)\n", ")\n\n\noutput_dir = os.path.join(project_dir, \"output\")\nnew_onnx_dir = os.path.join(output_dir, \"onnx_output_no_cache_new\")\nif not os.path.exists(new_onnx_dir):\n    os.mkdir(new_onnx_dir)\nelse:\n    for file in os.listdir(new_onnx_dir):\n        os.remove(os.path.join(new_onnx_dir, file))", "new_onnx_dir2 = os.path.join(output_dir, \"onnx_output_no_cache_new2\")\nif not os.path.exists(new_onnx_dir2):\n    os.mkdir(new_onnx_dir2)\nelse:\n    for file in os.listdir(new_onnx_dir2):\n        os.remove(os.path.join(new_onnx_dir2, file))\n\nonnx_path = os.path.join(output_dir, \"onnx_output_no_cache\", \"chatglm2_6b.onnx\")\nnew_onnx_path = os.path.join(new_onnx_dir, \"chatglm2_6b.onnx\")\nnew_onnx_path2 = os.path.join(new_onnx_dir2, \"chatglm2_6b.onnx\")", "new_onnx_path = os.path.join(new_onnx_dir, \"chatglm2_6b.onnx\")\nnew_onnx_path2 = os.path.join(new_onnx_dir2, \"chatglm2_6b.onnx\")\nmodel_dir = os.path.join(project_dir, \"models\")\ntrt_model_path = os.path.join(model_dir, \"model-no-cache-FP16-MarkAll.plan\")\ntime_cache_path = os.path.join(output_dir, \"fp16_markAll_no_cache.cache\")\nuse_time_cache = True\nnum_layers = 1\n\n\n# Data Loader", "\n# Data Loader\ndtype = np.dtype(np.int32)\ndata_loader = DataLoader(\n    input_metadata=TensorMetadata()\n    .add('input_ids', dtype=dtype, shape=(1, 512))\n    .add('position_ids', dtype=np.int32, shape=(1, 512))\n)\n# load onnx\n", "# load onnx\n\nprint(\"loading onnx model from\", onnx_path)\nonnx_model = onnx_from_path(onnx_path)\n# this layer will output None in onnxrt\nbool_tensor_list = [\"/transformer/encoder/layers.0/mlp/Sigmoid_output_0\"]\nfor node in onnx_model.graph.node:\n    # print(node.name, node.op_type)\n    # this layer is a bool tensor, it will cause error when run TensorRT engine\n    if node.op_type == \"Equal\":\n        print(\"find bool\", node.name)\n        bool_tensor_list.extend(node.output)", "input_list = onnx_model.graph.input\ninput_names = [i.name for i in input_list]\noutput_list = onnx_model.graph.output\noutput_names = [o.name for o in output_list]\nexclude_outputs = input_names + bool_tensor_list\n# mark all layers as output for onnx model\n# warning this will make the onnx model output all layers with no type and no shape\nnew_onnx_model = modify_outputs(\n    model=onnx_model,\n    outputs=constants.MARK_ALL,", "    model=onnx_model,\n    outputs=constants.MARK_ALL,\n    exclude_outputs=exclude_outputs,\n)\nnew_output_list = new_onnx_model.graph.output\nnew_output_names = [o.name for o in new_output_list]\nonnx_input_num = len(new_onnx_model.graph.input)\nonnx_output_num = len(new_onnx_model.graph.output)\nprint(\"onnx input num:\", onnx_input_num, \"onnx output num:\", onnx_output_num)\nonnx.save_model(", "print(\"onnx input num:\", onnx_input_num, \"onnx output num:\", onnx_output_num)\nonnx.save_model(\n    new_onnx_model,\n    new_onnx_path,\n    save_as_external_data=True,\n    all_tensors_to_one_file=False\n)\n# load onnx_runtime\nbuild_onnx_rt_session = SessionFromOnnx(new_onnx_path)\n# get onnx runtime output tensor info(name, type, shape)", "build_onnx_rt_session = SessionFromOnnx(new_onnx_path)\n# get onnx runtime output tensor info(name, type, shape)\nsess = build_onnx_rt_session()\nbool_tensor_list = []\nsess_output = sess.get_outputs()\nfor node in sess_output:\n    if node.type == \"tensor(float)\":\n        dtype = onnx.TensorProto.FLOAT\n    elif node.type == \"tensor(bool)\":\n        print(\"find bool\", node.name)\n        bool_tensor_list.append(node.name)\n        dtype = onnx.TensorProto.BOOL\n    elif node.type == \"tensor(int64)\":\n        dtype = onnx.TensorProto.INT64\n    else:\n        print(\"unknown dtype:\", node.type)\n        raise ValueError\n    output_tensor = onnx.helper.make_tensor_value_info(\n        node.name, dtype, None\n    )\n    # replace the output tensor\n    for i, vi in enumerate(new_onnx_model.graph.output):\n        if vi.name == node.name:\n            new_onnx_model.graph.output[i].CopyFrom(output_tensor)", "# save again\nfor file in os.listdir(new_onnx_dir):\n    shutil.copy(os.path.join(new_onnx_dir, file), new_onnx_dir2)\nonnx.save_model(\n    new_onnx_model,\n    new_onnx_path2,\n    save_as_external_data=True,\n    all_tensors_to_one_file=False\n)\nprint(\"===========onnx model loaded=========================\")", ")\nprint(\"===========onnx model loaded=========================\")\n# build trt engine\nprint(\"===========building trt engine=========================\")\n# prepare trt builder\nbuilder = trt.Builder(MyLogger())\nbuilder.max_threads = os.cpu_count() // 2\nconfig = builder.create_builder_config()\n# use fp16\nconfig.flags = 1 << int(trt.BuilderFlag.FP16)", "# use fp16\nconfig.flags = 1 << int(trt.BuilderFlag.FP16)\n# disable TF32\nconfig.flags = config.flags & ~(1 << int(trt.BuilderFlag.TF32))\nprint(\"use fp16? \", config.get_flag(trt.BuilderFlag.FP16))\n# read time cache\nif use_time_cache:\n    if os.path.exists(time_cache_path):\n        time_cache = open(time_cache_path, \"rb\").read()\n        if time_cache is None:\n            time_cache = b\"\"\n            print(stylize(\"read time cache failed\", fg(\"red\")))\n        else:\n            print(stylize(f\"read time cache from {time_cache_path}\", fg(\"green\")))\n    else:\n        time_cache = b\"\"\n        print(stylize(\"time cache will init with empty.\", fg(\"green\")))\n\n    # set time cache\n    cache = config.create_timing_cache(time_cache)\n    config.set_timing_cache(cache, False)", "profile_list = get_network_profiles(builder, num_layers=num_layers)\nfor profile in profile_list:\n    config.add_optimization_profile(profile)\n\n_b, network, _p = NetworkFromOnnxPath(new_onnx_path2)()\n\n# network = network_from_onnx_path(onnx_path)\n# set_network_outputs = ModifyNetworkOutputs(\n#     network=network,\n#     outputs=constants.MARK_ALL,", "#     network=network,\n#     outputs=constants.MARK_ALL,\n#     exclude_outputs=bool_tensor_list\n# )\n# wo_b, network, _p = set_network_outputs()\nnetwork_input_number = network.num_inputs\nnetwork_output_number = network.num_outputs\nprint(\"TensorRT input num:\", network_input_number, \"TensorRT output num:\", network_output_number)\nserialized_engine = builder.build_serialized_network(network, config)\nif serialized_engine is not None:\n    with open(trt_model_path, \"wb\") as f:\n        f.write(serialized_engine)\n    # save_engine(trt_engine, tensorrt_engine_path)\n    print(\"==tensorRT engine compile done==\")", "serialized_engine = builder.build_serialized_network(network, config)\nif serialized_engine is not None:\n    with open(trt_model_path, \"wb\") as f:\n        f.write(serialized_engine)\n    # save_engine(trt_engine, tensorrt_engine_path)\n    print(\"==tensorRT engine compile done==\")\n\n# save time cache\nif use_time_cache and not os.path.exists(time_cache_path):\n    time_cache = config.get_timing_cache()\n    if time_cache is not None:\n        time_cache_data = time_cache.serialize()\n        open(time_cache_path, \"wb\").write(time_cache_data)\n        print(\n            stylize(\n                \"save time cache to {}\".format(time_cache_path),\n                fg(\"green\")\n            )\n        )", "if use_time_cache and not os.path.exists(time_cache_path):\n    time_cache = config.get_timing_cache()\n    if time_cache is not None:\n        time_cache_data = time_cache.serialize()\n        open(time_cache_path, \"wb\").write(time_cache_data)\n        print(\n            stylize(\n                \"save time cache to {}\".format(time_cache_path),\n                fg(\"green\")\n            )\n        )", "\n# profiles = [\n#     Profile().add('input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value', min=[0, 1, 32, 128], opt=[0, 1, 32, 128], max=[0, 1, 32, 128])\n# ]\n# create_trt_config = CreateTrtConfig(memory_pool_limits=2 * (1024 ** 3), profiles=profile_list)\n# build_engine = EngineBytesFromNetwork(set_network_outputs, config=create_trt_config)\n# save_engine_bytes = SaveBytes(build_engine, path=trt_model_path)\nsave_engine_bytes = SaveBytes(serialized_engine, path=trt_model_path)\ndeserialize_engine = EngineFromBytes(save_engine_bytes)\nprint(\"===========trt engine build OK=========================\")", "deserialize_engine = EngineFromBytes(save_engine_bytes)\nprint(\"===========trt engine build OK=========================\")\n\n# Runners\nrunners = [\n    OnnxrtRunner(build_onnx_rt_session),\n    TrtRunner(deserialize_engine),\n]\n\n# Runner Execution", "\n# Runner Execution\nresults = Comparator.run(runners, data_loader=data_loader)\n\nsuccess = True\n# Accuracy Comparison\ncompare_func = CompareFunc.simple(rtol={'': 1e-4}, atol={'': 1e-4})\nsuccess &= bool(Comparator.compare_accuracy(results, compare_func=compare_func))\n\n# Report Results\nif not success:\n    raise PolygraphyException('FAILED')", "\n# Report Results\nif not success:\n    raise PolygraphyException('FAILED')\n"]}
{"filename": "tensorrt_export/trt_check_no_past.py", "chunked_list": ["import os\nimport sys\nimport torch\nfrom colored import stylize, fg\n\n\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\nsys.path.append(project_dir)\n", "sys.path.append(project_dir)\n\n# from kernel.pykernel_no_past_old import KernelNoPast\nfrom kernel.pykernel_no_past_new import KernelNoPast\n\n\n\ndef check_value(pre_value: torch.Tensor, true_value: torch.Tensor, diff=1e-3):\n    if pre_value.shape != true_value.shape:\n        raise Exception(\"compare shape must be same!\")\n    max_diff = (pre_value - true_value).abs_().max().item()\n    if max_diff > diff:\n        print(stylize(f\"compare diff failed, diff is {max_diff}\", fg(\"red\")))\n    else:\n        print(stylize(\"compare diff OK!\", fg(\"green\")))\n    return max_diff", "\n\ndef main():\n    assert torch.cuda.is_available(), print(\"you must has cuda to run TensorRT\")\n    output_dir = os.path.join(project_dir, \"output\")\n    model_dir = os.path.join(project_dir, \"models\")\n    engine_path1 = os.path.join(model_dir, \"chatglm6b2-bs1_with_cache.plan\")\n    input_path = os.path.join(output_dir, \"pt_input1.pt\")\n    output_path = os.path.join(output_dir, \"pt_output1.pt\")\n    device = torch.device(\"cuda:0\")\n    input_dict = torch.jit.load(input_path)\n    batch_size = 1\n    num_layers = 28\n    output_dict = torch.jit.load(output_path)\n    input_ids: torch.Tensor = input_dict.input_ids.int().to(device)\n    position_ids: torch.Tensor = input_dict.position_ids.int().to(device)\n    input_tensors = (input_ids, position_ids)\n    kernel = KernelNoPast(engine_path1, batch_size, num_layers)\n    output_tensors = kernel.forward(input_tensors)\n\n    # compare output\n    max_diff_ = 0\n    # compare logits\n    logits = output_dict.logits.to(device)\n    pred_logits = output_tensors[-1]\n    logits_diff = check_value(logits, pred_logits)\n    print(\"=\" * 20)\n    print(\"compare logits\")\n    if logits_diff > max_diff_:\n        max_diff_ = logits_diff\n    # compare past key values\n    for i in range(num_layers):\n        present_key_name = f\"present_key_values.{i}.key\"\n        present_value_name = f\"present_key_values.{i}.value\"\n        true_present_key = getattr(output_dict, present_key_name).to(device)\n        true_present_value = getattr(output_dict, present_value_name).to(device)\n        pre_present_key = output_tensors[i * 2]\n        pre_present_value = output_tensors[i * 2 + 1]\n        print(\"=\" * 20)\n        print(\"compare \", present_key_name)\n        temp_diff = check_value(pre_present_key, true_present_key)\n        if temp_diff > max_diff_:\n            max_diff_ = temp_diff\n\n        print(\"=\" * 20)\n        print(\"compare \", present_value_name)\n        temp_diff = check_value(pre_present_value, true_present_value)\n        if temp_diff > max_diff_:\n            max_diff_ = temp_diff\n    print(f\"max diff is {max_diff_}\")", "\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n"]}
{"filename": "tensorrt_export/compare.py", "chunked_list": ["#!/usr/bin/env python3\n# Template auto-generated by polygraphy [v0.47.1] on 06/05/23 at 12:36:31\n# Generation Command: /home/tlntin/anaconda3/envs/torch/bin/polygraphy run ../output/onnx_output/chatglm_6b.onnx --onnxrt --trt --workspace 1000000000 --save-engine=../models/model-FP32-MarkAll.plan --atol 1e-3 --rtol 1e-3 --verbose --onnx-outputs mark all --trt-outputs mark all --trt-opt-shapes input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value:[0,1,32,128]  --input-shapes input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value:[0,1,32,128] --gen-script compare.py\n# This script compares /home/tlntin/PycharmProjects/ChatGLM-6B-TensorRT/output/onnx_output/chatglm_6b.onnx between ONNX-Runtime and TensorRT.\n\nfrom polygraphy.logger import G_LOGGER\nG_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}\nfrom polygraphy import constants\nfrom polygraphy.backend.common import SaveBytes\nfrom polygraphy.backend.onnx import BytesFromOnnx, ModifyOutputs as ModifyOnnxOutputs, OnnxFromPath", "from polygraphy.backend.common import SaveBytes\nfrom polygraphy.backend.onnx import BytesFromOnnx, ModifyOutputs as ModifyOnnxOutputs, OnnxFromPath\nfrom polygraphy.backend.onnxrt import OnnxrtRunner, SessionFromOnnx\nfrom polygraphy.backend.trt import CreateConfig as CreateTrtConfig, EngineBytesFromNetwork, EngineFromBytes, ModifyNetworkOutputs, NetworkFromOnnxPath, Profile, TrtRunner\nfrom polygraphy.common import TensorMetadata\nfrom polygraphy.comparator import Comparator, CompareFunc, DataLoader\nfrom polygraphy.exception import PolygraphyException\n\n# Data Loader\ndata_loader = DataLoader(input_metadata=TensorMetadata().add('input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value', None, [0, 1, 32, 128]))", "# Data Loader\ndata_loader = DataLoader(input_metadata=TensorMetadata().add('input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value', None, [0, 1, 32, 128]))\n\n# Loaders\nload_onnx = OnnxFromPath('/home/tlntin/PycharmProjects/ChatGLM-6B-TensorRT/output/onnx_output/chatglm_6b.onnx')\nmodify_outputs = ModifyOnnxOutputs(load_onnx, outputs=constants.MARK_ALL)\nserialize_onnx = BytesFromOnnx(modify_outputs)\nbuild_onnxrt_session = SessionFromOnnx(serialize_onnx)\nparse_network_from_onnx = NetworkFromOnnxPath('/home/tlntin/PycharmProjects/ChatGLM-6B-TensorRT/output/onnx_output/chatglm_6b.onnx')\nset_network_outputs = ModifyNetworkOutputs(parse_network_from_onnx, outputs=constants.MARK_ALL)", "parse_network_from_onnx = NetworkFromOnnxPath('/home/tlntin/PycharmProjects/ChatGLM-6B-TensorRT/output/onnx_output/chatglm_6b.onnx')\nset_network_outputs = ModifyNetworkOutputs(parse_network_from_onnx, outputs=constants.MARK_ALL)\nprofiles = [\n    Profile().add('input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value', min=[0, 1, 32, 128], opt=[0, 1, 32, 128], max=[0, 1, 32, 128])\n]\ncreate_trt_config = CreateTrtConfig(max_workspace_size=1000000000, profiles=profiles)\nbuild_engine = EngineBytesFromNetwork(set_network_outputs, config=create_trt_config)\nsave_engine_bytes = SaveBytes(build_engine, path='../models/model-FP32-MarkAll.plan')\ndeserialize_engine = EngineFromBytes(save_engine_bytes)\n", "deserialize_engine = EngineFromBytes(save_engine_bytes)\n\n# Runners\nrunners = [\n    OnnxrtRunner(build_onnxrt_session),\n    TrtRunner(deserialize_engine),\n]\n\n# Runner Execution\nresults = Comparator.run(runners, data_loader=data_loader)", "# Runner Execution\nresults = Comparator.run(runners, data_loader=data_loader)\n\nsuccess = True\n# Accuracy Comparison\ncompare_func = CompareFunc.simple(rtol={'': 0.001}, atol={'': 0.001})\nsuccess &= bool(Comparator.compare_accuracy(results, compare_func=compare_func))\n\n# Report Results\nif not success:\n    raise PolygraphyException('FAILED')", "# Report Results\nif not success:\n    raise PolygraphyException('FAILED')\n"]}
{"filename": "tensorrt_export/test_data_outputs.py", "chunked_list": ["import os\nimport torch\nimport onnxruntime as ort\nfrom polygraphy.comparator import RunResults\nfrom polygraphy.json import save_json\n\n\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\noutput_dir = os.path.join(project_dir, \"output\")", "project_dir = os.path.dirname(now_dir)\noutput_dir = os.path.join(project_dir, \"output\")\nnew_onnx_dir = os.path.join(output_dir, \"onnx_output_no_cache_new\")\nnew_onnx_path = os.path.join(new_onnx_dir, \"chatglm2_6b.onnx\")\ninput_path1 = os.path.join(output_dir, \"pt_input1.pt\")\noutput_path1 = os.path.join(output_dir, \"pt_output1.pt\")\ninput_dict = torch.jit.load(input_path1)\nbatch_size = 1\nnum_layers = 1\noutput_dict = torch.jit.load(output_path1)", "num_layers = 1\noutput_dict = torch.jit.load(output_path1)\ninput_ids = input_dict.input_ids.numpy()\nposition_ids = input_dict.position_ids.numpy()\nsession = ort.InferenceSession(new_onnx_path, providers=[\"CPUExecutionProvider\"])\noutputs = session.get_outputs()\noutput_names = [out.name for out in outputs]\noutputs = session.run(output_names, {\"input_ids\": input_ids, \"position_ids\": position_ids})\nprint(len(outputs))\nprint(type(outputs))", "print(len(outputs))\nprint(type(outputs))\noutput_dict = {name: value for (name, value) in zip(output_names, outputs)}\nfor k, v in output_dict.items():\n    if v is None:\n        print(k, \"is None\")\ncustom_outputs = RunResults()\ncustom_outputs.add(out_list=[output_dict], runner_name=\"onnxrt\")\ncustom_outputs.save(\"custom_outputs.json\")\n", "custom_outputs.save(\"custom_outputs.json\")\n\n\n"]}
{"filename": "tensorrt_export/onnx2trt_no_cache.py", "chunked_list": ["import tensorrt as trt\nimport os\nimport time\nfrom colored import fg, stylize\nimport json\n# import onnx\nfrom polygraphy.backend.trt import network_from_onnx_path\nfrom itertools import tee\nfrom tensorrt import MemoryPoolType, PreviewFeature\n", "from tensorrt import MemoryPoolType, PreviewFeature\n\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\noutput_dir = os.path.join(project_dir, \"output\")\ntime_cache_path = os.path.join(output_dir, \"fp16_no_cache.cache\")\n\n# default is 1, maybe you can try 2, 4, 8, 16\nbatch_size = 1\nuse_time_cache = True", "batch_size = 1\nuse_time_cache = True\nmax_length = 2048\nopt_length = max_length // 2\n# if use force use fp16, may reduce the accuracy and memory usage\nforce_use_fp16 = False\n# default 3, max 5, 5 is the best but need more GPU memory and time\nbuilder_optimization_level = 3\n# lower memory GPU can try this option with True \\\n# it can use CPU memory/CPU compute to run some layers, but may reduce the speed", "# lower memory GPU can try this option with True \\\n# it can use CPU memory/CPU compute to run some layers, but may reduce the speed\nall_gpu_fallback = False\n\nif batch_size > 1 and builder_optimization_level != 5:\n    raise Exception(\"batch size > 1, please use builder_optimization_level = 5\")\n\n\nclass MyLogger(trt.ILogger):\n    def __init__(self):\n        trt.ILogger.__init__(self)\n\n    def log(self, severity, msg):\n        if severity == trt.Logger.ERROR:\n            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.Logger.WARNING:\n            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # \u9ec4\u8272\u5b57\u4f53\n        elif severity == trt.Logger.INTERNAL_ERROR:\n            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.Logger.INFO:\n            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # \u7eff\u8272\u5b57\u4f53\n        elif severity == trt.Logger.VERBOSE:\n            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # \u84dd\u8272\u5b57\u4f53\n        else:\n            print(\"[UNKNOWN] \" + msg)", "class MyLogger(trt.ILogger):\n    def __init__(self):\n        trt.ILogger.__init__(self)\n\n    def log(self, severity, msg):\n        if severity == trt.Logger.ERROR:\n            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.Logger.WARNING:\n            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # \u9ec4\u8272\u5b57\u4f53\n        elif severity == trt.Logger.INTERNAL_ERROR:\n            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.Logger.INFO:\n            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # \u7eff\u8272\u5b57\u4f53\n        elif severity == trt.Logger.VERBOSE:\n            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # \u84dd\u8272\u5b57\u4f53\n        else:\n            print(\"[UNKNOWN] \" + msg)", "\n\ndef get_network_profiles(trt_builder, num_layers=28):\n    # ----profile1 when past_key_values is None----\n    profile1 = trt_builder.create_optimization_profile()\n    profile1.set_shape(\n        \"input_ids\",\n        (1, 1),\n        (batch_size, opt_length),\n        (batch_size, max_length),\n    )\n    profile1.set_shape(\n        \"position_ids\",\n        (1, 1),\n        (batch_size, opt_length),\n        (batch_size, max_length),\n    )\n    profiles = [profile1]\n    return profiles", "\n\ndef get_network_definition(trt_network):\n    def pairwise(iterable):\n        a, b = tee(iterable)\n        next(b, None)\n        return zip(a, b)\n    layer_type_set = set() \n    num_layers = trt_network.num_layers\n    indices = list(range(num_layers))\n    for i, i_next in pairwise(indices):\n        layer = trt_network.get_layer(i)\n        next_layer = trt_network.get_layer(i_next)\n        if not all([\n            layer.get_output(i).is_execution_tensor\n            for i in range(layer.num_outputs)\n        ]):\n            continue\n        if layer.get_output_type(0) != trt.float32:\n            continue\n        layer_type_set.add(str(layer.type))\n        if layer.type == trt.LayerType.ELEMENTWISE and \\\n                next_layer.type == trt.LayerType.REDUCE:\n            layer.__class__ = getattr(trt, \"IElementWiseLayer\")\n            if layer.op == trt.ElementWiseOperation.POW:\n                layer.precision = trt.float32\n                layer.set_output_type(0, trt.float32)\n\n            next_layer.precision = trt.float32\n            next_layer.set_output_type(0, trt.float32)\n        #else:\n        #    layer.precision = trt.DataType.HALF\n    layer_type_path = os.path.join(output_dir, \"layer_type.json\")\n    with open(layer_type_path, \"wt\") as f1:\n        json.dump(list(layer_type_set), f1, indent=4)\n    return trt_network", "\n\nif __name__ == \"__main__\":\n    onnx_path = os.path.join(\n        output_dir, \"onnx_output_no_cache\", \"chatglm2_6b.onnx\"\n    )\n    model_dir = os.path.join(project_dir, \"models\")\n    if not os.path.exists(model_dir):\n        os.mkdir(model_dir)\n    \n    tensorrt_engine_path = os.path.join(\n        model_dir, f\"chatglm6b2-bs{batch_size}_no_cache.plan\"\n    )\n    builder = trt.Builder(MyLogger())\n    builder.max_threads = os.cpu_count() // 2\n    config = builder.create_builder_config()\n    profile_list = get_network_profiles(builder)\n    for profile in profile_list:\n        config.add_optimization_profile(profile)\n    # use fp16\n    # config.flags = 1 << int(trt.BuilderFlag.FP16)\n    # disable tf32\n    config.flags = config.flags & ~(1 << int(trt.BuilderFlag.TF32))\n    # use obey precision constraints\n    config.flags = config.flags | (1 << int(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS))\n    # config.set_memory_pool_limit(MemoryPoolType.WORKSPACE, 2 * 1024 * 1024 * 1024)\n    # use preview features\n    preview_features = [\n        # PreviewFeature.PROFILE_SHARING_0806,\n        PreviewFeature.FASTER_DYNAMIC_SHAPES_0805,\n        # PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805\n    ]\n    for feature in preview_features:\n        config.set_preview_feature(feature, True)\n    config.builder_optimization_level = builder_optimization_level\n\n    # use time cache\n    time_cache = b\"\"\n    # read time cache\n    if use_time_cache:\n        if os.path.exists(time_cache_path):\n            time_cache = open(time_cache_path, \"rb\").read()\n            if time_cache is None:\n                time_cache = b\"\"\n                print(stylize(\"read time cache failed\", fg(\"red\")))\n            else:\n                print(stylize(f\"read time cache from {time_cache_path}\", fg(\"green\")))\n        else:\n            time_cache = b\"\"\n            print(stylize(\"time cache will init with empty.\", fg(\"green\")))\n\n        # set time cache\n        cache = config.create_timing_cache(time_cache)\n        config.set_timing_cache(cache, False)\n\n    # load onnx model\n    print(\"loading onnx model from \", onnx_path)\n    # network =  builder.create_network(\n    #     1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n    # )\n    # trt_parser = trt.OnnxParser(network, builder.logger)\n    # onnx_model = onnx.load(onnx_path)\n    # trt_parser.parse(onnx_model)\n\n    _, network, _ = network_from_onnx_path(onnx_path)\n\n    network = get_network_definition(network)\n    print(\"=============tensorRT inference config =====================\")\n    if builder_optimization_level == 3:\n        print(\"==tensorRT engine begin compile, maybe you need wait 10-25 minute ==\")\n    elif builder_optimization_level == 5:\n        print(\"==tensorRT engine begin compile, maybe you need wait 30-60 minute ==\")\n    else:\n        print(\"==tensorRT engine begin compile, maybe you need wait a moment ==\")\n\n    # trt_engine = engine_from_network(\n    #     (trt_builder, network, onnx_parser),\n    #     trt_inference_config\n    # )\n    serialized_engine = builder.build_serialized_network(network, config)\n\n    if serialized_engine is not None:\n        # \u4fdd\u5b58\u5f15\u64ce\u5230\u6587\u4ef6\n        with open(tensorrt_engine_path, \"wb\") as f:\n            f.write(serialized_engine)\n        # save_engine(trt_engine, tensorrt_engine_path)\n        print(\"==tensorRT engine compile done==\")\n    else:\n        raise RuntimeError(\"build engine failed\")\n\n    # save time cache\n    if use_time_cache and not os.path.exists(time_cache_path):\n        time_cache = config.get_timing_cache()\n        if time_cache is not None:\n            time_cache_data = time_cache.serialize()\n            open(time_cache_path, \"wb\").write(time_cache_data)\n            print(\n                stylize(\n                    \"save time cache to {}\".format(time_cache_path),\n                    fg(\"green\")\n                )\n            )"]}
{"filename": "tensorrt_export/onnx2trt_with_cache.py", "chunked_list": ["import tensorrt as trt\nimport os\nimport time\nfrom colored import fg, stylize\nimport json\n# import onnx\nfrom polygraphy.backend.trt import network_from_onnx_path\nfrom itertools import tee\nfrom tensorrt import MemoryPoolType, PreviewFeature\n", "from tensorrt import MemoryPoolType, PreviewFeature\n\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\noutput_dir = os.path.join(project_dir, \"output\")\ntime_cache_path = os.path.join(output_dir, \"fp16_with_cache.cache\")\n\n# default is 1, maybe you can try 2, 4, 8, 16\nbatch_size = 1\nuse_time_cache = True", "batch_size = 1\nuse_time_cache = True\nmax_input_length = 2048\nopt_input_length = max_input_length // 2\n# if use force use fp16, may reduce the accuracy and memory usage\nforce_use_fp16 = False\n# default 3, max 5, 5 is the best but need more GPU memory and time\nbuilder_optimization_level = 3\n# lower memory GPU can try this option with True \\\n# it can use CPU memory/CPU compute to run some layers, but may reduce the speed", "# lower memory GPU can try this option with True \\\n# it can use CPU memory/CPU compute to run some layers, but may reduce the speed\nall_gpu_fallback = False\n\nif batch_size > 1 and builder_optimization_level != 5:\n    raise Exception(\"batch size > 1, please use builder_optimization_level = 5\")\n\n\nclass MyLogger(trt.ILogger):\n    def __init__(self):\n        trt.ILogger.__init__(self)\n\n    def log(self, severity, msg):\n        if severity == trt.Logger.ERROR:\n            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.Logger.WARNING:\n            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # \u9ec4\u8272\u5b57\u4f53\n        elif severity == trt.Logger.INTERNAL_ERROR:\n            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.Logger.INFO:\n            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # \u7eff\u8272\u5b57\u4f53\n        elif severity == trt.Logger.VERBOSE:\n            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # \u84dd\u8272\u5b57\u4f53\n        else:\n            print(\"[UNKNOWN] \" + msg)", "class MyLogger(trt.ILogger):\n    def __init__(self):\n        trt.ILogger.__init__(self)\n\n    def log(self, severity, msg):\n        if severity == trt.Logger.ERROR:\n            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.Logger.WARNING:\n            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # \u9ec4\u8272\u5b57\u4f53\n        elif severity == trt.Logger.INTERNAL_ERROR:\n            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # \u7ea2\u8272\u5b57\u4f53\n        elif severity == trt.Logger.INFO:\n            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # \u7eff\u8272\u5b57\u4f53\n        elif severity == trt.Logger.VERBOSE:\n            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # \u84dd\u8272\u5b57\u4f53\n        else:\n            print(\"[UNKNOWN] \" + msg)", "\n\ndef get_network_profiles(trt_builder, num_layers=28):\n    # ----profile2 when past_key_values is not None----\n    profile2 = trt_builder.create_optimization_profile()\n    profile2.set_shape(\n        \"input_ids\",\n        (1, 1),\n        (batch_size, opt_input_length),\n        (batch_size, max_input_length),\n    )\n    profile2.set_shape(\n        \"position_ids\",\n        (1, 1),\n        (batch_size, opt_input_length),\n        (batch_size, max_input_length),\n    )\n    for layer_idx in range(num_layers):\n        input_names = [\n            f\"past_key_values.{layer_idx}.key\",\n            f\"past_key_values.{layer_idx}.value\"\n        ]\n        for name in input_names:\n            profile2.set_shape(\n                name,\n                (0, 1, 2, 128),\n                (opt_input_length - 1, batch_size, 2, 128),\n                (max_input_length - 1, batch_size, 2, 128),\n            )\n    profiles = [profile2]\n    return profiles", "\n\ndef get_network_definition(trt_network):\n    def pairwise(iterable):\n        a, b = tee(iterable)\n        next(b, None)\n        return zip(a, b)\n    layer_type_set = set() \n    num_layers = trt_network.num_layers\n    indices = list(range(num_layers))\n    for i, i_next in pairwise(indices):\n        layer = trt_network.get_layer(i)\n        next_layer = trt_network.get_layer(i_next)\n\n        if not all([\n            layer.get_output(i).is_execution_tensor\n            for i in range(layer.num_outputs)\n        ]):\n            continue\n\n        if layer.get_output_type(0) != trt.float32:\n            continue\n        layer_type_set.add(str(layer.type))\n        if layer.type == trt.LayerType.ELEMENTWISE and next_layer.type == trt.LayerType.REDUCE:\n            layer.__class__ = getattr(trt, \"IElementWiseLayer\")\n            if layer.op == trt.ElementWiseOperation.POW:\n                layer.precision = trt.float32\n                layer.set_output_type(0, trt.float32)\n\n            next_layer.precision = trt.float32\n            next_layer.set_output_type(0, trt.float32)\n        #else:\n        #    layer.precision = trt.DataType.HALF\n    layer_type_path = os.path.join(output_dir, \"layer_type.json\")\n    with open(layer_type_path, \"wt\") as f1:\n        json.dump(list(layer_type_set), f1, indent=4)\n    return trt_network", "\n\nif __name__ == \"__main__\":\n    onnx_path = os.path.join(output_dir, \"onnx_output\", \"chatglm2_6b.onnx\")\n    model_dir = os.path.join(project_dir, \"models\")\n    if not os.path.exists(model_dir):\n        os.mkdir(model_dir)\n    \n    tensorrt_engine_path = os.path.join(\n        model_dir,\n        f\"chatglm6b2-bs{batch_size}_with_cache.plan\"\n    )\n    builder = trt.Builder(MyLogger())\n    builder.max_threads = os.cpu_count() // 2\n    config = builder.create_builder_config()\n    profile_list = get_network_profiles(builder)\n    for profile in profile_list:\n        config.add_optimization_profile(profile)\n    # use fp16\n    config.flags = 1 << int(trt.BuilderFlag.FP16)\n    # disable tf32\n    config.flags = config.flags & ~(1 << int(trt.BuilderFlag.TF32))\n    # use obey precision constraints\n    config.flags = config.flags | (1 << int(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS))\n    # config.set_memory_pool_limit(MemoryPoolType.WORKSPACE, 2 * 1024 * 1024 * 1024)\n\n    # use prewview features\n    preview_features = [\n        # PreviewFeature.PROFILE_SHARING_0806,\n        PreviewFeature.FASTER_DYNAMIC_SHAPES_0805,\n        # PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805\n    ]\n    for feature in preview_features:\n        config.set_preview_feature(feature, True)\n    config.builder_optimization_level = builder_optimization_level\n\n    # use time cache\n    time_cache = b\"\"\n    # read time cache\n    if use_time_cache:\n        if os.path.exists(time_cache_path):\n            time_cache = open(time_cache_path, \"rb\").read()\n            if time_cache is None:\n                time_cache = b\"\"\n                print(stylize(\"read time cache failed\", fg(\"red\")))\n            else:\n                print(stylize(f\"read time cache from {time_cache_path}\", fg(\"green\")))\n        else:\n            time_cache = b\"\"\n            print(stylize(\"time cache will init with empty.\", fg(\"green\")))\n\n        # set time cache\n        cache = config.create_timing_cache(time_cache)\n        config.set_timing_cache(cache, False)\n\n    # load onnx model\n    print(\"loading onnx model from \", onnx_path)\n    # network =  builder.create_network(\n    #     1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n    # )\n    # trt_parser = trt.OnnxParser(network, builder.logger)\n    # onnx_model = onnx.load(onnx_path)\n    # trt_parser.parse(onnx_model)\n\n    _, network, _ = network_from_onnx_path(onnx_path)\n\n    network = get_network_definition(network)\n    print(\"=============tensorRT inference config =====================\")\n    if builder_optimization_level == 3:\n        print(\"==tensorRT engine begin compile, maybe you need wait 10-25 minute ==\")\n    elif builder_optimization_level == 5:\n        print(\"==tensorRT engine begin compile, maybe you need wait 30-60 minute ==\")\n    else:\n        print(\"==tensorRT engine begin compile, maybe you need wait a moment ==\")\n\n    # trt_engine = engine_from_network(\n    #     (trt_builder, network, onnx_parser),\n    #     trt_inference_config\n    # )\n    serialized_engine = builder.build_serialized_network(network, config)\n\n    if serialized_engine is not None:\n        # \u4fdd\u5b58\u5f15\u64ce\u5230\u6587\u4ef6\n        with open(tensorrt_engine_path, \"wb\") as f:\n            f.write(serialized_engine)\n        # save_engine(trt_engine, tensorrt_engine_path)\n        print(\"==tensorRT engine compile done==\")\n    else:\n        raise RuntimeError(\"build engine failed\")\n\n    # save time cache\n    if use_time_cache and not os.path.exists(time_cache_path):\n        time_cache = config.get_timing_cache()\n        if time_cache is not None:\n            time_cache_data = time_cache.serialize()\n            open(time_cache_path, \"wb\").write(time_cache_data)\n            print(stylize(f\"save time cache to {time_cache_path}\", fg(\"green\")))"]}
{"filename": "tensorrt_export/test_data_inputs.py", "chunked_list": ["import os\nimport torch\nfrom polygraphy.json import save_json\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\noutput_dir = os.path.join(project_dir, \"output\")\ninput_path1 = os.path.join(output_dir, \"pt_input1.pt\")\noutput_path1 = os.path.join(output_dir, \"pt_output1.pt\")\ninput_dict = torch.jit.load(input_path1)\noutput_dict = torch.jit.load(output_path1)", "input_dict = torch.jit.load(input_path1)\noutput_dict = torch.jit.load(output_path1)\ninput_ids = input_dict.input_ids.numpy()\nposition_ids = input_dict.position_ids.numpy()\n\n\ndef load_data():\n    for _ in range(5):\n        yield {\"input_ids\": input_ids, \"position_ids\": position_ids}\n", "\n\ninput_data = list(load_data())\nsave_json(input_data, \"custom_inputs.json\", description=\"custom inputs\")"]}
{"filename": "tensorrt_export/read_trt_profile.py", "chunked_list": ["import tensorrt as trt\nimport os\n\n\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\nmodel_dir = os.path.join(project_dir, \"models\")\ntrt_enginge_path = os.path.join(model_dir, \"chatglm6b-bs1-18.5G.plan\")\n\n# Load TensorRT Engine\nwith open(trt_enginge_path, 'rb') as f, trt.Runtime(trt.Logger(trt.Logger.WARNING)) as runtime:\n    engine = runtime.deserialize_cuda_engine(f.read())", "\n# Load TensorRT Engine\nwith open(trt_enginge_path, 'rb') as f, trt.Runtime(trt.Logger(trt.Logger.WARNING)) as runtime:\n    engine = runtime.deserialize_cuda_engine(f.read())\n\n# print input and output\nprint(\"Inputs and Output:\")\nn_io = engine.num_io_tensors\ntensor_names = []\ntensor_modes = []\nfor i in range(n_io):\n    tensor_name = engine.get_tensor_name(i)\n    tensor_names.append(tensor_name)\n    tensor_mode = engine.get_tensor_mode(tensor_name)\n    tensor_modes.append(tensor_mode)\n    tensor_shape = engine.get_tensor_shape(tensor_name)\n    if tensor_mode == trt.TensorIOMode.INPUT:\n        print(f\"input_name:'{tensor_name}', input_shape: {tensor_shape}\", )\n    else:\n        print(f\"output_name:'{tensor_name}', input_shape: {tensor_shape}\", )", "tensor_names = []\ntensor_modes = []\nfor i in range(n_io):\n    tensor_name = engine.get_tensor_name(i)\n    tensor_names.append(tensor_name)\n    tensor_mode = engine.get_tensor_mode(tensor_name)\n    tensor_modes.append(tensor_mode)\n    tensor_shape = engine.get_tensor_shape(tensor_name)\n    if tensor_mode == trt.TensorIOMode.INPUT:\n        print(f\"input_name:'{tensor_name}', input_shape: {tensor_shape}\", )\n    else:\n        print(f\"output_name:'{tensor_name}', input_shape: {tensor_shape}\", )", "\n\nprint(\"Profile configuration:\")\nfor i in range(engine.num_optimization_profiles):\n    print('=' * 20) \n    print(\" - profile index: \", i)\n    for j in range(n_io):\n        tensor_name = tensor_names[j]\n        tensor_mode = tensor_modes[j]\n        if tensor_mode == trt.TensorIOMode.INPUT:\n            # profile = engine.get_profile_shape(i, j)\n            profile = engine.get_tensor_profile_shape(tensor_name, i)\n            print(\"   - input name: \", tensor_name)\n            print(\"     min shape: \", profile[0])\n            print(\"     opt shape: \", profile[1])\n            print(\"     max shape: \", profile[2])", ""]}
{"filename": "tensorrt_export/trt_check_with_past.py", "chunked_list": ["import os\nimport sys\nimport torch\nfrom colored import stylize, fg\n\n\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\nsys.path.append(project_dir)\n", "sys.path.append(project_dir)\n\n\n# from kernel.pykernel_no_past import KernelNoPast\nfrom kernel.pykernel_with_past import KernelWithPast\n\n\n\ndef check_value(pre_value: torch.Tensor, true_value: torch.Tensor, diff=1e-3):\n    if pre_value.shape != true_value.shape:\n        raise Exception(\"compare shape must be same!\")\n    max_diff = (pre_value - true_value).abs_().max().item()\n    if max_diff > diff:\n        print(stylize(f\"compare diff failed, diff is {max_diff}\", fg(\"red\")))\n    else:\n        print(stylize(\"compare diff OK!\", fg(\"green\")))\n    return max_diff", "def check_value(pre_value: torch.Tensor, true_value: torch.Tensor, diff=1e-3):\n    if pre_value.shape != true_value.shape:\n        raise Exception(\"compare shape must be same!\")\n    max_diff = (pre_value - true_value).abs_().max().item()\n    if max_diff > diff:\n        print(stylize(f\"compare diff failed, diff is {max_diff}\", fg(\"red\")))\n    else:\n        print(stylize(\"compare diff OK!\", fg(\"green\")))\n    return max_diff\n", "\n\ndef main():\n    assert torch.cuda.is_available(), print(\"you must has cuda to run TensorRT\")\n    output_dir = os.path.join(project_dir, \"output\")\n    model_dir = os.path.join(project_dir, \"models\")\n    engine_path1 = os.path.join(model_dir, \"chatglm6b2-bs1_with_cache.plan\")\n    input_path = os.path.join(output_dir, \"pt_input2.pt\")\n    output_path = os.path.join(output_dir, \"pt_output2.pt\")\n    device = torch.device(\"cuda:0\")\n    input_dict = torch.jit.load(input_path)\n    batch_size = 1\n    num_layers = 28\n    output_dict = torch.jit.load(output_path)\n    input_ids: torch.Tensor = input_dict.input_ids.int().to(device)\n    position_ids: torch.Tensor = input_dict.position_ids.int().to(device)\n    input_tensors = [input_ids, position_ids]\n    for i in range(num_layers):\n        input_names = [\n            f\"past_key_values.{i}.key\",\n            f\"past_key_values.{i}.value\"\n        ]\n        for name in input_names:\n            one_key_value = getattr(input_dict, name).to(device)\n            input_tensors.append(one_key_value)\n    kernel = KernelWithPast(engine_path1, batch_size, num_layers)\n    output_tensors = kernel.forward(tuple(input_tensors))\n    # compare output\n    max_diff_ = 0\n    # compare logits\n    logits = output_dict.logits.to(device)\n    pred_logits = output_tensors[-1]\n    logits_diff = check_value(logits, pred_logits)\n    print(\"=\" * 20)\n    print(\"compare logits\")\n    if logits_diff > max_diff_:\n        max_diff_ = logits_diff\n    # compare past key values\n    for i in range(num_layers):\n        present_key_name = f\"present_key_values.{i}.key\"\n        present_value_name = f\"present_key_values.{i}.value\"\n        true_present_key = getattr(output_dict, present_key_name).to(device)\n        true_present_value = getattr(output_dict, present_value_name).to(device)\n        pre_present_key = output_tensors[i * 2]\n        pre_present_value = output_tensors[i * 2 + 1]\n        print(\"=\" * 20)\n        print(\"compare: \", present_key_name)\n        temp_diff = check_value(pre_present_key, true_present_key)\n        if temp_diff > max_diff_:\n            max_diff_ = temp_diff\n\n        print(\"=\" * 20)\n        print(\"compare: \", present_value_name)\n        temp_diff = check_value(pre_present_value, true_present_value)\n        if temp_diff > max_diff_:\n            max_diff_ = temp_diff\n    print(f\"max diff is {max_diff_}\")", "\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n"]}
{"filename": "tensorrt_export/test.py", "chunked_list": ["import onnx\nfrom onnx import helper, TensorProto\nimport onnxruntime as ort\nimport os\nimport numpy as np\n\n\nproject_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nonnx_path = os.path.join(project_dir, \"output\", \"onnx_output\", \"chatglm_6b.onnx\")\nnew_onnx_path = os.path.join(project_dir, \"output\", \"new_onnx_output\", \"chatglm_6b.onnx\")", "onnx_path = os.path.join(project_dir, \"output\", \"onnx_output\", \"chatglm_6b.onnx\")\nnew_onnx_path = os.path.join(project_dir, \"output\", \"new_onnx_output\", \"chatglm_6b.onnx\")\n# \u52a0\u8f7d ONNX \u6a21\u578b\nonnx_model = onnx.load(onnx_path)\n\n# \u83b7\u53d6\u6240\u6709\u8282\u70b9\u540d\u79f0\nnode_names = set(node.name for node in onnx_model.graph.node)\n\n# \u83b7\u53d6\u6240\u6709\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u540d\u79f0\ninput_names = set(input.name for input in onnx_model.graph.input)", "# \u83b7\u53d6\u6240\u6709\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u540d\u79f0\ninput_names = set(input.name for input in onnx_model.graph.input)\noutput_names = set(output.name for output in onnx_model.graph.output)\n\n# \u5c06\u6240\u6709\u8282\u70b9\u6807\u8bb0\u4e3a\u8f93\u51fa\nfor node in onnx_model.graph.node:\n    for output in node.output:\n        # \u5982\u679c\u8f93\u51fa\u5f20\u91cf\u4e0d\u662f\u8f93\u5165\u5f20\u91cf\uff0c\u5219\u5c06\u5176\u6807\u8bb0\u4e3a\u8f93\u51fa\n        if output not in input_names:\n            onnx_model.graph.output.append(helper.make_tensor_value_info(output, TensorProto.UNDEFINED, None))", "\n# \u4fdd\u5b58\u4fee\u6539\u540e\u7684 ONNX \u6a21\u578b\nonnx.save(\n  onnx_model, \n  new_onnx_path,\n  save_as_external_data=True,\n  all_tensors_to_one_file=False\n)\nsess = ort.InferenceSession(new_onnx_path, providers=[\"CPUExecutionProvider\"])\ninput_feed = {", "sess = ort.InferenceSession(new_onnx_path, providers=[\"CPUExecutionProvider\"])\ninput_feed = {\n    \"input_ids\": np.random.randint(0, 100, size=(1, 512)).astype(np.int64),\n    \"position_ids\": np.random.randint(0, 100, size=(1, 2, 512)).astype(np.int64),\n    \"attention_mask\": np.random.randint(0, 1, size=(1, 1, 512, 512)).astype(np.bool_),\n    \"past_key_values.0.decoder.key\": np.random.randint(0, 100, size=(1, 1, 32, 128)).astype(np.float32),\n    \"past_key_values.0.decoder.value\": np.random.randint(0, 100, size=(1, 1, 32, 128)).astype(np.float32),\n}\n\noutput = sess.run(None, input_feed)", "\noutput = sess.run(None, input_feed)\nprint(output)"]}
{"filename": "tensorrt_export/onnx_trt_compare_fp32.py", "chunked_list": ["#!/usr/bin/env python3\n# Template auto-generated by polygraphy [v0.47.1] on 06/05/23 at 12:36:31\n# code gen with onnx_trt_compare.sh\n# but i edit the code to make it more readable\nimport tensorrt as trt\nimport os\nimport shutil\nimport numpy as np\nfrom polygraphy.logger import G_LOGGER\nG_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}", "from polygraphy.logger import G_LOGGER\nG_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}\nfrom colored import stylize, fg\nfrom polygraphy import constants\nimport onnx\nfrom polygraphy.backend.common import SaveBytes\nfrom polygraphy.backend.onnx import modify_outputs, onnx_from_path, ModifyOutputs\nfrom polygraphy.backend.onnxrt import OnnxrtRunner, SessionFromOnnx\nfrom polygraphy.backend.trt import CreateConfig as CreateTrtConfig, EngineBytesFromNetwork, EngineFromBytes, ModifyNetworkOutputs, NetworkFromOnnxPath, Profile, TrtRunner\nfrom polygraphy.common import TensorMetadata", "from polygraphy.backend.trt import CreateConfig as CreateTrtConfig, EngineBytesFromNetwork, EngineFromBytes, ModifyNetworkOutputs, NetworkFromOnnxPath, Profile, TrtRunner\nfrom polygraphy.common import TensorMetadata\nfrom polygraphy.comparator import Comparator, CompareFunc, DataLoader\nfrom polygraphy.exception import PolygraphyException\nfrom polygraphy.backend.trt import network_from_onnx_path\n\n\n\n# --dir info--\nnow_dir = os.path.dirname(os.path.abspath(__file__))", "# --dir info--\nnow_dir = os.path.dirname(os.path.abspath(__file__))\nproject_dir = os.path.dirname(now_dir)\nimport sys\nsys.path.append(project_dir)\nfrom tensorrt_export.onnx2trt_no_cache import (\n    get_network_profiles,\n    MyLogger,\n)\n", ")\n\n\noutput_dir = os.path.join(project_dir, \"output\")\nnew_onnx_dir = os.path.join(output_dir, \"onnx_output_no_cache_new\")\nif not os.path.exists(new_onnx_dir):\n    os.mkdir(new_onnx_dir)\nelse:\n    for file in os.listdir(new_onnx_dir):\n        os.remove(os.path.join(new_onnx_dir, file))", "new_onnx_dir2 = os.path.join(output_dir, \"onnx_output_no_cache_new2\")\nif not os.path.exists(new_onnx_dir2):\n    os.mkdir(new_onnx_dir2)\nelse:\n    for file in os.listdir(new_onnx_dir2):\n        os.remove(os.path.join(new_onnx_dir2, file))\n\nonnx_path = os.path.join(output_dir, \"onnx_output_no_cache\", \"chatglm2_6b.onnx\")\nnew_onnx_path = os.path.join(new_onnx_dir, \"chatglm2_6b.onnx\")\nnew_onnx_path2 = os.path.join(new_onnx_dir2, \"chatglm2_6b.onnx\")", "new_onnx_path = os.path.join(new_onnx_dir, \"chatglm2_6b.onnx\")\nnew_onnx_path2 = os.path.join(new_onnx_dir2, \"chatglm2_6b.onnx\")\nmodel_dir = os.path.join(project_dir, \"models\")\ntrt_model_path = os.path.join(model_dir, \"model-no-cache-FP32-MarkAll.plan\")\ntime_cache_path = os.path.join(output_dir, \"fp32_markAll_no_cache.cache\")\nuse_time_cache = True\nnum_layers = 1\n\n\n# Data Loader", "\n# Data Loader\ndtype = np.dtype(np.int32)\ndata_loader = DataLoader(\n    input_metadata=TensorMetadata()\n    .add('input_ids', dtype=dtype, shape=(1, 512))\n    .add('position_ids', dtype=np.int32, shape=(1, 512))\n)\n# load onnx\n", "# load onnx\n\nprint(\"loading onnx model from\", onnx_path)\nonnx_model = onnx_from_path(onnx_path)\n# this layer will output None in onnxrt\nbool_tensor_list = [\"/transformer/encoder/layers.0/mlp/Sigmoid_output_0\"]\nfor node in onnx_model.graph.node:\n    # print(node.name, node.op_type)\n    # this layer is a bool tensor, it will cause error when run TensorRT engine\n    if node.op_type == \"Equal\":\n        print(\"find bool\", node.name)\n        bool_tensor_list.extend(node.output)", "input_list = onnx_model.graph.input\ninput_names = [i.name for i in input_list]\noutput_list = onnx_model.graph.output\noutput_names = [o.name for o in output_list]\nexclude_outputs = input_names + bool_tensor_list\n# mark all layers as output for onnx model\n# warning this will make the onnx model output all layers with no type and no shape\nnew_onnx_model = modify_outputs(\n    model=onnx_model,\n    outputs=constants.MARK_ALL,", "    model=onnx_model,\n    outputs=constants.MARK_ALL,\n    exclude_outputs=exclude_outputs,\n)\nnew_output_list = new_onnx_model.graph.output\nnew_output_names = [o.name for o in new_output_list]\nonnx_input_num = len(new_onnx_model.graph.input)\nonnx_output_num = len(new_onnx_model.graph.output)\nprint(\"onnx input num:\", onnx_input_num, \"onnx output num:\", onnx_output_num)\nonnx.save_model(", "print(\"onnx input num:\", onnx_input_num, \"onnx output num:\", onnx_output_num)\nonnx.save_model(\n    new_onnx_model,\n    new_onnx_path,\n    save_as_external_data=True,\n    all_tensors_to_one_file=False\n)\n# load onnx_runtime\nbuild_onnx_rt_session = SessionFromOnnx(new_onnx_path)\n# get onnx runtime output tensor info(name, type, shape)", "build_onnx_rt_session = SessionFromOnnx(new_onnx_path)\n# get onnx runtime output tensor info(name, type, shape)\nsess = build_onnx_rt_session()\nbool_tensor_list = []\nsess_output = sess.get_outputs()\nfor node in sess_output:\n    if node.type == \"tensor(float)\":\n        dtype = onnx.TensorProto.FLOAT\n    elif node.type == \"tensor(bool)\":\n        print(\"find bool\", node.name)\n        bool_tensor_list.append(node.name)\n        dtype = onnx.TensorProto.BOOL\n    elif node.type == \"tensor(int64)\":\n        dtype = onnx.TensorProto.INT64\n    else:\n        print(\"unknown dtype:\", node.type)\n        raise ValueError\n    output_tensor = onnx.helper.make_tensor_value_info(\n        node.name, dtype, None\n    )\n    # replace the output tensor\n    for i, vi in enumerate(new_onnx_model.graph.output):\n        if vi.name == node.name:\n            new_onnx_model.graph.output[i].CopyFrom(output_tensor)", "# save again\nfor file in os.listdir(new_onnx_dir):\n    shutil.copy(os.path.join(new_onnx_dir, file), new_onnx_dir2)\nonnx.save_model(\n    new_onnx_model,\n    new_onnx_path2,\n    save_as_external_data=True,\n    all_tensors_to_one_file=False\n)\nprint(\"===========onnx model loaded=========================\")", ")\nprint(\"===========onnx model loaded=========================\")\n# build trt engine\nprint(\"===========building trt engine=========================\")\n# prepare trt builder\nbuilder = trt.Builder(MyLogger())\nbuilder.max_threads = os.cpu_count() // 2\nconfig = builder.create_builder_config()\nconfig.flags = config.flags & ~(1 << int(trt.BuilderFlag.TF32))\n# read time cache\nif use_time_cache:\n    if os.path.exists(time_cache_path):\n        time_cache = open(time_cache_path, \"rb\").read()\n        if time_cache is None:\n            time_cache = b\"\"\n            print(stylize(\"read time cache failed\", fg(\"red\")))\n        else:\n            print(stylize(f\"read time cache from {time_cache_path}\", fg(\"green\")))\n    else:\n        time_cache = b\"\"\n        print(stylize(\"time cache will init with empty.\", fg(\"green\")))\n\n    # set time cache\n    cache = config.create_timing_cache(time_cache)\n    config.set_timing_cache(cache, False)", "config.flags = config.flags & ~(1 << int(trt.BuilderFlag.TF32))\n# read time cache\nif use_time_cache:\n    if os.path.exists(time_cache_path):\n        time_cache = open(time_cache_path, \"rb\").read()\n        if time_cache is None:\n            time_cache = b\"\"\n            print(stylize(\"read time cache failed\", fg(\"red\")))\n        else:\n            print(stylize(f\"read time cache from {time_cache_path}\", fg(\"green\")))\n    else:\n        time_cache = b\"\"\n        print(stylize(\"time cache will init with empty.\", fg(\"green\")))\n\n    # set time cache\n    cache = config.create_timing_cache(time_cache)\n    config.set_timing_cache(cache, False)", "profile_list = get_network_profiles(builder, num_layers=num_layers)\nfor profile in profile_list:\n    config.add_optimization_profile(profile)\n\n_b, network, _p = NetworkFromOnnxPath(new_onnx_path2)()\n\n# network = network_from_onnx_path(onnx_path)\n# set_network_outputs = ModifyNetworkOutputs(\n#     network=network,\n#     outputs=constants.MARK_ALL,", "#     network=network,\n#     outputs=constants.MARK_ALL,\n#     exclude_outputs=bool_tensor_list\n# )\n# wo_b, network, _p = set_network_outputs()\nnetwork_input_number = network.num_inputs\nnetwork_output_number = network.num_outputs\nprint(\"TensorRT input num:\", network_input_number, \"TensorRT output num:\", network_output_number)\nserialized_engine = builder.build_serialized_network(network, config)\nif serialized_engine is not None:\n    with open(trt_model_path, \"wb\") as f:\n        f.write(serialized_engine)\n    # save_engine(trt_engine, tensorrt_engine_path)\n    print(\"==tensorRT engine compile done==\")", "serialized_engine = builder.build_serialized_network(network, config)\nif serialized_engine is not None:\n    with open(trt_model_path, \"wb\") as f:\n        f.write(serialized_engine)\n    # save_engine(trt_engine, tensorrt_engine_path)\n    print(\"==tensorRT engine compile done==\")\n\n# save time cache\nif use_time_cache and not os.path.exists(time_cache_path):\n    time_cache = config.get_timing_cache()\n    if time_cache is not None:\n        time_cache_data = time_cache.serialize()\n        open(time_cache_path, \"wb\").write(time_cache_data)\n        print(\n            stylize(\n                \"save time cache to {}\".format(time_cache_path),\n                fg(\"green\")\n            )\n        )", "if use_time_cache and not os.path.exists(time_cache_path):\n    time_cache = config.get_timing_cache()\n    if time_cache is not None:\n        time_cache_data = time_cache.serialize()\n        open(time_cache_path, \"wb\").write(time_cache_data)\n        print(\n            stylize(\n                \"save time cache to {}\".format(time_cache_path),\n                fg(\"green\")\n            )\n        )", "\n# profiles = [\n#     Profile().add('input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value', min=[0, 1, 32, 128], opt=[0, 1, 32, 128], max=[0, 1, 32, 128])\n# ]\n# create_trt_config = CreateTrtConfig(memory_pool_limits=2 * (1024 ** 3), profiles=profile_list)\n# build_engine = EngineBytesFromNetwork(set_network_outputs, config=create_trt_config)\n# save_engine_bytes = SaveBytes(build_engine, path=trt_model_path)\nsave_engine_bytes = SaveBytes(serialized_engine, path=trt_model_path)\ndeserialize_engine = EngineFromBytes(save_engine_bytes)\nprint(\"===========trt engine build OK=========================\")", "deserialize_engine = EngineFromBytes(save_engine_bytes)\nprint(\"===========trt engine build OK=========================\")\n\n# Runners\nrunners = [\n    OnnxrtRunner(build_onnx_rt_session),\n    TrtRunner(deserialize_engine),\n]\n\n# Runner Execution", "\n# Runner Execution\nresults = Comparator.run(runners, data_loader=data_loader)\n\nsuccess = True\n# Accuracy Comparison\ncompare_func = CompareFunc.simple(rtol={'': 0.001}, atol={'': 0.001})\nsuccess &= bool(Comparator.compare_accuracy(results, compare_func=compare_func))\n\n# Report Results\nif not success:\n    raise PolygraphyException('FAILED')", "\n# Report Results\nif not success:\n    raise PolygraphyException('FAILED')\n"]}
