{"filename": "llm/run.py", "chunked_list": ["\"\"\"Run the LLM finetuning and deployment pipeline.\"\"\"\nimport click\nfrom zenml.logger import get_logger\n\nfrom steps.finetune_model import finetune_model\nfrom steps.get_hg_model import get_huggingface_model\nfrom steps.download_data_step import download_dataset\nfrom steps.convert_to_hg_dataset_step import convert_to_hg_dataset\nfrom steps.preprocess_hg_dataset_step import preprocess_dataset\nfrom steps.deploy_model_step import seldon_llm_custom_deployment", "from steps.preprocess_hg_dataset_step import preprocess_dataset\nfrom steps.deploy_model_step import seldon_llm_custom_deployment\nfrom steps.fetch_trained_model_step import fetch_model\n\nfrom pipelines.llm_deployment_pipeline import llm_deployment_pipeline\nfrom pipelines.llm_pipeline import llm_pipeline\n\nlogger = get_logger(__name__)\n\n\ndef run_llm_pipeline():\n    \"\"\"Run all steps in the llm finetuning pipeline.\"\"\"\n    pipeline = llm_pipeline(\n        download_dataset(),\n        convert_to_hg_dataset(),\n        get_huggingface_model(),\n        preprocess_dataset(),\n        finetune_model(),\n    )\n    pipeline.run(config_path=\"pipelines/config_llm_pipeline.yaml\")", "\n\ndef run_llm_pipeline():\n    \"\"\"Run all steps in the llm finetuning pipeline.\"\"\"\n    pipeline = llm_pipeline(\n        download_dataset(),\n        convert_to_hg_dataset(),\n        get_huggingface_model(),\n        preprocess_dataset(),\n        finetune_model(),\n    )\n    pipeline.run(config_path=\"pipelines/config_llm_pipeline.yaml\")", "\n\ndef run_llm_deploy_pipeline():\n    \"\"\"Run all steps in llm deploy pipeline.\"\"\"\n    pipeline = llm_deployment_pipeline(fetch_model(), seldon_llm_custom_deployment)\n    pipeline.run(config_path=\"pipelines/config_llm_deployment_pipeline.yaml\")\n\n\n@click.command()\n@click.option(\"--train\", \"-t\", is_flag=True, help=\"Run training pipeline\")", "@click.command()\n@click.option(\"--train\", \"-t\", is_flag=True, help=\"Run training pipeline\")\n@click.option(\"--deploy\", \"-d\", is_flag=True, help=\"Run the deployment pipeline\")\ndef main(train: bool, deploy: bool):\n    \"\"\"Run all pipelines.\n\n    args:\n        train (bool): Flag for running the training pipeline.\n        deploy (bool): Flag for running the deployment pipeline.\n    \"\"\"\n    if train:\n        \"\"\"Run all pipelines.\"\"\"\n        logger.info(\"Running LLM fine-tuning pipeline.\")\n        run_llm_pipeline()\n\n    if deploy:\n        logger.info(\"Running LLM deployment pipeline.\")\n        run_llm_deploy_pipeline()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "llm/steps/finetune_model.py", "chunked_list": ["\"\"\"LLM finetuning step.\"\"\"\nfrom typing import Tuple\n\nimport torch\nfrom zenml.steps import BaseParameters, step, Output\nfrom transformers import (\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    PreTrainedTokenizerBase,", "    Seq2SeqTrainer,\n    PreTrainedTokenizerBase,\n    PreTrainedModel,\n)\nfrom datasets import Dataset\n\n\nclass TuningParameters(BaseParameters):\n    \"\"\"Fine-tuning step parameters.\"\"\"\n\n    # Learning rate\n    learning_rate: float\n\n    # Weight decay\n    weight_decay: float\n\n    # Use CUDA for training\n    use_cuda: bool\n\n    # Batch size per device for training\n    per_device_train_batch_size: int\n\n    # Batch size per device for evaluation\n    per_device_eval_batch_size: int\n\n    # Number of epochs to run tuning for\n    epochs: int\n\n    # Load the best checkpoint at the end flag\n    load_best_model_at_end: bool", "\n\ndef prepare_training_args(params: TuningParameters) -> Seq2SeqTrainingArguments:\n    \"\"\"Prepare training arguments.\n\n    Args:\n        params (TuningParameters): tuning step parameters\n\n    Returns:\n        Seq2SeqTrainingArguments: training arguments\n    \"\"\"\n    use_cuda = torch.cuda.is_available() if params.use_cuda else False\n\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=\"model\",  # can be hardcoded to anything, since we are not using this directory afterwards\n        learning_rate=params.learning_rate,\n        evaluation_strategy=\"epoch\",\n        per_device_train_batch_size=params.per_device_train_batch_size,\n        per_device_eval_batch_size=params.per_device_eval_batch_size,\n        weight_decay=params.weight_decay,\n        save_total_limit=1,\n        save_strategy=\"epoch\",\n        num_train_epochs=params.epochs,\n        predict_with_generate=True,\n        no_cuda=not use_cuda,\n        fp16=use_cuda,\n        load_best_model_at_end=params.load_best_model_at_end,\n    )\n    return training_args", "\n\ndef train(\n    tokenizer: PreTrainedTokenizerBase,\n    model: PreTrainedModel,\n    data: Dataset,\n    training_args: Seq2SeqTrainingArguments,\n) -> Tuple[PreTrainedTokenizerBase, PreTrainedModel]:\n    \"\"\"Perform sequence to sequence training.\n\n    Args:\n        tokenizer (PreTrainedTokenizerBase): Huggingface tokenizer to be used in fine-tuning.\n        model (PreTrainedModel): Huggingface pre-trained model to fine-tune\n        data (Dataset): dataset to fine-tune with\n        training_args (Seq2SeqTrainingArguments): training arguments\n\n    Returns:\n        Tuple[PreTrainedTokenizerBase, PreTrainedModel]: fine-tuned tokenizer and model\n\n    Raises:\n        Exception: when tokenizer or model is missing from the trainer\n    \"\"\"\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer, model=model.name_or_path\n    )\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=data[\"train\"],\n        eval_dataset=data[\"test\"],\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n\n    if trainer.tokenizer is None:\n        raise Exception(\"Trainer's tokenizer is None\")\n    if trainer.model is None:\n        raise Exception(\"Trainer's model is None\")\n\n    return trainer.tokenizer, trainer.model", "\n\n@step\ndef finetune_model(\n    params: TuningParameters,\n    tokenizer: PreTrainedTokenizerBase,\n    model: PreTrainedModel,\n    data: Dataset,\n) -> Output(tokenizer=PreTrainedTokenizerBase, model=PreTrainedModel):\n    \"\"\"A step to fine-tune a pre-trained model.\n\n    Args:\n        params (TuningParameters): Tuning parameters\n        tokenizer (PreTrainedTokenizerBase): A pre-trained tokenizer\n        model (PreTrainedModel): A pre-trained model\n        data (Dataset): A dataset to fine-tune with\n\n    Returns:\n        PreTrainedTokenizerBase: a tuned tokenizer\n        PreTrainedModel: a tuned model\n    \"\"\"\n    training_args = prepare_training_args(params)\n    tuned_tokenizer, tuned_model = train(tokenizer, model, data, training_args)\n    return tuned_tokenizer, tuned_model", ""]}
{"filename": "llm/steps/preprocess_hg_dataset_step.py", "chunked_list": ["\"\"\"Preprocess, tokenize and split the huggingface dataset into train/test set.\"\"\"\nfrom functools import partial\n\nfrom zenml.logger import get_logger\nfrom zenml.steps import step, BaseParameters\nfrom datasets import Dataset, DatasetDict\nfrom transformers import BatchEncoding\nfrom transformers import PreTrainedTokenizerBase\n\nlogger = get_logger(__name__)", "\nlogger = get_logger(__name__)\n\n\nclass PreprocessParameters(BaseParameters):\n    \"\"\"Parameters for preprocessing the Huggingface dataset.\"\"\"\n\n    # Prefix to be added to the input (required for T5 LLM family)\n    prefix: str = \"summarize: \"\n\n    # Max length of the input text\n    input_max_length: int = 4096\n\n    # Max length of the target summary\n    target_max_length: int = 512\n\n    # Split ratio for train/test\n    test_size: float = 0.2", "\n\ndef preprocess_function(\n    dataset: Dataset,\n    tokenizer: PreTrainedTokenizerBase,\n    prefix: str,\n    input_max_length: int,\n    target_max_length: int,\n) -> BatchEncoding:\n    \"\"\"Preprocess and tokenize the huggingface dataset.\n\n    Args:\n        dataset (Dataset): Dataset to preprocess and tokenize.\n        tokenizer (str): Huggingface tokenizer.\n        prefix (str): Prefix to add so that T5 model knows this is a summarization task.\n        input_max_length (int): Max length of the input text. Truncate sequences to be no longer than this length.\n        target_max_length (int): Max length of the target summary. Truncate sequences to be no longer than this length.\n\n    Returns:\n        BatchEncoding: Tokenized input and targets.\n    \"\"\"\n    # Preprocess input by adding the prefix so T5 knows this is a summarization task.\n    inputs = [prefix + doc for doc in dataset[\"text\"]]\n\n    # Tokenize input and target\n    model_inputs = tokenizer(inputs, max_length=input_max_length, truncation=True)\n    labels = tokenizer(\n        text_target=dataset[\"summary\"], max_length=target_max_length, truncation=True\n    )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs", "\n\n@step\ndef preprocess_dataset(\n    dataset: Dataset, tokenizer: PreTrainedTokenizerBase, params: PreprocessParameters\n) -> DatasetDict:\n    \"\"\"Preprocess the huggingface dataset.\n\n    Args:\n        dataset (Dataset): Dataset to preprocess, tokenize and split.\n        tokenizer (str): Huggingface tokenizer.\n        params (PreprocessParameters): Parameters for preprocessing the dataset.\n\n    Returns:\n        DatasetDict: Tokenized dataset split into train and test.\n    \"\"\"\n    # Tokenize and preprocess dataset\n    tokenized_data = dataset.map(\n        partial(\n            preprocess_function,\n            tokenizer=tokenizer,\n            prefix=params.prefix,\n            input_max_length=params.input_max_length,\n            target_max_length=params.target_max_length,\n        ),\n        batched=True,\n    )\n\n    # Split into train and test\n    tokenized_data = tokenized_data.train_test_split(test_size=params.test_size)\n\n    logger.info(f\"Number of examples in training set: {len(tokenized_data['train'])}\")\n    logger.info(f\"Number of examples in test set: {len(tokenized_data['test'])}\")\n    return tokenized_data", ""]}
{"filename": "llm/steps/zenml_llm_custom_model.py", "chunked_list": ["# Derived from zenml seldon integration; source : https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/seldon/custom_deployer/zenml_custom_model.py\n\"\"\"Zenml Custom LLM Class\"\"\"\nfrom typing import Any, Dict, List, Union, Optional\nimport subprocess\nimport numpy as np\nimport click\nimport os\n\nfrom zenml.logger import get_logger\nfrom zenml.utils.source_utils import import_class_by_path", "from zenml.logger import get_logger\nfrom zenml.utils.source_utils import import_class_by_path\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nDEFAULT_MODEL_NAME = \"models\"\nDEFAULT_LOCAL_MODEL_DIR = \"/mnt/models\"\n\nlogger = get_logger(__name__)\nArray_Like = Union[np.ndarray, List[Any], str, bytes, Dict[str, Any]]", "logger = get_logger(__name__)\nArray_Like = Union[np.ndarray, List[Any], str, bytes, Dict[str, Any]]\nDEFAULT_PT_MODEL_DIR = \"hf_pt_model\"\nDEFAULT_TOKENIZER_DIR = \"hf_tokenizer\"\n\n\nclass ZenMLCustomLLMModel:\n    \"\"\"Custom model class for ZenML and Seldon for LLM.\n    This class is used to implement a custom model for the Seldon Core integration,\n    which is used as the main entry point for custom code execution.\n\n    Attributes:\n        model_uri: The URI of the model.\n        tokenizer_uri: The URI of the tokenizer.\n        model_name: The name of the model.\n        predict_func: The predict function of the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_uri: str,\n        tokenizer_uri: str,\n        model_name: str,\n        predict_func: str,\n    ):\n        \"\"\"Initializes a ZenMLCustomModel object.\n\n        Args:\n            model_uri (str): The URI of the model.\n            tokenizer_uri (str): The URI of tokenizer.\n            model_name (str): The name of the model.\n            predict_func (str): The predict function of the model.\n        \"\"\"\n        self.name = model_name\n        self.model_uri = model_uri\n        self.tokenizer_uri = tokenizer_uri\n        self.predict_func = import_class_by_path(predict_func)\n        self.model = None\n        self.tokenizer = None\n        self.ready = False\n\n    def load(self) -> bool:\n        \"\"\"Load the model.\n\n        This function loads the model into memory and sets the ready flag to True.\n        The model is loaded using the materializer, by saving the information of\n        the artifact to a file at the preparing time and loading it again at the\n        prediction time by the materializer.\n\n        Returns:\n            bool: True if the model was loaded successfully, False otherwise.\n        \"\"\"\n        try:\n            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n                os.path.join(self.model_uri, DEFAULT_PT_MODEL_DIR)\n            )\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                os.path.join(self.tokenizer_uri, DEFAULT_TOKENIZER_DIR)\n            )\n        except Exception as e:\n            logger.error(\"Failed to load model: {}\".format(e))\n            return False\n        self.ready = True\n        return self.ready\n\n    def predict(\n        self,\n        X: Array_Like,\n        features_names: Optional[List[str]],\n        **kwargs: Any,\n    ) -> Array_Like:\n        \"\"\"Predict the given request.\n\n        The main predict function of the model. This function is called by the\n        Seldon Core server when a request is received. Then inside this function,\n        the user-defined predict function is called.\n\n        Args:\n            X (Array_Like): The request to predict in a dictionary.\n            features_names (Optional[List[str]]): The names of the features.\n            **kwargs (Any): Additional arguments.\n\n        Returns:\n            Array_Like: The prediction dictionary.\n\n        Raises:\n            Exception: If function could not be called.\n            NotImplementedError: If the model is not ready.\n            TypeError: If the request is not a dictionary.\n        \"\"\"\n        if self.predict_func is not None:\n            try:\n                prediction = {\n                    \"predictions\": self.predict_func(self.model, self.tokenizer, X)\n                }\n            except Exception as e:\n                raise Exception(\"Failed to predict: {}\".format(e))\n            if isinstance(prediction, dict):\n                return prediction\n            else:\n                raise TypeError(\n                    f\"Prediction is not a dictionary. Expected dict type but got {type(prediction)}\"\n                )\n        else:\n            raise NotImplementedError(\"Predict function is not implemented\")", "\n\n@click.command()\n@click.option(\n    \"--model_uri\",\n    default=DEFAULT_LOCAL_MODEL_DIR,\n    type=click.STRING,\n    help=\"The directory where the model is stored locally.\",\n)\n@click.option(", ")\n@click.option(\n    \"--tokenizer_uri\",\n    default=DEFAULT_LOCAL_MODEL_DIR,\n    type=click.STRING,\n    help=\"The directory where the model is stored locally.\",\n)\n@click.option(\n    \"--model_name\",\n    default=DEFAULT_MODEL_NAME,", "    \"--model_name\",\n    default=DEFAULT_MODEL_NAME,\n    required=True,\n    type=click.STRING,\n    help=\"The name of the model to deploy.\",\n)\n@click.option(\n    \"--predict_func\",\n    required=True,\n    type=click.STRING,", "    required=True,\n    type=click.STRING,\n    help=\"The path to the custom predict function defined by the user.\",\n)\ndef main(\n    model_name: str, model_uri: str, tokenizer_uri: str, predict_func: str\n) -> None:\n    \"\"\"Main function for the custom model.\n\n    Within the deployment process, the built-in custom deployment step is used to\n    to prepare the Seldon Core deployment with an entry point that calls this script,\n    which then starts a subprocess to start the Seldon server and waits for requests.\n    The following is an example of the entry point:\n    ```\n    entrypoint_command = [\n        \"python\",\n        \"-m\",\n        \"zenml.integrations.seldon.custom_deployer.zenml_custom_model\",\n        \"--model_name\",\n        config.service_config.model_name,\n        \"--predict_func\",\n        config.custom_deploy_parameters.predict_function,\n    ]\n    ```\n    Args:\n        model_name: The name of the model.\n        model_uri: The URI of the model.\n        tokenizer_uri: The URI of tokenizer.\n        predict_func: The path to the predict function.\n    \"\"\"\n    command = [\n        \"seldon-core-microservice\",\n        \"steps.zenml_llm_custom_model.ZenMLCustomLLMModel\",\n        \"--service-type\",\n        \"MODEL\",\n        \"--parameters\",\n        (\n            f'[{{\"name\":\"model_uri\",\"value\":\"{model_uri}\",\"type\":\"STRING\"}},'\n            f'{{\"name\":\"tokenizer_uri\",\"value\":\"{tokenizer_uri}\",\"type\":\"STRING\"}},'\n            f'{{\"name\":\"model_name\",\"value\":\"{model_name}\",\"type\":\"STRING\"}},'\n            f'{{\"name\":\"predict_func\",\"value\":\"{predict_func}\",\"type\":\"STRING\"}}]'\n        ),\n    ]\n    try:\n        subprocess.check_call(command)\n    except subprocess.CalledProcessError as ProcessError:\n        logger.error(\n            f\"Failed to start the seldon-core-microservice process. {ProcessError}\"\n        )\n        return", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "llm/steps/get_hg_model.py", "chunked_list": ["\"\"\"Get Huggingface model.\"\"\"\nfrom zenml.steps import step, Output, BaseParameters\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    PreTrainedTokenizerBase,\n    PreTrainedModel,\n)\nfrom zenml.logger import get_logger\n", "from zenml.logger import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass GetHuggingfaceModelParameters(BaseParameters):\n    \"\"\"Parameters for downloading the huggingface model.\"\"\"\n\n    # Huggingface model name\n    model_name: str", "\n\n@step\ndef get_huggingface_model(\n    params: GetHuggingfaceModelParameters,\n) -> Output(tokenizer=PreTrainedTokenizerBase, model=PreTrainedModel):\n    \"\"\"A step to get Huggingface model from the hub.\n\n    Args:\n        params: step parameters\n\n    Returns:\n        PreTrainedTokenizerBase: a pre-trained tokenizer\n        PreTrainedModel: a pre-trained model\n    \"\"\"\n    logger.info(\n        f\"Loading model and tokenizer from HuggingFace hub with model_name = {params.model_name}\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(params.model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(params.model_name)\n\n    return tokenizer, model", ""]}
{"filename": "llm/steps/llm_custom_predict.py", "chunked_list": ["from typing import Any, Dict, List, Union\nimport numpy as np\n\n\nArray_Like = Union[np.ndarray, List[Any], str, bytes, Dict[str, Any]]\n\n\ndef custom_predict(\n    model: Any,\n    tokenizer: Any,\n    request: Array_Like,\n) -> Array_Like:\n    \"\"\"Custom Prediction function for LLM models.\n    Request input is in the format: [{\"text\": \"I am a text!\"}]\n    where each dictionary is a sample of text to be summarized.\n    The custom predict function is the core of the custom deployment, the\n    function is called by the custom deployment class defined for the serving\n    tool. The current implementation requires the function to get the model\n    loaded in the memory and a request with the data to predict.\n\n    Args:\n        model (Any): The model to use for prediction.\n        tokenizer (Any): The tokenizer to use for prediction.\n        request (Array_Like): The request response is an array-like format.\n\n    Returns:\n        Array_Like: The prediction in an array-like format.\n                    (e.g: np.ndarray, List[Any], str, bytes, Dict[str, Any])\n    \"\"\"\n    inputs = []\n\n    for instance in request:\n        text = instance[\"text\"]\n        inp = tokenizer(text, return_tensors=\"pt\").input_ids\n        outputs = model.generate(\n            inp,\n            max_length=300,\n            min_length=30,\n            length_penalty=2.0,\n            num_beams=4,\n            no_repeat_ngram_size=3,\n            early_stopping=True,\n        )\n        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        inputs.append(pred)\n\n    return inputs", ""]}
{"filename": "llm/steps/__init__.py", "chunked_list": [""]}
{"filename": "llm/steps/convert_to_hg_dataset_step.py", "chunked_list": ["\"\"\"Convert a dataset to the huggingface dataset format.\"\"\"\nimport pandas as pd\n\nfrom zenml.logger import get_logger\nfrom zenml.steps import step\nfrom datasets import Dataset\n\nlogger = get_logger(__name__)\n\n", "\n\n@step\ndef convert_to_hg_dataset(data: dict) -> Dataset:\n    \"\"\"Convert a dataset to the huggingface dataset format.\n\n    Args:\n        data (dict): dataset in `dict` format.\n\n    Returns:\n        Dataset: dataset in huggingface `Dataset` format.\n    \"\"\"\n    # Extract text and summary from dataset\n    texts, summaries = [], []\n    for _, v in data.items():\n        texts.append(v[\"original_text\"])\n        summaries.append(v[\"reference_summary\"])\n\n    df = pd.DataFrame({\"text\": texts, \"summary\": summaries})\n    dataset = Dataset.from_pandas(df)\n\n    logger.info(\"Total number of examples in dataset: {}\".format(len(dataset)))\n    return dataset", ""]}
{"filename": "llm/steps/download_data_step.py", "chunked_list": ["\"\"\"Download summarization dataset for fine-tuning LLM.\"\"\"\nimport os\nimport requests\nimport json\nfrom requests.exceptions import HTTPError\n\nfrom zenml.logger import get_logger\nfrom zenml.steps import step, BaseParameters\n\n", "\n\nlogger = get_logger(__name__)\nDATASET_URL = \"https://raw.githubusercontent.com/lauramanor/legal_summarization/master/all_v1.json\"\n\n\nclass DownloadDataParams(BaseParameters):\n    \"\"\"Parameters for downloading dataset.\"\"\"\n\n    # Path to directory where dataset will be downloaded\n    data_dir: str = \"data\"", "\n\n@step\ndef download_dataset(params: DownloadDataParams) -> dict:\n    \"\"\"Zenml step to download summarization dataset.\n\n    Args:\n        params (DownloadDataParams): Parameters for downloading dataset.\n\n    Returns:\n        dict: Dataset in dictionary format.\n\n    Raises:\n        Exception: If dataset cannot be downloaded.\n    \"\"\"\n    data_path = os.path.join(params.data_dir, \"summarization_dataset.json\")\n    if os.path.exists(data_path):\n        logger.info(f\"Dataset already exists at {data_path}\")\n\n        json_data = open(data_path).read()\n        data = json.loads(json_data)\n        return data\n\n    else:\n        logger.info(\"Downloading dataset\")\n        os.makedirs(params.data_dir, exist_ok=True)\n\n        try:\n            response = requests.get(DATASET_URL)\n            response.raise_for_status()\n\n        except HTTPError as http_err:\n            err_msg = f\"HTTP Error: {http_err}\"\n            logger.error(err_msg)\n            raise Exception(err_msg)\n\n        except Exception as err:\n            err_msg = f\"An error occurred: {err}\"\n            logger.error(err_msg)\n            raise Exception(err_msg)\n\n        else:\n            data = response.json()\n            with open(data_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(data, f, ensure_ascii=False, indent=4)\n\n            logger.info(f\"Dataset downloaded to {params.data_dir}\")\n            return data", ""]}
{"filename": "llm/steps/deploy_model_step.py", "chunked_list": ["\"\"\"Deploy LLM model server using seldon.\"\"\"\nfrom zenml.integrations.seldon.seldon_client import SeldonResourceRequirements\nfrom zenml.integrations.seldon.services.seldon_deployment import (\n    SeldonDeploymentConfig,\n)\nfrom zenml.integrations.seldon.steps.seldon_deployer import (\n    CustomDeployParameters,\n    SeldonDeployerStepParameters,\n)\nfrom steps.seldon_llm_custom_deployer_step import seldon_llm_model_deployer_step", ")\nfrom steps.seldon_llm_custom_deployer_step import seldon_llm_model_deployer_step\n\nseldon_llm_custom_deployment = seldon_llm_model_deployer_step(\n    params=SeldonDeployerStepParameters(\n        service_config=SeldonDeploymentConfig(\n            model_name=\"seldon-llm-custom-model\",\n            replicas=1,\n            implementation=\"custom\",\n            resources=SeldonResourceRequirements(", "            implementation=\"custom\",\n            resources=SeldonResourceRequirements(\n                limits={\"cpu\": \"500m\", \"memory\": \"900Mi\"}\n            ),\n        ),\n        timeout=300,\n        custom_deploy_parameters=CustomDeployParameters(\n            predict_function=\"steps.llm_custom_predict.custom_predict\"\n        ),\n    )", "        ),\n    )\n)\n"]}
{"filename": "llm/steps/fetch_trained_model_step.py", "chunked_list": ["\"\"\"Fetch trained model for deploying.\"\"\"\nfrom typing import Optional\n\nfrom zenml.post_execution import PipelineView, get_pipeline\nfrom zenml.post_execution.artifact import ArtifactView\nfrom zenml.steps import step, BaseParameters, Output\nfrom zenml.logger import get_logger\n\nlogger = get_logger(__name__)\n", "logger = get_logger(__name__)\n\n\nclass FetchModelParameters(BaseParameters):\n    \"\"\"Parameters for fetch model step.\"\"\"\n\n    # Name of the pipeline to fetch from\n    pipeline_name: str\n\n    # Step name to fetch artifact from.\n    step_name: str\n\n    # Optional pipeline version\n    pipeline_version: Optional[int] = None", "\n\ndef get_output_from_step(pipeline: PipelineView, step_name: str) -> ArtifactView:\n    \"\"\"Fetch output from a step with last completed run in a pipeline.\n\n    Args:\n        pipeline (PipelineView): Post-execution pipeline class object.\n        step_name (str): Name of step to fetch output from\n\n    Returns:\n        ArtifactView: Artifact data\n\n    Raises:\n        KeyError: If no step found with given name\n        ValueError: If no output found for step\n    \"\"\"\n    # Get the step with the given name from the last run\n    try:\n        # Get last completed run of the pipeline\n        fetch_last_completed_run = pipeline.get_run_for_completed_step(step_name)\n\n        logger.info(f\"Run used: {fetch_last_completed_run}\")\n\n        # Get the output of the step from last completed run\n        fetch_step = fetch_last_completed_run.get_step(step_name)\n\n        logger.info(f\"Step used: {fetch_step}\")\n    except KeyError as e:\n        logger.error(f\"No step found with name '{step_name}': {e}\")\n        raise e\n\n    # Get the model artifacts from the step\n    outputs = fetch_step.outputs\n    if outputs is None:\n        logger.error(f\"No output found for step '{step_name}'\")\n        raise ValueError(f\"No output found for step '{step_name}'\")\n\n    return outputs", "\n\n@step\ndef fetch_model(\n    params: FetchModelParameters,\n) -> Output(model=str, tokenizer=str, decision=bool):\n    \"\"\"Step to fetch model artifacts from last run of nft_embedding pipeline.\n\n    Args:\n        params (FetchModelParameters): Parameters for fetch model step.\n\n    Returns:\n        str: Location of model artifact.\n        str: Location of tokenizer artifact.\n        bool: Decision to deploy the model.\n\n    Raises:\n        ValueError: if the pipeline does not exist\n    \"\"\"\n    # Fetch pipeline by name\n    pipeline: PipelineView = get_pipeline(\n        params.pipeline_name, version=params.pipeline_version\n    )\n\n    if pipeline is None:\n        logger.error(f\"Pipeline '{params.pipeline_name}' does not exist\")\n        raise ValueError(f\"Pipeline '{params.pipeline_name}' does not exist\")\n\n    logger.info(f\"Pipeline: {pipeline}\")\n\n    # Fetch the output for step from the pipeline\n    outputs = get_output_from_step(pipeline, params.step_name)\n\n    model, tokenizer = outputs[\"model\"], outputs[\"tokenizer\"]\n    return model.uri, tokenizer.uri, True", ""]}
{"filename": "llm/steps/seldon_llm_custom_deployer_step.py", "chunked_list": ["# Derived from zenml seldon integration; source : https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/seldon/steps/seldon_deployer.py\n\"\"\"Custom zenml deployer step for Seldon LLM.\"\"\"\nimport os\nfrom typing import cast\nfrom zenml.environment import Environment\nfrom zenml.exceptions import DoesNotExistException\n\nfrom zenml.integrations.seldon.constants import (\n    SELDON_CUSTOM_DEPLOYMENT,\n    SELDON_DOCKER_IMAGE_KEY,", "    SELDON_CUSTOM_DEPLOYMENT,\n    SELDON_DOCKER_IMAGE_KEY,\n)\nfrom zenml.integrations.seldon.model_deployers.seldon_model_deployer import (\n    SeldonModelDeployer,\n)\nfrom zenml.integrations.seldon.seldon_client import (\n    create_seldon_core_custom_spec,\n)\nfrom zenml.integrations.seldon.steps.seldon_deployer import SeldonDeployerStepParameters", ")\nfrom zenml.integrations.seldon.steps.seldon_deployer import SeldonDeployerStepParameters\nfrom zenml.integrations.seldon.services.seldon_deployment import (\n    SeldonDeploymentService,\n)\n\nfrom zenml.io import fileio\nfrom zenml.logger import get_logger\n\nfrom zenml.steps import (", "\nfrom zenml.steps import (\n    STEP_ENVIRONMENT_NAME,\n    StepEnvironment,\n    step,\n)\nfrom zenml.steps.step_context import StepContext\nfrom zenml.utils import io_utils\n\n", "\n\nlogger = get_logger(__name__)\nDEFAULT_PT_MODEL_DIR = \"hf_pt_model\"\nDEFAULT_TOKENIZER_DIR = \"hf_tokenizer\"\n\n\ndef copy_artifact(uri: str, filename: str, context: StepContext) -> str:\n    \"\"\"Copy an artifact to the output location of the current step.\n\n    Args:\n        uri (str): URI of the artifact to copy\n        filename (str): filename for the output artifact\n        context (StepContext): ZenML step context\n\n    Returns:\n        str: URI of the output location\n\n    Raises:\n        RuntimeError: if the artifact is not found\n\n    \"\"\"\n    served_artifact_uri = os.path.join(context.get_output_artifact_uri(), \"seldon\")\n    fileio.makedirs(served_artifact_uri)\n    if not fileio.exists(uri):\n        raise RuntimeError(f\"Expected artifact was not found at \" f\"{uri}\")\n    if io_utils.isdir(uri):\n        io_utils.copy_dir(uri, os.path.join(served_artifact_uri, filename))\n    else:\n        fileio.copy(uri, os.path.join(served_artifact_uri, filename))\n\n    return served_artifact_uri", "\n\n@step(enable_cache=False, extra={SELDON_CUSTOM_DEPLOYMENT: True})\ndef seldon_llm_model_deployer_step(\n    deploy_decision: bool,\n    model_uri: str,\n    tokenizer_uri: str,\n    params: SeldonDeployerStepParameters,\n    context: StepContext,\n) -> SeldonDeploymentService:\n    \"\"\"Seldon Core custom model deployer pipeline step for LLM models.\n    This step can be used in a pipeline to implement the\n    the process required to deploy a custom model with Seldon Core.\n\n    Args:\n        deploy_decision (bool): whether to deploy the model or not\n        model_uri (str): The URI of huggingface model\n        tokenizer_uri (str): The URI of huggingface tokenizer\n        params (SeldonDeployerStepParameters): parameters for the deployer step\n        context (StepContext): the step context\n\n    Raises:\n        ValueError: if the custom deployer is not defined\n        DoesNotExistException: if an entity does not exist raise an exception\n\n    Returns:\n        SeldonDeploymentService: Seldon Core deployment service\n    \"\"\"\n    # verify that a custom deployer is defined\n    if not params.custom_deploy_parameters:\n        raise ValueError(\n            \"Custom deploy parameter is required as part of the step configuration this parameter is\",\n            \"the path of the custom predict function\",\n        )\n    # get the active model deployer\n    model_deployer = cast(\n        SeldonModelDeployer, SeldonModelDeployer.get_active_model_deployer()\n    )\n\n    # get pipeline name, step name, run id\n    step_env = cast(StepEnvironment, Environment()[STEP_ENVIRONMENT_NAME])\n    pipeline_name = step_env.pipeline_name\n    run_name = step_env.run_name\n    step_name = step_env.step_name\n\n    # update the step configuration with the real pipeline runtime information\n    params.service_config.pipeline_name = pipeline_name\n    params.service_config.pipeline_run_id = run_name\n    params.service_config.pipeline_step_name = step_name\n    params.service_config.is_custom_deployment = True\n\n    # fetch existing services with the same pipeline name, step name and\n    # model name\n    existing_services = model_deployer.find_model_server(\n        pipeline_name=pipeline_name,\n        pipeline_step_name=step_name,\n        model_name=params.service_config.model_name,\n    )\n    # even when the deploy decision is negative if an existing model server\n    # is not running for this pipeline/step, we still have to serve the\n    # current model, to ensure that a model server is available at all times\n    if not deploy_decision and existing_services:\n        logger.info(\n            f\"Skipping model deployment because the model quality does not\"\n            f\" meet the criteria. Reusing the last model server deployed by step \"\n            f\"'{step_name}' and pipeline '{pipeline_name}' for model \"\n            f\"'{params.service_config.model_name}'...\"\n        )\n        service = cast(SeldonDeploymentService, existing_services[0])\n        # even when the deployment decision is negative, we still need to start\n        # the previous model server if it is no longer running, to ensure that\n        # a model server is available at all times\n        if not service.is_running:\n            service.start(timeout=params.timeout)\n        return service\n\n    # entrypoint for starting Seldon microservice deployment for custom model\n    entrypoint_command = [\n        \"python\",\n        \"-m\",\n        \"steps.zenml_llm_custom_model\",\n        \"--model_name\",\n        params.service_config.model_name,\n        \"--predict_func\",\n        params.custom_deploy_parameters.predict_function,\n    ]\n\n    # verify if there is an active stack before starting the service\n    if not context.stack:\n        raise DoesNotExistException(\n            \"No active stack is available. \"\n            \"Please make sure that you have registered and set a stack.\"\n        )\n\n    image_name = step_env.step_run_info.get_image(key=SELDON_DOCKER_IMAGE_KEY)\n\n    # Copy artifacts\n    model_path = os.path.join(model_uri, DEFAULT_PT_MODEL_DIR)\n    served_model_uri = copy_artifact(model_path, DEFAULT_PT_MODEL_DIR, context)\n\n    tokenizer_path = os.path.join(tokenizer_uri, DEFAULT_TOKENIZER_DIR)\n    copy_artifact(tokenizer_path, DEFAULT_TOKENIZER_DIR, context)\n\n    # prepare the service configuration for the deployment\n    service_config = params.service_config.copy()\n    service_config.model_uri = served_model_uri\n\n    # create the specification for the custom deployment\n    service_config.spec = create_seldon_core_custom_spec(\n        model_uri=service_config.model_uri,\n        custom_docker_image=image_name,\n        secret_name=model_deployer.kubernetes_secret_name,\n        command=entrypoint_command,\n    )\n\n    # deploy the service\n    service = cast(\n        SeldonDeploymentService,\n        model_deployer.deploy_model(\n            service_config, replace=True, timeout=params.timeout\n        ),\n    )\n\n    logger.info(\n        f\"Seldon Core deployment service started and reachable at:\\n\"\n        f\"    {service.prediction_url}\\n\"\n    )\n\n    return service", ""]}
{"filename": "llm/pipelines/__init__.py", "chunked_list": [""]}
{"filename": "llm/pipelines/llm_deployment_pipeline.py", "chunked_list": ["\"\"\"Deployment pipeline for the LLM example.\"\"\"\nfrom zenml.pipelines import pipeline\n\n\n@pipeline\ndef llm_deployment_pipeline(fetch_trained_model, deploy_model):\n    \"\"\"Pipeline to deploy fine-tuned LLM model.\n\n    Args:\n        fetch_trained_model : A step to fetch path to trained model and decision to deploy the model.\n        deploy_model: A step to deploy LLM model using Seldon.\n    \"\"\"\n    # Fetch the trained model path, tokenizer path and decision\n    model_uri, tokenizer_uri, decision = fetch_trained_model()\n\n    # Deploy the model\n    deploy_model(decision, model_uri, tokenizer_uri)", ""]}
{"filename": "llm/pipelines/llm_pipeline.py", "chunked_list": ["\"\"\"Pipeline for the llm finetuning model.\"\"\"\nfrom zenml.pipelines import pipeline\n\n\n@pipeline\ndef llm_pipeline(\n    download_dataset,\n    convert_to_hg_dataset,\n    get_huggingface_model,\n    preprocess_dataset,\n    tune_model,\n):\n    \"\"\"Pipeline for llm fine-tuning on summarization dataset.\n\n    Args:\n        download_dataset: A step to download the summarization dataset.\n        convert_to_hg_dataset: A step to convert summarization dataset into\n                            huggingface dataset format.\n        get_huggingface_model: A step to get pre-trained model from huggingface.\n        preprocess_dataset: A step to preprocess, tokenize and split the summarization dataset.\n        tune_model: A step to fine-tune a huggingface pre-trained model.\n    \"\"\"\n    # Download the summarization dataset\n    data = download_dataset()\n\n    # Convert dataset into huggingface dataset format\n    dataset = convert_to_hg_dataset(data)\n\n    # Get the pre-trained model\n    tokenizer, model = get_huggingface_model()\n\n    # Preprocess, tokenize and split dataset\n    tokenized_data = preprocess_dataset(dataset, tokenizer)\n\n    # Fine-tune\n    tuned_tokenizer, tuned_model = tune_model(tokenizer, model, tokenized_data)", ""]}
{"filename": "llm/tests/__init__.py", "chunked_list": ["\"\"\"Test functions to be imported.\"\"\""]}
{"filename": "llm/tests/test_steps/test_convert_to_hg_dataset.py", "chunked_list": ["\"\"\"Test suite to test the convert_to_hg_dataset step.\"\"\"\nimport pytest\nimport datasets\n\nfrom steps.convert_to_hg_dataset_step import convert_to_hg_dataset\n\n\n@pytest.fixture\ndef mock_data() -> dict:\n    \"\"\"Mock input dictionary data for step.\n\n    Returns:\n        dict: Dictionary with mocked input data\n    \"\"\"\n    mock_data = {\"data\": {\"original_text\": \"I am a text!\",\n                          \"reference_summary\": \"I am a summary!\"}}\n    return mock_data", "def mock_data() -> dict:\n    \"\"\"Mock input dictionary data for step.\n\n    Returns:\n        dict: Dictionary with mocked input data\n    \"\"\"\n    mock_data = {\"data\": {\"original_text\": \"I am a text!\",\n                          \"reference_summary\": \"I am a summary!\"}}\n    return mock_data\n", "\n\ndef test_convert_to_hg_dataset_step(mock_data: dict):\n    \"\"\"Test the convert_to_hg_dataset step.\n\n    Args:\n        mock_data (dict): Fixture to mock input data\n    \"\"\"\n    hg_dataset = convert_to_hg_dataset.entrypoint(mock_data)\n    expected_features = [\"text\", \"summary\"]\n\n    # Check if the output is a huggingface `Dataset` object\n    assert isinstance(hg_dataset, datasets.Dataset)\n\n    # Check if the output has the expected number of features\n    assert len(hg_dataset.features) == len(expected_features)\n\n    # Check if the output has the expected features\n    assert all([feat in hg_dataset.features for feat in expected_features])\n\n    # Check if the all features are a `tf.Tensor` object\n    assert all([isinstance(hg_dataset[feat][0], str) for feat in expected_features])", ""]}
{"filename": "llm/tests/test_steps/test_get_hg_model.py", "chunked_list": ["\"\"\"Tests for get_huggingface_model step.\"\"\"\nfrom unittest import mock\n\nfrom transformers import PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\nfrom steps.get_hg_model import get_huggingface_model, GetHuggingfaceModelParameters\nimport pytest\n\n\n@pytest.fixture\ndef params() -> GetHuggingfaceModelParameters:\n    \"\"\"Mock parameters required for step.\n\n    Returns:\n        GetHuggingfaceModelParameters: Parameters for step.\n    \"\"\"\n    return GetHuggingfaceModelParameters(\n        model_name=\"test_model\"\n    )", "@pytest.fixture\ndef params() -> GetHuggingfaceModelParameters:\n    \"\"\"Mock parameters required for step.\n\n    Returns:\n        GetHuggingfaceModelParameters: Parameters for step.\n    \"\"\"\n    return GetHuggingfaceModelParameters(\n        model_name=\"test_model\"\n    )", "\n\ndef test_get_huggingface_model(params: GetHuggingfaceModelParameters):\n    \"\"\"Test get_huggingface_model gets pre-trained tokenizer and model\n\n    Args:\n        params (GetHuggingfaceModelParameters): test parameters\n    \"\"\"\n    with mock.patch(\"steps.get_hg_model.AutoTokenizer\") as mock_tokenizer, \\\n            mock.patch(\"steps.get_hg_model.AutoModelForSeq2SeqLM\") as mock_model:\n        mock_tokenizer.from_pretrained.return_value = PreTrainedTokenizerBase()\n        mock_model.from_pretrained.return_value = PreTrainedModel(PretrainedConfig())\n        tokenizer, model = get_huggingface_model.entrypoint(params)\n        assert isinstance(tokenizer, PreTrainedTokenizerBase)\n        assert isinstance(model, PreTrainedModel)", ""]}
{"filename": "llm/tests/test_steps/test_finetune_model.py", "chunked_list": ["\"\"\"Test finetune_model step.\"\"\"\nfrom unittest import mock\n\nfrom steps.finetune_model import finetune_model, TuningParameters\nimport pytest\nfrom datasets import Dataset\nfrom transformers import PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n\n\n@pytest.fixture\ndef params() -> TuningParameters:\n    \"\"\"Mock parameters required for step.\n\n    Returns:\n        GetHuggingfaceModelParameters: Parameters for step.\n    \"\"\"\n    return TuningParameters(\n        learning_rate=2e-5,\n        weight_decay=0.01,\n        use_cuda=False,\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        epochs=5,\n        load_best_model_at_end=True\n    )", "\n@pytest.fixture\ndef params() -> TuningParameters:\n    \"\"\"Mock parameters required for step.\n\n    Returns:\n        GetHuggingfaceModelParameters: Parameters for step.\n    \"\"\"\n    return TuningParameters(\n        learning_rate=2e-5,\n        weight_decay=0.01,\n        use_cuda=False,\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        epochs=5,\n        load_best_model_at_end=True\n    )", "\n@pytest.fixture\ndef test_model() -> PreTrainedModel:\n    \"\"\"Get test dummy huggingface model.\n\n    Returns:\n        PreTrainedModel: test model\n    \"\"\"\n    return PreTrainedModel(PretrainedConfig())\n", "\n@pytest.fixture\ndef test_tokenizer() -> PreTrainedTokenizerBase:\n    \"\"\"Get test dummy huggingface tokenizer.\n\n    Returns:\n        PreTrainedTokenizerBase: test tokenizer\n    \"\"\"\n    return PreTrainedTokenizerBase()\n", "\n@pytest.fixture\ndef test_dataset() -> Dataset:\n    \"\"\"Get empty dataset.\n\n    Returns:\n        Dataset: empty dataset\n    \"\"\"\n    return Dataset.from_dict({\"train\": [], \"test\": []})\n", "\n\n@pytest.fixture\ndef expected_training_args() -> dict:\n    \"\"\"Get expected training arguments.\n\n    Returns:\n        dict: expected training arguments\n    \"\"\"\n    return {\n        \"output_dir\": \"model\",\n        \"learning_rate\": 2e-5,\n        \"evaluation_strategy\": 'epoch',\n        \"per_device_train_batch_size\": 2,\n        \"per_device_eval_batch_size\": 2,\n        \"weight_decay\": 0.01,\n        \"save_total_limit\": 1,\n        \"save_strategy\": 'epoch',\n        \"num_train_epochs\": 5,\n        \"predict_with_generate\": True,\n        \"no_cuda\": True,\n        \"fp16\": False,\n        \"load_best_model_at_end\": True,\n    }", "\n\ndef test_finetune_model(\n    params: TuningParameters,\n    test_tokenizer: PreTrainedTokenizerBase,\n    test_model: PreTrainedModel,\n    test_dataset: Dataset,\n    expected_training_args: dict\n):\n    \"\"\"Test finetune_model step fine-tunes a provided model.\n\n    Args:\n        params (TuningParameters): step parameters\n        test_tokenizer (PreTrainedTokenizerBase): test tokenizer\n        test_model (PreTrainedModel): test model\n        test_dataset (Dataset): test empty dataset\n        expected_training_args (dict): dictionary of expected training arguments\n    \"\"\"\n    with mock.patch(\"steps.finetune_model.Seq2SeqTrainer\") as mock_trainer, \\\n            mock.patch(\"steps.finetune_model.Seq2SeqTrainingArguments\") as mock_trainer_args, \\\n            mock.patch(\"steps.finetune_model.DataCollatorForSeq2Seq\") as mock_data_collator:\n        mock_trainer_instance = mock_trainer.return_value\n        mock_trainer_instance.tokenizer = test_tokenizer\n        mock_trainer_instance.model = test_model\n\n        tuned_tokenizer, tuned_model = finetune_model.entrypoint(params, test_tokenizer, test_model, test_dataset)\n\n        mock_trainer_args.assert_called_with(**expected_training_args)\n        mock_trainer.assert_called_with(\n            model=test_model,\n            args=mock_trainer_args.return_value,\n            train_dataset=test_dataset[\"train\"],\n            eval_dataset=test_dataset[\"test\"],\n            tokenizer=test_tokenizer,\n            data_collator=mock_data_collator.return_value\n        )\n        assert isinstance(tuned_tokenizer, PreTrainedTokenizerBase)\n        assert isinstance(tuned_model, PreTrainedModel)", "\n"]}
{"filename": "llm/tests/test_steps/test_preprocess_hg_dataset_setp.py", "chunked_list": ["\"\"\"Test suite to test the preprocess_hg_dataset step.\"\"\"\nimport pytest\nfrom types import SimpleNamespace\n\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, BatchEncoding, PreTrainedTokenizerBase\n\nfrom steps.convert_to_hg_dataset_step import convert_to_hg_dataset\nfrom steps.preprocess_hg_dataset_step import preprocess_dataset, preprocess_function\n", "from steps.preprocess_hg_dataset_step import preprocess_dataset, preprocess_function\n\n\n@pytest.fixture\ndef get_params() -> dict:\n    \"\"\"Mock parameters required for step.\n\n    Returns:\n        dict: Parameters for step.\n    \"\"\"\n    params = SimpleNamespace()\n    params.prefix = \"summarize: \"\n    params.input_max_length = 128\n    params.target_max_length = 128\n    params.test_size = 0.1\n    return params", "\n\n@pytest.fixture\ndef test_tokenizer() -> PreTrainedTokenizerBase:\n    return AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n\n\n@pytest.fixture\ndef mock_data() -> dict:\n    \"\"\"Mock input dictionary data for step.\n\n    Returns:\n        dict: Dictionary with mocked input data\n    \"\"\"\n    mock_data = {\"data\": {\"original_text\": \"I am a text!\",\n                          \"reference_summary\": \"I am a summary!\"\n                          },\n                \"data1\": {\"original_text\": \"I am another text!\",\n                         \"reference_summary\": \"I am another summary!\"\n                         }          \n                }\n    return mock_data", "def mock_data() -> dict:\n    \"\"\"Mock input dictionary data for step.\n\n    Returns:\n        dict: Dictionary with mocked input data\n    \"\"\"\n    mock_data = {\"data\": {\"original_text\": \"I am a text!\",\n                          \"reference_summary\": \"I am a summary!\"\n                          },\n                \"data1\": {\"original_text\": \"I am another text!\",\n                         \"reference_summary\": \"I am another summary!\"\n                         }          \n                }\n    return mock_data", "\n\n@pytest.fixture\ndef mock_hf_dataset(mock_data: dict) -> Dataset:\n    \"\"\"Fixture to create a mock hugging face dataset.\n\n    This uses convert_to_hg_dataset step to convert dict to Dataset.\n\n    Args:\n        mock_data (dict): Mocked input dict.\n\n    Returns:\n        Dataset: Mocked huggingface dataset using mocked input dict.\n    \"\"\"\n    hg_dataset = convert_to_hg_dataset.entrypoint(mock_data)\n    return hg_dataset", "\n\ndef test_preprocess_function(mock_hf_dataset: Dataset, get_params: dict, test_tokenizer: PreTrainedTokenizerBase):\n    \"\"\"Test the preprocess_function function.\n\n    Args:\n        mock_hf_dataset (Dataset): Fixture to mock huggingface dataset.\n        get_params (dict): Parameters required for step.\n        test_tokenizer (PreTrainedTokenizerBase): Test tokenizer to use.\n    \"\"\"\n    tokenized_dataset = preprocess_function(mock_hf_dataset,\n                                            test_tokenizer,\n                                            get_params.prefix,\n                                            get_params.input_max_length,\n                                            get_params.target_max_length)\n\n    expected_features = ['input_ids', 'attention_mask', 'labels']\n    expected_labels = [27, 183, 3, 9, 9251, 55, 1]\n    expected_input_ids = [21603, 10, 27, 183, 3, 9, 1499, 55, 1]\n\n    # Check if the output is a huggingface `BatchEncoding` object\n    assert isinstance(tokenized_dataset, BatchEncoding)\n\n    # Check if the output contains the expected features\n    assert all([feat in tokenized_dataset.keys() for feat in expected_features])\n\n    # Check if length of each feature is correct\n    assert all([len(v) == len(mock_hf_dataset) for _, v in tokenized_dataset.items()])\n\n    # Check if input and labels are correct\n    assert tokenized_dataset['input_ids'][0] == expected_input_ids\n    assert tokenized_dataset['labels'][0] == expected_labels", "\n\ndef test_preprocess_dataset_step(mock_hf_dataset: Dataset, get_params: dict, test_tokenizer: PreTrainedTokenizerBase):\n    \"\"\"Test the preprocess_dataset step.\n\n    Args:\n        mock_hf_dataset (Dataset): Fixture to mock huggingface dataset.\n        get_params (dict): Parameters required for step.\n        test_tokenizer (PreTrainedTokenizerBase): Test tokenizer to use.\n    \"\"\"\n\n    tokenized_dataset = preprocess_dataset.entrypoint(mock_hf_dataset, test_tokenizer, get_params)\n    expected_features = ['text', 'summary', 'input_ids', 'attention_mask', 'labels']\n\n    # Check if the output is a huggingface `DatasetDict` object\n    assert isinstance(tokenized_dataset, DatasetDict)\n\n    # Check if two sets: train and test are created\n    assert sorted(list(tokenized_dataset.keys())) == sorted([\"train\", \"test\"])\n\n    # Check length of train and test set\n    assert len(tokenized_dataset[\"train\"]) == 1\n    assert len(tokenized_dataset[\"test\"]) == 1\n\n    # Check if the output has the expected number of features for both train and test set\n    assert len(tokenized_dataset[\"train\"].features) == len(expected_features)\n    assert len(tokenized_dataset[\"test\"].features) == len(expected_features)\n\n    # Check if the output has the expected features for both train and test set\n    assert all([feat in tokenized_dataset[\"train\"].features for feat in expected_features])\n    assert all([feat in tokenized_dataset[\"test\"].features for feat in expected_features])", ""]}
{"filename": "llm/tests/test_steps/test_download_data_step.py", "chunked_list": ["\"\"\"Test suite to test the download data step.\"\"\"\nimport pytest\nfrom typing import Iterator\nfrom types import SimpleNamespace\nimport tempfile\nimport os\nfrom unittest import mock\nfrom requests import HTTPError\n\nfrom steps.download_data_step import download_dataset", "\nfrom steps.download_data_step import download_dataset\n\n\n@pytest.fixture\ndef temp_testing_directory() -> Iterator[str]:\n    \"\"\"A fixture for creating and removing temporary test directory for storing and moving files.\n\n    Yields:\n        str: a path to temporary directory for storing and moving files from tests.\n    \"\"\"\n    temp_dir = tempfile.TemporaryDirectory()\n\n    # tests are executed at this point\n    yield temp_dir.name\n\n    # delete temp folder\n    temp_dir.cleanup()", "\n\n@pytest.fixture\ndef get_params(temp_testing_directory: str) -> dict:\n    \"\"\"Mock parameters required for step.\n\n    Args:\n        temp_testing_directory (str): Path to temporary directory\n\n    Returns:\n        dict: Parameters for step.\n    \"\"\"\n    params = SimpleNamespace()\n    params.data_dir = temp_testing_directory\n    return params", "\n\ndef test_download_data_step(get_params: dict):\n    \"\"\"Test the download data step.\n\n    Args:\n        get_params (dict): Fixture containing paramters for step.\n    \"\"\"\n    dummy_dict = {'text': 'summary'}\n    with mock.patch(\"requests.get\") as mockresponse:\n        mockresponse.return_value.status_code = 200\n        mockresponse.return_value.json.return_value = dummy_dict\n\n        data = download_dataset.entrypoint(get_params)\n\n        # Check if returned data matches expected data\n        assert data == dummy_dict\n\n        # Check if folder is created\n        assert os.path.exists(get_params.data_dir)\n\n        # Check if file is created inside folder\n        file_path = os.path.join(get_params.data_dir, \"summarization_dataset.json\")\n        assert os.path.exists(file_path)", "\n\ndef test_download_data_step_invalid_url(get_params: dict):\n    \"\"\"Test the download data step when invalid url is passed.\n\n    Args:\n        get_params (dict): Fixture containing paramters for step.\n    \"\"\"\n    dummy_dict = {'text': 'summary'}\n    with mock.patch(\"requests.get\") as mockresponse:\n        mock_req_instance = mockresponse.return_value\n        mock_req_instance.status_code = 404\n        mock_req_instance.url = \"http://invalid_url\"\n        mock_req_instance.json.return_value = dummy_dict\n        mock_req_instance.raise_for_status.side_effect = HTTPError()\n\n        with pytest.raises(Exception) as exc_info:\n            _ = download_dataset.entrypoint(get_params)\n\n    assert (str(exc_info.value) == \"HTTP Error: \")", ""]}
{"filename": "llm/app/llm_demo.py", "chunked_list": ["\"\"\"Streamlit application.\"\"\"\nimport os\nimport requests\nimport streamlit as st\nimport json\nfrom zenml.integrations.seldon.model_deployers.seldon_model_deployer import (\n    SeldonModelDeployer,\n)\n\nst.title(\"LLM Legal Text Summarization Demo\")", "\nst.title(\"LLM Legal Text Summarization Demo\")\n\nPIPELINE_NAME = \"llm_deployment_pipeline\"\nPIPELINE_STEP = \"deploy_model\"\nMODEL_NAME = \"seldon-llm-custom-model\"\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\n@st.cache_data\ndef _get_prediction_endpoint() -> str:\n    \"\"\"Get the endpoint for the currently deployed LLM model.\n\n    Returns:\n        str: the url endpoint.\n    \"\"\"\n    try:\n        model_deployer = SeldonModelDeployer.get_active_model_deployer()\n\n        deployed_services = model_deployer.find_model_server(\n            pipeline_name=PIPELINE_NAME,\n            pipeline_step_name=PIPELINE_STEP,\n            model_name=MODEL_NAME,\n        )\n\n        return deployed_services[0].prediction_url\n    except Exception:\n        return None", "\n@st.cache_data\ndef _get_prediction_endpoint() -> str:\n    \"\"\"Get the endpoint for the currently deployed LLM model.\n\n    Returns:\n        str: the url endpoint.\n    \"\"\"\n    try:\n        model_deployer = SeldonModelDeployer.get_active_model_deployer()\n\n        deployed_services = model_deployer.find_model_server(\n            pipeline_name=PIPELINE_NAME,\n            pipeline_step_name=PIPELINE_STEP,\n            model_name=MODEL_NAME,\n        )\n\n        return deployed_services[0].prediction_url\n    except Exception:\n        return None", "\n\ndef _create_payload(input_text: str) -> dict:\n    \"\"\"Create a payload from the user input to send to the LLM model.\n\n    Args:\n        input_text (str): Input text to summarize.\n\n    Returns:\n        dict: the payload to send in the correct format.\n    \"\"\"\n    return {\"data\": {\"ndarray\": [{\"text\": str(input_text)}]}}", "\n\ndef _get_predictions(prediction_endpoint: str, payload: dict) -> dict:\n    \"\"\"Using the prediction endpont and payload, make a prediction request to the deployed model.\n\n    Args:\n        prediction_endpoint (str): the url endpoint.\n        payload (dict): the payload to send to the model.\n\n    Returns:\n        dict: the predictions from the model.\n    \"\"\"\n    response = requests.post(\n        url=prediction_endpoint,\n        data=json.dumps(payload),\n        headers={\"Content-Type\": \"application/json\"},\n    )\n    return json.loads(response.text)[\"jsonData\"][\"predictions\"][0]", "\n\ndef fetch_summary(seldon_url: str, txt: str) -> str:\n    \"\"\"Query seldon endpoint to fetch the summary.\n\n    Args:\n        seldon_url (str): Seldon endpoint\n        txt (str): Input text to summarize\n\n    Returns:\n        str: Summarized text\n    \"\"\"\n    with st.spinner(\"Applying LLM Magic...\"):\n        payload = _create_payload(txt)\n        summary_txt = _get_predictions(seldon_url, payload)\n    return summary_txt", "\n\ndef read_examples(file_path: str = \"example.json\") -> dict:\n    \"\"\"Read sample examples for LLM summarization demo.\n\n    Args:\n        file_path (str, optional): Path to json file. Defaults to 'example.json'.\n\n    Returns:\n        dict: Dictionary containing examples.\n    \"\"\"\n    with open(file_path, \"r\") as myfile:\n        data = myfile.read()\n    return json.loads(data)", "\n\ndef switch_examples(data: dict) -> str:\n    \"\"\"Switch between different examples.\n\n    Args:\n        data (dict): Dictionary containing examples.\n\n    Returns:\n        str: Input text to summarize.\n    \"\"\"\n    pages = [\"Fuzzy Labs\", \"Stack Overflow\", \"ZenML\", \"Your own example\"]\n    page = st.radio(\"Test Examples\", pages)\n    text = data.get(page, \"\")\n    input_text = st.text_area(label=\"Text to summarize\", value=text, height=400)\n    return input_text", "\n\ndef main():\n    example_file_path = os.path.join(BASE_DIR, \"example.json\")\n    data = read_examples(file_path=example_file_path)\n\n    txt = switch_examples(data)\n    result = st.button(label=\"Ready\")\n\n    if result:\n        seldon_url = _get_prediction_endpoint()\n\n        if seldon_url is None:\n            st.write(\"Hmm, seldon endpoint is not provisioned yet!\")\n\n        else:\n            if len(txt) > 0:\n                summarized_text = st.text_area(\n                    label=\"Summarized Text\",\n                    value=fetch_summary(seldon_url, txt),\n                    height=200,\n                )\n            else:\n                st.write(\"Hmm, input text is empty.\")", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "recommendation/run.py", "chunked_list": ["\"\"\"Run the recommendation example pipeline.\"\"\"\nimport click\nfrom steps.load_data_step import load_data\nfrom steps.train_step import train\nfrom steps.evaluate_step import evaluate\nfrom pipelines.recommendation_pipeline import recommendation_pipeline\nfrom steps.fetch_model import fetch_model\nfrom steps.deployer import seldon_surprise_custom_deployment\nfrom steps.deployment_trigger import deployment_trigger\nfrom pipelines.deploy_recommendation_pipeline import recommendation_deployment_pipeline", "from steps.deployment_trigger import deployment_trigger\nfrom pipelines.deploy_recommendation_pipeline import recommendation_deployment_pipeline\nfrom materializer import SurpriseMaterializer\n\nfrom zenml.logger import get_logger\nfrom zenml.integrations.mlflow.mlflow_utils import get_tracking_uri\n\n\nlogger = get_logger(__name__)\n", "logger = get_logger(__name__)\n\n\ndef run_recommendation_pipeline():\n    \"\"\"Run all steps in the example pipeline.\"\"\"\n    pipeline = recommendation_pipeline(\n        load_data().configure(output_materializers=SurpriseMaterializer),\n        train().configure(output_materializers=SurpriseMaterializer),\n        evaluate(),\n    )\n    pipeline.run(config_path=\"pipelines/config_recommendation_pipeline.yaml\")\n    \n    logger.info(\n        f\"Visit: {get_tracking_uri()}\\n \"\n        \"To inspect your experiment runs within the mlflow UI.\\n\"\n    )", "    \n    \ndef run_deployment_pipeline():\n    \"\"\"Run all steps in deployment pipeline.\"\"\"\n    deploy_pipeline = recommendation_deployment_pipeline(\n        fetch_model().configure(output_materializers=SurpriseMaterializer),\n        deployment_trigger(),\n        deploy_model=seldon_surprise_custom_deployment,\n    )\n    deploy_pipeline.run(config_path=\"pipelines/config_deploy_recommendation_pipeline.yaml\")", "    \n    \n@click.command()\n@click.option(\"--train\", \"-t\", is_flag=True, help=\"Run training pipeline\")\n@click.option(\"--deploy\", \"-d\", is_flag=True, help=\"Run the deployment pipeline\")\ndef main(train: bool, deploy: bool):\n    \"\"\"Run all pipelines.\n    \n    args:\n        train (bool): Flag for running the training pipeline.\n        deploy (bool): Flag for running the deployment pipeline.\n    \"\"\"\n    if train:\n        logger.info(\"Running recommendation training pipeline.\")\n        run_recommendation_pipeline()\n    \n    if deploy:\n        logger.info(\"Running deployment pipeline.\")\n        run_deployment_pipeline()\n    \n    if (not train) and (not deploy):\n        logger.info(\"Running recommendation training pipeline.\")\n        run_recommendation_pipeline()\n        \n        logger.info(\"Running deployment pipeline.\")\n        run_deployment_pipeline()", "\n\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "recommendation/inference.py", "chunked_list": ["\"\"\"A inference script to query the deployed recommendation model.\"\"\"\nimport requests\nimport json \nimport click\n\nfrom zenml.integrations.seldon.model_deployers.seldon_model_deployer import SeldonModelDeployer\n\nPIPELINE_NAME = 'recommendation_deployment_pipeline'\nPIPELINE_STEP = 'deploy_model'\nMODEL_NAME = 'seldon-svd-custom-model'", "PIPELINE_STEP = 'deploy_model'\nMODEL_NAME = 'seldon-svd-custom-model'\n\n\ndef _get_prediction_endpoint() -> str:\n    \"\"\"Get the endpoint for the currently deployed recommendation model.\n\n    Returns:\n        str: the url endpoint.\n    \"\"\"\n    model_deployer = SeldonModelDeployer.get_active_model_deployer()\n\n    deployed_services = model_deployer.find_model_server(\n        pipeline_name=PIPELINE_NAME,\n        pipeline_step_name=PIPELINE_STEP,\n        model_name=MODEL_NAME\n    )\n\n    return deployed_services[0].prediction_url", "\n\ndef _create_payload(user: str, movie: str) -> dict:\n    \"\"\"Create a payload from the user input to send to the recommendation model.\n\n    Args:\n        user (str): the id of the user.\n        movie (str): the id of the movie.\n\n    Returns:\n        dict: the payload to send in the correct format.\n    \"\"\"\n    return {\n        \"data\": {\n            \"ndarray\": [{\"uid\": str(user), \"iid\": str(movie)}]\n        }\n    }", "\n\ndef _get_predictions(prediction_endpoint: str, payload: dict) -> dict:\n    \"\"\"Using the prediction endpont and payload, make a prediction request to the deployed model.\n\n    Args:\n        prediction_endpoint (str): the url endpoint.\n        payload (dict): the payload to send to the model.\n\n    Returns:\n        dict: the predictions from the model.\n    \"\"\"\n    response = requests.post(\n        url=prediction_endpoint,\n        data=json.dumps(payload),\n        headers={'Content-Type': 'application/json'}\n    )\n\n    return json.loads(response.text)['jsonData']['predictions']", "\n\ndef _output(predictions: dict):\n    \"\"\"Output the results to the terminal.\n\n    Args:\n        predictions (dict): the predictions produced by the deployed model.\n    \"\"\"\n    for pred in predictions:\n        rating = round(pred['est'], 2)\n        print(f\"User {pred['uid']} is predicted to give the movie ({pred['iid']}) a rating of: {rating} out of 5.\")", "    \n\n@click.command()\n@click.option('--user', default=1, help='the user id.', type=click.IntRange(1, 943))\n@click.option('--movie', default=1, help='the movie id.', type=click.IntRange(1, 1682))\ndef main(user: int, movie: int):\n    \"\"\"The main runner function.\n\n    Args:\n        user (int): the user inputted user id.\n        movie (int): the user inputted movie id.\n    \"\"\"\n    endpoint = _get_prediction_endpoint()\n    payload = _create_payload(user, movie)\n    predictions = _get_predictions(endpoint, payload)\n    _output(predictions)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "recommendation/steps/deployment_trigger.py", "chunked_list": ["\"\"\"A step to decide whether to deploy the model or not.\"\"\"\nfrom zenml.logger import get_logger\nfrom zenml.post_execution import PipelineView, get_pipeline\nfrom zenml.post_execution.artifact import ArtifactView\nfrom zenml.steps import BaseParameters, Output, step\n\nfrom surprise import SVD\n\nlogger = get_logger(__name__)\n", "logger = get_logger(__name__)\n\n\n@step()\ndef deployment_trigger(\n    model: SVD,\n) -> Output(decision=bool):\n    \"\"\"Step to decide whether to deploy the model or not.\n\n    Args:\n        model (SVD): Model to check deployment.\n\n    Returns:\n        bool: Decision to deploy the model.\n    \"\"\"\n    return True"]}
{"filename": "recommendation/steps/load_data_step.py", "chunked_list": ["\"\"\"A step to load a movie ratings dataset.\"\"\"\nimport mlflow\nfrom zenml.steps import step, Output, BaseParameters\n\nfrom surprise import Dataset\nfrom surprise.model_selection import train_test_split\nfrom surprise.trainset import Trainset\n\n\nclass DataParameters(BaseParameters):\n    \"\"\"Load data parameters.\"\"\"\n\n    # The size of test set\n    test_size = 0.25", "\nclass DataParameters(BaseParameters):\n    \"\"\"Load data parameters.\"\"\"\n\n    # The size of test set\n    test_size = 0.25\n\n\n@step()\ndef load_data(params: DataParameters) -> Output(trainset=Trainset, testset=list):\n    \"\"\"Load the movie len 100k dataset.\n\n    Args:\n        params (DataParameters): Parameters for loading data\n\n    Returns:\n        trainset: data for training\n        testset: data for testing\n    \"\"\"\n    data = Dataset.load_builtin(\"ml-100k\", prompt=False)\n\n    trainset, testset = train_test_split(data, test_size=params.test_size)\n    \n    if mlflow.active_run():\n        mlflow.log_param(\"test_size\", params.test_size)\n        \n    return trainset, testset", "@step()\ndef load_data(params: DataParameters) -> Output(trainset=Trainset, testset=list):\n    \"\"\"Load the movie len 100k dataset.\n\n    Args:\n        params (DataParameters): Parameters for loading data\n\n    Returns:\n        trainset: data for training\n        testset: data for testing\n    \"\"\"\n    data = Dataset.load_builtin(\"ml-100k\", prompt=False)\n\n    trainset, testset = train_test_split(data, test_size=params.test_size)\n    \n    if mlflow.active_run():\n        mlflow.log_param(\"test_size\", params.test_size)\n        \n    return trainset, testset"]}
{"filename": "recommendation/steps/deployer.py", "chunked_list": ["\"\"\"Custom Seldon deployer step.\"\"\"\nfrom zenml.integrations.seldon.seldon_client import SeldonResourceRequirements\nfrom zenml.integrations.seldon.services.seldon_deployment import (\n    SeldonDeploymentConfig,\n)\nfrom zenml.integrations.seldon.steps.seldon_deployer import (\n    CustomDeployParameters,\n    SeldonDeployerStepParameters,\n    seldon_custom_model_deployer_step,\n)", "    seldon_custom_model_deployer_step,\n)\n\nseldon_surprise_custom_deployment = seldon_custom_model_deployer_step(\n    params=SeldonDeployerStepParameters(\n        service_config=SeldonDeploymentConfig(\n            model_name=\"seldon-svd-custom-model\",\n            replicas=1,\n            implementation=\"custom\",\n            resources=SeldonResourceRequirements(", "            implementation=\"custom\",\n            resources=SeldonResourceRequirements(\n                limits={\"cpu\": \"100m\", \"memory\": \"250Mi\"}\n            ),\n        ),\n        timeout=240,\n        custom_deploy_parameters=CustomDeployParameters(\n            predict_function=\"steps.svd_custom_deploy.custom_predict\"\n        ),\n    )", "        ),\n    )\n)"]}
{"filename": "recommendation/steps/train_step.py", "chunked_list": ["\"\"\"A step to train a Singular value decomposition (SVD) model.\"\"\"\nfrom zenml.steps import step, Output\n\nfrom surprise import SVD\nfrom surprise.trainset import Trainset\n\n\n@step\ndef train(trainset: Trainset) -> Output(model=SVD):\n    \"\"\"Train and return a SVD model.\n\n    Args:\n        trainset (Trainset): the data for model training.\n\n    Returns:\n        SVD: the trained SVD model\n    \"\"\"\n    model = SVD()\n\n    model.fit(trainset)\n\n    return model", "def train(trainset: Trainset) -> Output(model=SVD):\n    \"\"\"Train and return a SVD model.\n\n    Args:\n        trainset (Trainset): the data for model training.\n\n    Returns:\n        SVD: the trained SVD model\n    \"\"\"\n    model = SVD()\n\n    model.fit(trainset)\n\n    return model"]}
{"filename": "recommendation/steps/evaluate_step.py", "chunked_list": ["\"\"\"A step to evaluate the train model Root Mean Squared Error (rmse).\"\"\"\nimport mlflow\nfrom zenml.steps import step\n\nfrom surprise import accuracy\nfrom surprise import SVD\n\n@step()\ndef evaluate(model: SVD, testset: list) -> float:\n    \"\"\"Make predictions with the testset and compute for the rmse.\n\n    Args:\n        model (SVD): the trained model\n        testset (list): the test dataset\n\n    Returns:\n        float: rmse\n    \"\"\"\n    predictions = model.test(testset)\n    \n    rmse = accuracy.rmse(predictions)\n    \n    if mlflow.active_run():\n        mlflow.log_metric(\"rmse\", rmse)\n\n    return rmse", "def evaluate(model: SVD, testset: list) -> float:\n    \"\"\"Make predictions with the testset and compute for the rmse.\n\n    Args:\n        model (SVD): the trained model\n        testset (list): the test dataset\n\n    Returns:\n        float: rmse\n    \"\"\"\n    predictions = model.test(testset)\n    \n    rmse = accuracy.rmse(predictions)\n    \n    if mlflow.active_run():\n        mlflow.log_metric(\"rmse\", rmse)\n\n    return rmse", ""]}
{"filename": "recommendation/steps/fetch_model.py", "chunked_list": ["\"\"\"A step to fetch model artifacts for recommendation deployment pipeline.\"\"\"\nfrom typing import Optional\n\nfrom zenml.logger import get_logger\nfrom zenml.post_execution import PipelineView, get_pipeline\nfrom zenml.post_execution.artifact import ArtifactView\nfrom zenml.steps import BaseParameters, Output, step\n\nfrom surprise import SVD\n", "from surprise import SVD\n\nlogger = get_logger(__name__)\n\n\nclass FetchModelParameters(BaseParameters):\n    \"\"\"Parameters for fetch model step.\"\"\"\n\n    # Name of the pipeline to fetch from\n    pipeline_name: str\n\n    # Step name to fetch model from.\n    step_name: str\n\n    # Optional pipeline version\n    pipeline_version: Optional[int] = None", "\n\ndef get_output_from_step(pipeline: PipelineView, step_name: str) -> ArtifactView:\n    \"\"\"Fetch output from a step with last completed run in a pipeline.\n    Args:\n        pipeline (PipelineView): Post-execution pipeline class object.\n        step_name (str): Name of step to fetch output from\n\n    Returns:\n        ArtifactView: Artifact data\n\n    Raises:\n        KeyError: If no step found with given name\n        ValueError: If no output found for step\n    \"\"\"\n    # Get the step with the given name from the last run\n    try:\n        # Get last completed run of the pipeline\n        fetch_last_completed_run = pipeline.get_run_for_completed_step(step_name)\n\n        logger.info(f\"Run used: {fetch_last_completed_run}\")\n\n        # Get the output of the step from last completed run\n        fetch_step = fetch_last_completed_run.get_step(step_name)\n\n        logger.info(f\"Step used: {fetch_step}\")\n    except KeyError as e:\n        logger.error(f\"No step found with name '{step_name}': {e}\")\n        raise e\n\n    # Get the model artifacts from the step\n    output = fetch_step.output\n    if output is None:\n        logger.error(f\"No output found for step '{step_name}'\")\n        raise ValueError(f\"No output found for step '{step_name}'\")\n\n    return output", "\n\ndef get_model_from_step(pipeline: PipelineView, fetch_model_step_name: str) -> SVD:\n    \"\"\"Fetch recommender model from specified pipeline and step name.\n\n    Args:\n        pipeline (PipelineView): Post-execution pipeline class object.\n        fetch_model_step_name (str): Name of step to fetch model from.\n\n    Returns:\n        SVD: surprise SVD recommender model.\n    \"\"\"\n    # Get the output from the step\n    output = get_output_from_step(pipeline, fetch_model_step_name)\n\n    # Read the model artifact from the output\n    model = output.read()\n\n    return model", "\n\n@step()\ndef fetch_model(\n    params: FetchModelParameters,\n) -> Output(model=SVD):\n    \"\"\"Step to fetch model artifacts from last run of the recommendation pipeline.\n\n    Args:\n        params (FetchModelParameters): Parameters for fetch model step.\n\n    Returns:\n        bool: Decision to deploy the model.\n        SVD: Model artifacts.\n\n    Raises:\n        ValueError: if the pipeline does not exist\n        KeyError: if step name parameter is not correct.\n    \"\"\"\n    # Fetch pipeline by name\n    pipeline: PipelineView | None = get_pipeline(\n        params.pipeline_name, version=params.pipeline_version\n    )\n\n    if pipeline is None:\n        logger.error(f\"Pipeline '{params.pipeline_name}' does not exist\")\n        raise ValueError(f\"Pipeline '{params.pipeline_name}' does not exist\")\n\n    logger.info(f\"Pipeline: {pipeline}\")\n\n    # Fetch the model artifacts from the pipeline\n    model = get_model_from_step(pipeline, params.step_name)\n\n    return model", ""]}
{"filename": "recommendation/steps/svd_custom_deploy.py", "chunked_list": ["\"\"\"Functions required by ZenML and Seldon to deploy a custom SVD recommender model to Seldon Core.\"\"\"\nimport numpy as np\nfrom typing import Any, Dict, List, Union\nfrom zenml.logger import get_logger\n\nlogger = get_logger(__name__)\nArray_Like = Union[np.ndarray, List[Any], str, bytes, Dict[str, Any]]\n\n\ndef custom_predict(\n    model: Any,\n    request: Array_Like,\n) -> Array_Like:\n    \"\"\"Custom Prediction function for SVD models.\n    \n    Request input is in the format: [{\"iid\": \"2\", \"uid\": \"26\"}, {\"iid\": \"11\", \"uid\": \"7\"}] \n    where each dictionary is a sample containing a user ID and a item ID to get an expected rating for.\n\n    The custom predict function is the core of the custom deployment, the \n    function is called by the custom deployment class defined for the serving \n    tool. The current implementation requires the function to get the model \n    loaded in the memory and a request with the data to predict.\n\n    Args:\n        model (Any): The model to use for prediction.\n        request: The prediction response of the model is an array-like format.\n        \n    Returns:\n        The prediction in an array-like format. (e.g: np.ndarray, \n        List[Any], str, bytes, Dict[str, Any])\n    \"\"\"\n    inputs = []\n    \n    for instance in request:\n        pred = model.predict(instance['uid'], instance['iid'])\n        inputs.append(pred._asdict())\n        \n    return inputs", "\ndef custom_predict(\n    model: Any,\n    request: Array_Like,\n) -> Array_Like:\n    \"\"\"Custom Prediction function for SVD models.\n    \n    Request input is in the format: [{\"iid\": \"2\", \"uid\": \"26\"}, {\"iid\": \"11\", \"uid\": \"7\"}] \n    where each dictionary is a sample containing a user ID and a item ID to get an expected rating for.\n\n    The custom predict function is the core of the custom deployment, the \n    function is called by the custom deployment class defined for the serving \n    tool. The current implementation requires the function to get the model \n    loaded in the memory and a request with the data to predict.\n\n    Args:\n        model (Any): The model to use for prediction.\n        request: The prediction response of the model is an array-like format.\n        \n    Returns:\n        The prediction in an array-like format. (e.g: np.ndarray, \n        List[Any], str, bytes, Dict[str, Any])\n    \"\"\"\n    inputs = []\n    \n    for instance in request:\n        pred = model.predict(instance['uid'], instance['iid'])\n        inputs.append(pred._asdict())\n        \n    return inputs", "    "]}
{"filename": "recommendation/pipelines/__init__.py", "chunked_list": [""]}
{"filename": "recommendation/pipelines/deploy_recommendation_pipeline.py", "chunked_list": ["\"\"\"A pipeline to deploy recommendation model.\"\"\"\nfrom zenml.pipelines import pipeline\nfrom zenml.config import DockerSettings\n\n# This fixes an environment related issue with the Surprise module, see here: https://github.com/NicolasHug/Surprise/issues/364\ndocker_settings = DockerSettings(apt_packages=[\"gcc\", \"build-essential\"], \n                                 environment={\"SURPRISE_DATA_FOLDER\": \"/tmp\"})\n\n\n@pipeline(settings={\"docker\": docker_settings})\ndef recommendation_deployment_pipeline(\n    fetch_model, deployment_trigger, deploy_model\n):\n    \"\"\"Recommendation deployment pipeline.\n    Args:\n        fetch_model: This step fetches a model artifact from a training pipeline run\n        deploy_model: This step deploys the model with Seldon Core\n    \"\"\"\n    model = fetch_model()\n    \n    decision = deployment_trigger(model)\n    \n    deploy_model(decision, model)", "\n@pipeline(settings={\"docker\": docker_settings})\ndef recommendation_deployment_pipeline(\n    fetch_model, deployment_trigger, deploy_model\n):\n    \"\"\"Recommendation deployment pipeline.\n    Args:\n        fetch_model: This step fetches a model artifact from a training pipeline run\n        deploy_model: This step deploys the model with Seldon Core\n    \"\"\"\n    model = fetch_model()\n    \n    decision = deployment_trigger(model)\n    \n    deploy_model(decision, model)", ""]}
{"filename": "recommendation/pipelines/recommendation_pipeline.py", "chunked_list": ["\"\"\"A pipeline to load, train and evaluate a simple recommendation model.\"\"\"\nfrom zenml.pipelines import pipeline\nfrom zenml.config import DockerSettings\n\ndocker_settings = DockerSettings(apt_packages=[\"gcc\", \"build-essential\"])\n\n\n@pipeline(settings={\"docker\": docker_settings})\ndef recommendation_pipeline(\n    load_data,\n    train,\n    evaluate,\n):\n    \"\"\"Recommendation example pipeline.\n\n    Steps\n    1. load_data: This step load a built in dataset from the Surprise library and splits into train and test set.\n    2. train: This step creates and trains a SVD model with the train set.\n    3. evaluate: This step evaluate the trained model's rmse.\n\n    Args:\n        load_data: This step load a built in dataset from the Surprise library and splits into train and test set.\n        train: This step creates and trains a SVD model with the train set.\n        evaluate: This step evaluate the trained model's rmse.\n    \"\"\"\n    trainset, testset = load_data()\n    model = train(trainset)\n    score = evaluate(model, testset)", "def recommendation_pipeline(\n    load_data,\n    train,\n    evaluate,\n):\n    \"\"\"Recommendation example pipeline.\n\n    Steps\n    1. load_data: This step load a built in dataset from the Surprise library and splits into train and test set.\n    2. train: This step creates and trains a SVD model with the train set.\n    3. evaluate: This step evaluate the trained model's rmse.\n\n    Args:\n        load_data: This step load a built in dataset from the Surprise library and splits into train and test set.\n        train: This step creates and trains a SVD model with the train set.\n        evaluate: This step evaluate the trained model's rmse.\n    \"\"\"\n    trainset, testset = load_data()\n    model = train(trainset)\n    score = evaluate(model, testset)", ""]}
{"filename": "recommendation/tests/__init__.py", "chunked_list": ["\"\"\"Test functions to be imported.\"\"\""]}
{"filename": "recommendation/tests/conftest.py", "chunked_list": ["\"\"\"Fixtures to be used and shared among tests.\"\"\"\nimport pytest \nfrom types import SimpleNamespace\n\n@pytest.fixture\ndef data_parameters() -> dict:\n    \"\"\"Create a dictionary for parameters used in load_data step.\n\n    Returns:\n        dict: dictionary containing parameters for the step\n    \"\"\"\n    parameters = SimpleNamespace()\n    parameters.test_size = 0.25\n\n    return parameters"]}
{"filename": "recommendation/tests/test_steps/test_step_train.py", "chunked_list": ["\"\"\"Test suite for the train step.\"\"\"\nimport pytest\nfrom steps.load_data_step import load_data\nfrom steps.train_step import train\n\nfrom surprise import SVD\nfrom surprise.trainset import Trainset\n\n\n@pytest.fixture\ndef data(data_parameters: dict) -> Trainset:\n    \"\"\"A fixture to get the data used in training the model.\n\n    Args:\n        data_parameters (dict): parameters for train test split\n\n    Returns:\n        Trainset: training data\n    \"\"\"\n    trainset, _ = load_data.entrypoint(data_parameters)\n\n    return trainset", "\n@pytest.fixture\ndef data(data_parameters: dict) -> Trainset:\n    \"\"\"A fixture to get the data used in training the model.\n\n    Args:\n        data_parameters (dict): parameters for train test split\n\n    Returns:\n        Trainset: training data\n    \"\"\"\n    trainset, _ = load_data.entrypoint(data_parameters)\n\n    return trainset", "\n\ndef test_correct_type(data: Trainset):\n    \"\"\"Test whether the trained model has correct type.\n\n    Args:\n        data (Trainset): training data\n    \"\"\"\n    trainset = data\n\n    assert isinstance(train.entrypoint(trainset), SVD)"]}
{"filename": "recommendation/tests/test_steps/test_step_load_data_step.py", "chunked_list": ["\"\"\"Test suite for the load_data step.\"\"\"\nfrom steps.load_data_step import load_data\n\nfrom surprise.trainset import Trainset\n\n\nEXPECTED_DATA_LENGTH = 100000\n\n\ndef test_load_data_step_expected_output_types(data_parameters: dict):\n    \"\"\"Test whether the load_data step output the expected types.\n\n    Args:\n        data_parameters (dict): parameters for train test split\n    \"\"\"\n    trainset, testset = load_data.entrypoint(data_parameters)\n\n    assert isinstance(trainset, Trainset)\n    assert isinstance(testset, list)", "\ndef test_load_data_step_expected_output_types(data_parameters: dict):\n    \"\"\"Test whether the load_data step output the expected types.\n\n    Args:\n        data_parameters (dict): parameters for train test split\n    \"\"\"\n    trainset, testset = load_data.entrypoint(data_parameters)\n\n    assert isinstance(trainset, Trainset)\n    assert isinstance(testset, list)", "\n\ndef test_load_data_step_expected_data_amount(data_parameters: dict):\n    \"\"\"Test whether the load_data step split train and test data as expected.\n\n    Args:\n        data_parameters (dict): parameters for train test split\n    \"\"\"\n    trainset, testset = load_data.entrypoint(data_parameters)\n\n    expected_size_train = int(EXPECTED_DATA_LENGTH * (1 - data_parameters.test_size))\n    expected_size_test = int(EXPECTED_DATA_LENGTH * data_parameters.test_size)\n\n    assert trainset.n_ratings + len(testset) == EXPECTED_DATA_LENGTH\n\n    assert trainset.n_ratings == expected_size_train\n    assert len(testset) == expected_size_test", "\n"]}
{"filename": "recommendation/tests/test_steps/test_step_evaluate_step.py", "chunked_list": ["\"\"\"Test suite for the evaluate step.\"\"\"\nimport pytest\nfrom typing import Tuple\n\nfrom steps.load_data_step import load_data\nfrom steps.train_step import train\nfrom steps.evaluate_step import evaluate\n\nfrom surprise.trainset import Trainset\nfrom surprise import Dataset", "from surprise.trainset import Trainset\nfrom surprise import Dataset\nfrom surprise import SVD\n\nBENCHMARK_SVD_SCORE = 0.93\n\n@pytest.fixture\ndef data(data_parameters: dict) -> Tuple[Trainset, list]:\n    \"\"\"Pytest fixtures for getting train and test data.\n\n    Args:\n        data_parameters (dict): parameters for train test split\n\n    Returns:\n        Tuple[Trainset, list]: train and test data\n    \"\"\"\n    trainset, testset = load_data.entrypoint(data_parameters)\n\n    return trainset, testset", "\n\n@pytest.fixture\ndef model(data: Dataset) -> SVD:\n    \"\"\"Pytest fixture for getting a trained model.\n\n    Args:\n        data (Dataset): training data\n\n    Returns:\n        SVD: the trained model\n    \"\"\"\n    trainset, _ = data\n\n    return train.entrypoint(trainset)", "\n\ndef test_rmse_equals_benchmarks(model: SVD, data: Dataset):\n    \"\"\"Test whether the trained model rmse is roughly equals to the benchmark score.\n\n    Args:\n        model (SVD): model for recommendation\n        data (Dataset): dataset for testing\n    \"\"\"\n    _, testset = data\n\n    rmse = evaluate.entrypoint(model, testset)\n\n    # assert that the accuracy is 0.93 +/- 0.1\n    assert rmse == pytest.approx(BENCHMARK_SVD_SCORE, rel=0.1)", ""]}
{"filename": "recommendation/tests/test_pipelines/test_pipeline.py", "chunked_list": ["\"\"\"Integration tests for the pipeline as a whole.\"\"\"\nimport pytest\n\nimport os\nimport logging\nfrom steps.load_data_step import load_data\nfrom steps.train_step import train\nfrom steps.evaluate_step import evaluate\n\nfrom zenml.logger import disable_logging", "\nfrom zenml.logger import disable_logging\nfrom zenml.post_execution import get_unlisted_runs\nfrom zenml.post_execution.pipeline_run import PipelineRunView\n\nfrom pipelines.recommendation_pipeline import recommendation_pipeline\nfrom materializer import SurpriseMaterializer\n\nfrom surprise import SVD\n", "from surprise import SVD\n\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nEXPECTED_DATA_LENGTH = 100000\nBENCHMARK_SVD_SCORE = 0.93\n\n\n@pytest.fixture(scope=\"class\", autouse=True)\ndef pipeline_run():\n    \"\"\"Set up fixture for running the pipeline.\"\"\"\n    pipeline = recommendation_pipeline(\n        load_data().configure(output_materializers=SurpriseMaterializer),\n        train().configure(output_materializers=SurpriseMaterializer),\n        evaluate(),\n    )\n\n    with disable_logging(log_level=logging.INFO):\n        pipeline.run(config_path=BASE_DIR + \"/test_pipeline_config.yaml\", unlisted=True)", "@pytest.fixture(scope=\"class\", autouse=True)\ndef pipeline_run():\n    \"\"\"Set up fixture for running the pipeline.\"\"\"\n    pipeline = recommendation_pipeline(\n        load_data().configure(output_materializers=SurpriseMaterializer),\n        train().configure(output_materializers=SurpriseMaterializer),\n        evaluate(),\n    )\n\n    with disable_logging(log_level=logging.INFO):\n        pipeline.run(config_path=BASE_DIR + \"/test_pipeline_config.yaml\", unlisted=True)", "\n\n@pytest.fixture()\ndef get_pipeline_run() -> PipelineRunView:\n    \"\"\"Get the most recent pipeline run.\n\n    Args:\n        pipeline_run: the pipeline run fixture which is executed once.\n\n    Returns:\n        PipelineRunView: the test run\n    \"\"\"\n    return get_unlisted_runs()[0]", "\n\ndef test_pipeline_executes(get_pipeline_run: PipelineRunView):\n    \"\"\"Test the model training result from the the pipeline run.\n\n    Args:\n        get_pipeline_run (PipelineRunView): pipeline run.\n    \"\"\"\n    rmse = get_pipeline_run.get_step(step=\"evaluate\").output.read()\n\n    assert rmse == pytest.approx(BENCHMARK_SVD_SCORE, rel=0.1)", "\n\ndef test_pipeline_loads_and_splits_correctly(get_pipeline_run: PipelineRunView, data_parameters: dict):\n    \"\"\"Test whether the pipeline splits data into train and test correctly.\n\n    Args:\n        get_pipeline_run (PipelineRunView): the pipeline run\n        data_parameters (dict): parameters for train test split\n    \"\"\"\n    step_outputs = get_pipeline_run.get_step(step=\"load_data\").outputs\n    trainset = step_outputs[\"trainset\"].read()\n    testset = step_outputs[\"testset\"].read()\n\n    expected_size_train = int(EXPECTED_DATA_LENGTH * (1 - data_parameters.test_size))\n    expected_size_test = int(EXPECTED_DATA_LENGTH * data_parameters.test_size)\n\n    assert trainset.n_ratings + len(testset) == EXPECTED_DATA_LENGTH\n\n    assert trainset.n_ratings == expected_size_train\n    assert len(testset) == expected_size_test", "\n\ndef test_correct_model_type(get_pipeline_run: PipelineRunView):\n    \"\"\"Test whether the pipeline return the correct model type.\n\n    Args:\n        get_pipeline_run (PipelineRunView): the pipeline run\n    \"\"\"\n    step_output = get_pipeline_run.get_step(step='train').output.read()\n\n    assert isinstance(step_output, SVD)", ""]}
{"filename": "recommendation/materializer/__init__.py", "chunked_list": ["\"\"\"Materializer to be imported.\"\"\"\nfrom .surprise_materializer import SurpriseMaterializer"]}
{"filename": "recommendation/materializer/surprise_materializer.py", "chunked_list": ["\"\"\"Custom materializer for Surprise data and model.\"\"\"\nimport os\nimport pickle\n\nfrom zenml.io import fileio\nfrom zenml.materializers.base_materializer import BaseMaterializer\n\nfrom typing import Any, Type, Union\n\nfrom surprise import AlgoBase", "\nfrom surprise import AlgoBase\nfrom surprise.trainset import Trainset\n\nDEFAULT_FILENAME = \"surprise_output.pickle\"\n\nclass SurpriseMaterializer(BaseMaterializer):\n    \"\"\"Custom materializer for handling object created using Surprise such as dataset and models.\"\"\"\n\n    ASSOCIATED_TYPES = (AlgoBase, Trainset, list)\n\n    def __init__(self, uri: str):\n        \"\"\"Initializes a materializer with the given URI.\"\"\"\n        self.uri = uri\n\n    def load(self, data_type: Type[Any]) -> Union[AlgoBase, Trainset]:\n        \"\"\"This function loads the input from the artifact store and returns it.\n\n        Args:\n            data_type (Type[Any]): The type of the artifact.\n\n        Returns:\n            Union[AlgoBase, Trainset]: the output of the artifact.\n        \"\"\"\n        # read from self.uri\n        super().load(data_type)\n        \n        filepath = os.path.join(self.uri, DEFAULT_FILENAME)\n        with fileio.open(filepath, \"rb\") as fid:\n            obj = pickle.load(fid)\n        return obj\n\n    def save(self, obj: Any) -> None:\n        \"\"\"This function saves the artifact to the artifact store.\n\n        Args:\n            obj (any): The input artifact to be saved\n        \"\"\"\n        # write `data` to self.uri\n        super().save(obj)\n        \n        model_filepath = os.path.join(self.uri, DEFAULT_FILENAME)\n        with fileio.open(model_filepath, \"wb\") as fid:\n            pickle.dump(obj, fid)"]}
