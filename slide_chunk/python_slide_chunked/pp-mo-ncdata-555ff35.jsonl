{"filename": "setup.py", "chunked_list": ["# noqa\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=\"ncdata\",\n    version=\"0.0.1\",\n    url=\"https://github.com/pp-mo/ncdata.git\",\n    author=\"pp-mo\",\n    author_email=\"patrick.peglar@metoffice.gov.uk\",\n    description=\"NetCDF data interoperability between Iris and Xarray\",", "    author_email=\"patrick.peglar@metoffice.gov.uk\",\n    description=\"NetCDF data interoperability between Iris and Xarray\",\n    packages=find_packages(where=\"lib\"),\n    package_dir={\"\": \"lib\"},\n    install_requires=[\"numpy\"],\n)\n"]}
{"filename": "tests/_compare_nc_datasets.py", "chunked_list": ["\"\"\"\nUtility for comparing 2 netcdf datasets.\n\nWorks with file-specs, netCDF4.Datasets *or* NcData.\n\nFor purposes of testing ncdata.netcdf4 behaviour.\nTODO: one day might be public ?\n\"\"\"\n\nfrom pathlib import Path", "\nfrom pathlib import Path\nfrom typing import AnyStr, List, Union\nfrom warnings import warn\n\nimport netCDF4\nimport netCDF4 as nc\nimport numpy as np\n\nfrom ncdata import NcData", "\nfrom ncdata import NcData\n\n\ndef compare_nc_datasets(\n    dataset_or_path_1: Union[Path, AnyStr, nc.Dataset, NcData],\n    dataset_or_path_2: Union[Path, AnyStr, nc.Dataset, NcData],\n    check_dims_order: bool = True,\n    check_vars_order: bool = True,\n    check_attrs_order: bool = True,\n    check_groups_order: bool = True,\n    check_var_data: bool = True,\n    suppress_warnings: bool = False,\n) -> List[str]:\n    r\"\"\"\n    Compare netcdf data.\n\n    Accepts paths, pathstrings, open :class:`netCDF4.Dataset`\\\\s or :class:`NcData` objects.\n\n    Parameters\n    ----------\n    dataset_or_path_1, dataset_or_path_2 : str or Path or netCDF4.Dataset or NcData\n        two datasets to compare, either NcData or netCDF4\n    check_dims_order, check_vars_order, check_attrs_order, check_groups_order : bool, default True\n        If False, no error results from the same contents in a different order,\n        however unless `suppress_warnings` is True, the error string is issued as a warning.\n    check_var_data : bool, default True\n        If True, all variable data is also checked for equality.\n        If False, only dtype and shape are compared.\n    suppress_warnings : bool, default False\n        When False (the default), report changes in content order as Warnings.\n        When True, ignore changes in ordering.\n\n    Returns\n    -------\n    errs : list of str\n        a list of error strings.\n        If empty, no differences were found.\n\n    \"\"\"\n    ds1_was_path = not hasattr(dataset_or_path_1, \"variables\")\n    ds2_was_path = not hasattr(dataset_or_path_2, \"variables\")\n    ds1, ds2 = None, None\n    try:\n        if ds1_was_path:\n            ds1 = nc.Dataset(dataset_or_path_1)\n        else:\n            ds1 = dataset_or_path_1\n\n        if ds2_was_path:\n            ds2 = nc.Dataset(dataset_or_path_2)\n        else:\n            ds2 = dataset_or_path_2\n\n        errs = []\n        _compare_nc_groups(\n            errs,\n            ds1,\n            ds2,\n            group_id_string=\"Dataset\",\n            dims_order=check_dims_order,\n            vars_order=check_vars_order,\n            attrs_order=check_attrs_order,\n            groups_order=check_groups_order,\n            data_equality=check_var_data,\n            suppress_warnings=suppress_warnings,\n        )\n    finally:\n        if ds1_was_path and ds1:\n            ds1.close()\n        if ds2_was_path and ds2:\n            ds2.close()\n\n    return errs", "\n\ndef _compare_name_lists(\n    errslist, l1, l2, elemname, order_strict=True, suppress_warnings=False\n):\n    msg = f\"{elemname} do not match: {list(l1)} != {list(l2)}\"\n    ok = l1 == l2\n    ok_except_order = ok\n    if not ok:\n        ok_except_order = sorted(l1) == sorted(l2)\n\n    if not ok:\n        if not ok_except_order or order_strict:\n            errslist.append(msg)\n        elif ok_except_order and not suppress_warnings:\n            warn(\"(Ignoring: \" + msg + \" )\", category=UserWarning)", "\n\ndef _isncdata(obj):\n    \"\"\"\n    A crude test to distinguish NcData objects from similar netCDF4 ones.\n\n    Used to support comparisons on either type of data.\n    \"\"\"\n    return hasattr(obj, \"_print_content\")\n", "\n\ndef _compare_attributes(\n    errs,\n    obj1,\n    obj2,\n    elemname,\n    attrs_order=True,\n    suppress_warnings=False,\n    force_first_attrnames=None,\n):\n    \"\"\"\n    Compare attribute name lists.\n\n    Does not return results, but appends error messages to 'errs'.\n    \"\"\"\n    attrnames, attrnames2 = [\n        obj.attributes.keys() if _isncdata(obj) else obj.ncattrs()\n        for obj in (obj1, obj2)\n    ]\n    if attrs_order and force_first_attrnames:\n\n        def fix_orders(attrlist):\n            for name in force_first_attrnames[::-1]:\n                if name in attrlist:\n                    attrlist = [name] + [n for n in attrlist if n != name]\n            return attrlist\n\n        attrnames = fix_orders(attrnames)\n        attrnames2 = fix_orders(attrnames2)\n\n    _compare_name_lists(\n        errs,\n        attrnames,\n        attrnames2,\n        f\"{elemname} attribute lists\",\n        order_strict=attrs_order,\n        suppress_warnings=suppress_warnings,\n    )\n\n    # Compare the attributes themselves (dtypes and values)\n    for attrname in attrnames:\n        if attrname not in attrnames2:\n            # Only compare attributes existing on both inputs.\n            continue\n\n        attr, attr2 = [\n            (\n                obj.attributes[attrname].value\n                if _isncdata(obj)\n                else obj.getncattr(attrname)\n            )\n            for obj in (obj1, obj2)\n        ]\n\n        dtype, dtype2 = [\n            # Get x.dtype, or fallback on type(x) -- basically, for strings.\n            getattr(attr, \"dtype\", type(attr))\n            for attr in (attr, attr2)\n        ]\n\n        if dtype != dtype2:\n            msg = (\n                f'{elemname} \"{attrname}\" attribute datatypes differ : '\n                f\"{dtype!r} != {dtype2!r}\"\n            )\n            errs.append(msg)\n        else:\n            # If values match (only then), compare datatypes\n            # Cast attrs, which might be strings, to arrays for comparison\n            arr, arr2 = [np.asarray(attr) for attr in (attr, attr2)]\n            if arr.shape != arr2.shape or not np.all(arr == arr2):\n                msg = (\n                    f'{elemname} \"{attrname}\" attribute values differ : '\n                    f\"{attr!r} != {attr2!r}\"\n                )\n                errs.append(msg)", "\n\ndef _compare_nc_groups(\n    errs: List[str],\n    g1: Union[netCDF4.Dataset, netCDF4.Group],\n    g2: Union[netCDF4.Dataset, netCDF4.Group],\n    group_id_string: str,\n    dims_order: bool = True,\n    vars_order: bool = True,\n    attrs_order: bool = True,\n    groups_order: bool = True,\n    data_equality: bool = True,\n    suppress_warnings: bool = False,\n):\n    \"\"\"\n    Inner routine to compare either whole datasets or subgroups.\n\n    Note that, rather than returning a list of error strings, it appends them to the\n    passed arg `errs`.  This just makes recursive calling easier.\n    \"\"\"\n    # Compare lists of dimension names\n    dimnames, dimnames2 = [list(grp.dimensions.keys()) for grp in (g1, g2)]\n    _compare_name_lists(\n        errs,\n        dimnames,\n        dimnames2,\n        f\"{group_id_string} dimension lists\",\n        order_strict=dims_order,\n        suppress_warnings=suppress_warnings,\n    )\n\n    # Compare the dimensions themselves\n    for dimname in dimnames:\n        if dimname not in dimnames2:\n            continue\n        d1, d2 = [grp.dimensions[dimname] for grp in (g1, g2)]\n        dimlen, dimlen2 = [dim.size for dim in (d1, d2)]\n        if dimlen != dimlen2:\n            msg = (\n                f'{group_id_string} \"{dimname}\" dimensions '\n                f\"have different sizes: {dimlen} != {dimlen2}\"\n            )\n            errs.append(msg)\n\n    # Compare file attributes\n    _compare_attributes(\n        errs,\n        g1,\n        g2,\n        group_id_string,\n        attrs_order=attrs_order,\n        suppress_warnings=suppress_warnings,\n    )\n\n    # Compare lists of variables\n    varnames, varnames2 = [list(grp.variables.keys()) for grp in (g1, g2)]\n    _compare_name_lists(\n        errs,\n        varnames,\n        varnames2,\n        f\"{group_id_string} variable lists\",\n        order_strict=dims_order,\n        suppress_warnings=suppress_warnings,\n    )\n\n    # Compare the variables themselves\n    for varname in varnames:\n        if varname not in varnames2:\n            continue\n        v1, v2 = [grp.variables[varname] for grp in (g1, g2)]\n\n        var_id_string = f'{group_id_string} variable \"{varname}\"'\n\n        # dimensions\n        dims, dims2 = [v.dimensions for v in (v1, v2)]\n        if dims != dims2:\n            msg = f\"{var_id_string} dimensions differ : {dims!r} != {dims2!r}\"\n\n        # attributes\n        _compare_attributes(\n            errs,\n            v1,\n            v2,\n            var_id_string,\n            attrs_order=attrs_order,\n            suppress_warnings=suppress_warnings,\n            force_first_attrnames=[\n                \"_FillValue\"\n            ],  # for some reason, this doesn't always list consistently\n        )\n\n        # dtypes\n        dtype, dtype2 = [\n            v.dtype if _isncdata(v) else v.datatype for v in (v1, v2)\n        ]\n        if dtype != dtype2:\n            msg = f\"{var_id_string} datatypes differ : {dtype!r} != {dtype2!r}\"\n            errs.append(msg)\n\n        # data values\n        is_str, is_str2 = (dt.kind in (\"U\", \"S\") for dt in (dtype, dtype2))\n        # TODO: is this correct check to allow compare between different dtypes?\n        if data_equality and dims == dims2 and is_str == is_str2:\n            # N.B. don't check shapes here: we already checked dimensions.\n            # NOTE: no attempt to use laziness here.  Could be improved.\n            def getdata(var):\n                if _isncdata(var):\n                    data = var.data\n                    if hasattr(data, \"compute\"):\n                        data = data.compute()\n                else:\n                    # expect var to be an actual netCDF4.Variable\n                    # (check for obscure property NOT provided by mimics)\n                    assert hasattr(var, \"use_nc_get_vars\")\n                    data = var[:]\n                # Return 0D as 1D, as this makes results simpler to interpret.\n                if data.ndim == 0:\n                    data = data.flatten()\n                    assert data.shape == (1,)\n                return data\n\n            data, data2 = (getdata(v) for v in (v1, v2))\n            flatdata, flatdata2 = (\n                np.asanyarray(arr).flatten() for arr in (data, data2)\n            )\n\n            # For simpler checking, use flat versions\n            flat_diff_inds = (\n                []\n            )  # NB *don't* make this an array, it causes problems\n            if any(np.ma.is_masked(arr) for arr in (data, data2)):\n                # If data is masked, count mask mismatches and skip those points\n                mask, mask2 = (\n                    np.ma.getmaskarray(array)\n                    for array in (flatdata, flatdata2)\n                )\n                flat_diff_inds = list(np.where(mask != mask2)[0])\n                # Replace all masked points to exclude them from unmasked-point checks.\n                either_masked = mask | mask2\n                dtype = flatdata.dtype\n                if dtype.kind in (\"S\", \"U\"):\n                    safe_fill_const = \"\"\n                else:\n                    safe_fill_const = np.zeros((1,), dtype=flatdata.dtype)[0]\n                flatdata[either_masked] = safe_fill_const\n                flatdata2[either_masked] = safe_fill_const\n\n            flat_diff_inds += list(np.where(flatdata != flatdata2)[0])\n            n_diffs = len(flat_diff_inds)\n            if n_diffs:\n                msg = f\"{var_id_string} data contents differ, at {n_diffs} points: \"\n                ellps = \", ...\" if n_diffs > 2 else \"\"\n                diffinds = flat_diff_inds[:2]\n                diffinds = [\n                    np.unravel_index(ind, shape=data.shape) for ind in diffinds\n                ]\n                diffinds_str = \", \".join(repr(tuple(x)) for x in diffinds)\n                inds_str = f\"[{diffinds_str}{ellps}]\"\n                points_lhs_str = \", \".join(repr(data[ind]) for ind in diffinds)\n                points_rhs_str = \", \".join(\n                    repr(data2[ind]) for ind in diffinds\n                )\n                points_lhs_str = f\"[{points_lhs_str}{ellps}]\"\n                points_rhs_str = f\"[{points_rhs_str}{ellps}]\"\n                msg += (\n                    f\"@INDICES{inds_str}\"\n                    f\" : LHS={points_lhs_str}, RHS={points_rhs_str}\"\n                )\n                errs.append(msg)\n\n    # Finally, recurse over groups\n    grpnames, grpnames2 = [list(grp.groups.keys()) for grp in (g1, g2)]\n    _compare_name_lists(\n        errs,\n        grpnames,\n        grpnames2,\n        f\"{group_id_string} subgroup lists\",\n        order_strict=groups_order,\n        suppress_warnings=suppress_warnings,\n    )\n    for grpname in grpnames:\n        if grpname not in grpnames2:\n            continue\n        grp1, grp2 = [grp.groups[grpname] for grp in (g1, g2)]\n        _compare_nc_groups(\n            errs,\n            grp1,\n            grp2,\n            group_id_string=f\"{group_id_string}/{grpname}\",\n            dims_order=dims_order,\n            vars_order=vars_order,\n            attrs_order=attrs_order,\n            groups_order=groups_order,\n            data_equality=data_equality,\n        )", "\n\nif __name__ == \"__main__\":\n    fps = [\n        \"/home/h05/itpp/tmp.nc\",\n        \"/home/h05/itpp/tmp2.nc\",\n        \"/home/h05/itpp/mask.nc\",\n        \"/home/h05/itpp/tmps.nc\",\n        \"/home/h05/itpp/tmps2.nc\",\n    ]\n    fp1, fp2, fp3, fp4, fp5 = fps\n    pairs = [\n        [fp1, fp1],\n        [fp1, fp2],\n        [fp1, fp3],\n        [fp4, fp5],\n    ]\n    for p1, p2 in pairs:\n        errs = compare_nc_datasets(p1, p2, check_attrs_order=False)\n        print(\"\")\n        print(f\"Compare {p1} with {p2} : {len(errs)} errors \")\n        for err in errs:\n            print(\"  \", err)\n        print(\"-ends-\")", ""]}
{"filename": "tests/__init__.py", "chunked_list": ["\"\"\"Tests for :mod:`ncdata`.\"\"\"\nfrom pathlib import Path\n\ntestdata_dir = Path(__file__).parent / \"testdata\"\n"]}
{"filename": "tests/data_testcase_schemas.py", "chunked_list": ["\"\"\"\nDefine a set of \"standard\" test cases, built as actual netcdf files.\n\nThe main product is a pytest fixture `standard_testcase`, which is parametrised over\ntestcase names, and returns info on the testcase, its defining spec and a filepath it\ncan be loaded from.\n\nThese testcases also include various pre-existing testfiles, which are NOT built from\nspecs.  This enables us to perform various translation tests on standard testfiles from\nthe Iris and Xarray test suites.", "specs.  This enables us to perform various translation tests on standard testfiles from\nthe Iris and Xarray test suites.\n\"\"\"\nimport shutil\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport re\nfrom typing import Iterable, Union, Tuple, Dict\n\nimport netCDF4", "\nimport netCDF4\nimport netCDF4 as nc\nimport numpy as np\nimport pytest\n\nimport iris.tests\n\n\ndef data_types():\n    \"\"\"\n    Produce a sequence of valid netCDF4 datatypes.\n\n    All possible datatypes for variable data or attributes.\n    Not yet supporting variable or user-defined (structured) types.\n\n    Results are strings for all valid numeric dtypes, plus 'string'.\n    The strings are our choice, chosen to suit inclusion in pytest parameter naming.\n    \"\"\"\n    # unsigned int types\n    for n in (1, 2, 4, 8):\n        yield f\"u{n}\"\n    # signed int types\n    for n in (1, 2, 4, 8):\n        yield f\"i{n}\"\n    # float types\n    for n in (4, 8):\n        yield f\"f{n}\"\n    # .. and strings\n    # NB we use 'string' instead of 'S1', as strings always need handling differently.\n    yield \"string\"", "\ndef data_types():\n    \"\"\"\n    Produce a sequence of valid netCDF4 datatypes.\n\n    All possible datatypes for variable data or attributes.\n    Not yet supporting variable or user-defined (structured) types.\n\n    Results are strings for all valid numeric dtypes, plus 'string'.\n    The strings are our choice, chosen to suit inclusion in pytest parameter naming.\n    \"\"\"\n    # unsigned int types\n    for n in (1, 2, 4, 8):\n        yield f\"u{n}\"\n    # signed int types\n    for n in (1, 2, 4, 8):\n        yield f\"i{n}\"\n    # float types\n    for n in (4, 8):\n        yield f\"f{n}\"\n    # .. and strings\n    # NB we use 'string' instead of 'S1', as strings always need handling differently.\n    yield \"string\"", "\n\n# Just confirm that the above list of types matches those used by netCDF4, except for\n# 'string' replacing 'S1'.\nassert set(data_types()) == (\n    set(nc.default_fillvals.keys()) - set([\"S1\"]) | set([\"string\"])\n)\n\n\n# Suitable test values for each attribute/data type.", "\n# Suitable test values for each attribute/data type.\n_INT_Approx_2x31 = int(2**31 - 1)\n_INT_Approx_2x32 = int(2**32 - 2)\n_INT_Approx_2x63 = int(2**63 - 3)\n_INT_Approx_2x64 = int(2**64 - 4)\n\n_Datatype_Sample_Values = {\n    \"u1\": np.array([3, 250], dtype=\"u1\"),\n    \"i1\": np.array([-120, 120], dtype=\"i1\"),", "    \"u1\": np.array([3, 250], dtype=\"u1\"),\n    \"i1\": np.array([-120, 120], dtype=\"i1\"),\n    \"u2\": np.array([4, 65001], dtype=\"u2\"),\n    \"i2\": np.array([-30001, 30002], dtype=\"i2\"),\n    \"u4\": np.array([5, _INT_Approx_2x32], dtype=\"u4\"),\n    \"i4\": np.array([-_INT_Approx_2x31, _INT_Approx_2x31], dtype=\"i4\"),\n    \"u8\": np.array([6, _INT_Approx_2x64], dtype=\"u8\"),\n    \"i8\": np.array([-_INT_Approx_2x63, _INT_Approx_2x63], dtype=\"i8\"),\n    \"f4\": np.array([1.23e-9, -34.0e12], dtype=\"f4\"),\n    \"f8\": np.array([1.23e-34, -34.0e52], dtype=\"f8\"),", "    \"f4\": np.array([1.23e-9, -34.0e12], dtype=\"f4\"),\n    \"f8\": np.array([1.23e-34, -34.0e52], dtype=\"f8\"),\n    \"string\": [\"one\", \"three\"],\n}\n# Just confirm that the list of sample-value types matches 'data_types'.\nassert set(_Datatype_Sample_Values.keys()) == set(data_types())\n\n\ndef _write_nc4_dataset(\n    spec: dict,\n    ds: nc.Dataset,  # or an inner Group\n    parent_spec: dict = None,\n):\n    \"\"\"\n    Create a netcdf test file from a 'testcase spec'.\n\n    Inner routine for ``make_testcase_dataset``.\n    Separated for recursion and to keep dataset open/close in separate wrapper routine.\n    \"\"\"\n\n    # Convenience function\n    def objmap(objs):\n        \"\"\"Create a map from a list of dim-specs, indexing by their names.\"\"\"\n        return {obj[\"name\"]: obj for obj in objs}\n\n    # Get sub-components of the spec.\n    # NB most elements are lists of dicts which all have a 'name' item ..\n    var_dims = objmap(spec.get(\"dims\", []))\n    var_specs = objmap(spec.get(\"vars\", []))\n    group_specs = objmap(spec.get(\"groups\", []))\n    # .. but attr specs are a simple dict\n    attr_specs = spec.get(\"attrs\", {})\n\n    # Assign actual file attrs (group or global)\n    for name, value in attr_specs.items():\n        ds.setncattr(name, value)\n\n    # Create actual file dims\n    for dim_name, dim in var_dims.items():\n        # Dims are a simple list, containing names as a dict entry\n        size, unlimited = dim[\"size\"], dim.get(\"unlimited\", False)\n        if unlimited:\n            size = 0\n        ds.createDimension(dim_name, size)\n\n    # Create and configure actual file variables\n    for var_name, var_spec in var_specs.items():\n        var_dims = var_spec[\"dims\"]  # this is just a list of names\n        data = var_spec.get(\"data\", None)\n        if data is not None:\n            data = np.array(data)\n            dtype = data.dtype\n        else:\n            dtype = np.dtype(var_spec[\"dtype\"])\n        attrs = var_spec.get(\"attrs\", {})\n        fill_value = attrs.pop(\"_FillValue\", None)\n\n        # Create the actual data variable\n        nc_var = ds.createVariable(\n            varname=var_name,\n            dimensions=var_dims,\n            datatype=dtype,\n            fill_value=fill_value,\n        )\n\n        # Add attributes\n        for name, value in attrs.items():\n            nc_var.setncattr(name, value)\n\n        # Add data, provided or default-constructed\n        if data is None:\n            shape = nc_var.shape\n            n_points = int(np.product(shape))\n            if nc_var.dtype.kind == 'S':\n                data = np.array(\n                    'abcdefghijklmnopqrstuvwxyz'[:n_points],\n                    dtype='S1'\n                )\n            else:\n                data = np.arange(1, n_points + 1)\n            data = data.astype(dtype)\n            i_miss = var_spec.get(\"missing_inds\", [])\n            if i_miss:\n                data = np.ma.masked_array(data)\n                data[i_miss] = np.ma.masked\n            data = data.reshape(shape)\n        nc_var[:] = data\n\n    # Finally, recurse over sub-groups\n    for group_name, group_spec in group_specs.items():\n        nc_group = ds.createGroup(group_name)\n        _write_nc4_dataset(\n            spec=group_spec,\n            ds=nc_group,\n            parent_spec=parent_spec,\n        )", "def _write_nc4_dataset(\n    spec: dict,\n    ds: nc.Dataset,  # or an inner Group\n    parent_spec: dict = None,\n):\n    \"\"\"\n    Create a netcdf test file from a 'testcase spec'.\n\n    Inner routine for ``make_testcase_dataset``.\n    Separated for recursion and to keep dataset open/close in separate wrapper routine.\n    \"\"\"\n\n    # Convenience function\n    def objmap(objs):\n        \"\"\"Create a map from a list of dim-specs, indexing by their names.\"\"\"\n        return {obj[\"name\"]: obj for obj in objs}\n\n    # Get sub-components of the spec.\n    # NB most elements are lists of dicts which all have a 'name' item ..\n    var_dims = objmap(spec.get(\"dims\", []))\n    var_specs = objmap(spec.get(\"vars\", []))\n    group_specs = objmap(spec.get(\"groups\", []))\n    # .. but attr specs are a simple dict\n    attr_specs = spec.get(\"attrs\", {})\n\n    # Assign actual file attrs (group or global)\n    for name, value in attr_specs.items():\n        ds.setncattr(name, value)\n\n    # Create actual file dims\n    for dim_name, dim in var_dims.items():\n        # Dims are a simple list, containing names as a dict entry\n        size, unlimited = dim[\"size\"], dim.get(\"unlimited\", False)\n        if unlimited:\n            size = 0\n        ds.createDimension(dim_name, size)\n\n    # Create and configure actual file variables\n    for var_name, var_spec in var_specs.items():\n        var_dims = var_spec[\"dims\"]  # this is just a list of names\n        data = var_spec.get(\"data\", None)\n        if data is not None:\n            data = np.array(data)\n            dtype = data.dtype\n        else:\n            dtype = np.dtype(var_spec[\"dtype\"])\n        attrs = var_spec.get(\"attrs\", {})\n        fill_value = attrs.pop(\"_FillValue\", None)\n\n        # Create the actual data variable\n        nc_var = ds.createVariable(\n            varname=var_name,\n            dimensions=var_dims,\n            datatype=dtype,\n            fill_value=fill_value,\n        )\n\n        # Add attributes\n        for name, value in attrs.items():\n            nc_var.setncattr(name, value)\n\n        # Add data, provided or default-constructed\n        if data is None:\n            shape = nc_var.shape\n            n_points = int(np.product(shape))\n            if nc_var.dtype.kind == 'S':\n                data = np.array(\n                    'abcdefghijklmnopqrstuvwxyz'[:n_points],\n                    dtype='S1'\n                )\n            else:\n                data = np.arange(1, n_points + 1)\n            data = data.astype(dtype)\n            i_miss = var_spec.get(\"missing_inds\", [])\n            if i_miss:\n                data = np.ma.masked_array(data)\n                data[i_miss] = np.ma.masked\n            data = data.reshape(shape)\n        nc_var[:] = data\n\n    # Finally, recurse over sub-groups\n    for group_name, group_spec in group_specs.items():\n        nc_group = ds.createGroup(group_name)\n        _write_nc4_dataset(\n            spec=group_spec,\n            ds=nc_group,\n            parent_spec=parent_spec,\n        )", "\n\ndef make_testcase_dataset(filepath, spec):\n    \"\"\"\n    Generic routine for converting a test dataset 'spec' into an actual netcdf file.\n\n    Rather frustratingly similar to ncdata.to_nc4, but it needs to remain separate as\n    we use it to test that code (!)\n\n    specs are just a structure of dicts and lists...\n    group_spec = {\n        'name': str\n        'dims': [ *{'name':str, size:int [, 'unlim':True]} ]\n        'attrs': {'name':str, 'value':array}\n        'vars': [ *{\n                        'name':str,\n                        'dims':list[str],\n                        [, 'attrs': {'name':str, 'value':array}]\n                        [, 'dtype':dtype]\n                        [, 'data':array]\n                        [, 'missing_inds': list(int)]  # NB **flat** indexes\n                    }\n                ]\n        'groups': list of group_spec\n    }\n\n    From this we populate dims + vars, all filling data content according to certain\n    default rules (or the provided 'data' item).\n    \"\"\"\n    ds = nc.Dataset(filepath, \"w\")\n    try:\n        if 'ds__dtype__string' in filepath:\n            pass\n        _write_nc4_dataset(spec, ds)\n    finally:\n        ds.close()", "\n\n_minimal_variable_test_spec = {\n    \"vars\": [dict(name=\"var_0\", dims=[], dtype=int)]\n}\n\n\n_simple_test_spec = {\n    \"name\": \"\",\n    \"dims\": [dict(name=\"x\", size=3), dict(name=\"y\", size=2)],", "    \"name\": \"\",\n    \"dims\": [dict(name=\"x\", size=3), dict(name=\"y\", size=2)],\n    \"attrs\": {\"ga1\": 2.3, \"gas2\": \"this\"},\n    \"vars\": [dict(name=\"vx\", dims=[\"x\"], dtype=np.int32)],\n    \"groups\": [\n        {\n            \"name\": \"g_inner\",\n            \"dims\": [dict(name=\"dim_i1\", size=4)],\n            \"vars\": [\n                dict(", "            \"vars\": [\n                dict(\n                    name=\"vix1\",\n                    dims=[\"dim_i1\", \"y\"],\n                    dtype=\"u1\",\n                    missing_inds=[2, 5],\n                    attrs={\"_FillValue\": 100},\n                )\n            ],\n            \"attrs\": {\"ia1\": 777},", "            ],\n            \"attrs\": {\"ia1\": 777},\n        }\n    ],\n}\n\n# Define a sequence of standard testfile specs, with suitable param-names.\n_Standard_Testcases: Dict[str, Union[Path, dict]] = {}\n\n", "\n\n# A decorator for spec-generating routines.\n# It automatically **calls** the wrapped function, and adds all the results into the\n# global \"_Standard_Testcases\" dictionary.\ndef standard_testcases_func(func):\n    global _Standard_Testcases\n    _Standard_Testcases.update(func())\n\n", "\n\n@standard_testcases_func\ndef _define_simple_testcases():\n    testcases = {\n        \"ds_Empty\": {},\n        \"ds_Minimal\": _minimal_variable_test_spec,\n        \"ds_Basic\": _simple_test_spec,\n        \"testdata1\": (\n            Path(__file__).parent / \"testdata\" / \"toa_brightness_temperature.nc\"\n        ),\n    }\n    return testcases", "\nADD_IRIS_FILES = True\n# ADD_IRIS_FILES = False\n\n@standard_testcases_func\ndef _define_iris_testdata_testcases():\n    testcases = {}\n    if ADD_IRIS_FILES:\n        # Add all the netcdf files from the Iris testdata paths\n        _testdirpath = Path(iris.tests.get_data_path(\"NetCDF\"))\n        _netcdf_testfile_paths = _testdirpath.rglob(\"**/*.nc\")\n\n        # optional exclusions for useful speedup in test debugging.\n        # EXCLUDES = [\n        #     '_unstructured_',\n        #     '_volcello_',\n        #     '_GEMS_CO2',\n        #     '_ORCA2__votemper',\n        # ]\n        EXCLUDES = []\n        for filepath in _netcdf_testfile_paths:\n            param_name = str(filepath)\n            # remove unwanted path elements\n            param_name = param_name.replace(str(_testdirpath), '')\n            if param_name.endswith('.nc'):\n                param_name = param_name[:-3]\n            # replace path-separators and other awkward chars with dunder\n            for char in ('/', '.', '-'):\n                param_name = param_name.replace(char, '__')\n            # TEMPORARY: skip unstructured ones, just for now, as it makes the run faster\n            if not any(key in param_name for key in EXCLUDES):\n                testcases[f\"testdata__{param_name}\"] = filepath\n\n    return testcases", "\n\nADD_UNIT_TESTS = True\n# ADD_UNIT_TESTS = False\n@standard_testcases_func\ndef _define_unit_singleitem_testcases():\n    testcases = {}\n    if ADD_UNIT_TESTS:\n        # Add selected targetted test datasets.\n\n        # dataset with a single attribute\n        testcases['ds__singleattr'] = {\n            \"attrs\": {'attr1': 1}\n        }\n\n        # dataset with a single variable\n        testcases['ds__singlevar'] = {\n            \"vars\": [dict(name=\"vx\", dims=[], dtype=np.int32)]\n        }\n\n        # dataset with a single variable\n        testcases['ds__dimonly'] = {\n            \"dims\": [dict(name=\"x\", size=2)],\n        }\n\n    return testcases", "\n\n@standard_testcases_func\ndef _define_unit_dtype_testcases():\n    testcases = {}\n    if ADD_UNIT_TESTS:\n        # dataset with attrs and vars of all possible types\n        # TODO: .. and missing-data standard_testcases_func ???\n        for dtype_name in data_types():\n            dtype = 'S1' if dtype_name == 'string' else dtype_name\n            if dtype_name == 'string':\n                # Not working for now\n                # TODO: fix !\n                # continue\n                pass\n\n            testcases[f'ds__dtype__{dtype_name}'] = {\n                \"attrs\": {\n                    f\"tstatt_type__{dtype_name}__single\": _Datatype_Sample_Values[dtype_name][0],\n                    f\"tstatt_type__{dtype_name}__multi\": _Datatype_Sample_Values[dtype_name],\n                },\n                \"vars\": [dict(name=\"vx\", dims=[], dtype=dtype)],\n            }\n\n        testcases['ds__stringvar__singlepoint'] = {\n            \"dims\": [dict(name='strlen', size=3)],\n            \"vars\": [dict(\n                        name=\"vx\", dims=['strlen'], dtype='S1',\n                        data=np.array('abc', dtype='S1')\n                     )],\n        }\n\n        testcases['ds__stringvar__multipoint'] = {\n            \"dims\": [dict(name='x', size=2),\n                     dict(name='strlen', size=3),\n                     ],\n            \"vars\": [dict(\n                name=\"vx\", dims=['x', 'strlen'], dtype='S1',\n                data=np.array([list('abc'), list('def')], dtype='S1')\n            )],\n        }\n\n    return testcases", "\n\n@pytest.fixture(scope=\"session\")\ndef session_testdir(tmp_path_factory):\n    tmp_dir = tmp_path_factory.mktemp(\"standard_schema_testfiles\")\n    return tmp_dir\n\n\n@dataclass\nclass TestcaseSchema:\n    name: str = \"\"\n    spec: dict = None\n    filepath: Path = None", "@dataclass\nclass TestcaseSchema:\n    name: str = \"\"\n    spec: dict = None\n    filepath: Path = None\n\n\n@pytest.fixture(params=list(_Standard_Testcases.keys()))\ndef standard_testcase(request, session_testdir):\n    \"\"\"\n    A fixture which iterates over a set of \"standard\" dataset testcases.\n\n    Some of these are based on a 'testcase spec', from which it builds an actual netcdf\n    testfile : these files are created in a temporary directory provided by pytest\n    (\"tmp_path_factory\"), are are cached so they only get built once per session.\n    Other testcases are just pre-existing test files.\n\n    For each it returns a TestcaseSchema tuple (parameter-name, spec, filepath).\n    For those not based on a spec, 'spec' is None.\n    \"\"\"\n    name = request.param\n    spec = _Standard_Testcases[name]\n    if isinstance(spec, dict):\n        # Build a temporary testfile from the spec, and pass that out.\n        filepath = session_testdir / f\"sampledata_{name}.nc\"\n        if not filepath.exists():\n            # Cache testcase files so we create only once per session.\n            make_testcase_dataset(str(filepath), spec)\n    else:\n        # Otherwise 'spec' is a pre-existing test file: pass (filepath, spec=None)\n        filepath = spec\n        spec = None\n\n    return TestcaseSchema(name=name, spec=spec, filepath=filepath)", "def standard_testcase(request, session_testdir):\n    \"\"\"\n    A fixture which iterates over a set of \"standard\" dataset testcases.\n\n    Some of these are based on a 'testcase spec', from which it builds an actual netcdf\n    testfile : these files are created in a temporary directory provided by pytest\n    (\"tmp_path_factory\"), are are cached so they only get built once per session.\n    Other testcases are just pre-existing test files.\n\n    For each it returns a TestcaseSchema tuple (parameter-name, spec, filepath).\n    For those not based on a spec, 'spec' is None.\n    \"\"\"\n    name = request.param\n    spec = _Standard_Testcases[name]\n    if isinstance(spec, dict):\n        # Build a temporary testfile from the spec, and pass that out.\n        filepath = session_testdir / f\"sampledata_{name}.nc\"\n        if not filepath.exists():\n            # Cache testcase files so we create only once per session.\n            make_testcase_dataset(str(filepath), spec)\n    else:\n        # Otherwise 'spec' is a pre-existing test file: pass (filepath, spec=None)\n        filepath = spec\n        spec = None\n\n    return TestcaseSchema(name=name, spec=spec, filepath=filepath)", ""]}
{"filename": "tests/test_samplecode_cdlgen_comparablecdl.py", "chunked_list": ["\"\"\"\nPoC code for netcdf file CDL-based testing mechanisms.\n\nFile generation from CDL (with ncgen), and CDL comparison (with ncdump).\n\nThe status and usage of this are yet to be determined.\n\"\"\"\nimport os\nimport subprocess\nfrom pathlib import Path", "import subprocess\nfrom pathlib import Path\nfrom typing import AnyStr, List, Optional\n\nimport pytest\n\n# Note : `_env_bin_path` and `ncgen_from_cdl` are taken from Iris test code.\n# Code here duplicated from Iris v3.4.1\n# https://github.com/SciTools/iris/blob/v3.4.1/lib/iris/tests/stock/netcdf.py#L18-L63\n", "# https://github.com/SciTools/iris/blob/v3.4.1/lib/iris/tests/stock/netcdf.py#L18-L63\n\n\ndef _env_bin_path(exe_name: AnyStr = None):\n    \"\"\"\n    Return a Path object for (an executable in) the environment bin directory.\n\n    Parameters\n    ----------\n    exe_name : str\n        If set, the name of an executable to append to the path.\n\n    Returns\n    -------\n    exe_path : Path\n        A path to the bin directory, or an executable file within it.\n\n    Notes\n    -----\n    For use in tests which spawn commands which should call executables within\n    the Python environment, since many IDEs (Eclipse, PyCharm) don't\n    automatically include this location in $PATH (as opposed to $PYTHONPATH).\n    \"\"\"\n    exe_path = Path(os.__file__)\n    exe_path = (exe_path / \"../../../bin\").resolve()\n    if exe_name is not None:\n        exe_path = exe_path / exe_name\n    return exe_path", "\n\nNCGEN_PATHSTR = str(_env_bin_path(\"ncgen\"))\nNCDUMP_PATHSTR = str(_env_bin_path(\"ncdump\"))\n\n\ndef ncgen_from_cdl(\n    cdl_str: Optional[str], cdl_path: Optional[str], nc_path: str\n):\n    \"\"\"\n    Generate a test netcdf file from cdl.\n\n    Source is CDL in either a string or a file.\n    If given a string, will either save a CDL file, or pass text directly.\n    A netcdf output file is always created, at the given path.\n\n    Parameters\n    ----------\n    cdl_str : str or None\n        String containing a CDL description of a netcdf file.\n        If None, 'cdl_path' must be an existing file.\n    cdl_path : str or None\n        Path of temporary text file where cdl_str is written.\n        If None, 'cdl_str' must be given, and is piped direct to ncgen.\n    nc_path : str\n        Path of temporary netcdf file where converted result is put.\n\n    Notes\n    -----\n    For legacy reasons, the path args are 'str's not 'Path's.\n\n    \"\"\"\n    if cdl_str and cdl_path:\n        with open(cdl_path, \"w\") as f_out:\n            f_out.write(cdl_str)\n    if cdl_path:\n        # Create netcdf from stored CDL file.\n        call_args = [NCGEN_PATHSTR, \"-k3\", \"-o\", nc_path, cdl_path]\n        call_kwargs = {}\n    else:\n        # No CDL file : pipe 'cdl_str' directly into the ncgen program.\n        if not cdl_str:\n            raise ValueError(\"Must provide either 'cdl_str' or 'cdl_path'.\")\n        call_args = [NCGEN_PATHSTR, \"-k3\", \"-o\", nc_path]\n        call_kwargs = dict(input=cdl_str, encoding=\"ascii\")\n\n    subprocess.run(call_args, check=True, **call_kwargs)", "\n\n# CDL to create a reference file with \"all\" features included.\n_base_cdl = \"\"\"\nnetcdf everything {\ndimensions:\n\tx = 2 ;\n\ty = 3 ;\n\tstrlen = 5 ;\nvariables:", "\tstrlen = 5 ;\nvariables:\n\tint x(x) ;\n\t\tx:name = \"var_x\" ;\n\tint var_2d(x, y) ;\n\tuint var_u8(x) ;\n\tfloat var_f4(x) ;\n\tdouble var_f8(x) ;\n\tchar var_str(x, strlen) ;\n\tint other(x) ;", "\tchar var_str(x, strlen) ;\n\tint other(x) ;\n\t    other:attr_int = 1 ;\n\t    other:attr_float = 2.f ;\n\t    other:attr_double = 2. ;\n\t    other:attr_string = \"this\" ;\n    int masked_int(y) ;\n        masked_int:_FillValue = -3 ;\n    int masked_float(y) ;\n        masked_float:_FillValue = -4 ;", "    int masked_float(y) ;\n        masked_float:_FillValue = -4 ;\n        \n// global attributes:\n\t\t:global_attr_1 = \"one\" ;\n\t\t:global_attr_2 = 2 ;\n\ngroup: grp_1 {\n    dimensions:\n        y = 7 ;", "    dimensions:\n        y = 7 ;\n    variables:\n        int parent_dim(x) ;\n        int own_dim(y) ;\n} // group grp_1\n\ngroup: grp_2 {\n    variables:\n        int grp2_x(x) ;", "    variables:\n        int grp2_x(x) ;\n} // group grp_2\n}\n\"\"\"\n\n\ndef comparable_cdl(text: str) -> List[str]:\n    \"\"\"\n    Convert a CDL string to a list of stripped lines, with certain problematic things\n    removed.  The resulting list of strings should be comparable between identical\n    netcdf files.\n\n    \"\"\"\n    lines = text.split(\"\\n\")\n    lines = [line.strip(\" \\t\\n\") for line in lines]\n    lines = [\n        line\n        for line in lines\n        if (\n            len(line)\n            # Exclude global pseudo-attribute (some versions of ncdump)\n            and \":_NCProperties =\" not in line\n        )\n    ]\n    # Also exclude the first line, which includes the filename\n    hdr, lines = lines[0], lines[1:]\n    assert hdr.startswith(\"netcdf\") and hdr[-1] == \"{\"\n    return lines", "\n\n@pytest.fixture(scope=\"module\")\ndef testdata_cdl(tmp_path_factory):\n    tmpdir_path = tmp_path_factory.mktemp(\"cdltests\")\n    cdl_path = tmpdir_path / \"tmp_nccompare_test.cdl\"\n    nc_path = tmpdir_path / \"tmp_nccompare_test.nc\"\n    ncgen_from_cdl(cdl_str=_base_cdl, cdl_path=cdl_path, nc_path=nc_path)\n    bytes = subprocess.check_output([NCDUMP_PATHSTR, \"-h\", nc_path])\n    cdl_regen_text = bytes.decode()\n    return cdl_regen_text", "\n\ndef test_ncgen_from_cdl(testdata_cdl):\n    # Integration test for 'ncgen_from_cdl' (! tests-of-tests !)\n    def text_lines_nowhitespace(text):\n        lines = text.split(\"\\n\")\n        lines = [line.strip() for line in lines]\n        lines = [\n            \"\".join(char for char in line.strip() if not char.isspace())\n            for line in lines\n        ]\n        lines = [line for line in lines if line]\n        return lines\n\n    lines_result = text_lines_nowhitespace(testdata_cdl)\n    lines_original = text_lines_nowhitespace(_base_cdl)\n\n    # Full original lines definitely do NOT match, because dataset name =filename\n    assert lines_result != lines_original\n\n    # Lines beyond 1st may STILL not match because of _NCProperties\n    # ... however, skipping that line (and first line), they *should* match\n    lines_result = [l for l in lines_result if not \"_NCProp\" in l]\n    assert lines_result[1:] == lines_original[1:]", "\n\ndef test_comparable_cdl(testdata_cdl):\n    # Integration test for 'comparable_cdl' (! tests-of-tests !)\n    cdl_lines = comparable_cdl(_base_cdl)\n    dump_lines = comparable_cdl(testdata_cdl)\n    assert cdl_lines == dump_lines\n"]}
{"filename": "tests/integration/test_roundtrips_iris.py", "chunked_list": ["\"\"\"\nTest ncdata.iris by checking roundtrips for standard standard_testcases_func.\n\nTestcases start as netcdf files.\n(1) check equivalence of cubes : iris.load(file) VS iris.load(ncdata(file))\n(2) check equivalence of files : iris -> file VS iris->ncdata->file\n\"\"\"\nfrom subprocess import check_output\nfrom unittest import mock\n", "from unittest import mock\n\nimport numpy as np\n\nimport dask.array as da\nimport iris\nimport iris.fileformats.netcdf._thread_safe_nc as iris_threadsafe\nimport pytest\n\nfrom ncdata.netcdf4 import from_nc4, to_nc4", "\nfrom ncdata.netcdf4 import from_nc4, to_nc4\nfrom tests._compare_nc_datasets import compare_nc_datasets\nfrom tests.data_testcase_schemas import standard_testcase, session_testdir\n\n# Avoid complaints that imported fixtures are \"unused\"\nstandard_testcase, session_testdir\n\nfrom ncdata.iris import from_iris, to_iris\n", "from ncdata.iris import from_iris, to_iris\n\nimport iris.fileformats.netcdf._thread_safe_nc as ifnt\n\n_FIX_LOCKS = True\n# _FIX_LOCKS = False\nif _FIX_LOCKS:\n    @pytest.fixture(scope='session')\n    def use_irislock():\n        tgt = 'ncdata.netcdf4._GLOBAL_NETCDF4_LIBRARY_THREADLOCK'\n        with mock.patch(tgt, new=ifnt._GLOBAL_NETCDF4_LOCK):\n            yield", "\n\n_TINY_CHUNKS = True\n# _TINY_CHUNKS = False\nif _TINY_CHUNKS:\n    # Note: from experiment, the test most likely to fail due to thread-safety is\n    #   \"test_load_direct_vs_viancdata[testdata____testing__small_theta_colpex]\"\n    # Resulting errors vary widely, including netcdf/HDF errors, data mismatches and\n    # segfaults.\n    # The following _CHUNKSIZE_SPEC makes it fail ~70% of runs (run as a single test)\n    # HOWEVER, the overall test runs get a LOT slower (e.g. 110sec --> )\n    _CHUNKSIZE_SPEC = \"20Kib\"\n    # HOWEVER, the above '_FIX_LOCKS' operation seems to prevent this.\n    @pytest.fixture(scope='session', autouse=True)\n    def force_tiny_chunks():\n        import dask.config as dcfg\n        with dcfg.set({\"array.chunk-size\": _CHUNKSIZE_SPEC}):\n            yield", "\n\ndef test_load_direct_vs_viancdata(standard_testcase, use_irislock):\n    source_filepath = standard_testcase.filepath\n    ncdata = from_nc4(source_filepath)\n\n    # _Debug = True\n    _Debug = False\n    if _Debug:\n        print(f\"\\ntestcase: {standard_testcase.name}\")\n        print(\"spec =\")\n        print(standard_testcase.spec)\n        print(\"\\nncdata =\")\n        print(ncdata)\n        print(\"\\nncdump =\")\n        txt = check_output([f\"ncdump {source_filepath}\"], shell=True).decode()\n        print(txt)\n\n    # Load the testcase with Iris.\n    iris_cubes = iris.load(source_filepath)\n    # Load same, via ncdata\n    iris_ncdata_cubes = to_iris(ncdata)\n    # Unfortunately, cube order is not guaranteed to be stable.\n    iris_cubes, iris_ncdata_cubes = (\n        sorted(cubes, key=lambda cube: cube.name())\n        for cubes in (iris_cubes, iris_ncdata_cubes)\n    )\n\n    # There is also a peculiar problem with cubes that have all-masked data.\n    # Let's just skip any like that, for now...\n    def all_maskeddata_cube(cube):\n        return da.all(da.ma.getmaskarray(cube.core_data())).compute()\n\n    if len(iris_cubes) == len(iris_ncdata_cubes):\n        i_ok = [\n            i\n            for i in range(len(iris_cubes))\n            if not all_maskeddata_cube(iris_cubes[i])\n            and not all_maskeddata_cube(iris_ncdata_cubes[i])\n        ]\n        iris_cubes, iris_ncdata_cubes = (\n            [cube for i, cube in enumerate(cubes) if i in i_ok]\n            for cubes in (iris_cubes, iris_ncdata_cubes)\n        )\n\n        n_cubes = len(iris_cubes)\n        for i_cube in range(n_cubes):\n            if i_cube not in i_ok:\n                print(\n                    f'\\nSKIPPED testcase @\"{source_filepath}\" : cube #{i_cube}/{n_cubes} with all-masked data : '\n                    f\"{iris_cubes[i_cube].summary(shorten=True)}\"\n                )\n\n    # Check equivalence\n    # result = iris_cubes == iris_ncdata_cubes\n    # note: temporary fix for string-cube comparison.\n    #   Cf. https://github.com/SciTools/iris/issues/5362\n    #   TODO: remove temporary fix\n    def cube_equal(c1, c2):\n        \"\"\"\n        Cube equality test which works around string-cube equality problem.\n\n        \"\"\"\n        if (\n                (c1.metadata == c2.metadata)\n                and (c1.shape == c2.shape)\n                and all(cube.dtype.kind in ('U', 'S') for cube in (c1, c2))\n        ):\n            # cludge comparison for string-type cube data\n            c1, c2 = (cube.copy() for cube in (c1, c2))\n            c1.data = (c1.data == c2.data)\n            c2.data = np.ones(c2.shape, dtype=bool)\n\n        return c1 == c2\n\n    results = [\n        (c1.name(), cube_equal(c1, c2))\n        for c1, c2 in zip(iris_cubes, iris_ncdata_cubes)\n    ]\n    expected = [(cube.name(), True) for cube in iris_cubes]\n    result = results == expected\n\n    if not result:\n        # FOR NOW: compare with experimental ncdata comparison.\n        # I know this is a bit circular, but it is useful for debugging, for now ...\n        result = compare_nc_datasets(\n            from_iris(iris_cubes), from_iris(iris_ncdata_cubes)\n        )\n        assert result == []\n\n    # assert iris_cubes == iris_ncdata_cubes\n    assert result", "\n\ndef test_save_direct_vs_viancdata(standard_testcase, tmp_path):\n    source_filepath = standard_testcase.filepath\n    ncdata = from_nc4(source_filepath)\n\n    # Load the testcase into Iris.\n    iris_cubes = iris.load(source_filepath)\n\n    if standard_testcase.name in (\"ds_Empty\", \"ds__singleattr\", \"ds__dimonly\"):\n        # Iris can't save an empty dataset.\n        return\n\n    # Re-save from iris\n    temp_iris_savepath = tmp_path / \"temp_save_iris.nc\"\n    iris.save(iris_cubes, temp_iris_savepath)\n    # Save same, via ncdata\n    temp_ncdata_savepath = tmp_path / \"temp_save_iris_via_ncdata.nc\"\n    to_nc4(from_iris(iris_cubes), temp_ncdata_savepath)\n\n    # _Debug = True\n    _Debug = False\n    if _Debug:\n        print(f\"\\ntestcase: {standard_testcase.name}\")\n        print(\"spec =\")\n        print(standard_testcase.spec)\n        print(\"\\nncdata =\")\n        print(ncdata)\n        print(\"\\nncdump ORIGINAL TESTCASE SOURCEFILE =\")\n        txt = check_output([f\"ncdump {source_filepath}\"], shell=True).decode()\n        print(txt)\n        print(\"\\nncdump DIRECT FROM IRIS =\")\n        txt = check_output(\n            [f\"ncdump {temp_iris_savepath}\"], shell=True\n        ).decode()\n        print(txt)\n        print(\"\\nncdump VIA NCDATA =\")\n        txt = check_output(\n            [f\"ncdump {temp_ncdata_savepath}\"], shell=True\n        ).decode()\n        print(txt)\n\n    # Check equivalence\n    results = compare_nc_datasets(temp_iris_savepath, temp_ncdata_savepath)\n    assert results == []", ""]}
{"filename": "tests/integration/__init__.py", "chunked_list": ["\"\"\"Integration tests for :mod:`ncdata`.\"\"\"\n"]}
{"filename": "tests/integration/test_exercises.py", "chunked_list": ["\"\"\"\nTemporary : simple tests to exercise the 'ex_' scripts.\n\nThis ensures they at least don't crash, though they are not tests themselves\n(though they do also use asserts in some cases).\n\nTODO: add a discovery process to reduce boilerplate ?\nTODO: deal with possible thread safety issues, not in the scripts themselves ?\n\"\"\"\n", "\"\"\"\n\n\ndef test_ex_dataset_print():\n    from tests.integration.example_scripts.ex_dataset_print import sample_printout\n\n    sample_printout()\n\n\ndef test_ex_iris_saveto_ncdata():\n    from tests.integration.example_scripts.ex_iris_saveto_ncdata import (\n        example_ncdata_from_iris,\n    )\n\n    example_ncdata_from_iris()", "\ndef test_ex_iris_saveto_ncdata():\n    from tests.integration.example_scripts.ex_iris_saveto_ncdata import (\n        example_ncdata_from_iris,\n    )\n\n    example_ncdata_from_iris()\n\n\ndef test_ex_iris_xarray_conversion():\n    from tests.integration.example_scripts.ex_iris_xarray_conversion import example_from_xr\n\n    example_from_xr()", "\ndef test_ex_iris_xarray_conversion():\n    from tests.integration.example_scripts.ex_iris_xarray_conversion import example_from_xr\n\n    example_from_xr()\n\n\ndef test_ex_ncio_loadsave_roundtrip():\n    from tests.integration.example_scripts.ex_ncdata_netcdf_conversion import (\n        example_nc4_load_save_roundtrip,\n    )\n\n    example_nc4_load_save_roundtrip()", "\n\ndef test_ex_ncio_saveload_unlimited_roundtrip():\n    from tests.integration.example_scripts.ex_ncdata_netcdf_conversion import (\n        example_nc4_save_reload_unlimited_roundtrip,\n    )\n\n    example_nc4_save_reload_unlimited_roundtrip()\n", ""]}
{"filename": "tests/integration/test_roundtrips_netcdf.py", "chunked_list": ["\"\"\"\nTest ncdata.netcdf by checking load-save roundtrips for standard testcases.\n\"\"\"\nfrom subprocess import check_output\n\nfrom tests._compare_nc_datasets import compare_nc_datasets\nfrom tests.data_testcase_schemas import standard_testcase, session_testdir\n\nfrom ncdata.netcdf4 import from_nc4, to_nc4\n", "from ncdata.netcdf4 import from_nc4, to_nc4\n\n# Avoid complaints that the imported fixtures are \"unused\"\nstandard_testcase, session_testdir\n\n# _Debug = True\n_Debug = False\n\n\ndef test_basic(standard_testcase, tmp_path):\n    source_filepath = standard_testcase.filepath\n    intermediate_filepath = tmp_path / \"temp_saved.nc\"\n\n    # Load the testfile.\n    ncdata = from_nc4(source_filepath)\n    # Re-save\n    to_nc4(ncdata, intermediate_filepath)\n\n    _Debug = True\n    if _Debug:\n        print(f\"\\ntestcase: {standard_testcase.name}\")\n        print(\"spec =\")\n        print(standard_testcase.spec)\n        print(\"\\nncdata =\")\n        print(ncdata)\n        print(\"\\nncdump =\")\n        txt = check_output(\n            [f\"ncdump {intermediate_filepath}\"], shell=True\n        ).decode()\n        print(txt)\n\n    # Check that the re-saved file matches the original\n    results = compare_nc_datasets(source_filepath, intermediate_filepath)\n    assert results == []", "\ndef test_basic(standard_testcase, tmp_path):\n    source_filepath = standard_testcase.filepath\n    intermediate_filepath = tmp_path / \"temp_saved.nc\"\n\n    # Load the testfile.\n    ncdata = from_nc4(source_filepath)\n    # Re-save\n    to_nc4(ncdata, intermediate_filepath)\n\n    _Debug = True\n    if _Debug:\n        print(f\"\\ntestcase: {standard_testcase.name}\")\n        print(\"spec =\")\n        print(standard_testcase.spec)\n        print(\"\\nncdata =\")\n        print(ncdata)\n        print(\"\\nncdump =\")\n        txt = check_output(\n            [f\"ncdump {intermediate_filepath}\"], shell=True\n        ).decode()\n        print(txt)\n\n    # Check that the re-saved file matches the original\n    results = compare_nc_datasets(source_filepath, intermediate_filepath)\n    assert results == []", ""]}
{"filename": "tests/integration/example_scripts/ex_ncdata_netcdf_conversion.py", "chunked_list": ["\"\"\"\nA proof-of-concept example workflow for :mod:`ncdata.netcdf4`.\n\nShowing loading and saving ncdata to/from netcdf files.\n\"\"\"\nimport tempfile\nfrom pathlib import Path\nfrom shutil import rmtree\n\nimport netCDF4 as nc", "\nimport netCDF4 as nc\nimport numpy as np\n\nfrom ncdata import NcAttribute, NcData, NcDimension, NcVariable\nfrom ncdata.netcdf4 import from_nc4, to_nc4\nfrom tests import testdata_dir\nfrom tests._compare_nc_datasets import compare_nc_datasets\n\n\ndef example_nc4_load_save_roundtrip():  # noqa: D103\n    # Load an existing file, save-netcdf4 : check same (with Iris for now)\n    print(\"\\n----\\nNetcdf4 load-save example.\")\n\n    filepath = testdata_dir / \"toa_brightness_temperature.nc\"\n    ncdata = from_nc4(filepath)\n\n    tempdir_path = Path(tempfile.mkdtemp())\n    try:\n        filepath2 = tempdir_path / \"temp_nc_output.nc\"\n        to_nc4(ncdata, filepath2)\n\n        result = compare_nc_datasets(filepath, filepath2)\n        equals_result = result == []\n        print(\"\\nFiles compare? :\", equals_result)\n        assert equals_result\n\n    finally:\n        rmtree(tempdir_path)\n\n    print(\"\\n== Netcdf4 load-save roundtrip finished OK.\\n\")", "\n\ndef example_nc4_load_save_roundtrip():  # noqa: D103\n    # Load an existing file, save-netcdf4 : check same (with Iris for now)\n    print(\"\\n----\\nNetcdf4 load-save example.\")\n\n    filepath = testdata_dir / \"toa_brightness_temperature.nc\"\n    ncdata = from_nc4(filepath)\n\n    tempdir_path = Path(tempfile.mkdtemp())\n    try:\n        filepath2 = tempdir_path / \"temp_nc_output.nc\"\n        to_nc4(ncdata, filepath2)\n\n        result = compare_nc_datasets(filepath, filepath2)\n        equals_result = result == []\n        print(\"\\nFiles compare? :\", equals_result)\n        assert equals_result\n\n    finally:\n        rmtree(tempdir_path)\n\n    print(\"\\n== Netcdf4 load-save roundtrip finished OK.\\n\")", "\n\ndef example_nc4_save_reload_unlimited_roundtrip():\n    # Create arbitrary ncdata, save to netcdf4, re-load and check.\n    print(\"\\n----\\nNetcdf4 save-load example.\")\n\n    ncdata = NcData()\n    len_x, len_y = 4, 2\n    ncdata.dimensions[\"dim_x\"] = NcDimension(\"dim_x\", len_x, unlimited=True)\n    ncdata.dimensions[\"dim_y\"] = NcDimension(\"dim_y\", len_y)\n    ncdata.variables[\"var_x\"] = NcVariable(\n        \"var_x\",\n        dimensions=[\"dim_x\"],\n        dtype=np.float32,\n        data=np.arange(4),\n        # Just an an attribute for the sake of it.\n        attributes={\"varattr1\": NcAttribute(\"varattr1\", 1)},\n    )\n    ncdata.attributes[\"globalattr1\"] = NcAttribute(\"globalattr1\", \"one\")\n    print(\"Source ncdata object:\")\n    print(ncdata)\n\n    tempdir_path = Path(tempfile.mkdtemp())\n    try:\n        print(\"\")\n        filepath = tempdir_path / \"temp_nc_2_unlim.nc\"\n        print(\"Saving to file \", filepath)\n        to_nc4(ncdata, filepath)\n        import os\n\n        print(\"\\n\\nCreated output:\")\n        os.system(f\"ncdump -h {filepath!s}\")\n        print(\"\\n\")\n\n        ds = nc.Dataset(filepath)\n        assert list(ds.dimensions.keys()) == [\"dim_x\", \"dim_y\"]\n        assert ds.dimensions[\"dim_x\"].isunlimited()\n\n        assert list(ds.variables.keys()) == [\"var_x\"]\n        assert ds.variables[\"var_x\"].ncattrs() == [\"varattr1\"]\n\n        assert ds.ncattrs() == [\"globalattr1\"]\n        assert ds.getncattr(\"globalattr1\") == \"one\"\n        ds.close()\n\n        # Now readback\n        ds_back = from_nc4(filepath)\n        print(\"\\nRead-back ncdata from file:\")\n        print(ds_back)\n\n        assert list(ds_back.dimensions.keys()) == [\"dim_x\", \"dim_y\"]\n        dimx, dimy = [ds_back.dimensions[name] for name in (\"dim_x\", \"dim_y\")]\n        assert dimx.size == len_x and dimx.unlimited\n        assert dimy.size == len_y and not dimy.unlimited\n\n        assert list(ds_back.variables.keys()) == [\"var_x\"]\n        varx = ds_back.variables[\"var_x\"]\n\n        assert list(varx.attributes.keys()) == [\"varattr1\"]\n        assert varx.attributes[\"varattr1\"].value == 1\n\n        assert list(ds_back.attributes.keys()) == [\"globalattr1\"]\n        assert ds_back.attributes[\"globalattr1\"].value == \"one\"\n\n    finally:\n        rmtree(tempdir_path)\n\n    print(\"\\n== Netcdf4 save-load roundtrip finished OK.\\n\")", "\n\nif __name__ == \"__main__\":\n    example_nc4_load_save_roundtrip()\n    example_nc4_save_reload_unlimited_roundtrip()\n"]}
{"filename": "tests/integration/example_scripts/ex_iris_xarray_conversion.py", "chunked_list": ["\"\"\"\nA proof-of-concept example workflow for :mod:`ncdata.iris_xarray`.\n\nShowing conversion from Xarray to Iris, and back again.\n\"\"\"\nimport iris\nimport iris.tests as itsts\nimport numpy as np\nimport xarray as xr\n", "import xarray as xr\n\nfrom ncdata.iris_xarray import cubes_from_xarray, cubes_to_xarray\nfrom tests import testdata_dir\n\n\ndef example_from_xr():  # noqa: D103\n    filepath = testdata_dir / \"toa_brightness_temperature.nc\"\n    xrds = xr.open_dataset(filepath, chunks=\"auto\")\n    iris.FUTURE.datum_support = True\n    print(\"\\nOriginal Xarray dataset:\\n\", xrds)\n    cubes = cubes_from_xarray(xrds)\n    print(\"\\nxrds['time']:\\n\", xrds[\"time\"])\n    print(\"\\n\\n\")\n    print(\"============ CONVERT xr.Dataset TO cubes ... =========\\n\")\n    # print(\"Cubes:\")\n    # print(cubes)\n    cube = cubes[0]\n    print(\"\\nCube:\")\n    print(cube)\n\n    data = cube.core_data()\n    print(\"\\ncube.core_data():\")\n    print(data)\n    # match = data is xrds['data'].data\n    # print('\\ncube.core_data() is xrds[\"data\"].data:')\n    # print(match)\n    co_auxlons = cube.coord(\"longitude\")\n    print('\\ncube.coord(\"longitude\"):')\n    print(co_auxlons)\n    points = co_auxlons.core_points()\n    print('\\ncube.coord(\"longitude\").core_points():')\n    print(points)\n    print('\\ncube.coord(\"longitude\").points:')\n    print(points.compute())\n\n    print(\"\\n\")\n    print(\"============ CONVERT cubes TO xr.Dataset ... =========\")\n    print(\"\")\n    xrds2 = cubes_to_xarray(cubes)\n    print(\"\\nxrds2:\\n\", xrds2)\n    print(\"\\ntime:\\n\", xrds2[\"time\"])\n\n    print(\"\\n\")\n    print(\"============ Array identity checks ... =========\")\n\n    print(\n        \"xrds2['data'].data   is   cube.core_data() : \",\n        bool(xrds2[\"data\"].data is cube.core_data()),\n    )\n    assert xrds2[\"data\"].data is cube.core_data()\n\n    print(\n        \"xrds2['lon'].data   is   cube.coord('longitude').core_points() : \",\n        bool(xrds2[\"lon\"].data is cube.coord(\"longitude\").core_points()),\n    )\n    assert xrds2[\"lon\"].data is cube.coord(\"longitude\").core_points()\n\n    print(\n        \"xrds2['x'].data   is   cube.coord('projection_x_coordinate').core_points() : \",\n        bool(\n            xrds2[\"x\"].data\n            is cube.coord(\"projection_x_coordinate\").core_points()\n        ),\n    )\n    # NOTE: this one does **not** succeed.\n    # TODO: find out exactly why -- ? in some way, because it is a dim coord ?\n    # assert xrds2[\"x\"].data is cube.coord(\"projection_x_coordinate\").core_points()\n\n    # NOTE: This part is an actual data content comparison.\n    # When comparing actual (lazy) array content, we will often need to take extra\n    # measures to ensure thread-safe use of the netcdf library, if data may be fetched\n    # (computed) from both Iris and Xarray lazy data arrays **together**.\n    # In this case, however, the Iris coordinate \".points\" is fetched *first*, so no\n    # special care is needed.\n    print(\n        \"np.all(xrds2['x'].data == cube.coord('projection_x_coordinate').points) : \",\n        bool(\n            np.all(\n                xrds2[\"x\"].data == cube.coord(\"projection_x_coordinate\").points\n            )\n        ),\n    )\n    assert np.all(\n        xrds2[\"x\"].data == cube.coord(\"projection_x_coordinate\").points\n    )", "\n\nif __name__ == \"__main__\":\n    example_from_xr()\n"]}
{"filename": "tests/integration/example_scripts/ex_dataset_print.py", "chunked_list": ["\"\"\"Temporary integrational proof-of-concept example for dataset printout.\"\"\"\nfrom pathlib import Path\n\nimport iris\n\nimport ncdata.iris as nci\nfrom ncdata import NcAttribute, NcData, NcDimension, NcVariable\nfrom tests import testdata_dir\n\n\ndef sample_printout():  # noqa: D103\n    iris.FUTURE.datum_support = True\n    filepath = testdata_dir / \"toa_brightness_temperature.nc\"\n    cubes = iris.load(filepath)\n\n    ds = nci.from_iris(cubes)\n    # FOR NOW: Add a random extra group to exercise some extra behaviour,\n    # namely groups and shortform variables (vars with no attrs)\n    ds.groups[\"extra\"] = NcData(\n        name=\"extra\",\n        dimensions={\"extra_qq\": NcDimension(\"extra_qq\", 4, unlimited=True)},\n        variables={\n            \"noattrs\": NcVariable(\"noattrs\", [\"x\"]),\n            \"x\": NcVariable(\n                name=\"x\",\n                dimensions=[\"y\", \"extra_qq\"],\n                attributes={\n                    \"q1\": NcAttribute(\"q1\", 1),\n                    \"q_multi\": NcAttribute(\"q_multi\", [1.1, 2.2]),\n                    \"q_multstr\": NcAttribute(\"q_multstr\", [\"one\", \"two\"]),\n                },\n            ),\n        },\n        attributes={\"extra__global\": NcAttribute(\"extra__global\", \"=value\")},\n    )\n    print(ds)", "\n\ndef sample_printout():  # noqa: D103\n    iris.FUTURE.datum_support = True\n    filepath = testdata_dir / \"toa_brightness_temperature.nc\"\n    cubes = iris.load(filepath)\n\n    ds = nci.from_iris(cubes)\n    # FOR NOW: Add a random extra group to exercise some extra behaviour,\n    # namely groups and shortform variables (vars with no attrs)\n    ds.groups[\"extra\"] = NcData(\n        name=\"extra\",\n        dimensions={\"extra_qq\": NcDimension(\"extra_qq\", 4, unlimited=True)},\n        variables={\n            \"noattrs\": NcVariable(\"noattrs\", [\"x\"]),\n            \"x\": NcVariable(\n                name=\"x\",\n                dimensions=[\"y\", \"extra_qq\"],\n                attributes={\n                    \"q1\": NcAttribute(\"q1\", 1),\n                    \"q_multi\": NcAttribute(\"q_multi\", [1.1, 2.2]),\n                    \"q_multstr\": NcAttribute(\"q_multstr\", [\"one\", \"two\"]),\n                },\n            ),\n        },\n        attributes={\"extra__global\": NcAttribute(\"extra__global\", \"=value\")},\n    )\n    print(ds)", "\n\nif __name__ == \"__main__\":\n    sample_printout()\n"]}
{"filename": "tests/integration/example_scripts/__init__.py", "chunked_list": ["\"\"\"\nExample scripts for :mod:`ncdata`.\n\nN.B. exercised via test in standard test suite : tests/integration/test_exercises.py\nThis should help stop the example scripts code from going stale.\n\"\"\"\n"]}
{"filename": "tests/integration/example_scripts/ex_iris_saveto_ncdata.py", "chunked_list": ["\"\"\"\nA proof-of-concept example workflow for :func:`ncdata.iris.from_iris`.\n\nCheck that conversion succeeds and print the resulting dataset.\n\"\"\"\nimport iris\n\nfrom ncdata.iris import from_iris\nfrom tests import testdata_dir\n", "from tests import testdata_dir\n\n\ndef example_ncdata_from_iris():\n    print(\"\")\n    print(\"==============\")\n    print(\"TEMPORARY: iris save-to-ncdata test\")\n    iris.FUTURE.datum_support = True\n    filepath = testdata_dir / \"toa_brightness_temperature.nc\"\n    cubes = iris.load(filepath)\n    ds = from_iris(cubes)\n    # show result\n    print(ds)", "\n\nif __name__ == \"__main__\":\n    # TODO: save only for now :  TBD add a load case.\n    example_ncdata_from_iris()\n"]}
{"filename": "tests/unit/__init__.py", "chunked_list": ["\"\"\"Unit tests for :mod:`ncdata`.\"\"\"\n"]}
{"filename": "tests/unit/netcdf/__init__.py", "chunked_list": ["\"\"\"Unit tests for :mod:`ncdata.netcdf`.\"\"\"\n"]}
{"filename": "tests/unit/tests/test_compare_nc_datasets.py", "chunked_list": ["\"\"\"\nTests for :mod:`tests.unit.netcdf._compare_nc_files`\n\nYes I know, tests of tests.  But it seems necessary.\n\"\"\"\nimport os\nimport shutil\nimport subprocess\nimport warnings\nfrom pathlib import Path", "import warnings\nfrom pathlib import Path\nfrom typing import AnyStr, Optional\nfrom unittest import mock\n\nimport netCDF4 as nc\nimport numpy as np\nimport pytest\n\nimport ncdata.netcdf4", "\nimport ncdata.netcdf4\nfrom tests._compare_nc_datasets import (\n    _compare_attributes,\n    _compare_name_lists,\n    compare_nc_datasets,\n)\nfrom tests.test_samplecode_cdlgen_comparablecdl import ncgen_from_cdl\n\n# CDL to create a reference file with \"all\" features included.", "\n# CDL to create a reference file with \"all\" features included.\n_base_cdl = \"\"\"\nnetcdf everything {\ndimensions:\n\tx = 2 ;\n\ty = 3 ;\n\tstrlen = 5 ;\nvariables:\n\tint x(x) ;", "variables:\n\tint x(x) ;\n\t\tx:name = \"var_x\" ;\n\tint var_2d(x, y) ;\n\tuint var_u8(x) ;\n\tfloat var_f4(x) ;\n\tdouble var_f8(x) ;\n\tchar var_str(x, strlen) ;\n\tint other(x) ;\n\t    other:attr_int = 1 ;", "\tint other(x) ;\n\t    other:attr_int = 1 ;\n\t    other:attr_float = 2.0f ;\n\t    other:attr_double = 2.0 ;\n\t    other:attr_string = \"this\" ;\n    int masked_int(y) ;\n        masked_int:_FillValue = -3 ;\n    int masked_float(y) ;\n        masked_float:_FillValue = -4.0 ;\n        ", "        masked_float:_FillValue = -4.0 ;\n        \n// global attributes:\n\t\t:global_attr_1 = \"one\" ;\n\t\t:global_attr_2 = 2 ;\n\n// groups:\ngroup: grp_1 {\n    dimensions:\n        y = 7 ;", "    dimensions:\n        y = 7 ;\n    variables:\n        int parent_dim(x) ;\n        int own_dim(y) ;\n}\ngroup: grp_2 {\n    variables:\n        int grp2_x(x) ;\n}", "        int grp2_x(x) ;\n}\n}\n\"\"\"\n\n_simple_cdl = \"\"\"\nnetcdf test {\ndimensions:\n\tx = 5 ;\nvariables:", "\tx = 5 ;\nvariables:\n\tfloat x(x) ;\n\tint y(x) ;\n\ndata:\n  x = 0.12, 1.23, 2.34, _, _ ;\n  y = 1, 2, 3, _, _ ;\n}\n\"\"\"", "}\n\"\"\"\n\n\nclass Test__compare_name_lists:\n    # Test subsidiary routine for checking a list of names\n    def test_empty(self):\n        errs = []\n        _compare_name_lists(errs, [], [], \"named-elements\")\n        assert errs == []\n\n    def test_same(self):\n        tst = [\"a\", \"b\"]\n        errs = []\n        _compare_name_lists(errs, tst, tst, \"named-elements\")\n        assert errs == []\n\n    def test_diff(self):\n        errs = []\n        _compare_name_lists(errs, [\"a\"], [], \"named-elements\")\n        assert errs == [\"named-elements do not match: ['a'] != []\"]\n\n    def test_difforder(self):\n        errs = []\n        _compare_name_lists(errs, [\"a\", \"b\"], [\"b\", \"a\"], \"named-elements\")\n        assert errs == [\n            \"named-elements do not match: ['a', 'b'] != ['b', 'a']\"\n        ]\n\n    def test_difforder_tolerant_warns(self):\n        errs = []\n        with pytest.warns(\n            UserWarning, match=\"Ignoring: named-elements do not match\"\n        ):\n            _compare_name_lists(\n                errs,\n                [\"a\", \"b\"],\n                [\"b\", \"a\"],\n                \"named-elements\",\n                order_strict=False,\n            )\n        assert errs == []\n\n    def test_difforder_tolerant_nowarn(self):\n        errs = []\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            _compare_name_lists(\n                errs,\n                [\"a\", \"b\"],\n                [\"b\", \"a\"],\n                \"named-elements\",\n                order_strict=False,\n                suppress_warnings=True,\n            )\n        assert errs == []", "\n\nclass Test__compare_attributes:\n    def test_compare_attributes_namelists(self):\n        # Check that it calls the generic _compare_name_lists routine, passing all the\n        # correct controls\n        # Mimic 2 objects with NO attributes.\n        attrs1 = mock.MagicMock()\n        attrs2 = mock.MagicMock()\n        # Make the test objects look like real files (not NcData), and ensure that\n        # obj.ncattrs() is iterable.\n        obj1 = mock.Mock(\n            spec=\"ncattrs\", ncattrs=mock.Mock(return_value=attrs1)\n        )\n        obj2 = mock.Mock(\n            spec=\"ncattrs\", ncattrs=mock.Mock(return_value=attrs2)\n        )\n        errs = mock.sentinel.errors_list\n        elemname = \"<elem_types>\"\n        order = mock.sentinel.attrs_order\n        suppress = mock.sentinel.suppress_warnings\n        tgt = \"tests._compare_nc_datasets._compare_name_lists\"\n        with mock.patch(tgt) as patch_tgt:\n            _compare_attributes(\n                errs=errs,\n                obj1=obj1,\n                obj2=obj2,\n                elemname=elemname,\n                attrs_order=order,\n                suppress_warnings=suppress,\n            )\n        assert patch_tgt.call_args_list == [\n            mock.call(\n                errs,\n                attrs1,\n                attrs2,\n                \"<elem_types> attribute lists\",\n                order_strict=order,\n                suppress_warnings=suppress,\n            )\n        ]\n\n    class Nc4ObjectWithAttrsMimic:\n        def __init__(self, **attrs):\n            self._attrs = attrs or {}\n\n        def ncattrs(self):\n            return self._attrs.keys()\n\n        def getncattr(self, item):\n            # Mimic how netCDF4 returns attribute values\n            #  numeric arrays --> numpy ndarray\n            #  numeric scalars --> Python int / float\n            #  strings --> strings\n            #  char arrays --> list of string\n            # AND IN ALL CASES : [x] --> x\n            value = np.array(self._attrs[item], ndmin=1)\n            is_scalar = value.size == 1\n            if value.dtype.kind in (\"S\", \"U\"):\n                # strings are returned in lists, not arrays\n                value = [str(x) for x in value]\n            if is_scalar:\n                # single values are always returned as scalars\n                value = value[0]\n            return value\n\n    def test_compare_attributes_empty(self):\n        # Test two objects with no attributes\n        obj1 = self.Nc4ObjectWithAttrsMimic()\n        obj2 = self.Nc4ObjectWithAttrsMimic()\n        errs = []\n        _compare_attributes(errs, obj1, obj2, \"<object attributes>\")\n        assert errs == []\n\n    def test_compare_attributes_values__allok(self):\n        # Objects with matching attributes\n        obj1 = self.Nc4ObjectWithAttrsMimic(a=1, b=2)\n        obj2 = self.Nc4ObjectWithAttrsMimic(a=1, b=2)\n        errs = []\n        _compare_attributes(errs, obj1, obj2, \"<object attributes>\")\n        assert errs == []\n\n    def test_compare_attributes_values__data_mismatch(self):\n        # Attributes of different value (but matching dtype)\n        obj1 = self.Nc4ObjectWithAttrsMimic(a=1, b=2, c=3)\n        obj2 = self.Nc4ObjectWithAttrsMimic(a=1, b=-77, c=3)\n        errs = []\n        _compare_attributes(errs, obj1, obj2, \"<object attributes>\")\n        assert errs == [\n            '<object attributes> \"b\" attribute values differ : 2 != -77'\n        ]\n\n    def test_compare_attributes_values__dtype_mismatch(self):\n        # Attributes of different dtypes, even though values ==\n        obj1 = self.Nc4ObjectWithAttrsMimic(a=np.float32(0))\n        obj2 = self.Nc4ObjectWithAttrsMimic(a=np.float64(0))\n        errs = []\n        _compare_attributes(errs, obj1, obj2, \"<object attributes>\")\n        assert errs == [\n            (\n                '<object attributes> \"a\" attribute datatypes differ : '\n                \"dtype('float32') != dtype('float64')\"\n            )\n        ]\n\n    def test_compare_attributes_values__dtype_and_data_mismatch(self):\n        # Attributes of different dtypes, but values !=\n        obj1 = self.Nc4ObjectWithAttrsMimic(a=np.float32(0))\n        obj2 = self.Nc4ObjectWithAttrsMimic(a=np.float64(1))\n        errs = []\n        _compare_attributes(errs, obj1, obj2, \"<object attributes>\")\n        assert errs == [\n            '<object attributes> \"a\" attribute datatypes differ : '\n            \"dtype('float32') != dtype('float64')\"\n        ]\n\n    def test_compare_attributes_values__string_nonstring(self):\n        # Attributes of string and non-string types, since we handle that differently\n        obj1 = self.Nc4ObjectWithAttrsMimic(a=1)\n        obj2 = self.Nc4ObjectWithAttrsMimic(a=\"1\")\n        errs = []\n        _compare_attributes(errs, obj1, obj2, \"<object attributes>\")\n        assert errs == [\n            '<object attributes> \"a\" attribute datatypes differ : '\n            \"dtype('int64') != <class 'str'>\"\n        ]\n\n    def test_compare_attributes_values__string_match(self):\n        # Attributes of string type (since netCDF4 returns char attributes as string)\n        obj1 = self.Nc4ObjectWithAttrsMimic(S=\"this\")\n        obj2 = self.Nc4ObjectWithAttrsMimic(S=\"this\")\n        errs = []\n        _compare_attributes(errs, obj1, obj2, \"<object attributes>\")\n        assert errs == []\n\n    def test_compare_attributes_values__string_mismatch(self):\n        # Attributes of string type (since netCDF4 returns char attributes as string)\n        obj1 = self.Nc4ObjectWithAttrsMimic(S=\"this\")\n        obj2 = self.Nc4ObjectWithAttrsMimic(S=\"that\")\n        errs = []\n        _compare_attributes(errs, obj1, obj2, \"<object attributes>\")\n        assert errs == [\n            \"<object attributes> \\\"S\\\" attribute values differ : 'this' != 'that'\"\n        ]\n\n    def test_compare_attributes_values__string_array_match(self):\n        # Attributes of string type (since netCDF4 returns char attributes as string)\n        obj1 = self.Nc4ObjectWithAttrsMimic(S=[\"a\", \"b\"])\n        obj2 = self.Nc4ObjectWithAttrsMimic(S=[\"a\", \"b\"])\n        errs = []\n        _compare_attributes(errs, obj1, obj2, \"<object attributes>\")\n        assert errs == []\n\n    def test_compare_attributes_values__string_array_mismatch(self):\n        # Attributes of string type (since netCDF4 returns char attributes as string)\n        obj1 = self.Nc4ObjectWithAttrsMimic(S=[\"a\", \"b\"])\n        obj2 = self.Nc4ObjectWithAttrsMimic(S=[\"a\", \"c\"])\n        errs = []\n        _compare_attributes(errs, obj1, obj2, \"<object attributes>\")\n        assert errs == [\n            '<object attributes> \"S\" attribute values differ : '\n            \"['a', 'b'] != ['a', 'c']\"\n        ]", "\n\n@pytest.fixture(autouse=True, scope=\"module\")\ndef temp_ncfiles_dir(tmp_path_factory):\n    tmp_dirpath = tmp_path_factory.mktemp(\"samencfiles\")\n    return tmp_dirpath\n\n\n@pytest.fixture\ndef samefiles_filesonly(temp_ncfiles_dir):\n    file1_nc = temp_ncfiles_dir / \"tmp1.nc\"\n    file1_cdl = temp_ncfiles_dir / \"tmp1.cdl\"\n    file2_nc = temp_ncfiles_dir / \"tmp2.nc\"\n    file2_cdl = temp_ncfiles_dir / \"tmp2.cdl\"\n    ncgen_from_cdl(cdl_str=_simple_cdl, nc_path=file1_nc, cdl_path=file1_cdl)\n    ncgen_from_cdl(cdl_str=_simple_cdl, nc_path=file2_nc, cdl_path=file2_cdl)\n    return file1_nc, file2_nc", "@pytest.fixture\ndef samefiles_filesonly(temp_ncfiles_dir):\n    file1_nc = temp_ncfiles_dir / \"tmp1.nc\"\n    file1_cdl = temp_ncfiles_dir / \"tmp1.cdl\"\n    file2_nc = temp_ncfiles_dir / \"tmp2.nc\"\n    file2_cdl = temp_ncfiles_dir / \"tmp2.cdl\"\n    ncgen_from_cdl(cdl_str=_simple_cdl, nc_path=file1_nc, cdl_path=file1_cdl)\n    ncgen_from_cdl(cdl_str=_simple_cdl, nc_path=file2_nc, cdl_path=file2_cdl)\n    return file1_nc, file2_nc\n", "\n\n@pytest.fixture(params=[\"InputsFile\", \"InputsNcdata\"])\ndef sourcetype(request):\n    return request.param\n\n\n@pytest.fixture\ndef samefiles_bothtypes(samefiles_filesonly, sourcetype):\n    source1, source2 = samefiles_filesonly\n    if sourcetype == \"InputsNcdata\":\n        from ncdata.netcdf4 import from_nc4\n\n        source1, source2 = [from_nc4(src) for src in (source1, source2)]\n    return source1, source2", "def samefiles_bothtypes(samefiles_filesonly, sourcetype):\n    source1, source2 = samefiles_filesonly\n    if sourcetype == \"InputsNcdata\":\n        from ncdata.netcdf4 import from_nc4\n\n        source1, source2 = [from_nc4(src) for src in (source1, source2)]\n    return source1, source2\n\n\nclass Test_compare_nc_files__api:\n    def test_identical(self, samefiles_bothtypes):\n        source1, source2 = samefiles_bothtypes\n        result = compare_nc_datasets(source1, source2)\n        assert result == []\n\n    def test_identical_stringpaths(self, samefiles_filesonly):\n        source1, source2 = samefiles_filesonly\n        result = compare_nc_datasets(str(source1), str(source2))\n        assert result == []\n\n    def test_identical_datasets(self, samefiles_filesonly, sourcetype):\n        source1, source2 = samefiles_filesonly\n        ds1, ds2 = None, None\n        try:\n            ds1 = nc.Dataset(source1)\n            ds2 = nc.Dataset(source2)\n            result = compare_nc_datasets(ds1, ds2)\n            assert result == []\n        finally:\n            for ds in (ds1, ds2):\n                if ds:\n                    ds.close()\n\n    def test_small_difference(\n        self, samefiles_bothtypes, temp_ncfiles_dir, sourcetype\n    ):\n        source1, source2 = samefiles_bothtypes\n\n        if sourcetype == \"InputsFile\":\n            # Replace source2 with a modified, renamed file.\n            source2 = temp_ncfiles_dir / \"smalldiff.nc\"\n            shutil.copy(source1, source2)\n            ds = nc.Dataset(source2, \"r+\")\n            ds.extra_global_attr = 1\n            ds.close()\n        else:\n            # Source1/2 are NcData : just modify source2\n            source2.attributes[\"extra_global_attr\"] = 1\n\n        result = compare_nc_datasets(source1, source2)\n        assert result == [\n            \"Dataset attribute lists do not match: [] != ['extra_global_attr']\"\n        ]\n\n    def test_vardata_difference(\n        self, samefiles_bothtypes, temp_ncfiles_dir, sourcetype\n    ):\n        # Temporary test for specific problem encountered with masked data differences.\n        source1, source2 = samefiles_bothtypes\n        ds = None\n        try:\n            if sourcetype == \"InputsFile\":\n                # Make a modified, renamed file for the comparison.\n                source2 = temp_ncfiles_dir / \"vardiff.nc\"\n                shutil.copy(source1, source2)\n                ds = nc.Dataset(source2, \"r+\")\n                tgtx = ds.variables[\"x\"]\n                tgty = ds.variables[\"y\"]\n            else:\n                # Source1/2 are NcData : just modify source2\n                tgtx = source2.variables[\"x\"].data\n                tgty = source2.variables[\"y\"].data\n\n            tgtx[-1] = 101.23\n            tgtx[0] = 102.34\n            tgty[2:5] = [201, 202, 203]\n\n        finally:\n            if ds is not None:\n                ds.close()\n\n        result = compare_nc_datasets(source1, source2)\n        assert result == [\n            (\n                'Dataset variable \"x\" data contents differ, at 2 points: '\n                \"@INDICES[(4,), (0,)] : LHS=[masked, 0.12], RHS=[101.23, 102.34]\"\n            ),\n            (\n                'Dataset variable \"y\" data contents differ, at 3 points: '\n                \"@INDICES[(3,), (4,), ...] : \"\n                \"LHS=[masked, masked, ...], RHS=[202, 203, ...]\"\n            ),\n        ]", "\nclass Test_compare_nc_files__api:\n    def test_identical(self, samefiles_bothtypes):\n        source1, source2 = samefiles_bothtypes\n        result = compare_nc_datasets(source1, source2)\n        assert result == []\n\n    def test_identical_stringpaths(self, samefiles_filesonly):\n        source1, source2 = samefiles_filesonly\n        result = compare_nc_datasets(str(source1), str(source2))\n        assert result == []\n\n    def test_identical_datasets(self, samefiles_filesonly, sourcetype):\n        source1, source2 = samefiles_filesonly\n        ds1, ds2 = None, None\n        try:\n            ds1 = nc.Dataset(source1)\n            ds2 = nc.Dataset(source2)\n            result = compare_nc_datasets(ds1, ds2)\n            assert result == []\n        finally:\n            for ds in (ds1, ds2):\n                if ds:\n                    ds.close()\n\n    def test_small_difference(\n        self, samefiles_bothtypes, temp_ncfiles_dir, sourcetype\n    ):\n        source1, source2 = samefiles_bothtypes\n\n        if sourcetype == \"InputsFile\":\n            # Replace source2 with a modified, renamed file.\n            source2 = temp_ncfiles_dir / \"smalldiff.nc\"\n            shutil.copy(source1, source2)\n            ds = nc.Dataset(source2, \"r+\")\n            ds.extra_global_attr = 1\n            ds.close()\n        else:\n            # Source1/2 are NcData : just modify source2\n            source2.attributes[\"extra_global_attr\"] = 1\n\n        result = compare_nc_datasets(source1, source2)\n        assert result == [\n            \"Dataset attribute lists do not match: [] != ['extra_global_attr']\"\n        ]\n\n    def test_vardata_difference(\n        self, samefiles_bothtypes, temp_ncfiles_dir, sourcetype\n    ):\n        # Temporary test for specific problem encountered with masked data differences.\n        source1, source2 = samefiles_bothtypes\n        ds = None\n        try:\n            if sourcetype == \"InputsFile\":\n                # Make a modified, renamed file for the comparison.\n                source2 = temp_ncfiles_dir / \"vardiff.nc\"\n                shutil.copy(source1, source2)\n                ds = nc.Dataset(source2, \"r+\")\n                tgtx = ds.variables[\"x\"]\n                tgty = ds.variables[\"y\"]\n            else:\n                # Source1/2 are NcData : just modify source2\n                tgtx = source2.variables[\"x\"].data\n                tgty = source2.variables[\"y\"].data\n\n            tgtx[-1] = 101.23\n            tgtx[0] = 102.34\n            tgty[2:5] = [201, 202, 203]\n\n        finally:\n            if ds is not None:\n                ds.close()\n\n        result = compare_nc_datasets(source1, source2)\n        assert result == [\n            (\n                'Dataset variable \"x\" data contents differ, at 2 points: '\n                \"@INDICES[(4,), (0,)] : LHS=[masked, 0.12], RHS=[101.23, 102.34]\"\n            ),\n            (\n                'Dataset variable \"y\" data contents differ, at 3 points: '\n                \"@INDICES[(3,), (4,), ...] : \"\n                \"LHS=[masked, masked, ...], RHS=[202, 203, ...]\"\n            ),\n        ]", ""]}
{"filename": "tests/unit/tests/unit/__init__.py", "chunked_list": ["\"\"\"\nUnit tests for :mod:`tests.unit`.\n\nYes I know, tests of tests.  But it seems necessary.\n\"\"\"\n"]}
{"filename": "tests/unit/core/test_NcData.py", "chunked_list": ["\"\"\"Tests for class :class:`ncdata.NcData`.\n\nThere is almost no behaviour, but we can test some constructor usages.\n\"\"\"\nfrom ncdata import NcData\n\n\nclass Test_NcData__init__:\n    def test_noargs(self):\n        sample = NcData()\n        assert sample.name is None\n        assert sample.dimensions == {}\n        assert sample.variables == {}\n        assert sample.groups == {}\n        assert sample.attributes == {}\n\n    def test_allargs(self):\n        # Since there is no type checking, you can put what you like in the \"slots\".\n        name = \"data_name\"\n        dims = {f\"dimname_{i}\": f\"dim_{i}\" for i in range(3)}\n        vars = {f\"varname_{i}\": f\"var_{i}\" for i in range(5)}\n        attrs = {f\"varname_{i}\": f\"var_{i}\" for i in range(6)}\n        grps = {f\"groupname_{i}\": f\"group_{i}\" for i in range(3)}\n        sample1 = NcData(\n            name=name,\n            dimensions=dims,\n            variables=vars,\n            attributes=attrs,\n            groups=grps,\n        )\n        # Nothing is copied : all contents are simply references to the inputs\n        assert sample1.name is name\n        assert sample1.dimensions is dims\n        assert sample1.variables is vars\n        assert sample1.groups is grps\n        assert sample1.attributes is attrs\n\n        # Also check construction with arguments alone (no keywords).\n        sample2 = NcData(name, dims, vars, attrs, grps)\n        # result is a new object, but contents are references identical to the other\n        assert sample2 is not sample1\n        for name in dir(sample1):\n            if not name.startswith(\"_\"):\n                assert getattr(sample2, name) is getattr(sample1, name)", "\n\n# Note: str() and repr() of NcData are too complex to test unit-wise.\n# See integration tests for some sample results.\n"]}
{"filename": "tests/unit/core/test_NcDimension.py", "chunked_list": ["\"\"\"Tests for class :class:`ncdata.NcDimension`.\"\"\"\nfrom unittest import mock\n\nimport numpy as np\nimport pytest\n\nfrom ncdata import NcDimension\n\n\nclass Test_NcDimension__init__:\n    def test_simple(self):\n        name = \"dimname\"\n        # Use an array object for test size, just so we can check \"is\".\n        # N.B. assumes the constructor copies the reference - which could change.\n        size = np.array(4)\n        sample = NcDimension(name, size)\n        # No data, no dtype.  Variables don't have 'shape' anyway\n        assert sample.name is name\n        assert sample.size is size\n\n    @pytest.mark.parametrize(\"size\", [0, 2])\n    @pytest.mark.parametrize(\n        \"unlim\",\n        [0, 1, False, True, \"x\"],\n        ids=[\"unlim_0\", \"unlim_1\", \"unlim_F\", \"unlim_T\", \"unlim_x\"],\n    )\n    def test_unlimited(self, size, unlim):\n        name = \"dimname\"\n        sample = NcDimension(name, size, unlimited=unlim)\n        expected_unlim = bool(unlim) or size == 0\n        assert isinstance(sample.unlimited, bool)\n        assert sample.unlimited == expected_unlim", "\nclass Test_NcDimension__init__:\n    def test_simple(self):\n        name = \"dimname\"\n        # Use an array object for test size, just so we can check \"is\".\n        # N.B. assumes the constructor copies the reference - which could change.\n        size = np.array(4)\n        sample = NcDimension(name, size)\n        # No data, no dtype.  Variables don't have 'shape' anyway\n        assert sample.name is name\n        assert sample.size is size\n\n    @pytest.mark.parametrize(\"size\", [0, 2])\n    @pytest.mark.parametrize(\n        \"unlim\",\n        [0, 1, False, True, \"x\"],\n        ids=[\"unlim_0\", \"unlim_1\", \"unlim_F\", \"unlim_T\", \"unlim_x\"],\n    )\n    def test_unlimited(self, size, unlim):\n        name = \"dimname\"\n        sample = NcDimension(name, size, unlimited=unlim)\n        expected_unlim = bool(unlim) or size == 0\n        assert isinstance(sample.unlimited, bool)\n        assert sample.unlimited == expected_unlim", "        # assert sample.isunlimited() == sample.unlimited\n\n\nclass Test_NcDimension__str_repr:\n    @pytest.fixture(params=(0, 23), autouse=True)\n    def size(self, request):\n        return request.param\n\n    @pytest.fixture(params=(\"fixed\", \"unlimited\"), autouse=True)\n    def unlim_type(self, request):\n        return request.param\n\n    def test_repr_sized(self, size, unlim_type):\n        kwargs = {}\n        unlimited = unlim_type == \"unlimited\"\n        if unlimited:\n            kwargs[\"unlimited\"] = True\n        sample = NcDimension(\"this\", size, **kwargs)\n        result = repr(sample)\n        expected = f\"NcDimension('this', {size})\"\n        if size == 0 or unlimited:\n            expected = expected[:-1] + \", unlimited=True)\"\n        assert result == expected\n\n    def test_str_repr_same(self, size, unlim_type):\n        # In all cases, str == repr\n        sample = NcDimension(\"this\", size)\n        result = str(sample)\n        assert result == repr(sample)", ""]}
{"filename": "tests/unit/core/test_NcVariable.py", "chunked_list": ["\"\"\"Tests for class :class:`ncdata.NcVariable`.\n\nThere is almost no actual behaviour, but we can test some constructor behaviours,\nsuch as valid calling options and possibilities for data and dtype.\n\"\"\"\nimport re\n\nimport dask.array as da\nimport numpy as np\nimport pytest", "import numpy as np\nimport pytest\n\nfrom ncdata import NcAttribute, NcData, NcVariable\n\n\nclass Test_NcVariable__init__:\n    def test_minimum_args(self):\n        # We can create a variable with no args.\n        name = \"varname\"\n        sample = NcVariable(\"varname\")\n        # No data, no dtype.  Variables don't have 'shape' anyway\n        assert sample.name is name\n        assert sample.dimensions == ()\n        assert sample.data is None\n        assert sample.dtype is None\n        assert sample.attributes == {}\n\n    def test_never_nameless(self):\n        # Can't create a variable without a name.\n        msg = \"required positional argument: 'name'\"\n        with pytest.raises(TypeError, match=msg):\n            _ = NcVariable()\n\n    def test_allargs(self):\n        # Since there is no type checking, you can put whatever you like in the \"slots\".\n        name = \"data_name\"\n        dims = {f\"dimname_{i}\": f\"dim_{i}\" for i in range(3)}\n        vars = {f\"varname_{i}\": f\"var_{i}\" for i in range(5)}\n        attrs = {f\"varname_{i}\": f\"var_{i}\" for i in range(6)}\n        grps = {f\"groupname_{i}\": f\"group_{i}\" for i in range(3)}\n        sample1 = NcData(\n            name=name,\n            dimensions=dims,\n            variables=vars,\n            attributes=attrs,\n            groups=grps,\n        )\n        # Nothing is copied : all contents are simply references to the inputs\n        assert sample1.name is name\n        assert sample1.dimensions is dims\n        assert sample1.variables is vars\n        assert sample1.groups is grps\n        assert sample1.attributes is attrs\n        # Also check construction with arguments alone (no keywords).\n        sample2 = NcData(name, dims, vars, attrs, grps)\n        # result is new object, but all contents are references identical to the other\n        assert sample2 is not sample1\n        for name in [n for n in dir(sample1) if n[0] != \"_\"]:\n            assert getattr(sample2, name) is getattr(sample1, name)\n\n    @pytest.mark.parametrize(\"with_data\", [False, True])\n    def test_dtype__with_and_without_data(self, with_data):\n        test_dtype_nonfunctional = \"invalid_dtype\"\n        data = np.arange(2.0) if with_data else None\n        var = NcVariable(\"x\", data=data, dtype=test_dtype_nonfunctional)\n        expect_dtype = data.dtype if with_data else test_dtype_nonfunctional\n        assert var.dtype == expect_dtype\n\n    @pytest.mark.parametrize(\n        # All of these cases should deliver data wrapped as np.asanyarray()\n        \"data\",\n        [\n            12,\n            3.4,\n            \"a\",\n            \"bcd\",\n            [1, 2],\n            [(3.4, 4), (5, 6)],\n            [\"xxx\", \"yyy\", \"zzz\"],\n        ],\n    )\n    def test_data__pythontypes(self, data):\n        var = NcVariable(\"x\", data=data, dtype=\"ineffective\")\n        expect_array = np.asanyarray(data)\n        assert isinstance(var.data, np.ndarray)\n        assert var.dtype == expect_array.dtype\n        assert np.all(var.data == expect_array)\n        if isinstance(data, np.ndarray):\n            # Actual array data is stored directly\n            assert var.data is data\n\n    def test_data__listsofstrings(self):\n        # handling of a ragged string-length is a specific case worth exploring\n        data = [[\"one\", \"two\", \"three\"], [\"4\", \"5\", \"6\"]]\n        var = NcVariable(\"x\", data=data)\n        assert isinstance(var.data, np.ndarray)\n        assert var.data.shape == (2, 3)\n        assert var.dtype == var.data.dtype == np.dtype(\"U5\")\n\n    @pytest.mark.parametrize(\n        \"dtype\", [\"i4\", \"i8\", \"f4\", \"f8\", \"u1\", \"i1\", \"U4\"]\n    )\n    def test_data__numpy(self, dtype):\n        elem = \"abc\" if dtype.startswith(\"U\") else 12\n        dtype = np.dtype(dtype)\n        arr = np.zeros((2, 3), dtype=dtype)\n        arr[:] = elem\n        # The variable should preserve the original of a contained numpy array, and adopt\n        # it's dtype as it's own\n        var = NcVariable(\"x\", data=arr)\n        assert var.data is arr  # not copied or re-formed\n        assert var.dtype == dtype\n\n    @pytest.mark.parametrize(\"dtype\", [\"i4\", \"i8\", \"f4\", \"f8\", \"u1\", \"i1\"])\n    def test_data__numpy_scalar(self, dtype):\n        # As above, but for scalars.\n        dtype = np.dtype(dtype)\n        value = np.array(1.0, dtype=dtype)\n        var = NcVariable(\"x\", data=value)\n        assert var.data is value\n        assert var.dtype == dtype\n\n    def test_data__dask(self):\n        # Check that lazy data is also contained directly, without adjustment or\n        # modification\n        arr = da.ones((2, 3))\n        var = NcVariable(\"x\", data=arr)\n        assert var.data is arr", "\n\nclass TestNcVariable__str__repr:\n    def test_nameonly(self):\n        var = NcVariable(\"varname\")\n        result = str(var)\n        expected = \"<NcVariable: varname()>\"\n        assert result == expected\n\n    def test_dimsnoargs(self):\n        var = NcVariable(\"var_w_dims\", [\"dim0\", \"dim1\"])\n        result = str(var)\n        expected = \"<NcVariable: var_w_dims(dim0, dim1)>\"\n        assert result == expected\n\n    def test_oneattr(self):\n        var = NcVariable(\n            \"var_w_attrs\", attributes={\"a1\": NcAttribute(\"a1\", 1)}\n        )\n        result = str(var)\n        expected = \"\\n\".join(\n            [\"<NcVariable: var_w_attrs()\", \"    var_w_attrs:a1 = 1\", \">\"]\n        )\n        assert result == expected\n\n    def test_multiattrs(self):\n        var = NcVariable(\n            \"var_multi\",\n            attributes={\n                \"a1\": NcAttribute(\"a1\", 1),\n                \"a2\": NcAttribute(\"a2\", [\"one\", \"three\"]),\n            },\n        )\n        result = str(var)\n        expected = \"\\n\".join(\n            [\n                \"<NcVariable: var_multi()\",\n                \"    var_multi:a1 = 1\",\n                \"    var_multi:a2 = ['one', 'three']\",\n                \">\",\n            ]\n        )\n        assert result == expected\n\n    def test_repr(var):\n        var = NcVariable(\n            \"var\",\n            dimensions=(\"x\", \"y\"),\n            attributes={\"a1\": NcAttribute(\"a1\", 1)},\n        )\n        result = repr(var)\n        expected = f\"<ncdata._core.NcVariable object at 0x{id(var):012x}>\"\n        assert result == expected", ""]}
{"filename": "tests/unit/core/test_NcAttribute.py", "chunked_list": ["\"\"\"\nTests for class :class:`ncdata.NcAttribute`.\n\nVery simple for now, but we may add more behaviour in future.\n\"\"\"\nimport numpy as np\nimport pytest\n\nfrom ncdata import NcAttribute\n", "from ncdata import NcAttribute\n\n# Support for building testcases\n_data_types = [\n    \"int\",\n    \"float\",\n    \"string\",\n    \"numpyint\",\n    \"numpyfloat\",\n    \"numpystring\",", "    \"numpyfloat\",\n    \"numpystring\",\n]\n_container_types = [\"scalar\", \"vectorof1\", \"vectorofN\"]\n_attribute_testdata = {\n    \"int\": [1, 2, 3, 4],\n    \"float\": [1.2, 3.4, 5.6],\n    \"string\": [\"xx\", \"yyy\", \"z\"],\n}\n", "}\n\n\n@pytest.fixture(params=_data_types)\ndef datatype(request):\n    return request.param\n\n\n@pytest.fixture(params=_container_types)\ndef structuretype(request):\n    return request.param", "@pytest.fixture(params=_container_types)\ndef structuretype(request):\n    return request.param\n\n\ndef attrvalue(datatype, structuretype):\n    # get basic type = 'int', 'float', 'string'\n    (datatype,) = (\n        key for key in _attribute_testdata.keys() if key in datatype\n    )\n\n    # fetch a set of values\n    values = _attribute_testdata[datatype]\n\n    # pass as an array if specified\n    if \"numpy\" in datatype:\n        values = np.array(values)\n\n    # pass a single value, or a list of 1, or a list of N\n    if structuretype == \"scalar\":\n        result = values[0]\n    elif structuretype == \"vectorof1\":\n        result = values[:1]\n    else:\n        assert structuretype == \"vectorofN\"\n        result = values\n    return result", "\n\nclass Test_NcAttribute__init__:\n    def test_value(self, datatype, structuretype):\n        value = attrvalue(datatype, structuretype)\n\n        attr = NcAttribute(\"x\", value=value)\n        expected_array = np.asanyarray(value)\n\n        assert isinstance(attr.value, np.ndarray)\n        assert np.all(attr.value == expected_array)\n        if hasattr(value, \"dtype\"):\n            # An actual array is stored directly, i.e. without copying\n            assert attr.value is value", "\n\nclass Test_NcAttribute__as_python_value:\n    def test_value(self, datatype, structuretype):\n        value = attrvalue(datatype, structuretype)\n        attr = NcAttribute(\"x\", value)\n\n        result = attr.as_python_value()\n\n        # Note: \"vectorof1\" data should appear as *scalar*.\n        is_vector = \"vectorofN\" in structuretype\n        is_string = \"string\" in datatype\n        if is_string:\n            # String data is returned as (lists of) strings\n            if is_vector:\n                assert isinstance(result, list)\n                assert all(isinstance(x, str) for x in result)\n            else:\n                assert isinstance(result, str)\n        else:\n            # Numeric data is returned as arrays or array scalars.\n            if is_vector:\n                assert isinstance(result, np.ndarray)\n                assert result.ndim == 1\n            else:\n                # array scalar\n                assert result.shape == ()", "\n\nclass Test_NcAttribute__str_repr:\n    def test_str(self, datatype, structuretype):\n        value = attrvalue(datatype, structuretype)\n        attr = NcAttribute(\"x\", value)\n\n        result = str(attr)\n\n        # Work out what we expect/intend the value string to look like.\n        is_multiple = structuretype == \"vectorofN\"\n        if not is_multiple:\n            assert structuretype in (\"scalar\", \"vectorof1\")\n            # All single values appear as scalars.\n            value = np.array(value).flatten()[0]\n\n        value_repr = repr(value)\n\n        if is_multiple and \"string\" not in datatype:\n            # All *non-string* vectors appear wrapped as numpy 'array(...)'.\n            # N.B. but *string vectors* print just as a list of Python strings.\n            value_repr = f\"array({value_repr})\"\n\n        expect = f\"NcAttribute('x', {value_repr})\"\n        assert result == expect\n\n    def test_repr_same(self, datatype, structuretype):\n        value = attrvalue(datatype, structuretype)\n        attr = NcAttribute(\"x\", value)\n        result = str(attr)\n        expected = repr(attr)\n        assert result == expected", ""]}
{"filename": "tests/unit/core/__init__.py", "chunked_list": ["\"\"\"Unit tests for :mod:`ncdata.core`.\"\"\"\n"]}
{"filename": "lib/ncdata/iris_xarray.py", "chunked_list": ["r\"\"\"\nInterface routines for converting data between Xarray and Iris.\n\nConvert :class:`~xarray.Dataset` to and from Iris :class:`~iris.cube.Cube`\\\\s.\n\nBy design, these transformations should be equivalent to saving data from one package\nto a netcdf file, and re-loading into the other package.  There is also support for\npassing additional keywords to the appropriate load/save routines.\n\n\"\"\"", "\n\"\"\"\nimport xarray\nfrom iris.cube import CubeList\n\nfrom .iris import from_iris, to_iris\nfrom .xarray import from_xarray, to_xarray\n\n\ndef cubes_from_xarray(\n    xrds: xarray.Dataset, xr_save_kwargs=None, iris_load_kwargs=None\n) -> CubeList:\n    \"\"\"\n    Convert an xarray :class:`xarray.Dataset` to an Iris :class:`iris.cube.CubeList`.\n\n    Equivalent to saving the dataset to a netcdf file, and loading that with Iris.\n\n    Netcdf variable data in the output contains the same array objects as the input,\n    i.e. arrays (Dask or Numpy) are not copied or computed.\n\n    Parameters\n    ----------\n    xrds : :class:`xarray.Dataset`\n        input dataset\n\n    xr_save_kwargs : dict\n        additional keywords passed to :meth:`xarray.Dataset.dump_to_store`\n\n    iris_load_kwargs : dict\n        additional keywords passed to :func:`iris.fileformats.netcdf.load_cubes`\n\n    Returns\n    -------\n    cubes : :class:`iris.cube.CubeList`\n        loaded cubes\n\n    \"\"\"\n    # Ensure kwargs are dicts, for use with '**'\n    iris_load_kwargs = iris_load_kwargs or {}\n    xr_save_kwargs = xr_save_kwargs or {}\n    ncdata = from_xarray(xrds, **xr_save_kwargs)\n    cubes = to_iris(ncdata, **iris_load_kwargs)\n    return cubes", "\ndef cubes_from_xarray(\n    xrds: xarray.Dataset, xr_save_kwargs=None, iris_load_kwargs=None\n) -> CubeList:\n    \"\"\"\n    Convert an xarray :class:`xarray.Dataset` to an Iris :class:`iris.cube.CubeList`.\n\n    Equivalent to saving the dataset to a netcdf file, and loading that with Iris.\n\n    Netcdf variable data in the output contains the same array objects as the input,\n    i.e. arrays (Dask or Numpy) are not copied or computed.\n\n    Parameters\n    ----------\n    xrds : :class:`xarray.Dataset`\n        input dataset\n\n    xr_save_kwargs : dict\n        additional keywords passed to :meth:`xarray.Dataset.dump_to_store`\n\n    iris_load_kwargs : dict\n        additional keywords passed to :func:`iris.fileformats.netcdf.load_cubes`\n\n    Returns\n    -------\n    cubes : :class:`iris.cube.CubeList`\n        loaded cubes\n\n    \"\"\"\n    # Ensure kwargs are dicts, for use with '**'\n    iris_load_kwargs = iris_load_kwargs or {}\n    xr_save_kwargs = xr_save_kwargs or {}\n    ncdata = from_xarray(xrds, **xr_save_kwargs)\n    cubes = to_iris(ncdata, **iris_load_kwargs)\n    return cubes", "\n\ndef cubes_to_xarray(\n    cubes, iris_save_kwargs=None, xr_load_kwargs=None\n) -> xarray.Dataset:\n    r\"\"\"\n    Convert Iris :class:`iris.cube.Cube`\\\\s to an xarray :class:`xarray.Dataset`.\n\n    Equivalent to saving the dataset to a netcdf file, and loading that with Xarray.\n\n    Netcdf variable data in the output contains the same array objects as the input,\n    i.e. arrays (Dask or Numpy) are not copied or computed.\n\n    Parameters\n    ----------\n    cubes : :class:`iris.cube.Cube`, or iterable of Cubes.\n        source data\n\n    iris_save_kwargs : dict\n        additional keywords passed to :func:`iris.save`, and to\n        :func:`iris.fileformats.netcdf.saver.save`\n\n    xr_load_kwargs : dict\n        additional keywords passed to :meth:`xarray.Dataset.load_store`\n\n    Returns\n    -------\n    xrds : :class:`xarray.Dataset`\n        converted data in the form of an Xarray :class:`xarray.Dataset`\n\n    \"\"\"\n    # Ensure kwargs are dicts, for use with '**'\n    iris_save_kwargs = iris_save_kwargs or {}\n    xr_load_kwargs = xr_load_kwargs or {}\n    ncdata = from_iris(cubes, **iris_save_kwargs)\n    xrds = to_xarray(ncdata, **xr_load_kwargs)\n    return xrds", ""]}
{"filename": "lib/ncdata/xarray.py", "chunked_list": ["\"\"\"\nInterface routines for converting data between ncdata and xarray.\n\nConverts :class:`~ncdata.NcData` to and from Xarray :class:`~xarray.Dataset` objects.\n\nThis embeds a certain amount of Xarray knowledge (and dependency), hopefully a minimal\namount.  The structure of an NcData object makes it fairly painless.\n\n\"\"\"\nfrom pathlib import Path", "\"\"\"\nfrom pathlib import Path\nfrom typing import AnyStr, Union\n\nimport xarray as xr\n\nfrom . import NcAttribute, NcData, NcDimension, NcVariable\n\n\nclass _XarrayNcDataStore:\n    \"\"\"\n    An adapter class presenting ncdata as an xarray datastore.\n\n    Provides a subset of the\n    :class:`xarray.common.AbstractWriteableDataStore` interface, and which converts\n    to/from a contained :class:`~ncdata.NcData`.\n\n    This requires some knowledge of Xarray, but it is very small.\n\n    This code originated from @TomekTrzeciak.\n    See https://gist.github.com/TomekTrzeciak/b00ff6c9dc301ed6f684990e400d1435\n    \"\"\"\n\n    def __init__(self, ncdata: NcData = None):\n        if ncdata is None:\n            ncdata = NcData()\n        self.ncdata = ncdata\n\n    def load(self):\n        variables = {}\n        for k, v in self.ncdata.variables.items():\n            attrs = {\n                name: attr.as_python_value()\n                for name, attr in v.attributes.items()\n            }\n            xr_var = xr.Variable(\n                v.dimensions, v.data, attrs, getattr(v, \"encoding\", {})\n            )\n            # TODO: ?possibly? need to apply usual Xarray \"encodings\" to convert raw\n            #  cf-encoded data into 'normal', interpreted xr.Variables.\n            xr_var = xr.conventions.decode_cf_variable(k, xr_var)\n            variables[k] = xr_var\n        attributes = {\n            name: attr.as_python_value()\n            for name, attr in self.ncdata.attributes.items()\n        }\n        return variables, attributes\n\n    def store(\n        self,\n        variables,\n        attributes,\n        check_encoding_set=frozenset(),\n        writer=None,\n        unlimited_dims=None,\n    ):\n        for attrname, v in attributes.items():\n            if (\n                attrname in self.ncdata.attributes\n            ):  # and self.attributes[k] != v:\n                msg = (\n                    f're-setting of attribute \"{attrname}\" : '\n                    f\"was={self.ncdata.attributes[attrname]}, now={v}\"\n                )\n                raise ValueError(msg)\n            else:\n                self.ncdata.attributes[attrname] = NcAttribute(attrname, v)\n\n        for varname, var in variables.items():\n            if varname in self.ncdata.variables:\n                raise ValueError(f'duplicate variable : \"{varname}\"')\n\n            # An xr.Variable : remove all the possible Xarray encodings\n            # These are all the ones potentially used by\n            # :func:`xr.conventions.decode_cf_variable`, in the order in which they\n            # would be applied.\n            var = xr.conventions.encode_cf_variable(\n                var, name=varname, needs_copy=False\n            )\n\n            for dim_name, size in zip(var.dims, var.shape):\n                if dim_name in self.ncdata.dimensions:\n                    if self.ncdata.dimensions[dim_name].size != size:\n                        raise ValueError(\n                            f\"size mismatch for dimension {dim_name!r}: \"\n                            f\"{self.ncdata.dimensions[dim_name]} != {size}\"\n                        )\n                else:\n                    self.ncdata.dimensions[dim_name] = NcDimension(\n                        dim_name, size=size\n                    )\n\n            attrs = {\n                name: NcAttribute(name, value)\n                for name, value in var.attrs.items()\n            }\n            nc_var = NcVariable(\n                name=varname,\n                dimensions=var.dims,\n                attributes=attrs,\n                data=var.data,\n                group=self.ncdata,\n            )\n            self.ncdata.variables[varname] = nc_var\n\n    def close(self):\n        pass\n\n    #\n    # This interface supports conversion to+from an xarray \"Dataset\".\n    # N.B. using the \"AbstractDataStore\" interface preserves variable contents, being\n    # either real or lazy arrays.\n    #\n    @classmethod\n    def from_xarray(\n        cls, dataset_or_file: Union[xr.Dataset, AnyStr, Path], **xr_load_kwargs\n    ):\n        if not isinstance(dataset_or_file, xr.Dataset):\n            # It's a \"file\" (or pathstring, or Path ?).\n            dataset_or_file = xr.load_dataset(\n                dataset_or_file, **xr_load_kwargs\n            )\n        nc_data = cls()\n        dataset_or_file.dump_to_store(nc_data, **xr_load_kwargs)\n        return nc_data\n\n    def to_xarray(self, **xr_save_kwargs) -> xr.Dataset:\n        ds = xr.Dataset.load_store(self, **xr_save_kwargs)\n        return ds", "\nclass _XarrayNcDataStore:\n    \"\"\"\n    An adapter class presenting ncdata as an xarray datastore.\n\n    Provides a subset of the\n    :class:`xarray.common.AbstractWriteableDataStore` interface, and which converts\n    to/from a contained :class:`~ncdata.NcData`.\n\n    This requires some knowledge of Xarray, but it is very small.\n\n    This code originated from @TomekTrzeciak.\n    See https://gist.github.com/TomekTrzeciak/b00ff6c9dc301ed6f684990e400d1435\n    \"\"\"\n\n    def __init__(self, ncdata: NcData = None):\n        if ncdata is None:\n            ncdata = NcData()\n        self.ncdata = ncdata\n\n    def load(self):\n        variables = {}\n        for k, v in self.ncdata.variables.items():\n            attrs = {\n                name: attr.as_python_value()\n                for name, attr in v.attributes.items()\n            }\n            xr_var = xr.Variable(\n                v.dimensions, v.data, attrs, getattr(v, \"encoding\", {})\n            )\n            # TODO: ?possibly? need to apply usual Xarray \"encodings\" to convert raw\n            #  cf-encoded data into 'normal', interpreted xr.Variables.\n            xr_var = xr.conventions.decode_cf_variable(k, xr_var)\n            variables[k] = xr_var\n        attributes = {\n            name: attr.as_python_value()\n            for name, attr in self.ncdata.attributes.items()\n        }\n        return variables, attributes\n\n    def store(\n        self,\n        variables,\n        attributes,\n        check_encoding_set=frozenset(),\n        writer=None,\n        unlimited_dims=None,\n    ):\n        for attrname, v in attributes.items():\n            if (\n                attrname in self.ncdata.attributes\n            ):  # and self.attributes[k] != v:\n                msg = (\n                    f're-setting of attribute \"{attrname}\" : '\n                    f\"was={self.ncdata.attributes[attrname]}, now={v}\"\n                )\n                raise ValueError(msg)\n            else:\n                self.ncdata.attributes[attrname] = NcAttribute(attrname, v)\n\n        for varname, var in variables.items():\n            if varname in self.ncdata.variables:\n                raise ValueError(f'duplicate variable : \"{varname}\"')\n\n            # An xr.Variable : remove all the possible Xarray encodings\n            # These are all the ones potentially used by\n            # :func:`xr.conventions.decode_cf_variable`, in the order in which they\n            # would be applied.\n            var = xr.conventions.encode_cf_variable(\n                var, name=varname, needs_copy=False\n            )\n\n            for dim_name, size in zip(var.dims, var.shape):\n                if dim_name in self.ncdata.dimensions:\n                    if self.ncdata.dimensions[dim_name].size != size:\n                        raise ValueError(\n                            f\"size mismatch for dimension {dim_name!r}: \"\n                            f\"{self.ncdata.dimensions[dim_name]} != {size}\"\n                        )\n                else:\n                    self.ncdata.dimensions[dim_name] = NcDimension(\n                        dim_name, size=size\n                    )\n\n            attrs = {\n                name: NcAttribute(name, value)\n                for name, value in var.attrs.items()\n            }\n            nc_var = NcVariable(\n                name=varname,\n                dimensions=var.dims,\n                attributes=attrs,\n                data=var.data,\n                group=self.ncdata,\n            )\n            self.ncdata.variables[varname] = nc_var\n\n    def close(self):\n        pass\n\n    #\n    # This interface supports conversion to+from an xarray \"Dataset\".\n    # N.B. using the \"AbstractDataStore\" interface preserves variable contents, being\n    # either real or lazy arrays.\n    #\n    @classmethod\n    def from_xarray(\n        cls, dataset_or_file: Union[xr.Dataset, AnyStr, Path], **xr_load_kwargs\n    ):\n        if not isinstance(dataset_or_file, xr.Dataset):\n            # It's a \"file\" (or pathstring, or Path ?).\n            dataset_or_file = xr.load_dataset(\n                dataset_or_file, **xr_load_kwargs\n            )\n        nc_data = cls()\n        dataset_or_file.dump_to_store(nc_data, **xr_load_kwargs)\n        return nc_data\n\n    def to_xarray(self, **xr_save_kwargs) -> xr.Dataset:\n        ds = xr.Dataset.load_store(self, **xr_save_kwargs)\n        return ds", "\n\ndef to_xarray(ncdata: NcData, **kwargs) -> xr.Dataset:\n    \"\"\"\n    Convert :class:`~ncdata.NcData` to an xarray :class:`~xarray.Dataset`.\n\n    Parameters\n    ----------\n    ncdata : NcData\n        source data\n\n    kwargs : dict\n        additional xarray \"load keywords\", passed to :meth:`xarray.Dataset.load_store`\n\n    Returns\n    -------\n    xrds : xarray.Dataset\n        converted data in the form of an Xarray :class:`xarray.Dataset`\n\n    \"\"\"\n    return _XarrayNcDataStore(ncdata).to_xarray(**kwargs)", "\n\ndef from_xarray(xrds: Union[xr.Dataset, Path, AnyStr]) -> NcData:\n    \"\"\"\n    Convert an xarray :class:`xarray.Dataset` to a :class:`NcData`.\n\n    Parameters\n    ----------\n    xrds : :class:`xarray.Dataset`\n        source data\n\n    kwargs : dict\n        additional xarray \"save keywords\", passed to\n        :meth:`xarray.Dataset.dump_to_store`\n\n    Returns\n    -------\n    ncdata : NcData\n        data converted to an :class:`~ncdata.NcData`\n\n    \"\"\"\n    return _XarrayNcDataStore.from_xarray(xrds).ncdata", ""]}
{"filename": "lib/ncdata/_core.py", "chunked_list": ["\"\"\"\nOur main classes for representing netCDF data.\n\nThese structures support groups, variables, attributes.\nBoth data and attributes are stored as numpy-compatible array-like values (though this\nmay include dask.array.Array), and hence their types are modelled as np.dtype's.\n\nCurrent limitations :\n(1) we are *not* supporting user-defined or variable-length types.\n(2) there is no built-in consistency checking -- e.g. for correct length or existence", "(1) we are *not* supporting user-defined or variable-length types.\n(2) there is no built-in consistency checking -- e.g. for correct length or existence\nof dimensions referenced by variables.\n\n\"\"\"\nfrom typing import Dict, List, Optional, Tuple, Union\n\nimport numpy\nimport numpy as np\n", "import numpy as np\n\n#\n# A totally basic and naive representation of netCDF data.\n#\n\n\ndef _addlines_indent(text, indent=\"\"):\n    # Routine to indent each line within a newline-joined string.\n    return [indent + line for line in text.split(\"\\n\")]", "\n\n# common indent spacing\n_indent = \" \" * 4\n\n\nclass NcData:\n    \"\"\"\n    An object representing a netcdf group- or dataset-level container.\n\n    Containing dimensions, variables, attributes and sub-groups.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        dimensions: Dict[str, \"NcDimension\"] = None,\n        variables: Dict[str, \"NcVariable\"] = None,\n        attributes: Dict[str, \"NcAttribute\"] = None,\n        groups: Dict[str, \"NcData\"] = None,\n    ):  # noqa: D107\n        #: a group/dataset name (optional)\n        self.name: str = name\n        #: group/dataset dimensions\n        self.dimensions: Dict[str, \"NcDimension\"] = dimensions or {}\n        #: group/dataset variables\n        self.variables: Dict[str, \"NcVariable\"] = variables or {}\n        #: group/dataset global attributes\n        self.attributes: Dict[str, \"NcAttribute\"] = attributes or {}\n        #: sub-groups\n        self.groups: Dict[str, \"NcData\"] = groups or {}\n\n    def _print_content(self) -> str:\n        \"\"\"\n        Construct a string printout.\n\n        NcData classes all define '_print_content' (though they have no common base\n        class, so it isn't technically an abstract method).\n        This \"NcData._print_content()\" is called recursively for groups.\n        \"\"\"\n        global _indent\n        # Define a header line (always a separate line).\n        noname = \"<'no-name'>\"\n        lines = [f\"<NcData: {self.name or noname}\"]\n\n        # Add internal sections in order, indenting everything.\n        for eltype in (\"dimensions\", \"variables\", \"groups\", \"attributes\"):\n            els = getattr(self, eltype)\n            if len(els):\n                if eltype == \"attributes\":\n                    # Attributes a bit different: #1 add 'globol' to section title.\n                    lines += [f\"{_indent}global attributes:\"]\n                    # NOTE: #2 show like variable attributes, but *no parent name*.\n                    attrs_lines = [\n                        f\":{attr._print_content()}\"\n                        for attr in self.attributes.values()\n                    ]\n                    lines += _addlines_indent(\n                        \"\\n\".join(attrs_lines), _indent * 2\n                    )\n                else:\n                    lines += [f\"{_indent}{eltype}:\"]\n                    for el in els.values():\n                        lines += _addlines_indent(\n                            el._print_content(), _indent * 2\n                        )\n                lines.append(\"\")\n\n        # Strip off final blank lines (tidier for Groups as well as main dataset).\n        while len(lines[-1]) == 0:\n            lines = lines[:-1]\n\n        # Add closing line.\n        lines += [\">\"]\n        # Join with linefeeds for a simple string result.\n        return \"\\n\".join(lines)\n\n    def __str__(self):  # noqa: D105\n        return self._print_content()", "\n    # NOTE: for 'repr', an interpretable literal string is too complex.\n    # So just retain the default \"object\" address-based representation.\n\n\nclass NcDimension:\n    \"\"\"\n    An object representing a netcdf dimension.\n\n    Associates a length with a name.\n    A length of 0 indicates an \"unlimited\" dimension, though that is essentially a\n    file-specific concept.\n    \"\"\"\n\n    # TODO : I think the unlimited interpretation is limiting, since we will want to\n    #  represent \"current length\" too.\n    #  ? Change this by adopting a boolean \"is_unlimited\" property ?\n\n    def __init__(\n        self, name: str, size: int, unlimited: Optional[bool] = None\n    ):  # noqa: D107\n        #: dimension name\n        self.name: str = name\n        #: dimension size (current size, if unlimited)\n        self.size: int = size  # N.B. we retain the 'zero size means unlimited'\n        if size == 0:\n            unlimited = True\n        else:\n            unlimited = bool(unlimited)\n        #: whether dimension is unlimited\n        self.unlimited: bool = unlimited\n\n    def _print_content(self) -> str:  # noqa: D105\n        str_unlim = \"  **UNLIMITED**\" if self.unlimited else \"\"\n        return f\"{self.name} = {self.size}{str_unlim}\"\n\n    def __repr__(self):  # noqa: D105\n        str_unlim = \", unlimited=True\" if self.unlimited else \"\"\n        return f\"NcDimension({self.name!r}, {self.size}{str_unlim})\"\n\n    def __str__(self):  # noqa: D105\n        return repr(self)", "\n\nclass NcVariable:\n    \"\"\"\n    An object representing a netcdf variable.\n\n    With dimensions, dtype and data, and attributes.\n\n    'data' may be None, but if not is expected to be an array : either numpy (real) or\n    Dask (lazy).\n\n    The 'dtype' will presumably match the data, if any.\n\n    It has no 'shape' property, in practice this might be inferred from either the\n    data or dimensions.  If the dims are empty, it is a scalar.\n\n    A variable makes no effort to ensure that its dimensions, dtype + data are\n    consistent.  This is to be managed by the creator.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        dimensions: Tuple[str] = (),\n        # NOTE: flake8 objects to type checking against an unimported package, even in\n        # a quoted reference when it's not otherwise needed (and we don't want to\n        # import it).\n        # TODO: remove the flake8 annotation if this gets fixed.\n        data: Optional[\n            Union[np.ndarray, \"dask.array.array\"]  # noqa: F821\n        ] = None,\n        dtype: np.dtype = None,\n        attributes: Dict[str, \"NcAttribute\"] = None,\n        group: \"NcData\" = None,\n    ):\n        \"\"\"\n        Create a variable.\n\n        The 'dtype' arg relevant only when no data is provided :\n        If 'data' is provided, it is converted to an array if needed, and its dtype\n        replaces any provided 'dtype'.\n        \"\"\"\n        #: variable name\n        self.name: str = name\n        #: variable dimension names (a list of strings, *not* a dict of objects)\n        self.dimensions: List[str] = tuple(dimensions)\n        if data is not None:\n            if not hasattr(data, \"dtype\"):\n                data = np.asanyarray(data)\n            dtype = data.dtype\n        #: variable datatype, as a numpy :class:`numpy.dtype`\n        self.dtype: numpy.dtype = dtype\n        #: variable data (an array-like, typically a dask or numpy array)\n        self.data = data  # Supports lazy, and normally provides a dtype\n        #: variable attributes\n        self.attributes: Dict[str, NcAttribute] = attributes or {}\n        #: parent group\n        self.group: Optional[NcData] = group\n\n    # # Provide some array-like readonly properties reflected from the data.\n    # @property\n    # def dtype(self):\n    #     return self.data.dtype\n    #\n    # @property\n    # def shape(self):\n    #     return self.data.shape\n\n    def _print_content(self):\n        global _indent\n        dimstr = \", \".join(self.dimensions)\n        hdr = f\"<NcVariable: {self.name}({dimstr})\"\n        if not self.attributes:\n            hdr += \">\"\n            lines = [hdr]\n        else:\n            lines = [hdr]\n            attrs_lines = [\n                f\"{self.name}:{attr._print_content()}\"\n                for attr in self.attributes.values()\n            ]\n            lines += _addlines_indent(\"\\n\".join(attrs_lines), _indent)\n            lines += [\">\"]\n        return \"\\n\".join(lines)\n\n    def __str__(self):  # noqa: D105\n        return self._print_content()", "\n    # NOTE: as for NcData, an interpretable 'repr' string is too complex.\n    # So just retain the default \"object\" address-based representation.\n\n\nclass NcAttribute:\n    \"\"\"\n    An object representing a netcdf variable or dataset attribute.\n\n    Associates a name to a value which is a numpy scalar or 1-D array.\n\n    We expect the value to be 0- or 1-dimensional, and an allowed dtype.\n    However none of this is checked.\n\n    In an actual netcdf dataset, a \"scalar\" is actually just an array of length 1.\n    \"\"\"\n\n    def __init__(self, name: str, value):  # noqa: D107\n        #: attribute name\n        self.name: str = name\n        # Attribute values are arraylike, have dtype\n        # TODO: may need to regularise string representations?\n        if not hasattr(value, \"dtype\"):\n            value = np.asanyarray(value)\n        #: attribute value\n        self.value: np.ndarray = value\n\n    def as_python_value(self):\n        \"\"\"\n        Return the content, but converting any character data to Python strings.\n\n        An array of 1 value returns as a single scalar value, since this is how\n        attributes behave in actual netcdf data.\n\n        We don't bother converting numeric data, since numpy scalars are generally\n        interchangeable in use with Python ints or floats.\n        \"\"\"\n        result = self.value\n        # Attributes are either scalar or 1-D\n        if result.ndim >= 2:\n            raise ValueError(\n                \"Attribute value should only be 0- or 1-dimensional.\"\n            )\n\n        if result.shape == (1,):\n            result = result[0]\n\n        if result.dtype.kind in (\"U\", \"S\"):\n            if result.ndim == 0:\n                result = str(result)\n            elif result.ndim == 1:\n                result = [str(x) for x in result]\n\n        # TODO: is this a possibiliity ?\n        # if isinstance(result, bytes):\n        #     result = result.decode()\n        return result\n\n    def _print_value(self):\n        value = self.as_python_value()\n\n        # Convert numpy non-string scalars to simple Python values, in string output.\n        if getattr(value, \"shape\", None) in ((0,), (1,), ()):\n            op = {\"i\": int, \"u\": int, \"f\": float}[value.dtype.kind]\n            value = op(value.flatten()[0])\n\n        return repr(value)\n\n    def _print_content(self):\n        return f\"{self.name} = {self._print_value()}\"\n\n    def __repr__(self):  # noqa: D105\n        return f\"NcAttribute({self.name!r}, {self._print_value()})\"\n\n    def __str__(self):  # noqa: D105\n        return repr(self)", ""]}
{"filename": "lib/ncdata/netcdf4.py", "chunked_list": ["\"\"\"\nInterface routines for converting data between ncdata and netCDF4.\n\nConverts :class:`ncdata.NcData` to and from :class:`netCDF4.Dataset` objects.\n\n\"\"\"\nfrom pathlib import Path\nfrom threading import Lock\nfrom typing import Any, AnyStr, Dict, List, Optional, Union\n", "from typing import Any, AnyStr, Dict, List, Optional, Union\n\nimport dask.array as da\nimport netCDF4 as nc\nimport numpy as np\n\nfrom . import NcAttribute, NcData, NcDimension, NcVariable\n\n# The variable arguments which are 'claimed' by the existing to_nc4 code, and\n# therefore not valid to appear in the 'var_kwargs' parameter.", "# The variable arguments which are 'claimed' by the existing to_nc4 code, and\n# therefore not valid to appear in the 'var_kwargs' parameter.\n_Forbidden_Variable_Kwargs = [\"data\", \"dimensions\", \"datatype\", \"_FillValue\"]\n\n\ndef _to_nc4_group(\n    ncdata: NcData,\n    nc4object: Union[nc.Dataset, nc.Group],\n    var_kwargs: Optional[Dict[NcVariable, Any]] = None,\n) -> None:\n    \"\"\"\n    Inner routine supporting ``to_nc4``, and recursive calls for sub-groups.\n\n    See :func:``to_nc4`` for details.\n    **Except that** : this routine operates only on a dataset/group, does not accept a\n    filepath string or Path.\n    \"\"\"\n    if var_kwargs is None:\n        var_kwargs = {}\n\n    for dimname, dim in ncdata.dimensions.items():\n        size = 0 if dim.unlimited else dim.size\n        nc4object.createDimension(dimname, size)\n\n    for varname, var in ncdata.variables.items():\n        fillattr = \"_FillValue\"\n        if fillattr in var.attributes:\n            fill_value = var.attributes[fillattr].value\n        else:\n            fill_value = None\n\n        kwargs = var_kwargs.get(var, {})\n        forbidden_keys = [\n            kwarg in _Forbidden_Variable_Kwargs for kwarg in kwargs\n        ]\n        if forbidden_keys:\n            msg = (\n                f\"additional `var_kwargs` for variable {var} included key(s) \"\n                f\"{forbidden_keys}, which are disallowed since they are amongst those \"\n                \"controlled by the ncdata.netcdf interface itself : \"\n                f\"{_Forbidden_Variable_Kwargs!r}.\"\n            )\n            raise ValueError(msg)\n\n        nc4var = nc4object.createVariable(\n            varname=varname,\n            datatype=var.dtype,\n            dimensions=var.dimensions,\n            fill_value=fill_value,\n            **kwargs,\n        )\n\n        # Assign attributes.\n        # N.B. must be done before writing data, to enable scale+offset controls !\n        for attrname, attr in var.attributes.items():\n            if attrname != \"_FillValue\":\n                nc4var.setncattr(attrname, attr.as_python_value())\n\n        data = var.data\n        if hasattr(data, \"compute\"):\n            da.store(data, nc4var)\n        else:\n            nc4var[:] = data\n\n    for attrname, attr in ncdata.attributes.items():\n        nc4object.setncattr(attrname, attr.as_python_value())\n\n    for groupname, group in ncdata.groups.items():\n        nc4group = nc4object.createGroup(groupname)\n        _to_nc4_group(\n            ncdata=group,\n            nc4object=nc4group,\n            var_kwargs=var_kwargs.get(groupname, {}),\n        )", "\n\n_GLOBAL_NETCDF4_LIBRARY_THREADLOCK = Lock()\n\n\nclass _NetCDFDataProxy:\n    \"\"\"\n    A reference to the data payload of a single NetCDF file variable.\n\n    Copied from Iris, with some simplifications.\n    \"\"\"\n\n    __slots__ = (\"shape\", \"dtype\", \"path\", \"variable_name\", \"group_names_path\")\n\n    def __init__(\n        self, shape, dtype, filepath, variable_name, group_names_path=None\n    ):\n        self.shape = shape\n        self.dtype = dtype\n        self.path = filepath\n        if group_names_path is None:\n            group_names_path = []\n        self.group_names_path = group_names_path\n        self.variable_name = variable_name\n\n    @property\n    def ndim(self):\n        return len(self.shape)\n\n    def __getitem__(self, keys):\n        # Using a DatasetWrapper causes problems with invalid ID's and the\n        #  netCDF4 library, presumably because __getitem__ gets called so many\n        #  times by Dask. Use _GLOBAL_NETCDF4_LOCK directly instead.\n        with _GLOBAL_NETCDF4_LIBRARY_THREADLOCK:\n            dataset = nc.Dataset(self.path)\n            ds_or_group = dataset\n            try:\n                for group_name in self.group_names_path:\n                    ds_or_group = ds_or_group.groups[group_name]\n                variable = ds_or_group.variables[self.variable_name]\n                # Get the NetCDF variable data and slice.\n                var = variable[keys]\n            finally:\n                dataset.close()\n        return np.asanyarray(var)\n\n    def __repr__(self):\n        fmt = (\n            \"<{self.__class__.__name__} shape={self.shape}\"\n            \" dtype={self.dtype!r} path={self.path!r}\"\n            \" variable_name={self.variable_name!r}>\"\n        )\n        return fmt.format(self=self)\n\n    def __getstate__(self):\n        return {attr: getattr(self, attr) for attr in self.__slots__}\n\n    def __setstate__(self, state):\n        for key, value in state.items():\n            setattr(self, key, value)", "\n\ndef to_nc4(\n    ncdata: NcData,\n    nc4_dataset_or_file: Union[nc.Dataset, Path, AnyStr],\n    var_kwargs: Dict[str, Any] = None,\n) -> None:\n    \"\"\"\n    Write an NcData to a provided (writeable) :class:`netCDF4.Dataset`, or filepath.\n\n    Parameters\n    ----------\n    ncdata : NcData\n        input data\n    nc4_dataset_or_file : :class:`netCDF4.Dataset` or :class:`pathlib.Path` or str\n        output filepath or :class:`netCDF4.Dataset` to write into\n    var_kwargs : dict or None\n        additional keys for variable creation.  If present, ``var_kwargs[<var_name>]``\n        contains additional keywords passed to :meth:`netCDF4.Dataset.createVariable`,\n        for specific variables, such as compression or chunking controls.\n        Controls for vars within groups are contained within a\n        ``var_kwargs['/<group-name>']`` entry, which is itself a dict (recursively).\n        Should **not** include any parameters already controlled by the ``to_nc4``\n        operation itself, that is : ``varname``, ``datatype``, ``dimensions`` or\n        ``fill_value``.\n\n    Returns\n    -------\n    None\n\n    Note\n    ----\n    If a filepath is provided, a file is written and closed afterwards.\n    If a dataset is provided, it must be writeable and remains open afterward.\n\n    \"\"\"\n    caller_owns_dataset = hasattr(nc4_dataset_or_file, \"variables\")\n    if caller_owns_dataset:\n        nc4ds = nc4_dataset_or_file\n    else:\n        nc4ds = nc.Dataset(nc4_dataset_or_file, \"w\")\n\n    try:\n        _to_nc4_group(ncdata, nc4ds, var_kwargs=var_kwargs)\n    finally:\n        if not caller_owns_dataset:\n            nc4ds.close()", "\n\ndef _from_nc4_group(\n    nc4ds: Union[nc.Dataset, nc.Group],\n    parent_ds: Optional[nc.Dataset] = None,\n    group_names_path: List[str] = None,\n) -> NcData:\n    \"\"\"\n    Inner routine for :func:`from_nc4`.\n\n    See docstring there.\n    Provided mainly for recursion into groups, also to keep the dataset open/close separate.\n    \"\"\"\n    if parent_ds is None:\n        parent_ds = nc4ds\n    if group_names_path is None:\n        group_names_path = []\n\n    ncdata = NcData(name=nc4ds.name)\n\n    for dimname, nc4dim in nc4ds.dimensions.items():\n        size = len(nc4dim)\n        unlimited = nc4dim.isunlimited()\n        ncdata.dimensions[dimname] = NcDimension(dimname, size, unlimited)\n\n    for varname, nc4var in nc4ds.variables.items():\n        var = NcVariable(\n            name=varname,\n            dimensions=nc4var.dimensions,\n            dtype=nc4var.dtype,\n            group=ncdata,\n        )\n        ncdata.variables[varname] = var\n\n        # Assign a data object : for now, always LAZY.\n        # code shamelessly stolen from iris.fileformats.netcdf\n\n        # Work out the shape of the variable.\n        # It may refer to dimensions in enclosing groups.\n        group = parent_ds\n        dims_map = group.dimensions.copy()\n        for group_name in group_names_path:\n            # Inner groups take priority, of course :\n            # their dimensions mask any of the same name in outer groups\n            group = group.groups[group_name]\n            dims_map.update(group.dimensions)\n\n        shape = tuple(dims_map[dimname].size for dimname in var.dimensions)\n\n        proxy = _NetCDFDataProxy(\n            shape=shape,\n            dtype=var.dtype,\n            filepath=nc4ds.filepath(),\n            variable_name=varname,\n            group_names_path=group_names_path,\n        )\n        var.data = da.from_array(\n            proxy, chunks=shape, asarray=True, meta=np.ndarray\n        )\n\n        for attrname in nc4var.ncattrs():\n            var.attributes[attrname] = NcAttribute(\n                attrname, nc4var.getncattr(attrname)\n            )\n\n    for attrname in nc4ds.ncattrs():\n        ncdata.attributes[attrname] = NcAttribute(\n            attrname, nc4ds.getncattr(attrname)\n        )\n\n    # And finally, groups -- by the magic of recursion ...\n    for group_name, group in nc4ds.groups.items():\n        ncdata.groups[group_name] = _from_nc4_group(\n            nc4ds=nc4ds.groups[group_name],\n            parent_ds=parent_ds,\n            group_names_path=group_names_path + [group_name],\n        )\n\n    return ncdata", "\n\ndef from_nc4(\n    nc4_dataset_or_file: Union[nc.Dataset, nc.Group, Path, AnyStr]\n) -> NcData:\n    \"\"\"\n    Read an NcData from a provided :class:`netCDF4.Dataset`, or filepath.\n\n    Parameters\n    ----------\n    nc4_dataset_or_file\n        source of load data.  Can be either a :class:`netCDF4.Dataset`,\n        a :class:`netCDF4.Group`, a :class:`pathlib.Path` or a string.\n\n    Returns\n    -------\n    ncdata : NcData\n    \"\"\"\n    caller_owns_dataset = hasattr(nc4_dataset_or_file, \"variables\")\n    if caller_owns_dataset:\n        nc4ds = nc4_dataset_or_file\n    else:\n        nc4ds = nc.Dataset(nc4_dataset_or_file)\n\n    try:\n        ncdata = _from_nc4_group(nc4ds)\n    finally:\n        if not caller_owns_dataset:\n            nc4ds.close()\n\n    return ncdata", ""]}
{"filename": "lib/ncdata/__init__.py", "chunked_list": ["\"\"\"\nAn abstract representation of Netcdf data with groups, variables + attributes.\n\nAs provided by the NetCDF \"Common Data Model\" :\nhttps://docs.unidata.ucar.edu/netcdf-java/5.3/userguide/common_data_model_overview.html\n\nIt is also provided with read/write conversion interfaces to Xarray, Iris and netCDF4,\nthus acting as an efficient exchange channel between any of those forms.\n\nN.B. this file excluded from isort, as we want a specific class order for the docs", "\nN.B. this file excluded from isort, as we want a specific class order for the docs\n\"\"\"\nfrom ._core import NcAttribute, NcData, NcDimension, NcVariable\n\n__all__ = [\"NcAttribute\", \"NcData\", \"NcDimension\", \"NcVariable\"]\n"]}
{"filename": "lib/ncdata/dataset_like.py", "chunked_list": ["r\"\"\"\nAn adaptor layer allowing an :class:`~ncdata.NcData` to masquerade as a :class:`netCDF4.Dataset` object.\n\nNote:\n    This is a low-level interface, exposed publicly for extended experimental uses.\n    If you only want to convert **Iris** data to+from :class:`~ncdata.NcData`,\n    please use the functions in :mod:`ncdata.iris` instead.\n\n----\n", "----\n\nThese classes contain :class:`~ncdata.NcData` and :class:`~ncdata.NcVariable`\\\\s, but\nemulate the access APIs of a :class:`netCDF4.Dataset` / :class:`netCDF4.Variable`.\n\nThis is provided primarily to support a re-use of the :mod:`iris.fileformats.netcdf`\nfile format load + save, to convert cubes to+from ncdata objects (and hence, especially,\nconvert Iris :class:`~iris.cube.Cube`\\\\s to+from an Xarray :class:`~xarray.Dataset`).\n\nNotes", "\nNotes\n-----\nCurrently only supports what is required for Iris load/save capability.\nIt *should* be possible to use these objects with other packages expecting a\n:class:`netCDF4.Dataset` object, however the API simulation is far from complete, so\nthis may need to be extended, in future, to support other such uses.\n\n\"\"\"\nfrom typing import Dict, List", "\"\"\"\nfrom typing import Dict, List\n\nimport numpy as np\n\nfrom . import NcAttribute, NcData, NcDimension, NcVariable\n\n\nclass _Nc4DatalikeWithNcattrs:\n    # A mixin, shared by Nc4DatasetLike and Nc4VariableLike, which adds netcdf-like\n    #  attribute operations 'ncattrs / setncattr / getncattr', *AND* extends the local\n    #  objects attribute to those things also\n    # N.B. \"self._ncdata\" is the underlying NcData object : either an NcData or\n    #  NcVariable object.\n\n    def ncattrs(self) -> List[str]:\n        return list(self._ncdata.attributes.keys())\n\n    def getncattr(self, attr: str):\n        attrs = self._ncdata.attributes\n        if attr in attrs:\n            result = attrs[attr].as_python_value()\n        else:\n            # Don't allow it to issue a KeyError, as this upsets 'getattr' usage.\n            # Raise an AttributeError instead.\n            raise AttributeError(attr)\n        return result\n\n    def setncattr(self, attr: str, value):\n        # TODO: are we sure we need this translation ??\n        if isinstance(value, bytes):\n            value = value.decode(\"utf-8\")\n        # N.B. using the NcAttribute class for storage also ensures/requires that all\n        #  attributes are cast as numpy arrays (so have shape, dtype etc).\n        self._ncdata.attributes[attr] = NcAttribute(attr, value)\n\n    def __getattr__(self, attr):\n        # Extend local object attribute access to the ncattrs of the stored data item\n        #  (Unpleasant, but I think the Iris load code requires it).\n        return self.getncattr(attr)\n\n    def __setattr__(self, attr, value):\n        if attr in self._local_instance_props:\n            # N.B. use _local_instance_props to define standard instance attributes, to avoid a\n            #  possible endless loop here.\n            super().__setattr__(attr, value)\n        else:\n            # # if not hasattr(self, '_allsetattrs'):\n            # #     self._allsetattrs = set()\n            # self._allsetattrs.add(attr)\n            self.setncattr(attr, value)", "class _Nc4DatalikeWithNcattrs:\n    # A mixin, shared by Nc4DatasetLike and Nc4VariableLike, which adds netcdf-like\n    #  attribute operations 'ncattrs / setncattr / getncattr', *AND* extends the local\n    #  objects attribute to those things also\n    # N.B. \"self._ncdata\" is the underlying NcData object : either an NcData or\n    #  NcVariable object.\n\n    def ncattrs(self) -> List[str]:\n        return list(self._ncdata.attributes.keys())\n\n    def getncattr(self, attr: str):\n        attrs = self._ncdata.attributes\n        if attr in attrs:\n            result = attrs[attr].as_python_value()\n        else:\n            # Don't allow it to issue a KeyError, as this upsets 'getattr' usage.\n            # Raise an AttributeError instead.\n            raise AttributeError(attr)\n        return result\n\n    def setncattr(self, attr: str, value):\n        # TODO: are we sure we need this translation ??\n        if isinstance(value, bytes):\n            value = value.decode(\"utf-8\")\n        # N.B. using the NcAttribute class for storage also ensures/requires that all\n        #  attributes are cast as numpy arrays (so have shape, dtype etc).\n        self._ncdata.attributes[attr] = NcAttribute(attr, value)\n\n    def __getattr__(self, attr):\n        # Extend local object attribute access to the ncattrs of the stored data item\n        #  (Unpleasant, but I think the Iris load code requires it).\n        return self.getncattr(attr)\n\n    def __setattr__(self, attr, value):\n        if attr in self._local_instance_props:\n            # N.B. use _local_instance_props to define standard instance attributes, to avoid a\n            #  possible endless loop here.\n            super().__setattr__(attr, value)\n        else:\n            # # if not hasattr(self, '_allsetattrs'):\n            # #     self._allsetattrs = set()\n            # self._allsetattrs.add(attr)\n            self.setncattr(attr, value)", "\n\nclass Nc4DatasetLike(_Nc4DatalikeWithNcattrs):\n    \"\"\"\n    An object which emulates a :class:`netCDF4.Dataset`.\n\n    It can be both read and written (modified) via its emulated\n    :class:`netCDF4.Dataset`-like API.\n\n    The core NcData content, 'self._ncdata', is a :class:`ncdata.NcData`.\n    This completely defines the parent object state.\n    If not provided on init, a new, empty dataset is created.\n\n    \"\"\"\n\n    _local_instance_props = (\"_ncdata\", \"variables\", \"dimensions\")\n\n    # Needed for Iris to recognise the dataset format.\n    file_format = \"NETCDF4\"\n\n    def __init__(self, ncdata: NcData = None):\n        if ncdata is None:\n            ncdata = NcData()  # an empty dataset\n        #: the contained dataset.  If not provided, a new, empty dataset is created.\n        self._ncdata = ncdata\n        # N.B. we need to create + store our OWN variables, as they are wrappers for\n        #  the underlying NcVariable objects, with different properties.\n        self.variables = {\n            name: Nc4VariableLike._from_ncvariable(ncvar)\n            for name, ncvar in self._ncdata.variables.items()\n        }\n        self.dimensions = {\n            name: Nc4DimensionLike(dim)\n            for name, dim in self._ncdata.dimensions.items()\n        }\n\n    @property\n    def groups(self):  # noqa: D102\n        return None  # not supported\n\n    def createDimension(self, dimname: str, size: int):  # noqa: D102\n        if dimname in self.dimensions:\n            msg = f'creating duplicate dimension \"{dimname}\".'\n            raise ValueError(msg)\n\n        # Create a new actual NcDimension in the contained dataset.\n        dim = NcDimension(dimname, size)\n        self._ncdata.dimensions[dimname] = dim\n        # Create a wrapper for it, install that into self, and return it.\n        nc4dim = Nc4DimensionLike(dim)\n        self.dimensions[dimname] = nc4dim\n        return nc4dim\n\n    def createVariable(\n        self,\n        varname: str,\n        datatype: np.dtype,\n        dimensions: List[str] = (),\n        **encoding: Dict[str, str],\n    ):  # noqa: D102\n        if varname in self.variables:\n            msg = f'creating duplicate variable \"{varname}\".'\n            raise ValueError(msg)\n        # Add a variable into the underlying NcData object.\n\n        # N.B. to correctly mirror netCDF4, a variable must be created with all-masked\n        # content.  For this we need to decode the dims + work out the shape.\n        # NOTE: simplistic version here, as we don't support groups.\n        shape = tuple(\n            self._ncdata.dimensions[dim_name].size for dim_name in dimensions\n        )\n        # Note: does *not* allocate a full array in memory ...until you modify it.\n        initial_allmasked_data = np.ma.masked_array(\n            np.zeros(shape, dtype=datatype), mask=True\n        )\n\n        ncvar = NcVariable(\n            name=varname,\n            dimensions=dimensions,\n            data=initial_allmasked_data,\n            group=self._ncdata,\n        )\n        # Note: no valid data is initially assigned, since that is how the netCDF4 API\n        # does it.\n        self._ncdata.variables[varname] = ncvar\n        # Create a netCDF4-like \"wrapper\" variable + install that here.\n        nc4var = Nc4VariableLike._from_ncvariable(ncvar, dtype=datatype)\n        self.variables[varname] = nc4var\n        return nc4var\n\n    def sync(self):  # noqa: D102\n        pass\n\n    def close(self):  # noqa: D102\n        self.sync()\n\n    @staticmethod\n    def filepath() -> str:  # noqa: D102\n        # Note: for now, let's just not care about this.\n        # we *might* need this to be an optional defined item on an NcData ??\n        # .. or, we might need to store an xarray \"encoding\" somewhere ?\n        # TODO: more thought here ?\n        # return self.ncdata.encoding.get(\"source\", \"\")\n        return \"<Nc4DatasetLike>\"", "\n\nclass Nc4VariableLike(_Nc4DatalikeWithNcattrs):\n    \"\"\"\n    An object which contains a :class:`ncdata.NcVariable` and emulates a :class:`netCDF4.Variable`.\n\n    The core NcData content, 'self._ncdata', is a :class:`NcVariable`.\n    This completely defines the parent object state.\n\n    \"\"\"\n\n    _local_instance_props = (\"_ncdata\", \"name\", \"datatype\", \"_data_array\")\n\n    def __init__(self, ncvar: NcVariable, datatype: np.dtype):  # noqa: D107\n        self._ncdata = ncvar\n        self.name = ncvar.name\n        # Note: datatype must be known at creation, which may be before an actual data\n        #  array is assigned on the ncvar.\n        self.datatype = np.dtype(datatype)\n        if ncvar.data is None:\n            # temporary empty data (to support never-written scalar values)\n            # NOTE: significantly, does *not* allocate an actual full array in memory\n            array = np.zeros(self.shape, self.datatype)\n            ncvar.data = array\n        self._data_array = ncvar.data\n\n    @classmethod\n    def _from_ncvariable(cls, ncvar: NcVariable, dtype: np.dtype = None):\n        if dtype is None:\n            dtype = ncvar.dtype\n        self = cls(\n            ncvar=ncvar,\n            datatype=dtype,\n        )\n        return self\n\n    # Label this as an 'emulated' netCDF4.Variable, containing an actual (possibly\n    #  lazy) array, which can be directly read/written.\n    @property\n    def _data_array(self):\n        return self._ncdata.data\n\n    @_data_array.setter\n    def _data_array(self, data):\n        self._ncdata.data = data\n        self.datatype = data.dtype\n\n    def group(self):  # noqa: D102\n        return self._ncdata.group\n\n    @property\n    def dimensions(self) -> List[str]:  # noqa: D102\n        return self._ncdata.dimensions\n\n    #\n    # \"Normal\" data access is via indexing.\n    # N.B. we do still need to support this, e.g. for DimCoords ?\n    #\n    def __getitem__(self, keys):  # noqa: D105\n        if keys != slice(None):\n            raise IndexError(keys)\n        if self.ndim == 0:\n            return self._ncdata.data\n        array = self._ncdata.data[keys]\n        if hasattr(array, \"compute\"):\n            # When accessed as a data variable, we must realise lazy data.\n            array = array.compute()\n        return array\n\n    # The __setitem__ is not required for normal saving.\n    # The saver will assign ._data_array instead\n    # TODO: might need to support this for future non-Iris usage ?\n    #\n    # def __setitem__(self, keys, data):\n    #     if keys != slice(None):\n    #         raise IndexError(keys)\n    #     if not hasattr(data, \"dtype\"):\n    #         raise ValueError(f\"nonarray assigned as data : {data}\")\n    #     if not data.shape == self.shape:\n    #         msg = (\n    #             f\"assigned data has wrong shape : \"\n    #             f\"{data.shape} instead of {self.shape}\"\n    #         )\n    #         raise ValueError(msg)\n    #     self._ncdata.data = data\n    #     self.datatype = data.dtype\n\n    @property\n    def dtype(self):  # noqa: D102\n        return self.datatype\n\n    @property\n    def dims(self):  # noqa: D102\n        return self.dimensions\n\n    @property\n    def ndim(self):  # noqa: D102\n        return len(self.dimensions)\n\n    @property\n    def shape(self):  # noqa: D102\n        dims = self.group().dimensions\n        return tuple(dims[n].size for n in self.dimensions)\n\n    @property\n    def size(self):  # noqa: D102\n        return np.prod(self.shape)\n\n    def chunking(self):  # noqa: D102\n        \"\"\"\n        Return chunk sizes.\n\n        Actual datasets return a list of sizes by dimension, or 'contiguous'.\n\n        For now, simply returns ``None``.  Could be replaced when required.\n        \"\"\"\n        return None", "\n\nclass Nc4DimensionLike:\n    \"\"\"\n    An object which emulates a :class:`netCDF4.Dimension` object.\n\n    The core NcData content, 'self._ncdata', is a :class:`ncdata.NcDimension`.\n    This completely defines the parent object state.\n\n    \"\"\"\n\n    def __init__(self, ncdim: NcDimension):  # noqa: D107\n        self._ncdata = ncdim\n\n    @property\n    def name(self):  # noqa: D102\n        return self._ncdata.name\n\n    @property\n    def size(self):  # noqa: D102\n        return self._ncdata.size\n\n    def isunlimited(self):  # noqa: D102\n        return self._ncdata.unlimited\n\n    def group(self):  # noqa: D102\n        # Not properly supported ?\n        return None", ""]}
{"filename": "lib/ncdata/iris.py", "chunked_list": ["r\"\"\"\nInterface routines for converting data between ncdata and Iris.\n\nConvert :class:`~ncdata.NcData` to and from Iris :class:`~iris.cube.Cube`\\\\s.\n\nThis uses the :class:`ncdata.dataset_like` interface ability to mimic netCDF4.Dataset\nobjects, which are used like files to load and save Iris data.\nThis means that all we need to know of Iris is its netcdf load+save interfaces.\n\n\"\"\"", "\n\"\"\"\nfrom typing import AnyStr, Dict, Iterable, Union\n\nimport iris\nimport iris.fileformats.netcdf as ifn\nfrom iris.cube import Cube, CubeList\n\nfrom . import NcData\nfrom .dataset_like import Nc4DatasetLike", "from . import NcData\nfrom .dataset_like import Nc4DatasetLike\n\n#\n# The primary conversion interfaces\n#\n\n\ndef to_iris(ncdata: NcData, **kwargs) -> CubeList:\n    \"\"\"\n    Read Iris cubes from an :class:`~ncdata.NcData`.\n\n    Behaves like an Iris 'load' operation.\n\n    Parameters\n    ----------\n    ncdata : NcData\n        object to be loaded, treated as equivalent to a netCDF4 dataset.\n\n    kwargs : dict\n        extra keywords, passed to :func:`iris.fileformats.netcdf.load_cubes`\n\n    Returns\n    -------\n    cubes : CubeList\n        loaded results\n    \"\"\"\n    dslike = Nc4DatasetLike(ncdata)\n    cubes = CubeList(ifn.load_cubes(dslike, **kwargs))\n    return cubes", "def to_iris(ncdata: NcData, **kwargs) -> CubeList:\n    \"\"\"\n    Read Iris cubes from an :class:`~ncdata.NcData`.\n\n    Behaves like an Iris 'load' operation.\n\n    Parameters\n    ----------\n    ncdata : NcData\n        object to be loaded, treated as equivalent to a netCDF4 dataset.\n\n    kwargs : dict\n        extra keywords, passed to :func:`iris.fileformats.netcdf.load_cubes`\n\n    Returns\n    -------\n    cubes : CubeList\n        loaded results\n    \"\"\"\n    dslike = Nc4DatasetLike(ncdata)\n    cubes = CubeList(ifn.load_cubes(dslike, **kwargs))\n    return cubes", "\n\ndef from_iris(\n    cubes: Union[Cube, Iterable[Cube]], **kwargs: Dict[AnyStr, AnyStr]\n) -> NcData:\n    \"\"\"\n    Create an :class:`~ncdata.NcData` from Iris cubes.\n\n    Behaves like an Iris 'save' operation.\n\n    Parameters\n    ----------\n    cubes : :class:`iris.cube.Cube`, or iterable of Cubes\n        cube or cubes to \"save\" to an NcData object.\n    kwargs : dict\n        additional keys passed to :func:`iris.save` operation.\n\n    Returns\n    -------\n    ncdata : NcData\n        output data created from saving ``cubes``\n\n    \"\"\"\n    nc4like = Nc4DatasetLike()\n    delayed = iris.save(\n        cubes,\n        nc4like,\n        compute=False,  # *required* for save-to-dataset.\n        saver=ifn.save,\n        **kwargs,\n    )\n    delayed.compute()  # probably a no-op, but for sake of completeness.\n    return nc4like._ncdata", ""]}
{"filename": "docs/conf.py", "chunked_list": ["# noqa\n# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,", "\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n\n", "\n\n# -- Project information -----------------------------------------------------\n\nproject = \"ncdata\"\ncopyright = \"2023, pp-mo\"\nauthor = \"pp-mo\"\n\n# The full version, including alpha/beta/rc tags\nrelease = \"0.01\"", "# The full version, including alpha/beta/rc tags\nrelease = \"0.01\"\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [", "# ones.\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.napoleon\",\n]\n\nintersphinx_mapping = {\n    \"numpy\": (\"https://numpy.org/doc/stable/\", None),\n    \"python\": (\"https://docs.python.org/3/\", None),", "    \"numpy\": (\"https://numpy.org/doc/stable/\", None),\n    \"python\": (\"https://docs.python.org/3/\", None),\n    \"dask\": (\"https://docs.dask.org/en/stable/\", None),\n    \"xarray\": (\"https://docs.xarray.dev/en/stable/\", None),\n    \"iris\": (\"https://scitools-iris.readthedocs.io/en/latest/\", None),\n    # Can't make this work ??\n    # \"netCDF4\": (\"https://github.com/Unidata/netcdf4-python\", None),\n}\n\n", "\n\nfrom pathlib import Path\n\ndocsdir_pth = Path(__name__).parent.absolute()\nprint(\"docsdir import path:\", docsdir_pth)\nncdata_pth = (docsdir_pth.parent / \"lib\").absolute()\nprint(\"ncdata import path:\", ncdata_pth)\n\nimport sys", "\nimport sys\n\nsys.path.append(str(ncdata_pth))\nprint(\"PATH:\")\nprint(\"\\n\".join(p for p in sys.path))\n\n# Autodoc config..\nautopackage_name = [\n    \"ncdata\",", "autopackage_name = [\n    \"ncdata\",\n    \"ncdata.iris_xarray\",\n    \"ncdata.netcdf4\",\n    \"ncdata.xarray\",\n    \"ncdata.dataset_like\",\n]\n# api generation configuration\nautoclass_content = \"both\"\nautodoc_member_order = \"bysource\"", "autoclass_content = \"both\"\nautodoc_member_order = \"bysource\"\nautodoc_typehints = \"description\"\nautodoc_class_signature = \"separated\"\nautodoc_inherited_members = True\n\nautodoc_default_options = {\n    \"member-order\": \"bysource\",\n    \"inherited-members\": True,\n    \"class-signature\": \"separated\",", "    \"inherited-members\": True,\n    \"class-signature\": \"separated\",\n    \"autodoc_typehints\": \"description\",\n    \"autoclass_content\": \"both\",\n}\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# List of patterns, relative to source directory, that match files and", "\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"]\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for", "\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \"pydata_sphinx_theme\"\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]", "# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n"]}
