{"filename": "__init__.py", "chunked_list": ["from .AITemplate.AITemplate import AITemplateLoader, AITemplateControlNetLoader, AITemplateVAEDecode, AITemplateVAEEncode, VAEEncodeForInpaint, AITemplateEmptyLatentImage, AITemplateLatentUpscale\n\nNODE_CLASS_MAPPINGS = {\n    \"AITemplateLoader\": AITemplateLoader,\n    \"AITemplateControlNetLoader\": AITemplateControlNetLoader,\n    \"AITemplateVAEDecode\": AITemplateVAEDecode,\n    \"AITemplateVAEEncode\": AITemplateVAEEncode,\n    \"AITemplateVAEEncodeForInpaint\": VAEEncodeForInpaint,\n    \"AITemplateEmptyLatentImage\": AITemplateEmptyLatentImage,\n    \"AITemplateLatentUpscale\": AITemplateLatentUpscale,", "    \"AITemplateEmptyLatentImage\": AITemplateEmptyLatentImage,\n    \"AITemplateLatentUpscale\": AITemplateLatentUpscale,\n}\n\n# A dictionary that contains the friendly/humanly readable titles for the nodes\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"AITemplateLoader\": \"Load AITemplate\",\n    \"AITemplateControlNetLoader\": \"Load AITemplate (ControlNet)\",\n    \"AITemplateVAELoader\": \"Load AITemplate (VAE)\",\n    \"AITemplateVAEDecode\": \"VAE Decode (AITemplate)\",", "    \"AITemplateVAELoader\": \"Load AITemplate (VAE)\",\n    \"AITemplateVAEDecode\": \"VAE Decode (AITemplate)\",\n    \"AITemplateVAEEncode\": \"VAE Encode (AITemplate)\",\n    \"AITemplateVAEEncodeForInpaint\": \"VAE Encode (AITemplate, Inpaint)\",\n    \"AITemplateEmptyLatentImage\": \"Empty Latent Image (AITemplate)\",\n    \"AITemplateLatentUpscale\": \"Upscale Latent Image (AITemplate)\",\n}\n"]}
{"filename": "AITemplate/unet.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport logging\n\nimport click\nimport torch", "import click\nimport torch\nfrom aitemplate.testing import detect_target\ntry:\n    from diffusers import UNet2DConditionModel\nexcept ImportError:\n    raise ImportError(\n        \"Please install diffusers with `pip install diffusers` to use this script.\"\n    )\nfrom ait.compile.unet import compile_unet", "from ait.compile.unet import compile_unet\n\n@click.command()\n@click.option(\n    \"--hf-hub-or-path\",\n    default=\"runwayml/stable-diffusion-v1-5\",\n    help=\"the local diffusers pipeline directory or hf hub path e.g. runwayml/stable-diffusion-v1-5\",\n)\n@click.option(\n    \"--width\",", "@click.option(\n    \"--width\",\n    default=(64, 1024),\n    type=(int, int),\n    nargs=2,\n    help=\"Minimum and maximum width\",\n)\n@click.option(\n    \"--height\",\n    default=(64, 1024),", "    \"--height\",\n    default=(64, 1024),\n    type=(int, int),\n    nargs=2,\n    help=\"Minimum and maximum height\",\n)\n@click.option(\n    \"--batch-size\",\n    default=(1, 1),\n    type=(int, int),", "    default=(1, 1),\n    type=(int, int),\n    nargs=2,\n    help=\"Minimum and maximum batch size\",\n)\n@click.option(\"--clip-chunks\", default=10, help=\"Maximum number of clip chunks\")\n@click.option(\n    \"--include-constants\",\n    default=None,\n    help=\"include constants (model weights) with compiled model\",", "    default=None,\n    help=\"include constants (model weights) with compiled model\",\n)\n@click.option(\n    \"--down-factor\",\n    default=8,\n    type=int,\n    help=\"Down factor, this is 4 for x4-upscaler\",\n)\n@click.option(\"--fp32\", default=False, help=\"use fp32\")", ")\n@click.option(\"--fp32\", default=False, help=\"use fp32\")\n@click.option(\"--use-fp16-acc\", default=True, help=\"use fp16 accumulation\")\n@click.option(\"--convert-conv-to-gemm\", default=True, help=\"convert 1x1 conv to gemm\")\n@click.option(\"--controlnet\", default=False, help=\"UNet for controlnet\")\n@click.option(\"--model-name\", default=\"UNet2DConditionModel\", help=\"module name\")\n@click.option(\"--work-dir\", default=\"./tmp\", help=\"work directory\")\n@click.option(\"--out-dir\", default=\"./out\", help=\"out directory\")\ndef compile_diffusers(\n    hf_hub_or_path,\n    width,\n    height,\n    batch_size,\n    clip_chunks,\n    include_constants,\n    down_factor=8,\n    fp32=False,\n    use_fp16_acc=True,\n    convert_conv_to_gemm=True,\n    controlnet=False,\n    model_name=\"UNet2DConditionModel\",\n    work_dir=\"./tmp\",\n    out_dir=\"./out\",\n):\n    logging.getLogger().setLevel(logging.INFO)\n    torch.manual_seed(4896)\n\n    if detect_target().name() == \"rocm\":\n        convert_conv_to_gemm = False\n\n    pipe = UNet2DConditionModel.from_pretrained(\n        hf_hub_or_path,\n        subfolder=\"unet\" if not hf_hub_or_path.endswith(\"unet\") else None,\n        variant=\"fp16\",\n        use_safetensors=True,\n        torch_dtype=torch.float16,\n    ).to(\"cuda\")\n\n    compile_unet(\n       pipe,\n       batch_size=batch_size,\n       width=width,\n       height=height,\n       clip_chunks=clip_chunks,\n       use_fp16_acc=use_fp16_acc,\n       convert_conv_to_gemm=convert_conv_to_gemm,\n       hidden_dim=pipe.config.cross_attention_dim,\n       attention_head_dim=pipe.config.attention_head_dim,\n       use_linear_projection=pipe.config.get(\"use_linear_projection\", False),\n       block_out_channels=pipe.config.block_out_channels,\n       down_block_types=pipe.config.down_block_types,\n       up_block_types=pipe.config.up_block_types,\n       in_channels=pipe.config.in_channels,\n       out_channels=pipe.config.out_channels,\n       class_embed_type=pipe.config.class_embed_type,\n       num_class_embeds=pipe.config.num_class_embeds,\n       only_cross_attention=pipe.config.only_cross_attention,\n       sample_size=pipe.config.sample_size,\n       dim=pipe.config.block_out_channels[0],\n       time_embedding_dim = None,\n       conv_in_kernel=pipe.config.conv_in_kernel,\n       projection_class_embeddings_input_dim=pipe.config.projection_class_embeddings_input_dim,\n       addition_embed_type = pipe.config.addition_embed_type,\n       addition_time_embed_dim = pipe.config.addition_time_embed_dim,\n       transformer_layers_per_block = pipe.config.transformer_layers_per_block,\n       constants=True if include_constants else False,\n       controlnet=True if controlnet else False,\n       model_name=model_name,\n       work_dir=work_dir,\n       down_factor=down_factor,\n       dtype=\"float32\" if fp32 else \"float16\",\n       out_dir=out_dir,\n    )", "def compile_diffusers(\n    hf_hub_or_path,\n    width,\n    height,\n    batch_size,\n    clip_chunks,\n    include_constants,\n    down_factor=8,\n    fp32=False,\n    use_fp16_acc=True,\n    convert_conv_to_gemm=True,\n    controlnet=False,\n    model_name=\"UNet2DConditionModel\",\n    work_dir=\"./tmp\",\n    out_dir=\"./out\",\n):\n    logging.getLogger().setLevel(logging.INFO)\n    torch.manual_seed(4896)\n\n    if detect_target().name() == \"rocm\":\n        convert_conv_to_gemm = False\n\n    pipe = UNet2DConditionModel.from_pretrained(\n        hf_hub_or_path,\n        subfolder=\"unet\" if not hf_hub_or_path.endswith(\"unet\") else None,\n        variant=\"fp16\",\n        use_safetensors=True,\n        torch_dtype=torch.float16,\n    ).to(\"cuda\")\n\n    compile_unet(\n       pipe,\n       batch_size=batch_size,\n       width=width,\n       height=height,\n       clip_chunks=clip_chunks,\n       use_fp16_acc=use_fp16_acc,\n       convert_conv_to_gemm=convert_conv_to_gemm,\n       hidden_dim=pipe.config.cross_attention_dim,\n       attention_head_dim=pipe.config.attention_head_dim,\n       use_linear_projection=pipe.config.get(\"use_linear_projection\", False),\n       block_out_channels=pipe.config.block_out_channels,\n       down_block_types=pipe.config.down_block_types,\n       up_block_types=pipe.config.up_block_types,\n       in_channels=pipe.config.in_channels,\n       out_channels=pipe.config.out_channels,\n       class_embed_type=pipe.config.class_embed_type,\n       num_class_embeds=pipe.config.num_class_embeds,\n       only_cross_attention=pipe.config.only_cross_attention,\n       sample_size=pipe.config.sample_size,\n       dim=pipe.config.block_out_channels[0],\n       time_embedding_dim = None,\n       conv_in_kernel=pipe.config.conv_in_kernel,\n       projection_class_embeddings_input_dim=pipe.config.projection_class_embeddings_input_dim,\n       addition_embed_type = pipe.config.addition_embed_type,\n       addition_time_embed_dim = pipe.config.addition_time_embed_dim,\n       transformer_layers_per_block = pipe.config.transformer_layers_per_block,\n       constants=True if include_constants else False,\n       controlnet=True if controlnet else False,\n       model_name=model_name,\n       work_dir=work_dir,\n       down_factor=down_factor,\n       dtype=\"float32\" if fp32 else \"float16\",\n       out_dir=out_dir,\n    )", "\nif __name__ == \"__main__\":\n    compile_diffusers()\n"]}
{"filename": "AITemplate/download_pipeline.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport click\nimport torch\ntry:\n    from diffusers import StableDiffusionPipeline\nexcept ImportError:\n    raise ImportError(\n        \"Please install diffusers with `pip install diffusers` to use this script.\"\n    )", "try:\n    from diffusers import StableDiffusionPipeline\nexcept ImportError:\n    raise ImportError(\n        \"Please install diffusers with `pip install diffusers` to use this script.\"\n    )\n\n\n@click.command()\n@click.option(\"--token\", default=\"\", help=\"access token\")", "@click.command()\n@click.option(\"--token\", default=\"\", help=\"access token\")\n@click.option(\n    \"--hf-hub\",\n    default=\"runwayml/stable-diffusion-v1-5\",\n    help=\"hf hub\",\n)\n@click.option(\n    \"--save_directory\",\n    default=\"./tmp/diffusers-pipeline/runwayml/stable-diffusion-v1-5\",", "    \"--save_directory\",\n    default=\"./tmp/diffusers-pipeline/runwayml/stable-diffusion-v1-5\",\n    help=\"pipeline files local directory\",\n)\n@click.option(\"--revision\", default=\"fp16\", help=\"access token\")\ndef download_pipeline_files(token, hf_hub, save_directory, revision=\"fp16\") -> None:\n    StableDiffusionPipeline.from_pretrained(\n        hf_hub,\n        revision=revision if revision != \"\" else None,\n        torch_dtype=torch.float16,\n        # use provided token or the one generated with `huggingface-cli login``\n        use_auth_token=token if token != \"\" else True,\n    ).save_pretrained(save_directory)", "\n\nif __name__ == \"__main__\":\n    download_pipeline_files()\n"]}
{"filename": "AITemplate/controlnet.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport logging\n\nimport click\nimport torch", "import click\nimport torch\nfrom aitemplate.testing import detect_target\n\ntry:\n    from diffusers import ControlNetModel\nexcept ImportError:\n    raise ImportError(\n        \"Please install diffusers with `pip install diffusers` to use this script.\"\n    )", "from ait.compile.controlnet import compile_controlnet\n\n\n@click.command()\n@click.option(\n    \"--hf-hub-or-path\",\n    default=\"lllyasviel/sd-controlnet-canny\",\n    help=\"the local diffusers pipeline directory or hf hub path e.g. lllyasviel/sd-controlnet-canny\",\n)\n@click.option(", ")\n@click.option(\n    \"--width\",\n    default=(64, 2048),\n    type=(int, int),\n    nargs=2,\n    help=\"Minimum and maximum width\",\n)\n@click.option(\n    \"--height\",", "@click.option(\n    \"--height\",\n    default=(64, 2048),\n    type=(int, int),\n    nargs=2,\n    help=\"Minimum and maximum height\",\n)\n@click.option(\n    \"--batch-size\",\n    default=(1, 1),", "    \"--batch-size\",\n    default=(1, 1),\n    type=(int, int),\n    nargs=2,\n    help=\"Minimum and maximum batch size\",\n)\n@click.option(\"--clip-chunks\", default=10, help=\"Maximum number of clip chunks\")\n@click.option(\n    \"--include-constants\",\n    default=None,", "    \"--include-constants\",\n    default=None,\n    help=\"include constants (model weights) with compiled model\",\n)\n@click.option(\"--use-fp16-acc\", default=True, help=\"use fp16 accumulation\")\n@click.option(\"--convert-conv-to-gemm\", default=True, help=\"convert 1x1 conv to gemm\")\n@click.option(\"--model-name\", default=\"ControlNetModel\", help=\"module name\")\n@click.option(\"--work-dir\", default=\"./tmp\", help=\"work directory\")\n@click.option(\"--out-dir\", default=\"./out\", help=\"out directory\")\ndef compile_diffusers(\n    hf_hub_or_path,\n    width,\n    height,\n    batch_size,\n    clip_chunks,\n    include_constants,\n    use_fp16_acc=True,\n    convert_conv_to_gemm=True,\n    model_name=\"ControlNetModel\",\n    work_dir=\"./tmp\",\n    out_dir=\"./out\",\n):\n    logging.getLogger().setLevel(logging.INFO)\n    torch.manual_seed(4896)\n\n    if detect_target().name() == \"rocm\":\n        convert_conv_to_gemm = False\n\n    pipe = ControlNetModel.from_pretrained(\n        hf_hub_or_path,\n        use_safetensors=True,\n        # variant=\"fp16\",\n        torch_dtype=torch.float16,\n    ).to(\"cuda\")\n\n    compile_controlnet(\n        pipe,\n        batch_size=batch_size,\n        width=width,\n        height=height,\n        clip_chunks=clip_chunks,\n        convert_conv_to_gemm=convert_conv_to_gemm,\n        use_fp16_acc=use_fp16_acc,\n        constants=include_constants,\n        model_name=model_name,\n        work_dir=work_dir,\n        hidden_dim=pipe.config.cross_attention_dim,\n        use_linear_projection=pipe.config.get(\"use_linear_projection\", False),\n        block_out_channels=pipe.config.block_out_channels,\n        down_block_types=pipe.config.down_block_types,\n        in_channels=pipe.config.in_channels,\n        class_embed_type=pipe.config.class_embed_type,\n        num_class_embeds=pipe.config.num_class_embeds,\n        dim=pipe.config.block_out_channels[0],\n        time_embedding_dim=None,\n        projection_class_embeddings_input_dim=pipe.config.projection_class_embeddings_input_dim\n        if hasattr(pipe.config, \"projection_class_embeddings_input_dim\")\n        else None,\n        addition_embed_type=pipe.config.addition_embed_type\n        if hasattr(pipe.config, \"addition_embed_type\")\n        else None,\n        addition_time_embed_dim=pipe.config.addition_time_embed_dim\n        if hasattr(pipe.config, \"addition_time_embed_dim\")\n        else None,\n        transformer_layers_per_block=pipe.config.transformer_layers_per_block\n        if hasattr(pipe.config, \"transformer_layers_per_block\")\n        else 1,\n        out_dir=out_dir,\n    )", "@click.option(\"--out-dir\", default=\"./out\", help=\"out directory\")\ndef compile_diffusers(\n    hf_hub_or_path,\n    width,\n    height,\n    batch_size,\n    clip_chunks,\n    include_constants,\n    use_fp16_acc=True,\n    convert_conv_to_gemm=True,\n    model_name=\"ControlNetModel\",\n    work_dir=\"./tmp\",\n    out_dir=\"./out\",\n):\n    logging.getLogger().setLevel(logging.INFO)\n    torch.manual_seed(4896)\n\n    if detect_target().name() == \"rocm\":\n        convert_conv_to_gemm = False\n\n    pipe = ControlNetModel.from_pretrained(\n        hf_hub_or_path,\n        use_safetensors=True,\n        # variant=\"fp16\",\n        torch_dtype=torch.float16,\n    ).to(\"cuda\")\n\n    compile_controlnet(\n        pipe,\n        batch_size=batch_size,\n        width=width,\n        height=height,\n        clip_chunks=clip_chunks,\n        convert_conv_to_gemm=convert_conv_to_gemm,\n        use_fp16_acc=use_fp16_acc,\n        constants=include_constants,\n        model_name=model_name,\n        work_dir=work_dir,\n        hidden_dim=pipe.config.cross_attention_dim,\n        use_linear_projection=pipe.config.get(\"use_linear_projection\", False),\n        block_out_channels=pipe.config.block_out_channels,\n        down_block_types=pipe.config.down_block_types,\n        in_channels=pipe.config.in_channels,\n        class_embed_type=pipe.config.class_embed_type,\n        num_class_embeds=pipe.config.num_class_embeds,\n        dim=pipe.config.block_out_channels[0],\n        time_embedding_dim=None,\n        projection_class_embeddings_input_dim=pipe.config.projection_class_embeddings_input_dim\n        if hasattr(pipe.config, \"projection_class_embeddings_input_dim\")\n        else None,\n        addition_embed_type=pipe.config.addition_embed_type\n        if hasattr(pipe.config, \"addition_embed_type\")\n        else None,\n        addition_time_embed_dim=pipe.config.addition_time_embed_dim\n        if hasattr(pipe.config, \"addition_time_embed_dim\")\n        else None,\n        transformer_layers_per_block=pipe.config.transformer_layers_per_block\n        if hasattr(pipe.config, \"transformer_layers_per_block\")\n        else 1,\n        out_dir=out_dir,\n    )", "\n\nif __name__ == \"__main__\":\n    compile_diffusers()\n"]}
{"filename": "AITemplate/__init__.py", "chunked_list": [""]}
{"filename": "AITemplate/vae.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport logging\n\nimport click\nimport torch", "import click\nimport torch\nfrom aitemplate.testing import detect_target\ntry:\n    from diffusers import AutoencoderKL\nexcept ImportError:\n    raise ImportError(\n        \"Please install diffusers with `pip install diffusers` to use this script.\"\n    )\nfrom ait.compile.vae import compile_vae", "from ait.compile.vae import compile_vae\n\n@click.command()\n@click.option(\n    \"--hf-hub-or-path\",\n    default=\"runwayml/stable-diffusion-v1-5\",\n    help=\"the local diffusers pipeline directory or hf hub path e.g. runwayml/stable-diffusion-v1-5\",\n)\n@click.option(\n    \"--width\",", "@click.option(\n    \"--width\",\n    default=(64, 1024),\n    type=(int, int),\n    nargs=2,\n    help=\"Minimum and maximum width\",\n)\n@click.option(\n    \"--height\",\n    default=(64, 1024),", "    \"--height\",\n    default=(64, 1024),\n    type=(int, int),\n    nargs=2,\n    help=\"Minimum and maximum height\",\n)\n@click.option(\n    \"--batch-size\",\n    default=(1, 1),\n    type=(int, int),", "    default=(1, 1),\n    type=(int, int),\n    nargs=2,\n    help=\"Minimum and maximum batch size\",\n)\n@click.option(\"--fp32\", default=False, help=\"use fp32\")\n@click.option(\n    \"--include-constants\",\n    default=None,\n    help=\"include constants (model weights) with compiled model\",", "    default=None,\n    help=\"include constants (model weights) with compiled model\",\n)\n@click.option(\n    \"--input-size\",\n    default=64,\n    type=int,\n    help=\"Input sample size, same as sample size of the unet model. this is 128 for x4-upscaler\",\n)\n@click.option(", ")\n@click.option(\n    \"--down-factor\",\n    default=8,\n    type=int,\n    help=\"Down factor, this is 4 for x4-upscaler\",\n)\n@click.option(\n    \"--encoder\",\n    default=False,", "    \"--encoder\",\n    default=False,\n    type=bool,\n    help=\"If True, compile encoder, otherwise, compile decoder\",\n)\n@click.option(\"--use-fp16-acc\", default=True, help=\"use fp16 accumulation\")\n@click.option(\"--convert-conv-to-gemm\", default=True, help=\"convert 1x1 conv to gemm\")\n@click.option(\"--model-name\", default=\"AutoencoderKL\", help=\"module name\")\n@click.option(\"--work-dir\", default=\"./tmp\", help=\"work directory\")\n@click.option(\"--out-dir\", default=\"./out\", help=\"out directory\")\ndef compile_diffusers(\n    hf_hub_or_path,\n    width,\n    height,\n    batch_size,\n    fp32,\n    include_constants,\n    input_size=64,\n    down_factor=8,\n    encoder=False,\n    use_fp16_acc=True,\n    convert_conv_to_gemm=True,\n    model_name=\"AutoencoderKL\",\n    work_dir=\"./tmp\",\n    out_dir=\"./out\",\n):\n    logging.getLogger().setLevel(logging.INFO)\n    torch.manual_seed(4896)\n\n    if detect_target().name() == \"rocm\":\n        convert_conv_to_gemm = False\n\n    pipe = AutoencoderKL.from_pretrained(\n        hf_hub_or_path,\n        subfolder=\"vae\" if not hf_hub_or_path.endswith(\"vae\") else None,\n        torch_dtype=torch.float32 if fp32 else torch.float16\n    ).to(\"cuda\")\n\n    compile_vae(\n        pipe,\n        batch_size=batch_size,\n        width=width,\n        height=height,\n        use_fp16_acc=use_fp16_acc,\n        convert_conv_to_gemm=convert_conv_to_gemm,\n        constants=True if include_constants else False,\n        block_out_channels=pipe.config.block_out_channels,\n        layers_per_block=pipe.config.layers_per_block,\n        act_fn=pipe.config.act_fn,\n        latent_channels=pipe.config.latent_channels,\n        in_channels=pipe.config.in_channels,\n        out_channels=pipe.config.out_channels,\n        down_block_types=pipe.config.down_block_types,\n        up_block_types=pipe.config.up_block_types,\n        sample_size=pipe.config.sample_size,\n        input_size=(input_size, input_size),\n        down_factor=down_factor,\n        model_name=model_name,\n        dtype=\"float32\" if fp32 else \"float16\",\n        work_dir=work_dir,\n        vae_encode=encoder,\n        out_dir=out_dir,\n    )", "@click.option(\"--work-dir\", default=\"./tmp\", help=\"work directory\")\n@click.option(\"--out-dir\", default=\"./out\", help=\"out directory\")\ndef compile_diffusers(\n    hf_hub_or_path,\n    width,\n    height,\n    batch_size,\n    fp32,\n    include_constants,\n    input_size=64,\n    down_factor=8,\n    encoder=False,\n    use_fp16_acc=True,\n    convert_conv_to_gemm=True,\n    model_name=\"AutoencoderKL\",\n    work_dir=\"./tmp\",\n    out_dir=\"./out\",\n):\n    logging.getLogger().setLevel(logging.INFO)\n    torch.manual_seed(4896)\n\n    if detect_target().name() == \"rocm\":\n        convert_conv_to_gemm = False\n\n    pipe = AutoencoderKL.from_pretrained(\n        hf_hub_or_path,\n        subfolder=\"vae\" if not hf_hub_or_path.endswith(\"vae\") else None,\n        torch_dtype=torch.float32 if fp32 else torch.float16\n    ).to(\"cuda\")\n\n    compile_vae(\n        pipe,\n        batch_size=batch_size,\n        width=width,\n        height=height,\n        use_fp16_acc=use_fp16_acc,\n        convert_conv_to_gemm=convert_conv_to_gemm,\n        constants=True if include_constants else False,\n        block_out_channels=pipe.config.block_out_channels,\n        layers_per_block=pipe.config.layers_per_block,\n        act_fn=pipe.config.act_fn,\n        latent_channels=pipe.config.latent_channels,\n        in_channels=pipe.config.in_channels,\n        out_channels=pipe.config.out_channels,\n        down_block_types=pipe.config.down_block_types,\n        up_block_types=pipe.config.up_block_types,\n        sample_size=pipe.config.sample_size,\n        input_size=(input_size, input_size),\n        down_factor=down_factor,\n        model_name=model_name,\n        dtype=\"float32\" if fp32 else \"float16\",\n        work_dir=work_dir,\n        vae_encode=encoder,\n        out_dir=out_dir,\n    )", "\nif __name__ == \"__main__\":\n    compile_diffusers()\n"]}
{"filename": "AITemplate/AITemplate.py", "chunked_list": ["MAX_MODULES=1\nUSE_LARGEST_UNET=False\n\nimport os\nimport sys\nimport comfy.model_management\nimport comfy.samplers\nimport comfy.sample\nimport comfy.utils\nimport comfy.sd", "import comfy.utils\nimport comfy.sd\nimport comfy.k_diffusion.external as k_diffusion_external\nfrom comfy.model_base import ModelType\n# so we can import nodes and latent_preview\nsys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), \"..\", \"..\", \"..\"))\nimport nodes\nimport latent_preview\nimport torch\nimport contextlib", "import torch\nimport contextlib\nimport sys\nimport time\nimport tempfile\nimport math\n\nfrom .ait.inference import AITemplateModelWrapper\nfrom .ait import AIT\nfrom .ait.inference import clip_inference, unet_inference, vae_inference, controlnet_inference", "from .ait import AIT\nfrom .ait.inference import clip_inference, unet_inference, vae_inference, controlnet_inference\n\nMAX_RESOLUTION=8192\n\ndef cleanup_temp_library(prefix=\"ait\", extension=\".dll\"):\n    temp_dir = tempfile.gettempdir()\n    dir_list = os.listdir(temp_dir)\n    dir_list = [x for x in dir_list if x.startswith(prefix) and x.endswith(extension)]\n    for x in dir_list:\n        try:\n            os.remove(os.path.join(temp_dir, x))\n        except:\n            pass", "\nextension = \".dll\" if os.name == \"nt\" else \".so\"\ncleanup_temp_library(prefix=\"\", extension=extension)\n\nsupported_ait_extensions = set(['.so', '.xz', '.dll'])\nbase_path = os.path.dirname(os.path.realpath(__file__))\nmodules_dir = os.path.join(base_path, \"modules\")\nfolder_names_and_paths = {}\nfolder_names_and_paths[\"aitemplate\"] = ([modules_dir], supported_ait_extensions)\nfilename_list_cache = {}", "folder_names_and_paths[\"aitemplate\"] = ([modules_dir], supported_ait_extensions)\nfilename_list_cache = {}\ncurrent_loaded_model = None\n\nmodules_path = str(modules_dir).replace(\"\\\\\", \"/\")\nAITemplate = AIT(modules_path)\nAIT_OS = \"windows\" if os.name == \"nt\" else \"linux\"\ncuda = torch.cuda.get_device_capability()\nif cuda[0] == 7 and cuda[1] == 5:\n    AIT_CUDA = \"sm75\"\nelif cuda[0] == 7 and cuda[1] == 0:\n    AIT_CUDA = \"sm70\"\nelif cuda[0] >= 8:\n    AIT_CUDA = \"sm80\"\nelse:\n    raise ValueError(f\"Unsupported CUDA version {cuda[0]}.{cuda[1]}\")", "if cuda[0] == 7 and cuda[1] == 5:\n    AIT_CUDA = \"sm75\"\nelif cuda[0] == 7 and cuda[1] == 0:\n    AIT_CUDA = \"sm70\"\nelif cuda[0] >= 8:\n    AIT_CUDA = \"sm80\"\nelse:\n    raise ValueError(f\"Unsupported CUDA version {cuda[0]}.{cuda[1]}\")\n\n\ndef get_full_path(folder_name, filename):\n    global folder_names_and_paths\n    if folder_name not in folder_names_and_paths:\n        return None\n    folders = folder_names_and_paths[folder_name]\n    filename = os.path.relpath(os.path.join(\"/\", filename), \"/\")\n    for x in folders[0]:\n        full_path = os.path.join(x, filename)\n        if os.path.isfile(full_path):\n            return full_path\n\n    return None", "\n\ndef get_full_path(folder_name, filename):\n    global folder_names_and_paths\n    if folder_name not in folder_names_and_paths:\n        return None\n    folders = folder_names_and_paths[folder_name]\n    filename = os.path.relpath(os.path.join(\"/\", filename), \"/\")\n    for x in folders[0]:\n        full_path = os.path.join(x, filename)\n        if os.path.isfile(full_path):\n            return full_path\n\n    return None", "\ndef recursive_search(directory):\n    if not os.path.isdir(directory):\n        return [], {}\n    result = []\n    dirs = {directory: os.path.getmtime(directory)}\n    for root, subdir, file in os.walk(directory, followlinks=True):\n        for filepath in file:\n            #we os.path,join directory with a blank string to generate a path separator at the end.\n            result.append(os.path.join(root, filepath).replace(os.path.join(directory,''),''))\n        for d in subdir:\n            path = os.path.join(root, d)\n            dirs[path] = os.path.getmtime(path)\n    return result, dirs", "\ndef filter_files_extensions(files, extensions):\n    return sorted(list(filter(lambda a: os.path.splitext(a)[-1].lower() in extensions, files)))\n\n\ndef filter_files_contains(files, contains):\n    for x in contains:\n        files = list(filter(lambda a: x in a, files))\n    return sorted(files)\n\ndef get_filename_list_(folder_name):\n    global folder_names_and_paths\n    output_list = set()\n    folders = folder_names_and_paths[folder_name]\n    output_folders = {}\n    for x in folders[0]:\n        files, folders_all = recursive_search(x)\n        output_list.update(filter_files_extensions(files, folders[1]))\n        output_folders = {**output_folders, **folders_all}\n\n    return (sorted(list(output_list)), output_folders, time.perf_counter())", "\ndef get_filename_list_(folder_name):\n    global folder_names_and_paths\n    output_list = set()\n    folders = folder_names_and_paths[folder_name]\n    output_folders = {}\n    for x in folders[0]:\n        files, folders_all = recursive_search(x)\n        output_list.update(filter_files_extensions(files, folders[1]))\n        output_folders = {**output_folders, **folders_all}\n\n    return (sorted(list(output_list)), output_folders, time.perf_counter())", "\ndef cached_filename_list_(folder_name):\n    global filename_list_cache\n    global folder_names_and_paths\n    if folder_name not in filename_list_cache:\n        return None\n    out = filename_list_cache[folder_name]\n    if time.perf_counter() < (out[2] + 0.5):\n        return out\n    for x in out[1]:\n        time_modified = out[1][x]\n        folder = x\n        if os.path.getmtime(folder) != time_modified:\n            return None\n\n    folders = folder_names_and_paths[folder_name]\n    for x in folders[0]:\n        if os.path.isdir(x):\n            if x not in out[1]:\n                return None\n\n    return out", "\ndef get_filename_list(folder_name):\n    global filename_list_cache\n    out = cached_filename_list_(folder_name)\n    if out is None:\n        out = get_filename_list_(folder_name)\n        filename_list_cache[folder_name] = out\n    return list(out[0])\n\n\ndef common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False):\n    use_aitemplate = 'aitemplate_keep_loaded' in model.model_options\n    if use_aitemplate:\n        keep_loaded = model.model_options['aitemplate_keep_loaded']\n    device = comfy.model_management.get_torch_device()\n    latent_image = latent[\"samples\"]\n\n    if disable_noise:\n        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n    else:\n        batch_inds = latent[\"batch_index\"] if \"batch_index\" in latent else None\n        noise = comfy.sample.prepare_noise(latent_image, seed, batch_inds)\n\n    noise_mask = None\n    if \"noise_mask\" in latent:\n        noise_mask = latent[\"noise_mask\"]\n\n    preview_format = \"JPEG\"\n    if preview_format not in [\"JPEG\", \"PNG\"]:\n        preview_format = \"JPEG\"\n\n    previewer = latent_preview.get_previewer(device, model.model.latent_format)\n\n    pbar = comfy.utils.ProgressBar(steps)\n    def callback(step, x0, x, total_steps):\n        preview_bytes = None\n        if previewer:\n            x0 = x0.to(comfy.model_management.get_torch_device())\n            preview_bytes = previewer.decode_latent_to_preview_image(preview_format, x0)\n        pbar.update_absolute(step + 1, total_steps, preview_bytes)\n\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\n                                  denoise=denoise, disable_noise=disable_noise, start_step=start_step, last_step=last_step,\n                                  force_full_denoise=force_full_denoise, noise_mask=noise_mask, callback=callback, seed=seed)\n    out = latent.copy()\n    out[\"samples\"] = samples\n    return (out, )", "\n\ndef common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False):\n    use_aitemplate = 'aitemplate_keep_loaded' in model.model_options\n    if use_aitemplate:\n        keep_loaded = model.model_options['aitemplate_keep_loaded']\n    device = comfy.model_management.get_torch_device()\n    latent_image = latent[\"samples\"]\n\n    if disable_noise:\n        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n    else:\n        batch_inds = latent[\"batch_index\"] if \"batch_index\" in latent else None\n        noise = comfy.sample.prepare_noise(latent_image, seed, batch_inds)\n\n    noise_mask = None\n    if \"noise_mask\" in latent:\n        noise_mask = latent[\"noise_mask\"]\n\n    preview_format = \"JPEG\"\n    if preview_format not in [\"JPEG\", \"PNG\"]:\n        preview_format = \"JPEG\"\n\n    previewer = latent_preview.get_previewer(device, model.model.latent_format)\n\n    pbar = comfy.utils.ProgressBar(steps)\n    def callback(step, x0, x, total_steps):\n        preview_bytes = None\n        if previewer:\n            x0 = x0.to(comfy.model_management.get_torch_device())\n            preview_bytes = previewer.decode_latent_to_preview_image(preview_format, x0)\n        pbar.update_absolute(step + 1, total_steps, preview_bytes)\n\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\n                                  denoise=denoise, disable_noise=disable_noise, start_step=start_step, last_step=last_step,\n                                  force_full_denoise=force_full_denoise, noise_mask=noise_mask, callback=callback, seed=seed)\n    out = latent.copy()\n    out[\"samples\"] = samples\n    return (out, )", "\nnodes.common_ksampler = common_ksampler \n\ndef maximum_batch_area():\n    memory_free = comfy.model_management.get_free_memory() / (1024 * 1024)\n    if comfy.model_management.xformers_enabled() or comfy.model_management.pytorch_attention_flash_attention():\n        area = 200 * memory_free\n    else:\n        #TODO: this formula is because AMD sucks and has memory management issues which might be fixed in the future\n        area = ((memory_free - 1024) * 0.9) / (0.6)\n    return int(max(area, 0))", "\ncomfy.model_management.maximum_batch_area = maximum_batch_area\n\ndef load_additional_models(positive, negative):\n    \"\"\"loads additional models in positive and negative conditioning\"\"\"\n    control_nets = comfy.sample.get_models_from_cond(positive, \"control\") + comfy.sample.get_models_from_cond(negative, \"control\")\n    gligen = comfy.sample.get_models_from_cond(positive, \"gligen\") + comfy.sample.get_models_from_cond(negative, \"gligen\")\n    gligen = [x[1] for x in gligen]\n    models = control_nets + gligen\n    return models", "\n\ndef sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False, noise_mask=None, sigmas=None, callback=None, disable_pbar=False, seed=None):\n    global current_loaded_model\n    global AITemplate\n    use_aitemplate = 'aitemplate_keep_loaded' in model.model_options\n    if use_aitemplate:\n        keep_loaded = model.model_options['aitemplate_keep_loaded']\n        # Use cpu for tensors to save VRAM\n        device = torch.device(\"cpu\")\n    else:\n        device = comfy.model_management.get_torch_device()\n\n    has_loaded = False\n    if use_aitemplate:\n        \"\"\"\n        Determines which module to use\n        \"\"\"\n        keys = [key.replace(\"model.diffusion_model.\", \"\") for key in model.model.diffusion_model.state_dict().keys()]\n        sd = \"v1\"\n        if type(model.model) == comfy.model_base.SDXLRefiner:\n            sd = \"xlr\"\n        elif type(model.model) == comfy.model_base.SDXL:\n            sd = \"xl\"\n        context_dim = -1\n        control = False\n        for pos in positive:\n            for x in pos:\n                if type(x) is dict:\n                    if \"control\" in x:\n                        control = True\n                else:\n                    context_dim = x.shape[2]\n        for neg in negative:\n            for x in neg:\n                if type(x) is dict:\n                    if \"control\" in x:\n                        control = True\n                        break\n        if context_dim == 1024:\n            sd = \"v2\"\n        batch_size = noise.shape[0]\n        # Resolution is the maximum of height and width, multiplied by VAE scale factor, typically 8\n        resolution = max(noise.shape[2], noise.shape[3]) * 8\n        model_type = \"unet\"\n        if control:\n            model_type = \"unet_control\"\n        # Filters the modules\n        module = AITemplate.loader.filter_modules(AIT_OS, sd, AIT_CUDA, batch_size, resolution, model_type, largest=USE_LARGEST_UNET)[0]\n        if module['sha256'] not in AITemplate.unet:\n            if len(AITemplate.unet.keys()) >= MAX_MODULES:\n                to_delete = list(AITemplate.unet.keys())\n                for x in to_delete:\n                    del AITemplate.unet[x]\n            # Load the module if it is not loaded\n            AITemplate.unet[module['sha256']] = AITemplate.loader.load_module(module['sha256'], module['url'])\n            has_loaded = True\n\n    if noise_mask is not None:\n        noise_mask = comfy.sample.prepare_mask(noise_mask, noise.shape, device)\n\n    if use_aitemplate:\n        # Apply weights if module has loaded, model is not current_loaded_model or keep_loaded is \"disable\"\n        apply_aitemplate_weights = has_loaded or model is not current_loaded_model or keep_loaded == \"disable\"\n        # Patch the model for LoRAs etc\n        model.patch_model()\n        if apply_aitemplate_weights:\n            # Applies model weights to the module\n            # Uses compvis mapping\n            # in_channels and conv_in_key are supplied to determine if padding is required\n            AITemplate.unet[module['sha256']] = AITemplate.loader.apply_unet(\n                aitemplate_module=AITemplate.unet[module['sha256']],\n                unet=AITemplate.loader.compvis_unet(model.model.state_dict()),\n                in_channels=model.model.diffusion_model.in_channels,\n                conv_in_key=\"conv_in_weight\",\n                dim=model.model.diffusion_model.model_channels,\n            )\n        current_loaded_model = model\n    else:\n        comfy.model_management.load_model_gpu(model)\n\n    real_model = model.model\n\n    noise = noise.to(device)\n    latent_image = latent_image.to(device)\n\n    positive_copy = comfy.sample.broadcast_cond(positive, noise.shape[0], device)\n    negative_copy = comfy.sample.broadcast_cond(negative, noise.shape[0], device)\n\n    models = load_additional_models(positive, negative)\n\n    sampler = comfy.samplers.KSampler(real_model, steps=steps, device=device, sampler=sampler_name, scheduler=scheduler, denoise=denoise, model_options=model.model_options)\n    if use_aitemplate:\n        # Wrapper for AITemplate\n        model_wrapper = AITemplateModelWrapper(AITemplate.unet[module['sha256']], real_model.alphas_cumprod)\n        # Overrides sampler's model_denoise\n        sampler.model_denoise = comfy.samplers.CFGNoisePredictor(model_wrapper)\n        # Overrides sampler's model_wrap\n        if real_model.model_type == ModelType.V_PREDICTION:\n            sampler.model_wrap = comfy.samplers.CompVisVDenoiser(sampler.model_denoise, quantize=True)\n        else:\n            sampler.model_wrap = k_diffusion_external.CompVisDenoiser(sampler.model_denoise, quantize=True)\n            sampler.model_wrap.model_type = sampler.model.model_type\n        # Overrides sampler's model_k\n        sampler.model_k = comfy.samplers.KSamplerX0Inpaint(sampler.model_wrap)\n\n    samples = sampler.sample(noise, positive_copy, negative_copy, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    samples = samples.cpu()\n\n    comfy.sample.cleanup_additional_models(models)\n\n    if use_aitemplate and keep_loaded == \"disable\":\n        \"\"\"\n        Cleans up current unet module\n        Also any loaded controlnet modules\n        \"\"\"\n        del AITemplate.unet[module['sha256']]\n        del sampler\n        controlnet_keys = list(AITemplate.controlnet.keys())\n        for x in controlnet_keys:\n            del AITemplate.controlnet[x]\n        AITemplate.control_net = None\n        torch.cuda.empty_cache()\n        current_loaded_model = None\n\n    if use_aitemplate:\n        # Unpatches the model, prevents issues when switching models/loras\n        model.unpatch_model()\n    return samples", "\ncomfy.sample.sample = sample\n\n\nfrom comfy.sd import ControlBase\nclass ControlNet(ControlBase):\n    def __init__(self, control_model, global_average_pooling=False, device=None):\n        super().__init__(device)\n        # Checks if controlnet is in use\n        global AITemplate\n        if AITemplate.control_net is not None:\n            self.aitemplate = True\n        else:\n            self.aitemplate = None\n        self.control_model = control_model.to(\"cuda\")\n        self.cond_hint_original = None\n        self.cond_hint = None\n        self.strength = 1.0\n        if device is None:\n            # For ControlNet device is not changed to CPU for speed\n            device = comfy.model_management.get_torch_device()\n        self.device = device\n        self.previous_controlnet = None\n        self.global_average_pooling = global_average_pooling\n\n    def aitemplate_controlnet(\n        self, latent_model_input, timesteps, encoder_hidden_states, controlnet_cond\n    ):\n        global AITemplate\n        batch = latent_model_input.shape[0] / 2\n        resolution = max(latent_model_input.shape[2], latent_model_input.shape[3]) * 8\n        control_net_module = None\n        # This function is called every inference step\n        # Once a module is loaded modules are not filtered again for speed\n        #TODO: detection of v1, v2 and xl\n        if len(AITemplate.controlnet.keys()) == 0:\n            module = AITemplate.loader.filter_modules(AIT_OS, \"v1\", AIT_CUDA, batch, resolution, \"controlnet\")[0]\n            AITemplate.controlnet[module['sha256']] = AITemplate.loader.load_module(module['sha256'], module['url'])\n            AITemplate.controlnet[module['sha256']] = AITemplate.loader.apply_controlnet(\n                aitemplate_module=AITemplate.controlnet[module['sha256']],\n                controlnet=AITemplate.loader.compvis_controlnet(self.control_model.state_dict())\n            )\n            control_net_module = module['sha256']\n        else:\n            control_net_module = list(AITemplate.controlnet.keys())[0]\n        if self.aitemplate is None:\n            raise RuntimeError(\"No aitemplate loaded\")\n        return controlnet_inference(\n            exe_module=AITemplate.controlnet[control_net_module],\n            latent_model_input=latent_model_input,\n            timesteps=timesteps,\n            encoder_hidden_states=encoder_hidden_states,\n            controlnet_cond=controlnet_cond,\n        )\n\n    def get_control(self, x_noisy, t, cond, batched_number):\n        control_prev = None\n        if self.previous_controlnet is not None:\n            control_prev = self.previous_controlnet.get_control(x_noisy, t, cond, batched_number)\n        if self.aitemplate is not None:\n            # Moves inputs to GPU\n            x_noisy = x_noisy.to(self.device)\n            self.cond_hint_original = self.cond_hint_original.to(self.device)\n        output_dtype = x_noisy.dtype\n        if self.cond_hint is None or x_noisy.shape[2] * 8 != self.cond_hint.shape[2] or x_noisy.shape[3] * 8 != self.cond_hint.shape[3]:\n            if self.cond_hint is not None:\n                del self.cond_hint\n            self.cond_hint = None\n            self.cond_hint = comfy.utils.common_upscale(self.cond_hint_original, x_noisy.shape[3] * 8, x_noisy.shape[2] * 8, 'nearest-exact', \"center\").to(self.control_model.dtype).to(self.device)\n        if x_noisy.shape[0] != self.cond_hint.shape[0]:\n            self.cond_hint = comfy.sd.broadcast_image_to(self.cond_hint, x_noisy.shape[0], batched_number)\n        if self.aitemplate is None:\n            if self.control_model.dtype == torch.float16:\n                precision_scope = torch.autocast\n            else:\n                precision_scope = contextlib.nullcontext\n\n            with precision_scope(comfy.model_management.get_autocast_device(self.device)):\n                self.control_model = comfy.model_management.load_if_low_vram(self.control_model)\n                context = torch.cat(cond['c_crossattn'], 1)\n                y = cond.get('c_adm', None)\n                control = self.control_model(x=x_noisy, hint=self.cond_hint, timesteps=t, context=context, y=y)\n                self.control_model = comfy.model_management.unload_if_low_vram(self.control_model)\n        else:\n            # AITemplate inference, returns the same as regular\n            control = self.aitemplate_controlnet(x_noisy, t, cond, self.cond_hint)\n            control = list(control.items())\n        out = {'middle':[], 'output': []}\n        autocast_enabled = torch.is_autocast_enabled()\n\n        for i in range(len(control)):\n            if i == (len(control) - 1):\n                key = 'middle'\n                index = 0\n            else:\n                key = 'output'\n                index = i\n            x = control[i]\n            if self.global_average_pooling:\n                x = torch.mean(x, dim=(2, 3), keepdim=True).repeat(1, 1, x.shape[2], x.shape[3])\n\n            x *= self.strength\n            if x.dtype != output_dtype and not autocast_enabled:\n                x = x.to(output_dtype)\n\n            if control_prev is not None and key in control_prev:\n                prev = control_prev[key][index]\n                if prev is not None:\n                    x += prev\n            out[key].append(x)\n        if control_prev is not None and 'input' in control_prev:\n            out['input'] = control_prev['input']\n        return out\n\n    def copy(self):\n        c = ControlNet(self.control_model, global_average_pooling=self.global_average_pooling)\n        self.copy_to(c)\n        return c\n\n    def get_models(self):\n        out = super().get_models()\n        out.append(self.control_model)\n        return out", "\ncomfy.sd.ControlNet = ControlNet\n\nclass AITemplateLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"model\": (\"MODEL\",),\n                              \"keep_loaded\": ([\"enable\", \"disable\"], ),\n                              }}\n    RETURN_TYPES = (\"MODEL\",)\n    FUNCTION = \"load_aitemplate\"\n\n    CATEGORY = \"loaders\"\n\n    def load_aitemplate(self, model, keep_loaded):\n        model = model.clone()\n        model.model_options['aitemplate_keep_loaded'] = keep_loaded\n        return (model,)", "\n\n\nclass AITemplateVAEEncode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \n            \"pixels\": (\"IMAGE\", ),\n            \"vae\": (\"VAE\", ),\n            # \"keep_loaded\": ([\"enable\", \"disable\"], ),\n        }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"latent\"\n\n    @staticmethod\n    def vae_encode_crop_pixels(pixels):\n        x = (pixels.shape[1] // 8) * 8\n        y = (pixels.shape[2] // 8) * 8\n        if pixels.shape[1] != x or pixels.shape[2] != y:\n            x_offset = (pixels.shape[1] % 8) // 2\n            y_offset = (pixels.shape[2] % 8) // 2\n            pixels = pixels[:, x_offset:x + x_offset, y_offset:y + y_offset, :]\n        return pixels\n\n    def encode(self, vae, pixels):#, keep_loaded):\n        global AITemplate\n        resolution = max(pixels.shape[1], pixels.shape[2])\n        model_type = \"vae_encode\"\n        module = AITemplate.loader.filter_modules(AIT_OS, \"v1\", AIT_CUDA, 1, resolution, model_type)[0]\n        # if module[\"sha256\"] not in AITemplate.vae:\n        if len(AITemplate.vae.keys()) > 0:\n            to_delete = list(AITemplate.vae.keys())\n            for key in to_delete:\n                del AITemplate.vae[key]\n        AITemplate.vae[module[\"sha256\"]] = AITemplate.loader.load_module(module[\"sha256\"], module[\"url\"])\n        AITemplate.vae[module[\"sha256\"]] = AITemplate.loader.apply_vae(\n            aitemplate_module=AITemplate.vae[module[\"sha256\"]],\n            vae=AITemplate.loader.compvis_vae(vae.first_stage_model.state_dict()),\n            encoder=True,\n        )\n        pixels = self.vae_encode_crop_pixels(pixels)\n        pixels = pixels[:,:,:,:3]\n        pixels = pixels.movedim(-1, 1)\n        pixels = 2. * pixels - 1.\n        samples = vae_inference(AITemplate.vae[module[\"sha256\"]], pixels, encoder=True)\n        samples = samples.cpu()\n        # if keep_loaded == \"disable\":\n        del AITemplate.vae[module[\"sha256\"]]\n        torch.cuda.empty_cache()\n        return ({\"samples\":samples}, )", "\n\n\nclass VAEEncodeForInpaint:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \n            \"pixels\": (\"IMAGE\", ),\n            \"vae\": (\"VAE\", ),\n            \"mask\": (\"MASK\", ),\n            \"grow_mask_by\": (\"INT\", {\"default\": 6, \"min\": 0, \"max\": 64, \"step\": 1}),\n            # \"keep_loaded\": ([\"enable\", \"disable\"], ),\n        }}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"latent/inpaint\"\n\n    def encode(self, vae, pixels, mask, grow_mask_by=6):#keep_loaded, \n        global AITemplate\n        resolution = max(pixels.shape[1], pixels.shape[2])\n        model_type = \"vae_encode\"\n        module = AITemplate.loader.filter_modules(AIT_OS, \"v1\", AIT_CUDA, 1, resolution, model_type)[0]\n        # if module[\"sha256\"] not in AITemplate.vae:\n        if len(AITemplate.vae.keys()) > 0:\n            to_delete = list(AITemplate.vae.keys())\n            for key in to_delete:\n                del AITemplate.vae[key]\n        AITemplate.vae[module[\"sha256\"]] = AITemplate.loader.load_module(module[\"sha256\"], module[\"url\"])\n        AITemplate.vae[module[\"sha256\"]] = AITemplate.loader.apply_vae(\n            aitemplate_module=AITemplate.vae[module[\"sha256\"]],\n            vae=AITemplate.loader.compvis_vae(vae.first_stage_model.state_dict()),\n            encoder=True,\n        )\n        x = (pixels.shape[1] // 8) * 8\n        y = (pixels.shape[2] // 8) * 8\n        mask = torch.nn.functional.interpolate(mask.reshape((-1, 1, mask.shape[-2], mask.shape[-1])), size=(pixels.shape[1], pixels.shape[2]), mode=\"bilinear\")\n\n        pixels = pixels.clone()\n        if pixels.shape[1] != x or pixels.shape[2] != y:\n            x_offset = (pixels.shape[1] % 8) // 2\n            y_offset = (pixels.shape[2] % 8) // 2\n            pixels = pixels[:,x_offset:x + x_offset, y_offset:y + y_offset,:]\n            mask = mask[:,:,x_offset:x + x_offset, y_offset:y + y_offset]\n\n        #grow mask by a few pixels to keep things seamless in latent space\n        if grow_mask_by == 0:\n            mask_erosion = mask\n        else:\n            kernel_tensor = torch.ones((1, 1, grow_mask_by, grow_mask_by))\n            padding = math.ceil((grow_mask_by - 1) / 2)\n\n            mask_erosion = torch.clamp(torch.nn.functional.conv2d(mask.round(), kernel_tensor, padding=padding), 0, 1)\n\n        m = (1.0 - mask.round()).squeeze(1)\n        for i in range(3):\n            pixels[:,:,:,i] -= 0.5\n            pixels[:,:,:,i] *= m\n            pixels[:,:,:,i] += 0.5\n        pixels = pixels[:,:,:,:3]\n        pixels = pixels.movedim(-1, 1)\n        pixels = 2. * pixels - 1.\n        samples = vae_inference(AITemplate.vae[module[\"sha256\"]], pixels, encoder=True)\n        samples = samples.cpu()\n        # if keep_loaded == \"disable\":\n        del AITemplate.vae[module[\"sha256\"]]\n        torch.cuda.empty_cache()\n        return ({\"samples\":samples, \"noise_mask\": (mask_erosion[:,:,:x,:y].round())}, )", "\n\nclass AITemplateVAEDecode:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": \n                    { \n                    \"vae\": (\"VAE\",),\n                    # \"keep_loaded\": ([\"enable\", \"disable\"], ),\n                    \"samples\": (\"LATENT\", ), \"vae\": (\"VAE\", )\n                    }\n                }\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"decode\"\n\n    CATEGORY = \"latent\"\n\n    def decode(self, vae, samples):\n        global AITemplate\n        resolution = max(samples[\"samples\"].shape[2], samples[\"samples\"].shape[3]) * 8\n        model_type = \"vae_decode\"\n        module = AITemplate.loader.filter_modules(AIT_OS, \"v1\", AIT_CUDA, 1, resolution, model_type)[0]\n        # if module[\"sha256\"] not in AITemplate.vae:\n        if len(AITemplate.vae.keys()) > 0:\n            to_delete = list(AITemplate.vae.keys())\n            for key in to_delete:\n                del AITemplate.vae[key]\n        AITemplate.vae[module[\"sha256\"]] = AITemplate.loader.load_module(module[\"sha256\"], module[\"url\"])\n        AITemplate.vae[module[\"sha256\"]] = AITemplate.loader.apply_vae(\n            aitemplate_module=AITemplate.vae[module[\"sha256\"]],\n            vae=AITemplate.loader.compvis_vae(vae.first_stage_model.state_dict()),\n        )\n        output = (torch.clamp((vae_inference(AITemplate.vae[module[\"sha256\"]], samples[\"samples\"]) + 1.0) / 2.0, min=0.0, max=1.0).cpu().movedim(1,-1), )\n        # if keep_loaded == \"disable\":\n        del AITemplate.vae[module[\"sha256\"]]\n        torch.cuda.empty_cache()\n        return output", "\n\nclass AITemplateControlNetLoader:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"control_net\": (\"CONTROL_NET\",),\n                                \"keep_loaded\": ([\"enable\", \"disable\"], )\n                              }}\n    RETURN_TYPES = (\"CONTROL_NET\",)\n    FUNCTION = \"load_aitemplate_controlnet\"\n\n    CATEGORY = \"loaders\"\n\n    def load_aitemplate_controlnet(self, control_net, keep_loaded):\n        global AITemplate\n        AITemplate.control_net = keep_loaded\n        control_net.control_model = control_net.control_model.to(\"cpu\")\n        control_net.device = torch.device(\"cuda\")\n        torch.cuda.empty_cache()\n        return (control_net,)", "\nclass AITemplateEmptyLatentImage:\n    def __init__(self, device=\"cpu\"):\n        self.device = device\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"width\": (\"INT\", {\"default\": 512, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 64}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 64}),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 64})}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"generate\"\n\n    CATEGORY = \"latent\"\n\n    def generate(self, width, height, batch_size=1, latent_channels=4, down_factor=8):\n        latent = torch.zeros([batch_size, latent_channels, height // down_factor, width // down_factor])\n        return ({\"samples\":latent}, )", "\n\nclass AITemplateLatentUpscale:\n    upscale_methods = [\"nearest-exact\", \"bilinear\", \"area\", \"bicubic\", \"bislerp\"]\n    crop_methods = [\"disabled\", \"center\"]\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",), \"upscale_method\": (s.upscale_methods,),\n                              \"width\": (\"INT\", {\"default\": 512, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 64}),\n                              \"height\": (\"INT\", {\"default\": 512, \"min\": 64, \"max\": MAX_RESOLUTION, \"step\": 64}),\n                              \"crop\": (s.crop_methods,)}}\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"upscale\"\n\n    CATEGORY = \"latent\"\n\n    def upscale(self, samples, upscale_method, width, height, crop, down_factor=8):\n        s = samples.copy()\n        s[\"samples\"] = comfy.utils.common_upscale(samples[\"samples\"], width // down_factor, height // down_factor, upscale_method, crop)\n        return (s,)", ""]}
{"filename": "AITemplate/clip.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport logging\n\nimport click\nimport torch", "import click\nimport torch\nfrom aitemplate.testing import detect_target\nfrom transformers import CLIPTextModel, CLIPTextModelWithProjection\nfrom ait.compile.clip import compile_clip\n\n@click.command()\n@click.option(\n    \"--hf-hub-or-path\",\n    default=r\"runwayml/stable-diffusion-v1-5\",", "    \"--hf-hub-or-path\",\n    default=r\"runwayml/stable-diffusion-v1-5\",\n    help=\"the local diffusers pipeline directory or hf hub path e.g. runwayml/stable-diffusion-v1-5\",\n)\n@click.option(\n    \"--batch-size\",\n    default=(1, 2),\n    type=(int, int),\n    nargs=2,\n    help=\"Minimum and maximum batch size\",", "    nargs=2,\n    help=\"Minimum and maximum batch size\",\n)\n@click.option(\n    \"--output-hidden-states\",\n    default=False,\n    type=bool,\n    help=\"Output hidden states\",\n)\n@click.option(", ")\n@click.option(\n    \"--text-projection\",\n    default=False,\n    type=bool,\n    help=\"use text projection\",\n)\n@click.option(\n    \"--include-constants\",\n    default=False,", "    \"--include-constants\",\n    default=False,\n    type=bool,\n    help=\"include constants (model weights) with compiled model\",\n)\n@click.option(\n    \"--subfolder\",\n    default=\"text_encoder\",\n    help=\"subfolder of hf repo or path. default `text_encoder`, this is `text_encoder_2` for SDXL.\",\n)", "    help=\"subfolder of hf repo or path. default `text_encoder`, this is `text_encoder_2` for SDXL.\",\n)\n@click.option(\"--use-fp16-acc\", default=True, help=\"use fp16 accumulation\")\n@click.option(\"--convert-conv-to-gemm\", default=True, help=\"convert 1x1 conv to gemm\")\n@click.option(\"--model-name\", default=\"CLIPTextModel\", help=\"module name\")\n@click.option(\"--work-dir\", default=\"./tmp\", help=\"work directory\")\n@click.option(\"--out-dir\", default=\"./out\", help=\"out directory\")\ndef compile_diffusers(\n    hf_hub_or_path,\n    batch_size,\n    output_hidden_states,\n    text_projection,\n    include_constants,\n    subfolder=\"text_encoder\",\n    use_fp16_acc=True,\n    convert_conv_to_gemm=True,\n    model_name=\"CLIPTextModel\",\n    work_dir=\"./tmp\",\n    out_dir=\"./out\",\n):\n    logging.getLogger().setLevel(logging.INFO)\n    torch.manual_seed(4896)\n\n    if detect_target().name() == \"rocm\":\n        convert_conv_to_gemm = False\n\n    if text_projection:\n        pipe = CLIPTextModelWithProjection.from_pretrained(\n            hf_hub_or_path,\n            subfolder=subfolder,\n            variant=\"fp16\",\n            torch_dtype=torch.float16,\n            use_safetensors=True,\n        ).to(\"cuda\")\n    else:\n        pipe = CLIPTextModel.from_pretrained(\n            hf_hub_or_path,\n            subfolder=subfolder,\n            variant=\"fp16\",\n            torch_dtype=torch.float16,\n            use_safetensors=True,\n        ).to(\"cuda\")\n\n    compile_clip(\n        pipe,\n        batch_size=batch_size,\n        seqlen=pipe.config.max_position_embeddings,\n        use_fp16_acc=use_fp16_acc,\n        convert_conv_to_gemm=convert_conv_to_gemm,\n        output_hidden_states=output_hidden_states,\n        text_projection_dim=pipe.config.projection_dim if text_projection else None,\n        depth=pipe.config.num_hidden_layers,\n        num_heads=pipe.config.num_attention_heads,\n        dim=pipe.config.hidden_size,\n        act_layer=pipe.config.hidden_act,\n        constants=include_constants,\n        model_name=model_name,\n        work_dir=work_dir,\n        out_dir=out_dir,\n    )", "\nif __name__ == \"__main__\":\n    compile_diffusers()\n"]}
{"filename": "AITemplate/test.py", "chunked_list": ["from ait.ait import AIT\n\nif __name__ == \"__main__\":\n    ait = AIT()\n    ait.load(\"/home/user/ait_modules/unet_64_1024_1_1.so\", \"runwayml/stable-diffusion-v1-5\", \"unet\")\n    ait.test_unet()\n    ait = AIT()\n    ait.load(\"/home/user/ait_tmp/tmp/v1_vae_64_1024/test.so\", \"runwayml/stable-diffusion-v1-5\", \"vae\")\n    ait.test_vae()\n    ait = AIT()\n    ait.load(\"/home/user/ait_tmp/v1_clip_1/test.so\", \"runwayml/stable-diffusion-v1-5\", \"clip\")\n    ait.test_clip()\n    ait = AIT()\n    ait.load(\"/home/user/ait_tmp/v1_controlnet_512_512_1/test.so\", \"lllyasviel/sd-controlnet-canny\", \"controlnet\")\n    ait.test_controlnet()", "    # ait = AIT()\n    # ait.load_compvis(\"/home/user/ait_modules/unet_64_1024_1_1.so\", \"/home/user/checkpoints/v1-5-pruned-emaonly.ckpt\", \"unet\")\n    # ait.test_unet()"]}
{"filename": "AITemplate/ait/inference.py", "chunked_list": ["from typing import List\n\nimport torch\n\nfrom .module import Model\n\n\nclass AITemplateModelWrapper(torch.nn.Module):\n    def __init__(\n        self,\n        unet_ait_exe: Model,\n        alphas_cumprod: torch.Tensor,\n    ):\n        super().__init__()\n        self.alphas_cumprod = alphas_cumprod\n        self.unet_ait_exe = unet_ait_exe\n\n    def apply_model(\n        self,\n        x: torch.Tensor,\n        t: torch.Tensor,\n        c_crossattn = None,\n        c_concat = None,\n        control = None,\n        c_adm = None,\n        transformer_options = None,\n    ):\n        timesteps_pt = t\n        latent_model_input = x\n        encoder_hidden_states = None\n        down_block_residuals = None\n        mid_block_residual = None\n        add_embeds = None\n        if c_crossattn is not None:\n            encoder_hidden_states = torch.cat(c_crossattn, dim=1)\n        if c_concat is not None:\n            latent_model_input = torch.cat([x] + c_concat, dim=1)\n        if control is not None:\n            down_block_residuals = control[\"output\"]\n            mid_block_residual = control[\"middle\"][0]\n        if c_adm is not None:\n            add_embeds = c_adm\n        return unet_inference(\n            self.unet_ait_exe,\n            latent_model_input=latent_model_input,\n            timesteps=timesteps_pt,\n            encoder_hidden_states=encoder_hidden_states,\n            down_block_residuals=down_block_residuals,\n            mid_block_residual=mid_block_residual,\n            add_embeds=add_embeds,\n        )", "\n\ndef unet_inference(\n    exe_module: Model,\n    latent_model_input: torch.Tensor,\n    timesteps: torch.Tensor,\n    encoder_hidden_states: torch.Tensor,\n    class_labels: torch.Tensor = None,\n    down_block_residuals: List[torch.Tensor] = None,\n    mid_block_residual: torch.Tensor = None,\n    device: str = \"cuda\",\n    dtype: str = \"float16\",\n    benchmark: bool = False,\n    add_embeds: torch.Tensor = None,\n):\n    batch = latent_model_input.shape[0]\n    height, width = latent_model_input.shape[2], latent_model_input.shape[3]\n    timesteps_pt = timesteps.expand(batch)\n    inputs = {\n        \"latent_model_input\": latent_model_input.permute((0, 2, 3, 1))\n        .contiguous()\n        .to(device),\n        \"timesteps\": timesteps_pt.to(device),\n        \"encoder_hidden_states\": encoder_hidden_states.to(device),\n    }\n    if class_labels is not None:\n        inputs[\"class_labels\"] = class_labels.contiguous().to(device)\n    if down_block_residuals is not None and mid_block_residual is not None:\n        for i, y in enumerate(down_block_residuals):\n            inputs[f\"down_block_residual_{i}\"] = y.permute((0, 2, 3, 1)).contiguous().to(device)\n        inputs[\"mid_block_residual\"] = mid_block_residual.permute((0, 2, 3, 1)).contiguous().to(device)\n    if add_embeds is not None:\n        inputs[\"add_embeds\"] = add_embeds.to(device)\n    if dtype == \"float16\":\n        for k, v in inputs.items():\n            if k == \"class_labels \":\n                continue\n            inputs[k] = v.half()\n    ys = []\n    num_outputs = len(exe_module.get_output_name_to_index_map())\n    for i in range(num_outputs):\n        shape = exe_module.get_output_maximum_shape(i)\n        shape[0] = batch\n        shape[1] = height\n        shape[2] = width\n        ys.append(torch.empty(shape).cuda().half())\n    exe_module.run_with_tensors(inputs, ys, graph_mode=False)\n    noise_pred = ys[0].permute((0, 3, 1, 2)).float()\n    if benchmark:\n        t, _, _ = exe_module.benchmark_with_tensors(\n            inputs=inputs,\n            outputs=ys,\n            count=50,\n            repeat=4,\n        )\n        print(f\"unet latency: {t} ms, it/s: {1000 / t}\")\n    return noise_pred.cpu()", "\n\ndef controlnet_inference(\n    exe_module: Model,\n    latent_model_input: torch.Tensor,\n    timesteps: torch.Tensor,\n    encoder_hidden_states: torch.Tensor,\n    controlnet_cond: torch.Tensor,\n    add_embeds: torch.Tensor = None,\n    device: str = \"cuda\",\n    dtype: str = \"float16\",\n    benchmark: bool = False,\n):\n    if controlnet_cond.shape[0] != latent_model_input.shape[0]:\n        controlnet_cond = controlnet_cond.expand(latent_model_input.shape[0], -1, -1, -1)\n    if type(encoder_hidden_states) == dict:\n        encoder_hidden_states = torch.cat(encoder_hidden_states['c_crossattn'], 1)\n    inputs = {\n        \"latent_model_input\": latent_model_input.permute((0, 2, 3, 1))\n        .contiguous()\n        .to(device),\n        \"timesteps\": timesteps.to(device),\n        \"encoder_hidden_states\": encoder_hidden_states.to(device),\n        \"control_hint\": controlnet_cond.permute((0, 2, 3, 1)).contiguous().to(device),\n    }\n    if add_embeds is not None:\n        inputs[\"add_embeds\"] = add_embeds.to(device)\n    if dtype == \"float16\":\n        for k, v in inputs.items():\n            inputs[k] = v.half()\n    ys = {}\n    for name, idx in exe_module.get_output_name_to_index_map().items():\n        shape = exe_module.get_output_maximum_shape(idx)\n        shape = torch.empty(shape).to(device)\n        if dtype == \"float16\":\n            shape = shape.half()\n        ys[name] = shape\n    exe_module.run_with_tensors(inputs, ys, graph_mode=False)\n    ys = {k: y.permute((0, 3, 1, 2)).float() for k, y in ys.items()}\n    if benchmark:\n        ys = {}\n        for name, idx in exe_module.get_output_name_to_index_map().items():\n            shape = exe_module.get_output_maximum_shape(idx)\n            shape = torch.empty(shape).to(device)\n            if dtype == \"float16\":\n                shape = shape.half()\n            ys[name] = shape\n        t, _, _ = exe_module.benchmark_with_tensors(\n            inputs=inputs,\n            outputs=ys,\n            count=50,\n            repeat=4,\n        )\n        print(f\"controlnet latency: {t} ms, it/s: {1000 / t}\")\n    return ys", "\n\n\ndef vae_inference(\n    exe_module: Model,\n    vae_input: torch.Tensor,\n    factor: int = 8,\n    device: str = \"cuda\",\n    dtype: str = \"float16\",\n    encoder: bool = False,\n    latent_channels: int = 4,\n):\n    batch = vae_input.shape[0]\n    height, width = vae_input.shape[2], vae_input.shape[3]\n    if encoder:\n        height = height // factor\n        width = width // factor\n    else:\n        height = height * factor\n        width = width * factor\n    input_name = \"pixels\" if encoder else \"latent\"\n    inputs = {\n        input_name: torch.permute(vae_input, (0, 2, 3, 1))\n        .contiguous()\n        .to(device),\n    }\n    if encoder:\n        sample = torch.randn(batch, latent_channels, height, width)\n        inputs[\"random_sample\"] = torch.permute(sample, (0, 2, 3, 1)).contiguous().to(device)\n    if dtype == \"float16\":\n        for k, v in inputs.items():\n            inputs[k] = v.half()\n    ys = []\n    num_outputs = len(exe_module.get_output_name_to_index_map())\n    for i in range(num_outputs):\n        shape = exe_module.get_output_maximum_shape(i)\n        shape[0] = batch\n        shape[1] = height\n        shape[2] = width\n        ys.append(torch.empty(shape).to(device))\n        if dtype == \"float16\":\n            ys[i] = ys[i].half()\n    exe_module.run_with_tensors(inputs, ys, graph_mode=False)\n    vae_out = ys[0].permute((0, 3, 1, 2)).cpu().float()\n    return vae_out", "\n\ndef clip_inference(\n    exe_module: Model,\n    input_ids: torch.Tensor,\n    seqlen: int = 77,\n    device: str = \"cuda\",\n    dtype: str = \"float16\",\n):\n    batch = input_ids.shape[0]\n    input_ids = input_ids.to(device)\n    position_ids = torch.arange(seqlen).expand((batch, -1)).to(device)\n    inputs = {\n        \"input_ids\": input_ids,\n        \"position_ids\": position_ids,\n    }\n    ys = []\n    num_outputs = len(exe_module.get_output_name_to_index_map())\n    for i in range(num_outputs):\n        shape = exe_module.get_output_maximum_shape(i)\n        shape[0] = batch\n        ys.append(torch.empty(shape).to(device))\n        if dtype == \"float16\":\n            ys[i] = ys[i].half()\n    exe_module.run_with_tensors(inputs, ys, graph_mode=False)\n    return ys[0].cpu().float()", ""]}
{"filename": "AITemplate/ait/ait.py", "chunked_list": ["from typing import Literal, Union\n\nimport torch\nfrom safetensors.torch import load_file\n\nfrom .load import AITLoader\nfrom .module import Model\nfrom .inference import clip_inference, unet_inference, vae_inference, controlnet_inference\n\n\nclass AIT:\n    def __init__(self, path: str = None) -> None:\n        self.modules = {}\n        self.unet = {}\n        self.vae = {}\n        self.controlnet = {}\n        self.clip = {}\n        self.control_net = None\n        if path is not None:\n            self.loader = AITLoader(path)\n        else:\n            self.loader = AITLoader()\n        self.supported = ['clip', 'controlnet', 'unet', 'vae']\n\n    def load(self,\n        aitemplate_path: str,\n        hf_hub_or_path: str,\n        module_type: str,\n    ):\n        if module_type == \"clip\":\n            self.modules[\"clip\"] = self.loader.load(aitemplate_path)\n            clip = self.loader.diffusers_clip(hf_hub_or_path)\n            self.modules[\"clip\"] = self.loader.apply_clip(self.modules[\"clip\"], clip)\n        elif module_type == \"controlnet\":\n            self.modules[\"controlnet\"] = self.loader.load(aitemplate_path)\n            controlnet = self.loader.diffusers_controlnet(hf_hub_or_path)\n            self.modules[\"controlnet\"] = self.loader.apply_controlnet(self.modules[\"controlnet\"], controlnet)\n        elif module_type == \"unet\":\n            self.modules[\"unet\"] = self.loader.load(aitemplate_path)\n            unet = self.loader.diffusers_unet(hf_hub_or_path)\n            self.modules[\"unet\"] = self.loader.apply_unet(self.modules[\"unet\"], unet)\n        elif module_type == \"vae_decode\":\n            self.modules[\"vae_decode\"] = self.loader.load(aitemplate_path)\n            vae = self.loader.diffusers_vae(hf_hub_or_path)\n            self.modules[\"vae_decode\"] = self.loader.apply_vae(self.modules[\"vae_decode\"], vae)\n        elif module_type == \"vae_encode\":\n            self.modules[\"vae_encode\"] = self.loader.load(aitemplate_path)\n            vae = self.loader.diffusers_vae(hf_hub_or_path)\n            self.modules[\"vae_encode\"] = self.loader.apply_vae(self.modules[\"vae_encode\"], vae, encoder=True)\n        else:\n            raise ValueError(f\"module_type must be one of {self.supported}\")\n\n    def load_compvis(self,\n        aitemplate_path: str,\n        ckpt_path: str,\n        module_type: str,\n    ):\n        if ckpt_path.endswith(\".safetensors\"):\n            state_dict = load_file(ckpt_path)\n        elif ckpt_path.endswith(\".ckpt\"):\n            state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n        else:\n            raise ValueError(\"ckpt_path must be a .safetensors or .ckpt file\")\n        while \"state_dict\" in state_dict.keys():\n            \"\"\"\n            yo dawg i heard you like state dicts so i put a state dict in your state dict\n\n            apparently this happens in some models\n            \"\"\"\n            state_dict = state_dict[\"state_dict\"]\n        if module_type == \"clip\":\n            self.modules[\"clip\"] = self.loader.load(aitemplate_path)\n            clip = self.loader.compvis_clip(state_dict)\n            self.modules[\"clip\"] = self.loader.apply_clip(self.modules[\"clip\"], clip)\n        elif module_type == \"controlnet\":\n            self.modules[\"controlnet\"] = self.loader.load(aitemplate_path)\n            controlnet = self.loader.compvis_controlnet(state_dict)\n            self.modules[\"controlnet\"] = self.loader.apply_controlnet(self.modules[\"controlnet\"], controlnet)\n        elif module_type == \"unet\":\n            self.modules[\"unet\"] = self.loader.load(aitemplate_path)\n            unet = self.loader.compvis_unet(state_dict)\n            self.modules[\"unet\"] = self.loader.apply_unet(self.modules[\"unet\"], unet)\n        elif module_type == \"vae_decode\":\n            self.modules[\"vae_decode\"] = self.loader.load(aitemplate_path)\n            vae = self.loader.compvis_vae(state_dict)\n            self.modules[\"vae_decode\"] = self.loader.apply_vae(self.modules[\"vae_decode\"], vae)\n        elif module_type == \"vae_encode\":\n            self.modules[\"vae_encode\"] = self.loader.load(aitemplate_path)\n            vae = self.loader.compvis_vae(state_dict)\n            self.modules[\"vae_encode\"] = self.loader.apply_vae(self.modules[\"vae_encode\"], vae, encoder=True)\n        else:\n            raise ValueError(f\"module_type must be one of {self.supported}\")\n\n\n    def test_unet(\n        self,\n        batch_size: int = 2,\n        latent_channels: int = 4,\n        height: int = 64,\n        width: int = 64,\n        hidden_dim: int = 768,\n        sequence_length: int = 77,\n        dtype=\"float16\",\n        device=\"cuda\",\n        benchmark: bool = False,\n        add_embed_dim:int = 2816,\n        xl = False,\n    ):\n        if \"unet\" not in self.modules:\n            raise ValueError(\"unet module not loaded\")\n        latent_model_input_pt = torch.randn(batch_size, latent_channels, height, width).to(device)\n        text_embeddings_pt = torch.randn(batch_size, sequence_length, hidden_dim).to(device)\n        timesteps_pt = torch.Tensor([1] * batch_size).to(device)\n        if xl:\n            add_embeds = torch.randn(batch_size, add_embed_dim).to(device)\n        if dtype == \"float16\":\n            latent_model_input_pt = latent_model_input_pt.half()\n            text_embeddings_pt = text_embeddings_pt.half()\n            timesteps_pt = timesteps_pt.half()\n            if xl:\n                add_embeds = add_embeds.half()\n        output = unet_inference(\n            self.modules[\"unet\"],\n            latent_model_input=latent_model_input_pt,\n            timesteps=timesteps_pt,\n            encoder_hidden_states=text_embeddings_pt,\n            benchmark=benchmark,\n            add_embeds=add_embeds if xl else None,\n        )\n        print(output.shape)\n        return output\n\n    def test_vae_encode(\n        self,\n        batch_size: int = 1,\n        channels: int = 3,\n        height: int = 512,\n        width: int = 512,\n        dtype=\"float16\",\n        device=\"cuda\",\n    ):\n        if \"vae_encode\" not in self.modules:\n            raise ValueError(\"vae module not loaded\")\n        vae_input = torch.randn(batch_size, channels, height, width).to(device)\n        if dtype == \"float16\":\n            vae_input = vae_input.half()\n        output = vae_inference(\n            self.modules[\"vae_encode\"],\n            vae_input=vae_input,\n            encoder=True,\n        )\n        print(output.shape)\n        return output\n\n\n    def test_vae(\n        self,\n        batch_size: int = 1,\n        latent_channels: int = 4,\n        height: int = 64,\n        width: int = 64,\n        dtype=\"float16\",\n        device=\"cuda\",\n        benchmark: bool = False,\n    ):\n        if \"vae_decode\" not in self.modules:\n            raise ValueError(\"vae module not loaded\")\n        vae_input = torch.randn(batch_size, latent_channels, height, width).to(device)\n        if dtype == \"float16\":\n            vae_input = vae_input.half()\n        output = vae_inference(\n            self.modules[\"vae_decode\"],\n            vae_input=vae_input,\n            benchmark=benchmark,\n        )\n        print(output.shape)\n        return output\n    \n    def test_clip(\n        self,\n        batch_size: int = 1,\n        sequence_length: int = 77,\n        tokenizer=None,\n    ):\n        if \"clip\" not in self.modules:\n            raise ValueError(\"clip module not loaded\")\n        try:\n            from transformers import CLIPTokenizer\n        except ImportError:\n            raise ImportError(\n                \"Please install transformers with `pip install transformers` to use this script.\"\n            )\n        if tokenizer is None:\n            tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n        text_input = tokenizer(\n            [\"a photo of an astronaut riding a horse on mars\"] * batch_size,\n            padding=\"max_length\",\n            max_length=sequence_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        input_ids = text_input[\"input_ids\"].cuda()\n        output = clip_inference(\n            self.modules[\"clip\"],\n            input_ids=input_ids,\n            seqlen=sequence_length,\n        )\n        print(output.shape)\n        return output\n\n    def test_controlnet(\n        self,\n        batch_size: int = 2,\n        latent_channels: int = 4,\n        latent_height: int = 64,\n        latent_width: int = 64,\n        hidden_dim: int = 768,\n        sequence_length: int = 77,\n        control_height: int = 512,\n        control_width: int = 512,\n        control_channels: int = 3,\n        add_embed_dim:int = 2816,\n        xl: bool = False,\n        benchmark: bool = False,\n        device=\"cuda\",\n        dtype=\"float16\",\n    ):\n        latent_model_input_pt = torch.randn(batch_size, latent_channels, latent_height, latent_width).to(device)\n        text_embeddings_pt = torch.randn(batch_size, sequence_length, hidden_dim).to(device)\n        timesteps_pt = torch.Tensor([1] * batch_size).to(device)\n        controlnet_input_pt = torch.randn(batch_size, control_channels, control_height, control_width).to(device)\n        if xl:\n            add_embeds = torch.randn(batch_size, add_embed_dim).to(device)\n        if dtype == \"float16\":\n            latent_model_input_pt = latent_model_input_pt.half()\n            text_embeddings_pt = text_embeddings_pt.half()\n            timesteps_pt = timesteps_pt.half()\n            controlnet_input_pt = controlnet_input_pt.half()\n            if xl:\n                add_embeds = add_embeds.half()\n        outputs = controlnet_inference(\n            self.modules[\"controlnet\"],\n            latent_model_input=latent_model_input_pt,\n            timesteps=timesteps_pt,\n            encoder_hidden_states=text_embeddings_pt,\n            controlnet_cond=controlnet_input_pt,\n            add_embeds=add_embeds if xl else None,\n            benchmark=benchmark,\n        )\n        for block, value in outputs.items():\n            print(block, value.shape)\n        return outputs", "\n\nclass AIT:\n    def __init__(self, path: str = None) -> None:\n        self.modules = {}\n        self.unet = {}\n        self.vae = {}\n        self.controlnet = {}\n        self.clip = {}\n        self.control_net = None\n        if path is not None:\n            self.loader = AITLoader(path)\n        else:\n            self.loader = AITLoader()\n        self.supported = ['clip', 'controlnet', 'unet', 'vae']\n\n    def load(self,\n        aitemplate_path: str,\n        hf_hub_or_path: str,\n        module_type: str,\n    ):\n        if module_type == \"clip\":\n            self.modules[\"clip\"] = self.loader.load(aitemplate_path)\n            clip = self.loader.diffusers_clip(hf_hub_or_path)\n            self.modules[\"clip\"] = self.loader.apply_clip(self.modules[\"clip\"], clip)\n        elif module_type == \"controlnet\":\n            self.modules[\"controlnet\"] = self.loader.load(aitemplate_path)\n            controlnet = self.loader.diffusers_controlnet(hf_hub_or_path)\n            self.modules[\"controlnet\"] = self.loader.apply_controlnet(self.modules[\"controlnet\"], controlnet)\n        elif module_type == \"unet\":\n            self.modules[\"unet\"] = self.loader.load(aitemplate_path)\n            unet = self.loader.diffusers_unet(hf_hub_or_path)\n            self.modules[\"unet\"] = self.loader.apply_unet(self.modules[\"unet\"], unet)\n        elif module_type == \"vae_decode\":\n            self.modules[\"vae_decode\"] = self.loader.load(aitemplate_path)\n            vae = self.loader.diffusers_vae(hf_hub_or_path)\n            self.modules[\"vae_decode\"] = self.loader.apply_vae(self.modules[\"vae_decode\"], vae)\n        elif module_type == \"vae_encode\":\n            self.modules[\"vae_encode\"] = self.loader.load(aitemplate_path)\n            vae = self.loader.diffusers_vae(hf_hub_or_path)\n            self.modules[\"vae_encode\"] = self.loader.apply_vae(self.modules[\"vae_encode\"], vae, encoder=True)\n        else:\n            raise ValueError(f\"module_type must be one of {self.supported}\")\n\n    def load_compvis(self,\n        aitemplate_path: str,\n        ckpt_path: str,\n        module_type: str,\n    ):\n        if ckpt_path.endswith(\".safetensors\"):\n            state_dict = load_file(ckpt_path)\n        elif ckpt_path.endswith(\".ckpt\"):\n            state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n        else:\n            raise ValueError(\"ckpt_path must be a .safetensors or .ckpt file\")\n        while \"state_dict\" in state_dict.keys():\n            \"\"\"\n            yo dawg i heard you like state dicts so i put a state dict in your state dict\n\n            apparently this happens in some models\n            \"\"\"\n            state_dict = state_dict[\"state_dict\"]\n        if module_type == \"clip\":\n            self.modules[\"clip\"] = self.loader.load(aitemplate_path)\n            clip = self.loader.compvis_clip(state_dict)\n            self.modules[\"clip\"] = self.loader.apply_clip(self.modules[\"clip\"], clip)\n        elif module_type == \"controlnet\":\n            self.modules[\"controlnet\"] = self.loader.load(aitemplate_path)\n            controlnet = self.loader.compvis_controlnet(state_dict)\n            self.modules[\"controlnet\"] = self.loader.apply_controlnet(self.modules[\"controlnet\"], controlnet)\n        elif module_type == \"unet\":\n            self.modules[\"unet\"] = self.loader.load(aitemplate_path)\n            unet = self.loader.compvis_unet(state_dict)\n            self.modules[\"unet\"] = self.loader.apply_unet(self.modules[\"unet\"], unet)\n        elif module_type == \"vae_decode\":\n            self.modules[\"vae_decode\"] = self.loader.load(aitemplate_path)\n            vae = self.loader.compvis_vae(state_dict)\n            self.modules[\"vae_decode\"] = self.loader.apply_vae(self.modules[\"vae_decode\"], vae)\n        elif module_type == \"vae_encode\":\n            self.modules[\"vae_encode\"] = self.loader.load(aitemplate_path)\n            vae = self.loader.compvis_vae(state_dict)\n            self.modules[\"vae_encode\"] = self.loader.apply_vae(self.modules[\"vae_encode\"], vae, encoder=True)\n        else:\n            raise ValueError(f\"module_type must be one of {self.supported}\")\n\n\n    def test_unet(\n        self,\n        batch_size: int = 2,\n        latent_channels: int = 4,\n        height: int = 64,\n        width: int = 64,\n        hidden_dim: int = 768,\n        sequence_length: int = 77,\n        dtype=\"float16\",\n        device=\"cuda\",\n        benchmark: bool = False,\n        add_embed_dim:int = 2816,\n        xl = False,\n    ):\n        if \"unet\" not in self.modules:\n            raise ValueError(\"unet module not loaded\")\n        latent_model_input_pt = torch.randn(batch_size, latent_channels, height, width).to(device)\n        text_embeddings_pt = torch.randn(batch_size, sequence_length, hidden_dim).to(device)\n        timesteps_pt = torch.Tensor([1] * batch_size).to(device)\n        if xl:\n            add_embeds = torch.randn(batch_size, add_embed_dim).to(device)\n        if dtype == \"float16\":\n            latent_model_input_pt = latent_model_input_pt.half()\n            text_embeddings_pt = text_embeddings_pt.half()\n            timesteps_pt = timesteps_pt.half()\n            if xl:\n                add_embeds = add_embeds.half()\n        output = unet_inference(\n            self.modules[\"unet\"],\n            latent_model_input=latent_model_input_pt,\n            timesteps=timesteps_pt,\n            encoder_hidden_states=text_embeddings_pt,\n            benchmark=benchmark,\n            add_embeds=add_embeds if xl else None,\n        )\n        print(output.shape)\n        return output\n\n    def test_vae_encode(\n        self,\n        batch_size: int = 1,\n        channels: int = 3,\n        height: int = 512,\n        width: int = 512,\n        dtype=\"float16\",\n        device=\"cuda\",\n    ):\n        if \"vae_encode\" not in self.modules:\n            raise ValueError(\"vae module not loaded\")\n        vae_input = torch.randn(batch_size, channels, height, width).to(device)\n        if dtype == \"float16\":\n            vae_input = vae_input.half()\n        output = vae_inference(\n            self.modules[\"vae_encode\"],\n            vae_input=vae_input,\n            encoder=True,\n        )\n        print(output.shape)\n        return output\n\n\n    def test_vae(\n        self,\n        batch_size: int = 1,\n        latent_channels: int = 4,\n        height: int = 64,\n        width: int = 64,\n        dtype=\"float16\",\n        device=\"cuda\",\n        benchmark: bool = False,\n    ):\n        if \"vae_decode\" not in self.modules:\n            raise ValueError(\"vae module not loaded\")\n        vae_input = torch.randn(batch_size, latent_channels, height, width).to(device)\n        if dtype == \"float16\":\n            vae_input = vae_input.half()\n        output = vae_inference(\n            self.modules[\"vae_decode\"],\n            vae_input=vae_input,\n            benchmark=benchmark,\n        )\n        print(output.shape)\n        return output\n    \n    def test_clip(\n        self,\n        batch_size: int = 1,\n        sequence_length: int = 77,\n        tokenizer=None,\n    ):\n        if \"clip\" not in self.modules:\n            raise ValueError(\"clip module not loaded\")\n        try:\n            from transformers import CLIPTokenizer\n        except ImportError:\n            raise ImportError(\n                \"Please install transformers with `pip install transformers` to use this script.\"\n            )\n        if tokenizer is None:\n            tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n        text_input = tokenizer(\n            [\"a photo of an astronaut riding a horse on mars\"] * batch_size,\n            padding=\"max_length\",\n            max_length=sequence_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        input_ids = text_input[\"input_ids\"].cuda()\n        output = clip_inference(\n            self.modules[\"clip\"],\n            input_ids=input_ids,\n            seqlen=sequence_length,\n        )\n        print(output.shape)\n        return output\n\n    def test_controlnet(\n        self,\n        batch_size: int = 2,\n        latent_channels: int = 4,\n        latent_height: int = 64,\n        latent_width: int = 64,\n        hidden_dim: int = 768,\n        sequence_length: int = 77,\n        control_height: int = 512,\n        control_width: int = 512,\n        control_channels: int = 3,\n        add_embed_dim:int = 2816,\n        xl: bool = False,\n        benchmark: bool = False,\n        device=\"cuda\",\n        dtype=\"float16\",\n    ):\n        latent_model_input_pt = torch.randn(batch_size, latent_channels, latent_height, latent_width).to(device)\n        text_embeddings_pt = torch.randn(batch_size, sequence_length, hidden_dim).to(device)\n        timesteps_pt = torch.Tensor([1] * batch_size).to(device)\n        controlnet_input_pt = torch.randn(batch_size, control_channels, control_height, control_width).to(device)\n        if xl:\n            add_embeds = torch.randn(batch_size, add_embed_dim).to(device)\n        if dtype == \"float16\":\n            latent_model_input_pt = latent_model_input_pt.half()\n            text_embeddings_pt = text_embeddings_pt.half()\n            timesteps_pt = timesteps_pt.half()\n            controlnet_input_pt = controlnet_input_pt.half()\n            if xl:\n                add_embeds = add_embeds.half()\n        outputs = controlnet_inference(\n            self.modules[\"controlnet\"],\n            latent_model_input=latent_model_input_pt,\n            timesteps=timesteps_pt,\n            encoder_hidden_states=text_embeddings_pt,\n            controlnet_cond=controlnet_input_pt,\n            add_embeds=add_embeds if xl else None,\n            benchmark=benchmark,\n        )\n        for block, value in outputs.items():\n            print(block, value.shape)\n        return outputs", ""]}
{"filename": "AITemplate/ait/__init__.py", "chunked_list": ["from .load import AITLoader\nfrom .inference import unet_inference, clip_inference, vae_inference, controlnet_inference\nfrom .ait import AIT\n\n__all__ = [\"AIT\", \"AITLoader\", \"unet_inference\", \"clip_inference\", \"vae_inference\", \"controlnet_inference\"]\n"]}
{"filename": "AITemplate/ait/load.py", "chunked_list": ["from typing import Union\n\nimport json\nimport os\nimport lzma\nimport requests\nimport torch\ntry:\n    from diffusers import AutoencoderKL, ControlNetModel, UNet2DConditionModel\nexcept ImportError:\n    pass", "try:\n    from transformers import CLIPTextModel\nexcept ImportError:\n    pass\n\nfrom .module import Model\nfrom .util import torch_dtype_from_str, convert_ldm_unet_checkpoint, convert_text_enc_state_dict, convert_ldm_vae_checkpoint\nfrom .util.mapping import map_clip, map_controlnet, map_unet, map_vae\n\n\nclass AITLoader:\n    def __init__(self,\n      modules_path: str = \"./modules/\",\n      num_runtimes: int = 1,\n      device: Union[str, torch.device] = \"cuda\",\n      dtype: str = \"float16\",\n    ) -> None:\n        \"\"\"\n        device and dtype can be overriden at the function level\n        device must be a cuda device\n        \"\"\"\n        self.device = device\n        self.dtype = dtype\n        self.num_runtimes = num_runtimes\n        self.modules_path = modules_path\n        self.extension = \"dll\" if os.name == \"nt\" else \"so\"\n        try:\n            self.modules = json.load(open(f\"{modules_path}/modules.json\", \"r\"))\n            if type(self.modules) == dict:\n                self.modules = list(self.modules.values())\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"modules.json not found in {modules_path}\")\n        except json.decoder.JSONDecodeError:\n            raise ValueError(f\"modules.json in {modules_path} is not a valid json file\")\n\n    def download_module(self, sha256: str, url: str):\n        module_path = f\"{self.modules_path}/{sha256}.{self.extension}\"\n        temp_path = f\"{self.modules_path}/{sha256}.{self.extension}.xz\"\n        if os.path.exists(module_path):\n            return\n        r = requests.get(url, stream=True)\n        with open(temp_path, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\n        with lzma.open(temp_path, \"rb\") as f:\n            with open(module_path, \"wb\") as g:\n                g.write(f.read())\n        os.remove(temp_path)\n\n        \n\n    def load_module(\n        self, sha256: str, url: str\n    ):\n        module_path = f\"{self.modules_path}/{sha256}.{self.extension}\"\n        download = False\n        if not os.path.exists(module_path):\n            download = True\n        if download:\n            self.download_module(sha256, url)\n        return self.load(module_path)\n\n\n    def filter_modules(self, operating_system: str, sd: str, cuda: str, batch_size: int, resolution: int, model_type: str, largest: bool = False):\n        modules = [x for x in self.modules if x[\"os\"] == operating_system and x[\"sd\"] == sd and x[\"cuda\"] == cuda and x[\"batch_size\"] == batch_size and x[\"resolution\"] >= resolution and model_type == x[\"model\"]]\n        if len(modules) == 0:\n            raise ValueError(f\"No modules found for {operating_system} {sd} {cuda} {batch_size} {resolution} {model_type}\")\n        print(f\"Found {len(modules)} modules for {operating_system} {sd} {cuda} {batch_size} {resolution} {model_type}\")\n        modules = sorted(modules, key=lambda k: k['resolution'], reverse=largest)\n        print(f\"Using {modules[0]['sha256']}\")\n        return modules\n\n\n    def load(\n        self,\n        path: str,\n    ) -> Model:\n        return Model(lib_path=path, num_runtimes=self.num_runtimes)\n\n    def compvis_unet(\n        self,\n        state_dict: dict,\n    ) -> dict:\n        \"\"\"\n        removes:\n        model.diffusion_model.\n        diffusion_model.\n        from keys if present before conversion\n        \"\"\"\n        return convert_ldm_unet_checkpoint(state_dict)\n    \n    def compvis_clip(\n        self,\n        state_dict: dict,\n    ) -> dict:\n        \"\"\"\n        removes:\n        cond_stage_model.transformer.\n        cond_stage_model.model.\n        from keys if present before conversion\n        \"\"\"\n        return convert_text_enc_state_dict(state_dict)\n    \n    def compvis_vae(\n        self,\n        state_dict: dict,\n    ) -> dict:\n        \"\"\"\n        removes:\n        first_stage_model.\n        from keys if present before conversion\n        \"\"\"\n        return convert_ldm_vae_checkpoint(state_dict)\n    \n    def compvis_controlnet(\n        self,\n        state_dict: dict,\n    ) -> dict:\n        \"\"\"\n        removes:\n        control_model.\n        from keys if present before conversion\n        \"\"\"\n        return convert_ldm_unet_checkpoint(state_dict, controlnet=True)\n\n    def diffusers_unet(\n        self,\n        hf_hub_or_path: str,\n        dtype: str = \"float16\",\n        subfolder: str = \"unet\",\n        revision: str = \"fp16\",\n    ):\n        return UNet2DConditionModel.from_pretrained(\n            hf_hub_or_path,\n            subfolder=\"unet\" if not hf_hub_or_path.endswith(\"unet\") else None,\n            variant=\"fp16\",\n            use_safetensors=True,\n            torch_dtype=torch_dtype_from_str(dtype)\n        )\n    \n    def diffusers_vae(\n        self,\n        hf_hub_or_path: str,\n        dtype: str = \"float16\",\n        subfolder: str = \"vae\",\n        revision: str = \"fp16\",\n    ):\n        return AutoencoderKL.from_pretrained(\n            hf_hub_or_path,\n            subfolder=subfolder,\n            revision=revision,\n            torch_dtype=torch_dtype_from_str(dtype)\n        )\n\n    def diffusers_controlnet(\n        self,\n        hf_hub_or_path: str,\n        dtype: str = \"float16\",\n        subfolder: str = None,\n        revision: str = None,\n    ):\n        return ControlNetModel.from_pretrained(\n            hf_hub_or_path,\n            subfolder=subfolder,\n            revision=revision,\n            # variant=\"fp16\",\n            use_safetensors=True,\n            torch_dtype=torch_dtype_from_str(dtype)\n        )\n    \n    def diffusers_clip(\n        self,\n        hf_hub_or_path: str,\n        dtype: str = \"float16\",\n        subfolder: str = \"text_encoder\",\n        revision: str = \"fp16\",\n    ):\n        return CLIPTextModel.from_pretrained(\n            hf_hub_or_path,\n            subfolder=subfolder,\n            revision=revision,\n            torch_dtype=torch_dtype_from_str(dtype)\n        )\n\n    def apply(\n        self,\n        aitemplate_module: Model,\n        ait_params: dict,\n    ) -> Model:\n        aitemplate_module.set_many_constants_with_tensors(ait_params)\n        aitemplate_module.fold_constants()\n        return aitemplate_module\n\n    def apply_unet(\n        self,\n        aitemplate_module: Model,\n        unet,#: Union[UNet2DConditionModel, dict],\n        in_channels: int = None,\n        conv_in_key: str = None,\n        dim: int = 320,\n        device: Union[str, torch.device] = None,\n        dtype: str = None,\n    ) -> Model:\n        \"\"\"\n        you don't need to set in_channels or conv_in_key unless\n        you are experimenting with other UNets\n        \"\"\"\n        device = self.device if device is None else device\n        dtype = self.dtype if dtype is None else dtype\n        ait_params = map_unet(unet, in_channels=in_channels, conv_in_key=conv_in_key, dim=dim, device=device, dtype=dtype)\n        return self.apply(aitemplate_module, ait_params)\n\n    def apply_clip(\n        self,\n        aitemplate_module: Model,\n        clip,#: Union[CLIPTextModel, dict],\n        device: Union[str, torch.device] = None,\n        dtype: str = None,\n    ) -> Model:\n        device = self.device if device is None else device\n        dtype = self.dtype if dtype is None else dtype\n        ait_params = map_clip(clip, device=device, dtype=dtype)\n        return self.apply(aitemplate_module, ait_params)\n\n    def apply_controlnet(\n        self,\n        aitemplate_module: Model,\n        controlnet,#: Union[ControlNetModel, dict],\n        dim: int = 320,\n        device: Union[str, torch.device] = None,\n        dtype: str = None,\n    ) -> Model:\n        device = self.device if device is None else device\n        dtype = self.dtype if dtype is None else dtype\n        ait_params = map_controlnet(controlnet, dim=dim, device=device, dtype=dtype)\n        return self.apply(aitemplate_module, ait_params)\n\n    def apply_vae(\n        self,\n        aitemplate_module: Model,\n        vae,#: Union[AutoencoderKL, dict],\n        device: Union[str, torch.device] = None,\n        dtype: str = None,\n        encoder: bool = False,\n    ) -> Model:\n        device = self.device if device is None else device\n        dtype = self.dtype if dtype is None else dtype\n        ait_params = map_vae(vae, device=device, dtype=dtype, encoder=encoder)\n        return self.apply(aitemplate_module, ait_params)", "\n\nclass AITLoader:\n    def __init__(self,\n      modules_path: str = \"./modules/\",\n      num_runtimes: int = 1,\n      device: Union[str, torch.device] = \"cuda\",\n      dtype: str = \"float16\",\n    ) -> None:\n        \"\"\"\n        device and dtype can be overriden at the function level\n        device must be a cuda device\n        \"\"\"\n        self.device = device\n        self.dtype = dtype\n        self.num_runtimes = num_runtimes\n        self.modules_path = modules_path\n        self.extension = \"dll\" if os.name == \"nt\" else \"so\"\n        try:\n            self.modules = json.load(open(f\"{modules_path}/modules.json\", \"r\"))\n            if type(self.modules) == dict:\n                self.modules = list(self.modules.values())\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"modules.json not found in {modules_path}\")\n        except json.decoder.JSONDecodeError:\n            raise ValueError(f\"modules.json in {modules_path} is not a valid json file\")\n\n    def download_module(self, sha256: str, url: str):\n        module_path = f\"{self.modules_path}/{sha256}.{self.extension}\"\n        temp_path = f\"{self.modules_path}/{sha256}.{self.extension}.xz\"\n        if os.path.exists(module_path):\n            return\n        r = requests.get(url, stream=True)\n        with open(temp_path, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\n        with lzma.open(temp_path, \"rb\") as f:\n            with open(module_path, \"wb\") as g:\n                g.write(f.read())\n        os.remove(temp_path)\n\n        \n\n    def load_module(\n        self, sha256: str, url: str\n    ):\n        module_path = f\"{self.modules_path}/{sha256}.{self.extension}\"\n        download = False\n        if not os.path.exists(module_path):\n            download = True\n        if download:\n            self.download_module(sha256, url)\n        return self.load(module_path)\n\n\n    def filter_modules(self, operating_system: str, sd: str, cuda: str, batch_size: int, resolution: int, model_type: str, largest: bool = False):\n        modules = [x for x in self.modules if x[\"os\"] == operating_system and x[\"sd\"] == sd and x[\"cuda\"] == cuda and x[\"batch_size\"] == batch_size and x[\"resolution\"] >= resolution and model_type == x[\"model\"]]\n        if len(modules) == 0:\n            raise ValueError(f\"No modules found for {operating_system} {sd} {cuda} {batch_size} {resolution} {model_type}\")\n        print(f\"Found {len(modules)} modules for {operating_system} {sd} {cuda} {batch_size} {resolution} {model_type}\")\n        modules = sorted(modules, key=lambda k: k['resolution'], reverse=largest)\n        print(f\"Using {modules[0]['sha256']}\")\n        return modules\n\n\n    def load(\n        self,\n        path: str,\n    ) -> Model:\n        return Model(lib_path=path, num_runtimes=self.num_runtimes)\n\n    def compvis_unet(\n        self,\n        state_dict: dict,\n    ) -> dict:\n        \"\"\"\n        removes:\n        model.diffusion_model.\n        diffusion_model.\n        from keys if present before conversion\n        \"\"\"\n        return convert_ldm_unet_checkpoint(state_dict)\n    \n    def compvis_clip(\n        self,\n        state_dict: dict,\n    ) -> dict:\n        \"\"\"\n        removes:\n        cond_stage_model.transformer.\n        cond_stage_model.model.\n        from keys if present before conversion\n        \"\"\"\n        return convert_text_enc_state_dict(state_dict)\n    \n    def compvis_vae(\n        self,\n        state_dict: dict,\n    ) -> dict:\n        \"\"\"\n        removes:\n        first_stage_model.\n        from keys if present before conversion\n        \"\"\"\n        return convert_ldm_vae_checkpoint(state_dict)\n    \n    def compvis_controlnet(\n        self,\n        state_dict: dict,\n    ) -> dict:\n        \"\"\"\n        removes:\n        control_model.\n        from keys if present before conversion\n        \"\"\"\n        return convert_ldm_unet_checkpoint(state_dict, controlnet=True)\n\n    def diffusers_unet(\n        self,\n        hf_hub_or_path: str,\n        dtype: str = \"float16\",\n        subfolder: str = \"unet\",\n        revision: str = \"fp16\",\n    ):\n        return UNet2DConditionModel.from_pretrained(\n            hf_hub_or_path,\n            subfolder=\"unet\" if not hf_hub_or_path.endswith(\"unet\") else None,\n            variant=\"fp16\",\n            use_safetensors=True,\n            torch_dtype=torch_dtype_from_str(dtype)\n        )\n    \n    def diffusers_vae(\n        self,\n        hf_hub_or_path: str,\n        dtype: str = \"float16\",\n        subfolder: str = \"vae\",\n        revision: str = \"fp16\",\n    ):\n        return AutoencoderKL.from_pretrained(\n            hf_hub_or_path,\n            subfolder=subfolder,\n            revision=revision,\n            torch_dtype=torch_dtype_from_str(dtype)\n        )\n\n    def diffusers_controlnet(\n        self,\n        hf_hub_or_path: str,\n        dtype: str = \"float16\",\n        subfolder: str = None,\n        revision: str = None,\n    ):\n        return ControlNetModel.from_pretrained(\n            hf_hub_or_path,\n            subfolder=subfolder,\n            revision=revision,\n            # variant=\"fp16\",\n            use_safetensors=True,\n            torch_dtype=torch_dtype_from_str(dtype)\n        )\n    \n    def diffusers_clip(\n        self,\n        hf_hub_or_path: str,\n        dtype: str = \"float16\",\n        subfolder: str = \"text_encoder\",\n        revision: str = \"fp16\",\n    ):\n        return CLIPTextModel.from_pretrained(\n            hf_hub_or_path,\n            subfolder=subfolder,\n            revision=revision,\n            torch_dtype=torch_dtype_from_str(dtype)\n        )\n\n    def apply(\n        self,\n        aitemplate_module: Model,\n        ait_params: dict,\n    ) -> Model:\n        aitemplate_module.set_many_constants_with_tensors(ait_params)\n        aitemplate_module.fold_constants()\n        return aitemplate_module\n\n    def apply_unet(\n        self,\n        aitemplate_module: Model,\n        unet,#: Union[UNet2DConditionModel, dict],\n        in_channels: int = None,\n        conv_in_key: str = None,\n        dim: int = 320,\n        device: Union[str, torch.device] = None,\n        dtype: str = None,\n    ) -> Model:\n        \"\"\"\n        you don't need to set in_channels or conv_in_key unless\n        you are experimenting with other UNets\n        \"\"\"\n        device = self.device if device is None else device\n        dtype = self.dtype if dtype is None else dtype\n        ait_params = map_unet(unet, in_channels=in_channels, conv_in_key=conv_in_key, dim=dim, device=device, dtype=dtype)\n        return self.apply(aitemplate_module, ait_params)\n\n    def apply_clip(\n        self,\n        aitemplate_module: Model,\n        clip,#: Union[CLIPTextModel, dict],\n        device: Union[str, torch.device] = None,\n        dtype: str = None,\n    ) -> Model:\n        device = self.device if device is None else device\n        dtype = self.dtype if dtype is None else dtype\n        ait_params = map_clip(clip, device=device, dtype=dtype)\n        return self.apply(aitemplate_module, ait_params)\n\n    def apply_controlnet(\n        self,\n        aitemplate_module: Model,\n        controlnet,#: Union[ControlNetModel, dict],\n        dim: int = 320,\n        device: Union[str, torch.device] = None,\n        dtype: str = None,\n    ) -> Model:\n        device = self.device if device is None else device\n        dtype = self.dtype if dtype is None else dtype\n        ait_params = map_controlnet(controlnet, dim=dim, device=device, dtype=dtype)\n        return self.apply(aitemplate_module, ait_params)\n\n    def apply_vae(\n        self,\n        aitemplate_module: Model,\n        vae,#: Union[AutoencoderKL, dict],\n        device: Union[str, torch.device] = None,\n        dtype: str = None,\n        encoder: bool = False,\n    ) -> Model:\n        device = self.device if device is None else device\n        dtype = self.dtype if dtype is None else dtype\n        ait_params = map_vae(vae, device=device, dtype=dtype, encoder=encoder)\n        return self.apply(aitemplate_module, ait_params)", ""]}
{"filename": "AITemplate/ait/modeling/unet_blocks.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n# Copyright 2022 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.", "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n\n# flake8: noqa\nfrom aitemplate.compiler import ops\n\nfrom aitemplate.frontend import nn, Tensor\nfrom aitemplate.testing import detect_target\n\nfrom .attention import AttentionBlock", "\nfrom .attention import AttentionBlock\n\nfrom .clip import SpatialTransformer\nfrom .resnet import Downsample2D, ResnetBlock2D, Upsample2D\n\n# pylint: disable=W0102\n\n\ndef get_down_block(\n    down_block_type,\n    num_layers,\n    in_channels,\n    out_channels,\n    temb_channels,\n    add_downsample,\n    resnet_eps,\n    resnet_act_fn,\n    attn_num_head_channels,\n    transformer_layers_per_block=1,\n    cross_attention_dim=None,\n    downsample_padding=None,\n    use_linear_projection=False,\n    only_cross_attention=False,\n    resnet_groups=32,\n    dtype=\"float16\",\n):\n    down_block_type = (\n        down_block_type[7:]\n        if down_block_type.startswith(\"UNetRes\")\n        else down_block_type\n    )\n    if down_block_type == \"DownBlock2D\":\n        return DownBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            downsample_padding=downsample_padding,\n            dtype=dtype,\n        )\n    elif down_block_type == \"AttnDownBlock2D\":\n        return AttnDownBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            downsample_padding=downsample_padding,\n            attn_num_head_channels=attn_num_head_channels,\n        )\n    elif down_block_type == \"CrossAttnDownBlock2D\":\n        if cross_attention_dim is None:\n            raise ValueError(\n                \"cross_attention_dim must be specified for CrossAttnDownBlock2D\"\n            )\n        return CrossAttnDownBlock2D(\n            num_layers=num_layers,\n            transformer_layers_per_block=transformer_layers_per_block,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            downsample_padding=downsample_padding,\n            cross_attention_dim=cross_attention_dim,\n            attn_num_head_channels=attn_num_head_channels,\n            use_linear_projection=use_linear_projection,\n            only_cross_attention=only_cross_attention,\n            dtype=dtype,\n        )\n    elif down_block_type == \"SkipDownBlock2D\":\n        return SkipDownBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            downsample_padding=downsample_padding,\n        )\n    elif down_block_type == \"AttnSkipDownBlock2D\":\n        return AttnSkipDownBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            downsample_padding=downsample_padding,\n            attn_num_head_channels=attn_num_head_channels,\n        )\n    elif down_block_type == \"DownEncoderBlock2D\":\n        return DownEncoderBlock2D(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_layers=num_layers,\n        resnet_eps=resnet_eps,\n        resnet_act_fn=resnet_act_fn,\n        resnet_groups=resnet_groups,\n        output_scale_factor=1.0,\n        add_downsample=add_downsample,\n        downsample_padding=downsample_padding,\n        dtype=dtype,\n        )", "\ndef get_down_block(\n    down_block_type,\n    num_layers,\n    in_channels,\n    out_channels,\n    temb_channels,\n    add_downsample,\n    resnet_eps,\n    resnet_act_fn,\n    attn_num_head_channels,\n    transformer_layers_per_block=1,\n    cross_attention_dim=None,\n    downsample_padding=None,\n    use_linear_projection=False,\n    only_cross_attention=False,\n    resnet_groups=32,\n    dtype=\"float16\",\n):\n    down_block_type = (\n        down_block_type[7:]\n        if down_block_type.startswith(\"UNetRes\")\n        else down_block_type\n    )\n    if down_block_type == \"DownBlock2D\":\n        return DownBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            downsample_padding=downsample_padding,\n            dtype=dtype,\n        )\n    elif down_block_type == \"AttnDownBlock2D\":\n        return AttnDownBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            downsample_padding=downsample_padding,\n            attn_num_head_channels=attn_num_head_channels,\n        )\n    elif down_block_type == \"CrossAttnDownBlock2D\":\n        if cross_attention_dim is None:\n            raise ValueError(\n                \"cross_attention_dim must be specified for CrossAttnDownBlock2D\"\n            )\n        return CrossAttnDownBlock2D(\n            num_layers=num_layers,\n            transformer_layers_per_block=transformer_layers_per_block,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            downsample_padding=downsample_padding,\n            cross_attention_dim=cross_attention_dim,\n            attn_num_head_channels=attn_num_head_channels,\n            use_linear_projection=use_linear_projection,\n            only_cross_attention=only_cross_attention,\n            dtype=dtype,\n        )\n    elif down_block_type == \"SkipDownBlock2D\":\n        return SkipDownBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            downsample_padding=downsample_padding,\n        )\n    elif down_block_type == \"AttnSkipDownBlock2D\":\n        return AttnSkipDownBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            downsample_padding=downsample_padding,\n            attn_num_head_channels=attn_num_head_channels,\n        )\n    elif down_block_type == \"DownEncoderBlock2D\":\n        return DownEncoderBlock2D(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_layers=num_layers,\n        resnet_eps=resnet_eps,\n        resnet_act_fn=resnet_act_fn,\n        resnet_groups=resnet_groups,\n        output_scale_factor=1.0,\n        add_downsample=add_downsample,\n        downsample_padding=downsample_padding,\n        dtype=dtype,\n        )", "\n\ndef get_up_block(\n    up_block_type,\n    num_layers,\n    in_channels,\n    out_channels,\n    prev_output_channel,\n    temb_channels,\n    add_upsample,\n    resnet_eps,\n    resnet_act_fn,\n    attn_num_head_channels,\n    transformer_layers_per_block=1,\n    cross_attention_dim=None,\n    use_linear_projection=False,\n    only_cross_attention=False,\n    dtype=\"float16\"\n):\n    up_block_type = (\n        up_block_type[7:] if up_block_type.startswith(\"UNetRes\") else up_block_type\n    )\n    if up_block_type == \"UpBlock2D\":\n        return UpBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            dtype=dtype,\n        )\n    elif up_block_type == \"CrossAttnUpBlock2D\":\n        if cross_attention_dim is None:\n            raise ValueError(\n                \"cross_attention_dim must be specified for CrossAttnUpBlock2D\"\n            )\n        return CrossAttnUpBlock2D(\n            transformer_layers_per_block=transformer_layers_per_block,\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            cross_attention_dim=cross_attention_dim,\n            attn_num_head_channels=attn_num_head_channels,\n            use_linear_projection=use_linear_projection,\n            only_cross_attention=only_cross_attention,\n            dtype=dtype,\n        )\n    elif up_block_type == \"AttnUpBlock2D\":\n        return AttnUpBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            attn_num_head_channels=attn_num_head_channels,\n        )\n    elif up_block_type == \"SkipUpBlock2D\":\n        return SkipUpBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n        )\n    elif up_block_type == \"AttnSkipUpBlock2D\":\n        return AttnSkipUpBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            attn_num_head_channels=attn_num_head_channels,\n        )\n    elif up_block_type == \"UpDecoderBlock2D\":\n        return UpDecoderBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            dtype=dtype,\n        )\n    raise ValueError(f\"{up_block_type} does not exist.\")", "\n\nclass UNetMidBlock2DCrossAttn(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        transformer_layers_per_block=1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        attention_type=\"default\",\n        output_scale_factor=1.0,\n        cross_attention_dim=1280,\n        use_linear_projection=False,\n        dtype=\"float16\",\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.attention_type = attention_type\n        self.attn_num_head_channels = attn_num_head_channels\n        resnet_groups = (\n            resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n        )\n\n        # there is always at least one resnet\n        resnets = [\n            ResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=resnet_groups,\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n                dtype=dtype,\n            )\n        ]\n        attentions = []\n\n        for _ in range(num_layers):\n            attentions.append(\n                SpatialTransformer(\n                    in_channels,\n                    attn_num_head_channels,\n                    in_channels // attn_num_head_channels,\n                    depth=transformer_layers_per_block,\n                    context_dim=cross_attention_dim,\n                    use_linear_projection=use_linear_projection,\n                    dtype=dtype,\n                )\n            )\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=in_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                    dtype=dtype,\n                )\n            )\n\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)\n\n    def forward(self, hidden_states, temb=None, encoder_hidden_states=None):\n        hidden_states = self.resnets[0](hidden_states, temb)\n        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n            hidden_states = attn(hidden_states, encoder_hidden_states)\n            hidden_states = resnet(hidden_states, temb)\n\n        return hidden_states", "\n\nclass CrossAttnDownBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        transformer_layers_per_block: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        attention_type=\"default\",\n        output_scale_factor=1.0,\n        downsample_padding=1,\n        add_downsample=True,\n        use_linear_projection=False,\n        only_cross_attention=False,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n\n        resnets = []\n        attentions = []\n\n        self.attention_type = attention_type\n        self.attn_num_head_channels = attn_num_head_channels\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                    dtype=dtype,\n                )\n            )\n            attentions.append(\n                SpatialTransformer(\n                    out_channels,\n                    attn_num_head_channels,\n                    out_channels // attn_num_head_channels,\n                    depth=transformer_layers_per_block,\n                    context_dim=cross_attention_dim,\n                    use_linear_projection=use_linear_projection,\n                    only_cross_attention=only_cross_attention,\n                    dtype=dtype,\n                )\n            )\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    Downsample2D(\n                        in_channels,\n                        use_conv=True,\n                        out_channels=out_channels,\n                        padding=downsample_padding,\n                        name=\"op\",\n                        dtype=dtype,\n                    )\n                ]\n            )\n        else:\n            self.downsamplers = None\n\n    def forward(self, hidden_states, temb=None, encoder_hidden_states=None):\n        output_states = ()\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            hidden_states = resnet(hidden_states, temb)\n            hidden_states = attn(hidden_states, context=encoder_hidden_states)\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states", "\n\nclass DownBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                    dtype=dtype,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    Downsample2D(\n                        in_channels,\n                        use_conv=True,\n                        out_channels=out_channels,\n                        padding=downsample_padding,\n                        name=\"op\",\n                        dtype=dtype,\n                    )\n                ]\n            )\n        else:\n            self.downsamplers = None\n\n    def forward(self, hidden_states, temb=None):\n        output_states = ()\n\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, temb)\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states", "\n\nclass CrossAttnUpBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        prev_output_channel: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        transformer_layers_per_block: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        cross_attention_dim=1280,\n        attention_type=\"default\",\n        output_scale_factor=1.0,\n        downsample_padding=1,\n        add_upsample=True,\n        use_linear_projection=False,\n        only_cross_attention=False,\n        dtype=\"float16\"\n    ):\n        super().__init__()\n\n        resnets = []\n        attentions = []\n\n        self.attention_type = attention_type\n        self.attn_num_head_channels = attn_num_head_channels\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                    dtype=dtype\n                )\n            )\n            attentions.append(\n                SpatialTransformer(\n                    out_channels,\n                    attn_num_head_channels,\n                    out_channels // attn_num_head_channels,\n                    depth=transformer_layers_per_block,\n                    context_dim=cross_attention_dim,\n                    use_linear_projection=use_linear_projection,\n                    only_cross_attention=only_cross_attention,\n                    dtype=dtype\n                )\n            )\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList(\n                [Upsample2D(out_channels, use_conv=True, out_channels=out_channels,dtype=dtype)]\n            )\n        else:\n            self.upsamplers = None\n\n    def forward(\n        self,\n        hidden_states,\n        res_hidden_states_tuple,\n        temb=None,\n        encoder_hidden_states=None,\n        upsample_size=None,\n    ):\n        for resnet, attn in zip(self.resnets, self.attentions):\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = ops.concatenate()(\n                [hidden_states, res_hidden_states], dim=-1\n            )\n\n            hidden_states = resnet(hidden_states, temb=temb)\n            hidden_states = attn(hidden_states, context=encoder_hidden_states)\n\n        if self.upsamplers is not None:\n            for upsampler in self.upsamplers:\n                hidden_states = upsampler(hidden_states, upsample_size)\n\n        return hidden_states", "\n\nclass UpBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n        dtype=\"float16\"\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                    dtype=dtype\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList(\n                [Upsample2D(out_channels, use_conv=True, out_channels=out_channels, dtype=dtype)]\n            )\n        else:\n            self.upsamplers = None\n\n    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, upsample_size=None):\n        for resnet in self.resnets:\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = ops.concatenate()(\n                [hidden_states, res_hidden_states], dim=-1\n            )\n\n            hidden_states = resnet(hidden_states, temb)\n\n        if self.upsamplers is not None:\n            for upsampler in self.upsamplers:\n                hidden_states = upsampler(hidden_states, upsample_size)\n\n        return hidden_states", "\ndef shape_to_list(shape):\n    return [sample['symbolic_value'] if type(sample) == Tensor else sample._attrs[\"symbolic_value\"] for sample in shape]\n\n\nclass DownEncoderBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=None,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                    dtype=dtype,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    Downsample2D(\n                        out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\", dtype=dtype,\n                    )\n                ]\n            )\n        else:\n            self.downsamplers = None\n\n    def forward(self, hidden_states):\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, temb=None)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n        return hidden_states", "\n\nclass UpDecoderBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            input_channels = in_channels if i == 0 else out_channels\n\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=input_channels,\n                    out_channels=out_channels,\n                    temb_channels=None,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                    dtype=dtype\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_upsample:\n            self.upsamplers = nn.ModuleList(\n                [Upsample2D(out_channels, use_conv=True, out_channels=out_channels, dtype=dtype)]\n            )\n        else:\n            self.upsamplers = None\n\n    def forward(self, hidden_states):\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, temb=None)\n\n        if self.upsamplers is not None:\n            for upsampler in self.upsamplers:\n                hidden_states = upsampler(hidden_states)\n\n        return hidden_states", "\n\nclass UNetMidBlock2D(nn.Module):\n    def __init__(\n        self,\n        batch_size,\n        height,\n        width,\n        in_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        attn_num_head_channels=1,\n        attention_type=\"default\",\n        output_scale_factor=1.0,\n        dtype=\"float16\",\n        **kwargs,\n    ):\n        super().__init__()\n\n        if attention_type != \"default\":\n            raise NotImplementedError(\n                f\"attention_type must be default! current value: {attention_type}\"\n            )\n\n        resnet_groups = (\n            resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n        )\n\n        # there is always at least one resnet\n        resnets = [\n            ResnetBlock2D(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=resnet_groups,\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n                dtype=dtype\n            )\n        ]\n        attentions = []\n        for _ in range(num_layers):\n            attentions.append(\n                AttentionBlock(\n                    batch_size,\n                    height,\n                    width,\n                    in_channels,\n                    num_head_channels=attn_num_head_channels,\n                    rescale_output_factor=output_scale_factor,\n                    eps=resnet_eps,\n                    num_groups=resnet_groups,\n                    dtype=dtype\n                )\n            )\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=in_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                    dtype=dtype\n                )\n            )\n\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)\n\n    def forward(self, hidden_states, temb=None, encoder_states=None):\n        hidden_states = self.resnets[0](hidden_states, temb)\n        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n            hidden_states = attn(hidden_states)\n            hidden_states = resnet(hidden_states, temb)\n\n        return hidden_states", ""]}
{"filename": "AITemplate/ait/modeling/unet_2d_condition.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom typing import Optional, Tuple, Union\n\nfrom aitemplate.compiler import ops\n", "from aitemplate.compiler import ops\n\nfrom aitemplate.frontend import nn, Tensor\n\nfrom .embeddings import TimestepEmbedding, Timesteps\nfrom .unet_blocks import get_down_block, get_up_block, UNetMidBlock2DCrossAttn\n\n\nclass UNet2DConditionModel(nn.Module):\n    r\"\"\"\n    UNet2DConditionModel is a conditional 2D UNet model that takes in a noisy sample, conditional state, and a timestep\n    and returns sample shaped output.\n\n    This model inherits from [`ModelMixin`]. Check the superclass documentation for the generic methods the library\n    implements for all the model (such as downloading or saving, etc.)\n\n    Parameters:\n        sample_size (`int`, *optional*): The size of the input sample.\n        in_channels (`int`, *optional*, defaults to 4): The number of channels in the input sample.\n        out_channels (`int`, *optional*, defaults to 4): The number of channels in the output.\n        center_input_sample (`bool`, *optional*, defaults to `False`): Whether to center the input sample.\n        flip_sin_to_cos (`bool`, *optional*, defaults to `False`):\n            Whether to flip the sin to cos in the time embedding.\n        freq_shift (`int`, *optional*, defaults to 0): The frequency shift to apply to the time embedding.\n        down_block_types (`Tuple[str]`, *optional*, defaults to `(\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\")`):\n            The tuple of downsample blocks to use.\n        up_block_types (`Tuple[str]`, *optional*, defaults to `(\"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\",)`):\n            The tuple of upsample blocks to use.\n        block_out_channels (`Tuple[int]`, *optional*, defaults to `(320, 640, 1280, 1280)`):\n            The tuple of output channels for each block.\n        layers_per_block (`int`, *optional*, defaults to 2): The number of layers per block.\n        downsample_padding (`int`, *optional*, defaults to 1): The padding to use for the downsampling convolution.\n        mid_block_scale_factor (`float`, *optional*, defaults to 1.0): The scale factor to use for the mid block.\n        act_fn (`str`, *optional*, defaults to `\"silu\"`): The activation function to use.\n        norm_num_groups (`int`, *optional*, defaults to 32): The number of groups to use for the normalization.\n        norm_eps (`float`, *optional*, defaults to 1e-5): The epsilon to use for the normalization.\n        cross_attention_dim (`int`, *optional*, defaults to 1280): The dimension of the cross attention features.\n        attention_head_dim (`int`, *optional*, defaults to 8): The dimension of the attention heads.\n        use_linear_projection (`bool`, *optional*, defaults to False): Use linear projection instead of 1x1 convolution.\n    \"\"\"\n\n    def __init__(\n        self,\n        sample_size: Optional[int] = None,\n        in_channels: int = 4,\n        out_channels: int = 4,\n        center_input_sample: bool = False,\n        flip_sin_to_cos: bool = True,\n        freq_shift: int = 0,\n        down_block_types: Tuple[str] = (\n            \"CrossAttnDownBlock2D\",\n            \"CrossAttnDownBlock2D\",\n            \"CrossAttnDownBlock2D\",\n            \"DownBlock2D\",\n        ),\n        up_block_types: Tuple[str] = (\n            \"UpBlock2D\",\n            \"CrossAttnUpBlock2D\",\n            \"CrossAttnUpBlock2D\",\n            \"CrossAttnUpBlock2D\",\n        ),\n        block_out_channels: Tuple[int] = (320, 640, 1280, 1280),\n        layers_per_block: int = 2,\n        downsample_padding: int = 1,\n        mid_block_scale_factor: float = 1,\n        act_fn: str = \"silu\",\n        norm_num_groups: int = 32,\n        norm_eps: float = 1e-5,\n        cross_attention_dim: int = 1280,\n        attention_head_dim: Union[int, Tuple[int]] = 8,\n        use_linear_projection: bool = False,\n        class_embed_type: Optional[str] = None,\n        num_class_embeds: Optional[int] = None,\n        only_cross_attention=[True, True, True, False],\n        conv_in_kernel=3,\n        dtype=\"float16\",\n        time_embedding_dim=None,\n        projection_class_embeddings_input_dim=None,\n        addition_embed_type=None,\n        addition_time_embed_dim=None,\n        transformer_layers_per_block=1,\n    ):\n        super().__init__()\n        self.center_input_sample = center_input_sample\n        self.sample_size = sample_size\n        self.time_embedding_dim = time_embedding_dim\n        time_embed_dim = time_embedding_dim or block_out_channels[0] * 4\n\n        # input\n        self.in_channels = in_channels\n        if self.in_channels % 4 != 0:\n            in_channels = self.in_channels + (4 - (self.in_channels % 4))\n        else:\n            in_channels = self.in_channels\n        conv_in_padding = (conv_in_kernel - 1) // 2\n        print(\"in_channels\", in_channels)\n        if in_channels < 8:\n            self.conv_in = nn.Conv2dBiasFewChannels(\n                in_channels, block_out_channels[0], 3, 1, conv_in_padding, dtype=dtype\n            )\n        else:\n            self.conv_in = nn.Conv2dBias(\n                in_channels, block_out_channels[0], 3, 1, conv_in_padding, dtype=dtype\n            )\n        # time\n        self.time_proj = Timesteps(\n            block_out_channels[0],\n            flip_sin_to_cos,\n            freq_shift,\n            dtype=dtype,\n            arange_name=\"arange\",\n        )\n        timestep_input_dim = block_out_channels[0]\n\n        self.time_embedding = TimestepEmbedding(\n            timestep_input_dim, time_embed_dim, dtype=dtype\n        )\n        self.class_embed_type = class_embed_type\n        if class_embed_type is None and num_class_embeds is not None:\n            self.class_embedding = nn.Embedding(\n                [num_class_embeds, time_embed_dim], dtype=dtype\n            )\n        elif class_embed_type == \"timestep\":\n            self.class_embedding = TimestepEmbedding(\n                timestep_input_dim, time_embed_dim, dtype=dtype\n            )\n        elif class_embed_type == \"identity\":\n            self.class_embedding = nn.Identity(dtype=dtype)\n        else:\n            self.class_embedding = None\n\n        if addition_embed_type == \"text_time\":\n            # self.add_time_proj = Timesteps(addition_time_embed_dim, flip_sin_to_cos, freq_shift, dtype=dtype, arange_name=\"add_arange\")\n            self.add_embedding = TimestepEmbedding(\n                projection_class_embeddings_input_dim, time_embed_dim, dtype=dtype\n            )\n\n        self.down_blocks = nn.ModuleList([])\n        self.up_blocks = nn.ModuleList([])\n\n        if isinstance(attention_head_dim, int):\n            attention_head_dim = (attention_head_dim,) * len(down_block_types)\n\n        # down\n        output_channel = block_out_channels[0]\n        for i, down_block_type in enumerate(down_block_types):\n            input_channel = output_channel\n            output_channel = block_out_channels[i]\n            is_final_block = i == len(block_out_channels) - 1\n            down_block = get_down_block(\n                down_block_type,\n                num_layers=layers_per_block,\n                transformer_layers_per_block=transformer_layers_per_block[i],\n                in_channels=input_channel,\n                out_channels=output_channel,\n                temb_channels=time_embed_dim,\n                add_downsample=not is_final_block,\n                resnet_eps=norm_eps,\n                resnet_act_fn=act_fn,\n                attn_num_head_channels=attention_head_dim[i],\n                cross_attention_dim=cross_attention_dim,\n                downsample_padding=downsample_padding,\n                use_linear_projection=use_linear_projection,\n                only_cross_attention=only_cross_attention[i],\n                dtype=dtype,\n            )\n            self.down_blocks.append(down_block)\n\n        # mid\n        self.mid_block = UNetMidBlock2DCrossAttn(\n            transformer_layers_per_block=transformer_layers_per_block[-1],\n            in_channels=block_out_channels[-1],\n            temb_channels=time_embed_dim,\n            resnet_eps=norm_eps,\n            resnet_act_fn=act_fn,\n            output_scale_factor=mid_block_scale_factor,\n            resnet_time_scale_shift=\"default\",\n            cross_attention_dim=cross_attention_dim,\n            attn_num_head_channels=attention_head_dim[-1],\n            resnet_groups=norm_num_groups,\n            use_linear_projection=use_linear_projection,\n            dtype=dtype,\n        )\n\n        # up\n        reversed_block_out_channels = list(reversed(block_out_channels))\n        reversed_attention_head_dim = list(reversed(attention_head_dim))\n        reversed_transformer_layers_per_block = list(\n            reversed(transformer_layers_per_block)\n        )\n        output_channel = reversed_block_out_channels[0]\n        for i, up_block_type in enumerate(up_block_types):\n            prev_output_channel = output_channel\n            output_channel = reversed_block_out_channels[i]\n            input_channel = reversed_block_out_channels[\n                min(i + 1, len(block_out_channels) - 1)\n            ]\n\n            is_final_block = i == len(block_out_channels) - 1\n\n            up_block = get_up_block(\n                up_block_type,\n                num_layers=layers_per_block + 1,\n                transformer_layers_per_block=reversed_transformer_layers_per_block[i],\n                in_channels=input_channel,\n                out_channels=output_channel,\n                prev_output_channel=prev_output_channel,\n                temb_channels=time_embed_dim,\n                add_upsample=not is_final_block,\n                resnet_eps=norm_eps,\n                resnet_act_fn=act_fn,\n                attn_num_head_channels=reversed_attention_head_dim[i],\n                cross_attention_dim=cross_attention_dim,\n                use_linear_projection=use_linear_projection,\n                only_cross_attention=only_cross_attention[i],\n                dtype=dtype,\n            )\n            self.up_blocks.append(up_block)\n            prev_output_channel = output_channel\n\n        # out\n        self.conv_norm_out = nn.GroupNorm(\n            num_channels=block_out_channels[0],\n            num_groups=norm_num_groups,\n            eps=norm_eps,\n            use_swish=True,\n            dtype=dtype,\n        )\n\n        self.conv_out = nn.Conv2dBias(\n            block_out_channels[0], out_channels, 3, 1, 1, dtype=dtype\n        )\n\n    def forward(\n        self,\n        sample,\n        timesteps,\n        encoder_hidden_states,\n        down_block_residual_0=None,\n        down_block_residual_1=None,\n        down_block_residual_2=None,\n        down_block_residual_3=None,\n        down_block_residual_4=None,\n        down_block_residual_5=None,\n        down_block_residual_6=None,\n        down_block_residual_7=None,\n        down_block_residual_8=None,\n        down_block_residual_9=None,\n        down_block_residual_10=None,\n        down_block_residual_11=None,\n        mid_block_residual=None,\n        class_labels: Optional[Tensor] = None,\n        add_embeds: Optional[Tensor] = None,\n        return_dict: bool = True,\n    ):\n        \"\"\"r\n        Args:\n            sample (`torch.FloatTensor`): (batch, channel, height, width) noisy inputs tensor\n            timestep (`torch.FloatTensor` or `float` or `int): (batch) timesteps\n            encoder_hidden_states (`torch.FloatTensor`): (batch, channel, height, width) encoder hidden states\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`models.unet_2d_condition.UNet2DConditionOutput`] instead of a plain tuple.\n\n        Returns:\n            [`~models.unet_2d_condition.UNet2DConditionOutput`] or `tuple`:\n            [`~models.unet_2d_condition.UNet2DConditionOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n        \"\"\"\n        down_block_additional_residuals = (\n            down_block_residual_0,\n            down_block_residual_1,\n            down_block_residual_2,\n            down_block_residual_3,\n            down_block_residual_4,\n            down_block_residual_5,\n            down_block_residual_6,\n            down_block_residual_7,\n            down_block_residual_8,\n            down_block_residual_9,\n            down_block_residual_10,\n            down_block_residual_11,\n        )\n        mid_block_additional_residual = mid_block_residual\n        if down_block_additional_residuals[0] is None:\n            down_block_additional_residuals = None\n\n        # 1. time\n        t_emb = self.time_proj(timesteps)\n        emb = self.time_embedding(t_emb)\n        if self.class_embedding is not None:\n            if class_labels is None:\n                raise ValueError(\n                    \"class_labels should be provided when num_class_embeds > 0\"\n                )\n\n            if self.class_embed_type == \"timestep\":\n                class_labels = self.time_proj(class_labels)\n\n            class_emb = ops.batch_gather()(\n                self.class_embedding.weight.tensor(), class_labels\n            )\n            emb = emb + class_emb\n\n        if add_embeds is not None:\n            aug_emb = self.add_embedding(add_embeds)\n            emb = emb + aug_emb\n\n        # 2. pre-process\n        if self.in_channels % 4 != 0:\n            channel_pad = self.in_channels + (4 - (self.in_channels % 4))\n            sample = ops.pad_last_dim(4, channel_pad)(sample)\n\n        sample = self.conv_in(sample)\n\n        # 3. down\n        down_block_res_samples = (sample,)\n        for downsample_block in self.down_blocks:\n            if (\n                hasattr(downsample_block, \"attentions\")\n                and downsample_block.attentions is not None\n            ):\n                sample, res_samples = downsample_block(\n                    hidden_states=sample,\n                    temb=emb,\n                    encoder_hidden_states=encoder_hidden_states,\n                )\n            else:\n                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n\n            down_block_res_samples += res_samples\n            # return sample\n\n        if down_block_additional_residuals is not None:\n            new_down_block_res_samples = ()\n\n            for down_block_res_sample, down_block_additional_residual in zip(\n                down_block_res_samples, down_block_additional_residuals\n            ):\n                down_block_additional_residual._attrs[\n                    \"shape\"\n                ] = down_block_res_sample._attrs[\"shape\"]\n                down_block_res_sample += down_block_additional_residual\n                new_down_block_res_samples += (down_block_res_sample,)\n\n            down_block_res_samples = new_down_block_res_samples\n\n        # 4. mid\n        sample = self.mid_block(\n            sample, emb, encoder_hidden_states=encoder_hidden_states\n        )\n\n        if mid_block_additional_residual is not None:\n            mid_block_additional_residual._attrs[\"shape\"] = sample._attrs[\"shape\"]\n            sample += mid_block_additional_residual\n        upsample_size = None\n        # 5. up\n        for i, upsample_block in enumerate(self.up_blocks):\n            is_final_block = i == len(self.up_blocks) - 1\n            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n            down_block_res_samples = down_block_res_samples[\n                : -len(upsample_block.resnets)\n            ]\n            if not is_final_block:\n                upsample_size = ops.size()(down_block_res_samples[-1])\n\n            if (\n                hasattr(upsample_block, \"attentions\")\n                and upsample_block.attentions is not None\n            ):\n                sample = upsample_block(\n                    hidden_states=sample,\n                    temb=emb,\n                    res_hidden_states_tuple=res_samples,\n                    encoder_hidden_states=encoder_hidden_states,\n                    upsample_size=upsample_size,\n                )\n            else:\n                sample = upsample_block(\n                    hidden_states=sample,\n                    temb=emb,\n                    res_hidden_states_tuple=res_samples,\n                    upsample_size=upsample_size,\n                )\n\n        # 6. post-process\n        # make sure hidden states is in float32\n        # when running in half-precision\n        sample = self.conv_norm_out(sample)\n        sample = self.conv_out(sample)\n        sample._attrs[\"is_output\"] = True\n        sample._attrs[\"name\"] = \"latent_output\"\n        return sample", "class UNet2DConditionModel(nn.Module):\n    r\"\"\"\n    UNet2DConditionModel is a conditional 2D UNet model that takes in a noisy sample, conditional state, and a timestep\n    and returns sample shaped output.\n\n    This model inherits from [`ModelMixin`]. Check the superclass documentation for the generic methods the library\n    implements for all the model (such as downloading or saving, etc.)\n\n    Parameters:\n        sample_size (`int`, *optional*): The size of the input sample.\n        in_channels (`int`, *optional*, defaults to 4): The number of channels in the input sample.\n        out_channels (`int`, *optional*, defaults to 4): The number of channels in the output.\n        center_input_sample (`bool`, *optional*, defaults to `False`): Whether to center the input sample.\n        flip_sin_to_cos (`bool`, *optional*, defaults to `False`):\n            Whether to flip the sin to cos in the time embedding.\n        freq_shift (`int`, *optional*, defaults to 0): The frequency shift to apply to the time embedding.\n        down_block_types (`Tuple[str]`, *optional*, defaults to `(\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\")`):\n            The tuple of downsample blocks to use.\n        up_block_types (`Tuple[str]`, *optional*, defaults to `(\"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\",)`):\n            The tuple of upsample blocks to use.\n        block_out_channels (`Tuple[int]`, *optional*, defaults to `(320, 640, 1280, 1280)`):\n            The tuple of output channels for each block.\n        layers_per_block (`int`, *optional*, defaults to 2): The number of layers per block.\n        downsample_padding (`int`, *optional*, defaults to 1): The padding to use for the downsampling convolution.\n        mid_block_scale_factor (`float`, *optional*, defaults to 1.0): The scale factor to use for the mid block.\n        act_fn (`str`, *optional*, defaults to `\"silu\"`): The activation function to use.\n        norm_num_groups (`int`, *optional*, defaults to 32): The number of groups to use for the normalization.\n        norm_eps (`float`, *optional*, defaults to 1e-5): The epsilon to use for the normalization.\n        cross_attention_dim (`int`, *optional*, defaults to 1280): The dimension of the cross attention features.\n        attention_head_dim (`int`, *optional*, defaults to 8): The dimension of the attention heads.\n        use_linear_projection (`bool`, *optional*, defaults to False): Use linear projection instead of 1x1 convolution.\n    \"\"\"\n\n    def __init__(\n        self,\n        sample_size: Optional[int] = None,\n        in_channels: int = 4,\n        out_channels: int = 4,\n        center_input_sample: bool = False,\n        flip_sin_to_cos: bool = True,\n        freq_shift: int = 0,\n        down_block_types: Tuple[str] = (\n            \"CrossAttnDownBlock2D\",\n            \"CrossAttnDownBlock2D\",\n            \"CrossAttnDownBlock2D\",\n            \"DownBlock2D\",\n        ),\n        up_block_types: Tuple[str] = (\n            \"UpBlock2D\",\n            \"CrossAttnUpBlock2D\",\n            \"CrossAttnUpBlock2D\",\n            \"CrossAttnUpBlock2D\",\n        ),\n        block_out_channels: Tuple[int] = (320, 640, 1280, 1280),\n        layers_per_block: int = 2,\n        downsample_padding: int = 1,\n        mid_block_scale_factor: float = 1,\n        act_fn: str = \"silu\",\n        norm_num_groups: int = 32,\n        norm_eps: float = 1e-5,\n        cross_attention_dim: int = 1280,\n        attention_head_dim: Union[int, Tuple[int]] = 8,\n        use_linear_projection: bool = False,\n        class_embed_type: Optional[str] = None,\n        num_class_embeds: Optional[int] = None,\n        only_cross_attention=[True, True, True, False],\n        conv_in_kernel=3,\n        dtype=\"float16\",\n        time_embedding_dim=None,\n        projection_class_embeddings_input_dim=None,\n        addition_embed_type=None,\n        addition_time_embed_dim=None,\n        transformer_layers_per_block=1,\n    ):\n        super().__init__()\n        self.center_input_sample = center_input_sample\n        self.sample_size = sample_size\n        self.time_embedding_dim = time_embedding_dim\n        time_embed_dim = time_embedding_dim or block_out_channels[0] * 4\n\n        # input\n        self.in_channels = in_channels\n        if self.in_channels % 4 != 0:\n            in_channels = self.in_channels + (4 - (self.in_channels % 4))\n        else:\n            in_channels = self.in_channels\n        conv_in_padding = (conv_in_kernel - 1) // 2\n        print(\"in_channels\", in_channels)\n        if in_channels < 8:\n            self.conv_in = nn.Conv2dBiasFewChannels(\n                in_channels, block_out_channels[0], 3, 1, conv_in_padding, dtype=dtype\n            )\n        else:\n            self.conv_in = nn.Conv2dBias(\n                in_channels, block_out_channels[0], 3, 1, conv_in_padding, dtype=dtype\n            )\n        # time\n        self.time_proj = Timesteps(\n            block_out_channels[0],\n            flip_sin_to_cos,\n            freq_shift,\n            dtype=dtype,\n            arange_name=\"arange\",\n        )\n        timestep_input_dim = block_out_channels[0]\n\n        self.time_embedding = TimestepEmbedding(\n            timestep_input_dim, time_embed_dim, dtype=dtype\n        )\n        self.class_embed_type = class_embed_type\n        if class_embed_type is None and num_class_embeds is not None:\n            self.class_embedding = nn.Embedding(\n                [num_class_embeds, time_embed_dim], dtype=dtype\n            )\n        elif class_embed_type == \"timestep\":\n            self.class_embedding = TimestepEmbedding(\n                timestep_input_dim, time_embed_dim, dtype=dtype\n            )\n        elif class_embed_type == \"identity\":\n            self.class_embedding = nn.Identity(dtype=dtype)\n        else:\n            self.class_embedding = None\n\n        if addition_embed_type == \"text_time\":\n            # self.add_time_proj = Timesteps(addition_time_embed_dim, flip_sin_to_cos, freq_shift, dtype=dtype, arange_name=\"add_arange\")\n            self.add_embedding = TimestepEmbedding(\n                projection_class_embeddings_input_dim, time_embed_dim, dtype=dtype\n            )\n\n        self.down_blocks = nn.ModuleList([])\n        self.up_blocks = nn.ModuleList([])\n\n        if isinstance(attention_head_dim, int):\n            attention_head_dim = (attention_head_dim,) * len(down_block_types)\n\n        # down\n        output_channel = block_out_channels[0]\n        for i, down_block_type in enumerate(down_block_types):\n            input_channel = output_channel\n            output_channel = block_out_channels[i]\n            is_final_block = i == len(block_out_channels) - 1\n            down_block = get_down_block(\n                down_block_type,\n                num_layers=layers_per_block,\n                transformer_layers_per_block=transformer_layers_per_block[i],\n                in_channels=input_channel,\n                out_channels=output_channel,\n                temb_channels=time_embed_dim,\n                add_downsample=not is_final_block,\n                resnet_eps=norm_eps,\n                resnet_act_fn=act_fn,\n                attn_num_head_channels=attention_head_dim[i],\n                cross_attention_dim=cross_attention_dim,\n                downsample_padding=downsample_padding,\n                use_linear_projection=use_linear_projection,\n                only_cross_attention=only_cross_attention[i],\n                dtype=dtype,\n            )\n            self.down_blocks.append(down_block)\n\n        # mid\n        self.mid_block = UNetMidBlock2DCrossAttn(\n            transformer_layers_per_block=transformer_layers_per_block[-1],\n            in_channels=block_out_channels[-1],\n            temb_channels=time_embed_dim,\n            resnet_eps=norm_eps,\n            resnet_act_fn=act_fn,\n            output_scale_factor=mid_block_scale_factor,\n            resnet_time_scale_shift=\"default\",\n            cross_attention_dim=cross_attention_dim,\n            attn_num_head_channels=attention_head_dim[-1],\n            resnet_groups=norm_num_groups,\n            use_linear_projection=use_linear_projection,\n            dtype=dtype,\n        )\n\n        # up\n        reversed_block_out_channels = list(reversed(block_out_channels))\n        reversed_attention_head_dim = list(reversed(attention_head_dim))\n        reversed_transformer_layers_per_block = list(\n            reversed(transformer_layers_per_block)\n        )\n        output_channel = reversed_block_out_channels[0]\n        for i, up_block_type in enumerate(up_block_types):\n            prev_output_channel = output_channel\n            output_channel = reversed_block_out_channels[i]\n            input_channel = reversed_block_out_channels[\n                min(i + 1, len(block_out_channels) - 1)\n            ]\n\n            is_final_block = i == len(block_out_channels) - 1\n\n            up_block = get_up_block(\n                up_block_type,\n                num_layers=layers_per_block + 1,\n                transformer_layers_per_block=reversed_transformer_layers_per_block[i],\n                in_channels=input_channel,\n                out_channels=output_channel,\n                prev_output_channel=prev_output_channel,\n                temb_channels=time_embed_dim,\n                add_upsample=not is_final_block,\n                resnet_eps=norm_eps,\n                resnet_act_fn=act_fn,\n                attn_num_head_channels=reversed_attention_head_dim[i],\n                cross_attention_dim=cross_attention_dim,\n                use_linear_projection=use_linear_projection,\n                only_cross_attention=only_cross_attention[i],\n                dtype=dtype,\n            )\n            self.up_blocks.append(up_block)\n            prev_output_channel = output_channel\n\n        # out\n        self.conv_norm_out = nn.GroupNorm(\n            num_channels=block_out_channels[0],\n            num_groups=norm_num_groups,\n            eps=norm_eps,\n            use_swish=True,\n            dtype=dtype,\n        )\n\n        self.conv_out = nn.Conv2dBias(\n            block_out_channels[0], out_channels, 3, 1, 1, dtype=dtype\n        )\n\n    def forward(\n        self,\n        sample,\n        timesteps,\n        encoder_hidden_states,\n        down_block_residual_0=None,\n        down_block_residual_1=None,\n        down_block_residual_2=None,\n        down_block_residual_3=None,\n        down_block_residual_4=None,\n        down_block_residual_5=None,\n        down_block_residual_6=None,\n        down_block_residual_7=None,\n        down_block_residual_8=None,\n        down_block_residual_9=None,\n        down_block_residual_10=None,\n        down_block_residual_11=None,\n        mid_block_residual=None,\n        class_labels: Optional[Tensor] = None,\n        add_embeds: Optional[Tensor] = None,\n        return_dict: bool = True,\n    ):\n        \"\"\"r\n        Args:\n            sample (`torch.FloatTensor`): (batch, channel, height, width) noisy inputs tensor\n            timestep (`torch.FloatTensor` or `float` or `int): (batch) timesteps\n            encoder_hidden_states (`torch.FloatTensor`): (batch, channel, height, width) encoder hidden states\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`models.unet_2d_condition.UNet2DConditionOutput`] instead of a plain tuple.\n\n        Returns:\n            [`~models.unet_2d_condition.UNet2DConditionOutput`] or `tuple`:\n            [`~models.unet_2d_condition.UNet2DConditionOutput`] if `return_dict` is True, otherwise a `tuple`. When\n            returning a tuple, the first element is the sample tensor.\n        \"\"\"\n        down_block_additional_residuals = (\n            down_block_residual_0,\n            down_block_residual_1,\n            down_block_residual_2,\n            down_block_residual_3,\n            down_block_residual_4,\n            down_block_residual_5,\n            down_block_residual_6,\n            down_block_residual_7,\n            down_block_residual_8,\n            down_block_residual_9,\n            down_block_residual_10,\n            down_block_residual_11,\n        )\n        mid_block_additional_residual = mid_block_residual\n        if down_block_additional_residuals[0] is None:\n            down_block_additional_residuals = None\n\n        # 1. time\n        t_emb = self.time_proj(timesteps)\n        emb = self.time_embedding(t_emb)\n        if self.class_embedding is not None:\n            if class_labels is None:\n                raise ValueError(\n                    \"class_labels should be provided when num_class_embeds > 0\"\n                )\n\n            if self.class_embed_type == \"timestep\":\n                class_labels = self.time_proj(class_labels)\n\n            class_emb = ops.batch_gather()(\n                self.class_embedding.weight.tensor(), class_labels\n            )\n            emb = emb + class_emb\n\n        if add_embeds is not None:\n            aug_emb = self.add_embedding(add_embeds)\n            emb = emb + aug_emb\n\n        # 2. pre-process\n        if self.in_channels % 4 != 0:\n            channel_pad = self.in_channels + (4 - (self.in_channels % 4))\n            sample = ops.pad_last_dim(4, channel_pad)(sample)\n\n        sample = self.conv_in(sample)\n\n        # 3. down\n        down_block_res_samples = (sample,)\n        for downsample_block in self.down_blocks:\n            if (\n                hasattr(downsample_block, \"attentions\")\n                and downsample_block.attentions is not None\n            ):\n                sample, res_samples = downsample_block(\n                    hidden_states=sample,\n                    temb=emb,\n                    encoder_hidden_states=encoder_hidden_states,\n                )\n            else:\n                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n\n            down_block_res_samples += res_samples\n            # return sample\n\n        if down_block_additional_residuals is not None:\n            new_down_block_res_samples = ()\n\n            for down_block_res_sample, down_block_additional_residual in zip(\n                down_block_res_samples, down_block_additional_residuals\n            ):\n                down_block_additional_residual._attrs[\n                    \"shape\"\n                ] = down_block_res_sample._attrs[\"shape\"]\n                down_block_res_sample += down_block_additional_residual\n                new_down_block_res_samples += (down_block_res_sample,)\n\n            down_block_res_samples = new_down_block_res_samples\n\n        # 4. mid\n        sample = self.mid_block(\n            sample, emb, encoder_hidden_states=encoder_hidden_states\n        )\n\n        if mid_block_additional_residual is not None:\n            mid_block_additional_residual._attrs[\"shape\"] = sample._attrs[\"shape\"]\n            sample += mid_block_additional_residual\n        upsample_size = None\n        # 5. up\n        for i, upsample_block in enumerate(self.up_blocks):\n            is_final_block = i == len(self.up_blocks) - 1\n            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n            down_block_res_samples = down_block_res_samples[\n                : -len(upsample_block.resnets)\n            ]\n            if not is_final_block:\n                upsample_size = ops.size()(down_block_res_samples[-1])\n\n            if (\n                hasattr(upsample_block, \"attentions\")\n                and upsample_block.attentions is not None\n            ):\n                sample = upsample_block(\n                    hidden_states=sample,\n                    temb=emb,\n                    res_hidden_states_tuple=res_samples,\n                    encoder_hidden_states=encoder_hidden_states,\n                    upsample_size=upsample_size,\n                )\n            else:\n                sample = upsample_block(\n                    hidden_states=sample,\n                    temb=emb,\n                    res_hidden_states_tuple=res_samples,\n                    upsample_size=upsample_size,\n                )\n\n        # 6. post-process\n        # make sure hidden states is in float32\n        # when running in half-precision\n        sample = self.conv_norm_out(sample)\n        sample = self.conv_out(sample)\n        sample._attrs[\"is_output\"] = True\n        sample._attrs[\"name\"] = \"latent_output\"\n        return sample"]}
{"filename": "AITemplate/ait/modeling/embeddings.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport math\n\nfrom aitemplate.compiler import ops\nfrom aitemplate.frontend import nn, Tensor", "from aitemplate.compiler import ops\nfrom aitemplate.frontend import nn, Tensor\n\n\ndef get_shape(x):\n    shape = [it.value() for it in x._attrs[\"shape\"]]\n    return shape\n\n\ndef get_timestep_embedding(\n    timesteps: Tensor,\n    embedding_dim: int,\n    flip_sin_to_cos: bool = False,\n    downscale_freq_shift: float = 1,\n    scale: float = 1,\n    max_period: int = 10000,\n    dtype: str = \"float16\",\n    arange_name = \"arange\",\n):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models: Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param embedding_dim: the dimension of the output. :param max_period: controls the minimum frequency of the\n    embeddings. :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    assert timesteps._rank() == 1, \"Timesteps should be a 1d-array\"\n\n    half_dim = embedding_dim // 2\n\n    exponent = (-math.log(max_period)) * Tensor(\n        shape=[half_dim], dtype=dtype, name=arange_name\n    )\n\n    exponent = exponent * (1.0 / (half_dim - downscale_freq_shift))\n\n    emb = ops.exp(exponent)\n    emb = ops.reshape()(timesteps, [-1, 1]) * ops.reshape()(emb, [1, -1])\n\n    # scale embeddings\n    emb = scale * emb\n\n    # concat sine and cosine embeddings\n    if flip_sin_to_cos:\n        emb = ops.concatenate()(\n            [ops.cos(emb), ops.sin(emb)],\n            dim=-1,\n        )\n    else:\n        emb = ops.concatenate()(\n            [ops.sin(emb), ops.cos(emb)],\n            dim=-1,\n        )\n    return emb", "\ndef get_timestep_embedding(\n    timesteps: Tensor,\n    embedding_dim: int,\n    flip_sin_to_cos: bool = False,\n    downscale_freq_shift: float = 1,\n    scale: float = 1,\n    max_period: int = 10000,\n    dtype: str = \"float16\",\n    arange_name = \"arange\",\n):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models: Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param embedding_dim: the dimension of the output. :param max_period: controls the minimum frequency of the\n    embeddings. :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    assert timesteps._rank() == 1, \"Timesteps should be a 1d-array\"\n\n    half_dim = embedding_dim // 2\n\n    exponent = (-math.log(max_period)) * Tensor(\n        shape=[half_dim], dtype=dtype, name=arange_name\n    )\n\n    exponent = exponent * (1.0 / (half_dim - downscale_freq_shift))\n\n    emb = ops.exp(exponent)\n    emb = ops.reshape()(timesteps, [-1, 1]) * ops.reshape()(emb, [1, -1])\n\n    # scale embeddings\n    emb = scale * emb\n\n    # concat sine and cosine embeddings\n    if flip_sin_to_cos:\n        emb = ops.concatenate()(\n            [ops.cos(emb), ops.sin(emb)],\n            dim=-1,\n        )\n    else:\n        emb = ops.concatenate()(\n            [ops.sin(emb), ops.cos(emb)],\n            dim=-1,\n        )\n    return emb", "\n\nclass TimestepEmbedding(nn.Module):\n    def __init__(self, channel: int, time_embed_dim: int, act_fn: str = \"silu\", dtype: str = \"float16\"):\n        super().__init__()\n\n        self.linear_1 = nn.Linear(channel, time_embed_dim, specialization=\"swish\", dtype=dtype)\n        self.linear_2 = nn.Linear(time_embed_dim, time_embed_dim, dtype=dtype)\n\n    def forward(self, sample):\n        sample = self.linear_1(sample)\n        sample = self.linear_2(sample)\n        return sample", "\n\nclass Timesteps(nn.Module):\n    def __init__(\n        self, num_channels: int, flip_sin_to_cos: bool, downscale_freq_shift: float, dtype: str = \"float16\", arange_name = \"arange\"\n    ):\n        super().__init__()\n        self.num_channels = num_channels\n        self.flip_sin_to_cos = flip_sin_to_cos\n        self.downscale_freq_shift = downscale_freq_shift\n        self.dtype = dtype\n        self.arange_name = arange_name\n\n    def forward(self, timesteps):\n        t_emb = get_timestep_embedding(\n            timesteps,\n            self.num_channels,\n            flip_sin_to_cos=self.flip_sin_to_cos,\n            downscale_freq_shift=self.downscale_freq_shift,\n            dtype=self.dtype,\n            arange_name=self.arange_name,\n        )\n        return t_emb", ""]}
{"filename": "AITemplate/ait/modeling/controlnet.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom typing import Optional, Tuple, Union\n\nfrom aitemplate.compiler import ops\nfrom aitemplate.compiler.base import Tensor", "from aitemplate.compiler import ops\nfrom aitemplate.compiler.base import Tensor\nfrom aitemplate.frontend import nn\n\nfrom .embeddings import TimestepEmbedding, Timesteps\nfrom .unet_blocks import get_down_block, get_up_block, UNetMidBlock2DCrossAttn, CrossAttnDownBlock2D\n\n\nclass ControlNetConditioningEmbedding(nn.Module):\n    \"\"\"\n    Quoting from https://arxiv.org/abs/2302.05543: \"Stable Diffusion uses a pre-processing method similar to VQ-GAN\n    [11] to convert the entire dataset of 512 \u00d7 512 images into smaller 64 \u00d7 64 \u201clatent images\u201d for stabilized\n    training. This requires ControlNets to convert image-based conditions to 64 \u00d7 64 feature space to match the\n    convolution size. We use a tiny network E(\u00b7) of four convolution layers with 4 \u00d7 4 kernels and 2 \u00d7 2 strides\n    (activated by ReLU, channels are 16, 32, 64, 128, initialized with Gaussian weights, trained jointly with the full\n    model) to encode image-space conditions ... into feature maps ...\"\n    \"\"\"\n\n    def __init__(\n        self,\n        conditioning_embedding_channels: int,\n        conditioning_channels: int = 3,\n        block_out_channels: Tuple[int] = (16, 32, 96, 256),\n    ):\n        super().__init__()\n        self.conditioning_channels = conditioning_channels\n        if self.conditioning_channels % 4 != 0:\n            conditioning_channels = self.conditioning_channels + (4 - (self.conditioning_channels % 4))\n        else:\n            conditioning_channels = self.conditioning_channels\n        print(f\"Conditioning channels: {conditioning_channels}\")\n        self.conv_in = nn.Conv2dBias(conditioning_channels, block_out_channels[0], kernel_size=3, padding=1, stride=1)\n\n        self.blocks = nn.ModuleList([])\n        self.blocks.append(nn.Conv2dBias(16, 16, 3, 1, 1))\n        self.blocks.append(nn.Conv2dBias(16, 32, 3, 2, 1))\n        self.blocks.append(nn.Conv2dBias(32, 32, 3, 1, 1))\n        self.blocks.append(nn.Conv2dBias(32, 96, 3, 2, 1))\n        self.blocks.append(nn.Conv2dBias(96, 96, 3, 1, 1))\n        self.blocks.append(nn.Conv2dBias(96, 256, 3, 2, 1))\n\n        self.conv_out = nn.Conv2dBias(256, 320, 3, 1, 1)\n\n    def forward(self, conditioning):\n        \"\"\"\n        Padding required!\n        \"\"\"\n        conditioning = ops.pad_last_dim(4, 4)(conditioning)\n        embedding = self.conv_in(conditioning)\n        embedding = ops.silu(embedding)\n\n        for block in self.blocks:\n            embedding = block(embedding)\n            embedding = ops.silu(embedding)\n\n        embedding = self.conv_out(embedding)\n\n        return embedding", "class ControlNetConditioningEmbedding(nn.Module):\n    \"\"\"\n    Quoting from https://arxiv.org/abs/2302.05543: \"Stable Diffusion uses a pre-processing method similar to VQ-GAN\n    [11] to convert the entire dataset of 512 \u00d7 512 images into smaller 64 \u00d7 64 \u201clatent images\u201d for stabilized\n    training. This requires ControlNets to convert image-based conditions to 64 \u00d7 64 feature space to match the\n    convolution size. We use a tiny network E(\u00b7) of four convolution layers with 4 \u00d7 4 kernels and 2 \u00d7 2 strides\n    (activated by ReLU, channels are 16, 32, 64, 128, initialized with Gaussian weights, trained jointly with the full\n    model) to encode image-space conditions ... into feature maps ...\"\n    \"\"\"\n\n    def __init__(\n        self,\n        conditioning_embedding_channels: int,\n        conditioning_channels: int = 3,\n        block_out_channels: Tuple[int] = (16, 32, 96, 256),\n    ):\n        super().__init__()\n        self.conditioning_channels = conditioning_channels\n        if self.conditioning_channels % 4 != 0:\n            conditioning_channels = self.conditioning_channels + (4 - (self.conditioning_channels % 4))\n        else:\n            conditioning_channels = self.conditioning_channels\n        print(f\"Conditioning channels: {conditioning_channels}\")\n        self.conv_in = nn.Conv2dBias(conditioning_channels, block_out_channels[0], kernel_size=3, padding=1, stride=1)\n\n        self.blocks = nn.ModuleList([])\n        self.blocks.append(nn.Conv2dBias(16, 16, 3, 1, 1))\n        self.blocks.append(nn.Conv2dBias(16, 32, 3, 2, 1))\n        self.blocks.append(nn.Conv2dBias(32, 32, 3, 1, 1))\n        self.blocks.append(nn.Conv2dBias(32, 96, 3, 2, 1))\n        self.blocks.append(nn.Conv2dBias(96, 96, 3, 1, 1))\n        self.blocks.append(nn.Conv2dBias(96, 256, 3, 2, 1))\n\n        self.conv_out = nn.Conv2dBias(256, 320, 3, 1, 1)\n\n    def forward(self, conditioning):\n        \"\"\"\n        Padding required!\n        \"\"\"\n        conditioning = ops.pad_last_dim(4, 4)(conditioning)\n        embedding = self.conv_in(conditioning)\n        embedding = ops.silu(embedding)\n\n        for block in self.blocks:\n            embedding = block(embedding)\n            embedding = ops.silu(embedding)\n\n        embedding = self.conv_out(embedding)\n\n        return embedding", "\n\nclass ControlNetModel(nn.Module):\n    _supports_gradient_checkpointing = True\n\n    def __init__(\n        self,\n        in_channels: int = 4,\n        flip_sin_to_cos: bool = True,\n        freq_shift: int = 0,\n        down_block_types: Tuple[str] = (\n            \"CrossAttnDownBlock2D\",\n            \"CrossAttnDownBlock2D\",\n            \"CrossAttnDownBlock2D\",\n            \"DownBlock2D\",\n        ),\n        block_out_channels: Tuple[int] = (320, 640, 1280, 1280),\n        layers_per_block: int = 2,\n        downsample_padding: int = 1,\n        mid_block_scale_factor: float = 1,\n        act_fn: str = \"silu\",\n        norm_num_groups: Optional[int] = 32,\n        norm_eps: float = 1e-5,\n        cross_attention_dim: int = 768,\n        transformer_layers_per_block: Union[int, Tuple[int]] = 1,\n        attention_head_dim: Union[int, Tuple[int]] = 8,\n        num_attention_heads: Optional[Union[int, Tuple[int]]] = None,\n        use_linear_projection: bool = False,\n        class_embed_type: Optional[str] = None,\n        addition_embed_type: Optional[str] = None,\n        num_class_embeds: Optional[int] = None,\n        upcast_attention: bool = False,\n        resnet_time_scale_shift: str = \"default\",\n        projection_class_embeddings_input_dim: Optional[int] = None,\n        controlnet_conditioning_channel_order: str = \"rgb\",\n        global_pool_conditions: bool = False,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        self.controlnet_conditioning_channel_order = (\n            controlnet_conditioning_channel_order\n        )\n        self.global_pool_conditions = global_pool_conditions\n\n        # input\n        print(f\"Input channels: {in_channels}\")\n        self.conv_in = nn.Conv2dBias(in_channels, block_out_channels[0], 3, 1, 1)\n\n        # time\n        time_embed_dim = block_out_channels[0] * 4\n\n        self.time_proj = Timesteps(block_out_channels[0], flip_sin_to_cos, freq_shift)\n        timestep_input_dim = block_out_channels[0]\n\n        self.time_embedding = TimestepEmbedding(\n            timestep_input_dim,\n            time_embed_dim,\n        )\n        self.class_embed_type = class_embed_type\n        if class_embed_type is None and num_class_embeds is not None:\n            self.class_embedding = nn.Embedding(\n                [num_class_embeds, time_embed_dim], dtype=dtype\n            )\n        elif class_embed_type == \"timestep\":\n            self.class_embedding = TimestepEmbedding(\n                timestep_input_dim, time_embed_dim, dtype=dtype\n            )\n        elif class_embed_type == \"identity\":\n            self.class_embedding = nn.Identity(dtype=dtype)\n        else:\n            self.class_embedding = None\n\n        if addition_embed_type == \"text_time\":\n            # self.add_time_proj = Timesteps(addition_time_embed_dim, flip_sin_to_cos, freq_shift, dtype=dtype, arange_name=\"add_arange\")\n            self.add_embedding = TimestepEmbedding(\n                projection_class_embeddings_input_dim, time_embed_dim, dtype=dtype\n            )\n\n        # control net conditioning embedding\n        self.controlnet_cond_embedding = ControlNetConditioningEmbedding(block_out_channels[0])\n\n        self.down_blocks = nn.ModuleList([])\n        self.controlnet_down_blocks = nn.ModuleList([])\n\n        if isinstance(attention_head_dim, int):\n            attention_head_dim = (attention_head_dim,) * len(down_block_types)\n\n        # down\n        output_channel = block_out_channels[0]\n\n        controlnet_block = nn.Conv2dBias(output_channel, output_channel, 1)\n        controlnet_block = controlnet_block\n        self.controlnet_down_blocks.append(controlnet_block)\n\n        for i, down_block_type in enumerate(down_block_types):\n            input_channel = output_channel\n            output_channel = block_out_channels[i]\n            is_final_block = i == len(block_out_channels) - 1\n\n            down_block = get_down_block(\n                down_block_type,\n                num_layers=layers_per_block,\n                transformer_layers_per_block=transformer_layers_per_block[i],\n                in_channels=input_channel,\n                out_channels=output_channel,\n                temb_channels=time_embed_dim,\n                add_downsample=not is_final_block,\n                resnet_eps=norm_eps,\n                resnet_act_fn=act_fn,\n                cross_attention_dim=cross_attention_dim,\n                attn_num_head_channels=attention_head_dim[i],\n                downsample_padding=downsample_padding,\n                use_linear_projection=use_linear_projection,\n                dtype=dtype,\n            )\n            self.down_blocks.append(down_block)\n\n            for _ in range(layers_per_block):\n                controlnet_block = nn.Conv2dBias(output_channel, output_channel, 1)\n                controlnet_block = controlnet_block\n                self.controlnet_down_blocks.append(controlnet_block)\n\n            if not is_final_block:\n                controlnet_block = nn.Conv2dBias(output_channel, output_channel, 1)\n                controlnet_block = controlnet_block\n                self.controlnet_down_blocks.append(controlnet_block)\n\n        # mid\n        mid_block_channel = block_out_channels[-1]\n\n        controlnet_block = nn.Conv2dBias(mid_block_channel, mid_block_channel, 1)\n        controlnet_block = controlnet_block\n        self.controlnet_mid_block = controlnet_block\n\n        self.mid_block = UNetMidBlock2DCrossAttn(\n            transformer_layers_per_block=transformer_layers_per_block[-1],\n            in_channels=mid_block_channel,\n            temb_channels=time_embed_dim,\n            resnet_eps=norm_eps,\n            resnet_act_fn=act_fn,\n            output_scale_factor=mid_block_scale_factor,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n            cross_attention_dim=cross_attention_dim,\n            attn_num_head_channels=attention_head_dim[-1],\n            resnet_groups=norm_num_groups,\n            use_linear_projection=use_linear_projection,\n            dtype=dtype,\n        )\n\n    def get_shape(self, sample):\n        return [i._attrs[\"int_var\"]._attrs[\"values\"][0] for i in ops.size()(sample)]\n\n    def forward(\n        self,\n        sample,\n        timestep,\n        encoder_hidden_states,\n        controlnet_cond,\n        conditioning_scale: float = 1.0,\n        class_labels: Optional[Tensor] = None,\n        add_embeds: Optional[Tensor] = None,\n    ) -> Tuple:\n        print(\"sample shape\", self.get_shape(sample))\n        batch = sample._attrs['shape'][0]\n        t_emb = self.time_proj(timestep)\n        emb = self.time_embedding(t_emb)\n        if self.class_embedding is not None:\n            if class_labels is None:\n                raise ValueError(\n                    \"class_labels should be provided when num_class_embeds > 0\"\n                )\n\n            if self.class_embed_type == \"timestep\":\n                class_labels = self.time_proj(class_labels)\n\n            class_emb = ops.batch_gather()(\n                self.class_embedding.weight.tensor(), class_labels\n            )\n            emb = emb + class_emb\n\n        if add_embeds is not None:\n            aug_emb = self.add_embedding(add_embeds)\n            emb = emb + aug_emb\n            print(\"emb:\", self.get_shape(emb))\n        # 2. pre-process\n        sample = self.conv_in(sample)\n        print(\"sample after\", self.get_shape(sample))\n        print(\"controlnet_cond before: \", self.get_shape(controlnet_cond))\n        controlnet_cond = self.controlnet_cond_embedding(controlnet_cond)\n        print(\"controlnet_cond after: \", self.get_shape(controlnet_cond))\n        shape = sample._attrs[\"shape\"]\n        controlnet_cond._attrs[\"shape\"] = sample._attrs[\"shape\"]\n        sample = sample + controlnet_cond\n        # sample._attrs['shape'] = shape\n        # 3. down\n        down_block_res_samples = (sample,)\n        for downsample_block in self.down_blocks:\n            if isinstance(downsample_block, CrossAttnDownBlock2D):\n                sample, res_samples = downsample_block(\n                    hidden_states=sample,\n                    temb=emb,\n                    encoder_hidden_states=encoder_hidden_states,\n                )\n            else:\n                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n\n            down_block_res_samples += res_samples\n\n        # return sample\n\n        # 4. mid\n        sample = self.mid_block(\n            sample, emb, encoder_hidden_states=encoder_hidden_states\n        )\n        controlnet_down_block_res_samples = ()\n\n        for down_block_res_sample, controlnet_block in zip(\n            down_block_res_samples, self.controlnet_down_blocks\n        ):\n            down_block_res_sample = controlnet_block(down_block_res_sample)\n            controlnet_down_block_res_samples = controlnet_down_block_res_samples + (\n                down_block_res_sample,\n            )\n\n        down_block_res_samples = controlnet_down_block_res_samples\n        mid_block_res_sample = self.controlnet_mid_block(sample)\n\n        down_block_res_samples = [\n            sample * conditioning_scale for sample in down_block_res_samples\n        ]\n        mid_block_res_sample = mid_block_res_sample * conditioning_scale\n\n        output = ()\n\n        for i in range(len(down_block_res_samples)):\n            down_block_res_samples[i]._attrs[\"is_output\"] = True\n            down_block_res_samples[i]._attrs[\"name\"] = f\"down_block_res_sample_{i}\"\n            output += (down_block_res_samples[i],)\n        mid_block_res_sample._attrs[\"is_output\"] = True\n        mid_block_res_sample._attrs[\"name\"] = \"mid_block_res_sample\"\n        output += (mid_block_res_sample,)\n\n        return output", ""]}
{"filename": "AITemplate/ait/modeling/__init__.py", "chunked_list": ["from .vae import AutoencoderKL as AIT_AutoencoderKL\n\n__all__ = [\"AIT_AutoencoderKL\"]\n"]}
{"filename": "AITemplate/ait/modeling/vae.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\"\"\"\nTranslated from https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py.\n\"\"\"\n\nfrom typing import Tuple", "\nfrom typing import Tuple\n\nfrom aitemplate.frontend import nn, Tensor\nfrom aitemplate.compiler import ops\n\nfrom .unet_blocks import get_down_block, get_up_block, UNetMidBlock2D\n\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        batch_size,\n        height,\n        width,\n        in_channels=3,\n        out_channels=3,\n        up_block_types=(\"UpDecoderBlock2D\",),\n        block_out_channels=(64,),\n        layers_per_block=2,\n        act_fn=\"silu\",\n        dtype=\"float16\"\n    ):\n        super().__init__()\n        self.layers_per_block = layers_per_block\n\n        self.conv_in = nn.Conv2dBias(\n            in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1, dtype=dtype\n        )\n\n        # mid\n        self.mid_block = UNetMidBlock2D(\n            batch_size,\n            height,\n            width,\n            in_channels=block_out_channels[-1],\n            resnet_eps=1e-6,\n            resnet_act_fn=act_fn,\n            output_scale_factor=1,\n            resnet_time_scale_shift=\"default\",\n            attn_num_head_channels=None,\n            resnet_groups=32,\n            temb_channels=None,\n            dtype=dtype\n        )\n\n        # up\n        self.up_blocks = nn.ModuleList([])\n        reversed_block_out_channels = list(reversed(block_out_channels))\n        output_channel = reversed_block_out_channels[0]\n        for i, up_block_type in enumerate(up_block_types):\n            prev_output_channel = output_channel\n            output_channel = reversed_block_out_channels[i]\n\n            is_final_block = i == len(block_out_channels) - 1\n\n            up_block = get_up_block(\n                up_block_type,\n                num_layers=self.layers_per_block + 1,\n                in_channels=prev_output_channel,\n                out_channels=output_channel,\n                prev_output_channel=None,\n                temb_channels=None,\n                add_upsample=not is_final_block,\n                resnet_eps=1e-6,\n                resnet_act_fn=act_fn,\n                attn_num_head_channels=None,\n                dtype=dtype\n            )\n            self.up_blocks.append(up_block)\n            prev_output_channel = output_channel\n\n        # out\n        num_groups_out = 32\n        self.conv_norm_out = nn.GroupNorm(\n            num_channels=block_out_channels[0],\n            num_groups=num_groups_out,\n            eps=1e-6,\n            use_swish=True,\n            dtype=dtype\n        )\n        self.conv_out = nn.Conv2dBias(\n            block_out_channels[0], out_channels, kernel_size=3, padding=1, stride=1,dtype=dtype\n        )\n\n    def forward(self, z) -> Tensor:\n        sample = z\n        sample = self.conv_in(sample)\n\n        # middle\n        sample = self.mid_block(sample)\n\n        # up\n        for up_block in self.up_blocks:\n            sample = up_block(sample)\n\n        sample = self.conv_norm_out(sample)\n        sample = self.conv_out(sample)\n\n        return sample", "\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        batch_size,\n        height,\n        width,\n        in_channels=3,\n        out_channels=3,\n        up_block_types=(\"UpDecoderBlock2D\",),\n        block_out_channels=(64,),\n        layers_per_block=2,\n        act_fn=\"silu\",\n        dtype=\"float16\"\n    ):\n        super().__init__()\n        self.layers_per_block = layers_per_block\n\n        self.conv_in = nn.Conv2dBias(\n            in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1, dtype=dtype\n        )\n\n        # mid\n        self.mid_block = UNetMidBlock2D(\n            batch_size,\n            height,\n            width,\n            in_channels=block_out_channels[-1],\n            resnet_eps=1e-6,\n            resnet_act_fn=act_fn,\n            output_scale_factor=1,\n            resnet_time_scale_shift=\"default\",\n            attn_num_head_channels=None,\n            resnet_groups=32,\n            temb_channels=None,\n            dtype=dtype\n        )\n\n        # up\n        self.up_blocks = nn.ModuleList([])\n        reversed_block_out_channels = list(reversed(block_out_channels))\n        output_channel = reversed_block_out_channels[0]\n        for i, up_block_type in enumerate(up_block_types):\n            prev_output_channel = output_channel\n            output_channel = reversed_block_out_channels[i]\n\n            is_final_block = i == len(block_out_channels) - 1\n\n            up_block = get_up_block(\n                up_block_type,\n                num_layers=self.layers_per_block + 1,\n                in_channels=prev_output_channel,\n                out_channels=output_channel,\n                prev_output_channel=None,\n                temb_channels=None,\n                add_upsample=not is_final_block,\n                resnet_eps=1e-6,\n                resnet_act_fn=act_fn,\n                attn_num_head_channels=None,\n                dtype=dtype\n            )\n            self.up_blocks.append(up_block)\n            prev_output_channel = output_channel\n\n        # out\n        num_groups_out = 32\n        self.conv_norm_out = nn.GroupNorm(\n            num_channels=block_out_channels[0],\n            num_groups=num_groups_out,\n            eps=1e-6,\n            use_swish=True,\n            dtype=dtype\n        )\n        self.conv_out = nn.Conv2dBias(\n            block_out_channels[0], out_channels, kernel_size=3, padding=1, stride=1,dtype=dtype\n        )\n\n    def forward(self, z) -> Tensor:\n        sample = z\n        sample = self.conv_in(sample)\n\n        # middle\n        sample = self.mid_block(sample)\n\n        # up\n        for up_block in self.up_blocks:\n            sample = up_block(sample)\n\n        sample = self.conv_norm_out(sample)\n        sample = self.conv_out(sample)\n\n        return sample", "\n\nclass Encoder(nn.Module):\n    def __init__(\n        self,\n        batch_size,\n        height,\n        width,\n        in_channels=3,\n        out_channels=3,\n        down_block_types=(\"DownEncoderBlock2D\",),\n        block_out_channels=(64,),\n        layers_per_block=2,\n        norm_num_groups=32,\n        act_fn=\"silu\",\n        double_z=True,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        self.layers_per_block = layers_per_block\n\n        self.conv_in = nn.Conv2dBiasFewChannels(\n            in_channels,\n            block_out_channels[0],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            dtype=dtype,\n        )\n\n        self.mid_block = None\n        self.down_blocks = nn.ModuleList([])\n\n        # down\n        output_channel = block_out_channels[0]\n        for i, down_block_type in enumerate(down_block_types):\n            input_channel = output_channel\n            output_channel = block_out_channels[i]\n            is_final_block = i == len(block_out_channels) - 1\n\n            down_block = get_down_block(\n                down_block_type,\n                num_layers=self.layers_per_block,\n                in_channels=input_channel,\n                out_channels=output_channel,\n                add_downsample=not is_final_block,\n                resnet_eps=1e-6,\n                downsample_padding=0,\n                resnet_act_fn=act_fn,\n                resnet_groups=norm_num_groups,\n                attn_num_head_channels=None,\n                temb_channels=None,\n                dtype=dtype,\n            )\n            self.down_blocks.append(down_block)\n\n        # mid\n        self.mid_block = UNetMidBlock2D(\n            batch_size,\n            height,\n            width,\n            in_channels=block_out_channels[-1],\n            resnet_eps=1e-6,\n            resnet_act_fn=act_fn,\n            output_scale_factor=1,\n            resnet_time_scale_shift=\"default\",\n            attn_num_head_channels=None,\n            resnet_groups=norm_num_groups,\n            temb_channels=None,\n            dtype=dtype,\n        )\n\n        # out\n        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[-1], num_groups=norm_num_groups, eps=1e-6, dtype=dtype)\n        self.conv_act = ops.silu\n\n        conv_out_channels = 2 * out_channels if double_z else out_channels\n        self.conv_out = nn.Conv2dBias(block_out_channels[-1], conv_out_channels, kernel_size=3, stride=1, padding=1, dtype=dtype)\n\n    def forward(self, x):\n        sample = x\n        \n        sample = self.conv_in(sample)\n\n        for down_block in self.down_blocks:\n            sample = down_block(sample)\n\n        # middle\n        sample = self.mid_block(sample)\n\n        # post-process\n        sample = self.conv_norm_out(sample)\n        sample = self.conv_act(sample)\n        sample = self.conv_out(sample)\n\n        return sample", "\n\nclass AutoencoderKL(nn.Module):\n    def __init__(\n        self,\n        batch_size: int,\n        height: int,\n        width: int,\n        in_channels: int = 3,\n        out_channels: int = 3,\n        down_block_types: Tuple[str] = (\"DownEncoderBlock2D\",),\n        up_block_types: Tuple[str] = (\"UpDecoderBlock2D\",),\n        block_out_channels: Tuple[int] = (64,),\n        layers_per_block: int = 1,\n        act_fn: str = \"silu\",\n        latent_channels: int = 4,\n        norm_num_groups: int = 32,\n        sample_size: int = 32,\n        dtype=\"float16\"\n    ):\n        super().__init__()\n        self.decoder = Decoder(\n            batch_size,\n            height,\n            width,\n            in_channels=latent_channels,\n            out_channels=out_channels,\n            up_block_types=up_block_types,\n            block_out_channels=block_out_channels,\n            layers_per_block=layers_per_block,\n            act_fn=act_fn,\n            dtype=dtype\n        )\n        self.post_quant_conv = nn.Conv2dBias(\n            latent_channels, latent_channels, kernel_size=1, stride=1, padding=0, dtype=dtype\n        )\n\n        self.encoder = Encoder(\n            batch_size,\n            height,\n            width,\n            in_channels=in_channels,\n            out_channels=latent_channels,\n            down_block_types=down_block_types,\n            block_out_channels=block_out_channels,\n            layers_per_block=layers_per_block,\n            act_fn=act_fn,\n            norm_num_groups=norm_num_groups,\n            double_z=True,\n            dtype=dtype,\n        )\n        self.quant_conv = nn.Conv2dBias(\n            2 * latent_channels, 2 * latent_channels, kernel_size=1, stride=1, padding=0, dtype=dtype\n        )\n\n    def decode(self, z: Tensor, return_dict: bool = True):\n        z = self.post_quant_conv(z)\n        dec = self.decoder(z)\n        dec._attrs[\"is_output\"] = True\n        dec._attrs[\"name\"] = \"pixels\"\n        return dec\n\n    def encode(self, x: Tensor, sample: Tensor = None, return_dict: bool = True, deterministic: bool = False):\n        h = self.encoder(x)\n        moments = self.quant_conv(h)\n        if sample is None:\n            return moments\n        mean, logvar = ops.chunk()(moments, 2, dim=3)\n        logvar = ops.clamp()(logvar, -30.0, 20.0)\n        std = ops.exp(0.5 * logvar)\n        var = ops.exp(logvar)\n        if deterministic:\n            var = std = Tensor(mean.shape(), value=0.0, dtype=mean._attrs[\"dtype\"])\n        sample._attrs[\"shape\"] = mean._attrs[\"shape\"]\n        std._attrs[\"shape\"] = mean._attrs[\"shape\"]\n        z = mean + std * sample\n        z._attrs[\"is_output\"] = True\n        z._attrs[\"name\"] = \"latent\"\n        return z", "\n\n"]}
{"filename": "AITemplate/ait/modeling/clip.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom inspect import isfunction\nfrom typing import Optional\n\nfrom aitemplate.compiler import ops", "\nfrom aitemplate.compiler import ops\nfrom aitemplate.frontend import nn, Tensor\nfrom aitemplate.testing import detect_target\n\n# pylint: disable=W0102\n\nUSE_CUDA = detect_target().name() == \"cuda\"\n\n\ndef get_shape(x):\n    shape = [it.value() for it in x._attrs[\"shape\"]]\n    return shape", "\n\ndef get_shape(x):\n    shape = [it.value() for it in x._attrs[\"shape\"]]\n    return shape\n\n\ndef exists(val):\n    return val is not None\n", "\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\n\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        query_dim,\n        context_dim=None,\n        heads=8,\n        dim_head=64,\n        dropout=0.0,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        self.dim_head = dim_head\n\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False, dtype=dtype)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False, dtype=dtype)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False, dtype=dtype)\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, query_dim, dtype=dtype),\n            nn.Dropout(dropout, dtype=dtype),\n        )\n\n    def forward(self, x, context=None, mask=None, residual=None):\n        nheads = self.heads\n        d = self.dim_head\n\n        q = self.to_q(x)\n        context = default(context, x)\n        k = self.to_k(context)\n        v = self.to_v(context)\n\n        bs = q.shape()[0]\n\n        q = ops.reshape()(q, [bs, -1, self.heads, self.dim_head])\n        k = ops.reshape()(k, [bs, -1, self.heads, self.dim_head])\n        v = ops.reshape()(v, [bs, -1, self.heads, self.dim_head])\n        q = ops.permute()(q, [0, 2, 1, 3])\n        k = ops.permute()(k, [0, 2, 1, 3])\n        v = ops.permute()(v, [0, 2, 1, 3])\n\n        attn_op = ops.mem_eff_attention(causal=False)\n        out = attn_op(\n            (ops.reshape()(q, [bs, nheads, -1, d])),\n            (ops.reshape()(k, [bs, nheads, -1, d])),\n            (ops.reshape()(v, [bs, nheads, -1, d])),\n        )\n        out = ops.reshape()(out, [bs, -1, nheads * d])\n        proj = self.to_out(out)\n        proj = ops.reshape()(proj, [bs, -1, nheads * d])\n        if residual is not None:\n            return proj + residual\n        else:\n            return proj", "class CrossAttention(nn.Module):\n    def __init__(\n        self,\n        query_dim,\n        context_dim=None,\n        heads=8,\n        dim_head=64,\n        dropout=0.0,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        inner_dim = dim_head * heads\n        context_dim = default(context_dim, query_dim)\n\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        self.dim_head = dim_head\n\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=False, dtype=dtype)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=False, dtype=dtype)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=False, dtype=dtype)\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, query_dim, dtype=dtype),\n            nn.Dropout(dropout, dtype=dtype),\n        )\n\n    def forward(self, x, context=None, mask=None, residual=None):\n        nheads = self.heads\n        d = self.dim_head\n\n        q = self.to_q(x)\n        context = default(context, x)\n        k = self.to_k(context)\n        v = self.to_v(context)\n\n        bs = q.shape()[0]\n\n        q = ops.reshape()(q, [bs, -1, self.heads, self.dim_head])\n        k = ops.reshape()(k, [bs, -1, self.heads, self.dim_head])\n        v = ops.reshape()(v, [bs, -1, self.heads, self.dim_head])\n        q = ops.permute()(q, [0, 2, 1, 3])\n        k = ops.permute()(k, [0, 2, 1, 3])\n        v = ops.permute()(v, [0, 2, 1, 3])\n\n        attn_op = ops.mem_eff_attention(causal=False)\n        out = attn_op(\n            (ops.reshape()(q, [bs, nheads, -1, d])),\n            (ops.reshape()(k, [bs, nheads, -1, d])),\n            (ops.reshape()(v, [bs, nheads, -1, d])),\n        )\n        out = ops.reshape()(out, [bs, -1, nheads * d])\n        proj = self.to_out(out)\n        proj = ops.reshape()(proj, [bs, -1, nheads * d])\n        if residual is not None:\n            return proj + residual\n        else:\n            return proj", "\n\nclass GEGLU(nn.Module):\n    def __init__(self, dim_in, dim_out, dtype=\"float16\"):\n        super().__init__()\n        self.proj = nn.Linear(dim_in, dim_out, specialization=\"mul\", dtype=dtype)\n        self.gate = nn.Linear(dim_in, dim_out, specialization=\"fast_gelu\", dtype=dtype)\n\n    def forward(self, x):\n        return self.proj(x, self.gate(x))", "\n\nclass FeedForward(nn.Module):\n    def __init__(\n        self, dim, dim_out=None, mult=4, glu=False, dropout=0.0, dtype=\"float16\"\n    ):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        project_in = (\n            nn.Sequential(\n                nn.Linear(dim, inner_dim, specialization=\"fast_gelu\", dtype=dtype),\n            )\n            if not glu\n            else GEGLU(dim, inner_dim, dtype=dtype)\n        )\n\n        self.net = nn.Sequential(\n            project_in,\n            nn.Dropout(dropout, dtype=dtype),\n            nn.Linear(inner_dim, dim_out, dtype=dtype),\n        )\n\n    def forward(self, x, residual=None):\n        shape = ops.size()(x)\n        x = self.net(x)\n        x = ops.reshape()(x, shape)\n        if residual is not None:\n            return x + residual\n        else:\n            return x", "\n\nclass BasicTransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        n_heads,\n        d_head,\n        dropout=0.0,\n        context_dim=None,\n        gated_ff=True,\n        checkpoint=True,\n        only_cross_attention=False,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        self.only_cross_attention = only_cross_attention\n        self.attn1 = CrossAttention(\n            query_dim=dim,\n            context_dim=context_dim if only_cross_attention else None,\n            heads=n_heads,\n            dim_head=d_head,\n            dropout=dropout,\n            dtype=dtype,\n        )\n        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff, dtype=dtype)\n        if context_dim is not None:\n            self.attn2 = CrossAttention(\n                query_dim=dim,\n                context_dim=context_dim,\n                heads=n_heads,\n                dim_head=d_head,\n                dropout=dropout,\n                dtype=dtype,\n            )\n        else:\n            self.attn2 = None\n        self.norm1 = nn.LayerNorm(dim, dtype=dtype)\n        self.norm2 = nn.LayerNorm(dim, dtype=dtype)\n        self.norm3 = nn.LayerNorm(dim, dtype=dtype)\n        self.checkpoint = checkpoint\n\n        self.param = (dim, n_heads, d_head, context_dim, gated_ff, checkpoint)\n\n    def forward(self, x, context=None):\n        x = self.attn1(\n            self.norm1(x),\n            residual=x,\n            context=context if self.only_cross_attention else None,\n        )\n        if self.attn2 is not None:\n            x = self.attn2(self.norm2(x), context=context, residual=x)\n        x = self.ff(self.norm3(x), residual=x)\n        return x", "\n\ndef Normalize(in_channels, dtype=\"float16\"):\n    return nn.GroupNorm(\n        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True, dtype=dtype\n    )\n\n\nclass SpatialTransformer(nn.Module):\n    \"\"\"\n    Transformer block for image-like data.\n    First, project the input (aka embedding)\n    and reshape to b, t, d.\n    Then apply standard transformer action.\n    Finally, reshape to image\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        n_heads,\n        d_head,\n        depth=1,\n        dropout=0.0,\n        context_dim=None,\n        use_linear_projection=False,\n        only_cross_attention=False,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        inner_dim = n_heads * d_head\n        self.norm = Normalize(in_channels, dtype=dtype)  # Group Norm\n        self.use_linear_projection = use_linear_projection\n\n        if use_linear_projection:\n            self.proj_in = nn.Linear(in_channels, inner_dim, dtype=dtype)\n        else:\n            self.proj_in = nn.Conv2dBias(\n                in_channels, inner_dim, kernel_size=1, stride=1, padding=0, dtype=dtype\n            )\n\n        self.transformer_blocks = nn.ModuleList(\n            [\n                BasicTransformerBlock(\n                    inner_dim,\n                    n_heads,\n                    d_head,\n                    dropout=dropout,\n                    context_dim=context_dim,\n                    only_cross_attention=only_cross_attention,\n                    dtype=dtype,\n                )\n                for d in range(depth)\n            ]\n        )\n\n        if use_linear_projection:\n            self.proj_out = nn.Linear(inner_dim, in_channels, dtype=dtype)\n        else:\n            self.proj_out = nn.Conv2dBias(\n                inner_dim, in_channels, kernel_size=1, stride=1, padding=0, dtype=dtype\n            )\n\n    def forward(self, x, context=None):\n        # note: if no context is given, cross-attention defaults to self-attention\n        b, h, w, c = x.shape()\n        x_in = x\n        x = self.norm(x)\n        if self.use_linear_projection:\n            x = ops.reshape()(x, [b, -1, c])\n            x = self.proj_in(x)\n        else:\n            x = self.proj_in(x)\n            x = ops.reshape()(x, [b, -1, c])\n\n        for block in self.transformer_blocks:\n            x = block(x, context=context)\n\n        if self.use_linear_projection:\n            x = self.proj_out(x)\n            x = ops.reshape()(x, [b, h, w, c])\n        else:\n            x = ops.reshape()(x, [b, h, w, c])\n            x = self.proj_out(x)\n        return x + x_in", "class SpatialTransformer(nn.Module):\n    \"\"\"\n    Transformer block for image-like data.\n    First, project the input (aka embedding)\n    and reshape to b, t, d.\n    Then apply standard transformer action.\n    Finally, reshape to image\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        n_heads,\n        d_head,\n        depth=1,\n        dropout=0.0,\n        context_dim=None,\n        use_linear_projection=False,\n        only_cross_attention=False,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        inner_dim = n_heads * d_head\n        self.norm = Normalize(in_channels, dtype=dtype)  # Group Norm\n        self.use_linear_projection = use_linear_projection\n\n        if use_linear_projection:\n            self.proj_in = nn.Linear(in_channels, inner_dim, dtype=dtype)\n        else:\n            self.proj_in = nn.Conv2dBias(\n                in_channels, inner_dim, kernel_size=1, stride=1, padding=0, dtype=dtype\n            )\n\n        self.transformer_blocks = nn.ModuleList(\n            [\n                BasicTransformerBlock(\n                    inner_dim,\n                    n_heads,\n                    d_head,\n                    dropout=dropout,\n                    context_dim=context_dim,\n                    only_cross_attention=only_cross_attention,\n                    dtype=dtype,\n                )\n                for d in range(depth)\n            ]\n        )\n\n        if use_linear_projection:\n            self.proj_out = nn.Linear(inner_dim, in_channels, dtype=dtype)\n        else:\n            self.proj_out = nn.Conv2dBias(\n                inner_dim, in_channels, kernel_size=1, stride=1, padding=0, dtype=dtype\n            )\n\n    def forward(self, x, context=None):\n        # note: if no context is given, cross-attention defaults to self-attention\n        b, h, w, c = x.shape()\n        x_in = x\n        x = self.norm(x)\n        if self.use_linear_projection:\n            x = ops.reshape()(x, [b, -1, c])\n            x = self.proj_in(x)\n        else:\n            x = self.proj_in(x)\n            x = ops.reshape()(x, [b, -1, c])\n\n        for block in self.transformer_blocks:\n            x = block(x, context=context)\n\n        if self.use_linear_projection:\n            x = self.proj_out(x)\n            x = ops.reshape()(x, [b, h, w, c])\n        else:\n            x = ops.reshape()(x, [b, h, w, c])\n            x = self.proj_out(x)\n        return x + x_in", "\n\nclass CLIPAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(\n        self,\n        hidden_size=768,\n        num_attention_heads=12,\n        attention_dropout=0.0,\n        batch_size=1,\n        seq_len=16,\n        layer_norm_eps=1e-5,\n        hidden_dropout_prob=0.0,\n        causal=False,\n        mask_seq=0,\n    ):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(\n            dim=hidden_size,\n            batch_size=batch_size,\n            seq_len=seq_len,\n            num_heads=num_attention_heads,\n            qkv_bias=True,\n            attn_drop=attention_dropout,\n            proj_drop=hidden_dropout_prob,\n            has_residual=False,\n            causal=causal,\n            mask_seq=mask_seq,\n        )\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        attention_mask: Optional[Tensor] = None,\n        causal_attention_mask: Optional[Tensor] = None,\n        output_attentions: Optional[bool] = False,\n        residual: Optional[Tensor] = None,\n    ):\n        if residual is not None:\n            self_output = self.attn(hidden_states, residual)\n        else:\n            self_output = self.attn(hidden_states)\n        return self_output", "\n\nclass QuickGELUActivation(nn.Module):\n    \"\"\"\n    Applies GELU approximation that is fast but somewhat inaccurate. See: https://github.com/hendrycks/GELUs\n    \"\"\"\n\n    def forward(self, x):\n        x1 = x * 1.702\n        x1 = ops.sigmoid(x1)\n        x = x * x1\n        return x", "\n\nclass CLIPMLP(nn.Module):\n    \"\"\"MLP as used in Vision Transformer, MLP-Mixer and related networks\"\"\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=\"GELU\",\n        drop=0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        self.fc1 = nn.Linear(\n            in_features,\n            hidden_features,\n            specialization=\"gelu\",\n        )\n        self.fc2 = nn.Linear(hidden_features, out_features, specialization=\"add\")\n\n    def forward(self, x, res):\n        shape = x.shape()\n        x = self.fc1(x)\n        x = self.fc2(x, res)\n        return ops.reshape()(x, shape)", "\n\nclass CLIPMLPQuickGelu(nn.Module):\n    \"\"\"MLP as used in Vision Transformer, MLP-Mixer and related networks\"\"\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        self.fc1 = nn.Linear(\n            in_features,\n            hidden_features,\n        )\n        self.activation_fn = QuickGELUActivation()\n\n        self.fc2 = nn.Linear(hidden_features, out_features, specialization=\"add\")\n\n    def forward(self, x, res):\n        # shape = get_shape(x)\n        x = self.fc1(x)\n        x = self.activation_fn(x)\n        x = self.fc2(x, res)\n        return ops.reshape()(x, x.shape())", "\n\nclass CLIPEncoderLayer(nn.Module):\n    ACT_LAYER_TO_CLIP_MLP_MAP = {\n        \"gelu\": CLIPMLP,\n        \"quick_gelu\": CLIPMLPQuickGelu,\n    }\n\n    def __init__(\n        self,\n        hidden_size=768,\n        num_attention_heads=12,\n        attention_dropout=0.0,\n        mlp_ratio=4.0,\n        batch_size=1,\n        seq_len=16,\n        causal=False,\n        mask_seq=0,\n        act_layer=\"gelu\",\n    ):\n        super().__init__()\n        self.embed_dim = hidden_size\n        self.self_attn = nn.CrossAttention(\n            hidden_size,\n            seq_len,\n            seq_len,\n            num_attention_heads,\n            qkv_bias=True,\n            causal=causal,\n        )\n\n        self.layer_norm1 = nn.LayerNorm(self.embed_dim)\n        self.mlp = self.ACT_LAYER_TO_CLIP_MLP_MAP[act_layer](\n            hidden_size, int(hidden_size * mlp_ratio)\n        )\n        self.layer_norm2 = nn.LayerNorm(self.embed_dim)\n\n    def forward(\n        self,\n        hidden_states: Tensor,\n        output_attentions: Optional[bool] = False,\n    ):\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n                `(config.encoder_attention_heads,)`.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n        \"\"\"\n        residual = hidden_states\n\n        hidden_states = self.layer_norm1(hidden_states)\n        hidden_states = self.self_attn(\n            hidden_states, hidden_states, hidden_states, residual\n        )\n\n        residual = hidden_states\n        hidden_states = self.layer_norm2(hidden_states)\n        hidden_states = self.mlp(hidden_states, residual)\n\n        return hidden_states", "\n\nclass CLIPEncoder(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n    [`CLIPEncoderLayer`].\n    Args:\n        config: CLIPConfig\n    \"\"\"\n\n    def __init__(\n        self,\n        num_hidden_layers=12,\n        output_attentions=False,\n        output_hidden_states=False,\n        use_return_dict=False,\n        hidden_size=768,\n        num_attention_heads=12,\n        batch_size=1,\n        seq_len=64,\n        causal=False,\n        mask_seq=0,\n        act_layer=\"gelu\",\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList(\n            [\n                CLIPEncoderLayer(\n                    hidden_size=hidden_size,\n                    num_attention_heads=num_attention_heads,\n                    batch_size=batch_size,\n                    seq_len=seq_len,\n                    causal=causal,\n                    mask_seq=mask_seq,\n                    act_layer=act_layer,\n                )\n                for _ in range(num_hidden_layers)\n            ]\n        )\n        self.output_attentions = output_attentions\n        self.output_hidden_states = output_hidden_states\n        self.use_return_dict = use_return_dict\n\n    def forward(\n        self,\n        inputs_embeds,\n        attention_mask: Optional[Tensor] = None,\n        causal_attention_mask: Optional[Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ):\n        r\"\"\"\n        Args:\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n                than the model's internal embedding lookup matrix.\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n                [What are attention masks?](../glossary#attention-mask)\n            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Causal mask for the text model. Mask values selected in `[0, 1]`:\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n                [What are attention masks?](../glossary#attention-mask)\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.use_return_dict\n\n        encoder_states = () if output_hidden_states else None\n        # all_attentions = () if output_attentions else None\n\n        hidden_states = inputs_embeds\n        for _, encoder_layer in enumerate(self.layers):\n            if output_hidden_states and encoder_states is not None:\n                encoder_states = encoder_states + (hidden_states,)\n            layer_outputs = encoder_layer(hidden_states)\n            hidden_states = layer_outputs\n\n        last_hidden_state = hidden_states\n        output = last_hidden_state\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n            output = encoder_states\n        return output", "\n\nclass CLIPTextEmbeddings(nn.Module):\n    def __init__(\n        self,\n        hidden_size=768,\n        vocab_size=49408,\n        max_position_embeddings=77,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        embed_dim = hidden_size\n        self.max_position_embeddings = max_position_embeddings\n        self.embed_dim = hidden_size\n        self.vocab_size = vocab_size\n\n        self.token_embedding = nn.Embedding(shape=[vocab_size, embed_dim], dtype=dtype)\n        self.position_embedding = nn.Embedding(\n            shape=[max_position_embeddings, embed_dim], dtype=dtype\n        )\n\n    def forward(\n        self,\n        input_ids: Tensor,\n        position_ids: Tensor,\n        inputs_embeds: Optional[Tensor] = None,\n    ) -> Tensor:\n        input_shape = ops.size()(input_ids)\n\n        # [B * S]\n        token_embedding = self.token_embedding.tensor()\n        token_embedding = ops.reshape()(\n            token_embedding, [1, self.vocab_size, self.embed_dim]\n        )\n        token_embedding = ops.expand()(token_embedding, [input_shape[0], -1, -1])\n\n        if inputs_embeds is None:\n            inputs_embeds = ops.batch_gather()(token_embedding, input_ids)\n\n        position_embedding = self.position_embedding.tensor()\n        position_embedding = ops.reshape()(\n            position_embedding, [1, self.max_position_embeddings, self.embed_dim]\n        )\n        position_embedding = ops.expand()(position_embedding, [input_shape[0], -1, -1])\n\n        position_embeddings = ops.batch_gather()(position_embedding, position_ids)\n\n        embeddings = inputs_embeds + position_embeddings\n\n        embeddings = ops.reshape()(embeddings, [input_shape[0], input_shape[1], -1])\n\n        return embeddings", "\n\nclass CLIPTextTransformer(nn.Module):\n    def __init__(\n        self,\n        hidden_size=768,\n        text_projection_dim=None,\n        output_attentions=False,\n        output_hidden_states=False,\n        use_return_dict=False,\n        num_hidden_layers=12,\n        num_attention_heads=12,\n        batch_size=1,\n        seq_len=64,\n        causal=False,\n        mask_seq=0,\n        act_layer=\"gelu\",\n    ):\n        super().__init__()\n        self.embeddings = CLIPTextEmbeddings(hidden_size=hidden_size)\n        self.encoder = CLIPEncoder(\n            num_hidden_layers=num_hidden_layers,\n            hidden_size=hidden_size,\n            num_attention_heads=num_attention_heads,\n            batch_size=batch_size,\n            seq_len=seq_len,\n            causal=causal,\n            mask_seq=mask_seq,\n            act_layer=act_layer,\n        )\n        self.final_layer_norm = nn.LayerNorm(hidden_size)\n        if text_projection_dim is not None:\n            self.text_projection = nn.Linear(\n                hidden_size, text_projection_dim, bias=False\n            )\n        else:\n            self.text_projection = None\n\n        self.output_attentions = output_attentions\n        self.output_hidden_states = output_hidden_states\n        self.use_return_dict = use_return_dict\n        self.hidden_size = hidden_size\n        self.seq_len = seq_len\n        self.num_layers = num_hidden_layers\n\n    def forward(\n        self,\n        input_ids: Optional[Tensor] = None,\n        attention_mask: Optional[Tensor] = None,\n        position_ids: Optional[Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ):\n        r\"\"\"\n        Returns:\n        \"\"\"\n        batch = ops.size()(input_ids)[0]\n\n        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n\n        encoder_output = self.encoder(\n            inputs_embeds=hidden_states, output_hidden_states=self.output_hidden_states\n        )\n        if self.output_hidden_states:\n            last_hidden_state = encoder_output[-1]\n        else:\n            last_hidden_state = encoder_output\n        last_hidden_state = self.final_layer_norm(last_hidden_state)\n\n        argmax = ops.argmax(-1)(input_ids)\n        pooled_output = ops.index_select(dim=1)(last_hidden_state, argmax)\n        pooled_output = ops.reshape()(pooled_output, [batch, self.hidden_size])\n        last_hidden_state._attrs[\"is_output\"] = True\n        last_hidden_state._attrs[\"name\"] = \"last_hidden_state\"\n        pooled_output._attrs[\"is_output\"] = True\n        pooled_output._attrs[\"name\"] = \"pooled_output\"\n        output = (\n            last_hidden_state,\n            pooled_output,\n        )\n        if self.text_projection is not None:\n            text_embeds = self.text_projection(pooled_output)\n            text_embeds._attrs[\"is_output\"] = True\n            text_embeds._attrs[\"name\"] = \"text_embeds\"\n            output = output + (text_embeds,)\n\n        if self.output_hidden_states:\n            for idx, hidden_state in enumerate(encoder_output[:-1]):\n                hidden_state._attrs[\"is_output\"] = True\n                hidden_state._attrs[\"name\"] = f\"hidden_state_{idx}\"\n                output = output + (hidden_state,)\n\n        return output"]}
{"filename": "AITemplate/ait/modeling/attention.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\n\"\"\"\nImplementations are translated from https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py.\n\"\"\"", "Implementations are translated from https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py.\n\"\"\"\n\nfrom typing import Optional\n\nfrom aitemplate.compiler.ops import reshape\nfrom aitemplate.frontend import nn, Tensor\n\n\nclass AttentionBlock(nn.Module):\n    \"\"\"\n    An attention block that allows spatial positions to attend to each other. Originally ported from here, but adapted\n    to the N-d case.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n    Uses three q, k, v linear layers to compute attention.\n    Parameters:\n        batch_size (:obj:`int`): The number of examples per batch.\n        height (:obj:`int`): Height of each image example.\n        width (:obj:`int`): Width of each image example.\n        channels (:obj:`int`): The number of channels in the input and output.\n        num_head_channels (:obj:`int`, *optional*):\n            The number of channels in each head. If None, then `num_heads` = 1.\n        num_groups (:obj:`int`, *optional*, defaults to 32): The number of groups to use for group norm.\n        eps (:obj:`float`, *optional*, defaults to 1e-5): The epsilon value to use for group norm.\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        height: int,\n        width: int,\n        channels: int,\n        num_head_channels: Optional[int] = None,\n        num_groups: int = 32,\n        rescale_output_factor: float = 1.0,\n        eps: float = 1e-5,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        self.batch_size = batch_size\n        self.channels = channels\n        self.num_heads = (\n            channels // num_head_channels if num_head_channels is not None else 1\n        )\n        self.num_head_size = num_head_channels\n        self.group_norm = nn.GroupNorm(num_groups, channels, eps, dtype=dtype)\n        self.attention = nn.CrossAttention(\n            channels,\n            height * width,\n            height * width,\n            self.num_heads,\n            qkv_bias=True,\n            dtype=dtype\n        )\n        self.rescale_output_factor = rescale_output_factor\n\n    def forward(self, hidden_states) -> Tensor:\n        \"\"\"\n        input hidden_states shape: [batch, height, width, channel]\n        output shape: [batch, height, width, channel]\n        \"\"\"\n\n        residual = hidden_states\n\n        # norm\n        hidden_states = self.group_norm(hidden_states)\n        o_shape = hidden_states.shape()\n        batch_dim = o_shape[0]\n\n        hidden_states = reshape()(\n            hidden_states,\n            [batch_dim, -1, self.channels],\n        )\n\n        res = self.attention(hidden_states, hidden_states, hidden_states, residual) * (\n            1 / self.rescale_output_factor\n        )\n\n        res = reshape()(res, o_shape)\n        return res", "\nclass AttentionBlock(nn.Module):\n    \"\"\"\n    An attention block that allows spatial positions to attend to each other. Originally ported from here, but adapted\n    to the N-d case.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n    Uses three q, k, v linear layers to compute attention.\n    Parameters:\n        batch_size (:obj:`int`): The number of examples per batch.\n        height (:obj:`int`): Height of each image example.\n        width (:obj:`int`): Width of each image example.\n        channels (:obj:`int`): The number of channels in the input and output.\n        num_head_channels (:obj:`int`, *optional*):\n            The number of channels in each head. If None, then `num_heads` = 1.\n        num_groups (:obj:`int`, *optional*, defaults to 32): The number of groups to use for group norm.\n        eps (:obj:`float`, *optional*, defaults to 1e-5): The epsilon value to use for group norm.\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        height: int,\n        width: int,\n        channels: int,\n        num_head_channels: Optional[int] = None,\n        num_groups: int = 32,\n        rescale_output_factor: float = 1.0,\n        eps: float = 1e-5,\n        dtype=\"float16\",\n    ):\n        super().__init__()\n        self.batch_size = batch_size\n        self.channels = channels\n        self.num_heads = (\n            channels // num_head_channels if num_head_channels is not None else 1\n        )\n        self.num_head_size = num_head_channels\n        self.group_norm = nn.GroupNorm(num_groups, channels, eps, dtype=dtype)\n        self.attention = nn.CrossAttention(\n            channels,\n            height * width,\n            height * width,\n            self.num_heads,\n            qkv_bias=True,\n            dtype=dtype\n        )\n        self.rescale_output_factor = rescale_output_factor\n\n    def forward(self, hidden_states) -> Tensor:\n        \"\"\"\n        input hidden_states shape: [batch, height, width, channel]\n        output shape: [batch, height, width, channel]\n        \"\"\"\n\n        residual = hidden_states\n\n        # norm\n        hidden_states = self.group_norm(hidden_states)\n        o_shape = hidden_states.shape()\n        batch_dim = o_shape[0]\n\n        hidden_states = reshape()(\n            hidden_states,\n            [batch_dim, -1, self.channels],\n        )\n\n        res = self.attention(hidden_states, hidden_states, hidden_states, residual) * (\n            1 / self.rescale_output_factor\n        )\n\n        res = reshape()(res, o_shape)\n        return res", ""]}
{"filename": "AITemplate/ait/modeling/resnet.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom aitemplate.compiler import ops\nfrom aitemplate.frontend import nn, Tensor\n\n\ndef get_shape(x):\n    shape = [it.value() for it in x._attrs[\"shape\"]]\n    return shape", "\n\ndef get_shape(x):\n    shape = [it.value() for it in x._attrs[\"shape\"]]\n    return shape\n\n\nclass Upsample2D(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n\n    :param channels: channels in the inputs and outputs. :param use_conv: a bool determining if a convolution is\n    applied. :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 upsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        use_conv=False,\n        use_conv_transpose=False,\n        out_channels=None,\n        name=\"conv\",\n        dtype=\"float16\"\n    ):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_conv_transpose = use_conv_transpose\n        self.name = name\n\n        conv = None\n        if use_conv_transpose:\n            conv = nn.ConvTranspose2dBias(channels, self.out_channels, 4, 2, 1, dtype=dtype)\n        elif use_conv:\n            conv = nn.Conv2dBias(self.channels, self.out_channels, 3, 1, 1, dtype=dtype)\n\n        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n        if name == \"conv\":\n            self.conv = conv\n        else:\n            self.Conv2d_0 = conv\n\n    def forward(self, x, upsample_size=None):\n        if self.use_conv_transpose:\n            return self.conv(x)\n        out = None\n        if upsample_size is not None:\n            out = ops.size()(x)\n            out[1] = upsample_size[1]\n            out[2] = upsample_size[2]\n            out = [x._attrs[\"int_var\"] for x in out]\n            out = Tensor(out)\n        x = nn.Upsampling2d(scale_factor=2.0, mode=\"nearest\")(x, out)\n\n        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n        if self.use_conv:\n            if self.name == \"conv\":\n                x = self.conv(x)\n            else:\n                x = self.Conv2d_0(x)\n\n        return x", "\n\nclass Downsample2D(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n\n    :param channels: channels in the inputs and outputs. :param use_conv: a bool determining if a convolution is\n    applied. :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(\n        self, channels, use_conv=False, out_channels=None, padding=1, name=\"conv\", dtype=\"float16\"\n    ):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.padding = padding\n        stride = 2\n        self.name = name\n        self.dtype = dtype\n\n        if use_conv:\n            conv = nn.Conv2dBias(\n                self.channels, self.out_channels, 3, stride=stride, dtype=dtype, padding=padding\n            )\n        else:\n            assert self.channels == self.out_channels\n            conv = nn.AvgPool2d(kernel_size=stride, stride=stride, padding=0)\n\n        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n        if name == \"conv\":\n            self.Conv2d_0 = conv\n            self.conv = conv\n        elif name == \"Conv2d_0\":\n            self.conv = conv\n        else:\n            self.conv = conv\n\n    def forward(self, hidden_states):\n        if self.use_conv and self.padding == 0:\n            padding = ops.full()([0, 1, 0, 0], 0.0, dtype=self.dtype)\n            padding._attrs[\"shape\"][0] = hidden_states._attrs[\"shape\"][0]\n            padding._attrs[\"shape\"][2] = hidden_states._attrs[\"shape\"][2]\n            padding._attrs[\"shape\"][3] = hidden_states._attrs[\"shape\"][3]\n            hidden_states = ops.concatenate()([hidden_states, padding], dim=1)\n            padding = ops.full()([0, 0, 1, 0], 0.0, dtype=self.dtype)\n            padding._attrs[\"shape\"][0] = hidden_states._attrs[\"shape\"][0]\n            padding._attrs[\"shape\"][1] = hidden_states._attrs[\"shape\"][1]\n            padding._attrs[\"shape\"][3] = hidden_states._attrs[\"shape\"][3]\n            hidden_states = ops.concatenate()([hidden_states, padding], dim=2)\n            \n        hidden_states = self.conv(hidden_states)\n        return hidden_states", "\n\nclass ResnetBlock2D(nn.Module):\n    def __init__(\n        self,\n        *,\n        in_channels,\n        out_channels=None,\n        conv_shortcut=False,\n        dropout=0.0,\n        temb_channels=512,\n        groups=32,\n        groups_out=None,\n        pre_norm=True,\n        eps=1e-6,\n        non_linearity=\"swish\",\n        time_embedding_norm=\"default\",\n        kernel=None,\n        output_scale_factor=1.0,\n        use_nin_shortcut=None,\n        up=False,\n        down=False,\n        dtype=\"float16\"\n    ):\n        super().__init__()\n        self.pre_norm = pre_norm\n        self.pre_norm = True\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n        self.time_embedding_norm = time_embedding_norm\n        self.up = up\n        self.down = down\n        self.output_scale_factor = output_scale_factor\n\n        if groups_out is None:\n            groups_out = groups\n\n        self.norm1 = nn.GroupNorm(\n            num_groups=groups,\n            num_channels=in_channels,\n            eps=eps,\n            affine=True,\n            use_swish=True,\n            dtype=dtype\n        )\n\n        self.conv1 = nn.Conv2dBias(\n            in_channels, out_channels, kernel_size=3, stride=1, padding=1, dtype=dtype\n        )\n\n        if temb_channels is not None:\n            self.time_emb_proj = nn.Linear(temb_channels, out_channels, dtype=dtype)\n        else:\n            self.time_emb_proj = None\n\n        self.norm2 = nn.GroupNorm(\n            num_groups=groups_out,\n            num_channels=out_channels,\n            eps=eps,\n            affine=True,\n            use_swish=True,\n            dtype=dtype\n        )\n        self.dropout = nn.Dropout(dropout, dtype=dtype)\n        self.conv2 = nn.Conv2dBias(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1, dtype=dtype\n        )\n\n        self.upsample = self.downsample = None\n\n        self.use_nin_shortcut = (\n            self.in_channels != self.out_channels\n            if use_nin_shortcut is None\n            else use_nin_shortcut\n        )\n\n        if self.use_nin_shortcut:\n            self.conv_shortcut = nn.Conv2dBias(\n                in_channels, out_channels, 1, 1, 0,dtype=dtype\n            )  # kernel_size=1, stride=1, padding=0) # conv_bias_add\n        else:\n            self.conv_shortcut = None\n\n    def forward(self, x, temb=None):\n        hidden_states = x\n\n        # make sure hidden states is in float32\n        # when running in half-precision\n        hidden_states = self.norm1(\n            hidden_states\n        )  # .float()).type(hidden_states.dtype) # fused swish\n        # hidden_states = self.nonlinearity(hidden_states)\n\n        if self.upsample is not None:\n            x = self.upsample(x)\n            hidden_states = self.upsample(hidden_states)\n        elif self.downsample is not None:\n            x = self.downsample(x)\n            hidden_states = self.downsample(hidden_states)\n\n        hidden_states = self.conv1(hidden_states)\n        bs, h, w, dim = hidden_states.shape()\n        if temb is not None:\n            temb = self.time_emb_proj(ops.silu(temb))\n            bs, dim = temb.shape()\n            temb = ops.reshape()(temb, [bs, 1, 1, dim])\n            hidden_states = hidden_states + temb\n\n        # make sure hidden states is in float32\n        # when running in half-precision\n        hidden_states = self.norm2(hidden_states)\n\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.conv2(hidden_states)\n\n        if self.conv_shortcut is not None:\n            x = self.conv_shortcut(x)\n\n        out = hidden_states + x\n\n        return out", ""]}
{"filename": "AITemplate/ait/compile/unet.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport sys\nimport torch\nfrom aitemplate.compiler import compile_model\nfrom aitemplate.frontend import IntVar, Tensor, DynamicProfileStrategy", "from aitemplate.compiler import compile_model\nfrom aitemplate.frontend import IntVar, Tensor, DynamicProfileStrategy\nfrom aitemplate.testing import detect_target\n\nfrom ..modeling.unet_2d_condition import (\n    UNet2DConditionModel as ait_UNet2DConditionModel,\n)\nfrom .util import mark_output\nfrom .release import process\nfrom ait.util.mapping import map_unet", "from .release import process\nfrom ait.util.mapping import map_unet\n\ndef compile_unet(\n    pt_mod,\n    batch_size=(1, 8),\n    height=(64, 2048),\n    width=(64, 2048),\n    clip_chunks=1,\n    out_dir=\"./out\",\n    work_dir=\"./tmp\",\n    dim=320,\n    hidden_dim=1024,\n    use_fp16_acc=False,\n    convert_conv_to_gemm=False,\n    controlnet=False,\n    attention_head_dim=[5, 10, 20, 20],  # noqa: B006\n    model_name=\"UNet2DConditionModel\",\n    use_linear_projection=False,\n    constants=True,\n    block_out_channels=(320, 640, 1280, 1280),\n    down_block_types= (\n        \"CrossAttnDownBlock2D\",\n        \"CrossAttnDownBlock2D\",\n        \"CrossAttnDownBlock2D\",\n        \"DownBlock2D\",\n    ),\n    up_block_types=(\n        \"UpBlock2D\",\n        \"CrossAttnUpBlock2D\",\n        \"CrossAttnUpBlock2D\",\n        \"CrossAttnUpBlock2D\",\n    ),\n    in_channels=4,\n    out_channels=4,\n    sample_size=64,\n    class_embed_type=None,\n    num_class_embeds=None,\n    only_cross_attention=[\n        True,\n        True,\n        True,\n        False\n    ],\n    down_factor=8,\n    time_embedding_dim = None,\n    conv_in_kernel: int = 3,\n    projection_class_embeddings_input_dim = None,\n    addition_embed_type = None,\n    addition_time_embed_dim = None,\n    transformer_layers_per_block = 1,\n    dtype=\"float16\",\n):\n    _batch_size = batch_size\n    _height = height\n    _width = width\n    xl = False\n    if projection_class_embeddings_input_dim is not None:\n        xl = True\n    if isinstance(only_cross_attention, bool):\n        only_cross_attention = [only_cross_attention] * len(block_out_channels)\n    if isinstance(transformer_layers_per_block, int):\n            transformer_layers_per_block = [transformer_layers_per_block] * len(down_block_types)\n    if isinstance(attention_head_dim, int):\n        attention_head_dim = (attention_head_dim,) * len(down_block_types)\n\n    ait_mod = ait_UNet2DConditionModel(\n        sample_size=sample_size,\n        cross_attention_dim=hidden_dim,\n        attention_head_dim=attention_head_dim,\n        use_linear_projection=use_linear_projection,\n        up_block_types=up_block_types,\n        down_block_types=down_block_types,\n        block_out_channels=block_out_channels,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        class_embed_type=class_embed_type,\n        num_class_embeds=num_class_embeds,\n        only_cross_attention=only_cross_attention,\n        time_embedding_dim=time_embedding_dim,\n        conv_in_kernel=conv_in_kernel,\n        projection_class_embeddings_input_dim=projection_class_embeddings_input_dim,\n        addition_embed_type=addition_embed_type,\n        addition_time_embed_dim=addition_time_embed_dim,\n        transformer_layers_per_block=transformer_layers_per_block,\n        dtype=dtype,\n    )\n    ait_mod.name_parameter_tensor()\n\n    # set AIT parameters\n    pt_mod = pt_mod.eval()\n    params_ait = map_unet(pt_mod, dim=dim, in_channels=in_channels, conv_in_key=\"conv_in_weight\", dtype=dtype)\n\n    static_shape = width[0] == width[1] and height[0] == height[1]\n\n    if static_shape:\n        height = height[0] // down_factor\n        width = width[0] // down_factor\n        height_d = height\n        width_d = width\n        height_1_d = height\n        width_1_d = width\n        height_2 = height // 2\n        width_2 = width // 2\n        height_4 = height // 4\n        width_4 = width // 4\n        height_8 = height // 8\n        width_8 = width // 8\n        height_2_d = height_2\n        width_2_d = width_2\n        height_4_d = height_4\n        width_4_d = width_4\n        height_8_d = height_8\n        width_8_d = width_8\n    else:\n        height = [x // down_factor for x in height]\n        width = [x // down_factor for x in width]\n        height_d = IntVar(values=list(height), name=\"height_d\")\n        width_d = IntVar(values=list(width), name=\"width_d\")\n        height_1_d = IntVar(values=list(height), name=\"height_1_d\")\n        width_1_d = IntVar(values=list(width), name=\"width_1_d\")\n        height_2 = [x // 2 for x in height]\n        width_2 = [x // 2 for x in width]\n        height_4 = [x // 4 for x in height]\n        width_4 = [x // 4 for x in width]\n        height_8 = [x // 8 for x in height]\n        width_8 = [x // 8 for x in width]\n        height_2_d = IntVar(values=list(height_2), name=\"height_2_d\")\n        width_2_d = IntVar(values=list(width_2), name=\"width_2_d\")\n        height_4_d = IntVar(values=list(height_4), name=\"height_4_d\")\n        width_4_d = IntVar(values=list(width_4), name=\"width_4_d\")\n        height_8_d = IntVar(values=list(height_8), name=\"height_8_d\")\n        width_8_d = IntVar(values=list(width_8), name=\"width_8_d\")\n\n    batch_size = batch_size[0], batch_size[1] * 2  # double batch size for unet\n    batch_size = IntVar(values=list(batch_size), name=\"batch_size\")\n\n    if static_shape:\n        embedding_size = 77\n    else:\n        clip_chunks = 77, 77 * clip_chunks\n        embedding_size = IntVar(values=list(clip_chunks), name=\"embedding_size\")\n        \n\n    latent_model_input_ait = Tensor(\n        [batch_size, height_d, width_d, in_channels], name=\"latent_model_input\", is_input=True, dtype=dtype\n    )\n    timesteps_ait = Tensor([batch_size], name=\"timesteps\", is_input=True, dtype=dtype)\n    text_embeddings_pt_ait = Tensor(\n        [batch_size, embedding_size, hidden_dim], name=\"encoder_hidden_states\", is_input=True, dtype=dtype\n    )\n\n    class_labels = None\n    #TODO: better way to handle this, enables class_labels for x4-upscaler\n    if in_channels == 7:\n        class_labels = Tensor(\n            [batch_size], name=\"class_labels\", dtype=\"int64\", is_input=True\n        )\n\n    add_embeds = None\n    if xl:\n        add_embeds = Tensor(\n            [batch_size, projection_class_embeddings_input_dim], name=\"add_embeds\", is_input=True, dtype=dtype\n        )\n\n    down_block_residual_0 = None\n    down_block_residual_1 = None\n    down_block_residual_2 = None\n    down_block_residual_3 = None\n    down_block_residual_4 = None\n    down_block_residual_5 = None\n    down_block_residual_6 = None\n    down_block_residual_7 = None\n    down_block_residual_8 = None\n    down_block_residual_9 = None\n    down_block_residual_10 = None\n    down_block_residual_11 = None\n    mid_block_residual = None\n    if controlnet:\n        down_block_residual_0 = Tensor(\n            [batch_size, height_1_d, width_1_d, block_out_channels[0]],\n            name=\"down_block_residual_0\",\n            is_input=True,\n        )\n        down_block_residual_1 = Tensor(\n            [batch_size, height_1_d, width_1_d, block_out_channels[0]],\n            name=\"down_block_residual_1\",\n            is_input=True,\n        )\n        down_block_residual_2 = Tensor(\n            [batch_size, height_1_d,width_1_d, block_out_channels[0]],\n            name=\"down_block_residual_2\",\n            is_input=True,\n        )\n        down_block_residual_3 = Tensor(\n            [batch_size, height_2_d, width_2_d, block_out_channels[0]],\n            name=\"down_block_residual_3\",\n            is_input=True,\n        )\n        down_block_residual_4 = Tensor(\n            [batch_size, height_2_d, width_2_d, block_out_channels[1]],\n            name=\"down_block_residual_4\",\n            is_input=True,\n        )\n        down_block_residual_5 = Tensor(\n            [batch_size, height_2_d, width_2_d, block_out_channels[1]],\n            name=\"down_block_residual_5\",\n            is_input=True,\n        )\n        down_block_residual_6 = Tensor(\n            [batch_size, height_4_d, width_4_d, block_out_channels[1]],\n            name=\"down_block_residual_6\",\n            is_input=True,\n        )\n        down_block_residual_7 = Tensor(\n            [batch_size, height_4_d, width_4_d, block_out_channels[2]],\n            name=\"down_block_residual_7\",\n            is_input=True,\n        )\n        down_block_residual_8 = Tensor(\n            [batch_size, height_4_d, width_4_d, block_out_channels[2]],\n            name=\"down_block_residual_8\",\n            is_input=True,\n        )\n        down_block_residual_9 = Tensor(\n            [batch_size, height_8_d, width_8_d, block_out_channels[2]],\n            name=\"down_block_residual_9\",\n            is_input=True,\n        )\n        down_block_residual_10 = Tensor(\n            [batch_size, height_8_d, width_8_d, block_out_channels[3]],\n            name=\"down_block_residual_10\",\n            is_input=True,\n        )\n        down_block_residual_11 = Tensor(\n            [batch_size, height_8_d, width_8_d, block_out_channels[3]],\n            name=\"down_block_residual_11\",\n            is_input=True,\n        )\n        mid_block_residual = Tensor(\n            [batch_size, height_8_d, width_8_d, block_out_channels[3]],\n            name=\"mid_block_residual\",\n            is_input=True,\n        )\n\n\n    Y = ait_mod(\n        sample=latent_model_input_ait,\n        timesteps=timesteps_ait,\n        encoder_hidden_states=text_embeddings_pt_ait,\n        down_block_residual_0=down_block_residual_0,\n        down_block_residual_1=down_block_residual_1,\n        down_block_residual_2=down_block_residual_2,\n        down_block_residual_3=down_block_residual_3,\n        down_block_residual_4=down_block_residual_4,\n        down_block_residual_5=down_block_residual_5,\n        down_block_residual_6=down_block_residual_6,\n        down_block_residual_7=down_block_residual_7,\n        down_block_residual_8=down_block_residual_8,\n        down_block_residual_9=down_block_residual_9,\n        down_block_residual_10=down_block_residual_10,\n        down_block_residual_11=down_block_residual_11,\n        mid_block_residual=mid_block_residual,\n        class_labels=class_labels,\n        add_embeds=add_embeds,\n    )\n    mark_output(Y)\n\n    target = detect_target(\n        use_fp16_acc=use_fp16_acc, convert_conv_to_gemm=convert_conv_to_gemm\n    )\n    dll_name = model_name + \".dll\" if sys.platform == \"win32\" else model_name + \".so\"\n    total_usage = compile_model(\n        Y, target, work_dir, model_name, constants=params_ait if constants else None, dll_name=dll_name,\n    )\n    sd = \"v1\"\n    if hidden_dim == 1024:\n        sd = \"v2\"\n    elif hidden_dim == 2048:\n        sd = \"xl\"\n    vram = round(total_usage / 1024 / 1024)\n    model_type = \"unet_control\" if controlnet else \"unet\"\n    process(work_dir, model_name, dll_name, target._arch, _height[-1], _width[-1], _batch_size[-1], vram, out_dir, sd, model_type)"]}
{"filename": "AITemplate/ait/compile/controlnet.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport sys\nimport torch\nfrom aitemplate.compiler import compile_model\nfrom aitemplate.frontend import IntVar, Tensor", "from aitemplate.compiler import compile_model\nfrom aitemplate.frontend import IntVar, Tensor\nfrom aitemplate.testing import detect_target\n\nfrom ..modeling.controlnet import (\n    ControlNetModel as ait_ControlNetModel,\n)\nfrom .util import mark_output\nfrom .release import process\n", "from .release import process\n\nfrom ait.util.mapping import map_controlnet\n\n\ndef compile_controlnet(\n    pt_mod,\n    batch_size=(1, 4),\n    height=(64, 2048),\n    width=(64, 2048),\n    clip_chunks=1,\n    dim=320,\n    hidden_dim=768,\n    use_fp16_acc=False,\n    convert_conv_to_gemm=False,\n    model_name=\"ControlNetModel\",\n    constants=False,\n    work_dir=\"./tmp\",\n    out_dir=\"./out\",\n    down_factor=8,\n    use_linear_projection=False,\n    block_out_channels=(320, 640, 1280, 1280),\n    down_block_types= (\n        \"CrossAttnDownBlock2D\",\n        \"CrossAttnDownBlock2D\",\n        \"CrossAttnDownBlock2D\",\n        \"DownBlock2D\",\n    ),\n    in_channels=4,\n    out_channels=4,\n    sample_size=64,\n    class_embed_type=None,\n    num_class_embeds=None,\n    time_embedding_dim = None,\n    conv_in_kernel: int = 3,\n    projection_class_embeddings_input_dim = None,\n    addition_embed_type = None,\n    addition_time_embed_dim = None,\n    transformer_layers_per_block = 1,\n    dtype=\"float16\",\n):\n    _batch_size = batch_size\n    _height = height\n    _width = width\n    xl = False\n    if projection_class_embeddings_input_dim is not None:\n        xl = True\n    if isinstance(transformer_layers_per_block, int):\n        transformer_layers_per_block = [transformer_layers_per_block] * len(down_block_types)\n    if isinstance(transformer_layers_per_block, int):\n        transformer_layers_per_block = [transformer_layers_per_block] * len(down_block_types)\n    batch_size = batch_size  # double batch size for unet\n    ait_mod = ait_ControlNetModel(\n        in_channels=in_channels,\n        down_block_types=down_block_types,\n        block_out_channels=block_out_channels,\n        cross_attention_dim=hidden_dim,\n        transformer_layers_per_block=transformer_layers_per_block,\n        use_linear_projection=use_linear_projection,\n        class_embed_type=class_embed_type,\n        addition_embed_type=addition_embed_type,\n        num_class_embeds=num_class_embeds,\n        projection_class_embeddings_input_dim=projection_class_embeddings_input_dim,\n        dtype=\"float16\",\n    )\n    ait_mod.name_parameter_tensor()\n\n    pt_mod = pt_mod.eval()\n    params_ait = map_controlnet(pt_mod, dim=dim)\n\n    static_shape = width[0] == width[1] and height[0] == height[1] and batch_size[0] == batch_size[1]\n\n    if static_shape:\n        batch_size = batch_size[0] * 2  # double batch size for unet\n        height_d = height[0] // down_factor\n        width_d = width[0] // down_factor\n        height_c = height[0]\n        width_c = width[0]\n        clip_chunks = 77\n        embedding_size = clip_chunks\n    else:\n        batch_size = batch_size[0], batch_size[1] * 2  # double batch size for unet\n        batch_size = IntVar(values=list(batch_size), name=\"batch_size\")\n        height_d = height[0] // down_factor, height[1] // down_factor\n        height_d = IntVar(values=list(height_d), name=\"height_d\")\n        width_d = width[0] // down_factor, width[1] // down_factor\n        width_d = IntVar(values=list(width_d), name=\"width_d\")\n        height_c = height\n        height_c = IntVar(values=list(height_c), name=\"height_c\")\n        width_c = width\n        width_c = IntVar(values=list(width_c), name=\"width_c\")\n        clip_chunks = 77, 77 * clip_chunks\n        embedding_size = IntVar(values=list(clip_chunks), name=\"embedding_size\")\n\n    latent_model_input_ait = Tensor(\n        [batch_size, height_d, width_d, 4], name=\"latent_model_input\", is_input=True\n    )\n    timesteps_ait = Tensor([batch_size], name=\"timesteps\", is_input=True)\n    text_embeddings_pt_ait = Tensor(\n        [batch_size, embedding_size, hidden_dim], name=\"encoder_hidden_states\", is_input=True\n    )\n    controlnet_condition_ait = Tensor(\n        [batch_size, height_c, width_c, 3], name=\"control_hint\", is_input=True\n    )\n\n    add_embeds = None\n    if xl:\n        add_embeds = Tensor(\n            [batch_size, projection_class_embeddings_input_dim], name=\"add_embeds\", is_input=True, dtype=dtype\n        )\n\n\n    Y = ait_mod(\n        latent_model_input_ait,\n        timesteps_ait,\n        text_embeddings_pt_ait,\n        controlnet_condition_ait,\n        add_embeds=add_embeds,\n    )\n    mark_output(Y)\n\n    target = detect_target(\n        use_fp16_acc=use_fp16_acc, convert_conv_to_gemm=convert_conv_to_gemm\n    )\n    dll_name = model_name + \".dll\" if sys.platform == \"win32\" else model_name + \".so\"\n    total_usage = compile_model(\n        Y, target, work_dir, model_name, constants=params_ait if constants else None, dll_name=dll_name,\n    )\n    sd = \"v1\"\n    if hidden_dim == 1024:\n        sd = \"v2\"\n    elif hidden_dim == 2048:\n        sd = \"xl\"\n    vram = round(total_usage / 1024 / 1024)\n    process(work_dir, model_name, dll_name, target._arch, _height[-1], _width[-1], _batch_size[-1], vram, out_dir, sd, \"controlnet\")"]}
{"filename": "AITemplate/ait/compile/vae.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\nimport sys\nimport torch\nfrom aitemplate.compiler import compile_model", "import torch\nfrom aitemplate.compiler import compile_model\nfrom aitemplate.frontend import IntVar, Tensor\nfrom aitemplate.testing import detect_target\n\nfrom ..modeling.vae import AutoencoderKL as ait_AutoencoderKL\nfrom .util import mark_output\nfrom .release import process\n\nfrom ait.util.mapping import map_vae", "\nfrom ait.util.mapping import map_vae\n\ndef compile_vae(\n    pt_mod,\n    batch_size=(1, 8),\n    height=(64, 2048),\n    width=(64, 2048),\n    use_fp16_acc=True,\n    convert_conv_to_gemm=True,\n    model_name=\"AutoencoderKL\",\n    constants=True,\n    block_out_channels=[128, 256, 512, 512],\n    layers_per_block=2,\n    act_fn=\"silu\",\n    latent_channels=4,\n    sample_size=512,\n    in_channels=3,\n    out_channels=3,\n    down_block_types=[\n        \"DownEncoderBlock2D\",\n        \"DownEncoderBlock2D\",\n        \"DownEncoderBlock2D\",\n        \"DownEncoderBlock2D\",\n    ],\n    up_block_types=[\n        \"UpDecoderBlock2D\",\n        \"UpDecoderBlock2D\",\n        \"UpDecoderBlock2D\",\n        \"UpDecoderBlock2D\",\n    ],\n    input_size=(64, 64),\n    down_factor=8,\n    dtype=\"float16\",\n    work_dir=\"./tmp\",\n    out_dir=\"./tmp\",\n    vae_encode=False,\n):\n    _batch_size = batch_size\n    _height = height\n    _width = width\n    ait_vae = ait_AutoencoderKL(\n        batch_size[0],\n        input_size[0],\n        input_size[1],\n        in_channels=in_channels,\n        out_channels=out_channels,\n        down_block_types=down_block_types,\n        up_block_types=up_block_types,\n        block_out_channels=block_out_channels,\n        layers_per_block=layers_per_block,\n        act_fn=act_fn,\n        latent_channels=latent_channels,\n        sample_size=sample_size,\n        dtype=dtype\n    )\n\n    static_batch = batch_size[0] == batch_size[1]\n    static_shape = height[0] == height[1] and width[0] == width[1]\n    if not vae_encode:\n        height = height[0] // down_factor, height[1] // down_factor\n        width = width[0] // down_factor, width[1] // down_factor\n\n    if static_batch:\n        batch_size = batch_size[0]\n    else:\n        batch_size = IntVar(values=list(batch_size), name=\"batch_size\")\n    if static_shape:\n        height_d = height[0]\n        width_d = width[0]\n    else:\n        height_d = IntVar(values=list(height), name=\"height\")\n        width_d = IntVar(values=list(width), name=\"width\")\n\n    ait_input = Tensor(\n        shape=[batch_size, height_d, width_d, 3 if vae_encode else latent_channels],\n        name=\"pixels\" if vae_encode else \"latent\",\n        is_input=True,\n        dtype=dtype\n    )\n    sample = None\n    if vae_encode:\n        sample = Tensor(\n            shape=[batch_size, height_d, width_d, latent_channels],\n            name=\"random_sample\",\n            is_input=True,\n            dtype=dtype,\n        )\n    ait_vae.name_parameter_tensor()\n\n    pt_mod = pt_mod.eval()\n    params_ait = map_vae(pt_mod, dtype=dtype, encoder=vae_encode)\n    if vae_encode:\n        Y = ait_vae.encode(ait_input, sample)\n    else:\n        Y = ait_vae.decode(ait_input)\n    mark_output(Y)\n    target = detect_target(\n        use_fp16_acc=use_fp16_acc, convert_conv_to_gemm=convert_conv_to_gemm\n    )\n    dll_name = model_name + \".dll\" if sys.platform == \"win32\" else model_name + \".so\"\n    total_usage = compile_model(\n        Y, target, work_dir, model_name, constants=params_ait if constants else None, dll_name=dll_name,\n    )\n    sd = None\n    vram = round(total_usage / 1024 / 1024)\n    model_type = \"vae_encode\" if vae_encode else \"vae_decode\"\n    process(work_dir, model_name, dll_name, target._arch, _height[-1], _width[-1], _batch_size[-1], vram, out_dir, sd, model_type)"]}
{"filename": "AITemplate/ait/compile/util.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\ndef mark_output(ys):\n    if type(ys) != tuple:\n        ys = (ys, )\n    for i in range(len(ys)):\n        y = ys[i]\n        if type(y) == tuple:\n            for yy in y:\n                y_shape = [d._attrs[\"values\"] for d in yy._attrs[\"shape\"]]\n                y_name = yy._attrs[\"name\"]\n                print(\"AIT {} shape: {}\".format(y_name, y_shape))\n        else:\n            y_shape = [d._attrs[\"values\"] for d in y._attrs[\"shape\"]]\n            y_name = y._attrs[\"name\"]\n            print(\"AIT {} shape: {}\".format(y_name, y_shape))", ""]}
{"filename": "AITemplate/ait/compile/clip.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport sys\nfrom aitemplate.compiler import compile_model\nfrom aitemplate.frontend import IntVar, Tensor\nfrom aitemplate.testing import detect_target", "from aitemplate.frontend import IntVar, Tensor\nfrom aitemplate.testing import detect_target\n\nfrom ..modeling.clip import CLIPTextTransformer as ait_CLIPTextTransformer\nfrom .util import mark_output\nfrom .release import process\n\nfrom ait.util.mapping import map_clip\n\n\ndef compile_clip(\n    pt_mod,\n    batch_size=(1, 8),\n    seqlen=64,\n    dim=768,\n    num_heads=12,\n    depth=12,\n    output_hidden_states=False,\n    text_projection_dim=None,\n    use_fp16_acc=True,\n    convert_conv_to_gemm=True,\n    act_layer=\"gelu\",\n    constants=True,\n    model_name=\"CLIPTextModel\",\n    work_dir=\"./tmp\",\n    out_dir=\"./out\",\n):\n    _batch_size = batch_size\n    mask_seq = 0\n    causal = True\n\n    ait_mod = ait_CLIPTextTransformer(\n        num_hidden_layers=depth,\n        hidden_size=dim,\n        num_attention_heads=num_heads,\n        batch_size=batch_size,\n        seq_len=seqlen,\n        causal=causal,\n        mask_seq=mask_seq,\n        act_layer=act_layer,\n        output_hidden_states=output_hidden_states,\n        text_projection_dim=text_projection_dim,\n    )\n    ait_mod.name_parameter_tensor()\n\n    pt_mod = pt_mod.eval()\n    params_ait = map_clip(pt_mod)\n    \n    static_shape = batch_size[0] == batch_size[1]\n    if static_shape:\n        batch_size = batch_size[0]\n    else:\n        batch_size = IntVar(values=list(batch_size), name=\"batch_size\")\n\n    input_ids_ait = Tensor(\n        [batch_size, seqlen], name=\"input_ids\", dtype=\"int64\", is_input=True\n    )\n    position_ids_ait = Tensor(\n        [batch_size, seqlen], name=\"position_ids\", dtype=\"int64\", is_input=True\n    )\n\n    Y = ait_mod(input_ids=input_ids_ait, position_ids=position_ids_ait)\n    mark_output(Y)\n\n    target = detect_target(\n        use_fp16_acc=use_fp16_acc, convert_conv_to_gemm=convert_conv_to_gemm\n    )\n    dll_name = model_name + \".dll\" if sys.platform == \"win32\" else model_name + \".so\"\n    total_usage = compile_model(\n        Y, target, work_dir, model_name, constants=params_ait if constants else None, dll_name=dll_name,\n    )\n    sd = \"L\"\n    if dim == 1024:\n        sd = \"H\"\n    if dim == 1280:\n        sd = \"G\"\n    vram = round(total_usage / 1024 / 1024)\n    process(work_dir, model_name, dll_name, target._arch, None, None, _batch_size[-1], vram, out_dir, sd, \"clip_text\")", "\n\ndef compile_clip(\n    pt_mod,\n    batch_size=(1, 8),\n    seqlen=64,\n    dim=768,\n    num_heads=12,\n    depth=12,\n    output_hidden_states=False,\n    text_projection_dim=None,\n    use_fp16_acc=True,\n    convert_conv_to_gemm=True,\n    act_layer=\"gelu\",\n    constants=True,\n    model_name=\"CLIPTextModel\",\n    work_dir=\"./tmp\",\n    out_dir=\"./out\",\n):\n    _batch_size = batch_size\n    mask_seq = 0\n    causal = True\n\n    ait_mod = ait_CLIPTextTransformer(\n        num_hidden_layers=depth,\n        hidden_size=dim,\n        num_attention_heads=num_heads,\n        batch_size=batch_size,\n        seq_len=seqlen,\n        causal=causal,\n        mask_seq=mask_seq,\n        act_layer=act_layer,\n        output_hidden_states=output_hidden_states,\n        text_projection_dim=text_projection_dim,\n    )\n    ait_mod.name_parameter_tensor()\n\n    pt_mod = pt_mod.eval()\n    params_ait = map_clip(pt_mod)\n    \n    static_shape = batch_size[0] == batch_size[1]\n    if static_shape:\n        batch_size = batch_size[0]\n    else:\n        batch_size = IntVar(values=list(batch_size), name=\"batch_size\")\n\n    input_ids_ait = Tensor(\n        [batch_size, seqlen], name=\"input_ids\", dtype=\"int64\", is_input=True\n    )\n    position_ids_ait = Tensor(\n        [batch_size, seqlen], name=\"position_ids\", dtype=\"int64\", is_input=True\n    )\n\n    Y = ait_mod(input_ids=input_ids_ait, position_ids=position_ids_ait)\n    mark_output(Y)\n\n    target = detect_target(\n        use_fp16_acc=use_fp16_acc, convert_conv_to_gemm=convert_conv_to_gemm\n    )\n    dll_name = model_name + \".dll\" if sys.platform == \"win32\" else model_name + \".so\"\n    total_usage = compile_model(\n        Y, target, work_dir, model_name, constants=params_ait if constants else None, dll_name=dll_name,\n    )\n    sd = \"L\"\n    if dim == 1024:\n        sd = \"H\"\n    if dim == 1280:\n        sd = \"G\"\n    vram = round(total_usage / 1024 / 1024)\n    process(work_dir, model_name, dll_name, target._arch, None, None, _batch_size[-1], vram, out_dir, sd, \"clip_text\")"]}
{"filename": "AITemplate/ait/compile/release.py", "chunked_list": ["import os\nimport lzma\nimport hashlib\nimport json\nimport sys\n\ndef sha256sum(filename):\n    h = hashlib.sha256()\n    b = bytearray(128 * 1024)\n    mv = memoryview(b)\n    with open(filename, \"rb\", buffering=0) as f:\n        for n in iter(lambda: f.readinto(mv), 0):\n            h.update(mv[:n])\n    return h.hexdigest()", "\ndef filesize(filename):\n    return os.stat(filename).st_size\n\ndef compress_file(filename):\n    with open(filename, \"rb\") as f:\n        data = f.read()\n    with lzma.open(filename + \".xz\", \"wb\", preset=9) as f:\n        f.write(data)\n    sha256 = sha256sum(filename + \".xz\")\n    return sha256, filesize(filename + \".xz\")", "\ndef process_file(filename):\n    file_size = filesize(filename)\n    sha256 = sha256sum(filename)\n    sha256_xz, file_size_xz = compress_file(filename)\n    return sha256, file_size, sha256_xz, file_size_xz\n\ndef process(work_dir, model_name, dll_name, arch, height, width, batch_size, vram, out_dir, sd, model_type):\n    path = os.path.join(work_dir, model_name)\n    dll_path = os.path.join(path, dll_name)\n    sha256, file_size, sha256_xz, file_size_xz = process_file(dll_path)\n    _os = \"windows\" if sys.platform == \"win32\" else \"linux\"\n    cuda = f\"sm{arch}\"\n    if height is None or width is None:\n        _reso = None\n    else:\n        _reso = max(height, width)\n    _bs = batch_size\n    compressed_name = f\"{dll_name}.xz\"\n    compressed_path = os.path.join(path, compressed_name)\n    subpath = f\"{_os}/{cuda}/\"\n    if _reso is not None:\n        subpath = subpath + f\"bs{_bs}/{_reso}/\"\n    key = (subpath + compressed_name).replace(\"\\\\\", \"/\")\n    subpath = os.path.join(out_dir, subpath)\n    os.makedirs(subpath, exist_ok=True)\n    out_path = os.path.join(subpath, compressed_name)\n    os.rename(compressed_path, out_path)\n    data = {\n        \"os\": _os,\n        \"cuda\": cuda,\n        \"model\": model_type,\n        \"sd\": sd,\n        \"batch_size\": _bs,\n        \"resolution\": _reso,\n        \"vram\": vram,\n        \"url\": key,\n        \"compressed_size\": file_size_xz,\n        \"size\": file_size,\n        \"compressed_sha256\": sha256_xz,\n        \"sha256\": sha256,\n    }\n    if not os.path.exists(os.path.join(out_dir, \"modules.json\")):\n        with open(os.path.join(out_dir, \"modules.json\"), \"w\") as f:\n            json.dump({}, f)\n    with open(os.path.join(out_dir, \"modules.json\"), \"r\") as f:\n        modules = json.load(f)\n        modules[key.replace(\"/\", \"_\")] = data\n    with open(os.path.join(out_dir, \"modules.json\"), \"w\") as f:\n        json.dump(modules, f)", "\n"]}
{"filename": "AITemplate/ait/module/model.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\"\"\"\nPython bindings to the AIT runtime.\n\"\"\"\nimport os", "\"\"\"\nimport os\nimport ctypes\nimport enum\nimport logging\nimport math\nfrom typing import Callable, Dict, List, NamedTuple, Optional, Tuple, TypeVar, Union\n\nimport numpy as np\n", "import numpy as np\n\nfrom .dtype import dtype_str_to_enum\nfrom .misc import is_linux, is_windows\nfrom .torch_utils import torch_dtype_to_string\n\n# Controls how many runtimes will be used in ModelContainer by default.\n# See the runtime README.md for more information on the Model/ModelContainer\n# system and the num_runtimes parameter.\n# This value is used as the default for the num_runtimes argument", "# system and the num_runtimes parameter.\n# This value is used as the default for the num_runtimes argument\n# in both Model.__init__ and compile_model. Changing it will have no\n# effect since Python default arguments only get evaluated once.\nAIT_DEFAULT_NUM_RUNTIMES = 1\n\n# Stand-in for torch.Tensor. Use a TypeVar for some APIs since we can't introduce\n# a torch dependency.\nTorchTensor = TypeVar(\"TorchTensor\")\n", "TorchTensor = TypeVar(\"TorchTensor\")\n\n\nclass AITemplateMemcpyKind(enum.Enum):\n    HostToDevice = 0\n    DeviceToHost = 1\n    DeviceToDevice = 2\n\n\nclass AITemplateAllocatorKind(enum.Enum):\n    DEFAULT = 0\n    TRACKING = 1", "\nclass AITemplateAllocatorKind(enum.Enum):\n    DEFAULT = 0\n    TRACKING = 1\n\n\nclass AITData(NamedTuple):\n    \"\"\"\n    Input or output tensor for Model.run. We require the extra data for safety\n    checks inside the runtime.\n    \"\"\"\n\n    data_ptr: int\n    shape: List[int]\n    dtype: str", "\n\nclass _AITemplateShape(ctypes.Structure):\n    _fields_ = [\n        (\"shape_data\", ctypes.POINTER(ctypes.c_longlong)),\n        (\"size\", ctypes.c_size_t),\n    ]\n\n\nclass _CFormatAITData(ctypes.Structure):\n    _fields_ = [\n        (\"pointer\", ctypes.c_void_p),\n        (\"shape\", _AITemplateShape),\n        (\"dtype\", ctypes.c_int),\n    ]", "\nclass _CFormatAITData(ctypes.Structure):\n    _fields_ = [\n        (\"pointer\", ctypes.c_void_p),\n        (\"shape\", _AITemplateShape),\n        (\"dtype\", ctypes.c_int),\n    ]\n\n\ndef _dlclose(dll: ctypes.CDLL):\n    f_dlclose = None\n\n    if is_windows():\n        f_dlclose = ctypes.windll.kernel32.FreeLibrary\n    elif is_linux():\n        syms = ctypes.CDLL(None)\n        if not hasattr(syms, \"dlclose\"):\n            # Apline Linux\n            syms = ctypes.CDLL(\"libc.so\")\n\n        if hasattr(syms, \"dlclose\"):\n            f_dlclose = syms.dlclose\n\n    if f_dlclose is not None:\n        f_dlclose.argtypes = [ctypes.c_void_p]\n        f_dlclose(dll._handle)\n    else:\n        logging.warning(\n            \"dll unloading function was not found, library may not be unloaded properly!\"\n        )", "\ndef _dlclose(dll: ctypes.CDLL):\n    f_dlclose = None\n\n    if is_windows():\n        f_dlclose = ctypes.windll.kernel32.FreeLibrary\n    elif is_linux():\n        syms = ctypes.CDLL(None)\n        if not hasattr(syms, \"dlclose\"):\n            # Apline Linux\n            syms = ctypes.CDLL(\"libc.so\")\n\n        if hasattr(syms, \"dlclose\"):\n            f_dlclose = syms.dlclose\n\n    if f_dlclose is not None:\n        f_dlclose.argtypes = [ctypes.c_void_p]\n        f_dlclose(dll._handle)\n    else:\n        logging.warning(\n            \"dll unloading function was not found, library may not be unloaded properly!\"\n        )", "\n\ndef _check_tensors(\n    tensor_list: Union[Dict[str, TorchTensor], List[TorchTensor]],\n    is_error_fn: Callable[[TorchTensor], bool],\n    list_name: str,\n    condition_description: str,\n):\n    \"\"\"\n    Helper for various input/output sanity checks.\n    \"\"\"\n    if isinstance(tensor_list, dict):\n        tensor_list = tensor_list.values()\n\n    for i, tensor in enumerate(tensor_list):\n        if is_error_fn(tensor):\n            raise ValueError(f\"{list_name}[{i}] failed check: {condition_description}\")", "\n\ndef _check_tensors_contiguous_and_on_gpu(\n    tensors: Union[Dict[str, TorchTensor], List[TorchTensor]], name: str\n):\n    def is_bad_tensor(tensor: TorchTensor) -> bool:\n        return not tensor.is_contiguous() or not tensor.is_cuda\n\n    _check_tensors(tensors, is_bad_tensor, name, \"contiguous and on GPU\")\n", "\n\ndef _check_tensors_contiguous_and_on_host(\n    tensors: Union[Dict[str, TorchTensor], List[TorchTensor]], name: str\n):\n    def is_bad_tensor(tensor: TorchTensor) -> bool:\n        return not tensor.is_contiguous() or tensor.is_cuda\n\n    _check_tensors(tensors, is_bad_tensor, name, \"contiguous and on host\")\n", "\n\ndef torch_to_ait_data(tensor: TorchTensor) -> AITData:\n    \"\"\"\n    Convert a torch Tensor to a AITData.\n    \"\"\"\n    return AITData(\n        tensor.data_ptr(), list(tensor.size()), torch_dtype_to_string(tensor.dtype)\n    )\n", "\n\ndef _convert_tensor_args(params: Union[List[TorchTensor], Dict[str, TorchTensor]]):\n    \"\"\"\n    Helper function for the WithTensors APIs.\n    \"\"\"\n    if isinstance(params, dict):\n        result = {name: torch_to_ait_data(x) for name, x in params.items()}\n    else:\n        result = [torch_to_ait_data(x) for x in params]\n    return result", "\n\ndef _reshape_tensor(tensor: TorchTensor, shape: List[int]) -> TorchTensor:\n    \"\"\"\n    Reinterpret a blob of contiguous memory as some shape. Used to convert\n    outputs in RunWithTensors.\n    \"\"\"\n    assert tensor.ndim == len(\n        shape\n    ), f\"Expected output tensor's ndim to match the length of Run()'s return value: {tensor.ndim=} != {len(shape)=}\"\n    numel = math.prod(shape)\n    new_tensor = tensor.flatten()[:numel]\n    return new_tensor.reshape(shape)", "\n\nclass Model:\n    class _DLLWrapper:\n        def __init__(\n            self,\n            lib_path: str,\n        ):\n            self.lib_path = lib_path\n            self.extension = \".dll\" if os.name == \"nt\" else \".so\"\n            if lib_path.endswith(\".xz\"):\n                import lzma\n                import tempfile\n\n                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=self.extension)\n                temp_file.write(lzma.decompress(open(lib_path, \"rb\").read(), format=lzma.FORMAT_AUTO))\n                temp_file.close()\n                lib_path = temp_file.name.replace(\"\\\\\", \"/\")\n                self.temp_path = lib_path\n                \n            self.DLL = ctypes.cdll.LoadLibrary(lib_path)\n            self.is_open = True\n\n        def close(self):\n            if self.is_open:\n                _dlclose(self.DLL)\n                if self.lib_path.endswith(\".xz\"):\n                    try:\n                        os.unlink(self.temp_path)\n                    except:\n                        pass\n                self.is_open = False\n\n        def __getattr__(self, name):\n            if not self.is_open:\n                raise RuntimeError(f\"Cannot use closed AIT library: {self.lib_path}\")\n\n            method = getattr(self.DLL, name)\n\n            def _wrapped_func(*args):\n                err = method(*args)\n                if err:\n                    raise RuntimeError(f\"Error in function: {method.__name__}\")\n\n            return _wrapped_func\n\n    def __init__(\n        self,\n        lib_path: str,\n        num_runtimes: int = AIT_DEFAULT_NUM_RUNTIMES,\n        allocator_kind: Optional[AITemplateAllocatorKind] = None,\n    ):\n        \"\"\"\n        Instantiates a wrapper around the C++ model_interface.\n\n        Parameters\n        ----------\n        lib_path : str\n            The path to the compiled .so\n        num_runtimes : int, optional\n            How many runtimes should be stored in the internal pool. This\n            determines how many inferences can happen concurrently. By\n            default, set to 1. Must be positive.\n        allocator_kind : AITemplateAllocatorKind, optional\n            What type of allocator to use when allocating GPU memory.\n        \"\"\"\n        # Set of pointers allocated with numpy_to_ait_data.\n        # If the user forgets to free their data, we use this to\n        # avoid leaking memory.\n        self._allocated_ait_data = set()\n\n        if num_runtimes <= 0:\n            raise ValueError(f\"num_runtimes must be positive, but got {num_runtimes}\")\n\n        self.DLL = self._DLLWrapper(lib_path)\n        self.lib_path = lib_path\n        self.handle = ctypes.c_void_p()\n        self.allocator_handle = ctypes.c_void_p()\n        if allocator_kind is not None:\n            self.DLL.AITemplateAllocatorCreate(\n                ctypes.byref(self.allocator_handle),\n                ctypes.c_int(allocator_kind.value),\n            )\n\n        self.DLL.AITemplateModelContainerCreate(\n            ctypes.pointer(self.handle),\n            ctypes.c_size_t(num_runtimes),\n            self.allocator_handle,\n        )\n\n        # We use this list to add reference counts of Torch tensors\n        # to avoid lifetime issues caused by user misuse.\n        self.torch_constant_tensors = {}\n\n        # The corresponding sorted_graph. Optional. For debugging purpose.\n        self.debug_sorted_graph = None\n\n        self._output_name_to_index = self._construct_output_name_to_index_map()\n        self._input_name_to_index = self._construct_input_name_to_index_map()\n        self._output_ndims = [\n            len(self.get_output_maximum_shape(i))\n            for i in range(len(self._output_name_to_index))\n        ]\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n    def __del__(self):\n        self.close()\n\n    def close(self):\n        # Copy to avoid set size changed during iteration\n        for ptr in list(self._allocated_ait_data):\n            self.free_gpu_memory(ptr, sync=True)\n\n        # Check that it exists since we may have thrown\n        # an exception before initializing it.\n        if hasattr(self, \"DLL\"):\n            if self.handle:\n                self.DLL.AITemplateModelContainerDelete(self.handle)\n                self.handle = ctypes.c_void_p()\n\n            if self.allocator_handle:\n                self.DLL.AITemplateAllocatorDelete(self.allocator_handle)\n                self.allocator_handle = ctypes.c_void_p()\n\n            self.DLL.close()\n\n    def __getstate__(self):\n        return {\"lib_path\": self.DLL.lib_path}\n\n    def __setstate__(self, d):\n        if \"lib_path\" not in d:\n            raise RuntimeError(f\"Didn't find 'lib_path' property in {d}\")\n        self.__init__(d[\"lib_path\"])\n\n    def _convert_single_param_to_c_format(self, param: AITData) -> _CFormatAITData:\n        pointer, shape, dtype = param\n        c_pointer = ctypes.c_void_p(pointer)\n        c_shape_data = (ctypes.c_longlong * len(shape))()\n        for j, dim in enumerate(shape):\n            c_shape_data[j] = ctypes.c_longlong(dim)\n        c_shape = _AITemplateShape(c_shape_data, ctypes.c_size_t(len(shape)))\n        c_dtype = dtype_str_to_enum(dtype)\n        return _CFormatAITData(c_pointer, c_shape, c_dtype)\n\n    def _convert_params_to_c_format(self, params: List[AITData]):\n        c_params = (_CFormatAITData * len(params))()\n        for i, param in enumerate(params):\n            c_params[i] = self._convert_single_param_to_c_format(param)\n        return c_params\n\n    def _prepare_run(\n        self,\n        inputs,\n        outputs,\n        stream_ptr,\n    ):\n        c_inputs = self._convert_params_to_c_format(inputs)\n        c_outputs = self._convert_params_to_c_format(outputs)\n        c_stream = (\n            ctypes.c_void_p() if stream_ptr is None else ctypes.c_void_p(stream_ptr)\n        )\n\n        num_outputs = len(self._output_ndims)\n        c_output_shapes_out = (ctypes.POINTER(ctypes.c_int64) * num_outputs)()\n        for i in range(num_outputs):\n            c_output_shapes_out[i] = ctypes.cast(\n                (ctypes.c_int64 * self._output_ndims[i])(),\n                ctypes.POINTER(ctypes.c_int64),\n            )\n\n        return (\n            c_inputs,\n            c_outputs,\n            c_stream,\n            c_output_shapes_out,\n        )\n\n    def _dict_to_ordered_list(self, params, is_inputs):\n        if is_inputs:\n            index_map = self._input_name_to_index\n        else:\n            index_map = self._output_name_to_index\n        if len(params) != len(index_map):\n            raise ValueError(\n                f\"Did not get correct number of {'inputs' if is_inputs else 'outputs'} expected {len(index_map)}, got {len(params)}\"\n            )\n\n        result = [None] * len(index_map)\n        for name, tensor in params.items():\n            if name not in index_map:\n                raise ValueError(\n                    f\"Got unexpected {'input' if is_inputs else 'output'}: {name}\"\n                )\n\n            result[index_map[name]] = tensor\n\n        return result\n\n    def _make_ait_outputs(\n        self, outputs: List[AITData], c_output_shapes\n    ) -> Dict[str, AITData]:\n        output_shapes = []\n        for i, c_shape in enumerate(c_output_shapes):\n            shape = []\n            for j in range(self._output_ndims[i]):\n                shape.append(c_shape[j])\n            output_shapes.append(shape)\n\n        return {\n            name: AITData(outputs[idx].data_ptr, output_shapes[idx], outputs[idx].dtype)\n            for name, idx in self._output_name_to_index.items()\n        }\n\n    def _run_impl(\n        self,\n        inputs: Union[Dict[str, AITData], List[AITData]],\n        outputs: Union[Dict[str, AITData], List[AITData]],\n        stream_ptr: Optional[int] = None,\n        sync: bool = True,\n        graph_mode: bool = False,\n        outputs_on_host: bool = False,\n    ) -> Dict[str, AITData]:\n        if isinstance(inputs, dict):\n            inputs = self._dict_to_ordered_list(inputs, is_inputs=True)\n        if isinstance(outputs, dict):\n            outputs = self._dict_to_ordered_list(outputs, is_inputs=False)\n        (c_inputs, c_outputs, c_stream, c_output_shapes_out,) = self._prepare_run(\n            inputs,\n            outputs,\n            stream_ptr,\n        )\n\n        if not outputs_on_host:\n            self.DLL.AITemplateModelContainerRun(\n                self.handle,\n                c_inputs,\n                ctypes.c_size_t(len(inputs)),\n                c_outputs,\n                ctypes.c_size_t(len(outputs)),\n                c_stream,\n                ctypes.c_bool(sync),\n                ctypes.c_bool(graph_mode),\n                c_output_shapes_out,\n            )\n        else:\n            self.DLL.AITemplateModelContainerRunWithOutputsOnHost(\n                self.handle,\n                c_inputs,\n                ctypes.c_size_t(len(inputs)),\n                c_outputs,\n                ctypes.c_size_t(len(outputs)),\n                c_stream,\n                ctypes.c_bool(graph_mode),\n                c_output_shapes_out,\n            )\n\n        return self._make_ait_outputs(outputs, c_output_shapes_out)\n\n    def run(\n        self,\n        inputs: Union[Dict[str, AITData], List[AITData]],\n        outputs: Union[Dict[str, AITData], List[AITData]],\n        stream_ptr: Optional[int] = None,\n        sync: bool = True,\n        graph_mode: bool = False,\n    ) -> Dict[str, AITData]:\n        \"\"\"\n        Run the model.\n\n        Parameters\n        ----------\n        inputs: Union[Dict[str, AITData], List[AITData]]\n            The inputs to use. AITData is a named tuple containing\n            the tensor's data_ptr, size, and dtype. If inputs is a list,\n            it must be ordered correctly (as specified by GetInputNameToIndexMap).\n            This parameter can also be a dictionary (name -> AITData).\n        outputs: Union[Dict[str, AITData], List[AITData]]\n            The outputs to use. Similar to inputs, can either be a list of ordered\n            outputs, or a dictionary (output name -> AITData).\n            These should be allocated with enough memory to store their maximum\n            size (which can be queried with GetOutputMaximumSize).\n        stream_ptr: int\n            A pointer to CUDA stream to run on. If None, use the legacy stream.\n        sync: bool:\n            If True, synchronize the stream at the end of the run\n        graph_mode: bool\n            If True, use a CUDA graph kernel (experimental)\n\n        Returns\n        -------\n        AITDatas with output shapes that are computed by shape inference. This may not be\n        the maximum shape. The output memory blobs that are passed in to Run()\n        should be interpreted and possibly truncated according to these sizes.\n        \"\"\"\n        return self._run_impl(\n            inputs, outputs, stream_ptr, sync, graph_mode, outputs_on_host=False\n        )\n\n    def profile(\n        self,\n        inputs: Union[Dict[str, AITData], List[AITData]],\n        outputs: Union[Dict[str, AITData], List[AITData]],\n        num_iters: int,\n        filename: str,\n        stream_ptr: Optional[int] = None,\n    ) -> None:\n        if isinstance(inputs, dict):\n            inputs = self._dict_to_ordered_list(inputs, is_inputs=True)\n        if isinstance(outputs, dict):\n            outputs = self._dict_to_ordered_list(outputs, is_inputs=False)\n        (c_inputs, c_outputs, c_stream, c_output_shapes_out,) = self._prepare_run(\n            inputs,\n            outputs,\n            stream_ptr,\n        )\n        self.DLL.AITemplateModelContainerProfile(\n            self.handle,\n            c_inputs,\n            ctypes.c_size_t(len(inputs)),\n            c_outputs,\n            ctypes.c_size_t(len(outputs)),\n            c_stream,\n            ctypes.c_size_t(num_iters),\n            ctypes.c_char_p(filename.encode(\"utf-8\")),\n        )\n\n    def profile_with_tensors(\n        self,\n        inputs: Union[List[TorchTensor], Dict[str, TorchTensor]],\n        outputs: Union[List[TorchTensor], Dict[str, TorchTensor]],\n        num_iters: int,\n        filename: str,\n        stream_ptr: Optional[int] = None,\n    ) -> None:\n        _check_tensors_contiguous_and_on_gpu(\n            inputs,\n            name=\"inputs\",\n        )\n        _check_tensors_contiguous_and_on_gpu(\n            outputs,\n            name=\"outputs\",\n        )\n        self.profile(\n            _convert_tensor_args(inputs),\n            _convert_tensor_args(outputs),\n            num_iters,\n            filename,\n            stream_ptr,\n        )\n\n    def _interpret_tensors_as_shapes(\n        self,\n        outputs: Union[List[TorchTensor], Dict[str, TorchTensor]],\n        outputs_ait: Dict[str, AITData],\n    ) -> Dict[str, TorchTensor]:\n        if isinstance(outputs, dict):\n            return {\n                name: _reshape_tensor(tensor, outputs_ait[name].shape)\n                for name, tensor in outputs.items()\n            }\n        else:\n            return {\n                name: _reshape_tensor(outputs[idx], outputs_ait[name].shape)\n                for name, idx in self._output_name_to_index.items()\n            }\n\n    def run_with_tensors(\n        self,\n        inputs: Union[List[TorchTensor], Dict[str, TorchTensor]],\n        outputs: Union[List[TorchTensor], Dict[str, TorchTensor]],\n        stream_ptr: Optional[int] = None,\n        sync: bool = True,\n        graph_mode: bool = False,\n    ) -> Dict[str, TorchTensor]:\n        \"\"\"\n        Run the model with torch.Tensors. See Run() for information about the\n        arguments.\n\n        Inputs may either be a dictionary (name -> torch.Tensor), or a list\n        of torch.Tensors ordered according to GetInputNameToIndexMap. Outputs\n        can also be a dictionary, or a list ordered according to GetOutputNameToIndexMap.\n        \"\"\"\n\n        _check_tensors_contiguous_and_on_gpu(\n            inputs,\n            name=\"inputs\",\n        )\n        _check_tensors_contiguous_and_on_gpu(\n            outputs,\n            name=\"outputs\",\n        )\n        outputs_ait = self.run(\n            _convert_tensor_args(inputs),\n            _convert_tensor_args(outputs),\n            stream_ptr=stream_ptr,\n            sync=sync,\n            graph_mode=graph_mode,\n        )\n\n        return self._interpret_tensors_as_shapes(outputs, outputs_ait)\n\n    def _run_with_outputs_on_host(\n        self,\n        inputs: Union[Dict[str, AITData], List[AITData]],\n        outputs: Union[Dict[str, AITData], List[AITData]],\n        stream_ptr: Optional[int] = None,\n        graph_mode: bool = False,\n    ) -> Dict[str, AITData]:\n        \"\"\"\n        Like Run(), but takes host memory outputs. Note that there is no sync parameter;\n        the stream will always be synchronized after copying the outputs to the host.\n\n        Warning: don't use this! It's not optimal with respect to performance.\n        It's here for use if you need it for debugging purpose.\n        \"\"\"\n        return self._run_impl(\n            inputs, outputs, stream_ptr, graph_mode=graph_mode, outputs_on_host=True\n        )\n\n    def _run_with_tensors_outputs_on_host(\n        self,\n        inputs: Union[List[TorchTensor], Dict[str, TorchTensor]],\n        outputs: Union[List[TorchTensor], Dict[str, TorchTensor]],\n        stream_ptr: Optional[int] = None,\n        graph_mode: bool = False,\n    ) -> Dict[str, TorchTensor]:\n        \"\"\"\n        Like RunWithTensors(), but takes host memory tensors\n\n        Warning: don't use this! It's not optimal with respect to performance.\n        It's here for use if you need it for debugging.\n        \"\"\"\n        _check_tensors_contiguous_and_on_gpu(\n            inputs,\n            name=\"inputs\",\n        )\n        _check_tensors_contiguous_and_on_host(\n            outputs,\n            name=\"outputs\",\n        )\n        output_shapes = self._run_with_outputs_on_host(\n            _convert_tensor_args(inputs),\n            _convert_tensor_args(outputs),\n            stream_ptr=stream_ptr,\n            graph_mode=graph_mode,\n        )\n        return self._interpret_tensors_as_shapes(outputs, output_shapes)\n\n    def benchmark(\n        self,\n        inputs: Union[Dict[str, AITData], List[AITData]],\n        outputs: Union[Dict[str, AITData], List[AITData]],\n        stream_ptr: Optional[int] = None,\n        graph_mode: bool = False,\n        count: int = 10,\n        repeat: int = 1,\n        num_threads: int = 1,\n        use_unique_stream_per_thread: bool = False,\n    ) -> Tuple[float, float, Dict[str, AITData]]:\n        \"\"\"\n        Benchmark the model. See run() for information on most parameters.\n        \"\"\"\n        if isinstance(inputs, dict):\n            inputs = self._dict_to_ordered_list(inputs, is_inputs=True)\n        if isinstance(outputs, dict):\n            outputs = self._dict_to_ordered_list(outputs, is_inputs=False)\n        (c_inputs, c_outputs, c_stream, c_output_shapes_out,) = self._prepare_run(\n            inputs,\n            outputs,\n            stream_ptr,\n        )\n        time_ms = []\n        runtime_ms = ctypes.c_float()\n        for _ in range(repeat):\n            self.DLL.AITemplateModelContainerBenchmark(\n                self.handle,\n                c_inputs,\n                ctypes.c_size_t(len(inputs)),\n                c_outputs,\n                ctypes.c_size_t(len(outputs)),\n                c_stream,\n                ctypes.c_bool(graph_mode),\n                ctypes.c_size_t(count),\n                ctypes.c_size_t(num_threads),\n                ctypes.c_bool(use_unique_stream_per_thread),\n                ctypes.byref(runtime_ms),\n                c_output_shapes_out,\n            )\n            time_ms.append(runtime_ms.value)\n        mean = np.mean(time_ms)\n        std = np.std(time_ms)\n\n        return (mean, std, self._make_ait_outputs(outputs, c_output_shapes_out))\n\n    def benchmark_with_tensors(\n        self,\n        inputs: Union[List[TorchTensor], Dict[str, TorchTensor]],\n        outputs: Union[List[TorchTensor], Dict[str, TorchTensor]],\n        stream_ptr: Optional[int] = None,\n        graph_mode: bool = False,\n        count: int = 10,\n        repeat: int = 1,\n        num_threads: int = 1,\n        use_unique_stream_per_thread: bool = False,\n    ) -> Tuple[float, float, Dict[str, TorchTensor]]:\n        \"\"\"\n        Benchmark the model. See run_with_tensors() for information on most parameters.\n        \"\"\"\n\n        _check_tensors_contiguous_and_on_gpu(\n            inputs,\n            name=\"inputs\",\n        )\n        _check_tensors_contiguous_and_on_gpu(\n            outputs,\n            name=\"outputs\",\n        )\n\n        mean, std, ait_outputs = self.benchmark(\n            _convert_tensor_args(inputs),\n            _convert_tensor_args(outputs),\n            stream_ptr,\n            graph_mode,\n            count,\n            repeat,\n            num_threads,\n            use_unique_stream_per_thread,\n        )\n        return (mean, std, self._interpret_tensors_as_shapes(outputs, ait_outputs))\n\n    def _get_map_helper(self, n: int, get_name_func) -> Dict[str, int]:\n        result = {}\n        for i in range(n):\n            c_name = ctypes.c_char_p()\n            c_idx = ctypes.c_size_t(i)\n            get_name_func(c_idx, ctypes.byref(c_name))\n            name = c_name.value.decode(\"utf-8\")\n            result[name] = i\n        return result\n\n    def _construct_input_name_to_index_map(self) -> Dict[str, int]:\n        num_inputs = ctypes.c_size_t()\n        self.DLL.AITemplateModelContainerGetNumInputs(\n            self.handle, ctypes.byref(num_inputs)\n        )\n        get_input_name = (\n            lambda idx, name: self.DLL.AITemplateModelContainerGetInputName(\n                self.handle, idx, name\n            )\n        )\n        return self._get_map_helper(num_inputs.value, get_input_name)\n\n    def get_input_name_to_index_map(self) -> Dict[str, int]:\n        \"\"\"\n        Get the name to index mapping. Note that the ordering of inputs\n        is not guaranteed to be deterministic.\n\n        If using run()'s list interface, this ordering must be used!\n        \"\"\"\n        # Copy so people can't modify our version of the map\n        return self._input_name_to_index.copy()\n\n    def _construct_output_name_to_index_map(self) -> Dict[str, int]:\n        num_outputs = ctypes.c_size_t()\n        self.DLL.AITemplateModelContainerGetNumOutputs(\n            self.handle, ctypes.byref(num_outputs)\n        )\n        get_output_name = (\n            lambda idx, name: self.DLL.AITemplateModelContainerGetOutputName(\n                self.handle, idx, name\n            )\n        )\n        return self._get_map_helper(num_outputs.value, get_output_name)\n\n    def get_output_name_to_index_map(self) -> Dict[str, int]:\n        \"\"\"\n        Get the name to index mapping. Unlike inputs, outputs\n        have a guaranteed ordering; the order that outputs were\n        provided to `compile_model` is always used as the internal\n        name to index mapping.\n\n        If using run()'s list interface, this ordering must be used!\n        \"\"\"\n        # Copy so people can't modify our version of the map\n        return self._output_name_to_index.copy()\n\n    def set_constant(self, name: str, tensor: AITData):\n        \"\"\"\n        Set a constant. All constants must have values before calling run().\n\n        Note that the pointer inside tensor must be valid for the entire\n        duration of run().\n        \"\"\"\n        b_name = name.encode(\"utf-8\")\n        c_name = ctypes.c_char_p(b_name)\n        c_tensor = self._convert_single_param_to_c_format(tensor)\n        self.DLL.AITemplateModelContainerSetConstant(\n            self.handle, c_name, ctypes.byref(c_tensor)\n        )\n\n    def set_many_constants(self, tensors: Dict[str, AITData]):\n        \"\"\"\n        Bulk set many constants at once. More efficient than set_constant()\n        since it only has to acquire the lock once.\n        \"\"\"\n        c_names = (ctypes.c_char_p * len(tensors))()\n        c_tensors = (_CFormatAITData * len(tensors))()\n        ait_tensors = {\n            name.encode(\"utf-8\"): self._convert_single_param_to_c_format(tensor)\n            for name, tensor in tensors.items()\n        }\n        for i, (name_bytes, tensor) in enumerate(ait_tensors.items()):\n            c_names[i] = ctypes.c_char_p(name_bytes)\n            c_tensors[i] = tensor\n\n        num_tensors = ctypes.c_size_t(len(tensors))\n        self.DLL.AITemplateModelContainerSetManyConstants(\n            self.handle, c_names, c_tensors, num_tensors\n        )\n\n    def set_double_buffer_constant(\n        self, name: str, tensor: AITData, stream_ptr: Optional[int] = None\n    ):\n        \"\"\"\n        Set a constant. All constants must have values before calling run().\n\n        Note that the pointer inside tensor must be valid for the entire\n        duration of run().\n        \"\"\"\n        b_name = name.encode(\"utf-8\")\n        c_name = ctypes.c_char_p(b_name)\n        c_tensor = self._convert_single_param_to_c_format(tensor)\n        self.DLL.AITemplateModelContainerSetDoubleBufferConstant(\n            self.handle, ctypes.c_void_p(stream_ptr), c_name, ctypes.byref(c_tensor)\n        )\n\n    def set_many_double_buffer_constants(\n        self, tensors: Dict[str, AITData], stream_ptr: Optional[int] = None\n    ):\n        \"\"\"\n        Bulk set many constants at once. More efficient than set_constant()\n        since it only has to acquire the lock once.\n        \"\"\"\n        c_names = (ctypes.c_char_p * len(tensors))()\n        c_tensors = (_CFormatAITData * len(tensors))()\n        ait_tensors = {\n            name.encode(\"utf-8\"): self._convert_single_param_to_c_format(tensor)\n            for name, tensor in tensors.items()\n        }\n        for i, (name_bytes, tensor) in enumerate(ait_tensors.items()):\n            c_names[i] = ctypes.c_char_p(name_bytes)\n            c_tensors[i] = tensor\n\n        num_tensors = ctypes.c_size_t(len(tensors))\n        self.DLL.AITemplateModelContainerSetManyDoubleBufferConstants(\n            self.handle, ctypes.c_void_p(stream_ptr), c_names, c_tensors, num_tensors\n        )\n\n    def set_many_constants_with_tensors(self, tensors: Dict[str, TorchTensor]):\n        ait_tensors = {}\n        for name, tensor in tensors.items():\n            if not tensor.is_contiguous() or not tensor.is_cuda:\n                raise ValueError(f\"Constant {name} must be contiguous and on the GPU.\")\n            self.torch_constant_tensors[name] = tensor\n            ait_tensors[name] = torch_to_ait_data(tensor)\n        self.set_many_constants(ait_tensors)\n\n    def set_double_buffer_constant_with_tensor(\n        self, name: str, tensor: TorchTensor, stream_ptr: Optional[int] = None\n    ):\n        \"\"\"\n        Set a constant with a PyTorch tensor.\n        Model will store a reference to the given tensor in\n        torch_constant_tensors until it is explicitly deleted or replaced.\n        \"\"\"\n        if not tensor.is_contiguous() or not tensor.is_cuda:\n            raise ValueError(f\"Constant {name} must be contiguous and on the GPU.\")\n        self.torch_constant_tensors[name] = tensor\n        self.set_double_buffer_constant(name, torch_to_ait_data(tensor), stream_ptr)\n\n    def set_many_double_buffer_constants_with_tensors(\n        self, tensors: Dict[str, TorchTensor], stream_ptr: Optional[int] = None\n    ):\n        ait_tensors = {}\n        for name, tensor in tensors.items():\n            if not tensor.is_contiguous() or not tensor.is_cuda:\n                raise ValueError(f\"Constant {name} must be contiguous and on the GPU.\")\n            self.torch_constant_tensors[name] = tensor\n            ait_tensors[name] = torch_to_ait_data(tensor)\n        self.set_many_double_buffer_constants(ait_tensors, stream_ptr)\n\n    def set_constant_with_tensor(self, name: str, tensor: TorchTensor):\n        \"\"\"\n        Set a constant with a PyTorch tensor.\n        Model will store a reference to the given tensor in\n        torch_constant_tensors until it is explicitly deleted or replaced.\n        \"\"\"\n        if not tensor.is_contiguous() or not tensor.is_cuda:\n            raise ValueError(f\"Constant {name} must be contiguous and on the GPU.\")\n        self.torch_constant_tensors[name] = tensor\n        self.set_constant(name, torch_to_ait_data(tensor))\n\n    def get_output_maximum_shape(\n        self, output_idx_or_name: Union[int, str]\n    ) -> List[int]:\n        \"\"\"\n        Get the maximum output shape. The input here can either be an output name\n        or an index. The index is the runtime's internal index (as specified by\n        GetOutputNameToIndexMap)\n        \"\"\"\n        if isinstance(output_idx_or_name, int):\n            output_idx = output_idx_or_name\n        elif isinstance(output_idx_or_name, str):\n            if output_idx_or_name not in self._output_name_to_index:\n                raise ValueError(\n                    f\"Name {output_idx_or_name} not in OutputNameToIndexMap! Available names: {list(self._output_name_to_index.keys())}\"\n                )\n            output_idx = self._output_name_to_index[output_idx_or_name]\n        else:\n            raise TypeError(\n                f\"output_idx_or_name must be str or int, but got {type(output_idx_or_name)}\"\n            )\n\n        class Shape(ctypes.Structure):\n            _fields_ = [\n                (\"shape_data\", ctypes.POINTER(ctypes.c_longlong)),\n                (\"size\", ctypes.c_size_t),\n            ]\n\n        raw_shape = Shape()\n        self.DLL.AITemplateModelContainerGetMaximumOutputShape(\n            self.handle, output_idx, ctypes.byref(raw_shape)\n        )\n        return [raw_shape.shape_data[idx] for idx in range(raw_shape.size)]\n\n    def get_output_dtype(self, index):\n        \"\"\"\n        Get the expected dtype of an output.\n        \"\"\"\n        output = ctypes.c_int()\n        self.DLL.AITemplateModelContainerGetOutputDtype(\n            self.handle, index, ctypes.byref(output)\n        )\n        return output.value\n\n    def allocate_gpu_memory(\n        self, nbytes: int, stream_ptr: Optional[int] = None, sync: bool = True\n    ) -> int:\n        \"\"\"\n        Helper function for allocating memory on the GPU. Can be useful if\n        third-party libraries like PyTorch or pycuda are not available.\n\n        The pointer returned by this function must be freed by free_gpu_memory\n        to avoid memory leaks.\n        \"\"\"\n        ptr = ctypes.c_void_p()\n        self.DLL.AITemplateDeviceMalloc(\n            ctypes.byref(ptr),\n            ctypes.c_size_t(nbytes),\n            ctypes.c_void_p(stream_ptr),\n            ctypes.c_bool(sync),\n        )\n        return ptr.value\n\n    def free_gpu_memory(\n        self, ptr: int, stream_ptr: Optional[int] = None, sync: bool = True\n    ) -> None:\n        \"\"\"\n        Helper function for freeing memory on the GPU. Can be useful if\n        third-party libraries like PyTorch or pycuda are not available.\n        \"\"\"\n        if ptr in self._allocated_ait_data:\n            self._allocated_ait_data.remove(ptr)\n\n        self.DLL.AITemplateDeviceFree(\n            ctypes.c_void_p(ptr), ctypes.c_void_p(stream_ptr), ctypes.c_bool(sync)\n        )\n\n    def memcpy(\n        self,\n        dst: int,\n        src: int,\n        count: int,\n        kind: AITemplateMemcpyKind,\n        stream_ptr: Optional[int] = None,\n        sync: bool = True,\n    ) -> None:\n        \"\"\"\n        Helper function for copying memory on the GPU. Can be useful if\n        third-party libraries like PyTorch or pycuda are not available.\n\n        Supports D2H, H2D, and D2D copies. The copy direction can be\n        specified by the AITemplateMemcpyKind enum.\n        \"\"\"\n        self.DLL.AITemplateMemcpy(\n            ctypes.c_void_p(dst),\n            ctypes.c_void_p(src),\n            ctypes.c_size_t(count),\n            ctypes.c_int(kind.value),\n            ctypes.c_void_p(stream_ptr),\n            ctypes.c_bool(sync),\n        )\n\n    def get_num_runtimes(self) -> int:\n        \"\"\"\n        Get the number of runtimes this model container stores.\n        \"\"\"\n        out = ctypes.c_size_t()\n        self.DLL.AITemplateModelContainerGetNumRuntimes(self.handle, ctypes.byref(out))\n        return out.value\n\n    def numpy_to_ait_data(\n        self, arr: np.ndarray, stream_ptr: Optional[int] = None, sync: bool = True\n    ) -> AITData:\n        \"\"\"\n        Convert a numpy array to AIT-usable data. Mallocs and copies\n        on the given stream.\n\n        The allocated buffer should be manually freed with free_gpu_memory.\n        But, in case of misuse, Model will keep track of pointers allocated with\n        this method and free them all at the end.\n        \"\"\"\n        dtype = str(arr.dtype)\n        shape = list(arr.shape)\n        gpu_mem = self.allocate_gpu_memory(arr.nbytes, stream_ptr=stream_ptr, sync=sync)\n        self._allocated_ait_data.add(gpu_mem)\n        self.memcpy(\n            gpu_mem,\n            arr.ctypes._data.value,\n            arr.nbytes,\n            AITemplateMemcpyKind.HostToDevice,\n            sync=sync,\n            stream_ptr=stream_ptr,\n        )\n        return AITData(gpu_mem, shape, dtype)\n\n    def ait_data_to_numpy(\n        self,\n        ait_data: AITData,\n        stream_ptr: Optional[int] = None,\n        sync: bool = True,\n    ) -> np.ndarray:\n        \"\"\"\n        Create numpy array from an AITData.\n        Copies on the given stream.\n        \"\"\"\n        arr = np.empty(ait_data.shape, dtype=ait_data.dtype)\n        self.memcpy(\n            arr.ctypes._data.value,\n            ait_data.data_ptr,\n            arr.nbytes,\n            AITemplateMemcpyKind.DeviceToHost,\n            sync=sync,\n            stream_ptr=stream_ptr,\n        )\n        return arr\n\n    def fold_constants(\n        self,\n        stream_ptr: Optional[int] = None,\n        sync: bool = True,\n        double_buffer: bool = False,\n    ):\n        if double_buffer:\n            self.DLL.AITemplateModelContainerFoldConstantsInDoubleBuffer(\n                self.handle,\n                ctypes.c_void_p(stream_ptr),\n                ctypes.c_bool(sync),\n            )\n        else:\n            self.DLL.AITemplateModelContainerFoldConstants(\n                self.handle,\n                ctypes.c_void_p(stream_ptr),\n                ctypes.c_bool(sync),\n            )\n\n    def swap_constants(self):\n        self.DLL.AITemplateModelContainerSwapConstants(self.handle)\n\n    def _get_constant_names_impl(\n        self, unbound_constants_only: bool, constant_folding_only: bool\n    ) -> List[str]:\n        num_constants = ctypes.c_size_t()\n        constant_folding_inputs_only = ctypes.c_bool(constant_folding_only)\n        unbound_constants_only_ = ctypes.c_bool(unbound_constants_only)\n        self.DLL.AITemplateModelContainerGetNumConstants(\n            self.handle,\n            unbound_constants_only_,\n            constant_folding_inputs_only,\n            ctypes.byref(num_constants),\n        )\n        names = (ctypes.c_char_p * num_constants.value)()\n        self.DLL.AITemplateModelContainerGetConstantNames(\n            self.handle, unbound_constants_only_, constant_folding_inputs_only, names\n        )\n        return [name.decode(\"utf-8\") for name in names]\n\n    def get_constant_names(\n        self, unbound_constants_only: bool = True, constant_folding_only: bool = False\n    ) -> List[str]:\n        return self._get_constant_names_impl(\n            unbound_constants_only, constant_folding_only\n        )\n\n    def get_constant_folding_input_names(\n        self, unbound_constants_only: bool = True\n    ) -> List[str]:\n        return self._get_constant_names_impl(unbound_constants_only, True)", ""]}
{"filename": "AITemplate/ait/module/torch_utils.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\"\"\"\nFunctions for working with torch Tensors.\nAITemplate doesn't depend on PyTorch, but it exposes\nmany APIs that work with torch Tensors anyways.", "AITemplate doesn't depend on PyTorch, but it exposes\nmany APIs that work with torch Tensors anyways.\n\nThe functions in this file may assume that\n`import torch` will work.\n\"\"\"\n\n\ndef types_mapping():\n    from torch import bfloat16, bool, float16, float32, int32, int64\n\n    yield (float16, \"float16\")\n    yield (bfloat16, \"bfloat16\")\n    yield (float32, \"float32\")\n    yield (int32, \"int32\")\n    yield (int64, \"int64\")\n    yield (bool, \"bool\")", "def types_mapping():\n    from torch import bfloat16, bool, float16, float32, int32, int64\n\n    yield (float16, \"float16\")\n    yield (bfloat16, \"bfloat16\")\n    yield (float32, \"float32\")\n    yield (int32, \"int32\")\n    yield (int64, \"int64\")\n    yield (bool, \"bool\")\n", "\n\ndef torch_dtype_to_string(dtype):\n    for (torch_dtype, ait_dtype) in types_mapping():\n        if dtype == torch_dtype:\n            return ait_dtype\n    raise ValueError(\n        f\"Got unsupported input dtype {dtype}! \"\n        f\"Supported dtypes are: {list(types_mapping())}\"\n    )", "\n\ndef string_to_torch_dtype(string_dtype):\n    if string_dtype is None:\n        # Many torch functions take optional dtypes, so\n        # handling None is useful here.\n        return None\n\n    for (torch_dtype, ait_dtype) in types_mapping():\n        if string_dtype == ait_dtype:\n            return torch_dtype\n    raise ValueError(\n        f\"Got unsupported ait dtype {string_dtype}! \"\n        f\"Supported dtypes are: {list(types_mapping())}\"\n    )", ""]}
{"filename": "AITemplate/ait/module/dtype.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\"\"\"\ndtype definitions and utility functions of AITemplate\n\"\"\"\n", "\"\"\"\n\n\n_DTYPE2BYTE = {\n    \"bool\": 1,\n    \"float16\": 2,\n    \"float32\": 4,\n    \"float\": 4,\n    \"int\": 4,\n    \"int32\": 4,", "    \"int\": 4,\n    \"int32\": 4,\n    \"int64\": 8,\n    \"bfloat16\": 2,\n}\n\n\n# Maps dtype strings to AITemplateDtype enum in model_interface.h.\n# Must be kept in sync!\n# We can consider defining an AITemplateDtype enum to use on the Python", "# Must be kept in sync!\n# We can consider defining an AITemplateDtype enum to use on the Python\n# side at some point, but stick to strings for now to keep things consistent\n# with other Python APIs.\n_DTYPE_TO_ENUM = {\n    \"float16\": 1,\n    \"float32\": 2,\n    \"float\": 2,\n    \"int\": 3,\n    \"int32\": 3,", "    \"int\": 3,\n    \"int32\": 3,\n    \"int64\": 4,\n    \"bool\": 5,\n    \"bfloat16\": 6,\n}\n\n\ndef get_dtype_size(dtype: str) -> int:\n    \"\"\"Returns size (in bytes) of the given dtype str.\n\n    Parameters\n    ----------\n    dtype: str\n        A data type string.\n\n    Returns\n    ----------\n    int\n        Size (in bytes) of this dtype.\n    \"\"\"\n\n    if dtype not in _DTYPE2BYTE:\n        raise KeyError(f\"Unknown dtype: {dtype}. Expected one of {_DTYPE2BYTE.keys()}\")\n    return _DTYPE2BYTE[dtype]", "def get_dtype_size(dtype: str) -> int:\n    \"\"\"Returns size (in bytes) of the given dtype str.\n\n    Parameters\n    ----------\n    dtype: str\n        A data type string.\n\n    Returns\n    ----------\n    int\n        Size (in bytes) of this dtype.\n    \"\"\"\n\n    if dtype not in _DTYPE2BYTE:\n        raise KeyError(f\"Unknown dtype: {dtype}. Expected one of {_DTYPE2BYTE.keys()}\")\n    return _DTYPE2BYTE[dtype]", "\n\ndef normalize_dtype(dtype: str) -> str:\n    \"\"\"Returns a normalized dtype str.\n\n    Parameters\n    ----------\n    dtype: str\n        A data type string.\n\n    Returns\n    ----------\n    str\n        normalized dtype str.\n    \"\"\"\n    if dtype == \"int\":\n        return \"int32\"\n    if dtype == \"float\":\n        return \"float32\"\n    return dtype", "\n\ndef dtype_str_to_enum(dtype: str) -> int:\n    \"\"\"Returns the AITemplateDtype enum value (defined in model_interface.h) of\n    the given dtype str.\n\n    Parameters\n    ----------\n    dtype: str\n        A data type string.\n\n    Returns\n    ----------\n    int\n        the AITemplateDtype enum value.\n    \"\"\"\n    if dtype not in _DTYPE_TO_ENUM:\n        raise ValueError(\n            f\"Got unsupported input dtype {dtype}! Supported dtypes are: {list(_DTYPE_TO_ENUM.keys())}\"\n        )\n    return _DTYPE_TO_ENUM[dtype]", "\n\ndef dtype_to_enumerator(dtype: str) -> str:\n    \"\"\"Returns the string representation of the AITemplateDtype enum\n    (defined in model_interface.h) for the given dtype str.\n\n    Parameters\n    ----------\n    dtype: str\n        A data type string.\n\n    Returns\n    ----------\n    str\n        the AITemplateDtype enum string representation.\n    \"\"\"\n\n    def _impl(dtype):\n        if dtype == \"float16\":\n            return \"kHalf\"\n        elif dtype == \"float32\" or dtype == \"float\":\n            return \"kFloat\"\n        elif dtype == \"int32\" or dtype == \"int\":\n            return \"kInt\"\n        elif dtype == \"int64\":\n            return \"kLong\"\n        elif dtype == \"bool\":\n            return \"kBool\"\n        elif dtype == \"bfloat16\":\n            return \"kBFloat16\"\n        else:\n            raise AssertionError(f\"unknown dtype {dtype}\")\n\n    return f\"AITemplateDtype::{_impl(dtype)}\"", "\n\ndef is_same_dtype(dtype1: str, dtype2: str) -> bool:\n    \"\"\"Returns True if dtype1 and dtype2 are the same dtype and False otherwise.\n\n    Parameters\n    ----------\n    dtype1: str\n        A data type string.\n    dtype2: str\n        A data type string.\n\n    Returns\n    ----------\n    bool\n        whether dtype1 and dtype2 are the same dtype\n    \"\"\"\n    return normalize_dtype(dtype1) == normalize_dtype(dtype2)", ""]}
{"filename": "AITemplate/ait/module/__init__.py", "chunked_list": ["from .model import Model\n\n__all__ = [\"Model\"]\n"]}
{"filename": "AITemplate/ait/module/misc.py", "chunked_list": ["#  Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\"\"\"\nmiscellaneous utilities\n\"\"\"\nimport hashlib", "\"\"\"\nimport hashlib\nimport logging\nimport os\nimport platform\n\n\ndef is_debug():\n    logger = logging.getLogger(\"aitemplate\")\n    return logger.level == logging.DEBUG", "\n\ndef is_linux() -> bool:\n    return platform.system() == \"Linux\"\n\n\ndef is_windows() -> bool:\n    return os.name == \"nt\"\n\n\ndef setup_logger(name):\n    root_logger = logging.getLogger(name)\n    info_handle = logging.StreamHandler()\n    formatter = logging.Formatter(\"%(asctime)s %(levelname)s <%(name)s> %(message)s\")\n    info_handle.setFormatter(formatter)\n    root_logger.addHandler(info_handle)\n    root_logger.propagate = False\n\n    DEFAULT_LOGLEVEL = logging.getLogger().level\n    log_level_str = os.environ.get(\"LOGLEVEL\", None)\n    LOG_LEVEL = (\n        getattr(logging, log_level_str.upper())\n        if log_level_str is not None\n        else DEFAULT_LOGLEVEL\n    )\n    root_logger.setLevel(LOG_LEVEL)\n    return root_logger", "\n\ndef setup_logger(name):\n    root_logger = logging.getLogger(name)\n    info_handle = logging.StreamHandler()\n    formatter = logging.Formatter(\"%(asctime)s %(levelname)s <%(name)s> %(message)s\")\n    info_handle.setFormatter(formatter)\n    root_logger.addHandler(info_handle)\n    root_logger.propagate = False\n\n    DEFAULT_LOGLEVEL = logging.getLogger().level\n    log_level_str = os.environ.get(\"LOGLEVEL\", None)\n    LOG_LEVEL = (\n        getattr(logging, log_level_str.upper())\n        if log_level_str is not None\n        else DEFAULT_LOGLEVEL\n    )\n    root_logger.setLevel(LOG_LEVEL)\n    return root_logger", "\n\ndef short_str(s, length=8) -> str:\n    \"\"\"\n    Returns a hashed string, somewhat similar to URL shortener.\n    \"\"\"\n    hash_str = hashlib.sha256(s.encode()).hexdigest()\n    return hash_str[0:length]\n\n\ndef callstack_stats(enable=False):\n    if enable:\n\n        def decorator(f):\n            import cProfile\n            import io\n            import pstats\n\n            logger = logging.getLogger(__name__)\n\n            def inner_function(*args, **kwargs):\n                pr = cProfile.Profile()\n                pr.enable()\n                result = f(*args, **kwargs)\n                pr.disable()\n                s = io.StringIO()\n                pstats.Stats(pr, stream=s).sort_stats(\n                    pstats.SortKey.CUMULATIVE\n                ).print_stats(30)\n                logger.debug(s.getvalue())\n                return result\n\n            return inner_function\n\n        return decorator\n    else:\n\n        def decorator(f):\n            def inner_function(*args, **kwargs):\n                return f(*args, **kwargs)\n\n            return inner_function\n\n        return decorator", "\n\ndef callstack_stats(enable=False):\n    if enable:\n\n        def decorator(f):\n            import cProfile\n            import io\n            import pstats\n\n            logger = logging.getLogger(__name__)\n\n            def inner_function(*args, **kwargs):\n                pr = cProfile.Profile()\n                pr.enable()\n                result = f(*args, **kwargs)\n                pr.disable()\n                s = io.StringIO()\n                pstats.Stats(pr, stream=s).sort_stats(\n                    pstats.SortKey.CUMULATIVE\n                ).print_stats(30)\n                logger.debug(s.getvalue())\n                return result\n\n            return inner_function\n\n        return decorator\n    else:\n\n        def decorator(f):\n            def inner_function(*args, **kwargs):\n                return f(*args, **kwargs)\n\n            return inner_function\n\n        return decorator", ""]}
{"filename": "AITemplate/ait/util/torch_dtype_from_str.py", "chunked_list": ["import torch\n\ndef torch_dtype_from_str(dtype: str):\n    return torch.__dict__.get(dtype, None)"]}
{"filename": "AITemplate/ait/util/ckpt_convert.py", "chunked_list": ["import re\n\ndef assign_to_checkpoint(\n    paths, checkpoint, old_checkpoint, additional_replacements=None\n):\n    \"\"\"\n    This does the final conversion step: take locally converted weights and apply a global renaming to them. It splits\n    attention layers, and takes into account additional replacements that may arise.\n\n    Assigns the weights to the new checkpoint.\n    \"\"\"\n    assert isinstance(paths, list), \"Paths should be a list of dicts containing 'old' and 'new' keys.\"\n\n    for path in paths:\n        new_path = path[\"new\"]\n\n        # Global renaming happens here\n        new_path = new_path.replace(\"middle_block.0\", \"mid_block.resnets.0\")\n        new_path = new_path.replace(\"middle_block.1\", \"mid_block.attentions.0\")\n        new_path = new_path.replace(\"middle_block.2\", \"mid_block.resnets.1\")\n\n        if additional_replacements is not None:\n            for replacement in additional_replacements:\n                new_path = new_path.replace(replacement[\"old\"], replacement[\"new\"])\n\n        # proj_attn.weight has to be converted from conv 1D to linear\n        if \"proj_attn.weight\" in new_path:\n            checkpoint[new_path] = old_checkpoint[path[\"old\"]][:, :, 0]\n        else:\n            checkpoint[new_path] = old_checkpoint[path[\"old\"]]", "\n\ndef conv_attn_to_linear(checkpoint):\n    keys = list(checkpoint.keys())\n    attn_keys = [\"query.weight\", \"key.weight\", \"value.weight\"]\n    for key in keys:\n        if \".\".join(key.split(\".\")[-2:]) in attn_keys:\n            if checkpoint[key].ndim > 2:\n                checkpoint[key] = checkpoint[key][:, :, 0, 0]\n        elif \"proj_attn.weight\" in key:\n            if checkpoint[key].ndim > 2:\n                checkpoint[key] = checkpoint[key][:, :, 0]", "\n\ndef renew_attention_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside attentions to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        #         new_item = new_item.replace('norm.weight', 'group_norm.weight')\n        #         new_item = new_item.replace('norm.bias', 'group_norm.bias')\n\n        #         new_item = new_item.replace('proj_out.weight', 'proj_attn.weight')\n        #         new_item = new_item.replace('proj_out.bias', 'proj_attn.bias')\n\n        #         new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping", "\n\ndef renew_vae_attention_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside attentions to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        new_item = new_item.replace(\"norm.weight\", \"group_norm.weight\")\n        new_item = new_item.replace(\"norm.bias\", \"group_norm.bias\")\n\n        new_item = new_item.replace(\"q.weight\", \"query.weight\")\n        new_item = new_item.replace(\"q.bias\", \"query.bias\")\n\n        new_item = new_item.replace(\"k.weight\", \"key.weight\")\n        new_item = new_item.replace(\"k.bias\", \"key.bias\")\n\n        new_item = new_item.replace(\"v.weight\", \"value.weight\")\n        new_item = new_item.replace(\"v.bias\", \"value.bias\")\n\n        new_item = new_item.replace(\"proj_out.weight\", \"proj_attn.weight\")\n        new_item = new_item.replace(\"proj_out.bias\", \"proj_attn.bias\")\n\n        new_item = shave_segments(\n            new_item, n_shave_prefix_segments=n_shave_prefix_segments\n        )\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping", "\n\ndef shave_segments(path, n_shave_prefix_segments=1):\n    \"\"\"\n    Removes segments. Positive values shave the first segments, negative shave the last segments.\n    \"\"\"\n    if n_shave_prefix_segments >= 0:\n        return \".\".join(path.split(\".\")[n_shave_prefix_segments:])\n    else:\n        return \".\".join(path.split(\".\")[:n_shave_prefix_segments])", "\n\ndef renew_resnet_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside resnets to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item.replace(\"in_layers.0\", \"norm1\")\n        new_item = new_item.replace(\"in_layers.2\", \"conv1\")\n\n        new_item = new_item.replace(\"out_layers.0\", \"norm2\")\n        new_item = new_item.replace(\"out_layers.3\", \"conv2\")\n\n        new_item = new_item.replace(\"emb_layers.1\", \"time_emb_proj\")\n        new_item = new_item.replace(\"skip_connection\", \"conv_shortcut\")\n\n        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping", "\n\ndef renew_vae_resnet_paths(old_list, n_shave_prefix_segments=0):\n    \"\"\"\n    Updates paths inside resnets to the new naming scheme (local renaming)\n    \"\"\"\n    mapping = []\n    for old_item in old_list:\n        new_item = old_item\n\n        new_item = new_item.replace(\"nin_shortcut\", \"conv_shortcut\")\n        new_item = shave_segments(\n            new_item, n_shave_prefix_segments=n_shave_prefix_segments\n        )\n\n        mapping.append({\"old\": old_item, \"new\": new_item})\n\n    return mapping", "\n\ndef convert_ldm_unet_checkpoint(unet_state_dict, layers_per_block=2, controlnet=False):\n    \"\"\"\n    Takes a state dict and a config, and returns a converted checkpoint.\n    \"\"\"\n    temp = {}\n    if controlnet:\n        unet_keys = [\"control_model.\"]\n    else:\n        unet_keys = [\"model.diffusion_model.\", \"diffusion_model.\"]\n\n    for key, value in unet_state_dict.items():\n        for unet_key in unet_keys:\n            key = key.replace(unet_key, \"\")\n        temp[key] = value\n    unet_state_dict = temp\n    new_checkpoint = {}\n\n    new_checkpoint[\"time_embedding.linear_1.weight\"] = unet_state_dict[\"time_embed.0.weight\"]\n    new_checkpoint[\"time_embedding.linear_1.bias\"] = unet_state_dict[\"time_embed.0.bias\"]\n    new_checkpoint[\"time_embedding.linear_2.weight\"] = unet_state_dict[\"time_embed.2.weight\"]\n    new_checkpoint[\"time_embedding.linear_2.bias\"] = unet_state_dict[\"time_embed.2.bias\"]\n\n    if \"label_emb.0.0.weight\" in unet_state_dict:\n        new_checkpoint[\"add_embedding.linear_1.weight\"] = unet_state_dict[\"label_emb.0.0.weight\"]\n        new_checkpoint[\"add_embedding.linear_1.bias\"] = unet_state_dict[\"label_emb.0.0.bias\"]\n        new_checkpoint[\"add_embedding.linear_2.weight\"] = unet_state_dict[\"label_emb.0.2.weight\"]\n        new_checkpoint[\"add_embedding.linear_2.bias\"] = unet_state_dict[\"label_emb.0.2.bias\"]\n\n    new_checkpoint[\"conv_in.weight\"] = unet_state_dict[\"input_blocks.0.0.weight\"]\n    new_checkpoint[\"conv_in.bias\"] = unet_state_dict[\"input_blocks.0.0.bias\"]\n    if not controlnet:\n        new_checkpoint[\"conv_norm_out.weight\"] = unet_state_dict[\"out.0.weight\"]\n        new_checkpoint[\"conv_norm_out.bias\"] = unet_state_dict[\"out.0.bias\"]\n        new_checkpoint[\"conv_out.weight\"] = unet_state_dict[\"out.2.weight\"]\n        new_checkpoint[\"conv_out.bias\"] = unet_state_dict[\"out.2.bias\"]\n\n    # Retrieves the keys for the input blocks only\n    num_input_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in unet_state_dict if \"input_blocks\" in layer})\n    input_blocks = {\n        layer_id: [key for key in unet_state_dict if f\"input_blocks.{layer_id}\" in key]\n        for layer_id in range(num_input_blocks)\n    }\n\n    # Retrieves the keys for the middle blocks only\n    num_middle_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in unet_state_dict if \"middle_block\" in layer})\n    middle_blocks = {\n        layer_id: [key for key in unet_state_dict if f\"middle_block.{layer_id}\" in key]\n        for layer_id in range(num_middle_blocks)\n    }\n\n    # Retrieves the keys for the output blocks only\n    num_output_blocks = len({\".\".join(layer.split(\".\")[:2]) for layer in unet_state_dict if \"output_blocks\" in layer})\n    output_blocks = {\n        layer_id: [key for key in unet_state_dict if f\"output_blocks.{layer_id}\" in key]\n        for layer_id in range(num_output_blocks)\n    }\n\n    for i in range(1, num_input_blocks):\n        block_id = (i - 1) // (layers_per_block + 1)\n        layer_in_block_id = (i - 1) % (layers_per_block + 1)\n\n        resnets = [\n            key for key in input_blocks[i] if f\"input_blocks.{i}.0\" in key and f\"input_blocks.{i}.0.op\" not in key\n        ]\n        attentions = [key for key in input_blocks[i] if f\"input_blocks.{i}.1\" in key]\n\n        if f\"input_blocks.{i}.0.op.weight\" in unet_state_dict:\n            new_checkpoint[f\"down_blocks.{block_id}.downsamplers.0.conv.weight\"] = unet_state_dict.pop(\n                f\"input_blocks.{i}.0.op.weight\"\n            )\n            new_checkpoint[f\"down_blocks.{block_id}.downsamplers.0.conv.bias\"] = unet_state_dict.pop(\n                f\"input_blocks.{i}.0.op.bias\"\n            )\n\n        paths = renew_resnet_paths(resnets)\n        meta_path = {\"old\": f\"input_blocks.{i}.0\", \"new\": f\"down_blocks.{block_id}.resnets.{layer_in_block_id}\"}\n        assign_to_checkpoint(\n            paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path]\n        )\n\n        if len(attentions):\n            paths = renew_attention_paths(attentions)\n            meta_path = {\"old\": f\"input_blocks.{i}.1\", \"new\": f\"down_blocks.{block_id}.attentions.{layer_in_block_id}\"}\n            assign_to_checkpoint(\n                paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path]\n            )\n\n    resnet_0 = middle_blocks[0]\n    attentions = middle_blocks[1]\n    resnet_1 = middle_blocks[2]\n\n    resnet_0_paths = renew_resnet_paths(resnet_0)\n    assign_to_checkpoint(resnet_0_paths, new_checkpoint, unet_state_dict)\n\n    resnet_1_paths = renew_resnet_paths(resnet_1)\n    assign_to_checkpoint(resnet_1_paths, new_checkpoint, unet_state_dict)\n\n    attentions_paths = renew_attention_paths(attentions)\n    meta_path = {\"old\": \"middle_block.1\", \"new\": \"mid_block.attentions.0\"}\n    assign_to_checkpoint(\n        attentions_paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path]\n    )\n\n    for i in range(num_output_blocks):\n        block_id = i // (layers_per_block + 1)\n        layer_in_block_id = i % (layers_per_block + 1)\n        output_block_layers = [shave_segments(name, 2) for name in output_blocks[i]]\n        output_block_list = {}\n\n        for layer in output_block_layers:\n            layer_id, layer_name = layer.split(\".\")[0], shave_segments(layer, 1)\n            if layer_id in output_block_list:\n                output_block_list[layer_id].append(layer_name)\n            else:\n                output_block_list[layer_id] = [layer_name]\n\n        if len(output_block_list) > 1:\n            resnets = [key for key in output_blocks[i] if f\"output_blocks.{i}.0\" in key]\n            attentions = [key for key in output_blocks[i] if f\"output_blocks.{i}.1\" in key]\n\n            resnet_0_paths = renew_resnet_paths(resnets)\n            paths = renew_resnet_paths(resnets)\n\n            meta_path = {\"old\": f\"output_blocks.{i}.0\", \"new\": f\"up_blocks.{block_id}.resnets.{layer_in_block_id}\"}\n            assign_to_checkpoint(\n                paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path]\n            )\n\n            output_block_list = {k: sorted(v) for k, v in output_block_list.items()}\n            if [\"conv.bias\", \"conv.weight\"] in output_block_list.values():\n                index = list(output_block_list.values()).index([\"conv.bias\", \"conv.weight\"])\n                new_checkpoint[f\"up_blocks.{block_id}.upsamplers.0.conv.weight\"] = unet_state_dict[\n                    f\"output_blocks.{i}.{index}.conv.weight\"\n                ]\n                new_checkpoint[f\"up_blocks.{block_id}.upsamplers.0.conv.bias\"] = unet_state_dict[\n                    f\"output_blocks.{i}.{index}.conv.bias\"\n                ]\n\n                # Clear attentions as they have been attributed above.\n                if len(attentions) == 2:\n                    attentions = []\n\n            if len(attentions):\n                paths = renew_attention_paths(attentions)\n                meta_path = {\n                    \"old\": f\"output_blocks.{i}.1\",\n                    \"new\": f\"up_blocks.{block_id}.attentions.{layer_in_block_id}\",\n                }\n                assign_to_checkpoint(\n                    paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path]\n                )\n        else:\n            resnet_0_paths = renew_resnet_paths(output_block_layers, n_shave_prefix_segments=1)\n            for path in resnet_0_paths:\n                old_path = \".\".join([\"output_blocks\", str(i), path[\"old\"]])\n                new_path = \".\".join([\"up_blocks\", str(block_id), \"resnets\", str(layer_in_block_id), path[\"new\"]])\n\n                new_checkpoint[new_path] = unet_state_dict[old_path]\n\n    if controlnet:\n        # conditioning embedding\n\n        orig_index = 0\n\n        new_checkpoint[\"controlnet_cond_embedding.conv_in.weight\"] = unet_state_dict.pop(\n            f\"input_hint_block.{orig_index}.weight\"\n        )\n        new_checkpoint[\"controlnet_cond_embedding.conv_in.bias\"] = unet_state_dict.pop(\n            f\"input_hint_block.{orig_index}.bias\"\n        )\n\n        orig_index += 2\n\n        diffusers_index = 0\n\n        while diffusers_index < 6:\n            new_checkpoint[f\"controlnet_cond_embedding.blocks.{diffusers_index}.weight\"] = unet_state_dict.pop(\n                f\"input_hint_block.{orig_index}.weight\"\n            )\n            new_checkpoint[f\"controlnet_cond_embedding.blocks.{diffusers_index}.bias\"] = unet_state_dict.pop(\n                f\"input_hint_block.{orig_index}.bias\"\n            )\n            diffusers_index += 1\n            orig_index += 2\n\n        new_checkpoint[\"controlnet_cond_embedding.conv_out.weight\"] = unet_state_dict.pop(\n            f\"input_hint_block.{orig_index}.weight\"\n        )\n        new_checkpoint[\"controlnet_cond_embedding.conv_out.bias\"] = unet_state_dict.pop(\n            f\"input_hint_block.{orig_index}.bias\"\n        )\n\n        # down blocks\n        for i in range(num_input_blocks):\n            new_checkpoint[f\"controlnet_down_blocks.{i}.weight\"] = unet_state_dict.pop(f\"zero_convs.{i}.0.weight\")\n            new_checkpoint[f\"controlnet_down_blocks.{i}.bias\"] = unet_state_dict.pop(f\"zero_convs.{i}.0.bias\")\n\n        # mid block\n        new_checkpoint[\"controlnet_mid_block.weight\"] = unet_state_dict.pop(\"middle_block_out.0.weight\")\n        new_checkpoint[\"controlnet_mid_block.bias\"] = unet_state_dict.pop(\"middle_block_out.0.bias\")\n\n\n    return new_checkpoint", "\n\ndef convert_ldm_vae_checkpoint(vae_state_dict):\n    temp = {}\n    vae_key = \"first_stage_model.\"\n\n    for key, value in vae_state_dict.items():\n        if key.startswith(vae_key):\n            key = key.replace(vae_key, \"\")\n        temp[key] = value\n    vae_state_dict = temp\n\n    new_checkpoint = {}\n\n    new_checkpoint[\"encoder.conv_in.weight\"] = vae_state_dict[\"encoder.conv_in.weight\"]\n    new_checkpoint[\"encoder.conv_in.bias\"] = vae_state_dict[\"encoder.conv_in.bias\"]\n    new_checkpoint[\"encoder.conv_out.weight\"] = vae_state_dict[\n        \"encoder.conv_out.weight\"\n    ]\n    new_checkpoint[\"encoder.conv_out.bias\"] = vae_state_dict[\"encoder.conv_out.bias\"]\n    new_checkpoint[\"encoder.conv_norm_out.weight\"] = vae_state_dict[\n        \"encoder.norm_out.weight\"\n    ]\n    new_checkpoint[\"encoder.conv_norm_out.bias\"] = vae_state_dict[\n        \"encoder.norm_out.bias\"\n    ]\n\n    new_checkpoint[\"decoder.conv_in.weight\"] = vae_state_dict[\"decoder.conv_in.weight\"]\n    new_checkpoint[\"decoder.conv_in.bias\"] = vae_state_dict[\"decoder.conv_in.bias\"]\n    new_checkpoint[\"decoder.conv_out.weight\"] = vae_state_dict[\n        \"decoder.conv_out.weight\"\n    ]\n    new_checkpoint[\"decoder.conv_out.bias\"] = vae_state_dict[\"decoder.conv_out.bias\"]\n    new_checkpoint[\"decoder.conv_norm_out.weight\"] = vae_state_dict[\n        \"decoder.norm_out.weight\"\n    ]\n    new_checkpoint[\"decoder.conv_norm_out.bias\"] = vae_state_dict[\n        \"decoder.norm_out.bias\"\n    ]\n\n    new_checkpoint[\"quant_conv.weight\"] = vae_state_dict[\"quant_conv.weight\"]\n    new_checkpoint[\"quant_conv.bias\"] = vae_state_dict[\"quant_conv.bias\"]\n    new_checkpoint[\"post_quant_conv.weight\"] = vae_state_dict[\"post_quant_conv.weight\"]\n    new_checkpoint[\"post_quant_conv.bias\"] = vae_state_dict[\"post_quant_conv.bias\"]\n\n    # Retrieves the keys for the encoder down blocks only\n    num_down_blocks = len(\n        {\n            \".\".join(layer.split(\".\")[:3])\n            for layer in vae_state_dict\n            if \"encoder.down\" in layer\n        }\n    )\n    down_blocks = {\n        layer_id: [key for key in vae_state_dict if f\"down.{layer_id}\" in key]\n        for layer_id in range(num_down_blocks)\n    }\n\n    # Retrieves the keys for the decoder up blocks only\n    num_up_blocks = len(\n        {\n            \".\".join(layer.split(\".\")[:3])\n            for layer in vae_state_dict\n            if \"decoder.up\" in layer\n        }\n    )\n    up_blocks = {\n        layer_id: [key for key in vae_state_dict if f\"up.{layer_id}\" in key]\n        for layer_id in range(num_up_blocks)\n    }\n\n    for i in range(num_down_blocks):\n        resnets = [\n            key\n            for key in down_blocks[i]\n            if f\"down.{i}\" in key and f\"down.{i}.downsample\" not in key\n        ]\n\n        if f\"encoder.down.{i}.downsample.conv.weight\" in vae_state_dict:\n            new_checkpoint[\n                f\"encoder.down_blocks.{i}.downsamplers.0.conv.weight\"\n            ] = vae_state_dict.pop(f\"encoder.down.{i}.downsample.conv.weight\")\n            new_checkpoint[\n                f\"encoder.down_blocks.{i}.downsamplers.0.conv.bias\"\n            ] = vae_state_dict.pop(f\"encoder.down.{i}.downsample.conv.bias\")\n\n        paths = renew_vae_resnet_paths(resnets)\n        meta_path = {\"old\": f\"down.{i}.block\", \"new\": f\"down_blocks.{i}.resnets\"}\n        assign_to_checkpoint(\n            paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path]\n        )\n\n    mid_resnets = [key for key in vae_state_dict if \"encoder.mid.block\" in key]\n    num_mid_res_blocks = 2\n    for i in range(1, num_mid_res_blocks + 1):\n        resnets = [key for key in mid_resnets if f\"encoder.mid.block_{i}\" in key]\n\n        paths = renew_vae_resnet_paths(resnets)\n        meta_path = {\"old\": f\"mid.block_{i}\", \"new\": f\"mid_block.resnets.{i - 1}\"}\n        assign_to_checkpoint(\n            paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path]\n        )\n\n    mid_attentions = [key for key in vae_state_dict if \"encoder.mid.attn\" in key]\n    paths = renew_vae_attention_paths(mid_attentions)\n    meta_path = {\"old\": \"mid.attn_1\", \"new\": \"mid_block.attentions.0\"}\n    assign_to_checkpoint(\n        paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path]\n    )\n    conv_attn_to_linear(new_checkpoint)\n\n    for i in range(num_up_blocks):\n        block_id = num_up_blocks - 1 - i\n        resnets = [\n            key\n            for key in up_blocks[block_id]\n            if f\"up.{block_id}\" in key and f\"up.{block_id}.upsample\" not in key\n        ]\n\n        if f\"decoder.up.{block_id}.upsample.conv.weight\" in vae_state_dict:\n            new_checkpoint[\n                f\"decoder.up_blocks.{i}.upsamplers.0.conv.weight\"\n            ] = vae_state_dict[f\"decoder.up.{block_id}.upsample.conv.weight\"]\n            new_checkpoint[\n                f\"decoder.up_blocks.{i}.upsamplers.0.conv.bias\"\n            ] = vae_state_dict[f\"decoder.up.{block_id}.upsample.conv.bias\"]\n\n        paths = renew_vae_resnet_paths(resnets)\n        meta_path = {\"old\": f\"up.{block_id}.block\", \"new\": f\"up_blocks.{i}.resnets\"}\n        assign_to_checkpoint(\n            paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path]\n        )\n\n    mid_resnets = [key for key in vae_state_dict if \"decoder.mid.block\" in key]\n    num_mid_res_blocks = 2\n    for i in range(1, num_mid_res_blocks + 1):\n        resnets = [key for key in mid_resnets if f\"decoder.mid.block_{i}\" in key]\n\n        paths = renew_vae_resnet_paths(resnets)\n        meta_path = {\"old\": f\"mid.block_{i}\", \"new\": f\"mid_block.resnets.{i - 1}\"}\n        assign_to_checkpoint(\n            paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path]\n        )\n\n    mid_attentions = [key for key in vae_state_dict if \"decoder.mid.attn\" in key]\n    paths = renew_vae_attention_paths(mid_attentions)\n    meta_path = {\"old\": \"mid.attn_1\", \"new\": \"mid_block.attentions.0\"}\n    assign_to_checkpoint(\n        paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path]\n    )\n    conv_attn_to_linear(new_checkpoint)\n    return new_checkpoint", "\n\ntextenc_conversion_lst = [\n    (\"positional_embedding\", \"text_model.embeddings.position_embedding.weight\"),\n    (\"token_embedding.weight\", \"text_model.embeddings.token_embedding.weight\"),\n    (\"ln_final.weight\", \"text_model.final_layer_norm.weight\"),\n    (\"ln_final.bias\", \"text_model.final_layer_norm.bias\"),\n]\ntextenc_conversion_map = {x[0]: x[1] for x in textenc_conversion_lst}\n", "textenc_conversion_map = {x[0]: x[1] for x in textenc_conversion_lst}\n\ntextenc_transformer_conversion_lst = [\n    # (stable-diffusion, HF Diffusers)\n    (\"resblocks.\", \"text_model.encoder.layers.\"),\n    (\"ln_1\", \"layer_norm1\"),\n    (\"ln_2\", \"layer_norm2\"),\n    (\".c_fc.\", \".fc1.\"),\n    (\".c_proj.\", \".fc2.\"),\n    (\".attn\", \".self_attn\"),", "    (\".c_proj.\", \".fc2.\"),\n    (\".attn\", \".self_attn\"),\n    (\"ln_final.\", \"transformer.text_model.final_layer_norm.\"),\n    (\n        \"token_embedding.weight\",\n        \"transformer.text_model.embeddings.token_embedding.weight\",\n    ),\n    (\n        \"positional_embedding\",\n        \"transformer.text_model.embeddings.position_embedding.weight\",", "        \"positional_embedding\",\n        \"transformer.text_model.embeddings.position_embedding.weight\",\n    ),\n]\nprotected = {re.escape(x[0]): x[1] for x in textenc_transformer_conversion_lst}\ntextenc_pattern = re.compile(\"|\".join(protected.keys()))\n\n\ndef convert_text_enc_state_dict(clip_state_dict):\n    temp = {}\n    clip_keys = [\"cond_stage_model.transformer.\",\"cond_stage_model.model.\",]\n\n    for key, value in clip_state_dict.items():\n        for clip_key in clip_keys:\n            key = key.replace(clip_key, \"\")\n        temp[key] = value\n    clip_state_dict = temp\n\n    if \"transformer.resblocks.22.ln_1.bias\" not in clip_state_dict.keys():\n        return clip_state_dict  # SD1.x\n    new_state_dict = {}\n    d_model = 1024\n    for key, arr in clip_state_dict.items():\n        if \"resblocks.23\" in key:\n            continue  # diffusers skips the last layer\n        if key in textenc_conversion_map:\n            new_state_dict[textenc_conversion_map[key]] = arr\n        if key.startswith(\"transformer.\"):\n            new_key = key[len(\"transformer.\") :]\n            if new_key.endswith(\".in_proj_weight\"):\n                new_key = new_key[: -len(\".in_proj_weight\")]\n                new_key = textenc_pattern.sub(\n                    lambda m: protected[re.escape(m.group(0))], new_key\n                )\n                new_state_dict[new_key + \".q_proj.weight\"] = arr[:d_model, :]\n                new_state_dict[new_key + \".k_proj.weight\"] = arr[\n                    d_model : d_model * 2, :\n                ]\n                new_state_dict[new_key + \".v_proj.weight\"] = arr[d_model * 2 :, :]\n            elif new_key.endswith(\".in_proj_bias\"):\n                new_key = new_key[: -len(\".in_proj_bias\")]\n                new_key = textenc_pattern.sub(\n                    lambda m: protected[re.escape(m.group(0))], new_key\n                )\n                new_state_dict[new_key + \".q_proj.bias\"] = arr[:d_model]\n                new_state_dict[new_key + \".k_proj.bias\"] = arr[d_model : d_model * 2]\n                new_state_dict[new_key + \".v_proj.bias\"] = arr[d_model * 2 :]\n            else:\n                new_key = textenc_pattern.sub(\n                    lambda m: protected[re.escape(m.group(0))], new_key\n                )\n                new_state_dict[new_key] = arr\n    return new_state_dict", "def convert_text_enc_state_dict(clip_state_dict):\n    temp = {}\n    clip_keys = [\"cond_stage_model.transformer.\",\"cond_stage_model.model.\",]\n\n    for key, value in clip_state_dict.items():\n        for clip_key in clip_keys:\n            key = key.replace(clip_key, \"\")\n        temp[key] = value\n    clip_state_dict = temp\n\n    if \"transformer.resblocks.22.ln_1.bias\" not in clip_state_dict.keys():\n        return clip_state_dict  # SD1.x\n    new_state_dict = {}\n    d_model = 1024\n    for key, arr in clip_state_dict.items():\n        if \"resblocks.23\" in key:\n            continue  # diffusers skips the last layer\n        if key in textenc_conversion_map:\n            new_state_dict[textenc_conversion_map[key]] = arr\n        if key.startswith(\"transformer.\"):\n            new_key = key[len(\"transformer.\") :]\n            if new_key.endswith(\".in_proj_weight\"):\n                new_key = new_key[: -len(\".in_proj_weight\")]\n                new_key = textenc_pattern.sub(\n                    lambda m: protected[re.escape(m.group(0))], new_key\n                )\n                new_state_dict[new_key + \".q_proj.weight\"] = arr[:d_model, :]\n                new_state_dict[new_key + \".k_proj.weight\"] = arr[\n                    d_model : d_model * 2, :\n                ]\n                new_state_dict[new_key + \".v_proj.weight\"] = arr[d_model * 2 :, :]\n            elif new_key.endswith(\".in_proj_bias\"):\n                new_key = new_key[: -len(\".in_proj_bias\")]\n                new_key = textenc_pattern.sub(\n                    lambda m: protected[re.escape(m.group(0))], new_key\n                )\n                new_state_dict[new_key + \".q_proj.bias\"] = arr[:d_model]\n                new_state_dict[new_key + \".k_proj.bias\"] = arr[d_model : d_model * 2]\n                new_state_dict[new_key + \".v_proj.bias\"] = arr[d_model * 2 :]\n            else:\n                new_key = textenc_pattern.sub(\n                    lambda m: protected[re.escape(m.group(0))], new_key\n                )\n                new_state_dict[new_key] = arr\n    return new_state_dict", ""]}
{"filename": "AITemplate/ait/util/__init__.py", "chunked_list": ["from .ckpt_convert import convert_ldm_unet_checkpoint, convert_ldm_vae_checkpoint, convert_text_enc_state_dict\nfrom .torch_dtype_from_str import torch_dtype_from_str\n\n__all__ = [\"convert_ldm_unet_checkpoint\", \"torch_dtype_from_str\"]"]}
{"filename": "AITemplate/ait/util/mapping/unet.py", "chunked_list": ["import torch\nfrom ...util import torch_dtype_from_str\n\ndef map_unet(pt_mod, in_channels=None, conv_in_key=None, dim=320, device=\"cuda\", dtype=\"float16\"):\n    if in_channels is not None and conv_in_key is None:\n        raise ValueError(\"conv_in_key must be specified if in_channels is not None for padding\")\n    if not isinstance(pt_mod, dict):\n        pt_params = dict(pt_mod.named_parameters())\n    else:\n        pt_params = pt_mod\n    params_ait = {}\n    for key, arr in pt_params.items():\n        if key.startswith(\"model.diffusion_model.\"):\n            key = key.replace(\"model.diffusion_model.\", \"\")\n        arr = arr.to(device, dtype=torch_dtype_from_str(dtype))\n        if len(arr.shape) == 4:\n            arr = arr.permute((0, 2, 3, 1)).contiguous()\n        elif key.endswith(\"ff.net.0.proj.weight\"):\n            w1, w2 = arr.chunk(2, dim=0)\n            params_ait[key.replace(\".\", \"_\")] = w1\n            params_ait[key.replace(\".\", \"_\").replace(\"proj\", \"gate\")] = w2\n            continue\n        elif key.endswith(\"ff.net.0.proj.bias\"):\n            w1, w2 = arr.chunk(2, dim=0)\n            params_ait[key.replace(\".\", \"_\")] = w1\n            params_ait[key.replace(\".\", \"_\").replace(\"proj\", \"gate\")] = w2\n            continue\n        params_ait[key.replace(\".\", \"_\")] = arr\n\n    if conv_in_key is not None:\n        if in_channels % 4 != 0:\n            pad_by = 4 - (in_channels % 4)\n            params_ait[conv_in_key] = torch.functional.F.pad(params_ait[conv_in_key], (0, pad_by))\n\n    params_ait[\"arange\"] = (\n        torch.arange(start=0, end=dim // 2, dtype=torch.float32).to(device, dtype=torch_dtype_from_str(dtype))\n    )\n\n    return params_ait", ""]}
{"filename": "AITemplate/ait/util/mapping/controlnet.py", "chunked_list": ["import torch\nfrom ...util import torch_dtype_from_str\n\n\ndef map_controlnet(pt_mod, dim=320, device=\"cuda\", dtype=\"float16\"):\n    if not isinstance(pt_mod, dict):\n        pt_params = dict(pt_mod.named_parameters())\n    else:\n        pt_params = pt_mod\n    params_ait = {}\n    for key, arr in pt_params.items():\n        arr = arr.to(device, dtype=torch_dtype_from_str(dtype))\n        if len(arr.shape) == 4:\n            arr = arr.permute((0, 2, 3, 1)).contiguous()\n        elif key.endswith(\"ff.net.0.proj.weight\"):\n            w1, w2 = arr.chunk(2, dim=0)\n            params_ait[key.replace(\".\", \"_\")] = w1\n            params_ait[key.replace(\".\", \"_\").replace(\"proj\", \"gate\")] = w2\n            continue\n        elif key.endswith(\"ff.net.0.proj.bias\"):\n            w1, w2 = arr.chunk(2, dim=0)\n            params_ait[key.replace(\".\", \"_\")] = w1\n            params_ait[key.replace(\".\", \"_\").replace(\"proj\", \"gate\")] = w2\n            continue\n        params_ait[key.replace(\".\", \"_\")] = arr\n    params_ait[\"controlnet_cond_embedding_conv_in_weight\"] = torch.nn.functional.pad(\n        params_ait[\"controlnet_cond_embedding_conv_in_weight\"], (0, 1, 0, 0, 0, 0, 0, 0)\n    )\n    params_ait[\"arange\"] = (\n        torch.arange(start=0, end=dim // 2, dtype=torch.float32).cuda().half()\n    )\n    return params_ait", ""]}
{"filename": "AITemplate/ait/util/mapping/__init__.py", "chunked_list": ["from .clip import map_clip\nfrom .controlnet import map_controlnet\nfrom .vae import map_vae\nfrom .unet import map_unet\n\n__all__ = [\"map_clip\", \"map_controlnet\", \"map_vae\", \"map_unet\"]\n"]}
{"filename": "AITemplate/ait/util/mapping/vae.py", "chunked_list": ["import torch\nfrom ...util import torch_dtype_from_str\n\ndef map_vae(pt_module, device=\"cuda\", dtype=\"float16\", encoder=False):\n    if not isinstance(pt_module, dict):\n        pt_params = dict(pt_module.named_parameters())\n    else:\n        pt_params = pt_module\n    params_ait = {}\n    quant_key = \"post_quant\" if encoder else \"quant\"\n    vae_key = \"decoder\" if encoder else \"encoder\"\n    for key, arr in pt_params.items():\n        if key.startswith(vae_key):\n            continue\n        if key.startswith(quant_key):\n            continue\n        arr = arr.to(device, dtype=torch_dtype_from_str(dtype))\n        key = key.replace(\".\", \"_\")\n        if (\n            \"conv\" in key\n            and \"norm\" not in key\n            and key.endswith(\"_weight\")\n            and len(arr.shape) == 4\n        ):\n            params_ait[key] = torch.permute(arr, [0, 2, 3, 1]).contiguous()\n        elif key.endswith(\"proj_attn_weight\"):\n            prefix = key[: -len(\"proj_attn_weight\")]\n            key = prefix + \"attention_proj_weight\"\n            params_ait[key] = arr\n        elif key.endswith(\"to_out_0_weight\"):\n            prefix = key[: -len(\"to_out_0_weight\")]\n            key = prefix + \"attention_proj_weight\"\n            params_ait[key] = arr\n        elif key.endswith(\"proj_attn_bias\"):\n            prefix = key[: -len(\"proj_attn_bias\")]\n            key = prefix + \"attention_proj_bias\"\n            params_ait[key] = arr\n        elif key.endswith(\"to_out_0_bias\"):\n            prefix = key[: -len(\"to_out_0_bias\")]\n            key = prefix + \"attention_proj_bias\"\n            params_ait[key] = arr\n        elif key.endswith(\"query_weight\"):\n            prefix = key[: -len(\"query_weight\")]\n            key = prefix + \"attention_proj_q_weight\"\n            params_ait[key] = arr\n        elif key.endswith(\"to_q_weight\"):\n            prefix = key[: -len(\"to_q_weight\")]\n            key = prefix + \"attention_proj_q_weight\"\n            params_ait[key] = arr\n        elif key.endswith(\"query_bias\"):\n            prefix = key[: -len(\"query_bias\")]\n            key = prefix + \"attention_proj_q_bias\"\n            params_ait[key] = arr\n        elif key.endswith(\"to_q_bias\"):\n            prefix = key[: -len(\"to_q_bias\")]\n            key = prefix + \"attention_proj_q_bias\"\n            params_ait[key] = arr\n        elif key.endswith(\"key_weight\"):\n            prefix = key[: -len(\"key_weight\")]\n            key = prefix + \"attention_proj_k_weight\"\n            params_ait[key] = arr\n        elif key.endswith(\"key_bias\"):\n            prefix = key[: -len(\"key_bias\")]\n            key = prefix + \"attention_proj_k_bias\"\n            params_ait[key] = arr\n        elif key.endswith(\"value_weight\"):\n            prefix = key[: -len(\"value_weight\")]\n            key = prefix + \"attention_proj_v_weight\"\n            params_ait[key] = arr\n        elif key.endswith(\"value_bias\"):\n            prefix = key[: -len(\"value_bias\")]\n            key = prefix + \"attention_proj_v_bias\"\n            params_ait[key] = arr\n        elif key.endswith(\"to_k_weight\"):\n            prefix = key[: -len(\"to_k_weight\")]\n            key = prefix + \"attention_proj_k_weight\"\n            params_ait[key] = arr\n        elif key.endswith(\"to_v_weight\"):\n            prefix = key[: -len(\"to_v_weight\")]\n            key = prefix + \"attention_proj_v_weight\"\n            params_ait[key] = arr\n        elif key.endswith(\"to_k_bias\"):\n            prefix = key[: -len(\"to_k_bias\")]\n            key = prefix + \"attention_proj_k_bias\"\n            params_ait[key] = arr\n        elif key.endswith(\"to_v_bias\"):\n            prefix = key[: -len(\"to_v_bias\")]\n            key = prefix + \"attention_proj_v_bias\"\n            params_ait[key] = arr\n        else:\n            params_ait[key] = arr\n    if encoder:\n        params_ait[\"encoder_conv_in_weight\"] = torch.functional.F.pad(\n            params_ait[\"encoder_conv_in_weight\"], (0, 1, 0, 0, 0, 0, 0, 0)\n        )\n\n    return params_ait", ""]}
{"filename": "AITemplate/ait/util/mapping/clip.py", "chunked_list": ["try:\n    from transformers import CLIPTextConfig, CLIPTextModel\nexcept ImportError:\n    raise ImportError(\n        \"Please install transformers with `pip install transformers` to use this script.\"\n    )\n\nimport torch\nfrom ...util import torch_dtype_from_str\n", "from ...util import torch_dtype_from_str\n\n\ndef map_clip(pt_mod, device=\"cuda\", dtype=\"float16\"):\n    pt_params = dict(pt_mod.named_parameters())\n    params_ait = {}\n    for key, arr in pt_params.items():\n        arr = arr.to(device, dtype=torch_dtype_from_str(dtype))\n        name = key.replace(\"text_model.\", \"\")\n        ait_name = name.replace(\".\", \"_\")\n        if name.endswith(\"out_proj.weight\"):\n            ait_name = ait_name.replace(\"out_proj\", \"proj\")\n        elif name.endswith(\"out_proj.bias\"):\n            ait_name = ait_name.replace(\"out_proj\", \"proj\")\n        elif \"q_proj\" in name:\n            ait_name = ait_name.replace(\"q_proj\", \"proj_q\")\n        elif \"k_proj\" in name:\n            ait_name = ait_name.replace(\"k_proj\", \"proj_k\")\n        elif \"v_proj\" in name:\n            ait_name = ait_name.replace(\"v_proj\", \"proj_v\")\n        params_ait[ait_name] = arr\n    return params_ait", ""]}
