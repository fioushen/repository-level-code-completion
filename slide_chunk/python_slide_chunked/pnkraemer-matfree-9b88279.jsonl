{"filename": "setup.py", "chunked_list": ["\"\"\"Setup. Required for the installation in editable mode.\"\"\"\n\nfrom setuptools import setup\n\nsetup()\n"]}
{"filename": "tests/__init__.py", "chunked_list": ["\"\"\"Test configuration.\"\"\"\n"]}
{"filename": "tests/test_lanczos/test_bidiagonal_full_reortho.py", "chunked_list": ["\"\"\"Tests for GKL bidiagonalisation.\"\"\"\n\nfrom matfree import decomp, lanczos, test_util\nfrom matfree.backend import linalg, np, prng, testing\n\n\n@testing.fixture()\ndef A(nrows, ncols, num_significant_singular_vals):\n    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n    n = min(nrows, ncols)\n    d = np.arange(n) + 10.0\n    d = d.at[num_significant_singular_vals:].set(0.001)\n    return test_util.asymmetric_matrix_from_singular_values(d, nrows=nrows, ncols=ncols)", "\n\n@testing.parametrize(\"nrows\", [50])\n@testing.parametrize(\"ncols\", [49])\n@testing.parametrize(\"num_significant_singular_vals\", [4])\n@testing.parametrize(\"order\", [6])  # ~1.5 * num_significant_eigvals\ndef test_bidiagonal_full_reortho(A, order):\n    \"\"\"Test that Lanczos tridiagonalisation yields an orthogonal-tridiagonal decomp.\"\"\"\n    nrows, ncols = np.shape(A)\n    key = prng.prng_key(1)\n    v0 = prng.normal(key, shape=(ncols,))\n    alg = lanczos.bidiagonal_full_reortho(order, matrix_shape=np.shape(A))\n\n    def Av(v):\n        return A @ v\n\n    def vA(v):\n        return v @ A\n\n    Us, Bs, Vs, (b, v) = decomp.decompose_fori_loop(v0, Av, vA, algorithm=alg)\n    (d_m, e_m) = Bs\n\n    tols_decomp = {\"atol\": 1e-5, \"rtol\": 1e-5}\n\n    assert np.shape(Us) == (nrows, order + 1)\n    assert np.allclose(Us.T @ Us, np.eye(order + 1), **tols_decomp), Us.T @ Us\n\n    assert np.shape(Vs) == (order + 1, ncols)\n    assert np.allclose(Vs @ Vs.T, np.eye(order + 1), **tols_decomp), Vs @ Vs.T\n\n    UAVt = Us.T @ A @ Vs.T\n    assert np.allclose(linalg.diagonal(UAVt), d_m, **tols_decomp)\n    assert np.allclose(linalg.diagonal(UAVt, 1), e_m, **tols_decomp)\n\n    B = _bidiagonal_dense(d_m, e_m)\n    assert np.shape(B) == (order + 1, order + 1)\n    assert np.allclose(UAVt, B, **tols_decomp)\n\n    em = np.eye(order + 1)[:, -1]\n    AVt = A @ Vs.T\n    UtB = Us @ B\n    AtUt = A.T @ Us\n    VtBtb_plus_bve = Vs.T @ B.T + b * v[:, None] @ em[None, :]\n    assert np.allclose(AVt, UtB, **tols_decomp)\n    assert np.allclose(AtUt, VtBtb_plus_bve, **tols_decomp)", "\n\ndef _bidiagonal_dense(d, e):\n    diag = linalg.diagonal_matrix(d)\n    offdiag = linalg.diagonal_matrix(e, 1)\n    return diag + offdiag\n\n\n@testing.parametrize(\"nrows\", [5])\n@testing.parametrize(\"ncols\", [3])", "@testing.parametrize(\"nrows\", [5])\n@testing.parametrize(\"ncols\", [3])\n@testing.parametrize(\"num_significant_singular_vals\", [3])\ndef test_error_too_high_depth(A):\n    \"\"\"Assert that a ValueError is raised when the depth exceeds the matrix size.\"\"\"\n    nrows, ncols = np.shape(A)\n    max_depth = min(nrows, ncols) - 1\n\n    with testing.raises(ValueError):\n        _ = lanczos.bidiagonal_full_reortho(max_depth + 1, matrix_shape=np.shape(A))", "\n\n@testing.parametrize(\"nrows\", [5])\n@testing.parametrize(\"ncols\", [3])\n@testing.parametrize(\"num_significant_singular_vals\", [3])\ndef test_error_too_low_depth(A):\n    \"\"\"Assert that a ValueError is raised when the depth is negative.\"\"\"\n    min_depth = 0\n    with testing.raises(ValueError):\n        _ = lanczos.bidiagonal_full_reortho(min_depth - 1, matrix_shape=np.shape(A))", "\n\n@testing.parametrize(\"nrows\", [15])\n@testing.parametrize(\"ncols\", [3])\n@testing.parametrize(\"num_significant_singular_vals\", [3])\ndef test_no_error_zero_depth(A):\n    \"\"\"Assert the corner case of zero-depth does not raise an error.\"\"\"\n    nrows, ncols = np.shape(A)\n    algorithm = lanczos.bidiagonal_full_reortho(0, matrix_shape=np.shape(A))\n    key = prng.prng_key(1)\n    v0 = prng.normal(key, shape=(ncols,))\n\n    def Av(v):\n        return A @ v\n\n    def vA(v):\n        return v @ A\n\n    Us, Bs, Vs, (b, v) = decomp.decompose_fori_loop(v0, Av, vA, algorithm=algorithm)\n    (d_m, e_m) = Bs\n    assert np.shape(Us) == (nrows, 1)\n    assert np.shape(Vs) == (1, ncols)\n    assert np.shape(d_m) == (1,)\n    assert np.shape(e_m) == (0,)\n    assert np.shape(b) == ()\n    assert np.shape(v) == (ncols,)", ""]}
{"filename": "tests/test_lanczos/test_tridiagonal_full_reortho.py", "chunked_list": ["\"\"\"Tests for Lanczos functionality.\"\"\"\n\nfrom matfree import decomp, lanczos, test_util\nfrom matfree.backend import linalg, np, prng, testing\n\n\n@testing.fixture()\ndef A(n, num_significant_eigvals):\n    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n    d = np.arange(n) + 10.0\n    d = d.at[num_significant_eigvals:].set(0.001)\n\n    return test_util.symmetric_matrix_from_eigenvalues(d)", "\n\n@testing.parametrize(\"n\", [6])\n@testing.parametrize(\"num_significant_eigvals\", [6])\ndef test_max_order(A):\n    \"\"\"If m == n, the matrix should be equal to the full tridiagonal.\"\"\"\n    n, _ = np.shape(A)\n    order = n - 1\n    key = prng.prng_key(1)\n    v0 = prng.normal(key, shape=(n,))\n    alg = lanczos.tridiagonal_full_reortho(order)\n    Q, (d_m, e_m) = decomp.decompose_fori_loop(v0, lambda v: A @ v, algorithm=alg)\n\n    # Lanczos is not stable.\n    tols_decomp = {\"atol\": 1e-5, \"rtol\": 1e-5}\n\n    # Since full-order mode: Q must be unitary\n    assert np.shape(Q) == (order + 1, n)\n    assert np.allclose(Q @ Q.T, np.eye(n), **tols_decomp), Q @ Q.T\n    assert np.allclose(Q.T @ Q, np.eye(n), **tols_decomp), Q.T @ Q\n\n    # T = Q A Qt\n    T = _sym_tridiagonal_dense(d_m, e_m)\n    QAQt = Q @ A @ Q.T\n    assert np.shape(T) == (order + 1, order + 1)\n\n    # Fail early if the (off)diagonals don't coincide\n    assert np.allclose(linalg.diagonal(QAQt), d_m, **tols_decomp)\n    assert np.allclose(linalg.diagonal(QAQt, 1), e_m, **tols_decomp)\n    assert np.allclose(linalg.diagonal(QAQt, -1), e_m, **tols_decomp)\n\n    # Test the full decomposition\n    # (i.e. assert that the off-tridiagonal elements are actually small)\n    # be loose with this test. off-diagonal elements accumulate quickly.\n    tols_decomp = {\"atol\": 1e-5, \"rtol\": 1e-5}\n    assert np.allclose(QAQt, T, **tols_decomp)\n\n    # Since full-order mode: Qt T Q = A\n    # Since Q is unitary and T = Q A Qt, this test\n    # should always pass.\n    assert np.allclose(Q.T @ T @ Q, A, **tols_decomp)", "\n\n@testing.parametrize(\"n\", [50])\n@testing.parametrize(\"num_significant_eigvals\", [4])\n@testing.parametrize(\"order\", [6])  # ~1.5 * num_significant_eigvals\ndef test_identity(A, order):\n    \"\"\"Test that Lanczos tridiagonalisation yields an orthogonal-tridiagonal decomp.\"\"\"\n    n, _ = np.shape(A)\n    key = prng.prng_key(1)\n    v0 = prng.normal(key, shape=(n,))\n    alg = lanczos.tridiagonal_full_reortho(order)\n    Q, tridiag = decomp.decompose_fori_loop(v0, lambda v: A @ v, algorithm=alg)\n    (d_m, e_m) = tridiag\n\n    # Lanczos is not stable.\n    tols_decomp = {\"atol\": 1e-5, \"rtol\": 1e-5}\n\n    assert np.shape(Q) == (order + 1, n)\n    assert np.allclose(Q @ Q.T, np.eye(order + 1), **tols_decomp), Q @ Q.T\n\n    # T = Q A Qt\n    T = _sym_tridiagonal_dense(d_m, e_m)\n    QAQt = Q @ A @ Q.T\n    assert np.shape(T) == (order + 1, order + 1)\n\n    # Fail early if the (off)diagonals don't coincide\n    assert np.allclose(linalg.diagonal(QAQt), d_m, **tols_decomp)\n    assert np.allclose(linalg.diagonal(QAQt, 1), e_m, **tols_decomp)\n    assert np.allclose(linalg.diagonal(QAQt, -1), e_m, **tols_decomp)\n\n    # Test the full decomposition\n    assert np.allclose(QAQt, T, **tols_decomp)", "\n\ndef _sym_tridiagonal_dense(d, e):\n    diag = linalg.diagonal_matrix(d)\n    offdiag1 = linalg.diagonal_matrix(e, 1)\n    offdiag2 = linalg.diagonal_matrix(e, -1)\n    return diag + offdiag1 + offdiag2\n"]}
{"filename": "tests/test_lanczos/__init__.py", "chunked_list": ["\"\"\"Decomposition tests.\"\"\"\n"]}
{"filename": "tests/test_slq/test_logdet_spd_autodiff.py", "chunked_list": ["\"\"\"Tests for (selected) autodiff functionality.\"\"\"\n\n\nfrom matfree import montecarlo, slq, test_util\nfrom matfree.backend import np, prng, testing\n\n\n@testing.fixture()\ndef A(n, num_significant_eigvals):\n    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n    d = np.arange(n) + 10.0\n    d = d.at[num_significant_eigvals:].set(0.001)\n\n    return test_util.symmetric_matrix_from_eigenvalues(d)", "def A(n, num_significant_eigvals):\n    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n    d = np.arange(n) + 10.0\n    d = d.at[num_significant_eigvals:].set(0.001)\n\n    return test_util.symmetric_matrix_from_eigenvalues(d)\n\n\n@testing.parametrize(\"n\", [200])", "\n@testing.parametrize(\"n\", [200])\n@testing.parametrize(\"num_significant_eigvals\", [30])\n@testing.parametrize(\"order\", [10])\n# usually: ~1.5 * num_significant_eigvals.\n# But logdet seems to converge sooo much faster.\ndef test_check_grads(A, order):\n    \"\"\"Assert that log-determinant computation admits valid VJPs and JVPs.\"\"\"\n    key = prng.prng_key(1)\n\n    def fun(s):\n        return _logdet(s, order, key)\n\n    testing.check_grads(fun, (A,), order=1, atol=1e-1, rtol=1e-1)", "\n\ndef _logdet(A, order, key):\n    n, _ = np.shape(A)\n    fun = montecarlo.normal(shape=(n,))\n    return slq.logdet_spd(\n        order,\n        lambda v: A @ v,\n        key=key,\n        num_samples_per_batch=10,\n        num_batches=1,\n        sample_fun=fun,\n    )", ""]}
{"filename": "tests/test_slq/test_schatten_norm.py", "chunked_list": ["\"\"\"Test Schatten norm implementations.\"\"\"\n\nfrom matfree import montecarlo, slq, test_util\nfrom matfree.backend import linalg, np, prng, testing\n\n\n@testing.fixture()\ndef A(nrows, ncols, num_significant_singular_vals):\n    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n    n = min(nrows, ncols)\n    d = np.arange(n) + 1.0\n    d = d.at[num_significant_singular_vals:].set(0.001)\n    return test_util.asymmetric_matrix_from_singular_values(d, nrows=nrows, ncols=ncols)", "\n\n@testing.parametrize(\"nrows\", [30])\n@testing.parametrize(\"ncols\", [30])\n@testing.parametrize(\"num_significant_singular_vals\", [30])\n@testing.parametrize(\"order\", [20])\n@testing.parametrize(\"power\", [1, 2, 5])\ndef test_schatten_norm(A, order, power):\n    \"\"\"Assert that schatten_norm yields an accurate estimate.\"\"\"\n    _, s, _ = linalg.svd(A, full_matrices=False)\n    expected = np.sum(s**power) ** (1 / power)\n\n    _, ncols = np.shape(A)\n    key = prng.prng_key(1)\n    fun = montecarlo.normal(shape=(ncols,))\n    received = slq.schatten_norm(\n        order,\n        lambda v: A @ v,\n        lambda v: A.T @ v,\n        power=power,\n        matrix_shape=np.shape(A),\n        key=key,\n        num_samples_per_batch=100,\n        num_batches=5,\n        sample_fun=fun,\n    )\n    print_if_assert_fails = (\"error\", np.abs(received - expected), \"target:\", expected)\n    assert np.allclose(received, expected, atol=1e-2, rtol=1e-2), print_if_assert_fails", ""]}
{"filename": "tests/test_slq/test_logdet_spd.py", "chunked_list": ["\"\"\"Tests for Lanczos functionality.\"\"\"\n\nfrom matfree import montecarlo, slq, test_util\nfrom matfree.backend import linalg, np, prng, testing\n\n\n@testing.fixture()\ndef A(n, num_significant_eigvals):\n    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n    d = np.arange(n) / n + 1.0\n    d = d.at[num_significant_eigvals:].set(0.001)\n\n    return test_util.symmetric_matrix_from_eigenvalues(d)", "\n\n@testing.parametrize(\"n\", [200])\n@testing.parametrize(\"num_significant_eigvals\", [30])\n@testing.parametrize(\"order\", [10])\n# usually: ~1.5 * num_significant_eigvals.\n# But logdet seems to converge sooo much faster.\ndef test_logdet_spd(A, order):\n    \"\"\"Assert that the log-determinant estimation matches the true log-determinant.\"\"\"\n    n, _ = np.shape(A)\n    key = prng.prng_key(1)\n    fun = montecarlo.normal(shape=(n,))\n    received = slq.logdet_spd(\n        order,\n        lambda v: A @ v,\n        key=key,\n        num_samples_per_batch=10,\n        num_batches=1,\n        sample_fun=fun,\n    )\n    expected = linalg.slogdet(A)[1]\n    print_if_assert_fails = (\"error\", np.abs(received - expected), \"target:\", expected)\n    assert np.allclose(received, expected, atol=1e-2, rtol=1e-2), print_if_assert_fails", ""]}
{"filename": "tests/test_slq/test_logdet_product.py", "chunked_list": ["\"\"\"Test slq.logdet_prod().\"\"\"\n\nfrom matfree import montecarlo, slq, test_util\nfrom matfree.backend import linalg, np, prng, testing\n\n\n@testing.fixture()\ndef A(nrows, ncols, num_significant_singular_vals):\n    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n    n = min(nrows, ncols)\n    d = np.arange(n) + 1.0\n    d = d.at[num_significant_singular_vals:].set(0.001)\n    return test_util.asymmetric_matrix_from_singular_values(d, nrows=nrows, ncols=ncols)", "\n\n@testing.parametrize(\"nrows\", [50])\n@testing.parametrize(\"ncols\", [30])\n@testing.parametrize(\"num_significant_singular_vals\", [30])\n@testing.parametrize(\"order\", [20])\ndef test_logdet_product(A, order):\n    \"\"\"Assert that logdet_product yields an accurate estimate.\"\"\"\n    _, ncols = np.shape(A)\n    key = prng.prng_key(3)\n    fun = montecarlo.normal(shape=(ncols,))\n    received = slq.logdet_product(\n        order,\n        lambda v: A @ v,\n        lambda v: A.T @ v,\n        matrix_shape=np.shape(A),\n        key=key,\n        num_samples_per_batch=200,\n        num_batches=2,\n        sample_fun=fun,\n    )\n    expected = linalg.slogdet(A.T @ A)[1]\n    print_if_assert_fails = (\"error\", np.abs(received - expected), \"target:\", expected)\n    assert np.allclose(received, expected, atol=1e-2, rtol=1e-2), print_if_assert_fails", ""]}
{"filename": "tests/test_slq/__init__.py", "chunked_list": ["\"\"\"Tests for stochastic Lanczos quadrature.\"\"\"\n"]}
{"filename": "tests/test_montecarlo/test_van_der_corput.py", "chunked_list": ["\"\"\"Tests for Monte-Carlo machinery.\"\"\"\n\nfrom matfree import montecarlo\nfrom matfree.backend import np\n\n\ndef test_van_der_corput():\n    \"\"\"Assert that the van-der-Corput sequence yields values as expected.\"\"\"\n    expected = np.asarray([0, 0.5, 0.25, 0.75, 0.125, 0.625, 0.375, 0.875, 0.0625])\n    received = np.asarray([montecarlo.van_der_corput(i) for i in range(9)])\n    assert np.allclose(received, expected)\n\n    expected = np.asarray([0.0, 1 / 3, 2 / 3, 1 / 9, 4 / 9, 7 / 9, 2 / 9, 5 / 9, 8 / 9])\n    received = np.asarray([montecarlo.van_der_corput(i, base=3) for i in range(9)])\n    assert np.allclose(received, expected)", ""]}
{"filename": "tests/test_montecarlo/__init__.py", "chunked_list": ["\"\"\"Monte-Carlo tests.\"\"\"\n"]}
{"filename": "tests/test_montecarlo/test_estimate.py", "chunked_list": ["\"\"\"Tests for Monte-Carlo machinery.\"\"\"\n\nfrom matfree import montecarlo\nfrom matfree.backend import np, prng, testing\n\n\n@testing.parametrize(\"key\", [prng.prng_key(1)])\n@testing.parametrize(\"num_batches, num_samples\", [[1, 10_000], [10_000, 1], [100, 100]])\ndef test_mean(key, num_batches, num_samples):\n    \"\"\"Assert that the mean estimate is accurate.\"\"\"\n\n    def fun(x):\n        return x**2\n\n    received = montecarlo.estimate(\n        fun,\n        num_batches=num_batches,\n        num_samples_per_batch=num_samples,\n        key=key,\n        sample_fun=montecarlo.normal(shape=()),\n    )\n    assert np.allclose(received, 1.0, rtol=1e-1)", "def test_mean(key, num_batches, num_samples):\n    \"\"\"Assert that the mean estimate is accurate.\"\"\"\n\n    def fun(x):\n        return x**2\n\n    received = montecarlo.estimate(\n        fun,\n        num_batches=num_batches,\n        num_samples_per_batch=num_samples,\n        key=key,\n        sample_fun=montecarlo.normal(shape=()),\n    )\n    assert np.allclose(received, 1.0, rtol=1e-1)", ""]}
{"filename": "tests/test_montecarlo/test_multiestimate.py", "chunked_list": ["\"\"\"Tests for Monte-Carlo machinery.\"\"\"\n\nfrom matfree import montecarlo\nfrom matfree.backend import np, prng, testing\n\n\n@testing.parametrize(\"key\", [prng.prng_key(1)])\n@testing.parametrize(\"num_batches, num_samples\", [[1, 10_000], [10_000, 1], [100, 100]])\ndef test_mean_and_max(key, num_batches, num_samples):\n    \"\"\"Assert that the mean estimate is accurate.\"\"\"\n\n    def fun(x):\n        return x**2\n\n    mean, amax = montecarlo.multiestimate(\n        fun,\n        num_batches=num_batches,\n        num_samples_per_batch=num_samples,\n        key=key,\n        sample_fun=montecarlo.normal(shape=()),\n        statistics_batch=[np.mean, np.array_max],\n        statistics_combine=[np.mean, np.array_max],\n    )\n\n    assert np.allclose(mean, 1.0, rtol=1e-1)\n    assert mean < amax", "def test_mean_and_max(key, num_batches, num_samples):\n    \"\"\"Assert that the mean estimate is accurate.\"\"\"\n\n    def fun(x):\n        return x**2\n\n    mean, amax = montecarlo.multiestimate(\n        fun,\n        num_batches=num_batches,\n        num_samples_per_batch=num_samples,\n        key=key,\n        sample_fun=montecarlo.normal(shape=()),\n        statistics_batch=[np.mean, np.array_max],\n        statistics_combine=[np.mean, np.array_max],\n    )\n\n    assert np.allclose(mean, 1.0, rtol=1e-1)\n    assert mean < amax", ""]}
{"filename": "tests/test_decomp/__init__.py", "chunked_list": ["\"\"\"Decomposition tests.\"\"\"\n"]}
{"filename": "tests/test_decomp/test_svd.py", "chunked_list": ["\"\"\"Tests for SVD functionality.\"\"\"\n\n\nfrom matfree import decomp, test_util\nfrom matfree.backend import linalg, np, testing\n\n\n@testing.fixture()\n@testing.parametrize(\"nrows\", [10])\n@testing.parametrize(\"ncols\", [3])", "@testing.parametrize(\"nrows\", [10])\n@testing.parametrize(\"ncols\", [3])\n@testing.parametrize(\"num_significant_singular_vals\", [3])\ndef A(nrows, ncols, num_significant_singular_vals):\n    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n    n = min(nrows, ncols)\n    d = np.arange(n) + 10.0\n    d = d.at[num_significant_singular_vals:].set(0.001)\n    return test_util.asymmetric_matrix_from_singular_values(d, nrows=nrows, ncols=ncols)", "\n\ndef test_equal_to_linalg_svd(A):\n    \"\"\"The output of full-depth SVD should be equal (*) to linalg.svd().\n\n    (*) Note: The singular values should be identical,\n    and the orthogonal matrices should be orthogonal. They are not unique.\n    \"\"\"\n    nrows, ncols = np.shape(A)\n    depth = min(nrows, ncols) - 1\n\n    def Av(v):\n        return A @ v\n\n    def vA(v):\n        return v @ A\n\n    v0 = np.ones((ncols,))\n    U, S, Vt = decomp.svd(v0, depth, Av, vA, matrix_shape=np.shape(A))\n    U_, S_, Vt_ = linalg.svd(A, full_matrices=False)\n\n    tols_decomp = {\"atol\": 1e-5, \"rtol\": 1e-5}\n    assert np.allclose(S, S_)  # strict \"allclose\"\n\n    assert np.shape(U) == np.shape(U_)\n    assert np.shape(Vt) == np.shape(Vt_)\n    assert np.allclose(U @ U.T, U_ @ U_.T, **tols_decomp)\n    assert np.allclose(Vt @ Vt.T, Vt_ @ Vt_.T, **tols_decomp)", ""]}
{"filename": "tests/test_hutchinson/test_trace_and_diagonal.py", "chunked_list": ["\"\"\"Tests for basic trace estimators.\"\"\"\n\nfrom matfree import hutchinson, montecarlo\nfrom matfree.backend import func, linalg, np, prng, testing\n\n\n@testing.fixture(name=\"fun\")\ndef fixture_fun():\n    \"\"\"Create a nonlinear, to-be-differentiated function.\"\"\"\n\n    def f(x):\n        return np.sin(np.flip(np.cos(x)) + 1.0) * np.sin(x) + 1.0\n\n    return f", "\n\n@testing.fixture(name=\"key\")\ndef fixture_key():\n    \"\"\"Fix a pseudo-random number generator.\"\"\"\n    return prng.prng_key(seed=1)\n\n\n@testing.parametrize(\"num_samples\", [10_000])\n@testing.parametrize(\"dim\", [5])", "@testing.parametrize(\"num_samples\", [10_000])\n@testing.parametrize(\"dim\", [5])\n@testing.parametrize(\"sample_fun\", [montecarlo.normal, montecarlo.rademacher])\ndef test_trace_and_diagonal(fun, key, num_samples, dim, sample_fun):\n    \"\"\"Assert that the estimated trace and diagonal approximations are accurate.\"\"\"\n    # Linearise function\n    x0 = prng.uniform(key, shape=(dim,))\n    _, jvp = func.linearize(fun, x0)\n    J = func.jacfwd(fun)(x0)\n\n    # Estimate the trace\n    fun = sample_fun(shape=np.shape(x0), dtype=np.dtype(x0))\n    trace, diag = hutchinson.trace_and_diagonal(\n        jvp, key=key, num_levels=num_samples, sample_fun=fun\n    )\n\n    # Print errors if test fails\n    error_diag = linalg.vector_norm(diag - linalg.diagonal(J))\n    error_trace = linalg.vector_norm(trace - linalg.trace(J))\n    assert np.allclose(diag, linalg.diagonal(J), rtol=1e-2), error_diag\n    assert np.allclose(trace, linalg.trace(J), rtol=1e-2), error_trace", ""]}
{"filename": "tests/test_hutchinson/test_frobeniusnorm_squared.py", "chunked_list": ["\"\"\"Tests for basic trace estimators.\"\"\"\n\nfrom matfree import hutchinson, montecarlo\nfrom matfree.backend import func, linalg, np, prng, testing\n\n\n@testing.fixture(name=\"fun\")\ndef fixture_fun():\n    \"\"\"Create a nonlinear, to-be-differentiated function.\"\"\"\n\n    def f(x):\n        return np.sin(np.flip(np.cos(x)) + 1.0) * np.sin(x) + 1.0\n\n    return f", "\n\n@testing.fixture(name=\"key\")\ndef fixture_key():\n    \"\"\"Fix a pseudo-random number generator.\"\"\"\n    return prng.prng_key(seed=1)\n\n\n@testing.parametrize(\"num_batches\", [1_000])\n@testing.parametrize(\"num_samples_per_batch\", [1_000])", "@testing.parametrize(\"num_batches\", [1_000])\n@testing.parametrize(\"num_samples_per_batch\", [1_000])\n@testing.parametrize(\"dim\", [1, 10])\n@testing.parametrize(\"sample_fun\", [montecarlo.normal, montecarlo.rademacher])\ndef test_frobeniusnorm_squared(\n    fun, key, num_batches, num_samples_per_batch, dim, sample_fun\n):\n    \"\"\"Assert that the Frobenius norm estimate is accurate.\"\"\"\n    # Linearise function\n    x0 = prng.uniform(key, shape=(dim,))  # random lin. point\n    _, jvp = func.linearize(fun, x0)\n    J = func.jacfwd(fun)(x0)\n\n    # Estimate the trace\n    fun = sample_fun(shape=np.shape(x0), dtype=np.dtype(x0))\n    estimate = hutchinson.frobeniusnorm_squared(\n        jvp,\n        num_batches=num_batches,\n        key=key,\n        num_samples_per_batch=num_samples_per_batch,\n        sample_fun=fun,\n    )\n    truth = linalg.trace(J.T @ J)\n    assert np.allclose(estimate, truth, rtol=1e-2)", ""]}
{"filename": "tests/test_hutchinson/test_trace_moments.py", "chunked_list": ["\"\"\"Tests for estimating traces.\"\"\"\n\nfrom matfree import hutchinson, montecarlo\nfrom matfree.backend import func, linalg, np, prng, testing\n\n\n@testing.fixture(name=\"key\")\ndef fixture_key():\n    \"\"\"Fix a pseudo-random number generator.\"\"\"\n    return prng.prng_key(seed=1)", "\n\n@testing.fixture(name=\"J_and_jvp\")\ndef fixture_J_and_jvp(key, dim):\n    \"\"\"Create a nonlinear, to-be-differentiated function.\"\"\"\n\n    def fun(x):\n        return np.sin(np.flip(np.cos(x)) + 1.0) * np.sin(x) + 1.0\n\n    # Linearise function\n    x0 = prng.uniform(key, shape=(dim,))  # random lin. point\n    _, jvp = func.linearize(fun, x0)\n    J = func.jacfwd(fun)(x0)\n\n    return J, jvp", "\n\n@testing.parametrize(\"num_batches\", [1_000])\n@testing.parametrize(\"num_samples_per_batch\", [1_000])\n@testing.parametrize(\"dim\", [1, 10])\ndef test_variance_normal(J_and_jvp, key, num_batches, num_samples_per_batch, dim):\n    \"\"\"Assert that the estimated trace approximates the true trace accurately.\"\"\"\n    # Estimate the trace\n    J, jvp = J_and_jvp\n    fun = montecarlo.normal(shape=(dim,), dtype=float)\n    first, second = hutchinson.trace_moments(\n        jvp,\n        key=key,\n        num_batches=num_batches,\n        num_samples_per_batch=num_samples_per_batch,\n        sample_fun=fun,\n        moments=(1, 2),\n    )\n\n    # Assert the trace is correct\n    truth = linalg.trace(J)\n    assert np.allclose(first, truth, rtol=1e-2)\n\n    # Assert the variance is correct:\n    norm = linalg.matrix_norm(J, which=\"fro\") ** 2\n    assert np.allclose(second - first**2, norm * 2, rtol=1e-2)", "\n\n@testing.parametrize(\"num_batches\", [1_000])\n@testing.parametrize(\"num_samples_per_batch\", [1_000])\n@testing.parametrize(\"dim\", [1, 10])\ndef test_variance_rademacher(J_and_jvp, key, num_batches, num_samples_per_batch, dim):\n    \"\"\"Assert that the estimated trace approximates the true trace accurately.\"\"\"\n    # Estimate the trace\n    J, jvp = J_and_jvp\n    fun = montecarlo.rademacher(shape=(dim,), dtype=float)\n    first, second = hutchinson.trace_moments(\n        jvp,\n        key=key,\n        num_batches=num_batches,\n        num_samples_per_batch=num_samples_per_batch,\n        sample_fun=fun,\n        moments=(1, 2),\n    )\n\n    # Assert the trace is correct\n    truth = linalg.trace(J)\n    assert np.allclose(first, truth, rtol=1e-2)\n\n    # Assert the variance is correct:\n    norm = linalg.matrix_norm(J, which=\"fro\") ** 2\n    truth = 2 * (norm - linalg.trace(J**2))\n    assert np.allclose(second - first**2, truth, atol=1e-2, rtol=1e-2)", ""]}
{"filename": "tests/test_hutchinson/__init__.py", "chunked_list": ["\"\"\"Trace estimator tests.\"\"\"\n"]}
{"filename": "tests/test_hutchinson/test_diagonal.py", "chunked_list": ["\"\"\"Tests for basic trace estimators.\"\"\"\n\nfrom matfree import hutchinson, montecarlo\nfrom matfree.backend import func, linalg, np, prng, testing\n\n\n@testing.fixture(name=\"fun\")\ndef fixture_fun():\n    \"\"\"Create a nonlinear, to-be-differentiated function.\"\"\"\n\n    def f(x):\n        return np.sin(np.flip(np.cos(x)) + 1.0) * np.sin(x) + 1.0\n\n    return f", "\n\n@testing.fixture(name=\"key\")\ndef fixture_key():\n    \"\"\"Fix a pseudo-random number generator.\"\"\"\n    return prng.prng_key(seed=1)\n\n\n@testing.parametrize(\"num_batches\", [1_000])\n@testing.parametrize(\"num_samples_per_batch\", [1_000])", "@testing.parametrize(\"num_batches\", [1_000])\n@testing.parametrize(\"num_samples_per_batch\", [1_000])\n@testing.parametrize(\"dim\", [1, 10])\n@testing.parametrize(\"sample_fun\", [montecarlo.normal, montecarlo.rademacher])\ndef test_diagonal(fun, key, num_batches, num_samples_per_batch, dim, sample_fun):\n    \"\"\"Assert that the estimated diagonal approximates the true diagonal accurately.\"\"\"\n    # Linearise function\n    x0 = prng.uniform(key, shape=(dim,))  # random lin. point\n    _, jvp = func.linearize(fun, x0)\n    J = func.jacfwd(fun)(x0)\n\n    # Estimate the trace\n    fun = sample_fun(shape=np.shape(x0), dtype=np.dtype(x0))\n    estimate = hutchinson.diagonal(\n        jvp,\n        num_batches=num_batches,\n        key=key,\n        num_samples_per_batch=num_samples_per_batch,\n        sample_fun=fun,\n    )\n    truth = linalg.diagonal(J)\n    assert np.allclose(estimate, truth, rtol=1e-2)", ""]}
{"filename": "tests/test_hutchinson/test_trace.py", "chunked_list": ["\"\"\"Tests for basic trace estimators.\"\"\"\n\nfrom matfree import hutchinson, montecarlo\nfrom matfree.backend import func, linalg, np, prng, testing\n\n\n@testing.fixture(name=\"fun\")\ndef fixture_fun():\n    \"\"\"Create a nonlinear, to-be-differentiated function.\"\"\"\n\n    def f(x):\n        return np.sin(np.flip(np.cos(x)) + 1.0) * np.sin(x) + 1.0\n\n    return f", "\n\n@testing.fixture(name=\"key\")\ndef fixture_key():\n    \"\"\"Fix a pseudo-random number generator.\"\"\"\n    return prng.prng_key(seed=1)\n\n\n@testing.parametrize(\"num_batches\", [1_000])\n@testing.parametrize(\"num_samples_per_batch\", [1_000])", "@testing.parametrize(\"num_batches\", [1_000])\n@testing.parametrize(\"num_samples_per_batch\", [1_000])\n@testing.parametrize(\"dim\", [1, 10])\n@testing.parametrize(\"sample_fun\", [montecarlo.normal, montecarlo.rademacher])\ndef test_trace(fun, key, num_batches, num_samples_per_batch, dim, sample_fun):\n    \"\"\"Assert that the estimated trace approximates the true trace accurately.\"\"\"\n    # Linearise function\n    x0 = prng.uniform(key, shape=(dim,))  # random lin. point\n    _, jvp = func.linearize(fun, x0)\n    J = func.jacfwd(fun)(x0)\n\n    # Estimate the trace\n    fun = sample_fun(shape=np.shape(x0), dtype=np.dtype(x0))\n    estimate = hutchinson.trace(\n        jvp,\n        num_batches=num_batches,\n        key=key,\n        num_samples_per_batch=num_samples_per_batch,\n        sample_fun=fun,\n    )\n    truth = linalg.trace(J)\n    assert np.allclose(estimate, truth, rtol=1e-2)", ""]}
{"filename": "docs/benchmarks/control_variates.py", "chunked_list": ["\"\"\"Control_variates benchmark.\n\nRuntime: ~10 seconds.\n\"\"\"\n\nfrom matfree import benchmark_util, hutchinson, montecarlo\nfrom matfree.backend import func, linalg, np, plt, prng, progressbar\n\n\ndef problem(n):\n    \"\"\"Create an example problem.\"\"\"\n\n    # This function has a Jacobian with x-shaped sparsity pattern\n    # We expect control variates to do pretty well\n    # (But I don't know why)\n    def f(x):\n        return np.sin(np.roll(np.sin(np.flip(np.cos(x)) + 1) ** 2, 1)) * np.sin(x**2)\n\n    key = prng.prng_key(seed=2)\n    x0 = prng.uniform(key, shape=(n,))\n\n    _, jvp = func.linearize(f, x0)\n    J = func.jacfwd(f)(x0)\n    trace = linalg.trace(J)\n    sample_fun = montecarlo.normal(shape=(n,), dtype=float)\n\n    return (jvp, trace, J), (key, sample_fun)", "\ndef problem(n):\n    \"\"\"Create an example problem.\"\"\"\n\n    # This function has a Jacobian with x-shaped sparsity pattern\n    # We expect control variates to do pretty well\n    # (But I don't know why)\n    def f(x):\n        return np.sin(np.roll(np.sin(np.flip(np.cos(x)) + 1) ** 2, 1)) * np.sin(x**2)\n\n    key = prng.prng_key(seed=2)\n    x0 = prng.uniform(key, shape=(n,))\n\n    _, jvp = func.linearize(f, x0)\n    J = func.jacfwd(f)(x0)\n    trace = linalg.trace(J)\n    sample_fun = montecarlo.normal(shape=(n,), dtype=float)\n\n    return (jvp, trace, J), (key, sample_fun)", "\n\nif __name__ == \"__main__\":\n    dim = 100\n    num_samples = 2 ** np.arange(4, 10)\n    num_restarts = 5\n\n    (Av, trace, J), (k, sample_fun) = problem(dim)\n\n    error_fun = func.partial(benchmark_util.rmse_relative, expected=trace)\n\n    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n    @func.partial(func.jit, static_argnums=0)\n    def fun1(num, key):\n        \"\"\"Estimate the trace conventionally.\"\"\"\n        return hutchinson.trace(\n            Av, key=key, sample_fun=sample_fun, num_batches=num, num_samples_per_batch=1\n        )\n\n    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n    @func.partial(func.jit, static_argnums=0)\n    def fun2(num, key):\n        \"\"\"Estimate trace and diagonal jointly and discard the diagonal.\"\"\"\n        trace2, _ = hutchinson.trace_and_diagonal(\n            Av, key=key, num_levels=num, sample_fun=sample_fun\n        )\n        return trace2\n\n    errors1, stds1, times1 = [], [], []\n    errors2, stds2, times2 = [], [], []\n\n    for n in progressbar.progressbar(num_samples):\n        test_keys = prng.split(k, num=num_restarts)\n        e1, t1 = zip(*[fun1(int(n), ke) for ke in test_keys])\n        e2, t2 = zip(*[fun2(int(n), ke) for ke in test_keys])\n\n        errors1.append(np.mean(np.asarray(e1)))\n        stds1.append(np.std(np.asarray(e1)))\n        times1.append(min(t1))\n\n        errors2.append(np.mean(np.asarray(e2)))\n        stds2.append(np.std(np.asarray(e2)))\n        times2.append(min(t2))\n\n    errors1 = np.asarray(errors1)\n    stds1 = np.asarray(stds1)\n    times1 = np.asarray(times1)\n\n    errors2 = np.asarray(errors2)\n    stds2 = np.asarray(stds2)\n    times2 = np.asarray(times2)\n\n    fig, axes = plt.subplots(ncols=2, tight_layout=True, figsize=(10, 5), dpi=100)\n\n    ax_wp, ax_sparsity = axes\n    ax_wp.set_title(\"Trace estimation\")\n    ax_wp.loglog(\n        times1,\n        errors1,\n        \"o-\",\n        markeredgecolor=\"k\",\n        label=\"Conventional trace estimation\",\n    )\n\n    ax_wp.fill_between(\n        times1,\n        errors1 - stds1,  # type: ignore\n        errors1 + stds1,  # type: ignore\n        alpha=0.25,\n    )\n    ax_wp.loglog(\n        times2,\n        errors2,\n        \"^-\",\n        markeredgecolor=\"k\",\n        label=\"Joint trace and diagonal estimation\",\n    )\n    ax_wp.fill_between(\n        times2,\n        errors2 - stds2,  # type: ignore\n        errors2 + stds2,  # type: ignore\n        alpha=0.25,\n    )\n    ax_wp.set_ylabel(f\"Relative error (mean & std, {num_restarts} restarts)\")\n    ax_wp.set_xlabel(f\"Wall clock time (minimum, {num_restarts} restarts)\")\n    ax_wp.legend()\n\n    ax_sparsity.set_title(\"Sparsity pattern of Jacobian\")\n    ax_sparsity.spy(J)\n    ax_sparsity.set_xticks(())\n    ax_sparsity.set_yticks(())\n    plt.show()", ""]}
{"filename": "docs/benchmarks/jacobian_squared.py", "chunked_list": ["\"\"\"What is the fastest way of computing trace(A^5).\"\"\"\nfrom matfree import benchmark_util, hutchinson, montecarlo, slq\nfrom matfree.backend import func, linalg, np, plt, prng\nfrom matfree.backend.progressbar import progressbar\n\n\ndef problem(n):\n    \"\"\"Create an example problem.\"\"\"\n\n    # This function has a Jacobian with x-shaped sparsity pattern\n    # We expect control variates to do pretty well\n    # (But I don't know why)\n    def f(x):\n        return np.sin(np.roll(np.sin(np.flip(np.cos(x)) + 1) ** 2, 2)) * np.sin(x**2)\n\n    key = prng.prng_key(seed=2)\n    x0 = prng.uniform(key, shape=(n,))\n\n    _, jvp = func.linearize(f, x0)\n    J = func.jacfwd(f)(x0)\n    A = J @ J @ J @ J\n    trace = linalg.trace(A)\n    sample_fun = montecarlo.normal(shape=(n,), dtype=float)\n\n    def Av(v):\n        return jvp(jvp(jvp(jvp(v))))\n\n    return (lambda x: x**4, jvp, Av, trace, A), (key, sample_fun)", "\n\ndef evaluate_all(fun, outer_loop, inner_loop):\n    \"\"\"Evaluate all metrics of a function.\"\"\"\n    errors, stds, times = [], [], []\n    for n in outer_loop:\n        fun_bind = func.partial(fun, int(n))\n        err, tim = evaluate(fun_bind, inner_loop)\n\n        errors.append(np.mean(err))\n        stds.append(np.std(err))\n        times.append(np.array_min(tim))\n\n    return np.asarray(errors), np.asarray(stds), np.asarray(times)", "\n\ndef evaluate(fun, keys):\n    \"\"\"Evaluate all metrics of a function.\"\"\"\n    errors, times = zip(*[fun(key) for key in keys])\n    return np.asarray(errors), np.asarray(times)\n\n\nif __name__ == \"__main__\":\n    dim = 10\n    num_samples = 2 ** np.arange(4, 12)\n    num_restarts = 10\n\n    (matfun, jvp, Av, trace, JJ), (k, sample_fun) = problem(dim)\n\n    x = hutchinson.trace(Av, key=k, sample_fun=sample_fun)\n    y = slq.trace_of_matfun_spd(matfun, jvp, 5, key=k, sample_fun=sample_fun)\n    assert np.allclose(x, trace, atol=1e-1, rtol=1e-1), (x, trace)\n    assert np.allclose(y, trace, atol=1e-1, rtol=1e-1), (y, trace)\n\n    error_fun = func.partial(benchmark_util.rmse_relative, expected=trace)\n\n    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n    @func.partial(func.jit, static_argnums=0)\n    def matvec(num, key):\n        \"\"\"Matrix-vector mult.\"\"\"\n        return hutchinson.trace(\n            Av, key=key, sample_fun=sample_fun, num_samples_per_batch=num\n        )\n\n    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n    @func.partial(func.jit, static_argnums=0)\n    def slq_low(num, key):\n        \"\"\"SLQ(1)\"\"\"  # noqa: D400,D415\n        return slq.trace_of_matfun_spd(\n            matfun,\n            jvp,\n            1,\n            key=key,\n            sample_fun=sample_fun,\n            num_samples_per_batch=num,\n        )\n\n    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n    @func.partial(func.jit, static_argnums=0)\n    def slq_high(num, key):\n        \"\"\"SLQ(5)\"\"\"  # noqa: D400,D415\n        return slq.trace_of_matfun_spd(\n            matfun,\n            jvp,\n            5,\n            key=key,\n            sample_fun=sample_fun,\n            num_samples_per_batch=num,\n        )\n\n    test_keys = prng.split(k, num=num_restarts)\n    fig, axes = plt.subplots(ncols=2, tight_layout=True, figsize=(10, 5), dpi=100)\n    ax_wp, ax_sparsity = axes\n\n    ax_wp.set_title(\"Estimating trace(A^2)\")\n    for fun in [matvec, slq_low, slq_high]:\n        errors, stds, times = evaluate_all(fun, progressbar(num_samples), test_keys)\n        ax_wp.loglog(\n            times,\n            errors,\n            \"o-\",\n            markeredgecolor=\"k\",\n            label=fun.__doc__,\n        )\n        ax_wp.fill_between(times, errors - stds, errors + stds, alpha=0.25)\n\n    ax_wp.set_ylabel(f\"Relative error (mean & std, {num_restarts} restarts)\")\n    ax_wp.set_xlabel(f\"Wall clock time (minimum, {num_restarts} restarts)\")\n    ax_wp.legend()\n\n    ax_sparsity.set_title(\"Sparsity pattern of A^2\")\n    ax_sparsity.spy(JJ)\n    ax_sparsity.set_xticks(())\n    ax_sparsity.set_yticks(())\n    plt.show()", "if __name__ == \"__main__\":\n    dim = 10\n    num_samples = 2 ** np.arange(4, 12)\n    num_restarts = 10\n\n    (matfun, jvp, Av, trace, JJ), (k, sample_fun) = problem(dim)\n\n    x = hutchinson.trace(Av, key=k, sample_fun=sample_fun)\n    y = slq.trace_of_matfun_spd(matfun, jvp, 5, key=k, sample_fun=sample_fun)\n    assert np.allclose(x, trace, atol=1e-1, rtol=1e-1), (x, trace)\n    assert np.allclose(y, trace, atol=1e-1, rtol=1e-1), (y, trace)\n\n    error_fun = func.partial(benchmark_util.rmse_relative, expected=trace)\n\n    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n    @func.partial(func.jit, static_argnums=0)\n    def matvec(num, key):\n        \"\"\"Matrix-vector mult.\"\"\"\n        return hutchinson.trace(\n            Av, key=key, sample_fun=sample_fun, num_samples_per_batch=num\n        )\n\n    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n    @func.partial(func.jit, static_argnums=0)\n    def slq_low(num, key):\n        \"\"\"SLQ(1)\"\"\"  # noqa: D400,D415\n        return slq.trace_of_matfun_spd(\n            matfun,\n            jvp,\n            1,\n            key=key,\n            sample_fun=sample_fun,\n            num_samples_per_batch=num,\n        )\n\n    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n    @func.partial(func.jit, static_argnums=0)\n    def slq_high(num, key):\n        \"\"\"SLQ(5)\"\"\"  # noqa: D400,D415\n        return slq.trace_of_matfun_spd(\n            matfun,\n            jvp,\n            5,\n            key=key,\n            sample_fun=sample_fun,\n            num_samples_per_batch=num,\n        )\n\n    test_keys = prng.split(k, num=num_restarts)\n    fig, axes = plt.subplots(ncols=2, tight_layout=True, figsize=(10, 5), dpi=100)\n    ax_wp, ax_sparsity = axes\n\n    ax_wp.set_title(\"Estimating trace(A^2)\")\n    for fun in [matvec, slq_low, slq_high]:\n        errors, stds, times = evaluate_all(fun, progressbar(num_samples), test_keys)\n        ax_wp.loglog(\n            times,\n            errors,\n            \"o-\",\n            markeredgecolor=\"k\",\n            label=fun.__doc__,\n        )\n        ax_wp.fill_between(times, errors - stds, errors + stds, alpha=0.25)\n\n    ax_wp.set_ylabel(f\"Relative error (mean & std, {num_restarts} restarts)\")\n    ax_wp.set_xlabel(f\"Wall clock time (minimum, {num_restarts} restarts)\")\n    ax_wp.legend()\n\n    ax_sparsity.set_title(\"Sparsity pattern of A^2\")\n    ax_sparsity.spy(JJ)\n    ax_sparsity.set_xticks(())\n    ax_sparsity.set_yticks(())\n    plt.show()", ""]}
{"filename": "matfree/test_util.py", "chunked_list": ["\"\"\"Test utilities.\"\"\"\n\nfrom matfree.backend import linalg, np\n\n\ndef symmetric_matrix_from_eigenvalues(eigvals, /):\n    \"\"\"Generate a symmetric matrix with prescribed eigenvalues.\"\"\"\n    assert np.array_min(eigvals) > 0\n    (n,) = eigvals.shape\n\n    # Need _some_ matrix to start with\n    A = np.reshape(np.arange(1.0, n**2 + 1.0), (n, n))\n    A = A / linalg.matrix_norm(A, which=\"fro\")\n    X = A.T @ A + np.eye(n)\n\n    # QR decompose. We need the orthogonal matrix.\n    # Treat Q as a stack of eigenvectors.\n    Q, R = linalg.qr(X)\n\n    # Treat Q as eigenvectors, and 'D' as eigenvalues.\n    # return Q D Q.T.\n    # This matrix will be dense, symmetric, and have a given spectrum.\n    return Q @ (eigvals[:, None] * Q.T)", "\n\ndef asymmetric_matrix_from_singular_values(vals, /, nrows, ncols):\n    \"\"\"Generate an asymmetric matrix with specific singular values.\"\"\"\n    assert np.array_min(vals) > 0\n    A = np.reshape(np.arange(1.0, nrows * ncols + 1.0), (nrows, ncols))\n    A /= nrows * ncols\n    U, S, Vt = linalg.svd(A, full_matrices=False)\n    return U @ linalg.diagonal(vals) @ Vt\n", ""]}
{"filename": "matfree/montecarlo.py", "chunked_list": ["\"\"\"Monte-Carlo estimation.\"\"\"\n\nfrom matfree.backend import containers, control_flow, func, np, prng\nfrom matfree.backend.typing import Array, Callable, Sequence\n\n# todo: allow a fun() that returns pytrees instead of arrays.\n#  why? Because then we rival trace_and_variance as\n#  trace_and_frobeniusnorm(): y=Ax; return (x@y, y@y)\n\n\ndef estimate(\n    fun: Callable,\n    /,\n    *,\n    key: Array,\n    sample_fun: Callable,\n    num_batches: int = 1,\n    num_samples_per_batch: int = 10_000,\n    statistic_batch: Callable = np.mean,\n    statistic_combine: Callable = np.mean,\n) -> Array:\n    \"\"\"Monte-Carlo estimation: Compute the expected value of a function.\n\n    Parameters\n    ----------\n    fun:\n        Function whose expected value shall be estimated.\n    key:\n        Pseudo-random number generator key.\n    sample_fun:\n        Sampling function.\n        For trace-estimation, use\n        either [montecarlo.normal(...)][matfree.montecarlo.normal]\n        or [montecarlo.rademacher(...)][matfree.montecarlo.normal].\n    num_batches:\n        Number of batches when computing arithmetic means.\n    num_samples_per_batch:\n        Number of samples per batch.\n    statistic_batch:\n        The summary statistic to compute on batch-level.\n        Usually, this is np.mean. But any other\n        statistical function with a signature like\n        [one of these functions](https://data-apis.org/array-api/2022.12/API_specification/statistical_functions.html)\n        would work.\n    statistic_combine:\n        The summary statistic to combine batch-results.\n        Usually, this is np.mean. But any other\n        statistical function with a signature like\n        [one of these functions](https://data-apis.org/array-api/2022.12/API_specification/statistical_functions.html)\n        would work.\n    \"\"\"\n    [result] = multiestimate(\n        fun,\n        key=key,\n        sample_fun=sample_fun,\n        num_batches=num_batches,\n        num_samples_per_batch=num_samples_per_batch,\n        statistics_batch=[statistic_batch],\n        statistics_combine=[statistic_combine],\n    )\n    return result", "\n\ndef estimate(\n    fun: Callable,\n    /,\n    *,\n    key: Array,\n    sample_fun: Callable,\n    num_batches: int = 1,\n    num_samples_per_batch: int = 10_000,\n    statistic_batch: Callable = np.mean,\n    statistic_combine: Callable = np.mean,\n) -> Array:\n    \"\"\"Monte-Carlo estimation: Compute the expected value of a function.\n\n    Parameters\n    ----------\n    fun:\n        Function whose expected value shall be estimated.\n    key:\n        Pseudo-random number generator key.\n    sample_fun:\n        Sampling function.\n        For trace-estimation, use\n        either [montecarlo.normal(...)][matfree.montecarlo.normal]\n        or [montecarlo.rademacher(...)][matfree.montecarlo.normal].\n    num_batches:\n        Number of batches when computing arithmetic means.\n    num_samples_per_batch:\n        Number of samples per batch.\n    statistic_batch:\n        The summary statistic to compute on batch-level.\n        Usually, this is np.mean. But any other\n        statistical function with a signature like\n        [one of these functions](https://data-apis.org/array-api/2022.12/API_specification/statistical_functions.html)\n        would work.\n    statistic_combine:\n        The summary statistic to combine batch-results.\n        Usually, this is np.mean. But any other\n        statistical function with a signature like\n        [one of these functions](https://data-apis.org/array-api/2022.12/API_specification/statistical_functions.html)\n        would work.\n    \"\"\"\n    [result] = multiestimate(\n        fun,\n        key=key,\n        sample_fun=sample_fun,\n        num_batches=num_batches,\n        num_samples_per_batch=num_samples_per_batch,\n        statistics_batch=[statistic_batch],\n        statistics_combine=[statistic_combine],\n    )\n    return result", "\n\ndef multiestimate(\n    fun: Callable,\n    /,\n    *,\n    key: Array,\n    sample_fun: Callable,\n    num_batches: int = 1,\n    num_samples_per_batch: int = 10_000,\n    statistics_batch: Sequence[Callable] = (np.mean,),\n    statistics_combine: Sequence[Callable] = (np.mean,),\n) -> Array:\n    \"\"\"Compute a Monte-Carlo estimate with multiple summary statistics.\n\n    The signature of this function is almost identical to\n    [montecarlo.estimate(...)][matfree.montecarlo.estimate].\n    The only difference is that statistics_batch and statistics_combine are iterables\n    of summary statistics (of equal lengths).\n\n    The result of this function is an iterable of matching length.\n\n    Parameters\n    ----------\n    fun:\n        Same as in [montecarlo.estimate(...)][matfree.montecarlo.estimate].\n    key:\n        Same as in [montecarlo.estimate(...)][matfree.montecarlo.estimate].\n    sample_fun:\n        Same as in [montecarlo.estimate(...)][matfree.montecarlo.estimate].\n    num_batches:\n        Same as in [montecarlo.estimate(...)][matfree.montecarlo.estimate].\n    num_samples_per_batch:\n        Same as in [montecarlo.estimate(...)][matfree.montecarlo.estimate].\n    statistics_batch:\n        List or tuple of summary statistics to compute on batch-level.\n    statistics_combine:\n        List or tuple of summary statistics to combine batches.\n\n    \"\"\"\n    assert len(statistics_batch) == len(statistics_combine)\n    fun_mc = _montecarlo(fun, sample_fun=sample_fun, num_stats=len(statistics_batch))\n    fun_single_batch = _stats_via_vmap(fun_mc, num_samples_per_batch, statistics_batch)\n    fun_batched = _stats_via_map(fun_single_batch, num_batches, statistics_combine)\n    return fun_batched(key)", "\n\ndef _montecarlo(f, /, sample_fun, num_stats):\n    \"\"\"Turn a function into a Monte-Carlo problem.\n\n    More specifically, f(x) becomes g(key) = f(h(key)),\n    using a sample function h: key -> x.\n    This can then be evaluated and averaged in batches, loops, and compositions thereof.\n    \"\"\"\n    # todo: what about randomised QMC? How do we best implement this?\n\n    def f_mc(key, /):\n        sample = sample_fun(key)\n        return [f(sample)] * num_stats\n\n    return f_mc", "\n\ndef _stats_via_vmap(f, num, /, statistics: Sequence[Callable]):\n    \"\"\"Compute summary statistics via jax.vmap.\"\"\"\n\n    def f_mean(key, /):\n        subkeys = prng.split(key, num)\n        fx_values = func.vmap(f)(subkeys)\n        return [stat(fx, axis=0) for stat, fx in zip(statistics, fx_values)]\n\n    return f_mean", "\n\ndef _stats_via_map(f, num, /, statistics: Sequence[Callable]):\n    \"\"\"Compute summary statistics via jax.lax.map.\"\"\"\n\n    def f_mean(key, /):\n        subkeys = prng.split(key, num)\n        fx_values = control_flow.array_map(f, subkeys)\n        return [stat(fx, axis=0) for stat, fx in zip(statistics, fx_values)]\n\n    return f_mean", "\n\ndef normal(*, shape, dtype=float):\n    \"\"\"Construct a function that samples from a standard normal distribution.\"\"\"\n\n    def fun(key):\n        return prng.normal(key, shape=shape, dtype=dtype)\n\n    return fun\n", "\n\ndef rademacher(*, shape, dtype=float):\n    \"\"\"Construct a function that samples from a Rademacher distribution.\"\"\"\n\n    def fun(key):\n        return prng.rademacher(key, shape=shape, dtype=dtype)\n\n    return fun\n", "\n\nclass _VDCState(containers.NamedTuple):\n    n: int\n    vdc: float\n    denom: int\n\n\ndef van_der_corput(n, /, base=2):\n    \"\"\"Compute the 'n'th element of the Van-der-Corput sequence.\"\"\"\n    state = _VDCState(n, vdc=0, denom=1)\n\n    vdc_modify = func.partial(_van_der_corput_modify, base=base)\n    state = control_flow.while_loop(_van_der_corput_cond, vdc_modify, state)\n    return state.vdc", "def van_der_corput(n, /, base=2):\n    \"\"\"Compute the 'n'th element of the Van-der-Corput sequence.\"\"\"\n    state = _VDCState(n, vdc=0, denom=1)\n\n    vdc_modify = func.partial(_van_der_corput_modify, base=base)\n    state = control_flow.while_loop(_van_der_corput_cond, vdc_modify, state)\n    return state.vdc\n\n\ndef _van_der_corput_cond(state: _VDCState):\n    return state.n > 0", "\ndef _van_der_corput_cond(state: _VDCState):\n    return state.n > 0\n\n\ndef _van_der_corput_modify(state: _VDCState, *, base):\n    denom = state.denom * base\n    num, remainder = divmod(state.n, base)\n    vdc = state.vdc + remainder / denom\n    return _VDCState(num, vdc, denom)", ""]}
{"filename": "matfree/hutchinson.py", "chunked_list": ["\"\"\"Hutchinson-style trace and diagonal estimation.\"\"\"\n\n\nfrom matfree import montecarlo\nfrom matfree.backend import containers, control_flow, func, linalg, np, prng\nfrom matfree.backend.typing import Any, Array, Callable, Sequence\n\n\ndef trace(Av: Callable, /, **kwargs) -> Array:\n    \"\"\"Estimate the trace of a matrix stochastically.\n\n    Parameters\n    ----------\n    Av:\n        Matrix-vector product function.\n    **kwargs:\n        Keyword-arguments to be passed to\n        [montecarlo.estimate()][matfree.montecarlo.estimate].\n    \"\"\"\n\n    def quadform(vec):\n        return linalg.vecdot(vec, Av(vec))\n\n    return montecarlo.estimate(quadform, **kwargs)", "def trace(Av: Callable, /, **kwargs) -> Array:\n    \"\"\"Estimate the trace of a matrix stochastically.\n\n    Parameters\n    ----------\n    Av:\n        Matrix-vector product function.\n    **kwargs:\n        Keyword-arguments to be passed to\n        [montecarlo.estimate()][matfree.montecarlo.estimate].\n    \"\"\"\n\n    def quadform(vec):\n        return linalg.vecdot(vec, Av(vec))\n\n    return montecarlo.estimate(quadform, **kwargs)", "\n\ndef trace_moments(Av: Callable, /, moments: Sequence[int] = (1, 2), **kwargs) -> Array:\n    \"\"\"Estimate the trace of a matrix and the variance of the estimator.\n\n    Parameters\n    ----------\n    Av:\n        Matrix-vector product function.\n    moments:\n        Which moments to compute. For example, selection `moments=(1,2)` computes\n        the first and second moment.\n    **kwargs:\n        Keyword-arguments to be passed to\n        [montecarlo.multiestimate(...)][matfree.montecarlo.multiestimate].\n    \"\"\"\n\n    def quadform(vec):\n        return linalg.vecdot(vec, Av(vec))\n\n    def moment(x, axis, *, power):\n        return np.mean(x**power, axis=axis)\n\n    statistics_batch = [func.partial(moment, power=m) for m in moments]\n    statistics_combine = [np.mean] * len(moments)\n    return montecarlo.multiestimate(\n        quadform,\n        statistics_batch=statistics_batch,\n        statistics_combine=statistics_combine,\n        **kwargs,\n    )", "\n\ndef frobeniusnorm_squared(Av: Callable, /, **kwargs) -> Array:\n    r\"\"\"Estimate the squared Frobenius norm of a matrix stochastically.\n\n    The Frobenius norm of a matrix $A$ is defined as\n\n    $$\n    \\|A\\|_F^2 = \\text{trace}(A^\\top A)\n    $$\n\n    so computing squared Frobenius norms amounts to trace estimation.\n\n    Parameters\n    ----------\n    Av:\n        Matrix-vector product function.\n    **kwargs:\n        Keyword-arguments to be passed to\n        [montecarlo.estimate()][matfree.montecarlo.estimate].\n\n    \"\"\"\n\n    def quadform(vec):\n        x = Av(vec)\n        return linalg.vecdot(x, x)\n\n    return montecarlo.estimate(quadform, **kwargs)", "\n\ndef diagonal_with_control_variate(Av: Callable, control: Array, /, **kwargs) -> Array:\n    \"\"\"Estimate the diagonal of a matrix stochastically and with a control variate.\n\n    Parameters\n    ----------\n    Av:\n        Matrix-vector product function.\n    control:\n        Control variate.\n        This should be the best-possible estimate of the diagonal of the matrix.\n    **kwargs:\n        Keyword-arguments to be passed to\n        [montecarlo.estimate()][matfree.montecarlo.estimate].\n\n    \"\"\"\n    return diagonal(lambda v: Av(v) - control * v, **kwargs) + control", "\n\ndef diagonal(Av: Callable, /, **kwargs) -> Array:\n    \"\"\"Estimate the diagonal of a matrix stochastically.\n\n    Parameters\n    ----------\n    Av:\n        Matrix-vector product function.\n    **kwargs:\n        Keyword-arguments to be passed to\n        [montecarlo.estimate()][matfree.montecarlo.estimate].\n\n    \"\"\"\n\n    def quadform(vec):\n        return vec * Av(vec)\n\n    return montecarlo.estimate(quadform, **kwargs)", "\n\ndef trace_and_diagonal(Av: Callable, /, *, sample_fun: Callable, key: Array, **kwargs):\n    \"\"\"Jointly estimate the trace and the diagonal stochastically.\n\n    The advantage of computing both quantities simultaneously is\n    that the diagonal estimate\n    may serve as a control variate for the trace estimate,\n    thus reducing the variance of the estimator\n    (and thereby accelerating convergence.)\n\n    Parameters\n    ----------\n    Av:\n        Matrix-vector product function.\n    sample_fun:\n        Sampling function.\n        Usually, either [montecarlo.normal][matfree.montecarlo.normal]\n        or [montecarlo.rademacher][matfree.montecarlo.normal].\n    key:\n        Pseudo-random number generator key.\n    **kwargs:\n        Keyword-arguments to be passed to\n        [diagonal_multilevel()][matfree.hutchinson.diagonal_multilevel].\n\n\n    See:\n    Adams et al., Estimating the Spectral Density of Large Implicit Matrices, 2018.\n    \"\"\"\n    fx_value = func.eval_shape(sample_fun, key)\n    init = np.zeros(shape=fx_value.shape, dtype=fx_value.dtype)\n    final = diagonal_multilevel(Av, init, sample_fun=sample_fun, key=key, **kwargs)\n    return np.sum(final), final", "\n\nclass _EstState(containers.NamedTuple):\n    diagonal_estimate: Any\n    key: Any\n\n\ndef diagonal_multilevel(\n    Av: Callable,\n    init: Array,\n    /,\n    *,\n    key: Array,\n    sample_fun: Callable,\n    num_levels: int,\n    num_batches_per_level: int = 1,\n    num_samples_per_batch: int = 1,\n) -> Array:\n    \"\"\"Estimate the diagonal in a multilevel framework.\n\n    The general idea is that a diagonal estimate serves as a control variate\n    for the next step's diagonal estimate.\n\n\n    Parameters\n    ----------\n    Av:\n        Matrix-vector product function.\n    init:\n        Initial guess.\n    key:\n        Pseudo-random number generator key.\n    sample_fun:\n        Sampling function.\n        Usually, either [montecarlo.normal][matfree.montecarlo.normal]\n        or [montecarlo.rademacher][matfree.montecarlo.normal].\n    num_levels:\n        Number of levels.\n    num_batches_per_level:\n        Number of batches per level.\n    num_samples_per_batch:\n        Number of samples per batch (per level).\n\n    \"\"\"\n    kwargs = {\n        \"sample_fun\": sample_fun,\n        \"num_batches\": num_batches_per_level,\n        \"num_samples_per_batch\": num_samples_per_batch,\n    }\n\n    def update_fun(level: int, x: _EstState) -> _EstState:\n        \"\"\"Update the diagonal estimate.\"\"\"\n        diag, k = x\n\n        _, subkey = prng.split(k, num=2)\n        update = diagonal_with_control_variate(Av, diag, key=subkey, **kwargs)\n\n        diag = _incr(diag, level, update)\n        return _EstState(diag, subkey)\n\n    state = _EstState(diagonal_estimate=init, key=key)\n    state = control_flow.fori_loop(0, num_levels, body_fun=update_fun, init_val=state)\n    (final, *_) = state\n    return final", "\n\ndef _incr(old, count, incoming):\n    return (old * count + incoming) / (count + 1)\n"]}
{"filename": "matfree/lanczos.py", "chunked_list": ["\"\"\"Lanczos-style algorithms.\"\"\"\n\nfrom matfree.backend import containers, control_flow, linalg, np\nfrom matfree.backend.typing import Array, Callable, Tuple\n\n\nclass _Alg(containers.NamedTuple):\n    \"\"\"Matrix decomposition algorithm.\"\"\"\n\n    init: Callable\n    \"\"\"Initialise the state of the algorithm. Usually, this involves pre-allocation.\"\"\"\n\n    step: Callable\n    \"\"\"Compute the next iteration.\"\"\"\n\n    extract: Callable\n    \"\"\"Extract the solution from the state of the algorithm.\"\"\"\n\n    lower_upper: Tuple[int, int]\n    \"\"\"Range of the for-loop used to decompose a matrix.\"\"\"", "\n\ndef tridiagonal_full_reortho(depth, /):\n    \"\"\"Construct an implementation of **tridiagonalisation**.\n\n    Uses pre-allocation. Fully reorthogonalise vectors at every step.\n\n    This algorithm assumes a **symmetric matrix**.\n\n    Decompose a matrix into a product of orthogonal-**tridiagonal**-orthogonal matrices.\n    Use this algorithm for approximate **eigenvalue** decompositions.\n\n    \"\"\"\n\n    class State(containers.NamedTuple):\n        i: int\n        basis: Array\n        tridiag: Tuple[Array, Array]\n        q: Array\n\n    def init(init_vec: Array) -> State:\n        (ncols,) = np.shape(init_vec)\n        if depth >= ncols or depth < 1:\n            raise ValueError\n\n        diag = np.zeros((depth + 1,))\n        offdiag = np.zeros((depth,))\n        basis = np.zeros((depth + 1, ncols))\n\n        return State(0, basis, (diag, offdiag), init_vec)\n\n    def apply(state: State, Av: Callable) -> State:\n        i, basis, (diag, offdiag), vec = state\n\n        # Re-orthogonalise against ALL basis elements before storing.\n        # Note: we re-orthogonalise against ALL columns of Q, not just\n        # the ones we have already computed. This increases the complexity\n        # of the whole iteration from n(n+1)/2 to n^2, but has the advantage\n        # that the whole computation has static bounds (thus we can JIT it all).\n        # Since 'Q' is padded with zeros, the numerical values are identical\n        # between both modes of computing.\n        vec, length = _normalise(vec)\n        vec, _ = _gram_schmidt_orthogonalise_set(vec, basis)\n\n        # I don't know why, but this re-normalisation is soooo crucial\n        vec, _ = _normalise(vec)\n        basis = basis.at[i, :].set(vec)\n\n        # When i==0, Q[i-1] is Q[-1] and again, we benefit from the fact\n        #  that Q is initialised with zeros.\n        vec = Av(vec)\n        basis_vectors_previous = np.asarray([basis[i], basis[i - 1]])\n        vec, (coeff, _) = _gram_schmidt_orthogonalise_set(vec, basis_vectors_previous)\n        diag = diag.at[i].set(coeff)\n        offdiag = offdiag.at[i - 1].set(length)\n\n        return State(i + 1, basis, (diag, offdiag), vec)\n\n    def extract(state: State, /):\n        _, basis, (diag, offdiag), _ = state\n        return basis, (diag, offdiag)\n\n    return _Alg(init=init, step=apply, extract=extract, lower_upper=(0, depth + 1))", "\n\ndef bidiagonal_full_reortho(depth, /, matrix_shape):\n    \"\"\"Construct an implementation of **bidiagonalisation**.\n\n    Uses pre-allocation. Fully reorthogonalise vectors at every step.\n\n    Works for **arbitrary matrices**. No symmetry required.\n\n    Decompose a matrix into a product of orthogonal-**bidiagonal**-orthogonal matrices.\n    Use this algorithm for approximate **singular value** decompositions.\n    \"\"\"\n    nrows, ncols = matrix_shape\n    max_depth = min(nrows, ncols) - 1\n    if depth > max_depth or depth < 0:\n        msg1 = f\"Depth {depth} exceeds the matrix' dimensions. \"\n        msg2 = f\"Expected: 0 <= depth <= min(nrows, ncols) - 1 = {max_depth} \"\n        msg3 = f\"for a matrix with shape {matrix_shape}.\"\n        raise ValueError(msg1 + msg2 + msg3)\n\n    class State(containers.NamedTuple):\n        i: int\n        Us: Array\n        Vs: Array\n        alphas: Array\n        betas: Array\n        beta: Array\n        vk: Array\n\n    def init(init_vec: Array) -> State:\n        nrows, ncols = matrix_shape\n        alphas = np.zeros((depth + 1,))\n        betas = np.zeros((depth + 1,))\n        Us = np.zeros((depth + 1, nrows))\n        Vs = np.zeros((depth + 1, ncols))\n        v0, _ = _normalise(init_vec)\n        return State(0, Us, Vs, alphas, betas, 0.0, v0)\n\n    def apply(state: State, Av: Callable, vA: Callable) -> State:\n        i, Us, Vs, alphas, betas, beta, vk = state\n        Vs = Vs.at[i].set(vk)\n        betas = betas.at[i].set(beta)\n\n        uk = Av(vk) - beta * Us[i - 1]\n        uk, alpha = _normalise(uk)\n        uk, _ = _gram_schmidt_orthogonalise_set(uk, Us)  # full reorthogonalisation\n        uk, _ = _normalise(uk)\n        Us = Us.at[i].set(uk)\n        alphas = alphas.at[i].set(alpha)\n\n        vk = vA(uk) - alpha * vk\n        vk, beta = _normalise(vk)\n        vk, _ = _gram_schmidt_orthogonalise_set(vk, Vs)  # full reorthogonalisation\n        vk, _ = _normalise(vk)\n\n        return State(i + 1, Us, Vs, alphas, betas, beta, vk)\n\n    def extract(state: State, /):\n        _, uk_all, vk_all, alphas, betas, beta, vk = state\n        return uk_all.T, (alphas, betas[1:]), vk_all, (beta, vk)\n\n    return _Alg(init=init, step=apply, extract=extract, lower_upper=(0, depth + 1))", "\n\ndef _normalise(vec):\n    length = linalg.vector_norm(vec)\n    return vec / length, length\n\n\ndef _gram_schmidt_orthogonalise_set(vec, vectors):  # Gram-Schmidt\n    vec, coeffs = control_flow.scan(_gram_schmidt_orthogonalise, vec, xs=vectors)\n    return vec, coeffs", "\n\ndef _gram_schmidt_orthogonalise(vec1, vec2):\n    coeff = linalg.vecdot(vec1, vec2)\n    vec_ortho = vec1 - coeff * vec2\n    return vec_ortho, coeff\n"]}
{"filename": "matfree/decomp.py", "chunked_list": ["\"\"\"Matrix decomposition algorithms.\"\"\"\n\nfrom matfree import lanczos\nfrom matfree.backend import containers, control_flow, linalg\nfrom matfree.backend.typing import Array, Callable, Tuple\n\n\ndef svd(\n    v0: Array, depth: int, Av: Callable, vA: Callable, matrix_shape: Tuple[int, ...]\n):\n    \"\"\"Approximate singular value decomposition.\n\n    Uses GKL with full reorthogonalisation to bi-diagonalise the target matrix\n    and computes the full SVD of the (small) bidiagonal matrix.\n\n    Parameters\n    ----------\n    v0:\n        Initial vector for Golub-Kahan-Lanczos bidiagonalisation.\n    depth:\n        Depth of the Krylov space constructed by Golub-Kahan-Lanczos bidiagonalisation.\n        Choosing `depth = min(nrows, ncols) - 1` would yield behaviour similar to\n        e.g. `np.linalg.svd`.\n    Av:\n        Matrix-vector product function.\n    vA:\n        Vector-matrix product function.\n    matrix_shape:\n        Shape of the matrix involved in matrix-vector and vector-matrix products.\n    \"\"\"\n    # Factorise the matrix\n    algorithm = lanczos.bidiagonal_full_reortho(depth, matrix_shape=matrix_shape)\n    u, (d, e), vt, _ = decompose_fori_loop(v0, Av, vA, algorithm=algorithm)\n\n    # Compute SVD of factorisation\n    B = _bidiagonal_dense(d, e)\n    U, S, Vt = linalg.svd(B, full_matrices=False)\n\n    # Combine orthogonal transformations\n    return u @ U, S, Vt @ vt", "\n\ndef _bidiagonal_dense(d, e):\n    diag = linalg.diagonal_matrix(d)\n    offdiag = linalg.diagonal_matrix(e, 1)\n    return diag + offdiag\n\n\nclass _DecompAlg(containers.NamedTuple):\n    \"\"\"Matrix decomposition algorithm.\"\"\"\n\n    init: Callable\n    \"\"\"Initialise the state of the algorithm. Usually, this involves pre-allocation.\"\"\"\n\n    step: Callable\n    \"\"\"Compute the next iteration.\"\"\"\n\n    extract: Callable\n    \"\"\"Extract the solution from the state of the algorithm.\"\"\"\n\n    lower_upper: Tuple[int, int]\n    \"\"\"Range of the for-loop used to decompose a matrix.\"\"\"", "class _DecompAlg(containers.NamedTuple):\n    \"\"\"Matrix decomposition algorithm.\"\"\"\n\n    init: Callable\n    \"\"\"Initialise the state of the algorithm. Usually, this involves pre-allocation.\"\"\"\n\n    step: Callable\n    \"\"\"Compute the next iteration.\"\"\"\n\n    extract: Callable\n    \"\"\"Extract the solution from the state of the algorithm.\"\"\"\n\n    lower_upper: Tuple[int, int]\n    \"\"\"Range of the for-loop used to decompose a matrix.\"\"\"", "\n\nAlgorithmType = Tuple[Callable, Callable, Callable, Tuple[int, int]]\n\"\"\"Decomposition algorithm type.\n\nFor example, the output of\n[matfree.lanczos.tridiagonal_full_reortho(...)][matfree.lanczos.tridiagonal_full_reortho].\n\"\"\"\n\n", "\n\n# all arguments are positional-only because we will rename arguments a lot\ndef decompose_fori_loop(v0, *matvec_funs, algorithm: AlgorithmType):\n    r\"\"\"Decompose a matrix purely based on matvec-products with A.\n\n    The behaviour of this function is equivalent to\n\n    ```python\n    def decompose(v0, *matvec_funs, algorithm):\n        init, step, extract, (lower, upper) = algorithm\n        state = init(v0)\n        for _ in range(lower, upper):\n            state = step(state, *matvec_funs)\n        return extract(state)\n    ```\n\n    but the implementation uses JAX' fori_loop.\n    \"\"\"\n    # todo: turn the \"practically equivalent\" bit above into a doctest.\n    init, step, extract, (lower, upper) = algorithm\n    init_val = init(v0)\n\n    def body_fun(_, s):\n        return step(s, *matvec_funs)\n\n    result = control_flow.fori_loop(lower, upper, body_fun=body_fun, init_val=init_val)\n    return extract(result)", ""]}
{"filename": "matfree/__init__.py", "chunked_list": ["\"\"\"Matrix-free linear algebra.\"\"\"\n"]}
{"filename": "matfree/benchmark_util.py", "chunked_list": ["\"\"\"Benchmark utilities.\"\"\"\n\nfrom matfree.backend import func, linalg, np, time\n\n\ndef rmse_relative(received, *, expected):\n    \"\"\"Compute the relative root-mean-square error.\"\"\"\n    return linalg.vector_norm((received - expected) / expected) / np.sqrt(expected.size)\n\n\ndef error_and_time(fun, error_fun):\n    \"\"\"Compute error and runtime of a function with a single outputs.\"\"\"\n\n    @func.wraps(fun)\n    def fun_wrapped(*args, **kwargs):\n        # Execute once for compilation\n        _ = fun(*args, **kwargs)\n\n        # Execute and time\n        t0 = time.perf_counter()\n        result = fun(*args, **kwargs)\n        result.block_until_ready()\n        t1 = time.perf_counter()\n        return error_fun(result), (t1 - t0)\n\n    return fun_wrapped", "\n\ndef error_and_time(fun, error_fun):\n    \"\"\"Compute error and runtime of a function with a single outputs.\"\"\"\n\n    @func.wraps(fun)\n    def fun_wrapped(*args, **kwargs):\n        # Execute once for compilation\n        _ = fun(*args, **kwargs)\n\n        # Execute and time\n        t0 = time.perf_counter()\n        result = fun(*args, **kwargs)\n        result.block_until_ready()\n        t1 = time.perf_counter()\n        return error_fun(result), (t1 - t0)\n\n    return fun_wrapped", ""]}
{"filename": "matfree/slq.py", "chunked_list": ["\"\"\"Stochastic Lanczos quadrature.\"\"\"\n\nfrom matfree import decomp, lanczos, montecarlo\nfrom matfree.backend import func, linalg, np\n\n\ndef logdet_spd(*args, **kwargs):\n    \"\"\"Estimate the log-determinant of a symmetric, positive definite matrix.\"\"\"\n    return trace_of_matfun_spd(np.log, *args, **kwargs)\n", "\n\ndef trace_of_matfun_spd(matfun, order, Av, /, **kwargs):\n    \"\"\"Compute the trace of the function of a symmetric matrix.\"\"\"\n    quadratic_form = _quadratic_form_slq_spd(matfun, order, Av)\n    return montecarlo.estimate(quadratic_form, **kwargs)\n\n\ndef _quadratic_form_slq_spd(matfun, order, Av, /):\n    \"\"\"Quadratic form for stochastic Lanczos quadrature.\n\n    Assumes a symmetric, positive definite matrix.\n    \"\"\"\n\n    def quadform(v0, /):\n        algorithm = lanczos.tridiagonal_full_reortho(order)\n        _, tridiag = decomp.decompose_fori_loop(v0, Av, algorithm=algorithm)\n        (diag, off_diag) = tridiag\n\n        # todo: once jax supports eigh_tridiagonal(eigvals_only=False),\n        #  use it here. Until then: an eigen-decomposition of size (order + 1)\n        #  does not hurt too much...\n        diag = linalg.diagonal_matrix(diag)\n        offdiag1 = linalg.diagonal_matrix(off_diag, -1)\n        offdiag2 = linalg.diagonal_matrix(off_diag, 1)\n        dense_matrix = diag + offdiag1 + offdiag2\n        eigvals, eigvecs = linalg.eigh(dense_matrix)\n\n        # Since Q orthogonal (orthonormal) to v0, Q v = Q[0],\n        # and therefore (Q v)^T f(D) (Qv) = Q[0] * f(diag) * Q[0]\n        (dim,) = v0.shape\n\n        fx_eigvals = func.vmap(matfun)(eigvals)\n        return dim * linalg.vecdot(eigvecs[0, :], fx_eigvals * eigvecs[0, :])\n\n    return quadform", "def _quadratic_form_slq_spd(matfun, order, Av, /):\n    \"\"\"Quadratic form for stochastic Lanczos quadrature.\n\n    Assumes a symmetric, positive definite matrix.\n    \"\"\"\n\n    def quadform(v0, /):\n        algorithm = lanczos.tridiagonal_full_reortho(order)\n        _, tridiag = decomp.decompose_fori_loop(v0, Av, algorithm=algorithm)\n        (diag, off_diag) = tridiag\n\n        # todo: once jax supports eigh_tridiagonal(eigvals_only=False),\n        #  use it here. Until then: an eigen-decomposition of size (order + 1)\n        #  does not hurt too much...\n        diag = linalg.diagonal_matrix(diag)\n        offdiag1 = linalg.diagonal_matrix(off_diag, -1)\n        offdiag2 = linalg.diagonal_matrix(off_diag, 1)\n        dense_matrix = diag + offdiag1 + offdiag2\n        eigvals, eigvecs = linalg.eigh(dense_matrix)\n\n        # Since Q orthogonal (orthonormal) to v0, Q v = Q[0],\n        # and therefore (Q v)^T f(D) (Qv) = Q[0] * f(diag) * Q[0]\n        (dim,) = v0.shape\n\n        fx_eigvals = func.vmap(matfun)(eigvals)\n        return dim * linalg.vecdot(eigvecs[0, :], fx_eigvals * eigvecs[0, :])\n\n    return quadform", "\n\ndef logdet_product(*args, **kwargs):\n    r\"\"\"Compute the log-determinant of a product of matrices.\n\n    Here, \"product\" refers to $X = A^\\top A$.\n    \"\"\"\n    return trace_of_matfun_product(np.log, *args, **kwargs)\n\n\ndef schatten_norm(*args, power, **kwargs):\n    r\"\"\"Compute the Schatten-p norm of a matrix via stochastic Lanczos quadrature.\"\"\"\n\n    def matfun(x):\n        \"\"\"Matrix-function for Schatten-p norms.\"\"\"\n        return x ** (power / 2)\n\n    trace = trace_of_matfun_product(matfun, *args, **kwargs)\n    return trace ** (1 / power)", "\n\ndef schatten_norm(*args, power, **kwargs):\n    r\"\"\"Compute the Schatten-p norm of a matrix via stochastic Lanczos quadrature.\"\"\"\n\n    def matfun(x):\n        \"\"\"Matrix-function for Schatten-p norms.\"\"\"\n        return x ** (power / 2)\n\n    trace = trace_of_matfun_product(matfun, *args, **kwargs)\n    return trace ** (1 / power)", "\n\ndef trace_of_matfun_product(matfun, order, *matvec_funs, matrix_shape, **kwargs):\n    r\"\"\"Compute the trace of a function of a product of matrices.\n\n    Here, \"product\" refers to $X = A^\\top A$.\n    \"\"\"\n    quadratic_form = _quadratic_form_slq_product(\n        matfun, order, *matvec_funs, matrix_shape=matrix_shape\n    )\n    return montecarlo.estimate(quadratic_form, **kwargs)", "\n\ndef _quadratic_form_slq_product(matfun, depth, *matvec_funs, matrix_shape):\n    r\"\"\"Quadratic form for stochastic Lanczos quadrature.\n\n    Instead of the trace of a function of a matrix,\n    compute the trace of a function of the product of matrices.\n    Here, \"product\" refers to $X = A^\\top A$.\n    \"\"\"\n\n    def quadform(v0, /):\n        # Decompose into orthogonal-bidiag-orthogonal\n        algorithm = lanczos.bidiagonal_full_reortho(depth, matrix_shape=matrix_shape)\n        output = decomp.decompose_fori_loop(v0, *matvec_funs, algorithm=algorithm)\n        u, (d, e), vt, _ = output\n\n        # Compute SVD of factorisation\n        B = _bidiagonal_dense(d, e)\n        _, S, Vt = linalg.svd(B, full_matrices=False)\n\n        # Since Q orthogonal (orthonormal) to v0, Q v = Q[0],\n        # and therefore (Q v)^T f(D) (Qv) = Q[0] * f(diag) * Q[0]\n        _, ncols = matrix_shape\n        eigvals, eigvecs = S**2, Vt.T\n        fx_eigvals = func.vmap(matfun)(eigvals)\n        return ncols * linalg.vecdot(eigvecs[0, :], fx_eigvals * eigvecs[0, :])\n\n    return quadform", "\n\ndef _bidiagonal_dense(d, e):\n    diag = linalg.diagonal_matrix(d)\n    offdiag = linalg.diagonal_matrix(e, 1)\n    return diag + offdiag\n"]}
{"filename": "matfree/backend/np.py", "chunked_list": ["\"\"\"NumPy-style API.\"\"\"\n\n# In here, we loosely follow the Array API:\n#\n# https://data-apis.org/array-api/2022.12/\n#\n# But deviate in a few points:\n# * The functions here do not have all the arguments specified in the API\n#   (we only wrap the arguments we need)\n# * Our current version of diag/diagonal is slightly different", "#   (we only wrap the arguments we need)\n# * Our current version of diag/diagonal is slightly different\n# * We do not use methods on Array types, e.g. shape(), dtype(). Instead\n#   these are functions. (Not all backends might always follow this method interface.)\n# * We do not implement any constants (e.g. NaN, Pi). Instead, these are methods.\n# * We call max/min/amax/amin array_max and elementwise_max.\n#   This is more verbose than what the array API suggests.\n\nimport jax.numpy as jnp\n", "import jax.numpy as jnp\n\n# Creation functions:\n\n\ndef arange(start, /, stop=None, step=1):\n    return jnp.arange(start, stop, step)\n\n\ndef asarray(obj, /):\n    return jnp.asarray(obj)", "\ndef asarray(obj, /):\n    return jnp.asarray(obj)\n\n\ndef eye(n_rows):\n    return jnp.eye(n_rows)\n\n\ndef ones_like(x, /):\n    return jnp.ones_like(x)", "\ndef ones_like(x, /):\n    return jnp.ones_like(x)\n\n\ndef zeros(shape, *, dtype=None):\n    return jnp.zeros(shape, dtype=dtype)\n\n\ndef ones(shape, *, dtype=None):\n    return jnp.ones(shape, dtype=dtype)", "\ndef ones(shape, *, dtype=None):\n    return jnp.ones(shape, dtype=dtype)\n\n\n# Element-wise functions\n\n\ndef abs(x, /):  # noqa: A001\n    return jnp.abs(x)", "def abs(x, /):  # noqa: A001\n    return jnp.abs(x)\n\n\ndef log(x, /):\n    return jnp.log(x)\n\n\ndef isnan(x, /):\n    return jnp.isnan(x)", "def isnan(x, /):\n    return jnp.isnan(x)\n\n\ndef sin(x, /):\n    return jnp.sin(x)\n\n\ndef cos(x, /):\n    return jnp.cos(x)", "def cos(x, /):\n    return jnp.cos(x)\n\n\ndef sqrt(x, /):\n    return jnp.sqrt(x)\n\n\ndef sign(x, /):\n    return jnp.sign(x)", "def sign(x, /):\n    return jnp.sign(x)\n\n\n# Utility functions\n\n\ndef any(x, /):  # noqa: A001\n    return jnp.any(x)\n", "\n\ndef allclose(x1, x2, /, *, rtol=1e-5, atol=1e-8):\n    return jnp.allclose(x1, x2, rtol=rtol, atol=atol)\n\n\n# Statistical functions\n\n\ndef mean(x, /, axis=None):\n    return jnp.mean(x, axis)", "\ndef mean(x, /, axis=None):\n    return jnp.mean(x, axis)\n\n\ndef std(x, /, axis=None):\n    return jnp.std(x, axis)\n\n\ndef sum(x, /, axis=None):  # noqa: A001\n    return jnp.sum(x, axis)", "\ndef sum(x, /, axis=None):  # noqa: A001\n    return jnp.sum(x, axis)\n\n\ndef array_min(x, /):\n    return jnp.amin(x)\n\n\ndef array_max(x, /, axis=None):\n    return jnp.amax(x, axis=axis)", "\ndef array_max(x, /, axis=None):\n    return jnp.amax(x, axis=axis)\n\n\ndef elementwise_max(x1, x2, /):\n    return jnp.maximum(x1, x2)\n\n\ndef nanmean(x, /, axis=None):\n    return jnp.nanmean(x, axis)", "\ndef nanmean(x, /, axis=None):\n    return jnp.nanmean(x, axis)\n\n\n# Searching functions\n\n\ndef where(condition, x1, x2, /):\n    return jnp.where(condition, x1, x2)", "def where(condition, x1, x2, /):\n    return jnp.where(condition, x1, x2)\n\n\n# Manipulation functions\n\n\ndef reshape(x, /, shape):\n    return jnp.reshape(x, shape)\n", "\n\ndef flip(x, /):\n    return jnp.flip(x)\n\n\ndef roll(x, /, shift):\n    return jnp.roll(x, shift)\n\n", "\n\n# Functional implementation of what are usually array-methods\n\n\ndef shape(x, /):\n    return jnp.shape(x)\n\n\ndef dtype(x, /):\n    return jnp.dtype(x)", "\ndef dtype(x, /):\n    return jnp.dtype(x)\n\n\n# Functional implementation of constants\n\n\ndef nan():\n    return jnp.nan", "def nan():\n    return jnp.nan\n\n\n# Others\n\n\ndef convolve(a, b, /, mode=\"full\"):\n    return jnp.convolve(a, b, mode=mode)\n", ""]}
{"filename": "matfree/backend/prng.py", "chunked_list": ["\"\"\"Pseudo-random-number utilities.\"\"\"\n\n\nimport jax.random\n\n\ndef prng_key(seed):\n    return jax.random.PRNGKey(seed=seed)\n\n\ndef split(key, num=2):\n    return jax.random.split(key, num=num)", "\n\ndef split(key, num=2):\n    return jax.random.split(key, num=num)\n\n\ndef normal(key, *, shape, dtype=None):\n    if dtype is None:\n        return jax.random.normal(key, shape=shape)\n    return jax.random.normal(key, shape=shape, dtype=dtype)", "\n\ndef uniform(key, *, shape, dtype=None):\n    if dtype is None:\n        return jax.random.uniform(key, shape=shape)\n    return jax.random.uniform(key, shape=shape, dtype=dtype)\n\n\ndef rademacher(key, *, shape, dtype=None):\n    if dtype is None:\n        return jax.random.rademacher(key, shape=shape)\n    return jax.random.rademacher(key, shape=shape, dtype=dtype)", "def rademacher(key, *, shape, dtype=None):\n    if dtype is None:\n        return jax.random.rademacher(key, shape=shape)\n    return jax.random.rademacher(key, shape=shape, dtype=dtype)\n"]}
{"filename": "matfree/backend/plt.py", "chunked_list": ["\"\"\"Plotting functionality.\"\"\"\n\n# Not part of dependencies because not used by default, only for debugging.\nimport matplotlib.pyplot  # noqa: ICN001\n\n\ndef subplots(nrows=1, ncols=1, *, figsize, tight_layout=True, dpi=100):\n    return matplotlib.pyplot.subplots(\n        nrows=nrows, ncols=ncols, figsize=figsize, tight_layout=tight_layout, dpi=dpi\n    )", "\n\ndef show():\n    return matplotlib.pyplot.show()\n"]}
{"filename": "matfree/backend/progressbar.py", "chunked_list": ["\"\"\"Progress bars.\"\"\"\n\nimport tqdm\n\n\ndef progressbar(x, /):\n    return tqdm.tqdm(x)\n"]}
{"filename": "matfree/backend/control_flow.py", "chunked_list": ["\"\"\"Control flow.\"\"\"\n\nimport jax\n\n# API follows JAX, but we liberally work with positional- and keyword-only arguments\n# We also rename some arguments for improved consistency:\n# For example, we always use 'body_fun' and 'init_val',\n#  even though jax.lax.scan uses 'f' and 'init'.\n\n\ndef scan(body_fun, init_val, /, xs, *, reverse=False):\n    return jax.lax.scan(body_fun, init=init_val, xs=xs, reverse=reverse)", "\n\ndef scan(body_fun, init_val, /, xs, *, reverse=False):\n    return jax.lax.scan(body_fun, init=init_val, xs=xs, reverse=reverse)\n\n\ndef cond(pred, /, true_fun, false_fun, *operands):\n    return jax.lax.cond(pred, true_fun, false_fun, *operands)\n\n\ndef fori_loop(lower, upper, body_fun, init_val):\n    return jax.lax.fori_loop(lower, upper, body_fun, init_val)", "\n\ndef fori_loop(lower, upper, body_fun, init_val):\n    return jax.lax.fori_loop(lower, upper, body_fun, init_val)\n\n\ndef while_loop(cond_fun, body_fun, init_val):\n    return jax.lax.while_loop(cond_fun, body_fun, init_val)\n\n\ndef array_map(fun, /, xs):\n    return jax.lax.map(fun, xs)", "\n\ndef array_map(fun, /, xs):\n    return jax.lax.map(fun, xs)\n"]}
{"filename": "matfree/backend/time.py", "chunked_list": ["\"\"\"Timing.\"\"\"\n\nimport time\n\n\ndef perf_counter():\n    return time.perf_counter()\n"]}
{"filename": "matfree/backend/testing.py", "chunked_list": ["\"\"\"Test-utilities.\"\"\"\n\nimport jax.test_util\nimport pytest\nimport pytest_cases\n\n\ndef fixture(name=None):\n    return pytest_cases.fixture(name=name)\n", "\n\ndef parametrize(argnames, argvalues, /):\n    return pytest.mark.parametrize(argnames, argvalues)\n\n\ndef check_grads(fun, /, args, *, order, atol, rtol):\n    return jax.test_util.check_grads(fun, args, order=order, atol=atol, rtol=rtol)\n\n\ndef raises(err, /):\n    return pytest.raises(err)", "\n\ndef raises(err, /):\n    return pytest.raises(err)\n"]}
{"filename": "matfree/backend/__init__.py", "chunked_list": ["\"\"\"MatFree backend.\"\"\"\n"]}
{"filename": "matfree/backend/containers.py", "chunked_list": ["\"\"\"Container types.\"\"\"\n\n\nfrom typing import NamedTuple  # noqa: F401\n"]}
{"filename": "matfree/backend/typing.py", "chunked_list": ["\"\"\"Types.\"\"\"\n# fmt: off\nfrom collections.abc import Callable  # noqa: F401\nfrom typing import (Any, Generic, Iterable, Sequence,  # noqa: F401, UP035\n                    Tuple, TypeVar)\n\nfrom jax import Array  # noqa: F401\n\n# fmt: on\n", "# fmt: on\n"]}
{"filename": "matfree/backend/linalg.py", "chunked_list": ["\"\"\"Numerical linear algebra.\"\"\"\n\nimport jax.numpy as jnp\n\n\ndef vector_norm(x, /):\n    return jnp.linalg.norm(x)\n\n\ndef matrix_norm(x, /, which):\n    return jnp.linalg.norm(x, ord=which)", "\ndef matrix_norm(x, /, which):\n    return jnp.linalg.norm(x, ord=which)\n\n\ndef qr(x, /, *, mode=\"reduced\"):\n    return jnp.linalg.qr(x, mode=mode)\n\n\ndef eigh(x, /):\n    return jnp.linalg.eigh(x)", "\ndef eigh(x, /):\n    return jnp.linalg.eigh(x)\n\n\ndef slogdet(x, /):\n    return jnp.linalg.slogdet(x)\n\n\ndef vecdot(x1, x2, /):\n    return jnp.dot(x1, x2)", "\ndef vecdot(x1, x2, /):\n    return jnp.dot(x1, x2)\n\n\ndef diagonal(x, /, offset=0):\n    \"\"\"Extract the diagonal of a matrix.\"\"\"\n    return jnp.diag(x, offset)\n\n\ndef diagonal_matrix(x, /, offset=0):  # not part of array API\n    \"\"\"Construct a diagonal matrix.\"\"\"\n    return jnp.diag(x, offset)", "\n\ndef diagonal_matrix(x, /, offset=0):  # not part of array API\n    \"\"\"Construct a diagonal matrix.\"\"\"\n    return jnp.diag(x, offset)\n\n\ndef trace(x, /):\n    return jnp.trace(x)\n", "\n\ndef svd(A, /, *, full_matrices=True):\n    return jnp.linalg.svd(A, full_matrices=full_matrices)\n"]}
{"filename": "matfree/backend/func.py", "chunked_list": ["\"\"\"Function transformations (algorithmic differentiation, vmap, partial, and so on).\"\"\"\n\n# API-wise, we tend to follow JAX and functools.\n# But we only implement those function arguments that we need.\n# For example, algorithmic differentiation functions do not offer a 'has_aux' argument\n# at the moment, because we don't need it.\n\nimport functools\n\nimport jax", "\nimport jax\n\n# Vectorisation\n\n\ndef vmap(fun, /, in_axes=0, out_axes=0):\n    return jax.vmap(fun, in_axes=in_axes, out_axes=out_axes)\n\n", "\n\n# Partial and the like\n\n\ndef partial(func, /, *args, **kwargs):\n    return functools.partial(func, *args, **kwargs)\n\n\ndef wraps(func, /):\n    return functools.wraps(func)", "\ndef wraps(func, /):\n    return functools.wraps(func)\n\n\n# Algorithmic differentiation\n\n\ndef linearize(func, /, *args):\n    return jax.linearize(func, *args)", "def linearize(func, /, *args):\n    return jax.linearize(func, *args)\n\n\ndef jacfwd(fun, /, argnums=0):\n    return jax.jacfwd(fun, argnums)\n\n\n# Inferring input and output shapes:\n", "# Inferring input and output shapes:\n\n\ndef eval_shape(func, /, *args, **kwargs):\n    return jax.eval_shape(func, *args, **kwargs)\n\n\n# Compilation (don't use in source!)\n\n\ndef jit(fun, /, *, static_argnums=None, static_argnames=None):\n    return jax.jit(fun, static_argnames=static_argnames, static_argnums=static_argnums)", "\n\ndef jit(fun, /, *, static_argnums=None, static_argnames=None):\n    return jax.jit(fun, static_argnames=static_argnames, static_argnums=static_argnums)\n"]}
